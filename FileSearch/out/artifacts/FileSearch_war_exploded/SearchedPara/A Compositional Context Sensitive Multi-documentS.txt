 The usual approach for automatic summarization is sen-tence extraction, where key sentences from the input docu-ments are selected based on a suite of features. While word frequency often is used as a feature in summarization, its impact on system performance has not been isolated. In this paper, we study the contribution to summarization of three factors related to frequency: content word frequency, composition functions for estimating sentence importance from word frequency, and adjustment of frequency weights based on context. We carry out our analysis using datasets from the Document Understanding Conferences, studying not only the impact of these features on automatic summa-rizers, but also their role in human summarization. Our re-search shows that a frequency based summarizer can achieve performance comparable to that of state-of-the-art systems, but only with a good composition function; context sensi-tivity improves performance and significantly reduces repe-tition.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Measurement, Experimentation, Human Factors multi-document summarization, frequency, compositional-ity, context-sensitivity
Most current automatic summarization systems rely on sentence extraction 1 , where key sentences in the input docu-ments are selected to form the summary. Even systems that A description of some most recent systems can be found Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. go beyond sentence extraction, reformulating or simplifying the text of the original articles, must decide which sentences should be simplified, compressed, fused together or rewritten [10, 11, 28, 2, 6]. Common approaches for identifying im-portant sentences to include in the summary include train-ing a binary classifier (e.g., [12]), training a Markov model (e.g., [4]), or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (e.g., [26, 14]). But the question of which com-ponents and features of automatic summarizers contribute most to their performance has largely remained unanswered [18]. In this paper, we examine several design decisions and the impact they have on the performance of generic multi-document summarizers of news. More specifically, we study the following issues:
Content word frequency. Word frequency is one fea-ture that has been used in many summarization systems and originated in the earliest summarization research [17]. In this approach, content words such as nouns, verbs and ad-jectives serve as surrogates for the atomic units of meaning in text. While frequency has been used as a feature in many summarization systems, no study has isolated its impact on system performance. Only recently have large testsets for evaluation become available as a result of the annual Docu-ment Understanding Conference (DUC) run by NIST, which enable analysis of performance, and by the time DUC began, most systems were using a combination of features and not frequency alone. In this paper, we study the contribution of content word frequency in the input to system performance, showing that content word frequency also plays a role in human summarization behavior.

Choice of composition function. The frequency, and thus the importance, of content words can easily be esti-mated from the input to a summarizer. But is this enough to build a summarization syst em? Normally, a summarizer produces readable text as a summary, not a list of keywords, and thus it must estimate the importance of larger text units, typically sentences. A composition function needs to be chosen that will estimate the importance of a sen-tence as a function of the importance of the content words that appear in the sentence. There are many possibilities for the choice of composition function, and in Section 3 we will discuss three of them, showing that the choice can have a significant impact on the performance of the summarizer, ranging from close to baseline performance to overall state-of-the-art performance. in the online proceedings of the Document Understanding Conference http://duc.nist.gov
Context sensitivity. The notion of importance is not static: it depends on what has been already said in a sum-mary. Context adjustment is especially important for multi-document summarization (MDS), where the input consists of many articles on the same topic. Several articles might contain sentences expressing the same information. It is possible that they all get high importance weights and the summary will contain repetitive information. Avoiding rep-etition in the summary is a goal in summarization systems, since the very purpose of the summary is to reduce redun-dancy. We propose a method for context sensitivity and pinpoint its contribution to multi-document summarization performance. In Section 4 we show how context sensitivity adjustment improves content selection and reduces repeti-tion in the summary.

We now proceed to a detailed discussion of these three aspects in the following sections.
One of the issues studied since the inception of automatic summarization in the 60s is that of human agreement [24]: different people choose different content for their summaries [27, 23, 19]. More recently, others have studied the degree of overlap between input documents and human summaries [5, 1]. The natural question that arises if we combine the two types of studies is whether features in the input can al-low us to predict what content humans would choose in a summary, and what content they would agree on. If such predictors are identified, they could be used as features for content selection by an automatic system. In this section, we focus on frequency, investigating the association between content that appears frequently in the input, and the like-lihood that it will be selected by a human summarizer for inclusion in a summary. This question is especially impor-tant for the multi-document summarization task, where the input consists of several articles on the same topic and usu-ally contains a considerable amount of repetition of the same facts across documents. We first discuss the link between frequency in the input at the word level and the appearance of words in human summaries (Section 2.1), and then look at frequency at a semantic level, using manually identified semantic content units (Section 2.2).
In order to study how frequency influences human sum-marization choices, we used the 30 test sets for the multi-document summarization task from the large-scale common data set evaluation conduc ted within the DUC 2003. For each set, the input for summarization was available, along with four human abstracts for the input and the summaries produced by automatic summarizers that participated in the conference that year. Each of the inputs contained around 10 documents and the summaries were 100 words long. The counts for frequency in the input were taken over the con-catenation of the documents in the input set.

The following instructions had been given to the human summarizers:  X  X o write this summary, assume you have been given a set of stories on a news topic and that your job is to summarize them for the general news sections of the Washington Post. Your audience is the educated adult American reader with varied interestes and background in current and recent events. X 
We first turn to the question Are content words that are very frequent in the input likely to appear in at least one of the human summaries? We exclude stop words from consid-eration in this study, and use only nouns, verbs and adjec-tives. Table 1 shows the percentage of the N most frequent content words from the input documents that also appear in the human models, for N =5 , 8 , 12. In order to com-pare how many of these matches are achieved by a good automatic summarizer, we picked one of the top perform-ing summarizers and computed how many of the N most frequent words from the input documents appeared in its automatic summaries, and the numbers are shown in the second row of table 1. For example, the table shows that, across the 30 sets, 95% of the five most frequent content words in the input were also used in at least one of the sum-maries, while the automatic summarizer used only 84% (first column of table 1).

A comparison of this nature is helpful because in the commonly used intrinsic evaluations for summarization (dis-cussed in more detail in Section 5), automatic summaries are evaluated by measuring their overlap with multiple human summaries/models.
 Table 1: Percentage of the N most frequent words from the input documents that appear in the four human models and in a state-of-the-art automatic summarizer (average across 30 input sets).

Two observations can be made about the table: 1. The high frequency words from the input are very 2. For the automatic summarizer, the trend to include
Even though no rigorous study of the issue has been done, it Table 2: C i (the first column) is the class of words that appear in i human summaries, Average | C i | (the second column) is the average size of class C i , and the third column gives the average frequency of words in each class. The averages are computed for the 30 DUC X 03 test sets.
In the previous section we observed that the high fre-quency words in the input will tend to appear in some hu-man model. But will high frequency words be words that the humans will agree on, and that will appear in many human summaries? In other words, we want to partition the words in the input into five classes C n depending on how many human summaries they appear in, n =0 ... 4, and check if high class number is associated with higher frequency in the input for the words in the class. A word falls in C 0 if it does not appear in any of the human summaries, in C 1 if it appears in only one human summary and so on. Now we are interested to see how frequent the words in each class were in the respective input.

We found that, in fact, the words that human summarizers agreed to use in their summaries include the high frequency ones and the words that appear in only one human summary tend to be low frequency words as can be seen in table 2. The content words that were used by all four summarizers (in class C 4 ) had average frequency in the input equal to 31, while the words that never appeared in a human summary appeared on average about two times in the entire input of ten articles.

In the 30 sets of DUC 2003 data, the state-of-the-art ma-chine summary contained 69% of the words appearing in all 4 human models and 46% of the words that appeared in 3 models. This indicates that high-frequency words, which human summarizers will tend to select and thus will be re-warded for example during automatic evaluation, are miss-ing from the summary.
The findings from the previous sections suggest that fre-quency in the inputs is strongly indicative of whether a word will be used in a human summary. We start out with assess-ing the plausibility of a formal method capturing the relation between the occurrence of content words in the input and in summaries by modeling the appearance of words in the summary under a multinomial distribution estimated from the input. That is, for each word w in the input vocabulary, we associate a probability p ( w ) for it to be emitted into a can be considered that the content words that do not match any of the models describe  X  X ff-topic X  events. This is consis-tent with the results from the quality evaluation of machine summaries in which human judges perceived more than half of the summary content to be  X  X nnecessary, distracting or confusing. X  Table 3: Average log-likelihood for the summaries of human and automatic summarizers in DUC X 03. All summaries were truncated to 80 words to neutralize the effect of deviations from the required length of 100 words summary. It is obvious that words with high frequency in the input will be assigned hi gh emission probabilities. The likelihood of a summary then is where N is the number of words in the summary, r is the number of unique words in the summary, n 1 + ... + n r = N and for each i , n i is the number of times word w i appears in the summary and p ( w i ) is the probability of w i appearing in the summary estimated from the input documents. In or-der to confirm the hypothesis that human summaries have high likelihood under a multinomial model, we computed the log-likelihood log [ L ( sum ; p ( w i ))] of all human and ma-chine summaries from DUC X 03 (see Table 3). There were 30 summaries from each system, and 12 summaries from each person. The log-likelihood is computed rather than the likelihood in order to avoid numeric problems such as underflow for very small probabilities. If human summaries have higher likelihood under the model than machine ones, we can conclude that a multinomial model captures more aspects of the human summarization process than of that of current automatic summari zers. And indeed: the log-likelihood of summaries produced by human summarizers were overall higher than for those produced by systems and the fact that the top five highest log-likelihood scores be-long to humans indicate that some humans indeed employ a summarization strategy informed by frequency. 3
We established that high-frequency content words in the input are very likely to be used in human summaries, and that there will be a consensus about their inclusion in a summary between different human summarizers. But the co-occurrence of words in the inputs and the human sum-maries does not necessarily entail that the same facts have been covered. A better granula rity for such investigation is the semantic content unit, an atomic fact expressed in a
Other humans might have other strategies, such as giving maximum coverage of topics mentioned in the input, even those mentioned only once. Human10 appears to have such a strategy for example (after examination of his summaries). text, such as the summary content units that form the basis of the pyramid method used for evaluation in the last DUC [19, 22]. In this annotation procedure, the content units are manually annotated 4 , and expressions with the same mean-ing are linked together, even when there are differences in wording. For example, two documents can contain the sen-tences  X  X inochet was arrested in the UK X  and  X  X inochet X  X  arrest in Britain caused international controversy X . While the wording is not exactly the same, both sentences express the content units Pinochet was arrested and The arrest took place in Britain .

Evans and McKeown [8] annotated 11 sets of DUC 2004 input documents and human written summaries for content units following the pyramid approach. Based on their an-notation, we were able to measure how predictive the fre-quency of content units in the documents is for the selection of a content unit in a human summary. As in our study for words, we looked at the N most frequent content units in the inputs and calculated the percentage of these that appeared in any of the human summaries. Similarly to the case of words, of the 5 most frequent content units, 96% appeared in a human summary across the 11 sets. The re-spective percentages for the top 8 and top 12 content units were 92% and 85%. Thus content unit frequency is highly predictive for inclusion in a human summary, with the per-centage of high frequency content units that are expressed in human summaries almost identical to the percentage for content words, presented in table 1.

Content units that are expressed in more human sum-maries, also occurred more often in the input, in agreement with the conclusion we drew from the analogous investiga-tion on the word level.

In an additional experiment to confirm the hypothesis that frequency of content units is a predictive feature for summarization, we used the summarizer evaluation based on the 11 sets and reported in [7], and we computed the correlation between the weight of a content unit from the input documents (equal to the number of times the con-tent unit was expressed in the input/its frequency) and the content unit weight from human summaries (equal to the number of summarizers that expressed the content unit in their summaries of the input). The Pearson X  X  correlation co-efficient between the input and human summaries weights is 0.64 (p-value=0), strongly indicating that content units that are repeated in several documents are likely to be picked in consensus by several humans and showing that frequency in the input helps predict human agreement in terms of con-tent units. The lower than perfect correlation shows that there are other factors at play that influence human content selection decisions, which we do not find surprising at all and the discovery of which will be the focus of future work.
Now that we have shown that frequency is a good predic-tor of content in human summaries and that human sum-maries have higher likelihood under a multinomial model, how can we extend these empirical findings to building a summarizer? The question is not trivial: normally, only the frequency of content words can be easily obtained from the input, but how is the frequency of words to be combined in order to get an estimate for the importance of sentences, the
Using a convenient visualization tool, DUCView. usual units for extraction in summarization? We can define a family of summarizers, SUM CF ,where CF is the combi-nation function yielding the importance of a sentence based on the words contained in that sentence. Different choices of CF will give different summarizers from the frequency based summarizer family. Below we outline the overall sum-marization algorithm and discuss possible choices of CF . Context-sensitive frequency-based summarizer Step 1 Compute the probability distribution over the words Step 2 Assign an importance weight to each sentence S j in Step 3 Pick the best scoring sentence under the scoring Step 4 If the desired summary length has not been reached,
Different summarizers SUM CF can be obtained by making different choices for the composition function CF .Three obvious candidates for CF are: Product ( CF  X  Average ( CF  X  Avr ) For this choice of CF Sum ( CF  X 
Each of these choices for CF leads to a different frequency based summarizer and we will see that the specific choice has a huge impact on the performance of the summarizer; not all frequency-based summarizers perform well.

How does a summarizer SUM CF do in terms of inclusion of top frequency words compared to humans and other top performing systems? Table 4 shows the percentage of the N most frequent words from the DUC X 03 documents that also appear in SUM Avr summaries. As expected, these are much higher than the percentages for the non-frequency oriented machine summarizer; moreov er, they are even higher than in all four human models taken together.
Using frequency alone to determine summary content in multi-document summarization will result in a repetitive summary. We can adjust the algorithm to account for infor-mation included so far by adding Step 3.5, shown below. Used by 5 most freq 8 most freq 12 most freq Human 94.66% 91.25% 85.25% Machine 84.00% 77.87% 66.08% SUM Avr 96.00% 95.00% 90.83% Table 4: Percentage of the N most frequent words from the input documents that appear in one of the four human models, a state-of-the-art machine sum-marizer and SUM Avr , a new machine summarizer based on frequency that uses the average as a com-position function.
 Step 3.5 For each word w i in the sentence chosen at step
It serves a threefold purpose: 1. It gives the summarizer sensitivity to context. The 2. By updating the probabilities in this intuitive way, we 3. The update of word probability gives a natural way
In the next section, we evaluate the algorithm both with and without step 3.5, showing that when it is removed from the algorithm, the summarizer does worse on content se-lection and there is a substantial increase in information repetition in the summary.
To evaluate the performance of the three SUM CF sum-marizers, both with and without context sensitive adjust-ment, we use the test data from two large common data set evaluation initiatives X  X he 50 test sets for multi-document summarization task for DUC 2004 and the common test set provided in the 2005 Machine Translation and Summariza-tion Evaluation (MSE) initiative. Both tasks were to pro-duce a generic 100-word summary of several related articles, but in the MSE task some of the input consisted of machine translated text.
 We used the data from the 2003 DUC conference for de-velopment and the data from the 2004 DUC as test data, Table 5: Number of sentences in systems X  sum-maries: the choice of composition function CF af-fects systems X  preference to longer or shorter sen-tences and SUM Avr is the more balanced one. which we report on here. We tested the SUM CF family of summarizers on the 50 sets fro m the generic summary task in 2004 DUC.

Even before analysis of quantitative metrics, we can see that the choice of combination function CF has a signif-icant impact on summarizer performance. One would ex-pect that the probabilistic summarizer SUM Q would favor shorter sentences because as the sentence gets longer, their overall probability involves the multiplication of more word probabilities (numbers between 0 and 1) and thus overall longer sentences will have lower probability. Exactly the opposite would be expected from SUM P , which assigns sen-tences a weight equal to the sum of probabilities of the words in the sentence. The more words there are in the sentence, the higher the sentence weight will tend to be. SUM Avr is a compromise between the two extremes. To confirm this intuition about the behavior of the summarizers depending on the choice of CF , we looked at the length in sentences of the summaries that they produced. Table 5 shows the number of sentences across the 50 summaries produced by each of the systems. Our intuition is confirmed, with SUM producing summaries of about three sentences and SUM Q getting about five sentences per summary, for the same size in words. The average human summary for the same topics has around four sentences, close that for SUM Avr .
For the evaluation, we use the ROUGE-1 automatic met-ric, which has been shown to correlate well with human judg-ments based on comparison with a single model [15, 13] and which was found to have one of the best correlations with human judgment on the DUC 2004 data [21] among the sev-eral possible automatic metrics. In addition, we report the ROUGE-2 and ROUGE-SU4 metrics, which were used as of-ficial automatic evaluation metrics for MSE 2005 and DUC 2005.

The results are obtained with ROUGE version 1.5.5 with the settings used for DUC 2005 (with -s option for removing stopwords for ROUGE-1). 5
All summaries were truncated to 100 words (space delim-ited tokens) for the evaluation, as is normally done in DUC evaluations. The first column of table 6 also lists the number of words in the 50 summaries in the test set. Some systems did not generate the longest possible summary. Peer 120 was an extreme example, producing summaries with aver-age length of 78 words. But the impact of peer summary length on the final ranking of the systems is unlikely to be big, since most systems produced summaries very close to the required 100 word limit.

An approximate result on deter mining which differences in scores are significant can be obtained by comparing the 95% confidence intervals for each mean. Significant differences
The exact parameters we used were -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d are those where the confidence intervals for the estimates of the means for the two systems either do not overlap at all, or where the two intervals overlap but neither contains the best estimate for the mean of the other, though [25] warns that the latter approach may indicate significance more of-ten than it should.

Table 6 also shows scores for the 16 other participating systems from DUC 2004, and the baseline, which was se-lecting the beginning of the latest article as a summary. Several conclusions can be drawn from the table: Comparison between SUM CF summarizers: All three SUM CF summarizers use word frequency in the input as a feature but have a different composition function CF to as-sign weights to sentences. SUM Q is a probabilistic summa-rizer and the weight it assigns to each sentence is in fact the probability of the sentence. SUM Avr and SUM P assign to sentences weight equal to the average and the sum of the probabilities of the words in the sentence respectively. For these two latter summarizers , the raw frequency of words could be used instead of word probabilities. For all three au-tomatic metrics, SUM Q is significantly worse than SUM P and SUM Avr and is in fact very close to baseline perfor-mance. SUM Avr and SUM P are almost identical in terms of ROUGE scores.

The effect of context adjustment: In the table we have listed the automatic scores for SUM AvrNoAdjust .This is the summarizer for which the composition function CF  X  Avr , but without Step 3.5 from the summarization algo-rithm, which is responsible for adjusting the weights for words that appear in sentences already chosen for inclu-sion in the summary. All three metrics indicate that the content selection capability of the summarizer is affected by the removal of the context adjustment step. According to ROUGE-1, removing the context adjustment leads to sig-nificantly lower results, while for the other two metrics the deterioration is not significant. In order to assess how much Step 3.5 affected the occurance of repetition in the sum-maries, we analyzed 10 of the produced summaries for re-peated content units. There were 3 repeated content units in the SUM Avr summaries, and 13 repeated content units in the SUM AvrNoAdjust summaries, which is a substantial increase.
 Comparison with other DUC systems SUM P and SUM Avr perform extremely well compared to the other DUC 2004 systems. Peer 65 is the only system that significantly outperforms them, while ten (more than half) of the other systems are significantly worse. It is worth noting that peer 65 is a supervised HMM system [4], requring training data and parameter adjustment, while the SUM CF summarizers are non-supervised and totally data-driven. In sum, the SUM CF summarizers are about as good as the best DUC 2004 participants.

Overall, SUM Avr is the best of the SUM CF family in bal-ancing content selection scores and sentence length prefer-ence, and this is the summarizer we choose for later com-parisons. Its sentence selection scores are comparable to that of the best DUC 2004 summarizers, it has most success in avoiding repetition in the summary from the frequency summarizer family, and it is least sensitive to the influence of sentence length on the sentence weight.
 In April 2005, a multi-document summarization evaluation task was conducted as part of the Machine Translation and Summarization Workshop at ACL. 6 The task was to produce a 100-word summary from multi-document inputs consisting of a mixture of English documents and machine translations to English of Arabic documents on the same topic. Some summarizers were modified for this task to use redundancy to correct errors in the machine translations, or to avoid MT text altogether and choose only sentences from the English input.

We ran SUM Avr without any modifications to account for the non-standard input [29]. The light-weight version of the summarizer was run, which did not require part of speech http://www.isi.edu/  X  cyl/MTSE2005/MLSummEval.html Table 7: Results from the MSE evaluation. Pyra-mid scores and duplication is computed for 10 test sets, automatic scores for all 25 test sets. Numbers flagged by  X *** X  are significantly different from the results form SUM Avr . For repetition, higher num-bers are worse, indicating that there was more rep-etition in the summary. tags and which excluded stop words from a given stop word list.

The official evaluation metrics adopted for the workshop were the manual pyramid score, ROUGE-2 (the bigram over-lap metric) and ROUGE-SU4 (skip bigram). The skip bi-gram metric measures the occurrence of a pair of words in their original sentence order, permitting up to four interven-ing words. The metric was originally proposed for machine translation evaluation and was shown to correlate well with human judgments both for machine translation and for sum-marization [13, 16].

The pyramid method was used to evaluate only 10 of the test sets, while the automatic metrics were applied to all 25 test sets. The average results for each peer for the three metrics is shown in table 7. For the manual pyramid scores, none of the differences between systems were significant ac-cording to a paired t-test at the 5% level of significance. This is not surprising, given the small number of test points. There were only three peers with average scores larger than that of SUM Avr , and six systems with lower average pyra-mid performance. We again see that SUM Avr is competitive in comparison with other, more sophisticated, MDS systems in terms of content selection and is one of the best systems in avoiding repetion in the summaries.

For the automatic metrics, significance was based again on the 95% confidence interval provided by ROUGE. One system was significantly better than SUM Avr ,andforeach of the automatic metrics there were two systems that were significantly worse than SUM Avr . The rest of the differences were not significant. In table 7, results that are significantly different from those for SUM Avr are flagged by  X *** X .
During the annotation for the pyramid scoring, the con-tent units that were repeated in an automatic summary were marked up: we include in the results table the average num-ber of repeated SCUs per summary for all systems. SUM Avr was one of the systems with the lowest amount of repetition in its summaries, with three of the other peers including significantly more repetitive information. These results con-firm our intuition that the weight update of words to adjust for context is sufficient for dealing with duplication removal problems. This experiment also confirms that SUM Avr is a robust summarizer wit h good performance.
Maximal Marginal Relevance (MMR) is the method for redundancy removal mentioned most often in the context of summarization research. The method was first introduced in [3] and was applied for multi-document summarization in [9]. The MMR approach was developed primarily for in-formation retrieval and query -focused summarization, and gives a summarizer sensitivity to context by reweighting sen-tences using a linear combination of the similarity between the sentence and 1) the query and 2) the summary sentences already selected in the summary. The best sentence is con-sidered the one that is most similar to the query and least similar to the text that is already in the summary. In [9], the technique was used to create multi-document extracts of 25 sets of 10 articles each. The evaluation was done by comput-ing the cosine similarity between the extract and a human model extract for the same set. In this setting, extracts produced using MMR and those not using the technique re-ceived the same evaluation score, and thus the usefulness of the technique could not be demonstarted. Many systems use the MMR idea for generic multi-document summariza-tion, 7 where no user query is available, by setting a sin-gle paramater for similarity and rejecting all sentences that have similarity with the already chosen part of the summary that exceeds this predefined treshold. An evaluation of how changing this paramater influences the quality of the sum-maries has not been reported. I n addition to this similarity parameter, the similarity measure that is used makes a dif-ference for the success in duplication removal, as reported in [20], who focused on the study of different similarity metrics for duplication removal.
Our analysis using the DUC datasets shows that frequency has a powerful impact on the performance of summarization systems, provided that a good composition function is used. Our results show that averaging word probabilities yields a system that performs comparably to other state-of-the-art systems and that outperfoms many of the participating systems. When context is taken into account and proba-bilities are adjusted when the word has already appeared in the summary, performance basedoncontentshowsan improvement, but more importantly, repetition in the sum-mary significantly decreases.

These results suggest that the more complex combination of features used by state-of-the-art systems today may not be necessary and the contribution of such features needs to be precisely isolated. They highlight the fact that com-position plays an important role in performance, but is an unknown for most state-of-the-art systems, who often do not report the composition function that was used. Fur-thermore, they demonstrate that repetition can be reduced within the same frequency-based model.

It is worth noting that the presented summarization algo-rithm uses frequency in a gr eedy way, choosing the current best sentence at each iteration. Such an approach does not take advantage of the result we demostrated that human summaries tend to have high likelihood under a multino-mial model. This fact could be used in a global optimization algorithm, possibly leading to better results.
See for example the online DUC 2004 proceeding [1] M. Banko and L. Vanderwende. Using n-grams to [2] R. Barzilay and K. McKeown. Sentence fusion for [3] J. Carbonell and J. Goldstein. The use of mmr, [4] J. Conroy, J. Schlesinger, J. Goldstein, and [5] T. Copeck and S. Szpakowicz. Vocabulary agreement [6] H. Daum  X  e III and D. Marcu. Bayesian [7] D. K. Elson. Project logline: Rhetorical categorization [8] D. K. Evans and K. McKeown. Identifying similarities [9] J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. [10] H. Jing and K. McKeown. Cut and paste based text [11] K. Knight and D. Marcu. Summarization beyond [12] J. Kupiec, J. Perersen, and F. Chen. A trainable [13] C.-Y. Lin. Rouge: a package for automatic evaluation [14] C.-Y. Lin and E. Hovy. Automated multi-document [15] C.-Y. Lin and E. Hovy. Automatic evaluation of [16] C.-Y. Lin and F. J. Och. Automatic evaluation of [17] H. P. Luhn. The automatic creation of literature [18] D. Marcu and L. Gerber. An inquiry into the nature [19] A. Nenkova and R. Passonneau. Evaluating content [20] E. Newman, W. Doran, N. Stokes, J. Carthy, and [21] P. Over and J. Yen. An introduction to duc 2004 [22] R. Passonneau, A. Nenkova, K. McKeown, and [23] D. Radev, S. Teufel, H. Saggion, and W. Lam. [24] G. J. Rath, A. Resnick, and R. Savage. The formation [25] N. Schenker and J. Gentleman. On judging the [26] B. Schiffman, A. Nenkova, and K. McKeown.
 [27] H. van Halteren and S. Teufel. Examining the [28] L. Vanderwende, M. Banko, and A. Menezes.
 [29] L. Vanderwende and H. Suzuki. Frequency-based
