 Text summarization is the reductive transformation of source text to summaries through content condensation by selectio n or generalisation on what is important in the source [1]. With the exponential growth of textual information online, resulting in useless information blast, manually accessing to useful information becomes a highly difficult task, so summarization becomes a critically important technique to help users quickly gain the important knowledge by generating the essential information in a concise way. Concerning on the approaches used in text summarization, there are two types of summaries [2]: extractive summaries that composed by a set of original senten ces selected from the source documents, and abstractive summaries that composed by materials which may not appear in the source documents.

Extractive summarization is based on the extraction of important sentences from the source documents. The importance of a sentence is usually measured by some kind of scoring models such as term frequency, and sentences are ranked by their scores, then the sentences with the highest scores are selected in turn to constitute the summary with the required length. On the contrary, abstractive summarization attempts to understand the concepts in source documents and reorganize language to cover the essential information in a concise way. It usually involves prior knowledge [3] or deeper natural language generation technology [4], which either relies heavily on manual efforts or is domain specified.
Associating to the complexity of the lim ited abstractive methods, most sum-marizations today are based on the extra ctive methods [2]. However, the extrac-tion of original sentences from the source documents is flawed [5], which usually selecting verbose sentences that lead to incomplete information such as infor-mation loss or information redundancy in the summaries. Firstly, the extracted sentence is usually longer which contain much unnecessary terms and consuming the limited word space. Secondly, select ing sentences based o n importance leads to redundancy which reduces the coverage of different information in the sum-maries. Lastly, extracted sentences ofte n lead to semantical coherence problem because some sentences may lose their referents when extracted out of the text.
In this paper, we propose a graph-based summarization method to reduce content redundancy in the previous extractive methods. In contrast with the previous works, our method uses the word as basic unit rather than sentence, which represents the text by a textual graph and turns summarization into a new problem of finding key paths that contain the essential information. Some syntax rules are defined to avoid the selection of unreasonable paths, and common paths are recomposed to generate sentences with little redundancy. Earlier works on summarization explored the superficial approaches [6]. Term frequency is the simplest method to score the importance of a sentence as in [7], and tf*idf combines it with the document frequency [8]. The underlying assumption is that the frequency of a word is proportional to its relevant to the main information of the document, but the weight of a word decreases with the increasing documents that contain this word. The part of speech (POS) also influences the word weight [9], the nouns usually contain more semantic information than others like conjunction and get a higher score. The position of sentences is also relevant to the importa nce [10], the leading sentences usually contain the main information and select ing sentences from the beginning of the text could be a reasonable selection. What X  X  more, the titles usually summarize the essential information of source docu ments, so the sentences contain title words should be more important than others [11].

Knowledge-based approaches either extend superficial approaches by the in-corporation of lexical resources or discourse organization theories. Lexical the-saurus such as WordNet is used to identify related words (e.g., Synonyms) and connect the cohesive links between sentences, the summary sentences are se-lected based on the resulting linked structure [12]. The discourse organization theories consider the text as a linguistic unit, and the important sentences can be selected by the specific organization of clue information such as  X  X onclusions X ,  X  X urpose X ,  X  X esults X  [13]. Machine learning methods such as Bayesian classifier, Hidden Markov Models, Support Vector Machine etc., are also used to combine the superficial methods in text summarization [14].

Graph-based ranking algorithms have also been shown to be effective in text summarization [15],[16]. The basic idea is that the nodes of the graph represent text elements such as words or sentences , and edges represent the relations be-tween these text elements such as the similarity between sentences, then some graph-based random walk algorithms such as PageRank [17] can be used to compute the importance of sentences. T extRank [15] is a typical graph-based summarization method, which uses the PageRank algorithm to score these sen-tences in a graph structure.

As mentioned above, most extractive summarization methods treat sentences independently and select sentences based on the weight scores, which means that sentences in summary may repeat content and lead to content redundancy [6]. This is usually dealt by filtering sentences too similar to the selected ones, and Maximal Marginal Relevance [18] is commonly used to make a tradeoff between the similarity and difference be tween sentences. Although this kind of methods can reduce redundancy in some degree, they still confront the poor accuracy because of verbose sentences. Ins tead of selecting original sentences, our method regenerates sentences by the words to reduce redundancy and covers more different essential information so as to improve the content accuracy. The basic idea of our proposed method GSWR (Graph-based Summarization without Redundancy) is that, we firstly construct a textual graph by the words from the given source documents throu gh the preprocess procedure, and then we select and label the key paths in the graph, finally we regenerate sentences to compose the summary by the labeled nodes under the restraining of syntax rules. The main steps are shown as Fig. 1.

Given the source documents, the first s tep of preprocess is to segment the text into sentences and words with the pro perties such as term frequency, part of speech, position etc, which are the materials for textual graph constructing. The second step of preprocess is to classify the sentences into different categories based on the key words which are the nouns that contain the main information. The role of this step is to simplify the structure of textual graph by clustering the relevant sentences to form an independent subgraph, which can reduce the time consuming of path traversals. 3.1 Textual Graph Textual graph is also commonly used in other extractive summarization meth-ods such as TextRank [15] and LexRank [16]. The difference is that the pre-vious methods use the sentences as nodes and similarity between sentences as edges, and important sentences are sel ected by the graph rank algorithms such as PageRank [17]. While our graph uses the words as nodes and adjacent rela-tions between words as edges, which is cl oser to Opinosis [19] where the graph is used to enumerate all the possible sente nces, but our method GSWR regenerates sentences by key paths selecting and composing in the textual graph.
Figure. 2 is a sample textual graph of the given 6 sentences. Each node repre-sents a unique word, and the directed ed ges are the adjacent relations between the words. Different circles indicate different word frequencies and the hollow circles are stopwords (words without semantic meanings, such as  X  X  X ,  X  X f X ). By classifying sentences based on the nouns , we get two sets of sentences, sentences 1-4 and 5-6, which are corresponding to subgraphs 1 and 2, each of which is highly redundant. The same non-stopwords in different sentences are mapped into one node in the textual graph, then the phrases appeared more than once are represented by the common paths of same nodes and edges. The common paths indicate the redundant content of source text, and the summary will be more concise if we merge the common paths. 3.2 Syntax Rules Without the restraining of any rules, the path routing algorithm will enumer-ate all the possible paths in the graph and regenerate many grammatically or semantically incorrect sentences. In order to guarantee the reasonability of the path routing, we use the sample path P = { v 1 ,...,v m ,...,v n } to define the syntax rules. v 1 is start node and v n is the end node, v m is a node in path P . BeginWord. BeginWord is the word that can start a new sentence. v m is a BeginWord only if it satisfies the conditions: 1. IsStopWord ( v m )= false ; 2. POS ( v m )= NN ; 4. POS ( v i ) /  X  X  NN,VB,RB } ,m  X  2 ,  X  v i  X  X  v 1 ,...,v m  X  1 } . Firstly v m mustn X  X  be a stopword, and the POS (part of speech) is noun. The position of v m in the sentence path should not larger than the threshold  X  start , at the same time keeps a distance larger than  X  end from the last word v n .And the words before v m should not contains any nouns, verbs or adverbs. BranchPre. Path P is the previous branch of a selected path, only if it satisfy the conditions: W unlabel is the summation weights of the unlabeled nodes in path P ,and W P is the summation weights of all the nodes in P . The ratio between W unlabel and W P indicates the novelty of path P , which is compared with the threshold  X  ul . The ratio less than  X  ul means most of the words in P have been visited and P is a BranchPre. The definition of BranchPre is to avoid selecting the paths that have been selected before.
 BranchNext. A partial path P 1 = { v m ,...,v n } is the next branch of a selected path P 2 = { v x ,...,v y } only if it satisfies the conditions 1. POS ( v i )= VB,  X  v i  X  P 1 ; 2. v i  X  P 2 ,ifv i = v m ; 3. IsBeginWord ( v j )= true,  X  v j  X  P 1  X  P 2 ,j&gt; 1; 4. Labeled ( v k )= true,  X  v k  X  X  v i ,...,v j } .
 Firstly we trace back along path P 1 from node v m , and find a verb v i ,if v i is not equal to v m then v i must be a common word in path P 2 . Once we find the verb v , we trace back along common path of P 1 and P 2 from v i and find a BeginWord v . The words between v i and v j must all be labeled. If P 1 is a BranchNext then it can be jointed to the a lready selected path P 2 and regenerate a new sentence by the two paths.
 AccessNext. In the process of path composing, v m in P 1 = v 1 ,...,v n is the conditions 1. Labeled ( v m )= ture 2. IsBeginWord ( v i )= ture,  X  v i  X  P 2  X  X  v 1 ,...,v m } .
 The next accessible node of the current path P 2 must be labeled, and we must find a BeginWord in the common path between P 2 and P 1 . 3.3 Path Score We use the common textual features to compute the word weight, including term frequency, part of speech, whether is a stopword/title word or not. Given that P = { v 1 ,v 2 ,...,v n } is a path of n nodes in the textual graph, v 1 is the start point and v n is the end point, the path length L = n  X  1, then the path score of W ( P )isdefinedasfollows: the words X  weight along the path, and divided by the length function 1 + log L . The definition of word weight W ( v i )is: TFIDF is commonly used to score sentences and defines as follows: N i is the frequency of v i , is the number of documents that contains word v i . IsStopWord ( v i )weights the impact of stopwords, which is 0 for stopwords and 1 otherwise, since the stopwords contain not any useful semantic meanings. POS ( v i )measuresthe impact of part of speech, which is 2 for nouns and verbs that usually contain more important information, and 1 for others. IsTitleWord ( v i ) measures the impact of words that appear in the title, which is 3 for title words and 1 for others, because the title words usually contain the essential information of the source documents. Both coefficients for POS ( v i )and IsTitleWord ( v i ) are determined by the experiment results after testing a range of values. 3.4 Path Selecting The Path Selecting algorithm is used to select the word nodes that composes the summaries. The basic idea is as follows: Select the key paths in the textual graph iteratively until get a summary with the limited size, and the nodes in the Algorithm 1. PathSelect selected paths are labeled. At each iter ation, the path started with an unlabeled node with the maximal path score is selected, at the same time the path must satisfy the syntax rules.

Algorithm 1 describes the process of path selecting in detail. The input pa-rameters are the textual graph G ( V,E ) and the summary length L , the output parameter is a list of the begin words BV which will be used as the input pa-rameters of the path composing algorithm. The path selecting algorithm is an iteration process until we get a summary of length L or there is nothing to be se-lected any more. At each iter ation, we select a key path P max with the maximal path score, which starts with an unlabeled node or a BeginWord and satisfies the following conditions: P max is not a BranchPre of the labeled paths which will create content redundancy; P max is a BranchNext or BeginWord; The length of P max is less than the remain summary size L . If we can find such a key path, namely P max =  X  , then update the remain length of summary L by subtracting the length Len ( P max ), and label all the nodes on path P max , at the same time, if the first node of P max is a BeginWord then we add it into the list BV .Otherwise if we can X  X  find a valid key path, namely P max =  X  , it means all the key paths have been labeled and we return list BV to quit this procedure. 3.5 Path Composing After path selecting, we labeled the word nodes that will compose the summary and get a BeginWord list BV . The path composing is to use these labeled Begin-Words to start the depth-first traversal to regenerate sentences, and the common paths representing the redundant content are merged during this process. Finally we get the summary by these regenerat ed sentences from each BeginWord.
Algorithm 2 describes the path composing process of a single BeginWord V , and the regenerated sentence S is returned. Firstly we initialize the sentence S to be empty and get the collection of next accessible nodes V next by the syntax rule AccessNext .If V next is empty, then there are two cases: V is a punctuation or not. In the first case we return this punctuation, which means we find a valid sentence ending with V . In the second case V is not a punctuation, there is not any accessible next node for V , which means the current node path from V is invalid and we return an empty S =  X  . Otherwise when V next isn X  X  empty, namely there are several accessible next n odes, we recursively generate the sub-sentence for each next node V i and compose these subsentences with the common path before node V , which helps to reduce the content redundancy. In Fig. 2, if we start with the BeginWord  X  X ummarization X , we will get two accessible next nodes  X  X  X  and  X  X orth X  in the branch node  X  X s X , and the subsentences  X  X  useful technique X  and  X  X orth learning X  respect ively, then we regenerate a new sentence  X  X ummarization is a useful technique and worth learning X . In order to compare GSWR with the extractive summarization methods, we select two typical methods: MMR and TextRank. The former uses the MMR Algorithm 2. PathCompose and superficial methods to tackle with the content redundancy problem, the latter uses random walk algorithms in graph structure to weight sentences, both of which make the corresponding comparison with GSWR in different aspects. In this experiment we use the dataset 1 of DUC 2007, which is composed of 45 different news topics, and there are 25 news reports under each topic. we use the ROUGE [20] toolkit 2 for evaluation, which is an automatic summarization evaluation widely adopted by DUC.

Table 1 is the comparison of different summarization methods, it is obvious to find that GSWR gets a better experimental results compared to the baselines MMR and TextRank, which means GSWR makes an improvement of the content accuracy compared with the typical extractive summarization methods.
As a typical extractive summarization method, TextRank usually generates summaries contain much verbose sentences and redundant content, which wastes the limited number of words in summaries and leads to the incomprehensive cov-erage of main information, thus reducing the accuracy of summaries. MMR is also an extractive summarization method, but it considers both relevance and nov-elty when selecting a sentence, which reduces the content redundancy by making a tradeoff between similarity and differ ence. The results of MMR is better than TextRank for the processing of content redundancy, however, as an extractive summarization method, MMR also influenced by the drawbacks of verbose sen-tences, which weakens its advantage to TextRank. By using the idea of sentence regenerating, GSWR has the best results compared to TextRank and MMR, for it can cover more different main information by selecting non-overlapping node paths and reduce content redundancy by merging the common paths in the textual graph, which two factors together determine the good performance of GSWR on the content accuracy of summaries.

On the other hand, the summary length varies with the application envi-ronment, so it X  X  important to keep the robustness of summarization algorithms under different summary lengths. The s ummary length ranges from 50 to 250, because the summary with too little words is not enough to contain any main information, so we set the minimal length to 50 empirically, and we set the max-imal length 250 because the reference summaries are about 250 words. In the following, we will investigate the influence of different summary lengths to the ROUGE metrics by Fig. 3(a), Fig. 3(b), Fig. 3(c).

It is obviously to be found that all the ROUGE scores decrease with the de-creasing of summary length. The reason is that the reference summaries written by human is more subjective when the summary length is little, and it will be more difficult for the automatic summarization methods to identify these subjec-tive content. With the increasing of summary length, the summaries will contain more information which makes the cont ent to be more objective, and then im-prove the probability that the summarization methods identifies the important information which agrees to the human.

On the other hand, in Fig. 3(a) and Fig. 3(c), we can find that the perfor-mance of GSWR is closer to MMR when the summary length approaches to 50, because the summary can contain little content when the length is little and GSWR covers less different informa tion in this condition, which weakens GSWR X  X  advantage to MMR., but they are all better than TextRank because of the influence of redundancy reduction.
 In this paper we present a graph-based summarization method without redun-dancy GSWR. Unlike the commonly used extractive summarization methods that select the original sentences to constitute summaries, GSWR constructs a textual graph based on words to regenerate sentences which can make a com-prehensive coverage of different essential information and reduce content redun-dancy, so as to improve the accuracy of summaries. However, there are still much things we can do to improve our algorithm. Instead using the basic super-ficial features to score sentences, we can introduce the lexical resources such as WordNet to optimize the score model. And we need more different kinds of data corpus to validate and optimize our synta x rules in the process of path selecting. Finally, we will concentrate more on the readability of the summaries content in the future work, which is another big challenge for text summarization. Acknowledgment. This research is supported by the 863 project of China (2013AA013300), National Natural Sc ience Foundation of China (Grant No. 61375054) and Tsinghua University Initiative Scientific Research Program (20131089256).
