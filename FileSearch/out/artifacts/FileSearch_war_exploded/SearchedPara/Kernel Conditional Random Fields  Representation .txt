 Many classification problems involve the annotation of data items having multiple components, with each component requiring a classification label. Such problems are chal-lenging because the interaction between the components can be rich and complex. In text, speech, and image pro-cessing, for example, it is often useful to label individual words, sounds, or image patches with categories to enable higher level processing; but these labels can depend on one another in a highly complex manner. For biological se-quence annotation, it is desirable to annotate each amino acid in a protein with a label, with the collection of labels representing the global geometric structure of the molecul e. Here the labels in principle depend on the physical char-acteristics of the molecule and its ambient chemical envi-ronment. In each case, classification tasks naturally arise which clearly violate the assumption of independent and identically distributed instances that is made in the major ity of classification procedures in statistics and machine lear n-ing. It is therefore of central importance to extend recent advances in classification theory and practice to structure d, non-independent data classification problems.
 Conditional random fields (Lafferty et al., 2001) have been proposed as an approach to modeling the interactions be-tween labels in such problems using the tools of graphical models. A conditional random field (CRF) is a model that assigns a joint probability distribution over labels condi -tional on the input, where the distribution respects the in-dependence relations encoded in a graph. In general, the labels are not assumed to be independent, nor are the ob-servations conditionally independent given the labels, as is assumed in generative models such as hidden Markov models. The CRF framework has already been used to ob-tain promising results in a number of domains where there is interaction between labels, including tagging, parsing and information extraction in natural language processing (Collins, 2002; Sha &amp; Pereira, 2003; Pinto et al., 2003) and the modeling of spatial dependencies in image processing (Kumar &amp; Hebert, 2003). In related work, Taskar et al. (2003) have studied random fields (also known as Markov networks) fit using loss functions that incorporate a gener-alized notion of margin, and have observed how the  X  X ernel trick X  applies to this family of models.
 We present an extension of conditional random fields that permits the use of implicit features spaces through Mercer kernels, using the framework of regularization theory. Suc h an extension is motivated by the significant body of recent work that has shown kernel methods to be extremely effec-tive in a wide variety of machine learning techniques; for example, they enable the integration of multiple sources of information in a principled manner. Our introduction of Mercer kernels into conditional graphical models is also motivated by the problem of semi-supervised learning. In many domains, the collection of annotated training data is difficult and costly, as it requires the efforts of expert hu-man annotators, while the collection of unlabeled data may be relatively easy and inexpensive. The emerging theme in recent research in semi-supervised learning is that kernel methods, in particular those based on graphical representa -tions of unlabeled data, form a theoretically attractive an d empirically promising set of techniques for combining la-beled and unlabeled data (Belkin &amp; Niyogi, 2002; Chapelle et al., 2002; Smola &amp; Kondor, 2003; Zhu et al., 2003). In Section 2 we formalize the learning problem and present a version of the classical representer theorem of Kimeldorf and Wahba (1971). Unlike the classical result, for kernel conditional random fields the dual parameters depend on all potential assignments of labels to cliques in the graph, not only the observed labels. This motivates the need for algorithms to derive sparse representations, since the ful l representation has parameters for each labeled clique in the graphs appearing in the training data. In Section 3 we present a greedy algorithm for selecting a small number of representative cliques. This  X  X lique selection X  algorith m parallels the  X  X mport vector selection X  algorithms of kern el logistic regression (Zhu &amp; Hastie, 2001), and the feature selection methods that have been previously proposed for random fields and conditional random fields using explicit features (McCallum, 2003).
 In Section 4 the ideas and methods are demonstrated on two synthetic data sets, where the effects of the underly-ing graph kernels, clique selection, and sequential model-ing can be clearly seen. In Section 5 we report the results of experiments using kernel CRFs for protein secondary structure prediction. This is the task of mapping primary sequences of amino acids onto a string of secondary struc-ture assignments, such as helix, sheet, or coil. It is widely believed that secondary structure can contribute valuable information to discerning how proteins fold in three dimen-sions. We compare kernel conditional random fields, esti-mated using clique selection, against support vector ma-chine classifiers, with both methods using kernels derived from position-specific scoring matrices (PSI-BLAST pro-files) as input features. In addition, we give results for the use of graph kernels derived from the PSI-BLAST profiles in a transductive, semi-supervised framework for estimat-ing the kernel CRFs. The paper concludes with a brief dis-cussion in Section 6. Before proceeding with formalism, we give some intuition for what our framework is intended to capture. Our goal is to annotate structured data, where the structure is repre -sented by a graph. Labels are to be assigned to the nodes in the graph in order to minimize some loss function, such as 0-1 error; the labels come from a small set Y , for ex-ample, Y = { red , blue , green } . Each vertex in the graph is associated with a feature vector x v  X  X . In image processing, the feature vector at a node might in-clude a pixel intensity, as well as average pixel intensitie s smoothed over neighboring regions using wavelets. In pro-tein secondary structure prediction, each node might corre -spond to an amino acid in the protein, and the feature vector at a node may include an amino acid histogram of all pro-tein fragments in a database which closely match the given protein at that node. In the following section we present our notation and formal framework for such problems. 2.1. Cliques and labeled graphs Let G denote a collection of finite graphs. For example, G might be the set of finite chains, appropriate for sequence modeling, or the rectangular 2-dimensional grids, appro-priate for some image processing tasks. The set of ver-tices of a graph g  X  G is denoted by V ( g ) , and size of the graph is the number of vertices, denoted | g | = | V ( g ) | clique is a subset of the vertices which is fully connected, with any pair of vertices joined by an edge; we denote the set of cliques in the graph by C ( g ) . The number of vertices in a clique is denoted by | c | . Similarly, we de-note by C ( G ) = { ( g , c ) | g  X  G , c  X  C ( g ) } the collection of cliques across varying graphs. In other words, a mem-ber of C ( G ) consists of a graph and a distinguished clique of that graph. We will work with kernels that compare components of different graphs. For example, we could consider a kernel K : C ( G )  X  C ( G )  X  { 0 , 1 } given by K (( g , c ) , ( g 0 , c 0 )) =  X  ( | c | , | c 0 | ) . We next consider labelings of a graph. Let Y be a fi-nite set of labels; infinite Y is also possible in a re-gression framework, but we restrict to finite Y for sim-plicity. The set of Y -labelings of a graph g is denoted labeled graphs is Y ( G ) = { ( g , x ) | g  X  G , y  X  Y ( g ) } Similarly, let X be an input feature space; for example, X = R n . The set X ( g ) = x | x  X  X | g | denotes the set of assignments of a feature vector to each vertex of the graph g ; X ( G ) = { ( g , x ) | g  X  G , x  X  X ( g ) } collection of all such annotated graphs. Finally, let Y ( g ) = ( c, y c ) | c  X  C ( g ) , y c  X  Y | c | be the set of labeled cliques in a graph. As above, we similarly define X Y C ( g ) = { ( x , c, y c ) | x  X  X ( g ) , ( c, y c )  X  Y X Y C ( G ) = { ( g , x , c, y c ) | ( x , c, y c )  X  X Y 2.2. Representer Theorem The prediction task for conditional graphical models is to learn a function h : X ( G )  X  Y ( G ) where h ( g , x )  X  Y ( g ) is a labeling of g , with the goal of minimizing a suitably defined loss function. The classifier h = h based on a labeled sample ( g ( i ) , x ( i ) , y ( i ) ) ( g ( i ) , x ( i ) , y ( i ) ) being a labeled graph, the graph possibly changing from example to example.
 To limit the complexity of the hypothesis, we will as-sume that it is determined completely by a function X Y C ( G )  X  R . Let f ( g , x ) denote the collection of values of that clique. We assume that a loss function  X  ( y , f ( g , given. As an important example, and the loss function used in this paper, consider the negative log loss  X  ( y , f ( g , x )) = (1)  X  where f ative log marginal loss could also be considered for mini-mizing the per-node error. The negative log loss function corresponds to a conditional random field given by p ( y | g , x ) = Z  X  1 ( g , x , f ) exp We now discuss how the  X  X epresenter theorem X  of kernel machines (Kimeldorf &amp; Wahba, 1971) applies to condi-tional graphical models. While this is a simple extension, we X  X e not aware of an analogous formulation in the statis-tics or machine learning literature.
 Let K be a Mercer kernel on X Y for each ( x , c, y Intuitively, this assigns a measure of similarity between a labeled clique in one graph and a labeled clique in a (pos-sibly) different graph. We denote by H producing kernel Hilbert space, and by k X k norm on L 2 ( X Y Consider a regularized loss function of the form It is important to note that the loss depends on all possi-ble assignments y dence on the graph g in the notation, let K dard representer theorem, it can easily be shown that the minimizer of a regularized loss function of the above form can be expressed in terms of the basis functions f (  X  ) = Proposition (Representer theorem for CRFs) . Let K be a Mercer kernel on X Y k X k K , and let  X  : R +  X  R + be strictly increasing. Then the minimizer f ? of if it exists, has the form f (  X  ) =
The key property distinguishing this result from the stan-dard representer theorem is that the  X  X ual parameters X   X  2.3. Two special cases Let K be a Mercer kernel on Z = X  X  Y  X  Y . Thus, the kernel is defined in terms of the ma-trix entries K ( z , z 0 ) where z = ( x, y K we can define a kernel on edges in X Y C ( G ) by K (( g , x , ( v 1 , v 2 ) , ( y 1 , y 2 )) , ( g 0 , x 0 K (( x v minimization problem min where f  X  H the solution f ? has the form f In the special case of kernel K ( z , z 0 ) = K ( x, x 0 )  X  ( y it follows that f Under the probabilistic model (2), this is simply kernel logistic regression. In the special case of K ( z K ( x, x 0 )  X  ( y 1 , y 0 1 ) +  X  ( y 1 , y 0 1 )  X  ( y 2 f and we recover a simple type of semiparametric CRF. The representer theorem shows that the minimizing func-tion f is supported by labeled cliques over the training examples; however, this may result in an extremely large number of parameters. We therefore pursue a strategy of incrementally selecting cliques in order to greedily reduc e the regularized risk. The resulting procedure is parallel t o forward stepwise logistic regression, and to related meth-ods for kernel logistic regression (Zhu &amp; Hastie, 2001), as well as to the greedy selection procedure presented in (Della Pietra et al., 1997).
 Our algorithm will maintain an active set A = ( g ( i ) , c, y c )  X  Y C ( G ) of labeled cliques, where the la-belings are not restricted to those appearing in the trainin g data. Each such candidate clique can be represented by a basis function h (  X  ) = K (( g ( i ) , x ( i ) , c, y is assigned a parameter  X  regularized risk where  X  is the log-loss of equation (1). To evaluate a can-didate h , one strategy is to compute the gain sup R  X  ( f +  X h ) largest gain. This presents an apparent difficulty, since th e optimal parameter  X  cannot be computed in closed form, and must be evaluated numerically. For sequence models this involves forward-backward calculations for each can-didate h , the cost of which is prohibitive.
 As an alternative, we adopt the functional gradient descent approach, which evaluates a small change to the current function. For a given candidate h , consider adding h the current model with small weight  X  ; thus f 7 X  f +  X h . Then R the functional derivative of R computed as where e E [ h ] = P tion and E model expectation conditioned on x , combined with the empirical distribution on x . The idea is that in directions where the functional gradient dR is mismatched with the labeled data; this direction should be added to the model to make a correction. This results in the greedy clique selection algorithm summarized in Fig-ure 1.
 Following our earlier notation, is the sum over all cliques. The candidate functions h might include functions of the form Initialize with f = 0 , and iterate: 1. For each candidate h  X  H 2. Select the candidate h = arg max 3. Estimate parameters  X  where i is a specific instance, c is a particular clique of g ( i ) , and y c is a labeling of that clique. Alternatively, in a slightly less greedy manner, at each step in the selection procedure a specific instance and clique may be selected, and functions for each clique labeling may be added. In the experiments reported below for sequences, the marginal probabilities p ( y for the state transitions are required; these are computed using the forward-backward algorithm, with log domain arithmetic to avoid underflow. A quasi-Newton method (BFGS, cubic-polynomial line search) is used to estimate the parameters in step 3. Prediction is carried out using the forward-backward algorithm to compute marginals rather than using the Viterbi algorithm. 3.1. Combining multiple kernels The above use of kernels enables semi-supervised learn-ing for structured prediction problems. One of the emerg-ing themes in semi-supervised learning is that graph ker-nels can provide a useful framework for combining labeled and unlabeled data. Here an undirected graph is defined over labeled and unlabeled data instances, and generally the assumption is that labels vary smoothly over the graph. The graph is represented by the weight matrix W , and one can construct a kernel from the graph Laplacian, substitut-ing eigenvalues  X  by r (  X  ) , where r is a non-negative and (typically) decreasing function. This regularizes high fr e-quency components and encourages smooth functions on the graph; see (Smola &amp; Kondor, 2003) for a description of this unifying view of graph kernels.
 It is important to note that such a use of a graph kernel for semi-supervised learning introduces an additional graphi-cal structure, which should not be confused with the graph representing the explicit dependencies between labels in a CRF. For example, when modeling sequences, the natural CRF graph structure is a chain. By incorporating unla-beled data through the use of a graph kernel, an additional graph that will generally have many cycles is implicitly in-troduced. However, the graph kernel and a more standard kernel may be naturally combined as a linear combination; see, for example, (Lanckriet et al., 2004). To demonstrate the properties and advantages of KCRFs, we prepared two synthetic datasets: a  X  X alaxy X  dataset to investigate the relation to semi-supervised and sequentia l learning, and an HMM with Gaussian mixture emission probabilities to demonstrate the properties of clique sele c-tion and the advantages of incorporating kernels. Galaxy . The  X  X alaxy X  dataset is a variant of two spirals; see Figure 2 (left). Note the dense core of points from both classes. The sequences are generated from a 2-state hidden Markov model (HMM), where each state emits instances uniformly from one of the classes. There is a 90% chance of staying in the same state. The idea is that under a se-quence model, an example from the core will have a better than random chance to be labeled correctly based on the context. This is not true under a non-sequence model, and the dataset as a whole will thus have about a 20% Bayes error rate under the iid assumption. We sample 100 se-quences of length 20. Note the choice of semi-supervised vs. standard kernels and sequence vs. non-sequence mod-els are orthogonal; the four combinations are all tested on. We construct a semi-supervised graph kernel by first creating an unweighted 10-nearest neighbor graph. We then compute the graph Laplacian L , and form the kernel K = 10 L + 10  X  6 I  X  1 . This corresponds to a function r (  X  ) = 1 / (  X  + 10  X  6 ) on L  X  X  eigenvalues. The standard kernel is the radial basis function (RBF) kernel with band-width  X  = 0 . 35 . All parameters here and below are tuned by cross validation.
 Figure 2 (center) shows the results of using kernel logis-tic regression with the semi-supervised kernel and with the RBF kernel; here the sequence structure is ignored. For each training set size, which ranges from 20 to 400 points, 10 random trials were performed. The error inter-vals shown are one standard error. When the labeled set size is small, the graph kernel is much better than the RBF kernel. However both kernels saturate at the 20% Bayes error rate.
 Next we apply both kernels to the semiparametric KCRF model in section 2.3; see Figure 2 (right). Note the x -axis is the number of training sequences X  X ince each sequence has 20 instances, the range is the same as Figure 2 (center). The kernel CRF is capable of getting under the 20% Bayes error floor of the non-sequence model, with both kernels and sufficient labeled data. However, the graph kernel is able to learn the structure much faster than the RBF ker-nel. Evidently, the high error rate for low label data sizes prevents the RBF model from effectively using the context. HMM with Gaussian mixtures . This more difficult dataset is generated from a 3-state HMM. Each state is a mixture of 2 Gaussians with random mean and covariance. The Gaussians strongly overlap; see Figure 3 (left). The transition probabilities favor remaining in the state, wit h a probability of 0.8, and to transition to each of the other two states with equal probability 0.1; we generate 100 se-quences of length 30. We use an RBF kernel with  X  = 0 . 5 (A graph kernel is slightly worse than the RBF kernel on this dataset, and is not shown.) We perform 20 trials for each training set size, and in each trial we perform clique selection to select the top 20 vertices. The center and right plots in Figure 3 show that the semiparametric KCRF again outperforms kernel logistic regression with the same RBF kernel.
 Figure 4 shows clique selection, with a training size 20 se-quences, averaged over 20 random trials. The regularized risk (left), which is training set likelihood plus regulari zor, always decreases as we select more vertices into the KCRF. On the other hand, the test set likelihood (center) and ac-curacy (right) saturate and even worsen slightly, showing signs of overfitting. All curves change dramatically at first , demonstrating the effectiveness of the clique selection al -gorithm. In fact, fewer than 10 vertex cliques are sufficient for this problem. For the protein secondary structure prediction task, we use d the RS126 dataset, on which many current methods have been developed and tested (Cuff &amp; Barton, 1999). It is a non-homologous dataset, since among the 126 protein chains, no two proteins share more than 25% sequence identity over a length of more than 80 residues (Cuff &amp; Bar-ton, 1999). The dataset can be downloaded from http: //barton.ebi.ac.uk/ .
 We adopt the DSSP definition of protein secondary struc-ture (Kabsch &amp; Sander, 1983), which is based on hydrogen bonding patterns and geometric constraints. Following the discussion in (Cuff &amp; Barton, 1999), the 8 DSSP labels are reduced to a 3 state model as follows: H &amp; G map to helix (H), E &amp; B to sheets (E), and all other states to coil (C). The state-of-the-art performance for secondary structure prediction is achieved by window-base methods, using the position-specific scoring matrices (PSSM) as input fea-tures, i.e., PSI-BLAST profiles, together with Support Vec-tor Machines (SVMs) as the underlying learning algorithm (Jones, 1999; Kim &amp; Park, 2003). Finally, the raw predic-tions are fed into a second layer SVM to filter out physi-cally unrealistic predictions, such as one sheet residue su r-rounded by helix residues (Jones, 1999). In our experiments, we apply a linear transformation L to the PSSM matrix elements according to L ( x ) = 0 for x  X   X  5 , L ( x ) = 1 2 + x 10 for  X  5  X  x  X  5 , and L ( x ) = 1 5 . This is the same transform used by Kim and Park (2003), which achieved one of the best results in the recent CASP (Critical Assessment of Structure Predictions) competiti on. The window size is set to 13 by cross-validation. Therefore the number of features per position is 13  X  21 (the number of amino acids plus gap).
 Clique selection. We use an RBF kernel with bandwidth  X  = 0 . 1 chosen by cross-validation. Figure 5 (left) shows the kernel CRF risk reduction as clique selection proceeds, when only vertex clique candidates are allowed (note there are always position independent edge parameters in the KCRF models, to prevent the models from degrading into kernel logistic regression), and when both vertex and edge cliques are allowed. (The kernel between vertex cliques is it is K ( z , z 0 ) = K ( x, x 0 )  X  ( y number of clique candidates is about 4800 (vertex only) and 20000 (vertex and edge). The rapid reduction in risk indicates sparse training of kernel CRFs is successful. Als o when more flexibility is allowed by including edge cliques, the risk reduction is much faster. The more flexible model also has higher test set log likelihood (center) although th is does not improve the test set accuracy too much (right). These observations are generally true for other trials too. Per-residue accuracy. To evaluate prediction performance, we use the overall per-residue accuracy (also known as Q quences respectively. For each size we perform 10 trials where the training sequences are randomly sampled, and the remaining proteins are used as the test set. For ker-nel CRF we select 300 cliques, again from either vertex candidates alone or vertex and edge candidates. We com-pare them with the SVM-light package (Joachims, 1998) for SVM classifier. All methods use the same RBF kernel. See Table 1. KCRFs and SVMs have comparable perfor-mance.
 Transition accuracy. Further information can be obtained by studying transition boundaries, for example, the tran-sition from  X  X oil X  to  X  X heet. X  From the point of view of structural biology, these transition boundaries may provi de important information about how proteins fold in three di-mension. On the other hand, those are the positions where most secondary structure prediction systems will fail. The transition boundary is defined as a pair of adjacent position s ( i, i + 1) whose true labels differ. It is classified correctly only if both labels are correct. This is a very hard problem, as can be seen in Table 2 and KCRFs are able to achieve a considerable improvement over SVM.
 Semi-supervised learning. We start with an unweighted 10 nearest neighbor graph over positions in both training and test sequences, with the metric being Euclidean distance in the feature space. Then the eigensystem of the normalized Laplacian is computed. The semi-supervised graph kernel is obtained with the function r (  X  i  X  200 eigenvalues. The rest eigenvalues are set to zero. We use the graph kernel together with the RBF kernel in KCRF. As a clique candidate is associated with a kernel, we now select two best candidates per iteration, one with the graph kernel and the other with the RBF kernel. We still run for 300 iterations for all trials. We also report the re-sults using Transductive SVMs (TSVMs) (Joachims, 1999) with the RBF kernel. From the results in Table 3, we can see that the semi-supervised graph kernel is significantly better than TSVMs on the 5-protein dataset while achieves no improvement on the other one. To diagnose the cause, we look at the graph together with all the test labels. We find that the labels are not smooth w.r.t. the graph: on aver-age only 54.5% of a node X  X  neighbors have the same label as that node. Detecting faulty graphs without using large amount of labels, and constructing better graphs remain fu-ture research.
 The approximate average running time of each trial, in-cluding both training and testing, is 30 minutes for KCRFs, 7 minutes for SVMs, and 16 hours for TSVMs. For KCRFs the majority of the time is spent on clique selection. Kernel conditional random fields have been introduced as a framework for approaching graph-structured classifica-tion problems. A representer theorem was derived which shows how KCRFs can be motivated by regularization the-ory. The resulting techniques combine the strengths of hid-den Markov models, or more general Bayesian networks, kernel machines, and standard discriminative linear class i-fiers including logistic regression and SVMs. The formal-ism presented is quite general, and should apply naturally to a wide range of problems.
 Our experimental results on synthetic data, while care-fully controlled to be simple, clearly indicate how sequenc e modeling, graph kernels for semi-supervised learning, and clique selection for sparse representations work together within this framework. The success of these methods in real problems will depend on the choice of suitable kernels that capture the structure of the data.
 For protein secondary structure prediction, our results ar e only suggestive. Secondary structure prediction is a prob-lem that has been extensively studied for more than 20 years; yet the task remains difficult, with prediction accur a-cies remaining low. The major bottleneck lies in beta-sheet prediction, where there are long range interactions betwee n regions of the protein chain that are not necessarily consec -utive in the primary sequence. Our experimental results indicate that KCRFs and semi-supervised kernels have the potential to lead to progress on this problem, where the state of the art has been based on heuristic  X  X liding win-dow X  methods. However, our results also suggest that the improvement due to semi-supervised learning is hindered by the lack of a good similarity measure with which to con-struct the graph. The construction of an effective graph is a challenge that may best be tackled by biologists and ma-chine learning researchers working together.
 This work was supported in part by NSF ITR grants CCR-0122581, IIS-0205456 and IIS-0312814.
 Altun, Y., Tsochantaridis, I., &amp; Hofmann, T. (2003). Hid-den Markov support vector machines. ICML X 03 .
 Belkin, M., &amp; Niyogi, P. (2002). Semi-supervised learning on manifolds (Technical Report TR-2002-12). Univer-sity of Chicago.
 Chapelle, O., Weston, J., , &amp; Schoelkopf, B. (2002). Clus-ter kernels for semi-supervised learning. NIPS X 02 . Collins, M. (2002). Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. Proceedings of EMNLP X 02 .
 Cuff, J., &amp; Barton, G. (1999). Evaluation and improve-ment of multiple sequence methods for protein sec-ondary structure prediction. Proteins , 34 , 508 X 519. Della Pietra, S., Della Pietra, V., &amp; Lafferty, J. (1997). In -ducing features of random fields. IEEE PAMI , 19 , 380 X  393.
 Joachims, T. (1998). Text Categorization with Support Vec-tor Machines: Learning with Many Relevant Features. ECML .
 Joachims, T. (1999). Transductive inference for text class i-fication using support vector machines. ICML X 99 . Jones, D. (1999). Protein secondary structure prediction based on position-specific scoring matrices. J Mol Biol. , 292 , 195 X 202.
 Kabsch, W., &amp; Sander, C. (1983). Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical features. Biopolymers , 22 , 2577 X 2637.
 Kim, H., &amp; Park, H. (2003). Protein secondary struc-ture prediction based on an improved support vector ma-chines approach. Protein Eng. , 16 , 553 X 60.
 Kimeldorf, G., &amp; Wahba, G. (1971). Some results on
Tchebychean spline functions. J. Math. Anal. Applic. , 33 , 82 X 95.
 Kumar, S., &amp; Hebert, M. (2003). Discriminative fields for modeling spatial dependencies in natural images. NIPS X 03 .
 Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Condi-tional random fields: Probabilistic models for segment-ing and labeling sequence data. ICML X 01 .
 Lanckriet, G., Cristianini, N., an Laurent El Ghaoui, P. B., &amp; Jordan, M. (2004). Learning the kernel matrix with semi-definite programming. Journal of Machine Learn-ing Research , 5 , 27 X 72.
 McCallum, A. (2003). Efficiently inducing features of con-ditional random fields. UAI X 03 .
 Pinto, D., McCallum, A., Wei, X., &amp; Croft, W. B. (2003). Table extraction using conditional random fields. SI-GIR X 03 .
 Sha, F., &amp; Pereira, F. (2003). Shallow parsing with condi-tional random fields. Proceedings of HLT-NAACL .
 Smola, A., &amp; Kondor, R. (2003). Kernels and regulariza-tion on graphs. COLT X 03 .
 Taskar, B., Guestrin, C., &amp; Koller, D. (2003). Max-margin Markov networks. NIPS X 03 .
 Zhu, J., &amp; Hastie, T. (2001). Kernel logistic regression and the import vector machine. NIPS X 01 .
 Zhu, X., Gharahmani, Z., &amp; Lafferty, J. (2003). Semi-supervised learning using Gaussian fields and harmonic
