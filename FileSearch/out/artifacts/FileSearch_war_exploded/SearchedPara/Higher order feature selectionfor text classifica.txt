 REGULAR PAPER Jan Bakus  X  Mohamed S. Kamel Abstract In this paper. we present the MIFS-C variant of the mutual information feature-selection algorithms. We present an algorithm to find the optimal value of the redundancy parameter, which is a key parameter in the MIFS-type algorithms. Furthermore, we present an algorithm that speeds up the execution time of all the MIFS variants. Overall, the presented MIFS-C has comparable classification accuracy (in some cases even better) compared with other MIFS algorithms, while its running time is faster. We compared this feature selector with other feature selectors, and found that it performs better in most cases. The MIFS-C performed especially well for the breakeven and F -measure because the algorithm can be tuned to optimise these evaluation measures.
 Keywords Feature selection  X  Text classification 1 Introduction Growing numbers of text information available on the Internet is raising interest in automatic analysis of text data. One such technique is text classification, which is a problem of automatically assigning free text documents to one predefined cat-egory from a set of possible categories [ 30 ]. This is accomplished using machine learning techniques.
 ument is represented as a list of words present in the document. Each word is then considered as a feature, and the document is represented as a feature vector. Then a feature-selection filters out the useless features, leaving the useful ones that are used by the classifier to make a decision on the document class.
 Some of these are chi-squared [ 29 ], information gain [ 18 ], mutual information [ 7 ] and odds ratio [ 23 ]. All of these techniques measure the correlation between each feature and the class, and the feature selection is performed by selecting the required number of features with the highest correlation values, and a comparison of these techniques can be found in [ 34 ]. However, all of these techniques are simple filters, which ignore the correlation among the features.
 that use higher order decisions and take the feature-to-feature correlation into ac-lection presented by Hall in [ 11 ] and Markov blanket feature selection presented by Koller and Sahami in [ 16 ]. In addition, we consider the mutual information feature-selection (MIFS) class of feature selector.
 in [ 17 ] as the MIFS-U. In particular, we focus on the MIFS-C variant presented by Bakus and Kamel in [ 2 ] and present a number of improvements. As part of all the MIFS-type algorithms, a redundancy parameter is required, and we present a search technique to find an optimal value for this parameter. We also present an optimised algorithm that skips the unnecessary calculations, which results in a training time improvement.
 selection and text classification. Section 3 presents the MIFS feature-selection algorithms. Numerical results are presented in Sect. 4 , and Sect. 5 offers some concluding remarks. 2 Background 2.1 Text classification The objective of the text classification is to infer a classification rule from a sample of labelled training documents, such that it is able to assign an unknown document to one of L possible classes C ={ C 1 ,..., C L } . Each document, d , is represented by a vector of features { F 1 ,..., F N } that describe the document. The most popu-lar representation is called the bag-of-words approach, where each feature corre-sponds to one individual word [ 19 , 30 ].
 ument is classified into one of two categories. The document either belongs to a given topic or does not. We test the effectiveness of the feature selection using the following classifiers: naive Bayes, Rocchio, K-nearest neighbor, C4.5 decision tree and support vector machine classifiers. 2.1.1 Naive Bayes (NB) The assumption behind naive Bayes classifier is that the features are independent of each other. This is certainly not the case with words in a document. However, classifier will perform poorly. Given a document d , we calculate the probability of each class C j as follows: egory and therefore does not play any role in the decision. Assuming that the features that make up the document vector, d , are independent, the probability of aclass C j is The probabilities are estimated from the training set using the add-one estimator (also called Laplace estimator). The document, d , is assigned to the class C j with the highest probability, P ( C j | d ) P ( d ) . 2.1.2 Rocchio Rocchio is an effective method using relevance judgements for query expansion in information retrieval and filtering [ 20 , 27 ]. Applying it to text classification, each document is represented as a vector using term frequency/inverse document frequency weighting [ 28 ]. Each class in the classifier is represented as a centroid (prototype vector) of the set of the document vectors that belong to the class. The prototype vector for class C i is given as The cosine measure is used to measure the similarity of the document vector and a prototype vector and the document is assigned to the class with the highest sim-ilarity. 2.1.3 K-nearest neighbor (KNN) K-nearest neighbor is an instance-based classification method that has been an ef-fective approach to a range of pattern recognition and text classification problems [ 6 ]. During the training phase, all the training documents are stored in a database as document vectors using the term frequency/inverse document frequency weighting.
 k -nearest documents in the training set. Using this subset, each class is given a weight as a sum of all the cosine values between the unknown document and the documents that belong to the class and the subset. The document is then assigned to the class with the highest weight.
 2.1.4 Decision tree (C4.5) C4.5 is a popular decision tree algorithm developed by Quinlan [ 26 ]asanexten-sion of the ID3 algorithm. The decision tree is an approach to build a classifier using a tree structure, where each node corresponds to a decision about one fea-ture. Each branch from the node corresponds to a possible value that the feature can take.
 and descends down the tree, making the decisions according to the values of the corresponding features. The leaf nodes contain the class labels, so once the leaf node is reached, the class is simply read from the node.
 training set and begins dividing this set into subsets, which in turn may be further subdivided. At each step, the division is done based on the feature with the highest gain ratio.
 The numerator, I ( C ; F i ) , is the mutual information between the class labels C and the feature F i , and it determines the amount of information that the feature provides about the class. High value indicates that a feature is able to separate the documents well into class labels. The denominator, H ( F i ) , is the entropy of the feature for the given set of documents, where higher value indicates that the documents are subdivided into more branches. The mutual information tends to have inflated values if the number of branches is larger, and to counter this, it is divided by the entropy. 2.1.5 Support vector machines (SVM) Support vector machines have successfully been used to classify text [ 13 ]. Con-sider the problem of separating the set of training vectors belonging to two sepa-rate classes. Given the set of training example vectors, where the vector d i corresponds to the i th document vector and y i is the class label with a value  X  1 for class C 1 and + 1 for the class C 2 .
 such that The linear hyperplane with the largest margin is known as the optimal separating hyperplane, which separates all vectors without error and the distance between the closest vectors to the hyperplane is maximal [ 33 ]. Hence, the hyperplane that separates the data optimally is the one that minimises subject to the constraints of Eq. ( 7 ). This problem is difficult to handle numeri-cally, and therefore Lagrange multipliers are used to translate the problem into an equivalent quadratic optimisation problem [ 33 ].
 kernel function, K ( d i , d j ) , which is used to obtain the inner product of ( d i )  X  ( d The linear kernel has been reported to work well in the text classification domain [ 7 ]. In our experiments, we use the linear kernel and the SVM light software [ 13 ]. 2.2 Feature selection u features, F ={ F 1 ,..., F u } , and class label C that we wish to predict from the feature set, find the subset F with n features, that maximises a certain performance measure of the prediction performance for a given classifier.
 evaluated on its quality. The feature-selection strategies differ from each other in how the candidate sets are generated and evaluated. There are three popular search techniques to generate candidate sets: forward selection , backward elimination and random selection [ 4 , 21 ]. In the case of text classification, the number of can-didate features is very large, which makes the backward elimination and random selection impractical. In this paper, we use the forward selection algorithm. and wrapper . The filter approach evaluates the candidate features directly based on statistical correlation, information theory, or other methods. This evaluation is typically independent of the classifier algorithm and the performance measure used.
 tion measure into account. In this approach, the feature selection wraps around the classifier, such that each candidate feature set is tested using the actual classifier algorithm, and the evaluation function, such as accuracy, breakeven, or F -measure. The presented MIFS algorithm is a combination of a filter and a wrapper approach. The base algorithm is a filter approach that uses a wrapper approach to estimate one of the parameters.
 2.2.1 Simple filters Using a simple filter approach, a scoring metric is used to give a score to each feature, which estimates how useful the feature is to the classification task. These metrics are generally independent of the other features, making them insensitive to correlated features. The feature subset is generated by selecting the required number of features with the highest score. This approach is very efficient in terms of computational cost. The simple filter feature selection has been extensively we investigated in this paper are presented in Table 1 . 2.2.2 Higher order filters One of the problems with simple filters is the fact that, in the text domain, many of the features are correlated and the simple filters ignore this fact. Consider a feature set with a number of features that are very informative about the class; however, they are also highly correlated. The simple filters will indicate that all the features are useful; however, because of the correlation, only a small subset are actually needed for the classification and the rest are redundant. The higher order filters are able to select only the subset that is useful but not redundant.
 is based on ideas from information theory and probabilistic reasoning. Because the goal of an induction algorithm is to estimate the probability distributions over the class values, given the original feature set, feature subset selection should attempt to remain as close to these original distributions as possible. The features are re-moved to preserve the feature distribution over the class labels, where the mutual information is used to measure the difference between the distributions. each step, the quality of a feature F i is given as where F is a set of already selected features. The forward algorithm starts with F ={} , and at each step, a feature F is added to F until the desired number of features is reached. During the first iteration, the set F is empty and we use I ( C ; F i ) instead of I ( C ; F i | F j ) to calculate the quality of the feature F i .
 lection (CFS). A desirable feature subset contains features highly correlated with the class yet uncorrelated with each other. In test theory [ 10 ], the same principle is used to design a composite test (the sum or average of individual tests) to predict an external variable of interest. In the case of feature selection, the features are considered tests that measure traits related to the external variable, i.e. the class. where the selected feature subset, F , contains n features, r fc is the average feature-class correlation and r ff is the average feature X  X eature correlation. The forward algorithm starts with F ={} and, at each iteration, adds a feature F i such that the quality (defined by Eq. ( 12 )) of the resulting set is maximised. where I ( X ; Y ) is the mutual information between random variables X and Y , while the H ( X ) and H ( Y ) are the corresponding entropies. 3 Mutual information feature selection In the process of feature selection, it is desirable to remove the features that are irrelevant or redundant to the task of classification, which can be formulated in information theoretic terms. The set F is selected to maximise the mutual in-formation between the class labels and the selected feature set, i.e. I ( C ; F ) or, alternatively, minimises the entropy, H ( C | F ) .
 error of any reconstruction decision function in terms of conditional entropy of a decision about a random variable X given another random variable Y . The error, , is bounded from bellow by Let the variable Y be the selected feature set F and the variable X be the class label C ,then H ( C | F ) gives a lower bound on the classification error. Note that minimising H ( C | F ) is equivalent to maximising I ( C ; F ) .
 a candidate set, F , of all the possible features and, at each step, adds a feature F i from the candidate set to F . The inclusion of feature F i to the set F results in the mutual information between the feature set and the class label as Maximising the term I ( C ; F i | F ) over all possible features F i also maximises the mutual information between the feature set and the class label and thereby minimises the lower bound on the error. This minimisation does not minimise the error directly but rather minimises the lowest achievable classification error. The term I ( C ; F i | F ) represents the amount of information about class C that is added to the already selected set F by inclusion of the feature F i . The general mutual information feature-selection (MIFS) algorithm is as follows.
 number of features grows, the calculation of I ( C ; F i | F ) becomes very diffi-cult because the required joint probability is very difficult to estimate. Instead, a number of approximations to estimate this term are presented. 3.1 Information gain (IG) The simplest approach to estimate I ( C ; F i | F ) is to ignore the dependence on the selected set F , which essentially ignores the effect of correlation among the features. The information added by inclusion of feature F i is given as Using this equation, step 2 in Algorithm 3 becomes 3.2 Mutual information feature selection (MIFS) The mutual information feature-selection (MIFS) approach presented in [ 3 ] takes the correlation among features into account. Given the considered feature, F i , the information gain, I ( C ; F i ) , gives the amount of information that the feature provides about the class. From this, we subtract the correlation of the features, expressed as a sum of mutual information between feature F i and the features already selected in F .The I ( C ; F i | F ) is given as Here,  X  is called the redundancy parameter, which is used to control the effect of the redundancy among the input features. If  X  = 0, the correlation among input features is ignored and the result is the same as information gain feature selection. value between 0 . 5 and 1 works well for many classification tasks. We found that the optimal value is sensitive to the classifier used, number of features selected and the data set used. In Sect. 3.6 , we present a technique to estimate the value of  X  given all the system parameters. Using MIFS, step 2 in Algorithm 3 becomes 3.3 MIFS with uniform information distribution (MIFS-U) An improvement to MIFS, called the MIFS-U (mutual information feature selec-tion under uniform information distribution), was proposed by Kwak and Choi in [ 17 ]. Given a feature under consideration, F i in F , and a feature F j in the already selected set F , the conditional mutual information is given as The term ( C ; F i ; F j ) is the amount of information about the class label C that is shared by both features, F i and F j , and is given by The term in Eq. ( 18 ) is approximated with the assumption that the conditioning on class C does not change the ratio of the entropy of F j and mutual information between F j and F i , i.e. the following relation holds: Solving for the term I ( F i ; F j | C ) in Eq. ( 20 ) and substituting it into Eq. ( 18 ) results in Using Eq. ( 21 ), we can formulate I ( C ; F i | F ) in the feature-selection algorithm in a similar way to Eq. ( 17 ): Using MIFS-U variant of the algorithm, step 2 in Algorithm 3 becomes 3.4 MIFS with common information (MIFS-C) For the MIFS-U, the approximation in Eq. ( 20 ) was used because the feature selection was used on features with real-valued features. The calculation of I ( H ( F involve summations rather than integrals. The summation calculation does not pose a significant computational problem and therefore this approximation is not required.
 two features have common information about the class, i.e. they are redun-dant. A negative value indicates that both features combined give more in-formation than each feature alone and a zero indicates that the features are independent.
 mation, which means subtract the positive values from the information gain. We define the term + ( C ; F i ; F j ) as the positive information about class C shared between features F i and F j : I ( C ; F i | F ) becomes Using MIFS-C, step 2 in Algorithm 3 becomes 3.5 Implementation of the MIFS algorithms The MIFS type of algorithms select features one at a time. At each Iteration, the quality of a feature F i from the candidate set F given the already selected set F is calculated according to one of the Eqs. ( 16 ), (17), (22) or ( 24 ). These equations can be generalised as where f 1 ( i ) measures the quality of the considered feature F i and f 2 ( i , j ) is a measure of a correlation between features F i and F j .
 fined in Eq. ( 25 )). The difference between the quality of this feature at iteration l and the previous iteration l  X  1is  X  f 2 ( i , k ) ,where F k is the feature added to F at iteration l  X  1. This can be formulated in the following definition: where F k is the feature selected at iteration l  X  1.
 and the redundancy parameter is defined as nonnegative, i.e.  X   X  0. Based on this observation, as the quality of each feature F i begins as f 1 ( i ) and as the features are selected and added to F , the quality monotonically decreases. Therefore, the quality is updated for all the remaining features.
 ray term 1 , which corresponds to f 1 ( i ) term, is calculated once for all features and does not change during the selection algorithm. The array term 2 corresponds to summation of the f 2 ( i , j ) terms and is updated as features are added to F . The quality for a given feature F i is calculated from term 1 and term 2 arrays as tion is selected, the entries in term 2 corresponding to the remaining features are updated. This basic algorithm is given in Algorithm 3.2.
 of the elements of the array term 2 do not always need to be updated. Consider the update step of the algorithm, where the first feature F i is updated, resulting in a new value for term 2 [ i ] and therefore new quality. All the features with the current quality (i.e. before the update) that are lower than the new quality of the feature F i (after the update) need not be updated.
 and therefore would be selected over all the unupdated features. However, at some time in the future, feature F i may be selected, and some of the features that were not updated may in fact be the most optimal. Therefore, the omitted updates have to be carried out to find out the next optimal feature. The hope is that, if feature F i is never selected (because the required number of features is reached), the updates need not be carried out. As a result, the update operations are not omitted but rather delayed.
 search, the variable maxV al holds the maximum quality found so far and the corresponding index of the feature is stored in max I dx .Thevariable max Sel holds the number of features that were already selected and the array count holds the number of terms of the summation that have been added to term 2 .To find the feature with the highest quality, all the feature in the candidate set are examined.
 term 2 [ i ] . If this quality is less than the best quality so far (stored in maxV al ), then we skip this feature and proceed on to next feature, i.e. we do not update term 2 [ i ] . The feature is not optimal, and further update of term 2 [ i ] would reduce the quality, which makes the update not required.
 maxV al , then we proceed to update the feature. We add the terms f 2 ( i , j ) one by one to term 2 [ i ] , while incrementing count [ i ] . After each addition, we check if the quality is still better than maxV al and count [ i ] &lt; max Sel ,i.e.wehavenot summed up all the summation terms in Eq. ( 25 ). If the quality of this feature falls below the best quality, the feature is skipped.
 the new optimal feature and maxV al and max I dx are updated. Once all the features have been examined, then index of best feature is found in max I dx and the corresponding quality is found in maxV al . The algorithm is given as Algorithm 3.2. 3.6 Redundancy parameter estimation All the presented variants of the MIFS feature-selection algorithm (with the excep-tion of the IG) require the redundancy parameter  X  to control the effect of feature redundancy. In [ 3 ], Battiti suggests that an empirical value between 0 . 5and1is reasonable for most applications; however, no formal method is presented on how to select the optimal value for a given application. We present an algorithm that uses the wrapper approach [ 15 ] that searches for a value of  X  that maximises cer-tain performance measure. The benefit of this algorithm is that we can tune a clas-sification system to maximise different performance measure, such as breakeven or F -measure, unlike many other systems that are designed for accuracy only. training set, to estimate the performance measure for the entire classification sys-tem (composed of a feature selector and a classifier). The estimate is performed using a cross-validation test, which gives us an estimate of the mean and variance for the measure, from which we can calculate statistical significance between two systems with different values of  X  , and the optimal value of  X  is the one that re-sults in the most statistically significant improvement over the case of  X  = 0, i.e. information-gain feature selector.
 system using a particular value of  X  , we take the training set and randomly split trained using  X  and the validation training set. The feature selection is applied to the validation feature train set and the classifier is trained using this set. The combined feature selector and classifier are then tested using the validation test set and the performance measure is calculated. To get an estimate of the mean and the variance, we perform different validation training and test splits. mance difference between the two systems. The block diagram for the test train-ing splits is shown in Fig. 1 . In practice, we found that using four folds for the cross-validation is sufficient and using more folds does not greatly improve the overall performance. Higher fold count leads to a better estimate on the variance and mean; however, the training time also increases.
 for system with  X  =  X  i is higher than for the system with  X  =  X  j . The algorithm starts with some initial value of  X  =  X  0 and then tries different values for  X  until significance relative to the case of  X  = 0 is considered to be the optimal value, i.e. the optimal value  X  opt is given as The corresponding T -test value for  X  opt is given as T opt .
 it first searches upward and then downward. The searches are logarithmic, where subsequent values of  X  are generated by multiplying the current value by  X  or dividing by  X  for the upward and downward searches, respectively. In our exper-iments, we found that the value of  X  = 2 worked well.
 largest positive value of T ( X , 0 ) and T ( X ,  X  0 ) seen so far (not including the current iteration). These values are initialised to zero at iteration 0 and updated during each iteration i according to The algorithm terminates once T ( X  i , 0 )  X  T opt , i and T ( X  i , X  0 )  X  T i ,i.e.there is no more improvement in the optimality criterion define in Eq. ( 27 )orthe improvement over the starting point  X  =  X  0 . This algorithm is summarised as follows: 4 Numerical results The performance of the presented feature-selection algorithms was tested with two different text classification data sets. The first set is the 10 largest topics categories from the Reuters-21578 data set. To train and test the different classifier systems, we used the ModApte split with 9603 train documents and 3299 test documents. As the second set, we used all the categories from the 20 Newsgroups data set. The set contains 20,000 documents, and from these documents we randomly selected 3000 as a training set and 3000 as a test set. To get a sense of the variability of the results, we performed random test/training splits for the second test set 4 times, and we report the mean and the standard deviation of the results.
 the source text, all the words were translated to lower case and the leading and trailing punctuation removed. All digits are projected onto a single token  X  0  X , such that  X  3.14  X  X nd X  9.57  X  are both represented as  X  0.00  X . Features that occur less than five times are considered statistically not reliable and are also removed. where a different classifier is constructed for each category. Each of these clas-sifiers determines if the document belongs in the corresponding category. All the feature-selection algorithms were tested with the naive Bayes (NB), Rocchio, K-nearest neighbor (KNN), decision trees (C4.5) and a linear support vector machine (SVM). 4.1 MIFS algorithms training time Figure 2 shows the timing of the MIFS, MIFS-U and MIFS-C for both the basic (Algorithm 3.1) and the optimised (Algorithm 3.2) versions of the algorithm. The results are reported in seconds and the experiments were carried out using 3000 documents from the Reuters data set using a known value for the redundancy parameter,  X  = 0 . 1. This would correspond to one fold of each cross-validation test or the final feature selection once  X  opt is determined. The timing was obtained on a SUN Ultra 80 running at 450 MHz with Solaris 8.
 the basic MIFS or MIFS-U. This is due to the fact that the conditional mutual in-formation in MIFS-C with three nested summation takes longer to calculate than the mutual information in MIFS and MIFS-U, which only has two nested summa-tions. The basic MIFS and MIFS-U run in approximately the same time because their calculations are similar in complexity. Also, all three of the basic algorithms run in approximately linear time with respect to the number of features selected. sponding basic versions and, in some cases, up to 10 times faster. The improve-ment of the optimised algorithm over the basic algorithm is significant for low numbers of features and diminishes as more and more features are selected. The reason for this is because the optimised versions delay the update for some of the features. For the case of low number of features, many of the delayed updates are never actually executed because the required number of features is reached before the updated values are required and the algorithm terminates. However, for large numbers of features, many of the delayed calculations are eventually carried out and, in the extreme case of selecting all the features, the running time is the same. rithms for the complete feature-selection algorithm (including the redundancy parameter estimation in Algorithm 3.3) and the final feature selection with  X  opt . Because the classifier is part of the cross-validation in the redundancy parameter search, we see slightly different results for the different classifiers. algorithms, and this is because the IG feature selector does not perform the redun-dancy parameter estimation and, during the feature selection, this algorithm does not calculate the redundancy function, f 2 () . From the feature selectors that per-form the redundancy parameter estimation, we see that the MIFS is consistently slowest to train. Furthermore, the MIFS-C is almost always the fastest to train. actual feature selection is simply a matter of selecting the appropriate features from a list. The feature-selection time is the same, regardless of the feature selector used to obtain the optimal set. Therefore, using MIFS-type feature selectors has a high training cost; however, during the testing stage, the execution time should be comparable with any other feature selector. 4.2 MIFS algorithms classification performance Ta b l e s 2 and 3 show the breakeven and F -measure for all the MIFS feature selector types and all the classifiers for the Reuters and 20 Newsgroups datasets. We see that the MIFS, MIFS-U and MIFS-C almost always perform better than the IG feature selector and, in some cases, the improvement over the IG feature selector is substantial. From the four feature selectors, IG is the only one that does not attempt to remove the redundancy from the feature set. The features in a text clas-sification task are highly correlated with each other, and we believe that removing this redundancy is the reason for the improvement.
 performance of all the feature selectors. We believe that the reason for this im-provement lies in the way each of the feature selectors calculates I ( C ; F i | F ) . The MIFS algorithm estimates this quantity in Eq. ( 17 ) by subtracting from the information gain the redundancy. This is calculated by the mutual information between the selected feature F i and all the features in the currently selected set F . The MIFS-U estimates this in Eq. ( 22 ) in a similar way, where the difference is the approximation of the calculation. Both the MIFS and MIFS-U remove redundancy from the feature set independent of the class labels. We feel that the improvement in MIFS-C is the addition of the conditioning on the class label C in Eq. ( 24 ), prediction of class label C . 4.3 Simple filters and higher order feature selectors Ta b l e s 4 and 5 show the breakeven and F -measure for the simple filters and higher order feature selectors. The following simple filters are presented: information gain, chi squared, odds ratio, and the following higher order feature selectors are presented: correlation feature selector (CFS), Markov Blanket and MIFS-C. From better than the simple filters. In particular, the MIFS-C performs almost always better than the best of the simple filters. The Markov blanket outperforms the simple filters in many cases, especially for the case of 320 selected features while the CFS outperforms the simple filters only a few times. In most of the cases, the MIFS-C was the best performing feature selector. 5 Conclusions In this paper, we presented the MIFS-C mutual information feature-selection al-gorithm. We presented an algorithm to find the optimal value of the redundancy parameter, which is a key parameter in the MIFS-type algorithms. Furthermore, we presented an algorithmic improvement in the train time of all the MIFS vari-ants. Overall, the presented MIFS-C has improved classification breakeven and F -measure performance compared with the other MIFS algorithms. The training time for the MIFS-C is also lower than the other MIFS algorithms.
 literature. As a comparison, we used the following feature selectors: information gain, chi squared, odds ratio, CFS and Markov blanket. The MIFS-C performed better in most cases compared with all the feature selectors.
 References
