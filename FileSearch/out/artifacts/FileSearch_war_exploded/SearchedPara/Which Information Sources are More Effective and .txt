 It is common that users are interested in finding video seg-ments, which contain further information about the video contents in a segment of interest. To facilitate users to find and browse related video contents, video hyperlinking aims at constructing links among video segments with relevant information in a large video collection. In this study, we explore the effectiveness of various video features on the performance of video hyperlinking, including subtitle, meta-data, content features (i.e., audio and visual), surrounding context, as well as the combinations of those features. Be-sides, we also test different search strategies over different types of queries, which are categorized according to their video contents. Comprehensive experimental studies have been conducted on the dataset of TRECVID 2015 video hy-perlinking task. Results show that (1) text features play a crucial role in search performance, and the combination of audio and visual features cannot provide improvements; (2) the consideration of contexts cannot obtain better re-sults; and (3) due to the lack of training examples, machine learning techniques cannot improve the performance. Video Search, Video Hyperlinking
With the explosive growth and the widespread accessi-bility of multimedia contents on the Web, video is becom-ing one of the most valuable sources to assess information and knowledge [4, 19]. When consuming video content, users are highly interested in finding further information on some aspects of the topics of interest associated with a video segment. Therefore, it is crucial to develop effective video search and hyperlinking to help users explore, nav-igate and search interesting video contents in audiovisual archives. Video hyperlinking aims at linking a video seg-ment (or video anchor) to other video segments in a video collection, based on similarity or relatedness. Formally, the video hyperlinking task is defined as 1 : given a set of test videos with metadata and a defined set of anchors, each de-fined by start time and end time in the video, return for each anchor a ranked list of hyperlinking targets . Therefore, video hyperlinking enables users to navigate between video segments in a large video collection [3].

To facilitate the development and advancement of video hyperlinking systems, video hyperlinking has become a com-petition task since 2012 in MediaEval [6]. Standard test collections are provided and metrics are defined for the eval-uation of developed systems. The task is defined to find rel-evant anchors or short segments (e.g., 2 minutes) of video contents given a set of query anchors. Thus, the hyperlink-ing is generally addressed within an information retrieval framework. As videos in the test collection could be in hours of length, video hyperlinking consists of two steps: (1) video segmentation -separate a video into a number of short video segments and (2) video retrieval -retrieval potential links to videos or video segments. 2 For video segmentation, many systems apply fixed-length segmentation to separate videos into fixed-length of segments. Other video segmen-tation methods have also been developed and studied, such as video shot based and semantic-based segmentation , how-ever, no evidence shows that those segmentation methods are better than the fix-length segmentation method. More research efforts were devoted into the development of effec-tive retrieval methods, including the explorations of differ-ent information sources (e.g., subtitle, metadata, transcrip-tions, segment surrounding context, name entity, enrichment of concept and synonyms, as well as audio and visual fea-tures [8]) and search strategies (e.g., combination with or re-ranking with visual features, combination of video-level and segment-level retrieval).

In this paper, we use the fixed-length video segmentation method and mainly focus on studying the effects of different types of information sources on the performance of video hy-perlinking, including text (subtitle, metadata, transcription) and a variety of video content (audio, visual and motion) fea-tures. Nine different text-based retrieval methods are used based on the text information with and without the consid-eration of surrounding context (around the query or target segment). Besides, we also study the performance of multi-modal feature combination using weighted linear combina-tion. Further, we evaluate the performance of different com-bination weights over different categories of queries, which are classified according to their contents. Experiments show that surrounding context and video-content features have little contribution on the performance improvement.
In this study, we address the video hyperlinking as an ad-hoc retrieval problem. Given a query anchor indexed with certain features, video segments in the test collection are also indexed with the same features, and then a retrieval method is used to search and return the most relevant video segments to this query. In our experiments, we (1) first sep-arate each video in the collections into 50s fixed-length seg-ments without overlapping, as this configure achieved good performance in the CUNI2014 video hyperlinking system [8]; (2) different types of features are extracted from each seg-ment; (3) a variety of retrieval methods are explored; and (4) different strategies are used to combine the results ob-tained based on different features. In the next, we describe the features used and the retrieval methods considered in experiments.
We first introduce the dataset used in the study. Our study is performed on the dataset of TRECVID 2015 Video Hyperlinking task. For the ease of understanding, we first clarify several terminologies: (1) video refers a long video clip (usually longer than 20 minutes); (2) both video segment (or segment ) and video anchor (or anchor ) refer to a short segment of a video (usually less than 2 minutes), defined by a start timestamp and an end timestamp; (3) video meta-data (or metadata ) contains the title and a short program description of a video; (4) subtitle is the manual annotation of the speech in a video; and (5) transcript refers to the annotation obtained by using automatic speech recognition (ASR) methods.

The TRECVID 2015 Video Hyperlinking dataset consists of around 2686 hours of BBC television broadcasts content from 05/12/2008 to 07/31/2008. The data is accompanied by metadata, subtitle, three kinds of ASR transcripts (gen-erated by LIMSI [12], LIUM [16], and NST-Sheffield [14] respectively), two versions of key concepts (detected by two concept detectors), and the prosodic audio features [7].
The development set contains 30 query anchors. For each of them, a set of ground-truth anchors is provided. The number of positive segments for each query anchor varies from 17 to 122. Notice that many positive segments are from the same video from which the corresponding query anchor is extracted. The test set contains 135 query anchors, which are for the final evaluation in the competition.
Text Features We explore the effectiveness of different sources of textual features, namely the subtitle and three kinds of ASR transcripts. For each type of the features, we also consider their combinations with metadata and sur-rounding contexts. The tested lengths of surrounding con-text include 50s, 100s, and 200s. Hence, for each of subtitle, LIMSI, LIUM, and NST-Sheffield, there are eight indexing methods. 3 For a video segment, the combination of subtitle and metadata is to concatenate the subtitle of this segment with the metadata of the video from which the segment is extracted. Similarly, the combination of subtitle, metadata , and 50s surrounding context is the concatenation of the sub-title of this segment , 50-seconds-length passage before and after the segment , and the metadata of the corresponding video . All the textual sources are processed by punctuation &amp; stop-words removal and capitalization normalization.
Retrieval Methods For each type of feature, we ex-periment with nine different retrieval models: (1) BM25, (2) DFR version of BM25(DFR-BM25) [9], (3) DLH hyper-geometric DFR model (DLH13) [1], (4) DPH [2], (5) Hiemas-tra X  X  Language Model (Hiemastra-LM) [11], (6) InL2 -in-verse document frequency model for randomness, Laplace succession for first normalization, and normalization 2 for term frequency normalization [9], (7) TF-IDF, (8) LemurTF-IDF 4 , and (9) PL2 -poisson estimation for randomness, Laplace succession for first normalization, and normaliza-tion 2 for term frequency normalization [9]. We used Terrier IR system to run experiments with these retrieval methods (with default parameters) with different textual sources.
For the content-based method, we use various video fea-tures in the retrieval task. These video features include mo-tion features (e.g., improved dense trajectory [13]), audio features (e.g., MFCC) and visual semantic features [15]. Af-ter explicit feature mapping [18], the cosine similarity is used as the relevance score.
We explore the effects of the combination of different fea-tures, based on the assumption that different features can capture different aspects of video content.

Weighted Linear Combination (WLC) The relevant score of a video segment with respect to a query is computed by a weighted linear combination of the relevant scores ob-tained by different features. Let wlc ( q, v ) be the final rel-evance score obtained by the weighted linear combination, and rel ( f i ) be the relevance score obtained based on feature f . Given the selected feature { f 1 , f 2 ,  X  X  X  , f n } , the wlc ( q, v ) is computed by: wlc ( q, v ) = w 1  X  rel ( f 1 ) + w 2  X  rel ( f 2 ) +  X  X  X  + w where { w 1 , w 2 ,  X  X  X  , w n } are the linear combination weights, which characterize the contribution of different features on the final performance. These weights are tuned on the train-ing set. Due to the lack of training examples (refer to Table 1), we only used five features in our experiments. These features are selected based on on their individual performances and the consideration of feature heterogene-ity. Specifically, the selected features are: subtitle metadata LemurTF-IDF 6 , subtitle metadata DPH , keyconcept Lemur TF-IDF , improved trajectory , and MFCC . Keyconcept Lemur TF-IDF denotes the TF-IDF method based on the key con-cepts of keyframes. The key concepts are the concepts de-tected in the keyframes with normalized scores greater than 0.7, using the Leuven X  X  concept detectors of 1537 ImageNet concepts [17]. For a video segment, its key concept based representation is the concatenation of key concepts detected in all the keyframes of this segment.

For different types of videos, their contents or topics could be very diverse. Different types of features contribute to the representation of video contents differently. It would be useful to use different weights for different video cate-gories. Thus we classify the videos into categories based on the programme category ontology of BBC news. Due to the limited query examples in the development dataset, we further group the videos into two broad categories: In general, videos in the sub-categories of Category 1 en-joy more similar contents in text, audio, and visual features (such as news and music), and thus queries in Category 1 are easier to achieve better results. In contrast, for videos in the same sub-categories of Category 2, although their contents are about the same topic, the contents could be diverse in contents. For example, videos about history or health could be very different in words and scenes.

To evaluate the performance of this method, we randomly split the query anchors in the development set into training set and testing set. Table 1 presents the details of training set and testing set for global weighted linearly combination (GWLC -without the consideration of video categories) and categorized weighted linear combination (CWLC). Notice that the number of training examples is very small, espe-cially for the CWLC method, which limits the performance of the weighted linear combination.
 Table 1: Sizes of training set and testing set for the GWLC (whole) and CWLC (category 1 and category 2) methods.
Learning to Rank [10] We also explored the use of learning to ranking techniques for refining the retrieval re-sults. The retrieval scores obtained by different features are used as the input of different learning algorithms (e.g., linear regression, Naive Bayes, SVM, etc.). Unfortunately, these techniques cannot improve the performance, due to the lack of well-labeled data. Due to the space limitation, we have not reported the results of these methods in the following.
To evaluate the performance of video hyperlinking sys-tems, the top ranking results of submissions are accessed using a mechanical turk (MT) crowdsourcing approach. A test assessment on a smaller part of the data by a local team of target users is used to identify potential discrepancies be-tween the MT workers X  judgments and those of the target user group. Descriptions given by the anchor creators (an-chor descriptions, description and format requested targets) are used for evaluation purpose. In the generation of ground truth, only a subset of the submissions for each query will be used in evaluation. To reduce the workload of evaluators, for the anchors longer than 2 minutes, only the first two minutes will be used as the basis of relevance assessment. For more details about hyperlinking evaluation, please refer to [5]. The submissions are evaluated based on the preci-sion at a certain rank measure, adapted to unconstrained time segments. In this paper, we report the performance on evaluation metrics of Precision@ { 5 , 10 , 20 } and MAP.
In the next, we report the experiment results of different methods on dataset. The results of content-based methods have not presented because of the overall poor performance. Table 2: Results of using different transcripts, metadata, retrieval methods and contexts. In each row, the retrieval method is the best retrieval methods among the nine tested methods for the corresponding text source. 50s, 100s and 200s refer to the lengths of contexts. Please refer to Sect. 2.2.1 for the retrieval method in the  X  X ethod X  column: (1) BM25, (3) DLH13, (4) DHP, (5) Hiemastra-LM, (8) LemurTF-IDF, and (9) PL2. NST refers to NST-Sheffield transcript.

Text-based Retrieval Method Table 2 shows the re-sults of text-based retrieval methods using different text sources. For each type of text source, only the best perfor-mance obtained by the nine retrieval methods is reported. As a large set of text-based retrieval methods (different text sources and different retrieval methods) has been explored, we have not presented the results of all methods. The re-sults are grouped into three groups in the table. As the performance of using subtitle is much better than the use of ASR transcripts (LIMSI, LIUM and NST-Sheffield), we did not show the performance of ASR transcripts with the consideration of context. The performance based on ASR transcripts is limited by the speech recognition accuracy. Among the three ASR transcripts, LIMSI obtains the best performance, followed by LIUM. Not surprising, with the consideration of metadata, the performance of ASR tran-scripts can be significant improved, as the metadata is man-ually annotated and summarizes the video contents. The combination of subtitle and metadata obtains higher MAP over subtitle while slightly lower precisions on top results. Recall that the metadata is a summary of the contents in a video. Supppose a query segment v q is extracted from video V and a video segment v s is extracted from V 0 ; and V and V 0 is about the same topic, namely, their metadatas contain very similar content. The consideration of metadata in retrieval will increase the similarity score of v s with re-spect to the query v q , and thus move the video segment to a higher ranking position in the result list. If v s is irrelevant, the consideration of metadata may cause the video segment v s in a relatively high position, resulted in the decrease of precision; on the other hand, if v s is indeed relevant to the query v q , the resulted higher position due to metadata will lead to the increase of MAP.

From the results of the third group in the table, the consid-eration of context data makes the performance significantly decreased. The results imply that the incorporation of con-text data introduces noisy data, which misleads the search of relevant segment. By comparing the search methods of dif-ferent text sources, it can be found that better performances are obtained by the vector space (LemurTF-IDF) method for text information without context (i.e., relatively short documents), and better performances are obtained by prob-abilistic methods with the consideration of contexts (i.e., relatively long documents).

Weighted Linear Combination Table 3 reports the performance of weighted linear combination methods. Be-cause the performances of different queries varied in large ranges, we list the corresponding performance of the test queries using Subtitle Metadata LemurTF-IDF for compar-isons. Obviously, the queries from Category 1 obtained much better results than queries from Category 2. By com-paring with the performance of weighted linear combination methods, we can observe that the performance decreases with the combination of other features based on the simple late fusion method.
 Table 3: Results of weighted linear combination methods.
In this section, we present the results of three methods on the test data in the final evaluation of the TRECVID 2015 Video Hyperlinking task: (1) Subtitle Metadata LemurTF-IDF (SM LemurTF-IDF), (2) Global Weighted Linear Com-bination (GWLC), (3) Categorized Weighted Linear Combi-nation (CWLC). Table 4 shows the results of the submitted runs on the test data. The same conclusions can be ob-served: the best performance is obtained by only using tex-tual data. It is worth mentioning that the results of these methods achieved top positions in the competition.

In this paper, a large set of textual and video content features on the performance of video hyperlinking has been studied. The results show that the video hyperlinking per-formance relies on manual annotations (subtitle and meta-data). The performance based on the ASR transcriptions is still far from the one achieved by manual annotations, while it is much better than audio, visual and motion features. The combination of surrounding context information will decrease the performance. The use of video content based features (audio, visual, and motion) has negative effects on the performance of textual features. Due to the limitation of well-labeled data, it is difficult to study the effectiveness of machine learning techniques on video hyperlinking task.
