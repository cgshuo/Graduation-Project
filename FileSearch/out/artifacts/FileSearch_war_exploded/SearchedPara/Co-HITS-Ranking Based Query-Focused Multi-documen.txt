 lem. A revival of interest on multi-document summarization is spurred in the circum-and digested more conveniently. 
Multi-document summarization aims to provide a highly comprehensive over-view of a document set. As a particular kind of multi-document summarization, query-focused summarization exhibits high practicability in many demand-driven applications and a great amount of research has been concerned. In [1], a query-biased summary was created by incorporating the content similarity between each sentence and the given query into a generic multi-document summarizer. In [2], a novel query expansion method was presented to improve the sentence ranking re-sult. Wei et al. proposed a cluster-sensitive graph model and the corresponding iterative algorithm for query-focused multi-document summarization [3]. A variety of graph-based sentence ranking approaches have also been proposed recently [4,5,6,7]. 
However, these methods either make uniform use of inter-sentence recommenda-tion to evaluate the sentence's significance without considering the influence of the document-level information or only divide the links between sentences into intra-document relationship and inter-document relationship without considering the sentence-to-document correlation strength. So in this study, a novel Co-HITS-Ranking based extractive approach is proposed to extend the existing work by natu-rally fusing three kinds of relationships between sentences and documents, either homogeneous or heterogeneous, in a unified two-layer graph model. Experiments have been performed on the DUC benchmark datasets, and the results demonstrate that the proposed Co-HITS-Ranking based approach can outperform both the lead baseline method and the sentence-based manifold-ranking method on the sentence affinity graph over three ROUGE metrics. 
The rest of this paper is organized as follows: the proposed Co-HITS-Ranking Section 3. Section 4 presents our conclusion. 2.1 Overview The proposed Co-HITS-Ranking based approach is intuitively based on the following assumptions: Assumption 1: A sentence should be significant if it is heavily linked with the given query and other significant sentences. A document should be significant if it is heav-ily linked with the given query and other significant documents. with the significant documents. A document should be significant if it has high corre-lation strength with the significant sentences. 
Based on the assumptions, we develop a two-layer graph model to fuse three kinds of relationships (i.e. the homogeneous relationships between sentences or documents, and the heterogeneous relationships between sentences and documents), where the sentences and query, but also the significances of its closely related documents. 2.2 Two-Layer Graph Model E V
DD } includes all relationships between pairs of documents with the weight w ( D i , D j ) E the correlation strength between the sentence S i and document D j . Figure 1 gives an illustration of the two layer graph G and its sub-graphs. layers demonstrate the correlation between sentences and documents. The upper layer expresses both the relationships among all the sentences and the relationships between the given query and the sentences. The relationships among all the docu-ments and the relationships between the given query and the documents in the lower layer have been further investigated in this study. The given query q is treated as a pseudo-sentence when building the affinity graph G SS of sentences, which can be processed in the same way as other sentences. Similarly, when building the affinity which can then be processed in the same way as other documents. 
In the graph model, the link weight w ( S i , S j ) can be computed by adopting the cosine similarity measure between the corresponding term vectors taining term T k . Likewise, we can compute the link weight w ( D i , D j ) by adopting the cosine similarity measure between a pair of documents X  term vectors value can be computed by the TF dk *IDF k formula, where TF dk is the frequency of term T in the corresponding document and IDF k is the inverse document frequency of term T . be computed by the cosine similarity measure between the sentence X  X  term vector and the document X  X  term vector. Where 2.3 Ranking Homogeneous Objects Manifold-ranking algorithm is a general graph-based ranking method [8], which takes advantage of local recommendations among the neighboring nodes to rank nodes. In sentences and documents) individually, which is performed as follows: repeated until a stable state is achieved. 2.4 Co-ranking Heterogeneous Objects In Section 2.3, the initial ranking scores are only determined by homogeneous objects. However, the interactions between hetero geneous objects are not considered. To lev-erage the above information, the Co-HITS-Ranking algorithm is adopted to rank sen-tences and documents jointly [9], which can be summarized as follows. 
In the study, the interaction information between sentences and documents is en-strength between sentences and documents may have significant effect on the sen-tence ranking, so the Co-HITS-Ranking algorithm is used to incorporate the bipartite the initial ranking scores of sentences and documents determined by each layer alone. 
The final ranking scores of every sentence and document can be got through the above iteratively updating process, which can take into account the mutual influences same time. Therefore, the significance of a sentence is determined ultimately by both its initial significance and the document's significance that is related with it closely. 
After the significance score of each sentence has been obtained, a variant of MMR algorithm is employed to remove redundancy and extract summary sentences. 3.1 Dataset and Evaluation Metrics progress in automatic summarization. Query-focused multi-document summarization use the DUC 2005 dataset for evaluation. Table 3 gives a brief summary of the dataset. 
We use ROUGE toolkit [10] as the evaluation utility, which has provided multiple recall-oriented metrics to evaluate the quality of a candidate summary automatically. In our experiments, documents and queries were firstly segmented into sentences. The stop words in both documents and queries were removed. And the average recall scores of the above ROUGE metrics are demonstrated in the experimental results at a confidence level of 95%. 3.2 Experimental Results The proposed Co-HITS-Ranking based approach (denoted as "CoHR") was firstly compared with the systems participati ng in DUC 2005. Table 4 lists the ROUGE scores of our summaries and those of the DUC 2005 runs. 
From Table 4, we can find that the proposed Co-HITS-Ranking based approach can achieve comparable performance to the state-of-the-art systems on the DUC 2005 compared with many different summarization approaches. In addition, to gain a better insight into the proposed approach, we compared it with two baseline methods. One is words of the most recent document for each topic in the final summary. The other is the sentence-based manifold-ranking method (denoted as "SenMR"), which makes in a manifold-ranking process to computes each sentence X  X  information richness score in the documents. Then the same MMR like algorithm with same parameter configu-sentences with highest ranking scores and minimum duplicate information to create the summary according to the length limit. 
The "SenMR" method is performed only on the sentence affinity graph G SS without and G SD . Specifically, the important information ignored in the "SenMR" includes each document X  X  significance and the sentence-to-document correlation strength. For the purpose of comparison and simplicity, we use the same value of s  X  in the "SenMR" as in the "CoHR". Table 5 shows the comparison results with the above two baseline methods on the DUC 2005 dataset. The experimental results shown in Table 5 demonstrate that the proposed Co-HITS-Ranking based approach can outperform both the lead baseline method and the sentence-based manifold-ranking method over three ROUGE metrics. The encourag-ing performance can be attributed to the following major factors. Factor 1: Evaluating the initial significance of sentences and documents via the local recommendations between homogeneous objects To collaboratively evaluate single object X  X  importance via the local recommendations within the homogeneous objects, we make use of the manifold-ranking algorithm to integrate the relationships between homogeneous objects as well as the information about the given query in a unified graph-based score propagation process, which has been proved to be effective in the previous research [7]. Factor 2: Updating the significance sco res of sentences and documents via the global mutual reinforcement between heterogeneous objects In the model, by adopting the Co-HITS-Ranking algorithm, the initial biased informa-mutual reinforcement framework to co-rank heterogeneous objects jointly and updat-bipartite-graph-based score propagation process based on the heterogeneous relation-ships between sentences and documents, which can be used to co-rank sentences and documents more effectively. 
In summary, the proposed Co-HITS-Ranking based approach can benefit from the tion between both layers into a unified two-layer graph model, which has been inves-tigated in our preliminary experiment and has shown its superiority to the method that only considers the information from one layer. In this paper, we propose a novel approach to query-focused multi-document summari-between sentences and documents in a unified two-layer graph model. The main feature of the proposed approach is its ability to evaluate sentences comprehensively by making use of local recommendations within homogeneous objects as well as global mutual reinforcement between heterogeneous objects. Preliminary experimental results on the DUC2005 dataset demonstrate the effectiveness of the proposed approach. Acknowledgments. This work was supported by the Major Research Plan of National Natural Science Foundation of China (90820005, 90920005), National Natural Sci-ence Foundation of China (60773011, 60773167) and Wuhan University 985 Project (985yk004). 
