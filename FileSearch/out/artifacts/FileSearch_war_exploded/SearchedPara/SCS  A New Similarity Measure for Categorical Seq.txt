 quences is a fundamental process in many data mining applications. A key issue is to extract and make use of significant features hidden behind the chronological and structural dependencies found in these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chrono-logical order, but such sequences often have similar structural features in ch ronologically different posi-tions. measuring the similarity between categorical se-quences, based on an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS captures significant patterns that represent the natural structure of sequences, and reduces the influence of those representing noise. It cons titutes an effective approach for measuring the similarity of data such as biological sequences, natural language texts and financial trans-actions. To show its effectiveness, we have tested SCS extensively on a range of datasets, and compared the results with those obtained by various mainstream algorithms. formed by strings of related or unrelated categories, for which both chronological order and structural features (i.e., subsequences characterizing the intrinsic sequen-tial nature of related sequences) are very important. Many types of scientific and business data are in the form of categorical sequence s: for instance, biological sequences, natural language texts, retail transactions, etc. through the detection of chronological dependencies and structural features hidden within these sequences. This measure can lead to a better understanding of the nature of these sequences, in the context of their origin. For instance: linear chain made up of 20 possible amino acids, con-taining some structural features, known as conserved domains, which precisely define its biochemical activi-ty. Despite their uniqueness, many different proteins are involved in the same biochemical activities, since they share similar structural features. work has its own unique sequence of words, it is poss-ible to expose structural features that reveal a certain literary style, making it possible to identify the author, since each author marks his written work with some structural characteristics definitive of his own style. spending behavior, from which it is possible to extract some sequential factors describing his unique profile. From these sequential factors, it is possible to extract structural features that may predict customers who have a potential risk of bankruptcy. search areas such as computational biology and text processing, we have seen an increasing need to develop methods that deal efficiently with categorical se-quences. The most important known challenges pre-sented by these data, which are partially or wholly addressed by existing methods, are the following: pendencies of structural features which may have sig-nificant meaning is difficult to extract. with significant quantities of noise. Unlike numerical sequences, for which we can filter out noise by apply-ing signal processing techniques, categorical sequences need the use of a different, specific set of approaches to handle the non-dependency between the categories making up these data. between the values of the different categories forming these data makes it difficult to measure the similarity between the categorical sequences. important problem. tant challenge that needs be dealt with, which has un-fortunately been ignored by almost all the existing approaches. It is the fact that many categorical se-quences may include similar structural features with significant meaning in chronologically different posi-tions.
 measuring the similarity between categorical se-quences. One example is the very common Levensh-tein distance [1], usually named the  X  Edit Distance  X , which is calculated by finding the minimum cost re-quired to transform one sequence into another using  X  insertion  X ,  X  deletion  X  and  X  replacement  X  operations. Another, the commonly used sequence alignment ap-proach [2], finds the best matching for a pair of cate-gorical sequences by inserting  X  gaps  X  in the appropri-ate positions, so that the positions where identical or similar categories occur in the two sequences are aligned.
 due to the fact that they are based on the matching of subsequences in chronological order. They break down when applied to sequences comprising similar structur-al features in chronologically different positions. Pro-tein sequences often have similar conserved domains in non-equivalent positions when viewed in terms of pri-mary structure, which makes them difficult to match in chronological order. However, these domains might well be in equivalent positions when viewed in terms of three-dimensional structure [3].
 measures that depend heavily on the costs assigned by the user to the  X  insertion  X ,  X  deletion  X  and  X  replace-ment  X  operations in the case of the edit distance, or the  X  opening gap  X  and  X  extension gap  X  costs in the case of sequence alignment. This creates ambiguities and complicates the similarity measurement task, especial-ly for sequences of significantly different lengths. measuring the similarity between categorical se-quences. The  X  -grams approach is popular for its speed and simplicity. The  X  -grams are the set of all possible grams (i.e., patterns) of a fixed length  X  for which, with an  X  -letter alphabet, we obtain  X   X  possi-ble patterns.
 proach, the restriction to a fixed length  X  in collecting patterns from the sequences is a major drawback [4]. The value of  X  is set independently of the intrinsic structure of the sequences, as in the example of the  X  -letter alphabet, and the length of the sequences. De-pending on the value of  X  , this results in either the collection of patterns representing noise or the exclu-sion of significant patterns. Moreover, all patterns of length  X  are collected, without distinguishing between significant and non-significant patterns, which increas-es the probability of collecting a number of motifs representing noise. report any approach that simultaneously addresses all of the challenges cited above. To rectify this shortcom-ing, in this paper we present SCS, a new and original similarity measure. SCS allows us to extract hidden relations between categorical sequences, by capturing structural relations using global information extracted from a large number of sequences rather than merely comparing pairs of sequences. SCS detects and makes use of the significant patterns underlying the chrono-logical dependencies of the structural features, filtering out noise by collecting the significant patterns that best represent the properties of categorical sequences and discarding those patterns that occur by chance and represent only noise. Moreover, SCS measures similar-ity in a way that more efficiently reflects the structural relationships between the categorical sequences, with a worst-case computational cost that is linear with re-spect to sequence length. In addition, by utilizing an efficient subsequence matching scheme, SCS simulta-neously handles the within chronological order and the between non-chronological order of the structural fea-tures. This allows it to deal with categorical sequences that include similar structural features with significant meaning in chronologically non-equivalent positions. Our experiments showed that the patterns used in SCS are more significant in terms of representing the natu-ral structural features of categorical sequences and capturing chronological an d non-chronological depen-dencies.
 the similarity of categorical sequences. To show this, we have tested it extensively on different data types and compared the results with those obtained by many existing mainstream approaches. scheme, SCS extracts from a set of categorical se-quences a set of patterns with significant meaning, and filters out noise patterns. This is done by examining each pair of sequences for common identical patterns, as well as for patterns that are slightly different, known as  X  Paronyms  X  and  X  Cognates  X . In natural language text, paronyms such as  X  affect  X  and  X  effect  X  are words that are related and derive from the same root, while cognates such as  X  shirt  X  and  X  skirt  X  are words that have a common origin. Taking identical patterns, paro-nyms and cognates into account improves the extrac-tion of significant patterns. set of all significant patterns obtained from the pair-wise sequence matching, rather than to the original input categorical sequences. Then, by spectral decom-position, the categorical sequences are mapped onto a new vector space of reduced dimension [5], in which each categorical sequence is represented by a vector. Finally, the measure of the similarity between different sequences is computed simply by applying the cosine product between the corresponding vectors. The devel-opment of this idea is shown in the next sections. methods such as Latent Semantic Analysis are used to extract hidden relations between documents, by captur-ing semantic relations using global information ex-tracted from a large number of documents rather than merely comparing pairs of documents. These methods usually make use of a word-document matrix  X   X   X  X   X   X  , in which rows correspond to words and columns correspond to documents, where  X  is the number of possible words and  X  is the number of documents. The term  X   X , X  represents the occurrence of word  X  in docu-ment  X  . tinctive patterns like words in natural language text, categorical sequence data analysis is in many respects similar to natural language text analysis. However, the challenge is to be able to identify those patterns that map to a specific meaning in terms of sequence struc-ture and to distinguish significant patterns from pat-terns resulting from random phenomena. is used in natural language text processing to extract the hidden relations between documents, we use a pat-tern-sequence matrix on the categorical sequences to extract the hidden relations between these sequences. This is done by capturing structural relations using global information extracted from a large number of sequences rather than merely comparing pairs of se-quences. Henceforth, we use  X   X   X  X  X   X  to denote the pattern-sequence matrix, in which the term  X  represents the frequency of pattern  X  in sequence  X  , while  X  is the number of possible patterns, and  X  is the number of sequences. The significant patterns used to construct  X  are detected and collected using the approach described in the next subsection. the matching of a pair of sequences. We assume that the occurrence of significant patterns in non-chronological order is more likely to arise as a local phenomenon than a global one. This hypothesis is valid for many types of real data, particularly for biological sequences [7].  X  and  X  are a pair of sequences. Let  X  and  X  be a pair of subsequences belonging respectively to  X  and  X  . Here, the symbols  X  and  X  are simply used as va-riables, representing any subsequence belonging to the sequences  X  and  X  , respectively. terns by building a matching set  X   X , X  . This is done by collecting all the possible pairs of subsequences  X  and  X  that satisfy the following conditions:  X  used as variables, in the same way as  X  and  X  . The expression  X  . X .  X  means that the element to the left of the symbol  X  is not included in the one to the right, either in terms of the composition of the patterns or in terms of their positions in their respective sequences. We use the parameter  X   X , X  to represent the minimum number of matched positions with similar categories between  X  and  X  ; at the same time,  X   X , X  is also used to represent the maximum number of matched positions with different categories allowed. A detailed discussion on the choice of  X   X , X  is provided in the next subsec-tion. Here are a few explanations about the previous formula: 1. |  X  |  X  |  X  | : means that  X  and  X  have the same length. 2. |  X  X  X  |  X  X   X , X  : means that  X  and  X  include more than  X   X , X  matched positions with similar categories. 3. |  X  X  X  |  X  X   X , X  : means that  X  and  X  include fewer than  X   X , X  matched positions with different categories. 4.  X  X  X  X  X , X  X   X , X   X   X   X  X  X  X   X   X   X   X  X  X  X   X  : means that, for any pair of matched subsequences  X  X  and  X  X  belong-ing to  X   X , X  , at least one of  X  and  X  is not included in  X  X  or  X  X  , respectively, either in terms of their compositions or in terms of their respective positions in their corres-ponding sequences, according to the partial order in-duced by set inclusion. of the matching set  X   X , X  is to capture information shared between  X  and  X  , related to their structural features that manifest certain chronological dependen-cies. At the same time, by taking into account multiple occurrences of patterns in non-equivalent positions, the matching set  X   X , X  seeks to capture the structural fea-tures in non-chronological order. In fact, with this for-mula,  X   X , X  captures pairs of patterns  X  and  X  that show a within chronological similarity, even if they are in non-chronological order according to their respective positions within the sequences  X  and  X  . patterns which best represent the natural structure of categorical sequences, and to minimize the influence of those patterns that occur by chance and represent only noise. This motivates one of the major statistical fea-tures of our similarity measure, the inclusion of all long similar patterns (i.e., multiple occurrences) in the matching, since it is well known that the longer the patterns, the smaller the chance of their being identical by chance, and vice versa. For each pair of compared sequences  X  and  X  , we use the theory developed by Karlin et al. [8] to calculate the minimum length of matched significant patterns, which is the value to be assigned to  X   X , X  .  X   X , X  of the longest common pattern present by chance at least 2 times out of 2  X  -category sequences  X  and  X  is calculated as follows:  X  quency of the observed sequences  X  and  X  respective-ly, while  X   X , X  is the asymptotic standard deviation of  X  Karlin et al. [8], which states that, for a pair of se-quences  X  and  X  , a pattern observed 2 times is desig-nated statistically significant if it has a length that ex-ceeds  X   X , X  by at least two standard deviations. Thus, in building the matching set  X   X , X  , we extract all the com-mon patterns that satisfy this criterion. This means that, for the pair of sequences  X  and  X  , we calculate a spe-cific and appropriate value of  X   X , X   X  X   X , X   X  X 2  X , X  This criterion guarantees that a matched pattern desig-nated as statistically significant (i.e., a pattern that maps to a specific meaning in terms of sequence struc-ture) has less than a 1/100 probability of occurring by chance. which  X  and  X  are two different sequences, for which  X   X , X  is the minimum length of the significant patterns, and  X   X , X  is the set of collected pairs of significant pat-terns. Let  X  be the set of all possible matching sets, such that and collect all the  X   X  X  X  X  -grams from each significant pattern included in  X  . Thus, for a set of sequences made up of possible  X   X  X  X  X  -grams. involving the categorical sequence  X  , such that representing the intersection of row  X   X  X  X  with the col-umn corresponding to sequence  X  , is simply aug-mented by the occurrence of the  X   X  X  X  collected  X  grams belonging to the subset  X   X  . responding to  X   X  X  X  X  -grams that exist at most in only one sequence. In our experiments, we found that the number of remaining rows  X  is much smaller than tant for the next section. ticated approach is that each sequence in the set of sequences contributes to the capture of the structural features and chronological dependencies of all other sequences in the set. And the more frequently a pattern occurs in the sequences, the more heavily it is represented in the pattern-sequence matrix  X  . Moreo-ver, the matrix  X  is filled by using only the  X   X  X  X  X  -grams corresponding to the significant patterns collected. expressed as a column vector and each pattern as a row vector. This representation is known as a vector space model. Represented in this way, the sequences are seen as points in the multidimensional space spanned by patterns. However, this representation does not recog-nize related patterns or sequences, and the dimensions are too large [5]. To take advantage of this representa-tion, we perform a singular value decomposition (SVD) on the pattern-sequence matrix  X  . Let  X  X  |  X  | and  X  be the total ranks of  X  . Thus the matrix  X  can be decomposed into the product of three matrices, as fol-lows: where  X  is a  X  X  X  left singular matrix,  X  is a  X  X  X  diagonal matrix of positive singular values, and  X  is a  X  X  X  right singular matrix. By taking into account only the  X  X  (where  X  X  X  ) largest singular values from the matrix  X  , and their corresponding singular vectors from the matrices  X  and  X  , we get the rank  X  X  approx-imation of  X  with the smallest error according to the Frobenius norm [9]. Thus, the matrices  X  ,  X  and  X  are reduced to  X  X  X  X  matrix  X   X  ,  X  X  X  X  X  matrix  X   X   X  X  X  X  matrix  X   X  , respectively, such that theory [6], the sequences expressed as column vectors in the matrix  X  are projected via the spectral decompo-sition onto a new multidimensional space of reduced dimension  X   X   X  X  spanned by the column vectors of the matrix  X   X   X  . The representation of the sequences in the new  X  X  -dimension space corresponds to the column vectors of the  X  X  X  X  matrix  X  X  X   X  . of sequences  X  and  X  is simply computed by using the cosine product of their corresponding column vectors on the matrix  X  X  X   X  . we made use of the fast string matching approach de-veloped by Amir et al. [10], which allows us to find all the locations of any pattern from a sequence  X  in a sequence  X  in time  X  X  |  X  |  X   X   X , X   X log  X , X   X  . of the fast, incremental, low-memory and large-matrix SVD algorithm recently developed by Brand [11], which makes it possible to perform the SVD for a  X  rank matrix  X  X  X  in  X   X   X  X  X   X  time with  X  X   X  X  X  X   X   X , X   X  . measure approach, we tested SCS extensively on a variety of datasets from different fields and compared the results with those obtained by several mainstream algorithms. In all our experiments, we used these algo-rithms with their default input parameters. from speech data to assess its ability to recognize spo-ken words and speakers, and we compared the results with those of several mainstream algorithms designed to deal with categorical sequences. protein sequences according to their functional annota-tions and biological classifications, we tested SCS extensively on different protein databases, and com-pared the results with those obtained by algorithms designed especially to deal with such data. ural language texts, we tested SCS on the entire Reu-ters-21578 text categorization test collection, and com-pared the results with those obtained by algorithms especially designed to deal with texts and documents. well-known Receiver Operating Characteristic method, known also as the ROC Curve, was used. This method allows us to evaluate the discriminative power of each of the similarity measure approaches studied in our experiments. The ROC Curve makes it possible to quantify the Quality Index of the similarity measures obtained for a sequence  X  and all the sequences in a set  X  , by making use of the known classification of  X  in  X  . A brief description of how the Quality Index is com-puted is given below. creasing order of similarity with the sequence  X  , and considering the subset of sequences belonging to  X  that are correctly classified in  X  as  X  true positives  X , and the remaining sequences as  X  false positives  X , the ROC Curve can be generated by plotting the fraction of true positives vs. the fraction of false positives. A plotted point on this curve with the coordinates (  X  ,  X  ) (i.e., see Figure 1) means that the subset of sequences from the sorted set  X  that includes the first  X  percent of true positives also includes  X  percent of false positives. The best possible similarity measures of  X  with the se-quences in  X  would yield a point near the upper left corner of the ROC space, representing 100% sensitivi-ty, corresponding to  X  =1.0 (i.e., all sequences from the same class of  X  have the highest similarity measures) and 100% specificity, corresponding to  X  =0.0 (i.e., all sequences from different classes of  X  have the lowest similarity measures). In our experiments the value of the area under the ROC curve is defined as the Quality Index of the similarity measures obtained with a given protein sequence  X  , since the larger this area is, the greater the discriminative power of the similarity measure. computers to automatically identify who says what, by converting the human voice to something else much easier to comprehend and analyze using computers. The speech data used in this section come from the in-house speech database used in [12], made up of iso- X  u  X ) and numbers (i.e.,  X  1  X , ...,  X  9  X ) spoken by 5 men and 5 women, with each symbol pronounced 10 times by each speaker. Each recorded speech was used to produce a sequence made up of 20 different events, based on a bio-inspired treatment [12]. Each pronun-ciation is thus represented by a categorical sequence made up of 20 different categories. The details about the average lengths of the sequences produced for each letter and number by each speaker are shown in Table 1. The first row contains the list of the different speak-ers; the symbol  X  M  X  designates  X  male  X  and  X  F  X  desig-nates  X  female  X . The first column contains the pro-nounced letters and numbers. The rest of the table con-tains the average lengths of the sequences produced for each letter and number by each speaker. effectively, we used SCS to compute all-against-all similarity measures of the produced sequences (10 speakers x 14 words x 10 times = 1400 sequences). Then, we compared the results with those obtained by several mainstream approaches: for instance, the one proposed by Kohonen et al. [13] based on the set me-dian that has the smallest sum of distances from the other elements, the one proposed by Kondrak et al. [14] based on the  X  -grams approach with a predefined value of  X  , the one proposed by Oh et al. [15] based on a new matching scheme that takes into account the non-chronological order of matched subsequences, the one proposed by Cai et al. [16] based on the longest common subsequences similarity model, and the one proposed by Li et al. [17] based on sequence align-ment. speakers (10 speakers) or by words (5 letters and 9 numbers). In the first case, the sequences are classified into 10 classes; in the second, into 14 classes. In this experiment we used the ROC Curve to evaluate the results for both types of classification. Table 1. Average lengths of produced sequences by each algorithm. Each table shows the average Quality Index of the similarity measures obtained by each approach (i.e., column) for each subset of se-quences belonging to the same class (i.e., row). The last row in each table contains the global average Qual-ity Index obtained by each approach. In Table 2, words are used as known classification, while in Table 3 speakers are used as known classification. obtained the best similarity measures for both classifi-cations, by words and by speakers, according to the ROC Curve Quality Index. nize the produced categorical sequences, whether cate-gorized by pronounced words or speakers, and does so better than the other approaches. It more efficiently highlights the significant unseen information behind the chronological dependencies and structural features within these sequences. This is made possible by the detection and use of the significant patterns that best represent the natural structure of these sequences, and minimization of the influence of those patterns that occur by chance and represent only noise. In addition, the matching technique that allowed us to simulta-neously handle the within chronological order and the between non-chronological order of the structural fea-tures played an important role in reaching these con-clusive results. measure approach, we chose to apply SCS to predict the biochemical activities of protein sequences, since this process remains a difficult open problem in the field of computational biology. made up of 20 possible amino acids. Thus, a protein is a categorical sequence made up of 20 possible catego-ries. An important open problem in computational biology is to automatically predict the biochemical activity of a newly sequenced or not yet characterized protein sequence. To achieve this, biologists often compare the non-characterized protein sequence to those that are biochemically well-characterized, and assign to this protein the biochemical activity of the most similar proteins. we tested SCS on a variety of protein datasets and compared the results with those obtained by different mainstream algorithms designed specifically to deal with protein sequences. For instance, we considered SMS, introduced by Kelil et al. [18] based on a strict matching scheme that captures the most significant patterns in chronological and non-chronological order; tSMS, introduced by Kelil et al. [19], which is an im-proved version of SMS that allows mismatches; one of the most commonly used bioinformatics programs, Blast, introduced by Altschul et al. [20] based on the local sequence alignment; the one introduced by Wu et al. [21] based on short patterns used analogously to the index terms in information retrieval; and the one intro-duced by Bogan-Marta et al. [22] based on the cross-entropy measure applied over the collected  X  -gram patterns with a fixed value of  X  . Below, we report the results obtained for the different datasets, with support from the literature and functional annotations. in measuring the similarity between protein sequences according to their functional annotations and biological classifications, we performed extensive tests on the widely known databases COG [23], KOG [23] and PC [24]. We used three randomly generated sets of six subsets each [19]: C 1 to C 6 from the COG database, containing respectively 509, 448, 546, 355, 508, 509 protein sequences; K 1 to K 6 from the KOG database, containing respectively 317, 419, 383, 458, 480, 388 protein sequences; and finally P 1 to P 6 from the PC database, containing respectively 538, 392, 442, 595, 561, 427 protein sequences. Each generated subset includes protein sequences with at least 20 biochemical activities. effectively, we computed all-against-all similarity measures of the protein sequences within each generat-ed subset and evaluated the results using the ROC Curve Quality Index. obtained by each algorithm on each generated protein subset. Each table shows the Quality Index of the simi-larity measures obtained by each approach (i.e., col-umn) for each subset of protein sequences (i.e., row). The last row in each table contains the global average Quality Index obtained by each approach. show that tSMS obtained the best similarity measures over all generated subsets. The results obtained with tSMS are closely followed by those obtained by SCS and SMS, while Wu and Bogan obtained less good results. And a bit farther behind we find Blast, which obtained the poorest results. SMS Blast, Wu, and Bogan algorithms are all designed especially to handle protein sequences by taking into account the substitution relations between different amino acids, the results yielded by our new approach SCS are very close in quality to the best results ob-tained by tSMS. Furthermore, the results obtained by SCS are comparable to those of SMS, and much better than those obtained by Blast, Wu, and Bogan. better results over all generated subsets, with a relative advantage for tSMS. We believe strongly this is due to the fact that, apart from the approach proposed in this paper, tSMS and SMS are the only algorithms among those used here that significantly address the non-chronological order of struct ural features of protein sequences. However, tSMS and SMS need a substitu-tion matrix as input parameter, to decide which amino acids should be matched and compute the weights of the significant patterns. In our experiments, the results obtained by tSMS and SMS were made possible by the use of the substitution matrix that maximizes the quali-ty measure for each test. This means that one needs prior knowledge about the classes of the protein se-quences in order to choose the appropriate matrix for tSMS and SMS. This is the very reason why SCS is proposed in this paper: SCS does not depend on the use of a substitution matrix or any other input parameter. sequences is a fundamental process in many areas in natural language processing, such as text classification and information retrieval. The key issue is to measure this similarity without explicit knowledge of the statis-tical nature of these texts. The literature reports a num-ber of approaches developed to measure the similarity between texts and documents. Some of the most recent examples are the one introduced by Chim et al. [25] based on a suffix tree document model, the one intro-duced by Wan [26] based on the earth mover's distance, and the one introduced by Aslam et al. [27] based on an information-theoretic approach. These different approaches have demonstrated their ability to measure the similarity between natural language texts effective-ly. For this reason, and in the aim of evaluating the performance of our new similarity measure, we de-cided to perform extensive tests to compare the results obtained by SCS to those obtained by the approaches cited above. approach, we tested SCS on the entire Reuters-21578 text categorization test collection, the most widely used test collection for text categorization research. It com-prises 21,578 articles which appeared on the Reuters newswire in 1987. Each article was manually indexed according to which categories, from which sets, it be-longed to. The category sets are as follows: Exchanges (39), Orgs (56), People (267), Places (175) and Topics (135). To make these articles accessible to SCS, they were transformed into categorical sequences by with-drawing spaces and newline marks. T h concerns only SCS, since the other t e are designed to handle texts, phrases a n are. We computed all-against-all simil a the 21,578 articles, and evaluated the r ROC Curve Quality Index for each sets. each algorithm on each of the categor y shows the average Quality Index f o measures obtained by each approach ( i each subset of articles belonging to th e (i.e., row). The last row contains th e Index average obtained by each approa c approach introduced by Chim et al. [ 2 b est Quality Index over all categor y relatively closely by SCS, while the a p oped by Wan [26] and Aslam et al. [ 2 good results. for the category sets Exchange and O r In each figure, we show the averag e obtained by each algorithm for each s e longing to the same category set. that the approach introduced by Chi m tained the best Quality Index over a l Orgs categories. In Figure 2, we can s tained Quality Index results very clos e sults, and better than the results obtai n approaches. In Figure 3, our approa c introduced by Aslam et al. [27] obta i results relatively close to the best fi g approach introduced by Wan [26] ob t results over all categories. proaches of Chim et al. [25], Wan [2 6 al. [27] that we tested were all desig n handle natural language texts by taki the semantic concepts underlying wo r and despite the fact that the data use d ment were transformed by withdra w newline marks to make them accessi b results yielded by our new approach a quality to the best obtained results, in c the results obtained by the other appro a ferent types of categorical sequen c effectiveness of our new metho d over existing mainstream method similarity between categorical s e results obtained with speech dat a measures the similarity between c a more effectively and better than o signed to perform the same task. obtained with the protein sequenc is able to extract the significant underlying the biochemical acti v quences, without using the kno w tions between amino acids. Third, with texts and documents showed t data used in this experiment were gorical sequences by withdrawing marks, SCS was able to highligh t seen information behind the relat e and documents. significant unseen information be h cal dependencies and structural fe sequences. This is possible becaus Figure 2. Average Quality Ind e the significant patterns that best represent the natural structure of these sequences, and minimizes the influ-ence of those patterns that occur by chance and represent only noise. In addition, the matching tech-nique that allowed us to simultaneously handle the within chronological order and the between non-chronological order of the structural features, played an important role in reaching these conclusive results. 
