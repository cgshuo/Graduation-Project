 In this paper, we present a novel solution to multi-view cluster-ing through a structured low-rank representation. When assuming similar samples can be linearly reconstructed by each other, the resulting representational matrix reflects the cluster structure and should ideally be block diagonal. We first impose low-rank con-straint on the representational matrix to encourage better grouping effect. Then representational matrices under different views are allowed to communicate with each other and share their mutual cluster structure information. We develop an effective algorithm inspired by iterative re-weighted least squares for solving our for-mulation. During the optimization process, the intermediate repre-sentational matrix from one view serves as a cluster structure con-straint for that from another view. Such mutual structural constraint fine-tunes the cluster structures from both views and makes them more and more agreeable. Extensive empirical study manifests the superiority and efficacy of the proposed method.
 H.3 [ Information search and retrieval ]: Clustering Algorithms; Design; Experimentation Multi-view clustering; multi-modal learning; structure regularizer
Multi-view clustering concerns the problem of partitioning data points into a series of subsets in an unsupervised way given their feature representations under different views. Here in the context of this paper, a view simply refers to one feature modality of the data rather than the physical view angle such as front-view versus side-view face images [23]. In many applications, the data points being processed are collected from multiple sources and thus have different view-specific attributes. For instance, in image classifica-tion an image can be either represented using the traditional hand-crafted feature like SIFT or automatically learned feature obtained from deep learning techniques [7]. Even though the information from any view is somehow sufficient for the clustering task, taking advantage of the complementary information across views is bene-ficial and can better facilitate the clustering process in most cases.
In the literature, a spectrum of methods are proposed to seek better clustering results by capturing the view complementarity. Among them, one line of research is to directly unify the multi-view information in the clustering process. For instance, in [15] a co-training flavored spectral clustering algorithm was proposed to encourage the clustering agreement between views. Another one is [16] which attempted to regularize on the eigenvectors of view-specific graph laplacians and achieve consistent clusters across views. Another line of research is to first learn a latent representation for multi-view data and then perform clustering on such representation to learn the partition. A notable one is [19] which employed matrix factorization to discover a common latent structure shared by all views and give rise to compatible clustering results. Besides, CCA based multi-view clustering methods also fall into this category [9] [5]. Yet another line of research is by fusing the clustering results obtained from individual views toward a consensus [8] [13].
Different from the aforementioned methods, we tackle the multi-view clustering problem from the perspective of structured low-rank representation. Similar to [24] [21], for each view we also as-sume similar samples can be used to linearly reconstruct each oth-er. The resulting representational matrix reflects the cluster struc-ture which should be ideally block diagonal. As in [18] [17], low-rankness is a nice property favored by many subspace clustering algorithms due to its better grouping effect. So the key idea of our method is on one hand to impose a low-rank constraint on such representational matrix to let similar samples stay together. On the other hand, since the ideal clustering result should be unanimous ir-respective of views, the representational matrix derived from each view is further asked to conform with one another as much as pos-sible through a mutual structure constraint.

Concretely, when alternatively optimizing the objective with re-spect to one of the representational matrices once at a time while keeping the others fixed, our method actually solves a low-rank lin-ear regression problem. And it happens to enforce that the to-be-determined representational matrix for another view should refer to that fixed intermediate grouping structure. Over time the comple-mentary information among different views is communicated and shared. The grouping structure for one view derived from the pre-vious step helps rectify and fine-tune the to-be-decided grouping structure for a different view in the current step.

In summary, the contributions of this paper are the following:  X  We propose a novel multi-view clustering method based on structured low-rank representation. Such a joint regularization frame-work explicitly minimizes the grouping differences across views and gives rise to better clustering performance.  X  We develop an effective algorithm inspired by iterative re-weighted least squares for solving our formulation. And extensive experimental results on benchmark datasets validate the usefulness of our method.
In our model, under each view similar data points are used to lin-early reconstruct each other. Given the data matrix X v  X  and reconstruction coefficient matrix Z v  X  R n  X  n where v denotes one of the two views A and B , d v is the feature dimension of view v and n is the number of data points, our proposed method is for-mulated as follows. min
As in [24] [21], (1) is called a structured low-rank representation since when fixing Z B , the cluster structure from view A is con-trained to agree with a latent cluster structure from view B or vice versa. The first term in this objective is the linear reconstruction term and in particular we seek a lowest rank representation by im-posing a trace norm regularizer on Z v . By virtue of the grouping effect of such a low-rank constraint, the underlying intrinsic cluster structure can be unveiled.

Apart from pursuing better clustering structure for any single view, our model also favors a consistent cluster membership across views. In multi-view clustering, it is usually anticipated that a data point should be assigned to the same cluster irrespective of views. To this end, the last term in our objective is designed to minimize the difference of cluster structures from different views. When treating the candidate samples used to reconstruct the target sample as a dictionary, Z A and Z B are the view-specific representational responses on the dictionary, which indicates the sample affinity and cluster structure.  X  and  X  are the hyper-parameters that control the tradeoff be-tween corresponding terms.

Note that our model can be easily generalized to more than three views by summing the reconstruction and cluster difference terms over all views. Given the space limit and for simplicity, in this section we base our introduction on a two-view scenario.
When it comes to solving the proposed objective, it is not easy to optimize (1) directly given the existence of a trace norm regulariz-er. So we reformulate our objective by following a well established variational formulation for trace norm [6] [14], in which the state-ment below holds true for the representational matrix Z A for Z B ) where the infimum is obtained for S A = ( Z A Z T A ) 1 / 2 i -th column of matrix Z A . Here S A can be seen as an intermediate variable during the optimization procedure.

In the outer loop of our algorithm, we alternatively solve for one of the representational matrix Z A or Z B while keeping the other one fixed. In light of the results from (2), when we optimize the objective with respect to Z A in a column-wise fashion, (1) can be simplified into the following: The above objective is jointly convex in ( Z Ai , S solve it in the inner loop of our algorithm. In order to optimize this objective function by alternating the minimization over ( Z we need to add a term  X  X  A tr ( S  X  1 A ) which ensures S and thus the infimum can be attained [6] [14]. Here  X  A is a small scaler. And S A is then given by:
When S A is given, (3) becomes an iterative re-weighted least squares problem whose solution is the following:
Similarly, when we solve for Z B while fixing Z A , based on the same optimization strategy S B is given by and then we obtain the solution for each column of the representa-tional matrix Z B as follows:
When optimizing our method following the procedure as shown in Algorithm 1, empirically it can quickly converge after five to ten iterations. And one of the advantages for solving the objective in a column-wise fashion is that we can select a few nearest neighbors to approximately reconstruct a target sample. This makes sense be-cause Z should ideally be block diagonal which implies that candi-date samples less similar to the target play insignificant roles in the reconstruction. In such case we update Eqn.(4)-(7) only using s-maller data matrices or representational matrices whose entries are extracted from the nearest neighbor positions in the original large matrices. This strategy alleviates the burden of high computation-al cost due to the large number of samples in the databases. The most time-consuming part is computing (4) and (6) which involves Singular Value Decomposition (SVD) [12]. If we use k nearest neighbors ( k n ) for the linear reconstruction, we only need to decomposite multiple much smaller k  X  k matrices rather than the original large n  X  n matrix, which reduces the complexity from O ( n 3 ) to O ( nk 2 ) . Another advantage is that it is convenient to develop a paralleled solution which may further speed up the algo-rithm. Once Z A and Z B are obtained, we average them by letting Z = ( | Z A | + | Z B | ) / 2 . Then a spectral clustering algorithm like [20] is applied on Z to achieve the final clustering results.
In this section, we test our method on widely used benchmark databases and compare with a series of baselines in order to validate the usefulness of the proposed model.
UCI Handwritten Digit dataset [1] consists of features of hand-written digits (0 X 9). The dataset is represented in terms of six fea-tures and contains 2000 samples with 200 in each category. Similar Algorithm 1 Multi-view clustering via structured low-rank repre-sentation (MVCSL) Input: 1: while not converged do 2: // Solve Z A with Z B fixed 3: for i = 1 : n do 4: Update S A using Equation (4); 5: Update Z Ai using Equation (5) ; 6: end for 7: // Solve Z B with Z A fixed 8: for i = 1 : n do 9: Update S B using Equation (6); 10: Update Z Bi using Equation (7); 11: end for 12: end while Output: Z A , Z B and final clustering results to [16], we select the 76 Fourier coefficients of the character shapes and the 216 profile correlations as two views of the original dataset.
Movies617 dataset [3] consists of 617 movies with 17 labels extracted from IMDb. The two views are the 1878 keywords and the 1398 actors with a keyword used for at least 2 movies and an actor appeared in at least 3 movies.

Animal dataset [2] consists of 30475 images of 50 animals with six pre-extracted features for each image. Three kinds of features, namely PyramidHOG (PHOG), colorSIFT and SURF, are chosen as three views. We select the first ten categories with each including randomly chosen 50 samples as a subset for evaluation.

Pascal VOC 2007 dataset [4] consists of 20 categories with a total of 9,963 images. We use the Color feature and Bow feature as two-view visual representation. Furthermore, those images with multiple categories are removed, thus leaving 5,649 images for e-valuation.

NUS WIDE dataset [11] consists of 269,648 images of 81 cat-egories collected from Flickr. In our experiments, We select 500 images from each of the five classes with the most number of im-ages for evaluation. Six types of low level features are given and we use color correlogram and wavelet texture as two-view repre-sentations for multi-view clustering.
We extensively compare our method with many representative baselines including 1) S_Spectral : Use spectral clustering in [20] to cluster each view X  X  data and select the best clustering result. 2) S_LowRank : Use only single-view low-rank representation to con-struct the affinity matrix and then apply spectral clustering in [20] to cluster the dataset. We also report the best clustering results. 3) Combined : Concatenate features from two views and apply low-rank representation without the mutual structural constraint on the combined feature to perform clustering. 4) PairwiseSC, Centroid-SC : [16] Two objectives for co-regularizing the eigenvectors of all views X  Laplacian matrices. 5) Co_Training : [15] Alternately mod-ify one view X  X  graph structure using the other view X  X  information. 6) Multi_NMF : [19] A multi-view non-negative matrix factoriza-tion method to group the multi-view data. Note that this method is not applicable on NUS dataset since it requires all non-negative input features. 7) Multi_SS : [22] A structure sparsity based multi-view clustering and feature learning framework. The parameters in these methods are carefully selected to achieve their best results.
Whenever K-means is involved, it is run 20 times with random initialization. To speedup the optimization process, during the lin-ear reconstruction we select 100 nearest neighbors of a sample point for its reconstruction. To measure the clustering results, we use accuracy (ACC) and normalized mutual information (NMI). Readers can refer to [10] for more details about such measures. Both mean and standard deviation are reported.
It can be seen from Table 1 and 2 that our proposed method (MVCSL) consistently outperforms other baselines using both mea-sures. First of all, comparing with single-view methods like ei-ther S _ Spectral or S _ LowRank , our method always has an up-per hand, which evidences the necessity of utilizing the comple-mentary information among different views and exploring intrinsic group structure. Second, a naive concatenation of features from multiple views as the baseline Combined does is somehow inef-fective. However our method explicitly asks the view-specific clus-ter structure, which is manifested in the representational matrices arising from the data reconstruction, to agree with each other as much as possible. Therefore the additional complementary infor-mation across views is shared and thus more accurate clustering results can be obtained. Besides, our method also beats other base-lines by a considerable margin. The baseline Multi _ SS puts the data from all views together and explores its global structure. It enforces the sparsity between views while somehow neglecting the intrinsic structure for any individual view. But this is where our low-rank constraint imposed on each view stands out. Our method takes into account both the intra-view partition and inter-view as-sociation, which proves that such structured low-rank framework is quite helpful in the multi-view clustering problem.

As mentioned previously, our method can be extended to sce-narios involving three or more views, The superior results of our method on the three-view Animal dataset proves full well that the proposed method is also workable beyond two views.

When selecting the parameters  X  and  X  , we empirically grid-search in the interval [0.001, 10]. And their influences on the clus-tering performance are shown in Figure 1 and 2. By pairing proper  X  and  X  , it is not difficult to get satisfactory results. Given the s-pace limit, only results on the Movies617 dataset are reported and similar trends can be observed on the other datasets as well.
We have proposed a novel multi-view clustering method through a structured low-rank representation. On one hand, with the help of better grouping effect of a low-rank regularizer, similar data points are assigned together with higher accuracy. On the other hand, a mutual structure constraint is imposed to achieve consistent cluster memberships across views. The view-specific representational ma-trices resulting from the data reconstruction alternatively serve as the structural reference for one another. The experimental results demonstrate the effectiveness of our proposed method.
This work is jointly supported by National Basic Research Pro-gram of China (No.2012CB316300) and National Science Founda-tion of China (No.61175003, No.61135002 and No.61403390).
