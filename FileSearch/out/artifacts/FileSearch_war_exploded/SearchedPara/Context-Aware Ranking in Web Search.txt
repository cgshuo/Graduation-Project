 The context of a search query often provides a search en-gine meaningful hints for answering the current query bet-ter. Previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. Particularl y, about context-aware rank-ing for Web search, the followi ng two critical problems are largely remained unsolved. First, how can we take advan-tage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically. We develop different ranking principles for different types of contexts. Moreover, we adopt a learning-to-rank approach and integrate the ranking prin-ciples into a state-of-the-art ranking model by encoding the context information as features of the model. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The ex-perimental results clearly show that our context-aware rank-ing approach improves the rank ing of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers con-text information in ranking.
 H.3.m [ Information Storage and Retrieval ]: Search pro-cess Algorithms, Experimentation Context-aware ranking, learning-to-rank application  X 
The work was done when Biao Xiang was an intern at Mi-crosoft Research Asia.

In Web search, given a query, a search engine returns the matched documents in a ranked list to meet the user X  X  infor-mation need. Ranking models play a central role in search engines. Currently, almost all the existing ranking models consider only the current query and the documents, but do not take into account any context information such as the previous queries in the same session and the answers clicked on or skipped by the user to the previous queries. In other words, almost all the current ra nking models are insensitive to context.

Information Retrieval research has well recognized that context information is very helpful in achieving good search results. Context information may provide hints about users X  search intent and help to make better matching with docu-ments. For example, if a user raises a query  X  jaguar  X  X fter she searches  X  BMW  X , it is very likely that the user is seeking for information about a Jaguar car rather than a jaguar as an animal. The absence of context information in document ranking models is probably partially due to the difficulty of obtaining context information. Only recently have large amounts of search session data become available, which en-able large scale empirical studies on context-aware methods for Web search.

Several recent studies explore context-aware search meth-ods from different angles. Shen et al. [15] presented a context-aware ranking method by assuming that context information can better represent search intent. They incorporated the context information to build context-aware language mod-els, which were assumed to give rise to documents not only similar to the current query but also similar to the previous queries and the summaries of the documents clicked on. The study confirmed the effectiveness of the ranking model on TREC data ( http://trec.nist.gov ). However, the evalu-ation was based on a small data set consisting of only thirty sessions from three subjects under a controlled laboratory setting. It is unclear whether the assumption in the study holds for Web search engines in the real world.

More recently, Cao et al. [2,3,4]extractedcontextinfor-mation in Web search sessions by modeling search sessions as sequences of user queries and clicks. They learned se-quential prediction models such Hidden Markov Model and Conditional Random Fields from search log data. Different from our study here, their models were designed for predict-ing search intents based on context information, but not for ranking. Therefore, the models are more suitable for query suggestion, query categorization, and URL recommendation than search results ranking, as will be further analyzed in Section 2.

In spite of the several existing studies on context-aware search methods, the following two critical problems about context-aware ranking for Web search are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically and make the following contributions.

We develop four different ranking principles for different types of contexts. Those principles promote or demote doc-uments according to the context of the current query. We evaluate the four principles using real Web search sessions, and confirm the effectiveness of three principles through the significance test on the data. Interestingly, only one of the three effective principles is consistent with the findings ob-tained by Shen et al. [15] on TREC data, indicating that Web search is quite different from search on TREC data.
Moreover, we adopt a learning-to-rank approach and in-tegrate the ranking principles into a state-of-the-art ranking model, RankSVM [7], by encoding the context information as features. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and im-plicit user click data. The experimental results clearly show that our context-aware ranking approach improves the rank-ing of a commercial search engine which ignores context in-formation. Furthermore, our method outperforms a baseline method which considers context information in ranking.
The rest of the paper is organized as follows. We review related work in Section 2. We discuss the types of contexts, propose ranking principles, and evaluate the effectiveness of the principles in Section 3. We incorporate context in-formation into a learning-to-rank model in Section 4. The experimental results are reported in Section 5. The paper is concluded in Section 6.
Different users may prefer different results for the same query. Personalized search (e.g., [5, 12, 16, 17, 18]) aims to provide the most relevant search results to individual users based on their interests. Traditional personalization approaches usually build a profile of interests for each user from her/his search or browsing history.

Context information is useful in identifying users X  search needs. Context-aware search adapts search results to in-dividual search needs using contexts. While personalized search considers individual users X  long and/or short histo-ries, context-aware search focuses on short histories of all users. Research on context-aware search has been concen-trated on modeling contexts. For example, Cao et al. [2] mined frequent sequential pa tterns from search sessions for context-aware query suggestion. Cao et al. [4] modeled con-texts containing both queries and clicks within sessions by learning a variable length Hidden Markov Model for query suggestion, URL recommendation, and document re-ranking. Cao et al. [3] incorporated context information into a Con-ditional Random Field (CRF) model for better query clas-sification. Those models were mainly designed for inferring and predicting user search intents using context informa-tion. Therefore, they are mo re suitable for tasks such as query suggestions and query categorization than ranking. Although the generative HMM model in [4] can be applied to search results ranking, it learns parameters for individ-ual queries and contexts. Consequently, the learned HMM model in [4] can hardly be generalized to handle new queries and contexts not occurring in the training data. In this pa-per, we mainly focus on ranking principles and models which can be generalized to handle new queries and contexts.
Shen et al. [15] proposed a method for context-aware rank-ing, which is probably the work most related to this study. They enriched the current query by using context informa-tion, and then fitted the enriched query into language models for retrieval. The basic idea is to promote the documents that are more similar to the previous queries and clicked documents within the same session. The authors verified the effectiveness of the method using a small amount of ses-sion data created upon TREC data. In reality, user sessions for Web search are more complex. As indicated in previous studies (e.g, [9, 10, 13]), there are multiple possible relations between the current query and the previous queries, such as reformulation, specialization, generalization, and parallel movement. Considering only one situation as in [15] may not be sufficient in complicated cases.

Our work is fundamentally different from the previous studies in the following two aspects. First, different from the previous work on building context models for user in-tent understanding [2, 3, 4], our study targets at the prob-lem of context-aware ranking in Web search. To the best of our knowledge, we are the first to systematically explore context-aware ranking in real Web search scenarios. Sec-ond, compared to the previous work which applies a single ranking strategy to all kinds of contexts [15], our work recog-nizes different types of contexts, and proposes corresponding principles.
In this section, we propose context-aware ranking princi-ples according to the relations between the current query and the contexts, and evaluate the effectiveness of the prin-ciples using real log data extracted from a major commercial search engine.
In general, the context of a query q being asked contains any information that is related to q and available when q is asked. Specifically, in a Web search engine, the context often contains the queries asked before q in the same session as well as the answers (URLs) to those queries that are clicked on or skipped by the user. In the rest of this section, for a query q t in a session, we constrain the context of q t to only the query q t  X  1 asked right before q t in the session and the answers to q t  X  1 clicked on or skipped by the user. We will extend our consideration of context to all the queries and answers preceding q t in the session in Section 4.
The relations between queries in sessions have been stud-ied in several previous works [9, 10, 13], which agree on five general types. That is, the current query q t can be unrelated to, reformulating, specializing, generalizing, or generally as-sociated with the preceding query q t  X  1 inthesamesession. Obviously, for a query unrelated to its context, the context information cannot help. We discuss the other kinds of re-lations in this subsection.
A user may reformulate her previous query into a new one because the search results for the previous one do not or only partially fulfill her information need. The user X  X  information need does not change in the case of reformulation.
Example 1 (Reformulation). Table 1 shows two con-secutive queries in a session in a real log data set. The user first raised query  X  homes for rent in Atlanta  X  and clicked on the 1st and 4th search results. The user then issued the second query  X  houses for rent in Atlanta  X  X ndclickedonthe 5thsearchresult.

The two queries bear similar meaning. Unsurprisingly, 4 out of the top-5 results returned by the search engine for the second query were also among the top-5 results for the first query. Why did the user skip the top-4 search results for the second query but click on the 5th one?
The 1st and 3rd results for th e second query were clicked on by the user for the first query. Obviously, a user was unlikely to click again on pages she just browsed.
Moreover, according to some p revious user studies [6, 7, 8], a search result is likely to be viewed by a user if it is 1) among the top two search results; 2) ranked above the lowest clicked result; or 3) ranked one position below the lowest clicked result. If a search result is skipped (i.e., viewed but not clicked on) by a user, it suggests the result may not be interesting to the user. In Example 1, the 2nd and 4th results for the second query we re ranked either above or one position below the lowest clicked result for the first query. They were skipped by the user for the first query, and thus can be regarded uninteresting to the user. Therefore, they were unlikely to be clicked on for the second query, either.
Principle 1 (Reformulation). For consecutive queries q t  X  1 q t in a session such that q t reformulates q t  X  1 result d for q t  X  1 is clicked on or skipped, d as a result for q is unlikely to be clicked on and thus should be demoted.
When a user issues a specializing query, she likely wants to see results that are more specific about her interests.
Example 2 (Specialization). Table 2 shows two con-secutive queries. The user first asked  X  time life music  X  X nd clicked on the homepage of the store. The user further asked  X  time life Christian CDs  X  X ndclickedonthe4thand5thre-sults.

The information need of the second query consists of two parts: information about  X  X ime life X  and that about  X  X hris-tian CDs X . If we do not consider the context information, both components should be equally important in ranking search results of the second query. However, given the first query, the user likely wanted to see the search results for the second query specifically about the Christian CDs of the mu-sic store. This explains why the user skipped the first three results to the second query where the terms  X  X hristian X  and  X  X Ds X  do not appear in the titles of the search results.
Principle 2 (Specialization). For consecutive queries q t  X  1 q t in a session such that q t specializes q t  X  1 ,theuser likely prefers the search results specifically focusing on q The principle is particularly useful in several scenarios. For example, when q t is rare and q t  X  1 is popular, the answers fully matching q t  X  1 but partially matching q t may be ranked high for q t by a search engine. The principle can use the context information to demote the answers matching q t  X  1 given that q t  X  1 was just asked by the user.
 One possible way to implement the principle is as follows. Let q t \ q t  X  1 be the set of terms appearing in query q not in query q t  X  1 .If q t \ q t  X  1 =  X  , we should promote the results matching q t \ q t  X  1 in the set of answers to q
A user may ask a query more general than the previous one. In such a situation, the user may like to see some information not covered by the first query.

Example 3 (Generalization). Table 3 shows a gen-eralization scenario. A user first asked query  X  free online Tetris game  X  and clicked on the 1st and 2nd search results. The user then asked query  X  Tetris game  X  and clicked on the 3rd and 4th results.

The second query  X  Tetris game  X  carries multiple possible information needs. For example, the user may want to down-load the game or play it online. Alternatively, the user may be interested in the history or news of the game. The user may also look for the basic game rules or advanced cheats of the game. For such a query with ambiguous search needs, search engines often try to diversify search results. In this example, the top-5 results can be divided into two groups. The 1st, 2nd, and 5th results link to some free online Tetris game sites, while the 3rd and 4th results are about the back-ground information of the Tetris game.
 With the context that the previous query was  X  free online Tetris game  X  and the user clicked on two related sites, we may infer that the user X  X  interest in the second query may likely divert to something about the game but not the game sites. This may explain why the user clicked on the results about the background information of the game.

Principle 3 (Generalization). For consecutive queries q t  X  1 q t in a session such that q t generalizes q t  X  1 ,theuser would likely not prefer the search results specifically focusing One possible way to implement the principle is as follows. Let q t  X  1 \ q t be the set of terms appearing in q t  X  1 q .If q t  X  1 \ q t =  X  , we should demote the results matching q t  X  1 \ q t among the answers to query q t .
When a query (especially an ambiguous one) is generally associated with its context, the context may help to narrow down the user X  X  search intent.

Example 4 (General association). In Table 4, a user first raised query  X  Xbox 360 X  and clicked on the 1st and 3rd search results. Then, the user raised query  X  FIFA 2010  X  X nd clickedonthe4thresult.

The second query  X  FIFA 2010  X  bears multiple intents. It may refer to either the FIFA 2010 World Cup at South Africa or a new game of Xbox 360. Therefore, the sec-ond query  X  FIFA 2010  X  is generally associated with the first query  X  Xbox 360  X . Without the context, a search engine may retrieve search results for both intents behind query  X  FIFA 2010  X . However, using the first query  X  Xbox 360  X  as the con-text, which indicates that the user was interested in Xbox 360, we may rank the results about the soccer game in Xbox 360 higher than those about the World Cup event.

Principle 4 (General association). For consecutive queries q t  X  1 q t in a session such that q t and q t  X  1 ally associated, the user likely prefers the search results re-latedtoboth q t  X  1 and q t . Such results should be promoted for q t .

One possible way to implement the principle is the follow-ing. First we can choose any topic taxonomy such as the Table 5: The effectiveness of ranking principles in the corresponding types of contexts.
 Open Directory Project ( http://www.dmoz.org ). Let C t  X  1 and C t bethesetsoftopicsof q t  X  1 and q t , respectively, and C  X  be the set of common topics between C t  X  1 and C t .If C  X  =  X  , we should promote a search result u if the set of topics of u shares at least one topic with C  X  .
We use the search log data from a major commercial search engine to evaluate the effectiveness of the principles. We traced each user X  X  query &amp; click stream by the user-id information in the data. All users were completely anony-mous, and no action was taken to reveal the users X  identities. We segmented each user X  X  stream into sessions by a com-monly applied rule [19]: a boundary between two sessions was set if there was no activity by the user for thirty min-utes. From the resulted 37,320 user sessions, we extracted successive query pairs within the same sessions, and man-ually labeled the relations (i.e., reformulation, specializa-tion, generalization, general association, and unrelated) for 10 , 000 randomly selected successive query pairs.
We first evaluate the effectiveness of each principle in its corresponding type of contexts, i.e., when the two successive queries match the relation of the principle. Table 5 shows the number of successive query pairs for each type of contexts, where  X  X id X  indicates the principle-id. In our evaluation, a search result u is represented by the terms in its title, snippet, and URL. For Principle 4, we use a classifier in [14] and classify all the queries and documents into the 16 topics at the first level of Open Directory Project ( http://www. dmoz.org ). For each query or document, we keep the top three topics returned by the classifier.

According to the previous studies [6, 7, 8], a user views only a subset of search results and chooses to click on or skip them individually. Therefore, in each test case for a principle, we focus on those search results that are likely to be viewed by the user. Specifically, we adopt the methods in [6, 7, 8] and consider a search result is viewed if it is ranked above or one position below the last clicked result. To evaluate a principle, we aggregate the viewed search re-sults for all queries in the test cases and obtain a set U .We call a search result u  X  U satisfies the principle if u should be promoted (in cases of Principles 2 and 4) or not demoted (in cases of Principles 1 and 3) by the principle; otherwise, u violates the principle. Let U h 1  X  U be the set of search results that satisfy the principle, and U h 0 = U \ U h 1 taneously, U can also be divided into two subsets U c 1 and U c 0 ,where U c 1  X  U consists of the search results that were clicked on by the users, and U c 0 = U \ U c 1 .

The conditional probability for a search result u to be clickedonfor q t given that it satisfies a principle can be es-able c denotes whether a search result u was clicked on for q or not, and random variable h denotes whether u satisfies the principle or not. Analogously, the conditional proba-Table 6: The effectiveness of ranking principles in all contexts. bility for u tobeclickedonfor q t giventhatitviolatesa principle can be estimated as P ( c =1 | h =0)= | U c 1  X  U
We conduct a t-test on  X  = P ( c =1 | h =1)  X  P ( c =1 | h = 0), the difference between the two conditional probabilities. Intuitively, for each principle, this difference indicates how likely users would choose to click on a search result satisfying instead of violating the principle. One may wonder whether user clicks contain position bias. Since  X  value calculates the difference between two click probabilities, we may ex-pect that the position biases are canceled out. Therefore, if the difference  X  passes the significance test at confidence level 0.01, it confirms the effectiveness of the principle. From Table 5, we can see that Principles 1, 2, and 4 pass the sig-nificance test, which supports their effectiveness. However, Principle 3 does not pass the significance test. One reason is that generalization pairs are relatively rare, only 2.46% in the manually labeled data. We can hardly draw reliable conclusions from such a small size of test data.
Given two consecutive queries q t  X  1 q t in a session, a straight-forward way is to first determine the relation between q t q t  X  1 and then apply the corresponding principle. However, practical cases are often complicated and fuzzy. For exam-ple, it is not easy to determine whether query  X  X eneva food X  specializes or is generally associated with query  X  X eneva travel X  . It is very challenging to accurately classify the rela-tions between queries and contexts.

To tackle the above problems, we explore how well the principles can adapt to all possible contexts without explic-itly distinguishing the types of contexts, i.e., types of query relations. Empirically we evaluate the effectiveness of prin-ciples over all the query pairs extracted from user sessions. Each consecutive query pair q t  X  1 q t is used as a test case for a ranking principle if the principle demotes or promotes at least one search result for q t . It is possible for one query pair to be a test case for multiple principles. Table 6 shows the number of test cases for each principle as well as the eval-uation results, where  X  X id X  indicates the principle-id. For Principles 1, 2, and 4, the  X  values pass the significance tests at the confidence level of 0.01. Since the tests are con-ducted in all contexts, the results suggest that Principles 1, 2, and 4 adapt well to different types of contexts.
The  X  value for Principle 3 is negative, and it does not pass the significance test. By finer analysis on the test cases for Principle 3, we observe the following. First, Principle 3 is sensitive to query relations. We manually labeled the 1 , 539 test cases for Principle 3, and found that only 55% of them are of generalization relation. As shown in Table 5, the  X  value on generalization pairs is positive. Although that positive  X  value does not pass the significance test either, it suggests that Principle 3 may perform differently on different types of relations. Second, the test cases for Principle 3 in all contexts is only about 4% in our data set. This is because the search results for the current query q t are unlikely to contain the terms not in q t but in the previous query q t  X  1 Table 7: The major features in ranking models.
 Finally, since the number of test cases is small, it is unclear whether Principle 3 is effective. We may ignore Principle 3 due to the small amount of applicable cases.
Although we have developed effective context-aware rank-ing principles, to achieve fully context-aware ranking in Web search practice, there are still several challenges in applying the principles. First, a user session may contain more than two queries, while we only discuss the principles formulated based on two queries. How can we extend the applicability of the principles? Second, given a query as well as its context, there might be multiple principles applicable. How should we jointly execute the principles? Third, user sessions con-tain rich information. Many factors, such as the positions of the documents returned by the search engine and the terms shared by the current query a nd the previous ones, may all be useful in ranking documents. How can we incorporate those factors into the ranking model?
To address the above challenges, we employ a learning-to-rank approach to build context-aware ranking models. We derive features from the ranking principles developed in Section 3 and incorporate the features into learning-to-rank models. The ranking features extend the context infor-mation from the immediately preceding query and answers to all previous queries and answers within the same ses-sion of the current query. Besides the features derived from the previous ranking principles, we also incorporate other factors mentioned above as features of the ranking models. We create training data from search sessions and train the ranking models offline. In online ranking, the trained mod-els can carry out context-aware ranking using the available context information. It is not necessary to explicitly specify which principles to be used. By taking a learning-to-rank approach, we can address all the challenges identified above.
As a concrete example, we use RankSVM [7], a state-of-the-art learning-to-rank model to demonstrate our ap-proach. RankSVM learns an SVM model for classification on the preference between a pair of documents. In the train-ing stage, the RankSVM model takes as instance an ordered pair of documents with respect to a query under a context. Specifically, the i -th training example corresponds to query q vectors corresponding to the two documents, respectively, and y ( i ) denotes a preference label: if y ( i ) is 1, then u preferred to u ( i ) B ,otherwise, u ( i ) B is preferred to u
A feature in our RankSVM model is a function of query, document, and context. Table 7 lists the major features of the context-aware RankSVM model. Column  X  X id X  indi-cates from which principle the feature is derived. The model is flexible to combine features in addition to those from the principles (e.g., feature  X  X rgPos X ). In Table 7, u denotes a document for query q t . U c i and U s i denote the set of clicked and skipped documents for q i (1  X  i  X  t  X  1), respectively. q differences between the current query and previous queries in the session. q  X  = q t  X  ( q t  X  1  X  ...q 1 )isthesetofcom-mon terms among queries in the same session. Cosine (  X  , denotes Cosine similarity, Jaccard (  X  ,  X  ) denotes Jaccard In-dex. The common topics C  X  are derived by intersecting the topics of q t with those of previous queries.

One issue is how to combine the original ranking of the search engine. In general, there are two possible approaches. We can use the original posit ion of a document returned by the search engine as a feature in the RankSVM model. We denote this approach by RankSVM-F. Alternatively, we can train the RankSVM model without the original position feature. Given a test case, we combine the original ranking list R 0 from the search engine with the list R 1 from the RankSVM by Borda X  X  ranking fusion method [1], that is, where  X   X  [0 , 1] is a parameter, and R 0 ( u )and R 1 ( u )are the positions of document u in R 0 and R 1 , respectively. As a special case, when  X  = 0, the Borda X  X  fusion score com-pletely ignores the search engine ranking. We denote this case by RankSVM-R0. Otherwise, the model is denoted by RankSVM-R1. Both RankSVM-F and RankSVM-R1 are re-ranking models since they incorporate search engine X  X  rank-ing, while RankSVM-R0 is a ranking model.
We prepare the experimental data from a search log of a major commercial search engine. Since our ranking models use context features, we extract the search sessions with more than one query. In our search log, the percentage of such sessions is about 50%, which is consistent with the results reported by the previous studies [2, 4]. Among the 37,320 extracted sessions, we use half of them for training and validation, and the remaining half for testing.
In the following, we first describe the training process of the RankSVM models, including RankSVM-F, RankSVM-R0, and RankSVM-R1. We then compare the performance of our RankSVM models with a baseline proposed by Shen et al. [15] using both manually labeled data and user click data. We use the BatchUp method in [15] as the baseline since it has the best reported performance in [15]. BatchUp does not incorporate search engine X  X  ranking. Finally, we conduct case studies and discuss the experimental results.
We train the RankSVM models from manually judged document pairs with respect to given queries and their con-texts. Given a randomly selected search session with more than one query, we form an example ( q t ,c,d A ,d B ), where q is the last query in the selected session, context c con-sists of the previous queries and the search results clicked on or skipped by the user before q t in the session, and d and d B are among the top five documents for q t returned by the search engine. Please note d A is not necessarily ranked higher than d B by the search engine. Each document con-sists of its title, snippet, and URL.

For each example, a judge is asked to infer the user X  X  search intent based on q t as well as the context c . Then, Table 8: The performance of different methods on human-labeled data. the judge reviews the titles and snippets of d A and d B and gives the preference between d A and d B as if he or she were the user who issued query q t within the given context c . The judge does not know the original order of d A and d B returned by the search engin e. Thejudgecanchooseoneof the three options: 1) d A is more preferable than d B ;2) d is more preferable than d A ; and 3) unclear. A judge may choose the third option if he or she is not sure about the user X  X  search intent, or d A and d B are equally preferred. In our experimental setting, each example is labeled by three judges, and we take the majority of labeled results as the ground truth. An example is  X  X nclear X  if 1) at least two judges label it as  X  X nclear X ; or 2) one judge labels  X  X nclear X , and the other two judges have inconsistent preferences.
From the judged examples, we pick 1 , 500 cases which are not labeled as  X  X nclear X . We use 1,000 cases for training and the remaining 500 cases for validation. There are two parameters for our methods: C required by SVM for all the three RankSVM models and  X  in Equation 1 for the RankSVM-R1 model. We tune the parameters on the vali-dation data and set C =1 , 000 and  X  =0 . 45. We will use this parameter setting for all the following experiments. Performance on manually labeled data .

We first compare on the manually labeled data the perfor-mance of the three RankSVM models, the baseline, and the search engine. For each of the four context-aware methods (RankSVM models and the baseline), we randomly select 500 examples where the method reverses the original order of d A and d B returned by the search engine.
 Table 8 shows the results, where X  X S X  X tands for RankSVM. We consider that a method has a correct case against the search engine if the order given by the method is consistent with the judged preference. Otherwise, the method has an error case. We calculate the percentages of correct and error cases over all the 500 reversed pairs, denoted by P(correct) and P(error) in Table 8, respectively. The row  X  X mprove-ment X  in Table 8 is the difference between P(correct) and P(error), which indi cates how much a (re-)ranking method improves over the search engine. We also conduct signifi-cance tests on the improvements of different methods. All tests use the t-statistic and set the confidence level to 0.01. All of the four context-aware (re-)ranking methods (three RankSVM models and the baseline method) make significant improvement over the search engine. Moreover, all of our three models, RankSVM-F, RankSVM-R0, and RankSVM-R1, perform significantly better than the baseline. This is because the baseline applies a single ranking strategy and may not adapt well to various types of contexts in Web search. Our models encode multiple principles for context-aware ranking as features and automatically adapt to differ-ent types of contexts.
 We also compute the reverse ratio, i.e., the percentage Table 9: The performance of different methods on user click data. of document pairs for which a (re-)ranking method reverses the orders by the search engine, over all the document pairs from the test sessions. This measure indicates how likely a method will reverse the order of a random pair of search results returned by the search engine. Table 8 shows the reverse ratio for each method. The two general ranking models, RankSVM-R0 and the baseline, have the highest reverse ratio. One of the re-ranking model, RankSVM-F, has a reverse ratio comparable with those of the two gen-eral ranking models, suggesting that the original position feature may not play a critical role in the model. The other re-ranking model, RankSVM-R1, is the most conservative. This is because we set a large  X  value (  X  =0 . 45) in the Borda X  X  fusion method (Equation 1).
 Performance on user click data.

Although manually labeled data is usually of good quality, there could be two concerns. First, the judging process is expensive, and thus cannot be scaled up. Second, a search intent inferred by the judge may not be consistent with that of the real user. Therefore, we further evaluate the perfor-mance of the ranking methods by using real click data.
Since we consider users X  clicks as their preference on search results, we only select the sessions in the test data such that the last query in a selected session must have at least one click. This results in 13,651 sessions. We follow the previous studies [6, 7, 8] and assume that 1) a user views and clicks on search results from top to bottom; and 2) a user keeps viewing search results until the one that is one position lower than the last document clicked on. For example, suppose the last URL clicked on by a user for query impression q t is at position 5, we consider the user views all search results at positions from 1 to 6. Then, those six search results form a test case and will be (re-)ranked by our models and the baseline.

The performance of the (re-)ranking methods can be eval-uated by whether they promote the search results which are clicked on by users to higher positions. Specifically, for the i -th test case, we derive the set U ( i ) c of the clicked URLs for the last query q ( i ) t . Then, we aggregate all the test cases and where R ( u ) is the rank of u in a ranked list. A smaller MCP indicates a better ranking method.

Table 9 shows the performance of different methods on the test data, where  X  X S X  stands for RankSVM. The row  X  X CP Improvement X  records the differences between the search en-gine and the (re-)ranking methods. We also conduct signif-icance test on the improvements. All methods have sig-nificantly lower MCPs than that of the search engine. In other words, all methods perform better than the search en-gine. Again, the improvement of the baseline method passes the significance test, but it is not as large as those by the RankSVM models.

We also test for each method the percentage of lists in which the method reverses the order of at least one pair of search results. Similar to Table 8, the RankSVM-R1 method is most conservative, while the other three methods have comparable reverse ratios. For each method, the reverse ratio in Table 9 is much higher than that in Table 8. This is because, in Table 8, we consider the reverse ratio for pairs of search results, while in Table 9, we consider the reverse ratio for lists of search results. Since a list usually contains multiple pairs, the reverse possibility increases substantially. Summary of performance tests.

We consider the human labeled data and user click data complementary to each other. For example, the human la-beled data overcomes the noise and position bias in user clicks, while the user click data is large and truly reflects the preference of users. Interestingly, the experimental re-sults on both data agree with each other on the following aspects. First, all the four context-aware methods, i.e., the three RankSVM models and the baseline, are better than the search engine. This confirms the effectiveness of context-aware ranking. Second, all our three RankSVM methods perform better than the baseline in context-aware ranking. As explained before, this is because our models consider dif-ferent types of contexts in Web search. Third, RankSVM-F, RankSVM-R0, and the baseline have comparable reverse ratios, while the RankSVM-R1 method is relatively conser-vative due to a large  X  value (  X  =0 . 45). Finally, on both test data sets, the RankSVM-F and RankSVM-R1 methods show larger improvements than that of the RankSVM-R0 method, suggesting the usefulness of considering the origi-nal ranking of the search engine. However, the evidence is not strong enough to pass the significance test.
 Case studies and discussions.

We conduct case studies on both situations where our ranking models succeed and fail. Recall the examples in Tables 1-4. For all the four examples, all of our models yield a ranking in which the documents clicked on by the user are ranked higher than those skipped by the user. However, the baseline only works well on the last example. This is because the baseline gives rise to the documents which are similar to both the current query and its context, which consists of the previous queries as well as the summaries of the previously clicked documents. In the last example, the summaries of Consequently, the language model which incorporates the context information boosts the 4th result for q t to the top position. In this case, the baseline bears a similar spirit with our Principle 4.

In the first three examples, the users reformulate, special-ize, and generalize their initial queries in the hope to see some new results. However, in these cases, the baseline does not consider the types of contexts and still applies the single principle which tends to provide information similar to that appeared in the previous queries. Consequently, the rank-ing results may not meet the users X  information needs well. On the contrary, our ranking models incorporate multiple ranking principles and automatically adapt to various types of contexts.

We also investigate some cases for which our models make wrong decisions. We find three major reasons for those cases. First, the ranking principles developed in Section 3.1 do not necessarily hold. For example, in some sessions, peo-ple simply click on the documents which have been just clicked before in the same session. As explained in previ-ous studies [18], some users may use queries or keywords (such as  X  X sn news X  ) to  X  X ookmark X  a Web page (such as www.msnbc.msn.com ). In some other sessions, the search re-sults do not improve much after the users refine their queries. In such cases, the users may choose to click on some search results they skipped for previous queries.

The second reason for false re-ranking may be our imple-mentation of the ranking principles. In a real search session, the user first raised a query  X  X uper bowl X  and then submitted query  X  X uper bowl nine X  . Our implementation of Principle 2 promotes the search results containing the term  X  X ine X  to higher positions than that of the Wikipedia page for Super Bowl IX, which contains the term  X  X X X  instead of  X  X ine X . In fact, the Wikipedia page is the result the user clicked on. The remaining errors may come from our employment of RankSVM as the ranking model. Although RankSVM is one state-of-the-art ranking models, many other learning-to-rank models have been proposed in the literature [11]. It is still an open question which model is the best. Similarly, more studies are needed on what kind of models are most suitable for context-aware ranking.
In this paper, we studied the problem of using context information in ranking documents in Web search. We con-ducted an empirical study on real search logs and devel-oped four principles for context-aware ranking. We further adopted a learning-to-rank approach and incorporated our principles to ranking models. The experimental results ver-ified the effectiveness of our approach.
