 Unexp ected rules are interesting because they are either pre-viously unkno wn or deviate from what prior user kno wledge would suggest. In this pap er, we study three imp ortan t is-sues that have been previously ignored in mining unexp ected rules. First, the unexp ectedness of a rule dep ends on how the user prefers to apply the prior kno wledge to a given sce-nario, in addition to the kno wledge itself. Second, the prior kno wledge should be considered righ t from the start to focus the searc h on unexp ected rules. Third, the unexp ectedness of a rule dep ends on what other rules the user has seen so far. Thus, only rules that remain unexp ected given what the user has seen should be considered interesting. We de-velop an approac h that addresses all three problems above and evaluate it by means of exp erimen ts focusing on nding interesting rules.
 H.2.8 [ Database Managemen t ]: Database Applications| Data Mining Asso ciation rule, sub jectiv e interestingness, unexp ected rule
Data mining aims at nding previously unkno wn and in-teresting rules for the user. Most data mining algorithms use obje ctive measur es of interestingness, suc h as statistical sig-ni cance [14, 15], Chi-square test [6] and supp ort/con dence [2]. Often, rules that satisfy suc h measures are kno wn and unin teresting to the human user [7, 8, 11]. For example, a relationship between two items is deemed interesting by the
Researc h was supp orted in part by a researc h gran t from the Net works of Cen tres of Excellence/Institute for Rob otics and Intelligen t Systems, and in part by a researc h gran t from the Natural Science and Engineering Researc h Council of Canada Chi-square test if it is signi can tly stronger than exp ected when items are assumed to be indep enden t, but not inter-esting to the human user who knows suc h or similar relation-ships. On the other hand, interesting rules may not satisfy an objectiv e measure. For example, the minim um supp ort requiremen t in [2] tends to prune unkno wn asso ciation rules that often do not have a large supp ort.

To nd interesting rules, subje ctive measur es taking into accoun t what the user kno ws have been suggested [15, 17, 18]. One sub jectiv e measure is unexp ectedness , where a rule is unexp ected if it \surprises" the user. Another sub jectiv e measure is actionability , where a rule is actionable if the user can act upon it to her adv antage. Due to the di-cult y of mo deling the user kno wledge, works on sub jectiv e measures are much few er [8, 9, 11, 13, 16, 15]. This pa-per focuses on the unexp ectedness measure. Tw o previous approac hes, the syntax based [8, 9] and the logic based [12], follo w the paradigm of pairwise comparison: compare a rule r found in the data against a single rule represen ting the user kno wledge. If the comparison rev eals a syn tax distance (i.e., a similar body but a dissimilar head) or a logical con-tradiction (i.e., a specialized body but a negated head), r is considered unexp ected.

In this pap er, we examine three imp ortan t issues that have been previously ignored in mining unexp ected rules.
Kno wledge dynamics : Whether a rule is unexp ected
Kno wledge push : The prior kno wledge should be con-
Unexp ectedness dynamics : The unexp ectedness of a
The follo wing example motiv ates our approac h.
Example 1.1 (Unexpected rules). Consider nding unexp ected rules about how people mak e donation in a fund raising campaign. Supp ose that the user believ es, based on past exp erience, that movie stars tend to donate more than $500 and well paid people tend to donate more than $200, represen ted as the \kno wledge rules": R 1 : Salary = hig h ! Donation = Abov e 200, R 2 : Job = mov ie star ! Donation = Abov e 500.
 Supp ose that the follo wing \data rule" is found from the data: r : Loc = BH; House = yes ! Donation = Below 50, whic h says that people owning houses in Bev erly Hills tend to donate less than $50. r would not be iden ti ed as unex-pected by the syn tax based approac h [8, 9] because it is not related in syn tax to the kno wledge rules. Neither would r be iden ti ed as unexp ected by the logic based approac h [11] because it does not logically con tradict the kno wledge rules.
However, supp ose the user kno ws that Bev erly Hills is an exp ensiv e place and the home of movie stars, thus decides it mak es sense to apply rules R 1 and R 2 to the subp opulation summarized by r , viz., people owning a house in Bev erly Hills. Her rationale migh t be these rules describ e donation patterns by wealth y people and rule r concerns that as well. Thus, she prefers to compare r with rules R 1 and R 2 , to judge if the former is unexp ected. This comparison rev eals that r is indeed unexp ected, since its conclusion is signi -can tly di eren t from that of R 1 ; R 2 with resp ect to exp ected donation levels.

The above example illustrates sev eral interesting points that motiv ate our work. 1. Giv en a data rule r , often the human user has her own 2. To sim ulate the above user pro cess of determining the 3. The above approac h emphasizes two imp ortan t asp ects
Remarks . One may argue that the syn tax and logic based approac hes [8, 9, 12] migh t iden tify the specialize d rules of r as unexp ected, obtained by adding the conditions of R 1 and R 2 to r . However, these specialized rules may not be found in the rst place because they may not pass the objectiv e measures required (suc h as the minim um supp ort) in those approac hes (say, if movie stars are paid by con tract, not by salary). Also, these rules are less desirable than the simple structure r because they fragmen t a single large violating subp opulation into sev eral small violating subp opulations.
We presen t a new approac h to the problem of mining unexp ected rules. First, we prop ose a new notion of un-exp ectedness by incorp orating a preference mo del speci ed by the user. The preference mo del is either a de ning cri-terion or an enumeration of the covering kno wledge. Sec-ond, we presen t an algorithm for mining all unexp ected rules that satisfy user-sp eci ed minim um \unexp ectedness signif-icance" and minim um \unexp ectedness strength". The al-gorithm pushes the signi cance requiremen t and examines only those rules that satisfy this requiremen t. Finally , we presen t a metho d for selecting a speci ed num ber of most unexp ected rules, taking into accoun t the unexp ectedness dynamics discussed earlier.

The rest of the pap er is organized as follo ws. Section 2 reviews related works. Section 3 presen ts our represen tation of user kno wledge. Section 4 de nes the problem of mining unexp ected rules and the problem of selecting unexp ected rules. Section 5 presen ts an algorithm for mining unexp ected rules, and Section 6 presen ts an algorithm for selecting un-exp ected rules. Section 7 evaluates the e ectiv eness of the prop osed metho ds. Section 8 concludes the pap er.
Interestingness measures have been studied in the eld of data mining [3, 6, 7, 8, 13, 20, 16, 15, 17, 18], with most works on objectiv e measures [20]. Mining unexp ected rules was studied in [8, 9, 11, 13, 16, 15]. None of these works con-siders the user preference in applying kno wledge rules. The second di erence is that we consider the user kno wledge at the beginning of searc h. In con trast, the post-analysis ap-proac h [8, 9, 16] considers the user kno wledge at the end of searc h, as one additional lter to the result found by an objectiv e measure metho d. In this resp ect, our approac h is similar to [11], but the di eren t notion of unexp ected-ness mak es the two studies very di eren t. Finally , we con-sider the unexp ectedness dynamics discussed earlier. This is quite di eren t from the work on eliminating redundan t rules within the framew ork of objectiv e measures (e.g., [23]).
The work on mining asso ciation rules [2] searc hes for rules of supp ort above a given threshold. The supp ort measures the frequency of a rule in the data, whic h does not con-sider the novelty with resp ect to the user kno wledge. Con-sequen tly, if the supp ort threshold is too high, unexp ected rules may not pass the threshold, and if the supp ort thresh-old is too low, man y rules kno wn to the user are returned. The correlation approac h suc h as [6] measures the supp ort relativ e to the indep endence assumption, whic h mak es sense only if the user kno ws nothing about the domain. Our no-tion of unexp ectedness signi cance captures the part of sup-port that is resp onsible for violating the user kno wledge. In this sense, it addresses the novelty of rules.

The work on constrained data mining [4, 10, 19, 21, 22] considers what the user wants , i.e., constrain ts, and searc hes for rules that satisfy the speci ed constrain ts. The un-exp ected rule mining considers what the user knows , i.e., kno wledge, and searc hes for rules that surprise the user with new information. Unexp ected rules may not satisfy the speci ed constrain ts, and satisfying rules may not be unexp ected. Indeed, it is a rather dicult task for the user to specify what she wants for something that she wishes to be a surprise.
We describ e our represen tation of user kno wledge. Con-sider a relational table T of tuples over sev eral attributes. Mining in suc h data is typically targeted at a speci c at-tribute because the user normally wants to kno w how other attributes are related to this target attribute. Our objectiv e is to nd data rules of the form where A i is a non-target attribute, a i is a domain value for A , C is the target attribute, c is a domain value for C . We C = c is the head of the rule. For a data rule r , b ( r ) denotes the body of r and h ( r ) denotes the head of r .
 A tuple matches a data rule r if b ( r ) holds on the tuple. A tuple satis es a data rule r if both b ( r ) and h ( r ) hold on the tuple. M AT ( r ) denotes the set of tuples that matc h r and SAT ( r ) denotes the set of tuples that satisfy r . sup ( r ), called supp ort of r , denotes the fraction of tuples that satisfy conf ( r ), called con denc e of r , denotes the fraction of tu-ples that satisfy r given that they matc h r , i.e., conf ( r ) = sup ( r ) =sup ( b ( r )).

We represen t the user kno wledge K in two parts, the kno wledge rules and the preference mo del.
While data rules are of the form above, we allo w for sligh tly more general form for the rules capturing user's prior kno wledge, called know ledge rules and denoted K , by allo w-ing fuzzy terms like "high", "low", etc., as suggested in [8, 9]. It will then be necessary to de ne a degree of matc h between domain values app earing in data rules and in data tuples on the one hand and these fuzzy terms app earing in kno wledge rules on the other. We use the upp er case letter R for a kno wledge rule, and use the lower case letter r for a data rule. Lik e for a data rule, b ( R ) and h ( R ) denote the body and head of a kno wledge rule R . m ( v; v 0 ) measures the match degree between a domain value v and a user value v (for the same attribute), whic h can be either speci ed by the user or chosen from a pre-determined list. The matc h degree is in the range [0,1], with a larger value corresp onding to a better matc h. The follo wing is an example of kno wledge rules involving fuzzy terms.

Example 3.1 (Degree of match) . Supp ose that the domain of the attribute Education is A fuzzy term Low (serving a user value) could be describ ed by a fuzzy set 1 and 0.5 are the matc h degrees between the domain values P rimar y; Secondar y and the fuzzy term Low . So, P rimar y perfectly matc hes Low , and Secondar y partly matc hes Low . Since the domain values U niv ersity and Graduate are not listed in the fuzzy set, they do not matc h Low , i.e., the matc h degree is 0. Supp ose that a kno wledge rule r has the form Education = Low ! Loan = N o . A tuple t con taining Education = Secondar y and Loan = Y es will have a body matc h degree of 0.5 and head matc h degree of 0.

The purp ose of the matc h degree measure m is to de-ne the notions of matc h and violation between a tuple and a kno wledge rule. First, consider two bags of values: V = f v 1 ; ; v d g and V 0 = f v 0 1 ; ; v 0 d g , where v are in the range [0,1]. We write V V 0 if for 1 j d , there is a distinct i j suc h that v j v 0 i tion agg is wel l-behave d if (1) min ( V ) agg ( V ) max ( V ), and (2) for V V 0 , agg ( V ) agg ( V 0 ). For example, avg; max; min; medium are well-b eha ved, but not sum be-cause it is not in the range [0,1].

Definition 3.1 (Rule match) . Consider a tuple t , a kno wledge rule R and a well-b eha ved aggregate function agg . bm ( t; R ) = agg ( f m ( v; v 0 ) g ) measures the body match degree of t with R , where v and v 0 are the values in t and R over the attributes in b ( R ). hm ( t; R ) measures the head match degree of t with R , de ned as the matc h degree m of the class of t and h ( r ). bm ( t; R ) and hm ( t; R ) are in the range [0,1], with a larger value corresp onding to a better matc h. Let hm ( t; R ) = 1 hm ( t; R ). For a given threshold , we say that t matches R if bm ( t; R ) .

The intuition is that the more the body matc hes degree and the less the head matc hes degree, the more the violation. This is formalized below.
 Definition 3.2 (Single knowledge rule viola tion). The violation of R by t , denoted v ( t; R ), is de ned as v ( t; R ) =
Other functions are possible, suc h as q hm ( t; R ) bm ( t; R ), as long as they capture the intuition of violation. We leave the choice to the application. v ( t; R ) essen tially dep ends on the measure of matc h degree m , of whic h we also leave the choice to the application. Imp ortan tly, our discussion below does not dep end on these choices.
The prefer ence model speci es the user's kno wledge about how to apply kno wledge rules to a given scenario or a tu-ple. This is done by specifying the \co vering kno wledge" for eac h tuple. For a given tuple, the covering know ledge refers to one or more kno wledge rules that matc h the tuple and the user prefers to apply to the tuple. The covering depth d refers to the num ber of kno wledge rules in the covering kno wledge, usually a small integer suc h as 1, 2, or 3. The reason for allo wing the covering depth d greater than 1 is simple: the user sometimes has sev eral (preferred) kno wl-edge rules about a scenario and want to factor in all of them in the unexp ectedness measure. This does not mean that the total num ber of kno wledge rules will be j T j d , because the covering kno wledge for di eren t tuples is not required to be disjoin t. Here are sev eral examples of specifying the cover-ing kno wledge: (1) enumerating the covering kno wledge for eac h tuple by the user, (2) ranking kno wledge rules so that the covering kno wledge is the rst d matc hing kno wledge rules in the list, (3) specifying a preference measure, suc h as maximum strength prefer ence (e.g, belief degree, con dence, etc.), best match prefer ence (i.e,, maximizing m ( t; R )), or minimum violation prefer ence (i.e,, minimizing v ( t; R )). (1) is not scalable for large databases, whereas (2) and (3) are because they can be automated.

Example 3.2. Supp ose that a bank er has the follo wing kno wledge rules about loan appro val, rank ed in this order: For an applican t who owns a house but has no job, the rst kno wledge rule will serv e the covering kno wledge (of covering depth 1), whic h implies the application is exp ected to be appro ved. If the rank is rev ersed, Job = no ! Loan = no will be the covering kno wledge of the applican t instead, whic h implies the di eren t exp ectation that the application should be rejected. This example sho ws how the user applies kno wledge rules will a ect the user's exp ectation, and hence the unexp ectedness of the rules mined from the data.
The above has assumed that at least d kno wledge rules matc h a given tuple, where d is the covering depth. In gen-eral, we can add d duplicates of the default rule , denoted N U LL , to the user kno wledge K , with bm ( t; N U LL ) = 1 and hm ( t; N U LL ) = 0 for any tuple t . The default rules are used only if there are not enough matc hing kno wledge rules, therefore, are least preferred in any preference mo del. The choice of bm ( t; N U LL ) = 1 and hm ( t; N U LL ) = 0 im-plies v ( t; N U LL ) = 1 for any tuple t . Hence, with the lowest preference, the default rules implemen t the unexp ectedness presumption in the absence of kno wledge.
With the notion of covering kno wledge, we can now mea-sure the violation of user kno wledge K by a data tuple t . Definition 4.1 (Viola tion of user knowledge) . Let C be the covering kno wledge of a tuple t with resp ect to K . The violation of t with resp ect to K is de ned by for some well-b eha ved aggregate function agg .

To de ne the notion of unexp ectedness of a data rule r , we consider eac h tuple t that satis es r but violates the covering kno wledge of the tuple as an evidence that r is unexp ected with resp ect to K . This evidence is quan titativ ely measured by v K ( t ). For r to be considered as unexp ected, there must be \sucien t" evidence in the data as formalized by sev eral measures de ned below.

Definition 4.2 (Unexpectedness measures) . Consider a database T , a data rule r , and the user kno wledge K . The unexp ectedness supp ort of r with resp ect to K is de ned by The unexp ectedness con denc e of r with resp ect to K is de-ned by The unexp ectedness of r with resp ect to K is de ned by (Note that U sup ( r ) ; U conf ( r ) ; U nexp ( r ) are in the range [0,1].)
Remarks : 1. U sup ( r ) measures the unexp ectedness signi cance in 2. U conf ( r ) measures the unexp ectedness strength in terms 3. U nexp ( r ) measures the unexp ectedness strength in terms when we don't want to distinguish between them.
 Definition 4.3 (Unexpected rule mining problem).
 Giv en a database T , the user kno wledge K and thresh-olds 1 ; 2 (in the range [0,1]), the unexp ected rule min-ing is to nd all data rules r suc h that U sup ( r ) 1 and U str ( r ) 2 .

Tw o points are worth noting. First, if the user has no prior kno wledge, the covering kno wledge of eac h tuple t is the NULL rule and v K ( t ) = 1. In this case, U sup ( r ) = sup ( r ), U nexp ( r ) = 1 and U conf ( r ) = conf ( r ). Therefore, the clas-sic asso ciation rule mining [2] is the degenerate case of the unexp ected rule mining where every satisfying tuple serv es as a perfect violation. From this standp oint, it mak es much sense to discriminate among the satisfying tuples in terms of their degree of con rming or violating the user kno wl-edge and use those with a large violation to characterize whic h rules are unexp ected. Second, this approac h dep ends on the functions bm , hm , v K ( t ) and a preference mo del, but does not dep end on how they are speci ed, therefore, a ords a great deal of exibilit y for incorp orating domain speci c kno wledge through particular speci cations of these functions.

An algorithm for mining unexp ected rules is given in Sec-tion 5.
Using unexp ectedness in place of mere numerical thresh-olds like supp ort/con dence already is a huge value-add. Still, the num ber of unexp ected rules found can be too large for a human user. In the rule selection problem, we would like to select a speci e d (usually small) num ber of rules from a set of rules found so that they are as unexp ected as possi-ble. Simply ranking all rules by a criterion based on U sup ( r ) and U str ( r ) does not serv e this purp ose because sev eral sim-ilar rules could be rank ed high but may not pro vide new insigh ts. Our approac h is to mo del the unexp ectedness of remaining rules in the presence of the rules already pre-sen ted to the user. Consider the user kno wledge K and a set of data rules R found. Supp ose that we have selected R i = f r 1 ; : : : ; r i 1 g from R , where i 1. We want to select the next most unexp ected rule r i from R R i , assuming that the user has seen R i . The implication of having the rules in R i is stated in the follo wing assumption.
The See-and-Kno w assumption : After seeing R i , the user is awar e of the rules in R i , ther efor e, is inter este d in only those rules that are unexp ected with respect to R Therefore, if r i is \similar" to any rule in R i , r i is not unex-pected to the user. We can mo del this seman tics by treat-ing R i as new kno wledge rules with the minimum violation prefer ence (see subsection 3.2): the covering kno wledge of a tuple t is de ned by the least violated (i.e., closest) matc h-ing rules in R i , denoted by C i t . Let C t denote the (usual) covering kno wledge with resp ect to K . Whenev er r i is unex-pected with resp ect to either of K and R i , r i is unexp ected with resp ect to &lt; K ; R i &gt; as a whole. Therefore, we de ne the covering kno wledge of t with resp ect to &lt; K ; R i the d least violated rules in C t [C i t , where d is the cover-ing depth. This preference mo del is called See-and-Know prefer ence for &lt; K ; R i &gt; .

Definition 4.4 (Rule selection problem) . Let U sup ( r ) T , the user kno wledge K , a set of data rules R , thresholds , an integer k , and a selection criterion f ( U sup; U str ) (for a single rule), the unexp ected rule sele ction is to nd k data rules r 1 ; : : : ; r k from R suc h that, for 1 i k , 1. U sup ( r i ) 1 , and 2. f ( U sup ( r i ) ; U str ( r i )) is maxim um in R R i
In words, r 1 ; : : : ; r k are the list of rules from R suc h that, for 1 i k , r i has sucien t unexp ectedness supp ort and is most unexp ected (as per the selection criterion f ) with re-spect to the kno wledge K plus the previously selected rules r a way for the user to weigh between U sup and U str . For ex-ample, f can corresp ond to the ranking criterion using the key ( U sup; U str ) (or ( U str; U sup )) where U sup and U str are the primary and secondary keys, resp ectiv ely, or in gen-eral, f can be some weighed sum or com bination of U sup and U str . f is a criterion for individual rules r i , not for a collection of selected rules. The latter has the disadv an-tage of high complexit y of nding an optimal collection of rules. We address the interaction among selected rules by dynamically updating the kno wledge &lt; K ; R i &gt; wrt whic h U sup ( r i ) and U str ( r i ) are de ned.
 An algorithm for selecting unexp ected rules is given in Section 6.
 Algorithm 1 UMINE: the unexp ected rule mining Input: a database T , user kno wledge K , thresholds 1 and Output: data rules r suc h that U sup ( r ) 1 and U str ( r ) 2 1: /* The violation computing phase */ 2: T 0 = ; ; 3: for all tuple t in T do 4: compute v K ( t ); 5: if v K ( t ) &gt; 0 then 6: add &lt; t; v K ( t ) &gt; to T 0 ; 7: end if ; 8: end for ; 9: /* The rule generating phase */ 10: U 1 all rules r of length 1 with U sup ( r ) 1 ; 11: k = 1; 12: while U k 6 = ; do 13: generate candidate rules C k +1 using U k ; 14: for all tuple &lt; t; v K ( t ) &gt; in T 0 do 15: for all rule r 2 C k +1 suc h that t 2 SAT ( r ) do 16: U sup ( r ) = U sup ( r ) + v K ( t ) = j T j ; 17: end for ; 18: end for ; 19: U k +1 all rules r in C k +1 with U sup ( r ) 1 ; 20: k + +; 21: end while ; 22: /* The nal phase */ 23: for all tuple t in T do 24: for all rule r 2[ U k suc h that t 2 SAT ( r ) do 25: sup ( r ) + +; 26: sup ( b ( r )) + +; 27: end for ; 28: end for ; 29: output the rules r in [ U k suc h that U str ( r ) 2 .
We presen t the algorithm for mining unexp ected rules, called UMINE, in Algorithm 1. There are three phases: the violation phase, the rule phase, and the nal phase.
The violation phase computes and stores v K ( t ) for all tu-ples t in the database T . &lt; t; v K ( t ) &gt; denotes the tuple t with stored v K ( t ). An imp ortan t optimization is to prune all tuples t with v K ( t ) = 0 because suc h tuples have no con tribution to U sup ( r ). This is established below and im-plemen ted in lines 1-8 by storing only &lt; t; v K ( t ) &gt; with v ( t ) &gt; 0 in T 0 .
 Theorem 5.1. Let T 0 = f &lt; t; v K ( t ) &gt; j t 2 T ^ v 0 g . Then for any data rule r , U sup ( r ) is the same with resp ect to T as well as T 0 .
 In words, we can replace T with T 0 without a ecting U sup ( r ). Note that v K ( t ) = 0 if, for every covering kno wl-edge rule R of t , hm ( t; R ) &lt; 0 in De nition 3.2 (note that bm ( t; R ) for suc h R ). In this case, t is consisten t with the user kno wledge. This pruning strategy mak es much sense: if the data is largely consisten t with or con rming the user kno wledge, the con rming tuples have no value for mining unexp ected rules.
The rule phase (lines 9-21) generates all rules with U sup ( r ) 1 using T 0 . Simply enumerating all possible rules r is pro-hibitiv e. We are interested in a metho d that generates only promising rules. First, we sho w a prop erty about U sup ( r ) that forms the basis of this metho d.

Theorem 5.2. U sup ( r ) is anti-monotone with resp ect to ther, this prop erty is indep enden t of the choice of the pref-erence mo del and violation function v K ( t ).
 Proof : Consider two data rules r and r 0 suc h that the body of r 0 has a sup erset of conditions compared to the body of r . Thus, the anti-monotonicit y follo ws. To complete the pro of, note that v K ( t ) on both sides of is the same for the same tuple t , indep enden tly of how the covering kno wledge of t is speci ed and how v K ( t ) is de ned.

Follo wing Theorem 5.2, we need to examine a longer rule x ; : : : ; x k 1 ; x k ; x k +1 ! c only if both shorter rules pass the threshold 1 for U sup ( r ), where x k and x k +1 domain values for di eren t attributes. Thus, we can searc h for all rules r with U sup ( r ) 1 as follo ws. In the rst iteration, we generate all rules r of length 1 of the form x 1 ! c suc h that U sup ( r ) 1 . These rules are stored in U 1 . Subsequen tly, in the ( k + 1)th iteration ( k &gt; 0), we generate all candidate rules r of length k + 1, denoted by C k +1 , of the form x 1 ; : : : ; x k 1 ; x k ; x k +1 rules in U k of the above form. From Theorem 5.2, C k +1 con tains all rules r with k + 1 conditions in the body suc h that U sup ( r ) 1 . We compute U sup ( r ) for all r in C in one scan of T 0 while accum ulating v K ( t ) over the tuples &lt; t; v K ( t ) &gt; 2 T 0 suc h that t 2 SAT ( r ). U k +1 those rules r in C k +1 with U sup ( r ) 1 . These steps are describ ed in line 9-21 in Algorithm 1.

The above pruning strategy was rst suggested by Agra wal et al. [2] for mining asso ciation rules of sucien t supp ort sup . However, the notion of supp ort does not tak e into ac-coun t the \no velty" of a rule with resp ect to the user kno wl-edge: rules of sucien t supp ort may not be unexp ected, and unexp ected rules may not have sucien t supp ort. We ad-dress this problem using the notion of unexp ectedness sup-port to capture the part of supp ort that is resp onsible for violating the user kno wledge. In this sense, our pruning is based on the \no velty" of rules.
The nal phase (line 22-29) computes sup ( r ) and sup ( b ( r )) for rules r in [ U k pro duced in the rule phase, and outputs the rules r with U str ( r ) 2 . This time, we scan T instead for all rules r suc h that t 2 SAT ( r ) and t 2 M AT ( r ), resp ec-tively. After computing sup ( r ) and sup ( b ( r )) for all rules r in [ U k , we output those rules r suc h that U str ( r )
A note on implemen tation . First, at lines 15 and 24, for every tuple t in T 0 or T , we need to nd all rules r suc h that t 2 SAT ( r ). We can borro w the subset function [2] for these operations. Second, curren tly we compute U sup ( r ) using the pruned database T 0 in one phase, compute sup ( r ) Algorithm 2 USELECT: the unexp ected rule selection al-gorithm Input: a database T , a set of data rules R , user kno wledge K , threshold 1 , an integer k , and f ( U sup; U str ) Output: as in De nition 4.4 1: /* initialization */ 2: R 1 = ; ; i = 1; s = j T j ; 3: for all tuple t in T do 4: compute C t = f R 1 ; ; R d g and replace t with &lt; 5: for all rule r in R suc h that t 2 SAT ( r ) do 6: U sup ( r ) = U sup ( r ) + v &lt; K ; R 1 &gt; ( t ) =s ; 7: sup ( r ) + +; 8: sup ( b ( r )) + +; 9: end for 10: end for 11: while i k and R6 = ; do 12: /* select the next most unexp ected rule */ 13: select the rule r i from R suc h that U sup ( r i ) 1 14: /* update the information */ 16: update the tuple with v ( t; r i ); 18: for all r in R f r i g suc h that t 2 SAT ( r ) do 19: U sup ( r ) = U sup ( r ) + v &lt; K ; R i &gt; ( t ) =s ; 20: if U sup ( r ) &lt; 1 then 21: delete r from R 22: end if ; 23: end for ; 25: delete t from T 26: end if ; 27: end for ; 28: output r i ; 29: R i = R i [f r i g ; 30: R = R f r i g ; 31: end while ; 32: output the rules in R i in the order added; and sup ( b ( r )) using the unpruned database T in another phase. Alternativ ely, we can compute all three using the unpruned T in a single phase. We distinguish these strate-gies by UMINE-Pruned and UMINE-Unpruned. Third, the above description of the rule generating phase is based on the Apriori-lik e breadth-rst generation of rules [2]. What is essen tially required is just the anti-monotonicit y of the measure pushed, whic h in our case is U sup . Therefore, any implemen tation based on the anti-monotonicit y, suc h as the depth-rst generation, can be adopted as well.
We presen t the algorithm for selecting a speci ed num-ber of unexp ected rules, called USELECT, in Algorithm 2. First, the initialization (line 1-9) computes U sup ( r ), sup ( r ), sup ( b ( r )) for all rules r in R in one database scan. It also computes the violation bag V t = f v ( t; R 1 ) ; ; v ( t; R stores it with eac h tuple t , denoted by &lt; t; V t &gt; , where C = f R 1 ; ; R d g is the covering kno wledge of t with re-spect to &lt; K ; R i &gt; . d is usually a small num ber like 1 or keeping the violation bag V t rather than v &lt; K ; R i &gt; updating the covering kno wledge C t when rules are added to R . See below.

In the iterativ e part (lines 11-31), eac h iteration selects the most unexp ected rule r i from remaining rules R and adds it to R i , until either k rules are selected or there is no rule to select. Also, we update the violation bag V t for a ected tuples t to accoun t for the new rule r i added to R The See-and-Kno w preference implies that if r i matc hes a currence of max ( V t ) in V t . This is done at line 16 with the detail omitted. The induced (negativ e) change v &lt; K ; R is propagated to U sup ( r ) for the remaining rules r suc h that t 2 SAT ( r ) at lines 17-19. At lines 20-21 we prune any re-maining rule r with U sup ( r ) &lt; 1 , and at lines 24-25 we These pruning optimizations follo w from the follo wing the-orem.

Theorem 6.1. (1) v &lt; K ; R i &gt; ( t ) is anti-monotone with re-resp ect to R i .
 Proof : By adding a rule r i to R i , the See-and-Kno w pref-erence prefers r i if v ( t; r i ) is smaller than max ( V fore, V 0 t V t , where V t is the violation bag before adding r to R i and V 0 t is the violation bag after adding r i to R So we have agg ( V 0 t ) agg ( V t ) for well-b eha ved agg . This (2) follo ws from this anti-monotonicit y and the de nition of U sup ( r; &lt; K ; R i &gt; ).

Tw o pruning strategies follo w from Theorem 6.1. First, if v &lt; K ; R i &gt; ( t ) = 0 for some iteration i , then v for all subsequen t iterations j because R j is a sup erset of R . Therefore, suc h tuples t can be pruned in subsequen t iterations without a ecting the result. This is done at lines 24-25. Second, for any remaining rule r , U sup ( r; &lt; K ; R R immediately . This is done at lines 20-21.

A note on implemen tation . The searc h of all tuples that matc h a given rule at line 15 requires the superset func-tion instead of the subset function. This can be done by rev ersing the roles of tuples and rules in the subset func-tion implemen tation discussed earlier, that is, storing tuples t , instead of rules, in a hash-tree and using the given rule to descend the hash-tree. The subset/sup erset function im-plemen tations and pruning rules and tuples at lines 20-21 and 24-25 mak e the algorithm much more ecien t than its worst-case complexit y. Typically , the selection algorithm is applied to the unexp ected rules R found by the algorithm in Section 5. Hence, the num ber of rules in R is much smaller than the num ber of rules found without pushing user kno wl-edge.
In this section, we evaluated UMINE and USELECT with two objectiv es. The rst is to evaluate the e ectiv eness of dealing with the computational bottlenec k by pushing the user kno wledge. The second objectiv e is to evaluate the e ectiv eness of nding interesting rules in terms of unex-pectedness. The exp erimen ts were conducted on the well kno wn KDD-CUP-98 dataset [1] about the result of the 1997 Paralyzed Veterans of America fund raising mailing campaign. We chose this dataset mainly for the consideration of obtaining the user kno wledge based on common sense in this applica-tion. The dataset con tains 191,778 tuples over 482 attributes about donors' demographic information and donation his-tory . We picked N K 97 as the target attribute, whic h rep-resen ts the donation amoun t (in dollars) in resp onse to the 1997 mailing campaign. We discretized the dollar amoun t of N K 97 into ve donation scales: c 0 = 0 ; c 1 = (0 ; 5] ; c are small). We picked up the follo wing 23 non-target at-tributes, as they seem to be related to the target attribute and their meanings are easier to understand than most of the other attributes.
 A 1 NOEX CH: whether it can be exc hanged for list ren tal A 2 RECINHSE: whether donor has given to PVA's in A 3 RECP3: whether donor has given to PVA's P3 program A 4 RECPGV G: whether it is a planned giving record A 5 RECSWEEP: whether it is a Sweepstak es record MDMA UD: RFA status for donors who have given a $100+ gift at any time in their giving history: A 6 Recency of Giving; A 7 Frequency of Giving; A 8 Amoun t of Giving; A 9 Blank/ meaningless/ ller DOMAIN: A 10 Urbanicit y level of donor's neigh borho od; A 11 Socio-Economic status of neigh borho od A 12 AGE: Overla y Age A 13 HOMEO WNR: Home Owner Flag A 14 NUMCHLD: Num ber of Children A 15 INCOME: Household Income A 16 GENDER: Gender A 17 MAJOR: Ma jor ($$) Donor Flag A 18 WEAL TH2: Wealth Rating A 19 Percen t Households with Income, replaced with the A 20 Percen t Families with Income, replaced with the AD ATE 2: RFA status as of 97NK promotion date: A 21 Recency based on date of the last gift; A 22 Frequency based on the perio d of recency; A 23 Amoun t of the last gift
We need to have some user kno wledge about how donors would resp ond (in donation) to the 1997 mailing campaign in terms of non-target attributes. This information is not available from the dataset, nor is it easily available else-where. Our approac h is to \sim ulate" the user kno wledge us-ing some rules found in the data, whic h is reasonable because in real applications often the user kno wledge does come from the analysis of the data. In particular, we sim ulated the fol-lowing user kno wledge.

The user kno wledge : People tend to remain unchange d in donation behaviors, that is, those who wer e active/inactive in the past are likely to remain so this time (i.e., for NK97) . This kno wledge is represen ted by the 4 rules R 1 to R 4 low that were extracted from the data. We have included the supp ort and con dence of these rules, whic h sho ws their strong existence in the data.
 For non-target attributes, we use the binary matc h degree measure m ( v; v 0 ): m ( v; v 0 ) = 1 if v = v 0 , or m ( v; v otherwise. For the target attribute, we use the distance based measure m ( c i ; c j ) = 1 j i j j = 4, i.e., the further away the two scales c i and c j , the less the matc h degree between them. We use the average avg for the aggregate in De nition 3.1, and consider the covering depth of 1.
UMINE was implemen ted based on an adoption of the depth-rst based BUC for mining frequen t itemsets [5]. We compared UMINE with the metho d of not using user kno wl-edge, denoted by UMINE(NULL), whic h we implemen ted as UMINE with only the default rule as the user kno wledge. UMINE-Pruned denotes UMINE exactly as describ ed in Al-gorithm 1, and UMINE-Unpruned denotes UMINE that merges the rule phase and the nal phase into one phase but scans the unpruned database.

Figure 1(a) and (b) sho ws the execution time versus the unexp ectedness supp ort threshold, for the minim um vio-lation preference and the maxim um con dence preference, resp ectiv ely. After the unexp ectedness supp ort threshold is reduced to 10%, we have to abort UMINE(NULL) be-cause the execution time is too long. UMINE-Pruned and UMINE-Unpruned, however, can go to a much lower thresh-old thanks to the focus on unexp ectedness supp ort. This indicates that, without pushing the user kno wledge, the threshold has to be set very high, in whic h case man y un-exp ected rules will not pass the threshold, therefore, will not be found by the post-analysis approac h. Figure 1(a') and (b') sho ws that the num ber of rules generated, whic h measures the searc h space explored, is much smaller after pushing the user kno wledge. This suggests that, as far as the unexp ected rule mining is concerned, man y useless rules are generated if the user kno wledge is not pushed. UMINE-Pruned is more ecien t than UMINE-Unpruned, due to the pre-pruning of up to 85% of the database. This exp erimen t clearly demonstrated the e ectiv eness of the prop osed meth-ods in focusing searc h e ort.

In the rest of the exp erimen t, the minim um violation pref-erence is used as the preference mo del, and the covering depth is 1.
Instead of listing all rules found, whic h are too man y, we examine a few top rules to con vey the idea about the e ec-tiveness. We consider eac h rule indep enden tly. The inter-action between rules will be considered in Subsection 7.4. We applied UMINE at the minim um unexp ectedness sup-port of 0.2% and found 11,722 rules. The 20 rules of highest U nexp are presen ted below. For eac h rule r presen ted, we also presen t the violation distribution R i = ( x; y ), where the kno wledge rule R i covers x tuples satisfying r , with y being the total violation by these tuples. 1. A 21 = Lapsing donor ! N K 97 = c 4 (Usup=0.38%; 2. A 1 = Can be exchang ed ! N K 97 = c 4 (Usup=0.38%; 3. A 4 = N ot planned giving ! N K 97 = c 4 (Usup = 4. A 17 = N ot maj or donor ! N K 97 = c 4 (Usup=0.38%; 5. A 5 = N ot sweepstak es ! N K 97 = c 4 (Usup = 0.38%; 6. A 9 = N ot maj or donor ! N K 97 = c 4 (Usup=0.38%; 7. A 3 = N ot P 3 ! N K 97 = c 4 (Usup=0.37%; Un-8. A 2 = N ot in house ! N K 97 = c 4 (Usup = 0.35%; 9. A 13 = Home owner ! N K 97 = c 4 (Usup=0.26%; 10. A 22 = One gif t in theper iod of recency ! N K 97 = 11. A 23 = $25 : 00 and abov e ! N K 97 = c 4 (Usup = 12. A 1 = Can be exchang ed; A 21 = Lapsing donor ! 13. A 4 = N ot planned giving ; A 21 = Lapsing donor ! 14. A 1 = Can be exchang ed; A 4 = N ot planned giving ! 15. A 9 = N ot maj or donor ; A 21 = Lapsing donor ! N K 97 = 16. A 9 = N ot maj or donor ; A 17 = N ot maj or donor ! 17. A 17 = N ot maj or donor ; A 21 = Lapsing donor ! 18. A 5 = N ot sweepstak es; A 21 = Lapsing donor ! N K 97 = 19. A 4 = N ot planned giving ; A 9 = N ot maj or donor ! 20. A 4 = N ot planned giving ; A 17 = N ot maj or donor !
Generally speaking, these rules are found because their bodies are either the same as or correlated with the bod-ies of R 1 or R 2 , but their heads say very di eren t things from the heads of R 1 or R 2 . For example, the body of rule No 3, A 4 = N ot planned giving , tends to be correlated with a non-ma jor donor (i.e., A 17 = N ot maj or donor ), and that is why the minim um violation preference (whic h acts on behalf of the user) chose R 1 to cover 723 tuples satisfy-ing this rule. Rule No 3 is unexp ected to the exten t that it summarizes man y tuples that violate the user preferred kno wledge on them. This example also illustrates that our metho d nds not only \directly unexp ected rules" that use attributes used by the user, suc h as rule No 1, but also \indi-rectly unexp ected rules" that use attributes correlated with those used by the user, suc h as rule No 3. Since correlation does not require syn tax similarit y, \indirectly unexp ected rules" cannot be found by the syn tax based [8, 9] or the logic based [12]. If the user kno ws these correlations, the unexp ectedness of \indirectly unexp ected rules" is immedi-ate to the user. Otherwise, \indirectly unexp ected rules" together with the violated kno wledge rules (i.e., R 1 here) pro vides new information to the user.

If we go further down the list, some rules violate two or more kno wledge rules. Here is an example: The subp opulation that satis es this rule violates two kno wl-edge rules, R 1 and R 2 . This is di eren t from previous ap-proac hes where the unexp ectedness is alw ays measured with resp ect to a single kno wledge rule.
In the above exp erimen t, man y rules were found because of some conditions correlated with the bodies of kno wledge rules. Suc h rules are related in the sense of capturing the same or similar violating subp opulation, i.e., U sup ( r ) com-ing from the same or similar violating subp opulation. We can treat all suc h rules as one cluster and presen t only one represen tativ e of the cluster to the user. The other rules in a cluster can still be computed, but will not be presen ted to the user, unless the user requests them. This strategy is implemen ted in the selection algorithm USELECT by prun-ing all satisfying tuples once a data rule is selected. This pruning is also a consequen t optimization of the See-and-Kno w preference dealing with the unexp ectedness dynamics, as sho wn in Section 6.

To evaluate the e ectiv eness of the selection algorithm, we applied USELECT to the 11,722 rules found in Subsection 7.3 to select 5 most unexp ected rules, with f ( U sup; U str ) = U nexp as the selection criterion. The follo wing 5 rules 1 are found in that order (recall that the minim um unexp ected-ness supp ort is 0.2%):
As discussed in Section 6, eac h time a data rule is selected, it is made a new kno wledge denoted R i (5 i 9) in the subsequen t selection to prev ent similar rules from being se-lected again. As we can see, the above 5 rules selected this 1 Despite their syn tactic similarit y, rules R 6 and R 7 rize disjoin t subp opulations in the data! way pro vide more insigh ts than simply rep orting the top 5 unexp ected rules in Subsection 7.3. For example, from R 8 and R 9 , the user learns unexp ected rules about star and ac-tive donors not donating this time. Man y of the top 20 rules in Subsection 7.3 are not in this list, therefore, not presen ted to the user (who only wants to see 5 rules in our exam-ple!). However, as men tioned above, eac h presen ted rule R (5 i 9) in fact represen ts a cluster of rules whose U sup mainly comes from the satisfying subp opulation of R i and whic h are not presen ted because of the pruning of this sub-population. USELECT can be mo di ed without dicult y to compute this cluster for eac h selected rule. Therefore, the user still has the choice of kno wing the rules not presen ted if she wishes to do so.

In summary , our evaluation indicated that the prop osed metho ds sho w a promise in addressing the three issues raised in the introduction. Ultimately , whether a rule is interesting is up to the end user to judge. This is because often the user kno wledge speci ed is incomplete and certain kno wledge is not easy to mo del. It would be unrealistic to exp ect every rule found to be a surprise to the user. A more realistic goal of our work is to tak e into accoun t the user kno wledge that can be mo deled so that a large num ber of obvious rules can be pruned. However, the user still needs to examine the surviving (but much few er) rules to iden tify interesting ones.
The user kno wledge plays a crucial role in determining what is interesting to the user. However, most data mining algorithms do not consider suc h kno wledge and merely let the user choose some parameters and thresholds required by the algorithms. The consequences are that: (1) a large num ber of rules are found, while a human user can only comprehend a small num ber; (2) man y unin terested rules are found, but man y interesting rules are not. These conse-quences are ma jor obstacles to today's data mining applica-tions. In this pap er, we addressed these issues by examining the fundamen tal question of how the user kno wledge should be represen ted and mo deled in mining unexp ected rules. An additional novel con tribution is that we recognized the hu-man user as a dynamic entity in applying her kno wledge to have a say in what is most unexp ected to her. Mo deling these dynamics can better help capture the intuition that what is unexp ected can change as the user starts seeing dis-covered rules. Another con tribution is a general framew ork for pushing the user kno wledge into the searc h pro cess. Our exp erimen ts supp ort our claim that the prop osed approac h is not only ecien t in mining rules, but it is also e ectiv e in terms of the qualit y (i.e., unexp ectedness) of the rules pro duced. [1] Kdd-cup-98 dataset. [2] R. Agra wal and R. Srik ant. Fast algorithm for mining [3] R. Bayardo and R. Agra wal. Mining the most [4] R. Bayardo, R. Agra wal, and D. Gunopulos.
 [5] K. Bey er and R. Ramakrishnan. Bottom-up [6] S. Brin, R. Mot wani, and C. Silv erstein. Bey ond [7] M. Klemettinen, H. Mannila, P. Ronk ainen, [8] B. Liu and W. Hsu. Post-analysis of learned rules. In [9] B. Liu, W. Hsu, and S. Chen. Using general [10] R. Ng, L. V. Lakshmanan, J. Han, and A. Pang. [11] B. Padmanabhan and A. Tuzhilin. A belief-drievn [12] B. Padmanabhan and A. Tuzhilin. Unexp ectedness as [13] B. Padmanabhan and A. Tuzhilin. Small is beautiful: [14] G. Piatesky-Shapiro. Disco very, analysis and [15] G. Piatesky-Shapiro and C. Matheus. The [16] S. Sahar. Interestingnes via what is not interesting. In [17] A. Silb ersc hatz and A. Tuzhilin. On sub jectiv e [18] A. Silb ersc hatz and A. Tuzhilin. What mak es patterns [19] R. Srik ant, Q. Vu, and R. Agra wal. Mining asso ciation [20] P.-N. Tan and V. Kumar. Interestingness measures for [21] K. Wang, Y. He, and J. Han. Pushing supp ort [22] K. Wang, Y. Jiang, J. Yu, G. Dong, and J. Han. [23] M. Zaki. Generating non-redundan t asso ciation rules.
