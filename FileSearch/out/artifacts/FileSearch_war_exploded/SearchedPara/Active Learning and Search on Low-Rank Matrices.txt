 Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scien-tific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collabora-tive prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to estimate the posterior distribution over models, we can choose query points to maximize our un-derstanding of the model, to best predict unknown elements of the data matrix, or to find as many  X  X ositive X  data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.
 H.2.8 [ Database Management ]: Database Applications X  data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information filtering ; I.2.6 [ Artificial Intelligence ]: Learning Collaborative filtering; active learning; active search; cold-start; matrix factorization; recommender systems; drug dis-covery
Collaborative prediction and collaborative filtering have in recent years been a topic of great interest, largely be-cause they form the core component of many corporations X  systems that recommend products or other items to their users. One of the most popular techniques for collabora-tive filtering is matrix factorization: since it is assumed that only a few factors affect a user X  X  opinion of a movie, the ma-trix of users X  ratings for items should be low-rank (or have a low trace norm, or any of other similar conditions). We can then perform a factorization similar to that of singular value decomposition to reconstruct the full matrix from the relatively few elements we know [20].

The same general approach, however, is applicable to a wide variety of problems, including tasks in computer vision [4, 32], network latency prediction [21], predicting the out-comes of sporting events [1], and many others. It can be applied in any situation where we expect  X  X sers X  to behave similarly on  X  X tems X , whether the  X  X sers X  are professional basketball teams X  offenses and the  X  X tems X  are their defenses (c.f. [1]), or the  X  X sers X  are drugs and the  X  X tems X  are bio-logical targets. Traditional collaborative filtering includes no side information about the content items, but there are various methods for adding this information [1, 7, 30, 32].
Most research in this area has focused on how well users X  ratings may be predicted given a fixed training set. That was the only criterion considered for the well-known Net-flix Prize, 1 for example. In many areas of machine learning, however, the problem of active learning is also important: how well an algorithm can select points to add to the train-ing set that will lead to the best final result. We can ask the same question of matrix factorization methods as well [22]: if we do not know all the elements of a matrix, but we are allowed to query the labels of certain points in the matrix, then which points should we choose to gain the best understanding of the full matrix? To approach this task, we must define a selection criterion in addition to the learning model that will attempt to bring the learner to the greatest understanding as quickly as possible.

One practical situation in which this is particularly impor-tant is the  X  X ew user X  (or  X  X old start X ) problem for recom-mender systems: such systems must quickly learn a rough sense of a new user X  X  preferences based on little available information before users abandon the system. This cold start problem has seen a fair amount of research, but is far from the only collaborative filtering application which ben-efits from active learning. In the product recommendation domain, it is also often the case that companies add items to their system in fairly large  X  X atches X , at which point few or no user recommendations will be available. The problem of learning the attributes of new products is different than the task of recommending items to a new user, both because the products come in batches and because a product will not become frustrated if it is not immediately recommended to a variety of viewers.

In another application area entirely, pharmaceutical com-panies and researchers wish to discover which of various http://www.netflixprize.com/ candidate drugs will interact with many different biologi-c al targets. Since drugs X  behavior typically has similarities to that of other drugs, and targets are acted on in simi-lar fashion to other targets, collaborative filtering (perhaps with additional side information based on biologically rele-vant features) is likely to perform fairly well at predicting drug interactions. Determining whether an interaction oc-curs, however, is an expensive procedure that requires per-forming experiments in the lab; since it is impossible to test all possible actions, the researcher must choose a subset to examine. The active learning paradigm described here can assist in choosing the subset to examine [17].

In these and many other applications, accurate predic-tions are not the goal of the system, but rather simply a means to an end. In recommender systems, we ultimately want to suggest items that a user will like , not just build an accurate model of their preferences. In the drug-target sce-nario, we care more about finding new drugs that interact with a certain target, or finding the targets a drug affects, than we do about listing all the targets with which a given drug does not interact. Here our ultimate goal is not to actively predict all the unknown elements of the data ma-trix, but instead to find the largest unknown elements in the matrix. This problem adds a layer of the exploration-exploitation trade-off not present in the active learning for prediction error task. It can, however, also be effectively ap-proached through the same framework; we simply need to define different selection criteria.

The main components of this work are:
There has been a significant amount of prior research on various methods for low-rank matrix factorization. These methods play an important role in numerous machine learn-ing and statistical tools, including principal component anal-ysis, factor analysis, independent component analysis, dic-tionary learning, and collaborative filtering, just to name a few. One of the most influential recent models, which we will employ in this paper, is the Probabilistic Matrix Factoriza-tion (PMF) method [24], as well as its Markov chain Monte Carlo (MCMC) extension (Bayesian PMF, or BPMF) [23]. PMF, which will be reviewed in Section 4, is a generative model for matrices assumed to be of a certain rank. Earlier work by [27] yielded the Maximum Margin Matrix Factorization (MMMF) model, which frames the matrix fac-torization problem as a semidefinite program based on the margin of predictions, and can be viewed as a generalization of support vector machines (SVMs). MMMF minimizes the trace norm of the factorization, which is a convex surrogate for the rank. Although the standard model predicts binary class labels, it can be modified for ordinal labels.
Active learning for recommender systems and collabora-tive filtering in general has also received a fair amount of attention. Rubens et al. [22] provide an overview of how general-purpose active learning techniques may be applied to recommendation systems. Much of the published re-search on this topic has focused on the Aspect Model [9], which assigns latent  X  X spect X  variables to users and items. In this model, Yu et al. [31] select query points by consider-ing the expected reduction in entropy in the model distribu-tion. Boutilier et al. [2] instead seek out the item which will bring the greatest change in value to the highest ratings. Jin and Si [11] note that estimation based on the belief about a given rating under the currently most likely model can be misleading when that point estimate of the model is not very good, and give a full Bayesian treatment, which uses a posterior distribution on model states for inference. Karimi et al. [12], by contrast, give a much faster selection criterion based on considering which points will update the current user X  X  parameters, under certain assumptions about the new user case for recommender systems.

There is less work on active learning specifically for ma-trix factorization. Karimi et al. [13] give an approach for the new user case which uses an exploration step, where the al-gorithm queries the item with the highest expected change to the user at hand X  X  model, followed by an exploitation step, where the algorithm picks items based on the current parameters. Karimi et al. [14] give a method they describe as a step towards the  X  X ptimal X  strategy based on minimiz-ing the expected test error, but which makes several drastic approximations for the sake of speed. The same authors more recently developed a method which queries a new user with items popular among users with similar latent factors, to avoid the problem of asking about an item unknown to the user [15]. All three of these criteria are extensively tai-lored to the new user case and inapplicable in general matrix factorization settings.

Rish and Tesauro [21] use MMMF to carry out active learning for general matrix factorization problems. Follow-ing work by Tong and Koller [29] and others on active learn-ing for SVMs, they choose to query the candidate point that has the smallest margin, representing the point about which the model is least certain. This criterion has the advantage of being simple to compute once the model has been learned. Their work considers only two-class problems, though it could potentially be extended to multi-class problems by choosing the point nearest to any label threshold. They also consider only active learning with the goal of minimizing re-construction error, but a very similar algorithm applies to the case of finding positive instances.

Silva and Carin [26] approach the problem of active learn-ing in a general matrix factorization problem with a similar learning model to PMF, but using a different variational approach than those discussed in Section 4.1. Whereas we assume a variational distribution of a Gaussian form allow-ing for general covariance structures, they assume a fully factored distribution with respect to each model parameter. This assumption allows for much more efficient learning pro-cedures than discussed here, but also represents a far more stringent restriction on the model. This work as well con-siders only the goal of minimizing reconstruction error and is not directly applicable to finding positive values.
The general problem of active learning to find values in a class is termed active search by Garnett et al. [5], who develop a strategy optimal in the sense of Bayesian decision theory. This strategy, however, requires computation expo-nential in the number of lookahead steps, and is therefore i mpractical to apply with more than a very small lookahead window. Later work [6] provides a branch-and-bound ap-proach for pruning the search tree, which is effective in their domain of searching on graphs with nearest-neighbor classi-fiers, but inapplicable in the matrix factorization setting.
We suppose that our data lie in a matrix R  X  R N  X  M , only certain elements of which are initially known. The binary matrix I of the same shape as R represents the known points, so that I ij is 1 if R ij is observed and 0 otherwise. The set of ( i, j ) indexes where I ij = 1 will be denoted by O ; we will use R
O to represent the set of R ij with ( i, j )  X  O . Some of the labels for elements ( i, j ) not in O may be requested; we call this pool of available labels P . Our algorithm will proceed by building a model for R and using that model to select a query point in P ; it then receives the value for that point, updates the model to account for the new information, and evaluation repeats. We call the set of query points chosen by the algorithm over its execution A .

The way in which we select elements depends on our aim in the active learning process. We consider four possible goals in this work: Prediction: minimize prediction error on the unknown val-Model: minimize uncertainty in the distribution of models Magnitude Search: when the active search process is com-Search: when the active search process is complete, have The Prediction and Model goals are closely related, as are the Magnitude Search and Search goals. These goals cover a variety of use cases, although in some settings we might prefer others, such as the portion of the top k rat-ings that are positive [30], or the recall or precision of our predictions when viewed as a binary classifier. The basic modeling framework we will adopt here is the PMF model of [24]. This matrix factorization technique assumes that R  X  U V T for some U  X  R N  X  D , V  X  R M  X  D where the rank D is a hyperparameter of the model. In the setting of movie rating predictions, the i th row of U , denoted u , is the feature vector for the i th user. The j th row of V , denoted v j , is the feature vector for the j th movie. User i  X  X  rating for movie j is then predicted as u T i v j .
Specifically, PMF assumes i.i.d. Gaussian noise around the prediction U V T , so that R ij = u T i v j +  X  ij where  X  N (0 ,  X  2 ). It further regularizes the parameters U and V via zero-mean spherical Gaussian priors with variances  X  2 U and  X  , respectively. For constant hyperparameters  X  2 ,  X  2 U , and  X  , the joint log-density ln p ( U, V | R O ) then becomes where  X  denotes the elementwise (Hadamard) product, k X k 2 the squared Frobenius norm, and C a constant that does not depend on U or V . To obtain the MAP estimates b U and b V , we maximize (1) in U and V , e.g. through gradient ascent.
It is worth noting that (1) is biconvex in U and V , and is highly multimodal: for any invertible matrix  X   X  R D  X  D with k U  X  k F = k U k F , k  X   X  1 V k F = k V k F , we have p ( U, V | R O ) = p ( U  X  , V  X   X  1 | R O ), since ( U  X ) V  X   X  1 (Any  X  which simply permutes the order of the latent dimen-sions satisfies this property.) In practice gradient descent and similar optimizations will typically choose one such local maximum and stay in its vicinity as we update the problem with new ratings. This does, however, somewhat complicate the interpretation of our Model learning goal.

Unfortunately, (1) lends itself only to MAP estimation; the full joint posterior distribution is intractable for direct inference. In order to carry out active learning, we need some more information about the posterior p ( U, V | R O ), in particular statistics such as its variance or its Shannon en-tropy. We will therefore need to add some more information about the posterior to our model.
One method for making inferences about the joint distri-bution is to make a deterministic, variational approxima-tion. In this approach, we model the joint distribution of all the elements of U and V as a distribution q from some tractable family of distributions, so that the KL divergence of our approximation q ( U, V ) from the modeled distribution conditional on the observed elements, p ( U, V | R O ) in (1), is KL( q k p ) = where H[ q ] =  X  sity q , E q stands for the expectation operator w.r.t. distri-bution q , and C is the constant of (1), independent of q . We then choose the  X  X est X  approximation q by minimizing (2).
One option is to select q ( U, V ) from the family of mul-tivariate normal distributions, with an arbitrary mean  X   X  R We then have a closed-form expression for each of the ex-pectations in (2), via Isserlis X  Theorem [10], whose gradient is simple; the details are in the supplement. 2 This allows cs.cmu.edu/~dsutherl/active-pmf/ us to minimize (2) in  X  a nd  X  through projected gradient descent, projecting the covariance matrix  X  to be strictly positive-definite at each step (by replacing any nonpositive eigenvalues in its spectrum with a small positive eigenvalue; this unfortunately takes time cubic in the dimension).
 X , however, is of dimension D 2 ( N + M ) 2 , which can quickly become impractically large for moderately-sized matrices R . We can ease this requirement by assuming a more restric-tive form on the distribution of ( U, V ), for instance a ma-trix normal distribution on the  X  X tacked X  matrix of U and V . This distribution is parameterized by a mean matrix  X   X  R ( N + M )  X  D , a symmetric positive-definite row covari-definite column covariance matrix  X   X  R D  X  D . It is equiv-alent to a general multivariate normal distribution with co-variance equal to the Kronecker product  X   X   X . This more restrictive distribution, while still having a fairly large num-ber of parameters, is easier to handle, and as a subset of the full multivariate normal distribution has a similar closed form for (2) and its gradient.

In some sense, these are clearly bad models for p ( U, V ), since the q are unimodal while the p have many equiva-lent modes, at least D !, and many more local maxima. If we choose a distribution centered around one of the global modes, however, we may still get useful inferences out.
Note that previous variational approximations to PMF, such as  X  X arametric PMF X  [25], have different goals: they use EM methods to obtain a point estimate and do not ac-tually give more information about the posterior.
Another approach for learning the posterior distribution of U and V is to sample them through Markov chain Monte Carlo, as in  X  X ayesian PMF X  [23]. In this way, we know that at least asymptotically we are sampling from the full joint distribution of the original model, rather than the quite restrictive variational approximation. BPMF extends the Gaussian priors for u i and v j to allow any mean  X  and co-variance  X , and places Gaussian-Wishart hyperpriors on  X  and  X . Specifically, this version of the model has R ij  X  N ( u T i v j ,  X  2 ); u i  X  N (  X  U ,  X  U ); v i  X  N (  X  N (  X  0 , 1  X 
Salakhutdinov and Mnih [23] initialized the chain with the b U , b V estimates from the MAP procedure (1) and then sampled from the posterior of U and V through Gibbs sam-pling, which is simple thanks to the use of conjugate priors. We choose a somewhat different sampling scheme via Hamil-tonian Monte Carlo, which can exploit the gradient of the probability density to allow for much more efficient conver-gence to a high-dimensional target distribution [18]. This variant of MCMC simulates the motion of fictional particles with positions  X  in the parameter space, potential energies defined by the log probability, momentum r . We numerically simulate their behavior according to Hamilton dynamics: w here mass matrix M (which primarily allows for rescaling of variables), and r is initially drawn from a standard normal distribution. We perform Metropolis rejection based on the total change in the Hamiltonian due to integration error.
The step size and the number of steps in the numerical in-tegration must be tuned to the distribution at hand for good performance. The No-U-Turn Sampler (NUTS) [8], however, provides a method to choose the step size via dual averaging and the number of steps by stopping when the particle would begin to  X  X urn around, X  resulting in wasted computation. Our implementation uses the Stan inference engine [28]. Es-pecially after appropriate reparameterizations to make the geometry of the space more uniform (sampling from stan-dardized versions of the distributions and then making ap-propriate transformations to obtain the true parameters), this sampler explores the parameter space more efficiently than Gibbs sampling, allowing us to obtain a good under-standing of the posteriors much more quickly.
Once we have learned a suitable variational model or ob-tained samples from an MCMC procedure, we have several options for how we select points to query.

One simple option for the Prediction goal is to query the element ( i, j )  X  P with the highest posterior variance: arg max ( i,j )  X  X  Var[ R ij | R O ]. In the variational model, al-though the distribution of R ij (the sum of products of corre-lated normal distributions) is not a known distribution, we can compute its mean and variance under q using the same types of identities as in (2); details are in the supplement.
In MCMC, we use the sample variance. Although it would also be possible to estimate the differential entropy of R with one-dimensional sample entropy methods, we found in practice that the marginal posterior distributions of R ij typically close to Gaussian. Entropy methods would thus in-cur significant additional computational expense with little to no added information over the variance.

For the Magnitude Search task, the simplest approach is to choose the value with the largest prediction. That is, we select arg max ( i,j )  X  X  E [ R ij ], where the expectation is either under the variational distribution or approximated by the sample mean. This approach could also be used with a point estimate of U and V .

In the Search task, we instead partition the real line into  X  X ositive X  and  X  X egative X  classes, which we denote as sets + and  X  . For example, in the movie rating task we might choose 4 or 5 to be positive, and 1 through 3 to be negative, so that + = { x  X  R | x  X  3 . 5 } (the set of reals which round to the positive class). We would then choose arg max ( i,j )  X  X  P ( R ij  X  + | R O ). This is the optimal no-lookahead algorithm for active search in the framework of [5]. In MCMC, we approximate this probability by the portion of samples where R ij  X  +; in the variational setting, we cannot compute this probability in closed form but instead choose to approximate it via a normal distribution with first two moments matched to those of p ( R ij | R O ).
We can also take a greedy lookahead approach, where we define some measure f ( q ) of the quality of our model and choose the element ( i , j ) which maximizes (or minimizes) E [ f ( q ) | R O X  ( i,j ) ]. We will present only the one-step version here for simplicity; the extension to multiple steps of looka-head is straightforward, though its computational expense grows exponentially.

When the ratings in R are from a small, discrete set X (e.g. X = { 1 , 2 , 3 , 4 , 5 } in the movie ratings setting; this is true in all of the cases considered here except that of Section 6.1.1), we can compute this expectation by fitting the model for each possible value for R ij , computing f for each such fit model, and taking their mean weighted by our belief about the probability of R ij taking on that value: P
If the rating values are continuous or there are too many of them, we exploit the previously-mentioned fact that the marginal distributions of R ij are approximately normal. We estimate P ( R ij  X  x ) by a normal distribution and integrate with the trapezoid rule. We take 25 values of a evenly spaced between 0.001 and 0.999, and break up the integral over f at each a th quantile of the normal distribution.
In the variational setting, we choose to take the prob-ability P ( R ij ) by moment-matching a normal distribution to the variational approximation q  X  X  belief about R ij . It would also be possible to use the MAP model, in which R ij  X  N ( u T i v j ,  X  2 ), but our experiments suggested that q  X  X  belief performed slightly better.

In MCMC, with discrete output ratings, we estimate P ( R ij by the MAP fit of a categorical distribution with a Dirichlet prior, where the prior is used to smooth out any probabili-ties that would otherwise be zero. (This prior could be com-puted based on the observed or expected distributions of the ratings for the full matrix; we simply use a flat prior where each rating obtains a pseudocount of 0.1.) For continuous outputs, we use the MLE.

Possible functions f include: In our setting, unfortunately, the sheer number of queryable points makes lookahead methods extremely expensive. On a reasonably large problem, computing even one-step looka-head for each queryable point is infeasible. Practical im-plementations therefore need to subsample the queryable points, perhaps evaluating only points that an easier-to-compute heuristic finds most promising. Due to this ex-pense, we evaluate the lookahead methods only on small synthetic datasets in this work.
We will now present empirical evaluations of our active learning approaches on synthetic datasets, movie ratings, and drug discovery. We compare to the minimum-margin selection approach of [21] when possible; we do not compare to the work of [26] because there is no publicly available im-plementation. Lookahead criteria are evaluated only for the synthetic matrices of Section 6.1, due to their computational expense. The code and data used in these experiments are available from the supplement website.

For the regularization parameters of the variational ap-proach, we used  X  2 = 1 and  X  2 U =  X  2 V = 10  X  2 . We found that when the number of observed elements is small, choos-ing parameters to maximize the likelihood (1) as suggested by [24] resulted in values that were far too small, even after adding a fairly strong log-normal hyperprior.

In MCMC sampling, we used the same hyperparameters as in [23]:  X  = 1 2 ,  X  0 = 0,  X  0 = 2, W 0 = I , and  X  0 All of the experiments presented here used a warmup of 100 samples to for NUTS adaptation and to approach a lo-cal maximum, followed by inference based on 200 samples. When computing lookaheads, we use a warmup of 50 fol-lowed by 100 samples.

We typically learn on a centered version of the data ma-trix, so that we can assume that U and V have mean 0. This helps make the priors more sensible and allows for easy initialization.

In both approaches, we initialized the parameters at ran-dom elements near the origin (for means) or the identity matrix (for covariances). After learning the label of a query point, we initialized optimization or sampling for the next step at the parameters obtained by the previous one. (In MCMC, we initialized at the sample from the previous step with the highest probability density.) For the comparisons to MMMF, we used code from Nathan Srebro 3 which employs an SDP solver; we used regulariza-tion parameter C = 1 throughout. For higher-dimensional problems the SDP solver became quite slow; the direct for-mulation of [19] would probably be preferable. ttic.uchicago.edu/~nati/mmmf/ method outperformed random selection.
We will first evaluate our methods on small synthetic problems to see some characteristics of their performance.
To motivate our selection criteria, we first consider a sim-ple problem where the value of selecting points is known. Let us reconstruct a rank 1 matrix R  X  R 10  X  10 , where the off-diagonal elements and all but one element of the bottom row have already been observed (the white squares in Fig-ure 1). The bottom-left 9  X  9 square is thus constrained perfectly, as the diagonal establishes the factor by which the bottom row must be multiplied. The rightmost column and top row are unknown, but learning any entry there will give us enough information to know the full matrix. Thus, picking an element in the bottom-left 9  X  9 square provides no information, while picking any element in the rightmost column or top row allows us to know the entire matrix per-fectly.

Figure 1 shows our evaluation criteria on one such matrix, generated by sampling 10  X  1 U and V from a normal dis-tribution with mean 10 and standard deviation 2. 4 Colors represent the value of the criterion at hand; the square with the black x is the best choice according to that criterion. We can see that the MCMC lookahead method of Figure 1a per-forms quite well, with a clear separation between the good and the bad choices. The method based on sample vari-ance (Figure 1b) also performs well: all the bad choices are evaluated as bad, though the margin between good and bad points is much narrower. The variational criteria, by con-trast, both seem essentially random; each one picks a useless point, indicating that the approximation is ineffective here.
Wi th small matrix sizes, the biconvexity of (2) often causes problems in gradient descent when the factors are zero mean. If all the known elements of a row and column are close to zero, there will be an asymptotic non-global maximum with some of the factors X  signs flipped. This does not occur when the factor means are far from zero. On the other hand, the MCMC algorithm does poorly if started too far away from a local mean. In practical situations, normalizing the ratings to be zero mean is sufficient, but that makes this matrix no longer rank 1; we instead initialize the sampling at the MAP estimate from PMF and set  X  0 = 10.
We now turn to a slightly more realistic example: 10  X  10 matrices with integer values in the range 1 to 5, approxi-mately of rank 4. 5 Figure 2 shows the mean advantage (in terms of RMSE) of each method compared to random se-lection over the course of the full evaluation. That is, we draw the curve where the horizontal axis is the number of points queried and the vertical axis the RMSE for selection with the given criterion and random selection; Figure 2 then shows the difference between the area under each curve.
MCMC methods and variational approaches that do looka-head based on the MAP belief about rating distributions seem to all do somewhat better than random. In MCMC methods, criteria related to the Search goals tend to hurt, while uncertainty sampling clearly helps and the lookahead methods help somewhat. Variational methods fare more or less similarly, though with a wider spread. In this case, MMMF active learning does not appear useful, though it is worth noting that even random MMMF outperforms the best of the PMF-based methods here. Figure 3 shows the same analysis for the Search criterion, treating 4 or 5 as positive and 1 through 3 as negative; here we see that the MCMC Search methods seem to help, the variational meth-ods may as well but less consistently, and the MMMF max margin positive method helps only a little.
The Movielens-100k dataset consists of 100,000 ratings of 1682 movies by 943 users of movielens.org . Ratings range from 1 (worst) to 5 (best). We ran on a subset consisting of the 50% of users with the most ratings and enough of their most-rated movies to cover 70% of their ratings, which resulted in a set of 472 users and 413 movies. There are
T hese matrices are constructed by choosing a random ma-trix with values 1-5, reconstructing based on the first four singular values, and then rounding to be 1-5 valued. 58,271 ratings in this subset, so that just under 30% of the matrix is known, as opposed to the full dataset where only 6% of the ratings are known.

In our experiments, we started from a X  X ear-scratch X  X earn-ing state where 5% of the ratings are known. The subset of known entries is chosen randomly in such a way that at least one entry is known in each row and each column. We selected a test set of another 5% of the known ratings uni-formly from the unknown ratings, and then ran our MCMC learning algorithms for 200 steps, allowing the model to up-date its parameters and choose any element not in either the known or test sets at each step. 200 steps is insufficient to see any improvement in the RMSE on this larger model, but Figure 4 shows the number of positives selected as the algorithm proceeds. (Error bars are not shown for clarity of presentation, but each individual run looked similar.) Figure 4: Mean numbers of positive elements selected in fi ve independent runs on the 58,000-rating Movielens subset, with a rank-15 model.
As mentioned previously, collaborative prediction algo-rithms are applicable to a large number of domains outside those of recommender systems. One such possibility is the task of predicting interactions between drugs and various targets for those drugs, including diseases, genes, proteins, and organisms. The DrugBank dataset [16] is a comprehen-sive source of this information, containing information on over 6,000 drugs and 4,000 targets. We extracted only the presence or absence of interactions into a matrix with drugs as rows and targets as columns. Only positive interactions are present in the database, consisting of about one in 2,000 possible pairs. We therefore assumed that all interactions not listed in the database truly do not occur.

We used a subset of this matrix containing 94 drugs and 425 targets, such that each drug had at least one interaction with a present target (maximum 59, median 16) and each target had interactions with multiple drugs (most had 2 or 3; some had as many as 22). This matrix contains 1,521 interactions and 38,429 non-interactions.

We chose an initial training set containing exactly one in-teraction for each drug, and 406 negatives selected such that each target had at least one initially known point. We chose a test set of 500 positives and 1,000 negatives uniformly from the remaining data, and as before ran the learning process for 200 steps. We used a model of rank 20 and did five in-dependent runs (which used the same 94  X  425 data subset but different training and test sets).

Because of the binary nature of the problem and the skewed test distribution, we evaluate not on RMSE but on the area under the ROC curve of binary classifier defined by the pre-dictions (on the test set). Figure 5a shows the mean of these AUCs over the learning process for various MCMC selection criteria and for the MMMF criterion. We can see that all three of our active learning criteria strongly help boost the ROC curve of the predictions in the MCMC setting, while the assistance due to the MMMF active learning approach of [27] is small if present at all. In this case, where positives are quite rare (around 2% of the points available to query), it seems that discovering an element is positive is likely to con-vey much more information than finding an element is neg-ative, so it is unsurprising that our Search -oriented heuris-tics outperformed uncertainty sampling in terms of perfor-mance. It is also worth noting that the baseline performance of the MCMC approach (e.g. with random selection) is sub-stantially superior to that of MMMF.

Figure 5b shows the effectiveness of various criteria for finding positives in the data. We see that the MCMC-based criteria far outstrip the MMMF-based ones in their rate of finding positives, though the max-margin positive criterion is better than random.
We gave approaches for active learning and active search in the PMF framework with four goals ( Prediction , Model , Magnitude Search , and Search ). We examined these criteria on synthetic examples, and then showed the effec-tiveness of the non-lookahead versions on two real-world datasets. On the important problem of understanding and seeking out interactions in the drug discovery process, our methods greatly outperformed the MMMF-based methods in both Prediction and Search .

We found that variational approaches based on a matrix-normal factorization of the posterior were both computa-tionally expensive and did not perform especially well. It seems that the MCMC approaches considered here, or the fullly-factorized variational approach of [26], are superior. Many potential enhancements to this model are possible. Perhaps most important is a method for choosing elements to examine in looakahead criteria. It is also worth not-ing that our methods may be applied almost unchanged to models which incorporate side information into matrix fac-torization through Gaussian Process priors (e.g. [1, 7, 32]). Combining the power of collaborative filtering with that of feature-based methods might yield an effective method for guiding experimental processes such as seeking out drug-target interactions or protein-protein interactions. [ 1] R. P. Adams, G. E. Dahl, and I. Murray.
 [2] C. Boutilier, R. S. Zemel, and B. Marlin. Active [3] P. Dutilleul. The MLE algorithm for the matrix [4] A. Eriksson and A. Van Den Hengel. Efficient [5] R. Garnett, Y. Krishnamurthy, D. Wang, J. Schneider, [6] R. Garnett, Y. Krishnamurthy, X. Xiong, [7] M. G  X  onen, S. A. Khan, and S. Kaski. Kernelized [8] M. D. Hoffman and A. Gelman. The no-U-turn [9] T. Hofmann and J. Puzicha. Latent class models for [10] L. Isserlis. On a formula for the product-moment [11] R. Jin and L. Si. A Bayesian approach toward active [12] R. Karimi, C. Freudenthaler, A. Nanopoulos, and [13] R. Karimi, C. Freudenthaler, A. Nanopoulos, and [14] R. Karimi, C. Freudenthaler, A. Nanopoulos, and [15] R. Karimi, C. Freudenthaler, A. Nanopoulos, and [16] C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, [17] R. F. Murphy. An active role for machine learning in [18] R. M. Neal. MCMC using Hamiltonian dynamics. In [19] J. Rennie and N. Srebro. Fast maximum margin [20] F. Ricci, L. Rokach, B. Shapira, and P. Kantor. [21] I. Rish and G. Tesauro. Active collaborative prediction [22] N. Rubens, D. Kaplan, and M. Sugiyama. Active [23] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [24] R. Salakhutdinov and A. Mnih. Probabilistic matrix [25] H. Shan and A. Banerjee. Generalized probabilistic [26] J. Silva and L. Carin. Active learning for online [27] N. Srebro, J. Rennie, and T. Jaakkola.
 [28] Stan Development Team. Stan: A C++ library for [29] S. Tong and D. Koller. Support vector machine active [30] X. Yang, H. Steck, Y. Guo, and Y. Liu. On top-k [31] K. Yu, A. Schwaighofer, and V. Tresp. Collaborative [32] T. Zhou, H. Shan, A. Banerjee, and G. Sapiro. This work was funded in part by the National Science Foundation under grant NSF-IIS0911032 and the Depart-ment of Energy under grant DESC0002607.
