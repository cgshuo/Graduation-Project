 Sequential decision-making problems under uncertainty and partial observability are typically mod-eled using Partially Observable Markov Decision Processes (POMDPs) [1], where the objective is to decide how to act so that the sequence of visited states optimizes some performance criterion. However, this formalism is not expressive enough to model problems with any kind of objective functions.
 Let us consider active sensing problems, where the objective is to act so as to acquire knowledge about certain state variables. Medical diagnosis for example is about asking the good questions and performing the appropriate exams so as to diagnose a patient at a low cost and with high certainty. This can be formalized as a POMDP by rewarding X  X f successful X  X  final action consisting in ex-pressing the diagnoser X  X   X  X est guess X . Actually, a large body of work formalizes active sensing with POMDPs [2, 3, 4].
 An issue is that, in some problems, the objective needs to be directly expressed in terms of the cases, POMDPs are not appropriate because the reward function depends on the state and the action, not on the knowledge of the agent. Instead, we need a model where the instant reward depends on the current belief state . The belief MDP formalism provides the needed expressiveness for these problems. Yet, there is not much research on specific algorithms to solve them, so they are usually forced to fit in the POMDP framework, which means changing the original problem definition. One sequential-decision making problem with partial observability must always be modeled as a normal POMDP. However, in a number of cases the problem designer has decided to separate the task of looking for information from that of exploiting information. Let us mention two examples: (i) the surveillance [5] and (ii) the exploration [2] of a given area, in both cases when one does not know what to expect from these tasks X  X nd thus how to react to the discoveries.
 After reviewing some background knowledge on POMDPs in Section 2, Section 3 introduces  X  POMDPs X  X n extension of POMDPs where the reward is a (typically convex) function of the belief state X  X nd proves that the convexity of the value function is preserved. Then we show how classical solving algorithms can be adapted depending whether the reward function is piecewise linear (Sec. 3.3) or not (Sec. 4). The general problem that POMDPs address is for the agent to find a decision policy  X  choosing, at each time step, the best action based on its past observations and actions in order to maximize its future gain (which can be measured for example through the total accumulated reward or the average reward per time step). Compared to classical deterministic planning, the agent has to face the difficulty to account for a system not only with uncertain dynamics but also whose current state is imperfectly known. 2.1 POMDP Description Formally, POMDPs are defined by a tuple  X  X  , A ,  X  ,T,O,r,b 0  X  where, at any time step, the system being in some state s  X  S (the state space ), the agent performs an action a  X  A (the action over states. Unless stated otherwise, the state, action and observation sets are finite [6]. The agent can typically reason about the state of the system by computing a belief state b  X   X  = Bayes rule) when performing action a and observing o : rewritten as an MDP over the belief space, or belief MDP ,  X   X  , A , X , X   X  , where the new transition  X  and reward functions  X  are defined respectively over  X   X A X   X  and  X   X A . With this refor-mulation, a number of theoretical results about MDPs can be extended, such as the existence of a deterministic policy that is optimal. An issue is that, even if a POMDP has a finite number of states, the corresponding belief MDP is defined over a continuous X  X nd thus infinite X  X elief space. In this continuous MDP, the objective is to maximize the cumulative reward by looking for a policy  X   X  = argmax reward obtained at time step t , and  X  a discount factor. Bellman X  X  principle of optimality [7] lets us compute the function J  X   X  recursively through the value function horizon of the problem).
 The POMDP framework presents a reward function r ( s,a ) based on the state and action. On the other hand, the belief MDP presents a reward function  X  ( b,a ) based on beliefs. This belief-based reward function is derived as the expectation of the POMDP rewards: An important consequence of Equation 2 is that the recursive computation described in Eq. 1 has the property to generate piecewise-linear and convex (PWLC) value functions for each horizon [1], i.e., each function is determined by a set of hyperplanes (each represented by a vector), the value at a given belief point being that of the highest hyperplane. For example, if  X  n is the set of vectors representing the value function for horizon n , then V n ( b ) = max  X   X   X  n P s b ( s )  X  ( s ) . 2.2 Solving POMDPs with Exact Updates Using the PWLC property, one can perform the Bellman update using the following factorization of Eq. 1: with 2  X  n ( b ) = argmax  X  -sets, each one of size |  X  n  X  1 | . These sets are defined as representation of the value function, one can compute ( L being the cross-sum between two sets): Yet, these  X  n a,o sets X  X nd also the final  X  n  X  X re non-parsimonious : some  X  -vectors may be use-less because the corresponding hyperplanes are below the value function. Pruning phases are then required to remove dominated vectors. There are several algorithms based on pruning techniques like Batch Enumeration [8] or more efficient algorithms such as Witness or Incremental Pruning [6]. 2.3 Solving POMDPs with Approximate Updates The value function updating processes presented above are exact and provide value functions that can be used whatever the initial belief state b 0 . A number of approximate POMDP solutions have been proposed to reduce the complexity of these computations, using for example heuristic estimates of the value function, or applying the value update only on selected belief points [9]. We focus here on the latter point-based (PB) approximations, which have largely contributed to the recent progress in solving POMDPs, and whose relevant literature goes from Lovejoy X  X  early work [10] via Pineau et al.  X  X  PBVI [11], Spaan and Vlassis X  Perseus [12], Smith and Simmons X  HSVI2 [13], through to Kurniawati et al.  X  X  SARSOP [14].
 At each iteration n until convergence, a typical PB algorithm: The various PB algorithms differ mainly in how belief points are selected, and in how the update discretization or a random sampling of the belief simplex, picking reachable points (by simulating action sequences starting from b 0 ), adding points that reduce the approximation error, or looking in particular at regions relevant to the optimal policy [15]. 3.1 Introducing  X  POMDPs All problems with partial observability confront the issue of getting more information to achieve some goal. This problem is usually implicitly addressed in the resolution process, where acquiring information is only a means for optimizing an expected reward based on the system state. Some active sensing problems can be modeled this way (e.g. active classification), but not all of them. A special kind of problem is when the performance criterion incorporates an explicit measure of the agent X  X  knowledge about the system, which is based on the beliefs rather than states. Surveillance for example is a never-ending task that does not seem to allow for a modeling with state-dependent rewards. Indeed, if we consider the simple problem of knowing the position of a hidden object, it have been visited). However, the reward of a POMDP cannot model this since it is only based on the current state and action. One solution would be to include the whole history in the state, leading to a combinatorial explosion. We prefer to consider a new way of defining rewards based on the acquired knowledge represented by belief states. The rest of the paper explores the fact that belief MDPs can be used outside the specific definition of  X  ( b,a ) in Eq. 2, and therefore discusses how to solve this special type of active sensing problems.
 As Eq. 2 is no longer valid, the direct link with POMDPs is broken. We can however still use to generalize the POMDP framework to a  X  -based POMDP (  X  POMDP), where the reward is not depends on the problem, but is usually related to some uncertainty or error measure [3, 2, 4]. Most common methods are those based on Shannon X  X  information theory, in particular Shannon X  X  entropy or the Kullback-Leibler distance [16]. In order to present these functions as rewards, they have to P is used rather than Shannon X  X  original entropy. Also, other simpler functions based on the same idea can be used, such as the distance from the simplex center (DSC),  X  dsc ( b ) = k b  X  c k m , where c is the center of the simplex and m a positive integer that denotes the order of the metric space. Please note that  X  ( b,a ) is not restricted to be only an uncertainty measurement, but can be a combination of the expected state-action rewards X  X s in Eq. 2 X  X nd an uncertainty or error measurement. For example, Mihaylova et al.  X  X  work [3] defines the active sensing problem as optimizing a weighted sum of uncertainty measurements and costs, where the former depends on the belief and the latter on the system state.
 In the remainder of this paper, we show how to apply classical POMDP algorithms to  X  POMDPs. To that end, we discuss the convexity of the value function, which permits extending these algorithms using PWLC approximations. 3.2 Convexity Property An important property used to solve normal POMDPs is the result that a belief-based value function is convex, because r ( s,a ) is linear with respect to the belief, and the expectation, sum and max operators preserve this property [1]. For  X  POMDPs, this property also holds if the reward function  X  ( b,a ) is convex, as shown in Theorem 3.1.
 Theorem 3.1. If  X  and V 0 are convex functions over  X  , then the value function V n of the belief MDP is convex over  X  at any time step n . [Proof in [17, Appendix]] uncertainty (or information) measures, because the objective is to avoid belief distributions that do not give much information on which state the system is in, and to assign higher rewards to those beliefs that give higher probabilities of being in a specific state. Thus, a reward function meant to reduce the uncertainty must provide high payloads near the corners of the simplex, and low payloads near its center. For that reason, we will focus only on reward functions that comply with convexity in the rest of the paper.
 The initial value function V 0 might be any convex function for infinite-horizon problems, but by definition V 0 = 0 for finite-horizon problems. We will use the latter case for the rest of the paper, to provide fairly general results for both kinds of problems. Plus, starting with V 0 = 0 , it is also (respectively piecewise differentiable). 3.3 Piecewise Linear Reward Functions This section focuses on the case where  X  is a PWLC function and shows that only a small adaptation of the exact and approximate updates in the POMDP case is necessary to compute the optimal value function. The complex case where  X  is not PWLC is left for Sec. 4. 3.3.1 Exact Updates a . The reward is computed as: Using this definition leads to the following changes in Eq. 3 where  X  a  X  ( b,s ) = argmax where P a,o ( s,s 0 ) = T ( s,a,s 0 ) O ( s 0 ,a,o ) .
 Exact algorithms like Value Iteration or Incremental Pruning can then be applied to this POMDP extension in a similar way as for POMDPs. The difference is that the cross-sum includes not only reward: Thus, the cross-sum generates | R | times more vectors than with a classic POMDP, | R | being the number of  X  -vectors specifying the  X  ( b,a ) function 3 . 3.3.2 Approximate Updates Point-based approximations can be applied in the same way as PBVI or SARSOP do to the original POMDP update. The only difference is again the reward function representation as an envelope of hyperplanes. PB algorithms select the hyperplane that maximizes the value function at each belief point, so the same simplification can be applied to the set  X  a  X  . Uncertainty measurements such as the negative entropy or the DSC (with m &gt; 1 and m 6 =  X  ) are not piecewise linear functions. In theory, each step of value iteration can be analytically computed using these functions, but the expressions are not closed as in the linear case, growing in complexity and making them unmanageable after a few steps. Moreover, pruning techniques cannot be applied approximated by piecewise linear functions, making it possible to apply the techniques described in Section 3.3 with a bounded error, as long as the approximation of  X  is bounded. 4.1 Approximating  X  Consider a continuous, convex and piecewise differentiable reward function  X  ( b ) , 4 and an arbitrary (and finite) set of points B  X   X  where the gradient is well defined. A lower PWLC approximation of  X  ( b ) can be obtained by using each element b 0  X  B as a base point for constructing a tangent is the linear function that represents the tangent hyperplane. Then, the approximation of  X  ( b ) using a set B is defined as  X  B ( b ) = max b 0 (  X  b 0 ( b )) .
 At any point b  X   X  the error of the approximation can be written as and if we specifically pick b as the point where B ( b ) is maximal (worst error), then we can try to bound this error depending on the nature of  X  .
 It is well known that a piecewise linear approximation of a Lipschitz function is bounded because so this result is not generic enough to cover a wide range of active sensing problems. Yet, under certain mild assumptions a proper error bound can still be found.
 The aim of the rest of this section is to find an error bound in three steps. First, we will introduce some basic results over the simplex and the convexity of  X  . Informally, Lemma 4.1 will show that, for each b , it is possible to find a belief point in B far enough from the boundary of the simplex but within a bounded distance to b . Then, in a second step, we will assume the function  X  ( b ) verifies the  X  -H  X  older condition to be able to bound the norm of the gradient in Lemma 4.2. In the end, Theorem 4.3 will use both lemmas to bound the error of  X   X  X  approximation under these assumptions. the point in B whose tangent hyperplane gives the best approximation of  X  at b . Consider the point b  X   X  where B ( b ) is maximum: this error can be easily computed using the gradient  X   X  ( b  X  ) . Unfortunately, some partial derivatives of  X  may diverge to infinity on the boundary of the simplex in the non-Lipschitz case, making the error hard to analyze. Therefore, to ensure that this error can be bounded, instead of b  X  , we will take a safe b 00  X  B (far enough from the boundary) by using an Thus, for a given b  X   X  and  X   X  (0 , 1 N ] , we define the point b 0 = argmin x  X   X  of B , defined as  X  B = min Lemma 4.1. The distance (1-norm) between the maximum error point b  X   X  and the selected b 00  X  B is bounded by k b  X  b 00 k 1  X  2( N  X  1)  X  +  X  B . [Proof in [17, Appendix]] minimum distance from the boundary of  X  =  X   X   X  B . This will allow finding bounds for the PWLC approximation of convex  X  -H  X  older functions, which is a broader family of functions including the of the Lipschitz condition. In our setting it means, for a function f : D 7 X  R with D  X  R n , that it complies with on the boundary of the simplex  X  (due to the convexity), and therefore the point b 00 will be free of this predicament because of  X  . More precisely, an  X  -H  X  older function in  X  with constant K  X  in 1-norm complies with the Lipschitz condition on  X   X  with a constant K  X   X   X  (see [17, Appendix]). Moreover, the norm of the gradient k X  f ( b 00 ) k 1 is also bounded as stated by Lemma 4.2. Lemma 4.2. Let  X  &gt; 0 and f be an  X  -H  X  older (with constant K  X  ), bounded and convex function from  X  to R , f being differentiable everywhere in  X  o (the interior of  X  ). Then, for all b  X   X   X  , k X  f ( b ) k 1  X  K  X   X   X   X  1 . [Proof in [17, Appendix]] Under these conditions, we can show that the PWLC approximation is bounded.
 Theorem 4.3. Let  X  be a continuous and convex function over  X  , differentiable everywhere in  X  o (the interior of  X  ), and satisfying the  X  -H  X  older condition with constant K  X  . The error of an approximation  X  B can be bounded by C X   X  b , where C is a scalar constant. [Proof in [17, Appendix]] 4.2 Exact Updates Knowing that the approximation of  X  is bounded for a wide family of functions, the techniques described in Sec. 3.3.1 can be directly applied using  X  B ( b ) as the PWLC reward function. These algorithms can be safely used because the propagation of the error due to exact updates is bounded. This can be proven using a similar methodology as in [11, 10]. Let V t be the value function using the PWLC approximation described above and V  X  t the optimal value function both at time t , H being the exact update operator and  X  H the same operator with the PWLC approximation. Then, the error from the real value function is For these algorithms, the selection of the set B remains open, raising similar issues as the selection of belief points in PB algorithms. 4.3 Approximate Updates In the case of PB algorithms, the extension is also straightforward, and the algorithms described in Sec. 3.3.2 can be used with a bounded error. The selection of B , the set of points for the PWLC approximation, and the set of points for the algorithm, can be shared 5 . This simplifies the study of the bound when using both approximation techniques at the same time. Let  X  V t be the value function at time t calculated using the PWLC approximation and a PB algorithm. Then the error between  X  V t k  X 
V  X  ( b ) respectively. This is because the worst case for an  X  vector is R min  X  is only R max 1  X   X  because the approximation is always a lower bound. We have introduced  X  POMDPs, an extension of POMDPs that allows for expressing sequential objective. In this model, the reward  X  is typically a convex function of the belief state. Using the convexity of  X  , a first important result that we prove is that a Bellman backup V n = V n is also PWLC and it is straightforward to adapt many state-of-the-art POMDP algorithms. Yet, if  X  is not PWLC, performing exact updates is much more complex. We therefore propose employing PWLC approximations of the convex reward function at hand to come back to a simple case, and show that the resulting algorithms converge to the optimal value function in the limit. Previous work has already introduced belief-dependent rewards, such as Spaan X  X  discussion about POMDPs and Active Perception [19], or Hero et al. X  X  work in sensor management using POMDPs [5]. Yet, the first one only presents the problem of non-PWLC value functions without giving a specific solution, meanwhile the second solves the problem using Monte-Carlo techniques that do not rely on the PWLC property. In the robotics field, uncertainty measurements within POMDPs These techniques use only state-dependent rewards, but uncertainty measurements are employed to speed up the solving process, at the cost of losing some basic properties (e.g. Markovian property). Our work paves the way for solving problems with belief-dependent rewards, using new algorithms approximating the value function (e.g. point-based ones) in a theoretically sound manner. An important point is that the time complexity of the new algorithms only changes due to the size of the approximation of  X  . Future work includes conducting experiments to measure the increase in complexity. A more complex task is to evaluate the quality of the resulting approximations due to the lack of other algorithms for  X  POMDPs. An option is to look at online Monte-Carlo algorithms [20] as they should require little changes.
 This research was supported by the CONICYT-Embassade de France doctoral grant and the CO-MAC project. We would also like to thank Bruno Scherrer for the insightful discussions and the anonymous reviewers for their helpful comments and suggestions.
 [1] R. Smallwood and E. Sondik. The optimal control of partially observable Markov decision [2] S. Thrun. Probabilistic algorithms in robotics. AI Magazine , 21(4):93 X 109, 2000. [3] L. Mihaylova, T. Lefebvre, H. Bruyninckx, K. Gadeyne, and J. De Schutter. Active sensing for [5] A. Hero, D. Castan, D. Cochran, and K. Kastella. Foundations and Applications of Sensor [6] A. Cassandra. Exact and approximate algorithms for partially observable Markov decision [7] R. Bellman. The theory of dynamic programming. Bull. Amer. Math. Soc. , 60:503 X 516, 1954. [8] G. Monahan. A survey of partially observable Markov decision processes. Management Sci-[9] M. Hauskrecht. Value-function approximations for partially observable Markov decision pro-[10] W. Lovejoy. Computationally feasible bounds for partially observed Markov decision pro-[11] J. Pineau, G. Gordon, and S. Thrun. Anytime point-based approximations for large POMDPs. [12] M. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for POMDPs. [13] T. Smith and R. Simmons. Point-based POMDP algorithms: Improved analysis and imple-[14] H. Kurniawati, D. Hsu, and W. Lee. SARSOP: Efficient point-based POMDP planning by [15] R. Kaplow. Point-based POMDP solvers: Survey and comparative analysis. Master X  X  thesis, [16] T. Cover and J. Thomas. Elements of Information Theory . Wiley-Interscience, 1991. [17] M. Araya-L  X  opez, O. Buffet, V. Thomas, and F. Charpillet. A POMDP extension with belief-[18] R. Saigal. On piecewise linear approximations to smooth mappings. Mathematics of Opera-[19] M. Spaan. Cooperative active perception using POMDPs. In AAAI 2008 Workshop on Ad-[20] S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online planning algorithms for POMDPs.
