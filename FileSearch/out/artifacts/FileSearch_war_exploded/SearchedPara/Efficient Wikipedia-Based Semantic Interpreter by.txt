 Proper representation of the meaning of texts is crucial to enhancing many data mining and information retrieval tasks, including clustering, computing semantic relatedness between texts, and searching. Representing of texts in the concept-space derived from Wikipedia has received growing atten-tion recently, due to its comprehensiveness and expertise. This concept-based representation is capable of extracting semantic relatedness between texts that cannot be deduced with the bag of words model. A key obstacle, however, for using Wikipedia as a semantic interpreter is that the sheer size of the concepts derived from Wikipedia makes it hard to efficiently map texts into concept-space. In this paper, we develop an efficient algorithm which is able to represent the meaning of a text by using the concepts that best match it. In particular, our approach first computes the approximate top-k concepts that are most relevant to the given text. We then leverage these concepts for representing the meaning of the given text. The experimental results show that the proposed technique provides significant gains in execution time over current solutions to the problem.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods Experimentation, Performance Wikipedia, Concept, Semantic interpretation
The bag of words (BOW) model has been shown to be very effective in diverse areas which span a large spectrum from traditional text-based applications, such as clustering and classification, to web and social media. While there Figure 1: A Wikipedia-based semantic interpreter maps documents from the keyword-space into the Wikipedia concept-space. have been a number of models in information retrieval us-ing the bag of words, the vector-space model [6] is the most commonly used in the literature. In the word-based vec-tor model, given a dictionary, U ,with u distinct words, a document is represented as u -dimensional vector, d ,where only those positions in the vector that correspond to the document words are set to &gt; 0andallothersaresetto0.
Although the BOW-based vector model is the most popu-lar scheme, it has limitations: these include sparsity of vec-tors and lacking semantic relationship between words. One way to tackle these limitations is to enrich the individual documents with the background knowledge obtained from existing human-contributed knowledge, such as Wikipedia [1]. Due to its comprehensiveness and expertise, Wikipedia has been applied to diverse applications where it is used as a semantic interpreter which re-interprets (or enriches) orig-inal documents based on the concepts of Wikipedia. As shown in Figure 1, such semantic re-interpretation equals to a mapping of original documents from the keyword-space into the concept-space [5]. Generally, the mapping between the original dictionary and the concept is performed by (a) matching concepts to the keywords appearing in the docu-ment and (b) replacing the keywords in the document with these matched concepts. In the literature, this process is commonly defined as the matrix multiplication between the original document-keyword matrix and the keyword-concept matrix (Figure 1). Such a Wikipedia-based semantic re-interpretation has the potential to ensure that documents mapped into the Wikipedia concept-space are semantically informed, significantly improving the effectiveness on vari-ous tasks, including text categorization [5].
The main obstacle in leveraging the Wikipedia as a seman-tic interpreter stems from efficiency concerns. Considering the sheer size of Wikipedia articles (more than 4M concepts), reinterpreting original documents based on all possible con-cepts of Wikipedia can be prohibitively expensive. Thus, our contributions in the paper include the following: We develop a novel algorithm that efficiently reinterprets docu-ments based on the concepts of Wikipedia. The presented algorithm is able to approximately but efficiently compute the most significant k -concepts in Wikipedia for a given doc-ument, and leverage these concepts to map an original doc-ument from the keyword-space into the concept-space.
Let U be a dictionary with u distinct words. The concepts in Wikipedia are represented in the form of a u  X  m keyword-concept matrix , C ,where m is the number of concepts that corresponds to articles of Wikipedia and u is the number of distinct keywords in the dictionary. Let C i,r denote the weight of the i -th keyword, t i ,inthe r -th concept, c r Without loss of generality, we assume that each concept-vector , C  X  ,r , is normalized into a unit length.
Given a dictionary, U , a document, d , is represented as a l -dimensional vector, d =[ w 1 ,w 2 ,  X  X  X  ,w u ].
Computing d with all possible Wikipedia concepts may be prohibitively expensive. Thus, our goal in this paper is to reinterpret a document with the best k concepts in Wikipedia that are relevant to it . Given a re-interpreted document, d =[ w 1 ,w 2 ,  X  X  X  ,w m ], let S k be a set of k concepts, such that the following holds: In other words, S k contains k concepts whose contributions to d are greater than or equal to the others.

Exactly computing the best k concepts that are relevant to agivendocumentoftenrequirestoscananentirekeyword-concept matrix which is very expensive. Thus, in order to achieve further efficiency gains, we relax S k as follows: given adocument, d ,let S k, X  be a set of k concepts such that at least  X k answers in S k, X  belong to S k ,where0  X   X   X  1. Then, our objective in this paper is defined as follows: Problem 1 (Semantic re-interpretation with S k, X  ). Given a keyword-concept matrix, C , a document vector, d , and the corresponding approximate best k concepts, S k, X  a semantic re-interpretation of d based on the approximate top-k concepts in Wikipedia that match it is defined as d = [ w 1 ,w 2 ,  X  X  X  ,w m ] where In other words, the original document, d , is approximately mapped from the word-space into the concept-space which consists of the approximate k concepts in Wikipedia that best match a document d . Thus, the key challenge to this problem is how to efficiently identify such approximate top-k concepts, S k, X  . To address this problem, in this paper, we present a novel ranked processing algorithm to efficiently compute S k, X  for a given document.
 Figure 2: A general framework of the Wikipedia-based semantic interpreter which relies on ranked processing schemes.
In this section, we describe the algorithm for the efficient semantic interpreter using Wikipedia. The proposed algo-rithm consists of two phases: (1) computing the approximate top-k concepts, S k, X  , of a given document and (2) mapping an original document into the concept-space using S k, X  .
The threshold-based algorithms are based on the assump-tion that given sorted-lists, each object has a single score in each list [4]. The possible scores of unseen objects in NRA [4] algorithm are computed based on this assumption. This as-sumption, however, does not hold for the sparse keyword-concept matrix where most of entries are 0. Thus, in this subsection, we first describe a method to estimate the scores of unseen objects with the sparse keyword-concept matrix, and then present a method to obtain the approximate top-k concepts of a given document leveraging the expected scores.
Since the assumption that each object has a single score in each input list is not valid for a sparse keyword-concept matrix, in this subsection we aim to correctly estimate a bound on the number of input lists where each object is ex-pected to be found during the computation. A histogram is usually used to approximate data distributions (i.e., prob-ability density function). Many existing approximate top-k processing algorithms maintain histograms for input lists and estimate the scores of unknown objects by convoluting histograms [3, 7]. Generally, approximate methods are more efficient than exact schemes. Nevertheless, considering that there are a huge number of lists for the keyword-concept ma-trix, maintaining such histograms and convoluting them in run-time for computing possible aggregated scores is not a viable solution. Thus, in order to achieve further efficiency, we simplify the data distribution of each inverted list by re-lying on the binomial distribution: i.e., the case in which an inverted list contains a given concept or the other one in which it does not. Such simplified data distribution does not cause a significant reduction in the quality of the top-k results, due to the extreme sparsity of the concept matrix.
Given a keyword t i and a keyword-concept matrix C ,the length of the corresponding sorted list, L i , is defined as Given a u  X  m keyword-concept matrix, C , we formulate the probability that an instance c r ,C i,r is in L i as | L i
Generally, the threshold-based algorithms sequentially scan the each sorted list. Let us assume that the algorithm se-quentially scans the first f i instances from the sorted list L , and the instance c r ,C i,r was not seen during the scans. Then, we can compute the probability, P C i,r ,f i , that an in-stance c r ,C i,r will be found in the unscanned parts of the Note that P C i,r ,f i will be 1 under the assumption that each object has a single score in each input list (i.e., | L i However, the keyword-concept matrix is extremely sparse, and thus, in most cases, P C i,r ,f i is close to 0.
Let us be given a document, d , and a corresponding u -dimensional vector, d =[ w 1 ,w 2 ,  X  X  X  ,w u ]. Furthermore, given d ,let L be a set of sorted lists such that: In other words, L is a set of sorted lists whose corresponding wordsappearinagivendocument d . Other lists not in L do not contribute to the computation of the semantically reinterpreted vector, d , because their corresponding weights in the original vector d equal to 0 (Figure 2).

Let us further assume that the occurrences of words in a document are independent of each other. The word-independence assumption has long been used by many applications due to the concept c r , which was not yet seen in any list so far, will be found in exactly n lists in L afterward. Then, it can be computed as follows: P where, probability that a fully unseen concept c r will be found in up to n lists in L during the computation as follows:
As described earlier, our objective is to find the approx-imate top-k concepts, S k, X  , satisfying that at least  X k an-swers in S k, X  belong to the exact top-k results, S k .Given an application (or user) provided acceptable precision rate  X  , in order to compute the bound, b r , on the number of lists where a fully unavailable concept, c r , will be found, we chose the smallest value b r satisfying In summary, b r is the smallest value satisfying the probabil-ity of an unseen concept c r being less than b r input lists is higher than an acceptable precision rate,  X  .
Once we estimate the number of lists where any fully un-seen object will be found, we can compute the expected scores of fully (or partially) unseen objects.

Given a current threshold vector th =[  X  1 , X  2 ,  X  X  X  , X  u an original document vector d =[ w 1 ,w 2 ,  X  X  X  ,w u ], we define W as follows: Then, the expected score of the fully unseen concept c r is bounded by where W h is the h -th largest value in W .

Each list in an inverted index is sorted on weights rather than concept IDs, which results in a partially available (seen) concept-vector of a given concept, c r , during the top-k com-putation. Thus, we also need to estimate the expected scores of partially seen objects. Let c r be a partially seen concept. Furthermore, let KN r be a set of positions in the concept-vector, C  X  ,r , whose weights have been seen before by the algorithm. Then, the expected score of partially seen con-cept c r is defined as follows:
Figure 3 describes the pseudo-code for the proposed algo-rithm to compute the approximate top-k concepts, S k, X  ,of a given document.
Once the approximate top-k concepts of a given docu-ment are identified, next step is to map an original document from the keyword-space into the concept-space. Figure 4 de-scribes the pseudo-code for mapping an original document from the keyword-space into the concept-space using S k, X 
In this section, we describe the experiments we carried out to evaluate the proposed methods introduced in Section 3. Experimental setup: In the experiments, we compute the top-k results using following alternatives: the inverted file based method ( IF ) which enables to scan only those entries whose corresponding values in the keyword-concept matrix are greater than 0 and the the proposed scheme in this paper ( Sparse topk ).
 For the keyword-concept matrix , C , we downloaded the Wikipedia dump which was released at September 2009. This data con-tains  X  4M articles. After discarding redirect pages and arti-cles containing insufficient non-stopwords, we generated the Figure 3: Phase 1: Pseudo-code for computing the approximate top-k Wikipedia concepts Input : a original document vector d =[ w 1 ,w 2 ,  X  X  X  ,w Output : an approximate top-k concepts S k, X  2: min k  X  0 // cut off score 3: Cnd  X  X  X  // set of candidates 4: th =[1 , 1 ,  X  X  X  , 1] // threshold vector 5: unseen exp  X  compute the expected score of fully 6: while | Cnd | = k and unseen exp  X  min k do 8: i  X  Select-Next-List ( L ) 9: c r ,C i,r  X  read an instance from L i 10: w r,wst  X  update the worst-score with c r ,C i,r 11: Cnd  X  update the candidate set with c r ,w r,wst 12: min k  X  choose the cut of score from Cnd 13: th [ i ]= C i,p 15: for each r p ,w p,wst  X  Cnd do 16: w p,exp  X  compute the expected score of 17: if w p,exp &lt;min k then 18: Cnd = Cnd  X  c p ,w p,wst 19: end if 20: end for 21: unseen exp  X  compute the expected score of 22: end while 23: S k, X   X  Cnd 24: return S k, X  2 , 394 , 723  X  812 , 926 keyword-concept matrix , C .Fordoc-ument vectors, d , we used the news articles collected from Wikinews [2]. This data contains 100 documents. The re-sults reported in this section are the averages of all runs for 100 documents.
 Experimental results: Figure 5 shows the execution times for varying target precisions,  X  ,and k respectively. Note that the proposed approach, Sparse topk , can vary the pre-cision rates of the top-k results depending on the application needs, while the precision of IF scheme always equals to 1. Key observations based on Figure 5 can be summarized as follows: Figure 4: Phase 2: Pseudo-code for mapping an original document into the concept-space Input : a original document vector d =[ w 1 ,w 2 ,  X  X  X  ,w an approximate top-k concepts S k, X  , identified in Phase 1 Output : a semantically re-interpreted document, 1: d =[0 , 0 ,  X  X  X  , 0] // a semantically reinterpreted vector 2: for each c p ,w p,wst  X  S k, X  do 3: w p,exp  X  compute the expected score of 4: d [ p ]= w p,exp 5: end for 6: return d
In this paper, we proposed a semantic interpreter for effi-ciently enriching original documents based on the concepts of Wikipedia. Our proposed approach enables to efficiently identify the most significant k -concepts in Wikipedia for a given document and leverage these concepts to semanti-cally re-interpret an original document by mapping it from keyword-space to the concept-space. Experimental results show that the proposed technique significantly improves ef-ficiency of semantic reinterpretation without causing signif-icant reduction in precision.
