 Because of the fast growth of the World Wide Web and the amount of informa-tion existing in different languages, cross language information retrieval (CLIR) has become a very important task. CLIR is the task of retrieving documents where queries and documents are in different languages. One of the most impor-tant issues in CLIR is where to obtain translation knowledge. Different trans-lation resources have been used for this purpose. Comparable corpora are one of the useful resources widely used in different languages. However, few studies have been done on constructing and using comparable corpora in Persian [8,6]. Karimi X  X  Persian-English comparable corpus [8] consists of 1100 loosely trans-lated BBC News documents and UTPECC Persian-English comparable corpus [6] is constructed using Persian articles of Hamshahri newspaper 1 and English articles of BBC News 2 .
Different resources have been used to construct comparable corpora. Many researches use news articles to obtain comparable corpora [10,4,16,1,6], some methods crawl the web [18,13,20,7], and some others use the research corpora like CLEF and TREC collections [17,3,14] to obtain comparable corpora. Many studies to construct comparable corpora, easily align news articles in different languages by date [19,20,1]. Although this approach may work in some languages, this is not the case in Persian because of the lack of news agencies that pub-lish appropriate articles in both Persian and English. Some studies try to align documents in comparable corpora based on both date criteria and content sim-ilarities [3,17,6]. These methods can align news articles which are related to the same event, however, are not able to identify all possible alignments with distant dates. Intuitively, some news articles which are published in distant dates, may have similar concepts, or be about related events or discuss the same topic even though are not related to any specific event, for example two documents that discuss  X  X kin cancer prevention X . Thes e kinds of alignments can also be useful for the CLIR task. However these alignmen ts should be considered very carefully to avoid aligning unrelated documents with common keywords.

In this paper, we propose a method for aligning related documents that cannot be identified using date criteria. On top of the previous methods for aligning the documents, we propose to align documen ts based on  X  X oncept similarities X  and  X  X opics X  and without the date criteria. For this purpose, we propose to first cluster the collections based on their topics and then align the documents in the corresponding clusters in the two langu ages based on their concept similarities. This enables us to align similar documents whose publication dates are distant, like scientific articles. These docume nts with similar concepts not only have common keywords, but also are about the same topic, thus we can avoid aligning unrelated documents in different t opics that share common keywords.
For topical clustering of the documents, we propose a method based on edge betweenness measure [5] and extract major features of each cluster using the mutual information (MI) graph. These features represent the topics of the col-lection and are used to cluster the docum ents in the collect ion. To construct the comparable corpus, we follow the general procedure proposed in [6]. We first apply the proposed alignment method on the whole collection to find the align-ments based on concept similarities and d ate criteria. We then apply a similar method on each cluster to align the docume nts based on concept similarities and topics. Our experiment results show that the proposed approach improves both the quality and size of the existing Persian-English comparable corpora. The rest of the paper is organized as follows. We explain the details of constructing the comparable corpus in sect ion 2. We discuss the exper iment results in section 3 and finally bring the conclusions and future work of our study in section 4. We use two independent news collections, one in English and another in Persian, to construct the comparable corpus. We construct the comparable corpus con-sidering two kinds of alignments: (1) alignments based on the concept similarity of the documents and the publication dates, similar to the method proposed in [6], and (2) alignments based on topics and concept similarities.

In the first phase, two different news articles are aligned if their publication dates are not distant and they have common describing keywords. Using date criteria, we cannot align news with similar concepts which are not published in same period of time. Intuitively, some news articles which are published in distant dates may have similar concepts. F or these reasons we try to further align related news that cannot be aligned with date criteria. We cannot align such news simply by omitting date criteria and jus t based on concept similarity because a lot of noise will be added to the corpus this way. For example we may mistakenly align news articles that talk about different topics, but share some keywords. In order to avoid such alignments, we propose to first cluster the collections based on their topics and then align the documents in the corresponding clusters based on their concept similarities.

We follow these steps in the proposed method: (1) extract best representative features of the English collection, (2) construct the mutual information graph of the features (the MI graph describes the relatedness of the features and by analyzing these relations, we can recognize different topics of the collection), (3) cluster the MI graph and extract independent components that represent the topics of the collection, (4) enrich the features of each component using Chi-Square method and cluster the documents based on these features, (5) extract the corresponding clusters in the Persian collection based on translation of the features and Persian MI graph, (6) and finally align the documents in each cluster. In the rest of this section, we present the details of the method for constructing and evaluating the comparable corpus. 2.1 Base Features Selection and Construction of the MI Graph In these steps, we extract the best repre sentative features of the English col-lection and construct a pruned mutual information graph using these features. Extraction of the features is necessary because using all unique words of the collection for constructing the MI graph can considerably d ecrease the quality of clusters and the performance of the clustering method. In order to select the words which best represent the news collection, we apply the RATF formula [11] and extract the main features of e ach document of the collection. In this formula, we set the parameters to their best values reported in [17]. We then sort all these extracted features b ased on their collection frequencies. Each feature is representative of at lea st one document. If the feature occurs rarely in the collection, it cannot be a good representative of a topic in the collection and thus we omit it. In this way, we come up with the list of features that are expected to be the representatives of major domains of the collection. We use these features to construct the Mutual Information (MI) Graph. The features are vertices of th is directional graph and e ach vertex is connected to 100 top related vertices based on their MI score. We decrease the noise in the graph and the size of the graph by pruning unimportant edges and vertices in the following steps: 1. Omit the edge from vertex X to vertex Y if the Mutual Information between 2. Omit the edge between two vertices if it is not bidirectional. If the edge is 3. Omit the vertices with few edges beca use such words are loosely connected 4. Repeat the steps 2 and 3 until the convergence, i.e., to the point that no This final graph is less noisy than the MI graph of all features of the collection because it doesn X  X  contain high frequen cy and low frequency words. Besides, the size of the graph is much smaller which increases the performance of our further computations. 2.2 Extraction of Major Topics Using MI Graph The goal in this step is to recognize major topics of the collection by clustering of the MI graph. Our intuition is that the features related to each topic mostly co-occur, so in the MI graph they constr uct tightly connected components that are loosely connected to the other compon ents. In order to cluster the graph, we propose to use the edge betweenness measure [5]. Edge betweenness measures the betweenness centrality of the edges by summing up the number of all shortest paths that run along them. If the graph contains tightly connected components that are loosely connected to the other ones by some inter-group edges, the shortest paths between different components must go along these few inter-group edges and these edges will have high edge betweenness. By removing the edges with the highest betweenness in a hierarchical algorithm, we can separate different components that represen t different topics of the collection.
In [2], the authors propose a fast algorithm for calculating betweenness cen-trality of vertices in O ( mn ) for unweigthed graphs where m is the number of edges and n is the number of nodes. We make a minor change in this algorithm for generalizing the node betweenness cen trality to edge betw eenness centrality.
Starting from the constructed MI graph, we remove edges to the point that the components have acceptable number of vertices and the size of all the com-ponents are in an equal range. It is important to note that by removing the edges, we will come up with some verti ces with zero degree which are ignored. Applying this algorithm to the MI graph, we achieve the major clusters of the graph. Each resulting component will be representative of a major domain of the collection. 2.3 Construction of Document Clusters In the previous step, we obtained features that are representative of major topics in the English collection. In order to improve the quality of clusters, we enrich the features. We extract representative features of each topic of the collection using the Chi-Square feature selection. If each new feature extracted by chi-square method has relation with more than 10% of the representative features of the topic on the MI graph, we add it to the feature word list.
 We use these extracted features to cluster the documents in the collection. To this end, we construct queries using the MI graph of extracted features and retrieve documents using these queries. In each topic, we first use the node centrality algorithm [2] to select the mos t central words. The high centrality score shows the vertices are on considerably large number of shortest paths and that they can reach to the other vertices on relatively short paths. By removing the central vertices, we divide the graph to a number of subcomponents. We then construct the queries based on th ese subcomponents. The words of each subcomponent are added to one query. In order to prevent constructing large queries, we divide large subcomponents to more than one query and add specific number of words to each query. Central words are added to the query if their corresponding vertices in the graph are connected to one or more query words.
After constructing the queries, we use a retrieval model to rank the top 1000 documents based on their similarities to the queries. The more times a document appears in the results, and the higher it is ranked, the higher score it gets. The documents are then sorted based on their scores. In this model documents can appear in more than one topic, we assign a document to a cluster if its score in that topic is higher than its score in the other topics. Major topics of the collection are extracted and at next st ep, we derive the same topics from the Persian collection. 2.4 Extraction of Related Topics in the Persian Collection Now that we have clustered the English c ollection to major topics, we extract corresponding clusters in the Persian collection. Having the English features of a topic, we use an English-Persian dictionary to translate the features to Persian. We use the top 3 translations for each word in the dictionary. Some English words are translated to phrases in Persian (containing verbs, prepositions, and stop words), and some of the translations are not related to the considered topic, so we need to clean up the translated fe atures and remove the noise as much as possible. To this end, we first break down all translated phrases to corresponding words, and remove the stop words. We then use the Persian mutual information graph extracted from the Persian coll ection to recognize words related to the topic: we first remove the words that are not connected to the other words and then we construct Persian queries using the final feature words for the cluster. We extract central features and subcomponents from the MI graph and construct the queries using discussed methods in section 2.3. We finally rank and cluster the Persian documents in the same way as we did for English documents. At the end of this step, we come up with an English and Persian cluster for each topic.
 2.5 Document Alignment As discussed before, we align the docu ments in the compar able corpus either based on concept similarity and publication date or based on concept similarity and topic. In order to align the documents in the whole collections based on the concept similarities and publication dat es,weusethemethodproposedin[6]. We first construct a query for each English document by applying the RATF formula. We then translate the query words to Persian using a dictionary, Google machine translation system for the words not found in the dictionary, and Google transliteration system for the words which are still not translated. The idea for this kind of alignment is that if an English document and a Persian document have a high similarity score and are published at close dates, they are likely talking about a related event. We apply a combination of three different score thresholds (  X  1 &lt; X  2 &lt; X  3 ) to search for suitable document alignments. The threshold becomes tighter for more distant publication dates.

In the second phase, in order to align t he documents based on the concept similarities and topics, we extract topics as explained in section 2.2, construct the corresponding English and Persian clusters and align the documents in these two clusters. We align two documents in the corresponding clusters of a topic if their similarity score is higher than  X  % of all the similarity scores on the topic. We should set the  X  threshold to a high value in order to prevent low quality alignments. The combination of all obtained alignments comprises our final comparable corpus. In this section, we explain our experiments on creating the Persian-English com-parable corpus as well as extracting word associations from the corpus and using them in cross-lingual IR. In our experiments, we used two independent news col-lections, the English news articles of BBC News dated from Jan. 2002 to Dec. 2006, and the Persian news articles of Ham shahri newspaper published between 1996 and 2007. We used the Lemur toolkit 3 as our retrieval system and Porter stemmer to stem the English words. All the parameters are tuned to their best values. 3.1 Creating and Evaluating the Comparable Corpus To construct the comparable corpus, first w e select major representative features of the collection and construct their mutual information graph using the method discussed in section 2.1. We further om it the features that rarely occur in the collection: we remove the words with co llection frequency less than 80 and we set MI thr =0 . 001. In this way, we come up with 53142 extracted features. We then cluster the MI graph to its subcomponents by removing central edges. In our experiments, we force the size of each cluster to be greater than 100 words. By removing 6000 edges, the termination conditions are met and 6 different clusters are obtained in the English collection. To cluster the documents in the collection using the method discussed i n 2.3, we select 20 central words of each topic and add them to the queries. We also divide large subcomponents to more than one query, adding 10 words to each query.
 BBC % of Coverage % of Coverage Class in UTPECC in Proposed CC Health 3.3 19.4 Technology 3.8 20.3 Business 6.1 20
Nature 8.1 24
In the experiments we se t the score thresholds  X  1 , X  2 , X  3 to the best values reported in [6] which are  X  1 = 60,  X  2 =80and  X  3 = 85. In the second phase in which we align documents in clusters based on concept similarities and topics, we set the threshold to a tighter value,  X  = 98, to prevent adding low quality alignments. Table 1 shows some statistics about the proposed comparable corpus.
To assess the coverage of the comparable corpus on different topics, we lever-age the BBC News classes. Each article in BBC News is tagged based on its topic. We have not used these tags for our topical clustering for two reasons. First these classes have different granu lites. For example, the topic of one class is  X  X ealth X  which is very general and the topic of another class is  X  X oxing X . Sec-ond the topics of some of the classes are not very meaningful. For example some articles are classified based on the regio n that the event occurred in like Europe, Africa, and pacific Asia. Although these tags are not good topic indicators for our clustering purposes, we can use them to compare the coverage of our compa-rable corpus with the existing ones. We compute the coverage percentage of the most meaningful classes for the UTPECC. We also measure the coverage per-centage of these classes in the proposed corpus. The results are shown in Table 2 (a). As can be seen, the proposed method increases the coverage percentage of the classes considerably.

We further show that although we increase size of the corpus, the proposed corpus has proper quality. We evaluate the proposed corpus by measuring the quality of alignments. We manually assess the quality of alignments whose BBC news publication dates are between 1st of Dec. 2006 and 13th of Dec. 2006, on a five-level relevance scale [3]. The five levels of relevance are: (Class 1) Same story, (Class 2) Related story, (Class 3) Shared aspect, (Class 4) Common terminology, and (Class 5) Unrelated. The results in Table 2 (b) show that most of the alignments belong to classes (1) through (4) and only few unrelated documents are aligned in the proposed comparable corpus. As discussed in [3], for extracting terms for CLIR applications, classes (1) through (4) are helpful and a corpus with such alignments is considered a high quality corpus. 3.2 Extracting Word Associations As the second criterion to examine the proposed comparable corpus, we try to extract word association knowledge from the corpus. Using the method proposed in [17] with a minor change. We replace the Maxtf k , maximum term frequency in document d k , in the formula [17] by Avgtf k . As discussed in [15], and confirmed in our experiments, Avgtf normalization performs better than Maxtf . Intuitively, a high quality corpus should lead to high quality associations. Table 3 shows some sample Persian-English word associations extracted from the proposed corpus and its comparison with the associations extracted from UTPECC. For each English word, we show the top associated Persian words. The English words are stemmed and suffixes are omitted. We also report the Persian word Google translations for the readers not familiar with Persian. As can be seen, most of the extracted associations from the proposed corpus have very high quality and they show considerable improvement compared to the associations extracted from UTPECC corpus.
 3.3 Cross-Language Information Retrieval Experiments In this step, we use the extracted word associations to do cross language infor-mation retrieval. As the cross-language information retrieval task, we focus on the CLIR task of CLEF-2008. We also repeat some of the CLIR experiments on the INFILE collection (CLEF INFILE track). In our experiments, we use the top K extracted word associations to translate English queries to Persian. We do an exponential transformation on the scores of the associations and then normalize the scores to obtain word pair probabilities. We use the obtained probabilities to construct the Persian query language model for each English query and retrieve documents using the KL-divergence re trieval model [9]. We run a monolingual retrieval as baseline. We did not stem th e Persian queries because of the lack of high performance Persian stemmer and we only use the topic field of each query. For evaluating our experiments we use mean average precision (MAP), precision at top 5, precision at top 10 and recall of 1000 top documents.
 We repeat our experiments using different values for K . Figure 1 depicts the MAP-K graph which shows resulting MAP for different values of K using the associations extracted from both the proposed corpus and UTPECC. The result shows that using this weighting system and by increasing in the amount of K , the resulting MAP does not change and thus the selection of best K is not an issue in our retrieval system. It also shows the improvement in the quality of extracted associations co mpared to the associations extracted from UTPECC.
In our experiments on CLEF-2008 CLIR task, the Persian document collec-tion and CLIR are both on the same Hamshahri collection. In order to have a fair set of experiments and to make sure that the extracted associations are Method (Collection) MAP %of Prec @5 %of Prec @10 %of Recall %of Mono (Hamshahri) 0.4081 0.648 0.624 0.868 The Proposed CC 0.1331 32.61 0.252 38.8 0.218 34.9 0.546 62.9
The Proposed CC 0.1272 60.28 0.1917 55.72 0.1854 57.57 0.615 97 not biased toward the Hamshahri corpus, we ran the set of experiments on the INFILE data set as well. Table 4 shows the results for both collections. The results on Hamshahri collection show tha t using the extracted associations from the proposed comparable corpus, compared to the monolingual retrieval, we can achieve up to 32 . 61% of MAP and also compared to the experiments using the UTPECC, we can achieve 13 . 85% improvement in MAP. The results on INFILE collection also show that usi ng the extracted associations from the proposed com-parable corpus we can achieve up to 60 . 28% of monolingual MAP and compared to the experiments using the UTPECC, we can achieve 49 . 64% improvement in MAP. This improvement shows the improvement in quality of the associations and quality of the proposed comparable corpus.
 CC+Wiki(k=2) 0.227 55.6 0.428 66 0.371 59.4 0.596 68.6 CC+Dic+wiki(k=1) 0.24 58.8 0.476 73.4 0.419 67.2 0.636 73.2
We set up another experiment to combin e two other translation resources with our extracted word associations to improve the performance of cross language IR. The first resource is an English-Persian dictionary which contains more than 50,000 entries. The second resource is the Persian-English translation knowledge extracted from Wikipedia using the method proposed in [12]. For each English query word, we compute the intersection o f Persian translations extracted from the proposed comparable corpus and those extracted from Wikipedia and/or the dictionary. We sort the result Persian word list, I, based on the association scores. If the size of I is smaller than K, the number of translations for the query word, we extend the list with translations extracted from the comparable corpus. In this approach we do not blindly add translations from dictionary and translation knowledge of Wikipedia to the queries, but instead using these resources, we emphasize the importance of common tra nslations in different resources.
In this experiment, we retrieve documents using the simple-KL-divergence model and we set up K to different values and we report the best results. Ta-ble 5 shows the results of CLIR using different combinations of translation re-sources. The results show that using the dictionary or the translation knowledge extracted from Wikipedia improves the quality of the extracted associations from the corpus by rearranging the associations. Results in Table 5 show this method improves the MAP of the proposed method using the associations as the only resource by 80 . 4% and the MAP of the method using the dictionary and translation knowledge of Wikipedia by 14 . 8%. In this work, we constructed a Persian-English comparable corpus from two independent news collections. We proposed a method to construct a compara-ble corpus based on concept similarities and topics. This approach enabled us to align similar documents whose publication dates are distant, avoiding align-ments of unrelated documents with common words. We assessed the quality of the proposed corpus using a five-level relevance scale. We also extracted the word associations from the corpus and used these associations as translation knowledge in cross language information r etrieval. Experiment results show con-siderable improvement in performance of CLIR compared to the same method extracting translation knowledge from the existing Persian-English comparable corpus [6]. We also set up an experiment to combine a dictionary and the trans-lation knowledge extracted from Wikipedia [12] with extracted associations from the corpus. Using the three resources, we achieved up to 58 . 8% of MAP com-pared to the monolingual baseline and were able to improve the MAP of the method using the same dictionary and extracted knowledge from Wikipedia [12] by 14 . 8% that shows the high quality of the extracted associations.
In future works, we are going to construct a comparable corpus using other resources like web pages instead of the n ews collections. We will also try to construct a bigger corpus that covers more topics. We are going to focus on extraction of translation knowledge from the corpus and improving the quality of the translations using other useful resources like the web pages and search engine results. We will also focus on CLIR task to improve the performance of Persian-English cross language IR.
 Acknowledgments. This research is partially supported by Iran Telecommu-nication Research Center (ITRC).

