 Extracting semantic relations between entities from texts is one of the main tasks in text mining, and the unsupervised approach can find unknown relations without human input and is widely applicable to new domains. Unsupervised re-lation extraction (URE) can be viewed as a combination of two steps: extracting relational tuples from texts, and clustering those that have synonymous rela-tions. (The tuples consist of two entities and a word sequence that indicate the relation between them.) This clustering step involves the main issue of URE. The tuples that mean  X  X ollaboration between two musicians M1 and M2 X , for example, are described with different word sequences, such as (M1, collaborated with ,M2),(M1, worked alongside , M2) or (M1, partnered with ,M2).Wewant to cluster such tuples into a single cluster which indicates the  X  X ollaboration X  relation.

In existing URE methods [5,8,14] synonymous tuples are found by looking for tuples having the same pair of entities. Tuples with the same entity pair, however, do not necessarily have the same meaning. Moreover, multiple appearances of the same entity pair are so rare in a target corpus that the exiting methods need a huge amount of corpus data (in [14], e.g., 2.1 million tuples) and the extracted relations tend to be general. These methods are thus not suitable when we are interested in a specific domain, such as a long tail one or a technical one in which a relatively small amount of text is available, and want to extract domain-specific relations.

We have therefore developed a URE that works well for a small specific corpus in which almost all the entity pairs of tuples appear only once. Our URE doesn X  X  use the frequently appearing entity pairs used by the existing URE methods. We extract features of the tuples mainly fr om the word sequences. Because only a few features are extracted from the word sequence, however, we have sparse feature vectors with elements that are mostly zer os. It is widely known that clustering sparse vectors does not work well.

Our approach to the sparseness is dimension reduction, which projects the original feature vectors in a high-dimensional space to compressed feature vectors in a lower-dimensional space and conseque ntly mitigates the sparseness. Directly applying a standard dimension reduction such as Latent Semantic Indexing (LSI) [3] to feature vectors of tuples does not work well, however, because the feature vectors of tuples are sparser than those of targets that often used in natural language processing (e.g., documents).

Our solution is to leverage contexts in the whole corpus together with dimen-sion reduction. Unlike existing URE methods, our URE utilizes the sentences from which a tuple is not extracted. A part of features are extracted from words, and the features appear not only in tuples but in contexts of such sentences. The contexts have information o n meanings of such features [4] and thus help to esti-mate the compressed feature vectors that represent meanings of the tuples. The contexts in this case are captured with f eature co-occurrences, which indicate that two features appear in a single sentence, and are leveraged by a probabilis-tic matrix factorization (PMF) that jointly factorizes the feature vector matrix (each row of which is the feature vector ) and the feature co-occurrence matrix (each element of which is the frequency of feature co-occurrences). A part of estimated parameters of the PMF are us ed as the compressed feature vectors. We call this PMF context-leveraged PMF (CL-PMF) and experimentally show that the feature vectors compressed by CL-PMF are clustered with higher purity than those compressed by existing dimension-reduction methods. Our dimension reduction with matrix factorization is related to LSI [3], which uses singular value decomposition (SVD) to factorize a document-term matrix into low rank matrices and obtains a low-rank approximation. The row vector of the left/right singular matrix represents a latent concept (a large portion of information) of the corresponding document/term and is therefore used as its compressed vector in the low-d imensional space. Analysis tasks such as document clustering and word sense disambiguati on can be performed with the compressed vectors.

Dimension-reduction methods for semantic relations between entities were proposed in [13]. In that work, entiti es (assumed to be general nouns) were extended with a thesaurus and queried t oasearchenginetogatherfeatures. The resultant feature vectors are compressed with LSI. This work shows the effectiveness of a dimension reduction. T he method uses rich linguistic resources not expected in a specific domain, whereas our method uses the feature co-occurrences in the target corpus, which are naturally obtained.
 Word sense disambiguation using word co-occurrences was proposed in [11]. Each element of a word co-occurrence mat rix is the frequency with which two words appear in a single sentence in a cor pus. The similarities between meanings of words can be calculated with the corresponding row vectors of the matrix. The vectors are compressed with LSI, and th e compressed vector, which represents the latent concept of the word, is used for the word sense disambiguation. In a similar way, CL-PMF uses a feature co-occurrence matrix to estimate the latent concepts of features (this is explained in Section 3.3.1).

Joint matrix factorizations related to CL-PMF are the link-content matrix factorization using link information for Web page classification [15] and the joint matrix factorization used with collaborative filtering using social connections to make recommendations [6]. While these methods model two matrices of the same size, in our model one matrix is bigger than the other. As explained in Section 3.3.2, this size difference plays an important role to exploit kinds of features that appear not in extracted tuples but in the target corpus. In addition, we introduce a full Bayesian approach, which are reported to boost the performance of matrix factorization [10]. The outline of our URE is as follows: First we discover relations from texts and extract relational tuples (S ection 3.1). Then we extrac t features from the tuples and construct the feature vectors (Section 3.2). We then use CL-PMF to com-press the feature vectors into a low-dimensional space (Section 3.3). Finally, we use a conventional clustering method to cluster the compressed feature vectors. 3.1 Relation Discovery The relations that we want to extract take the form of a tuple t =( e 1 ,s,e 2 ), where e 1 and e 2 indicate entities, and s is a word sequence that indicates the relation between them. The following process for extracting tuples was inspired by the full-scale relation extraction in [1,2].

The sentences in the target corpus are first parsed with a dependency parser to obtain their dependency graph repres entations. For the parsed sentences, the system finds all the named entities, which are used as entities, e 1 or e 2 .When a sentence contains more than two entities, the system detects the dependency path (path of the dependency graph) connecting two entities. The system then extracts tuples when the dependency path from e 1 to e 2 does not contain a sentence-like boundary, such as a relative clause, and the number of words be-tween e 1 and e 2 is less than 10. The word sequence s is extracted as the part of the sentence based on the parse structure between e 1 and e 2 . Two examples of extracted tuples are ( Sasha Allen, has also toured with, Alicia Keys )and( Kings Cross, shared the stage with, Kasabian ). 3.2 Feature Extraction The following three kinds of features are extracted from a tuple.
 Unigram Feature: A unigram feature, which is extracted from a word, is a pair consisting of the word X  X  stemmed form and part of speech. We extract this feature from each word in the word sequence s . The unigram features of stop words (common words that are filtered out prior to a processing) on the dependency path from e 1 to e 2 are used to capture the word X  X  impact (e.g.,  X  studied  X  X nd X  studied at  X ), and stop words not on the path are filtered out. Bigram Feature: A bigram feature is a conjunction of the two unigram features alongside the dependency path from e 1 to e 2 .
 Entity Tag Feature: An entity tag feature consists of named entity tags for the two entities. The tags are tagged by a named entity tagger. The used tags are PERSON, LOCATION, and ORGANIZATION.

We use a vector space model to represent a tuple with these features. An element of a feature vector is a frequen cy of the corresponding feature, and a large number of kinds of features are extracted from tuples in a target corpus. The feature vector of a tuple is therefore high-dimensional and sparse. 3.3 CL-PMF for Dimension Reduction Here we first present an intuitive description of CL-PMF. Then we define a feature co-occurrence matrix, describe CL-PMF, and describe its inference. 3.3.1 Intuitive Description of CL-PMF Let X = { x ij ; i =1 ,...,N,j =1 ,...,M } be a feature vector matrix, where the row vector x i  X  X  M corresponds to the feature vector of the i -th tuple, N is the number of tuples, and M is the number of kinds of features for X . Under the assumptions of linear transform, dimension reduction can be expressed as the matrix factorization, where we define D ( D&lt;M ) as the number of dimensions of a low dimensional space, U is the D  X  N matrix in which the column U i  X  X  D corresponds to the compressed feature vector of the i -th tuple, and V is the D  X  N matrix in which the column V j  X  X  D corresponds to the compresse d vector representing the latent concept of the j -th kind of feature. This matrix factorization represents a low rank approximation like LSI. In our task, matrix U and matrix V are not well estimated because t he feature vector matrix X is sparse in that each row x i is sparse as explained in Section 3.2 and the number of the rows of X (i.e., the number of feature vectors) is small because the appearances of tuples are not frequent in a small corpus.

To resolve the shortage of the feature vectors we utilize the contexts of all sentences in the corpus. We use a feature vector matrix of sentences X s ,where each row is the feature vector of each se ntence in the corpus. The features of a sentence are unigram features extracted from words in the sentence. Note that the unigram features used in X is also used in X s . Consider the following matrix factorization: where U s and V s are defined in the same way as they are in Eq. (1). In this case, V s is well estimated because we have an enough number of the sentences. Now, we regard a tuple as a short sentence and use V s , which represents the latent concepts of features estimated by the sentences, for the tuples. To utilize V s for V , we combine the two matrix factorizations by sharing the columns corresponding to the same unigram features in V and V s . However, we don X  X  need U s in Eq. (2). For a compact parameter representation, we calculate as follows: where we assume U T s U s = I ( I is the identity matrix), using a freedom of the matrix factorization in Eq. (2) and for example, SVD meets the assumption. We use X T s X s instead of X s and consequently we do not have to consider U s . X s X s is a feature co-occurrence matrix, wh ere each element is a frequency of corresponding feature co-occurrences. We combine the two matrix factorizations in Eq. (1) and Eq. (3). Since the shared columns of V are well estimated, we expect U and the rest of V (i.e., the columns of bigram features and entity tag features) to be well estimated, too. 3.3.2 Feature Co-occurrence Matrix We incorporate bigram feature s and entity tag features in X T s X s and define the feature co-occurrence matrix F = { f jk ; j =1 ,...,L,k =1 ,...,L } ,where L ( L  X  M ) is the number of kinds of features for F . F is a symmetric matrix and both rows and columns of F correspond to kinds of features. The first M kinds of features of F are same as those of X and the rest kinds of features correspond to kinds of unigram features not in any of tuples (i.e., kinds of uni-gram features not used in X , but used in X T s X s ). Note that L is greater than M because to capture the contexts in the whole corpus, we additionally use new kinds of unigram features not in any of the tuples. An element corresponding to two unigram features describes a frequency with which the two unigram features appear in a single sentence. For the count, all the sentences in the target corpus are used. Elements not corresponding to two unigram features (i.e., elements related to bigram features and entity tag features) are set at zero. The size of V is increased to L with the increase of kinds of features. Here are some samples of the feature co-occurrences that extracted from the sentence  X  Spector had recently produced his solo single  X : (had/VB, recent/NN), (had/VB, produc /VB), (solo/JJ, singl/NN).

We apply the weighting function W ( x )=log(1+ x ) [7] to the values of the elements of the data matrices, X and F . 3.3.3 Context-Leveraged PMF CL-PMF models the combination of the matrix factorizations in Eq. (1) and Eq.(3). CL-PMF is a probabilistic model over observed elements in X and F with Gaussian observation noise. A zero value in the matrices is interpreted to be un-observed. We use a full Bayesian approach [10].

CL-PMF is given by: p ( X , F | U , V , X  X , X  F )= p ( f jk | V j , V k , X  F )= N f jk | V T j V k , X  F , where I X ij is the indicator variable of X which is equal to 1 if an element x ij is denotes the Gaussian distribution with mean  X  and precision (inverse of variance)  X  ,  X  X is the precision of all the elements of X ,and  X  F is the precision of all the elements of F . The graphical model for CL-PMF is shown in Fig. 1. Both X and F are conditioned on V . This corresponds to the share of V between the two matrix factorizations explained in Section 3.3.1.

The prior distributions are as follows: p ( U |  X  U , A U )= where  X  U and A U are the mean and the precision of the Gaussian distribution and Gam ( x | k,  X  )denotes the Gamma distribution with shape parameter k and scale parameter  X  . Because of the symmetry of the model, the prior distribution of V is identical to that of U . The prior of  X  F is same as the prior of  X  X .These priors are conjugate priors for the model described above.

Given the data and the prior distributions, we can now write the posterior distribution, which is the distribution of parameters conditioned on observed data and hyper parameters. The posterior distribution is given by the prior distributions. We use the expectation of U i over the posterior distri-bution as the compressed feature vector of the i -th tuple. 3.3.4 Inference We use Gibbs sampling for inferring the parameters. Gibbs sampling cycles through the parameters, sampling each from its distribution conditioned on the current values of the other parameters . Because of the use of conjugate priors for the parameters, the posterior distributions are easy to sample from.
First, the posterior over parameter U is given. Each U i is conditionally inde-pendent. The posterior distribution over U i is given by: where Next, the posterior distribution of V j is as follows: where Here V  X  j = { V k | k = j } and I X ij is extended in such a way that it is equal to 0 if i or j exceeds the size of the matrix.

Next, we present the posterior distributions over the precisions of the data: where and where That completes our presentation of the Gibbs sampling algorithm for CL-PMF. We examined the performance of CL-P MF, and compared it with the perfor-mance of existing dimension-reduction methods, by measuring the performance for the clustering of the compressed feature vectors. We assume that the better a dimension-reduction method compresse d the feature vector, the better clusters are obtained with the compressed vectors. We used k-means clustering and used cosine similarity to measure the distance between two vectors. Since clustering performance depends on the number of clusters K , we ran k-means, varying the cluster number K .

In our experiments, we didn X  X  directly cluster the tuples but instead defined the sequence of words on the dependency path from e 1 to e 2 as a relevant word sequence and clustered the relevant w ord sequences. We assume that the tuples sharing a relevant word sequence expres s a same relation. A feature vector of a relevant word sequence is the sum of the f eature vectors of the tuples having the relevant word sequence.
 4.1 Annotated Corpus We performed clustering on three datasets: the published dataset SENT500 [2], a set of articles from the  X  X usicians X  category in Wikipedia, and a set of articles from the  X  X ompanies X  category in Wikipedia. Because there X  X e few published dataset for our task, we manually built the second and the third dataset. The sizes of the datasets are shown in Table 1. Our target corpus is a small specific corpus in which almost all the entity pairs of tuples appear once, and each of the datasets from Wikipedia is such a corpus. In the  X  X usicians X / X  X ompanies X  dataset, the 98.6%/98.8% of the entity pairs appear once.
 SENT500: The dataset consists of 500 sentences. Each sentence has an identi-fied entity pair in one of the four relations:  X  X cquisition X ,  X  X irthplace X ,  X  X nven-tor X ,  X  X in (award) X  1 . The four relations are used as the gold standard. From SENT500 we obtained 328 relevant word sequences. Because we don X  X  have an original corpus from which the sentences in SENT500 have extracted, we don X  X  have enough sentences to gather feature co-occurrences. We used the sentences in the Wikipedia articles corresponding to the 50 entities identified in SENT500. (SENT500 includes the 50 unique entities and we used all of them.)  X  X usicians X  and  X  X ompanies X : We applied the relation discovery method described in Section 3.1 to the Wikipedia articles in the  X  X usicians X  category and the  X  X ompanies X  category for evaluation in a realistic situation. In the case of the Wikipedia articles, one entity of a pair must be the title of an article, and the other needs to be an anchor text that links to another article and start with a capital letter. The relevant word sequences from both categories were labeled by hand for evaluation of clustering. For each dataset we prepared 10 labels of relations likely to have a large number of relevant word sequences in each corpus. We then labeled the relevan t word sequences in one of the 10 labels. The labels differed between  X  X usicians  X  and  X  X ompanies. X  The used labels are shown Table 2. Because of the limited availability of space, we omit the definition of the labels. In the  X  X usicians X / X  X ompanies X  category we used the 4,997/4,063 articles and labeled 2,159/861 relevant word sequences. 4.2 Evaluation We used as a measure of clustering performance the purity defined as follows: where N most c is the number of elements of the label that is most frequent in cluster c , N c is the number of elements in cluster c , and the summation is over all the clusters. High purity in a small number of clusters indicates a good clustering performance. Our goal is to obtain a small number of clusters with high purity. 4.3 Methods We used the following (compressed) feature vectors for input of k-means: Raw Feature Vector (RFV): We used the raw feature vector x i as a baseline and used the weighting function W ( x ). The URE proposed in [5] employs this method.
 Second-Order Feature Vector (2ndOrder): The feature vector was con-structed by integrating the feature co-occurrence matrix in the feature vec-tor x i [12]. The feature vector x i =( x ij ; j =1 ,...,L ) is defined as x ij = I occurrence matrix F . We applied W ( x ) to each element of the feature vector x i . LSI: LSI inputs the feature vector matrix X and outputs the row vectors of the left singular matrix corresponding to the top D singular values. These row vec-tors are used as the compressed feature vectors.
 PMF: We applied the standard PMF [9] to the feature vector matrix X and used the expectation of U i over the posterior as the compressed feature vector. CL-PMF: We chose the prior distributions for U and V with zero mean,  X 
U =  X  V = 0 , and precision matrix, A U = A V = a I .Thevalueof a deter-mined by the experimental optimization was 15 for SENT500 and was 60 for  X  X usicians X  and  X  X ompanies X . We discuss the value of the precision a in Sec-tion 4.5. For the prior distributions of  X  X and  X  F we set k X = k F =1and  X  X =  X  F =1.

The dimension-reduction methods described above input the feature vectors of the relevant word sequences includi ng unlabeled ones, while we use labeled ones for clustering. We set D to 8 for SENT500, and to 20 for  X  X usicians X  and  X  X ompanies X . We discuss the value of D in Section 4.5.
 4.4 Results and Discussion In Fig. 2 the purities obtained with each of the methods are, for each dataset, plotted against the number of clusters. CL-PMF achieves the highest purity for each dataset and for each number of clusters. Note that CL-PMF outperforms others in a small number of clusters. The performances of LSI and PMF are infe-rior to or not far superior to those of RFV. This implies that a direct application of a standard dimension-reduction method doesn X  X  work with the sparse feature vectors of the tuples, whereas CL-PMF works well because it uses the informa-tion of the feature co-occurrences in the e ntire corpus. CL-PMF performs better than 2ndOrder, which also utilizes the inf ormation of the feature co-occurrences. 2ndOrder is inferior to RFV because 2 ndOrder is believed to be suffered from noise. CL-PMF leverages the feature co-occurrences better than 2ndOrder by introducing the compressed feature vectors U and assuming observation noise.
The purities of the CL-PMF are not relatively high in the large numbers of clusters in SENT500, where the clustering problem is so simple that RFV performs well. The feature co-occurrences are believed to have information for estimating the compressed feature vectors but also contain noise. In this simple situation, the feature co -occurrences can become nois e rather than a help for the estimation. 4.5 Parameters for CL-PMF We performed experiments to see how the clustering was affected by the di-mension D of the compressed feature vectors and the precision a of the prior distribution, using the  X  X usicians X  dataset and setting K to 10.

The left panel in Fig. 3 shows that the purity peaks as the dimension D increases. The number of the dimension is believed to indicate the granular-ity of the semantic information. A small D prevents CL-PMF from capturing sufficient detail for accurate clustering. On the other hand, a large D causes over-estimation, or excessive noise adaptation. The full Bayesian approach mitigates that.

The right panel in Fig. 3 shows a high value for the precision of the prior distributions, a , performs better than a low value. The prior distribution that has a high value of the precision keeps the compressed vectors U i around the zero mean of the prior. This heavily takes into account their directions rather than their lengths when the compressed feature vectors are estimated. The similarities between the compressed vectors are measured well by the cosine similarity that we employed. In this paper we have proposed CL-PMF for URE. CL-PMF compresses dimen-sions of sparse feature vectors of relational tuples, utilizing feature co-occurrences in the whole corpus including sentences where tuples are not extracted. Since our method doesn X  X  assume the redundancy extracted from a huge amount of corpus, it works well for a small corpus such as that of a long tail domain. The experimental results show that the dimension reduction with CL-PMF is more effective for clustering of the spars e feature vectors of tuples than existing dimension-reduction methods and baseline methods.

