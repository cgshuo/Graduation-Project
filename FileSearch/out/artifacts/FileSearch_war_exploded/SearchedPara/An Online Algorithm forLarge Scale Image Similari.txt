
Mountain View, CA gal@google.com Learning a pairwise similarity measure from data is a fundam ental task in machine learning. Pair a user may wish to find images that are similar to (but not ident ical copies of) an image she has; a user watching an online video may wish to find additional vid eos about the same subject. In all of an image, in an enormous search space. Learning a relatedn ess function from examples could be a useful tool for such tasks.
 A large number of previous studies of learning similarities have focused on metric learning, like in the case of a positive semidefinite matrix that defines a Mah alanobis distance [19]. However, is useful for reducing overfitting and improving generaliza tion. However, when sufficient data is available, as in many modern applications, adding positive semi-definitiveness constraints is very image search engine.
 of magnitude larger than current published approaches. Thr ee components are combined to make two images p W is not required to be positive, or even symmetric. Second we u se a sparse representation of we developed, OASIS, Online Algorithm for Scalable Image Similarity learning , is an online dual approach based on the passive-aggressive algorithm [2]. It minimizes a large margin target function a small fraction of the training pairs.
 with computation times that are shorter by an order of magnit ude. For web-scale datasets, OASIS can be trained on more than two million images within three da ys on a single CPU. On this large neighbors of a given image are semantically relevant to that image. of samples and the number of features, by using fast online up dates and a sparse representation. Formally, we are given a set of images P , where each image is represented as a vector p  X  R d . We assume that we have access to an oracle that, given a query image p images, p + we could write that relevance ( p that a numerical value of the similarity is available, relevance ( p form of supervision, and only assume that some pairs of image s can be ranked by their relevance to a query image p same class of images as the query image, or reflect any other se mantic property of the images. Our goal is to learn a similarity function S larity scores to the pairs of more relevant images (with a saf ety margin), In this paper, we consider a parametric similarity function that has a bi-linear form, with W  X  R d  X  d . Importantly, if the image vectors p non-zero entries k computed very efficiently even when d is large. Specifically, S of in Eq. (1), we define a global loss L the training set: L l
W ( p i , p To minimize the global loss L family of algorithms [2]. First, W is initialized to the identity matrix W 0 = I algorithm iteratively draws a random triplet ( p with a soft margin: where kk optimize a trade-off between staying close to the previous p arameters W i  X  1 and minimizing the loss on the current triplet l To solve the problem in Eq. (3) we follow the derivation in [2] . When l that W i = W i  X  1 satisfies Eq. (3) directly. Otherwise, we define the Lagrangi an where  X   X  0 and  X   X  0 are the Lagrange multipliers. The optimal solution is obtai ned when the current step V gradient V L  X  back into the Lagrangian in Eq. (4), we obtain L (  X  ) =  X  1 small cumulative online loss, and selecting the best W set was shown to achieve good generalization [2].
 It should be emphasized that OASIS is not guaranteed to learn a parameter matrix that is positive, or even symmetric. We study variants of OASIS that enforce sy mmetry or positivity in Sec. 4.3.2. similarity learning approaches. The first approach, learni ng Mahalanobis distances, can be viewed Euclidean distance is defined among pairs of objects. Such ap proaches include Fisher X  X  Linear Dis-criminant Analysis (LDA), relevant component analysis (RC A) [1], supervised global metric learn-ing [18], large margin nearest neighbor (LMNN) [16], and met ric learning by collapsing classes [5] (MLCC). Other constraints like sparseness are sometimes in duced over the learned metric [14]. See also a review in [19] for more details.
 The second family of approaches, learning kernels, is used t o improve performance of kernel based kernels [11] where the weights are learned from data. In some applications this was shown to be also studied in the context of dimensionality reduction, as in [8].
 Table 1: OASIS: Successful cases from the web dataset. The re levant text queries for each image are shown beneath the image (not used in training). distance, imposing positive definiteness constraints, whi ch makes the algorithm more complex and throughout could be detrimental.
 Learning a semantic similarity function between images was also studied in [13]. There, semantic grows with the number of semantic classes. 2.7 million images collected from the web. Then, to quantita tively compare the precision of OASIS with other, small-scale metric-learning methods, we teste d OASIS using Caltech-256 , a standard machine vision benchmark.
 Image representation . We use a sparse representation based on bags of visual words [6]. These extracted by dividing each image into overlapping square bl ocks, representing each block by edge word was present in it, yielding vectors in R d with an average of 70 non-zero values. same-class images among the top k images (the k nearest neighbors) is computed, and then averaged across test images. We also calculated the mean average precision (mAP), a measure that is widely used in the information retrieval community. 4.1 Web-Scale Experiment We first tested OASIS on a set of 2.7 million images scraped fro m the Google image search engine. We collected a set of  X  150K anonymized text queries, and for each of these queries, we had access to a set of relevant images. To compute an image-image releva nce measure, we first obtained mea-sures of relevance between images and text queries. This was achieved by collecting anonymized clicks over images collected from the set of text queries. We used this query-image click counts C (query,image) to compute the (unnormalized) probability t hat two images are co-queried as Rel-evance(image,image) = C T C . The relevance matrix was then thresholded to keep only the t op 1 0.4 million images. The number of training iterations (each corresponding to sampling one triplet) was selected using a second validation set of around 20000 im ages, over which the performance CPU of a standard modern machine.
 Table 1 shows the top five images as ranked by OASIS on two examp les of query-images in the test set. In these examples, OASIS captures similarity that goes beyond visual appearance: most top ranked images are about the same concept as the query image, e ven though that concept was never provided in a textual form, and is inferred in the viewers min d ( X  X og X ,  X  X now X ). This shows that queries are not explicitly used during training.
 benchmark, by asking human evaluators to mark if a set of cand idate images were semantically the top-10 images ranked by OASIS, mixed with 10 random image s. Given the relevance ranking from 30 evaluators, we computed the precision of each OASIS r ank as the fraction of people that marked each image as relevant to the query image. On average a cross all queries and evaluators, OASIS rankings yielded precision of  X  40% at the top 10 ranked images.
 As an estimate of an  X  X pper bound X  on the difficulty of the task , we also computed the precision obtained by human evaluators: For every evaluator, we used t he rankings of all other evaluators as ground truth, to compute his precision. As with the ranks o f OASIS, we computed the fraction of evaluators that marked an image as relevant, and repeated this separately for every query and human evaluator, providing a measure of  X  X oherence X  per que ry. Fig. 1(a) shows the mean precision obtained by OASIS and human evaluators for every query in our data. For some queries OASIS achieves precision that is very close to that of the mean huma n evaluator. In many cases OASIS achieves precision that is as good or better than some evalua tors. (a) (b) human evaluators as a ground truth. (b) Comparison of the runtime of OASIS and fast-LMNN[17], subsets of the web data. However LMNN scales quadratically w ith the number of samples, hence is three times slower on 60K images, and may be infeasible for ha ndling 2.3 million images. We further studied how the runtime of OASIS scales with the si ze of the training set. Figure 1(b) shows that the runtime of OASIS, as found by early stopping on a separate validation set, grows on a fast implementation of LMNN [17]. The LMNN algorithm sca les quadratically with the number linearly. This could be because MNIST has 10 classes only. (a) 10 classes (b) 20 classes (c) 50 classes Figure 2: Comparison of the performance of OASIS, LMNN, MCML , LEGO and the Euclidean bars are standard error of the means (s.e.m.), black dashed l ine denotes chance performance. 4.2 Caltech256 Dataset To compare OASIS with small-scale methods we used the Caltech256 dataset [7], containing im-ages collected from Google image search and from PicSearch.com . Images were assigned to 257 categories and evaluated by humans in order to ensure image q uality and relevance. After we have pre-processed the images, and filtered images that were too s mall, we were left with 29461 images in 256 categories. To allow comparisons with methods that we re not optimized for sparse represen-tation, we also reduced the block vocabulary size d from 10000 to 1000.
 We compared OASIS with the following metric learning method s. (1) Euclidean -The standard Euclidean distance in feature space (equival ent to using the identity matrix W = I ples are mapped to the same point, formulated as a convex prob lem. (3) LMNN [16] -learning a Mahalanobis distance for aiming to have the k -nearest neighbors of a given sample belong to the same class while separating different-class samples by a la rge margin. As a preprocessing phase, images were projected to a basis of the principal components (PCA) of the data, with no dimen-sionality reduction. (4) LEGO [9] -Online learning of a Mahalanobis distance using a Log-D et images from the same class were treated as similar. Each subs et was built such that it included 20 and 50 classes, each spanning the range of difficulties.
 ( Results reported below were obtained by selecting the best v alue of the hyper parameter and then training again on the full training set (40 images per class) .
 Figure 2 compares the precision obtained with OASIS, with th e four competing approaches. OASIS and on all four sets studied. LMNN performance on the training set was often high, suggesting that it overfits the training set, as was also observed sometimes b y [16].
 Table 2 shows the total CPU time in minutes for training all al gorithms compared, and for four subsets of classes at sizes 10, 20, 50 and 249. Data is not give n when runtime was longer than 5 days or performance was worse than the Euclidean baseline. F or the purpose of a fair comparison, we tested two implementations of OASIS: The first was fully im plemented Matlab. The second had the core loop of the algorithm implemented in C and called fro m Matlab. All other methods used code supplied by the authors implemented in Matlab, with cor e parts implemented in C. Due to the same time scale as all other algorithms. LEGO is fully imp lemented in Matlab. All other code was compiled (mex) to C. The C implementation of OASIS is sign ificantly faster, since Matlab does not use the potential speedup gained by sparse images.
 OASIS is significantly faster, with a runtime that is shorter by orders of magnitudes than MCML even on small sets, and about one order of magnitude faster th an LMNN. The run time of OASIS takes 100 M floats, or 0 . 4 Giga bytes of memory. (a) (b) Figure 3: (a) Comparing symmetric variants of OASIS on the 20-class subse t, similar results ob-tained with other sets. (b) mAP along training for three PSD projection schemes. 4.3 Symmetry and positivity The similarity matrix W learned by OASIS is not guaranteed to be positive or even symm etric. Some applications, like ranking images by semantic relevan ce to a given image query are known to be non-symmetric when based on human judgement [15]. Howeve r, in some applications symmetry discuss variants of OASIS that learn a symmetric or positive matrices. 4.3.1 Symmetric similarities A simple approach to enforce symmetry is to project the OASIS model W onto the set of symmetric matrices W  X  = sym ( W ) = 1 function S and used to derive an OASIS-like algorithm (which we name Dissim-Oasis ). The optimal update for this loss has a symmetric gradient V  X  i = ( p if
W 0 is initialized with a symmetric matrix (e.g., the identity) all W i are guaranteed to remain symmetric. Dissim-Oasis is closely related to LMNN [16]. This can be seen be casting th e batch objective of LMNN, into an online setup, which has the form err ( W ) =  X   X  S  X  SIS. All symmetric variants performed slightly worse, or eq ual, to the original asymmetric OA-ric OASIS actually converged to an almost-symmetric model ( as measured by a symmetry index 4.3.2 Positive similarity Most similarity learning approaches focus on learning metr ics. In the context of OASIS, when W is root of W , A T A = W can then be used to project the data into a new space in which th e Euclidean distance is equivalent to the W distance in the original space.
 We experimented with positive variants of OASIS, where we re peatedly projected the learned model position W = V D V T where V is the eigenvector matrix and D is a the diagonal eigenvalues for various values of t .
 to reduce overfitting, as can be observed by the slower declin e of the blue curve (upper smooth curve) compared to the orange curve (lowest curve). However , when projection is performed after many steps, (instead of continuously), performance of the p rojected model actually outperforms that estimating the positive sub-space is very noisy when on ly based on a few samples. Indeed, accurate estimation of the negative subspace is known to be a hard problem, in that the estimated the best performance in our experiments.
 An interesting alternative to obtain a PSD matrix was explor ed by [10, 9]. Using a LogDet diver-gence between two matrices D effect of using LogDet regularization in the OASIS setup. We have presented OASIS, a scalable algorithm for learning i mage similarity that captures both OASIS. First, using a large margin online approach allows tr aining to converge even after seeing a small fraction of potential pairs. Second, the objective f unction of OASIS does not require the level features which allows to compute scores very efficient ly.
 OASIS learns a class-independent model: it is not aware of wh ich queries or categories were shared dependent similarity models could improve precision. On th e other hand, class-independent models a useful tool to address real-world problems with a large num ber of classes.
 hashing could be used to speed up evaluation, but this is outs ide the scope of this paper. [2] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y . Singer. Online passive-aggressive [5] A. Globerson and S. Roweis. Metric Learning by Collapsin g Classes. NIPS , 18:451, 2006. [6] D. Grangier and S. Bengio. A discriminative kernel-base d model to rank images from text [7] G. Griffin, A. Holub, and P. Perona. Caltech-256 object ca tegory dataset. Technical Report [8] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality redu ction by learning an invariant map-[10] B. Kulis, M.A. Sustik, and I.S. Dhillon. Low-rank kerne l learning with bregman matrix diver-[12] W. S. Noble. Multi-kernel learning for biology. In NIPS workshop on kernel learning , 2008. [13] N. Rasiwasia and N. Vasconcelos. A study of query by sema ntic example. In 3rd International [14] R. Rosales and G. Fung. Learning sparse metrics via line ar programming. In Proceedings of [15] A. Tversky. Features of similarity. Psychological Review , 84(4):327 X 352, 1977. [19] L. Yang. Distance metric learning: A comprehensive sur vey. Technical report, Michigan State
