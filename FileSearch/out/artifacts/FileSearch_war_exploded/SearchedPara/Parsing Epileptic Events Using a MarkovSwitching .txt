 Drausin F. Wulsin wulsin@seas.upenn.edu Emily B. Fox ebfox@uw.edu Dept. of Statistics, University of Washington, Seattle, WA USA Brian Litt littb@mail.med.upenn.edu Despite over three decades of research, we still have very little idea of what defines a seizure. This ig-norance stems both from the complexity of epilepsy as a disease and a paucity of quantitative tools that are flexible enough to describe epileptic events but re-strictive enough to distill intelligible information from them. Much of the recent machine learning work in EEG analysis has focused on seizure prediction, (cf., D X  X lessandro et al., 2005; Mirowski et al., 2009), an important area of study but one that generally has not focused on parsing the EEG directly, as a human EEG reader would. Such parsings are central for di-agnosis and relating various types of abnormal activ-ity. Recent evidence shows that the range of epilep-tic events extends beyond clinical seizures to include shorter, sub-clinical  X  X ursts X  lasting fewer than 10 sec-onds (Litt et al., 2001). What is the relationship be-tween these shorter bursts and the longer seizures? In this work, we demonstrate that machine learning tech-niques can have substantial impact in this domain by unpacking how seizures begin, progress, and end. In particular, we build a Bayesian nonparametric time series model to analyze intracranial electroencephalo-gram (iEEG) data. We take a modeling approach sim-ilar to a physician X  X  in analyzing EEG events: look di-rectly at the evolution of the raw EEG voltage traces. EEG signals exhibit nonstationary behavior during a variety of neurological events, and time-varying au-toregressive (AR) processes have been proposed to model single channel data (Krystal et al., 1999). Here we aim to parse the recordings into interpretable re-gions of activity and thus propose to use autoregressive hidden Markov models (AR-HMMs) to define locally stationary processes. In the presence of multiple chan-nels of simultaneous recordings, as is almost always the case in EEG, we wish to share AR states between the channels while allowing for asynchronous switches. The recent beta process (BP) AR-HMM of (Fox et al., 2009) provides a flexible model of such dynamics: a shared library of infinitely many possible AR states is defined and each time series uses a finite subset of the states. The process encourages sharing of AR states, while allowing for time-series-specific variability. The BP-AR-HMM assumes independence between time series. In the case of iEEG, this assumption is al-most assuredly false. Fig. 1 shows an example of a 4x8 intracranial electrode grid and the residual EEG traces of 16 channels after subtracting the predicted value in each channel using a conventional BP-AR-HMM. While the error term in some channels remains low throughout the recording, other channels X  X specially those spatially adjacent in the electrode grid X  X ave very correlated error traces. We propose to capture correlations between channels by modeling a multi-variate innovations process that drives independently evolving channel dynamics. We demonstrate the im-portance of accounting for this error trace in predicting heldout seizure recordings, making this a crucial mod-eling step before undertaking large-scale EEG analysis. To aid in scaling to large electrode grids, we exploit a sparse dependency structure for the multivariate inno-vations process. In particular, we assume a graph with known vertex structure that encodes conditional in-dependencies in the multivariate innovations process. The graph structure is based on the spatial adjacencies of the iEEG channels, with a few exceptions to make the graphical model fully decomposable. Fig. 1 (bot-tom left) shows an example of such a graphical model over the channels. Although the relative position of channels in the electrode grid is clear, determining the precise 3D location of each channel is extremely diffi-cult. Unlike in scalp EEG or magentoencephalogram (MEG), which have generally consistent channel posi-tions from patient to patient, iEEG channels vary in number and position for each patient, impeding the use of alternative spatial and multivariate time series modeling techniques.
 It is well-known that the correlations between EEG channels usually vary during the beginning, middle, and end of a seizure (Schiff et al., 2005; Schindler et al., 2007). Prado et al. (2006) employ a mixture-of-expert vector autoregressive (VAR) model to describe the different dynamics present in seven channels of scalp EEG. We take a similar approach by allowing for a Markov evolution to an underlying innovations covariance state.
 An alternative modeling approach is to treat the chan-nel recordings as a single multivariate time series, per-haps using a switching VAR process as in (Prado et al., 2006). However, such an approach (i) assumes syn-chronous switches in dynamics between channels, (ii) scales poorly with the number of channels, and (iii) re-quires identical numbers of channels between patients to share dynamics between event recordings.
 We show that our model for correlated time series has better out-of-sample predictions of iEEG data than standard AR-and BP-AR-HMMs and demon-strate the utility of our model in comparing short, sub-clinical epileptic bursts with longer, clinical seizures. Our inferred parsings of iEEG data concur with key features hand-annotated by clinicians but provide ad-ditional insight beyond what can be extracted from a visual read of the data. The importance of our methodology is multifold: (i) the output is inter-pretable to a practitioner and (ii) the parsings can be used to relate seizure types both within and between patients even with different electrode setups. Enabling such broad-scale automatic analysis, and identifying dynamics unique to sub-clinical seizures, can lead to new insights in epilepsy treatments.
 Although we are motivated by the study of seizures from iEEG data, our work is much more broadly appli-cable in time series analysis. For example, perhaps one has a collection of stocks and wants to model shared dynamics between them while capturing changing cor-relations. The BP-AR-HMM was applied to the anal-ysis of a collection of motion capture data assuming independence between individuals; our modeling ex-tension could account for coordinated motion with a sparse dependency structure between individuals. Re-gardless, we find the impact in the neuroscience do-main to be quite significant. Observation model Consider an event, a seizure for example, comprised of N univariate time series, which in our case are the voltage measurements of N different EEG electrode channels. We assume that each time series in an event contains T scalar observa-tions, y ( i ) t . We model y ( i ) t as an order r AR-HMM: where z ( i ) t denotes the dynamical state of channel i at time t ,  X  previous state z ( i ) t  X  1 , a k the r AR coefficients associated with channel state k , and Importantly, our channels do not evolve independently. We capture the channel correlations via the driving in-novations process t = [ (1) t ,..., ( N ) t ] T . In particular, we assume event-state-specific correlations via where Z t denotes a Markov-evolving event state, which is distinct from the individual channel states z ( i ) t , and  X  denotes the event state transition distribution. The flexibility introduced by the event state is particularly important in applications like seizure modeling, where the channels may display one innovation covariance before a seizure (e.g., relatively independent and low-magnitude) but quite a different covariance during a seizure (e.g., correlated, higher magnitude).
 Emission parameters To scale to large numbers of electrodes, and to incorporate the physical relation-ships of the channels, we define a sparse channel de-pendency structure by introducing a graphical model G and specifying a hyper-inverse Wishart (HIW) prior on  X  l . The HIW prior (Dawid &amp; Lauritzen, 1993) enforces the hyper-Markov conditions specified by G , leading to conditional independencies in t (and thus in y t ). The AR coefficients a k are given a multivariate normal prior. Together, we have Here, b 0 denotes the degrees of freedom and D 0 the scale matrix. We consider m = 0 throughout.
 For compactness, we sometimes alternately write where y t is the concatenation of N channel observa-tions at time t and z t is the vector of channel states. One can think of this process as a factorial HMM (Ghahramani &amp; Jordan, 1997) since we have N + 1 in-dependently evolving Markov chains that jointly gen-erate our observation vector y t . However, here we have a sparse dependency structure in how the Markov chains influence a given observation y t , as induced by the conditional independencies in t . See Fig. 1 (right). Feature constrained channel transition distri-butions A key goal in modeling the event is to cap-ture shared dynamics across the N related time series (channels). Each channel exhibits some subset of a shared library of AR coefficients { a k } . Let f ( i ) be a binary feature vector associated with channel i with f k = 1 indicating that channel i uses the dynamic a . The BP-AR-HMM of Fox et al. (2009) provides our sought after framework for defining such a fea-ture model in order to constrain a set of AR-HMM transitions. In particular, through employing a beta process prior (Thibaux &amp; Jordan, 2007), the BP-AR-HMM allows for an infinite library of AR parameters and encourages each time series to use a sparse subset of these parameters with a flexible sharing pattern. More formally, in our scenario the feature assignments f k and their corresponding parameters a k are gener-ated by an underlying beta process random measure:
B  X  BP(1 ,B 0 ) , B = B defines an infinite collection of feature inclusion probabilities  X  k and AR coefficients a k  X   X , with B 0 a base measure on our parameter space  X . The resulting feature vectors f ( i ) constrain the set of available states z t can take by constraining the transition distribu-tions,  X  ( i ) j , to be 0 when f ( i ) k = 0. In particular, we use f ( i ) along with a set of gamma random variables, to produce the desired transition distribution  X  ( i ) j from state j to state k ,  X  jk  X  Gamma(  X  c +  X  j,k  X  c ) ,  X  where  X  denotes the Hadamard (element-wise) product and  X  j,k the Kronecker delta. The positive elements of  X  j can also be thought of as a sample from a Dirichlet distribution with only K ( i ) dimensions, where K ( i ) = P The parameter  X  c encourages self-transitions, as in the sticky HDP-HMM (Fox et al., 2011a).
 Unconstrained event transition distributions We assume a Bayesian nonparametric formulation for the Markov event state process { Z t } , as well, by tak-ing  X  to be as in the HDP-HMM (Fox et al., 2011a; Teh et al., 2006). For simplicity, we consider the weak limit approximation (Ishwaran &amp; Zarepour, 2002): where e l is the l th column of identity and L is as-sumed much greater than the expected number of states. Again, the sticky parameter  X  e promotes self-transitions, reducing state redundancy.
 Our resulting structured Bayesian nonparametric fac-torial HMM is depicted in the graphical model of Fig. 1. We refer to this model as the HIW-spatial BP-AR-HMM to denote the dependencies introduced via the innovations process. We note that the infi-nite factorial HMM of Van Gael et al. (2008) consid-ers a very different structure, allowing for an infinite collection of chains each with a binary state space. The infinite hierarchical HMM (Heller et al., 2009) also considers infinitely many chains with finite state spaces, but with constrained transitions between the chains in a top down fashion. The infinite DBN of Doshi-Velez et al. (2011) considers more general con-nection structures and arbitrary state spaces. Alterna-tively, the graph-coupled HMM of Dong et al. (2012) allows graph-structured dependencies in the underly-ing states of some N Markov chains. Here, we con-sider a finite set of chains with infinite state spaces that evolve independently and instead capture sparse dependencies in the observations via the innovations driving the AR dynamics. Although the components of our model related to the individual channel dynamics are similar to those in the Algorithm 1 Outline of one MCMC iteration for channels i = 1 ,...,N do end for for active features k  X  X  k | P i f ( i ) k &gt; 0 } do end for sample event state sequence, sample event transition parameters, for event states l = 1 ,...,L do end for BP-AR-HMM, our posterior computations are signif-icantly different due to the coupling of the Markov chains via the observations y t . In the BP-AR-HMM, conditioned on the feature assignments, each time se-ries is independent. Here, however, we are faced with a factorial HMM structure and the associated chal-lenges. For example, consider the observation model of Eq. (4). Even conditioned on the event sequence Z 1: T and model parameters, we cannot analytically marginalize the state vector sequence z 1: T (e.g., via a forward-backward algorithm) since the state space of z t is exponentially large. Luckily, the scale of these challenges is mitigated by our underlying graph struc-ture. Conditioned on channel sequences { z ( j ) 1: T } j  X  i can marginalize z ( i ) 1: T ; because of the graph structure, we need only condition on a sparse set of other chan-nels (i.e., neighbors in the graph denoted here by i 0 ). Of course, the dependencies also have to be accounted for in sampling the dynamic parameters a k .
 Algorithm 1 provides a high level overview of the steps involved in one iteration of MCMC sampling, with more details below and complete derivations provided in Supplement B. For brevity, we omit the hyperpa-rameters from the conditioning set throughout. Individual channel variables We harness the fact that we can compute the marginal conditional likeli-hood given f ( i ) and the neighborhood set of other chan-nels z ( i 0 ) 1: T in order to block sample { f ( i ) is, we first sample f ( i ) marginalizing z ( i ) 1: T sample z ( i ) 1: T given the sampled f ( i ) . Sampling the ac-tive features f ( i ) for channel i follows as in Fox et al. (2009), using the Indian buffet process (IBP) (Griffiths &amp; Ghahramani, 2005) predictive representation associ-ated with the beta process, but using a likelihood term that conditions on neighboring channel state sequences z 1: T and observations y on the event state sequence Z 1: T to define the sequence of distributions on the innovations. Generically, this yields (omitting the dependency on  X  ( i ) , { a k } , {  X  Here, the first term is given by the IBP prior and the second term is the marginal conditional likelihood as derived in Supplement A. The quantity F  X  ik defines the indicators for features other than k for time series other than i . Supplement B contains details on the feature sampling of Fox et al. (2009).
 Conditioned on f ( i ) , we block sample the state se-quence z ( i ) 1: T by first calculating backward messages  X  for t = 1 ,...,T and then forward sampling (again omitting dependency on  X  ( i ) , { a k } , {  X  l } ), where u t denotes the likelihoods under each of the available K ( i ) channel states. Recall that  X  ( i ) function of f ( i ) and  X  ( i ) . Details are in Supplement B. For sampling the transition parameters  X  ( i ) , we fol-low the correction described by Hughes et al. (2012, Supplement) and sample from the posterior given by where n ( i ) jk denotes the number of times channel i tran-sitions from state j to state k . We sample  X  ( i ) C j  X   X  where n j gives the transition counts from state j . AR coefficients Each observation y t is generated based on a vector of AR parameters [ a Thus, sampling a k involves conditioning on { a k 0 } k 0 6 = k and disentangling the contribution of a k on each y t . As derived in Supplement B, a where  X  The vectors k + and k  X  denote the indices of chan-nels assigned and not assigned to state k at time t , respectively. We use these to index into the rows and columns of the vectors t , y t , and matrix  X  Z t . Each column of matrix  X  Y ( k + ) t is the previous r observations for one of the channels assigned to state k at time t . Event variables Conditioned on the channel state sequences z 1: T and AR coefficients { a k } , we can com-pute an innovations sequence as t = y t  X  A z t e Y t These innovations are the observations of the HMM of Eq. (2). Conditioned on the truncated HDP-HMM event transition distributions  X  and emission param-eters {  X  l } , we can use a standard backward filtering forward sampling scheme to block sample Z 1: T . In sampling the event transition distributions  X  , we recall the L weak limit approximation of Eq. (7) and first sample the parent transition distribution  X  as de-scribed in Supplement B and then sample each  X  l from its Dirichlet posterior, where n l is a vector of transition counts of Z 1: T from state l to the L different states.
 Event state parameters Finally, we sample the in-novation covariance  X  l for each event state l from its HIW posterior:  X  l  X  HIW G ( b l ,D l ), with b l = b 0 + |{ t | Z t = l }| , D l = D 0 + Hyperparameters The prior and conditional pos-teriors of the hyperparameters  X  c ,  X  c ,  X  e ,  X  e ,  X  e  X  c = B 0 ( X ) are provided in Supplement B. Simulated data We first simulated six channels of toy data from five different first-order AR processes and three different event innovation covariances to con-firm that our model was able to accurately estimate the true channel and event state sequences, the true channel state AR coefficients, and the true event state innovation covariances. We found our model 1 able to estimate all of these parameters remarkably well and describe the simulation parameters, experimental de-tails, and results in Supplement C.
 Analyzing seizures We tested the HIW-spatial BP-AR-HMM on two similar seizures (events) from a patient of the Children X  X  Hospital of Pennsylvania. These seizures were chosen because qualitatively they displayed a variety of dynamics throughout the be-ginning, middle, and end of the seizure and thus are ideal for exploring the extent to which our HIW-spatial BP-AR-HMM can parse a set of rich neurophysio-logic signals. We used the 90 seconds of data after the clinically-determined starts of each seizure from 16 channels, whose spatial layout in the electrode grid is shown in Fig. 2 along with the graph encoding our conditional independence assumptions. The data were low-pass filtered and downsampled from 200 to 50 Hz, preserving the clinically important signals but reduc-ing the computational burden of the analysis. The data was also scaled to have 99% of values within [-10, 10] for numerical reasons. We examined an order 5 HIW-spatial BP-AR-HMM and ran 10 MCMC chains for 6000 iterations, discarding 1000 samples as burn-in. Details are in Supplement D.
 The HIW-spatial BP-AR-HMM inferred state se-quences for the sample corresponding to a minimum expected Hamming distance criterion are shown in Fig. 2. The results were analyzed by a board-certified epileptologist (B.L.). He agreed with the model X  X  judgement in identifying the subtle changes from the background dynamic (cyan) initially present in all channels. Furthermore, the model X  X  grouping of spatially-proximate channels into similar state transi-tion patterns (e.g., channels 03, 07, 11, 15) was clini-cally intuitive and consistent with his own reading of the raw EEG. Using only the raw EEG and prior to disclosing our results, he identified roughly six points in the duration of the seizure where the dynamic fun-damentally changes. The three main event state tran-sitions shown in Fig. 2 occurred almost exactly at the same time as three of his own marked transitions. These event states allow for a more global summary of the dynamics of the seizure and provide an important addition to the channel state sequences of the standard (non-spatial) and our HIW-spatial BP-AR-HMMs.
 In Supplement E, we show how the event state pars-ing of the same seizure X  X  offset is similarly clinically intuitive and distinguishes subtle transitions in the dy-namics: strong correlations in the spikings of a few channels to a more widespread correlation structure and synchronized discharge pattern. The automatic identification of brief intervals of synchronized spiking makes it easy for a clinician to calculate changes in the inter-spike interval, a quantity of clinical impor-tance. While interpreting these state sequences and covariances from the model, it is important to keep in mind that they are ultimately estimates of a system whose parsing even highly-trained physicians disagree upon. Nevertheless, the prospect of a reproducible, ob-jective, and automated method for parsing such com-plex, multichannel events that closely mirrors those of practicing clinicians opens the possibility of large-scale analysis of hundreds or even thousands of such events. We compared our HIW-spatial BP-AR-HMM to a full-covariance model with an IW prior on  X  l (IW-spatial). We additionally compared it to non-spatial alterna-tives where channels evolve independently: the BP-AR-HMM of Fox et al. (2009) and an AR-HMM with-out the feature-based modeling provided by the beta process (Fox et al., 2011b). Both of these models use inverse gamma priors on the individual channel inno-vation variances. Fig. 3 (left) shows how condition-ing on the innovations of neighboring channels in the HIW-spatial model improves the prediction of an indi-vidual channel, as seen by its reduced innovation trace relative to the IG-non-spatial model. The quantitative benefits of accounting for these correlations are seen in our predictions of heldout events, as depicted in Fig. 3 (right). We infer a set of AR coefficients { a k } and event covariances {  X  l } on one seizure, and then com-pute the heldout log-likelihood on a separate seizure, constraining it to only select among the inferred AR and event states. We can analytically marginalize the heldout event state sequence Z 1: T but perform a Monte Carlo integration over the feature vectors f ( i ) and channel states z 1: T using our MCMC sampler 2 . Fig. 3 (right) compares the heldout log-likelihoods for the IG-non-spatial and (H)IW-spatial models listed above, collected over 5000 samples across 10 chains, each with a 1000-sample burn in and 10-sample thinning. As ex-pected, the HIW-spatial model has significantly larger predictive power than the non-spatial models. Though hard to see due to the large spatial/non-spatial differ-ence, the BP-based model also improves on the stan-dard non-feature-based AR-HMM. Performance is also at least as competitive as a full-covariance model (IW-spatial) but most importantly has significant computa-tional gains based on the graph structure. The model scales linearly with the number of events, and the con-ditional independencies introduced by using a hyper-inverse Wishart prior allow the matrix operations to grow linearly with the number of channels rather than quadratically, as they do in the IW-spatial model. In Supplement F, we give results from a similar compar-ison on a much larger dataset of 50 events.
 Comparing epileptic bursts and seizures We applied the HIW-spatial BP-AR-HMM to six chan-nels of iEEG over 15 events from one patient. These events comprise 14 short sub-clinical epileptic bursts of roughly five to eight seconds and a final, 2-3 minute clinical seizure. Our hypothesis was that the sub-clinical bursts display initiation dynamics similar to those of a full, clinical seizure and thus contain infor-mation about the seizure-generation process.
 The events were automatically extracted from the pa-tient X  X  continuous iEEG record by taking sections of iEEG whose median line-length feature (Esteller et al., 2001) crossed a preset threshold, also including 10 sec-onds before and after each event. The iEEG was pre-processed in the same way as in the previous section. The six channels studied came from a depth electrode implanted in the left temporal lobe of the patient X  X  brain. We ran our MCMC sampler on the 15 events ( N = 15  X  6 with disconnected channel graphs between events) and selected a representative sample as in (Fox et al., 2011a). The hyperparameter settings, number of MCMC iterations, chains, and thinning was as in the previous experiment.
 Fig. 4 compares two of the 14 sub-clinical bursts and the onset of the single seizure. We have aligned the three events relative to the beginnings of the red event state common to all three, which we take roughly as the unequivocal start of the epileptic activity. The in-dividual states of the four middle channels are also all green throughout most of the red event state. It is interesting to note that at this time the fifth channel X  X  activity in all three events is much lower than those of the three channels above it, yet it is still assigned to the green state and continues in that state along with the other three channels as the event state switches from the red to the lime green state in all three events. While clinical opinions can vary widely in EEG read-ing, a physician would most likely not consider this segment of the fifth channel similar to the other three, as our model consistently does. But on a relative volt-age axis, the segments actually look quite similar. In a sense, the fifth channel has the same dynamics as the other three but just with smaller magnitude. This kind of relationship is difficult for the human EEG readers to identify and shows how models such as ours are capable of providing new perspectives not readily apparent to a human reader. Additionally, we note the commonalities in event state transitions.
 The similarities mentioned above, among others, sug-gest some relationship between these two different classes of epileptic events. However, all bursts make a notable departure: a large one-second depolariza-tion in the middle three channels, highlighted at the end by the magenta event state and followed shortly thereafter by the end of the event. Neither the states assigned by our model nor the iEEG itself indicates that dynamic present in the clinical seizure. This dif-ference leads us to posit that perhaps these sub-clinical bursts are a kind of false-start seizure, with similar on-set patterns but a disrupting discharge that prevents the event from escalating to a full-blown seizure. Val-idation of such a hypothesis through a more compre-hensive study would greatly improve our basic-science understanding of seizures and epileptogenesis. We presented a modeling framework for automating the parsing of EEG data, especially in the challeng-ing scenario of multiple recordings taken from patients with variable numbers of channels, as is common in iEEG data. Our framework builds on the BP-AR-HMM, enabling learning a shared dictionary of AR dynamics that are asynchronously switched between by the individual channels. In contrast to the BP-AR-HMM, our model captures correlations between the time series, which we demonstrated is crucial in fitting heldout seizure data. We harness the spatial structure of the channels to define a set of conditional indepen-dencies that both improve out-of-sample predictions and reduce the computational burden, allowing scala-bility to large electrode grids. We additionally intro-duce a Markov event state to capture the time-varying correlations. We showed how this event state further improved the clinical interpretability of our model. In addition to providing clinically intuitive parsings of the onset and offset of a seizure, we demonstrated how our event and channel state estimates facilitate com-parisons between sub-clinical epileptic bursts and clin-ical seizures, suggesting new clinical hypotheses about their relationship. Clearly, validating such specula-tions necessitates testing on more epileptic events from a large class of patients. We have delved into a clin-ical analysis of the iEEG to illustrate how our model brings a quantitative structure to these highly complex multi-channel events. We see the model and its parsing capabilities as a data exploration tool that will help clinicians make sense of the vast quantities of iEEG data collected from epilepsy patients.
 While we focus on modeling epileptic EEG events in this work, our model is more generally applicable to multiple correlated time series, especially in scenar-ios where there are multiple recordings with variable numbers of time series (e.g., motion capture sensors, financial data streams, etc.).

