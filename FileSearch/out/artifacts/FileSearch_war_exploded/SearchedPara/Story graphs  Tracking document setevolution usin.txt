 K.U. Leuven, Leuven-Heverlee, Belgium 1. Introduction
In many genres, text collections are not static and change over time, creating a story about the content they discuss. We define stories as corpora of time-indexed textual documents, all relevant to a top-level theme (the whole story, e.g.,  X  X sia Tsunami 2004 X  or  X  X nron X ). For users (readers), keeping abreast with the changes in large text collections such as news, blogs, or scientific publications is a challenging task. Many services such as RSS feeds aggregators, real-time social media updates, professional news outlets and search engines largely facilitate staying up to date with the current events. Users often follow documents belonging to the same story over an extended period of time. We refer to this activity as story tracking . For example, a user repeatedly searches for news articles reporting on a sport competition (e.g.  X  X opa America X ) from the start until the end of the competition. While tracking a story, users are inter-ested in discovering novel developments regarding the subjects, topics, and events described in the story. This contrasts with regular web search: in story tracking users expect to find both relevant and novel information represented in the documents. We refer to this type of information as story developments (developments in short). Such a service will implement a story tracking method . Story tracking meth-ods should capture developments with their output ( story representation ). A number of research fields employ different types of story representation ranging from natural language summaries (in document summarization [22]) via sentences (in sentence retrieval [20]), to temporal text patterns  X  sets of differ-ent abstract representations (e.g. a list of words, a distribution of words over topics) used in temporal text mining (TTM) research [10].

The question arises whether graphs, and in particular dynamic graphs, can make a difference in this domain: Do graphs as a special form of temporal summarization have properties and/or allow for oper-ations that can help users in the task of story tracking? To answer this question, one first has to better understand the task and how a computational method could support it. The ultimate motivation for this is to build tools that help users by providing understandable overviews of developments, alerting them to potentially interesting new developments, and of course ensuring that the graphs accurately reflect the content. We propose that a computational story tracking method has to solve three sub-tasks, which we will refer to as components. It has to (a) produce human-comprehensible output ( understandabil-ity ), (b) detect the emergence of new developments ( detection ), and (c) discover the details behind these developments ( discovery ). In this paper, we focus on formal questions to provide a basis for such user-centered functionalities. We argue that graph-based structures can be used for story tracking, and present a graph-based story tracking method that solves all the important story tracking sub-tasks. First, we cre-ate graph-based patterns, and continue by investigating graph evolution and topology for detecting and discovering developments of a story. In [35] we showed this approach being understandable for humans.
In this paper, we show that graphs are useful for representing the evolution of a document set. To demonstrate our approach we collected five real-life news stories and conducted a case study tracking their evolution.

The major contributions of this paper are: based on a graph-based approach to tracking document-set evolution, described in [35], we (a) investigate topological properties of the dynamic graph (and the story graphs it generates) as indicators of story evolution (detection), (b) study the discovery of novel sentences using graph structure (discovery), and (c) present an evaluation framework for temporal text mining. The paper is a substantial revision and extension of the ideas presented in [2,36].
The paper is structured as follows: After an overview of related work in Section 2, we describe relevant terminology and the method for creating story graphs in Section 4. The paper then continues with the discussion on the detection and discovery components in 5 and 6. Two examples will be used to illustrate key ideas. In Section 7 we present the results of our case study. Section 8 closes with a summary and an outlook. 2. Related work
Major related areas to the work presented in this paper include story tracking methods, story tracking evaluation frameworks, and graph-based text analysis. 2.1. Story tracking
In the past decade, a number of methods for story tracking have been proposed. Text-oriented ver-sions of the story-tracking task have been described in the Document Understanding Conference (DUC) Update Summarization task [22] and in the TREC Novelty Detection Track [33]. The basic task of the Update Summarization is to produce a short textual summary of new developments, which is then evalu-ated against human-written summaries using text-similarity metrics. In the Novelty Detection Track, the most similar task to story tracking is the Novel Sentence Retrieval Task in which the goal is to retrieve sentences previously judged as  X  X ew X  by human judges. Various text-summarization methods such as [3, 4] and sentence-retrieval methods such as [20,37] have been applied to these tasks.

Both the DUC Update Summarization and the TREC Novelty Detection tasks are discovery-oriented, and output story representation in a priori defined formats, namely textual summaries and sentences. In contrast to this, more recent methods have focused on mining for lower-level elements or patterns such as keywords, N-grams/term sequences, or LDA components. We refer to these approaches collectively as temporal text mining (TTM), e.g. [10,12,18]. The key notion of TTM is burstiness  X  sudden increases in frequency of text fragments, and all TTM methods aim to model burstiness.

Apart from the three groups of methods mentioned above, topic detection and tracking (TDT) [1] tasks such as New Event Detection or First Story Detection tackle the same problem. These TDT tasks decide whether a new document is reporting on an already existing story/topic or a novel (emerging) one.
TDT-oriented methods use documents or groups of documents as story representation, while in this paper we explore methods that extract the story representation from the documents. Although reading full documents may provide a deeper understanding of the story, interacting with story representations such as for example graphs provides users with a different user experience for story tracking.
Relations between developments have been explored in [21,30,31]. These methods aim to create a chain of developments in a story. In contrast to this, we are more focused on detecting the intensity of developments in a story and summarizing the story over time periods. To focus on this, we regard developments as independent rather than as (causally) chained.

Like other TTM approaches, we propose a new method for extracting temporal patterns from text. In addition, we also investigate what makes a method suitable for story tracking, and how to evaluate this across methods. 2.2. Evaluation frameworks
The DUC evaluation framework [22] for update summarization is a combination of human and auto-matic evaluation. In this paper we only address the automatic evaluation framework. It has been shown that the automatic evaluation using the ROUGE framework [17] is highly correlated with human eval-uation. The ROUGE framework measures the recall of N-grams between the human and machine pro-duced summaries. The most commonly used ROUGE scores are measured on bi-grams ( ROUGE. 2 ) and skip-4 bi-grams ( ROUGE.SU 4 ). The TREC Novelty Track [33] which ran from 2002 until 2004 , based on the match of retrieved and pre-judged  X  X ew X  sentences. The main problem of adopting one of these two frameworks for the evaluation of TTM methods is the limited number of documents they use ( 10 for DUC and 25 for TREC per one topic). TTM methods rely on data mining techniques which require a larger document set for pattern extraction. Another problem is the difference in patterns that TTM methods extract. Summaries for DUC, and sentences for TREC methods are standardized outputs which are directly comparable. The diversity of patterns and the absence of standardised tasks and eval-uation procedures, such as in DUC or TREC, render it basically impossible to compare their quality for the story tracking task.

In [25] Roy et al. present a semi-automatic detection of emerging topics in a corpus, and compare these topics with an editor-created list of topics. The application of modified LDA in [39] uses time as one of the latent variables for bursty topic detection, and compares them to topics extracted with the standard LDA. The results show that there are differences in the distribution of bursty patterns over time between these methods. The idea behind this comparison is that more bursty words will have different distributions over topics in different time periods, while the less or non-bursty patterns will have more similar distribution over topics in different periods. To test the accuracy of their measures of burstiness defined on word-topic distribution, Knights et al. [13] create an artificial document set by drawing words from a set of word-topic distributions. In selected periods the words are drawn from a subset of topics making these topics bursty. The authors measure whether their method captures this artificial burst. Wang et al. in [40] test their method by comparing bursts discovered in multilingual corpora on the same topic.
Most of the TTM evaluation procedures are tailored for evaluating only one method, assessing how well it discovers bursts in evolving corpora. We wish to measure not only whether a burst is discovered, but also how story representation helps in detection and discovery of the developments that created the burst. In sum, therefore, none of the existing methods satisfy our evaluation goal complexity. 2.3. Text as graphs A number of papers looked into representing text using graphs and use its structure for specific tasks. Schenker et al. in [27] describe possible uses of graph analytics methods for analyzing content of Web pages. Inspired by Page Rank, G X nes and Radev [4] present the LexRank summarization algorithm. This algorithm, creates a summary by selecting the most salient sentences in a sentence similarity matrix. of visualising pairwise associations of words using graph-based representation. Templates of natural language expression of events in news corpora using graph-based representation of text is presented semantic (grammatical) relationships between words in a document. Lexical graphs [41] are used for many text mining tasks, such as clustering [29] and recommendation [23]. Similar to our work, Heyer et al. in [9] employ graphs based on co-occurrences to track topics over time. However, there graphs are focused on one  X  X opic X  word and they track how co-occurrences with this world alone changes.
In contrast of most above mentioned graph-based text mining methods, in this paper we include the time dimension and explore how to use graph-representation of text content along a timeline. 3. Preliminaries
The developments of a story are conceptual and therefore in many cases elusive, hidden in the text, and hard to express using natural language. To investigate the effectiveness of story graphs in understanding, detecting, and discovering the developments, we need to define a way of representing them ( develop-ment representation ). Depending on the genre of the corpus that a user explores, developments can be expressed in a variety of ways, most common of which are sentences. News reports story developments can be expressed using natural language sentences describing topics, events, and subjects around which the story revolves. An important reason for this is the mainly narrative structure of news reports: de-velopments are described by propositional statements. For example, one development in the story about  X  X amine in Africa X  could be represented by a sentence:  X  X he United Nations declared a famine in two regions of southern Somalia on July 20 X . In the following, we equate sentences with developments and say that each sentence is a (potential) development.

For other document genres, developments may be harder to express. For example, in a corpus of scientific publications where topics of focus change over time, representing developments as sentences does not fully capture the semantics behind them. Similarly for blog corpora, where users are interested in discovering novel opinions, stands, and sentiment of other bloggers, sentences do not fully capture the developments.

For evaluating story graphs for all three story tracking components, we wish to control for the effects of developments representation. Our goal is to minimize the possible effects artificial (not natural language) development representation might have on our results. Therefore, in this paper we focus on news articles.
Story representations produced by different story tracking methods often differ from development representation of the story. Should this be the case? Concretely for this work, why should we output graphs when the developments are represented using sentences? There are two reasons to do this; the first is formal, the second user-related. Formally, we expect that the structure of graphs (their topology and evolution as dynamic graphs) may suggest effective mechanisms for detection and discovery  X  in particular, subgraphs with specific properties may emerge that correspond to story developments. The second, user-related, reason is that story tracking is not only about discovering developments, but also understanding and summarizing the changes in a document set, following how a story evolves over time, and providing hints when new developments occur. Different story representations may provide users with summaries of developments, give them freedom to explore them, facilitate detecting the changes in a document set, and keep a way of linking to the development representations. Therefore, having story representations differ from development representations may improve user experience during story tracking, moving away from  X  X imple X  retrieval into a more engaging and intuitive way of story tracking. We approach story tracking as an interactive task in which users interact with the system which enables understanding, detection, and discovery of story developments. 4. Creating story graphs: Method and understandability 4.1. The STORIES method: Story graphs and the evolution graph The graph-based method we use in this papers takes a corpus of time-stamped documents as an input. First, this corpus D is transformed into a sequence-of-terms representation. Subsequently, the content-bearing terms are extracted. We defined content-bearing terms as 100 most frequent terms plus 50 most frequent named-entities appearing in the corpus. Next, the corpus is partitioned by publication periods, e.g. calendar weeks. Thus, D is the union of all document sets D t , with t = 1 ,...,n the time periods. malised by its counterpart in the whole corpus to yield time relevance as the measure of burstiness: To obtain a view into document evolution over multiple time periods we define an evolution graph EG . be viewed as a dynamic graph, and each story graph as its current graph obtained by restricting it to a selected time period. Algorithm 1 illustrates the story graphs and the evolution graph generation process. Algorithm 1 Story graphs and evolution graph extraction algorithm.

Figure 1 illustrates the relations between story graphs and the evolution graph over three time periods ( t periods story graphs summarize developments of the periods they belong to, while the evolution graph summarizes the developments starting with the first time period. For example, in t 1 evolution graph contains all nodes and edges from t 0 plus the newly appeared nodes and edges in t 1 (E X  X ), while the story graph N 1 contains only edges that are bursty for the period t 1 (edges A  X  D and A  X  C are not in the story graph for t 1 ).
 Example 1: Figure 2a shows a story graph. The concrete story in this case regards a missing child and the criminal investigation around it. The graph shown in the figure describes the first four days after the initial event. The graph summarizes the main questions in a news report: who (missing child as a central node in the largest component, and the parents in the top right corner), what ( X  X isappear X  and  X  X iss X  nodes are linked to the child), and where ( X  X oliday X  and  X  X partment X  are linked to the child, and the country is shown in the top left corner). Another important news question, when , is implied by the period users explore. In total the graph shown in the image summarizes 263 documents. 4.2. Understandability
A first question to ask of such a method is whether it generates human-understandable story represen-tations. To test this (as well as to make the method as a whole accessible to human users for summariz-ing news), we created the STORIES tool [34] that supports story tracking using the method described in this section. The STORIES tool demo is available at http://people.cs.kuleuven.be/~bettina.berendt/ STORIES.

Previously, in [35], we evaluated the understandability component in a series of user studies and auto-matic procedures. In the first study we presented the participants with a set of story graph visualizations and a set of descriptions of important developments for the same period. Participants X  task was to match the edges of the graph to the descriptions. In total about 80% of the descriptions were matched to the edges of the graph. We also report the results on story search evaluation in which participants were pre-sented with a set of story graphs (with search functionality) and a set of YES/NO questions. The task in this study was to explore the documents in a time period using the story graph and to answer the ques-tions. In total participants answered correctly on about 75% of the questions. Additionally, we looked at the coherence of the data sets created by path restriction from story graphs. We compared them to state-of-the art document clustering algorithms. The results show that using story graphs to create structure in a document sets yields a more coherent document grouping. 5. Detection
When users track a story over time, they are particularly interested to learn when  X  X omething has happened X . In a first step, they want to be alerted to periods in which a lot has happened (detection), and in a second step, they want to get  X  X he best overview of what it was X  (discovery). In this section, we deal with the first step (in Section 6 we explore the second step). We call periods in which a lot has happened  X  X ventful X  and measure this concept via a ground truth that we obtain from external sources. Formally, the ground truth is a set of sentences, and we measure eventfulness by the number of sentences in this set. A story tracking method is good at detection, if the patterns it generates have a measurable and human-detectable property that correlates with the eventfulness. Example 1 (cont.) : an example of changes in story over time is shown in Fig. 2. The figure shows (in 2b) the story graph around the 4th of September (with an interval of seven days), a rather eventless time. The change effected by moving the search forward by one day to the 5th is shown in Fig. 2c. It marks a major change in the real-life story (a crime case: after a long period of no new findings in the criminal investigation, DNA evidence was found, which led to the identification of new suspects); the event becomes visible by the large increase in graph size and connectedness. 5.1. Hypotheses and measures of developments and graphs
We investigated the relationships between topological properties and indicators of eventfulness. We measure the local eventfulness ( LE ) as the number of developments in a time period. Local eventfulness corresponds to discovering developments by observing properties of a single story graph. We measure the global eventfulness ( GE ) at time t n as a number of all developments occurring in time periods up to and including t n . The properties are at two levels  X  local and global. A single story graph has local properties . Local properties can be graph properties and node properties. An evolution graph has global properties . The idea of this is that changes of an evolution graph can point to new developments. We investigated the following properties and hypotheses:
We also considered the number of connected components. This was because informal inspection of the story graphs indicated that low-LE times were often marked by many small subgraphs, while high-LE times may have more cross-connected  X  X tory-lines X . In Fig. 1 the size of LCC for t 0 is 4 and there is only 1 component, while in t 1 the size of LCC is 2, and there are 2 components. On the other hand, if connected components mark different events, their number should be positively related to LE.
Local properties of individual story-graph nodes: degree (in the graph layout chosen, leading to  X  X entral-looking X  nodes), degree centrality (normalized by the maximum possible degree given by the number of nodes  X  1, which visually requires the user to relate the node to the whole graph), and sum of TR weights (see Section 4.1) of the adjacent edges. The sum of weights was used in a non-normalized fashion because average weights led to artifacts for small connected components. For each of the mea-sures we take the maximum value from the nodes of a story graphs. The hypothesis is that the existence of a central, highly and strongly connected node points to developments. Thus we expect that these mea-sures are positively related to LE. Observe the difference between node A in story graphs for periods t and t 2 in Fig. 1. In period t 2 node A has higher degree centrality and appears to be more central than in t 1 . This arises from many bursty co-occurrences A is involved in this period. Thus, it is reasonable to assume that A is put into a spotlight due to its importance in representing the developments of a period.
Global properties: the number of nodes, the number of edges, the size of the largest connected com-ponent, the number of connected components and the average size of connected components of the evolution graph.
We expect that the inclusion of new (previously not present) concepts (nodes) and relations between them (edges) is a result of new developments. In Fig. 1 observe the difference between the evolution and 6 new edges. We suspect that the newly included nodes in the evolution graph are related to the new developments. Thus, we investigate the correlation of the size and connectedness of the evolution graph and the total number of developments.

By measuring graph topology of story graphs and the evolution graph we aim to discover which properties can detect the existence of new developments in a story. These properties can be used to alert users of the existence of possible new developments in an observed time period. This can be done, for example, by highlighting certain parts of a story graph. 6. Discovery
The question whether certain graph properties reflect eventfulness can obviously be asked of a method that outputs dynamic graphs as story representation, but not of methods that generate natural language or keyword representations. In addition, even if we find that a certain graph property signals eventfulness, we do not know whether the corresponding graph highlights  X  X he right X  events. To address these two questions we developed a framework for cross-method evaluation of discovery.

As explained in Section 5, we focus on the news domain where developments can be represented using sentences to which we refer as ground-truth sentences. Although many TTM methods generate story representations using different bursty patterns, there is no well-defined approach to linking them with the ground-truth sentences. Our basic assumption is that the format of the patterns suggests  X  to human users and also to a formal approach  X  a way of combining them into sentences and that these sentences can then be compared against the ground-truth sentences.

In this section we define a procedure for linking story representation with sentential description of developments in a story. The procedure is based on sentence retrieval using queries generated from story representation. The query generation procedure is defined for three major groups of TTM methods listed in Section 6.2. To evaluate the results from this procedure, we define an evaluation framework. 6.1. Linking to sentences
To enable the direct comparison of different bursty patterns we developed a procedure for obtaining sentences which these patterns resemble in the best way. We use sentence retrieval methods to obtain the same representation (sentence representation) for different methods. The large number of developed sentences retrieval algorithms and the lack of benchmark method for sentence retrieval (in the TREC framework [33]) make the decision on using one method challenging. However, a detailed analysis of sentence retrieval presented in [20] used Query-likelihood retrieval method ( QL ) with Jelinek-Mercer smoothing on a pseudo-document index of sentences as a baseline. Therefore, we consider the QL method to be a sensible choice for our framework. Input for this model is an index of pseudo-documents and a set of queries used for retrieving. Pseudo-documents are created from the sentences of a corpus D , and queries are generated from story representations. 6.2. Query generation
For generating the queries used for sentence retrieval, we combine elements of the story representation (story elements)  X  e.g. an element of keyword list story representation is a single keyword. Each element in story representation has a burst score attached to it. The combination of elements greatly depends on the internal structure of the story representation. We consider two approaches to query generation. The first one (general query generation) is the same for all methods, and uses top maxR story elements from story representation. The second approach (model-specific query generation) takes into account the structure of different story representations. The idea of model-specific query generation is to combine the basic bursty pattern elements into more complex queries. In our previous work [36] we tested the difference between general and model-specific query generation procedures. The results of that analysis showed that in almost all cases model-specific query generation improves upon the results of sentence retrieval. Therefore, in this paper we concentrate on model-specific query generation. 6.2.1. Story graphs query generation
The basic story element of our method is keyword association, while graphs are more complex pattern graph structure of story graphs. We first extract paths from each story graph up to size maxQ and order them by descending average TR path weight. Then we sort them based on the average edge weight. The final list of queries is obtained by cutting the the sorted path list at position maxR . The query is formulated by joining the node names (with white space between them) starting from the first node in a path. 6.2.2. Query generation for other models
The lack of query generation procedures and a large number of developed TTM methods makes defin-ing query generation a challenging task. It would be virtually impossible to retrospectively define these procedures for each model. Therefore, we propose a grouping of the methods based on story represen-tation. We divide TTM tracking methods in three major groups: (a) keyword representation, (b) group representation, and (c) combo representation methods. For story representation, the first group [5,7,8, 12,32] uses a list of bursty N-grams ranked by their burst scores. The second one [5,11,18,28,39] joins bursty N-grams into groups which point to developments. The last group combines simple patterns into more complex ones; story graphs are an example of this group.

The model-specific query generation of keyword representation methods is based on the combination of bursty keywords. First, we extract the maxR highest ranked story elements from model X  X  story repre-sentation. Then we combine them by creating all possible combinations not larger than maxQ . We rank these newly formed combinations based on the average burst scores of their elements and use the top maxR as queries for retrieving sentences.

Group representation methods output groups of text content as story representation (e.g. a distribution of words over topics, where each topic is one group) describing subjects. The procedure for query gen-eration based on the story representation with k groups is as follows. For each group we extract maxR / k story elements with the highest in-group burst score and combine them into all possible combinations not larger than maxQ . Then we rank the new in-group element combinations based on the average burst scores of their elements. For each group we use the top maxR / k combinations as queries for retrieving sentences. We assume that all groups are equally important and use maxR / k story elements from each group.
 procedure for all three groups of methods. 6.3. Evaluation framework
We first obtain a corpus of news-article documents and development representation ground-truths, all of them time-stamped. We divide the corpus into time periods t = { t 1 ,t 2 ,...,t n } of equal length. For every time period, we build an index I t out of sentences belonging to the documents from the time period. For each t , we obtain the development representation as a external (not in a corpus) set of ground-truth sentences G t = { g 1 t ,g 2 t ,...,g nt } , where n is the index number of ground-truth sentences in period t . Given a set of model-specific generated queries Q we retrieve the top ranked sentence using the QL retrieval method, and create a set of retrieved sentences R . 6.3.1. Metrics We define the measures of similarity between retrieved and ground-truth sentences using the ROUGE [17] framework.

Ultimately, we want a set of measures which shows to what extent retrieved sentences capture the ground truth, as well as how many sentences are needed to obtain a  X  X ood X  ground truth match. 6.3.2. Atomic measures
We use the ROUGE framework, which measures the recall of n-grams between the method-generated and the ground-truth sentences. The most commonly used ROUGE scores are measured on bigrams ( ROUGE.2 ) and skip-4 bigrams ( ROUGE.SU4 ). For every retrieved sentence r tk ( 1 6 k 6 n ) we calculate the ROUGE.2 ( s 1 tkj ) and ROUGE.SU4 ( s 2 tkj ) scores against every ground truth sentence in the same time period  X  g tj ( 1 6 j 6 n ). This will give us a Cartesian product of R t and G t where each 6.3.3. Aggregated measures
We define a set of aggregated measures to quantify how well the set of retrieved sentenced matches the ground-truth set, what percentage of the best possible ground truth match is obtained from the retrieved set, and how the number of retrieved sentences influence these scores.
 maximum score any retrieved sentence in a period t has for the ground truth j . For ROUGE.2 scores maxM is defined as: With maxM we measure how retrieved sentences match the ground truth sentences. However, since the ground truth sentences do not originate from the same corpus as the retrieved sentences it is hard, if not impossible, to obtain the maximum match of 1 . The maximum maxM score a ground truth can obtain varies from one ground truth to the other. So, in order to normalize the maxM score, we introduce maxMR measure. It shows what percentage of the optimal match with the ground truth is obtained by maximum ROUGE.2 and ROUGE.SU4 scores between G t and S t , where S t = { s 1 t ,s 2 t ,...,s ht } is a set of all sentences from a period t . For ROUGE.2 scores it is defined as: maxMR is a  X  X ecall oriented X  measure. However, different methods may retrieve different number of sentences, and the ones with a larger retrieved sentence set increase the chance of having a better match. We take this into account and define a new measures  X  maxMP . For ROUGE.2 scores maxMP is defined as: The maxMP rewards the methods that produce a good fit with a small number of retrieved sentences (matching the usually small number of ground-truth sentences). In this sense the measures correspond to  X  X op heavy precision-oriented X  measures like precision@k.
 Analogously with the Eqs (1), (2), and (3) we define maxM , maxMR , and maxMP measures for ROUGE.SU4 .

The motivation behind these metrics is to punish those methods which retrieve too many sentences and reward those which retrieve approximately the same number of sentences as the number of ground truth sentences. 6.3.4. Testing procedure
Lacking a baseline method, we turn to the cross evaluation testing procedure, assessing the perfor-mance of multiple methods against each other. Given a set of time periods T , a set of methods M , a set of ground truths G , a set of metrics X , and a set of indices I : for every t  X  T and for every M  X   X  M , we calculate all measures from the previous paragraphs. This gives rise to, for every metric x  X  X , then test results of different methods using Friedman X  X  and Tukey X  X  multiple comparison test to asses the differences between different methods. 6.4. Framework illustration Example 2 (cont.): Figure 3 illustrates an example of the development discovery framework described in this section. Rectangle I shows the editor-selected, ground truth sentences  X  descriptions of developments in a time period. Rectangle II shows examples of story representation (bursty patterns) extracted from the corpus for the three method groups. The left-most rectangle shows bursty keyword list generated by [12]  X  keyword representation; the middle rectangle shows 2 bursty topics (in embedded rectangles) generated by [18]  X  group representation, and the right-most rectangle contains a story graph represen-tation generated by STORIES [35]  X  combo representation. Based on these story representations we generate the queries shown in rectangle III. Top 5 queries are generated from the bursty patterns in rect-angle II following the generation procedure described in 6.2.2. Using the queries and the QL retrieval method, the  X  X est X  sentences for all 3 methods are retrieved from the corpus (shown in rectangle IV). Best sentences are here defined as the ones most similar to the editor-selected sentences in rectangle I. The maxMR and maxMP scores for sentences are shown in the brackets. 7. Case study discovery components and report on the results for 5 news stories from different domains. 7.1. Corpora and ground truth 7.1.1. Corpora
To demonstrate our approach we needed a large-enough (for pattern extraction) and timestamped doc-ument set describing the same (news) story. Many available evaluation corpora only partly satisfy these conditions. Therefore, we decided to collect corpora using Google News 1 and Google News Archive search. 2 In total, we collected 5 stories using queries issued to the search engines as the identifiers of a story. We collected stories following pop singer Britney Spears ( D 1 ), the accident in a Chilean mine platform oil spill ( D 5 ). 3 Table 1 gives an overview of sizes and time spans of these corpora. 4 After harvesting the documents we applied the content extraction algorithm described in [24] to remove all auxiliary content such as navigation, related pages, and ads. The rest of the pre-processing was done in the same way as in [35]. 7.1.2. Ground truth
Apart from creating corpora, we also needed to define a ground truth describing the developments in the observed stories. In similar evaluation frameworks [22,33] the ground truth was established using several human editors. The Web itself provides us with the documents that contain a similar result to a multi-rater ground truth definition. We first turned to Wikipedia, and searched for timeline 5 pages about the stories we collected. We consider that the crowd-sourcing strength of Wikipedia does a good job in emulating a large number of human editors. For D 2 , 6 D 3 7 and D 5 8 we were able to find pages containing the annotated timeline of the developments. The Wikipedia page about D 4 9 contained only free text, and not a timeline format description of the developments. To extract the timeline from Wikipedia text we employed two human judges. Our previous work [35] contains details about the annotation procedure. For D 1 we did not find enough content in Wikipedia, and turned to other sources. 10 The procedure for extracting the ground truth for D 1 was the same as for D 4 . 7.2. Detection results 7.2.1. Settings
Before the analysis of the detection component, we generated story and evolution graphs. This in-cluded setting a number of parameters to the method described in Section 4.1. Our initial experiments showed that 30 edges is a reasonable number of edges to both represent developments and be  X  X uman understandable X . Since detection can be automated, and does not necessarily include users, the under-standability constraint could be relaxed. Therefore, we used two settings for story graphs, one with a maximum of 30 edges referred to as D 30  X  , and the other with no constraint on the number of edges, 1 week. The thresholds  X  1 and  X  2 were set to 4 and 2 respectively. In total we created 148 story graphs for each edge number setting. For each story, we also created an evolution graph. 7.2.2. Results and discussion
We measured the Pearson correlation between number of developments in a time period, and proper-ties of story and evolution graphs in the same time period. 7.2.3. Local graph properties and LE
Table 2 shows the correlation coefficients between local graph properties and local eventfulness. De-tection is somewhat more successful using a restriction on the number of edges (rows 1 X 5). This is illustrated by a higher number of significant correlations, when compared to the non-restricted story graphs (rows 6 X 10).

For the story graphs with maximum 30 edges the highest number of significant correlations (4 out 5) is for the number of connected components. Other important local graph properties for the same graphs are the number of nodes and the number of edges. For the non-restricted story graphs there is no property that stands out.

Looking at the individual stories, the number of significant correlations differs in many cases. We found the reason for this in the distribution of developments over time periods. For example, in the most extreme case for D 4 , where all properties are highly significant, the average number of developments developments followed by time periods with a high number of developments. In such cases it was easier to  X  X apture X  the important properties. On the other side, for stories with more uniform development dis-in the story graphs. 7.2.4. Local node properties and LE The next analysis we performed investigated the local node properties correlation to local eventfulness. The results are summarized in Table 3. Based on our initial user tests, we suspected that centrality of nodes in a story graph points to developments. However, we were not able to find consistent significant correlations. This leads us to the conclusion that centrality of nodes does not suggest new developments. 7.2.5. Global properties and GE
Finally we analyzed the correlation between evolution graph properties and global eventfulness. The results are shown in Table 4. We found that the most important property is the addition of new nodes. Both with and without restriction on the number edges the number nodes was significant in 4 out of 5 cases. Important properties are also the number edges ( | E | ) and size of the largest connected component ( | LCC | ). 7.2.6. Discussion
We analysed properties of the whole graphs and specific nodes. The most important properties for development detection we discovered are: number of connected components and number of nodes for static graphs and the number of newly added nodes for dynamic graphs. In contrast with the local prop-erties, for global properties, connected components were not systematically significant. The reason for this is that the increase in graph size does not always lead to an increase in the number of connected com-ponents. In this case, the number of connected components does not change over time, while the global eventfulness changes, yielding no correlation. The analysis of global properties showed that tracking the newly used concepts (nodes) evolution graphs can be used to detect new developments. 7.3. Discovery results 7.3.1. Settings
The framework for evaluating discovery component (Section 6) described a way of obtaining sen-tences from story representation. We used the same settings for story graph generation as for the de-tection evaluation. Apart from the STORIES method, referred to in this section as M 1 (with the same settings as for the detection evaluation), we needed to choose methods for cross-evaluation. The ques-tion is how to choose the methods which both represent the groups described in Section 6.2.2 and are of high quality. We decided to use the methods that received the most attention by the community over the last few years. 11 Namely, for keyword representation we chose Kleinberg X  X  burst detection algorithm ( M 2 ) [12] and for group representation we used a probability mixture method developed by Mei and Zhai ( M 3 ) [18].

Method M 2 [12] discovers bursty words by minimizing their state-transition cost between bursty and non-bursty states. The cost is defined as the increase in proportion of documents relevant to an observed word between two time periods. The method takes a scaling factor as a parameter, which determines the lift in the proportion of relevant documents between two time periods needed for a word to reach the bursty state. We use the parameter value 2, as set in [12]. In the original paper, the algorithm was applied to scientific publications. The same method was later used to find bursts in blogs [14] and to track quotes in news [16]. We coded the algorithm following the description in the original paper.

An extension of the probabilistic mixture method for topic discovery presented described in [43] was used in [18]. This method, M 3 , outputs set of bursty topics represented by word distributions. Each of the different distributions points to a subject. The original paper reports on a number of parameters for document-topic, topic-time, and word-topic distributions. We used the same values as reported by the authors. However, the authors report on a threshold value set by empirical testing which is not described in detail in the papers. We were therefore not able to replicate this, and manually set the threshold to 5 developments ( X  X opics X ). We coded the algorithm employing a modification of the implementation of [43] in the DRAGON NLP Toolkit. 12
As discussed in Section 6.2.2, for each method we generated a set of retrieved sentences using model-specific query generation. We set the query-generation parameter maxQ to 5, and we varied maxR from 5 to 30 with a step of 5. The value of the first parameter was chosen based on query length in major search engines [15]. Variations of the second parameter simulate the situation in which different number of top story elements are used.

We calculated maxM , maxMR , and maxMP for all settings and for both ROUGE.2 and ROUGE.SU4 , and tested them as described in Section 6.3, resulting in a total of 24 tests. 7.3.2. Results
The large number of test settings and methods made it difficult to aggregate the results of the tests, so we created multiple aggregations. With these aggregations we aimed at capturing the following infor-mation: (a) which methods are robust to different maxR settings, and (b) whether there is a difference between  X  X recision-like X  and  X  X ecall-like X  performance.

To aggregate the tests of maxMR and maxMP results and summarize the performance of models, we grouped by methods to highlight similarities across corpora. Each row show results for one of the stories starting with D 1 .

Columns show test settings: the use of only the top maxR patterns or pattern elements for sentence retrieval.  X  X op X  was measured by the respective methods X  quality measure ( M 1 : time relevance, M 2 : burstiness score, M 3 : probability of being in a bursty topic); maxR is equal to the number of bursty story elements (edges) in M 1 , the number of bursty keywords in method group M 2 , and in M 3 to keywords with highest probability from each topic.

A cell is black if, according to the Friedman test, this method-setting combination was in the group all the methods with a black cell in one column, but all of these significantly outperformed all of the others (the ones with a white cell in this column). Further significant differences between these lower-quality methods existed, but are not shown.

The heatmap to the right of the table shows the robustness of the method quality over settings, ranging from low ( = only in good group for few settings, light grey) to high ( = always in good group, black). Dif-ferentiation is measured as: number of blocks  X  (number of white cells + number of block holes) . The larger number of holes shows that the method is less robust. Values were then binned into four cate-gories of grey shades.

Figure 4a shows that for the  X  X ecall-oriented X  maxMR measure, the differentiations between methods are low. Story graphs extracted using M 1 perform slightly better than the other two methods. However, there is small differentiation (maximum 2 blocks out of 30) leading to the conclusion that for maxMR methods have similar scores. This means that differences between the ground-truth sentences and all sentences retrieved by all methods are low. For example, M 1  X  M 3 were indistinguishable in quality for D 1 , regardless of number of patterns used. This is shown by each cell (settings @5 X 30 patterns) being in row 2 are both black), but significantly better than M 3 (whose first cell in row 2 is white). For @10 and D 2 , M 2 and M 3 outperform M 1 . For settings &gt; @15, all three methods were indistinguishable for D
On the other hand, when we look at the results of the  X  X recision-oriented X  measure maxMP (Fig. 4b), the differentiation between method is higher. As for maxMR , method M 1 performs the best. Results for M 3 vary most between the two measures. We suspect that the reason for this is the structure of M 3 group based story representation. Each group should point to one development. However, the number of ground-truth sentences and the number of groups (topics) was fixed to 5 following the approach in [18], while on average for all stories we had 2.54 events. This means that there were fewer developments than groups that describe them, and this reduces precision. 8. Future work and conclusions 8.1. Summary and conclusions
Stories change over time, and we explored how to use graphs for tracking these changes. In this paper we focused on how graphs help solve two story tracking sub-tasks: detecting the emergence of new developments and discovery of original description of the developments. Indicating the existence of new developments during the story is an essential task of story tracking. In Section 5 we explored static and dynamic properties of graphs to identify the ones correlated with the existence of new developments. The results presented in Section 7.2 show that often the size of connected component for static graphs, and the number of edges and nodes for dynamic graphs indicate new developments. On the application compared to the previous one. This can draw users attention to new developments, and to automatically detect interesting time periods.

As shown in Section 6, obtaining a representation of developments in the news domain is a challenging task. To evaluate our method, we first developed a framework for evaluating temporal text mining algo-rithms, and then cross-evaluated our method in Section 7.3. The results show that our method performs higher or at the same level as other methods in most tested settings.

To sum up, in this paper we investigated graphs as a possible solution for story tracking. We showed that our graph-based method can be used for detecting new developments and discovering their repre-sentation in a corpus. 8.2. Limitations
Story tracking is a wide research area, and we focused this paper on a number of issues surrounding it. Most of our goals were oriented towards exploring the news domain, and it could be the case that in other domains some of our assumptions are weaker. The case study presented in this paper is of limited size, and results should be understood as such. Testing more methods with more stories would require a large effort from the community in setting standardized data sets and benchmarks. We consider that by defining an evaluation framework and conducting a case study, we have made a first step necessary towards this goal.
 8.3. Future work
There are many ways in which we can expand on the graph-based story tracking. One of the possible directions is to define an automatic procedure for development detection based on the graph properties, and build an automatic detection component. More generally, we believe that many text mining tasks can benefit from graph mining algorithms, and that graph-based representations of text present a interesting challenge in graph mining. For example, possible tasks that would benefit from graph based approach in graph-based text analytics, we hope that this paper will increase the interest of the graph mining community in further exploration of textual data and tasks related to them.
 References
