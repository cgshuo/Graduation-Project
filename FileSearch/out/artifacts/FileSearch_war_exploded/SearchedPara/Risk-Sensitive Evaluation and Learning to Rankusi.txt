 A robust retrieval system ensures that user experience is not damaged by the presence of poorly-performing queries. Such robustness can be measured by risk-sensitive evalua-tion measures, which assess the extent to which a system performs worse than a given baseline system. However, us-ing a particular, single system as the baseline suffers from the fact that retrieval performance highly varies among IR systems across topics. Thus, a single system would in gen-eral fail in providing enough information about the real base-line performance for every topic under consideration, and hence it would in general fail in measuring the real risk asso-ciated with any given system. Based upon the Chi-squared statistic, we propose a new measure Z Risk that exhibits more promise since it takes into account multiple baselines when measuring risk, and a derivative measure called GeoRisk, which enhances Z Risk by also taking into account the over-all magnitude of effectiveness. This paper demonstrates the benefits of Z Risk and GeoRisk upon TREC data, and how to exploit GeoRisk for risk-sensitive learning to rank, thereby making use of multiple baselines within the learning objective function to obtain effective yet risk-averse/robust ranking systems. Experiments using 10,000 topics from the MSLR learning to rank dataset demonstrate the efficacy of the proposed Chi-square statistic-based objective function.
The classical evaluation of information retrieval (IR) sys-tems has focused upon the arithmetic mean of their effec-tiveness upon a sample of queries. However, this does not address the robustness of the system, i.e. its effectiveness upon the worst performing queries. For example, while some retrieval techniques (e.g. query expansion [2, 8]) perform effectively for some queries, they can orthogonally cause a decrease in effectiveness for other queries. To address this, various research into robust and risk-sensitive mea-sures has taken place. For instance, in the TREC Robust track, systems were measured by geometric mean average precision [23, 25] to determine the extent to which they per-form well on all queries. More recently, the notion of risk-sensitivity has been introduced, in that an evaluation mea-sure should consider per-query losses and gains compared to a particular baseline technique [11]. Within this framework, measures such as U Risk [27] and T Risk [13] have been pro-posed. Both measures can be adapted to integrate with the state-of-the-art LambdaMART learning to rank technique.
Since risk-sensitive measures compare to a specific base-line, such measures are most naturally applied in experi-ments using a before-and-after design, where different treat-ments are applied to a particular baseline system, e.g. query expansion. However, when simply considering a single base-line, a full knowledge of the difficulty of a particular query cannot be obtained. For instance, a single baseline system may perform lowly for a query that other systems typically perform well. For this reason, the inference of risk based upon a population of baseline systems is attractive. One can easily draw an analogy with the building of ranking methods that combine multiple weighting models, such as data fusion or learning to rank, to obtain a more effective final ranking. Moreover, the use of multiple baselines permits a deployed search engine to evaluate the risk of an alternative retrieval approach not only with respect to its own baseline, but also to other competitor systems.

In this paper, we show how a risk-sensitive evaluation based on the Chi-square test statistic permits the consid-eration of multiple baselines, unlike the existing measures U Risk &amp; T Risk which can only consider a single baseline. In doing so, we argue that a robust system should not be less effective for a given topic than an expectation of perfor-mance given a population of other (baseline) systems upon that topic. In particular, this paper contributes: a new risk-sensitive evaluation measure, namely Z Risk , based on Chi-square test statistic, and a derivative called GeoRisk that enhances Z Risk by also taking into account the overall mag-nitude of effectiveness; Moreover, we demonstrate the use of Z
Risk and GeoRisk upon a TREC comparative evaluation of Web retrieval systems; Finally, we show how to directly and effectively integrate GeoRisk within the state-of-the-art LambdaMART learning to rank technique.

This paper is organised as follows: Section 2 provides a background on robust and risk-sensitive evaluation; Sec-tion 3 defines Z Risk based upon Chi-squared statistic, as well as the GeoRisk derivative; Section 4 &amp; Section 5 demon-strate the proposed measures upon synthetic &amp; real TREC data, while Section 6 shows the integration of GeoRisk within the LambdaMART learning to rank technique; Related work and concluding remarks follow in Sections 7 &amp; 8.
Risk-sensitive evaluation [11] aims at quantifying the trade-off between risk and reward for any given retrieval strat-egy. Information retrieval performance, which is usually measured by a given retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [9]) over a set of topics Q , can be expressed in terms of risk and reward as a risk function. Such a risk function takes into account the downside-risk of a new system s with respect to a given baseline system b (i.e. a loss: performing a topic q worse than the baseline accord-ing to the effectiveness measure, s q &lt; b q ) and an orthogonal reward function that takes into account the upside-risk (i.e. a win: performing a topic better than the baseline, s q &gt; b
A single measure, U Risk [27], which allows the tradeoff between risk and reward to be adjusted, is defined as: where c = | Q | and  X  q = s q  X  b q . The left summand in the square brackets, which is the sum of the score differences  X  for all q where s q &gt; b q (i.e. q  X  Q + ), gives the total win (or upside-risk) with respect to the baseline. On the other hand, the right summand, which is the sum of the score differences  X  for all q where s q &lt; b q , gives the total loss (or downside-risk). The risk sensitivity parameter  X   X  0 controls the tradeoff between reward and risk (or win and loss):  X  = 0 calculates the average change in effectiveness between s and b , while for higher  X  , the penalty for under-performing with respect to the baseline is increased: typically  X  = 1 , 5 , 10 [12] to penalise risky systems, where  X  = 1 doubles the emphasis of down-side risk compared to  X  = 0.

Recently, Din  X cer et al. [13] introduced a statistically-ground-ed risk-reward tradeoff measure, T Risk , as a generalisation of U Risk , for the purposes of hypothesis testing: where SE (U Risk ) is the standard error in the risk-reward tradeoff score U Risk . Here, T Risk is a linear monotonic transformation of U Risk . This transformation is called stu-dentisation in statistics (c.f., t -scores) [16], and T Risk be used as the test statistic of the Student X  X  t -test. More-over, the aforementioned work shows that T Risk permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics that lead to a significant level of risk in order to learn effective yet risk-averse ranking systems.
On the other hand, the comparative risk-sensitive evalu-ation of different IR systems is challenging, as the systems may be based upon a variety of different (base) retrieval models  X  such as learning to rank or language models  X  or upon different IR platforms (Indri, Terrier etc.). It has been shown that using a particular system as the baseline in a comparative risk-sensitive evaluation of a set of diverse IR systems  X  as attempted by the TREC 2013 and 2014 Web track  X  yields biased risk-reward tradeoff measurements [14], especially when the systems under evaluation are not varia-tions of the provided baseline system. To address this, the use of the within-topic mean system performance was pro-posed as an unbiased baseline (as well as the within-topic median system performance and the within-topic maximum system performance). Given a particular topic q and a set of r systems, the arithmetic mean of the r performance scores according to an evaluation measure observed on q is the un-biased baseline score: where s i ( q ) is the performance score of system i on topic q measured by a given evaluation measure (e.g. ERR@20) for i = 1 , 2 ,...,r . Since the arithmetic mean gives equal weight to every retrieval strategy in determining the within-topic mean system performance, a baseline system that is deter-mined by the Mean q scores will be unbiased with respect to the retrieval strategies yielding the r system scores.
However, as shown in [14], the use of Mean q exposes a problem about the validity of the comparative risk-sensitive evaluation of different IR systems. This issue is related to the risk-based rankings of the systems obtained using Mean q . Indeed, such a comparison of the risk-sensitive per-formances of different IR systems actually implies the com-parison of the retrieval effectiveness of the individual sys-tems based on the underlying effectiveness measure, i.e. ERR-@20 [14]. That is, the ranking of the systems obtained by using the underlying effectiveness measure will be the same as the risk-based ranking of the systems obtained using the unbiased baseline Mean q , irrespective of the value of the risk sensitivity parameter  X  .

Most importantly, the previously proposed risk measures are only sensitive to the mean and the variance of the ob-served losses and wins, i.e. U Risk is sensitive to mean and T
Risk is sensitive to mean and variance (c.f. SE (U Risk However in a comparative risk-sensitive evaluation, we argue that it is necessary to be sensitive to the shape of the score distributions, as well as the mean and the variance. As such, in the next section, we propose the Z Risk measure, which sat-isfies the aforementioned variance and shape requirements of a comparative risk-sensitive IR evaluation, while the deriva-tive GeoRisk measure enhances Z Risk by naturally incorpo-rating the overall effectiveness of the considered system.
Each existing robust and risk-sensitive evaluation measure each encodes properties about what a good (or bad) IR sys-tem should exhibit. Firstly, the classical mean measure (e.g. MAP or mean NDCG) stipulates that a good system should perform well on a population of topics on average; The geo-metric mean (e.g. as proposed in [24] for Mean Average Pre-cision as GMAP) says that a good system should avoid per-forming lowly on any topics, while comparing GMAP values permits identifying improvements in low performing topics, in contrast to mean, which gives equal weight to absolute changes in per-topic scores, regardless of the relative size of the change [4]. Risk-sensitive evaluation measures such as U
Risk and T Risk use the notion of a baseline -a good system should perform well, but preferably no worse than the given baseline. Hence U Risk responds to changes in the mean ef-fectiveness of the system, but emphasises those worse than the baseline. Building upon U Risk , T Risk is also sensitive to the variance exhibited by a system across the population of topics. These attributes are highlighted in Table 1.
In this section, we argue for a risk measure that considers the  X  X hape X  of a system X  X  performance across topics. In par-ticular, we consider that the distribution of the effectiveness scores of a set of baseline systems across the topics, mapped to the same overall mean effectiveness as the system at hand, represents an expected performance for each topic that the system should not underperform. In other words, we cal-culate the expectation of the system X  X  performance for each topic, by considering the overall performance of the current system and the observed performances of other baseline sys-tems. This allows to determine topics that the system should be performing better on. It follows that our proposal encap-M easure B aseline P enalty of S ensitive to: l ow topics M ean Var. Shape M ean AP N one N one 4 77
Geo. MAP N one F ocus on low-GeoRisk M ultiple 1 +  X  4 44 T able 1: Comparison of existing and proposed robustness/risk-sensitive measures.
 sulates two separate measures: Z Risk , introduced in Sec-tion 3.1, which measures the shape of the system X  X  perfor-mance irrespective of the overall magnitude of effectiveness; and later in Section 3.2 we show how to create a risk-measure responsive to mean effectiveness called GeoRisk. The sub-sequent Section 4 &amp; Section 5 demonstrate the Z GeoRisk measures upon artificial and TREC data.

The first measure, Z Risk , is inspired by the Chi-square statistic used in the Chi-square test for goodness-of-fit, which is one of the well-established nonparametric hypothesis tests in categorical data analysis [1]. In statistics, goodness-of-fit tests are used to decide whether two distributions are signif-icantly different from each other in shape/form. In relation to risk-sensitive evaluation, this means that, given a sample of topics, a risk measure based on Chi-square statistic per-mits quantifying the difference in the performance profiles of two IR systems across the topics. As mentioned above, none of the previously proposed risk measures are sensitive to the score distributions of IR systems on topics. However, risk-sensitive evaluation, by nature, should take into account all of shape, mean and variance, while Z Risk is independent of overall mean effectiveness. Hence, building upon Z Risk , we propose the GeoRisk measure, which covers all of the afore-mentioned aspects including the overall mean effectiveness of the system at hand, as highlighted in Table 1. Z Risk is best explained by deriving it directly from the Chi-square statistic used in the Chi-square test for goodness-of-fit. In particular, the Chi-square statistic is calculated over a data matrix of r  X  c cells, called the contingency ta-ble. The result of an IR experiment involving r systems and c topics can be represented by a r  X  c data matrix X , whose rows and columns correspond respectively to the r systems and c topics, where the cells x ij (for i = 1 , 2 ,...,r and j = 1 , 2 ,...,c ) contain the observed performances of the corresponding systems for the associated topics, mea-sured by an effectiveness measure such as ERR@20. Table 2 provides a graphical portrayal of data matrix X .

For such a data matrix, the row and the column marginal totals are given by S i = P c j =1 x ij and T j = P r i =1 tively, and the grand total is given by N = P r i =1 P c j =1 The average effectiveness of a system i over c topics is given by S i /c and similarly, the within-topic mean system effec-tiveness is given by T j /r .

Given a data matrix X , the Chi-square statistic, G 2 , can be expressed as where the expected value for cell ( i,j ), e ij , is given by
In Equation (5), p j = T j N can be described as the density or mass of column j , for j = 1 , 2 ,...,c . If a row total S distributed on columns proportional to the column masses p , then the Chi-squared differences of the associated cell values from the corresponding expected values will sum up to zero, i.e. P c j =1 ( x ij  X  e ij ) 2 = 0. Note that e ij 1, where p j = x ij /S i since N = S i . Intuitively, when there is only one IR system, the expected system performance for any topic j will be equal to the score observed for that sys-tem. When r = 1, G 2 = 0, meaning that the observed score distribution of the system across topics is perfectly fit to it-self. Thus, G 2 values that are greater than zero indicate a discordance between two distributions, above or below ex-pectations. This makes G 2 not directly applicable as a risk-sensitive evaluation measure, since it equally and uniformly penalises both downside (losses) and upside risk (wins). In contrast, risk-sensitive measures should favour wins and or-thogonally penalise losses. Hence, we propose below a mea-sure derived from G 2 that addresses this limitation.
For large samples, the Pearson X  X  Chi-square statistic G 2 Eq. (4) follows a Chi-square distribution with ( r  X  1)( c  X  1) degrees of freedom and the observed cell values x ij follow a Poisson distribution with mean e ij and variance e ij [1]. This means that the Chi-square statistic can also be expressed as the sum of the square of standard normal deviates [1]:
The square root of the components of Chi-square statis-tic, z ij , gives the standardised deviation in cell ( i,j ) from the expected value e ij (i.e. z -scores). Thus, for large sam-ples, the distribution of z ij values on the population can be approximated by the standard normal distribution with zero mean and unit variance.

It follows that a risk-reward tradeoff measure can be ex-pressed in terms of the standard normal deviates from the expected effectiveness, as given by: for any system i , i = 1 , 2 ,...,r . Q + ( Q  X  ) is the set of queries where z iq &gt; 0 ( z iq &lt; 0, respectively), determined by whether system i outperforms its expectation on topic j (c.f. x ij  X  e ij ).

Z Risk takes the classical form of a risk-sensitive evaluation measure, in that upside risk is rewarded and the effective-ness penalty of downside risk is amplified by  X  -i.e. the higher the Z Risk , the more safe and less risky a system is. In addition, Z Risk calculates the risk of a system in rela-tion to the shape of effectiveness across topics exhibited by multiple baselines. In this way, Z Risk brings a new dimen-sion to the measurement of robustness, originally defined by Voorhees [24] as  X  X he ability of the system to return reason-able results for every topic X  , in that for Z Risk , robustness is measured compared to a per-topic expectation calculated from a population of baseline systems.
As noted before, a limitation of Z Risk is that it measures robustness irrespective of the mean effectiveness of IR sys-tems. Indeed, one may consider that the baseline for any given system i is composed of the expected per-topic scores of the system, e ij , such that the sum of expected per-topic scores is equal to the sum of the observed per-topic scores of the system, i.e. P j e ij = S i . This means that Z Risk sures robustness using individual baselines for every system, each of which is derived on the basis of the observed total effectiveness of the system (i.e. S i ) and the observed topic masses (i.e. T j ). This makes the robustness/risk measure-ments of Z Risk independent of the observed mean effective-ness of the systems, i.e. P j x ij = P j e ij for i = 1 , 2 ,...,r .
On the other hand, for the purposes of the comparative risk-sensitive evaluation of different IR systems, we combine the risk measure with the effectiveness measure in use, Z and ERR@20 for example, into a final measure. A natural method for such a combination is the geometric mean , which is expressed as the n th root of the product of n numbers. The geometric mean is a type of average, like arithmetic mean, that represents the central tendency in a given set of numbers. In contrast to the arithmetic mean, the geometric mean normalises the ranges of the variables, so that each datum has an equal impact on the resulting geometric mean. Hence, the geometric mean of the ERR@20 scores and the Z
Risk scores represents, evenly, both the effectiveness and the robustness of system s i under evaluation: where 0  X   X ()  X  1 is the cumulative distribution function of the standard normal distribution. In this way, we use  X () to normalise Z Risk into [0,1], because  X  X  X  X  Z Risk /c  X  X  X  . To illustrate Z Risk and GeoRisk introduced in Section 3, Table 3 presents an example data matrix X composed of 8 systems and 5 topics. The effectiveness scores of the exam-ple systems are artificially determined so that the resulting performance profiles of the systems across the topics serve as a basis to exemplify some potential differences in per-formance profiles of IR systems in relation to their mean effectiveness. Figure 1 shows the performance profiles of the 8 systems, which can be characterised as follows:  X  Systems s 1 and s 2 have the same mean effectiveness over the 5 topics (i.e. 0.3000) but the scores of s 1 are monoton-ically increasing in magnitude across the topics, whereas, the scores of s 2 are monotonically decreasing. That is, s and s 2 have contrasting performance profiles across the topics, with respect to the same mean effectiveness score of 0.3000.  X  Systems s 3 and s 4 have constant scores across the top-ics that are equal to their respective mean effectiveness scores. In other words, these systems have constant per-formance profiles, while system s 3 has the same mean ef-fectiveness as both s 1 and s 2 .  X  Systems s 5 and s 6 again have the same mean effectiveness as systems s 1 and s 2 , but have alternating scores across the topics, such that one has a higher score in magnitude
Figure 1: Example systems X  performance profiles. than the other for one topic and vice versa for the next topic. We describe such systems as having alternating performance profiles across the topics.  X  Systems s 7 and s 8 have different mean effectiveness scores from each other and also from that of the other systems.
Their performance profiles are visually parallel to each other, and concordant with the profile of the mean topic scores, i.e. the row  X  X ean X  of Table 3.
Measuring the level of risk associated with a given IR sys-tem s with respect to a particular single baseline system b means that, in total, there are two systems under consider-ation, i.e. r = 2. For such a risk-sensitive evaluation, the Chi-square statistic G 2 is given by and, under the null hypothesis that the observed score distri-butions of both systems follow a common distribution with mean  X  and variance  X  2 , it can be expressed as where x sj is the observed score of the system s for topic j , and x bj is the observed score of the baseline system b . Note that, when there are only two systems, T j = x sj + x bj , and hence x bj = T j  X  x sj and x sj = T j  X  x bj . Here, where N = S s + S b .

In fact, given two IR systems, the level of risk associated with any one of the two systems can be measured by taking the other system as the baseline, as implied by Eq. (8). Most importantly, Eq. (8) suggests, in this respect, that, if the systems show an equal mean performance over a given set of c topics (i.e. S s = S b ), the measured level of risk will be the same for both systems. In risk-sensitive evaluations, a baseline system defines what is a robust system, so that risk can be quantified as the degree of divergence from that baseline. However, given a set of IR systems, taking every system as a baseline, actually contributes information for the qualification of a robust (i.e. not  X  X isky X  or safe) system on the population of topics. In this regard, multiple baselines can provide more information about the real level of risk associated with any IR system.

Let system s 1 in Table 3 be the baseline system. The level of risk associated with system s 2 , which has the same mean performance with s 1 , is Z Risk = 0 . 1141, while the level of risk associated with s 1 for baseline s 2 is the same in magni-tude but different in sign, i.e.  X  0 . 1141. The sign of the Z scores indicates the direction of the observed level of risk-reward tradeoff, where minus indicates down-side risk and plus indicates up-side risk. Table 4 shows the calculated values of Z Risk at  X  = 0 for each system i = 2 , 3 ,..., 8. As can be seen, for those systems whose mean performances are equal to the mean performance of s 1 , only the sign of the cal-culated Z Risk values changes when the baseline is swapped.
Based on the calculated Z Risk values when the baseline is s 1 , system s 4 is the least  X  X isky X  system among the 8 ex-ample runs with the highest Z Risk value of 0 . 1583 (i.e. the s vs. s 1 row of the table). However, as can be seen in Fig-ure 1, s 3 , s 7 , or s 8 are relatively less  X  X isky X  than s those three systems have performance profiles that are con-cordant/parallel with that of s 1 and also they have relatively higher mean effectiveness scores than s 4 : thus, s 4 could not be considered less  X  X isky X  than s 3 , s 7 , or s 8 . The reason be-hind this counter-intuitive result is two-fold. Firstly, base-line system s 1 has performance scores that are monotonically increasing in magnitude across the topics. Thus, as a base-line, it suggests that the expected system performance on the population of topics that is represented by the sample topic t 1 would be low, and for the population of topics repre-sented by t 2 it would be relatively higher than that of t so on. However, as seen from Figure 1, considering the ob-served scores of the other systems, it would appear that the expected per-topic system performances are in general dif-ferent from those that the system s 1 suggests, i.e. the  X  X ean X  row of Table 3. Secondly, the risk that is measured by Z is related to the distribution of the total system performance S i on topics with respect to the expected per-topic system performances, and is not dependent on the magnitude of the mean performance of the systems across topics.

These two issues explain the above counter-intuitive result that s 4 is declared as the least  X  X isky X  system. Indeed, the former issue can be resolved by employing multiple baselines Table 5: Z Risk and GeoRisk for the example systems. as shown in the following Section 4.2, and the latter issue of independence from the magnitude of mean effectiveness can be resolved as shown in Section 4.3, where the risk measure Z
Risk and the measure of effectiveness are combined into a single measure of effectiveness, GeoRisk.
Chi-square statistic allows the use of all systems in data matrix X as multiple baselines for risk-reward tradeoff mea-surements using Z Risk . Recall that the expected value for cell ( i,j ), e ij , is given by For the case of a single baseline system b , given a particular system s , to calculate the mass or density p j of topic j , the within topic total performance score T j is taken as x sj i.e. p j = ( x sj + x bj ) /N . Similarly, given a set of baselines, the topic masses can be calculated as for each topic j = 1 , 2 ,...,c . Intuitively this means that, given a set of r systems, the level of risk associated with every system is measured by taking the remaining ( r  X  1) systems as the baseline. Compared to the case of taking a particular system as the baseline, as the number of baseline systems increases, the accuracy of the estimates of expected system performance for each topic increases, and hence the accuracy of the estimates of real risk increases.

Table 5 shows the calculated Z Risk values for each of the 8 example runs at  X  = 0 , 1 , 5 , 10. We observe from the table that, as the risk sensitivity parameter  X  increases, example systems s 7 and s 8 exhibit the lowest levels of risk relative to the other systems, (i.e.  X  = 1 , 5 , 10), while s the highest level of risk ( Z Risk =  X  6 . 835 at  X  = 10). As can be seen, using multiple baselines resolves the effect of the lack of information about the expected per-topic system performance in assessing the risk levels of systems, i.e. s vs. s 7 and s 4 vs s 8 . In the following section, we show how to combine Z Risk with mean system effectiveness in order to solve the last issue about Z Risk , i.e. the counter-intuitive case of s 4 vs. s 3 , where the measured level of risk for s higher than that of s 4 (e.g. the Z Risk score of s 3 is  X  0 . 368 and it is  X  0 . 336 for s 4 at  X  = 5), while s 3 has higher effec-tiveness score than s 4 (i.e. 0 . 300 vs. 0 . 250) and it has also a performance profile concordant with that of s 4 .
Table 5 shows the calculated GeoRisk values for each of the 8 example runs. As can be seen, for the case of s 4 vs. s , the issue of the independence of Z Risk measurements from the magnitude of the mean effectiveness of IR systems is solved. The example system s 3 is now measured as less
Figure 2: Example systems X  GeoRisk as 0  X   X   X  10 .
Figure 3: GeoRisk plot for 8 TREC 2012 runs.  X  X isky X  than s 4 , as suggested by the magnitude of the ob-served mean effectiveness scores.

Figure 2 shows the plot of GeoRisk scores for each example system for  X  = 0 , 1 , 2 ,..., 10, where the systems with lines sloping downward along the increasing values of  X  (i.e. x -axis) are those that exhibit a risk of abject failure, (i.e. s s , s 5 , and s 6 ) while, in contrast, the robust systems such as s 3 , s 4 , s 7 and s 8 have nearly a straight, horizontal lines.
In summary, the GeoRisk measure takes into account both the mean effectiveness of IR systems and the difference in the shapes of their performance profiles. As a result, GeoRisk is sensitive to mean (i.e. the component S i /c ), variance and the shape of the observed effectiveness scores across topics (i.e. the Z Risk component).
In this section, we demonstrate the use of the risk-reward tradeoff measure derived from the Chi-square statistic, Z and the aggregate measure GeoRisk, on real systems submit-ted to the TREC 2012 Web track [10] 1 , in comparison with the existing measures U Risk and T Risk . In the subsequent year of the Web track [12], a standard baseline called indriC-ASP and based on the Indri retrieval platform was provided. Similar to [13, 14], we use the same indriCASP system as the nominal single baseline on the 2012 Web track topics.
In particular, out of the 48 runs submitted to TREC 2012, we select the top runs of the highest 8 performing groups, based on the mean ERR@20 score, While we omit other submitted runs for brevity, the following analysis would be equally applicable to them. For each run, we report the risk-reward tradeoff scores obtained using the official TREC 2012 evaluation measure, ERR@20. Although our analysis is equally applicable to the TREC 2013 Web track, due to the lack of space, we report results from TREC 2012, which are also directly comparable to that of previous works [13, 14].

Table 6 lists the U Risk , T Risk , T  X  Risk , Z Risk and GeoRisk risk-reward tradeoff scores for the 8 runs. For the mea-sures U Risk and T Risk , the baseline run is indriCASP ; for the measures Z Risk and GeoRisk, we use as multiple base-lines all 48+1 TREC 2012 runs including indriCASP ; T  X  Risk denotes T Risk calculated using the per-topic mean effective-ness of the 49 runs as the baseline, i.e. Mean q Note that Din  X cer et al. [13] showed that T Risk is inferential, i.e. the T Risk scores correspond to scores of the Student X  X  t statistic. For this reason, for the U Risk scores in TREC 2012 in Table 6 (where c = 50), T Risk &gt;  X  2 indicates that the observed U Risk score exhibits a significant level of risk.
Table 6 shows in general that the notion of risk quan-tified by the Chi-square statistic-based risk measure Z Risk differs from that of the U Risk , T Risk and T  X  Risk as illustrated by the contrasting systems X  rankings (the col-umn R next to each measure) for Z Risk . In particular, at  X  = 0, U Risk and T Risk agree with the effectiveness mea-sure ERR@20 on the rankings of the 8 TREC 2012 runs. However, at  X  = 5, although U Risk and T Risk still agree with each other, they both diverge from the agreement with ERR@20. On the other hand, the risk measure Z Risk agrees neither with ERR@20 nor with the risk measures U Risk and T
Risk . Note that, except for the determination of baselines, the three risk measures U Risk , T Risk , and Z Risk rely on the same notion of risk and reward, i.e. down-side risk and up-side risk. Thus, comparing Z Risk with U Risk and T it follows that multiple baselines (i.e. 49 TREC 2012 runs) provide information that is different from the information provided by the single baseline system indriCASP .

According to Z Risk , the most robust run is  X  X ogTrA44xu X  with a Z Risk value of 0 . 962 at  X  = 0, and the next is  X  X rra12c X  with Z Risk = 0 . 265, and so on, given the expected per-topic performance scores representing the baselines for each system. Based on the definition of Z Risk , it is expected that  X  X ogTrA44xu X  would perform any given topic with an ERR@20 score that is better than or equal to the expected score for that topic on a population of systems with mean ERR@20 scores equal to 0.3406. Conversely, the least robust or most  X  X isky X  run is  X  X rchvrs12c00 X  with a Z Risk =  X  0 . 912.
Recall that Z Risk is independent of the observed mean effectiveness scores of the systems, which is, by definition, inappropriate for the purpose of a comparative IR evalua-tion. Thus, as an aggregate measure, GeoRisk, the geomet-ric mean of Z Risk and ERR@20, can be used to tackle this challenge. As can be seen in Table 6, GeoRisk agrees with ERR@20 at  X  = 0 on the rankings of the 8 TREC 2012 runs. Here, GeoRisk gives equal weights to ERR@20 and Z Risk , and similarly, at  X  = 0, Z Risk gives equal weights to down-side risk and up-side risk. Thus, the observed agreement between GeoRisk and ERR@20 implies that the measured Z
Risk scores for each of the 8 TREC 2012 runs at  X  = 0 are negligible compared to the observed differences in effective-ness between the runs. In other words, every TREC 2012 run exhibits risk, to a certain extent, but none of the mea-sured risk levels are high enough to compensate for the ob-served difference in mean effectiveness between two systems, so that a swap between risk and reward for a given topic is likely to occur for two systems on the population of top-ics. Note that the agreements of T Risk , as well as U Risk  X  = 0, with ERR@20 also give support in favour of the same conclusion, i.e. the practical insignificance of the measured levels of risk at  X  = 0.

On the other hand, as  X  increases (i.e. as the emphasis of down-side risk increases in Z Risk measurements), GeoRisk GeoRisk at  X  = 0 , 1 , 5 , 10 , 20 . For U Risk and T Risk over all 48 + 1 TREC 2012 runs including indriCASP , and for Z diverges from ERR@20 and ranks the 8 TREC 2012 runs in a way that is different from all of the risk measures under consideration including Z Risk (for example,  X  = 5), and at a high value of  X  , it converges to an agreement with Z Risk the systems X  rankings in Table 6 at  X  = 20 for GeoRisk and at  X  = 5 for Z Risk . The tendency of GeoRisk towards Z Risk as  X  increases is expected from the definition of GeoRisk.
Figure 3 plots the GeoRisk scores calculated for each run at  X  = 0 , 1 ,..., 20. As can be seen, each of the 8 TREC 2012 runs has a decreasing GeoRisk score in magnitude, as the risk sensitivity parameter  X  increases. This means in gen-eral that -to a varying extent -every run under evalua-tion is subject to the risk of committing an abject failure, as the importance of getting a reasonable result for every topic increases. In particular, the runs  X  X ogTrA44xu X  and  X  X CTNET12ADR2 X  keep their relative positions in the ob-served rankings across all  X  values, while the ranks of the other runs considerably change. For example, the rank of  X  X rchvrs12c00 X  changes from 2 at  X  = 0 to 7 at  X  = 5. At  X  = 0, the run with rank 7 is indriCASP . However, the calculated risk for  X  X rchvrs12c00 X  at any level of  X  cannot be considered as empirical evidence to favour indriCASP over  X  X rchvrs12c00 X  for any given topic from the popula-tion, since the mean effectiveness of  X  X rchvrs12c00 X  is sig-nificantly higher than the mean effectiveness of indriCASP ( p &lt; 0 . 0239, paired t -test).

Note that, for two IR systems whose mean effectiveness scores are significantly different from each other, a measured risk level could have no particular meaning from a user per-spective. This is because the system with higher mean ef-fectiveness would be the one that can fulfil the users X  infor-mation needs better than the other on average, no matter what level of risk is associated with it. The system with sig-nificantly low mean effectiveness would, on average, fail to return a  X  X easonable X  result for any given topic, compared to the other system X  X  effectiveness. For a declared signifi-cance with a p -value of 0 . 05, a swap in scores between the two systems for a topic (i.e. a transition from risk to reward or vice versa between the systems) is likely to occur 5% of the time on average [26].

Nevertheless, the same case is not true for runs  X  X Falah-121A X  and  X  X UTparaBline X , whose observed mean effective-ness scores are not significantly different from the mean ef-fectiveness of  X  X rchvrs12c00 X . The paired t -test, which is performed at a significance level of 0.05, fails to give signif-icance to the observed difference in mean effectiveness be-tween  X  X Falah121A X  and  X  X rchvrs12c00 X  with a p -value of 0.7592, and similarly for  X  X UTparaBline X  with a p -value of 0.7003. This means that, for a given topic, a transition from risk to reward, or vice versa, between the runs  X  X Falah121A X  and  X  X rchvrs12c00 X , or between runs  X  X UTparaBline X  and  X  X rchvrs12c00 X , is highly likely to occur on the population of topics. Thus, both systems can be considered less  X  X isky X  or more robust than  X  X rchvrs12c00 X .

In summary, this analysis of the TREC 2012 Web track runs demonstrates the suitability of GeoRisk for balancing risk-sensitivity and overall effectiveness, and the importance of using multiple baselines within the appropriate statistical framework represented by Z Risk . The analysis performed for the 8 TREC runs shows overall that, in a comparative IR evaluation effort, relying only on the observed mean effec-tiveness of the systems may be misleading, even for a best performer system like  X  X rchvrs12c00 X , where the risk asso-ciated with such a system is high enough that it can cause an over-estimation of the real performance of the system. However, we showed that GeoRisk provides a solution for identifying systems exhibiting such risks.
In this section, we show how GeoRisk can be integrated within the state-of-the-art LambdaMART learning to rank technique [7, 28]. Indeed, Wang et al. [27] showed how their U Risk measure could be integrated within LambdaMART. Similarly, Din  X cer et al. [13] proposed variants of T Risk resulted in learned models that exhibited less risk. The method of integration of risk-sensitive measures into LambdaMART requires adaptation of its objective function. In short, LambdaMART X  X  objective function is a product of (i) the derivative of a cross-entropy that was originally de-fined in the RankNet learning to rank technique [6], based on the scores of two documents a and b , and (ii) the abso-lute change  X  M ab in an evaluation measure M due to the swapping of documents a and b [28]. Various IR measures (e.g. NDCG) can be used as M , as long as the measure is consistent : for each pair of documents a and b with differing relevance labels, making an  X  X mproving swap X  (moving the higher labelled document above the lesser) must result in  X  M ab  X  0, and orthogonally for  X  X egrading swaps X  .
In adapting LambdaMART to be more robust within a risk-sensitive setting, the  X  M is replaced by a variant  X  M that considers the change in risk observed by swapping doc-uments a and b , according to the underlying evaluation mea-sure M , e.g. NDCG. In the following, we summarise existing instantiations of  X  M 0 arising from U Risk and T Risk (called U-CRO, T-SARO and T-FARO), followed by our proposed instantiation of GeoRisk within LambdaMART.
 U-CRO: Constant Risk Optimisation based upon the U Risk measure [27] (U-CRO) maintains a constant risk-level, re-gardless of the topic. In particular, let M m define the effec-tiveness of the learned model m according to measure M , and let M b define the effectiveness of the baseline b . Corre-spondingly, M m ( j ) ( M b ( j )) is the effectiveness of the learned model (baseline) for query j , then:  X  M 0 =  X  M if M m ( j ) +  X  M  X  M b ( j );  X  M  X  (1 +  X  ) otherwise . T-SARO &amp; T-FARO: Adaptive Risk-sensitive Optimisa-tion makes use of the fact that T Risk can identify queries that exhibit real (significant) levels of risk [13] compared to the baseline b . Din  X cer et al. [13] proposed two Adaptive Risk-sensitive Optimisation adaptations of LambdaMART, namely T-SARO and T-FARO, which use this observation to focus on improving those risky queries. Indeed, in U-CRO,  X  M is multiplied by (1 +  X  ), for a static  X   X  0. In T-SARO and T-FARO,  X  is replaced by a variable  X  0 , which varies according to the probability of observing a query with a risk-reward score greater than that observed. By modelling this probability using the standard normal cumulative dis-tribution function Pr Z  X  T R j  X  1  X   X  T R j , T-SARO replaces the original  X  in Eq. (9) with  X  0 as: where T R j =  X  j /SE (U Risk ) determines the level of risk ex-hibited by topic j . T-SARO and T-FARO contrast on the topics for which  X  0 is adjusted  X  while T-SARO only adjusts topics with downside risk as per Eq. (9), T-FARO adjusts all topics: The experiments in [13] found that T-FARO exhibited higher effectiveness than T-SARO, thus we compare GeoRisk to only U-CRO and T-FARO in our experiments.
 GeoRisk: Our adaptation of  X  M for the newly proposed GeoRisk is more straightforward than the T Risk Adaptive Risk-sensitive Optimisations, in that we use GeoRisk di-rectly as the measure within LambdaMART.  X  M 0 = GeoRisk ( M m +  X  M )  X  GeoRisk ( M ) Indeed, GeoRisk is suitable for LambdaMART as it exhibits the consistency property: an improving  X  X wap X  will increase both the left factor ( Si/c ) and the right factor Z therefore the value of GeoRisk for M m + X  M will likewise in-crease. Moreover, as  X  M is calculated repeatedly during the learning process, the speed of the implementation is critical for efficient learning. In this respect, it is important to note that retaining the values of the separate z ij summands for each query in the Z Risk calculation (see Equation (6)) allows the new Z Risk value to be calculated by only recomputing the z ij for the query affected, then recalculating GeoRisk.
Recall from Section 3 that z ij encapsulates differential weighting of downside and upside risks, but with respect to the expected performance on the topic. Hence, by us-ing GeoRisk, the objective function of LambdaMART will favour learned models that make improving swaps of doc-uments on topics where the learned model performs below expectation as calculated on the set of baselines.

Naturally, the instantiation of GeoRisk within a learning to rank setting depends on the set of baselines X , to allow the estimation of the topic expectations e ij (see Eq. (5)). The choice of baselines to provide for learning can impact upon which topics the learner aims to improve. Previous works on risk-sensitive learning [13, 27] have used the per-formance of a single BM25 retrieval feature as the baseline. Indeed, single weighting models such as BM25 are typically used to identify the initial set of documents, which are then re-ranked by the learned model [19, 20], and hence it is a baseline that the learner should outperform. However, it does not represent the typical performance of a learned ap-proach upon the queries, as it cannot encapsulate the effec-tiveness of refined ranking models using many other features.
Hence, instead of using GeoRisk to learn a model more effective than a set of BM25-like baselines, we argue for the use of state-of-the-art baselines, which portray repre-sentative estimations of query difficulty to the learner. Such baselines are more akin to the systems submitted to TREC (which themselves have been trained on previous datasets), rather than a single weighting model feature. In a deployed search engine, such state-of-the-art-baselines could represent the effectiveness of the currently deployed search engine, or other deployed search engines for the same query. In an ex-perimental setting, such as in this paper, we use held-out data to pre-learn several learned models before conducting the main comparative experiments. Finally, for compari-son purposes, we also deploy T*-FARO in our experiments, where the mean performance of the state-of-the-art baselines for each topic is used as the single learning baseline.
Our experiments use the open source Jforests learning to rank tool [15] 2 , which implements U-CRO, and T-FARO, as well as plain LambdaMART. Our implementations of T*-FARO &amp; GeoRisk are also built upon Jforests 3 . As base-lines, in addition to LambdaMART, we also deploy a plain gradient boosted regression trees learner (also implemented by Jforests), and two linear learned models, Automatic Fea-ture Selection (AFS) [21] &amp; AdaRank [19, Ch. 4]. We conduct experiments using the MSLR-Web10k dataset 4 . This learning to rank dataset contains 136 query-dependent and query-independent feature values for documents retrieved for 10,000 queries, along with corresponding relevance labels. As highlighted above, our baselines require separate train-ing. For this reason, we hold out 2000 queries for initial training (two thirds) and validation (one third). The re-maining 8000 queries are then split into 5 folds, each with a balance of 60% queries for training, 20% for validation, and 20% for testing. Hence, our reported results are not compa-rable to previous works using all 10000 queries of the MSLR dataset, but instead performances for LambdaMART, U-CRO &amp; T-FARO are presented on the 8000 queries. The underlying evaluation measure M in each learner is NDCG.
For GeoRisk &amp; T*-FARO, the multiple baselines are eval-uated for each query in the main 5 folds, which represent  X  X nseen X  queries for those systems. For U-CRO, T-SARO and T-FARO, the baseline is depicted by the performance of the BM25.wholedocument feature.

Finally, we note that LambdaMART has several hyper-parameters, namely the minimum number of documents in each leaf m , the number of leaves l , the number of trees in the ensemble nt and the learning rate r . Our experiments use a uniform setting for all parameters across all folds, namely m = 1500, l = 50, nt = 500, l = 0 . 1, which are similar to those reported in [27] for the same dataset.
In Table 7, we report the NDCG@10 effectiveness and ro-bustness for LambdaMART, U-CRO, T-FARO, T*-FARO https://github.com/yasserg/jforests
We have contributed GeoRisk as open source to Jforests. http://research.microsoft.com/en-us/projects/mslr/ and GeoRisk for  X  = { 1 , 5 } 5 . The table is split into two halves: comparison to the effectiveness of the single BM25 baseline, and comparison viz. the 4 baseline learned models. For each of the baselines, we report the reward to risk ra-tio (denoted  X  X eward/Risk X ), which measures the gain over the effectiveness of the baseline. Similarly, the win to loss ratio (denoted  X  X /L X ) measures the number of queries that the risk-sensitive optimisation contributed to reward against risk. Finally, the number of queries that each model wins or looses relative to the baseline are also shown for each  X  value, along with the number of queries that experience a relative loss greater than 20% NDCG@10. For clarity, the header of Table 7 denote arrows to show the favourable di-rection of each measure, e.g.  X  means that higher is better. On analysis of the top half of Table 7, we observe that GeoRisk generates the highest mean NDCG effectiveness, marginally improving over LambdaMART. This is also ob-servable for the Reward measure, in comparison to the BM25 baseline. However, for the risk measures, T-FARO and U-CRO obtain the best values. For  X  ,  X  = 1 is deemed the appropriate setting across all risk-sensitive learners, which has the effect of doubling the penalty of a query under-performing the corresponding baseline for that learner.
In the bottom half of Table 7, we examine the performance profiles of the different learners compared to the effectiveness of the 4 learned model baselines -the measures reported are the macro-average, i.e. the mean when each measure is cal-culated with respect to each learned baseline in turn (rather than compared to the micro-averaged effectiveness of the 4 learned baselines). In this half of the table, we note that, for  X  = 1, GeoRisk demonstrates the highest Reward and num-ber of Wins and lowest Losses (and the resulting best Re-ward/Risk &amp; Win/Loss ratios (the latter is a 2.7% improve-ment over LambdaMART). These improvements in the risk profile of the systems are achieved while still attaining the highest mean NDCG effectiveness among the systems. All differences are statistically significant ( n = 8000 queries).
Finally, we note that the effectiveness and risk profiles at-tained by T*-FARO are markedly different, with T*-FARO attaining the lowest Reward/Risk &amp; Win/Loss ratios. This verifies that the use of expected topic performance by Geo-Risk rather than a mean per-topic performance (as used by T*-FARO) results in a learned model more attuned to the normal performances of state-of-the-art baseline systems.
Overall, this empirical evidence confirms our claim that the new risk-sensitive objective function GeoRisk for the LambdaMART learning to rank technique allows effective yet robust learned models to be obtained using multiple baselines. Moreover, we would highlight the more impressive risk-profiles attained in the bottom half of Table 7, which demonstrate that given a set of state-of-the-art baselines, using GeoRisk can generate an effective model that is as effective as LambdaMART with better risk profiles, and al-lows learning-to-rank to benefit from natural incremental improvements as practiced in real deployment settings.
One aspect of this paper is the assessment of the robust-ness of IR systems, initiated first by the TREC Robust track [24] based on the geometric mean average precision measure [23, 25], and developed further by the introduc-tion of new measures of risk/robustness, such as U Risk [27] and T Risk [13]. In this respect, while Din  X cer et al. [13] in- X  =0 is equivalent to the normal LambdaMART algorithm. vestigated the use of the Student X  X  t -test for risk-sensitive evaluation, this is the first work to investigate the use of Pearson X  X  Chi-square statistic for risk-sensitive evaluation, thereby facilitating the use of multiple baselines. Instead, previous usages of the Chi-square statistic has encompassed index term weighing [18] and document classification [22].
In IR experimentation, Armstrong et al. [3] noted that many papers appeared to show improvement upon older, weaker baselines. More recent work by Kharazmi et al. [17] showed that testing upon state-of-the-art baselines is neces-sary to demonstrate an advance. Similarly, this paper advo-cates the use of multiple state-of-the-art baselines, both in experimental and learning settings.

We also note several attempts to develop robust learning to rank techniques: of note, the AdaRank technique [19, Ch. 4] focuses on improving hard queries using boosting. Since then, risk-sensitive optimisation techniques such as U-CRO [27], T-SARO &amp; T-FARO [13] have aimed to adapt the LambdaMART technique by identifying risky topics with re-spect to a single baseline. Our work goes further by making use of multiple state-of-the-art baseline systems when cal-culating risk-estimation in the learning to rank objective function. Finally, Bennett et al.[5] take a different route, by developing personalised risk-averse ranking strategies upon the LambdaMART technique. As they build upon U-CRO, it may be possible to combine both approaches in the future.
This is the first paper that thoroughly investigated the use of multiple baselines in risk-sensitive evaluation. It argued for a new definition of risk-sensitivity related to the expected performance upon a given topic, calculated from a popula-tion of existing baseline systems. In particular, the paper in-troduced two new risk-sensitive evaluation measures, Z Risk and GeoRisk that are based upon the Chi-square statis-tic. Moreover, while Z Risk estimates risk independent of the overall mean retrieval effectiveness, GeoRisk enhances Z Risk by additionally accounting for overall effectiveness.
Our new measures were demonstrated on the results of a comparative system evaluation from the TREC Web track. Finally, the paper showed how the proposed GeoRisk mea-sure can be directly integrated within the objective func-tion of the state-of-the-art LambdaMART learning to rank technique. Experiments upon 8000 queries from a learning to rank dataset showed that the resulting learned models were as effective as LambdaMART, but also more risk-averse when compared to four learned baselines.
This work is partially supported by TUBITAK, The Scien-tific and Technological Research Council of Turkey (Project No: 114E558). All opinions are that of the authors. [1] A. Agresti. Categorical Data Analysis . Wiley, 2002. [2] G. Amati, C. Carpineto, and G. Romano. Query [3] T. Armstrong, A. Moffat, W. Webber, and J. Zobel. [4] S. Beitzel, E. Jensen, and O. Frieder. GMAP. In differences are statistically significant over the n = 8000 queries. [5] P. N. Bennett, M. Shokouhi, and R. Caruana. Implicit [6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [7] C. J. Burges. From RankNet to LambdaRank to [8] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [10] C. L. A. Clarke, N. Craswell, and E. Voorhees. [11] K. Collins-Thompson. Reducing the risk of query [12] K. Collins-Thompson, P. Bennett, F. Diaz, C. Clarke, [13] B. T. Din  X cer, C. Macdonald, and I. Ounis. Hypothesis [14] B. T. Din  X cer, I. Ounis, and C. Macdonald. Tackling [15] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging [16] D. Hoaglin, F. Mosteller, and J. Tukey, eds. Understa-[17] S. Kharazmi, F. Scholer, D. Vallet and M. Sanderson. [18] I. Kocaba  X s, B. T. Din  X cer, and B. Karao X glan. A [19] T.-Y. Liu. Learning to rank for information retrieval. [20] C. Macdonald, R. L. Santos, and I. Ounis. The whens [21] D. A. Metzler. Automatic feature selection in the [22] M. Oakes, R. Gaaizauskas, H. Fowkes, A. Jonsson, [23] S. Robertson. On GMAP -and other transformations. [24] E. M. Voorhees. Overview of the TREC 2003 Robust [25] E. M. Voorhees. The TREC Robust retrieval track. [26] E. M. Voorhees and C. Buckley. The effect of topic set [27] L. Wang, P. N. Bennett, and K. Collins-Thompson. [28] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao.
