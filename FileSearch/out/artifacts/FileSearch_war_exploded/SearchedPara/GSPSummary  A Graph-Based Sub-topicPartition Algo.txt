 With the rapid increasing of online information and fast development of sci-ence and technology, a lot of researc h efforts have been made on web mining, text mining, information extraction, and information retrieval (IR). However, the conventional IR technologies are becoming more and more insufficient for obtain-ing useful information effectively. Which makes how to summarize documents with all kinds of information increasingly urgent. Therefore, MDS -capable of summarizing either complete document s sets, or a series of documents in the context of previously ones -is likely to be essential in such situations. The goal of text summarization is to take an information source, extract content from it, and present the most important content to the user in a condensed form and in a manner sensitive to the user X  X  application needs [3]. If the summarization system can make an effective summary, whi ch can be a substitute of the original documents, the retrieval effectiveness o r efficiency can be improved and the user can save the reading time.

Usually, the topic of a document collect ioniscomposedofsomeaspectsofin-formation, each aspect is named sub-topic of the document collection. In order to model the sub-topics, many cluster-based approaches have been proposed. These approaches employ a clustering method to model the logic structure of the topic based on the structure of the topic covered in the document collection in the first, follows by a sentence selection method in a a specified cluster. However, the im-plicit logic structure of the topic covered in the document collection is not only represented by the explicit distribution of f eatures (statistical features in usual), but also represented by the implicit distribution of features (centrality, etc).
In this paper, we argue that information selection in a MDS can be based on the implicit logic structure of the topic covered in the document collection. Using the relationship information with graph representation, we investigate the use of sub-topics as a model of the document collection for the purpose of pro-ducing summaries. Furthermore, unlike the two-step cluster-based approaches, we aim to obtain an approach can select important information when modeling sub-topics.

It would be worthwhile to highlight several aspects of our proposed algorithm here: 1. Presenting a new framework of MDS wi th sub-topic model, according to the 2. Proposing a scalable criterion to rank the salience of sentences, in which both 3. Proposing a novel MDS algorithm to determine the sub-topics in global space
The rest of this paper is organized as fo llows. Section 2 relates a review of the previous work. In section 3, we present the proposed graph-based summarization approach using sub-topic partition. The experimental methodologies and results are reported in section 4 and 5, followed by the conclusion and future work in section 6. Generating an effective summary requires the summarizer to select, evaluate, or-der and aggregate items of information according to their relevance to a particular subject or purpose. These tasks can either be approximated by IR techniques or done in great depth with full natural language processing (NLP). Most previous work in summarization has attempted to d eal with the issues by focusing more on a related, but simple problem. Most of the work in sentence extraction applied sta-tistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. (e.g., [9]). Other approaches include the utility of discourse structure [10], the combination of information extraction and language generation [1], and using machine learning to find patterns in text [6][7].
Several researchers have extended various aspects of the single document sum-marization approach to look at MDS [12][13]. These include comparing templates filled in by extracting information -using specialized, domain specific knowledge sources -from the document, and then generating natural language summaries from the templates, comparing named-enti ties -extracted using specialized lists -between documents and selecting the most relevant section, finding co-reference chains in the document collection to identify common sections of interest, or build-ing activation networks of related lexica l items (identity mappings, synonyms, hypernyms, etc.) to extra ct text spans from the document collection [13].
Many of recent researches put emphasis o n the comprehensiveness while keep-ing readability of summaries or maximizing the coverage and the anti-redundancy to keep the comprehensiveness and readability to some extent. For example, Radev et al. [14] proposed a method that classifie s given documents into some clusters and made one sub-summary for each cluster, then placed them in an order.
Carbnell [1] proposed the Maximal Marginal Relevance (MMR) criterion for combining query relevance with information novelty in the context of text re-trieval and summarization. Goldstein et al. [11] proposed a method called MMR-MD (Maximal Marginal Relevance -Multi-Document), which collects passages related to the query from newspaper ar ticles retrieved by an IR system and arranged them into one summary.

As first proposed in [17], the central to the MDS approach has been gained a lot of interest. In 2005, Harabagiu et al. [15] proposed a topic themes method that a MDS can be based on the structure of the topic covering in the document collection. Although the document collection use d to generate a summary may be relevant to the same general topic, they do not nece ssarily include the same information. Extracting all similar sentences would produce a verbose and repetitive sum-mary, while extracting some similar sentences could produce a summary biased towards some sources, as it was noted in [8]. However, the graph-based extractive summarization algorithm succeeds in ident ifying the most important sentences in a document collection based on information exclusively drawn from the collec-tion itself. In this section, we propose a graph-based algorithm -GSPSummary -to obtain the important sub-topics. GSPSummary starts from the assumption that capturing sub-topic structure of document collection is essential for sum-marization. It firstly creates a graph repr esentation of the document collection, then selects the salient (or more central ) sentences with GSPRank and obtains the most important sub-topics in global graph space iteratively, finally forms the summary supported by the salient sentences of different sub-topics. We will give the definition of graph-based sub-topic representation, the GSPRank criterion, and the GSPSummary algorithm in more details in the subsections below. 3.1 Problem Formalization Let G =( V, E ) be an undirected graph with the set of nodes V and set of edges E ,where E is a subset of V  X  V . Then a graph G of a related document collection can be represented by the set of sentences V , and the similarities to each other E . Figure 1 is a sentences distance undirected graph representing a document collection with 22 sentences (the nodes in Figure 1), the edge stands for the A threshold is defined to eliminate the edges whose distance is higher, since we are interested in significant similarities. This reduction in the graph also provides us with computational savings. To define distance, we use the bag-of-words model to represent each sentenc e as an N-dimension vector, where N is the number of all possible words. Formally, the distance between two sentences is then defined by the following equation: u j using measure has been proven in IR and NLP.

Suppose there are three sub-topics in this document collection (the three circlesmarkedinFigure2),andnode1,node2andnode3arethesalient sentences of the three sub-topics.

In order to generate the sub-topics set, we need a ranking method which can generate sub-topics using combined criterion of relevance to the given topic and its centrality. Here, relevance and cen trality are not two conflicting concepts while belong to two different dimension. Relevance is the relationship of the topic with retrieved sentences set, centr ality is based on the relationship among its similar sentences.

We used the following notation throughout this paper:  X  subtopic ( S | T ): the sub-topic coverage of the document collection correspond- X  u : a single node in the document collection, in usual, is a sentence.  X  p c ( u ): the salient node u of a certain sub-topic S .  X  neighbor ( u ): the nodes near to the salient node u , also these nodes belong More precisely, we define the sub-topics subtopic ( S | T )as:
Then the summarization problem can be formulated as a graph partition problem: Given a sentences distance graph G of a document collection of a certain topic T , composed of a set of N nodes U = u 1 ,u 2 , ..., u N , and a length l , partition K sub-graphs of nodes S i  X  U as K sub-topics such that: (1) each sub-graph has a salient node u and its neighborhood neighbour ( u ) and (2) using u as a representative node of S i ; (3) sum of the length of all the K salient nodes should not be more than l .

The key for our task here is to find the appropriate salient node p c ( u )andits neighborhood neighbour ( u )inG. 3.2 GSPRank Criterion Many existing approaches explore the mos t important units (clauses/ sentences/ paragraphs) in texts [4] with statistics scoring methods and other higher seman-tic/syntactic structure such as rhetorical analysis, lexical chains, co-reference chains [6]. Unfortunately, these methods are still hard to obtain the really im-portant units, for the important units are not only decided by the statistical features, but also decided by the semantic features and other fields X  features. To explore the most important units or assess the salient nodes in graph, we propose a new sentence ranking criterion -GSPRank -served as basis for our GSPSummary method. This criterion has inspired by the ideas in information retrieval and feature selection. Since the summarization is controlled by choosing the central sentences, which we call  X  X alie nt sentences X , it is in principle possible for the salient sentences to be scored acco rding to the word based features -the statistical features or semantic featu res according to words or phrases -and the global features. where g ( u ) is the salience score of sentence u , f 1 ( u ) is the score of word based features, and f 2 ( u ) is the score of global features. We can use the product of the two classes of features to assess the salience of sentence u , for they belong to two different feature spaces.
 Word Based Features Metrics. Among the word based features proposed previously, the tf idf score of word is the most widely used approach. In the course of our investigation, the word based features can be presented with a linear combination as the following: s.t. where f wi ( u ) is a single word based feature, and the parameter  X  i is the factor to adjust different word based features. Normally, we can express these m word based features with a linear combination. In practice, we use the following word based features: vance score between sentence u and the document collection X  X  topic T ,and f u located and the topic T .
 To compute the over all centrality f w 1 ( u ) of a sentence given to other sentences, Radev et al. [5] proposed a LexRank approach based on the concept of graph-based centrality. The LexRank value of a sentence gives the limiting probability that such a random walk will visit that sentence in the long run. By LexRankthe score of sentence u can be computed as: where l ( u ) is the LexRank value of sentence u, N is the total number of nodes in the graph, d is a damping factor for the convergence of method, and w ( u, v )is the weight of the link form sentence u to sentence v. Equation 6 can be written in the matrix form as where U is a square matrix with all elements being equal to 1 /N . The transition kernel [ d U +(1  X  d ) B ] of the resulting Markov chain is a mixture of two kernels U and B .
 Global Features Metrics. Here, global features mainly consider the length, the position, the text pattern of a sentence, and so on. A simple fact is that short sentences cannot carry enough information corresponding to the topic. Thus, they are not appropriate candidates of summary sentences. And due to the constraint of summary length, too long sentences are not appropriate, either. There are some patterns which are unsuitable for appearing in the summary. The sentences which have these patterns will be discounted for summary sentence. Normally, we can consider the global features are independent, then the global features can illustrated in a form of conditional probability in Equation 9. where F g are the global features, and P ( F g | u ) is the probability of sentence u in global features space, and P ( F g | u ) equals to the product of k global features. In our work, global feature space involves three salient phases: the sentence length, sentence position, and sentence pattern. That is, p ( f g 1 | u ) is the probability that the observation of length feature was of u ,and p ( f g 3 | u ) is the probability of sentence pattern of u .What X  X more, the global features can be exploited from a supervised way by using a machine learning method based on a training corpus of documents, such as HMM. GSPRank. As mentioned above, we can obtain the new sentence ranking crite-rion -GSPRank -combining with word based features and global features. From the Equation 3, 5, 6, and 9, we can induce s.t. for sentence u ,and l ( u ) is the centrality score of sentence u ,whichissameto f as a Markov chain model. The convergence property of Markov chains provides a simple iterative algorithm, called Power Method 1 , to compute the stationary distribution. The algorithm starts with a uniform distribution. At each iteration, the eigenvector is updated by multiplying with the transpose of the stochastic matrix. Since the Markov chain is irreducible and aperiodic, the algorithm is guaranteed to terminate.

Based on these, we can write Equation 3 in the matrix notation as Equation 11. Here, the salience scores of the sentences set U can be formulated with the product of a feed matrix J and a vector L as the following equation. where R is the vector of GSPRank scores of the sentences set U , J is the feed matrix corresponding to U , each diagonal element in J is a feed function for sentence u in Equation 12, and L is the centrality score vector of U ,whichcan be calculate with the Power Method. Since the procedure of calculating L is iterative, the procedure of calculating R can also be presented as an iterative method with the Markov model. 3.3 GSPSummary Algorithm In section 3.2, we proposed a novel ranking criterion -GSPRank -to assess salience of sentence. The GSPRank can be expressed as an iterative way. Equiv-alently, our procedure of sub-topic partition algorithm can be described itera-tively. This way, a GSPSummary algorithm (in Algorithm 1) should include the following stages as the Fig.3 illustrates: 1. Partition a sub-topic: Generate a ranked list G of U with GSPRank, select 2. Modify adjacency matrix for next partition: Reduce all the nodes of sub-A brief sketch of our GSPSummary algorithm by looking at the graphs in Fig.3 is to find the salient node p c ( u ) and its neighborhood neighbor ( u )ingraph G of the document collection based on sentences relation. Suppose M is the adjacency matrix of G (in Fig.3(1)), M 0 is the initial matrix of G ,andeach circle is an element. As seen in Fig.3(2), we can use GSPRank to obtain the be generated with graph partition or searching algorithms -e.g, BFS -with a specified neighborhood threshold. At the first iteration, the salient node 3 and its neighborhood node 4 denote the first sub-topic S 1 . After eliminated the elements corresponding to S 1 , the matrix is be adjusted into a lower dimension one M 1 in Fig.3(3), follows by the next iteration to find the next sub-topic in Fig.3(4). In order to rank the scores of salient nodes, we used the GSPRank method (in Algorithm 2) in each iteration.  X  is the convergence factor for Power Method.
The following GSPRank Method (Algorithm 2) describes how to select a salient sentence for a given set of sent ences with GSPRank. Note that the cen-trality score vector L is also computed as a side product of the algorithm, and is the distance threshold used to eliminate some high distance. In order to evaluate our GSPSummary approach, we use the ROUGE 2 metrics on DUC2005 data sets for comparison. And the ROUGE score of the DUC2005 start-of-the-art systems came from Hoa X  X  overview of DUC2005 in [2]. 4.1 DUC Task Description Every year, Document Unde rstanding Conferences (DUC 3 ) evaluates competing research group X  X  summarization syst ems on a set of summarization tasks. In DUC2005, the task is to produce summaries of sets of documents in response to short topic statements that define what the summaries should address. The
Algorithm 2. GSPRank Method for obtaining salient sentence in global space summaries are limited to 250 words in length. The DUC 2005 task was a complex question-focused summarization task th at required summaries to piece together information from multiple documents to answer a question or set of questions as posed in a DUC topic. NIST 4 Assessors developed a total of 50 DUC topics to be used as test data. For each topic, the a ssessor selected 25-50 related documents from the Los Angeles Times and Financial Times of London and formulated a DUC topic statement, which was a request for information that could be answered using the selected document s. The topic statement could be in the form of a question or set of related questions and could include background information that the assessor thought would help clarify his/her information need. The assessor also indicated the granularity of the desired response for each DUC topic. That is, they indicate d whether they wanted the answer to their question(s) to name specific events , people, places, etc., or whether they wanted a general, high-level answer. Only one value of granularity was given for each topic, since the goal was not to measure the effect of different granularity on system performance for a given topic, but to provide additional information about the user X  X  preferences to both hu man and automatic summarization. 4.2 ROUGE Automatic text summarization has drawn a lot of interest in the NLP and IR communities in the past years. Recently, a s eries of government-sponsored eval-uation efforts in text summarization have taken place in both the United States and Japan. The most famous DUC evaluation is organized yearly to compare the summaries created by systems with those created by humans. Following the recent adoption of automatic evalua tion techniques by the machine trans-lation community, a similar set of evaluation metrics -known as ROUGE [16] -were introduced for both single and mu lti-document summarization. ROUGE includes four automatic evaluation methods that measure the similarity between summaries: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. 5.1 Threshold Selection Results In order to obtain an appropriate sub-topic, the threshold selection of neighbor-hood is also significant. The higher the threshold, the less the informative; while the lower the threshold, the higher redundancy. On the extreme point where we have a very high threshold or a very low threshold, the GSPSummary algo-rithm would be of no expected use. Fig. 4 demonstrates the effect of threshold for GSPSummary on DUC2005 data set with ROUGE-2 and ROUGE-SU4 met-rics. We have experimented with 13 differ ent thresholds -from 0.09 to 0.81 with step 0.06. It is apparent in the figure that the threshold of 0.21 can produce the best summaries together. When the threshold is too lower, the ROUGE scores are decreased for no node in the neighbo rhood. Similarly, when the threshold is too higher, the ROUGE scores are rapidly decreased for too many nodes in the neighborhood. Therefore, the curves less than 0.08 and higher than 0.81 were notplottedinFig.4. 5.2 Performance Comparison We evaluated the performance of our system in terms of both comparison with LexRank and comparison with DUC2005 results. These comparisons indicate their applicability for real data, DUC2005. Comparison of GSPSummary and LexRank. In the experiment, the pro-posed approach was compared with LexRank. With a unit matrix replaced the feed matrix J in Equation 12, our system will degenerate to a hierarchical LexRank system. In practice, we can introduce a hierarchical LexRank method to obtain the most important sub-topics. Unfortunately, for centrality only in LexRank, it is hard to measure the real salience of a sub-topic. Table 1 shows the results of two systems with two different ranks -LexRank and GSPRank. The ROUGE scores of Table 1 illustrates that our system with GSPRank can be quite more effective than the system with LexRank.
 Comparison of GSPSummary and DUC2005 Results. Table 2 shows the results of our two summarization systems GSP-S1, GSP-S2 on the data set of DUC2005 with ROUGE-2, ROUGE-L, and ROUGE-SU4. The baseline is the result provided by NIST, NUS3 is the best system in the two NIST official ROUGE scores: ROUGE-2 and ROUGE-SU4 recall. The GSP-S1 is used the GSPRank without consideration global features, while the GSP-S2 is used the GSPRank with the global features consideration. The score of our GSP-S1 in ROUGE-2 can obtain the 3rd rank, and the score of ROUGE-SU4 can obtain the 3rd place in DUC2005. Furthermore, comparing with IIITH-Sum -the third ranked system in ROUGE-2 and the 5th ranked system in ROUGE-SU4 -our GSP-S1 system has significant superiority in performance. The scores of our GSP-S2 can both obtain the 1st place in DUC2005. In comparison with the scores in GSP-S1, the ROUGE-2 score an d the ROUGE-SU4 score increase 5.0% and 2.4% respectively, which demonstrates the influence of the global features in the proposed approach. The results confirm that our graph-based sub-topic partition summarizer performs well as comparing to the state-of-the-art systems competing in DUC. Summarization is a product of electroni c document explosion, and can be seen as the condensation of the document coll ection. As summary is concise, accurate and explicit, it became more and more important. In this paper, we present a new sub-topic representation model for MDS , and a new rank criterion is presented to obtain sub-topics. Furthermore, a new procedure and algorithm for generic and topic-oriented summarization is proposed. With the representation of graph, our algorithm can obtain the appropriate sub-topic with an iterative procedure in global space. We test our algorithm with DUC2005 data set, and the results suggest that our algorithm is effective in MDS.
 The study has three main contributions: (1) we propose a new framework of MDS with sub-topic representation model, according to the logic structure of the topic covered in the document collect ion. (2) we propose a new ranking cri-terion GSPRank, in which both the word based and global features are modeled explicitly and effectively. (3) we present a new graph-based sub-topic partition algorithm GSPSummary for MDS.

As future work, we plan to explore in how to generate neighborhood with some other graph searching and partition algorithms. To some extent, our GSPSum-mary approach can be viewed as a simple version of hierarchical Markov model, with the scalable ranking criterion GSPRank, our algorithm can be further im-proved. Thus, in future work, we will study how to deal with such issues, and use fitful neighborhood searching or partition algorithms to model sub-topics. Acknowledgments. The work is supported by the National Grand Funda-mental Research 973 Program of China  X  X arge-Scale Tex Content Computing X  under Grand NO.2004CB318109 an Grand NO.2007CB311100.

