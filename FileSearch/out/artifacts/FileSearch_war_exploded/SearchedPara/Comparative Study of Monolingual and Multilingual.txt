 JACQUES SAVOY Universit X  de Neuch X tel, Neuch X tel, Switzerland 1 MONOLINGUAL IR FOR ASIAN LANGUAGES During the last few years, interest in Asian languages, particularly in Chinese (C), Japanese (J), and Korean (K) has been increas ing. Given the growing number of Internet pages and sites available in these language s, along with an ever-expanding number of online users 1 working with them, a better understanding of the automated procedures used to process them is clearly needed. These Asian languages also represent various external differences that, compared to Eu ropean languages, present the IR community with very interesting challenges. the Arabic alphabets), standard Asian la nguages require quite a larger number of characters (around 13,000 for the Chinese BIG5 encoding system, around 8,200 for Korean, and 8,800 for Japanese). When processing the languages of the far East, the implicit assumption that one byte corresponds to one character is no longer valid. These facts lead to additional challenges for anyone using typical Unix functions like wc, sort, and grep, and generally entail the use of more complex input and output methods [Lunde 1998]. precisely, ideographs 2 ) without any delimiting spaces sepa rating them. So finding words within such a continuous string becomes a major problem, one that has to be resolved before tackling various other problems such as linguistic analysis, machine translation, or information retrieval. The Chinese language may be written using one of two main character formats. These are th e traditional (usually encoded by using the BIG5 standard) and the simplified formats (using the GB standard system [Lunde 1998]), not to mention considerable orthographic vari ations encountered when spe lling foreign names [Halpern 2002]. 3 There are four writing systems in Japanese, namely the Hiragana syllabic character set (representing around 37.3% of the total number of characters 4 ); the Katakana (9.7% of a syllabic character set used mainly to write words of foreign origin such as  X  X omputer, X  foreign names like  X  X acIntosh, X  or onomatopoeic words like  X  X uzz X ); the Kanji (corresponding to Chinese characters and making up 46.3% of characters used); and finally ASCII characters (about 6.7%, used to write numbers or company names like  X  X onda X ). are morphological differences too [Sproat 1992]. On the basis of morphological information or word structure, the languages studied in our evaluations can be broadly grouped, based on Bloomfield X  X  classification [1933], 5 into three different types: (1) English, Latin, and most other European languages are inflectional, within which certain distinct features are used to create single or fairly unified suffix formats added to a given stem (inflectional suffixes such as  X -s X  in  X  X uns X  or derivational suffixes like  X -ment X  in  X  X stablishment X ). (2) Chinese belongs to an isolated language family in which the vast majority of words are invariable, meaning that, in IR, system-stemming procedures would play a less important role. (3) While both the Japanese or Korean languages may be considered members of the agglutinative language family in which various affixes are nor close relationship with any other language. languages, it is important to verify whether th e efficient search models already developed for European languages will perform as well w ith Asian languages. The first section in this article addresses this question and is organized as follows. Section 1.1 briefly characteristics of the nine vector-space sche mes and the two probabilistic IR models; Section 1.3 presents the indexing strategies used in our experiments; Section 1.4 provides an evaluation of various indexing and search strategies; Section 1.5 evaluates a pseudo-relevance feedback approach intended to improve retrieval effectiveness; finally, Section 1.6 compares the relative merits of various data-fusion operators. 1.1 Overview of the NT CIR-4 Test-Collection The corpora in our experiments were put together during the fourth NTCIR 6 evaluation campaign [Kishida et al. 2004a]. Created to promote the study of the information four different languages. The English collection is taken from the Mainichi Daily News (Japan), Taiwan News , China Times English News (Taiwan), the Xinhua News Service (China), the Korean Times, and the Hong Kong Standard . The Chinese collection contains news extracted from the United Daily News , China Times , China Times Express , Commercial Times , China Daily News, and Central and Daily News . These documents were written in Mandarin using the traditional Chinese character set. The Japanese collection contains articles taken from the Mainichi and Yomiuri newspapers (Japan), while the Korean cor pus was extracted from both the Hankookilbo and Chosunilbo newspapers (Korea). as the largest, the English corpus as second, the Chinese corpus as third and the Korean per document, showing that this value is clearly larger for the Chinese collection (363.4 bigrams/article) when compared to the Korean (236.2 bigrams/article) or the Japanese corpus (114.5 bigrams/article). For the English collection, the mean number of distinct words per document is 96.6. When analyzing the number of pertinent documents per topic, only rigid assessments were considered. Thus in this article only  X  X  ighly relevant X  and  X  X elevant X  items are seen as relevant, under the assumption that only highly relevant or relevant items are useful for all topics. In certain circumstances, however, we also assumed that records found to system, the retrieval effectiveness measures depicted in this article show lower performance levels than they would with mo re relaxed assessments. However, we believe that our conclusions would be similar, whether we used rigid or relaxed assessments. indicates that for the Japanese collection the median number of relevant items per topic is 88, while for the Chinese corpus it is only 19. By contrast, the number of relevant articles is greater for the Japanese (7,137) and English (5,866) corpora, when compared to the Korean (3,131) or Chinese (1,318) collections. These divergences may have an impact on some of our merging strategies (see Section 3). 
Following the TREC model, the structure of each topic was ba sed on four logical sections: a brief title ( X &lt;TITLE&gt; X  or T), a one -sentence description ( X &lt;DESC&gt; X  or D), a ( X &lt;CONC&gt; X  or C) that provides some related terms (see Table 2 for examples). Rather than limiting them to a narrow subject range, the topics made available were chosen to reflect a variety of informa tion needs (such as  X  X iagra, X   X  X orth Korea, Starvation, Response, X   X  X anotechnology, Realization, Research Trends X  or  X  X apan, Amendment, Law, Self-Defense Force X ). 1.2 Search Models In order to ensure that useful conclusions are obtained when handling new test collections, we considered it important to evaluate retrieval performance under varying conditions. Thus, in order to obtain this broa der view, we evaluated a variety of indexing and search models, ranging from very simple binary-indexing schemes to more complex vector-processing schemes. topic) is represented by a set of key words, without assigning any weights (IR model denoted  X  X ocument=bnn, query=bnn X  or  X  X nn-bnn X ). Binary logical restrictions may often be too restrictive for document-and query-indexing. It is also not always clear whether a document should be indexed by a given term (in this article, a single word or bigram). Given that a more appropriate answer is neither  X  X es X  or  X  X o, X  but something in between, term-weighting should allow for be tter differentiation of terms, and thus increase indexing flexibility. In this vein, we may also assume that the frequency with which a term occurs in a document or in a que ry (denoted tf) can be a useful feature (IR model denoted  X  X nn-nnn X ). the collection do not help us discriminate be tween relevant and non-relevant items. For their inverse document frequency (denoted id f), resulting in larger weights for more that this specificity does not depend on a given term's semantic properties, but is derived from a statistical notion, or as Sparck Jones sa ys,  X  X e think of specificity as a function of term use X  [Sparck Jones 1972]. For example, the word  X  X omputer X  may be viewed as science collection it is viewed as a broader term, one that may have a variety of meanings. effectiveness (IR model:  X  X tc-ntc X ); s ee the Appendix for the exact weighting formulations for the IR models in this article. document is viewed as a rare event. Thus, it may be good practice to give more occurrences. Therefore, the tf component may be computed as ln(tf) + 1.0 ( X  X tc X ,  X  X nc X , or  X  X tn X ) or as 0.5+0.5 X [tf / max tf in a document] ( X  X tn X ). We might also consider that a term's presence in a shorter document represents stronger evidence than it does in a longer document. In order to take docum ent length into account, more complex IR models have been suggested, including the  X  X nu X  [Buckley et al. 1996] or the  X  X tu X  IR model [Singhal et al. 1999]. such as the Okapi probabilistic model [Robe rtson et al. 2000]. We implemented the Prosit model as a second probabilistic a pproach (or  X  X eviation from randomness X ) [Amati and van Rijsbergen 2002; Amati et al. 2003]. As shown in Eq. (1), this IR model combines two information measures. The first component measures the informative content (denoted Inf 1 ij ( tf )), based on the observation that in the document D i we found tf occurrences of term t j . may correspond to a noncontent-bearing word in the context of the entire collection [Harter 1975]. In the English language these words generally correspond to determinants like  X  X he, X  prepositions like  X  X ith, X  or verb fo rms like  X  X s X  or  X  X ave, X  and are considered nouns that may often appear in numerous documents within a particular corpus, such as  X  X omputer X  and  X  X lgorithm, X  particularly when the articles in which they are found are extracted from computer science literature. On the other hand, if Prob 1 ij ( tf ) is small (or if expressed as a geometric distribution, where p = 1/(1+  X  ). Other stochastic distributions have been suggested in Amati and van Rijsbergen [2002]. t , since tf occurrences of this term have already been found in document D i . This probability can be evaluated using th e Laplace law of succession, as Prob 2 ij ( tf ) = document length into account, and in our expe riments we have included it as shown in Eq. (3). where w ij represents the indexing weight attached to term t j in document D i ; tc j indicates the corpus; mean dl is the mean length of a document; and l i the length of document D i . 1.3 Indexing In the previous section, we described how each indexing un it was weighted so that it This section will explain how such indexing units are extracted from documents and topic formulations. process on the SMART stop-word list (571 terms) and stemmers (in this case, Lovins [1968] stemming algorithm). When indexing Indo-European languages, it is natural to consider words as indexing units. For seve ral European languages this approach has usually produced the best retrieval results, as demonstrated by various CLEF evaluation campaigns [Peters et al. 2004; 2005]. In this case, delimiting words within a sentence is a relatively easy task (with some problems, for example,  X  X BM360 X  or  X  X est-suite X  can be viewed as being made-up of either one or two words). For various languages there is a list of high-frequency or stop-list words that are usually found irrelevant when describing the semantic content of documents or queries. to adapt a stemming algorithm for each Europ ean language. To achieve this goal, we defined a light stemming procedure by re moving only inflectional suffixes used to indicate number (singular vs. plural), gender (feminine, masculine, or neutral), or case (nominative, genitive, ab lative, locative, etc.) of a given noun or adjective. 7 Based on the CLEF 2001 test-collections, Savoy [2002] demonstrated that we can obtain mean average precision improvements of 10% (English), 15% (Italian), 18% (Spanish), 21% (German), and 24% (French) when applying a light stemmer, compared to a system that uses no stemming (T queries). With TDN queries, these improvements are less significant, ranging from 4% (English) to 10% (Spanish a nd German) to 14% (French and Italian). suffixes (e.g.,  X -ize, X   X -ably, X   X -ship X  in the English language). The difference in retrieval effectiveness between light and mo re complex stemming approaches is usually small. In the French language, for example, Savoy [2002] shows that improvements of 5% (T queries) to 2% (TDN queries) are possible when using an extended stemming procedure. effective stemming procedure requires a more complex morphological analysis, one based on a dictionary. For example, Tomlinson [2004] found a statistically significant difference of around 13% in favor of a dictionary-based stemmer, compared to a derivational stemmer ( X  X nowball X  in this case). For the same language, and based on stemming problem for Finnish is that stems are often modified when suffixes are added. For example,  X  matto  X  (carpet in the nominative singular form) becomes  X  maton  X  (in the genitive singular form, with  X -n X  as suffix) or  X  mattoja  X  (in the partitive plural form, with  X -a X  as suffix). Once we remove the corresponding suffixes, we are left with three these also occur in other languages--they usua lly help to make the spoken language flow irregularities are more common, and thus render the conflation of various word forms into the same stem more problematic. compound word constructions being the most important (e.g., handgun, worldwide). Braschler and Ripplinger [2004] show that decompounding German words would significantly improve retrieval performance, resulting in improvements from 16% to 34% for T queries and 9% to 28% for TDN requests. Mayfield [2004] suggest using an overlapping n -gram approach to define the indexing units. In this scheme, each sentence is decomposed into sequences of n characters. For example, when analyzing the phrase  X  X he white house X , the following 4-grams are extracted { X  X he_ X ,  X  X e_w X ,  X  X _ wh X ,  X  X whi X ,  X  X hit X ,  X  X ite X , ...  X  X ous X ,  X  X use X  X . With this type of indexing approach, stop-word lists and stemmers adapted for the corresponding language are not required, since during indexing the n -grams that appear in all documents (e.g.,  X  X ith X ,  X  X ave X , or very frequent suffixes like  X -ment X ) will be assigned null, or at least insignificant, weights. According to McNamee and Mayfield [2004], and based on eight European languages, the most effective n -gram decomposition seems to be between 4-grams and 5-grams. delimited. We therefore indexed documents written in Asian languages by using an overlapping bigram approach, an indexing sc heme found to be effective for various Chinese collections [Kwok 1999; Luk and Kwok 2002], or during the NTCIR-3 evaluation campaign [Chen and Gey 2003]. There are also other factors involved in our choice of an indexing tool. When considering the Korean language for example, Lee et al. [1999] found that more than 80% of Korean nouns were composed of one or two Hangul characters; Sproat [1992] reported a similar finding for Chinese. When analyzing the mean length of continuous characters in the Japanese corpus, we found its value to be 2.3 for Kanji characters, with more than 70% of continuous Kanji sequences composed of one or two characters. When studying the mean length of continuous Hiragana characters, we calculated an average va lue of 2.1, and for sequences composed of only Katakana characters, with a mean value of 3.96.  X  X BCD EFG X  sequence generates the followi ng bigrams { X  X B, X   X  X C, X   X  X D, X   X  X F X  and  X  X G X  X . In order to stop bigram generation, in our work we generated these overlapping bigrams for Asian characters only, usi ng spaces and other punctuation marks (as collected for each language from their respectiv e encodings). Moreover, we did not split any words written in ASCII characters. In our experiments the most frequent bigrams were removed before indexing. For the Ch inese language, for example, we defined and removed a list of the 215 most frequent bigr ams; for Japanese, 105 bigrams; and for Korean, 80 bigrams. For Chinese, we also evaluated the unigram (or character) indexing approach. generating bigrams for the Japane se documents, we removed all Hirakana characters, given that they are mainly used to write grammatical words (e.g., doing , do , in , of ), and the inflectional endings of verbs, adjectives, and nouns. In this vein, various authors suggest indexing Chinese documents by using words generated by a segmentation procedure (e.g., one based on the longest matching principle [Nie and Ren 1999; Foo and Li 2004]). But Nie and Ren [1999] indicated that retrieval performance based on word indexing does not really depend on an accurate word segmentation procedure; this was confirmed by Foo and Li [2004]. Nie and Ren [1999] also stated that segmenting a Chinese se ntence affects retrieval performance; and recognizing a greater number of 2-character words usually contributes to enhanced segmentation accuracy and retrieval effectiv eness. Moreover, manual segmentation does not always produce better performance when comp ared to character-based segmentation. and D topics, obtained a mean average precision value of 0.2802 when combining overlapping bigrams and characters, versus 0.2758 for a word-based indexing strategy (words were segmented with the Chasen morphological analyzer [Matsumoto et al. 1999]). This difference in performance is small (1.6%), and seems to indicate that both indexing schemes result in similar retrieval effectiveness. even though word boundaries are marked by spaces, this language also uses numerous suffixes and even prefixes. Compound constructions are also used very frequently; a morphological analyzer can be used to separate compound words into simple nouns. However, Lee et al. [1999] showed that n -gram indexing could provide similar and sometimes better retrieval effectiveness when compared to word-based indexing applied in conjunction with a decompounding scheme. 1.4 Evaluating IR Systems Having described the various IR models, it w ould be useful to know how these search strategies will behave when used with the Asian test-collections. In order to measure retrieval performance we have adopted noninterpolated mean average precision (MAP), as computed by TREC -EVAL . To determine whether or not any given search strategy might be better than another, we based our statistical validation on the bootstrap approach [Savoy 1997]. Thus, in the tables in this article, statistically significant differences are shown underlined (two-sided nonparametric bootstrap test, significance level fixed at 5%). We evaluated the vari ous IR schemes under three topic formulations: first, the queries were built using only the title (T) section; second, using the descriptive (D) section; and third, using all topic logical sections (TDNC). III for the English and Korean collections, with the best performance under a given condition shown in boldface type (t hese values were used as a baseline for our statistical tests in Tables III, IV and V). For the Japanese, Table IV depicts the performance achieved when generating bigrams from both Kanji and Katakana characters (left side), where in this case a bigram may be composed of one Kanji and one Katakana character. As a variant, we generated bigrams for Kanji characters only, with each continuous Katakana character sequence considered as a single indexing unit or term. Table V shows the performance for the Chinese cor pus, using the unigram (or character) and bigram indexing schemes. Surprisingly, this data shows that the best retrieval scheme for short queries was not always the same as that for long topics. For example, for long query formulations (TDNC) and for the Korean collection, the Ok api was the best search model while for short queries (T or D) the vector-space  X  X  nu-ltc X  approach provided better performance. Based on our statistical testing, these differences in performance were not always significant (e.g., for the Japanese corpus, differences between the Okapi and  X  X nu-ltc X  models were only significant for T queries). indexing (English collection, Table III) or the n -gram scheme for Asian languages Prosit,  X  X nu-ltc, X  and  X  X tu-dtn X . Thus, using n -grams or words to describe the semantic content of a document (or a request) does not result in any real performance differences among search models. This main conclusion was corroborated by other studies that compared n -gram and word-based indexing strategies when analyzing various European languages [McNamee and Mayfield 2004; Savoy 2002]. Table VI depicts the mean average precision obtained by using the CLEF 2003 test-collection [Peters et al. 2004] for various European languages belonging to di fferent language groups such as the Latin family (French and Spanish), the German family (German and Dutch, evaluations included a decompounding stage), the Slavic group (Russian), and the Uralic language family (Finnish) [Savoy 2004c]. As shown in this table, the best performing IR models usually incorporate either th e Okapi or the Prosit approach , showing that the performance differences between these two and the  X  X nu-ltc X  and  X  X tu-dtn X  vector-space models are not always statistically significant. document (tf component), the models tend to attribute more weight to the first occurrence when weighting the search term in the Okap i or  X  X nu-ltc X  models (or the Prosit approach best-performing search strategies take doc ument length into account by favoring short documents (usually more focused on a narrow subject). of Table III), the best retrieval scheme s eems to be query-dependant, and the best retrieval performance for T queries is the Okapi model,  X  X nu-ltc X  for D queries and Prosit for TDNC. Moreover, differences in performance for these three search models models provide the best retrieva l performance [Savoy 2004c; 2005], the good performance by the  X  X nu-ltc X  model using D queries must be viewed as an outlier. scheme ( X  X nn-bnn X ) resulted in a surprisingly high retrieval performance compared to the D or TDNC query formulations (0.1944, 0.0725 and 0.0148, respectively). been generated for both Kanji and Katakana characters (left side) or only for Kanji characters (right side of Table IV). When using title-only queries, the Okapi model provides the best mean average precision of 0.2972 (bigrams on Kanji only) compared to 0.2873 when generating bigrams on both Kanji and Katakana . This difference is rather small, and is even smaller in the opposite direction for long queries (0.3510 vs. 0.3523). Based on these results we cannot infer that for the Japanese language one indexing procedure is always significantly better than another. seems that longer queries (TDNC) tend to perform better with bigram indexing. For T or D query constructions, the difference between character and bigram indexing usually favors the bigram approach (the performance of the  X  X nu-ltc X  model when using T queries must be viewed as an exception). Th e question that arises is the following: How can we improve the retrieval effectiveness of these retrieval models? To answer this question, we suggest incorporating a bli nd query expansion stage during the search process (Section 1.5) and then applying a fusion strategy (Section 1.6). 1.5 Blind Query Expansion It is known that once a ranked list of retrieved items has been computed, we can automatically expand the original query by including terms that appear frequently in the top retrieved documents. Called blind query expansion or pseudo-relevance feedback, study, we adopted Rocchio's approach [Buckley et al. 1996] with  X  = 0.75,  X  = 0.75, whereby the system is allowed to add m terms extracted from the k best-ranked documents from the original search, as depicted in the following formula, in which Q X  indicates the expanded query composed of the previous query Q and of m terms extracted from the k best-ranked documents D i , assumed to be relevant to the query Q. Of course, other relevance feedback st rategies have been proposed; for example, Robertson [1990] suggested making a clear distinction between the term-selection procedure and the term-weighting scheme. In a similar vein, Carpineto et al. [2001] suggested using a theoretic information measure, in this case the Kullback-Leibler divergence, for both selecting and weighting terms. Table VII summarizes the best results for the English and Korean language collections; Table VIII lists the best retrieval results for the Japanese corpus (and with our two indexing strategies), as does Table IX for th e Chinese collection (character or bigram mean average precision before applying the blind query expansion procedure. The rows starting with  X #doc. / #terms X  indicate the number of top-ranked documents and number of terms used to enlarge the original query, and thus obtain the best retrieval effectiveness. Finally, the rows labeled  X &amp; Q expansion X  depict the mean average precision achieved after applying the blind que ry expansion (using the parameter setting specified in the previous row). technique improved mean average precision, and this improvement is usually statisticallysignificant (values underlined in the tables). When comparing both Okapi model. For some unknown reason, it seems that we must include more terms with the Prosit model than with the Okapi approach. In addition, the percentage enhancement expansion improved mean performance fro m 0.2637 to 0.3396 (+28.8% in relative effectiveness), compared from 0.3442 to 0.3724 (+8.5%) for TDNC topics. queries, several variants are proposed. For example, Grunfeld et al. [2003] suggest using the Web to find additional search terms, while Luk and Wong [2004] suggest various term-weighting schemes, depending on the term X  X  occurrence in the collection. 1.6 Data Fusion As an additional strategy to enhance retrieval effectiveness, we considered adopting a data-fusion approach that combines two or more result lists provided by different search models. retrieve different pertinent and nonrelevant items; hence combining the different search models would improve retrieval effectiveness. More precisely, when combining document representations might retrieve di fferent pertinent items [Vogt and Cottrell 1999]. On the other hand, when combining different search schemes, we assume that the than they would the same nonrelevant documen ts (viewed as outliers). Thus combining them could improve retrieval effectivene ss by ranking pertinent documents higher and assumption is that character and bigram indexing schemes are distinct and independent sources of evidence regarding the content of documents. Due to the first effect described above, we expect to improve recall for the Chinese language only.  X  X R X ), whereby we took, in turn, one doc ument from all individual lists and removed operators have been suggested [Fox and Shaw 1994], but the simple linear combination (denoted  X  X umRSV X ) seems to, usually, pr ovide the best performance [Savoy 2004a; Fox and Shaw 1994]. In this case, for any given set of result lists, the combined operator retrieval performance among the various IR models. fusion strategy we normalized document scores within each collection by dividing them by the maximum score (i.e., the document score of the retrieved record in the first position). As a variant of this normalized score-merging scheme (denoted  X  X ormRSV X ), we might normalize the document RSV k scores within the i th result list, according to where Min i (Max i ) denotes the minimal (maximal) RSV value in the i th result list. 
As a new data-fusion strategy, we sugge sted merging the retrieved documents according to the Z-score computed for each result list. For the i th result list within this scheme we needed to com pute the average of the RSV k (denoted Mean i ) and the standard deviation (denoted Stdev i ). Based on these values, we then normalized the document score for each document D k provided by the i th result list, as computed using the following formula: where the value of  X  i is used to generate only positive values, and  X  i (usually fixed at 1) is used to reflect the relative retrieval performance of the i th retrieval model. When the coefficients  X  i are not all fixed at 1, the data-fusion operator is denoted  X  X -scoreW X . The use of the Z-score was also suggested for the topic-detection and tracking contexts [Leek et al. 2002]. Japanese, and Korean collections for each of the T, D, and TDNC queries. The top part of Table X shows the individual performances of various retrieval models in our data-fusion experiments. For example, for the T queries in Japanese, we combined the Prosit and Okapi probabilistic models with the  X  X nu-ltc X  and  X  X tn-ntc X  vector-space schemes. The Chinese language data-fusion experiments also included the Okapi and  X  X nu-ltc X  models based on character indexing. The r ound-robin ( X  X R X ) scheme shown in this table was intended to serve as a baseline for our statistical testing. 
From this data we can see that combining two or more IR models sometimes improves retrieval effectiveness. Moreover, linear combinations ( X  X umRSV X ) usually result in good performance, and the Z-score scheme tends to produce the best performance. As shown in Table X under the heading  X  X -scoreW X , we attached a weight of 2 to the Prosit model, 1.5 to the Okapi, and 1 to other IR models. 
But combining separate result lists did not always enhance performance, as shown in the Korean collection using TDNC queries. In this case, none of the data-fusion operators performed significantly better than the round-robin scheme, while the best single retrieval model (Okapi in this case) was shown to have the best mean average precision (0.5141). However, it is difficult to predict exactly which data-fusion operator will produce the best results. The Z-score or the weighted Z-score schemes did seem to produce good results when handling different languages and query formulations. Our experiments also indicate that combining short queries resulted in better improvement than combining longer topics. 2 BILINGUAL IR In order to retrieve information written in one of our far-east languages based on a topic automatically provide translations into the Chinese, Japanese, or Korean languages. In this study we chose four different machine-translation (MT) systems and two machine-readable bilingual dictionaries (MRDs) to translate the topics, namely BabelFish, FreeTranslation, InterTran, WorldLingo, EvDict, Babylon, available at the following locations: BabelFish babel.altavista.com/translate FreeTranslation www.freetranslation.com InterTran www.tranexp.com:2000/InterTran WorldLingo www.worldlingo.com EvDict www.samlight.com/ev/ Babylon www.babylon.com When translating a topic into another language , we could also consider parallel and/or comparable corpora. Such an approach is based on document-level alignments where, in order to find terms statistically related to the target language, documents in various languages are paired according to their si milarity [Braschler and Sch X uble 2000]. Comparable corpora were not r eadily available however, so as a partial solution Nie et al. [1999] suggested using their PTMiner system to extract parallel corpora from the Web. Using these Web page collections, sentences from two pages written in two different languages were aligned using a length-base d alignment algorithm [Gale and Church 1993] and the system then computed the pr obabilities of translating one term into another. When using this type of statistical translation model, however, source quality (e.g., Web sites) and available corpora size are of prime importance [Nie and Simard 2001]. Cultural, thematic, and time differences could also play a role in the effectiveness of these approaches [Kwok et al. 2001]. in one language can be viewed as misspelle d forms from another language (for example, English topics are viewed as misspelled Fren ch expressions). Following this example, Gey [2004] assumes that Chinese topics can be converted into their Japanese equivalents (after carrying out character set conversion), and hopefully so me of the resulting search terms would in fact be the appropriate Japanese words. Based on the NTCIR-4 test-collection and using TDNC queries, this author obtained a MAP of 0.0893 when performance represents 25.6% of the corresponding monolingual MAP. Of course, such bilingual searches would only work when deali ng with related languages, such as Italian and French or, in our context, Chinese and Japanese. by-word. In response to each submitted word , the Babylon system provides not only one but several translation terms (in an unspecified order). In our experiments, we decided to  X  X abylon 2 X ), or the first th ree (labeled  X  X abylon 3 X ). employing our two MRDs, the four MT systems, and the Okapi model. The first row ( X  X kapi-npn X ) also contains the retrieval performances of manually translated topics that will be used as a baseline. The symbol  X  X /a X  in Table XI represents missing entries, language. performance level was only 45.2% of a monolingual search for the Chinese language (0.0795 vs. 0.1755); 67.9% for Japanese (0.1952 vs. 0.2873); and 46% for Korean (0.1855 vs. 0.4033). Hence we can see that machine translation systems resulted in generally poor performance levels. Moreover, the differences in mean average precision were always statistically significant and fa vored manual topic translation approaches. When compared to our previous work with European languages [Savoy 2004c], the differences are clearly larger. For example, during the CLEF 2003 evaluation campaign and using the FreeTranslation MT system, we obtained 82.7% of the performance level achieved by a monolingual search for the French language (0.4270 vs. 0.5164); 80.6% for Spanish (0.3997 vs. 0.4885); and 77.4% fo r Italian (0.3777 vs. 0.4880) (evaluation based on English queries). Using the same MT system (FreeTranslation in this case), this comparison reveals that automatic translation from English to other Indo-European languages seems more effective than translating from English into Asian languages. translation software tends to produce better query translation than dictionary-based approaches (namely,  X  X abylon X  or  X  X vDic t X ). Thus automatic query translation operating within a context (topic formulation in this case) may reduce translation ambiguity. A query-by-query analysis reveals, however, that in these experiments the main underlying translation problem is related to the presence of proper nouns (e.g.  X  X arter X ,  X  X orrijos X ), geogra phical terms (e.g.,  X  X outh Korean X ) or other proper names (e.g.,  X  X iagra X ). By inspecting the Korean queries we found that, in the automatically translated queries, proper nouns were usually not translated and were written in the Latin alphabet (with some exceptions, e.g.,  X  X ichael X ). Even though the machine usually returned a translation when terms were translated into Japanese, the suggested translation usually differed from the term used by humans (e.g.,  X  X outh Korean X , while  X  X pple Computer X  seemed to be translated corr ectly). Moreover, there was no correlation between the performance of translated queries in Japanese and Korean. For example, the machine-based translated Query #7 ( X  X arte r-Torrijos Treaty X ) performs reasonably well in Korean (0.9188 (bilingual search) vs. 0.9733 (monolingual)), its performance on the Japanese corpus was better than the m onolingual run (0.6847 (bilingual search) vs. 0.3651 (monolingual)). This analysis seems to indicate that we need to consider introducing a supplementary stage during which the Web can be used to provide translations or at least useful related key words when handling English proper nouns. Kwok et al. [2004] were able to improve the English to Korean search by 18% when using such a technique. Chen and Gey [2003] suggested a similar approach for cases when untranslated English words (mainly proper nouns) are found. These terms were submitted to Yahoo!Chinese (or Yahoo!Japan) and the first 200 entries were then downloaded and segmented into words. Af ter this step, from each line containing the specific English word, they extr acted the five Chinese words immediately to the left and words). tends to produce the best translations of topics in Japanese and Korean, and both BabelFish and WorldLingo MT for Chinese. In order to improve retrieval performance, we developed three possible strategies. First, we concatenated the output of two translation tools into a single query. For Chinese, we combined the translations given by WorldLingo with those of  X  X vDict X ; for Japanese we concatenated the translations provided by WorldLingo with those of  X  X abylon 1 X ; and for Korean, we combined WorldLingo and BabelFish. As shown in the last two rows of Table XI, the combined translation strategy seems to enhance retrie val effectiveness for Chinese and Japanese, but not for Korean. the combined translated topics. As shown in Table XII, this technique clearly enhanced retrieval effectiveness when we used the Okapi or Prosit probabilistic models. As for monolingual IR (see Tables VII to IX) and fo r the Chinese and Japanese collections, the results achieved by the Prosit system after ps eudo-relevance feedback were usually better than those obtained by the Okapi search model. Surprisingly, for T queries in the Japanese corpus, the Okapi combined with blind query expansion achieved a perform-ance level of 0.2733 (or 95.1% of the monolingual performance, however without blind query expansion). When compared to othe r bilingual runs, blind query feedback seems to be a very attractive strategy for enhancing retrieval effectiveness. a data-fusion approach that combines two or more result lists provided by different search models (as shown with the monolingual search; see Section 1.6). translation is good or when a given search mi ght produce a proper response. In this vein, Kishida et al. [2004b] suggest using a linear regression model to predict the average precision of the current query, based on both manual evaluations of translation quality for the current query and the underlying topic difficu lty. Using the 55 queries written in Japanese, together with their machine-base d translations from Korean, Chinese, and English, these authors found that the 64% va riability in average performance was due to both translation quality and intrinsic query difficulties. In a related paper, however, Cronen-Townsend et al. [2002] showed that, in monolingual IR, a query's idf average value might adequately predict its retrieval effectiveness or intrinsic difficulty. Based on such findings, it may be worthwhile to combine various translations on a per-query basis or to select the most appropriate parameters when expanding the original query, also on a per-query basis. 3. MULTILINGUAL INFORMATION RETRIEVAL order to retrieve relevant documents in E nglish, Chinese, and Japanese (CJE) or in English, Chinese, Japanese, and Korean (CJK E). To deal with this multilanguage hurdle, we based our approach on bilingual IR systems, as described in the previous section. Thus, the various collections were indexed separately, and once the original requests were received, they were translated into different languages and s ubmitted to the various collections or search engines. As a re sponse, a ranked list of retrieved items was returned from each collection. From these lists we needed to produce a unique ranked result list, using the merging strategy describe d further on in this section. Moreover, in our multilingual experiments, only one search engine would be available, which is a common situation in digital libraries or in other office environments. We wanted to compare our various merging strategies via a good general search engine, so we selected the Prosit model. Based on the same test-co llection, Savoy [2004b] evaluated various multilingual merging strategies by using a variety of search engines. translate all documents into a single common language [Braschler and Peters 2004; Chen and Gey 2004]. In such a case we might form a huge unique collection with all available documents, and, since the search would be performed by comparing to a single collection, no merging procedure is required. As shown in the CLEF 2003 evaluation campaign, such an indexing and search st rategy usually provides very good retrieval effectiveness. The document translati on approach does, however, require more computational effort; if we allow users to write their queries in k languages, we need to translate each document into k-1 other languages. merging problem in which a query has been sent to three collections. In response, three result lists were received, and so we had to merge the retrieved items in order to form a request. 1995], whereby we took one document in turn from all individual lists. In this case, we might assume that each collection (or language in this study) contai ns approximately the same number of pertinent items and that the distribution of relevant documents is similar across the result lists. Under these hypotheses, the rank of the retrieved documents would be the key feature in generating the final unique result list presented to the user. each retrieved item (denoted RSV k for document D k ), we might formulate the hypothesis that each collection could be searched by the same, or a very similar, search engine, and raw-score merging, produced a final list sorted by document score, as computed by each collection. Since we used the same retrieval model (Prosit) for searching within all collections separately, we could expect resulting document scores to be more comparable, and thus the document score could be used to sort the retrieved items. However, the document scores were not alwa ys comparable, and this merging strategy favors documents with a high retrieval status value from the Japanese or Korean corpus, as illustrated in Table XIII. represented by document or query weights may vary widely among collections; this phenomenon may therefore invalidate the raw-score merging hypothesis, even when the using the retrieved record document score listed in the first position ( X  X axRSV X ) or by using eq. (5) ( X  X ormRSV X ). Under these merging strategies, we assume that document scores computed by search engines working with different corpora are not comparable. Therefore, these document scores must be norma lized before they can be used as keys to sort the retrieved items . As depicted in Table XIII, such a merging strategy would account for the difference between a given document score and the document score for between the first and the third item in the Korean collection is relatively small compared Therefore, the third document of the Korean corpus  X  X R710 X  must appear before the second document extracted form the Chinese corpus  X  X H572 X . not just one document per collection per r ound, but one document from both the English and Chinese collections and two from the Japane se and Korean. A merging strategy such as this exploits the fact that the Japanese and Korean corpora possess more articles than the English or the Chinese collections (see Table 1). So we may assume that the Japanese or Korean corpus will contain more pertinent information than the English or Chinese collection. Eq. (6)) to define a comparable document sc ore across the collections. This merging provide much greater RSV values than do the others, and such documents must be presented to the user. Manmatha et al. [2001] propose a similar idea when modeling the document score distribution in the form of a mixture model. On the other hand, when the document scores from a given result list are all more or less the same, we must consider that such a distribution contains a very large number of irrelevant documents. numbers of pertinent items or that each co llection is searched by IR models having different mean retrieval performances. To refl ect this bias when using a given collection or search engine, we could multiply each normalized document score by a corresponding Japanese and Korean result lists and 1.0 to th e English and Chinese runs. In this study, we increased the weight att ached to the Japanese and Ko rean languages because these collections contained more documents, a nd hopefully more relevant documents. outcome, according to a set of explanatory va riables [Le Calv X  and Savoy 2000]. In our current case, we predicted the probability that document D k would be relevant, given independently for each language using S+ software), we sort ed the records retrieved from separate collections in order to obtain a single ranked list. But in order to estimate the underlying parameters, this approach requires that a training set be developed. In our evaluations we di d this by using the leaving-one-out approach was made up of all queries except one; this la st request was used to compute the average precision for this single query. Finally, we iterated them over the query samples, generating 60 different training sets (compos ed of 59 queries) and 60 query evaluations from which a mean average precision could be computed. average precision obtained i ndependently for each language, using the Prosit search model along with query expansion (number of top-ranked documents / number of additional search terms). The middle part depicts the mean average precision when searching the Chinese, Japanese, and English collections (CJE), while the bottom part also includes the Korean language (CJKE) . In this table and for both multilingual environments, the round-robin merging strategy serves as a baseline upon which using manually translated topics. As depicted in Tables XIV and XV, we could then estimate retrieval effectiveness due to automatic query translation strategies. Moreover, the experiments shown in Table XV may be used to confirm the findings in Table XIV. that might be viewed as statistically superior to that of the round-robin baseline. When considering manually translated queries as shown in Table XV, more merging strategies resulted in significantly better performance than the round-robin scheme did.  X  X ormRSV X ) provide reasonable performance levels, with the  X  X ormRSV X  merging scheme being slightly better. In our experiments, while the retrieval effectiveness of the raw-score approach was not very good, decreases in performance were usually not statistically significant compared to the round-robin scheme (except for manually translated queries and CJKE search, as shown in the bottom part of Table XV). Our biased round-robin scheme seems to perform better when compared to the simple round-# postings Inverted file size Building time T queries Mean query size Search time per query TDNC queries Mean query size Search time per query actually contain more relevant items than another. In this study we assume that the number of documents in a given collection is correlated with the number of relevant for the English and Chinese corpora, and 1.2 for both the Japanese and Korean languages) usually achieved better performance levels than the round-robin approach Table XIV). For all multilingual searches, our logistic merging scheme produced the best mean average precision, and was always statistically superior to the round-robin approach. As a second-best approach, Tables XIV and XV indicate that our weighted Z-score merging scheme always produced the second-best retrieval performance. queries, the best automatic run had a mean average precision rate of 0.1978 compared to 0.2505 (or a 21% difference in relative performance). When compared with the CJKE multilingual search and T queries, the diffe rence was greater (0.1676 vs. 0.2549, or 34.2%). computational efforts required to build and sear ch these test collections. The top part of number of documents. The "# postings" row indicates the number of terms (words for the English corpus, bigrams for the three Asian languages) in the inverted file. The next row shows the inverted file size and the following row depicts the time (user CPU time + query size and search time (in seconds) re quired for both short (T) and long (TDNC) queries (measured without blind query expansion). III/600 (memory: 1 GB, swap: 2 GB, disk: 6 x 35 GB). CONCLUSION Successful access to multilingual document collec tions requires an effective monolingual indexing and search system, a combined query translation approach, and a simple but efficient merging strategy [Braschler and Peters 2004; Chen and Gey 2004; Savoy 2004a]. Using this blueprint, derived during the latest CLEF evaluation campaigns, we evaluations when indexing As ian languages based on bigrams, the  X  X nu-ltc X  vector space or the Okapi probabilistic IR models (see Ta bles III to V) achieve the best retrieval performance levels. Blind-query expansion has proven to be a worthwhile approach, especially when processing short queries a nd using the Prosit IR model (see Tables VII could be considered, although this technique would require additional computational resources (see Table X). those for certain European languages [Savoy 2004a; 2004c], especially given the number and questionable quality of freely available translation resources. Thus, when compared with corresponding monolingual searches in which we translated user information from English into Chinese, Japanese, or Korean languages, overall retrieval effectiveness decreases more than 30% for the Japanese , and more than 50% for the Chinese and Korean languages (see Table XI). To improve this poor performance, we could concatenate two (or more) translations (see th e last two rows of Table XI), employ a blind query expansion approach (see Ta ble XII), and a data-fusion approach. that when merging ranked lists of retrieve d items provided by separate collections, good retrieval effectiveness is obtained with th e Z-score merging procedure (around 5% better that the round-robin approach). When a representative query sample is available, however, the logistic merging scheme always produces the best retrieval effectiveness (between 10% (CJKE, manual query translation, T queries) to 24% (CJE, automatic query translation, T queries) better that the round-robin approach). The author would like to thank the task NTCIR-4 CLIR organizers for their efforts in developing the Asian test-collections and the anonymous referees for their useful suggestions and comments. The author would also like to thank C. Buckley from SabIR for giving us the opportunity to use the SMART system, together with Samir Abdou and Pierre-Yves Berger for their help in translating the English topics. This research was supported in part by the Swiss National Science Foundation under Grant #200020-103420. In Table A.1, w ij represents the indexing weight assigned to term t j in document D i . To define this value, we use n to indicate the number of documents in the collection and nt i the number of distinct indexing units (bigrams or terms) included in the representation of D . We assigned values to the constant b as follows: 0.5 for both the Chinese and Japanese corpora; 0.55 for the English; and 0.75 for the Korean. While we fixed the assigned c = 2 for the Japanese and Korean corpus; c = 1 for the English; and c = 1.5 for the Chinese. These values were chosen b ecause they usually result in better retrieval performance levels. Finally, the value  X  mean dl X  was fixed at 151 for the English, 480 for the Chinese, 144 for the Japanese, and 295 for the Korean corpus. 
