 Determiner placement (choosing if a noun phrase needs a determiner, and if so, which one) is a non-trivial problem in several language processing tasks. While context beyond that of the current sen-tence can sometimes be necessary, native speakers of languages with determiners can select determin-ers quite well for most NPs. Native speakers of lan-guages without determiners have a much more diffi-cult time.

Automating determiner selection is helpful in sev-eral applications. A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by non-native speakers (Lee, 2004), and choose determiners for text generation programs.

Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998). Knight and Chander (1994) use decision trees to choose between the and a/an , ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus. (Deciding between a and an is a trivial postprocessing step.)
Minnen et al. (2000) use a memory-based learner (Daelemans et al., 2000) to choose determiners of base noun phrases. They choose between no deter-miner (hencefore null ), the , and a/an . They use syn-tactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al., 1991). They report 83.58% accuracy.

In this paper, we use the Charniak language model (Charniak, 2001) for determiner selection. Our ap-proach significantly improves upon the work of Min-nen et al. (2000). We also use additional automat-ically parsed data from the North American News Text Corpus (Graff, 1995), further improving our re-sults. The language model we use is described in (Char-niak, 2001). It is based upon a parser that, for a sentence s , tries to find the parse  X  defined as: The parser can be turned into a language model p ( s ) describing the probability distribution over all pos-sible strings s in the language, by considering all parses  X  of s : Here p (  X , s ) is zero if the yield of  X  6 = s .
The parsing model assigns a probability to a parse  X  by a top-down process. For each constituent c in  X  it first guesses the pre-terminal of c , t ( c ) ( t  X  X ag X ), then the lexical head of c , h ( c ) , and then the expansion of c into further constituents e ( c ) . Thus the probability of a parse is given by the equation where l ( c ) is the label of c (e.g., whether it is a noun phrase NP, verb phrase, etc.) and H ( c ) is the rel-evant history of c  X  information outside c deemed important in determining the probability in question. H ( c ) approximately consists of the label, head, and head-part-of-speech for the parent of c : m ( c ) , i ( c ) , and u ( c ) respectively and also a secondary head (e.g., in  X  X onday Night Football X  Monday would be conditioned on both the head of the noun-phrase  X  X ootball X  and the secondary head  X  X ight X ).

It is usually clear to which constituent we are re-ferring and we omit the ( c ) in, e.g., h ( c ) . In this no-tation the above equation takes the following form: p (  X  ) = Y
Next we describe how we assign a probability to the expansion e of a constituent. We break up a tra-ditional probabilistic context-free grammar (PCFG) rule into a left-hand side with a label l ( c ) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. For each expansion we distinguish one of the right-hand side labels as the  X  X iddle X  or  X  X ead X  symbol M ( c ) . M ( c ) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a con-stituent from among the heads of its children. To the left of M is a sequence of one or more left labels L ( c ) including the special termination symbol  X  , which indicates that there are no more symbols to the left. We do the same for the labels to the right, R ( c ) . Thus, an expansion e ( c ) looks like: The expansion is generated first by guessing M , then in order L through R
Let us turn to how this works in the case of de-terminer recovery. Consider a noun-phrase, which, missing a possible determiner, is simply  X  X BI. X  The language model is interested in the probability of the strings  X  X he FBI, X   X  X /an FBI X  and  X  X BI. X  The ver-sion with the highest probability will dictate the de-terminer, or lack thereof. So, consider (most of) the probability calculation for the answer  X  X he FBI: X  Of these, the first two terms, the probability that the head will be an NNP (a singular proper noun) and the probability that it will be  X  X BI X , are shared by all three competitors, null , the , and a/an . These terms can therefore be ignored when we only wish to identify the competitor with the highest probability. The next two probabilities state that the noun-phrase contains a determiner to the left of  X  X BI X  and that the determiner is the last constituent of the left-hand side. The last of the probabilities states that the de-terminer in question is the . Ignoring the first two probabilities, the critical probabilities for  X  X he FBI X  are: Conversely, to evaluate the probability of the noun-phrase  X  X BI X   X  i.e., no determiner, we evaluate: We ask the probability of the NP stopping immedi-ately to the left of  X  X BI. X  For  X  X /an FBI X  we evalu-ate: This equation is very similar to Equation 6 (the equation for  X  X he FBI X , except the term for the prob-ability of the is replaced by the sum of the probabil-ities for a and an .

To choose between null , the , or a/an , the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability. 2.1 Training the model As with (Minnen et al., 2000), we train the lan-guage model on the Penn Treebank (Marcus et al., 1993). As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al., 2006) to train our language model on up to 20 million addi-tional words. The best results of Minnen et al. (2000) are using leave-one-out cross-validation. We also test our lan-guage model using leave-one-out cross-validation on the Penn Treebank (Marcus et al., 1993) (WSJ), giving us 86.74% accuracy (see Table 1).

Leave-one-out cross-validation does not make sense in this case. When choosing determiners, we can train a language model on similar data, but not on other NPs in the article. Therefore, for the rest of our tests, we use tenfold cross-validation. The difference between leave-one-out and tenfold cross-validation is due to the co-occurrence of NPs within an article. Church (2000) shows that a word appears with much higher probability when seen elsewhere in an article. Thus, a rare NP might be unseen in tenfold cross-validation, but seen in leave-one-out.
For each of our sets in tenfold cross validation, we use 80% of the Penn Treebank for training, 10% for development, and 10% for testing. The divisions occur at article boundaries. On our development set with tenfold cross-validation, we get 84.72% accu-racy using the language model (Table 1).

As expected, we achieve significant improvement when adding NANC data over training on data from the Penn Treebank alone (Table 1). With 20 mil-lion additional words, we seem to be approaching an upper bound on the language model features. We obtain improvement despite the fact that the parses were automatic, but there may have been errors in determiner selection due to parsing error.

Table 2 gives  X  X rror X  examples. Some errors are wrong (either grammatically or yielding a signifi-cantly different interpretation), but some  X  X ncorrect X  answers are reasonable possibilities. Furthermore, even all the text of the article is not enough for clas-sification at times. In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help. With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al., 2000) on the determiner selection task. This shows the benefits of the language model features in determining the most grammatical deter-miner to use in a noun phrase. Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance. There is a high rate of ambiguity for many of the misclassified sentences.
The success of using a state-of-the-art language
Guess Correct Sentence null the (2) In addition, the Apple II was an affordable $1,298. model in determiner selection also suggests that one would be helpful in making other decisions in the surface realization stage of text generation. This is an avenue worth exploring.

