 This paper proposes a neural language model to capture the inter-action of text units of different levels, i.e.., documents, paragraphs, sentences, words in an hierarchical structure. At each paralleled level, the model incorporates Markov property while each higher-level unit hierarchically influences its containing units. Such an architecture enables the learned word embeddings to encode both global and local information. We evaluate the learned word embed-dings and experiments demonstrate the effectiveness of our model. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing abstracting methods; I.2.7 [ Computing Methodolo-gies ]: Artificial Intelligence Natural Language Processing Algorithms; Experimentation neural network; hierarchical model; distributed representations; word embeddings
Word embeddings represent words using real-valued, abstract and condensed vectors. There are two main families for learn-ing embeddings for words: The first family leverages document-level word-occurrence statistics, such as LDA [Blei et al., 2003], GloVe [JeffreyPennington and Manning, 2014], or matrix factor-ization based approaches (e.g., LSA and SVD) given the intuition that co-occurrent words are relevant. Those global co-occurrence statistics based models neglect word order information about how local meanings are formed by neighboring words. The second cate-gory corresponds to local context window approaches (e.g.,[Bengio et al., 2006, Collobert and Weston, 2008, Mikolov et al., 2013a]). The downside of such models is that they poorly harness the global information at document, paragraph or sentence level. Some at-tempts try to bridge the gap between the two families: [Huang c  X  et al., 2012] proposes a document-level vector leveraged from tf-idf into local learning process and paragraph vector [Le and Mikolov, 2014] makes word prediction with the help of the leveraged doc-ument/paragraph/sentence level information. [Li et al., 2015b] ex-plores hierarchical autoencoder for paragraph and document repre-sentations. Their efforts prove useful for learning sentence, para-graph and document representations. But they do not consider us-ing the relations between different levels to improve the word em-beddings which are the basis of all. Towards better word embed-dings, we consider the intrinsic structure of text about how units are arranged to form meaningful context: (1) Horizontally : as we look at discourse theory in early days (Mann and Thompson (1988)), in a coherent text, not only words, but clauses, sentences, and larger multi-clause groupings are tightly connected. Text units take their respective roles and interact with units at the same level (token-to-token, sentence-to-sentence and paragraph to paragraph) semantically, syntactically, and logically. (2) Vertically : Words form the meanings of sentences; sentences form paragraphs, and then paragraphs form documents, which or-ganizes the arrangements into a tree structure vertically.
The importance of tree structures in sentence, paragraph and document representation has been revealed by previous research [Socher et al., 2013, Li et al., 2014, Li et al., 2015a]. Here we show how they can be used to improve the word embeddings. The pro-posed model captures the two aforementioned aspects of meanings in a unified embedding learning framework which holds promise to bridge the gap between the co-occurrence based and prediction based embedding learning frameworks. Horizontally, we model each level of units based on the Markovian manner, where neigh-bouring unites are correlated based on the similar assumption we make in language model. Vertically, each unit (e.g., sentence) ex-erts its impact on its containing lower-level text units (e.g., words). Unlike [Huang et al., 2012] where document-level information is harshly incorporated, the proposed approach gently incorporates the order information at paragraph level and sentence level, and therefore preserves the semantic integrity of the contexts.
The adopted type of architecture arranges all text units in a uni-fied structure, where influence of one unit is propagated to others (the siblings and children), naturally bridging the gap of and tak-ing the merits from the aforementioned two learning families. To note, our approach is inspired by paragraph vector model [Le and Mikolov, 2014] where models paragraph and tokens within it in a two-level hierarchy where words are predicted given neighbours and its resided paragraph.

The proposed architecture is ultimately grounded on the lowest level of the hierarchical structure, words, by predicting the current token, where the embeddings for neighboring tokens, and higher levels text units are simultaneously updated. The proposed algo-rithm is a general one and can be adjusted to currently prevailed frameworks, e.g., skip-grammar models, CBOW, recurrent neural models [Mikolov et al., 2010]. The system can be optimized by standard strategies taken in embedding learning literature, either through standard softmax, or others like hierarchical softmax, NCE or negative sampling.

Note that the proposed algorithm ends up with distributed rep-resentations for documents, sentences and words, which could be used as input for different applications for different levels of units. But here we just keep the word embeddings for the proposed model has not been optimized regarding to upper level text units. For these large text units, it is still not clear how their meanings are made up from their containing words. Though several composition based neural network models [Tai et al., 2015, Zaremba and Sutskever, 2014, Socher et al., 2013] have been proposed and proved useful in a range of tasks, none of them manage to achieve the expected level of performance as word embedding models do.

We evaluate the learning frameworks on word analogy and word similarity tasks, two basic tasks for word embedding evaluation. Experimental results demonstrate that by harnessing the hierarchi-cal structure of documents, we obtain better performances.
Document D is comprised of a sequence of paragraphs D = f
P 1 ; P 2 ; ::; P N D g , paragraph P is comprised of a sequence of sen-tences P = f S 1 ; S 2 ; :::S N P g and sentence S is comprised of a sequence of words S = f w 1 ; w 2 ; :::; w N S g , where N N
S respectively denote the number of correspondent children in the document, paragraph and sentence. Each level text unit D , P , S , w is associated with a K dimensional embedding e D , e and e w . All text units are therefore arranged in into a tree hierar-chy with L = 4 levels. Let denote any node in the tree, where could be document, paragraph, sentence or word with embedding e . parent ( ) , sibling ( ) and kid ( ) respectively denote the par-ent, siblings and kids of .
We first revisit the general neural learning framework for word embedding widely adopted in existing research.

Consider a sequence of word tokens f w 1 ; w 2 ; :::; w N ditional probability of current word is given by: where denotes the parameter space involved in the probability function f () . g () denotes the operation performed on neighboring vectors. Many different types of g () have been explored such as av-eraging neighboring embeddings (CBOW) [Mikolov et al., 2013a], getting the dot product between w n and each of its neighbors (skip-grams) [Mikolov et al., 2013a], concatenating neighboring vec-tors and projecting the concatenating into a low-dimensional space (e.g., [Collobert et al., 2011, Vaswani et al., 2013]), or convolv-ing the preceding words using a recurrent network [Mikolov et al., 2011].

Commonly used forms of f () include predicting current word using a softmax function or contrastive sampling. Many alterna-tives have been proposed for easy training, such as hierarchical softmax [Mikolov et al., 2013b] and Noise-contrastive Estimation [Vaswani et al., 2013, Mnih and Teh, 2012]. Our model take advantages of the hierarchitecture of text. The model extends standard embedding learning framework by sub-sequently predicting embedding of every node along the tree structure given its parent and siblings:
Thus, the probability of the whole document is given by: As can be seen, for two words that do not reside in the same sen-tence, they will still distantly interact with each other as the influ-ence is propagated up to the containing sentence embedding, para-graph embedding and document embedding, and then down to the other word. Therefore, the proposed model can to some extent cap-ture global level statistics without losing the advantages of local neural composition.

On the other hand, based on the Markov property along each level of the trees, the meanings of adjacent text units interact with each other and preserves the integrity of meanings at each level, potentially leading to better representations at lower levels. Even-tually these merits will be further propagated to word level predic-tion, leading to better word level embeddings.

For illustration purpose, we assume g () takes the form the con-catenation of sibling embeddings and parent embedding. f ( ) takes the form of sigmoid function at sentence/paragraph level and soft-max at word level. Let P denote the the paragraph that sentence S resides in, and S denotes the sentence that word w i resides in, we have: where ( ) denotes sigmoid function.

Parameters and embeddings are estimated by making MLE estimation:
Parameters and word embeddings are to be estimated from the training corpus. Meanwhile we also estimate embeddings of document, paragraph and sentence given containing words and the correspondent embeddings. MLE estimation is implemented as is the same with previous work. A similar strategy can be found in [Le and Mikolov, 2014]. The estimated embeddings can be used as feature for downstream applications. Paragraph Vector 69.2 77.8 72.9 58.0 39.6 Paragraph Vector 62.4 79.1 65.8 56.9 34.2 Paragraph Vector 70.0 77.1 72.5 57.2 37.9 T able 1: Spearman correlation results on word similarity tasks. Dimensionality of vectors are set to 300. All reported results are based on embeddings trained from the same Wiki2014 dataset. For each subset, Paragraph Vector and Joint Learning use the same f ( ) and g ( ) as the model at the top.

We employ three forms of operational functions. (1) Skip-grammar model [Mikolov et al., 2013a]: (2) CBOW like model [Mikolov et al., 2013a] which first averages the embeddings of parent and siblings and dot products with current node embedding: (3) Concatenation model which can takes sequence order informa-tion by first concatenating embeddings of parent and siblings and then projects the concatenated vector sharing same dimensionality with current node embedding: where [ ] denotes the concatenation of its containing vectors and W denotes the (1 + N ) K dimensional convolutional matrix. For concatenation approach, we use a dropout [Hinton et al., 2012, Srivastava, 2013] rate of 0.5.

The initialization of embeddings for sentences, paragraphs and sentences are conducted by averaging embeddings for its contain-ing tokens using tf-idf, similar as in [Huang et al., 2012].
Word embeddings are evaluated in terms of standard word sim-ilarity measures to see whether taking account text hierarchy can improve those measures. We train our models using Wikipedia2014 dataset. We adopt a hierarchical softmax function for word predic-tion. The window size is set to 11.

We employ standard ontology evaluation metrics include Tofel-353 [Finkelstein et al., 2001], MC [Miller and Charles, 1991], RG [Rubenstein and Goodenough, 1965], SCWS [Huang et al., 2012], T able 2: Results on word analogy task. Models are trained on the same Wiki2014 corpus. Skip-Grammar and CBOW are trained on Word2Vec. and RW [Luong et al., 2013]. Each dataset is comprised of pairs of words with gold-standard human annotations, indicating the simi-larity score between the pair of words. For example,  X  X ook, paper, 7.46" denotes the similarity score for word pair (book, paper) is 7.46. Standardly, we adopt cosine similarity. Spearmans rank cor-relation coefficient is then obtained between this score and human judgement.

Baselines include Skip-Grammar, CBOW, Concatenation para-graph vector [Le and Mikolov, 2014]. Experimental results are illustrated in Table 1. As can be seen, by considering the hierar-chical structure of text units, better performance can be and have been achieved.

Word analogy evaluation aims at answering questions like  X  X  is to b as c is to what". Question types include semantic ones like  X  X eijing is to China like London to what" (captial) or syntactic ones like  X  X ance to dancing as fly to what" (tense). The dataset is introduced in [Mikolov et al., 2013a] and contains 8,869 semantic questions and 10,675 syntactic questions. We follow the protocols described in [Mikolov et al., 2013c, JeffreyPennington and Man-ning, 2014] that to answer questions  X  X  is to b as c is to what", we do the simple math by computing E b E a + E c , where E denotes the embedding for current word, and find the word d with closest representation based on cosine similarity.

Performances regarding different models are illustrated in Table 2. Similar phenomenon are observed as word similarity tasks where better performances are observed when text structure are consid-ered. The proposed model gives better performances than previous models for word embeddings. Comparing with previous models, we consider both the local and global information.

The sole aim of this paper is to use text structure to improve word embeddings. We do generate embeddings for sentences, para-graphs and documents. But we cannot expect them to produce sat-isfying performances for a range of tasks without further improve-ments, which is our future work.
We present a hierarchical neural network model for word embed-dings learning. Experiments verify the effectiveness of the learned word embeddings. As stated above, the learning of large text unit embeddings remains a problem. In the future we will explore new architectures to learn powerful embeddings for sentences, para-graphs and documents. [Bengio et al., 2006] Bengio, Y., Schwenk, H., Sen X cal, J.-S.,
Morin, F., and Gauvain, J.-L. (2006). Neural probabilistic language models. In Innovations in Machine Learning , pages 137 X 186. Springer. [Blei et al., 2003] Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of machine
Learning research , 3:993 X 1022. [Collobert and Weston, 2008] Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning , pages 160 X 167. ACM. [Collobert et al., 2011] Collobert, R., Weston, J., Bottou, L.,
Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch. The Journal of
Machine Learning Research , 12:2493 X 2537. [Finkelstein et al., 2001] Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., and Ruppin, E. (2001). Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World
Wide Web , pages 406 X 414. ACM. [Hinton et al., 2012] Hinton, G. E., Srivastava, N., Krizhevsky,
A., Sutskever, I., and Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 . [Huang et al., 2012] Huang, E. H., Socher, R., Manning, C. D., and Ng, A. Y. (2012). Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1 , pages 873 X 882.

Association for Computational Linguistics. [JeffreyPennington and Manning, 2014] JeffreyPennington, R. and Manning, C. (2014). Glove: Global vectors for word representation. [Le and Mikolov, 2014] Le, Q. V. and Mikolov, T. (2014).
Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053 . [Li et al., 2015a] Li, J., Jurafsky, D., and Hovy, E. (2015a). When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185 . [Li et al., 2014] Li, J., Li, R., and Hovy, E. (2014). Recursive deep models for discourse parsing. [Li et al., 2015b] Li, J., Luong, M.-T., and Jurafsky, D. (2015b).
A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057 . [Luong et al., 2013] Luong, M.-T., Socher, R., and Manning, C. (2013). Better word representations with recursive neural networks for morphology. CoNLL-2013 , 104. [Mikolov et al., 2013a] Mikolov, T., Chen, K., Corrado, G., and
Dean, J. (2013a). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 . [Mikolov et al., 2010] Mikolov, T., Karafi X t, M., Burget, L.,
Cernock ` y, J., and Khudanpur, S. (2010). Recurrent neural network based language model. In INTERSPEECH , pages 1045 X 1048. [Mikolov et al., 2011] Mikolov, T., Kombrink, S., Deoras, A.,
Burget, L., and Cernocky, J. (2011). Rnnlm-recurrent neural network language modeling toolkit. In Proc. of the 2011 ASRU
Workshop , pages 196 X 201. [Mikolov et al., 2013b] Mikolov, T., Sutskever, I., Chen, K.,
Corrado, G. S., and Dean, J. (2013b). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems , pages 3111 X 3119. [Mikolov et al., 2013c] Mikolov, T., Yih, W.-t., and Zweig, G. (2013c). Linguistic regularities in continuous space word representations. In HLT-NAACL , pages 746 X 751. Citeseer. [Miller and Charles, 1991] Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and cognitive processes , 6(1):1 X 28. [Mnih and Teh, 2012] Mnih, A. and Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426 . [Rubenstein and Goodenough, 1965] Rubenstein, H. and Goodenough, J. B. (1965). Contextual correlates of synonymy.
Communications of the ACM , 8(10):627 X 633. [Socher et al., 2013] Socher, R., Perelygin, A., Wu, J. Y., Chuang,
J., Manning, C. D., Ng, A. Y., and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP . [Srivastava, 2013] Srivastava, N. (2013). Improving neural networks with dropout . PhD thesis, University of Toronto. [Tai et al., 2015] Tai, K. S., Socher, R., and Manning, C. D. (2015). Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075 . [Vaswani et al., 2013] Vaswani, A., Zhao, Y., Fossum, V., and
Chiang, D. (2013). Decoding with large-scale neural language models improves translation. In EMNLP , pages 1387 X 1392.
Citeseer. [Zaremba and Sutskever, 2014] Zaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv preprint arXiv:1410.4615 .
