 Assistance technology is undoubtedly one of the important ele-ments in the commercial search engines, and routing the user to-wards the right direction throughout the search sessions is of great importance for providing a good search experience. Most search assistance methods in the literature that involve query generation, query expansion and other techniques consider each suggestion can-didate individually, which implies an independence assumption. We challenge this independence assumption and give a method to maximize the utility of a given set of suggestions. For this, we will define a measure of conditional utility for query pairs using query-URL bipartite graphs based on the session logs (clicked and viewed URLs). Afterwards, we remove the redundant queries from the suggestion set using a greedy algorithm to be able to replace them with more useful ones. Both offline (based on user studies and session log analysis) and online (based on millions of user in-teractions) evaluations show that modeling the conditional utility and maximizing the utility of the set of queries (by eliminating re-dundant ones) significantly increases the effectiveness of the search assistance both for the presubmit and postsubmit modes.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval Experimentation, Algorithms Search Assistance, Diversification, User Browsing Models
Search assistance modules are significant elements in commer-cial search engines. Two main flavors of the search assistance are the presubmit and the postsubmit modes, namely the assistance before and after the users submit their queries. Presubmit technol-ogy is based on target auto-completion, whereas in the postsub-mit, suggestions might be specializations, generalizations or lateral moves. Specializations are suggestions that add terms to the origi-nal query such as  X  X ython tutorial X , X  X onthy python X  for the query  X  X ython X  and generalizations are queries that drop terms from the original query such as suggesting  X  X fo X  for  X  X fo flight status X . Lat-eral moves are suggestions to related entities or concepts that do not impose any lexical overlap in either direction, such as  X  X anon 50d X  to  X  X ikon d90 X  or  X  X almart X  to  X  X arget X .

The dominant presubmit search assistance technology in the search industry is based on completing user X  X  partially typed query (will be referred as the prefix hereafter) to help the users save time by not requiring them to manually type the whole query into the search box, and also help them phrase their query when they start typing but do not exactly know which query terms to use or how to spell them. The common solution to this problem is to suggest the most popular (hence, most frequent) queries that match the user X  X  prefix. Presubmit search assistance is a very clear case where the intent of the user is not defined. Let alone the ambiguities in the search queries, in presubmit, the input from the user is not even a full query yet. In this application, the aim is to minimize the number of characters that the user types before she finds at least one useful suggestion. This understanding assumes that as long as the user does not find at least one useful suggestion in the suggestion set she will keep typing the query manually. Therefore, if the aim is to minimize the time/effort by the user before she finds a useful sug-gestion, one needs to maximize the utility of the suggestion set, by trading some popularity (analogous to the relevance in web search ranking) for some diversity.

In the postsubmit mode, after the user issues the query, the intent is much more clear. Therefore, postsubmit assistance technology includes more sophisticated techniques, but still the core techniques are some derivatives of the marginal query frequency that lexically matches the query and popular query reformulations -which han-dle each suggestion independently. Query reformulations use the probability distribution of possible next queries that the current query is formulated to; more formally p ( q next | q current more, to ensure that the next query is related to the current query, rather than having a high reformulation probability just because it has a high marginal probability, the pointwise mutual information p ( q next ,q current ) denotes the probability that these two queries were issued by the same user within a short time frame. In addition to the above, there are also more sophisticated methods that use the geolocation and the search history of the user [18, 6] to provide a more contextualized and personally relevant information. Re-gardless of the underlying suggestion technology, if the approach is handling each suggestion candidate independently and sorting by some suggestion level score, there might be redundant suggestions in the suggestion sets in postsubmit.

Understanding the user intent is the key for designing an effec-tive search assistance system and when such knowledge is lacking, search engines need to diversify the query suggestions in order to improve the quality of search assistance. The need for diversify-ing the presented query suggestions to maximize the total utility of a given suggestion set finds a good analogy to the web search ranking problem. In web search ranking, since a particular query might have multiple intents, it is well-known that presenting the most relevant results on the top positions might be suboptimal, and one should trade-off some relevance in return for some diversity in the result set [7]. In this view, the goal is to maximize the proba-bility that the user will find at least one relevant document in the result set, hence minimizing query abandonment. The need to do some trade-off between relevance and diversity can be regarded as finding a relevant but also diverse set of documents. It is not well-defined how to adjust the trade-off between these two some-what conflicting criteria, and the ill-defined nature of the problem yields many different techniques and formulations [1, 8, 9, 26, 2, 20]. A good summary and comparison of the approaches and a unifying view is presented in [13]. Although not nearly as much in web ranking, diversity has also been studied in the context of query suggestions. There are two recent publications on this sub-ject based on random walks over query-URL bipartite graph [17] and manifold ranking [27]. In the next section, we will describe these two recently published methods in more detail, and we will also provide experimental comparisons.

We give a model to maximize the utility of a given suggestion set based on a diversity definition that we derive from a intuitive user browsing model. Since both the hitting time method and the mani-fold ranking method require the input query, to our knowledge, our approach is the only query suggestion diversification method that is applicable to presubmit, where only the partially-typed query is available. We show that in postsubmit the proposed examination model based utility maximization works better than existing meth-ods, especially for less frequent queries. Another contribution is a path-based evaluation metric for measuring the utility of post-submit suggestions that we call path gain/loss (PGL). PGL aims to measure the quality of the postsubmit suggestions by the average time it takes the user to reach a destination URL.

This paper is structured as follows. In the next section, we present several related query expansion/generation/rewriting methods in the recent literature, and our suggestion utility model is given in Sec-tion 3. In Section 4, we provide the details of our experimental setup. We present results on both offline (based on user studies and analysis of past session logs) and online (based on user inter-actions in session logs of a major search engine) evaluations of the model in Section 5, and conclude the paper with a discussion and proposed future work in Section 6.
Our suggestion set utility model is based on defining the condi-tional utility between pairs of queries, which has similar charac-teristics to defining a query similarity. There are many methods in the literature that define similarities between queries, and also utilize these for various problems, such as query rewriting, query generation or query suggestions.

Cui et al. look at the co-occurrences of query terms and docu-ment terms of the clicked documents and some term clustering to find similar queries [11]. Jones et al. use independence hypothesis log-likelihood ratio to find popular reformulations and use these as candidate suggestions [16], and build a supervised model to build a scoring scheme for suggestions. Probably Baeza-Yates &amp; Tiberi and Craswell &amp; Szummer are the first to use a query-url biparite graph for finding semantic relations [4, 10]. In a more recent work, Mei et al. build a bipartite graph of query-URL pairs, and use the click frequencies as the edges [18]. Using this graph, they gener-ate query suggestions by looking at the pairs of queries that lead to clicks on same set of URL X  X . They also suggest using the same con-ditioning on a particular user X  X  -or a user group X  X -click data only to generate personalized query suggestions. Cao et. al. use the same query-URL clickthrough bipartite graph idea, but they compliment it with a concept-tree [6]. Sahami and Heilman take another ap-proach by investigating the common terms in the top n retrieved documents, and they build a similarity function using tf-idf vectors of these terms [21]. A similar idea is presented by Baeza-Yates et al., where they do clustering based on the aggregation of the term-weight vectors of the clicked URL X  X  [3].

Overall, similarity measures defined over a query-URL bipartite graph that has the clickthrough values on the edges, and cosine sim-ilarity or Jensen-Shannon (or Kullback-Leibler) divergence over a set of most common terms in the top retrieved URL X  X  are the most commonly used methods to define query similarity. In our view, the utility of each query is conditioned on the results of the earlier (higher-ranked) suggestions -also the results of the original query in postsubmit mode. Note that all above work focuses on defin-ing a similarity measure between queries and our work is different mostly because we do not model the similarity of two queries; our aim is to model the conditional utility of one query given the other one is already presented.

Recently, studying diversity in the context of query suggestions has also attained some interest [27, 17, 5]. In [5], although the au-thors do not specifically aim for diversifying the query suggestions, they define an intuitive measure -distinct URLs in the multiset-for measuring the diversity of the suggestion sets that produce. In [27], authors build a query nearest neighbor graph to obtain a query man-ifold, and use a manifold ranking approach to generate a diverse set of suggestions. Although this approach is theoretically appealing and works effectively for frequent queries, practical usefulness of this approach is rather limited since the relevance of the sugges-tions drop very significantly for infrequent queries, since the query manifold becomes very sparse. In [17], Ma et al. use random walks over query-URL bipartite graph and hitting time to get a diversified set of query suggestions, and this is similar to the hitting time based method presented earlier by Mei et al. [18].

To our knowledge, the manifold ranking idea by Zhu et al. [27] and the hitting time approach by Ma et al. [17] are the only papers in the literature that are aiming to introduce diversity into query suggestions. One common shortcoming of both these two methods is they define the objective as  X  given a query, generate a di-verse and relevant suggestion set  X , hence neither one of them is -neither directly nor with trivial modifications-suitable for presub-mit, where the query is yet to be defined. We define the problem as  X  given a set of suggestions of size N, rerank the suggestions such that the overall utility in top K rank is maximized for all K  X  N  X , which is suitable for presubmit as well as postsubmit.
Our model is based on estimating the pairwise conditional utility on the returned and clicked URLs 1 for each query pair. We argue that a query suggestion should be presented only if it can lead the user to the URLs that are not reachable more easily via the already presented suggestions -as well as the URLs presented for the orig-inal query for the postsubmit mode. We define a measure of utility that a query suggestion provides, conditioned on the original query and higher ranked suggestions. Using this, we define a submodu-lar objective function over the set of queries, and develop a greedy algorithm to maximize the utility of a given suggestion set.
We build the click bipartite graph of queries and clicked URLs, and the edges are the clickthrough values. Namely the weight on the edge that connects a particular URL u and a query q we have, where c ( u ) is a binary random variable, denoting whether the URL u is clicked or not, and N u disp and N u click are the number of times u is displayed and clicked for query q , respectively.

Similarly, we also build a view bipartite graph for queries and displayed URLs, where the edges are the average rank discount of the URL. In search, it is reasonable to assume that the importance of a document depends on its probability of being examined by the user. Therefore, Discounted Cumulative Gain (DCG) is among the most commonly used information retrieval metrics [14], which is given by where r represents the rank of the results, and the typical val-ues for N are 1,3,5. Note that the DCG formula can be consid-ered as the inner product of two vectors, relevance vector rel = [ rel 1 ,...,rel r ] and discount vector d =[1 ,..., 1 / log where the relevance vector represents the gain, and the discount vector represents the examination probabilities. In our view bipar-tite graph, the edges are discount values that DCG uses; however, since the results that the search engine change in time, if one looks into the query logs for a long period of time, one can see that a par-ticular URL might have been returned at different ranks for a given query. Therefore here we use the average rank discount given by where K is the number of times the URL u is returned for the query q in the session logs.
We assert that a query suggestion should be presented only if it leads to a sufficiently different result set as compared to those of the already presented suggestions and the original query. Therefore, we define U ( q s | q p ) , the utility of the suggestion q q , a query that is already presented to the user. Simply, q the original query in the postsubmit mode or the partial query at the word boundaries in the presubmit mode or an already presented suggestion.

Intuitively, given two queries q s and q p , for each URL in the result set of q s , we would like to check whether the same URL is in the result set of q p ; and if so, whether the URL is ranked as high as in q s or not (so that the user is at least as likely to examine it). The simplest case is that the two queries q s and q p having exactly the same result sets. There are many such pairs of queries, mainly due to query rewriting widely used by search engines [24, 15, 22]. For example, for the prefix  X  X ed X , the two most frequent suggestions are  X  X ed bath and beyond X  and  X  X ed bath &amp; beyond X , leading to exactly the same result set. If queries are sorted by popularity (for a given prefix), there are examples of such duplicate suggestions for even the simplest case of exactly same result sets.

Note that, in general using the query similarity of some type is not a good choice to model the utility of the suggestions, since it assumes U ( q s | q p ) . = U ( q p | q s ) by definition, which is fundamen-tally incorrect. The relevant URLs in the result set of q a superset of those in q s . A symmetric query similarity measure would not capture the required difference between a missing good result in the result set of q s versus an additional good result in q and the similarity drops in both cases.

Given q s and q p , let us define the result sets of these two queries as URL s =[ u s 1 ,...,u sN ] and URL p =[ u p 1 ,...,u pN q , the click probabilities on the URL X  X  are given as in (1). Here we don X  X  consider any constraints in the rank of the URL X  X .
To make the model more stable for queries with very few clicks and pageviews, we introduce a prior on the click probabilities based on the average rank, which is available in the view bipartite graph. where  X  adjusts the strength of the prior distribution. Low values of  X  leads to a low precision model for queries with very few clicks, whereas a high  X  value leads to a too conservative model; hence a significant decrease in recall. We experimented with different values of  X  , and empirically found  X  =1 works best.

For each u si  X  URL s , we define the examination probability that the user will examine the URL in the result page of q lows, where e ( u ) is a binary random variable denoting whether u is ex-amined or not. In words,
Combining (5) and (6), we define the conditional utility U ( q
U ( q s | q p )=1  X  where both terms in the summation can be obtained from the click and view bipartite graphs directly. Intuitively, the first term in the summation gives how important this particular URL is for the query q , and the second term gives how likely it is that the same user would examine this URL in q p , with the assumption that the user would go as deep into the result set in q p as well. Hence, U ( q is, by definition 0 if the results of the two queries are exactly the same (since p ( e ( u )=1 | q p )=1 for all URLs, and p ( c ( u )=1 sums up to 1). Also, U ( q s | q p ) would be close to 0 for queries that share many URLs and rank them similarly, and vice versa.
One missing piece in the utility definition is that it measures the amount of novelty that the suggestion may bring, however it does not model the relevance of the suggestion in any way. For example for two queries that are totally irrelevant, we have U ( q very close to 1. Same can be said about the diversity measures in web ranking; they are not useful by themselves, and have to be used along with some relevance criteria. Next section presents how we use this pairwise measure for a set of suggestions also taking their relevance into account.
People with the same/similar intent do not phrase their queries in exactly the same way. Therefore, a search assistance system that is handling each suggestion candidate independently may perform poorly. For example, in the presubmit, the most popular queries that are matching the prefix  X  X al X  include  X  X al mart X ,  X  X al mart stores X , and  X  X al mart online X , leading to almost identical result pages. Similarly, in the postsubmit, two most popular reformula-tions of the query  X  X al mart X  includes  X  X arget X  and  X  X arget stores X . The missing piece is to model the additional utility of a suggestion, given the other ones that are already presented.

The pairwise conditional utility definition given in (7) can be regarded as the probability that q s might satisfy the user given that she is not satisfied by q p . We now state the problem of suggestion set utility maximization, as given an input (prefix or full query) maximizing the probability that an average user finds at least one useful suggestion within the suggestion set of size K .

Let sat be a binary random variable that denotes the proba-bility that the user is satisfied, and S denote the suggestion set. For a particular user input q in , given the satisfaction probabilities p ( sat =1 | s ) for each suggestion s  X  S , we would like to find the subset of suggestions S  X  S with | S | = K that maximizes the probability of satisfaction The satisfaction probability p ( sat =1 | s ) depends on the appli-cation and might be popularity for presubmit, and reformulation probability for postsubmit. For brevity, we drop the conditioning on q in since the formulation and all below derivation is given for a particular user input.

Intuitively, we would like to find a set of queries, with highest in-dividual probability of satisfaction and all of which are sufficiently different from each other. One important observation is that the ob-jective p ( sat =1 | S ) is submodular, and a greedy algorithm that picks the query with highest marginal utility at each step can ap-proximately solve the problem [19]. Submodularity is defined as follows, Definition 1: Given a finite set N ,asetfunction f :2 N  X  R is submodular iff for all sets S 1 and S 2  X  N such that S 1  X  S 2 ,and e  X  NS 2 , f ( S 1 + e )  X  f ( S 1 )  X  f ( S
In words, a function is submodular if it satisfies the principle of diminishing returns . In our case the utility of adding a sug-gestion into a larger set is no more than the utility of adding the same suggestion into a smaller set, because a larger set of sugges-tions has higher probability that more of the users have already been satisfied, and the added value of the same additional sugges-tion gets smaller as the size of the set increases. More formally, let T 1 = S 1  X  e ,and T 2 = S 2  X  e . Following the notation in Definition 1, one can write Then, the additional utility becomes Very similarly, one can write the same for the other sets, S Sincewehave 0  X  U ( e | s j )  X  1 ,and T 1  X  T 2 , we can conclude Hence, the objective function given in (8) is submodular. For a submodular function, Nemhauser et al. show that the error of a greedy algorithm that selects the maximum marginal utility at each step is bounded [19], and the constructed set satisfies where S  X  is the optimal set and S is the set selected by the greedy algorithm. We will design a such greedy algorithm to select S , however one thing to consider is that this submodular objective function is defined over the set, hence it does not take the order-ing into account. This is due to the assumption that the users will examine all K suggestions, which in practice is not true; users have higher probability of examining the suggestions that are ranked higher. Therefore, our algorithm is also designed to give an ordered list rather than a set, which is of the following form;
One constraint we have here is that the algorithm should not in-crease the latency significantly, and the bottleneck here is the utility function has to be stored for all query pairs. To increase the time efficiency of the algorithm we binarize the conditional utility, and use U ( s | s j ) &lt; X  at the last step. For the optimal  X  value that we found with editorial tests (will be presented shortly), the utility function does fit into the memory, even for the web scale.
Another observation is that the above approach captures the util-ity optimization within the presented suggestions and rank them with optimal order, but one piece that is missing is to compare the suggestion candidates against the original query, to ensure that all presented suggestions satisfy U ( q s | q in ) as well. However, one thing that one should consider in the prefix phase is that we don X  X  know q in is a full query or not. For example,  X  X aceb X  is a very com-mon misspelling of  X  X acebook X  and since the search engine cor-rects this misspelling, the result sets of these two queries are iden-tical, therefore U ( X  facebook |  X  faceb )=0 . This suggests that  X  X acebook X  should not be displayed for the prefix  X  X aceb X , which is quite counter-intuitive. Therefore we examine the conditional utility with the user input (and remove the suggestion q s U ( q s | q in ) &lt; X  ), only if the marginal query frequency of q greater than q s .

One final remark is, to preserve the optimal ordering of the out-put set S , one should account for the probability of satisfaction of the queries that are not included into the set. This is in fact one of the main differences of our greedy algorithm compared to the ex-isting greedy algorithms proposed for result set diversification [1]. For a query, p ( sat =1 | s ) should be its satisfaction probability plus the satisfaction probabilities of all of its near-duplicates (all sug-gestions that were not added to the output set S since they failed to pass the conditional utility thresholding due to this particular query). For example, assume for the prefix  X  X wk X  the suggestions in the input set S are,  X  X wkward X ,  X  X wkward tv moments X ,  X  X wk example X ,  X  X wk examples X  and  X  X nix awk example X , with decreas-ing p ( sat =1 | s ) . If  X  X wk examples X  and  X  X nix awk example X  are removed from the list since they are a duplicate of an earlier pre-sented suggestion  X  X wk example X , then one should adjust p ( sat = 1 |  X  awk example ) as p ( sat =1 |  X  awk example )+ = p ( sat = 1 |  X  awk examples )+ p ( sat =1 |  X  unix awk example ) ,so that after the duplicate removal the total satisfaction probability of this intent is properly represented, and this query has the chance to move upwards in the suggestion list.

Let us give a few definitions for formalizing the algorithm. f ( q ) : The marginal query frequency
S list ( q ) : A function which given an input query q , returns the set of query suggestions in descending probability of satisfaction.
S dup ( q, X  ) : A function which given an input query and a thresh-old  X  , returns the set of query suggestions with utility lower than the given threshold, U ( q s | q p ) &lt; X  . We implement this as a look up table for a predetermined  X  value, which yields a very fast compu-tation time. Even for the web scale, the database required for this step fits into the memory for the optimal  X  value.

S pre : A function which given an input query q and the output set S , returns the set of queries within S ,that q is a duplicate of. Simply, S pre ( q ) is the query (or queries) that caused q to be removed. In almost all cases this only returns one query, but in general, a new query might be a near-duplicate of more than one query in S .

The pseudo-code of the set utility maximization algorithm is pre-sented in Algorithm 1. After the initializations (lines 1-4), the first for-loop eliminates the duplicates of the user input (prefix or full query) by also considering the marginal frequency of the input (lines 5-9). Afterwards, the second for-loop does the greedy utility optimization within the suggestion set (lines 10-21). Every time a suggestion is decided to be added to the output set S , this query and all its duplicates are added to the set Dup , the set of queries that we remove the duplicates of which keeps growing as the for-loop proceeds (line 14). Every time a query is decided not to be added to the output set S , we move its satisfaction probability to the corresponding query (or queries) in the output set S (lines 16-19), and once we reach the desired set size K , we sort results with decreasing probability of satisfaction to present them in the opti-mal order (line 22). At the end, all queries in S are sufficiently different from each other (and sufficiently different from the user input q in if q in is a full query), and the queries in S are sorted by descending satisfaction probability.
 Algorithm 1 Greedy Suggestion Set Utility Maximization
Before going into the results that we obtained with our model, in this section we describe the session logs data that we used to build the bipartite graphs presented in Section 3.1, and the details of our offline and online test setup.
We use 6-month of anonymized session logs of a major commer-cial search engine to build the view and click bipartite graphs. To make the data size tractable we ignored queries that appeared less than 3 times in this 6-month period, and bipartite graphs consist of roughly 119M unique queries and 658M unique URLs in total. The view graph is much denser than the click graph, since by definition the edges in the view graph are a superset of the edges in the click graph. After building the graphs we construct the non-symmetric conditional utility matrix, whose entries are U ( q s | q pairs. Given a  X  value, constructing the look-up table needed for S dup is rather straightforward from this conditional utility matrix.
In the user study, we provide a pair of queries to the judges and ask them to assess the utility of the second query suggestion, given that the first one is already presented. Note that small differences in queries that seem insignificant to people just by looking at the query strings can have very different intents and search results (for exam-ple,  X  X ffice X  vs  X  X he office X ). Therefore, in this task, we specifically ask the judges to examine the search results of each query before assessing the utility/redundancy. We also do a side-by-side test, where we present two sets of queries to the judges and ask them to assess which of the suggestion sets is better. All judges are pro-fessionals trained to evaluate search engines, and this test includes 12 individuals. The judges were asked to check the search result pages of each suggestion before making a decision according to the following guidelines in a 4-level grade scale. 3-Useful : This suggestion introduces distinct, useful, unique information that is different from that of the presented suggestion. 2 -Somewhat useful : There is some content overlap with the presented suggestion, but there is enough of a difference that you do not consider the suggestions wholly redundant. 1 -Somewhat redundant : There is a minor difference in intent between the first suggestion and second, but they are mostly the same. Showing this suggestion might offer a somewhat differ-ent experience for the user, but the overlap between first and second suggestion is great and the second is not likely to be useful. 0 -Redundant : This suggestion definitely does not add any new information to presented suggestion. It would not be useful to show to any user because its intent/content is wholly redundant with the presented suggestion.
In the session log analysis, we perform comparisons of sugges-tion relevance and diversity, with respect to the following evalua-tion metrics.

Relevance metric: In presubmit, the query is yet to be de-fined and relevance cannot be defined. Therefore, we use average query popularity -average marginal query frequency-instead of rel-evance. In postsubmit evaluation, we will use the reformulation probability p ( q next | q current ) estimated from the query logs and averaged over the suggestions in the set, as the relevance measure-ment. Reformulation probability is defined as follows, given the user issued q current the probability that the same user issues q within 10 minutes. Not to introduce a presentation bias, here we only consider manual reformulations (reformulations due to query suggestion clicks are not counted).

Diversity metric: To measure diversity, we adopt a simi-lar measure as in [5], the total number of URLs in the multiset per query: the total number of unique URLs at top 5 ranks for all queries in the suggestion set, divided by the number of queries in the set.

The set of algorithms to be compared in the session log analysis are given below.

REL: Optimize for relevance; at any point add the query that maximizes the relevance metric into the suggestion set. Optimizing for relevance is in fact the dominant approach in the commercial search engines.

DIV: Optimize for diversity and use relevance to break ties; start with the most popular query, and at any point among the queries that maximize the diversity metric, add the query with maximum query frequency into the suggestion set.
 DQS: The random walk hitting time based method in [17].
 UM: Our suggestion utility maximization algorithm.

Note that we decided not to include manifold ranking method in [27] into the comparisons. Although this method is theoreti-cally appealing, it seems applicable to only frequent queries, where there exist a sufficient number of related queries in the query neigh-borhood to construct the query manifold reliably. In their paper, the authors provide results over queries that have appeared at least 700 times in a month of query logs, which roughly corresponds to 0.002% of the overall unique query traffic. For infrequent queries, the query manifold becomes very unreliable and the relevance of the query suggestions drop very significantly, making this method impractical for real applications.
The online test is a classical A/B test setting, where the search traffic is split into two exclusive groups, the baseline (control) and the test buckets. In the A/B test the method and the baseline is pre-sented to millions of users, and their interactions are recorded for the analysis. The user populations in the baseline and the test buck-ets are uniformly sampled, hence they are not biased towards heavy or light users. The query volume, and query distributions (for ex-ample the proportion of navigational and informational queries or other similar statistics etc.) are very similar in both buckets. The test bucket implements our suggestion set utility maximization al-gorithm, both in the presubmit and postsubmit. For both presubmit and postsubmit, we log when our algorithm modifies the sugges-tion set in order to be able to find the subset of queries which we will refer to as affected queries . In the following, we describe the configuration in the baseline bucket as well as the search as-sist modules within which the test and baseline configurations are applied. Except these search assist modules mentioned below, ev-erything on the search result page is identical for both the test and baseline buckets, including the search results, sponsored search re-sults, page layout and user interface. The test is kept alive for a sufficient period of time such that all metrics that we are interested can reach statistically significant values.

Baseline Configuration: The baseline configuration is a simple modification of REL . It disregards diversity and does not consider the conditional utility of each suggestion and it is opti-mized for relevance (popularity in presubmit), except that it imple-ments two simple query string based deduping operations:  X 
Test Configuration: UM , our suggestion utility maximiza-tion method.

Search Assistance Modules: There are three search as-sistance modules that we will apply the changes in this A/B test. Suggest-as-you-type (SAYT) is the presubmit search assistance module. Given a prefix, the top 5 queries with the highest score that match the prefix are presented under the query box like a drop-down menu. North Also Try (NAT) is the postsubmit search assis-tance module that is displayed above the search results. The user is likely to see these suggestions -probably even before examining the search results. South Also Try (SAT) is the postsubmit search as-sistance module that is displayed below the 10 search results, right above the next page button. For a given query, the suggestions presented in NAT and SAT modules are exactly the same (same suggestion set in the same order), and depending on the number of characters in each suggestions, up to 5 suggestions can be dis-played.
Our experiments consist of three parts, (i) user studies that we did for selecting the  X  value and to measure the accuracy at this operating point, (ii) session log analysis to compare our method (at this  X  value) to relevance and diversity baselines as well as an earlier proposed method for both presubmit and postsubmit, (iii) a thorough analysis of the results from the A/B test, again indepen-dently for presubmit and postsubmit.
We present two user studies. First one is a pairwise test for tuning the  X  value presented in the greedy algorithm, which is basically the threshold of U ( q s | q p ) that we will consider q s is already presented. The second test is a side-by-side test at the particular  X  value selected. Figure 2: U ( q s | q p ) vs. the editorial grades (3-useful, 2-somewhat useful, 1-somewhat redundant, 0-redundant) Query pairs sampled from presubmit (red) and postsubmit (blue) are presented. Figure 3: precision-recall curve of the U ( q s | q p ) and the precision-recall curve of the majority classi-fier. The marker shows the selected operating point where  X  =0 . 24 .
To find the optimal  X  value to execute the greedy algorithm, we sampled 2400 unique ( q s ,q p ) pairs. 1600 of these samples are coming from query pairs that were displayed together in Suggest-as-you-type. While collecting the supervised learning data for train-ing/testing a query classifier or some retrieval model, it is a com-mon practice to generate sample set weighted with the traffic, hence the frequent queries are respected more and added into the training set with greater likelihood. With the same motivation, we collect this sample in a way that the sampling probability of a particu-lar query pair is directly proportional to the number of times that they are displayed together (so pairs like  X  X acebook X  and  X  X ace-book login X  displayed millions of times in the suggest-as-you-type together for the same prefix have much higher likelihood of getting into this sample set). Similarly, we sampled 800 query pairs from the postsubmit suggestion pairs that are popular reformulations of the same query, where the sampling probability is directly propor-tional with the reformulation probability. Hence, we again sample more from more frequent query pairs.

The editorial test evaluates how useful the suggestion q s that q p is already presented to the user in a four grade scale (3-useful, 2-somewhat useful, 1-somewhat redundant, 0-redundant). Figure 2 shows the distribution of the data samples, U ( q the editorial grade. We binarize this 4-grade by mapping useful and somewhat useful to 1, and redundant and somewhat redundant to 0. We investigate the precision-recall curve of this dataset (shown in Figure 3) to select  X  , and decided to use  X  =0 . 24 . This corre-sponds to 0.85 precision and 0.65 recall on this dataset.
To measure the performance of UM at the particular  X  value se-lected, we designed a side-by-side test. For a given prefix or full Table 1: A side-by-side sample of REL (left) and UM (right) query, we provide the judges the top 5 suggestions of the base-line configuration and the test configurations defined in Section 4.4 (
REL with some tweaks and UM ), and ask them which of the sugges-tion sets is better.

To construct the sample set first we get the 500 most frequent queries in our session logs. From these, for each query we grab 3 random prefixes between 2-6 characters, and we end up with 758 unique prefixes. For these 758 prefixes, the side-by-side test signif-icantly favors the UM against REL . The judgments are 42 (5.5%) is better, 191 (25.2%) UM is better, and 525 (69.3%) no noticeable difference. Table 1 shows some samples from this side-by-side test.
To follow up with the cases that the editor thinks UM is worse, for these 42 cases we provided the editors the individual ( q and asked them to do a pairwise comparison (exactly as in the pre-vious subsection), to identify the useful suggestions that we incor-rectly removed. For 28 of these 42 cases, the editors did not report any useful suggestion that is removed, and mentioned that the prob-lem is not the useful suggestions that are incorrectly removed, but the relevance of the suggestions that rank higher after removing these. For example, see the  X  X al m X  example in Table 1 where the algorithm removes  X  X al mart stores X ,  X  X al mart supercenter X ,  X  X al mart online X  as reduntant since  X  X al mart X  is presented, and the ed-itor agrees that these are all redundant suggestions. However, the editor notes that the relevance of the suggestions (originally ranked 7 th , 8 th and 9 th ) that bump up after removing the redundant ones are really low, and despite the redundancy, the original suggestion set looks better.

The small follow-up study over the negative side-by-side exam-ples suggests that tuning  X  in a query-dependent manner can fur-ther improve the accuracy, and will be the focus of our future work. Overall, the side-by-side test shows that the judges think that leads to better results as compared to the baseline REL more than 82% of the time, when it introduces noticeable changes.
Having set the  X  value, before designing an online test we eval-uate UM against the baseline algorithms ( REL and DIV ) as well as a recently proposed algorithm DQS by analyzing our past session logs. We present two comparisons, one for presubmit and one for postsubmit using the metrics defined in Section 4.3.

For presubmit evaluation, we randomly sample 25K prefixes from query logs and evaluate the diversity and relevance metrics (rele-vance metric at prefix is marginal query frequency) at each rank up to 5. Note that DQS method is not included in the presubmit evalu-ation, since by definition it requires the input query, which is yet to be defined in presubmit. Figure 4 shows the presubmit evaluation from session logs, and we normalize the relevance and diversity metrics to [0,1]. Similarly for postsubmit evaluation, we randomly sample 25K queries and repeat the same experiment with DQS included, results are shown in 5.

In both presubmit and postsubmit, the diversity baseline DIV seems that it is always -at least up to rank 5-able to find some query suggestion that keeps the diversity metric maximal at 1.0, however it has significantly worse relevance at all ranks. Also, the relevance baseline REL with the optimal relevance metric has the worst diver-sity metrics at all ranks for both presubmit and postsubmit. Note that unlike the diversity metric, the optimal relevance metric, hence average marginal query frequency (presubmit) and average refor-mulation probability (postsubmit) decreases with rank.

In presubmit, for the relevance baseline REL , the average diver-sity at rank 2 is significantly lower than the average over top 3, 4 or 5 ranks. This is presumably due to the fact that popular queries of generally navigational nature and their redundant variants appear together at top two ranks more frequently. Overall, in presubmit, UM improves the diversity metric over the relevance baseline significantly, without inducing any noticeable loss in relevance.
In postsubmit, relevance baseline REL again has the worst di-versity metric, and diversity baseline DIV has the worst relevance metric. In terms of diversity, UM and DQS both improve significantly over the baseline with similar amount of compromise on the rele-vance metric. In terms of diversity, UM performs better than similar to the postsubmit, it brings roughly a 30% improvement in the diversity metric over the relevance baseline REL .
In this section we will provide the evaluation of the session logs obtained by the A/B test on a major search engine, and unless stated otherwise, all presented results are statistically significant. The key for evaluating the online tests effectively is to interpret the user interactions correctly. Therefore, throughout this section we will Figure 6: The probability of prefix length (as a func-tion of number of characters), for queries that the algorithm modified the results before a query sug-gestion is clicked. also briefly provide the reason why we think the metrics mentioned here are useful ones.
The overall aim of Suggest-as-you-type is to auto-complete the user X  X  prefix, hence to minimize the time and effort it takes to the users while they enter their search queries. Therefore, a natural metric for comparing the test and baseline configurations in the pre-submit phase is measuring the change in the time and effort.
Let us define the affected queries as follows: the algorithm af-fected the query if the Suggest-as-you-type suggestion set was mod-ified by the algorithm at anytime before the query is submit-ted . We found that, for this definition, on the affected queries the utility maximization model decreases the average number of char-acters typed by the user before submitting the query by 1.2% (p-value&lt;0.0001) against the baseline. Although in the right direction, this does not seem to be a big difference. The reason why becomes clearer after observing Figure 6, where we show the probability that the user stops (either clicks a suggestion, or finishes typing the whole query) at a given prefix length, as a function of number of characters typed. Both distributions show very similar character-istics except the 3 to 8 characters region, and that is presumably because this is not a good definition of affected queries. The reason is, for example, if what the user has in mind was  X  X aucet handle opens wrong way X  she probably did not care too much about that our algorithm modified results along the way, for example the fact that the  X  X acebook X  variants were removed for the prefix  X  X a X .
Let us now define the affected queries as: Suggest-as-you-type suggestion set was modified by the algorithm at the prefix that a query suggestion is clicked . For this definition of affected queries, we see a much more significant difference as shown in Fig-ure 7. The difference in average prefix length is -7.4% (p&lt;0.0001), the probability that the user finished the search before 5 characters given that the query is longer than 5 characters increases by 31.3% (p=0.0004). Also, the time it takes the user to get to a long-dwell-time URL decreases by roughly one second on average (p=0.0021). One second might not sound that significant in the beginning, but considering the overall query volume, it adds up to a lot.
For these two definitions of affected queries, the overall suggest-as-you-type click through rate change is 1.14% increase, and 0.75% decrease against baseline, both not statistically significant. Hence, the overall clickthrough rate of the module is flat. Note that unlike the web search scenario, multiple clicks per query (here prefix) is not possible. One can either type the whole query manually (which can be regarded as abandonment of the suggestion module) or se-lect a single suggestion, and a flat click through difference between the baseline and the bucket can be interpreted as UM does not re-move many useful queries while introducing diversity into the sug-Figure 7: The probability of prefix length (as a func-tion of number of characters), for queries that the algorithm modified the results at the prefix that a query suggestion is clicked. gestion set, and the suggest-as-you-type does not get abandoned more. We don X  X  directly aim to increase the number of clicks in this module, we just want the users to get what they want sooner.
In the postsubmit, we tested both algorithms for two different suggestion placements, above the search results (NAT) and beneath the search results (SAT). For both the baseline and the bucket, the NAT and SAT suggestions are the same, hence we have four con-figurations to compare: 1) REL at NAT, 2) g UM at NAT, 3) SAT, and 4) UM at SAT.

An argument similar to that in the prefix phase also applies here, we would like to present the suggestion set that minimizes the time it takes to the user to get to what they want. Note that suggestion click trough rate is not a good metric for this, since what the user want is not a suggestion anymore, it is a search result. In fact, there are many cases that decreasing the suggestion click through rate by removing low utility suggestions can save the user a lot of time. Suppose for the query  X  X ank of america X , the suggestion is  X  X ank of america online X . Clicking on this suggestion very rarely takes the user to a destination URL that was not already on the first search result page, and this suggestion just distracts and causes the user to lose time. We want the users to spend less time for reaching the destination URLs, and this is exactly the sort of suggestion click that we would like to get rid of. On the other hand, if a useful suggestion is removed that could take the user to a destination URL that is not on the original page, the user would have to reformulate the query manually, which would take longer time.

One thing that we have observed is when not distracted with low utility suggestions, users tend to click more on the search results. All result set click trough rate on the bucket is 1.296% (p=0.0007) higher than the baseline, where the biggest gains were in position 2 to 5 with 4.0%, 8.3%, 9.8% and 11.0% increase in clickthrough, respectively (all with p&lt;0.0001). Also, while they show exactly the same suggestions and NAT click trough rate decreases very sharply -7.9% (p&lt;0.0001) against the baseline, there is no statistically sig-nificant change in the SAT results -0.4% (p=0.11). Given that the same suggestions were displayed at NAT and SAT, the difference can be interpreted as UM is not removing suggestions that would be useful to the user after examining the first 10 results.
In addition to the click trough rates above, we also use a metric that based on user X  X  search paths to evaluate the effectiveness of our utility maximization algorithm. A search path is the sequence of search events conducted by a search user that started from an initial query, possibly went through a few query reformulations and result clicks, finally reaching a destination page. Intuitively, if either the retrieval results and/or the suggestions are better, the average time of the search paths to final destination URL should get smaller. Table 2: PGL results for four test configurations
We introduce a path-based metric which measures the perfor-mance of a suggestion algorithm in terms of additional gain/loss of suggestions observed by users. We call this metric as  X  X ath gain/loss X  (PGL) metric. To calculate the PGL metric, we first mine the session logs of the A/B test and pulled out the competi-tive search paths that start from the same query and end at the same destination URL. There are several identification methods for des-tination URL in the literature [25, 12]. Without loss of generality, we used a simple destination URL classifier based on page dwell time. We consider a clicked page is a destination URL if its dwell time is greater than 100 seconds. Among those competitive search paths (i.e., paths having the same start and end points), we further divided them into two classes.
 The first class of paths all involve some query suggestion clicks. We call this class of paths as assisted paths . For the other class of paths, they do not have any query suggestion click. This class of paths is called algo paths . Intuitively, if we see the time distribu-tion for the assisted paths is shorter than that for algo paths, we can conclude the query suggestions are good at saving user X  X  time.
To compare the time distribution between these two classes of paths, we sorted the algo paths by their path times and chose the path time at 20th percentile as the pivotal path time. We then com-pared the assisted path one by one against the pivotal path time. If the path time is shorter than the pivotal path time, we count it as a gain for the clicked query suggestion. If the path time is longer than the pivotal path time, we count it as a loss for the clicked query sug-gestion. (Note that the reason we chose 20th percentile point is our selected system requirement, and this selection is a design choice -we want our assisted paths to be significantly faster than algo paths on average.) In this process, we updated the gain/loss counters as-sociated to each suggestion. We repeated the above gain/loss anal-ysis for all combinations of starting query and destination page ob-served in the session logs. At the end, we aggregate the gain/loss counts across all clicked suggestions for the total gain/loss counts. The total gain/loss counts reflect the overall query suggestion per-formance observed by search users. We simply define PGL as the following equation: Note that PGL value is in the range of 0 to 1, and higher PGL value means the query suggestion algorithm that is being evaluated is on average reducing the time that it takes the user to reach a destina-tion URL. Table 2 shows the PGL results for four test configura-tions. For both NAT and SAT, UM performs better than the baseline algorithm REL , with 8.5% and 12% relative improvements in PGL ratio, respectively.

Note that for both NAT and SAT, we see the utility maximization algorithm has fewer assisted paths than the baseline algorithm. This is expected since our goal is to remove useless suggestions that the user is likely end up at a destination URL that she could have found in the first result page anyway. Therefore, the algorithm tends to offer suggestions for the minor intents, saving time of the people that are interested in these minor intents to help them get where they want without typing the query manually, and saving time of the people that are interested in the major intent by not distracting them. On the affected queries, we found that the average search path in the test bucket is 0.87 seconds shorter than the baseline on average. In Table 5.3.2, we show the NAT results of some example queries in our A/B test. The table columns shows the query itself, query suggestion algorithm, total number of queries received, total loss count for the query, total gain count, PGL value, and average path time, and the corresponding suggestions.
Most query expansion and query generation methods in the lit-erature that are being used for search assistance handle the sugges-tion candidates individually, and diversity of the query suggestions is generally neglected. We present a utility maximization algorithm to introduce diversity into the suggestion sets by modeling the con-ditional utility for pairs of queries, and we use this pairwise mea-sure to build an algorithm to maximize the utility of a suggestion set. Overall, the problem that we tackle and the method we provide is analogous to the result set diversification problem, while the for-malization of added utility of a suggestion is novel, and the greedy algorithm we develop is customized for the search assistance.
Our problem formulation is different from existing suggestion diversification models in the literature that define the problem as  X  given a query , generate a diverse and relevant suggestion set X  [27, 17]. Our approach is to frame the problem as  X  given a set of suggestions of size N, rerank the suggestions such that the overall utility in top K rank is maximized for all K  X  N  X , which not require the query to be given, hence the model is suit-able for both presubmit and postsubmit suggestions. Our model is based on a user examination model, and its performance does not drop for infrequent queries -presumably thanks to the smoothing in (5) and the fact that the examination probabilities in (6), which are derived from the rank discount factors in DCG, can reliably be es-timated even with a few observations. In the session logs analysis, our model performed better than the random walk based diversifi-cation approach presented earlier by Ma et al. [17]. In the online evaluations, we show that our a model gets the users to what they want -a query completion suggestion that they find useful in the presubmit mode, and a destination URL in the postsubmit mode-in shorter time.
