 Eric Wiewiora wiewiora@cs.ucsd.edu Garrison Cottrell gary@cs.ucsd.edu Charles Elkan elkan@cs.ucsd.edu Departmen t of Computer Science and Engineering Univ ersit y of California, San Diego La Jolla, CA 92093-0114, USA Humans rarely approac h a new task without presump-tions on what type of beha viors are likely to be ef-fectiv e. This bias is a necessary comp onen t to how we quic kly learn e ectiv e beha vior across various do-mains. Without suc h presumptions, it would tak e a very long time to stum ble upon e ectiv e solutions. In its most general de nition, one can think of advice as a means of o ering exp ectations on the usefulness of various beha viors in solving a problem. Advice is crucial during early learning so that promising beha v-iors are tried rst. This is necessary in large domains, where reinforcemen t signals may be few and far be-tween. A good example of suc h a problem is chess. The objectiv e of chess is to win a matc h, and an appro-priate reinforcemen t signal would be based on this. If an agen t were to learn chess without prior kno wledge, it would have to searc h for a great deal of time before stum bling onto a winning strategy . We can speed up this pro cess by advising the agen t suc h that it real-izes that taking pieces is rew arding and losing pieces is regretful. This advice creates a much richer learning environmen t but also runs the risk of distracting the agen t from the true goal { winning the game. Another domain where advice is extremely imp ortan t is in rob otics and other real-w orld applications. In the real world, learning time is very exp ensiv e. In order to mitigate \thrashing" { rep eatedly trying ine ectiv e ac-tions { rew ards should be supplied as often as possible (Mataric, 1994). If the problem is inheren tly describ ed by sparse rew ards it is very dicult to change the re-ward structure of the environmen t without disrupting the goal.
 Advice is also necessary in highly stochastic environ-men ts. In suc h an environmen t, the exp ected e ect of an action is not immediately apparen t. In order to get a fair assessmen t of the value of an action, the action must be tried man y times. If advice can focus this exploration on actions that are likely to be optimal, a good deal of exploration time can be saved. Incorp orating bias or advice into reinforcemen t learn-ing tak es man y forms. The most elemen tary metho d for biasing learning is to choose some initialization based on prior kno wledge of the problem. A brief study of the e ect of di eren t Q-v alue initializations for one domain can be found in Hailu and Sommer (1999). The relev ance of this metho d is highly dep enden t on the internal represen tations used by the agen t. If the agen t simply main tains a table, initialization is easy , but if the agen t uses a more complex represen tation, it may be very dicult or imp ossible to initialize the agen t's Q-v alues to speci c values.
 A more subtle approac h to guiding the learning of an agen t is to manipulate the policy of the agen t directly . The main motiv ation for suc h an approac h is that learning from the outcomes of a reasonably good policy is more bene cial than learning from random explo-ration. A metho d for incorp orating an arbitrary num-ber of external policies into an agen t's policy can be found in Malak and Kholsa (2001). Their system uses an elab orate policy weigh ting scheme to determine when follo wing an external policy is no longer bene -cial. Another twist on this idea is to learn directly from other agen ts' exp eriences (Price &amp; Boutilier, 1999). These metho ds have the adv antage that they do not rely on the internal represen tations the agen t uses. On the other hand, they only allo w advice to come in the form of a policy . Also, because the policy the agen t is learning from may be very di eren t from the policy an agen t is trying to evaluate, the types of learning algorithms the agen t can use are restricted. If reliable advice on whic h actions are safe and e ec-tive is kno wn, one can restrict the agen t's available actions to these. Deriving suc h exp ert kno wledge has been hea vily studied in the con trol literature and has been applied to reinforcemen t learning by Perkins and Barto (2001). This metho d requires extensiv e domain kno wledge and may rule out optimal actions.
 The metho d we dev elop in this pap er closely resem bles shaping. With shaping, the rew ards from the environ-men t are augmen ted with additional rew ards. These rew ards are used to encourage beha vior that will even-tually lead to goals, or to discourage beha vior that will later be regretted. When done haphazardly , shaping may alter the environmen t suc h that a policy that was previously sub optimal now becomes optimal with the incorp oration of the new rew ard. The new beha vior that is now optimal may be quite di eren t from the intended policy , even when relativ ely small shaping re-wards are added. A classic example of this is found in Randlv and Alsrm (1998). While training an agen t to con trol a bicycle sim ulation, they rew arded an agen t whenev er it moved towards a target destination. In re-sponse to this rew ard, the agen t learned to ride in a tigh t circle, receiving rew ard whenev er it moved in the direction of the goal. Poten tial-based shaping, whic h we will describ e in detail later, was dev elop ed to pre-vent learning suc h policies (Ng et al., 1999). We follo w the standard reinforcemen t learning frame-work, making as few assumptions as possible about ac-cess to the dynamics of the environmen t or the internal represen tations of the agen t. Our metho d alters learn-ing by adding an advisor that is capable of changing the reinforcemen t the agen t receiv es, as well as altering the agen t's policy . Belo w we list common assumptions made about the environmen t and learning mec hanisms used by the agen t. 3.1. Terminology Most reinforcemen t learning techniques mo del the learning environmen t as a Mark ov decision pro cess (MDP) (see Sutton &amp; Barto, 1998). An MDP is de-ned as ( S; S 0 ; A; T; R; ), where S is the (possibly in-nite) set of states, S 0 ( s ) is the probabilit y of the agen t starting in state s , A is the set of actions, T ( s 0 j s; a ) is the probabilit y of transitioning to state s 0 when per-forming action a in state s , R ( s; a; s 0 ) is a stochastic function de ning reinforcemen t receiv ed when action a is performed in state s resulting in a transition to state s 0 , and is the discoun t rate that weighs the imp ortance of short term and long term rew ard. The usual reinforcemen t learning task is to nd a pol-icy : S ! A that maximizes the exp ected total dis-coun ted reinforcemen t: where r t is the reinforcemen t receiv ed at time t . Some MDPs con tain special terminal states to represen t ac-complishing a task's goal or entering an irreco verable situation. When an agen t transitions to one of these states all further actions transition to a null state, and all further reinforcemen ts are zero.
 We focus on reinforcemen t learning algorithms that use Q-v alue estimates to determine the agen t's policy . Q-v alues represen t the exp ected future discoun ted re-ward after taking action a in state s . When the Q-values for a particular policy are accurate, they sat-isfy the follo wing recursion relation: Q ( s; a ) = X The greedy policy , g ( s ) = argmax a Q ( s; a ), is optimal if the Q-v alues are accurate for this policy . In order to learn the prop er Q-v alues for the greedy policy , we could use either Q-learning or Sarsa. Both of these metho ds update the Q-v alues based on exp e-riences with the MDP . An experienc e is a quadruple h s; a; r; s 0 i where action a is tak en in state s , result-ing in reinforcemen t r and a transition to next state s 0 . For eac h exp erience, these metho ds update the Q-values according to the rule where is the learning rate, and a 0 is an action spec-i ed by the speci c learning metho d. For Sarsa learn-ing, a 0 is the next action the agen t will perform. Q-learning sets a 0 to be the greedy action for state s 0 . See Sutton and Barto (1998) for more details on these and other reinforcemen t learning algorithms. 3.2. Potential-based Shaping Ng et al. prop osed a metho d for adding shaping re-wards to an MDP in a way that guaran tees the optimal policy main tains its optimalit y. They de ne a poten-tial function () over the states. The shaping rew ard for transitioning from state s to s 0 is de ned in terms of () as: The advisor adds this shaping rew ard to the environ-men tal rew ard for every state transition the learner exp eriences.
 The poten tial function can be view ed as de ning a to-pograph y over the state space. The shaping rew ard for transitioning from one state to another is there-fore the discoun ted change of this poten tial function. Because the total discoun ted change in poten tial along any path that starts and ends at the same state is zero, this metho d guaran tees that no cycle yields a net ben-e t due to the shaping. This was the problem faced in the bicycle sim ulation men tioned before. In fact, Ng et al. pro ve that any policy that is optimal for an MDP augmen ted with a poten tial-based shaping rew ard will also be optimal for the unaugmen ted MDP . Although poten tial-based shaping is an elegan t tool for giving guidance to a reinforcemen t learner, it is not general enough to represen t any type of advice. Poten tial-based shaping can give the agen t a hin t on whether a particular state is good or bad, but it cannot pro vide the same sort of advice about various actions. We extend poten tial-based shaping to the case of a po-ten tial function de ned over both states and actions. We de ne potential-b ased advic e as a supplemen tal re-ward determined by the states the agen t visits and the actions the agen t chooses.
 One of the consequences of this extension is that the mo di cation to the MDP cannot be describ ed as the addition of a shaping function. A shaping function's parameters are the curren t state, the action chosen, and the resulting state. This is the same information that determines the rew ard function. The advice func-tion requires an additional parameter related to the policy the agen t is curren tly evaluating. Note that if the policy being evaluated is static, this parameter is e ectiv ely constan t, and therefore the advice may be represen ted as a shaping function.
 We prop ose two metho ds for implemen ting poten tial-based advice. The rst metho d, whic h we call look-ahead advic e , is a direct extension of poten tial-based shaping. A second metho d, called look-b ack advic e , is also describ ed. This metho d pro vides an alternativ e when the agen t's policy cannot be directly manipu-lated or when Q-v alue generalization may mak e look-ahead advice unattractiv e. 4.1. Look-Ahead Advice In look-ahead advice, the augmen ted rew ard receiv ed for taking action a in state s , resulting in a transition to s 0 is de ned as Where a 0 is de ned as in the learning rule. We refer to the advice comp onen t of the rew ard given the the agen t at time t as f t .
 We analyze how look-ahead advice changes the Q-values of the optimal policy in the original MDP . Call the optimal Q-v alue for some state and action in the original MDP Q ( s; a ). We kno w that this value is equal to the exp ected rew ard for follo wing the optimal policy (): When this policy is held constan t and its Q-v alues are evaluated in the MDP with the addition of advice re-wards, the Q-v alues di er from their true value by the poten tial function: In order to reco ver the optimal policy in a MDP aug-men ted with look-ahead advice rew ards, the action with the highest Q-v alue plus poten tial must be cho-sen. We call this policy biase d greedy . It is formally de ned as Notice that when the Q-v alues are initialized are zero, the biased greedy policy chooses actions with the high-est value in the poten tial function, encouraging explo-ration of the highly advised actions rst. Any policy can be made biased by adding the poten tial function to the curren t Q-v alue estimates for the purp ose of choosing an action. 4.1.1. Learnability of the Optimal Policy Although we can reco ver the optimal policy using the biased greedy policy , we still need to determine whether the optimal policy is learnable. While we can-not mak e a claim on the learnabilit y of the optimal policy under any learning scheme, we can mak e claims on its learnabilit y when the state and action space are nite. In this case, the learning dynamics for the agen t using look-ahead advice and a biased policy are essen-tially the same as an unbiased agen t whose Q-v alues were initialized to the poten tial function.
 We de ne two reinforcemen t learners, L and L 0 , that will exp erience the same changes in Q-v alues through-out learning. Let the initial values of L 's Q-table be Q ( s; a ) = Q 0 ( s; a ). Look-ahead advice F (), based upon the poten tial function () will be applied during learning. The other learner, L 0 , will have a Q-table ini-tialized to Q 0 0 ( s; a ) = Q 0 ( s; a ) + ( s; a ). This learner will not receiv e advice rew ards.
 Both learners' Q-v alues are updated based on an ex-perience using the standard reinforcemen t learning up-date rule describ ed previously: One can think of the above equations as updating the Q-v alues with an error term scaled by , the learn-ing rate. We refer to the error terms as Q ( s; a ) and Q 0 ( s; a ). We also trac k the total change in Q ( ) and Q 0 ( ) during learning. The di erence between the orig-inal and curren t values in Q ( ) and Q 0 ( ) are referred to as Q ( ) and Q 0 ( ), resp ectiv ely. The Q-v alues for the learners can be represen ted as their initial values plus the change in those values that resulted from the updates: Theorem 1 Given the same sequenc e of experienc es during learning, Q ( ) always equals Q 0 ( ) . Pro of: Pro of by induction. The base case is when the Q-table entries for s and s 0 are still their initial values. The theorem holds for this case, because the entries in Q ( ) and Q 0 ( ) are both uniformly zero.
 For the inductiv e case, assume that the entries
Q ( s; a ) = Q 0 ( s; a ) for all s and a . We sho w that in resp onse to exp erience h s; a; r; s 0 i , the error terms Q ( s; a ) and Q 0 ( s; a ) are equal.
 First we examine the update performed on Q ( s; a ) in the presence of the advice: Now we examine the update performed on Q 0 ( s; a ): Both Q-tables are updated by the same value, and thus Q ( ) and Q 0 ( ) are still equal. 2 Because the Q-v alues of these two agen ts change the same amoun t given the same exp eriences, they will al-ways di er by the amoun t they di ered in their initial-ization. This amoun t is exactly the poten tial function. Corollary 1 After learning on the same experienc es using standar d reinfor cement learning update rules, the biase d policy for an agent receiving look-ahe ad advic e is identic al to the unbiase d policy of an agent with Q-values initialize d to the potential function. This immediately follo ws from the pro of. The impli-cation of this is that any theoretical results for the con vergence of a learner's greedy policy to the opti-mal policy will hold for the biased greedy policy of an agen t receiving look-ahead advice.
 If the poten tial function mak es ner distinctions in the state space than the agen t's Q-v alue appro ximator, the agen t may perceiv e the poten tial function as stochas-tic. Because the di erence in Q-v alues between the optimal action and a sub-optimal action can be ar-bitrarily close, any amoun t of perceiv ed randomness in the poten tial function may cause the biased greedy policy to choose a sub optimal action 1 . This remains true after any amoun t of learning. So far we have assumed that the poten tial function is deterministic and stable throughout the lifetime of the agen t, and that we can manipulate the agen t's policy . If either of these conditions is violated, look-ahead ad-vice may not be desirable.
 An alternate approac h to poten tial-based biasing ex-amines the di erence in the poten tial function of the curren t and previous situations an agen t exp erienced. The advice receiv ed by choosing action a t in state s t , after being in state s t 1 and choosing a t 1 in the pre-vious time step is When the agen t starts a trial, the poten tial of the pre-vious state and action is set to 0.
 Let's examine what the Q-v alues are exp ected to con-verge to while evaluating a stationary policy and receiving look-bac k advice: Here E [( s 1 ; a 1 )] is the exp ected value of the po-ten tial function of the previous state, given . Because the agen t's exploration history factors into the advice, only on-p olicy learning rules suc h as Sarsa should be used with this metho d.
 The correct Q-v alues for all actions in a given state dif-fer from their value in the absence of look-bac k advice by the same amoun t. This means that we can use most policies with the assurance that the advised agen t will beha ve similarly to a learner without advice after both have learned sucien tly. The policies where this holds true share the prop erty that they are invarian t to a constan t addition to all the Q-v alues in a given state. Some examples of suc h policies are greedy , -greedy , and (perhaps surprisingly) softmax. Because we do not have to manipulate the agen t's policy to preserv e optimalit y, this advising metho d can also be used in conjunction with an actor-critic learning architecture. This analysis also suggests that look-bac k advice is less sensitiv e to perceiv ed randomness in the poten tial function. Learning with this form of advice already faces randomness in appro ximation the value of the poten tial function of the state and action previous to the curren t choice. Extra randomness in the poten tial function would be indistinguishable from other sources of randomness in the agen t's exp erience. This robust-ness to noise will likely come at a cost of learning time, however.
 At this point we do not have a pro of that an agen t's Q-v alues will con verge to the the exp ected values de-rived above. All exp erimen ts in tabular environmen ts supp ort this claim, however. We have tested our advice-giving algorithms in a stochastic gridw orld to gain some insigh t into the algo-rithms' beha vior. Our gridw orld exp erimen ts replicate the metho dology found in Ng et al. (1999). We use a 10 10 gridw orld with a single start state in the lower left corner. A rew ard of 1 is given for eac h action the agen t tak es, except that the agen t receiv es a re-ward of 0 for transitioning to the upp er righ t corner. When the agen t reac hes the upp er righ t corner, the trial ends and the agen t is placed bac k at the start state. Agen ts choose from four actions, represen ting an inten tion to move in one of the four cardinal di-rections. An action moves the agen t the intended di-rection with probabilit y 0 : 8, and a random direction otherwise. Any movemen t that would move the agen t o the grid instead leaves the agen t in its curren t po-sition. All learners use one-step Sarsa learning with a learning rate of 0 : 02, a tabular Q-table initialized uni-formly 0, and follo w a policy where the greedy action is tak en with probabilit y 0 : 9, and a random action is tak en otherwise.
 Under our framew ork, advice is interpreted by the agen t as a hin t on the Q-v alues. This advice may tak e two qualitativ ely di eren t forms. State-v alue advice pro vides an estimate of the value of a state while fol-lowing the agen t's objectiv e policy . This is the only type of advice poten tial-based shaping can use. The poten tial function used in state-v alue advice is equal to the minim um steps from the curren t state to the goal, divided by the probabilit y an action will move the agen t in the intended diretion.
 Adv antage advice pro vides an estimate of the relativ e adv antage of di eren t actions in a given state. In man y situations, adv antage advice is much simpler and more readily available than state-v alue advice. In our do-main, adv antage advice has a poten tial function equal to 1 for the move down or move left actions, and a value of 0 for the other two actions. This is appro xi-mately the di erence between the true Q-v alues of the sub-optimal actions and the preferable move up or left actions.
 It is also possible to receiv e com binations of the two forms of advice. In this case, the poten tial functions are added together. We de ne optimal advice as the sum of the previously men tioned state-v alue and ad-vantage advice. This advice is very close to the true Q-v alues for all states and actions, making it nearly optimal in terms of reducing learning time.
 Figure 2 sho ws the results of exp erimen ts with di er-ent types of advice using both of the algorithms. For the look-ahead advice algorithm, advice on the value of states app ears more useful than the adv antage of dif-feren t actions. This is due to the large discrepancy be-tween the agen t's initial Q-v alues and the values they con verge to. The average value of a state in the en-vironmen t is -12. Without advice on the magnitude of the Q-v alues, a good deal of exploration is required before the agen t learns a good appro ximation of the value of states.
 When advice only consists of the adv antage of di eren t actions, an interesting beha vior emerges. The agen t begins learning follo wing an optimal policy . However, later during learning the agen t abandons the optimal policy . Because the agen t has explored the optimal actions more than others, the agen t learns a better appro ximation for their Q-v alues, whic h are negativ e. The sub optimal actions are explored less, and thus have values closer to zero.
 The look-bac k advising algorithm sho ws the opp osite result. Value advice starts o very bad. The reason for this beha vior can be explained by examining how the advice rew ard function interacts with the poten tial function. When the agen t transitions from a state with low poten tial to one with a higher poten tial, it will receiv e a positiv e rew ard. Unfortunately , the agen t receiv es this rew ard only after it has tak en another action. Thus, if the agen t tak es a step towards the goal, and then immediately steps away from the goal, the Q-v alue for stepping away from the goal will be encouraged by the advice rew ard.
 The look-bac k advice algorithm performed much bet-ter with adv antage advice. The agen t immediately nds a reasonable policy , and main tains a good level of performance throughout learning. The agen t us-ing look-bac k advice is able to main tain good perfor-mance because it follo ws the policy suggested by the bias less diligen tly than the look-ahead agen t. Thus, the look-bac k agen t can pace exploration more evenly as learning progresses.
 These exp erimen ts shed ligh t on when one of these metho ds should be preferred over the other. When ad-vice on state values prev ails, look-ahead advice should be used. When the advice comes in the form of a pref-erence for action selection, look-bac k advice should be given. When both types of advice are presen t, both algorithms do very well. 6.1. Moun tain-Car Problem Our second exp erimen t examines how simple advice can impro ve learning performance in a con tinuous-state con trol task. The problem we examine is the moun tain car problem, a classic testb ed for RL algo-rithms. The task is to get a car to the top of the moun tain on the righ t side of the environmen t. The moun tain is too steep for the car to driv e up directly . Instead the car must rst bac k up in order to gain enough momen tum to reac h the summit. Lik e the grid-world environmen t, the agen t receiv es a 1 penalt y for every step the agen t tak es until the goal condition is met.
 We took existing code that solv es the moun tain car problem written by Sridhar Mahadev an 2 . By testing on existing code, we sho w that our algorithm can treat the agen t as a blac k box, and that advice can impro ve the performance of agen ts who can already solv e the problem ecien tly.
 In order to impro ve learning, We use a poten tial func-tion that encourages the agen t to increase its total me-chanical energy . This is accomplished by setting the poten tial to 1 for choosing an action that accelerates the car in the direction opp osite its curren t velocity, and 0 otherwise. Follo wing this strategy will cause the agen t to mak e consisten tly faster swings through the valley , reac hing higher positions on slop es before the car's momen tum is exp ended. Note that this poten-tial function dep ends upon the agen t's actions and the car's curren t velocity, but ignores the agen t's position in the world. Also, this strategy is not the fastest way for the agen t to reac h the goal.
 Because the advice mak es suggestions on appropriate actions but not states, we used the look-bac k algorithm to add advice. The agen t's learning algorithm, Q-v alue represen tation and policy remain unaltered with the incorp oration of the advice.
 As can be seen in gure 3, the advice reduces early learning time by 75%. We sho w results with the eli-gibilit y trace deca y rate, , individually optimized for eac h metho d. The remaining parameters are left at their original value. Without advice, a = 0 : 9 yields good results. With advice, however, = 0 : 2 pro vided the best performance. When lam bda is set near one, the in uence of the advice tends to be cancelled by future exp eriences, leaving little impro vemen t over no advice. This e ect can be mitigated by scaling the ad-vice by 1 = (1 ) if this parameter value is available to the advisor. We have presen ted a metho d for incorp orating advice into an arbitrary reinforcemen t learner in a way that preserv es the value of any policy in terms of the origi-nal MDP . Although the advice in itself does not alter the agen t's abilit y to learn a good policy , the learn-ing algorithm and state represen tation the agen t uses must be capable of represen ting a good policy to begin with. Also, it should be stressed that our metho d does not act as a replacemen t for the original rew ards in the MDP . Without environmen tal reinforcemen t, an agen t recieving advice will eventually learn at Q-v alues for every state.
 Although we have assumed no inheren t structures in the reinforcemen t learning agen t, our advising metho d has analogues in man y speci c learning architectures. We have already men tioned the connection between look-ahead advice and Q-v alue initialization. If the agen t uses a linear com bination of features to represen t its Q-v alues, the poten tial function can be incorp o-rated into the feature set. If the poten tial feature had a xed weigh t of 1 throughout learning, it would be an exact emulation of look-ahead advice. Schemes where advice is built directly into the agen t's Q-v alue appro x-imator have been prop osed in Bertsek as and Tsitsiklis (1996); Maclin and Sha vlik (1996).
 Acknowledgemen ts We would like to ackno wledge supp ort for this researc h from Matsushita Electric Industrial Co., Ltd. and helpful commen ts from GUR U.
 Bertsek as, D. P., &amp; Tsitsiklis, J. N. (1996). Neur o-dynamic programming . Athena Scien ti c.
 Hailu, G., &amp; Sommer, G. (1999). On amoun t and qualit y of bias in reinforcemen t learning. IEEE In-ternational Confer enc e on Systems, Man and Cy-bernetics .
 Maclin, R., &amp; Sha vlik, J. W. (1996). Creating advice-taking reinforcemen t learners. Machine Learning , 22 , 251{281.
 Malak, R. J., &amp; Kholsa, P. K. (2001). A framew ork for the adaptiv e transfer of rob ot skill kno wledge among reinforcemen t learning agen ts. Robotic Automation, IEEE International Confer enc e .
 Mataric, M. J. (1994). Rew ard functions for acceler-ated learning. Machine Learning, Proceedings of the Ninth International Confer enc e . Morgan Kaufmann. Ng, A. Y., Harada, D., &amp; Russell, S. (1999). Policy in-variance under rew ard transformations: theory and application to rew ard shaping. Machine Learning,
Proceedings of the Sixte enth International Confer-enc e . Bled, Slovenia: Morgan Kaufmann.
 Perkins, T., &amp; Barto, A. (2001). Lyapuno v design for safe reinforcemen t learning con trol. Machine Learn-ing, Proceedings of the Sixte enth International Con-ferenc e . Morgan Kaufmann.
 Price, B., &amp; Boutilier, C. (1999). Implicit imitation in multiagen t reinforcemen t learning. Machine Learn-ing, Proceedings of the Sixte enth International Con-ferenc e . Bled, Slovenia: Morgan Kaufmann.
 Randlv, J., &amp; Alsrm, P. (1998). Learning to ride a bicycle using reinforcemen t learning and shaping.
Machine Learning, Proceedings of the Fifte enth In-ternational Confer enc e . Morgan Kaufmann.
 Sutton, R. S., &amp; Barto, A. G. (1998). Reinfor cement
