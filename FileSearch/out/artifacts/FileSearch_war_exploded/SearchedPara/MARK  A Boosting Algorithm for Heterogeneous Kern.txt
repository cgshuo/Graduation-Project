
Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference. Practi-cal issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadrat-ically with the data. Inspired by ensemble and boosting methods like MART, we propose the Multiple Additive Re-gression Kernels (MARK) algorithm to address these issues. 
MARK considers a large (potentially infinite) library of ker-nel matrices formed by different kernel functions and param-eters. Using gradient boosting/column generation, MARK constructs columns of the heterogeneous kernel matrix (the base hypotheses) on the fly and then adds them into the kernel ensemble. Regularization methods such as used in 
SVM, kernel ridge regression, and MART, are used to pre-vent overfitting. We investigate how MARK is applied to heterogeneous kernel ridge regression. The resulting algo-rithm is simple to implement and efficient. Kernel parame-ter selection is handled within MARK. Sampling and "weak" kernels are used to further enhance the computational effi-ciency of the resulting additive algorithm. The user can in-corporate and potentially extract domain knowledge by re-stricting the kernel library to interpretable kernels. MARK compares very favorably with SVM and kernel ridge regres-sion on several benchmark datasets. 
Support Vector Machines (SVMs) and other Kernel Meth-ods have proven to be very effective inference tools for many applications [20]. By introducing kernels, many linear meth-ods for classification, regression and unsupervised learning can be transformed into nonlinear methods [18]. For each application, an appropriate kernel and any associated par bear this notice and the full citation on the first permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. to families of "weak kernels" that would not be adequate for an algorithm using a single kernel. Computational results demonstrate that MARK efficiently produces sparse hetero-geneous kernel models based on simple interpretable weak kernels that generalize as well, and require dramatically less storage and testing time than baseline kernel methods pro-duced using a single kernel. 
This paper is organized as follows. In Section 2, we ex-amine the heterogeneous kernel model. A regularized loss function is minimized to construct a model. We examine the gradient of this loss function and discuss coordinate descent strategies for optimizing the model. In Section, 3 we demon-strate how MARKing can be applied to least squares kernel models for regression which are known by various names including least squares SVM [19] and Kernel Ridge Regres-sion (KRR) [17]. We review the basic KRR method. Then propose two different approaches for heterogeneous KRR, 
MARK-L and MARK-S. In Section 4, we introduce the idea of a "weak kernel" functions. In Section 5, the computa-tional costs of MARK and strategies for improving it are examined. Computational results are also provided in Sec-tion 5. We conclude with a discussion of the approach and future research issues. 
We define the heterogeneous kernel model (HKM) prob-lem as follows. Giving a training data set consisting of 
N M-dimensional points x, with associated output y,, we would like to construct a regression function F that min-imizes some convex loss function L(y, F) on the training data. To prevent overfitting the function F is regularized by the convex function P(F). The regularization function 
P(F) helps to constrain the possible set of regression func-tions F considered. Typically P(F) is chosen such that some norm of the parameters of F is minimized. Thus our goal is to find the solution of where C _&gt; 0 is a fixed parameter. With the choice of ap-propriate loss functions, these general models are applied to both classification and regression problems, but we focus on regression in this paper. 
For a single kernel problem like SVM, F is based on a single kernel K and would be defined as 
For example, K could be a radial basis function kernel with a fixed parameter a: where ]]  X  t] is the Euclidean or 2-norm. For HKM, the func-tion F is composed of a linear combination of heterogeneous kernel functions K1,... , Kq 
The kernels can be of any type and they need not all be of the same type. For example, each Kq could be a RBF kernel with a different parameter aq. 
In this section, we assume a least square error loss function for regression with 2-norm regulaxization. Without kernels, such a problem is usually called ridge regression [9]. With kernels, we will call the problem kernel ridge regression al-though it is sometime referred to by other names [4]. For a single kernel function K1, we can define the kernel matrix (K1)ij = Kl(xi, xj), so for N training points, Ki is a N x N matrix. The kernel ridge regression problem is then wherec~ E R ~v, bE R, C &gt; 0isaconstant and eisaN dimensional vector of ones. 
Kernel regression is very attractive because optimization problem (8) can be solved by simply solving a system of equalities. Since the problem is an unconstrained convex optimization problem, the necessary and sufficient condition for optimality is VH(c~, b) = 0. Thus by doing an analysis such as in [8], the solutions satisfies where G ~-[K e]. 
If (&amp;, b) solve equation (9), then the final regression model is 
Unlike classic SVM or least squares model with LASSO type regularization, this optimization solution can be found by simply solving a system of equations, without introduc-ing an optimization package. The catch is that the resulting alternative objective functions such as in SVM, only rela-tively few multipliers for the kernel columns, the support vectors, will be nonzero. KRR has the advantage of sim-ple implementations and relatively fast training times. The method can be applied to any type of kernel function. There axe no limitations on the kernel matrix K. In SVM, the ker-nel matrix is assumed to be a square positive definite matrix. Typically the choice of kernels is limited to those that obey 
Mercer's Condition since this ensures that K is positive def-inite (see [20]). In the approach studied here, the matrix 
K need not be positive definite or square. Kernel functions that do not satisfy Mercer's condition may be utilized. But the disadvantage of KRR without MARKING is that all the training data has to be stored in order to test new data. 
The flexibility in the choice of K in KRR allows the model to be easily extended to Heterogeneous kernel ridge regres-sion (HKRR), but the resulting optimization problem is more challenging. Define the heterogeneous kernel matrix K as the concatenation of Q kernel matrices Kq E R g  X  N, 
K = [Ki,... ,KQ] E R N X QN. The above definition of the kernel ridge regression model (8) then applies to HKMs but now o~ E R QN. 
If we treat each column of K as a hypothesis in an ensem-ble method, we can use ensemble algorithms to generate a small set of kernel columns/hypotheses: that approximately solve the model. If we denote the i th column of K as K~ The kernel Kj is weak in the sense that if used in a regular 
SVM, it would not typically be sufficient for solving a prob-lem alone. It is only as part of a HKM that such choice of kernels can be used. The benefit of using HKM composed of weak RBF kernels is that the kernels selected provide more interpretation than in the case of regular RBF kernels. A weak RBF kernel models inherently performs feature selec-tion. Only a subset of the attributes will be used in the final model, and this is valuable information to the domain experts. In addition each weak RBF kernel has a somewhat clear meaning. For example if the attribute j is size, the weak RBF kernel can be interpreted as identifying if the size is about that of instance x~. Weak RBF kernels are only one possible type of weak kernel. The domain experts can introduce new kernels that make sense in a particular domain. Since there are no restriction on the matrix K in 
MARK-L and MARK-S, these kernels need not obey Mer-cer's condition (see Section 3.) The only important thing is that one can define efficient algorithms to generate them. 
For regular RBF kernels we use the following column gen-eration algorithm. With an exhaustive search, the RBF col-umn generation can be computationally expensive. To ob-tain better scalability, we try sampling uniformly from the data matrix X and choose the best kernel column in the sample. 
For weak regular RBF kernels we use the following column generation algorithm: 
There are many benefits to using weak RBF kernels. As discussed above, by analyzing the final weak kernel model, we can investigate the importance and influence of attributes by looking at the kernel columns added to the model and their associated weights. This leads to a way of non-linear feature selection. Another advantage of weak RBFs is that the number of parameters associated with a weak RBF ker-nel is dramatically reduced from M+i for regular RBF ker-nels to 2 for weak RBF. Thus the storage and computational time required for testing a HKM based on weak RBF ker-nels can be dramatically reduced from that of HKM based on regular kernels. The computational costs of regular RBF and weak RBF column generation are both dependent on the choice of the sample size N'. The experimental results in the next section shows how efficient sampling can be in practice as well as comparison between regular and weak 
RBF kernels. running SVM or KRR. So one reason for the advantage of 
MARK-L may be that it inherently selects feature through the use of the weak RBF kernels. KRR used 63 support vec-tors since it is not a sparse method and there are 63 points in each training set. SVM requires only 27 full RBF ker-nels. MARK-L is set to always use 100 weak RBF kernels. 
The total number of parameters used by MARK-L can be found by the following argument; For each weak kernel we need to store two parameters, namely the center of kernel and the width a, and the associated a~. In addition For T weak kernels, therefore, the number of elements we store is 3T + 1 = 300 + 1 (the 1 is for the threshold b). However, for SVMs, we cannot throw away any attributes of a sup-port vector and only one parameter a is required therefore, the total number of information that has to be recorded is ((# support vectors)xM + 2. In this case, 3T -t-1 = 301 and (# support vectors) x M + 2 = 6304 (average). There-fore, the MARK model requires significantly less memory and is in some sense a simpler model. Specificaly, the ra-tio of storage required by SVM to MARK is 20.9. With a dataset like this, dimension reduction is necessary to obtain a good model. MARK with weak RBF kernels automati-cally performs feature selection, which is a key feature for success oa QSAR type problems. In addition the reduction in model complexity produces an even greater reduction in computational costs in the testing phase. 
Table 1: Leave-one-out test results for HIV. SV is the average number of support vectors or kernel columns used by each methods. Storage is the ratio of model storage required for the indicated method versus MARK-L 
Ten-fold cross validation results for Boston and Abalone are shown in Table 2 and 3. MARK performed comparably to SVMTorch on both datasets. KRR performs uniformly worse. Tuning of MARK parameters may further improve results. Pattern Search methodologies for tuning MARK are currently being tested. Moreover, as the size of dataset grows, storage requirements of the prediction model, in gen-eral, grows. The computational cost and storage requirer ments for reproducing the model for prediction unknown test data cannot be ignored. In SVM, if the number of support vector is S, the computational cost to produce a prediction is O(M  X  S). In MARK with weak kernels, it is just O(1  X  T). This advantage cannot be ignored when dealing with a large dataset. On Abalone, a predictive SVM model requires 1375  X  9 + 2 units of information (4 bytes for a double precision variable). The factor '9' accounts for the number of attributes. Addition of '2' accounts for threshold b and the kernel parameter a. For MARK, it is 100  X  3 + 1 where '3' accounts for c~ and a for each center of the col-umn kernel, '1' accounts for the threshold b. The ratio is ~-= 41. Therefore, in this example, the memory requirement for MARK is about 41 times less than that for 
SVM. There would also be a corresponding decrease in clas-sification costs. Similar ratios are provided for all the results 
