 The low-rank regression model has been studied and ap-plied to capture the underlying classes/tasks correlation pat-terns, such that the regression/classification results can be enhanced. In this paper, we will prove that the low-rank regression model is equivalent to doing linear regression in the linear discriminant analysis (LDA) subspace. Our new theory reveals the learning mechanism of low-rank regres-sion, and shows that the low-rank structures exacted from classes/tasks are connected to the LDA projection results. Thus, the low-rank regression efficiently works for the high-dimensional data.

Moreover, we will propose new discriminant low-rank ridge regression and sparse low-rank regression methods. Both of them are equivalent to doing regularized regression in the regularized LDA subspace. These new regularized objectives provide better data mining results than existing low-rank re-gression in both theoretical and empirical validations. We evaluate our discriminant low-rank regression methods by six benchmark datasets. In all empirical results, our dis-criminant low-rank models consistently show better results than the corresponding full-rank methods.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms Low-Rank Regression, Low-Rank Ridge Regression, Sparse Low-Rank Regression, Linear Discriminant Analysis
As one of most important data mining and machine learn-ing technique, multivariate linear regression attempts to model the relationship between predictors and responses by fitting a linear equation to observed data. Such linear regression models suffer from two deficiencies when they are applied to the real-world applications. First, the linear regression models usually have low performance for analyzing the high-dimensional data. In many data mining and machine learn-ing applications, such as gene expression, document classifi-cation, face recognition, the input data have a large number of features. To perform accurate regression or classification tasks on such data, we have to collect an enormous number of samples. However, due to the data and label collection difficulty, we often cannot obtain enough samples and suf-fer from the curse-of-dimensionality problem [8]. To solve this problem, the dimensionality reduction methods, such as linear discriminant analysis (LDA) [10], were often used to reduce the feature dimensionality first.

Second, the linear regression models don X  X  emphasize the correlations among different responses. Standard least squares regression is equivalent to regressing each response on the predictors separately. To incorporate the response ( i.e. classes or tasks) correlations into the regression model, Anderson introduced the reduced rank regression method [1], which is a multivariate regression model with a coefficient matrix with reduced rank. Later many researchers worked on the low-rank (or reduced) regression models [26, 5, 13, 1, 2, 20], in which the classes/tasks correlation patterns are explored by the low-rank structure and utilized to enhance the re-gression/classification results.

In this paper, we propose new and important theoretical foundations of the low-rank regression. We first present the discriminant low-rank linear regression, which reformulates the standard low-rank regression to a more interpretable objective. After that, we prove that the low-rank regres-sion model is indeed equivalent to doing linear regression in the LDA subspace, i.e. the learned low-rank classes/tasks correlation patterns are connected to the LDA projection results. Our new theorem explains the underlying compu-tational mechanism of low-rank regression, which performs the LDA projection and the linear regression on data points simultaneously. In our special case, when the low-rank re-gression coefficient matrix becomes a full-rank matrix, our result is connected to Ye X  X  work on the equivalence between the multivariate linear regression and LDA [27].

Motivated by our new theoretical analysis, we propose two new discriminant low-rank regression models, including low-rank ridge regression (LRRR) and sparse low-rank regression (SLRR). Both methods are equivalent to performing the reg-ularized regression tasks in the regularized LDA subspace (two methods have different regularization terms). Because the regularization term avoids the rank deficiency problem in both regression and LDA, our LRRR method outperforms the low-rank regression in both theoretical analysis and ex-perimental results. Using the structured sparsity-inducing norm based regularization term, our SLRR method can ex-plore both classes/tasks correlations and feature structures. All our new discriminant low-rank regression models can si-multaneously analyze the high-dimensional data in the dis-criminant subspace without any pre-processing step and in-corporate the classes/tasks correlations. We evaluate the proposed methods on six benchmark data sets. In all ex-perimental results, our discriminant low-rank models con-sistently outperform their corresponding full-rank counter-parts.
 Notations. In this paper, matrices are written as uppercase letters and vectors are written as bold lowercase letters. For matrix W = { w ij } ,its i -th row, j -th column are denoted as w , w j respectively. Tr ( W ) means the trace operation for matrix W and || W ||  X  means the trace norm of matrix W .
One of the main result of this paper is to prove that the low-rank linear regression (LRLR) is equivalent to do-ing standard linear regression in LDA subspace (we call this as  X  X DA+LR X ) .
Traditional Linear Regression model for classification is to solve the following problem: where X =[ x 1 , x 2 , ...., x n ]  X  d  X  n is the centered training data matrix and Y  X  n  X  k is the normalized class indicator matrix, i.e. Y i,j =1 / the j -th class and Y i,j = 0 otherwise and n j is the sample size of the j -th class. The model outputs the parameter matrix W  X  d  X  k , which can be used to predict any test data point x  X  d  X  1 by W T x .

When the class or task number is large, there are often underlying correlation structures between classes or tasks. To explore these hidden structures and utilize such patterns to improve the learning model, in recent work [3], researchers presented to learn a low-rank projection W in the regression model by imposing the trace norm regularization as: The trace norm regularization can discover the low-rank structures existing between classes or tasks. Using Eq. (2), the rank of coefficient matrix W , which is decided by the selection of parameter  X  , cannot be explicitly selected and tuned.

In related research work, the low-rank regression was stud-ied in statistics and machine learning communities [26, 5, 13, 1, 2, 20]. In the low-rank regression, the rank of W is explicitly decided by constraining the rank of W to be s&lt;min ( n, k ) and solving the following problem: Because the rank of coefficient matrix can be explicitly de-termined, the low-rank regression in Eq. (3) is better than the trace norm based objective in Eq. (2) in practical appli-cations. Although the general rank minimization is a non-convex and NP-hard problem, the objectives with rank con-straints are solvable, e.g. the global solution was given in [26, 5].
In this section, we will show that the low-rank linear re-gression (LRLR) is equivalent to perform Linear Discrimi-nant Analysis (LDA) and linear regression simultaneously (LDA+LR). In other words, the learned low-rank structures and patterns are induced by the LDA projection (with re-gression). The low rank s is indeed the projection dimension of LDA.

Before introducing our main theorems, we first propose the following discriminant Low-Rank Linear Regression for-mulation (LRLR): where A  X  d  X  s , B  X  s  X  k , s&lt;min ( n, k ). Thus W = AB has low-rank s . The above LRLR objective has the same solutions as Eq. (3), but it has clearer discriminant projection interpretation. Eq. (4) can be written as This shows A can be viewed as a projection. Interestingly as we show in Theorem 1, A is exactly the optimal subspace defined by the classic LDA.
 Theorem 1. The low-rank linear regression method of Eq. (4)) is identical to doing standard linear regression in LDA subspace.
 Proof: Denoting J 1 ( A, B )= || Y  X  X T AB || 2 F and taking its derivative w.r.t. B ,wehave, Setting Eq. (6) to zero, we obtain, Substituting Eq. (7) back into Eq. (4), we have, which is equivalent to Note that where S t and S b are the total-class scatter matrix and the between-class scatter matrix defined in the LDA, respec-tively. Therefore, the solution of Eq. (9) can be written as: which is exactly the problem of LDA, and the global opti-mal solution to Eq. (11) is the top s eigenvectors of S  X  1 corresponding to the nonzero eigenvalues (if S t is singular, we compute the eigenvectors of S + t S b corresponding to the nonzero eigenvalues, where S + t denotes the pseudo-inverse of S ). Now Eq. (5) implies that we do linear regression on the projected data X = A T B .Since A is the LDA projection, thus Eq. (5) implies we do regression on the LDA subspace.
Note that in Eq. (4), the class indicator matrix Y is nor-malized, but not centered. However X is centered. The following Theorem 2 shows that we obtain the optimal so-lution whatever Y is centered or not.

Theorem 2. The optimal solution ( A  X  ,B  X  ) for the fol-lowing problem is identical to those of Eq. (4); here P = I  X  ee T /n  X  n  X  n is the centering matrix, and e =(1  X  X  X  1) T .

For this reason, the bias (intercept) term are already au-tomatically incorporated in Eq. (4).
 Proof: The key point of the proof is the fact that in the solution for both B and A of Eq. (7) and Eq. (9), Y always appears together with X as combination because X is centered and P 2 = P .Inotherwords,aslong as X is centered, Y is automatically centered.

This results can be easily extended to the standard linear regression. In fact we have
Remark 1. As long as X is centered, the optimal solu-tion W  X  for the standard linear regression of Eq.(1) remains identical no matter Y is centered or not.

Our new results provide the theoretical foundation to ex-plain the mechanism behind the low-rank regression meth-ods. Meanwhile, the above proof process also indicates a concise algorithm to achieve the global solution of LRLR in Eq. (4), as well as Eq. (3). The Algorithm to solve Eq. (4) issummarizedinAlg.1.

Moreover, we note that Theorem 1 also provides clari-fication to a long-standing puzzle in multi-class LDA, as explained below.
The original Fisher LDA is on 2-class problem, where only k  X  1 = 1 projection direction a is needed. The Fisher objective is The generalization to multi-class has two natural formula-tions [10], either the trace-of-ratio formulation Algorithm 1 The algorithm to solve LRLR
Input: 1. The centralized training data X  X  d  X  n . 2. The normalized training indicator matrix Y  X  n  X  k . 3. The low-rank parameter s .

Output: 1. The matrices A  X  d  X  s and B  X  s  X  k .

Process: 1. Calculate A by Eq. (11) 2. Calculate B by Eq. (7) where A =( a 1  X  X  X  a k  X  1 ), or the ratio-of-trace formulation Our Theorem 1 lends support to the trace-of-ratio objective function because this formulation arises directly from the linear regression.
Here we note an important connection. In the special case, the low-rank regression coefficient matrix W becomes a full-rank matrix. Without loss of generality we assume s = k  X  n , because the number of data points n is usually larger than the number of classes k .Thematrix B  X  k  X  k becomes a square matrix. Because rank ( W )= rank ( AB )= k and k  X  n , rank ( A )  X  k and rank ( B )  X  k .Thus, rank ( B )= k and B is a full rank matrix, i.e. the matrix B is invertible.
The Theorem 1 is still correct for the special case. More-over, we can further conclude the equivalence between the multivariate linear regression and LDA results. We can sim-ply prove this conclusion. Because the matrix A includes the LDA subspaces and the matrix B canbeconsideredas an invertible rotational matrix, thus AB is also one of the infinite number global solutions of LDA [15]. Thus, in the special full-rank case, the multivariate linear regression is equivalent to the LDA result, which was shown in Ye X  X  work [27] with the assumptions: the reduced dimension is k  X  1 and rank ( S b )+ rank ( S w )= rank ( S t ). Our proof is more general and doesn X  X  need the rank assumption.
As we know, by adding a Frobenius norm based regulariza-tion on the linear regression loss, ridge regression can achieve better performance than linear regression [12]. Thus, it is important and necessary to add the ridge regularization into low-rank regression formulation. We propose the following Low-Rank Ridge Regression (LRRR) objective as, where A  X  d  X  s , B  X  s  X  k , s&lt;min ( n, k ),  X  is the regu-larization parameter. Sim ilarly, we can see that the LRRR objective is equivalent to the following objective: Compared to Eq. (16), Eq. (15) provides better chance for us to understand the learning mechanism of LRRR. We will In Eqs.(13,14), the optimal solution remains the same when S w is replaced by S t . Algorithm 2 The algorithm to LRRR
Input: 1. The centralized training data X  X  d  X  n . 2. The normalized training indicator matrix Y  X  n  X  k . 3. The low-rank parameter s . 4. The regularization parameter  X  .

Output: 1. The matrices A  X  d  X  s and B  X  s  X  k .

Process: 1. Calculate A by Eq. (21) 2. Calculate B by Eq. (18) show that our new LRRR objective is connected to the reg-ularized discriminant analysis, which provides better projec-tion results than the standard LDA. We will also derive the global solution of the non-convex problems in Eq. (15) and Eq. (16).

Theorem 3. The proposed Low-Rank Ridge Regression (LRRR) method (both Eq. (15) and Eq. (16)) is equivalent to doing the regularized regression in the regularized LDA subspace.
 Proof: Denoting J 2 ( A, B )= || Y  X  X T AB || 2 F +  X  || and taking its derivative w.r.t. B ,wehave,  X  X  2 ( A, B ) Setting Eq. (17) to zero, we get, where I  X  d  X  d is the identity matrix. Substituting Eq. (18) back into Eq. (15), we have which is equivalent to the following problem: Similarly, the solution of Eq. (20) can be written as: which is exactly the problem in regularized LDA [9]. After we get the optimal solution A , we can re-write Eq. (15) as: which is the regularized regression, and the optimal solu-tion is given by Eq. (18). Thus, the LRRR of Eq. (15) is equivalent to performing ridge regression in regularized-LDA subspace.

Similar to Theorem 2, we can show that Y is automatically centered as long as X is centered.

Another interest point is that although our LRRR model is a non-convex problem, Theorems 1 and 3 show that they have the global optimal solutions. The Algorithm to solve LRRR of Eq. (15) is described in Alg. 2.
In the special case, the low-rank regression coefficient ma-trix W becomes a full-rank matrix. Similar to  X  2.4, we have the following lemma:
Lemma 1. The full-rank ridge regression result is equiva-lent to the solution of regularized LDA ( S t is replaced by the regularized form S t +  X I ).
 Similar to the proof in  X  2.4, we can easily prove the coef-ficient matrix W in full-rank ridge regression is one of the global solutions of LDA regularized by  X I .
Besides exploring and utilizing the class/task correlations and structure information, the learning models also prefer to select and use the important features to avoid the  X  X urse of dimensionality X  problem in high-dimensional data analysis. Thus, it is important to extend our discriminant low-rank regression formulations to feature selection models.
Due to the intrinsic properties of real world data, the structured sparse learning models have shown superior fea-ture selection results in previous research [22, 28, 21, 17, 6, 23, 25, 24, 7]. One of the most effective ways for selecting features is to impose sparsity by inducing hybrid structured 1 -norm on the coefficient matrix W as the regularization term [19, 3]. Therefore, following our LRLR and LRRR methods, we propose a new Sparse Low-Rank Regres-sion (SLRR) method, which reserves the low-rank con-straint and adds the mixed 2 , 1 -norm regularization term to induce both desired low-rank structure of classes/tasks cor-relations and structured sparsity between features. To be specific,  X  X ow-rank X  means rank ( AB )= s&lt;min ( n, k )and  X  X tructured sparsity X  means most rows of AB are zero to help feature selection. Thus, we solve: where A  X  d  X  s , B  X  s  X  k , s&lt;min ( n, k ). Similarly, we can see that the above SLRR objective is equivalent to the following objective: Both Eq. (23) and Eq. (24) are new objectives to simultane-ously learn low-rank classes correlation patterns and features structured sparsity.
Interestingly our new SLRR method also connects to the regularized discriminant analysis by the following theorem.
Theorem 4. The optimal solution of the proposed SLRR method (Eq. (23) and Eq. (24)) has the same column space of a special regularized LDA.
 Proof: Eq. (23) is equivalent to the following problem, where D  X  d  X  d is a diagonal matrix and each element on the diagonal is defined as follows: Algorithm 3 The algorithm to SLRR
Input: 1. The centralized training data X  X  d  X  n . 2. The normalized training indicator matrix Y  X  n  X  k . 3. The low-rank parameter s . 4. The regularization parameter  X  .

Output: 1. The matrices A  X  d  X  s and B  X  s  X  k .

Initialization: 1. Set t =0 2. Initialize D ( t ) = I  X  d  X  d .

Repeat: 1. Calculate A ( t +1) by Eq. (30) 2. Calculate B ( t +1) by Eq. (28) 3. Update the diagonal matrix D ( t +1)  X  d  X  d ,wherethe i -th diagonal element is 1 2 || ( 4. Update t = t +1
Until Converge . where g i is the i -th row of matrix G = A  X  B  X  .Denoting J ( A, B )= || Y  X  X T AB || 2 F +  X  Tr ( B T A T DAB ) and taking its derivative w.r.t. B ,wehave,  X  X  3 ( A, B ) Setting the above equation to be zero, we can get, where D  X  d  X  d is the diagonal matrix defined in Eq. (26). Substituting Eq. (28) back into Eq. (25), then we need solve the following problem to get A , The solution of Eq. (29) is: Since the column space of W  X  = A  X  B  X  is identical to the column space of A  X  , the proposed SLRR has the same col-umn space of a special regularized LDA ( S t is replaced with S +  X D ).

After we get the optimal solution A , we can solve Eq. (23) through Eq. (25), which is the regularized regression prob-lem. Again, similar to Theorm 2, we can prove that if Y is centered or not will not affect the learnt model A  X  and B
Solving SLRR objective in Eq. (23) is nontrivial, there are two variables A and B needed to be optimized, and the non-smooth regularization also makes the problem more difficult to solve. Interestingly, a concise algorithm can be derived to solve this problem based on the above proof. The detailed algorithm is described in Algorithm 3. In next subsection, we will prove that the algorithm converges. Our experimen-tal results show that the algorithm always converges in 5-20 iterations.
Because Alg. 3 is an iterative algorithm, we will prove its convergence.
 Theorem 5. Alg. 3 decreases the objective function of Eq. (23) monotonically.
 Proof: In the t -th iteration, we have In other words, definition of matrix D in the algorithm, Eq. (32) can be rewritten as, where g i ( t ) and g i ( t +1) are the i -th row of the matrix G and G ( t +1) respectively. Since for each i ,wehave Thus, summing up d inequalities and multiplying the sum-mation with the regularization parameter  X  , we obtain: Combining Eq. (33) and Eq. (35), we get: Therefore, we have: Since A and B are updated according to gradient, Alg. 3 will monotonically decrease the objective in Eq. (23) in each iteration.
In the special case, the low-rank regression coefficient ma-trix W becomes a full-rank matrix. Similar to  X  2.4, we also have the following lemma:
Lemma 2. The optimal solution of the full-rank sparse linear regression is one of the global solutions of LDA regu-larized by  X D .
 Similar to the proof in  X  2.4, we can easily prove the coeffi-cient matrix W in full-rank sparse linear regression is one of the global solutions of LDA regularized by  X D .
In this section, we will evaluate the performance of our proposed LRLR, LRRR, SLRR with their corresponding full-rank counterparts. We firstly introduce the six bench-mark datasets used in our experiments.
UMIST face dataset [11] contains 20 persons and totally 575 images. All images are cropped and resized into 112  X  pixels per image.

Binary Alphadigits 36 dataset [4] contains binary digits of 0 through 9 and capital A through Z with size 20  X  16. There are 39 examples of each class.

Binary Alphadigits 26 dataset [4] contains binary cap-ital A through Z with size 20  X  16. There are 39 examples of each class.

VOWEL dataset [18] consists of 990 vowel recognition data used for the study of recognition of the eleven steady state vowels of British English. The speakers are indexed by integers 0-89. (Actually, there are fifteen individual speak-ers, each saying each vowel six times.) The vowels are in-dexed by integers 0-10. For each utterance, there are ten floating-point input values, with array indices 0-9.
MNIST hand-written digits dataset [14] consists of 60 , 000 training and 10 , 000 testing digits. It has 10 classes, from digit 0 to 9. Each image is centralized (according to the center of mass of the pixel intensities) on a 28  X  28 grid. We randomly select 15 images for each class in our experiment.
JApanese Female Facial Expressions (JAFFE) data set [16] contains 213 photos of 10 Japanese female models. Each image has been rated on 6 emotion adjectives by 60 Japanese subjects.

We summarize the datasets that we will use in our exper-iments in Table 1
All the datasets in our experiments have large number of classes (at least 10 classes). For each dataset, we ran-domly split the data into 5 parts. According to the stan-dard 5-fold cross validation, in each round, we use 4 parts for training and the remaining part for testing. The aver-age classification accuracy rates for different methods are reported. In the training stage, we use different full-rank linear regression models, i.e. full-rank linear regression, full-rank ridge regression, sparse full-rank linear regression to learn the coefficient matrix W directly or we solve the proposed low-rank counterparts (LRLR, LRRR, SLRR) to calculate W indirectly by W = AB .Inallexperiments, we automatically tune the regularization parameters by se-lecting the best parameters among the values { 10 r : r  X  { X  5 ,  X  4 ,  X  3 , ... 3 , 4 , 5 }} with 5-fold cross validation on the corresponding training data only. In addition, for LRLR, LRRR, SLRR, we calculate the classification results with respect to different low-rank parameters s in the range of [ k/ 2 ,k ), where k is the number of classes. At last, in the testing stage, we utilize the following decision function to classify the coming testing data x t  X  d  X  1 into one and only one out of k classes, Please note that all the data are centered and we consider the model without bias. The code is written in MATLAB Table 1: The summary of the datasets used in our experiments. k is the number of classes, d is the number of feature dimensions, n is the number of data points.
 and we terminate our iterative optimization procedure of sparse regression when the relative change in the objective function is below 10  X  5 .
Our proposed methods can find the low-rank structure of the regression models, which are equivalent to doing re-gression in the regularized LDA subspace. For illustration purpose, in Fig. 1 we plot the ranked singular value of the learnt coefficient matrix W = AB on the left hand side and draw the absolute value of the learnt W of the 1st fold (of the 5 fold cross validation, other folds show similar result) on the right hand side for each dataset. The corresponding rank parameter is selected based on which SLRR achieves the best classification accuracy. For example, in Fig. 1(a) shows the UMIST results, we can see the number of non-zero singular value of W is 15, i.e., the rank of the learnt coefficient matrix is 15, less than its full rank value of 20. In addition, the learnt W is sparse and is effectively used for feature selection, e.g. selecting the important features (non-zero rows) across all the classes.

Fig. 2 shows the average classification accuracy compar-isons of the above three types of full-rank regressions with the proposed low-rank counterparts with respect to differ-ent low-rank constraints. From Fig. 2, we can obviously con-clude that the discriminant low-rank regressions consistently outperform their full-rank counterparts, when the specified low-rank parameter s falls in a proper range. For five out of six datasets in our experiments, the low-rank property can boost the result greatly. Only in JAFFE dataset (as shown in Fig. 2.(l)), the performance of sparse low-rank regression is competitive with that of the full-rank counterpart.
To help the researchers easily compare all methods, we also list the best classification results in terms of average ac-curacy and standard deviation for different regression meth-ods in Table 2.

Our experimental results also verify our previous key point that the RLRR method is better than LRLR method. On all six datasets, the RLRR outperforms the LRLR. Surpris-ingly, the standard ridge regression even has better perfor-mance than the LRLR method. The LRLR is equivalent to existing low-rank regression models, and both methods may have suboptimal results due to the rank deficiency problem. In standard ridge regression or RLRR methods, because the rank constraint is imposed, both of them alleviate such ma-trix rank deficiency issue. Now we showed the connection between low-rank constraint and LDA projection, such that we can uncover this problem. (b) VOWEL low-rank structure and sparse structure (d) JAFFE low-rank structure and sparse structure (e) BINALPHA36 low-rank str ucture and sparse structure (f) BINALPHA26 low-rank structure and sparse structure
For some data with very large feature dimension ( d&gt;&gt;n ), like UMIST, MNIST and JAFFE, feature selection is nec-essary to reduce the redundancy between features and alle-viate the curse of dimensionality. Our classification results both in Fig. 2 and Table 2 have shown that under such cir-cumstances, SLRR and its full rank counterpart can achieve better classification result than RLRR and ridge regression since the 2 , 1 -norm can impose sparsity and select the fea-tures for all the classes.

Thus, our newly proposed RLRR as well as SLRR meth-ods are more important and more practical low-rank models for machine learning applications.
In this paper, we provide theoretical analysis on low-rank regression models. We proved that the low-rank regression is equivalent to doing linear regression in the LDA subspace. More important, we proposed two new discriminant low-rank ridge regression and sparse low-rank regression meth-ods. Both of them are equivalent to doing regularized regres-sion in the regularized LDA subspace. From both theoret-ical and empirical views, we showed that both LRRR and SLRR methods provide better learning results than stan-dard low-rank regression. Extensive experiments have been conducted on six benchmark datasets to demonstrate that our proposed low-rank regression methods consistently out-perform their corresponding full-rank counterparts in terms of average classification accuracy. Corresponding Author: Heng Huang (heng@uta.edu).
 This research was partially supported by NSF-IIS 1117965, NSF-CCF 0830780, NSF-DMS 0915228, NSF-CCF 0917274. [1] T. Anderson. Estimating linear restrictions on [2] T. Anderson. Asymptotic distribution of the reduced [3] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [4] P. Belhumeur, J. Hespanha, and D. Kriegman.
 [5] F. Bunea, Y. She, and M. Wegkamp. Optimal [6] X. Cai, F. Nie, H. Huang, and C. H. Q. Ding.
 [7] C.H.Q.Ding,D.Zhou,X.He,andH.Zha. R 1 -pca: [8] D. Donoho. High-dimensional data analysis: The [9] J. Friedman. Regularized discriminant analysis. [10] K. Fukunaga. Introduction to statistical pattern [11] D. Graham and N. Allinson. Characterising virtual [12] A. Hoerl and R. Kennard. Ridge regression: Biased [13] A. Izenman. Reduced-rank regression for the [14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [15] D. Luo, C. Ding, and H. Huang. Linear discriminant [16] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. [17] F. Nie, H. Huang, X. Cai, and C. H. Q. Ding. Efficient [18] M. Niranjan and F. Fallside. Neural networks and [19] G. Obozinski, B. Taskar, and M. Jordan. Multi-task [20] G. Reinsel and R. Velu. Multivariate reduced-rank [21] L. Sun, R. Patel, J. Liu, K. Chen, T. Wu, J. Li, [22] R. Tibshirani. Regression shrinkage and selection via [23] H. Wang, F. Nie, H. Huang, S. L. Risacher, C. Ding, [24] H.Wang,F.Nie,H.Huang,S.L.Risacher,A.J.
 [25] H.Wang,F.Nie,H.Huang,J.Yan,S.Kim, [26] S. Xiang, Y. Zhu, X. Shen, and J. Ye. Optimal exact [27] J. Ye. Least squares linear discriminant analysis. In [28] P. Zhao and B. Yu. On model selection consistency of
