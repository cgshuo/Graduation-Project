 Steffen Gr  X unew  X alder steffen@cs.ucl.ac.uk Arthur Gretton  X  arthur.gretton@gmail.com John Shawe-Taylor jst@cs.ucl.ac.uk One of the important ideas that make functional anal-ysis a powerful tool in all branches of mathematics is that basic mathematical operations, like multiplication or composition, may be represented and studied with linear operators. Multiplication fg , for example, is for a fixed f a linear operation in g , and under suitable restrictions, this operation can be described with the help of a bounded linear operator M f , i.e. M f g = fg . The study of such basic operations in reproducing ker-nel Hilbert spaces (RKHSs, Aronszajn (1950); Berlinet &amp; Thomas-Agnan (2004)) suffers from a crucial diffi-culty: these spaces are not closed under many such operations. For example, if we consider an RKHS H X and two functions f,g  X  H X then in most cases fg will not lie in H X . This simple fact has far reaching consequences, both for theoretical and practical prob-lems, as one cannot simply apply basic mathematical operations on functions in the RKHS and expect to ob-tain an RKHS function. In many practical problems, for example, the reproducing property is of major im-portance in keeping computation costs at bay, and to avoid dealing explicitly with high dimensional feature spaces. To each RKHS H X there corresponds an as-sociated reproducing kernel k ( x,y ), and the reproduc-ing property states that f ( x ) =  X  f,k ( x,  X  )  X  k function f from H X . Since the product of two RKHS functions is likely not in H X , however, the reproducing property will not hold for this product.
 Our main contribution is a way to address these diffi-culties by approximating linear operators such as M f with operators F f : H X  X  X  X that map back into the RKHS H X . We will refer to such operators as smooth operators . By smooth we mean in a broad sense RKHS functions with low norm. The intuition is that an RKHS-norm is a measure of smoothness very similar to a Sobolev-norm, which measures (weak) derivatives of functions and calculates the norm based on how large these derivatives are. The operator F f preserves smoothness in this sense, but we also model F f itself as an element of a more complex RKHS.
 This more complex RKHS is one of the key tools in the paper. It is based on a vector-valued kernel func-tion  X  ( f,g ), where f,g  X  X  X . The importance of this kernel is that the corresponding RKHS H  X  consists only of bounded linear operators mapping from H X to a second RKHS H Y (in the case of a product of functions, we have the special case H Y = H X ). This vector-valued kernel is in the simplest case a subset of the Hilbert-Schmidt operators. We will make use of well established vector-valued RKHS tools to approx-imate and estimate operators like M f .
 It turns out that for the intuitive risk functions in many settings, an adjoint trick is useful to make es-timation tractable. Typically, we have an expression of the form ( F h )( x ), where h  X  H Y , and we want to separate h from F (recall that our goal is to estimate F , which is assessed by its action on some test func-tion h evaluated at x ). The trick is simple: as F is a bounded linear operator, there exists an adjoint oper-ator F  X  with which we can transform the term with l being the kernel of H Y ; thus h is separated from F . We prove there exists a natural adjoint kernel  X   X  for  X  such that F  X   X  H  X   X  iff F  X  H  X  . This is important as we gain explicit control over the adjoint and the link between F and F  X  .
 We can view this move to the adjoint operator as trans-forming our learning problem from one of estimating an operator F to that of estimating a mapping into an RKHS, x 7 X  F  X  k ( x,. ) , which can be viewed as a regres-sion problem. We are thus able to obtain F through standard regression techniques. Since this is couched in the general RKHS framework, it can be applied to a very general class of mappings and applications. Our results show that these estimation problems are tractable both algorithmically and statistically. Besides the problem of learning smooth approxima-tions of non-smooth functions, an important appli-cation of smooth operators is in integration theory. Basic integrals of RKHS functions are studied with the help of mean embeddings (Berlinet and Thomas-Agnan, 2004; Smola, Gretton, Song, and Sch  X olkopf, 2007; Sriperumbudur, Gretton, Fukumizu, Lanckriet, and Sch  X olkopf, 2010). These mean embeddings are representer m X  X  X  X of an integral or expectation, in that the expectation over an RKHS function f  X  H X can be efficiently calculated as E f =  X  m X ,f  X  k . In-tegration theory itself is a field rich in sophisticated methods to transform integrals for all sorts of prac-tical problems. We focus here on two such trans-formations: the change of measure rule, and condi-tional expectations. We show these can be approached within the operator framework, and produce sample based estimates for these transformations which do not leave the underlying RKHSs. The covariate shift problem (Huang, Smola, Gretton, Borgwardt, and Sch  X olkopf, 2007; Gretton, Smola, Huang, Schmittfull, Borgwardt, and Sch  X olkopf, 2009; Yu and Szepesvari, 2012) is closely related to the change of measure trans-formation, and our conditional expectation approach follows up on the work of Song, Huang, Smola, and Fukumizu (2009); Gr  X unew  X alder, Lever, Baldassarre, Patterson, Gretton, and Pontil (2012a).
 The Radon-Nikod  X ym theorem often allows us to re-duce a change of measure transformation to a multi-plication: an integral of a function f over a changed measure reduces to an integral of the product f with a Radon-Nikod  X ym derivative r over the original mea-sure. This problem is close to that of learning a mul-tiplication operator M r , however a Radon-Nikod  X ym derivative is almost everywhere positive. Constraints of this form occur often and are difficult to enforce. If we consider the space L 2 with inner product  X  f,g  X  L 2 = R fg , and a multiplication operator M r with r  X  L 2 , then r is a.e. positive when the multiplication opera-tor M r is positive; that is, if  X  M r f,f  X  L 2 = R rf 2  X  for all square integrable f . The important point is that positivity of M r can be enforced by a convex con-straint, illustrating the broader principle that difficult constraints can in certain cases be replaced or approx-imated with convex constraints on the operators. Finally, we consider the problem of combining basic operations to perform more complex operations. Key applications of conditional expectations and changes of measure include the sum rule for marginalising out a random variable in a multivariate distribution (Song, Huang, Smola, and Fukumizu, 2009), and kernel-based approximations to Bayes X  rule for inference without parametric models (Fukumizu, Song, and Gretton, 2011; Song, Fukumizu, and Gretton, 2013). We show that these problems can be addressed naturally with smooth operators. In particular, the development of estimators is considerably simplified: we derive natural estimators for both rules in a few lines, by first trans-forming the relevant integrals and then approximating these transformations with estimated operators. This is a significant shortening of the derivation of an es-timator when performing approximate Bayesian infer-ence, albeit at the expense of a non-vanishing bias. We give a brief overview of the sum rule approach. The task is to estimate the expected value of a function h wrt. a measure Q Y that is unobserved. We observe Q
X , a second measure P X  X  Y , and we know that the conditional measures are equal, i.e. P Y | x = Q Y | x . It is easy to obtain the quantity E Q Y h from these observed measures, via the integral transformations We can approximate the two operations on the right, i.e. the expectations E Q X and E P The advantage of the approach is that the two opera-tors can be composed together, since the approxima-tion of E P Our approach to composition of operators has another advantage: the error of the composite operation is bounded by the errors of the basic operations that are combined. We demonstrate this on the sum rule and on the kernel Bayes X  rule, by bounding the risk of the estimators via the risk of the conditional expectations, means, and approximation errors, which are easily esti-mated. We show in the case of the sum rule that these bounds can yield state-of-the-art convergence rates. The problems that can be addressed with our approach have direct practical application. Besides covariate shift and Bayesian inference as discussed above, ad-ditional applications include spectral methods for in-ference in hidden Markov models, and reinforcement learning (Song et al., 2010; Gr  X unew  X alder et al., 2012b; Nishiyama et al., 2012).
 We like to think that the main text of this paper is readable with a basic knowledge of functional analysis and scalar valued RKHS theory. Obviously, we also use techniques from the vector-valued RKHS litera-ture, however this is kept to a minimum in the main text, and the reader can go a long way with the con-crete form of the kernel  X  from eq. 3, and treat terms of the form k F k  X  by analogy with the scalar case k f k In the supplement, a basic understanding of vector-valued RKHSs is needed. Excellent introductions to this topic are Micchelli and Pontil (2005); Carmeli, De Vito, and Toigo (2006). We begin by introducing a natural risk function and a generic way of minimising it to motivate the approach. We then introduce the operator valued kernel and its adjoint. For the purposes of illustration, we apply this basic approach to the multiplication, composition and quotient (Suppl. A.3) operations. 2.1. A Natural Risk Function Assume we have a linear operator G , acting on func-tions h from an RKHS H Y with kernel l ( y,y 0 ) and mapping to some function space F , which we want to approximate with an operator F  X  X   X  mapping from H
Y to H X . We first need to define in which sense we want to approximate G . A natural choice is to consider the actions of G and F on elements h , and minimise the difference between the two, i.e. to min-imise the error (( F h )( x )  X  ( G h )( x )) 2 . There are two free variables here, x and h . An intuitive choice is now to average the error over x wrt. a suitable mea-sure and to take the supremum over k h k l  X  1 to be robust against the worst case h . The corresponding risk function, which we call the natural risk, is 2.2. A Generic Approach This natural risk has the disadvantage that h can be a rather complicated object, and optimising over all possible h is difficult. We can transform the problem into a simpler problem, however. As we will see, there often exists an operator X acting directly on data x and mapping to H Y such that (we will provide examples shortly). Furthermore, as F is in H  X  , we can use the adjoint trick to transform tions to the natural risk gives us We still have h in the equation, but it is separated from F . Applying Cauchy-Schwarz removes h altogether, This is an upper bound for the natural risk which contains no supremum, but only observable quanti-ties that depend on the data x . The objective is still difficult to optimise, as we may not easily be able to compute the expectation E X . We fall back on a sam-pling approach, and replace E X with a finite sample estimate. We further add a regulariser that penalizes the complexity of F  X  , to guarantee solutions that are robust in sparsely sampled regions. This gives us a vector-valued regression problem, where  X   X  [0 ,  X  [ is the regularisation parameter and { x i } n i =1 a sample from the underlying probability mea-sure. The minimiser of this problem is known to be with W = ( K +  X  I )  X  1 and K the kernel matrix, in case that the kernels  X  from (3) below are used with A and B being the identities (Micchelli &amp; Pontil, 2005). We have a one-to-one relation between operators in H  X  and their adjoints by Theorem 2.3 below, so we extract F together with F  X  . In summary, the recipe to approximate the operator G is extremely simple: find a transformation X and use the adjoint of the corre-sponding estimator in (2). There remains an impor-tant question, however: How tight is the upper bound? While in general this bound is not tight, the minima of the upper bound and the natural risk are often related (see Supplement A.1). 2.3. An RKHS of Bounded Linear Operators We now develop the necessary mathematical tools for the smooth operator approach. The first step is to define a vector-valued kernel  X  , such that the corre-sponding RKHS H  X  consists of linear bounded opera-tors between H X and H Y . A suitable choice is where A  X  L ( H X ) , B  X  L ( H Y ) are positive, self-adjoint operators. The most important case is where A and B are the identities.
 As in the case of scalar kernels, there exist point eval-uators that are closely related to the kernel. These are  X  f [ h ], where  X  f : H Y  X  H  X  with  X  F ,  X  f [ h ]  X   X  F f,h  X  l (see Micchelli &amp; Pontil (2005)[Sec. 2]). These point evaluators have a natural interpretation as a ten-sor product in case that A and B are the identities; that is,  X  f [ h ] = h  X  f . We have in this case that The theorems we prove hold for the general form in eq. 3, as long as all the scalar kernels used are bounded, e.g. sup x  X  X k ( x,x ) &lt;  X  . In the applications we re-strict ourselves for ease of exposition to the case that A and B are the identities. Finally, we often need to integrate scalar valued RKHS functions, and we as-sume that these integrals are well defined (Supp. F). In Carmeli et al. (2006)[Prop.1] a criterion is given which, if fulfilled, guarantees that a vector-valued RKHS exists with  X  as its reproducing kernel. It is easy to verify this criterion applies, and that  X  has an associated RKHS H  X  (see Supp. A.2). The impor-tance of this space is that it consists of bounded linear operators. A standard tensor product argument shows that H  X  is a subset of the Hilbert-Schmidt operators in case that A and B are the identities.
 Corollary 2.1. If A and B are the identities then H  X   X  HS and the inner products are equal.
 In the general case we still have: Theorem 2.1 (Proof in supplement, p. 11) . Each F  X  X   X  is a bounded linear operator from H X to H Y . Another useful fact about this RKHS is that all F are uniquely defined by the values F k ( x,  X  ).
 Theorem 2.2 (Proof in supp., p. 11) . If for F , G  X  H
 X  and all x  X  X it holds that F k ( x,  X  ) = G k ( x,  X  ) then F = G . Furthermore, if k ( x,  X  ) is continuous in dense subset of X . 2.4. Adjoint Kernels and Operators We now define an adjoint kernel  X   X  ( h,u ) =  X  h, B u  X  l for  X  . Here l ( y,y 0 ) denotes the kernel corresponding to H Y , and  X  X  ,  X  X  l is the H Y inner product. With the same argument as for  X  we show  X   X  is a kernel with an associated RKHS H  X   X  such that each element of H  X   X  is a bounded linear operator from H Y to H X . The following theorem is important for the adjoint trick. Theorem 2.3 (Proof in supp., p. 12) . For every F  X  H
 X  there exists an adjoint F  X  in H  X   X  such that for all f  X  X  X and h  X  X  Y In particular, we have for F f = P n i =1  X  f i [ h i P i =1  X  f, A f i  X  k B h i that the adjoint is (T F ) h = F  X  h = The operator T F = F  X  is an isometric isomorphism 2.5. Constraints As in the introductory example, it is usually known that the operation we estimate fulfills certain proper-ties, like being symmetric in the sense that and one might want to have an estimate that shares this property of self-adjointness with F .
 In the case of operators acting on L 2 , certain proper-ties can be enforced by imposing convex constraints. We mentioned already the a.e. positive Radon-Nikod  X ym derivative in the introduction, which can be enforced by a positivity constraint on the operator. Symmetry of an operation can be enforced by a linear constraint on the corresponding operator, to make the operator self-adjoint. Enforcing a multiplication op-erator is very similar to this case, as every bounded multiplication operator is self-adjoint and every self-adjoint operator is a multiplication operator in a suit-able coordinate system, due to the spectral theorem. Self-adjointness might therefor be used as an easy to optimise proxy constraint. Other examples are expec-tation operators , which can be difficult to learn due to the required normalisation. Convex constraints can be used to guarantee that the inferred operator repre-sents an integral, however. This is similar to the pos-itivity constraint discussed before: we have F f  X  0 for all positive continuous f iff there exists a (Radon-)measure  X  such that F f = R fd X  under suitable con-ditions. This is the Riesz representation theorem for linear functionals (Fremlin, 2003)[436J].
 The same constraints can be applied in the RKHS set-ting, although a real-valued RKHS is usually a proper subset of L 2 and this can weaken the implications. Quantifying this effect is a major piece of work on its own. Here, we illustrate on an example the relation between self-adjointness and linear constraints: Theorem 2.4 (Proof in supp., p. 13) . The set of self-adjoint operators in H  X  is a closed linear subspace. 2.6. Smooth Multiplication Operators We demonstrate our approach on the example from the introduction by approximating the multiplication operator G g = fg with a smooth operator M f : H
X  X  H X , where g  X  H X and f is an arbitrary function. As noted in the introduction, fg is not in the RKHS even for f  X  H X : in this case, the prod-uct fg =  X  f  X  g,  X ( x )  X  HS is a linear operation in the tensor feature space  X ( x ) := k ( x,  X  )  X  k ( x,  X  ) with the standard Hilbert-Schmidt inner product, which corre-sponds to the RKHS with the squared kernel (Stein-wart &amp; Christmann, 2008, Theorem 7.25).
 We apply the generic approach from Section 2.2, where in eq. 1 we use the mapping X ( x ) := f ( x ) k ( x,  X  ), which is in H X for a given x as required. An approximation M f of G can now be gained from eq. 2 by moving from the adjoint M  X  f in eq. 2 to M f , This is an intuitive solution: f and g are multiplied on our sample points x j and this product is interpolated with the help of k ( x i ,  X  ). Indeed, it is the solution of the scalar-valued ridge regression, Returning to our setting from the introduction: if we wish to take the inner product of this approximation with a new function h  X  X  X , we get  X   X  fg,h  X  k  X   X  X  M f g,h  X  k = It would further be useful to constrain the estimate either to be a multiplication operator or to be self-adjoint. In this case no closed form solution is avail-able, and a numerical optimisation is needed. 2.7. Smooth Composition Operators Assume we have given a function  X  : X  X  Y , a func-tion h  X  X  Y , and we want a smooth approximation of G h = h  X   X  with  X  h , where  X   X  H  X  maps from H Y to H X . We again use the relation of eq. 1, where this time X ( x ) := l (  X  ( x ) ,  X  ), which is in H Y for a given x . We then get the approximation We discuss the change of measure rule and conditional expectations. The supplementary material contains a discussion of products and the Fubini theorem. 3.1. Covariate Shift: Ch. of Meas. on X A standard integral transformation is the change of measure: given a measure P and a measure Q that is absolute continuous wrt. P ( Q P ) there exists a Radon-Nikod  X ym derivative r such that E Q f = E P f  X  r . As in the multiplication case we have in general no guarantee that f  X  r is in H X , and it is useful to have an approximation R f that maps to H X . Furthermore, we do not know r , and we need to work with data. A po-tential risk function is sup k f k a first optimisation approach would be to replace ex-pectations with empirical expectations and minimize wrt. R , where { y j } m j =1 is a sample from Q and { x i } n i =1 The following R  X  makes both errors zero, where m P = P n i =1 k ( x i ,  X  ) and m Q = P m i =1 k ( x This is the minimum norm solution which fits both sides exactly (Micchelli &amp; Pontil, 2005)[Th. 3.1]. The approach differs from our generic approach since we have no expectation in the risk function over which the error is averaged. Instead, we have an interpola-tion problem. This interpolation transforms P com-pletely to Q , which can be interpreted as overfitting. There are at least two points where we can improve matters. First, R does not necessarily represent a multiplication, and constraints can be used to enforce this, or to enforce self-adjointness of R , which is eas-ier. Second, we do not verify the absolute continuity condition. If the measures are not absolutely contin-uous then it is not possible to transform one measure into the other by a multiplication operator. We further discuss absolute continuity in Suppl. C.1.1.
 A heuristic to solve the constrained problem is to es-timate a Radon-Nikod  X ym derivative r from data and then, in a second step, to approximate the multiplica-tion with an operator R to guarantee that R f  X  X  X . There are several possible ways to estimate such a function. In Huang et al. (2007); Gretton et al. (2009); Yu &amp; Szepesvari (2012) a quadratic program is given to estimate a weight vector  X  with non-negative en-tries, such that the following cost function is min-imised, k P m j =1 k ( y j ,  X  )  X  P n i =1  X  i k ( x i ,  X  ) k 4 with  X  instead of R  X  .
 We can interpolate these  X  i  X  X  with a non-negative func-tion r if the x i are disjoint. Applying the uncon-strained multiplication estimate from Sec. 2.6 to r  X  f gives us the change-of-measure operator 3.2. Conditional Expectation Kernel-based approximations to conditional expecta-tions have been widely studied, and their links with vector-valued regression are established (Song et al., 2009; Gr  X unew  X alder et al., 2012a). The conditional ex-pectation estimate introduced in these works can be represented by a vector-valued function  X  : X  X  H Y . The approximation is E [ h | x ]  X  X  h, X  ( x )  X  l . Now, in line with our earlier reasoning, we can define a smooth op-erator E to represent the operation. To define such an operator, it is useful to treat the conditional expecta-tion as an operator on h , i.e. ( h 7 X  E [ h | x ]). By using our natural cost function and applying Jensen X  X  inequality, we gain an upper bound that is very similar to the one in the generic case,
E c [ E ] := sup This differs from our approach of Section 2.2 in that X ( x ) is no longer deterministic, but takes the values l ( y,  X  ) according to the product distribution. With the usual (regularised) empirical version we get the esti-mate where W is defined in eq. 2. The expression is very similar to the solution  X  in (Gr  X unew  X alder et al., 2012a), since  X  ( x ) = E  X  k ( x,  X  ) (see Supp. C.3). 4.1. Sum Rule  X  Change of Measure on Y We next consider a smooth approximation to the sum rule, as introduced by Song et al. (2009)[eq. 6]; see also Fukumizu et al. (2012, Theorem 3.2). We have two measures P and Q on the product space X  X  Y . We assume that for each x we have conditional measures P
Y | x = Q Y | x . The task is to estimate the marginal distribution of Q on Y , i.e. Q Y , based on samples In our setting the task is formulated naturally in a weak sense, i.e. we want to infer an RKHS element m Y such that E m [ m Y ] := sup k h k small. We can reformulate the expectation to reduce it to quantities we observe. Formally, we have The problem of performing these transformations when we have only samples can now be addressed naturally in the operator framework. Using the sam-ples from P X  X  Y we can infer a conditional expecta-tion estimate E [ h ]( x )  X  E [ h | x ] via Sec. 3.2, and us-ing samples { z i } m i =1 from Q X , we can infer an m X m compositions of the approximate conditional expecta-tion operation E and the approximate expectation op-eration  X  m X ,  X  X  k as E maps into H X :  X  m X , E h  X  k  X  E  X  m X ,h  X  l . A natural estimate m Y is hence E  X  m X . With the expectation estimate from eq. 5 and W from eq. 2 we have which is the estimate of Song et al. (2009). 4.1.1. Estimation Error Assuming we have control over the approximation er-ror E c [ E ] of E and E m [ m X ] of m X , and we want to get error approximations for m Y , i.e. upper bounds on E m [ m Y ]. The next theorem provides these. The proof uses the transformation in eq. 6 and the link of the involved quantities to the estimates E and m X . The kernel function is  X  ( h,h 0 ) :=  X  h, A h 0  X  l B . Theorem 4.1 (Proof in supp., p. 16) . We assume that the integrability assumptions from Supp. F hold, Q
X P X , and the corresponding Radon-Nikod  X ym derivative r is a.e. upper bounded by b . Defining The error is controlled by scaled versions of the er-rors of E and m X , which is as we would hope. The convergence rate of E m [ m Y ] in terms of sample size is controlled by the slower rate of E c [ E ] and E m [ m when k E k 2  X  stays bounded. 4.2. Bayes X  Rule  X  Ch. of Meas. on X | y Closely related to the approximate sum rule is an ap-proximate Bayesian inference setting, as described by Fukumizu et al. (2011); Song et al. (2013). As in the case of the sum rule, we have two measures P and Q on the product space X  X  Y , samples { ( x i ,y i ) } n i =1 from P X  X  Y , samples { z i } m i =1 from Q X , and we assume P
Y | x = Q Y | x . The difference compared with the sum rule is that we are not interested in the marginal Q Y , It is intuitive to consider this problem in a weak sense: that is, instead of estimating the full distribution, we want to learn a version of the conditional expectation acting on functions f , i.e., to minimise Unlike the problem of estimating conditional expecta-tions, however, we observe only P on the product space X  X  Y , and not the Q for which we want the condi-tional expectation. In this setting multiple operations must be combined, and the operator approach shows its strength in terms of keeping the manipulations sim-ple.
 We begin by linking the problem of estimating E [ f | y ] with G to the easier problem of estimating E [ h | x ] with E . The latter problem is easier since Q Y | x = P Y | x we can use the usual approach to estimate the condi-tional expectation with samples from P . As with the sum rule, the quality of this estimate as an estimate of
Q Y | x depends on the Radon-Nikod  X ym derivative of the marginal measures, as the estimate is optimised We can use integral transformations to link the condi-tional expectations. One of the challenges is the intro-duction of an integral over Q Y such that we can move from E [ f | y ] to a product integral, and from the prod-uct integral to the conditional expectation E [ h | x ]. One way to do this is to approximate a  X  -peak at y with a function  X   X  y . This function should be concentrated around y , and should be normalised to 1 wrt. Q Y to approximate the point evaluator at y . In this case we can approximate E [ f | y ] with An RKHS kernel function l ( y,  X  ) can serve as a smoothed approximation to a point-evaluator. For ex-ample, a Gaussian kernel with a bandwidth parameter  X  becomes concentrated around y for small  X  . We thus choose  X   X  y = l ( y,  X  ), bearing in mind that this will in-troduce a non-vanishing bias. With this choice, and by approximating the last term with the estimate E , we get The term E Y l ( y,  X  ) is approximated by the mean esti-mate  X  m Y ,l ( y,  X  )  X  l , computed via change of measure. We next approximate the above with G [ f ]( y ) to esti-mate E [ f | y ]. By defining a suitable distribution over Y to approximate E [ f | y ], and following the usual approach, we get sup  X  E X  X  Y k G  X  l ( y,  X  )  X  u ( x,y ) k ( x,  X  ) k 2 k , where the product measure is over the independent probability measures Q X and R Y which we choose, and we are approximating the function The above is an estimate (via E ) of a ratio of smoothed densities, the numerator being a smoothed conditional density. If the bandwidth parameter of the kernel on H
Y is fixed, then this smoothing remains a source of bias, and shows up as an approximation error in Th. 4.2 below. If we now use the empirical and  X  -regularised version of the upper bound, we get an es-timate for E [ f | y ],
G f = with W = ( L +  X  I )  X  1 , L being the kernel matrix, { x Note that this expression is not the same as the kernel Bayes X  rule of Fukumizu et al. (2012, Figure 1); an empirical comparison of the two approaches remains a topic for future work. 4.2.1. Estimation Error The error of the estimator G can be bounded by the errors of the mean estimate m X , the error of E , an approximation error
E a [ l ] := sup where y,y 0  X  Q Y , and the risk of G in the top line of eq. 7. We denote this risk with E K [ G ]. The following theorem states the bound. The risks in the theorem are measured wrt. Q for all but the estimate E and the constant C , and can be found in the supplement. Theorem 4.2 (Proof in supp., p. 17) . We as-sume that the integrability assumptions from Supp. F hold, that Q X P X , and that the corresponding Radon-Nikod  X ym derivative is a.e. upper bounded by b . Furthermore, we assume that there exists a con-stant q &gt; 0 such that E y 0  X  P Y l ( y,y 0 )  X  q for all y  X  Y and that the approximation error of m Y is such that |
E There exists a positive constant C such that E [ G ]  X E K [ G ] + C E a [ l ] + k E k 2  X  E m [ m X ] + E The assumption on m Y guarantees that we are rea-sonably close to the true expectation. This is fulfilled with high probability after finitely many steps for the standard estimate. The assumption E y 0  X  P Y l ( y,y 0 )  X  q guarantees that we have a good approximate point evaluator at y 0 . 4.3. A Short Note on Convergence Rates Convergence rates are obviously a big topic and we do not want to go into too much depth here. We there-fore keep the necessary assumptions simple, and we derive rates only for the approximate sum rule, which we compare with the rates of (Fukumizu et al., 2012). We make a number of assumptions, which can be found in Sec. E.1. The main assumption is that H X and H Y are finite dimensional. The H Y assumption is crucial, however the H X assumption can be avoided with some extra effort. Another assumption concerns the proba-bility measures over which the convergence occurs. We refer the reader here to Caponnetto &amp; De Vito (2007) for details, and we take P to be the class of priors from Def. 1 with b =  X  . There is an approximation error in the theorem which measures how well we can ap-proximate the true conditional expectation (see Supp. E for the definition). Finally, we assume that we have a rate of  X   X  ]0 , 1] to estimate the mean of Q X . Theorem 4.3 (Proof in Supp. E) . Let E  X  be a minimiser of the approximation error E A , and let the schedule for the regulariser for E n be chosen accord-ing to Caponnetto &amp; De Vito (2007)[Thm 1]. Under assumptions E.1 and if Q X P X with a bounded Radon-Nikod  X ym derivative, we have that for every &gt; 0 there exist constants a,b,c,d such that lim sup The value k E n k  X  is of obvious importance. E n is the minimiser of the empirical regularised risk, and if this minimiser converges with high probability to the min-imiser of the regularised risk, then one can infer from Caponnetto &amp; De Vito (2007)[Prop. 3] that E n will be bounded with high probability. This then guaran-tees a rate of convergence of n  X   X  , which matches the state of art rates of Fukumizu et al. (2012)[Th. 6.1] which are between n  X  2 / 3  X  and n  X   X  , depending on the smoothness assumptions made. We have presented an approach for estimating linear operators acting on an RKHS. Derivations of estimates are often generic, and operations can naturally be com-bined to form complex estimates. Risk bounds for these complex rules can be expressed straightforwardly in terms of risk bounds of the basic estimates used in building them. There are obviously many routes to explore from here. Most immediately, improved es-timation techniques would be helpful, incorporating sparsity and other constraints. It would also be inter-esting to consider additional machine learning settings in this framework.
 Acknowledgements The authors want to thank for the support of the EPSRC #EP/H017402/1 (CARDyAL) and the European Union #FP7-ICT-270327 (Complacs), as well as the reviewers for helpful suggestions.
 Aronszajn, N. Theory of reproducing kernels. Trans-actions of the American Mathematical Society , 68 (3):337 X 404, 1950.
 Berlinet, A. and Thomas-Agnan, C. Reproducing kernel Hilbert spaces in Probability and Statistics . Kluwer, 2004.
 Caponnetto, A. and De Vito, E. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics , 7(3):331 X 368, 2007. Carmeli, C., De Vito, E., and Toigo, A. Vector val-ued reproducing kernel Hilbert spaces of integrable functions and mercer theorem. Analysis and Appli-cations , 4(4):377 X 408, 2006.
 Fremlin, D.H. Measure Theory -Volume 1: The Irre-ducible Minimum . Torres Fremlin, 2000.
 Fremlin, D.H. Measure Theory -Volume 2: Broad Foundations . Torres Fremlin, 2001.
 Fremlin, D.H. Measure Theory -Volume 4: Topological Measure Spaces . Torres Fremlin, 2003.
 Fukumizu, K., Song, L., and Gretton, A. Kernel bayes X  rule. In NIPS , 2011.
 Fukumizu, K., Song, L., and Gretton, A. Kernel bayes X  rule: Bayesian inference with positive definite ker-nels. ArXiv , 1009.5736v4, 2012.
 Gretton, A., Smola, A., Huang, J., Schmittfull, M.,
Borgwardt, K., and Sch  X olkopf, B. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning . 2009.
 Gr  X unew  X alder, S., Lever, G., Baldassarre, L., Patter-son, S., Gretton, A., and Pontil, M. Conditional mean embeddings as regressors. In ICML , 2012a. Gr  X unew  X alder, S., Lever, G., Baldassarre, L., Pontil,
M., and Gretton, A. Modelling transition dynamics in MDPs with RKHS embeddings. In ICML , 2012b. Huang, J., Smola, A. J., Gretton, A., Borgwardt, K., and Sch  X olkopf, B. Correcting sample selection bias by unlabeled data. In NIPS , 2007.
 Micchelli, C.A. and Pontil, M.A. On learning vector-valued functions. Neural Computation , 17(1), 2005. Nishiyama, Y., Boularias, A., Gretton, A., and Fuku-mizu, K. Hilbert space embeddings of POMDPs. In UAI , 2012.
 Smola, A., Gretton, A., Song, L., and Sch  X olkopf, B. A
Hilbert space embedding for distributions. In ALT , 2007.
 Song, L., Huang, J., Smola, A.J., and Fukumizu, K.
Hilbert space embeddings of conditional distribu-tions with applications to dynamical systems. In ICML , 2009.
 Song, L., Boots, B., Siddiqi, S. M., Gordon, G. J., and Smola, A. J. Hilbert space embeddings of hidden Markov models. In ICML , 2010.
 Song, L., Fukumizu, K., and Gretton, A. Kernel em-beddings of conditional distributions. IEEE Signal Processing Magazine , To Appear, 2013.
 Sriperumbudur, B., Gretton, A., Fukumizu, K.,
Lanckriet, G., and Sch  X olkopf, B. Hilbert space em-beddings and metrics on probability measures. Jour-nal of Machine Learning Research , 11:1517 X 1561, 2010.
 Steinwart, I. and Christmann, A. Support Vector Ma-chines . Springer, 2008.
 Werner, D. Funktionalanalysis . Springer, 4th edition, 2002.
 Yu, Y. and Szepesvari, C. Analysis of kernel mean
