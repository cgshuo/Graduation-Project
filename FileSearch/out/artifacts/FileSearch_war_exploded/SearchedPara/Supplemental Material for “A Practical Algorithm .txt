 Sanjeev Arora arora@cs.princeton.edu Rong Ge rongge@cs.princeton.edu Yoni Halpern halpern@cs.nyu.edu David Mimno mimno@cs.princeton.edu Ankur Moitra moitra@ias.edu David Sontag dsontag@cs.nyu.edu Yichen Wu yichenwu@princeton.edu Michael Zhu mhzhu@princeton.edu Recall that the correctness of the algorithm depends on the following Lemma: Lemma 1.1. The point d j found by the algorithm must be  X  = O ( / X  2 ) close to some vertex v i . In par-ticular, the corresponding a j O ( / X  2 ) -covers v i . In order to prove this Lemma, we first show that even if previously found vertices are only  X  close to some vertices, there is still another vertex that is far from the span of previously found vertices.
 Lemma 1.2. Suppose all previously found vertices are O ( / X  2 ) close to distinct vertices, there is a vertex v whose distance from span( S ) is at least  X / 2 . In order to prove Lemma 1.2, we use a volume ar-gument. First we show that the volume of a robust simplex cannot change by too much when the vertices are perturbed.
 Lemma 1.3. Suppose { v 1 ,v 2 ,...,v K } are the vertices of a  X  -robust simplex S . Let S 0 be a simplex with ver-tices { v 0 1 ,v 0 2 ,...,v 0 K } , each of the vertices v turbation of v i and k v 0 i  X  v i k 2  X   X  . When 10 the volume of the two simplices satisfy Proof: As the volume of a simplex is proportional to the determinant of a matrix whose columns are the edges of the simplex, we first show the following per-turbation bound for determinant.
 Claim 1.4. Let A , E be K  X  K matrices, the small-est eigenvalue of A is at least  X  , the Frobenius norm k E k F  X  Proof: Since det( AB ) = det( A ) det( B ), we can mul-tiply both A and A + E by A  X  1 . Hence det( A + E ) / det( A ) = det( I + A  X  1 E ).
 The Frobenius norm of A  X  1 E is bounded by Let the eigenvalues of A  X  1 E be  X  1 , X  2 ,..., X  K , then by definition of Frobenius Norm P K i =1  X  2 i  X  A  X  1 E 2 K X  2 / X  2 .
 The eigenvalues of I + A  X  1 E are just 1 +  X  1 , 1 +  X  ,..., 1 +  X  K , and the determinant det( I + A  X  1 E ) = Q i =1 (1 +  X  i ). Hence it suffices to show min To do this we apply Lagrangian method and show the minimum is only obtained when all  X  i  X  X  are equal. The optimal value must be obtained at a local optimum of Taking partial derivatives with respect to  X  i  X  X , we get using hand side is a constant, so each  X  i must be one of the two solutions of this equation. However, only one of the solution is larger than 1 / 2, therefore all the  X  are equal.
 For the lower bound, we can project the perturbed subspace to the K  X  1 dimensional space. Such a projection cannot increase the volume and the pertur-bation distances only get smaller. Therefore we can apply the claim directly, the columns of A are just v i +1  X  v 1 for i = 1 , 2 ,...,K  X  1; columns of E are just v is at least  X  because the polytope is  X  robust, which is equivalent to saying after orthogonalization each col-umn still has length at least  X  . The Frobenius norm of E is at most 2 directly by applying the claim.
 For the upper bound, swap the two sets S and S 0 and use the argument for the lower bound. The only thing we need to show is that the smallest eigenvalue of the matrix generated by points in S 0 is still at least  X / 2. This follows from Wedin X  X  Theorem(Wedin, 1972) and the fact that k E k X k E k F  X  Now we are ready to prove Lemma 1.2.
 gorithm, when we try to find the farthest point to the origin. Here essentially S = { ~ 0 } . For any two vertices v 1 ,v 2 , since the simplex is  X  robust, the dis-tance between v 1 and v 2 is at least  X  . Which means  X / 2.
 For the later steps, recall that S contains vertices of a perturbed simplex. Let S 0 be the set of original vertices corresponding to the perturbed vertices in S . Let v be any vertex in { v 1 ,v 2 ,...,v K } which is not in S . Now we know the distance between v and S is equal to vol( S  X  X  v } ) / ( | S | X  1)vol( S ). On the other hand, Lemma 1.3 to bound the ratio between the two pairs when  X  &gt; 20 K 0 .
 Lemma 1.1 is based on the following observation: in a simplex the point with largest ` 2 is always a vertex. Even if two vertices have the same norm if they are not close to each other the vertices on the edge connecting them will have significantly lower norm.
 Proof: (Lemma 1.1) Since d j is the point found by the algorithm, let us consider the point a j before perturbation. The point a j is inside the simplex, therefore we can write a j as a convex combination of the vertices: Let v t be the vertex with largest coefficient c t . Let  X  be the largest distance from some vertex to the space spanned by points in S ( X  = max l dis( v l , span( S )). By Lemma 1.2 we know  X  &gt;  X / 2. Also notice that we are not assuming dis( v t , span( S )) =  X .
 Now we rewrite a j as c t v t + (1  X  c t ) w , where w is a vector in the convex hull of vertices other than v t . Observe that a j must be far from span( S ), because d j is the farthest point found by the algorithm. Indeed dis( a j , span( S ))  X  dis( d j , span( S ))  X  The second inequality is because there must be some point d l that correspond to the farthest vertex v l and have dis( d l , span( S ))  X   X   X  . Thus as d j is the far-The point a j is on the segment connecting v t and w , the distance between a j and span( S ) is not much smaller than that of v t and w . Following the intuition in ` 2 norm when v t and w are far we would expect a j to be very close to either v t or w . Since c t  X  1 /K it cannot be really close to w , so it must be really close to v t . We formalize this intuition by the following cal-culation (see Figure 2): Project everything to the orthogonal subspace of span( S ) (points in span( S ) are now at the origin). Af-ter projection distance to span( S ) is just the ` 2 norm of a vector. Without loss of generality we assume k v t k 2 = k w k 2 =  X  because these two have length at most  X , and extending these two vectors to have length  X  can only increase the length of d j .
 The point v t must be far from w by applying Lemma 1.2: consider the set of vertices V 0 = { v i v does not correspond to any point in S and i 6 = t } . The set V 0  X  S satisfy the assumptions in Lemma 1.2 so there must be one vertex that is far from span( V 0  X  S ), and it can only be v t . Therefore even after projecting to orthogonal subspace of span( S ), v t is still far from any convex combination of V 0 . The vertices that are not in V 0 all have very small norm after projecting to orthogonal subspace (at most  X  0 ) so we know the distance of v t and w is at least  X / 2  X   X  0 &gt;  X / 4. Now the problem becomes a two dimensional calcu-lation. When c t is fixed the length of a j is strictly increasing when the distance of v t and w decrease, so we assume the distance is  X / 4. Simple calculation (us-ing essentially just pythagorean theorem) shows The right hand side is largest when  X  = 2 (since the vectors are in unit ball) and the maximum value is O ( / X  2 ). When this value is smaller than 1 /K , we must have 1  X  c t  X  O ( / X  2 ). Thus c t  X  1  X  O ( / X  2 ) and  X   X  (1  X  c t ) +  X  O ( / X  2 ).
 The cleanup phase tries to find the farthest point to a subset of K  X  1 vertices, and use that point as the K -th vertex. This will improve the result because when we have K  X  1 points close to K  X  1 vertices, only one of the vertices can be far from their span. Therefore the farthest point must be close to the only remaining vertex. Another way of viewing this is that the al-gorithm is trying to greedily maximize the volume of the simplex, which makes sense because the larger the volume is, the more words/documents the final LDA model can explain.
 The following lemma makes the intuitions rigorous and shows how cleanup improves the guarantee of Lemma 1.1.
 Lemma 1.5. Suppose | S | = K  X  1 and each point in S is  X  = O ( / X  2 ) &lt;  X / 20 K close to distinct vertices v the farthest point found by the algorithm is d j , then the corresponding a j O ( / X  ) -covers the remaining vertex. Proof: We still look at the original point a j and express it as P K t =1 c t v t . Without loss of general-ity let v 1 be the vertex that does not correspond to anything in S . By Lemma 1.2 v 1 is  X / 2 far from span( S ). On the other hand all other vertices are at least  X / 20 r close to span( S ). We know the distance be true unless c 1  X  1  X  O ( / X  ).
 These lemmas directly lead to the following theorem: Theorem 1.6. FastAnchorWords algorithm runs in time  X  O ( V 2 + V K/ 2 ) and outputs a subset of { d 1 ,...,d V } of size K that O ( / X  ) -covers the vertices provided that 20 K/ X  2 &lt;  X  .
 Proof: In the first phase of the algorithm, do induc-tion using Lemma 1.1. When 20 K/ X  2 &lt;  X  Lemma 1.1 shows that we find a set of points that O ( / X  2 )-covers the vertices. Now Lemma 1.5 shows after cleanup phase the points are refined to O ( / X  )-cover the ver-tices. In order to show RecoverL2 learns the parameters even when the rows of  X  Q are perturbed, we need the follow-ing lemma that shows when columns of  X  Q are close to the expectation, the posteriors c computed by the algorithm is also close to the true value.
 Lemma 2.1. For a  X  robust simplex S with vertices { v 1 ,v 2 ,...,v K } , let v be a point in the simplex that can be represented as a convex combination v = P K i =1 c i v If the vertices of S are perturbed to S 0 = { ...,v 0 i ,... } where k v 0 i  X  v i k  X   X  1 and v is perturbed to v 0 where closest to v 0 , and v  X  = P K i =1 c 0 i v i , when 10 for all i  X  [ K ] | c i  X  c 0 i | X  4(  X  1 +  X  2 ) / X  . Proof: Consider the point u = P K i =1 c i v 0 i , by triangle inequality: k u  X  v k  X  P K i =1 c i k v i  X  v 0 i k  X   X  1 k u  X  v 0 k X k u  X  v k + k v  X  v 0 k X   X  1 +  X  2 , and u is in S The point v  X  is the point in conv { S 0 } that is closest to v , so k v  X   X  v 0 k X   X  1 +  X  2 and k v  X   X  u k X  2(  X  1 +  X  Then we need to show when a point ( u ) moves a small distance, its representation also changes by a small amount. Intuitively this is true because S is  X  robust. By Lemma 1.2 when 10 also  X / 2 robust. For any i , let Proj i ( v  X  ) and Proj be the projections of v  X  and u in the orthogonal sub-space of span( S 0 \ v 0 i ), then | c i  X  c 0 i | = k Proj i ( v  X  )  X  Proj i ( u ) k / dis( v i and this completes the proof.
 With this lemma it is not hard to show that RecoverL2 has polynomial sample complexity.
 Theorem 2.2. When the number of documents M is at least max { O ( aK 3 log V/D (  X p ) 6 ) ,O (( aK ) 3 log V/D 3 (  X p ) our algorithm using the conjunction of FastAnchor-Words and RecoverL2 learns the A matrix with entry-wise error at most .
 Proof: (sketch) We can assume without loss of generality that each word occurs with probability at least / 4 aK and furthermore that if M is at least 50 log V/D 2 Q then the empirical matrix  X  Q is entry-wise within an additive Q to the true Q = details. Also, the K anchor rows of  X  Q form a simplex that is  X p robust.
 The error in each column of  X  Q can be at most  X  2 = p 4 aK/ . By Theorem 1.6 when 20 K X  2 / (  X p ) 2 &lt;  X p (which is satisfied when M = O ( aK 3 log V/D (  X p ) 6 )) to the true anchor words. Hence by Lemma 2.1 every entry of C has error at most O (  X  2 / (  X p ) 2 ). With such number of documents, all the word proba-bilities p ( w = i ) are estimated more accurately than the entries of C i,j , so we omit their perturbations here for simplicity. When we apply the Bayes rule, we know A i,k = C i,k p ( w = i ) /p ( z = k ), where p ( z = k ) is  X  k which is lower bounded by 1 /aK . The numerator and denominator are all related to entries of C with positive coefficients sum up to at most 1. Therefore the errors  X  num and  X  denom are at most the error of a single entry of C , which is bounded by O (  X  2 / (  X p ) 2 ). Applying Taylor X  X  Expan-sion to ( p ( z = k,w = i ) +  X  num ) / (  X  k +  X  denom ), the error on entries of A is at most O ( aK X  2 / (  X p ) 2 ). When , and get the desired accuracy of A . The number of document required is M = O (( aK ) 3 log V/D 3 (  X p ) 4 ). The sample complexity for R can then be bounded using matrix perturbation theory.
 For RecoverKL, we observe that the dimension and minimum values of v i  X  X  are all bounded by polynomials of , a , r (see Section 3.5 Reducing Dictionary Size of (Arora et al., 2012)). In this case, when distance  X  is small enough, we know the KL-divergence is both upper and lowerbounded by some polynomial factor times ` 2 norm squared.
 Lemma 2.3. When all values in the vectors { v 0 i } are convex hull of perturbed vertices { v 0 1 ,v 0 2 ,...,v 0 v k X  2 / 100 a 2 r 2 , then D KL ( u k v )  X  2 k u  X  v k 2 Proof: Let s i = u i  X  v i , apply Taylor X  X  expansion on s + s 2 i / 2 v i  X  log( v i + s i ) /v i  X  s i + 2 s 2 i /v Adding this up, using the fact P s i = P u i  X  P v i = 0, we know the KL-divergence is bounded by On the other hand, by Pinsker X  X  inequality, we know D KL ( u k v )  X  2 | u  X  v | 2 1  X  2 k u  X  v k 2 . Using these two bounds we can easily prove a replace-ment for Lemma 2.1.
 Lemma 2.4. For a  X  robust simplex S with vertices { v 1 ,v 2 ,...,v K } , let v be a point in the simplex that can be represented as a convex combination v = P K i =1 c i v If the vertices of S are perturbed to S 0 = { ...,v 0 i ,... } where k v 0 i  X  v i k  X   X  1 and v is perturbed to v 0 where k v  X  v 0 k  X   X  2 . Further assume all entries of v 0 and v are at least l = 2 / 20 a 2 r 2 . Let v KL be the point in conv { S 0 } that has smallest D KL ( v 0 k v KL ) , and v KL = P i  X  [ K ] | c i  X  c 0 i | X  4(  X  1 +  X  2 ) / X  Proof: Let v  X  be the closest point (in ` 2 distance) of v in conv { S 0 } . By proof of Lemma 2.1 we know k v  X   X  v k  X   X  1 +  X  2 . Hence by Lemma 2.3 D KL ( v 0 k v  X  )  X  2(  X  1 +  X  2 ) 2 /l .
 Since v KL is the point with smallest divergence, we know in particular D KL ( v 0 k v KL )  X  2(  X  1 +  X  ) 2 /l . On the other hand, by Pinkser X  X  inequality D k v 0  X  v KL k X  (  X  1 +  X  2 ) / Now we follow the proof of Lemma 2.1 and define u = P v
KL k  X  2(  X  1 +  X  2 ) / know | c i  X  c 0 i | X  4(  X  1 +  X  2 ) / X  We can simply replace Lemma 2.1 with this Lemma and get provable guarantee of RecoverKL. However, the argument here is not tight (in particular it gives worse bound than ` 2 ). This section contains plots for ` 1 , held-out probability, coherence, and uniqueness for all semi-synthetic data sets. Up is better for all metrics except ` 1 error. The advantage of the non-negative recovery methods over the original Recover method on the real data is con-sistent with the results observed on the semi-synthetic data. For example, one can compare the mean log likelihood on real NY Times data from Figure 5 of the main paper (100 topics; 236k docs) with the semi-synthetic NY Times data shown in Figure 3 of the supplementary materials (100 topics; 250k docs). The values for the real data are [Recover: -8.42, Recov-erL2: -8.16, RecoverKL: -8.09, Gibbs -7.93] and for semi-synthetic are [Recover: -8.23, RecoverL2: -8.08, RecoverKL: -8.08, Gibbs: -8.076]. 3.1. Sample Topics Tables 1, 2, and 3 show 100 topics trained on real NY Times articles using the RecoverL2 algorithm. Each topic is followed by the most similar topic (measured by ` 1 distance) from a model trained on the same documents with Gibbs sampling. When the anchor word is among the top six words by probability it is highlighted in bold. Note that the anchor word is fre-quently not the most prominent word. 4.1. Generating Q matrix For each document, let H d be the vector in R V such that the i -th entry is the number of times word i ap-pears in document d , n d be the length of the docu-ment and W d be the topic vector chosen according to Dirichlet distribution when the documents are gener-ated. Conditioned on W d  X  X , our algorithms require the expectation of Q to be 1 M P M d =1 AW d W T d A T . In order to achieve this, similar to (Anandkumar et al., 2012), let the normalized vector  X  H d = H d  X  diagonal matrix  X  H d = Diag ( H d ) n Here z d,i is the i -th word of document d , and e i R
V is the basis vector. From the generative model, the expectation of all terms e z AW d W T d A T , hence by linearity of expectation we know E [  X  H d  X  H T d  X   X  H d ] = AW d W T d A T . If we collect all the column vectors  X  H d to form a large sparse matrix  X  H , and compute the sum of all  X  H d to get the diagonal matrix  X  H , we know Q =  X  H  X  H T  X   X  has the desired expectation. The running time of this step is O ( MD 2 ) where D 2 is the expectation of the length of the document squared. 4.2. Applying Recover to Small Datasets The original Recover algorithm from Arora et al. (2012) can fail on small datasets if the Q S , S matrix which holds the anchor-anchor co-occurrence counts is rank deficient due to sparsity. When Recover fails, we use a modified version of the algorithm, solving for ~z by finding a least squares solution to Q S , S ~z = ~p and solving for A T with a pseudoinverse: A T = ( Q matrix in which some columns contain all 0s. In that case we replace columns of 0s with a uniform distribu-tion over the vocabulary words, 1 V 1 .
 Negative values also often occur in the A matrix re-turned by the original Recover method. To project back onto the simplex, we clip all negative values to 0 and normalize the columns before evaluating the learned model.
 4.3. Exponentiated gradient algorithm The optimization problem that arises in RecoverKL and RecoverL2 has the following form: where d (  X  ,  X  ) is a Bregman divergence (in particular it is squared Euclidean distance for RecoverL2 and KL divergence for RecoverKL), ~x is a column vector of size K , S is the set of K anchor indices,  X  Q i is a row vector of size V , and  X  Q S is the K  X  V matrix formed by stacking the rows of  X  Q corresponding to the indices in S .
 This is a convex optimization problem with simplex constraints, which can be solved with the Exponenti-ated Gradient algorithm (Kivinen &amp; Warmuth, 1995), described in Algorithm 1. The Exponentiated Gradi-ent algorithm iteratively generates values of ~x which are feasible and converge to the optimal value ~x  X  . In our experiments we show results using both squared Euclidean distance and KL divergence for the diver-gence measure.
 To determine whether the algorithm has converged, we test whether the KKT conditions (which are sufficient for optimality in this problem) hold to within some tolerance, . In our experiments varies between 10  X  6 and 10  X  9 depending on the data set.
 The KKT conditions for our constrained minimization problem are: 2. Primal Feasibility: ~x  X  0, P K i =1 x i = 1. 3. Dual Feasibility:  X  i  X  0 for i  X  X  1 , 2 ,...,K } . 4. Complementary Slackness:  X  i x i = 0 for i  X  We define the following approximation to Condition 4: 4 0 . -Complementary Slackness: 0  X  ~  X  T ~x &lt; . Let ~x t be the t th value generated by Exponentiated Gradient. ~x t is -optimal if there exist ~  X  and  X  such that Conditions 1-3 and 4 0 are satisfied.
 We initialize ~x 0 = 1 K 1 and Exponentiated Gradient preserves primal feasibility, so ~x t satisfies Condition 2. conditions 1 and 3: The algorithm converges when Condition 4 0 is satisfied (i.e. ~  X  T t ~x t &lt; ). ~  X  ~x t can also be understood as the gap between an upper and lower bound on the objective. To see this, note that the Lagrangian function is: The first term in the Lagrangian is exactly the primal Algorithm 1. Exponentiated Gradient Input: Matrix  X  Q S , vector  X  Q i T , divergence measure d (  X  ,  X  ), tolerance parameter Output: non-negative normalized vector ~x close to ~x  X  , the minimizer of d (  X  Q T i ,  X  Q S ~x )) t  X  0
Converged  X  False while not Converged do end while return x t Since the Lagrangian lower bounds the objective, ~  X  T t ~x is the value of the gap. Strong duality holds for this problem, so at optimality, this gap is 0. Testing that the gap is less than is an approximate optimality test. Stepsizes at each iteration are chosen with a line search to find an  X  t that satisfies the Wolfe and Armijo con-ditions (For details, see Nocedal &amp; Wright (2006)). The running time of RecoverL2 is the time of solving V small ( K  X  K ) quadratic programs. When using Exponentiated Gradient to solve the quadratic pro-gram, each word requires O ( KV ) time for preprocess-ing and O ( K 2 ) per iteration. The total running time is O ( KV 2 + K 2 V T ) where T is the average number of iterations. The value of T is about 100  X  1000 depend-ing on data sets.
 Anandkumar, A., Foster, D., Hsu, D., Kakade, S., and
Liu, Y. Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. In NIPS , 2012. 4.1 Arora, S., Ge, R., and Moitra, A. Learning topic mod-els  X  going beyond svd. In FOCS , 2012. 2, 4.2 Kivinen, Jyrki and Warmuth, Manfred K. Exponenti-ated gradient versus gradient descent for linear pre-dictors. Inform. and Comput. , 132, 1995. 4.3 Nocedal, J. and Wright, S. J. Numerical Optimization . Springer, New York, 2nd edition, 2006. 4.3 Wedin, P. Perturbation bounds in connection with sin-gular value decomposition. BIT Numerical Mathe-
