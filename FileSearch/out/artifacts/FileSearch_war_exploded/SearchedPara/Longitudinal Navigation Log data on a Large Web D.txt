 We have collected the access logs for our university X  X  web domain over a time span of 4.5 years. We now release the pre-processed data of a 3-month period for research into user navigation behavior. We preprocessed the data so that only successful GET requests of web pages by non-bot users are kept. The resulting 3-month collection comprises 9.6M page visits (190K unique URLs) by 744K unique visitors.
  X  Information systems  X  Web log analysis; Traffic analysis; Web searching and information discovery; data collection; access logs; navigation behavior; link analy-sis; graph clustering
Finding information on the web requires a combination of searching and browsing. Browsing a large web domain can sometimes be challenging because traditional navigation structures do not support a complex hierarchy of pages and the possibility of multiple entry points [4]. In this paper we address the navigation behavior of users on a so-called mega-site . Mega-sites are extremely large, many levels deep, made up of many subsections, cater to many audiences and have multiple entry points [4]. Organizations with mega-sites are typically large organizations with many divisions and multiple target groups. Universities are prototypical examples of such organizations.

We present a newly released data collection that comprises longitudinal navigation log data on a mega-site: a university web domain. The complete data set consists of 4.5 years of access data. A first set of 3 months will now be released as Table 1: Statistics for the data collection, showing the # of entries in raw sample 164,923,821 # of entries after success status filtering 121,840,127 # of entries after request type filtering 115,866,318 # of entries after file type filtering 12,963,852 # of entries after web bot filtering 9,633,438 # of entries after anonymization 9,555,418 # of unique users in sample 744,340 # of unique URLs in sample 190,457 a resource for research. The data collection is unique in two ways: (1) its size: the pre-processed data of only 3 months comprises almost 10 Million page visits; (2) its richness: all information from the original access log entries, including URL strings, referrers and timestamps, has been preserved.
The data collection is valuable for a range of research top-ics: user navigation, browsing and stopping behavior, eval-uation of clustering techniques on large (directed) graphs, evaluation of link prediction and page recommendation tech-niques, clustering of web users based on their online activi-ties, evaluation of content-based and navigation-based user profiles, and the evaluation of usability of mega-sites.
In this paper, we describe the preprocessing of the data, present some statistics of the resulting data collection and show two examples of research that can be conducted with this data collection: link prediction and graph clustering.
We have been collecting (and are still collecting) server access logs since September 2011 on the web domain of our university. The data is constantly growing, with a speed of more than 50 Million page visits per month. The data that we make available for research covers three months of navi-gation logs: from October 1st to December 31st 2014. The raw data comprises 156M page visits (1.3M unique URLs).
The log data are formatted as Apache log files. 1 We fil-tered the raw data as follows: We removed all requests that did not result in a successful response (status codes starting
See https://httpd.apache.org/docs/1.3/logs.html for a def-inition of the format. with 3 or higher); all requests that are no GET requests; and all requests for images and other files that do not result from a navigational process. 2 In addition, we removed all re-quests that supposedly come from web bots, using the regu-lar expression .*(Yahoo! Slurp|bingbot|Googlebot).* on the log entry. We anonymized the data by taking the fol-lowing measures: The effect of each of the filtering steps is shown in Table 1. The information that is retained per entry is: unique user id, timestamp, GET request (URL), status code, the size of the object returned to the client, and the referrer URL. A sam-ple of the resulting data is shown in Figure 1. The sample illustrates that the content (URLs and referrers) is multi-lingual: predominantly Dutch, and English and German in smaller proportions.
The final size of the collection is 9.6M page visits (190K unique URLs) by 744K unique visitors. Figure 2 shows the distribution of the number of occurrences of GET requests in the collection, on a log-log scale. The plot indicates the expected long-tail (power law) distribution: very few pages occur highly frequently, and many occur infrequently.
All files with one of the extensions axd , aspx , bmp , doc , docx , jpg , jpeg , gif , css , js , png , sav , swf , ttf, txt , xls , xlsx , xml , zip , ppt , pptx , ico
The Electronic Frontier Foundation shows the potential of this information: https://panopticlick.eff.org/ Figure 2: Distribution of the number of occurrences of The data collection is available to researchers through DANS (Data Archiving and Networked Services). 4 Users of the dataset are asked to sign a data agreement stating that they will use the dataset for research purposes, that they will not attempt to discover the identities of the per-sons whose navigation behavior is logged in the collection, and that they will erase the data after completion of their research.
In this section, we describe the results of two experiments with the data, one addressing the task of link prediction and one addressing the task of URL clustering. Both tasks have only been evaluated before on much smaller datasets. The results presented here demonstrate the utility of the new data collection for the evaluation and validation of graph methods on large data sets.
Our first goal was to investigate how well we could predict the next request of a user based on the navigation behavior of other users. This task is called link prediction. Link prediction can be employed for recommending relevant pages to the user and/or webmaster and thereby improving the user experience on a web domain. We use a subset of our data for our experiments: one week of logs (7 days) were used as training data, and one day (the 8th day) was used as test data. This resulted in a training set with 35,000 unique URLs and 85,000 unique users. Although this is a http://dx.doi.org/10.17026/dans-28m-mwht very small portion of our data, it is much larger than the data sets used in previous work on link prediction, with 100 to 1000 unique URLs [8, 11].

Method. We adopted the most commonly used method for link prediction: Markov chains [8, 11]. A Markov chain describes a system of transitions between states, where the probability of going to a state only depends on the previous state. In our model, the states represent the visited pages and the transitions represent the probability of a user visit-ing two pages directly after each other.

Let s ( t  X  1) be the vector representing the state during time t  X  1, and A the transition matrix with the probabilities of going from one state to another. Then the probability of going to s ( t ) is computed with: The transition matrix A is computed with: where A ( s, s 0 ) is the probability of going from state s to state s 0 , and C ( s, s 0 ) is the number of times s preceeds s in the training data [8]. The resulting transition matrix for our data is sparse.

We investigated the effect of the history size (using 1, 2 or 3 previously visited pages) on the quality of prediction of the next page. We compared two interpolation strategies from the literature [8, 11] for combining the previous stages: where s ( t ) denotes the probability of going to state s during time t , i t is the vector representing the state, with a proba-bility 1 at that state at that time, and a i are user specified weights; we use uniform weights in our experiment.

Experimental setup. The requests in the training and test data were grouped by user and sorted by timestamp, resulting in a full path of requested pages per user. We di-vided these paths into sessions: after at least thirty minutes of inactivity a new session was started. We used sessions from 1,000 randomly selected users from the test data for evaluation; we repeated the experiment five times with a different random seed and report the average accuracy over the seeds. We vary the size of the history (1, 2 or 3 previ-ous pages) and the interpolation strategy for previous stages (summation or maximization).

For each page in the test data that has sufficient history in its session, our algorithms try to predict the next visited page. Note that the interpolation over previous stages func-tions as a back-off strategy for missing links in the training data: if we try to predict the navigation sequence A  X  B  X  C , after seeing A  X  B in the test data, it might be that the path A  X  B  X  C is not present in the training data, but the path B  X  C is. In both interpolation strategies, the probability of B  X  C is a factor in prediction. However, if C does not occur in the training data at all, predicting C is not possible. In our experiment, 3% of the URLs in the test data does not occur in the training data.

Results. We let the Markov chain return the page pre-dictions in decreasing order of probability. We evaluated the prediction quality in terms of Success@1 and Success@3: Table 2: Evaluation of link prediction (Success@1 and the proportion of predictions with the correct next URL re-turned in the top-1 or the top-3 respectively. The results, presented in Table 2, show that there is no clear preference for one interpolation method over the other. If we use a his-tory of three URLs in the session, 90% of the returned URL suggestions has the correct next URL in the top-3. This is a result that would be useful in a recommendation setting on the web domain: providing the user with three suggestions for the next URL to visit.

Interestingly, the results for a history of two URLs are worse than the results for a history of one URL, even though the interpolation of previous stages ensures a back-off mech-anism. We speculate that there are many sessions with two URLs  X  the first URL serving as history for the second  X  and these are relatively easy to predict, compared to the sessions with three URLs, where there is more variety in the history: If A  X  B  X  C is not present in the training data, but A  X  B  X  X and A  X  B  X  Y are, with high probabilities, then the back-off probability of B  X  C cannot compensate for the wrong X and Y predictions.

This initial result shows the potential of link prediction on the navigation graph, and provides future directions for research into the use of Markov chains on large directed graphs, and the modeling of user navigation behavior.
In a second study, we investigated the clustering of web pages based on the transitions of visitors between URLs. In previous work, techniques for clustering navigation pat-terns were only evaluated on smaller data sets [9, 10]. Most graph clustering techniques that can handle large graphs have been developed for undirected graphs (for community detection) [6, 5]. Spectral clustering methods are often used to cluster directed graphs, but applying them to large graphs is not feasible [5].

Method. A solution to our problem is provided by Ros-vall and Bergstrom (2008) [7]. They use a random walk method to find a good clustering, by compressing the bit sequence of a random walk with Huffman coding. Huffman coding is a compression technique that assigns short code-words to common bit sequences and long codewords to rare ones; the most frequently visited nodes in a random walk get the shortest codes. The average number of bits to de-scribe one step in the graph is a measure for the quality of the clustering. The idea is that a good clustering will re-sult in a better compression for the bit sequence than a bad clustering, because when performing a random walk, one is more likely to stay in a good cluster for a long time before leaving. A clustering that minimizes the entropy of the bit sequence would be optimal. Finding the minimal entropy can be achieved with the Louvain algorithm, which greedily Table 3: The first 6 URLs from the five largest clus-minimizes the entropy in O ( n log n ) (with n being the num-ber of nodes). In the case of sparse graphs, its complexity appears to be linear with the number of nodes [3].

We cluster the transition matrix A from the Markov chain, in order to get a clustering of the web pages in the data col-lection. The already available transition probabilities from one node to another are used to construct the random walk. One parameter that needs to be set is the teleportation prob-ability, which is the probability to jump to a random node in the graph at any time. This is needed when the random walk gets stuck in dead ends or cycles. We set the tele-portation probability to 0.15, which is the same value as in Google X  X  PageRank algorithm and proposed in the paper by Rosvall and Bergstrom [7].

Experimental setup. We again used the sample of one week for these experiments, comprising 35,000 unique URLs. Note that we only use transitions between pages for the clus-tering, not the URL strings or the content of the URLs. Results. 2,915 clusters were generated for the set of URLs in our sample. The majority of clusters contains only one or two URLs, which is probably caused by the sparsity of the graph. We manually went through the clusters and we found that the 20 largest clusters represent sensible groups of URLs. As illustration, Table 3 shows the first 8 URLs from the five largest clusters. Clusters 2, 3, 4 and 5 turn out to comprise specific subdomains. Cluster 1 (which is the largest cluster) contains general and often-visited pages.
This initial result provides future directions for research into the development of clustering techniques on large di-rected graphs, especially for modeling user navigation be-havior, and clustering users into visitor groups.
We have prepared a data collection of user interaction logs with a university web site. The collection comprises 3 months of data, filtered for unsuccessful requests, download file types and web bots, and anonymized while preserving unique visitor IDs. The size of the preprocessed data is 9.6M page visits (190K unique URLs) by 744K unique visi-tors. The data collection allows for research on, among other things, user navigation, browsing and stopping behavior and web user clustering. We have conducted two exploratory studies on a one-week subset of the data, directed at link prediction and graph clustering. The results show the po-tential of the data, especially given its size and its richness.
For experiments that require larger data sets it will be possible to release more data in the future: we have been collecting data for 4.5 years and the raw data is growing at a speed of 50 Million page visits per month. 5 In addition, we will investigate the possibility to release the domain X  X  query logs together with the navigation log data.
 This publication was supported by the Dutch national pro-gram COMMIT (project P7 SWELL). [1] Adar, E.: User 4xxxxx9: Anonymizing query logs. In: [2] Antonellis, I., Garcia-Molina, H., Karim, J.: Tagging [3] Blondel, V.D., Guillaume, J.L., Lambiotte, R., [4] Boag, P.: Navigation for mega-sites. Smashing [5] Fortunato, S.: Community detection in graphs. [6] Malliaros, F.D., Vazirgiannis, M.: Clustering and [7] Rosvall, M., Bergstrom, C.T.: Maps of random walks [8] Sarukkai, R.R.: Link prediction and path analysis [9] Smith, K.A., Ng, A.: Web page clustering using a [10] Xu, J., Liu, H.: Web user clustering analysis based on [11] Zhu, J., Hong, J., Hughes, J.G.: Using Markov chains
Please contact the authors if you are interested in obtaining a larger collection.
