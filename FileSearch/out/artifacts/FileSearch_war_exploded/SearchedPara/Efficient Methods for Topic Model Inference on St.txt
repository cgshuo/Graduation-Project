 Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace. Fitting a topic model given a set of training documents requires approximate inference tech-niques that are computationally expensive. With today X  X  large-scale, constantly expanding document collections, it is useful to be able to infer topic distributions for new doc-uments without retraining the model. In this paper, we empirically evaluate the performance of several methods for topic inference in previously unseen documents, including methods based on Gibbs sampling, variational inference, and a new method inspired by text classification. The classification-based inference method produces results similar to iterative inference methods, but requires only a single matrix multi-plication. In addition to these inference methods, we present SparseLDA, an algorithm and data structure for evaluat-ing Gibbs sampling distributions. Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods, while also using substan-tially less memory.
 H.4 [ Information Systems Applications ]: Miscellaneous Experimentation, Performance, Design Topic modeling, inference
Statistical topic modeling has emerged as a popular method for analyzing large sets of categorical data in applications from text mining to image analysis to bioinformatics. Topic Copyright 2009 ACM 978-1-60558-495-9/09/06 ... $ 5.00. models such as latent Dirichlet allocation (LDA) [3] have the ability to identify interpretable low dimensional components in very high dimensional data. Representing documents as topic distributions rather than bags of words reduces the ef-fect of lexical variability while retaining the overall semantic structure of the corpus.

Although there have recently been advances in fast infer-ence for topic models, it remains computationally expensive. Full topic model inference remains infeasible in two common situations. First, data streams such as blog posts and news articles are continually updated, and often require real-time responses in computationally limited settings such as mobile devices. In this case, although it may periodically be possi-ble to retrain a model on a snapshot of the entire collection using an expensive  X  X ffline X  computation, it is necessary to be able to project new documents into a latent topic space rapidly. Second, large scale collections such as information retrieval corpora and digital libraries may be too big to pro-cess efficiently. In this case, it would be useful to train a model on a random sample of documents, and then project the remaining documents into the latent topic space inde-pendently using a MapReduce-style process. In both cases there is a need for accurate, efficient methods to infer topic distributions for documents outside the training corpus. We refer to this task as  X  X nference X , as distinct from  X  X itting X  topic model parameters from training data.

This paper has two main contributions. First, we present a new method for topic model inference in unseen documents that is inspired by techniques from discriminative text clas-sification. We evaluate the performance of this method and several other methods for topic model inference in terms of speed and accuracy relative to fully retraining a model. We carried out experiments on two datasets, NIPS and Pubmed. In contrast to Banerjee and Basu [1], who evaluate different statistical models on streaming text data, we focus on a sin-gle model (LDA) and compare different inference methods based on this model. Second, since many of the methods we discuss rely on Gibbs sampling to infer topic distributions, we also present a simple method, SparseLDA, for efficient Gibbs sampling in topic models along with a data structure that results in very fast sampling performance with a small memory footprint. SparseLDA is approximately 20 times faster than highly optimized traditional LDA and twice the speedup of previously published fast sampling methods [7].
A statistical topic model represents the words in docu-ments in a collection W as mixtures of T  X  X opics, X  which are multinomials over a vocabulary of size V . Each docu-ment d is associated with a multinomial over topics  X  d . The probability of a word type w given topic t is represented by  X  w | t . We refer to the complete V x T matrix of topic-word probabilities as  X . The multinomial parameters  X  d and  X  t are drawn from Dirichlet priors with parameters  X  and  X  respectively. In practice we use a symmetric Dirichlet with  X  = . 01 for all word types and a T -dimensional vector of distinct positive real numbers for  X  . Each token w i in a given document is drawn from the multinomial for the topic represented by a discrete hidden indicator variable z i .
Fitting a topic model given a training collection W in-volves estimating both the document-topic distributions,  X  and the topic-word distributions,  X . MAP estimation in this model is intractable due to the interaction between these terms, but relatively efficient MCMC and variational meth-ods are widely used [4, 3]. Both classes of methods can produce estimates of  X , which we refer to as  X   X . In the case of collapsed Gibbs sampling, the Markov chain state consists of topic assignments z for each token in the training corpus. An estimate of P ( w | t ) can be obtained from the predictive distribution of a Dirichlet-multinomial distribution: where n w | t is the number of tokens of type w assigned to
The task of topic model inference on unseen documents is to infer  X  for a document d not included in W . The likelihood function for  X  is where n t | d is the total number of tokens in the document assigned to topic t . Even with a fixed estimate  X   X , MAP estimation of  X  is intractable due to the large number of discrete hidden variables z . Sections 3 through 5 present an array of approximate inference methods that estimate  X  which are empirically evaluated in the remaining sections of the paper.
We evaluate three different sampling-based inference meth-ods for LDA. Gibbs sampling is an MCMC method that in-volves iterating over a set of variables z 1 , z 2 , ...z each z i from P ( z i | z \ i , w ). Each iteration over all variables is referred to as a Gibbs sweep. Given enough iterations, Gibbs sampling for LDA [4] produces samples from the pos-terior P ( z | w ). The difference between the three methods we explore is in the set of variables z that are sampled, as illustrated in Figure 1, and which portion of the complete data is used in estimating  X   X .
In this method we define the scope of a single Gibbs sweep to be the hidden topic variables for the entire collection, including both the original documents and the new docu-ments. After sampling the topic variables for the training documents to convergence without the new documents, we randomly initialize topic variables for the new documents Figure 1: The three Gibbs sampling-based methods it-and continue sampling over all documents until the model converges again. We can then estimate the topic distribu-tion  X  d for a given document given a single Markov chain state as where n  X | d is the length of the document. Increasingly ac-curate estimates can be generated by averaging over values of Eq. 4 for multiple Markov chain states, but may cause problems due to label swapping.

Inference method Gibbs1 is equivalent to fitting a new model for the complete data, including both the original documents and the new documents, after separately initial-izing some of the topic variables using Gibbs sampling. This method is as computationally expensive as simply starting with random initializations for all variables. It is useful, however, in that we can use the same initial model for all other inference methods, thereby ensuring that the topics will roughly match up across methods. We consider this inference procedure the most accurate, and the topic distri-bution for each test document estimated using Gibbs1 as a reference for evaluating other inference methods.
Inference method Gibbs2 begins with the same initializa-tion as Gibbs1, but saves computation by holding all of the topic assignments for the training documents fixed. Under this approximation, one Gibbs sweep only requires updating topic assignments for the new documents.

Inference involves sampling topic assignments for the train-ing data as in Gibbs1, randomly assigning values to the topic indicator variables z for the new documents, and then sam-pling as before, updating  X   X  as in Eq. 1 after each variable update.
Inference method Gibbs2 performs Gibbs sampling as a batch. As such, it samples from the posterior distribution over all z variables in the new documents given all the words in the new documents, accessing all the new documents in each iteration. Unfortunately, handling topic inference in a batch manner is both unrealistic in time-sensitive stream-ing document collections, and inefficient because it cannot be parallelized across documents without substantial inter-process communication.

Gibbs3 is an online version, which processes all docu-ments independently. When a test document arrives, we sample topics for a number of iterations using only topic-word counts in  X   X  from the training corpus and the current document. For the next incoming document we reset  X   X  to include only counts from the training corpus and that new document.

This algorithm differs from the previous two methods in that it produces estimates of  X  d given only the words in the training documents and in document d . Gibbs1 and Gibbs2 produce estimates given the entire data set.
The efficiency of Gibbs sampling-based inference meth-ods depends almost entirely on how fast we can evaluate the sampling distribution over topics for a given token. We therefore present SparseLDA, our new algorithm and data structure that substantially improves sampling performance. Although we apply this method to topic inference on new documents, the method is applicable to model fitting as well.
The probability of a topic z in document d given an ob-served word type w is
Sampling from this distribution involves calculating the unnormalized weight in Eq. 5, which we refer to as q ( z ), for each topic; sampling a random variable U  X  U (0 , P z q ( z )); and finding t such that P t  X  1 z =1 q ( z ) &lt; U &lt; P algorithm requires calculating q ( z ) for all topics in order to determine the normalizing constant for the distribution P z q ( z ), even though probability mass is generally concen-trated on a small number of topics. Porteous et al. [7] approach this problem by iteratively refining an approxima-tion to P z q ( z ). We take an arguably simpler approach by caching most of the computation required to compute the normalizing constant. By rearranging terms in the numera-tor, we can divide Eq. 5 into three parts: P ( z = t | w )  X   X  t  X 
Note that the first term is constant for all documents and that the second term is independent of the current word type w . Furthermore, P z q ( z ) is equal to the sum over topics of each of the three terms in Equation 6:
This process divides the full sampling mass into three  X  X uckets. X  We can now sample U  X  U (0 , s + r + q ). If U &lt; s , we have hit the  X  X moothing only X  bucket. In this case, we can step through each topic, calculating and adding than x . If s &lt; x &lt; ( s + r ), we have hit the  X  X ocument topic X  bucket. In this case, we need only iterate over the set of topics t such that n t | d 6 = 0  X  a number that is usually substantially less than the total number of topics. Finally, if x &gt; ( s + r ), we have hit the  X  X opic word X  bucket, and we need only consider topics such that n w | t 6 = 0. Again, this number is usually very small compared to T .

The values of the three components of the normalization constant, s, r, q , can be efficiently calculated. The constant s only changes when we update the hyperparameters  X  . The constant r depends only on the document-topic counts, so we can calculate it once at the beginning of each document and then update it by subtracting and adding values for the terms involving the old and new topic at each Gibbs update. This process takes constant time, independent of the number of topics.

The topic word constant q changes with the value of w , so we cannot as easily recycle earlier computation. We can, however, substantially improve performance by observing that the expression for q can be broken into two components: topic, so calculating q for a given w consists of one multiply operation for every topic such that n w | t 6 = 0. As n t | d for almost all topics in any given document, this vector of coefficients will also almost entirely consist of only  X  t so we can save additional operations by caching these coef-ficients across documents, only updating those topics that have non-zero counts in the current document as we begin each document, and resetting those values to the  X  -only values as we complete sampling for each document.
If the values of  X  and  X  are small, q will take up most of the total mass. Empirically, we find that more than 90% of sam-ples fall within this bucket. In a Dirichlet-multinomial dis-tribution with small parameter magnitudes, the likelihood of the distribution is roughly proportional to the concentra-tion of the counts on a small number of dimensions. We find that the wallclock time per iteration is roughly proportional to the likelihood of the model. As the sampler approaches a region of high probability, the time per iteration declines, leveling off as the sampler converges.

Clearly, the efficiency of this sampling algorithm depends on the ability to rapidly identify topics such that n w | t Furthermore, as the terms in Eq. 10 are roughly propor-tional to n w | t , and since we can stop evaluating terms as soon as the sum of terms exceeds U  X  ( s + r ), it is desir-able to be able to iterate over non-zero topics in descending order. We now present a novel data structure that meets these criteria.

We encode the tuple ( t, n w | t ) in a single 32 bit integer by dividing the bits into a count segment and a topic segment. The number of bits in the topic segment is the smallest m such that 2 m  X  T . We encode the values by shifting n w | t left by m bits and adding t . We can recover n w | t by shift-ing the encoded integer right m bits and t by a bitwise and with a  X  X opic mask X  consisting of m 1s. This encoding has two primary advantages over a simple implementation that stores n w | t in an array indexed by t for all topics. First, in natural languages most word types occur rarely. As the Figure 2: A comparison of time and space efficiency encoding no longer relies on the array index, if a word type w only occurs three times in the corpus, we need only re-serve an array of at most three integers for it rather than T . Second, since the count is in the high bits, we can sort the array using standard sorting implementations, which do not need to know anything about the encoding.

By storing encoded values of ( t, n w | t ) in reverse-sorted ar-rays, we can rapidly calculate q , sample U , and then (if U &gt; ( s + r )) usually within one calculation find the sampled t . Maintaining the data structure involves reencoding val-ues for updated topics and ensuring that the encoded values remain sorted. As n w | t changes by at most one after every Gibbs update, a simple bubble sort is sufficient.
In order to evaluate the efficiency of SparseLDA, we mea-sured the average time per iteration and memory usage, 1 shown in Figure 2 averaged over five runs for each value of T on NIPS data. As we increase the number of topics, the time per iteration increases slowly, with much lower overall time than a carefully optimized implementation of standard Gibbs sampling. Memory usage is also substantially lower and grows more slowly than standard Gibbs sampling. Em-pirical comparisons with the FastLDA method of Porteous et al. [7] are difficult, but on the standard NIPS corpus with 800 topics, we found the ratio of per-iteration times between standard Gibbs sampling and SparseLDA to be ap-proximately 20, compared to a speedup of approximately 11 times reported for FastLDA over standard Gibbs sampling using the same corpus and number of topics with  X  t = 0 . 001. In addition, FastLDA does not address memory efficiency, one of the primary benefits of SparseLDA.
Another class of approximate inference method widely used in fitting topic models is variational EM. Variational in-ference involves defining a parametric family of distributions
Memory use in Java was calculated as runtime.totalMemory() -runtime.freeMemory() that forms a tractable approximation to an intractable true joint distribution. In the case of LDA, Blei, Ng, and Jordan [3] suggest a factored distribution consisting of a variational Dirichlet distribution  X   X  d for each document and a varia-tional multinomial  X   X  di over topics for each word position in the document. The parameters of these distributions can then be iteratively fitted using the following update rules: where  X ( x ) is the digamma function. Variational approxi-mations converge in fewer iterations than Gibbs sampling, but each iteration generally takes substantially longer due to the calculation of exp and  X  functions.

We evaluate a variational method based on the update rules specified. Given a new document, we initialize  X   X   X  and clamp  X   X  to the expression in Eq. 1 evaluated us-ing the topic-word counts from the training documents. We then apply the update rules in Eq. 11 and 12 in turn until convergence. We then estimate  X   X  t  X   X   X  t . This method is a variational equivalent to inference method Gibbs3, being appropriate for parallelized, streaming environments. More complicated variational distributions have been shown to have lower bias [9], but are generally more computationally intensive per iteration than the simple fully factored varia-tional distribution.
The previous inference methods theoretically require many iterations to allow a Markov chain to achieve its stationary distribution or to fit a variational approximation. In some settings, however, it is necessary to estimate a topic dis-tribution as fast as possible. As an alternative to iterative methods, we propose a classification-based approach to pre-dicting  X  for a new document.

In this task, each document has a labeled topic distribu-tion, obtained from the trained topic model. Specifically, each document can be classified as multiple classes, each with a probability. This setting is an extension to the tradi-tional classification task, in which an instance has only one class membership. In a traditional classifier, the training data is represented as D = { ( x i , y i ) | i = 1 . . . n } , where x is a representation of the document (e.g. a feature vector) and y i is x i  X  X  label. y i is typically a vector of indicator variables, such that if x i belongs to the j th class, only the j th element of y i will be 1, and other elements will be 0. For simplicity, we often use the class label instead of the indicator vector to represent y i . For our task, we allow each element of y i to take values between 0 and 1 and the sum of all elements should be 1, so that y i indicates a probabilis-tic distribution over all classes. Classes in this setting are equivalent to topics. We investigate two classifiers in this paper, one of which is useful only as a baseline.
We first extend a traditional MaxEnt or logistic regression classifier to our new task. In the simple scenario in which each instance has one fully observed class label, any real-valued function f i ( x , y ) of the object x and class y can be treated as a feature . The constraints are the expected val-ues of features computed using training data. Given a set Y of classes, and features f k , the parameters to be estimated  X  , the learned distribution p ( y | x ) is of the parametric ex-ponential form [2]: Given training data D = { X  x 1 , y 1  X  ,  X  x 2 , y 2  X  , ...,  X  x log likelihood of parameters  X  is The last term represents a zero-mean Gaussian prior on the parameters, which reduces overfitting and provides identifi-ability. We find values of  X  that maximize l ( X  | D ) using a standard numerical optimizer. The gradient with respect to feature index k is
In the topic distribution labeling task, each data point has a topic distribution, and is represented as ( x i , y can also use the maximum log likelihood method to solve this model. The only required change is to substitute y for y . Using a distribution changes the empirical features of the data ( f k ( x i , y i )), also known as the constraints in a maximum entropy model, which are used to compute the gradient. Whereas in a traditional classifier we use f k ( x as empirical features, we now use f k ( x i , y i ) instead, where y i is the labeled topic distribution of the i th data point. Suppose that we have two classes (i.e. topics) and each in-stance can contain two features (i.e. words). Training data might consist of x = ( x 1 , x 2 ) , y = 1 for a traditional clas-classifier, such that p 1 and p 2 are the proportions of topic 1 and topic 2 for data point x and p 1 + p 2 = 1. Empirical features (sufficient statistics of training data) for traditional classifier would be ( x 1 , 1) and ( x 2 , 1). While the empirical features for a topic distribution classifier would be ( x ( x 2 , 1), ( x 1 , 2), and ( x 2 , 2), with the first two weighted by p , and the remaining two weighted by p 2 . This substitution changes the penalized log likelihood function: Correspondingly, the gradient at feature index k is:  X  ( X  | D ) Where p i ( y ) stands for the probability of topic y in the cur-rent instance, i.e. one of the elements of y i .

Once we have trained a topic proportion classifier, we can use it to estimate  X  for a new document. We compute the scores for each topic using Eq. 13. This process is essentially a table lookup for each word type, so generating  X   X  requires a single pass through the document.

In experiments, we found that the output of the topic pro-portion classifier is often overly concentrated on the single largest topic. We therefore introduce a temperature param-eter  X  . Each feature value is weighted by 1  X  . Values of  X  &lt; 1 increase the peakiness of the classifier, while values  X  &gt; 1 decrease peakiness. We chose 1.2 for NIPS data and 0.9 for Pubmed data based on observation of the peakiness of predicted  X   X  values for each corpus.
From the trained topic model, we can estimate  X , a matrix with T (#topics) rows and W (#words) columns represent-ing the probability of words given topics. Combined with a uniform prior over topic distributions, we can use this ma-trix as a classifier, similar to the classifier we obtained from MaxEnt. This method performs poorly, and is presented only as a baseline in our experiments. A document d is represented as a vector, with each element an entry in the vocabulary, denoted as w and the value as the number of times that word occurs in the document, denoted as n w | d Using Bayes X  rule, the score for each topic is: The estimated  X  distribution is then simply the normalized scores.

In experiments, we compare both classification methods against the inference methods discussed in previous sections. The two classifiers take less time to predict topic distribu-tions, as they do not require iterative updates. Provided they can achieve almost the same accuracy as the three in-ference methods or their performance is not much worse, for some particular task which requires real-time response, we can choose classification-based inference methods instead of sampling based or variational updated methods. The choice of estimator can be a trade-off between accuracy and time efficiency.
A hybrid classification/sampling-based approach can be constructed by generating an estimate of  X  d given w d using the MaxEnt classifier and then repeatedly sampling topic indicators z given  X   X  and  X   X . Note that given  X   X  , P ( z  X   X   X   X  w | t is independent of all z \ i . After the initial cost of set-ting up sampling distributions, sampling topic indicators for each word can be performed in parallel and at minimal cost. After collecting sampled z indicators, we can re-estimate the topic distribution  X   X  according to the topic assignments as in Eq. 4. In our experiments, we find that this re-sampling pro-cess results in more accurate topic distributions than Max-Ent alone.
In this section we empirically compare the relative accu-racy of each inference method. We train a topic model on training data with a burn-in period of 1000 iterations for all inference methods. We explore the sensitivity of each method to the number of topics, the proportion between Table 1: Top five topics predicted by different meth-ods for a testing document training documents and  X  X ew X  documents, and the effect of topic drift in new documents.

We evaluate different inference methods using two data sets. The first is 13 years of full papers published in the NIPS conference, in total 1,740 documents. The second is a set of 51,616 journal article abstracts from Pubmed. The NIPS data set contains fewer documents, but each document is longer. NIPS has around 70K unique words and 2.5M tokens. Pubmed has around 105.4K unique words and about 7M tokens. We also carried out experiments on New York Times data from LDC. We used the first six months of 2007, comprising 39,218 documents, around 12M tokens, about 900K unique words. We preprocessed the data by removing stop words.

We implemented the three sampling-based inference meth-ods (using SparseLDA), the variational updated method, and the classification-based methods in the MALLET toolkit [5]. They will be available as part of its standard open-source release.
It is difficult to evaluate topic distribution prediction re-sults, because the  X  X rue X  topic distribution is unobservable. We can, however, compare different methods with each other. We consider the Gibbs1 inference method to be the most ac-curate, as it is closest to Gibbs sampling over the entire cor-pus jointly, a process that is guaranteed to produce samples from the true posterior over topic distributions. In order to determine whether sampling to convergence is necessary, for inference methods Gibbs2 and Gibbs3, we report results using 1000 iterations of sampling (Gibbs2 and Gibbs3) and two iterations (Gibbs2.S and Gibbs3.S), which is the mini-mum number of Gibbs sweeps for all topic indicators to be sampled using information from all other tokens.
 We represent the prediction results of each method as a T -dimensional vector  X   X  d for each document. We compare methods using three metrics. In all results we report the average of these measures over all  X  X ew X  documents. 1. Cosine distance This metric measures the angle be-2. KL Divergence Another metric between distribu-3. F1 The previous two metrics measure the divergence For classification based inference methods we use unigram counts as input features. Normalized term frequency fea-tures (term counts normalized by document length) pro-duced poorer results. We also tried including word-pair features, on the intuition that the power of topic models comes from cooccurrence patterns in words, but these fea-tures greatly increased inference time, never improved re-sults over unigram features, and occasionally hurt perfor-mance. We hypothesize that the power of unigram features in the discriminatively trained MaxEnt classifier may be a result of the fact that the classifier can assign negative weights to words as well as positive weights. This capability provides extra power over the Na  X  X ve Bayes classifier, which cannot distinguish between words that are strongly nega-tively indicative of a topic and words that are completely irrelevant.
We first compare each method to the Gibbs1 inference method, which is equivalent to completely retraining a model given the original data and new data. We split the NIPS data set into training and testing documents in a 7:3 ratio and run an initial model on the training documents with 70 topics. We explore the effect of these settings later in this section.
 Figure 3 shows results for the three evaluation metrics. The converged sampling methods Gibbs2 and Gibbs3 are closest to Gibbs1 in terms of cosine distance, F1, and KL divergence, but do not exactly match. The two-iteration ver-sions Gibbs2.S and Gibbs3.S are close to Gibbs1 in terms of cosine distance and KL divergence, but MaxEnt and Vari-ational EM are closer in terms of F1. Hybrid MaxEnt con-sistently outperforms MaxEnt. Figure 4 shows similar mea-sures vs. Gibbs2, arguably a more meaningful comparison Figure 5: Examples of topic distribution obtained by as Gibbs2 does not resample topics for the training docu-ments. These results indicate that the MaxEnt inference method provides a reasonable ordering of topics for a new document, but that if the exact topic proportions are re-quired, two-iteration Gibbs sampling and hybrid MaxEnt should be considered.

We analyzed errors in precision and recall for the MaxEnt inference method. We discovered that for some documents, recall is high. In this case, MaxEnt can predict the ma-jor topics correctly, but it will include more topics within the 80% threshold, each with a lower weight compared to Gibbs1, Gibbs2 and Gibbs3. For some documents, preci-sion is high. In this case, MaxEnt assigns more weight to the most prominent topics, causing other topics to fall out-side the threshold. To demonstrate the comparison between topic proportions proposed by different inference methods, we show values of  X   X  for a single document under different inference methods in Figure 5. The proportions assigned by the MaxEnt inference method roughly match those of the sampling-based inference methods, especially compared to the na  X  X ve Bayes inference method.

We next compare the efficiency in time of each inference method. We perform experiments on a cluster of 2.66GHz Xeon X5355 processors. Figure 6 shows on a log scale that the methods can be loosely grouped into  X  X ast X  methods (classification-based, two-iteration sampling, and hybrid Max-Ent) that require a small number of passes through the data and use simple computations, and  X  X low X  methods that re-quire many iterations and complicated functions and 50-200 times slower.

Figure 7 shows the F1 measure of different methods given different values of T . The results indicate that the  X  X ast X  methods are relatively insensitive to the number of topics. Thus for more topics, the slow methods will be more than 100-200 times slower.

One scenario for topic model inference is in very large col-lections. Even with fast sampling methods, training topic models remains computationally expensive. Parallel im-plementations are hindered by the need for frequent inter-process communication [6]. We would like to be able to train Figure 7: Performance of different methods VS. Topic Figure 8: Performance VS. Training proportion for a model on a representative subset of a collection and then estimate topic distributions for the remaining documents us-ing light-weight MapReduce-style processing, in which every document can be handled independently. In order to study whether training size will affect prediction performance, we carried out experiment on Pubmed. We fixed 30% of the whole data as test data, and vary the training proportion from 10% to 70%. Results are shown in Figure 8 for the most light-weight inference method, MaxEnt, relative to Gibbs2. Although the size of the training set does affect performance, the magnitude of variation is relatively small.

Another application area is in streaming document col-lections. In this setting new documents may arrive more quickly than it is possible to train a complete model. Alter-natively, we may wish to train a topic model on a central compute cluster and then  X  X ublish X  a model to distributed devices that do not have the computational power to per-form full topic inference, for example in an email tagging application on a smart phone. One difficulty in this set-ting is that the underlying topic distribution may shift over Figure 9: The effect of  X  X opic drift X  on MaxEnt perfor-time, rendering the trained model less applicable. One eas-ily measured proxy for topic drift is the proportion of words that are included in the training data. We analyze how the percentage of previously seen vocabulary affects the perfor-mance of topic distribution prediction on Pubmed. On the left side of Figure 9 the x axis is the ratio of unique words seen in training data to the number of unique words in test data, and the y axis is the values of all evaluation measures. Again, we present only results for the most light-weight in-ference method, MaxEnt. The performance of the estimator increases as the proportion of previously seen vocabulary in-creases, leveling off at around 70% for cosine and F1 met-rics (the KL divergence metric appears more noisy). This result suggests a simple heuristic for determining when a new model should be retrained, i.e. the training set should contain roughly 70% of the distinct word types in the  X  X ew X  documents.
 We also explore the effect of topic drift in news data in the New York Times data set, using June 2007 documents as test data. We use five training corpora: Jan 2007, Jan and Feb, etc. Retraining after each month improves performance, as shown on the right side of Figure 9.
Topic models provide a useful tool in analyzing compli-cated text collections, but their computation complexity has hindered their use in large-scale and real-time applications. Although there has been recent work on improving training-time estimation in topic models [6, 7], there has been little attention paid to the practically important problem of in-ferring topic distributions given existing models. Exceptions include the dynamic mixture model for online pattern dis-covery in multiple time-series data [10], the online LDA al-gorithm [8], and especially the work of Banerjee and Basu [1], who advocate the use of mixture of von Mises-Fisher distributions for streaming data based on a relatively sim-ple document-level clustering criterion. In this paper, we focus on the more flexible topic modeling framework, which is more effective at capturing combinations of aspects of doc-uments, rather than simple document similarity. Focusing on LDA, we compare a number of different methods for in-ferring topic distributions.

We find that a simple discriminatively trained classification-based method, MaxEnt, produces reasonable results with extremely small computational requirements. If more accu-racy is required, small numbers of iterations of Gibbs sam-pling or the hybrid MaxEnt method provide improved re-sults with minimal extra computation.

Finally, the efficiency of Gibbs sampling both for train-ing topic models and inferring topic distributions for new documents can be significantly improved by the SparseLDA method proposed in this paper. This method is valuable not only because of its reduction in sampling time (at least two times the speedup reported in previous work [7]), but also because of its dramatically reduced memory usage, an issue not addressed previously.
This work was supported in part by the Center for Intel-ligent Information Retrieval, in part by The Central Intel-ligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, and in part by UPenn NSF medium IIS-0803847. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] A. Banerjee and S. Basu. Topic models over text [2] A. Berger, S. D. Pietra, and V. D. Pietra. A maximum [3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [4] T. Griffiths and M. Steyvers. Finding scientific topics. [5] A. K. McCallum. MALLET: A machine learning for [6] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [7] I. Porteous, D. Newman, A. Ihler, A. Asuncion, [8] X. Song, C.-Y. Li, B. L. Tseng, and M.-T. Sun. [9] Y. W. Teh, D. Newman, and M. Welling. A collapsed [10] X. Wei, J. Sun, and X. Wang. Dynamic mixture
