 honored, and I share this honor with the outstanding collaborators and students I have been lucky to have over my lifetime.
 proaches to data and analysis and in their fundamental concepts. What I call the garden is traditional linguistics, including generative grammar. In the garden linguists primarily analyze what I call  X  X ultivated X  data X  X hat is, data elicited or introspected by the linguist X  X nd form qualitative generalizations expressed in symbolic represen-tations such as syntactic trees and prosodic phrases. What I am calling the bush could produced by speakers, and form quantitative generalizations based on concepts such as conditional probability and information content.
My talk will recount the path I have taken from the linguistic garden into the bush. 1. MIT Years: Into the Garden
I came into linguistics with a bachelor X  X  degree in philosophy from Reed College, grammar in the Department of Linguistics and Philosophy with Chomsky and Halle in the heyday of generative grammar. At MIT, Chomsky was my doctoral advisor, and my mentor was Morris Halle, who ran the Department at that time. the structure of human language, viewed as a purely combinatorial set of formal patterns , like the formulas of symbolic logic. It was apparently exciting even to Fred Jelinek as an MIT doctoral student in information theory ten years before me (Jelinek 2009). In his Lifetime Award speech he recounts how as a grad student he attended some of
Chomsky X  X  lectures with his wife, got the  X  X razy notion X  that he should switch from information theory to linguistics, and went as far as discussing it with Chomsky when his advisor Fano got wind of it and said he had to complete his Ph.D. in Information Theory. He had no choice. The rest is history. 1 specific to human language. This approach had methodological advantages for a philo-sophical linguist: First, a limitless profusion of data in our own minds came from our messy relationship to the world of facts and data was not required; and third, (with the proper training) scientific research could conveniently be done from an armchair using introspection. 2. Psychological (Un)reality
I got my Ph.D. from MIT in 1972 and taught briefly at Stanford and at UMass, Amherst, early on in my career as a linguist I had become aware of discrepancies between the MIT transformational grammar models and the findings of psycholinguists. For example, the theory that more highly transformed syntactic structures would require more complex processing during language comprehension and development did not work.
 designing a more psychologically realistic system of transformational grammar that made much less use of syntactic transformations in favor of an enriched lexicon and pragmatics. The occasion was a 1975 symposium jointly sponsored by MIT and AT&amp;T to assess the past and future impact of telecommunications technology on society, From Harvard Psychology, George Miller invited Eric Wanner, Mike Maratsos, and Ron Kaplan.
 computational psycholinguistics, and we began to collaborate. In 1977 we each taught courses at the IV International Summer School in Computational and Mathematical Linguistics, organized by Antonio Zampoli at the Scuola Normale Superiore, Pisa.
In 1978 Kaplan visited MIT and we taught a joint graduate course in computational psycholinguistics. From 1978 to 1983, I consulted at the Computer Science Labora-tory, Xerox Corporation Palo Alto Research Center (1978 X 1980) and the Cognitive and
Instructional Sciences Group, Xerox PARC (1981 X 1983). 600 3. Lexical-Functional Grammar During the 1978 fall semester at MIT we developed the LFG formalism (Kaplan and
Bresnan 1982; Dalrymple et al., 1995). Lexical-functional grammar was a hybrid of augmented recursive transition networks (Woods 1970; Kaplan 1972) X  X sed for com-putational psycholinguistic modeling of relative clause comprehension (Wanner and
Maratsos 1978) X  X nd my  X  X ealistic X  transformational grammars, which offloaded a huge amount of grammatical encoding from syntactic transformations to the lexicon and pragmatics (Bresnan 1978) (see Figure 1).

MacWhinney, and Lavie 2004; de Marneffe and Manning 2008) (see Figure 2). Some early statistical NLP parsers such as the Stanford Parser were dual-structure models like LFG with dependency graphs labeled by grammatical functions replacing f-structures (de Marneffe and Manning 2008).
 stored (or created by bounded lexical rules) (see Figure 3). Independent evidence for lexical storage is that passive verbs undergo lexical rules of word-formation (Bresnan 1982b; Bresnan et al. 2015); surface features of passive SUBJ tag questions). All relation-changing transformations are re-analyzed lexically in this way: passive, dative, raising, there -insertion, etc., etc.
 mapped into the appropriate predicate X  X rgument relations. The respective subjects of the upper and lower c-structure trees are first and third person singular pronouns, which give rise to the f-structures indexed i . The lexical forms are, respectively, active and passive, mapping each subject f-structure to the appropriate argument role of the verb hit .
 and psychology (initially represented in Bresnan, 1982a). The original group included Steve Pinker as a young postdoc from Harvard, Marilyn Ford as an MIT postdoc from
Australia, Jane Grimshaw then teaching at Brandeis, and doctoral students at MIT and Harvard: Lori Levin, K. P. Mohanan, Carol Neidle, Avery Andrews, and Annie Zaenen.
 then considered more realistic theories of the dynamics of sentence production (Ford 1982), comprehension (Ford, Bresnan, and Kaplan 1982; Ford 1983), and language de-velopment (Pinker 1984). 4. Stanford and Becoming an Africanist In 1982 I took a sabbatical leave from MIT at the Center for the Study of Behavioral
Sciences at Stanford, where I also spent time at Xerox PARC nearby. 602 ing launched with a grant from the System Development Corporation. I decided to stay in California by joining the Stanford Linguistics Department and CSLI the following year, half-time. From 1983 X 1992, I worked the other half of my time as a member of the Research Staff, Intelligent Systems Laboratory, Xerox Corporation Palo Alto Research
Center, which John Seely Brown headed during that time. LFG some computational linguistics applications such as MT.
 I had corresponded with him a few years earlier when I was a young faculty member at
MIT and he had received his Ph.D. from the University of London. We both had lexical syntactic inclinations, and he had evidence from the Bantu language Chiche  X  wa, one of the major languages of Mala  X  wi. His name was Sam Mchombo.

Chiche  X  wa in the LFG framework: Chiche  X  wa has 18 genders (but not masculine/feminine), tone morphemes, pronouns incorporated into verb morphology, relation changes all expressed by verb stem suffixation (which undergo derivational morphology), configurational discourse functions, . . . ! functions syntactically as a full-blooded object pronoun. In morphology, syntax, discourse, and language change (Bresnan and Mchombo 1987, 1995). Similar analyses have been explored in many disparate languages (e.g., Austin and Bresnan 1996; Bresnan et al. 2015). This and other work by many colleagues and students on a wide variety of languages helped to establish developed linguistic theory useful to typologists and field linguists.

Sam and other Bantuists including Katherine Demuth and Lioba Moshi, and in the summer of 1986 did field work in Tanzania. In Tanzania I took time out to celebrate my birthday by hiking up Mt. Kilimanjaro with a group of young physicians from Europe doing volunteer work in Africa.
 home in a village on the slopes of Mt. Kilimanjaro, being served the eyeball of a freshly slaughtered young goat, which I was told was a particular honor normally reserved for men. . . . But I was still in the garden of linguistics: change my research paradigm. 5. The Shock of Constraint Conflict
The first shock was the discovery that universal principles of grammar may be inconsistent and conflict with each other . The expressions of a language are not those that perfectly late some constraints in order to satisfy other more important constraints, optimizing constraint satisfaction. This insight came into linguistics from outside the field, from neural network approaches to cognition (Prince and Smolensky 1997). Yet as my former student Jane Grimshaw pointed out, we can see traces of it everywhere, even in corners of English syntax that had seemed exception-ridden.
 optimality-theoretic ( OT ) syntax using LFG as the representational basis (e.g., Bresnan 2000). OT -style constraint ranking in large-scale LFG grammars was adopted in standard LFG parsing systems for ambiguity management (Frank et al. 1998; Kaplan et al. 2004;
King et al. 2004). And Jonas Kuhn (2001, 2003) solved general computational problems of generation and parsing for OT syntax with LFG representations.
 tic generalizations. For example, suppose that the grammatical function hierarchy has to align with the person hierarchy (Aissen 1999):
Imposing the person-alignment constraints on f-structures either requires or prohibits passivization, depending on who does what to whom. Compare Example (2) and
Figure 6.
An event in which the first person hits someone referred to in the third person must be described in the active voice of hit , not the passive, because the passive subject would be higher on the function hierarchy than the non-SUBJ but lower on the person hierar-violate the alignment of hierarchies. There are person-driven passives like this in Lummi (Salish, British Columbia), Picur  X   X s (Tanoan, New Mexico), and Nootka (Wakashan,
British Columbia) X  X ll unrelated languages. 604 6. Grammars Hard and Soft
My former student Chris Manning (who wrote his Linguistics doctoral dissertation at Stanford under my supervision) joined the Stanford faculty as Assistant Professor of Computer Science and Linguistics in 1999, and I began to attend his lectures corpus, we found that English has soft, statistical shadows of hard person constraints in other languages: For example, it has person-driven active/passive alternations (Bresnan, Dingare, and Manning 2001), and person-driven dative alternations (Bresnan and Nikitina 2009). As Bresnan, Dingare, and Manning (2001) observe,  X  X he same categorical phenomena which are attributed to hard grammatical constraints in some languages continue to show up as statistical preferences in other languages, motivating a grammatical model that can account for soft constraints. X  Maslova 2007; Bresnan and Nikitina 2009), maximum-entropy
Johnson 2003; Gerhard Jaeger 2007), random fields (Johnson and Riezler 2003), data-oriented parsing (Bod and Kaplan 2003; Bod 2006), and other exemplar-based theories of grammar (Hay and Bresnan 2006; Walsh et al. 2010). 7. Into the Bush In addition to Chris Manning, another person who helped me go into the bush was Harald Baayen. I first met Harald while attending a 2003 LSA workshop on Probability
Theory in Linguistics. The presenters included Harald, Janet Pierrehumbert (with her student Jen Hay), Chris Manning, and others. As I watched the presenters give graphic visualizations of quantitative data showing dynamic linguistic phenomena, I thought,  X  X  want to do that! X  ditransitive verbs appear in alternative dative constructions, with the recipient realized as a dative PP  X  X  prepositional to -phrase X  X r as the first of two noun phrases X  X he dative NP . Spontaneously produced alternations occur:
Which of these alternative constructions is used depends on multiple and often conflict-ing syntactic, informational, and semantic properties.
 or PP constructions from the (unparsed) Switchboard Corpus (Godfrey et al. 1992) and another 905 from the Treebank Wall Street Journal corpus, using the manually parsed Treebank corpora (Marcus et al. 1993) to discover the set of dative verbs. With Harald
Baayen X  X  help we fit a series of generalized linear and generalized linear mixed-effect mended to me; I learned R, the programming language and environment for compu-that his name is R. Harald Baayen). Then I was able to show that under bootstrapping of speaker clusters and cross-validation, the models were highly accurate. paired parameter estimates for the recipient and theme have opposite signs. I dubbed this phenomenon quantitative harmonic alignment after the qualitative harmonic alignment in syntax studied by Aissen (1999) and others in the framework of Optimality
Theory. Figure 8 provides a qualitative schematic depiction of the quantitative phenom-ena. The hierarchies of discourse accessibility, animacy, definiteness, pronominality, and weight are aligned with the initial/final syntactic positions of the postverbal arguments across constructions. In LFG , the linear order follows from alignment with the hierarchy of grammatical functions in f-structure (Bresnan and Nikitina 2009).
 nations in some languages (Rosenbach 2005; Bresnan 2007a; Rosenbach 2008), and even hard weight constraints (O X  X onnor, Maling, and Skarabela 2009). The facts suggest that external to grammar and out of bounds to the theoretical linguist.
 constraints on LFG f-structures. Annie Zaenen saw how this kind of work might be
Mark Steedman and colleagues at Edinburgh on an animacy annotation project (Zaenen et al. 2004). 8. Data Shock
The second intellectual shock that pushed me further into the bush was realizing that in 606 underestimated the human language capacity .
 the predictive power of English language users (Bresnan 2007b). Inspired by Anette
Rosenbach X  X  (2003) beautiful experiment on the genitive alternation, and with her ad-vice, I made questionnaires asking participants to rate the naturalness of contextual-ized alternative dative constructions sampled from our dative data set, by allocating 100 points between the alternatives (see Figure 9).
 with their own intuitions; in another I asked them to guess what the original speaker the same way, splitting 100 points between the alternatives. The findings were similar:
As the log odds of a PP dative construction increased, the ratings of each participant showed a linear increase as well. The participants could tell which dative construction the
Figure 10). This finding has been replicated across speakers of other varieties of English (Bresnan and Ford 2010). Speaker:
About twenty-five, twenty-six years ago, my brother-in-law showed up in my front yard pulling a trailer. And in this trailer he had a pony, which I didn X  X  know he was bringing. And so over the weekend I had to go out and find some wood and put up some kind of a structure to house that pony, (1) because he brought the pony to my children. (2) because he brought my children the pony.
 that raise or lower probability to see whether they influence grammaticality judgments. double object construction are nevertheless found in actual usage (Bresnan et al. 2007).
For example, whisper is reported to be ungrammatical in the double object construction, but Internet queries yield whisper me the answer , along with whisper the password to the aligned and far more probable. The reportedly ungrammatical examples constructed by linguists tend to utilize the far less probable positionings of argument types, like grammatical constructions in the less probable contexts.
 had been greatly exaggerated (Bresnan 2007a). Our linguistic intuitions of what is ungrammatical may merely reflect our implicit knowledge of what is highly improbable (see also Manning 2003). 9. Probabilistic Syntactic Knowledge
The simplifying assumption in the garden of linguistic theory has been that speakers X  knowledge of their language is characterized by a static, categorical system of grammar. 608 ratings
Although this has been a fruitful idealization, I came to see that it ultimately underes-timates human language capacities. My own research showed that language users can match the probabilities of linguistic features of the environment and they have powerful predictive capabilities that enable them to anticipate the variable linguistic choices of others. Therefore, the working hypothesis I have adopted contrasts strongly with that of the garden: Grammar itself is inherently variable and probabilistic in nature, rather than categorical and algebraic.
 in language development (de Marneffe et al. 2012; Van den Bosch and Bresnan 2015); syntactic variation across varieties of English (Bresnan and Ford 2010; Ford and Bresnan 2015). 10. Deeper into the Bush
My research program is taking me even deeper into the bush, as I will now illustrate with several visualizations of wild data.
 unit with the immediately preceding word, called the host . In Example (4) the host is bolded: important predictor of verb contraction (Frank and Jaeger 2008; Bresnan and Spencer 2012; Barth and Kapatsinski 2014). Informativeness derives from predictability: More predictable events are less informative, and can even be redundant (Shannon 1948). The less predictable the host X  X erb combination, the more informative it is, and the less likely to contract.
 Tily, and Gibson 2011). We define the informativity of a host-verb bigram B as in
Equation (5): where B is the bigram consisting of a host and a verb in either contracted or uncon-tracted form (for example, blood X  X  or blood is ), N is the total frequency of B , and next the following context word for the i th occurrence of B in the corpus.
 tivity of host X  X erb bigrams for the verb is/ X  X  in data from the Canterbury Corpus of New
Zealand English (Gordon et al. 2004), which I collected with Jen Hay in the summer and fall of 2015. 2 Jen and I discussed implications of the informativity of verb contraction. As vocabulary richness grows, local word combinations become less predictable. Average predictability across contexts is what makes a host X  X erb combination more or less infor-verb combinations, potentially causing dynamic changes in verb contractions over time. These implications led me to ask whether children decrease their use of verb contractions in periods of increasing vocabulary richness during language development X  X  question never before asked, as far as I could tell from the literature.
 data from longitudinal corpora in CHILDES (MacWhinney 2000). 610 proportion contracted automatically parsed, and manually checked. As a matter of fact, the syntactic parses use dependency graphs derived from LFG functional structure relations (Sagae,
MacWhinney, and Lavie 2004; Sagae, Lavie, and MacWhinney 2005). Computational tools are provided, including the CLAN VOCD tool (MacWhinney 2015), which calculates vocabulary richness based on a sophisticated algorithm for averaging morpheme or lemma counts (lemmas being the distinct words disregarding inflections). panel plots in Figure 12 show increasing vocabulary richness as age increases between about 20 and 60 months, with the exception of one child ( X  X dam X ), whose vocabulary recordings.
 we would expect children X  X  subject X  X erb contractions to decrease as their vocabulary
VOCD (lemma) children X  X  contractions are tending to decrease, and only one child shows an increase in contraction during the period from 20 to 60 months: Adam, the child whose vocabulary richness shows the dip in Figure 12.
 contractions in conversational interactions during the same periods? Presumably the parents themselves would not be experiencing rapid vocabulary growth during this period, so they would not show a decline in verb contractions for that reason. Figure 14 shows the aggregated is contraction data by children vs. parents for all host-is/ X  X  bigrams and for the frequent bigrams what is/ X  X  and Mommy is/ X  X  . Strikingly, the children in the aggregate show declines in the proportion of contractions while their parents X  propor-tions remain constant.
 knowledge of syntactic probability. But they also raise many, many questions for further research X  X  good indicator of their potential fertility. This will be the focus of my next research project.
 island in English syntax, the contractible host X  X erb sequence. It does make me wonder topography of overlapping peaks and valleys of informativeness, requiring parallel computations on multiple scales X  X omething that one of you perhaps could figure out how to do. 612 proportion contracted 11. Looking Forward structure of human language. But now I know that linguistic structure is quantitative as well as qualitative, and I can use methods that I have been learning in the bush  X  putational linguistic theory, techniques, and resources to deepen our understanding of human language and cognition. proportion contracted References 614 616
