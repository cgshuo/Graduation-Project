
The identification of relevant subsets of random variables among thousands of potentially irrelevant and redundant variables is a challenging topic of pattern recognition re-search that has attracted much attention over the last few years [1], [2], [3], [4]. In supervised learning, feature se-lection (FS) algorithms maximize some function of pre-dictive accuracy. But in unsupervised learning, we are not given class labels. It becomes unclear which features we should keep as there are no obvious criteria to guide the search. Intuitively, all features are not equally important. Some of the features may be redundant, some may be irrelevant, and some can even misguide clustering results. Broadly speaking, the FS in unsupervised learning aims at finding relevant subsets of variables that produce  X  X atural X  groupings by grouping  X  X imilar X  objects together based on some similarity measure. Reducing the number of features increases comprehensibility and ameliorates the problem that some unsupervised learning algorithms break down with high dimensional data.
 Databases have increased many fold in recent years. Important recent problems (i.e., DNA data in biology) often have the property that there are hundreds or thousands of variables, with each one containing only a small amount of information. A single clustering model is known to produce very bad groupings as the learning algorithms break down with high dimensional data. Clustering ensembles is an effective solution to overcome the dimensionality problem and to improve the robustness of the clustering [5], [6], [7], [8], [9]. The idea is to combine the results of multiple clusterings into a single data partition without accessing to the original features. The strategy follows a split-and -merge approach: 1) construct a diverse and accurate ensemble com-mittee of clusterings, and 2) combine the clustering results of the committee using a consensus function. Although considerable attention has been given on the problem of constructing an accurate and diverse ensemble committee of clusterings, little attention has been given to exploiting the multiple clusterings of the ensemble with a view to identify and remove the irrelevant features.

The framework pursued in this article attempts to bridge the gap between supervised and unsupervised FS approaches in ensemble learning. The way internal estimates are used to measure variable importance in the Random Forests (RF) paradigm [10] have been influential in our thinking. In this study, we show that these ideas are also applicable to unsu-pervised FS. We emphasize that RF was already extended for unsupervized clustering by Breiman however we think his approach suffers from many problems as explained in the sequel. In our proposal, we extend the RF paradigm to unlabeled data by introducing a clustering ensemble termed as RCE (for Random Cluster Ensembles). RCE combines both data resampling ( bagging ) and random selection of features ( random subspaces ) strategies for generating an ensemble of component clusterings. A combination of these two main strategies for producing clustering ensembles leads to exploration of distinct views of inter-pattern relationships. Many approaches can be used to combine the multiple obtained partitions [9]. For sake of simplicity, we will use the evidence accumulation technique proposed in [5] in our experiments. The method consists of taking the co-occurences of pairs of patterns in the same cluster as votes for their association. The co-association matrix of patterns represents a new similarity measure between patterns. The final (consensus) clustering is obtained by running a tra-ditional average-link hierarchical agglomerative algorithm on this matrix. Once the consensus clustering is obtained, we select the relevant features locally in each final cluster based on the out-of-bag importance measure discussed by Breiman [10]. Strictly speaking, our method computes the feature relevance; the FS is performed afterwards by a statistical test called the Scree Test [11]. Empirical results on UCI and real labeled data sets will be presented to answer the following questions: (1) Is our FS for unsupervised learning algorithm better than clustering on all features? (2) Is it competitive with other unsupervised FS methods? (3) How does the performance degrade as more irrelevant variables are included?
The problem of unsupervised FS has attracted a great deal of interest recently. Like in supervised FS, the methods can be divided into three categories, depending on how they interact with the clustering algorithm: wrapper , embedded and filter approaches. Wrapper methods perform a search in the space of feature subsets, guided by the outcome of the clustering model. For that, they wrap unsupervised FS process around a clustering algorithm. Typically, a criterion is first defined for evaluating the quality of a candidate feature subset and wrapper approaches aim to identify a feature subset such that the clustering algorithm trained on this feature subset can achieve the optimal value of the predefined criterion, such as normalized scatter separability (for k-means) [12] or normalized likelihood (for EM clus-tering) [12] or DB-index [13]. Another example is given by the algorithm described in [14]. It tries to search for a subset of all features such that the clustering algorithm trained on this feature subset can achieve the most similar clustering solution to the one obtained by an ensemble learning algorithm. Furthermore, RF has been also extended to unlabeled data leading to unsupervised learning. The idea is to construct an RF predictor that distinguishes the observed data from suitably generated synthetic data [15], [16]. This method will be included in our experiments. In the aforementioned algorithms, the candidate feature subsets are evaluated globally. Regardless of the evaluation criteria, global FS approaches compute them over the entire dataset. Thus, they can only find one relevant feature subset for all clusters. However, it is the local intrinsic properties of data that counts during clustering [17]. As a solution, authors in [17] proposed a localized FS algorithm for clustering. The proposed algorithm computes adjusted and normalized scatter separability for individual clusters and a sequential backward search is then applied to search for the optimal feature subsets for each cluster. However this approach is not scalable to high-dimensional data. Several important works have proposed embedded formalisms using variable weighting to solve the problem of unsupervised FS [18], [19]. For such approaches, the search for an optimal subset of features is built into the clustering construction making these techniques specific to a given learning algorithm. In contrast to wrapper and embedded approaches, filter meth-ods discover the relevant and redundant features through analyzing the correlation and dependence among features without involving any clustering algorithms [20], [21]. The most common filter strategies are based on feature ranking. Recently, [22] proposed a consensus unsupervised feature ranking approach that combines multiple rankings of the full set of features into a single consensus one. The ranking of features is obtained using their relevance measured by the linear correlation coefficient and symmetrical uncertainty. Unfortunately, the authors only report experimental results on very low-dimensional data sets.
 A. Building and combining multiple clustering solutions
Ensemble methods have been applied successfully in the area of unsupervised learning to improve the accuracy and the robustness of clustering algorithms. Resampling methods such as bagging, were one of the first approaches that exploited this idea of ensemble learning. A group of clustering models was built over a bootstrapped replicate of the former dataset. Finally, the last partition was made by a consensus function over the set of partitions of each single clustering model [6], [23]. Another trend appeared with the use of random subspaces . Like bagging, random subspaces are another excellent source of clustering diversity that provides different views of the data and improves the quality of unsupervised classification solutions [6], [7]. Since ensemble methods are designed to improve performance of supervised and unsupervised classification methods, it is reasonable to think that they can also be used to tackle the unsupervised FS problem.

Contrary to all the previous described approaches, we propose to combine both bagging and random subspaces for producing an ensemble of component clusterings. RCE achieves a population of clustering solutions with the fol-lowing steps employed: a new training set is drawn, with replacement, from the original data set. Then m features are randomly selected from the entire feature set (leading to a partial view of the bootstrap data set) and a clustering solution is obtained through executing the  X  X ase X  clustering algorithm on the selected features. The same steps are repeated r times. There are many reasons for using bagging in tandem with random feature subspaces. The first is that bagging can be used to give estimates of both the variable importance and the pattern proximities that will serve to build a final consensus clustering from the ensemble of clusterings. The second is that the ensemble method com-bines many weak learners in an attempt to produce a strong learner [5]. Moreover, clustering in the projected subspace allows us to mitigate the curse of dimensionality.

Assume a method h for constructing a clustering from any training set. Given a specific data set D = { x 1 ,...,x with M input variables, form r bootstrap training sets ( k  X  X  1 ,...,r } ) in a random feature subspace (a m view of the bootstrap training set where m = construct clustering C k = h ( x, D k ) . The way these ( k  X  X  1 ,...,r } ) should be combined together is called the cluster ensemble problem. Several feasible approaches are introduced in the literature to solve the cluster ensemble problem. In this study, we adopt the average-link consen-sus function based on co-association values between data, proposed in [5]. Proximity between pair of cases simply counts the fraction of clusters shared by these objects in the initial partitions. Numerous similarity-based clustering algorithms can be applied to the proximity (or similarity) matrix to obtain the final partition. We chose the Agglom-erative Hierarchical Classification. The number of clusters is the one that maximizes the average mutual information criterion [6]. The overall framework is also applicable to any feasible consensus clustering procedure.
 B. Out-of-bag estimates to measure variable importance
In the RCE method, bagging is used in tandem with feature subspace to give ongoing estimates of the feature importance of the combined ensemble of clusterings. A start on this problem is made by using internal out-of-bag (oob) estimates, and verification by reruns using only selected variables. These estimates are done out-of-bag, exactly as done in RF. After each clustering is constructed, the values of the v th variable in the oob patterns are randomly permuted and the oob data are re-assigned into clusters. This is repeated for v =1 , 2 ,...,M . At the end of the run, the oob cluster assignments for x , with the v th variable permuted, is compared with the original cluster assignment of x 1 . The average number of times the oob pattern with the variable v permuted, is misclassified divided by the number of clusterings in the ensemble r is the local importance score for variable v for this pattern. Now, the importance of the v th variable for a given cluster (local) in the final consensus clustering is calculated as the sum of all the importance values over all the patterns that fall into this particular cluster. Various methods can be employed to select the most important local variables in view of their importance estimates, including: 1) statistical tests (e.g., Scree Test [11]), 2) selecting a predefined percentage of variables [24] 3) the same recursive feature elimination scheme used with SVM [1]. In this study, the Scree Test [11] is used. It consists in selecting the features preceding a threshold value called the  X  X cree X . The  X  X cree X  corresponds to the point where the maximum deceleration of the curve occurs. See [11] for more details.

Our FS algorithm has several advantages when compared to other existing unsupervised FS algorithms: First, as men-tioned in [14], most existing unsupervised FS algorithms are dimensionality-biased. For example, if the scatter sep-arability based FS algorithm is adopted, high-dimensional feature subsets are selected more easily [12], [14]. The problem should be circumvented as RCE operates on low-dimensional feature spaces. Second, as RCE leverages dif-ferent clustering solutions to measure feature importance, it is expected to improve the robustness and stability of the feature subset compared to FS algorithms based on a single clustering method. Third, the estimates of variable importance is obtained from the oob patterns only. As noted by Breiman, it should therefore be as accurate as using a test set of the same size as the training set. Therefore, using the oob error estimate removes the need for a set aside test set. In each bootstrap training set, about one-third of the patterns are left out. Therefore, the oob estimates are based on combining only about one-third as many clustering models as in the ongoing main combination.
 Given a data set D = { x 1 ,...,x n } with M input features F = { f 1 ,...,f M } , the overall proposed RCE framework for localized unsupervised FS is summarized below: 1) Initialize an n  X  n matrix A and n  X  M matrix I to 2) Form the k th bootstrap sample D k = { x 1 k ,...,x n 3) Create a random feature subset F k = { f 1 k ,...,f m 4) Project D k and D k oob onto feature subset F k : D 5) Apply the clustering procedure h to the bootstrap 6) For each pair of observations ( x i k ,x j k ) , update the 7) Classify each oob data x i oob into C k and obtain its 8) For each feature v in F k randomly permute the values 9) Repeat Steps 2-8 r times and define a new dissimilar-10) Cluster the n original observations on the basis of this 11) For each obtained cluster c k in C and each feature v 12) For each obtained cluster c k in C use the scree test to
The evaluation of the performance of RCE was conducted as follows: A) quality of the selected features using k-means as the  X  X ase X  clustering algorithm, B) quality of the selected features using Self-Organizing Map (SOM) [25] as the  X  X ase X  clustering algorithm, C) performances of RCE on large feature/small sample size domains, and D) impact of noisy features on RCE performances. Several benchmark (UCI) data sets [26] were selected to test the performance of RCE. They are described in Table I. It should be noted that these data sets have already been used by other authors as benchmark data sets for testing the performance of their unsupervised FS algorithms. As their code is not freely available, we used the same experimental setup (in Section A and B) as these authors and compared the average performance values of RCE with the performance values displayed in their paper [14], [27].
 A. Evaluation using k-means as the base clustering algo-rithm
First, the k-means clustering algorithm was adopted as the  X  X ase X  clustering algorithm. The number of clusters was set to the one optimizing the Davies Bouldin index. The size of clustering ensembles r was set to 200 in our experiments. In these experiments, the number and the quality of the features selected by RCE was studied and compared with those obtained by several state-of-the-art unsupervised FS algorithms, namely the scatter separability wrapper unsupervised FS algorithm [12], the DB-index wrapper unsupervised FS algorithm [13], the clustering ensembles guided FS algorithm (CEFS) [14] and the unsu-pervised RF FS algorithm [15] 2 . To make fair comparisons, the same experimental approach (protocol and evaluation measure) in [14] was adopted here. The quality of the selected feature subset obtained by each approach on the first four data sets in Table I was evaluated by running k-means on this feature subset and the clustering accuracy was calculated by the Rand Index method [28]. Results were averaged over 20 independent runs since k-means is very unstable. The union of all the local feature subsets given by RCE was considered as the overall selected feature subset. Experimental results are reported in Table II. Again, the performance values in Table II for CEFS, DB-index wrapper unsupervised FS and scatter separability wrapper unsupervised FS are taken from [14]. The accuracies of k-means on the features selected by RCE were consistently higher than those obtained by k-means on all the features. k-means on all the features achieved accuracies around 80.54% on Ecoli, 86.32% on Iris, 63.25% on Lung and 64.17% on Vehicle; k-means on the features selected by RCE achieved accuracies around 84.02% on Ecoli, 93.20% on Iris, 68.27% on Lung and 66.98% on Vehicle. The signficant gain in accuracy on these four data sets confirms the ability of the RCE approach to improve the quality of the clustering and to generate meaningful clusters. RCE always identified a better feature subset when compared with those obtained by the scatter separability wrapper method, the DB-index wrapper algorithm, the clustering ensembles guided FS algorithm (CEFS) and the unsupervised RF FS algorithm.
 B. Evaluation using the SOM as the base clustering algo-rithm
In this section, the SOM [25] was applied to generate the partitions for the combination. For the map clustering, we used the ward-link hierarchical classification combined with Davies Bouldin index to choose the optimal number of clusters. The size of the clustering ensembles r set to 200. To evaluate the quality of our FS procedure, we chose to compare the clusters found with the SOM on the features selected by RCE to (1) the clusters returned by the SOM on all the features, (2) those returned by the SOM on the features selected by the unsupervised RF algorithm [15] and (3) the clusters obtained by two recently embedded unsupervised FS methods called lwd-SOM and lwo-SOM [27]. The results of both lwd-SOM and lwo-SOM approaches are taken from [27]. All data sets ( Wave , Wdbc , Spamb , Madelon and Isolet ) include the class information ( label ) for each data pattern. To compare RCE with lwd-SOM and lwo-SOM , the evaluation was based this time on the Purity rate , antoher simple and transparent evaluation measure that also assess clustering accuracy . Unfortunately, the Rand index was not given in their work.
 Table III shows the clustering results according to the Purity rate . The results confirm again (1) the effectiveness of the RCE framework for unsupervised learning, and (2) the usefulness of the consensus clustering combined with feature importance measure of RF to offer an unsupervised FS approach that is able to identify a better feature subset than that obtained by these state-of-the-art algorithms. The results output by the SOM on the features selected by RCE were shown to be more relevant in view of the Purity rate than those obtained by the SOM on all the features, as well as the lwd-SOM , lwo-SOM and the unsupervised RF FS algorithm. lwd-SOM unsupervised FS algorithm performed the worst in most cases (on Wave, Wdbc, Spamb and Madelon data sets) and achieved much lower accuracies than those obtained by SOM on all features.
The description of the selected features for each unsuper-vised FS algorithm are reported in Table IV for the sake of illustration. We also compared the features provided by each unsupervised approach to the features returned by the supervised FS method used in the RF method (taken as our gold standard), on each data set, ( cf. Table V). This compar-ison was made using the Jaccard Index (Eq.(1)) between the feature subset corresponding to the union of local selected features and the one returned by the (supervised) RF. The Jaccard Index is defined as: where S i and S j are two feature subsets. Again, the RCE method compared favorably to lwd-SOM , lwo-SOM and unsupervised RF . It can be observed from Tables V and IV that RCE achieved the most similar feature subsets to those returned by the supervised RF, except for the wave data set. However, for this data set composed of 21 relevant features and 19 noisy features, no false positives we found in the output of RCE. As may be observed, the purity of the SOM clustering on the features selected by RCE (66.24%) was significantly better than those obtained by lwd-SOM (53.47%), lwo-SOM (54.16%) and unsupervised RF (55.74%).
 C. Results on large feature/small sample size domains
In this section, we report the results of the RCE unsuper-vised FS technique on large feature/small sample size do-mains. The SOM was used as the base clustering algorithm in RCE. The unsupervised FS technique was assessed using 10-fold cross validation on two real data sets Ovarian [29] and Leukemia [30]. The data set was split into 10 disjoint subsets of equal size (subsample contains 90% of the data for training and 10% for test). This percentage was small because we could not discard too much data when building the models. To demonstrate the effectiveness of RCE, its performance was compared to those of our gold standard ensemble supervised FS algorithm (RF). For each approach, the strategy explained before was employed. The average classification accuracy on the hold-out data was used as the performance measure. For each fold, the selection of the features was performed using only the training part of the data, and each approach (RCE and supervised RF) was run again using the 1% best features returned by its first run, as it was used in [24]. Each model was then evaluated on the test part of the data for each fold, and the results were averaged over all 10 folds. The results of these experiments are reported in Table VI.
 The returned results confirm again the usefulness of the RCE strategy. The results output by the consensus clustering of RCE on the selected features were shown to be more relevant in view of the accuracy than that obtained by the consensus clustering of RCE on all the features. Nonethe-less, the classification accuracies of RCE were about 10% lower than those returned by the supervised FS approach (RF), that has access to the labels during the learning phase. D. Effect of noisy features on RCE performances
The wave data set [26] has 40 variables where the last 19 ones are totally irrelevant with mean 0 and variance 1. We conducted several experiments on this data set in order to study the impact of adding noisy features on the performance of RCE. The SOM was used again as the base clustering algorithm. We first performed a FS on the original data set; then 190 irrelevant variables were added sequentially (the 19 irrelevant features duplicated 10 times) and the procedure was repeated several times. At each step, the clustering quality of the SOM on all features was compared to the one returned by (1) the clustering ensemble before the FS ( consensus with all features ), (2) the SOM clustering on the features selected by RCE ( SOM with selected features ) and (3) the clustering ensemble on the features selected by RCE ( consensus with selected features ). For the sake of completeness, we report the results using two performance metrics: ( i ) the Purity rate and ( ii ) the Adjusted Rand index proposed by Hubert and Arabie [31]. The latter assesses the degree of agreement between two partitions (the one obtained with the clustering algorithm ( clusters ) and the correct predefined one ( labels )) by considering relations upon patterns. Figures 1 and 2 show the evolution of the Adjusted Rand and the Purity rate as more irrelevant features are added. The following conclusions can be drawn from these experiments: (1) the performance of a single application of SOM on all the features deteriorated markedly with an increasing number of noisy features, (2) although RCE still breaks down with high dimensional data, the selected features based on the internal estimates of RCE were found to be highly relevant. Indeed, when RCE was retrained with the selected features, the consensus clustering quality considerably increased. Therefore, the pertinence of the RCE FS approach to offer an important preprocessing step for unsupervised learning and its robustness to noisy features are confirmed. Both the the consensus clustering with the selected features and the SOM with the selected features achieved better results than with all the features. Despite the degradation of the consensus clustering with all features, the selected features were all correct as there were no false positives among the selected variables. In our opinion, this is the most interesting result of this study.
In this paper, we extended the Random Forest paradigm to unlabeled data by introducing a clustering ensemble termed as RCE. We showed that the way internal estimates are used to measure variable importance in Random Forests are also applicable to feature selection in unsupervised learning. The clustering performance of RCE was illustrated on various data sets based on widely used external criteria of clustering quality, using both k-means and the Self-Organizing Map as the base clustering algorithm. Although RCE still breaks down with high dimensional data, we found that the selected features based on the internal estimates of RCE were highly relevant. When retrained with the selected features, the consensus clustering quality considerably increased. Future substantiation through more experiments on DNA expression data are currently being undertaken and comparisons with both supervised and unsupervised feature selection methods will be reported in due course. A straightforward extension of the proposed method is to use an iterative backward selection where only a portion of the features with the smallest importance values is removed at each iteration of the selection process as proposed in [32]. Another extension of this work is to consider the problem of combining labeled and unlabeled data for unsupervised learning. These extensions are left for future work.

