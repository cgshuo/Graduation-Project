 New types of document collections are being developed by various web services. The service providers keep track of non-textual features such as click counts. In this paper, we present a framework to use non-textual features to pre-dict the quality of documents. We also show our quality measure can be successfully incorporated into the language modeling-based retrieval model. We test our approach on a collection of question and answer pairs gathered from a community based question answering service where people ask and answer questions. Experimental results using our quality measure show a significant improvement over our baseline.
 H.3.0 [ Information Search and Retrieval ]: General Algorithms, Measurement, Experimentation Information Retrieval, Language Models, Document Qual-ity, Maximum Entropy
Every day new web services become available and these services accumulate new types of documents that have never before existed. Many service providers keep non-textual in-formation related to their document collections such as click-through counts, or user recommendations. Depending on the service, the non-textual features of the documents may be numerous and diverse. For example, blog users often rec-ommend or send interesting blogs to other people. Some Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. blog services store this information for future use. Movie sites saves user reviews with symbolic representations rat-ing the movie (such as A or ? ? ? ? ? ).

This non-textual information has great potential for im-proving search quality. In the case of the homepage finding, link information has proved to be very helpful in estimating the authority or the quality of homepages [2, 10]. Usually textual features are used to measure relevance of the docu-ment to the query and non-textual features can be utilized to estimate the quality of the document. While smart use of non-textual features is crucial in many web services, there has been little research to develop systematic and formal approaches to process these features.

In this paper, we demonstrate a method for systemat-ically and statistically processing non-textual features to predict the quality of documents collected from a specific web service. For our experiment, we choose a community based question answering service where users ask and an-swer questions to help each other. Google Answers 1 , Ask Yahoo 2 , Wondir 3 and MadSciNet 4 are examples of this kind of service. These services usually keep lots of non-textual features such as click counts, recommendation counts, etc. and therefore can be a good testbed for our experiments.
In order to avoid the lag time involved with waiting for a personal response, these services typically search their col-lection of question and answer (Q&amp;A 5 ) pairs to see if the same question has previously been asked. In the retrieval of Q&amp;A pairs, estimating the quality of answers is important because some questions have bad answers. This happens because some users make fun of other users by answering nonsense. Sometimes irrelevant advertisements are given as answers. The followings are examples of bad answers found from community based question answering services. http://answers.google.com/ http://ask.yahoo.com/ http://www.wondir.com/ http://www.madsci.org/
In this paper, Q&amp;A means  X  X uestion and answer X  and is used only as an adjective such as  X  X &amp;A pairs X  and  X  X &amp;A collections X  . Q&amp;A must be discerned from QA that is of-ten used to refer to automated question answering. There-fore, in this paper,  X  X &amp;A service X  means services such as Google Answers where people answer other people X  X  ques-tions. The answer quality problem becomes important when there are many duplicated questions, or many responses to a single question. The duplicated questions are generated because some users post their questions without carefully searching existing collections. These semantically duplicated ques-tions have answers with varying quality levels, therefore measuring relevance alone is not enough and the quality of answers must be considered together.

We use kernel density estimation [5] and the maximum entropy approach [1] to handle various types of non-textual features and build a stochastic process that can predict the quality of documents associated with the features. We do not use any service or collection specific heuristics, therefore our approach can be used in many other web services. The experimental results show the predictor has the ability to distinguish good answers from bad ones.

In order to test whether quality prediction can improve the retrieval results, we incorporate our quality measure into the query likelihood retrieval model [18]. Our goal in the retrieval experiments is to retrieve relevant and high quality Q&amp;A pairs for a given query. In other words, the question and the query must describe the same information needs and the quality of answer must be good. Experimental results show significant improvement in retrieval performance can be achieved by introducing the quality measure.

We discuss related work in section 2. Section 3 describes our data collection. Section 4 explains in detail how we calculate the quality of answers. The retrieval experiments and results are presented in section 5. Section 6 concludes this paper.
Many factors decide the quality of documents (or an-swers). Strong et al. [20] listed 15 factors and classified those factors into 4 categories: contextual, intrinsic, repre-sentational and accessibility. Zhu and Gauch [24] came up with 6 factors to define the quality of web pages. However, so far, there is no standard metric to measure and represent the quality of documents.

There has been extensive research to estimate the quality of web pages. Much of the work is based on link analysis [2, 10]. A few researchers [24, 23] tried to use textual features. Zhou and Croft [23] proposed a document quality model that uses only content based features such as the information-noise ratio and the distance between the document language model and the collection model. However, little research has been done to estimate the quality of answers in a collection of question and answer pairs.

FAQ retrieval research [3, 19, 13, 21, 9] has focused on finding similar questions from FAQ collections. More re-cently, Jeon et al. [6] proposed a retrieval method based on machine translation to find similar questions from com-munity based question and answering services. However, Table 1: The relationships between questions and answers in Q&amp;A pairs are manually judged. The test samples consist of 1700 Q&amp;A pairs. The training samples have 894 Q&amp;A pairs. Both training and test samples show similar statistics. none of them have considered the quality of answers in the retrieval process.

The language modeling framework [18] provides a natural way of combining prior knowledge in the form of a prior probability. Prior information such as time, quality and popularity has been successfully integrated using as a prior probability on the document [11, 23, 14]. We also use the prior probability to combine quality and relevance.
Berger et al. [1] proposed the use of the maximum entropy approach for various natural language processing tasks in mid 1990 X  X  and after that many researchers have applied this method successfully to a number of other tasks including text classification [16, 17] and image annotation [7].
We collected 6.8 million Q&amp;A pairs from the Naver Q&amp;A service 6 . All questions and answers are written in Korean. We randomly selected 125 queries from the search log of a single day. We used a pooling technique [4] to find relevant Q&amp;A pairs for those queries. We ran 6 different search en-gines and gathered the top 20 Q&amp;A pairs from each search result. Annotators manually judged the candidates in three levels: Bad, Medium and Good. Annotators read the ques-tion part of the Q&amp;A pair. If the question part addressed the same information need as the query, then the Q&amp;A pair was judged as relevant. When the information need of a query was not clear, annotators looked up click-through logs of the query and guessed the intent of the user. In all, we found 1,700 relevant Q&amp;A pairs.
The quality of a Q&amp;A depends on both the question part and the answer part. The followings are examples of bad questions that can be found from community based Q&amp;A services.
 http://www.naver.com/ Naver provides a community based question answering service in South Korea. In this service, users help each other by posting and answering ques-tions. This service is very popular and has accumulated more than 10 million Q&amp;A pairs over last 3 years. Figure 1: Architecture of the quality predictor. Users can not get any useful information by reading an-swers for these bad questions. We found that bad questions always lead to bad quality answers. Answers for these bad questions usually blame the questioner with short insulting words. Therefore, we decide to estimate only the quality of answers and consider it as the quality of the Q&amp;A.
In general, good answers tend to be relevant, informa-tive, objective, sincere and readable. We may separately measure these individual factors and combine scores to cal-culate overall the quality of the answer. But this approach requires development of multiple estimators for each factor and the combination is not intuitive. Therefore, we propose to use a holistic view to decide the quality of an answer. Our annotators read answers, consider all of the above fac-tors and specify the quality of answers in just three levels: Bad, Medium and Good. This holistic approach shifts the burden of combining individual quality metrics to human annotators.

In subsection 3.1, we explained how we found 1700 rele-vant Q&amp;A pairs to the 125 queries. For the 1,700 Q&amp;A pairs, we manually judged the quality of answers. In this step, the query was ignored and only the relationships between ques-tions and answers in Q&amp;A pairs are considered. The results of the quality judgment are in Table 1. Around 1/3 of the answers have some sort of quality problems. Approximately 1/10 of the answers are bad. Therefore, we need to properly handle these bad documents (Q&amp;A pairs).

To build a machine learning based quality predictor, we need training samples. We randomly selected 894 new Q&amp;A pairs from the Naver collection and manually judged the quality of the answers in the same way. Table 1 shows the test and the training samples have similar statistics.
In this section, we explain how to predict the quality of answers. Figure 1 shows the architecture of our quality pre-diction system. The input of the system is a Q&amp;A pair and the output is the probability that the Q&amp;A pair has a good answer. The following subsections discuss each component in detail. First we need to extract feature vectors from a Q&amp;A pair. We extract 13 non-textual features. Table 2 shows the list of the features. In the Naver Q&amp;A service, multiple answers are possible for a single question and the questioner selects the best answer. Unless otherwise mentioned, we extract features only from the best answer. The following is a de-tailed explanation of each individual feature.
 Answerer X  X  Acceptance Ratio The ratio of best answers Answer Length The length of the answer. Depending on Questioner X  X  Self Evaluation The questioner gives from Answerer X  X  Activity Level If a user asks and answers Answerer X  X  Category Specialty If a user answers many Print Counts The number of times that users print the Copy Counts The number of times that users copy the Users X  Recommendation The number of times the Q&amp;A Editor X  X  Recommendation Sometimes editors of the ser-Sponsor X  X  Answer For some categories, there are approved Click Counts The number of times the Q&amp;A pair is clicked Number of Answers The number of answers for the given Users X  Dis-Recommendation The number of time the Although some features are specific to the Naver service, other features such as answer length, the number of an-swers and click counts are common in many Q&amp;A services. Some features such as recommendation counts and evalu-ation scores using stars can be found in many other web services. As can be seen from table 2, various numerical types are used to represent diverse features. Table 2: List of features. The second column shows numerical types of the features. The last column shows the correlation coefficients between the fea-ture values and the manually judged quality scores. Higher correlation means the feature is a better in-dicator to predict the quality of answers. Minus values means there are negative correlations.
We calculate the correlation coefficient (or Pearson X  X  cor-relation) between individual features and the manual quality judgment scores (good answers have higher scores: Bad=0, Medium=1, Good=2). The third column in table 2 shows the coefficient values.

Surprisingly,  X  X uestioner X  X  Self Evaluation X  is not the fea-ture that has the strongest correlation with the quality of the answer. This means the questioner X  X  self evaluation is subjective and often does not agree with other users opinion about the answer. Many people simply appreciate getting answers from other people regardless of the quality of the answers, and give high scores for most of the answers. This user behavior may be related to the culture of Korean users. Performing similar analysis with other user groups, for ex-ample with North American users, may give an interesting comparison.  X  X ponsor X  X  Answer X  and  X  X ditor X  X  Recommendation X  are good features because they always guarantee the quality of answers but only small number of Q&amp;A pairs are recom-mended by editors or written by sponsors. Therefore, these features have little impact on overall performance and the coefficient values are relatively small.

With the exception of the answer length, most of the im-portant features are related to the expertise or the quality of the answerer. This result implies that knowing about the answerer is very important in estimating the quality of answers. We may get better estimations using these non-textual features than analyzing contents of answers using textual features because accurately understanding the con-tents of the text is very hard with the current technology.
Maximum entropy models require monotonic features that always represent stronger evidence with bigger values. For example, the number of recommendations is a monotonic feature since more recommendations means better quality. However, the length of an answer is not a monotonic feature because longer answers do not always mean better answers.
Features Corr Corr Answer Length 0.1733 0.4285 Answerer X  X  Activity Level 0.1430 0.1982 Answerer X  X  Category Specialty 0.1037 0.2103 Table 3: Feature conversion results. The second column represents the correlation between the raw feature value and the quality scores. The third col-umn shows the correlation coefficients after convert-ing features using kernel density estimation. Much stronger correlations are observed after the conver-sion.

Most of the previous work [16, 17] on text classification using the maximum entropy approach used only monotonic features such as frequency of words or n-grams. There-fore little attention was given to solve the problem of non-monotonic features. However, we have non-monotonic fea-tures and need to convert these features into monotonic fea-tures.
 We propose using kernel density estimation (KDE) [5]. KDE is a nonparametric density estimation technique that overcomes the shortcomings of histograms. In KDE, neigh-boring data points are averaged to estimate the probability density of a given point. We use the Gaussian kernel to give more influence to closer data points. The probabil-ity of having a good answer given only the answer length, P ( good | AL ), can be calculated from the density distribu-tions.
 P ( good | AL ) = P ( good ) F ( good | AL ) where AL denotes the answer length and F () is the den-sity function estimated using KDE. P ( good ) is the prior probability of having a good quality answer estimated from the training data using the maximum likelihood estimator. P ( bad ) is measured in the same way.

Figure 2 shows density distributions of good quality an-swers and bad quality answers according to the answer length. Good answers are usually longer than bad answers but very long and bad quality answers also exist. The graph shows P ( good | AL ) calculated from the density distributions. The probability initially increases as the answer length becomes longer but eventually starts decreasing. The probability that an answer is high quality is high for average-length answers, but low for very long answers. This accurately reflects what we see in practice in the Naver data.

We use P ( good | AL ) as our feature value instead of us-ing the answer length directly. This converted feature is monotonic since a bigger value always means stronger evi-dence. The 894 training samples are used to train the ker-nel density estimation module. Table 3 shows the power of this conversion. We calculate the correlation coefficients again after converting a few non-monotonic features. In the case of the answer length, the strength of the correlation is dramatically improved and it becomes the most significant feature. Figure 2: Density distributions of good answers and bad answers measured using KDE. The x axis is log(answer length) and the y axis is the density or the probability. The graph also shows the probabil-ity of having a good answer given the answer length.
We use the maximum entropy approach to build our qual-ity predictor for the following reasons. First, the approach generates purely statistical models and the output of the models is a probability. The probability can be easily in-tegrated into other statistical models. Our experimental results show the output can be seamlessly combined with statistical language models. Second the model can handle a large number of features and it is easy to add or drop features. The models are also robust to noisy features.
We assume that there is a random process that observes a Q&amp;A pair and generates a label y , an element of a finite set Y = { good, bad } . Our goal is making a stochastic model that is close to the random process. We construct a training dataset by observing the behavior of the random process. The training dataset is ( x 1 , y 1 ) , ( x 2 , y 2 ) , ..., ( x question and answer pair and y i is a label that represents the quality of the answer. We make 894 training samples from the training data.
We can extract many statistics from the training samples and the output of our stochastic model should match these statistics as much as possible. In the maximum entropy ap-proach, any statistic is represented by the expected value of a feature function. To avoid confusion with the document features, we refer to the feature functions as predicates. We use 13 predicates. Each predicate corresponds to each doc-ument feature that we explained in the previous section. f i ( x, y ) = kde ( x fi ) if i where f i ( x, y ) is the i th predicate and x fi is the raw value of the i th feature in Q&amp;A pair x .

The expected value of a predicate with respect to the training data is defined as follows, where  X  p ( x, y ) is a empirical probability distribution that can be easily calculated from the training data. The expected value of the predicate with respect to the output of the sto-chastic model should be the same with the expected value measured from the training data.
 where p ( y | x ) is the stochastic model that we want to con-struct. We call the equation (4) a constraint. We have to choose a model that satisfy these constraints for all predi-cates.
In many cases, there are infinite number of models that satisfy the constraints explained in the previous subsection. In the maximum entropy approach, we choose the model that has maximum conditional entropy There are a few algorithms that find an optimal model which satisfy the constraints and maximize the entropy. General-ized Iterative Scaling and Improved Iterative Scaling have been widely used. We use Limited Memory Variable Metric method which is very effective for Maximum Entropy para-meter estimation [15]. We use Zhang Le X  X  maximum entropy toolkit 7 for the experiment.

The model is represented by a set of parameters  X  . Each predicate has a corresponding parameter and the following is the final equation to get the probability of having a good answer or bad answer. where Z ( x ) is a normalization factor.
We build the predictor using the 894 training samples and test using the 1700 test samples. The output of the predictor is the probability that the answer of the given Q&amp;A pair is good. The average output for good Q&amp;A pairs is 0.9227 and the average output for bad Q&amp;A pairs is 0.6558. In both cases, the averages are higher than 0.5 because the prior probability of having a good answer is high. As long as this difference is consistent, it is possible to build an accurate classifier using this probability estimate.

We rank 208 bad examples and 1099 good examples in the test collection together by the descending order of the output values. Figure 3 shows the quality of the ranking using the recall-precision graph. The predictor is significantly better than random ranking. In the top 100, all Q&amp;A pairs are good. The top 250 pairs contain 2 bad pairs and the top 500 pairs contain 9 bad pairs. The results show that the predictor has the ability to discriminate good answers from http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html Figure 3: Performance of the quality predictor. 11pt recall-precision graph. Note that the y-axis scale starts from 0.75.  X  X andom X  is the result of random ranking that positions Q&amp;A pairs randomly. bad answers. In future work, by increasing the size of the training samples, we may get better performance. In the next section, we investigate the effectiveness of the predictor in the context of retrieval.
We test whether the quality measure can improve retrieval performance. As a baseline experiment, we retrieve Q&amp;A pairs using the query likelihood retrieval model[18]. The 125 queries are used and the question part of the Q&amp;A pair is searched to find relevant Q&amp;A pairs to the query, because the question part is known to be much more useful than the answer part in finding relevant Q&amp;A pairs [8, 6]. This base-line system may return relevant Q&amp;A pairs, but there is no guarantee about the quality of the answers. We incorporate the quality measure into the baseline system and compare retrieval performance.
In the query likelihood retrieval model, the similarity be-tween a query and a document is given by the probability of the generating the query from the document language model. P ( Q ) is independent of documents and does not affect the ranking. For the document model, usually, i.i.d sampling and unigram language models are used.
 P ( D ) is the prior probability of document D . Query inde-pendent prior information such as time, quality and popular-ity have been successfully integrated into the model using the prior probability [11, 23, 14]. Since our estimation of the quality is given by a probability and query independent, the output of the quality predictor can be plugged into the retrieval model using the prior probability without any mod-ification such as normalization. Therefore, in our approach, P ( D ) = p ( y | x = D ) and p ( y | x ) is given as in equation (6).
To avoid zero probabilities and estimate more accurate document language models, documents are smoothed using a background collection, P ml ( w | C ) is the probability that the term w is generated from the collection C. P ml ( w | C ) is estimated using the max-imum likelihood estimator.  X  is the smoothing parameter. We use Dirichlet smoothing [22]. The optimal parameter value is found by exhaustive search of the parameter space. We use the implementation of the query likelihood retrieval model in the Lemur toolkit 8 .
In order to automatically evaluate retrieval performance, usually a relevance judgment file is made. This file contains lists of relevant documents to queries and an evaluation sys-tem looks up this file to automatically assess the perfor-mance of search engines. We made three different relevance judgment files. The first one (Rel 1) considers only the rel-evance between the query and the question, if the question part of a Q&amp;A pair addresses the same information need as the query, the Q&amp;A pair is considered to be relevant to the query. The second file (Rel 2) considers both the relevance and the quality of Q&amp;A pairs. If the quality of the the answer is judged as  X  X ad X , then the Q&amp;A pair is removed from the relevance judgment file even if the question part is judged as relevant to the query. The last judgment file (Rel 3) requires a stronger requirement of quality. If the quality of the answer is judged  X  X ad X  or  X  X edium X , then the Q&amp;A pair is removed from the file and only relevant and good quality Q&amp;A pairs remain in the file.
 Rel 2 is a subset of Rel 1 and Rel 3 is a subset of Rel 2. From table 1, Rel 1 contains 1700 Q&amp;A pairs, Rel 2 has 1492 pairs and Rel 3 includes 1099 pairs. Most of the previous experiments in FAQ retrieval, only the relevance of the ques-tion is considered and they used relevance judgment file like Rel 1. We believe the performance measured using Rel 2 or Rel 3 is closer to real user satisfaction, since they take into account both relevance and quality.
We measure retrieval performance using various standard evaluation metrics such as the mean average precision, R-precision and 11pt recall-precision graphs. Table 4 and Fig-ure 4 show the retrieval results.

Table 4 shows that the retrieval performance is signif-icantly improved regardless of the evaluation metric after adding the quality measure. Surprisingly, the retrieval per-formance is significantly improved even when we use the relevance judgment file that does not consider quality. This implies bad quality Q&amp;A pairs tend not to be relevant to any query and incorporating the quality measure pulls down these useless Q&amp;A pairs to lower ranks and improves the re-trieval results overall.

Because Rel 2 has smaller number of relevant Q&amp;A pairs and Rel 3 contains even smaller number of the pairs, the retrieval performance is lower. However, the performance drop becomes much less dramatic when we integrate the quality measure. The retrieval performance evaluated by Rel 2 is better than the performance evaluated by Rel 1, if we incorporate the quality measure. http://www.lemurproject.org/ Table 4: Comparison of retrieval performance. The upper table shows mean average precisions and the lower table shows R-precisions at rank 10. The P-value is calculated using the sign test. Smaller value means more significant difference.

We do a sign test to check whether the improvements are statistically significant. The third rows in Table 4 show the P-values of the test. The results show all the improvements are significant at the 99% confidence level. The significance of the improvement is higher when we use stricter require-ments for the correct Q&amp;A pairs.

Figure 4 shows 11pt recall-precision graphs. In all recall levels, we get improved precisions by adding the quality mea-sure. The improvement becomes bigger when we use Rel 3 than Rel 1.
In this paper, we showed how we could systematically and statistically process non-textual features that are commonly recorded by web services, to improve search quality. We did not use any service or collection specific heuristics. We used statistical methods in every step of the development. Therefore, we believe our approach can be applied to other web services.

We applied our method to improve the quality of the re-trieval service that is attached to a community-based ques-tion answering web site. We predicted the quality of answers accurately using the maximum entropy approach and ker-nel density estimation. The predicted quality scores were successfully incorporated into the language modeling-based retrieval model. We achieved significant improvement in re-trieval performance.

We plan to improve the feature selection mechanism and develop a framework that can handle both textual and non-textual feature together and apply it to other web services.
This work was supported by NHN Corp. and the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this ma-terial are the authors X  and do not necessarily reflect those of the sponsor. [1] A. Berger, S. D. Pietra, and V. D. Pietra. A maximum [2] S. Brin and L. Page. The anatomy of a large-scale [3] R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. [4] D. Harman. Overview of the first text retrieval [5] J. Hwang, S. Lay, and A. Lippman. Nonparametric [6] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [7] J. Jeon and R. Manmatha. Using maximum entropy [8] V. Jijkoun and M. de Rijke. Retrieving answers from [9] H. Kim and J. Seo. High-performance faq retrieval [10] J. M. Kleinberg. Authoritative sources in a [11] W. Kraaij, T. Westerveld, and D. Hiemstra. The [12] L. S. Larkey. Automatic essay grading using text [13] M. Lenz, A. Hubner, and M. Kunze. Question [14] X. Li and W. B. Croft. Time-based language models. [15] R. Malouf. A comparison of algorithms for maximum [16] K. Nigam, J. Lafferty, and A. McCallum. Using [17] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [18] J. M. Ponte and W. B. Croft. A language modeling [19] E. Sneiders. Automated faq answering: Continued [20] D. M. Strong, Y. W. Lee, and R. Y. Wang. Data [21] C.-H. Wu, J.-F. Yeh, and M.-J. Chen. Domain-specific [22] C. Zhai and J. Lafferty. A study of smoothing [23] Y. Zhou and W. B. Croft. Document quality models [24] X. Zhu and S. Gauch. Incorporating quality metrics in
