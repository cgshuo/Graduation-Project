 In recent years, the taxonomy integration pr oblem has obtained much attention in many research studies (e.g. [1,2,3,4,5,6,7]). A taxonomy, or catalog, usually contains a set of objects divided into several categories acco rding to some classified characteristics. In the taxonomy integration problem, the objects in a taxonomy, the source taxonomy S , are integrated into another taxonomy, the destination taxonomy D . As shown in past research, this problem is more than a traditional document classification problem be-cause the implicit information in the source taxonomy can greatly help integrate source documents into the destination taxonomy. For example, a Naive Bayes classification approach can be enhanced with the source implicit importatio n to achieve accuracy improvements [1], and SVM (Support Vector Machines) approaches have similar im-provements [6].

The implicit source information studied in previous enhanced approaches generally includes following features: (1) co-occurrence relationship of source objects [1,6], (2) latent source-destination mappings [2,4], (3) inter-category centroid information [3], and (4) parent-children relationship in the source hierarchy [5,7]. To the best of our sur-vey, however, the semantic information embedded in the source taxonomy has not been discussed. Since different applications ha ve shown that the semantic information can benefit the task performance [8,9], such information should be able to achieve similar improvements for taxonomy integration.

In this paper, we propose an enhanced integr ation approach by exploiting the implicit semantic information in the source taxonomy with a semantic feature expansion (SFE) mechanism. The basic idea be hind SFE is that some semantically descriptive terms can be found to represent a source category, and these representative terms can be further viewed as the additional common category labels for all documents in the category. Augmented with these additional semantic category labels, the source documents can be more precisely integrated into the correct destination category.

To study the effectiveness of SFE, we implemented it based on a hierarchical tax-onomy integration approach (ECI) proposed in [7] with the Maximum Entropy (ME) model classifiers. We have conducted experiments with real-world Web catalogs from Yahoo! and Google, and measured the integra tion performance with precision, recall, and F 1 measures. The results show that the SFE mechanism can further consistently improve the integration performance of the ECI approach.

The rest of the paper is organized as follo ws. Section 2 describes the problem defini-tion and Section 3 reviews previous related research. Section 4 elaborates the proposed semantic feature expansion approach and the hi erarchical integra tion process. Section 5 presents the experimental results, and disc usses the factors that influence the experi-ments. Section 6 concludes the paper and di scusses some future directions of our work. Following the definitions in [7], we assume that two homogeneous hierarchical tax-onomies, the source taxonomy S and the destination taxonomy D , participates in the integration process. The taxonomies are said to be homogeneous if topics of two tax-onomies are similar. The taxonomies under cons ideration are additionally required to be overlapped with a significant number of common documents. In our experimental data sets, 20 . 6 % of the total documents (436/2117) in the Autos directory of Yahoo! also appear in the corresponding Google directory.
 The source taxonomy S has a set of m categories, or directories, S 1 , S 2 , ..., S m . These categories may have subcategories, such as S 1 , 1 and S 2 , 1 . Similarly, the destina-tion catalog D has a set of n categories. The integration process is to directly decide the destination category in D for each document d x in S . In this study, we allow that d x can be integrated into multiple destination categories because a document commonly appears in several different directories in a real-world taxonomy.

Fig. 1 depicts a typical scenario of the int egration process on two hierarchical tax-onomies. For illustration, we assume that the source category S 1 , 1 has significantly some overlapped documents with the destination categories D 1 , 1 and D 2 , 2 . This means that the documents appear in S 1 , 1 should have similar descriptive information as the should be intensively integrated into both two destination categories D 1 , 1 and D 2 , 2 . In previous studies, different sorts of implicit information embedded in the source tax-onomy are explored to help the integration process. As described in Section 1, these implicit source features can be mainly categorized into four kinds. (1) co-occurrence relationship of source objects, (2) latent source-destination mappings, (3) inter-category centroid information, and (4) parent-children relationship in the source hierarchy. The co-occurrence relationships of source objects are first studied to enhance a Naive Bayes classifier based on an intuition that if two documents are in the same source category, they are more likely to be in the same destination category [1]. The enhanced Naive Bayes classifier (ENB) is shown to have more than 14 % accuracy improvement on av-erage. The work in [6] also has the similar concept in its iterative pseudo relevance feed-back approach. As reported in [6], the enhanced SVM classifiers consistently achieve improvements.

Latent source-destination mappings are explored in [2,4]. The cross-training (CT) approach [2] extracts the mappings from the first semi-supervised classification phase using the source documents as the training sets. Then the destination documents are augmented with the latent mappings for the second semi-supervised classification phase to complete the integration. The co-bootstrapping (CB) approach [4] exploits the pre-dicted source-destination mappings to repeatedly refine the classifiers. The experimen-tal results show that both CT and CB outperform ENB [2,4].

In [3], a cluster shrinkage (CS) approach is proposed in which the feature weights of all objects in a documents category are shrunk toward the category centroid. Therefore, the cluster-binding relationships among all documents of a category are strengthened. The experimental results show that the CS-enhanced Transductive SVMs have signifi-cant improvements to the original T-SVMs and consistently outperform ENB.
In [5,7], the parent-children information embedded in hierarchical taxonomies is in-tentionally extracted. Based on the hierarc hical characteristics, Wu et al. extend the CS and CB approach to improve the integration p erformance. In [7], an enhanced approach called ECI is proposed to further extract the hierarchical relationships as a conceptual thesaurus. Their results show that the implic it hierarchical info rmation can be effec-tively used to boost the accuracy performance.
The semantic information embedded in the source taxonomy has not been discussed in past studies. This observation motivates us to study the embedded taxonomical se-mantic information and its effectiveness. In our work, the proposed semantic feature expansion (SFE) approach is currently stud-ied with a hierarchical taxonomy integration approach (ECI) to further study the hi-erarchical integration problem. The Ma ximum Entropy (ME) model is used because of its prominent performance in many tasks, such as natural language processing [10] and flattened taxonomy integration [5]. In the following, the ME model and the ECI approach are introduced first. T hen, SFE is presented in detail. 4.1 The Maximum Entropy Model Here we briefly describe the ME model according to [10] where more details can be the uniformity of p ( y | x ) ,where y is an instance of all outcomes Y in a random process and x denotes a contextual environment of the contextual space X , or the history space. To express the relationship between x and y , we can have an indicator function f ( x, y ) (usually known as feature function) defined as The entropy H ( p ) is defined by The Maximum Entropy Principle is to find a probability model p  X   X  X  such that where C is a set of allowed conditional probabilities. However, there are two constraints: and fined in Equation 6 and E p { f } is the observed expectation of f with the observed distribution  X  p ( x ) from the training data as defined in Equation 7.
 As indicated in [10], the conditional probability p ( y | x ) can be computed by where  X  i is the Lagrange multiplier for feature f i ,and z ( x ) is defined as With the improved iterative scaling (IIS) algorithm [10,11], the  X  i values can be esti-mated. Then the classifiers are built accord ing to the ME model and the training data. 4.2 The ECI Integration Schemes In ECI, the conceptual relationships (categor y labels) is first extracted from the hierar-chical taxonomy structure as a thesaurus [7]. Then the features of each document are extended with the thesaurus by adding the weighted label features. A weighting for-mula is designed to control the impact of th e semantic concepts of each hierarchical level. Equation 10 calculates the ECI feature weight f e x,d of each term x in document d ,where L i is the relevant label weight assigned as 1 / 2 i with an i -level depth, f x,d is the original weight, and  X  is used to control the magnitude relation. The weight f x,d is assigned by TF x / TF i ,where TF x is the term frequency of x ,and i denotes the number of the stemmed terms in each document. The label weight L i of each thesaurus is exponentially decreased and accumula ted based on the increased levels. Table 1 shows the label weights of different levels, where L 0 is the document level, L 1 is one level upper, and so on to fiers for destination categories, the same enhancement on hierarchical label information is also applied to the destination taxonomy t o strengthen the discriminative power of the classifiers. 4.3 Semantic Feature Expansion To further improve the integration performance, the semantic information of inter-taxonomy documents is explored in the proposed approach to perform semantic feature expansion (SFE). The main idea is to augment the feature space of each document with representative topic words. As noted in [12], the hypernyms of documents can be con-sidered as the candidates of the representative topic words for the documents. Hereby, SFE adopts the similar approach as in [12] to first select important term features from the documents and then decide the representative topic terms from hypernyms.
According to previous studies [12,13,14], although the  X  2 -test (chi-square) method is very effective in feature selection for text classification, it cannot differentiate nega-tively related terms from positively related ones. For a term t and a category c ,their  X  2 measure is defined as: where N is the total number of the documents, N + T ( N + F ) is the number of the doc-uments of category c (other categories) containing the term t ,and N  X  T ( N  X  F )isthe number of the documents of category c (other categories) not containing the term t .
Therefore, the correlation coefficient (CC) method is suggested to filter out the neg-atively related terms [12,13]. Since N is the same for each term, we can omit it and get the following equation to calcu late the CC value for each term: Since the categories in a taxonomy are in a hierarchical relationship, SFE only considers the categories of the same parent in the CC method.

Then five terms with the highest CC values are selected to perform semantic feature expansion. As indicated by [12,13], the terms selected with CC are highly representative for a category. However, the category-specific terms of a source category may not be topic-genetic to the corresponding destination category. Therefore, SFE uses them as the basis to find more topic-indicative terms for each category.

Some lexical dictionaries, such as InfoMap [15] and WordNet [16], can be used to extract the hypernyms of the category-specific terms to get the topic indicative features of a category. For example, if a category has the following five category-specific terms: output , signal , circuit , input ,and frequency , SFE gets the following hypernyms from InfoMap: signal , signaling , sign , communication , abstraction , relation , etc. These hy-pernyms are more topic-generic than the category specific terms. Then SFE calculates the weight HW x of each extracted hypernym x by where HF x is the term frequency of x ,and i denotes the number of the hypernyms in each category.

For each document d k , its SFE feature vector sf k is changed by extending Equa-tion 10 as follows: where l k denotes the feature vector of the hierarchical thesaurus information computed from the left term of Equation 10, h k denotes the feature vector of the topic-generic terms of the category computed from Equation 13), and f k denotes the original feature vector of the document derived from the right term of Equation 10. We have conducted experiments with real-world catalogs from Yahoo! and Google to study the performance of the SFE scheme with an Maximum Entropy classification tool from Edinburgh University (ver. 20041229) [17]. Two versions were implemented. The baseline is ME with ECI (ECI-ME), and the other is ME with ECI and SFE (SFE-ME). We measured three scores with different  X  and  X  settings: precision, recall, and F 1 mea-sures. The experimental results show that SFE-ME can effectively improve the integra-tion performance. In precision and recall, SFE-ME outperforms ECI-ME in more than 70% of all cases. SFE-ME can also achieve the best recall and precision performance. In
F are detailed in the following. Due to the pape r length limitation, this paper only reports part of our results of integrating Google taxonomies into Yahoo! taxonomies. 5.1 Data Sets In the experiments, five directories from Yahoo! and Google were extracted to form two experimental taxonomies (Y and G). Table 2 shows these directories and the number of the extracted documents after ignoring the documents that could not be retrieved. As in previous studies [1,2,7], the documents appearing in only one category were used as the training data ( | Y-G | and | G-Y | ), and the common documents were used as the testing data ( | YTest | and | GTest | ). Since some documents may appear in more than one category in a taxonomy, | YTest | is slightly different with | GTest | . For simplicity experiments. If the number of the documents of a certain subcategory is less than 10, the subcategory would be merged upward to its parent category.

Before the integration, we used the stopword list in [18] to remove the stopwords, and the Porter algorithm [19] for stemming. In the integration process, we allow that each source document d x can be integrated into multiple destination categories (one-to-many) as what we can find in real-world taxonomies. Different  X  values from 0 . 1 to 1 . 0 were applied to the source taxonomy (  X  s ) and the destination taxonomy (  X  d ). To both taxonomies, the same  X  value ranging from 0 . 1 to 1 . 0 was applied for semantic feature expansion. The lexical dictionary used in the experiments was InfoMap [15] to get hypernyms. As reported in [12], we believe that WordNet will result in similar hypernym performance.
 In the experiments, we measured the integration performance of ECI-ME and SFE-ME in six scores: macro-averaged recall ( MaR), micro-averaged recall (MiR), macro-averaged precision (MaP), micro-aver aged precision (MiP), macro-averaged F 1 measure (MaF) and micro-averaged F 1 measure (MiF). The standard F 1 measure is defined as the harmonic mean of recall and precision: F 1 =2 rp/ ( r + p ) , where re-p puting the scores globally over all categories in five directories. The macro-averaged scores were measured by first computing the scores for each individual category, and then averaging these scores. The recall measures are used to reflect the traditional per-formance measurements on in tegration accuracy. The precision measures show the de-grees of false integration. The standard F 1 measures show the compromised scores between recall and precision. 5.2 Experimental Results and Discussion Although we have measured the integration performance with different  X  values, this Considering  X  , we have also measured the integration p erformance with different values ranging from 0.1 to 1.0. When  X  is between 0.2 to 0.5, SFE-ME is superior to ECI-ME. Here we only report the  X  =0 . 4 case to save paper space.
 Table 3 and Table 4 show the macro-averag ed and micro-averaged recall results of ECI-ME and SFE-ME in different  X  settings. Table 5 and Table 6 show the macro-averaged and micro-averaged precision results of ECI-ME and SFE-ME in different  X  settings. Table 7 and Table 8 show the macro-averaged and micro-averaged F 1 measure results of ECI-ME and SFE-ME in different  X  settings.

From Table 3 and Table 4, we can notice that SFE-ME is superior to ECI-ME in more than 75% of all MaR scores and in more than 60% of all MiR scores. Among these cases, SFE-ME can achieve the best MaR of 0.8915 and the best MiR of 0.9301 when  X  s =0 . 1 and  X  d =0 . 1 .When  X  d =0 . 1 and  X  s  X  0 . 3 , ECI-ME outperforms SFE-ME. It appears that the imbalanced weighting between  X  d and  X  s seriously impairs the SFE improvement.

From Table 5 and Table 6, we can notice that SFE-ME is superior to ECI-ME in more than 80% of all MaP and MiP scores. Among th ese cases, SFE-ME can achieve the best MaR of 0.6662  X  s =1 . 0 and  X  d =0 . 1 and the best MiR of 0.6078 when  X  s =0 . 9 and  X  has lower recall scores but better precision performance.

For many applications, a compromised p erformance may be required with a high F 1 score. From Table 7 and Table 8, we can notice that SFE-ME is superior to ECI-ME in 88% of all MaF and MiF scores. In our experiments with  X  =0 . 4 , SFE-ME achieves the best MaF (0.6839) and the best MiF (0.6764) when  X  s =0 . 6 and  X  d =0 . 1 .It reveals that the SFE scheme can mostly get m ore balanced improvements in both recall and precision considerations.

We have also measured these six scores for the  X  s =0 . 0 ,  X  d =0 . 0 ,and  X  =0 . 0 case which means that the integration is performed by only ME without ECI and SFE enhancements. In this configuration, ME can achieve the highest MaR (0.9578) and MiR (0.9616) but with very low MaP (0.0111) and MiP (0.0111). Its MaF and MiF are 0.022 and 0.0219, respectively. Although ME can get the best recall performance, it allows many documents of other categories to be mis-integrated.

The experimental results are positive to show that SFE-ME can get more improved in-tegration performance with the SFE scheme. Compared with ECI-ME, SFE-ME shows that the semantic information of the hypernyms of the category-specific terms can be used to facilitate the integration proces s between two hierarchical taxonomies. In recent years, the taxonomy integration problem has been progressively studied for integrating two homogeneous hierarchical taxonomies. Many sorts of implicit informa-tion embedded in the source taxonomy are explored to improve the integration perfor-mance. However, the semantic information embedded in the source taxonomy has not been discussed in the past research.

In this paper, an enhanced integration approach (SFE) is proposed to exploit the semantic information of the hypernyms of the category-specific terms. Augmented with these additional semantic category features, the source documents can be more precisely integrated into the correct destination category in the experiments. The experimental results show that SFE-ME can ach ieve the best macro-averaged F 1 score and the best micro-averaged F 1 score. The results also show that the SFE scheme can get precision and recall enhancements in a signi ficant portion of call cases.

There are still some issues left for further discussion. For example, we do not yet clearly know whether SFE can be applied to oth er classification schemes, such as SVM embedded in the source taxonomy with more po werful discriminat ive capability. We believe that the integration performance can be further improved with appropriate as-sistance of more effective auxiliary information and advanced classifiers. This work was supported in part by National Science Council of R.O.C. under grant NSC 96-2422-H-006-002 and NSC 96-2221-E-155-067. The authors would also like to express their sincere thanks to anonymous reviewers for their precious comments.
