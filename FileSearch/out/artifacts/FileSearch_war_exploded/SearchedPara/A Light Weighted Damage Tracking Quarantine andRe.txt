 As online applications gain popularity in today X  X  E-Business world, surviving DBMS from an attack is becoming crucial because of the increasingly critical role that database servers are playing. Although a number of research projects have been done to tackle the emerging data corruption threats, existing mechanisms are still limited in meeting four highly desired requirements: near zero run time overhead , zero sys-tem down time . In this paper, we propose TRACE , a light weighted database Damage Tracking, Quarantine, and Re-covery (DTQR) solution with negligible run time overhead. H.2 [ DATABASE MANAGEMENT ]; H.2.7 [ Database Administration ]: [Security, Integrity, and Protection] Security, Algorithm, Performance Security, Database Management System, Transaction Pro-cessing, Data Integrity
Database Damage Management (DDM), especially trans-parent Damage Tracking, Quarantine and Recovery (DTQR), is an important problem faced today by a great number of mission/life/business-critical applications. These applica-tions are the cornerstones of a variety of crucial information systems (e.g., banking, online stock trading, and air traf-fic control, etc) that must manage risk, business continuity, and data assurance in the presence of severe cyber-attacks. Today, many of the nation X  X  critical infrastructures (e.g., financial services, telecommunication infrastructure, trans-portation control) rely on these crucial information systems to function. Although significant progress has been made in protecting applications and systems, mission/life/business-critical applications still have a  X  X ood X  chance to suffer from a big  X  X it X  from attacks. Furthermore, due to data shar-ing, interdependencies, and interoperability between busi-ness processes and applications (e.g., the emerging Web Ser-vices), the hit could greatly magnify its damage by causing catastrophic cascading effects, which may X  X orce X  X n applica-tion to shut down itself for hours or even days before the ap-plication is recovered (e.g., the transaction damage spread-ing example shown in Figure 1). In addition, because not all intrusions can be prevented, DTQR is an indispensable part of the corresponding security solution, and the DTQR scheme quality may have significant impact on risk manage-ment, business continuity, and data assurance. Hence, these mission/life/business critical applications highly demand a high quality transparent damage quarantine and recovery scheme. Although a good number of research projects have been done to tackle the emerging data corruption threats, existing DTQR mechanisms are still quite limited in meet-ing four highly desired requirements: (R1) near zero run time overhead , (R2) zero system down time . As a result, these proposed approaches introduce three apparent issues: 1) substantial run time overhead, 2) long system outage, 3) substantial legitimate work loss. To overcome the above limitations, we propose TRACE , a zero system down time database damage tracking, quarantine, and recovery solu-tion with negligible run time overhead. The service outage is minimized by (a) cleaning up the compromised data on-the-fly, and (b) doing damage assessment and damage cleansing concurrently to minimize delay time for read-write transac-tions.
TRACE has two working modes, the standby mode and the cleansing mode. If no data corruption is reported by Intrusion Detection System (IDS), TRACE works in the standby mode and is invisible to the incoming transactions which are executed normally. If the IDS raises an alarm, TRACE will be activated and works in the cleansing mode to execute quarantine/assessment/cleansing procedures. Fig-ure 2 illustrates the workflow of TRACE system. We use  X  X leansing X  rather than  X  X ecovering X  throughout this paper to emphasize the additional feature of our approach, which preserves the legitimate data in the process of restoring the database back to the consistent status in the recent past.
In the following sections, we overview our approach that enables TRACE system to meet the desired requirements. To build the TRACE that offers the feature of identify-ing/cleansing the corrupted data objects and meets the four requirements, we make several changes to the source code of the standard PostgreSQL 8.1 database [3].
We implement TRACE as a subsystem in PostgreSQL database system, and evaluate the performance of TRACE based on TPC-C benchmark [2]. We present the experimen-tal results based on the following evaluation metrics. First, motivated by requirement R1, we demonstrate the system run time overhead imposed by TRACE. Second, we demon-strate the comparison of TRACE and  X  X oint-in-time X  (PIT) recovery method in terms of the requirement R2. The exper-iments conducted in this paper run on Debian GNU/Linux with Intel Core Due Processors 2400GHz, 1GB of RAM. We choose PostgreSQL 8.1 as the host database system and compile it with GCC 4.1.2.
We evaluate the system run time overhead of transactions with only update statements. We use the application built up based on TPC-C benchmark. Up to 20,000 transactions execute on each application. For TPC-C application, we set up each transaction containing no larger than 5 update statements. Figure 3(a) shows the comparison of system overhead of TRACE and the raw PostgreSQL system on TPC-C based application. Because TRACE provides addi-tional functionalities, it has system overhead on the Post-greSQL by the size of transaction in terms of the number of update statements. The overhead introduced by TRACE comes from the following possible reasons: 1) for every in-sert/update operation, TRACE needs to create a CT entry and updates the timestamp field in CT. 2) To identify the invalid data records, TRACE maintains a causality table, which needs to allocate and access more disk storage when storing the causality information. For the TPC-C case of 20K transactions, we run the experiment 50 times and the average time of executing a transaction is 7.1 ms. Addi-tional 0.58 ms is added to each transaction (8% on average) to support causality tracking. In comparison with [1], which adds approximate 25%-35% run time overhead to the sys-tem, TRACE achieves a great improvement.
To evaluate the performance of TRACE on the require-ment R2, we demonstrate in Figure 3(b) the system through-put of the PostgreSQL with/without TRACE based on the TPC-C application. To filter out potential damage spread-ing transactions, we assume the transaction dependence is tight. For example, if a transaction T x does not access com-promised data but rely on the result of a transaction T y transaction T x will still be filtered out (held in the active transaction queue) if transaction T y is filtered out due to accessing compromised data because the result directly from transaction T y is dirty. In Figure 3(b), we present an ap-proximate 40 seconds system running-time window. Until the time point 11, the database system runs normally. Dur-ing this partial time window, on average the throughput of PostgreSQL is slightly higher than the PostgreSQL with TRACE because TRACE will add system overhead into the system. At time point 11 (around 33 sec), a malicious trans-action is identified. For traditional PostgreSQL system, the system shutdowns itself and stops providing service. For the PostgreSQL with TRACE, the system enables TRACE to carry out the damage quarantine/assessment/cleansing pro-cedure. However, the database service is not harmed and the database system continues providing data access to new transactions while TRACE functions. During this partial time window (point 11 to point 16), the database armed with TRACE can still achieve near 57 T/s system through-put. In the worst time point, the throughput degradation ratio of TRACE is less than 40%, and the degradation ratio is quickly improved to 20% within 3 seconds. Overall, the requirement R2 is achieved.
We have dealt with the problem of malicious transactions that result in corrupted data. TRACE identifies the invalid data records and all subsequent data submitted by legiti-mated transactions affected by the malicious transactions directly or indirectly. Our marking scheme used in damage assessment enables us only de-commit the effects from af-fected transactions. Overall, our system removes far fewer transactions than the conventional recovery mechanisms. [1] P. Ammann, S. Jajodia, and P. Liu. Recovery from [2] http://www.tpc.org/tpcc/. TPC-C Benchmark . [3] Postgresql. http://www.postgresql.org/.
