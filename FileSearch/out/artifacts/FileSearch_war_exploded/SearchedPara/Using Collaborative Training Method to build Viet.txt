 exchanges between the two countries. A large-s cale dependency treeban k can provide strong support for machine translation and other upper applications. Therefore, building Vietnamese dependency treebank has an important practical significance. Currently, the construction of dependency treebank for English and other large languages has got some achievements. But research about Vietnamese is still relatively less and there are a lack of large-scale Vietnamese dependency treebanks.
 achievements in morphology and bilingual alignment method[1,2,3]. But research on building dependency treebank is relatively inadequate. With the rapid development of statistical learning, today more and more researchers use this method to study language information processing. Among them, Lai and others used the idea of span and statistical learning to solve the problem of Chinese dependency parsing in 2001[4]. Yamada and others converted the English sentences in the Penn Treebank to the dependency structure completely in 2003. Then they used the statistical learning method to analyze these sentences and finally achieved the accuracy of 90.3%[5]. Ma Jinshan built the SVM dependency parsing model through using the marked Chinese dependency treebankin2004andfinallysolvedtheChines e dependency parsing[6]. These methods above mainly relied on supervised learning of depende ncy treebank resource to achieve dependency parsing. P.T.Nguyen and others converted ten thousand phrase trees in the Penn Treebank to dependency trees in 2013[7]. But the scale was still relatively small.
 marking dependency treebank is very difficult and currently there isn X  X  a mature dependency parser. For the construction of Vietnamese dep endency treebank, marking it manually is very difficult and this process requires a lot of manpower and other material resource. Moreover, in reality there is a lot of unmarked crude corpus and the corpus has not undergone any processing. Therefore, how to use the corpus to build Vietnamese dependency treebank effectively has become an important issue for Vietnamese dependency parsing.
 maximum spanning tree (MST) algorithm with the improved Nivre algorithm to build Vietnamese dependency treebank. The method was aimed to explore how to use unmarked crude corpus effectively with the help of collaborative training. Firstly, we selected some Vietnamese sentences marked manually as the initial training corpus and used them to build two weak learners. Secondly, we used a large number of unmarked Vietnamese sentences to mark each other and extracted a sample of high trust to train and update on the two learners repeatedly. Finally, we achieved building a Vietnamese dependency treebank of hig h accuracy successfully. Experimental results showed that the proposed method in this paper improved the UAS, LAS, RA and the accuracy of other aspects more significantly than other methods. proposed by McDonald et al.(2005c). This formulation leads to ef fi cient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm(Eisner, 1996) and the Chu-Liu-Edmonds algorithm(Chu and Liu, 1965; Edmonds, 1967) respectively. The formulation works by de fi ning the score of a dependency tree to be the sum of edge scores: of tree edges and write (i,j)  X  y to indicate an edge in y from word xi to word xj.
 high-dimensional feature representation of the edge with a corresponding weight vector: learned during training. We should note that f (i,j) can be based on arbitrary features of the edge and the input sequence x.
 highest scoring subgraph of G that satis fi es the tree constraint over the vertices V . By de fi ning a graph in which the words in a sentence are the ver tices and there is a directed edge between all words with a score as calculated above, McDonald e t al. (2005c) showed that dependency parsing is equivalent to fi nding the MST in this graph. Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms. Although the MST formulation applies to any directed graph, our feature representations and one of the words in the sentence.
  X  as a directed graph  X   X  , GVE  X  , where the words in the sentence constituted a set of vertexs of G and connection from vertex i to vertex j in the dependency tree, there was a directed edge between i and j. The weight of each directed edge was defined as probability of j depending on I and y was a dependency type. The weight of a dependency tree was the sum of the weights of all directed edges. Therefore, this dependency parsing method would convert looking for the best result into searching for the maximum spanning tree in the directed graph model of dependency parsing through training. The model can predict the next state according to the current state and the features of input sentences and previous decisions. During the dependency parsing, the analyzer transfers greedily from a primitive state to a subsequent state according to the forecast sets of the model until it reaches the end state.
 operation is not very accurate. In order to solve this problem, the paper proposed an improved Nivre algorithm.
 The input sequence to be parsed is stored in I. A is a set and it can be used to store the determinate dependency items in the process of parsing. Given an input sequence Sen, the parser is firstly initialized as ,, nil Sen stack S and the top element n of stack I. Then, the parser takes appropriate action to move the elements and control the algorithm iteration until stack I is empty. At the moment, the parser stops iterativing and outputs the dependency sequences of set A. The Nivre algorithm defines a total of four operations: into set A and the element n is pushed into stack S. Finally, the triad becomes || ,, {( )} nt SIA n t  X  X  X  X  X  .
 definition about the Reduce operation and the Shift operation. side and there is a dependency between its parent node and n, the parser pops up t from stack S. Finally, the triad becomes ,|, Sn I A  X  X  . Finally, the triad becomes || ,, nt SIA  X  X  . Ramakrishnan 1999). BUC processes the partitions starting from a single attribute and moves towards the apex of the lattice. BUC relies on APRIORI-like pruning to reduce the computation space. BUC is a divide and conquer strategy, and partitioning is its major cost. BUC can be used to compute either a full data cube or an iceberg cube. Due to its pruning power, BUC works especially well at computing iceberg cubes for sparse database tables. As well, BUC is not memory intensive. When the database is dense, the dividing into partitions costs more and the pruning is less effective, so the overall perfo rmance of BUC degrades. According to extensive studies, BUC is faster than TDC in most cases (Findlater and Hamilton 2003).
 2003) is a top-down approach that starts from the least aggregated group-bys at the top of the lattice and works its way down to the most aggregated group-bys at the bottom. Each underlined group-by is also an ordering, i.e., a child group-by that permits the shared computation of its parent and other ancestor group-bys during the pass in which it is being computed. Using orderings, the number of passes over the database can be reduced. TDC uses orderings to cover all (Findlater and Hamilton 2003). When processing an ordering, TDC simultaneously aggregates all group-bys that are pre fi xes of it. Shared computation is the main advantage of TDC. The main disadvantage of TDC is weak pruning, i.e., it is relatively poor at identifying cases where pruning is possible. that the data set has two fully redundant views, that is to say the data set has several attribute sets to meet the following two conditions: Firstly, the training data of each attribute set is enough to describe the problem and each attribute set can obtain a weak learner through learning. Secondly, any two attribute sets are independent conditionally in the process of marking.
 views respectively. Secondly, each classifier selects some samples of high confidence from unmarked samples. Next, the selected samples are added into another classifier to train after being marked, so that the classifier can use the newly added marked samples to train and update. Finally, the two classifiers update and iterate constantly until the parameters of the model converge. Research has shown that when the assumption about fully redundant views is established, cooperative training can effectively use unmarked samples to improve training performance. The standard collaborative training method is shown in figure 1, where X1 and X2 respectively represent the corresponding samples of view 1 and view 2.
 of corpus is very important for the construction of treebank. Because corpus is important for both annotation and experiment.
 Radio The Voice of Vietnam(Abbreviation:VOV). The news corpus was covered with politics, economy, military, sports, entertainment and other aspects, thus ensuring a diversity of experimental data. The next step was to process the original crude data manually to obtain 100,000 standard Vietnamese sentences. Then 10,000 Vietnamese sentences among them were selected to do manual annotation and repeated proo freader in order to obtain the initial training corpus and experimental test corpus. Either of th em was a small Vietnamese dependency treebank and they both contained 5000 marked Vietnamese sentences. The remaining 90,000 unmarked Vietnamese sentences were used as the experimental extended corpus. The selecting and processing of corpus was shown in figure 2. of the most important work. The marking standard of high quality should be able to accurately reflect the inherent regular pattern of language and lay a good foundation for the next research. improving the marking quality and manual proofreader efficiency. In addition, the appropriate standard will play a positive role for training and testing data.
 standard table in line with the features of Vietnamese. The marking standard should contain two elements: the first is that which words will exist dependency in a Vietnamese sentence; the second is how to define their dependency types.
 sentence, there should be a dependency between words which have some relationship in semantic level, that is to say generating dependency bet ween them can promote new semantic. The paper called it the semantic principle. When marking dependency, the semantic principle should be given the priority.
  X  X  p(beautiful) c X  g X i(girl) X , the two words  X  X   X  t (is) and c X  g X i (girl) X  generate relationship can just promote new semantic. So there is a dependency between the two words.
 They are essential in the sentence, so they are key words. However, some words play a auxiliary role in the sentence, only modifying the key words, even removing them will not affect the expression of the sentence, so they are minor words. When marking dependency, it should be ensure that the key words must be located in the core of the dependency. The minor words should depend on the key words. The paper called it the trunk principle. So that it is easy to extract the main components of a sentence in subsequent applications through the dependency. Vietnamese dependency parsing is mainly led by the predicate and analyzes the relationship between the predicate and other components.
 th  X  nh_t X nh (heart) ti  X  p_  X   X i (hospitality) X  ,thewords  X  X   X  m_  X  n (thank) and ti  X  p_  X   X i (hospitality) X  are the key words of the sentence. Other words only modify them. So, there should be a dependency between the two words.
 Vietnamese dependency marking standard table. In order to cover various grammatical phenomena more accurately, but not lead to the problem of marking difficulty and sparse data because of excessive dependency types, the paper developed 14 kinds of dependency marking standard in line with the features of Vietnamese through analyzing Vietnamese grammar. They wereshownintable1.
 initial corpus. The annotation storage and structure of the dependencies was shown in table 2. relatively simple. Therefore, the paper selected the current word W0, its previous word W-1, its front second word W-2, the next word W1, the next second word W2 and the part-of-speech of the current word POS0, the part-of-speech of its previous word POS-1, the part-of-speech of its front second word POS-2, the part-of -speech of the next word POS1, the part-of-speech of the next second word POS2 as the features. The feature selection was shown in table 3.
 coverage on them. Because it not only met the basic needs but also effectively avoided the sparse data due to excessive feature selection. is to build two learner models and do collaborative learning. This paper proposed the method which combined the MST algorithm[8] and the improved Nivre algorithm[9] to build two weak learner models. In the process of collaborative training, this paper used the K-Best algorithm to select one learner X  X  forecast results, and regarded the results of high confidence as the input of the other learner to train and update repeatedly until the parameters of the learner models converged. large number of unmarked samples effectively for collaborative training. In the process of predicting unmarked samples, the confidence judgment criterion was particularly important. In order to measure the forecast results, we used the K-Best algorithm to determine the confidence. If the K weight scores of forecast results were closer, it showed that the confidence was lower. If the weight difference of forecast results was greater, the results were more accurate. Then we chose the forecast result of the highest weight score as the marked result of Vietnamese sentence. results: where results.
 confidence was more higher. Method three used the entropy to determine the confidence. combination method[10] which regarded the foreca st results of one model as the training corpus of the other to promote their mutual learning of the two models.
 weak dependency parsing learners S1and S2 through training as two fully redundant views. The learner S1 was based on the MST algorithm. The learner S2 was based on the improved Nivre algorithm. Secondly, the paper randomly selected some of unmarked Vietnamese sentences as set A and set B from many unmarked samples. Then the paper respectively used the set A and the set B to predict Vietnamese dependency. The paper regareded 100 unmarked Vietnamese sentences as a unit and used the learner S1 to predict the 100 sentences. Next, the paper used the formula 1 to select 20 sentences of high confidence to mark, and then added these marked sentences into the learner S2 to train and update. Conversely, the paper also regareded 100 unmarked Vietnamese sentences as a unit and used the learner S2 to predict these 100 sentences. Next, the paper used the formula 1 to select 20 sentences of high confidence to mark, and then added these marked sentences into the learner S1 to train and update. This cycle repeated until the parameters of the learner S1 and the learner S2 became unchanged. The process of collaborative training was shown in figure 4. Vietnamese dependency parsing for a large number of unmarked Vietnamese sentences and build Vietnamese dependency treebank. In the process of building Vietnamese d ependency treebank, if the forecast results of the two learners were consistent, the results were correct. If the results were inconsistent, the paper used the formula 2 and the formula 3 to calculate confidence.
 calculate their average score for the forecast results of the two learners and the paper selected a higher score as the correct prediction. The paper used the model to do Vietnamese dependency parsing for 90,000 unmarked Vietnamese sentences. Then the paper also used the model to do Vietnamese dependency parsing for 5000 Vietnamese sentences in the test corpus and ultimately build a large-scale Vietnamese dependency treebank. Attachment Score(LAS) and the Root Accuracy(R A) as the evaluation standard of the final built dependency treebank. They were defined as follows: Vietnamese sentences corpus effectively and improve the accuracy of dependency treebank, the paper designed three groups of comparative experiments to respectively build Vietnamese dependency treebank and compared the experimental results of different methods.
 maximum entropy to design experiments to build Vietnamese dependency treebank. They were respectively the Bottom-Up algorithm, the Top-Down algorithm and the MST algorithm. It was easy to find that the MST algorithm in USA, LAS and RA was the highest after the comparison of the experimental results. The first comparative experimental results were shown in table 4. algorithm to design experiments to build Vietnamese dependency treebank. Its experimental result in USA, LAS and RA was higher than that of the MST algorithm. Therefore, in order to take full advantage of these two algorithms and enhance their complementarity, the paper used the combination of the MST algorithm and the improved Nivre algorithm through collaborative training to build Vietnamese dependency treebank.
 design experiments to build Vietnamese dependency treebank. In this experiment, the paper expanded 90000 Vietnamese sentences corpus. It was easy to find that the UAS, LAS and RA of the dependency treebank based on collaborative training had been significantly improved compared with the improved Nivre algorithm after the comparison of the experimental results. In addition, we compared the collaborative training method with the latest Chinese-Vietnamese bilingual-word-alignment-corpus-based met hod. It was easy to find that the UAS, LAS and RA of the dependency treebank based on the former were all higher than the latter. It fully proved the effectiveness of the collaborative training method. The second and third comparative experimental results were shown in table 5.
 Vietnamese dependency treebank based on the collaborative training method in UAS, LAS and RA was the highest. Because the MST algorithm uses the dependency tree of the whole sentence for training and utilizes the maximum spanning tree to search for the optimal dependency tree in the process of building dependency treebank. The intermediate results of parsing cannot be applied to the subsequent analysis, leading to the low accuracy. However, the improved Nivre algorithm is based on state transition process for training and it searches for the partial optimum transfer status until the whole sentence parsing ends in the process of building dependency treebank. So the improved Nivre algorithm has the features of locality and greed and this is the reason why it has low accuracy.
 complementarity of the MST algorithm and the improved Nivre algorithm. It regards the forecast results of one model as the input of the other. When the analysis accuracy of the two models differs little, the combined model can improve the accuracy of UAS, LAS and RA significantly. In this paper, the final built dependency treebank contained 100,000 Vietnamese sentences and it eventually obtained the accuracy of 76.33%. Compared with other methods to build Vietnamese dependency treebank, the final built dependency treebank in this paper had a larger scale and higher accuracy.
 Vietnamese, there would be inevitably some errors in the final built treebank. Although these errors were less, they were difficult to find. So the repeated manual correction for the final built dependency treebank was necessary and this work had a great significance for improving the quality of the final built dependency treebank. the maximum entropy and the improved Nivre algorithm in depth and found that only using one algorithm to build Vietnamese dependency treeb ank was not very satisfactory. Therefore, the paper proposed the method which combined the MST algorithm and the improved Nivre algorithm to build Vietnamese dependency treebank. The method was based on the idea of collaborative training. Experimental results showed that the proposed method had a better effect than only using an algorithm. It also had a stable parsing performance and could effectively improve the accuracy of the final built depen dency treebank. The Vietnamese dependency treebank resource was relatively inadequate. But the proposed method in this paper could effectively use unmarked Vietnamese sentences to build Vietnamese dependency treebank. The method solved the experimental difficulty due to the lack of sample corpus. At the same time, the method avoided the process of manually marking Vietnamese dependency treebank and fully saved the time of manpower and other material resource.
 Vietnamese. In some Vietnamese sentences, some words represent a development trend of things. But these words doesn X  X  make much sense for the expression of the whole sentence. Moreover, there isn X  X  any dependency between these words and other ingredients of the sentence. For example, there is a Vietnamese sentence  X  X oa (Flower)  X  ang (is) d  X  n(slowly)d  X  n(slowly)n  X  (open) ra. X  . In the sentence,  X  X a X  is such a word. It represents a development trend that the flower is slowly open. In this case, the parsed dependencies based on the proposed method in this paper must be wrong. Moreover, this type of error is difficult to find through manual correction. So in the future work, we will do further research about how to remove these wrong dependencies in the process of building dependency treebank. If these dependencies are removed, the accuracy of the dependency treebank can also be significantly improved.
 through collaborative training and compare these methods with the proposed method in this paper. Our ultimate goal is to build a more-fusion-method, higher-accuracy and more-larger-scale Vietnamese dependency treebank.
 References
