 Look at the two persons in Fig. 1(a), can you tell they are doing two different actions? Once the entire contexts of these two images are revealed (Fig. 1(b)) and we observe the interaction of the person with other persons in the group, it is immediately clear that the first person is queuing, while the second person is talking. In this paper, we argue that actions of individual humans often cannot modeling the collective behaviors of individuals in the group.
 Before we proceed, we first clarify some terminology used throughout the rest of the paper. We use action to denote a simple, atomic movement performed by a single person. We use activity to refer to a more complex scenario that involves a group of people. Consider the examples in Fig. 1(b), each frame describes a group activity: queuing and talking, while each person in a frame performs a lower level action: talking and facing right, talking and facing left, etc.
 Our proposed approach is based on exploiting two types of contextual information in group activ-(we call it the group-person interaction ) for each other, hence should be modeled jointly in a unified framework. As shown in Fig. 1, knowing the group activity (queuing or talking) helps disambiguate individual human actions which are otherwise hard to recognize. Similarly, knowing most of the persons in the scene are talking (whether facing right or left) allows us to infer the overall group activity (i.e. talking). Second, the action of an individual can also benefit from knowing the actions of other surrounding persons (which we call the person-person interaction ). For example, consider Fig. 1(c). The fact that the first two persons are facing the same direction provides a strong cue that both of them are queuing. Similarly, the fact that the last two persons are facing each other indicates they are more likely to be talking.
 Related work: Using context to aid visual recognition has received much attention recently. Most of the work on context is in scene and object recognition. For example, work has been done on ex-ploiting contextual information between scenes and objects [13], objects and objects [5, 16], objects and so-called  X  X tuff X  (amorphous spatial extent, e.g. trees, sky) [11], etc.
 Most of the previous work in human action recognition focuses on recognizing actions performed provided by scenes [12] or objects [10] to help action recognition. In still image action recognition, object-action context [6, 9, 23, 24] is a popular type of context used for human-object interaction. The work in [3] is the closest to ours. In that work, person-person context is exploited by a new feature descriptor extracted from a person and its surrounding area.
 Our model is directly inspired by some recent work on learning discriminative models that allow the use of latent variables [1, 6, 15, 19, 25], particularly when the latent variables have complex structures. These models have been successfully applied in many applications in computer vision, e.g. object detection [8, 18], action recognition [14, 19], human-object interaction [6], objects and attributes [21], human poses and actions [22], image region and tag correspondence [20], etc. So far only applications where the structures of latent variables are fixed have been considered, e.g. a tree-structure in [8, 19]. However in our applications, the structures of latent variables are not fixed and have to be inferred automatically.
 Our contributions: In this paper, we develop a discriminative model for recognizing group ac-tivities. We highlight the main contributions of our model. (1) Group activity: most of the work in human activity understanding focuses on single-person action recognition. Instead, we present a model for group activities that dynamically decides on interactions among group members. (2) Group-person and person-person interaction: although contextual information has been exploited for visual recognition problems, ours introduces two new types of contextual information that have not been explored before. (3) Adaptive structures: the person-person interaction poses a challenging problem for both learning and inference. If we naively consider the interaction between every pair of persons, the model might try to enforce two persons to have take certain pairs of labels even though these two persons have nothing to do with each other. In addition, selecting a subset of connec-tions allows one to remove  X  X lutter X  in the form of people performing irrelevant actions. Ideally, we would like to consider only those person-person interactions that are strong. To this end, we propose to use adaptive structures that automatically decide on whether the interaction of two persons should be considered. Our experimental results show that our adaptive structures significantly outperform other alternatives. the interactions among them. We introduce two new types of contextual information, group-person interaction and person-person interaction . Group-person interaction represents the co-occurrence between the activity of a group and the actions of all the individuals. Person-person interaction indicates that the action of an individual can benefit from knowing the actions of other people in the same scene. We present a graphical model representing all the information in a unified framework. One important difference between our model and previous work is that in addition to learning the parameters in the graphical model, we also automatically infer the graph structures (see Sec. 3). We assume an image has been pre-processed (i.e. by running a person detector) so the persons in the image have been found. On the training data, each image is associated with a group activity label, and each person in the image is associated with an action label. 2.1 Model Formulation A graphical representation of the model is shown in Fig. 2. We now describe how we model an image I . Let I 1 ,I 2 ,...,I m be the set of persons found in the image I , we extract features x from vector extracted from the person I i . We denote the collective actions of all the persons in the image possible action labels. The image I is associated with a group activity label y  X  Y , where Y is the set of all possible activity labels.
 We assume there are connections between some pairs of action labels ( h j ,h k ) . Intuitively speaking, this allows the model to capture important correlations between action labels. We use an undirected label h i , and an edge ( v j ,v k )  X  X  corresponds to the interactions between h j and h k . by w and is defined as follows: The model parameters w are simply the combination of four parts, w = { w 1 ,w 2 ,w 3 ,w 4 } . The details of the potential functions in Eq. 1 are described in the following: the j -th person X  X  action label h j and its image feature x j . It is parameterized as: function. The parameter w 1 is simply the concatenation of w 1 b for all b  X  X  . the group activity label y and the j -th person X  X  action label h j . It is parameterized as: corresponds to an edge in the graph. It is parameterized as: compatibility between the activity label y and the root feature vector x 0 of the whole image. It is parameterized as: The parameter w 0 a can be interpreted as a root filter that measures the compatibility of the class label a and the root feature vector x 0 . We now describe how to infer the label given the model parameters (Sec. 3.1), and how to learn the model parameters from a set of training data (Sec. 3.2). If the graph structure G is known and fixed, we can apply standard learning and inference techniques of latent SVMs. For our application, a good graph structure turns out to be crucial, since it determines which person interacts (i.e. provides action context) with another person. The interaction of individuals turns out to be important for group activity recognition, and fixing the interaction (i.e. graph structure) using heuristics does not work well. We will demonstrate this experimentally in Sec. 4. We instead develop our own inference and learning algorithms that automatically infer the best graph structure from a particular set. 3.1 Inference Given the model parameters w , the inference problem is to find the best group activity label y  X  for a new image x . Inspired by the latent SVM [8], we define the following function to score an image x and a group activity label y : We use the subscript y in the notations h y and G y to emphasize that we are now fixing on a particular Since we can enumerate all the possible y  X  Y and predict the activity label y  X  of x , the main difficulty of solving the inference problem is the maximization over G y and h y according to Eq. 6. Note that in Eq. 6, we explicitly maximize over the graph G . This is very different from previous work which typically assumes the graph structure is fixed.
 The optimization problem in Eq. 6 is in general NP-hard since it involves a combinatorial search. We instead use an coordinate ascent style algorithm to approximately solve Eq. 6 by iterating the following two steps: 1. Holding the graph structure G y fixed, optimize the action labels h y for the  X  x ,y  X  pair: 2. Holding h y fixed, optimize graph structure G y for the  X  x ,y  X  pair: The problem in Eq. 7 is a standard max-inference problem in an undirected graphical model. Here we use loopy belief propagation to approximately solve it. The problem in Eq. 8 is still an NP-hard problem since it involves enumerating all the possible graph structures. Even if we can enumerate all the graph structures, we might want to restrict ourselves to a subset of graph structures that will lead to efficient inference (e.g. when using loopy BP in Eq. 7). One obvious choice is to restrict G 0 to be a tree-structured graph, since loopy BP is exact and tractable for tree structured models. However, as we will demonstrate in Sec. 4, the tree-structured graph built from simple heuristic (e.g. minimum spanning tree) does not work that well. Another choice is to choose graph structures that are  X  X parse X , since sparse graphs tend to have fewer cycles, and loopy BP tends to be efficient in graphs with fewer cycles. In this paper, we enforce the graph sparsity by setting a threshold d on the maximum degree of any vertex in the graph. When h y is fixed, we can formulate an integer linear program (ILP) to find the optimal graph structure (Eq. 8) with the additional constraint that the maximum vertex degree is at most d . Let z jk = 1 indicate that the edge ( j,k ) is included in the graph, and 0 otherwise. The ILP can be written as: where we use  X  jk to collectively represent the summation of all the pairwise potential functions in of [0 , 1] . The solution of the LP relaxation might have fractional numbers. To get integral solutions, we simply round them to the closest integers. 3.2 Learning parameter w that tends to produce the correct group activity y for a new test image x . Note that the action labels h are observed on training data, but the graph structure G (or equivalently the variables z ) are unobserved and will be automatically inferred. A natural way of learning the model is to adopt the latent SVM formulation [8, 25] as follows: where  X ( y,y n ) is a loss function measuring the cost incurred by predicting y when the ground-defined as: The constrained optimization problem in Eq. 10 can be equivalently written as an unconstrained problem: min where L n = max We use the non-convex bundle optimization in [7] to solve Eq. 12. In a nutshell, the algorithm iteratively builds an increasingly accurate piecewise quadratic approximation to the objective func-tion. During each iteration, a new linear cutting plane is found via a subgradient of the objective function and added to the piecewise quadratic approximation. Now the key issue is to compute two subgradients  X  w L n and  X  w R n for a particular w , which we describe in detail below. tion problem: The inference problem in Eq. 13 is similar to the inference problem in Eq. 6, except for an additional enumerate all possible y  X  X  and solve the inference problem in Eq. 6 for each fixed y . Now we describe how to compute  X  w R n , let  X  G be the solution to the following optimization prob-lem: problem in Eq. 14 can be approximately solved using the LP relaxation of Eq. 9. Using the two subgradients  X  w L n and  X  w R n , we can optimize Eq. 10 using the algorithm in [7]. We demonstrate our model on the collective activity dataset introduced in [3]. This dataset contains 44 video clips acquired using low resolution hand held cameras. In the original dataset, all the persons in every tenth frame of the videos are assigned one of the following five categories: crossing, waiting, queuing, walking and talking , and one of the following eight pose categories: right, front-five activity categories including crossing, waiting, queuing, walking and talking . We define forty action labels by combining the pose and activity information, i.e. the action labels include crossing and facing right, crossing and facing front-right, etc. We assign each frame into one of the five activity categories, by taking the majority of actions of persons (ignoring their pose categories) in that frame. We select one fourth of the video clips from each activity category to form the test set, and the rest of the video clips are used for training.
 Rather than directly using certain raw features (e.g. the HOG descriptor [4]) as the feature vector x individual and their associated action labels. In the end, each feature vector x i is represented as a 40-dimensional vector, where the k -th entry of this vector is the score of classifying this instance to the k -th class returned by the SVM classifier. The root feature vector x 0 of an image is also represented as a 40-dimensional vector, which is obtained by taking an average over all the feature vectors x i ( i = 1 , 2 ,...,m ) in the same image.
 Results and Analysis: In order to comprehensively evaluate the performance of the proposed model, we compare it with several baseline methods. The first baseline (which we call global bag-of-words ) is a SVM model with linear kernel based on the global feature vector x 0 with a bag-of-words style representation. The other baselines are within our proposed framework, with various ways of setting the structures of the person-person interaction. The structures we have considered are illus-trated in Fig. 3(a)-(c), including (a) no pairwise connection; (b) minimum spanning tree; (c) graph obtained by connecting any two vertices within a Euclidean distance  X  (  X  -neighborhood graph ) with  X  = 100 , 200 , 300 . Note that in our proposed model the person-person interactions are latent (shown in Fig. 3(d)) and learned automatically. The performance of different structures of person-person in-teraction are evaluated and compared. We summarize the comparison in Table 1. Since the test set is imbalanced, e.g. the number of crossing examples is more than twice that of the queuing or talking examples, we report both overall and mean per-class accuracies. As we can see, for both overall and mean per-class accuracies, our method achieves the best performance. The proposed model signif-icantly outperforms global bag-of-words . The confusion matrices of our method and the baseline global bag-of-words are shown in Fig. 4. There are several important conclusions we can draw from these experimental results: Importance of group-person interaction : The best result of the baselines comes from no connec-tion between any pair of nodes, which clearly outperforms global bag-of-words . It demonstrates the effectiveness of modeling group-person interaction , i.e. connection between y and h in our model. Importance of adaptive structures of person-person interaction : In Table 1, the pre-defined structures such as the minimum spanning tree and the  X  -neighborhood graph do not perform as well as the one without person-person interaction. We believe this is because those pre-defined structures are all based on heuristics and are not properly integrated with the learning algorithm. As a result, they can create interactions that do not help (and sometimes even hurt) the performance. However, if we consider the graph structure as part of our model and directly infer it using our learning algorithm, we can make sure that the obtained structures are those useful for differentiating various activities. Evidence for this is provided by the big jump in terms of the performance by our approach. We visualize the classification results and the learned structure of person-person interaction of our model in Fig. 6. We have presented a discriminative model for group activity recognition which jointly captures the group activity, the individual person actions, and the interactions among them. We have exploited two new types of contextual information: group-person interaction and person-person interaction . We also introduce an adaptive structures algorithm that automatically infers the optimal structure of person-person interaction in a latent SVM framework. Our experimental results demonstrate that our proposed model outperforms other baseline methods.
