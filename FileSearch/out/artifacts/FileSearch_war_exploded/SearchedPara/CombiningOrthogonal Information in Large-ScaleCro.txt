 System combination is an effective strategy to boost re-trieval performance, especially in complex applications such as cross-language information retrieval (CLIR) where the aspects of translation and retrieval have to be optimized jointly. We focus on machine learning-based approaches to CLIR that need large sets of relevance-ranked data to train high-dimensional models. We compare these mod-els under various measures of orthogonality, and present an experimental evaluation on two different domains (patents, Wikipedia) and two different language pairs (Japanese-Eng-lish, German-English). We show that gains of over 10 points in MAP/NDCG can be achieved over the best single model by a linear combination of the models that contribute the most orthogonal information, rather than by combining the models with the best standalone retrieval performance. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.7 [ Arti cial Intelligence ]: Natural Language Processing Algorithms, Experimentation Machine translation, cross-lingual retrieval, patent search
Cross-Language Information Retrieval (CLIR) needs to jointly optimize the tasks of translation and retrieval, how-ever, it is standardly approached with a focus on one aspect. For example, the industry standard leverages state-of-the-art statistical machine translation (SMT) to translate the query into the target language, in which standard retrieval is performed [4]. Most research approaches start from a re-trieval perspective [13], or, more recently, from a machine learning direction [11]. Besides two different tasks, CLIR also needs to incorporate different languages and specialized domains. Thus, techniques that combine specialized systems into an improved joint system are a promising research direc-tion. In this paper we show that a linear system combination can yield improvements of more than 10 MAP/NDCG points over the best single system, if the combined systems repre-sent orthogonal information. We focus on machine learning-based approaches to CLIR that need large sets of relevance-ranked data to train high dimensional models. The systems investigated in this paper are systems based on direct use of SMT technology, systems that apply learning-to-rank tech-niques, systems based on probabilistic neural networks, and methods that incorporate domain-speci c meta-information into linear learners. We present various measures of corre-lation/orthogonality on the level of scores (Pearson's corre-lation coefficient and principal component analysis), ranks (Kendall's rank correlation coefficient), and retrieved docu-ments (Jaccard coefficient), and show on two different do-mains (patents, Wikipedia) and two different language pairs (Japanese-English, German-English) that the contribution of a single system to the combination is best determined by the orthogonality of the information it represents, rather than by its standalone retrieval performance.
Various publications have investigated different methods of system combination for CLIR, including logical opera-tions on retrieved sets [3], voting procedures based on re-trieval scores [1], or machine learning techniques that learn combination weights directly from relevance rankings [14]. The focus of this paper is on machine learning-based CLIR approaches and on metrics to measure orthogonality be-tween these systems. Since all of our models require large sets of relevance-ranked training data, e.g. for learning high-dimensional cross-lingual word matrices, we cannot use stan-dard CLIR datasets from CLEF or TREC campaigns that consist of a few hundred queries with precomputed features. Instead, we use specialized domains such as patents or Wiki-pedia where relevance information can be induced from the citation or link structure. Translation Models. SMT-based models translate a query and then perform monolingual retrieval. Our rst model is called Direct Translation (DT) and uses the SMT framework cdec [5] to generate a single best query translation.
A second model is called Probabilistic Structured Queries (P SQ) . The central idea of this approach is to project query terms into the target language by probabilistically weighted translations from the n -best list of a full SMT system [17].
In both models, we use the Okapi BM25 scoring scheme for document retrieval.
 Ranking Models. Let q 2 f 0 ; 1 g Q be a query and d 2 f 0 ; 1 g D be a document where the n th vector dimension indi-cates the occurrence of the n th word for dictionaries of size Q and D . A linear ranking model is de ned as where W 2 IR Q D encodes a matrix of ranking-speci c word associations [2, 14] . We optimize this model by pairwise ranking, which assumes labeled data in the form of a set R tuples ( q ; d + ; d ), where d + is a relevant (or higher ranked) document and d an irrelevant (or lower ranked) document for query q . We compare two methods to nd a weight matrix W such that an inequality f ( q ; d + ) &gt;f ( q ; d ) is violated for the fewest number of tuples from R .
The rst method uses the Vowpal Wabbit (VW) toolkit [6] to optimize the following  X  1 -regularized hinge loss objective: where ( x ) + = max(0 ;m x ) with margin m and is the regularization parameter. VW was run on a data sample of 5M to 10M tuples from R . On each step, W is updated with a scaled gradient vector  X  W L hng and clipped to account for  X  -regularization.

The second method is a boosting model (BM) that opti-mizes an exponential loss [16]: where D ( q ; d + ; d ) is a non-negative importance function on tuples. The algorithm combines batch boosting with bag-ging over independently drawn bootstrap data samples of 100k instances each from R . In every step, the single word pair feature is selected that provides the largest decrease of L exp . The nal scoring function comprises the averaged re-sulting models. For regularization we rely on early stopping. Neural Network Models. These models utilize the bilin-gual compositional vector model (biCVM) of [9] to train a retrieval system based on a bilingual autoencoder. The training task is to learn two functions f : Q ! R d and g : D ! R d , which map a query q and a relevant document d from a corpus C onto a distributed semantic represen-tation in R d . The energy of a query-document pair ( q ; d ) is de ned by E bi ( q ; d ) = jj f ( q ) g ( d ) jj 2 . Introducing a large margin m into the noise-contrastive update prevents the model from degenerating. This results in the following regularized hinge-loss objective: H = wh ere we treat less relevant documents d as noise samples during training. represents the model parameters.
While [9] train their system exclusively on parallel data on sentence and document level, we examine different training setups where we let the architecture learn distributed repre-sentations from: (a) data based on expert translations (fam-ily patents) and comparable data (Wikipedia articles on the same topic in different languages), which we call CVM F M and, (b) generally relevant documents (cited patents, linked Wikipedia articles), which we refer to as CVM R . comparison uses highly informative dense features which capture similar aspects of e.g. patents or Wikipedia arti-cles. Domain knowledge features for patents were inspired by [8]: a feature res if two patents share similar aspects, e.g. a common inventor, similar number of claims, or com-mon patent classes in the IPC hierarchy.

For Wikipedia, we implemented features that compare the relative length of documents, number of links and im-ages, the number of common links and common images, and Wikipedia categories (hypernym and hyponym relations). the percentage of overlap between two sets. In the retrieval setup, we limit our attention to the relevant documents with-in the top-k results for each query. The overlap metric ex-pressing the similarity of two candidate systems is then: where retrieved@k are the relevant documents retrieved with-in the top-k results. We report the pairwise overlap of two systems s i and s j for k = 100.
 ment correlation coefficient, Pearson's , is used to measure the linear correlation between the scores assigned to each retrieved relevant document. Kendall's works directly on the ranks and is insensitive to the absolute score values.
We calculate the metrics on a per-query basis and report the arithmetic mean. Again, we discard all irrelevant docu-ments from the retrieved results by assigning them a score of 0. Then for each pair of systems, we select the queries which have at least 3 data points (i.e. relevant documents) in com-mon, as 2 data points are always correlated. On average, this method selects about 75% of the queries for evaluation. to nd the set of n principal components (PC) that span the subspace of the data where most of the data variance resides. The straightforward approach would be to identify all the PCs (or eigenvectors) describing the retrieved data and to compare them. Our experiments showed that at least 850 PCs are required to capture more than 90% of the data variance, making a thorough comparison infeasible. Thus, we opt for a simpli ed approach where we consider only a small subset of the most important PCs.

We start by creating j q j j d j matrices of retrieval scores for each system, where j q j and j d j are the numbers of queries and documents. PCA returns the rst k principal compo-nents for each system. By calculating their dot products we obtain a sequence of values, or a k -dimensional vector, which describes the difference between the retrieval results of two candidate systems. To further reduce this vector to a single value, we report the normalized  X  2 -norm of this vector of the top-k PC's similarity. This re ects our requirement that only the dimension of variance is of interest: Th e vector b n i represents the n th normalized PC describ-ing the space of relevant documents retrieved by system s thus the range of values for jj PC jj @ k lies between 0 (all or-thogonal) and 1 (all similar). In this sense, our PCA-based analysis is directly connected to the notion of orthogonality. We used k = 10 principal components in our experiments. Patent Prior-art Search. Our rst dataset consists of a Japanese-English (JP-EN) corpus of patent abstracts from the MAREC and NTCIR data. 1 It contains automatically induced relevance judgments for patent abstracts [7]: EN patents are regarded as relevant to a JP query patent with level (3) if they are in a family relationship (e.g., same inven-tion), (2) if cited by the patent examiner, or (1) if cited by the applicant. On average, queries and documents contain about 5 sentences. Table 1 shows the size of the dataset, consisting of over 100k queries and nearly 1M documents, with approximately 13 relevant documents per query. of relevance-linked Wikipedia pages. 2 Relevance judgments were extracted by aligning German (DE) queries with their English (EN) counterparts (\mates") via the graph of inter-language links available in articles and Wikidata. The high-est relevance level is assigned to the EN mate, the next rel-evance level to all other EN articles that link to the mate, and are linked to by the mate. Instead of using all outgoing links from the mate, only articles with bidirectional links are used. EN documents are restricted to the rst 200 words to reduce the number of features for BM and VW models. To avoid rendering the task too easy for literal keyword match-ing of queries about named entities, title words are removed from German queries. Data statistics are given in Table 1. SMT system trained on parallel corpora. A JP-EN system was trained on 1.8M parallel sentences from the NTCIR-7 JP-EN PatentMT subtask. For Wikipedia, we trained a DE-EN system on 4.1M parallel sentences provided by WMT 3 . System Combination. We reapply the VW ranking ap-proach described in Section 3 on dev set data for system combination. This method shows stable gains over three dif-ferent IR-metrics: the precision-based MAP [11] and NDCG ww w.cl.uni-heidelberg.de/boostclir www.cl.uni-heidelberg.de/wikiclir www.statmt.org/wmt11/translation-task.html [10], where the latter considers relevance levels, and the recall-oriented PRES [12]. All scores were computed on the top 1,000 retrieved documents.
 Results. Table 2 shows the performance of single retrieval systems according to MAP, NDCG, and PRES. SMT-based CLIR-methods clearly outperform all others. Only on spe-cialized domains like patent-prior-art-search and by training on very clean data (expert translations), the neural network-based CVM F M model is competitive. On the task of Wikipe-dia article retrieval, SMT-based methods outperform other approaches by a large margin.

Our hypothesis is that rather than combining the sys-tems with the best standalone retrieval performance, the best overall system is gained by combining systems that are least similar and contribute orthogonal information to the combination. Table 3 lists all possible pairwise system combinations, together with their retrieval performance and their orthogonality/correlation.

An inspection of the patents in Table 3 shows that all measures of orthogonality/correlation capture the high sim-ilarity of the two SMT-based methods, DT and PSQ . Com-bining these two models results only in a small improve-ment in retrieval performance. Similar relations are found for all pairs of systems from same groups: ranking-based approaches such as VW and BM or neural network ap-proaches such as CVM F M and CVM R are similar accord-ing to all measures of orthogonality/correlation, and lead to small improvements in retrieval performance in combination. Picking the least similar systems among the four groups, ir-respective of their standalone retrieval performance, yields much higher improvements in combination. This is very pro-nounced for the DK system that is orthogonal to all other models. The biCVM-models also seem to contribute new information, where the gains are mostly higher for combina-tions with CVM R despite its lower performance as a stan-dalone model compared to CVM F M . The last row in the Patents section presents the best performing combination of the four groups' systems, showing that the improvements by orthogonal combinations add up.

On Wikipedia data, shown in the lower part of Table 3, we nd similar relations. The lower similarity between CVM R and CVM F M can be explained by training data dif-ferences: the latter expects pairs of comparable documents, thus we employed the rst 200 un ltered article words as queries for training. As a result, both models are less sim-ilar and the combination shows notable gains compared to the patent task. The similarity measures between VW and BM on Wikipedia are blurred for an analogous reason: BM is trained on the full vocabulary, while VW uses correlated feature hashing to lower the memory footprint [2].
We presented an empirical validation of the conjecture that best results in CLIR system combination are achieved by combining systems that comprise orthogonal informa-tion. We measured correlation/orthogonality on various lev-els, and identi ed the groups of translation-based models ag-nostic of ranking, direct ranking optimizers unapt for trans-lation, distributed semantic representations by neural net-works, and linear learners based on meta-information. We showed experimentally that combining models from these orthogonal groups outperforms standalone models or com-binations of best-performing models.

Acknowledgments. This research was supported in part by DFG grant RI-2221/1-2 \Weakly Supervised Learning of Cross-Lingual Systems". [1] J. A. Aslam and M. Montague. Models for [2] B. Bai, J. Weston, D. Grangier, R. Collobert, [3] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. [4] J. Chin, M. Heymans, A. Kojoukhov, J. Lin, and [5] C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, [6] S. Goel, J. Langford, and A. L. Strehl. Predictive [7] E. Graf and L. Azzopardi. A methodology for building [8] Y. Guo and C. Gomes. Ranking structured [9] K. M. Hermann and P. Blunsom. Multilingual models [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [11] H. Li. Learning to Rank for Information Retrieval and [12] W. Magdy and G. J. Jones. PRES: a score metric for [13] J.-Y. Nie. Cross-Language Information Retrieval . [14] S. Schamoni, F. Hieber, A. Sokolov, and S. Riezler. [15] M. D. Smucker, J. Allan, and B. Carterette. A [16] A. Sokolov, L. Jehl, F. Hieber, and S. Riezler. [17] F. Ture, J. Lin, and D. W. Oard. Looking inside the
