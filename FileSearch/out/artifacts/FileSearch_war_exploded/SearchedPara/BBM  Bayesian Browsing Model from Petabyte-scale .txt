 Given a quarter of petabyte click log data, how can we esti-mate the relevance of each URL for a given query? In this paper, we propose the Bayesian Browsing Model ( BBM ), a new modeling technique with following advantages: (a) it does exact inference; (b) it is single-pass and parallelizable; (c) it is effective.

We present two sets of experiments to test model effec-tiveness and efficiency. On the first set of over 50 million search instances of 1.1 million distinct queries, BBM out-performs the state-of-the-art competitor by 29.2% in log-likelihood while being 57 times faster. On the second click-log set, spanning a quarter of petabyte data, we showcase the scalability of BBM : we implemented it on a commercial MapReduce cluster, and it took only 3 hours to compute the relevance for 1.15 billion distinct query-URL pairs. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -retrieval models Algorithms, Experimentation, Performance Bayesian models, click log analysis, web search
Web search has become indispensable from everyday life: questions ranging from navigating to  X  X ive.com X  to how to bleach wine stains are all directed to search engines. While responding to users X  information needs, search engines also log down the interaction with users, a typical form of which is what URLs are presented and which are clicked in each search result. Such click log constitutes an invaluable source of user feedbacks, which can be leveraged in many search-related applications [4], e.g. , query recommendation [5, 31], learning to rank [1, 18, 22, 26], and personalized search [28], just to name a few.

A central question in click log analysis is to infer user-perceived relevance for each query-URL pair. The task could be as trivial as counting the click through rate, should the search engine know the snippets of which URLs are exam-ined by the user in addition to those clicked. As such in-formation is not available in the log, we need to first under-stand how users examine search results and make decisions on clicks. Thanks to previous work in this direction ( e.g. , [19, 20, 8]), it is well known that a number of factors must be considered in an accurate interpretation of user clicks, such as the position-bias of examination and clicks, and depen-dency between clicks over different documents in the same search result page. Quite a few statistical models have been recently developed to leverage user clicks for inferring user-perceived relevance, some representatives of which are the cascade model [8], Dependent Click Model [17], and User Browsing Model [11]. We will review them with details in Section 2.

In practice, it is common to have tens to hundreds of ter-abytes log data accumulated at the server side every day. This data stream nature of click logs imposes two compu-tational challenges for click modeling. First, the scalability: a click model must scale comfortably to terabyte-scale or even petabyte-scale data. Even better, we would expect the modeling to be highly parallelizable for better scalability. Second, the capability to be incrementally updated, i.e. ,in-cremental learning: for not losing sync with the evolving world wide web, a click model (and its relevance estimate thereof) has to be continuously updated. Considering the ever-growing search volume, we would expect a practical click model to be single-pass , which minimizes the storage and I/O cost for revisiting historical data.

The previous models, despite of their different user behav-ior assumptions, all follow the point-estimation philosophy: the estimated relevance for each query-URL pair is a sin-gle number normalized to [0,1], and a larger value indicates stronger relevance. While this single-number estimate could suffice for many usages, it nevertheless falls short of capac-ity for broader applications, for example, it is unspecified how to estimate the probability that URL u i is preferred to u j for the same query while their relevance are r i and r respectively. Existing learning-to-rank algorithms derive pairwise preference relationship either from human ratings ( e.g. , [7]) or from certain heuristics ([18, 24]). To the best of our knowledge, no principled approach has been proposed to compute a preference probability , which could support broader applications in both web search and online adver-tising.

In this paper, we propose the Bayesian Browsing Model (
BBM ), which builds upon the state-of-the-art User Brows-ing Model (UBM) and scales to petabyte-scale data with easy incremental updates. By virtue of the Bayesian ap-proach to modeling the document relevance, the preference probability between multiple documents is well-defined and can be computed based on document relevance posteriors. Moreover, we present an exact inference algorithm for BBM that exploits the particular probabilistic dependency in the model, and reveals the relevance posterior in closed form after a single pass over the log data.

In summary, we make the following contributions in this study:
The rest of the paper is organized as follows. Section 2 introduces preliminaries and discusses previous click models. We elaborate on BBM and relevant algorithms in Section 3. Performance comparison results are reported in Section 4, and the petabyte-scale experiment is presented in Section 5. Section 6 covers related work, and the paper is concluded in Section 7.
We first introduce notations in this paper. A web user submits a query q to a search engine, gets back top-M (usu-ally M = 10) URLs, examines them, and clicks some or none of them. This whole process is called a search instance ,and any subsequent re-submission or reformulation is regarded as another instance. Because relevance is defined w.r.t. query-URL pairs ,wehold q fixed in the following discussion unless noted otherwise. We will use document and URL exchange-ably in this paper.

Suppose there are n search instances for a given query, de-noted by I k (1  X  k  X  n ), and a total of N distinct URLs are showninthe n instances, denoted by u j (1  X  j  X  N ). For each search instance I k , the document impression is defined by a function  X  k such that  X  k ( i )= j if u j is the document impressed at the i th position (or rank) in I k . A higher rank for u j corresponds to a smaller i .

In click models, examination and clicks are treated as probabilistic events. For a particular search instance ( k is fixed), we use binary random variables E i to denote the ex-amination event of the document snippet at position i (doc-uments in bottom positions may not be examined) and C i for the click event. Therefore, P ( E i = 1) is the examination probability for position i and P ( C i = 1) is the corresponding click probability.

The popular examination hypothesis [23] can thus be sum-marized as follows: for i =1 ,...,M , where r  X  ( i ) , defined formally as the document relevance ,is the conditional probability of click after examination. It helps to disentangle clickthroughs of documents with vari-ous ranks as being caused by examination position-bias and relevance. Click models based on the examination hypothe-sis share this definition but differ in the derivation of P ( E
The cascade hypothesis in [8] states that users always start the examination from the first document. And the exami-nation is strictly linear to the rank: a document is examined only if all documents in higher ranks are examined. Under the cascade hypothesis, E i +1 is conditionally inde-pendent of all examine/click events above i once we know E , but E i +1 may also depend on the click C i .

The cascade model proposed in [8] puts together previous two hypotheses and further constrains that which implies that a user keeps examining the next doc-ument until reaching the first click, after which the user abandons the search instance and stops the examination.
The dependent click model (DCM) [17] further generalizes the cascade model to instances with multiple clicks by in-troducing a set of query-independent global parameters as conditional probabilities of examining the next document after a click, i.e. ,Eq.5isreplacedby where  X  i (1  X  i&lt;M ) are position-dependent user behavior parameters .

The User Browsing Model (UBM) [11] is also based on the examination hypothesis, but does not follow the cas-cade hypothesis. Instead, it assumes that the examination probability E i is determined by the preceding click position r =argmax l&lt;i { C l =1 } and the distance from the current position to the preceding click d i = i  X  r i : If there is no preceding click, r i =0. Wedenoteallpossible ( r, d ) X  X  by the set User behavior parameters in UBM are {  X  r,d | ( r, d )  X  X } and the size of this set is M ( M +1) / 2,whichisanorder of magnitude larger than DCM. The log-likelihood of the search instance under UBM is We will compute the average log probability of click se-quences over test search instances to compare model effec-tiveness. In our on-going work, we find that UBM is slightly Figure 1: The graphical model representation of BBM . Observed click variables are shaded. better than DCM by less than 5 percent, but at the expense of a much higher computation cost.
UBM represents the state-of-the-art of click model design with a sophisticated user behavior model and better perfor-mance than DCM. Estimates of the document relevance r d i and global parameter  X  r,d  X  X  can be obtained by maximizing the log-likelihood over all search instances across queries. However, because of the coupling between r d i  X  X  and  X  r,d in the last term of Eq. 10, iterative coordinate-ascent al-gorithm has to be employed which requires multiple passes of updates for the relevance of all query-URL pairs until convergence. While still applicable, the iterative algorithm will nevertheless hamper the model scalability when the in-put log goes beyond terabytes. In addition, the maximum-likelihood estimation still leave the problem of computing preference probabilities for multiple documents unsolved.
We therefore propose BBM , the Bayesian Browsing Model, which builds upon UBM by modeling document relevance as hidden random variables and imposing independent priors on them. This augmentation, although seemingly trivial, achieves several theoretical and practical advantages over its non-Bayesian counterpart:
In the remainder of this section, we first go over the for-mal model specification in Section 3.1, then proceed to the exact inference algorithm in Section 3.2 and the maximum-likelihood parameter estimation in Section 3.3. A toy exam-ple is given in Section 3.4 to illustrate inference procedures. Finally, we discuss the parallel version of the algorithm in Section 3.5. For a given query with n search instances and N distinct URLs. Let R =( R 1 ,R 2 ,...R N ) be the corresponding rel-evance variables ranging in [0 , 1]. While any independent prior can be applied to R ,the iid non-informative uniform prior is the most straightforward choice, i.e. , We will mention the general case for other priors later, and note that when the priors over different documents are in-dependent, so are their posteriors and in the BBM family. Therefore, BBM is consistent.

Figure 1 plots the graphical model of BBM for a partic-ular search instance. The model consists of three layers of random variables. The top layer S =( S 1 ,S 2 ,...,S M )are nominal relevance variables such that S i = R  X  ( i ) . The other two layers E and C represent examination and click events respectively. The full model specification that accompanies Fig.1isasfollows: where r i ,d i are defined in the same way as in Eq. 8. For a general Bayesian network with similar structures as BBM , computation of the posterior over R given the click data C 1: n is intractable. Here we provide an exact inference procedure for BBM , which not only gives the relevance pos-terior in closed form, but also reveals how to obtain these posteriors through a single pass of the data. This algorithm exploits the particular dependency structure of BBM ,andis therefore preferred to the existing off-the-shelf approximate inference algorithms such as expectation propagation [21] and generalized belief propagation [30].

First, based on the Bayes theorem, we have Our goal is to first obtain an un-normalized posterior dis-tribution over R , and then to normalize it later. Since the prior p ( R ) is already known, what is needed is P C k R , a quantity computed for each instance I k . Again, an un-normalized version of P C k R suffices.

In order to compute P C k R , we notice, from the graph-ical model in Fig. 1, that C k i is independent of S k j ( j&lt;i ) conditioned on C k j ( j&lt;i ). With this conditional indepen-dence property, Because Eq. 18 also holds for i =1given r 1 =0and d 1 =1, we can multiply the terms for the M positions together to obtain P ( C k | S k )= Using S k i = R  X  k ( i ) , Eq. 19 translates into where r k i denotes the preceding click position before position i in the instance I k and d k i = i  X  r k i .

Plugging Eq. 20 back into Eq. 17 and using the uniform prior over R , the un-normalized posterior over R is p ( R | C 1: n )  X  This posterior is a product of 2 nM linear factors over R R
N ,andforeach R j there are at most |T| +1= M ( M + 1) / 2 + 1 distinct linear factors, namely, R j itself and (1  X  r,d R j ) for each possible ( r, d )  X  X  . Therefore, we can reor-ganize Eq. 21 as where the exponents are calculated by N N
We note that the posterior as written by Eq. 22 indicates that the joint posterior over R factorizes over R j  X  X , so the posterior of R j  X  X  are independent of each other, and we can consequently compute the un-normalized posterior (and nor-malize it) for each R j independently.

Specifically, by letting so that  X  1 =  X  0 , 1 , X  2 =  X  0 , 2 ,..., X  M =  X  0 ,M , X  terior of R j is represented by Algorithm 1 : LearnBBM( C ,  X  , N )
Input: C = { C 1 ,..., C n } : click data
Output: N = { N j , N j,r,d } for j =1 ,...,N and ( r, d ) 01: initialize every value in N to 0; 02: for each search instance k =1 ,...,n 03: initialize the preceding click position r =0; 04: for each position i =1 ,...,M 05: set index for current document j =  X  k ( i ); 06: if C k i == 1 07: N j ++; 08: update the preceding click position r = i ; 09: else 10: set the distance to the preceding click d = i  X  r ; 12: end 13: end 14: end
Complexity: Time O ( nM ), Space O ( NM 2 ). characterizes the posterior of R j . Normalizing Eq. 26 can be done through a univariate numerical integration over R j  X  [0 , 1]. In practice, we find that mid-point interpo-lation with B = 100 uniformly distributed points is suffi-cient. Once normalized, both the density function p ( R and the cumulative distribution function  X ( R j ) can be eval-uated with the desired precision.

The numerical integration approach could be also applied to compute any posterior expectations at interest such as posterior mean and variance, which gives the single-number estimate of the relevance. In addition, by further employing the independence property between posteriors, we can cal-culate the preference probability between documents u i and u
Finally, we conclude this subsection with Algorithm 1, which shows how to obtain these exponents with a single pass through the click log for a given query with n search instances. As shown by Eq. 26, the posterior of R j is fully characterized by M ( M +1) / 2 + 1 numbers, so Algorithm 1 is essentially a counting procedure, and we will further illus-trate the algorithm by an example in Section 3.4.
A more flexible choice for each univariate prior p ( R j )is the beta distribution, such that the prior mean and smooth-ness can be set flexibly by choosing appropriate parameters in the beta family. Algorithm 1 still applies to this case, and the only amendment is that we may need to augment the exponent vector e by a single additional dimension to accommodate the (1  X  r j ) term coming from the prior. Sim-ilarly, other prior distribution in the parametric polynomial form could also be imposed.
To estimate model parameters  X  = {  X  r,d | ( r, d )  X  X } ,we first compute the likelihood for each search instance I k the training set, assuming independent prior on document relevance. From Eq. 20, we have P ( C k )= Therefore the log-likelihood can be derived and rewritten as = = where the counts N r,d and N r,d are defined in a similar fash-ion as the exponents in Eqs. 23 and 24: Maximizing the log-likelihood function in Eq. 29 can be again decomposed to |T| univariate problem, and for each ( r, d ), we have with the optimized value Note that the counts needed in the parameter estimation can be integrated in Algorithm 1 by adding  X  N ( j, d )++ X  X nd  X  N r,d + + X  to lines 07 and 11 respectively. The additional space complexity is only O ( M 2 ) and they are shared across all queries. Parameters can be computed before evaluating the posterior, and there is no iteration between inference and estimation.
Figure 2(a) depicts 3 search instances that involve 4 dis-tinct URLs and 3 positions, so n =3 ,M =3 ,N =4. Nodes shaded with darker color indicate clicks whereas those with lighter color are not clicked. The set of  X  r,d parameters are shown on the right with pre-defined values. Under this setting, the posterior for each document is specified by 7 numbers. We now take u 4 as an example, and initialize its (a) Click Data and Parameter Values of the uniform prior. The first dimension is for the exponent N 4 and others for N 4 ,r,d .

Since u 4 is not impressed in Instance 1, e 4 is not updated after scanning Instance 1. Then in Instance 2, because u is not clicked on position 3 ( i = 3) with the preceding click which is equivalent to multiplying the factor (1  X   X  2 , 1 to the existing posterior. Finally, in Instance 3, since u is clicked, the first element of e 4 is incremented to 1, and e =(1 , 0 , 0 , 0 , 0 , 0 , 1), so given the values for parameters  X  , we have with the normalization constant equals 1 / 6. The normalized posterior is plotted in Fig. 2(b).

While we use pre-defined values, the model parameters  X  need to be estimated across all queries in practice. Given the 3 search instances, the update formulae for the counts for parameter estimation are
A straightforward way of implementing the sequential in-ference algorithm (Algorithm 1) on an array of machines is to distributively store and update the exponents. However, a number of practical issues remain to be solved, e.g. , load balancing, fault tolerance, and data distributions, etc. .In this subsection, we discuss how to leverage the MapReduce paradigm to scale BBM to petabyte-scale data.

MapReduce [9] is a programming model and an associated commodity computer-based infrastructure that provide au-tomatic and reliable parallelization once a computation task is expressed as a Map and a Reduce functions. Specifically, the Map function reads in a logical record, and emits a set of (intermediate key, value) pairs. The MapReduce infrastruc-ture then groups together all values with the same interme-diate key and pass them to the Reduce function. The Reduce function accepts an intermediate key and a set of values for that key, and outputs the final results. In this way, a user of the MapReduce infrastructure only needs to provide the two functions, and let the infrastructure deal with practi-cal issues in parallelization. Open-source implementations of MapReduce infrastructure are readily available, e.g. ,the Apache Hadoop.

To compute the relevance for each query-URL pair, we record all the exponents in a ( M ( M +1) / 2 + 1)-dimentional vector e in a similar format to Eq. 26. In the context of BBM , each exponent value can be uniquely identified by the tuple ( q, u, val ), where ( q, u ) forms the query-URL pair and val = r (2 M  X  r  X  1) / 2+ d (from Eq. 25) is the index of the exponent. The variables r, M, d are the same as defined in Section 2. The Map functions reads a search instance, scans each position, and emits (( q, u ) , 0) if the position is ( q, u ) serves as the intermediate key for MapReduce, and the index in the exponent is the value emitted by Map function. Then the Reduce function simply retrieves the exponents and update the corresponding exponent in the vector. Both functions are sketched in the following: Algorithm 2 : Map(I)  X  Mapping a search instance I Input: I : current search instance.

Output: (( q, u ) ,val ): intermediate (key, value) pairs 01: q = I.qry ; r =0; 02: for each position i =1 ,...,M 03: u = I.phi ( i ); 04: if I.clk [ i ]==1 05: r = i ; 06: val =0; 08: else 09: d = i  X  r ; 10: val = r (2 M  X  r  X  1) / 2+ d ; 11: end 12: Emit (( q, u ), val ); 13: end Algorithm 3 : Reduce((q,u), valList)
Input: ( q, u ): the intermediate key
Output: (( q, u ) , e ): e is the exponent vector for ( q, u ) 1: e = 0 ; 2: for each val in valList 3: e [ val ]++ 4: end 5: return (( q, u ) , e )
In the actual implementation on a commercial MapRe-duce infrastructure, we modify the Map function to accom-modate the counts for parameter estimation, and use a more complicated Reduce function that also calculates the poste-rior mean for each query-URL pair. To get the preference probability for each ( q, u i ,u j ) tuple, we could design another Reduce function over q , which computes the preference prob-abilities for all documents associated with the same query based on the exponent vectors. For incremental updates, we only need to modify line 1 of the Reduce function so that we keep the existing values of exponents instead of initializing them to 0. Experimental results will be covered in Section 5.
We report on performance evaluation of BBM in this sec-tion. We compare BBM with UBM on both effectiveness (using test data log-likelihood) and efficiency (using train-ing time). Most of the experimental settings as introduced below are consistent with our previous work [17].
The data set was sampled from a commercial search en-gine over 2 months in 2008. Only search instances with at least one click are kept for evaluation, because discarded in-stance tend to have noisy clicks on other search elements, e.g. , sponsored ads and query suggestions. We kept at most 10,000 instances for each distinct query even if the query is highly frequent, because clicks for most frequent queries are very similar. For each distinct query, we evenly split its search instances for training and test based on the time stamp, and queries with at least 3 search instances in the training set are kept for subsequent evaluation. This left us 1,154,459 distinct queries and over 51 million search in-stances in total, which is 10 times larger than the previous experiment in [17]. A summary over the test data is pro-vided in Table 1, where the cutoffs on query frequency are 10 1 ,10 1 . 5 ,10 2 , etc. .

To facilitate robust comparison, the data were further di-vided into 20 batches of approximately the same size. Each batch consists of 57,723 queries and around 2.5 million search instances on average, whose corresponding standard devia-tions are 232 and 0.06 million, respectively.

Both models were implemented in MATLAB 2008a on an 8-core machine with 32GB RAM running 64-bit Windows Server 2008. We fit two sets of parameters for navigational queries and informational queries according to [16]. We set the number of bins B in BBM to 100 to get adequate level of accuracy. Similarly, all parameter values for UBM ranges from 0.01 to 1.00, with 0.01 increment. For each query, we compute document relevance and position relevance based on both models. Position relevance is computed by treat-ing each position as a pseudo-document. The position rel-evance can substitute the document relevance estimates for documents that appear zero or very few times in the train-ing set (the threshold is 2log 10 (Query Frequency) ) but do appear in the test set. This essentially smoothes the pre-dictive model and improves the performance on the test set, and also reduces the training cost.
Log-likelihood (LL) is a common evaluation metric for model fitness, and it has been previously used to compare click model effectiveness [17]. For each query within ev-ery batch, two models were applied on the same training set to obtain document relevance and parameter estimates. Then we computed LL for each search instance in the test set based on Eq. 10, by replacing relevance r d i by posterior mean (for BBM ) or the learned value (for UBM) as well as inserting corresponding values for  X  parameters. The arith-metic mean over all qualified search instances is reported as the average LL, and a larger value indicates a better model fit. If the average LL improves from 1 to 2 , the improve-ment rate is (exp( 2  X  1 )  X  1)  X  100%. It measures how much more likely the click sequences in the test data are generated from the more effective click model.

Figure 3 compares the two models in log-likelihood. Specif-ically, Fig. 3(a) plots the average LL across 20 batches, and Fig. 3(b) illustrates the corresponding improvement rate for each batch. BBM consistently outperforms UBM, with an average improvement of 29.2%.

While Fig. 3 on average LL provides an overall picture, it is also instructive to investigate the performance for each query frequency category. Intuitively, it is easier to predict the clicks for frequent queries than for less frequent ones because of the larger training data size and the relatively more uniform click pattern associated with frequent queries. To testify this intuition and understand the relative perfor-mance of BBM and UBM, we compute the average LL for the 6 query frequency categories according to Table 1.
Figure 4(a) presents the comparison of average LL across different frequencies, whereas Figure 4(b) reports the mean and standard improvement ratio computed over 20 batches. BBM consistently outperforms UBM for all 20*6 batch-frequency combinations. The performance margin is more dramatic for less frequent queries, because the Bayesian modeling of document relevance in BBM significantly re-duces the overall prediction risk. As tail queries constitute a significant portion of search instances (as shown in Table 1), we expect BBM wouldbemoreusefulinpracticethanUBM.
Figure 5 compares the model training time for both BBM and UBM on each of the 20 batches. BBM is 57 times faster on average than its competitor. The speedup can be decom-posed into two parts: (1) the iterative algorithm for learning UBM takes multiple iterations to converge. This number ranges from 16 to 27 (median = 24); (2) A single iteration Time in Seconds Figure 5: Comparison of Model Training Time. BBM is 57 times faster than UBM on average. of UBM is still 2.5 times as expensive as BBM ,becauseit has to scan through each query to optimize the relevance for each query-URL pair of interest (although we have already reduced the total number with position relevance computa-tion).

The difference in computational cost would be more dra-matic for larger-size data because all the counts can still be held in memory in this experiment and no expensive disk I/O is needed to perform multiple passes over the training set. But when the data becomes larger and the numbers can no longer be held in memory, the I/O cost of UBM would be prohibitive. We therefore discuss the parallel-computation alternative in the next section.
In this section, we demonstrate how BBM scales to petabyte-scale data with a MapReduce infrastructure. Al-though UBM can also be implemented in a parallel fashion, through multiple iterations of Map and Reduce ,thetimeand resource demand is much more expensive than BBM .We therefore focus on the scalability of BBM in this subsection.
We collected 0.26PB (1PB = 2 50 bytes) log data over a time interval of 8 consecutive time spans of the same length. Then we created 8 data sets with the k th data set consists of the first k time spans, respectively. Details of these data are summarized in Fig. 6(a); specifically, the largest data set contains 1.16 billion query-URL pairs for 103 million distinct queries.

We implemented BBM on a commercial MapReduce clus-ter and fit the model to each of the eight data sets. We recorded the total computational time, which is the sum of the CPU time spent on all machines, for each model learn-ing job. Due to confidentiality, the computational time for each job was normalized by being divided by the time of the first job. The normalized computational time w.r.t. the size of the input log is plotted in Fig. 6(b), which exhibits the expected linear relationship because BBM only needs a single pass over the log.

Figure 6(c), on the other hand, demonstrates the power of parallelism. On the cluster the first job takes 1.4 hours to finish, whereas all the rest jobs take no more than 3 hours, despite of their much larger computation loads (as shown in Figure 6(b)). The MapReduce system automatically al-locates more machines for the Map function for a more de-manding job with larger input size. Consequently, given a larger output rate from the Map function, a proportionally larger number of machines would be allocated to the Reduce function. For this reason as well as the inherent variations in MapReduce scheduling and load balancing, a larger job may end up with a comparable or even shorter elapsed time than a smaller job. However, when all machines in the clus-ter are exhausted (or busy with other jobs), larger jobs do take longer elapsed time.
One of the earliest publications on large scale query log analysis appeared in 1999 [25], which presented interesting statistics as well as a simple correlation analysis from the Alta Vista search engine. Thereafter, search logs, especially the click-through data, have been utilized for various appli-cations, e.g. , learning to rank [1, 6, 18, 29], query recom-mendation [5, 31], search boosting and result reordering [4, 24]. A central task in utilizing search log is to understand and model user search and browsing behaviors and click de-cision processes. Joachims and his collaborators pioneered this direction by presenting a series of studies around some eye-tracking experiments [19, 20], which inspired a series of models that interpret user behaviors with increasing capac-ity, namely, the cascade model [8], ICM and DCM [17], and UBM [11]. These models were reviewed with details in Sec-tion 2 because of their close relationship with the proposed BBM . What distinguishes BBM from the existing models is the combination of enhanced modeling capacity and the scalability that meets the requirements of real-world prac-tice.

Because of the data stream nature of search log, this work also relates to mining data streams [3, 13]. There have been many scalable algorithms developed for different min-ing tasks on data streams, e.g. , classification [10], clustering [15], and frequent pattern mining [14]. In lieu of these stud-ies, the exponent vector e in BBM is actually the synopsis that is maintained over the stream of search log. Since this synopsis fully characterizes the relevance posterior, it can be utilized by various downstream applications. Note that although this synopsis tremendously reduces the data size, it exactly describes the relevance posterior with no approx-imation.

BBM can be utilized for a number of downstream applica-tions. In the first place, quantities such as mean and stan-dard deviation can be extracted from the relevance poste-rior, and can be used as new features to train a better search ranker, in a much similar way as [1, 2]. In the second place, the preference probability from BBM (Eq. 27) can substi-tute or supplement preferences that are heuristically derived from clicks or human ratings. For example, Joachims and his colleagues proposed a set of rules like  X  X lick skip above X  to derive preferences from click-throughs within a search instance [18], and later extended them to across multiple instances [22]. The derived preferences are taken as con-strains to train a Ranking SVM [18]. With BBM , the pref-erence probabilities can be computed outright from the rel-evance posteriors, and can actually serve as soft constraints in Ranking SVM, i.e. , one URL is preferred to another with a certain probability. We foresee that the soft preferences will lead to a cost-sensitive Ranking SVM, which we will investigate in the future. As a matter fact, the utility of preference probability is not limited to augmenting pref-erences derived from click-through data or Ranking SVM; instead, we expect most pairwise preference-based learning to rank algorithms ( e.g. , RankNet [7], RankBoost [12] and FRank [27]) can be married with BBM preference proba-bilities with appropriate algorithmic designs. For example, RankNet tries to learn a ranking function that agrees the most with known preferences as derived from human rat-ings. We could replace the human rating preferences with preference probability, and study how well the preference probabilities derived from feedbacks of millions of users com-pare with well-trained judges. In this paper, we proposed BBM , the Bayesian Browsing Model, together with a single-pass exact inference algorithm that can be parallelized. BBM is both effective and efficient: it outperforms a state-of-the-art click model by 29.2% in log-likelihood, while being 57 times faster in a real world click data set. We further implemented the model on a MapRe-duce cluster, and the model was able to compute the rel-evance for 1.15 billion query-URL pairs within 3 hours by processing 0.26 petabyte click log. There are many topics to be further explored in the future, among which the usage of preference probability in downstream applications is in the particular spotlight.
 We would like to thank Wenjie Fu for his great help on preparing this manuscript. Fan Guo and Christos Faloutsos are supported in part by the National Science Foundation under Grant No . DBI-0640543. An y opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [3] B. Babcock, S. Babu, M. Datar, R. Motwani, and [4] R. Baeza-Yates. Applications of web query mining. [5] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query [6] M. Bilenko and R. W. White. Mining the search trails [7] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [9] J. Dean and S. Ghemawat. MapReduce: Simplified [10] P. Domingos and G. Hulten. Mining high-speed data [11] G. E. Dupret and B. Piwowarski. A user browsing [12] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [13] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. [14] C.Giannella,J.Han,J.Pei,X.Yan,andP.S.Yu.
 [15] S. Guha, A. Meyerson, N. Mishra, and R. Motwani. [16] F. Guo, L. Li, and C. Faloutsos. Tailoring click models [17] F. Guo, C. Liu, and Y.-M. Wang. Efficient [18] T. Joachims. Optimizing search engines using [19] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [20] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [21] T. Minka. Expectation propagation for approximate [22] F. Radlinski and T. Joachims. Query chains: learning [23] M. Richardson, E. Dominowska, and R. Ragno. [24] M. Shokouhi1, F. Scholer, and A. Turpin. Investigating [25] C. Silverstein, H. Marais, M. Henzinger, and [26] A. Trotman. Learning to rank. Information Retrieval , [27] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. [28] S. Wedig and O. Madani. A large-scale analysis of [29] G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma, [30] J. S. Yedidia, W. T. Freeman, and Y. Weiss.
 [31] Z. Zhang and O. Nasraoui. Mining search engine
