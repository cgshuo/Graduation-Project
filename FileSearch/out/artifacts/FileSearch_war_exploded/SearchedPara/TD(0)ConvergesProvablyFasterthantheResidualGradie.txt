 Ralf Schoknec ht ralf.schoknecht@ilkd.uni-karlsr uhe.de Artur Merk e artur.merke@udo.edu Reinforcemen t Learning (RL) is concerned with  X nd-ing optimal policies for dynamical optimisation prob-lems. An agent interacting with its environmen t se-lects actions depending on the curren t state of the en-vironmen t. As a result it obtains a reward and the environmen t makes a transition to a new state. The objectiv e of the agent is to optimise its policy, i.e. the mapping from states to actions, through learning. For a  X xed policy the utilit y of a state is given by the ex-pected accum ulated discoun ted reward that is receiv ed over time when starting in that state and behaving ac-cording to the  X xed policy. One metho d to learn an optimal policy is policy iteration (Bertsek as &amp; Tsitsik-lis, 1996). This algorithm consists of two steps that are executed in turns. In the step called policy evaluation the agent learns a value function that represen ts these utilities for every state and a  X xed policy. Following that the policy is changed in the policy impro vemen t step and the new policy is evaluated again. For the problem of policy evaluation a popular algorithm is the TD(0) algorithm (Sutton, 1988). This algorithm is guaran teed to converge if the value function is repre-sented by a table, i.e. every state has a separate entry to store the value. For large problems, however, such a tabular represen tation is no longer feasible with re-spect to time and memory considerations. Therefore, linear feature-based function appro ximation is often used. However, in the presence of linear function ap-proximation the TD(0) algorithm may diverge when transitions are arbitrarily sampled. For that reason the residual gradien t (RG) algorithm has been pro-posed as a convergen t alternativ e (Baird, 1995). Baird (1995) also stated the phenomenon of slow con-vergence of the RG algorithm by giving an example. However, the reasons for slow convergence were not in-vestigated further. In this paper we use the asymptotic convergence rate to measure the speed of convergence. We form ulate the TD(0) and the RG algorithm as lin-ear iterations. Our convergence analysis is based on the eigen values of the iteration matrices corresp ond-ing to the two algorithms. Using such spectral meth-ods in RL proves quite powerful. We show that un-der certain conditions the TD(0) algorithm converges asymptotically faster than the RG algorithm if a tabu-lar represen tation is used for the value function. This is the  X rst theoretical result comparing the conver-gence behaviour of two RL algorithms. We apply our theorem to the rooms gridw orld benchmark of Sutton et al. (1999). For a stochastic random walk policy and for an optimal deterministic policy we show that the TD(0) algorithm converges asymptotically faster than the RG algorithm as predicted by our theorem. We also demonstrate that in the example of Baird with function appro ximation the RG algorithm may con-verge asymptotically faster than the TD(0) algorithm. Thus, the TD(0) algorithm is not always superior. Before we investigate the convergence speed of the TD(0) and the RG algorithm we  X rst formally intro-duce these two algorithms in their synchronous o X -policy versions. 2.1. Appro ximate Policy Evaluation For a Mark ov decision process (MDP) with  X nite state space S = f s 1 ;::: ;s N g ,  X nite action space A , state transition probabilities p : ( S;S;A ) ! [0 ; 1] and stochastic reward function r : ( S;A ) ! R policy eval-uation is concerned with solving the Bellman equation for a  X xed policy  X  : S ! A . V  X  i denotes the value and  X  is the discoun t factor. As the policy  X  is  X xed we will omit it in the following to make notation easier. If the state space S gets too large the exact solution of equation (1) becomes very costly with respect to both memory and computation time. Therefore, often lin-ear feature-based function appro ximation is applied. The value function V is represen ted as a linear combi-nation of F basis functions H := f  X  1 ;::: ;  X  F g which can be written as V =  X  w , where w 2 R F is the pa-rameter vector describing the linear combination and  X  = ( X  1 j ::: j  X  F ) 2 R N  X  F is the matrix with the basis functions as columns. The rows of  X  are the feature vectors ' ( s i ) 2 R F for the states s i .
 The appro ximate policy evaluation problem deriv ed from (1) is then given by  X  w =  X  X   X  w + R . If this problem were exactly solvable we obtained where I N is the identity in R N  X  N . However, in gen-eral there will be no w such that this equation is ful- X lled. Therefore, we have to trade o X  the errors made in the di X eren t comp onen ts. This is done by a dia-gonal weighting matrix D which yields the quadratic error measure based on the norm 1 k X k D that is known as Bellman error E
B ( w ) = 2.2. The TD(0) Algorithm The TD(0) algorithm (Sutton, 1988) is one way to solve the appro ximate policy evaluation problem. It is generally stated as an async hronous algorithm, that updates the parameters after each transition. We do not require the transitions to be sampled in an on-policy manner along a trajectory corresp onding to the policy that is evaluated. They can rather be sampled o X -p olicy. Let us assume that a transition x i ! z i with reward r i is observ ed. The TD(0) learning rule corresp onding to this single transition updates the pa-rameters w of a general linear function appro ximator. With the learning rate  X  this TD(0) update can be written in vector notation as follows w n +1 = w n +  X ' ( x i )[ r i +  X  X  ( z i )  X  V ( x i )] where A i = ' ( x i )[  X ' ( z i )  X  ' ( x i )] &gt; , b i I Often a whole set of transitions has been sampled from the system that is to be controlled. If these transi-tions are stored in a database they can also be used for a synchronous update. Such synchronous update rules are more convenien t for a mathematical conver-gence analysis than the corresp onding async hronous versions. There is evidence that a synchronous up-date rule appro ximates the behaviour of async hronous update rules in the limit of the iteration (Bertsek as &amp; Tsitsiklis, 1996). Thus, the study of synchronous update rules will also be useful to better understand the behaviour of async hronous update rules. The sit-uation is similar to the relation of batch learning and pattern learning in supervised learning. From now on, by referring to the TD(0) and the RG algorithm we mean the synchronous o X -p olicy versions of these two algorithms.
 In the following we consider the synchronous update for a  X xed set of m arbitrary transitions denoted by T = f ( x i ;z i ;r i ) j i = 1 ;::: ;m g . The start states x sampled with respect to the probabilit y distribution  X  , the next states z i are sampled according to p ( x i ;  X  ) and the rewards r i are sampled from r ( x i ). The syn-chronous update for the transition set T can then be written in matrix notation as with A T D = A 1 + ::: + A m and b T D = b 1 + ::: + b m . Let X 2 R m  X  N with X i;j = 1 if x i = s j and 0 otherwise. Then, X  X  2 R m  X  F is the matrix with feature vector ' ( x i ) as its i -th row. We de X ne Z 2 R m  X  N accord-ingly with Z i;j = 1 if z i = s j and 0 otherwise. Then, the matrix Z  X  2 R m  X  F contains the feature vector ' ( z i ) as its i -th row. With the vector of rewards r = ( r 1 ;::: ;r m ) &gt; this yields 2.3. A Di X eren t Represen tation We have stated the TD(0) algorithm in terms of X , Z and r . In the following we will show how these entities relate to D , P and R that are used in (3). We de X ne the matrix b D = X &gt; X which is diagonal and contains the absolute frequencies with which the states s i occur as starting states in the transition set T . The entry ( i;j ) of matrix X &gt; Z can be interpreted as the absolute frequency of transition s i ! s j . We de X ne b P as the matrix containing the respectiv e relativ e frequencies, i.e. b P = (1 = b D i;i )  X  ( X &gt; Z ) i;j if b D i;i 6 = 0 and otherwise. This yields b D b P = X &gt; Z . Furthermore, the i -th entry of the vector X &gt; r contains the sum of rewards that have been obtained on transitions from state s i . With the vector b R containing the average rewards that occur on transitions from the di X eren t starting states s i we obtain b D b R = X &gt; r . Thus, the entities b P , b R and b D can be computed from the sampled transition data T . It can be shown that as the number of transitions m goes to in X nit y the entities b P , b R and 1 m b D converge to P , R and D with probabilit y one. In the case of a  X nite sample size we have estimates of the true entities and the Bellman error (3) becomes Moreo ver, we have deduced a relation between X , Z , r and b P , b R , b D . The TD(0) algorithm given by (5) can therefore be represen ted in an equiv alent form, which will be importan t for the proof of our main theorem 2.4. The Residual Gradien t Algorithm In case function appro ximation is used the residual gradien t (RG) algorithm was introduced as a conver-gent alternativ e to the possibly divergen t TD(0) algo-rithm. The RG algorithm directly minimises the Bell-man error by gradien t descen t. The gradien t of (6) is given by @E B ( w ) From this gradien t we can deduce the update rule of the RG algorithm. It has a form similar to (4) with A RG w + b RG =  X  m @E B ( w ) @w . This update rule uses the negativ e gradien t and moves the factor 1 m into the learning rate  X  . For A RG and b RG we obtain This represen tation will be used in the proof of the main theorem, which we will state in the next section. 3.1. Asymptotic Convergence Rate The iteration of the TD(0) and the RG algorithm is of the form In the following we investigate the asymptotic conver-gence prop erties of such iterations. Let w  X  be the  X xed point of iteration (10), i.e. Aw  X  + b = 0 holds. And let e n = ( w n  X  w  X  ) be the error after n steps. This error evolves according to the following iteration From this iterativ e form ula we obtain the following explicit represen tation Instead of the error vector e n we consider the norm k e n k of this vector, where k  X  k denotes an arbitrary vector norm in R F . Every vector norm in R F induces a compatible matrix norm in R F  X  F , which we also denote by k X k . It is de X ned as follows Asymptotic convergence is concerned with the ques-tion, how the error k e n k after n steps is related to the initial error k e 0 k in the worst case if n goes to in X n-ity. This leads to the notion of asymptotic convergence rate, which is de X ned in Green baum (1997) and Varga (1962) for nonsingular matrices A .
 De X nition 1 Let e n be the n -th error vector of itera-tion (10) . Then is the asymptotic convergence rate of this iteration. Thus, the asymptotic convergence rate indicates by what factor the error e n is at least reduced asymptot-ically in every step on the average.
 In the following we will see that the spectral radius of the iteration matrix plays a crucial role in the de-termination of the asymptotic convergence rate. The spectral radius of a matrix M is the absolute value of the eigen value of M with largest absolute value, all eigen values of M . According to Corollary 1.3.1 in Green baum (1997) for every matrix norm k X k and ev-ery Matrix M with spectral radius % ( M ) the following holds For the asymptotic convergence rate we therefore ob-tain 3.2. Optimal Learning Rate This asymptotic convergence rate is sharp for nonsin-gular matrices because there is at least one error vector that decreases according to this rate, namely the eigen-vector of I F +  X A corresp onding to the spectral radius % ( I +  X A ). Therefore, the iteration (10) is asymptot-ically governed by ( % ( I +  X A )) n . This entity depends on the learning rate  X  . Hence, we can deduce an opti-misation criterion for the learning rate.
 Lemma 1 Let A be a nonsingular matrix. If there ex-ists a learning rate such that iteration (10) conver ges then the optimal asymptotic conver gence rate is ob-taine d for the learning rate We denote  X   X  as optimal learning rate .
 Proof: According to (15) the asymptotic conver-gence rate depends on the learning rate  X  . From the de X nition of the spectral radius it follows that c (  X  ) = max  X  2  X  ( A ) j 1 +  X  X  j . Thus, the optimal asymp-totic convergence rate is obtained by minimising over  X  which yields c  X  = min  X  c (  X  ). And therefore the op-timal learning rate is  X   X  = arg min  X  c (  X  ).  X  In case A has only real eigen values the following corol-lary yields a particularly simple expression for  X   X  . Corollary 1 Let A be a nonsingular matrix that has only negative real eigenvalues f  X  1 ;::: ; X  n g  X  R &lt; dered such that  X  1  X  :::  X   X  F &lt; 0 . Then the optimal learning rate of iteration (10) is given by Proof: The eigen values of I +  X A lie in the interval [1 +  X  X  1 ; 1 +  X  X  F ]. Thus, % ( I +  X A ) is either j 1 +  X  X  or j 1 +  X  X  F j . It can easily be seen that % is minimal if the two eigen values lie symmetrically around zero. Therefore, 1 +  X   X   X  1 &lt; 0, 1 +  X   X   X  F &gt; 0 and j 1+  X   X   X  1 j = j 1+  X   X   X  F j ()  X  (1+  X   X   X  1 ) = 1+  X  ()  X   X  (  X   X  1  X   X  F ) = 2 ()  X   X  = Note, the optimal learning rate is usually not the largest possible learning rate that leads to convergence of the iteration (10). Therefore, a larger learning rate does not necessarily lead to faster asymptotic conver-gence (Schoknec ht &amp; Merk e, 2003). 3.3. The Bellman Error Measure In section 3.1, we used k e n k = k w n  X  w  X  k to measure the convergence speed of RL algorithms, where k X k is an arbitrary vector norm. However, in reinforcemen t learning the Bellman error is generally used instead. In the following we will show that this is no contradiction for a quasi tabular represen tation, i.e. with nonsingular  X  2 R N  X  N . In this case, the Bellman error can be expressed as k e n k 2 with respect to a certain norm for both the TD(0) and the RG algorithm. From (2) we can deduce that b R =  X  (  X  b P  X  I N ) X  w  X  holds for the appro ximate entities b P and b R . Thus, for the Bellman error (6) we obtain
E B ( w n ) = As  X  was assumed to be nonsingular the matrix B =  X  positiv e de X nite and therefore de X nes a norm k  X  k B . Thus, the Bellman error is equiv alent to the norm of e n with respect to this matrix.
 However, if function appro ximation is used, i.e.  X  is singular, the TD(0) algorithm and the RG algorithm in general converge to di X eren t solutions w  X  T D and w
RG respectiv ely. In this case the TD(0) algorithm no longer minimises the Bellman error and k w n  X  w  X  T D k 2 has to be considered instead (Schoknec ht, 2003). 3.4. Comparison of TD(0) and RG For a certain class of problems the following theorem states that the synchronous o X -p olicy TD(0) algorithm converges asymptotically faster than the synchronous o X -p olicy RG algorithm. The proof can be found in the appendix Theorem 1 Let ( I F +  X  T D A T D ) and ( I F +  X  RG A RG be the iteration matric es of the synchr onous TD(0) and the synchr onous RG algorithm respectively. We assume that the corresponding value functions are rep-resente d in tabular form (  X  = I N , F = N ) and that the set of transitions contains each starting state with equal frequency ( b D = kI N ;k 2 N ). Assume that the matrix A T D has only real eigenvalues (which is always the case for A RG ). Let the learning rates  X   X  T D and  X 
RG be the optimal learning rates according to Corol-lary 1. Moreover, let not all of the eigenvalues of A
RG be identic al. Then, the TD(0) algorithm con-verges asymptotic ally faster than the RG algorithm. If all eigenvalues of A RG are identic al, then both the RG algorithm and the TD(0) algorithm conver ge equally fast asymptotic ally.
 This is the  X rst theoretical result that compares the speed of convergence of the TD(0) and the RG algo-rithm. Baird (1995) only stated the phenomenon of slow convergence of the RG algorithm by giving an ex-ample. However, the reasons for slow convergence were not investigated further. We use the asymptotic con-vergence rate to prove that the TD(0) algorithm con-verges asymptotically faster than the RG algorithm. Our result is restricted to the class of matrices A T D with real eigen values that represen t a uniform sam-pling of the state transitions and a tabular represen-tation of the value function.
 Although the RG algorithm has originally been for-mulated as an alternativ e to the TD(0) algorithm to ensure convergence for an appro ximate represen tation of the value function it can also be used with a tabular represen tation. However, for a tabular represen tation the TD(0) algorithm is known to converge and the RG algorithm would not be necessary . Nevertheless, we think the problem of a tabular represen tation is worth studying because it constitutes the basic prob-lem. Understanding the mechanisms that are impor-tant for convergence in this basic problem can serve as a basis to study more general problems with func-tion appro ximation. Even if no general result con-cerning the convergence speed of RL algorithms with function appro ximation is available yet, the concept of asymptotic convergence rate can be used to analyse individual problems with function appro ximation. In section 5 we will fully analyse the convergence speed of the two RL algorithms applied to the example of Baird (1995). In this example the eigen values of A T D can also be complex (Schoknec ht &amp; Merk e, 2003). This shows that our analysis is not only applicable if the eigen values are real. However, theoretical results for complex eigen values do not exist yet.
 The above theorem involves the optimal learning rate that is only computable if the eigen values of the ma-trix A are known. In practice this is not always the case. However, the theorem indicates the behaviour in the optimal case and, as the asymptotic convergence rate is a continuous function of the learning rate, we can also expect the TD(0) algorithm to converge faster than the RG algorithm in the neigh bourho od of the re-spectiv e optimal learning rates. In this section we apply Theorem 1 to the rooms grid-world (Sutton et al., 1999), a benchmark for reinforce-ment learning. We demonstrate that the TD(0) al-gorithm converges faster than the RG algorithm as predicted by Theorem 1. However, it will also become clear, that faster convergence should always be seen in an asymptotical sense.
 Figure 1(a) shows the rooms gridw orld. It consists of four rooms each of them having 2 doors. The example has 121 states with one goal state G. The position of the goal state determines the reward structure: every transition into the goal state has reward 1, all other transitions have reward zero.
 We will consider two di X eren t transition sets T r and T . T d belongs to an optimal deterministic policy, and T r corresp onds to a random walk policy. We begin with T r that contains four transitions for every state corresp onding to the four actions f up,do wn,righ t,left g . If a transition results in a crash with one of the walls, the transition is a self transition, otherwise it is a reg-PSfrag replacemen ts ular transition to the neigh bouring state. We use as discoun t factor  X  = 0 : 9. The matrix A T D 2 R 121  X  121 computed according to (7) has only real eigen val-ues in the range [  X  7 : 0482 ;  X  0 : 4], which, according to Corollary 1, results in the optimal learning rate  X 
T D  X  0 : 2685. The matrix A RG computed accord-ing to (9) has also only real eigen values in the range [  X  12 : 4194 ;  X  0 : 0151] and therefore  X   X  RG  X  0 : 1608. In Figure 2 we can see that the TD algorithm quickly reaches a very small Bellman error. After 8 iterations the error drops below 10  X  5 , whereas the RG algo-rithms decays quite slowly reaching a Bellman error of appro ximately 10  X  5 only after more than 100 iter-ations. This strong discrepancy can be explained by comparing the spectral radii % ( I F +  X   X  T D A T D )  X  0 : 893 &lt; 0 : 996  X  % ( I F where the spectral radius of the RG algorithm is very close to one. According to the argumen t in sec-tion 3.2 the learning curves are asymptotically similar to 0 : 893 n and 0 : 996 n , which explains the faster con-vergence of the TD(0) algorithm.
 We now consider our second transition set T d . Here, from every state we have a transition with respect to an optimal policy. There are many optimal policies so we depicted the one used here in Figure 1(b). We again choose  X  = 0 : 9 as the discoun t factor. The matrix A T D has only two eigen values, namely  X  1 and  X  0 : 1 which results in the optimal learning rate  X   X  T D  X  1 : 818. The matrix A RG has only real eigen values in the range [  X  5 : 3231 ;  X  0 : 00033] and therefore, according to Corol-lary 1,  X   X  RG  X  0 : 376. For the spectral radii we obtain % ( I F +  X   X  T D A T D )  X  0 : 818 &lt; 0 : 999  X  % ( I F where now the di X erence is even stronger then in the random walk case. In Table 1 some values of the TD(0) iteration and the RG iteration are depicted. In step 347 the TD(0) algorithm for the  X rst time has a smaller Bellman error than the RG algorithm and the error is decreasing much more rapidly . However, in the be-ginning of the iteration the error of the TD(0) algo-rithm rises extremely , whereas the error of the RD algorithm slowly but constan tly falls due to its gradi-ent descen t prop erty. The reason for the behaviour of the TD(0) algorithm can be explained by looking at the iteration in the generalized eigenspaces. We will not discuss this in detail here but just give a very brief idea of what is happ ening. The convergence rate of the iteration is determined by how fast the powers of the Jordan normal form of I F +  X   X  A approac h diag-onal form. These powers contain entries of the form where  X  is an eigen value of A and i  X  j denotes the distance from the main diagonal. In high dimensional generalized eigenspaces i  X  j can be large. As a conse-quence, in the beginning the binomial coe X cien ts dom-inate the matrix J k and it takes longer for the powers (1 +  X  ) k  X  ( i  X  j ) to annihilate the binomial coe X cien ts. This is what happ ens when the TD(0) algorithm is ap-plied to the deterministic example because the largest generalised eigenspace of I F +  X   X  T D A T D has dimension 16. However, after the initial rise the iteration val-ues drop quite quickly so that the iteration converges asymptotically faster than for the RG algorithm. As soon as linear function appro ximation is involved the TD(0) algorithm need not converge faster than the RG algorithm. We show this using the example of Baird (Baird, 1995) with F = 8 features, N = 7 states and m = 7 transitions. In this example the matrices  X  and b P are given by The matrix b D equals the identity matrix I N in R N  X  N and the vector of rewards is given by b R = 0. With (7) and (9) we obtain the iterations (4) and (8) of the TD(0) and the RG algorithm respectiv ely. Both the matrix A T D and A RG are singular. It can be shown that in this case the asymptotic convergence rate is given by the largest eigen value unequal one if the iter-ation converges. According to Lemma 1 we choose the optimal learning rates  X   X  T D and  X   X  RG for each of the two algorithms in this example. Figure 5 shows the asymptotic convergence rate c for the two algorithms depending on the discoun t factor  X  , where a smaller value of c yields faster convergence. This shows that the TD(0) algorithms converges asymptotically faster than the RG algorithm only for  X  2 (0 : 34 ; 0 : 85). For all other  X  the RG algorithm converges asymptotically faster than the TD(0) algorithm. Note, that the TD(0) algorithm even diverges for  X  2 (0 : 88 ; 1) (Schoknec ht &amp; Merk e, 2003). Until now there was only experimen tal evidence that in many cases the residual gradien t (RG) algorithm converges slower than the TD(0) algorithm. In this paper, we stated the  X rst theoretical result comparing the asymptotic convergence behaviour of the two algo-rithms. We proved that under certain conditions the TD(0) algorithm converges asymptotically faster than the RG algorithm if a tabular represen tation is used for the value function.
 We applied our theorem to the rooms gridw orld bench-mark of Sutton et al. (1999). For a stochastic random walk policy and for an optimal deterministic policy we showed that the TD(0) algorithm converges asymp-totically faster than the RG algorithm as predicted by our theorem. For the example of Baird (1995) that in-volves linear function appro ximation we demonstrated that each of the two algorithms can converge faster than the other depending on the discoun t factor. Our convergence analysis is based on the eigen values of the iteration matrices corresp onding to the two al-gorithms. We showed that the spectral radius plays a crucial role for the asymptotic convergence. The reason for slower asymptotic convergence of the RG algorithm lies in a larger spectral radius. This result shows the strength of such spectral metho ds and gives new insigh ts in the behaviour of the TD(0) and the RG algorithm. We hope that the work presen ted in this paper will also be a starting point for more general theoretical results concerning the convergence speed of the TD(0) and the RG algorithm when linear function appro ximation is involved.
 Baird, L. C. (1995). Residual algorithms: Reinforce-ment learning with function appro ximation. Pro-ceedings of the Twelfth International Confer ence on Machine Learning . Morgan Kaufmann.
 Bertsek as, D. P., &amp; Tsitsiklis, J. N. (1996). Neur o dynamic programming . Athena Scien ti X c.
 Green baum, A. (1997). Iterative metho ds for solving linear systems , vol. 17 of Frontiers in Applie d Math-ematics . SIAM.
 Horn, R. A., &amp; Johnson, C. A. (1985). Matrix analysis . Cam bridge Univ ersity Press.
 Schoknec ht, R. (2003). Optimalit y of reinforcemen t learning algorithms with linear function appro xima-tion. Advanc es in Neur al Information Processing Systems 15 .
 Schoknec ht, R., &amp; Merk e, A. (2003). Convergen t combinations of reinforcemen t learning with func-tion appro ximation. Advanc es in Neur al Informa-tion Processing Systems 15 .
 Sutton, R. S. (1988). Learning to predict by the meth-ods of temp oral di X erences. Machine Learning , 3 , 9{44.
 Sutton, R. S., Precup, D., &amp; Singh, S. (1999). Between mdps and semi-mdps: A framew ork for temp oral abstraction in reinforcemen t learning. Arti X cial In-telligenc e , 112 , 181{211.
 Varga, R. S. (1962). Matrix iterative analysis . Pren tice-Hall.
 Before we proof Theorem 1 we recall some common de X nitions and prove one auxiliary lemma.
 For an arbitrary matrix A 2 C n  X  n we denote by f  X  1 ( A ) ;::: ; X  n ( A ) g  X  C the set of eigen values of A ordered such that j  X  1 ( A ) j  X  :::  X  j  X  n ( A ) j . The Hermitian adjoin t A ? of A is de X ned as the conju-gate transp ose of A , i.e. A ? =  X  A &gt; . In the case that A is a real matrix the Hermitian adjoin t of A is just the transp ose of A . The matrix A ? A is Her-mitian (i.e. ( A ? A ) = ( A ? A ) ? ) and positiv e semide X -nite and therefore all it's eigen values are nonnegativ e, f  X  1 ( A ? A ) ;::: ; X  n ( A ? A ) g  X  R  X  0 . The square roots  X  ( A ) := called singular values of A . A matrix norm k  X  k is a norm for which the usual norm axioms are satis X ed, and additionally for arbitrary matrices A;B 2 C n  X  n it holds that k AB k  X  k A k k B k . Now we are prepared for the following Lemma 2 Consider a nonsingular matrix A 2 C n  X  n . Let A have singular values  X  1 ( A )  X  :::  X   X  n ( A ) &gt; 0 and eigenvalues f  X  1 ( A ) ;::: ; X  n ( A ) g  X  C ordered such that j  X  1 ( A ) j  X  :::  X  j  X  n ( A ) j &gt; 0 . Then Proof: The absolute value of the largest eigen value % ( A ) := j  X  1 ( A ) j is also called the spectral radius of A . It is easily seen that % ( A  X  1 ) = 1 j  X  more % ( A ) has the advantageous prop erty (cf. (Horn &amp; Johnson, 1985)) that for every matrix norm k X k we have In the special case of the matrix norm k  X  k 2 induced by the Euclidean vector norm, which according to (12) is de X ned as k A k 2 = max k x k holds (cf. (Horn &amp; Johnson, 1985)) Putting it all together we obtain our assertion: Proof of Theorem 1 According to our assumptions  X  = I N and b D = kI N . Without loss of generalit y we just consider the case b D = I N , the case k &gt; 1 can be obtained by analogous calculations. Thus, from (7) and (9) we directly obtain A T D =  X  b P  X  I N ; (18) As usual, we denote the eigen values with the largest and smallest absolute values as  X  1 :=  X  1 ( A T D ) and  X 
N :=  X  N ( A T D ) respectiv ely. It can be shown that the matrix A T D de X ned according to (18) has only eigen values with negativ e real parts. A T D and A RG are therefore nonsingular. With the assumption that A
T D has only real eigen values we obtain j  X  1 j =  X   X  1 , j  X 
N j =  X   X  N and  X  1  X   X  N &lt; 0. According to Corol-lary 1 the optimal learning rate  X   X  T D is given by  X   X  T D we therefore obtain The matrix A RG is symmetric and negativ e de X nite and therefore has only negativ e eigen values. Using the same argumen ts as for A T D we conclude where  X  1 denotes the largest and  X  N is the smallest eigen value value of A &gt; T D A T D . As all eigen values of A T D A T D are positiv e the square root can be taken. This yields the singular values of A T D As stated above, the matrices A T D and A RG are both nonsingular. Therefore, according to the argumen ts from section 3 the asymptotic convergence rate is de-termined by the corresp onding spectral radius, where smaller spectral radius implies faster convergence and vice versa. Thus, looking at (20) and (21) it remains to show that or equiv alently that j  X  1 j = j  X  N j  X   X  1 = X  N . Using (22) this is equiv alent to showing that j  X  1 j = j  X  N j  X  (  X  1 = X  N ) 2 . From Lemma 2 we already know that j  X  1 j = j  X  N j  X   X  1 = X  N . Because  X  1 = X  N  X  1 we can ex-tend this to j  X  1 j = j  X  N j  X   X  1 = X  N  X  (  X  1 = X  N ) 2 is only possibly if  X  1 = X  N = 1, i.e. if all eigen values of A
RG are identical. In this case both algorithms con-verge equally fast asymptotically . In all other cases we obtain j  X  1 j = j  X  N j  X   X  1 = X  N &lt; (  X  1 = X  N ) 2  X  = X  N &gt; 1. Thus, the TD(0) algorithm converges
