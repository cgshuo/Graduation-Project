 and healthcare. Many of these historical data are high dimensional data, which have a large number of dimensions. There are needs to analyze and mine these high dimensional data to find patterns, general trends and anomalies for many applications. But the curse of dimensionality makes many existing data mining algorithms become computationally intractable and ther efore inapplicable in many real applications. In this paper, we try to design a novel model to mine correlated member clusters in the high dimensional database environment.
 We use an example to explain some concepts. Example: Analysis of economic data. ( Product , City , Month ). 
Fig1 plots the production series of 8 products ( computer , tobacco , office-machine , of them are correlated. Two correlated product series ( Computer and Office Machine ) shown in Fig3. These correlated series exhibit similar patterns. The curve goes up as its correlated series increases, and it goes down as its correlated series decreases. 
As we have seen, some products or cities may have correlated patterns. Their exhibit fluctuation of a similar shape wh en conditions change. Discovering these correlated products or cities is helpful for us to perform more intensive research work. The question is how to find such correlated patterns among a great number of series. correlation between variable vectors, which is used in many kinds of applications. In this example, we use Pearson X  X  coefficient to define the similarity of datasets corresponding to members . 
Our work is first related to correlation mining. Correlation analysis and mining has played an important role in data mining applications. A common data-mining task is the search for associations of item sets in a database of transactions. There have been many works about association relations, since Agrawal et al. proposed association rule mining [1]. The works in first category are about fast algorithms for association rule mining, such as Apriori algorithm to generate frequent item sets [2] and frequent-pattern tree approach to mine frequent patterns without generating candidate item sets [4]. The works in the second category adopt other interesting measure to mine frequent closed patterns [11], maximal frequent patterns [3], and condensed frequent pattern base [7]. In nature, association rules generated by frequent item sets represents the relationships of concurrence in historical transactions. It reflects the relationships of binary variables, but doesn X  X  describe relationships among nominal variables. pattern similarity. In paper [9], Wang et al. defined the similarity of objects by pscore , patterns, Liu et al. [6] designed a more flexible op-cluster model to find patterns that coherence measure to mine coherent patterns in the GST(Gene-Sample-Time) micro-weak is that these algorithms have low efficiency for high dimensional data. all Tri-correlation inequation and design a heuristic approach to prune the unrelated member pairs. Generating Maximal-Correlated-Member-Cluster (MCMC) algorithm is similar to maximal number of tree branches is O( 2 m ). Ordinary set-enumeration tree is infeasible for high dimensional data. Instead, we design an Inverse-Order-Enumeration-Tree (IOET) algorithm, in which the tails of MCMC are generated first, and then the heads are added to them. Two advantages will benefit the IOET algorithm. The first one is redundant sub branches can be detected and pruned as soon as possible. 
In summary, our work has the following contributions. 1. This paper proposes a MCMC model to mine correlated member sets from high dimensional database. MCMC model borrows Pearson X  X  Correlation coefficient as but also nominal variables. 2. In order to compute correlated member pairs efficiently, we prove a tri-correlation inequation (Lemma2) in theory, which can be used to prune a lot of unrelated member pairs without calculating their coefficients. 3. We design an IOET algorithm to gene rate complete MCMCs from correlated member pairs. Compared to the set-enum eration tree algorithm in Max-Miner, IOET algorithm will reduce the search space dramatically. 
The rest of the paper is organized as the following. Section 2 describes our model presents our experiment results. Section 5 summarizes our work. In statistics, a measure of correlation is a numerical grade, which describes the degree of a relationship among variables. Support of frequent item sets and Jaccard Correlation Coefficient represents relationships among ordinal variables. Pearson X  X  Correlation Coefficient measures relationships among nominal variables. 2.1 Pearson X  X  Correlation Coefficient and Its Property y ..., k 2 y n ) , we have r(k 1 X,k 2 Y)=r(X,Y).
 Proof: obvious. r(y, z)  X  2  X  2 -1 . z y distinguish X, Y, and Z from X X , Y X  and Z X . Let
So the lemma is proven. 2.2 Correlated Member Clusters rows or columns depending on the analyzer X  X  view. In this paper, we choose columns as members, and rows as features of members. 
Given a user-specified minimum correlation threshold  X  and a database with m A m }, we define the following terminologies. Definition 1: Correlated-Member-Pair (CMP) 
A member pair P={A s , A t } A P  X  is a Correlated-Member-Pair, if their Pearson X  X  coefficient is above the threshold  X  , r(A s , A t )  X   X  . Definition 2: Correlated-Member-Cluster (CMC) A  X   X   X  , their Pearson X  X  coefficient is above the threshold  X  , r(A s , A t )  X   X  . Definition 3: Maximal-Correlated-Member-Cluster (MCMC) Correlated-Member-Cluster. 
From the definitions, we have the following facts: paper we won X  X  consider these trivial CMP. (2) A CMP is also a CMC. (3) The MCMC is a concise representation of many CMC. For a t -member MCMC, it contains 2 t -1 different sub CMC. Problem definition: MCMCs with correlations above predefined threshold  X  from database DB. In this section, we present the algorithm to mine the complete MCMCs from databases. It X  X  a two-phase method. In the first phase, procedure Calculate-CMP() calculates Pearson X  X  coefficient for member pairs one by one and gets all CMP. The In the second phase, according to the re sult members X  order, procedure Construct-MCMC-Tree() constructs a MCMC-Tree in an inverse order, and then travel the MCMC-Tree to generate the complete MCMCs. 3.1 Calculate Correlated Member Pairs Procedure Calculate-CMP() scans database and calculates Pearson X  X  coefficient for A by rule (2). For any member A p being processed currently, 
Member A p  X  s correlated members are in group G 1 . According to Lemma2, for any coefficients in the following steps. This technique will prune some member pairs. We process members A 1 , A 2 , ..., A m one by one. 
For each member A i  X  A, its possible correlated member set is S(A i ). Card(S(A i )) is descending order according to Card(S(A i )). A* is the sorted member set, which defines the member X  X  order that we will follow in the procedure of Construct-MCMC-others before A i are eliminated. 3.2 IOET Algorithm A CMC is a member set, and in the given members X  order, it can be represented by a  X  head (  X  )={b}, and its tail part tail (  X  )={cgf}. We enumerate all MCMCs by their heads in a Maximal-Correlated-Member-Cluster-Tree (MCMC-Tree). Definition 4: Maximal-Correlated-Member-Cluster-Tree (MCMC-Tree) 
Given a member set A={A 1 , A 2 , . . . , A m }, a MCMC-Tree is a 3 levels tree. Nodes in different levels are defined as the following: (1) In level-1, there X  X  only one {root} node that has pointers to level-2 nodes. (3) In level-3, nodes are indexed by members as the same as their fathers X  indexies. Constructing MCMC-Tree each member A i . S(A i ) contains the possible members that immediately follows A i in the MCMC sequence. We can use S(A i ) to expand MCMC sequence headed by A i . 2 -1 in worst, it will be expensive for a big m . This technique isn X  X  applicable to high dimensional data. Here, we design an Inverse-Order-Enumeration-Tree (IOET) algorithm to construct the MCMC-Tree and get the complete MCMC. 
The output of procedure Calculate-CMP() A* defines the members sequential order. For the expression simplicity, the members order in A* is assumed to be A 1 -A 2 -When we construct nodes in level-3, we won X  X  follow the order A 1 -A 2 -. . . -A m , but in an inverse order A m -A m-1 -. . .  X  A 1 . We generate Local-MCMCs headed by A m , A m-generate a Local-MCMC  X  headed by A i , tail(  X  ) are already calculated. We use an example to illustrate th e MCMC-Tree construction procedure. A sorted member set is A={a,b,c,d,e,f,g,h}. Inverse order set A*={h,g,f,e,d,c,b,a}. 
Considering member b as an example, S(b)={c,d,f,g}, when we construct node(b) S(b)  X  {g}.content={g}. Finally, we get {bcf,bdfg,bfg,bg} after we add {b} as the head. Sequences {bfg , bg } are eliminated, because they are subsequence of {bdfg}. 
After we generate all nodes in level-3, we travel these nodes in ordinary order, and output MCMC. Local-MCMC that is a subset of MCMC is eliminated, such as {h }, {g }, {fg , fh } etc. We implement the algorithm in Microsoft visual c++ 6.0 on the windows2000 platform with a 1.7 GHz CPU and 512 MB main memory. First, we generate the synthetic data sets in tabular forms. A data set is a relational table that has m columns (members) and n rows (records). In order to evaluate the performance of the algorithm, we test the algorithm on these synthetic data sets as we change numbers m , n , and user predefined threshold  X  . As the main algorithm contains 2 major subroutines Calculate-CMP() and Construct-MCMC-Tree(), we will examine their performance separately. Performance of procedure Calculate-CMP() For procedure Calculate-CMP() and the original algorithm without pruning, Fig4a illustrates the CPU time cost when the number of columns increases from 100 to 10k, and Fig4b shows the CPU time cost when the number of rows increases from 200 to 10k. Form experiment results, we can see that procedure Calculate-CMP() columns and rows. 
Fig4c shows the percentage of unrelated member pairs being pruned for different user predefined threshold  X  . We can see that about 80% member pairs are pruned when  X  =0.9. The percentage is defined to be: 2 Performance of procedure Construct-MCMC-Tree() Fig5a compares the CPU time cost of IOET algorithm with the original enumeration-tree algorithm. Using the original algorithm, the CPU time cost rises rapidly when the number of columns is above 100. Because th e time cost of enumeration-tree algorithm is algorithm shows extraordinary scalability when the number members over 1k. Fig5b displays the number of MCMCs generated when the number of columns increases. when m=9k and  X  =0.7). Although there X  X e not other works exactly as the same as ours, we compare the IOET algorithm with the traditional enumeration-tree algorithm. Works in [5][3][9] use the similar enumeration-tree algorithms, and we notice that the column number of eliminated sub CMCs (more than 8M in Fig5c), and IOET cut these branches in search space as soon as possible in order to prevent them from growing exponentially. It will help to explain the reason that IOET has an excellent performance. Experiment on real life data set Back to the example, we experiment on economic data set that has 99 columns and 29 techniques. 
Fig6a plots a result MCMC series that include 18 members, and Fig6b plots another result MCMC series that include 8 members. It is clear that correlated members in a MCMC exhibit similar trend patterns, while different MCMCs show different patterns. Correlation mining has been studied widely and intensively since association rule mining was first proposed in 1993, and now it attracts more attentions than ever time before. Another useful tool for similarity search, pattern recognition, and trend analysis is clustering model, which defines closeness of nominal variables by distance based on a statistical measure. This extended model will discover patterns of rise and fall among data series, which will benefit a lot of applications. 
Mining MCMCs from high dimensional database is an interesting and challenging on pattern similarity, the MCMC model considers all combinations of members. Its computational complexity problem is getting worse than traditional clustering models. For this reason, we design optimizing algorithms to make the MCMC model be applicable to high dimensional data (more than 1k members). 
Discovering MCMCs is the first stage of data analyzing. From MCMCs, we will deduce the hierarchy of members naturally. Then we will employ other methods to MCMC model is a very useful tool in correlation mining, and can be used in a wide range of applications. 
