 Web Directories have emerged as an altern ative to the well-established Web Search Engines, for locating information on the Web. Typically, a Web Directory, e.g. the Dmoz Directory [2], organizes Web pages in a subject hierarchy and allows users to locate interesting information by navigating through the hierarchy. Despite the sim-plicity of navigating in the contents of Web Directories, their editing and maintenance are tedious and time-consuming, since the task of assigning Web pages to topic Direc-tories relies exclusively on th e indispensable effort of human editors. However, the sheer quantity of information that is available on the Web restrains the exhaustive investigation of each and every Web page be fore these are assigne d to topical catego-ries. To make things worse, the staggering rates of Web X  X  evolution [22] get humans overwhelmed by the amount of data that they need to painstakingly examine and categorize within the Directories X  contents . Clearly, if we could help Web editors automate their task we would save a lot of time for a number of people. 
One way to alleviate the problem of categorizing Web pages inside a Directory X  X  topics is to employ machine learning techniques in order to build a classifier, which will then assign every Web page to a topic. However, this approach, requires a con-siderable number of training examples to build accurate classifiers, and might prove inefficient for Web scale classification. This is due to the Web X  X  dynamic nature, which imposes the need for re-training the classifier (possibly on a new dataset) every time a change is made. In this paper, we present an alternative approach for the effective population of Web Directories, which does not require training and, therefore, it can cope easily with changes on the Web. The only input that our method requires is a subject hierar-chy that one would like to use and a collection of Web pages that one would like to assign to the hierarchy X  X  subjects. Besides the automatic population of Web Directo-ries, our approach offers an efficient way of ordering the Web pages inside the Directory X  X  topics, by ranking the pages based on how  X  X escriptive X  they are of the category they are assigned to. At a high level our method proceeds as follows: First, we leverage ontological content from freely available resources created by the Natural Language Processing community, in order to build a subject hierarchy. Then, given a collection of Web pages, we pre-process them in order to extract the words that  X  X est X  communicate every page X  X  theme, the so-called thematic words. We use the pages X  thematic words and the hierarchy to compute one or more subjects to assign to every page. Moreover, we employ a ranking algorithm, which measures the pages X  close-ness to the subjects, as well as the semantic correlations among the pages in the same subject, and sorts the pages listed in each Di rectory topic, so that pages of good qual-ity appear earlier in the results. 
In Section 5, we experimentally evaluate the performance of our approach in cate-gorizing a sample of nearly 320,000 Web pages and we compare it to the performance of other classification schemes. Obtained results show that the categorization accu-racy of our automatic classification method is comparable to the accuracy of machine learning classification techniques. However, in our classification method, no training set is required. 
We start our discussion by presenting the subject hierarchy that we developed for the top level topics used in a popular Web Directory, i.e. Dmoz. Then, in Section 3, we describe how we identify thematic words inside every Web page and we show how we employ thematic words to assign Web pages to the hierarchy X  X  subjects. In Section 4, we introduce a ranking formula, which sorts the pages listed in every Di-rectory topic by prioritizing pages of higher classification accuracy. Our experimental results are presented in Section 5 and we conclude our work in Sections 6 and 7. Web Directories offer a browsable topic hierarchy that is used for organizing Web pages into topics. Currently, topic hierarchies are constructed and maintained by hu-man editors, who manually locate interesti ng Web pages. Based on the pages X  con-tent, the editors find the best fit for the page among the hierarchy X  X  topics. Appar-ently, the manual construction of Web Directories is tedious and may suffer from inconsistencies. To overcome the difficultie s associated with editing Web Directories, we built a topic hierarchy, which we use for automatically categorizing Web pages. Our hierarchy essentially integrates domain information from the Suggested Upper Merged Ontology (SUMO) [3] and the MultiWordNet Domains (MWND) [1], into WordNet 2.0 [4]. Since a fraction of WordNe t X  X  hierarchies is already annotated with domain information, our task was essentially to anchor a domain label to the remain-ing hierarchies. To that end, we firstly an chored to those WordNet hierarchies that are uniquely annotated in either SUMO or MWND their corresponding domain labels. In selecting a domain label, for the hierarchies that are assigned a different domain be-tween SUMO and MWND, we merged those hierarchies together and we picked the domains of the merged hierarchies X  parent nodes. Merging was generally determined by the semantic similarity that the concepts of the distinct hierarchies exhibit, where nects two concepts in the shared hierarch y and (ii) the number of common concepts that subsume two concepts in the hierarchy [25]. Lastly, we attached to each of the hierarchy X  X  lower level concepts those Word Net hierarchies that encounter a speciali-zation (is-a) relation to it. A detailed description of the process we followed for building our hierarchy can be found in [11]. 
To demonstrate the usefulness of our hierarchy in a real-world setting, we aug-mented the hierarchy with topics that are currently used by Web cataloguers for classifying Web data. For that purpose, we explored the first level topics in the Dmoz Directory and, using WordNet, we selected the Dmoz topics that are super-ordinates of the merged hierarchies X  root concepts. The selected Dmoz topics (shown in Table 1) were incorporated, through the is-a re lation, in our hierarchy and formed the hierarchy X  X  first level topics. 
At the end of this merging process, we came down to a hierarchy of 489 concepts that are organized into 13 topics. The resulting hierarchy is a directed acyclic graph where each node represents a concept, denoted by a unique label, and linked to other concepts via a specialization (is-a) link. The maximum depth of the hierarchy X  X  graph is 4 and the maximum number of children concepts (i.e. branching factor) from a node is 26. An important note here is that our hierarchy can be tailored to accommo-date any first level topics that one would like to use, as long as these are represented in WordNet. In addition, the hierarchy could be used in a multilingual setting, through the use of aligned WordNets [29]. The main intuition in our approach for categorizing Web pages is that topic relevance estimation of a page relies on the page X  X  lexical coherence, i.e. having a substantial portion of words associated with the same topic. To capture this property, we adopt the lexical chaining approach and, for every page, we generate a sequence of semanti-cally related terms, known as lexical chain. The computational model we used for generating lexical chains is presented in the work of [6] and it generates lexical chains in a three-step process: (i) select a set of candidate terms 1 from the page, (ii) for each candidate term, find an appropriate chain relying on a relatedness criterion among members of the chains, and (iii) if it is found, insert the term in the chain. The relat-edness factor in the second step is determined by the type of WordNet links that con-nect the candidate term to the terms stored in existing chains. We then disambiguate the words inside every generated lexical chain, using the scoring function f introduced in [27], which indicates the possibility that a word relation is a correct one. the words X  association score, their depth in WordNet and their respective relation weight. The association score ( Assoc ) of the word pair ( w 1 , w 2 ) is determined by the words X  corpus co-occurrence frequency and it is given by: where, p(w 1 ,w 2 ) is the corpus co-occurrence probability of the word pair (w 1 ,w 2 ) and N (w) is a normalization factor, which indicates the number of WordNet senses that WordNet hierarchy and is defined as: where, Depth (w) is the depth of word w in WordNet. Semantic relation weights ( Re-lationWeight ) have been experimentally fixed to 1 for reiteration, 0.2 for synonymy and hyper/hyponymy 0.3 for antonymy, 0.4 for mero/holonymy and 0.005 for sib-lings. The scoring function f of w 1 and w 2 is defined as: 
The score of the lexical chain C i that comprises w 1 and w 2 , is calculated as the sum of the score of each relation r j in C i . Formally: 
To compute a single lexical chain for every downloaded Web page, we segment the latter into shingles [8], and for every shingle, we generate scored lexical chains, as described before. If a shingle produces multiple chains, the lexical chain of the high-we eliminate chain ambiguities. We then compare the overlap between the elements of all shingles X  lexical chains consecutively . Elements that are shared across chains are deleted so that lexical chains display no redundancy. The remaining elements are merged together into a single chain, repres enting the contents of the entire page, and a new Score ( C i ) for the resulting chain C i is computed. 3.1 Categorizing Web Pages In order to assign a topic to a Web page, our method operates on the page X  X  thematic words. Specifically, we map ev ery thematic word of a page to the hierarchy X  X  topics and we follow the hierarchy X  X  hypernymic links of every matching topic upwards until we reach a root node. For short documen ts with very narrow subjects this proc-ess might yield only one matching topic. However, due to both the great variety of the thematic words corresponding to multiple root topics. 
To accommodate multiple topic assignment, a Relatedness Score ( RScore ) is com-puted for every Web page to each of the hierarchy X  X  matching topics. This RScore indicates the expressiveness of each of the hi erarchy X  X  topics in describing the pages X  hierarchy X  X  topic D k is defined as the product of the chain X  X  Score ( C i ) and the fraction the page to each of the hierarchy X  X  matching topics as: 
The denominator is used to remove any effect the length of a lexical chain might between 0 and 1, with 0 corresponding to no relatedness at all and 1 indicating the category that is highly expressive of the page X  X  topic. Finally, a Web page is assigned RScores above a threshold  X  , with T been experimentally fixed to  X  = 0.5. The page X  X  indexing score is: 
Pages with chain elements matching several topics in the hierarchy, and with related-ness scores to any of the matching topics below T, are categorized in all their matching topics. By allowing pages to be categorized in multiple topics, we ensure there is no information loss during the Directories X  population and that pages with short content (i.e. short lexical chains) are not unquestionably discarded as less informative. Admittedly, the relatedness score of a page to a Directory topic does not suffice as a measurement for ordering the pa ges that are listed in the same Directory topic. This is because RScore is not a good indicator of the amount of content that these pages share. Herein, we report on the computatio n of semantic similarities among the pages that are listed in the same Directory topic. Semantic similarity is indicative of the pages X  correlation and helps us determine the ordering of the pages that are deemed related to the same topic. 
To estimate the semantic similarity between a set of pages, we compare the ele-ments in a page X  X  lexical chain to the elements in the lexical chains of the other pages have in common, the more correlated the pages are to each other. To compute simi-larities between pages, P i and P j that are assigned to the same topic, we first need to identify the common elements between their lexical chains, represented as PC i and PC j respectively Then, we use the hierarchy to augment the elements of the chains PC i and PC j with their synonyms. Chain augmentation ensures that pages of compa-rable content are not regarded unrelated if their lexical chains contain distinct but semantically equivalent elements (i.e. synonyms). The augmented elements of PC i and PC j respectively, are defined as: where, Synonyms ( C i ) denotes the set of the hierarchy X  X  concepts that are synonyms to cepts that are synonyms to any of the elements in C j . The common elements between the augmented lexical chains PC i and PC j are determined as: 
We formally define the problem of computing pages X  semantic similarities as fol-lows: if the lexical chains of pages p i and p j share elements in common, we produce the correlation look up table with tuples of the form &lt; AugElements ( PC i ), AugEle-ments (PC j ), ComElements &gt;. The similarity measurement between the lexical chains PC i , PC j of the pages P i and P j is given by: where, the degree of semantic similarity is normalized so that all values are between zero and one, with 0 indicating that the two pages are totally different and 1 indicating that the two pages talk about the same thing. 4.1 Ranking Web Pages in Directory Topics We sort the pages assigned to a Directory topic, in terms of a DirectoryRank ( DR ) metric, which estimates the  X  X  mportance X  of pages in a Directory. DirectoryRank is inspired by, and thus resembles, the PageRank measure [23] in the sense that the importance of a page is high if it is somehow connected to other important pages, and that important pages are valued more highly than less important ones. While PageR-ank realizes the connection between pages in terms of their in/out-going links to other pages, DirectoryRank defines the connection between pages in terms of their semantic coherence to other pages in the Directory, this is; it estimates the importance of pages from their degree of semantic similarity to other important pages. 
Intuitively, an important page in a Directory topic, is a page that has a high relat-edness score to the Directory X  X  topic and th at is semantically close (similar) to many relatedness score and its overall similarity to the fraction of pages with which it corre-lates in the given topic. This way, if a page is highly related to topic D and also corre-consider that page p i is indexed in Directory topic T k with some RScore (p i , T k ) and let p ( DR ) of p i is given by: where n corresponds to the total number of pages in topic T k with which p i semanti-cally correlates. High DR values imply that: (i) there are some  X  X ood quality X  sources among the data stored in a Directory, and th at (ii) more users are likely to visit them while browsing the Directory X  X  contents. Summarizing, the DirectoryRank metric determines the ranking order of the pages a ssociated with a Directory and serves to-wards giving higher rankings to the more  X  X mportant X  pages of the Directory. To study the effectiveness of our method in automatically assigning Web pages into a subject hierarchy, we run an experiment where we compared the efficiency of our method in categorizing Web pages in the Dmoz topics, to the efficiency of a Na X ve Bayesian classifier in categorizing the same set of pages in the same topics. 5.1 Experimental Setup In selecting our experimental data, we wanted to pick a useful yet representative sam-ple of the Dmoz X  X  content. By useful, we mean that our sample should comprise Web pages with textual content and not only lin ks, frames or audiovisual data. By repre-sentative, we mean that our sample should span those Dmoz X  X  categories, whose top-ics are among the top level topics in our subject hierarchy. 
To obtain such a sample, we downloaded a set of 318,296 Web pages listed in the 13 Dmoz topics that are represented in ou r hierarchy. We parsed the downloaded pages and generated their shingles, after removing HTML markup. Pages were then tokenized, part-of-speech tagged, lemmatized and submitted to our classification system, which following the process described above, computed and weighted a sin-gle lexical chain for every page. To compute lexical chains, our system relied on a resources index, which comprised (i) the 12.6M WordNet 2.0 data for determining the semantic relations that exist between the pages X  thematic words, (ii) a 0.5GB com-pressed TREC corpus from which we extracted a total of 340MB binary files for obtaining statistics about word co-occurrence frequencies, and (iii) the 11MB top level concepts in our hierarchy. Table 2 shows some statistics of our experimental data. Our system generated and scored simple and augmented lexical chains for every page and based on a combined analysis of this information it indicates the most ap-propriate topic in the hierarchy to categorize each of the pages. 
To measure our system X  X  effectiveness in categorizing Web pages, we experimen-tally studied its performance against the performance of a Na X ve Bayes classifier, which has proved to be efficient for Web scale classification [14]. In particular, we trained a Bayesian classifier by performing a 70/30 split to our experimental data and we used the 70% of the downloaded pages in each Dmoz topic as a learning corpus. We then tested the performance of the Bayesian classifier in categorizing the remain-ing 30% of the pages in the most suitable Dmoz category. For evaluating the classifi-cation accuracy of both the Bayesian and our classifier, we used the Dmoz categoriza-tions as a comparison testbed, i.e. we comp ared the classification delivered by each of the two classifiers to the classification done by the Dmoz cataloguers for the same set of pages. Although, our experimental pages are listed in all sub-categories of the Dmoz X  X  top level topics, for the experiment presented here, we focus on classifying the Web pages only for the top-level topics. 5.2 Discussion of the Experimental Results The overall accuracy results are given in Ta ble 3, whereas Table 4 compares the accu-racy rates for each category between the two classifiers. Since our classifier allows pages with low RScores to be categorized in multiple topics, in our comparison we explored only the topics of the highest RScores . Note also that we run the Bayesian classifier five times on our data, every time on a random 70/30 split and we report on the best accuracy rates among all runs for each category. 
The overall accuracy rates show that our method has improved classification accu-racy compared to Bayesian classification. The most accurate categories in our classi-fication method are Arts and Society , which give 90.70% and 88.54% classification accuracy respectively. The underlying reason for the improved accuracy of our classifier topics. This argument is also attested by the fact that for the topics Home and News , for which our hierarchy contains a small number of lexical nodes, the classification accu-racy of our method is relatively low, i.e., 40.16% and 55.75% respectively. Neverthe-less, even in those topics our classifier outperforms the Bayesian classifier, which gives for the above topics a classification accuracy of 36.565% and 8.90%. The most straight-forward justification for the Bayesian X  X  classifier low accuracy in the topics Home and News is the limited number of pages that our collection contains about those two topics. This is also in line with the observation that the Bayesian classifier outperforms our documents comprising specialized terminology. The above can be attested in the im-proved classification accuracy of the Bayesian classifier for the categories Business and Shopping , which both have many documents and whose documents contain specialized terms (e.g. product names) that are underrepresented in our hierarchy. 
A general conclusion we can draw from our experiment is that, given a rich topic hierarchy, our method is quite promising in automatically classifying pages and incurs little overhead for Web-scale classification. While there is much room for improve-ment and further testing is needed before judging the full potential of our method, nevertheless, based on our findings, we argue that the current implementation of our system could serve as a Web cataloguers' assistant by delivering preliminary categori-zations for Web pages. These categorizations could be then further examined by hu-man editors and reordered when necessary. Finally, in our approach, we explore the pages X  classification probability (i.e. RScore ) so that, upon ranking, pages with higher RScores are prioritized over less related pages. This, in conjunction with the pages X  semantic similarities, forms the basis of our ranking formula (DirectoryRank). An early study about the potential of DirectoryRank can be found in [28]. The automated categorization of Web documents into pre-defined topics has been investigated in the past. Previous work mainly focuses on using machine learning techniques to build text clas sifiers. Several methods have been proposed in the litera-ture for the construction of document classifiers, such as decision trees [5], Support Vector Machines [13], Bayesian classifiers [24], hierarchical te xt classifiers [19], [11], [9], [20], [26], [12], [21], [7], [17]. The main commonality in previous methods is that their classification accuracy depends on a training phase, during which statisti-cal techniques are used to learn a model based on a labeled set of training exampled. This model is then applied for classifyi ng unlabeled data. While these approaches provide good results, they are practically inconvenient for Web data categorization, mainly because it is computationally expensive to con tinuously gather training exam-ples for the ever-changing Web. The distinctive feature in our approach from other text classification techniques is that our method does not require a training phase, and therefore it is convenient fo r Web scale classification. 
An alternative approach in categorizing Web data implies the use of the Web pages X  hyperlinks and/or anchor text in conjunction with text-based classification methods [10], [15], [16]. The main intuition in exploring hypertext for categorizing Web pages relies on the assumption that both the links and the anchor text of Web pages communicate information about the pages X  content. But again, classification relies on a training phase, in which labeled examples of anchor text from links point-ing to the target documents are employed for building a learning model. This model is subsequently applied to the anchor text of unlabeled pages and classifies them accord-ingly. Finally, the objective in our work (i.e. populating Web Directories) could be addressed from the agglomerative clustering perspective; a technique that treats the generated clusters as a topical hierarchy for clustering documents [18]. The agglom-erative clustering methods build the subject hierarchy at the same time as they gener-ate the clusters of the documents. Therefore, the subject hierarchy might be different between successive runs of such an algorithm. In our work, we preferred to build a hierarchy by using existing ontological content, rather than to rely on newly generated clusters, for which we would not have perceptible evidence to support their usefulness for Web data categorization. However, it would be interesting for the future to take a sample of categorized pages and explore it using an agglomerative clustering module. We have presented a method, which uses a subject hierarchy to automatically catego-rize Web pages in Directory structures. Ou r approach extends beyond data classifica-tion and challenges issues pertaining to the Web pages X  organization within Directo-ries and the quality of the categorizations delivered. We have experimentally studied the effectiveness of our approach in categorizing a fraction of Web pages into topical categories, by comparing its classification accuracy to the accur acy of a Bayesian classifier. Our findings indicate that our approach has a promising potential in facili-tating current tendencies in editing and maintaining Web Directories. However, in this work, we are leaving open for future investigation issues such as ranking pages within Directories, users X  perception of our system X  X  performance, etc. It is our hope though, that our approach, will road the map fo r future improvement s in populating Web Directories and in handling the proliferating Web data. 
We now discuss a number of advantages that our approach entails and which we believe could be fruitfully explored by others. The implications of our findings apply primarily to Web cataloguers and catalogue users. Since cataloguers are challenged by the prodigious volume of the Web data that they need to process and categorize into topics, it is of paramount importance that they are equipped with a system that carries out on their behalf a preliminary cat egorization of pages. We do not imply that humans do not have a critical role to play in Directories X  population, but we deem their  X  X ine-qua-non X  involvement in the evaluation and improvement of the automati-cally produced categorizations, rather than in the scanning of the numerous pages enqueued for categorization. In essence, we argue that our approach compensates for the rapidly evolving Web, by offering We b cataloguers a preliminary categorization for the pages that they have not processed yet. On the other side of the spectrum, end users are expected to benefit from the Dir ectories X  updated content. Given that users get frustrated when they encounter outd ated pages every time they access Web cata-and scales up with the evolving Web, enabling immediacy of new data. 
