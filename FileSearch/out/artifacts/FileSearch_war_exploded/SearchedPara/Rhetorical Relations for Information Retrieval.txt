 Typically, every part in most coherent text has some plausi-ble reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation , describe how the parts of a text are linked to each other. Knowledge about this so-called discourse structure has been applied successfully to several natural language processing tasks. This work stud-ies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical rela-tions and retrieval performance? Can knowledge about a document X  X  rhetorical relations be useful to IR?
We present a language model modification that considers rhetorical relations when estimating the relevance of a doc-ument to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably ( &gt; 10% in mean average precision over a state-of-the-art baseline). H.3.3 [ Information Search and Retrieval ]: Retrieval Models; H.3.1 [ Information Storage and Retrieval ]: Con-tent Analysis and Indexing X  linguistic processing Rhetorical relations, discourse structure, retrieval model, probabilistic retrieval
According to discourse analysis, every part in most coher-ent text tends to have some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, explana-tion, condition , are considered critical for text interpre-tation, because they signal how the parts of a text are linked to each other to form a coherent whole [23]. Unlike gram-matical relations, which are generally explicitly manifest in Figure 1: Rhetorical relations example (from [11]). language, rhetorical relations may be unstated. The goal of discourse analysis is therefore to infer rhetorical relations, and specifically to identify their span, constraints and func-tion.

There is a large body of research on both descriptive and predictive models of rhetorical structure and discourse analysis in natural language text. For instance, annotation projects have taken significant steps towards developing se-mantic [12, 18] and discourse [5] annotated corpora. Some of these annotation efforts have already had a computational impact, making it possible to automatically induce semantic roles [15] and to automatically identify rhetorical relations [14], achieving near-human levels of performance on certain tasks [27]. In addition, applications of discourse analysis to automatic language processing tasks such as summarisa-tion or classification (overviewed in section 2) indicate that rhetorical relations can enhance the performance of well-trained natural language processing systems.

Motivated by these advances, this work brings perspec-tives from discourse analysis into Information Retrieval (IR) with the aim of investigating if and how rhetorical relations can benefit retrieval effectiveness. Is there a correlation be-tween certain rhetorical relations and retrieval performance? Can knowledge about a document X  X  rhetorical relations be useful to IR? For example, consider the rhetorical relations of the text shown in Figure 1 (borrowed from [11]). Should some of the terms in this sentence be given extra weight by an IR system, according to their rhetorical relations? Can some rhetorical relations be considered more informative and hence more useful for IR ranking than others? These ques-tions have been posed before (see discussion in section 2), however to our knowledge this is the first time that a prin-cipled integration of rhetorical relations into a probabilistic IR model improves precision by &gt; 10%.
Reasoning about query -document relevance using the language modeling formalism [9], we present a model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that doc-ument. We present an application of this model to an IR re-ranking task, where, given a list of documents initially retrieved for a query, the goal is to improve the ranking of the documents by refining their estimation of relevance to the query. Experimental evaluation of different versions of our model on TREC data and standard settings demon-strates that certain rhetorical relations can be beneficial to retrieval, with notable improvements to retrieval effective-ness ( &gt; 10% in mean average precision and other standard TREC evaluation measures over a state-of-the-art baseline).
Discourse analysis and rhetorical structures have been stud-ied in the context of several automatic text processing ap-plications. This has been partly enabled by the availability of discourse parsers -see [11, 14] for up-to-date overviews of discourse parsing technology. Studies of discourse analy-sis in relation to IR and its broader applications are briefly overviewed below. For a more general overview of discourse analysis approaches, see Wang et al. [33], section 2.
Sun &amp; Chai [28] investigate the role of discourse process-ing and its implication on query expansion for a sequence of questions in scenario-based context question answering (QA). They consider a sequence of questions as a mini dis-course. An empirical examination of three discourse theo-retic models indicates that their discourse-based approach can significantly improve QA performance over a baseline of plain reference resolution.

In a different task, Wang et al. [33] parse Web user forum threads to determine the discourse dependencies between posts in order to improve information access over Web fo-rum archives. They present three different methods for clas-sifying the discourse relationships between posts, which are found to outperform an informed baseline.

Heerschop et al. [16] perform document sentiment analy-sis (partly) based on a document X  X  discourse structure. They hypothesise that by splitting a text into important and less important text spans, and by subsequently making use of this information by weighting the sentiment conveyed by distinct text spans in accordance with their importance, they can improve the performance of a sentiment classifier. A document X  X  discourse structure is obtained by applying rhetorical structure theory on a sentence level. They re-port a 4.5% improvement in sentiment classification accu-racy when considering discourse, in comparison to a non-discourse based baseline. Similarly to this study, Somasun-daran et al. [26] report improvements to opinion polarity classification when using discourse, and Morato et al. [24] report a positive dependence between classification perfor-mance and certain discourse variables. An overview of dis-course analysis for opinion detection can be found in Zhou et al. [36].

In the area of text compression, Louis et al. [21] study the usefulness of rhetorical relations between sentences for sum-marisation. They find that most of the significant rhetorical relations are associated to non-discriminative sentences, i.e. sentences that are not important for summarisation. They report that rhetorical relations that may be intuitively per-ceived as highly salient do not provide strong indicators of informativeness; instead, the usefulness of rhetorical rela-tions is in providing constraints for navigating through the text X  X  structure. These findings are compatible with the study of Clarke &amp; Lapata [7] into constraining text com-pression on the basis of rhetorical relations. For a more in-depth look into the impact of individual rhetorical relations to summarisation see Teufel &amp; Moens [30].

In domain-specific IR, Yu et al. [34] focus on psychiatric document retrieval, which aims to assist users to locate doc-uments relevant to their depressive problems. They propose the use of high-level discourse information extracted from queries and documents, such as negative life events, depres-sive symptoms and semantic relations between symptoms, to improve the precision of retrieval results. Their discourse-aware retrieval model achieves higher precision than the vec-tor space and Okapi models.

Closer to our work, Wang et al. [31] extend an IR ranking model by adding a re-ranking strategy based on document discourse. Specifically, their re-ranking formula consists of the original retrieval status value computed with the BM11 model, which is then multiplied by a function that linearly combines inverse document frequency and term distance for each query term within a discourse unit. They focus on one discourse type only ( advantage-disadvantage )whichthey identify manually in queries, and show that their approach improves retrieval performance for these queries. Our work differs on several points. We use an automatic (not man-ual) discourse parser to identify rhetorical relations in the documents to be retrieved (not queries). We consider 15 rhetorical relations (not 1) and we study their impact to re-trieval performance using a modification of the IR language model.

Finally, Suwandaratna &amp; Perera [29] also present a re-ranking approach for Web search that uses discourse struc-ture. They report a heuristic algorithm for refining search results based on their rhetorical relations. Their implemen-tation and evaluation is partly based on a series of ad-hoc choices, making it hard to compare with other approaches. They report a positive user-based evaluation of their system for ten test cases.
There may be various ways of considering rhetorical rela-tions in an IR setting. In this work, we view rhetorical rela-tions as non-overlapping text spans, rather than a graph or a tree with structure and overlapping nodes [27]. We select a principled integration of rhetorical relation information into the retrieval model that ranks documents with respect to queries. The goal is to enable evidence about the rhetorical relations in a document to have a quantifiable impact upon the estimation of relevance of this document to a query, and to study that impact.
Let q be a query, d adocument, D a collection of docu-ments, and  X  g a rhetorical relation in the collection (so that ranked by its probability p ( d | q ) of being relevant to q .Using Bayes X  law: where the right-hand side of Equation 1 is derived as follows: p ( q )isdroppedbecauseitisfixedforalldocuments,and p ( d ) can be dropped on the assumption that it is uniform in the absence of any prior knowledge about any document. Using the language modeling approach to IR [9], p ( q | d )can be interpreted as the probability of generating the terms in q from a model induced by d , or more simply how likely it is that the document is about the same topic as the query. p ( q | d ) can be estimated in different ways, for instance using Dirichlet, Jelinek-Mercer, or two-stage smoothing [35].
We introduce into Equation 1 the probability of generat-ing the query terms from a model induced by d and by its rhetorical relations  X  g  X  d as follows:
We now explain the two components in Equation 2. The first component, p ( q | d,  X  g ), can be interpreted as the prob-ability of generating the query terms from a model induced by d and  X  g . We estimate p ( q | d,  X  g ) as a simple mixture of the probabilities of generating q from d and  X  g : where p ( q | d ) is the (baseline) probability of relevance be-tween q and d mentioned in the beginning of this section,  X  is a free parameter, and p ( q |  X  g ) can be interpreted as the probability of generating q from a model induced by the rhetorical relation  X  g , or more simply, the  X  X ikelihood of rel-evance X  between the terms in the query and the terms in the rhetorical relation.

The second component of Equation 2, p (  X  g | d ), is the prob-ability of the rhetorical relation given the document. Simi-larly to above, this can be interpreted as the probability of generating the terms in  X  g from a model induced by d ,or more simply the likelihood of relevance between the terms in the rhetorical relation and the terms in the document.
To make Equations 2-3 operational we need to compute p ( q |  X  g )and p (  X  g | d ). One simple way of doing so is using the respective maximum likelihood estimations: where f ( q i , X  g ) is the frequency of the query term q and |  X  g | is the number of terms in  X  g . where f (  X  gj ,d ) is the frequency of the rhetorical relation term  X  gj in d ,and | d | is the number of terms in d .Inthis work, we use the above equations and, to compensate for zero-frequency cases, we apply add-one smoothing.
Alternative principled estimations of Equations 4-5 are possible (e.g. Dirichlet, Good-Turing) and could poten-tially improve the performance reported in this work. For instance, one could discount the frequencies in Equations 4-5 by a respective collection model using Dirichlet smooth-be the smoothing parameter and  X  would be the collec-tion of all rhetorical relations in D . A similarly Dirichlet smoothed alternative estimation of Equation 5 would be: maximum likelihood instead of Dirichlet to avoid introduc-ing the extra Dirichlet smoothing parameter  X  when inves-tigating the effect of rhetorical relations upon retrieval.
Another alternative would be to use Good-Turing smooth-ing, however doing so would scale down the maximum like-lihood estimations in Equations 4-5 by a factor of 1  X  E (1) estimate of how many items in the numerator of Equation 4 (resp. Equation 5) have occurred once in the sample of the denominator (see Gale &amp; Sampson [13] for more on Good-Turing smoothing). In effect, for Equation 4 this scaling down would reduce the probability of the query terms that we have seen in  X  g , making room for query terms that we have not seen. For our setting this would not be necessary, because in practice most queries and most rhetorical rela-tions correspond to rather short text spans. Good-Turing smoothing might be better suited for larger samples [13].
Overall, the model presented in this section can be seen as a  X  X asic model X  for ranking documents (partly) according to their rhetorical relations. Different variations on this basic model are certainly possible, however we choose to use the simple maximum likelihood version of this model for this exploratory investigation into the potential benefits of using rhetorical relations for IR.
We evaluate our model on the task of re-ranking an initial list of documents, which has been retrieved in response to a query. Re-ranking is a well-known IR practice that can enhance retrieval performance notably [19]. The baseline of our experiments consists of the top 1000 documents re-trieved for each query using a state-of-the-art retrieval model (language model with Dirichlet smoothing 1 [9]). Our ap-proach reranks these documents using Equation 2.
We experiment with the TREC datasets of the Web 2009 (queries 1-50) and Web 2010 (queries 51-100) tracks, that contain collectively 100 queries and their relevance assess-ments on the Clueweb09 cat. B dataset 2 (50,220,423 web pages in English crawled between January and February 2009). We choose these datasets because they are used widely in the community, allowing comparisons with state-of-the-art. We remove spam using the spam rankings of Cor-mack et al. [8] with the recommended setting of percentile-score &lt; 70 indicating spam 3 .

We consider a subset of this collection, consisting of the top 1000 documents that have been retrieved in response to each query by the baseline retrieval model on tuned settings
We also experimented with Jelinek-Mercer and two-stage smoothing for the baseline retrieval model. Dirichlet and two-stage gave higher scores. We chose Dirichlet over two-stage because it includes one less parameter to tune. http://lemurproject.org/clueweb09.php/
Note that removing spam from Clueweb09 cat B. is known to give overall lower retrieval scores than keeping spam [3]. discourse parser [27] condition ... Conditional money based upon care for the pet ... elaboration ... order accutane no prescription required ... temporal ... Take time out before you start writing ... (described in section 4.1.2) using the Indri IR system 4 for indexing and retrieval. For this subset, we strip HTML an-notation using our in-house WHU-REAPER crawling and web parsing toolkit 5 . Rhetorical relations are identified us-ing the freely available SPADE discourse parser [27]. Table 1 shows the 15 types of rhetorical relations identified by this process, with examples taken from the re-ranking dataset. Two parameters are involved in these experiments: the Dirichlet smoothing parameter  X  of the retrieval model (used by both the baseline and our approach) and the mixture parameter  X  of our model. Both parameters are tuned using 5-fold cross validation for each query set separately; results reported are the average over the five test sets.  X  is tuned across { 100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000 } (using the range of Zhai &amp; Lafferty [35]) and  X  is tuned across { 0.1, 0.3, 0.5, 0.7, 0.9 } .
 Performance is reported and tuned separately for Mean Average Precision (MAP), Binary Preference (BPREF), and Normalised Discounted Cumulated Gain (NDCG). These measures contribute different aspects to the overall evalua-tion: BPREF measures the average precision of a ranked list; it differs from MAP in that it does not treat non-assessed documents as explicitly non-relevant (whereas MAP does) [4]. This is a useful insight, especially for a collection as large as Clueweb09 cat. B where the chances of retrieving non-assessed documents are higher. NDCG measures the gain of a document based on its position in the result list. The gain is accumulated from the top of the ranked list to the bottom, with the gain of each document discounted at lower ranks. This gain is relative to the ideal based on a known recall base of relevance assessments [17]. Finally, we test the statistical significance of our results using the t-test at 95% and 99% confidence levels [25].
Figure 2 shows the distribution of the rhetorical rela-tions in our re-ranking dataset as a percentage of the total number of rhetorical relations. Elaboration, attribution http://www.lemurproject.org/
Freely available by emailing the third author. and background are the most frequent rhetorical relations, whereas topic-comment is the most infrequent. This hap-pens because quite often in text a topic forms the nucleus of the discourse, which is then linked by a number of differ-ent rhetorical relations, for instance about its background, elaborating on an aspect, or attributing parts of it to some entity. As a result, several types of other rhetorical rela-tions can correspond to a single topic-comment . Note that the distribution of rhetorical relations reported here is in agreement with the literature, e.g. Teufel &amp; Moens [30] also report a 5% occurrence of contrast , albeit in the domain of scientific articles.
Table 2 shows the performance of our model against the baseline, for each rhetorical relation and evaluation measure. The baseline performance is among the highest reported in the literature for these setings; for instance Bendersky et al. [3] report MAP=0.1605 for a tuned language model baseline with the Web 2009 track queries on Clueweb cat. B without spam.

We observe that different rhetorical relations perform dif-ferently across evaluation measures and query sets. The four rhetorical relations that improve performance over the base-line consistently for all evaluation measures and query sets (shaded rows in Table 2) are: background, cause-result, condition and topic-comment . Topic-comment is one of the overall best-performing rhetorical relations, which in simple terms means that boosting the weight of the topical part of a document improves its estimation of relevance.

A closer look at which rhetorical relations decrease per-formance presents a more uneven picture as no relations consistently underperform for all measures and query sets. Some relations, such as explanation and enablement for Web 2009, and summary and evaluation for Web 2010, are among the lowest performing, but are not under the baseline across all measures and both query sets. This implies that separating rhetorical relations into those that generally can enhance retrieval performance and those that cannot may not be straight-forward. Even though exploring the fam-ily likeness between useful relations and ones that give no mileage is an interesting discussion, in the rest of the pa-Table 3: Effect of the rhetorical relation to the re-trieval model as indicated by parameter  X  (see Equa-tion 3), for the tuned runs of Table 2. Shaded rows indicate rhetorical relations that consistently improve performance over the baseline at all times. Figure 2: % distribution of rhetorical relations in our dataset. per we focus on those rhetorical relations that consistently improve retrieval performance (for these datasets). Improvements over the baseline are generally higher for Web 2010 than Web 2009, possibly because the former base-line is weaker, with potentially more room for improvement. An interesting trend is that more rhetorical relations im-prove performance according to BPREF than according to MAP and NDCG. As BPREF is the only of these evaluation measures that does not consider non-assessed documents as non-relevant, this indicates the presence of non-assessed doc-uments in the ranking.

The scores shown in Table 2 are averaged over tens of queries, meaning that they can be affected by outliers. Fig-ure 3 presents a detailed per-query overview of the perfor-mance of each query in relation to the baseline for each of the 15 rhetorical relations 6 . The plotted points represent the difference in MAP between our approach and the base-line. Positive points indicate that our approach outperforms the baseline. The points are sorted.
 We observe that although the overall performance of the Web 2010 query set is lower than that of the Web 2009 query set, the improvements over the baseline of the 2010 set are consistently larger. Only in one case, topic-comment ,dothe plotted points clearly cross. Overall both query sets show similar plots with outliers at both ends of the scale. How-ever, the 2009 query set tends to have a somewhat larger proportion of negative outliers, which goes some way to-wards explaining the lower improvements over the baseline observed for Web 2009. The Web 2010 set shows improve-ments over the baseline for most of the rhetorical relations and for the majority of the queries.
Exactly how much impact each rhetorical relation has on the ranking can be seen in Table 3. The table lists the  X  values for the best performing tuned runs from Table 2, where high  X  values mean that the rhetorical relations are given more weight in the ranking (see Equation 3). We see that none of the values are above 0.5 for MAP and NDCG, indicating that too much emphasis on the rhetorical rela-Similar trends are observed in the corresponding figures for BPREF and NDCG, which are not included here for brevity. tions may not be beneficial to performance. Consistent with Table 2, BPREF follows a different trend than MAP and NDCG, which could be due to the fact that it is a differ-ent type of evaluation measure as discussed above in section 4.1.2. With BPREF, non-assessed documents are not explic-itly penalised in the evaluation (as in MAP and NDCG) -resulting in overall higher  X  values for best performing runs, typically of around 0.5-0.7.

Further we observe that the rhetorical relations that con-sistently improve performance over the baseline, as indicated in Table 2, differ in  X  values for their best performing runs. For example,  X  = 0.2 -0.3 for background and  X  =0.5for topic-comment . This implies that, to use rhetorical rela-tions successfully for IR, it is not sufficient to know which rhetorical relations should be considered in the ranking and which not; also knowledge about how much emphasis to put on each rhetorical relation is needed for optimal IR perfor-mance.

Finally, note that the frequency of rhetorical relations does not affect their impact to retrieval. For instance, the three best performing rhetorical relations, topic-comment, background and cause-result constitute respectively ap-proximately  X  1%, 11% and 5% of all rhetorical relations, as shown in Figure 2.
The findings in section 4.2 show that some rhetorical re-lations can be more beneficial to retrieval performance than others. An ideal solution would not consider the lexical statistics of all rhetorical relations in a document, but rather it would select to include in the ranking only those rhetorical relations that have a higher likelihood of enhancing retrieval performance. This can be formulated as finding the optimal rhetorical relation  X   X  g that maximises the expected retrieval scores according to an evaluation measure (e.g. MAP) for a query-document pair: where E denotes the expectation and y the retrieval score (rest of notation as defined in section 3).

Bayesian decision theory allows to reason about this type of expectation, for instance see [32]. In this work, we treat this as a problem of Bayesian posterior inference, where the goal is to estimate the retrieval performance associated with a rhetorical relation, given the observed retrieval scores it fetches on a number of queries. Then, we can consider the rhetorical relation associated with the highest retrieval performance as optimal. For this estimation, we split our dataset into different parts so that we use the observations from one to make inferences about the other (see section 5.2 for details).

Let n = 15 be the rhetorical relations shown in Table 2, and x j be the number of queries for which retrieval with the j th rhetorical relation gets a retrieval score y j .Fornow we assume that all rhetorical relations may be expected to have similar retrieval performance, with the j th rhetorical relation having an average performance ratio per query  X  j (estimated as y j x lar data [22], one of which is the Poisson distribution. Let us assume that, conditional on  X  j , the retrieval scores y j independent Poisson distributions with means  X  j x j .Letus further assume that the  X  j are independent realisations of a gamma variable with parameters  X  and  X  ,andthat  X  itself has a prior gamma distribution with parameters  X  and  X  . Thus so that the joint probability density of the retrieval scores y , the average performance ratios  X  ,and  X  is f ( y |  X  ) f (  X  |  X  )  X  (  X  )= c where c is a constant of proportionality.

The conditional density of  X  can be computed by vari-ous numerical approximations, one of which is the Laplace method [2], which we use here. To find the conditional den-sity of  X  we integrate over the  X  j to obtain f ( y, X  )= c from which the marginal density of y is obtained by further integration to give where h (  X  )=  X  X   X  ( n X  +  X   X  1) log X  + ( y j +  X  ) log ( x Let I denote the integral in this expression. In this work, we take an uninformative prior for  X  ,with  X  =0 . 1and  X  =1anduse  X  =1 . 8 7 . We then apply Laplace X  X  method to I , resulting in the approximate posterior density for  X  ,  X   X  (  X  | y )=  X  I  X  1 e  X  h (  X  ) .

To calculate approximate posterior densities for  X  j we integrate Equation 7 over  X  i , i = j andthenweapply Laplace X  X  method to the numerator and denominator inte-grals of where h (  X  )=(  X  +  X  j )  X   X  ( n X  +  X   X  1) log X  + The resulting denominator is again  X  I 1 , while the numerator must be recalculated at each of a range of values for  X  j The output is the (posterior) expected retrieval performance associated with each rhetorical relation.
These values are not tuned; they are the default values of this approach as illustrated in [10], chapter 11.3, pages 603-604. relations. Bold marks better than baseline.
The observations required to make the above inference are triples of rhetorical relation -query number -retrieval score . To avoid overfitting, we pool randomly 50% of the observations from the 2009 Web query scores and 50% of the observations from the 2010 Web query scores. We use this pool to infer the expected retrieval performance of each rhetorical relation. We repeat this randomised pooling five times, each time randomly pertrubing the data, producing five different sets of observations. We then use each set to infer the expected best performing rhetorical relation per query, in accordance to Equation 6. Following this, we use the model introduced in section 3, Equation 2, to rank docu-ments with respect to queries only for optimal (as inferred) rhetorical relations. We evaluate the above method using the same experimental settings described in section 4.1.
Table 4 shows the runs corresponding to the five differ-ent inferences of the best rhetorical relation that use our model (optimal inferred (1)-(5) respectively). We also report the optimal retrieval performance actually observed in the dataset when using the best rhetorical relation per query (optimal observed ). Optimal here means with respect to the choice of rhetorical relation, not with respect to the Dirichlet  X  parameter of the baseline retrieval model.

Table 4 shows that our optimised ranking model for rhetor-ical relations is better than the baseline for any of the five random inferences on all three evaluation measures. The probability of getting such a positive result by chance is 2 5 &lt; 0 . 05, and thus the improvements are statistically sig-nificant. The improvements over the baseline are consider-able, a very promising finding given the relatively low num-ber of observations used for optimising the choice of rhetor-ical relations. Experiments involving larger query sets can be reasonably expected to perform on a par with state-of-the-art performance.

More generally, the improvements in Table 4 signal that rhetorical relations (derived automatically as shown in this work) could potentially be useful features for  X  X inguistically-uninformed X  learning-to-rank approaches.
The distribution of the 15 rhetorical relations we identi-fied in our dataset is not the same for all rhetorical relations (see Figure 2). Some types, e.g. topic-comment ,tendtobe very sparse, whereas relations such as elaboration prevail. This has no impact on the model presented in section 3, but it can bias the optimised inference of the model presented in section 5. The lower the occurrence of a rhetorical rela-tion in the dataset, the fewer the observations of retrieval performance associated with it, and hence the weaker the predictions we can infer about whether it is optimal or not. A fairer setting would be to have the same number of  X  X uery -retrieval performance X  observations for all rhetorical rela-tions -however that would imply fiddling with the document distribution of our dataset significantly, potentially harming its quality as a test collection.
A general limitation of discourse analysis is that not all types of text are susceptible to it. For instance, legal text, contracts, or item lists often lack rhetorical structure. In this work, we made no effort to identify and exempt such types of text from the discourse parsing. We reasoned that, as the SPADE parser includes a first-step grammatical parsing, the initial grammatical parsing of these types of text would flag out ill-formed parts (e.g. missing a verb, or consisting of ex-tremely long sentences), which would then be skipped by the discourse analysis. This was indeed the case, however at a certain efficiency cost. Overall processing speed for SPADE was approximately 19 seconds per document (including the initial grammatical parsing), on a machine of 9 GB RAM, 8 core processor at 2.27GHz. One way of improving this performance would be to update the first-step grammatical parsing. Currently this depends on the well-known Charniak parser [6], which is one of the best performing grammatical parsers, however no longer supported. Other state-of-the-art faster grammatical parsers, e.g. the Stanford parser could be adapted and plugged into SPADE instead.

The choice of applying our model for re-ranking as op-posed to ranking all documents was closely related to the efficiency concerns discussed above. Our model is not spe-cific to re-ranking only, however, using SPADE on approxi-mately 50 million documents was too expensive at this point. Improving the discourse parser X  X  efficiency is something we are currently working on, with the aim to apply our model for full ranking and see if the conclusions drawn from this work hold.

Finally, the accuracy of the discourse parser was not con-sidered in this work, apart from indications in the litera-ture that SPADE is a generally well-performing parser [27]. Given that the default version of the parser we used is trained on news articles, one may reason that its accuracy could improve if we train it on the retrieval collection, or on doc-http://nlp.stanford.edu/software/lex-parser.shtml uments of the same domain. Note that, parsing accuracy aside, rhetorical relations assignment is not an entirely un-ambiguous process, even to humans [23]. For the purposes of this work, this type of fine-grained ambiguity may however not be important to retrieval performance.
Future extensions include primarily making SPADE scal-able on large collections of documents as discussed above, as well as using more than one rhetorical relation per docu-ment. For instance, the posterior probabilities estimated in section 5.1 could be used to weight the text in each rhetor-ical relation. If those posteriors are too flat, an exponent could make them peakier. As the exponent goes to infin-ity, the maximum relation model presented in section 5.2 would be recovered. In addition, we intend to refine the discourse analysis by considering the nucleus (i.e. central) versus satellite (i.e. peripheral) rhetorical relations for IR, as well as to improve the effectiveness of the discourse parser by training it on data of the same domain. As discussed in section 3.2, we will also investigate alternative estimations of Equations 2-3.

An interesting future research direction is the potential relation between rhetorical relations and user context: for instance, in a search session including several query refor-mulations, is there a correlation between the progression of the information need of the user and the rhetorical rela-tions that the retrieval system should boost in a document (e.g. elaboration ), as indicated by Sun &amp; Chai [28]? An-other interesting future extension of this work is in relation to evaluation measures of graded relevance measures on an inter-document level, as investigated in XML retrieval [20] for instance. If parts of a document can be regarded as more or less relevant, this may be reflected to their discourse struc-ture. This might be especially useful for multi-threaded doc-uments, such as multiple-user reviews and opinions, where the discourse relations tend to shift markedly. Finally, the current operationalisation of our model is simplistic in the sense that the term  X  X hetorical relation X  is coerced into mean-ing  X  X on-overlapping text fragment X  and the actual relation between bits of text is discarded in the process. In future work we could apply fielded XML retrieval models in order to investigate nested structuring among rhetorical relations.
Rhetorical relations, e.g. contrast, explanation, con-dition , indicate the different ways in which the parts of a text are linked to each other to form a coherent whole. This work studied two questions: Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document X  X  rhetorical relations be use-ful to IR? To address these, we presented a retrieval model that conditions the probability of relevance between a query and a document on the rhetorical relations occurring in that document. We applied that model to an IR re-ranking sce-nario for Web search. Experimental evaluation of different versions of our model on TREC data and standard settings demonstrated that certain rhetorical relations can be bene-ficial to retrieval, with &gt; 10% improvements to retrieval pre-cision. Furthermore, we showed that these improvements over the baseline can improve significantly, when the opti-mal rhetorical relation per document is selected for retrieval.
Overall, three rhetorical relations were found to benefit retrieval performance notably and consistently for different evaluation measures and query sets: background, cause-result and topic-comment . In retrospect, this is perhaps not surprising, since these are among the most salient dis-course relations on an intuitive basis: the main topic or theme of a text, its background, causes and results [21]. Fu-ture extensions and research directions of this work include considering more than one rhetorical relation per document, applying our model for ranking all documents (as opposed to re-ranking only) and experimenting with alternative esti-mations of its components. We thank Kasper Hornb X k, Jakob Grue Simonsen, Raf Guns, Qikai Cheng and the anonymous reviewers for help-ing improve this paper. Work partially funded by the Dan-ish International Development Agency DANIDA (grant no. 10-087721) and the National Natural Science Foundation of China (grant no. 71173164). [1] Proceedings of the 2011 Conference on Empirical [2] A. Azevedo-Filho and R. D. Shachter. Laplace X  X  [3] M. Bendersky, W. B. Croft, and Y. Diao.
 [4] C. Buckley and E. M. Voorhees. Retrieval evaluation [5] L. Carlson, D. Marcu, and M. E. Okurowski. Building [6] E. Charniak. A maximum-entropy-inspired parser. In [7] J. Clarke and M. Lapata. Discourse constraints for [8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. [9] W.B.CroftandJ.Lafferty. Language Modeling for [10] A. C. Davison. Statistical Models . Cambridge [11] D. A. duVerle and H. Prendinger. A novel discourse [12] C. J. Fillmore, C. F. Baker, and S. Hiroaki. The [13] W. A. Gale and G. Sampson. Good-turing frequency [14] S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli. [15] D. Gildea and D. Jurafsky. Automatic labeling of [16] B. Heerschop, F. Goossen, A. Hogenboom, [17] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [18] P. Kingsbury and M. Palmer. From treebank to [19] E. Krikon and O. Kurland. A study of the integration [20] M. Lalmas. XML Retrieval . Synthesis Lectures on [21] A. Louis, A. K. Joshi, and A. Nenkova. Discourse [22] R. Manmatha, T. M. Rath, and F. Feng. Modeling [23] W. C. Mann and S. A. Thompson. Rhetorical [24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro. [25] M. D. Smucker, J. Allan, and B. Carterette.
 [26] S. Somasundaran, G. Namata, J. Wiebe, and [27] R. Soricut and D. Marcu. Sentence level discourse [28] M. Sun and J. Y. Chai. Discourse processing for [29] N. Suwandaratna and U. Perera. Discourse marker [30] S. Teufel and M. Moens. Summarizing scientific [31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L. [32] J. Wang and J. Zhu. On statistical analysis and [33] L.Wang,M.Lui,S.N.Kim,J.Nivre,and [34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric [35] C. Zhai and J. D. Lafferty. Two-stage language models [36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong.
