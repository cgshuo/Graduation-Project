 Natural language processing (NLP) systems, like machine translation (Xie et al., 2011), resource-low languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambigua-tion (Fauceglia et al., 2015) , and entity corefer-ence resolution (Durrett and Klein, 2013), are be-coming more sophisticated, in part because of uti-lizing syntacitc knowledges such as dependency parsing trees.

Dependency parsers predict dependency struc-tures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factoriza-tions are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be im-portant to provide more accurate tree structures and labels (Nivre and Scholz, 2004).

Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single la-bel features. Other work (Johansson and Nugues, 2008; Bohnet, 2010) used richer label features but increased systems X  complexities significantly, while achieving better parsing accuracy. Yet, there are no previous work addressing the problem of good balance between parsing accuracy and com-putational costs for joint parsing models.

In this paper, we propose a new dependency parsing algorithm that can utilize edge-label infor-mation of more than one edge, while simultane-ously maintaining low computational complexity. The component needed to solve this dilemma is an inner-to-outer greedy approximation to avoid an exhaustive search. The contributions of this work are (i) showing the effectiveness of edge-label in-formation on both UAS and LAS. (ii) proposing a joint learning parsing model which achieves both effectiveness and efficience. (iii) giving empiri-cal evaluations of this parser on different treebanks over 14 languages. 2.1 Basic Notations In the following, x represents a generic input sen-tence, and y represents a generic dependency tree. Formally, for a sentence x , dependency parsing is the task of finding the dependency tree y with the highest-score for x :
Here Y ( x ) denotes the set of possible dependency trees for sentence x .

In this paper, we adopt the second-order sib-ling factorization (Eisner, 1996; McDonald and Pereira, 2006), in which each sibling part consists of a tuple of indices ( h,m,c ) where ( h,m ) and ( h,c ) are a pair of adjacent edges to the same side of the head h . By adding labele information to this factorization, Score( x,y ) can be rewritten as: where S sib ( h,m,c,l 1 ,l 2 ) is the score function for the sibling part ( h,m,c ) with l 1 and l 2 being the labels of edge ( h,m ) and ( h,c ) , respectively. f are feature functions and  X  is the parameters of parsing model. 2.2 Exact Search Parsing The unlabeled sibling parser introduces three types of dynamic-programming structures: com-plete spans C ( s,t ) , which consist of the headword s and its descendents on one side with the endpoint t , incomplete spans I ( s,t ) , which consist of the de-pendency ( s,t ) and the region between the head s and the modifier t , and sibling spans S ( s,t ) , which represent the region between successive modifiers s and t of some head. To capture label infor-mation, we have to extend each incomplete span I s,t to I s,t,l to store the label of dependency edge from s to t . The reason is that there is an edge shared by two adjacent sibling parts (e.g. ( h,m,c ) and ( h,c,c 0 ) share the edge ( h,c ) ). So the in-complete span I ( s,t ) does not only depend on the label of dependency ( s,t ) , but also the label of the dependency ( s,r ) for each split point r . The dynamic-programming procedure for new incom-where L is set of all edge labels. Then we have,
The graphical specification of the this parsing al-gorithm is provided in Figure 1 (a). The compu-tational complexity of the exactly searching algo-rithm is O ( | L | 2 n 3 ) time and O ( | L | n 2 ) space. In practice, | L | is probably large. For English, the number of edge labels in Stanford Basic Depen-dencies (De Marneffe et al., 2006) is 45, and the Figure 1: The dynamic-programming structures and derivation of four parsing algorithms. I ( s,t,l ) and I ( s,t ) are depicted as trapezoids with solid and dashed lines, respectively. C ( s,t ) are depicted as triangles and S ( s,t ) are depicted as boxes. Sym-metric right-headed versions are elided for brevity. number in the treebank of CoNLL-2008 shared task is 70. So it is impractical to perform an ex-haustive search for parsing, and more efficient ap-proximating algorithms are needed. 2.3 Two Intermediate Models In this section, we describe two intuitive simpli-fications of the labeled parsing model presented above. For the two simplified parsing models, ef-ficient algorithms are available. 2.3.1 Model 0: Single-edge Label In this parsing model, labeled features are re-stricted to a single edge. Specifically,
Then the dynamic-programming derivation for each incomplete span becomes where l ( s,t ) = argmax
In this case, therefore, we do not have to ex-tend incomplete spans. The computational cost to calculate l ( s,t ) is O ( | L | n 2 ) time, so the compu-tational complexity of this algorithm is O ( n 3 + | L | n 2 ) time and O ( n 2 ) space. 2.3.2 Model 1: Sibling with Single Label As remarked in McDonald (2006), Model 0 can be slightly enriched to include single label features associated with a sibling part. Formally, Now, the dynamic-programming derivation is where l ( s,r,t ) = argmax
The additional algorithm to calculate the best edge label l ( s,r,t ) takes O ( | L | n 3 ) time. There-fore, this algorithm requires O ( | L | n 3 ) time and O ( n 2 ) space 2 . Figure 1 (b) and Figure 1 (c) pro-vide the graphical specifications for Model 0 and Model 1, respectively. 2.4 Inner-to-outer Greedy Search Though the two intermediate parsing models, model 0 and model 1, encode edge-label infor-mation and have efficient parsing algorithms, the labeled features they are able to capture are rela-tively limited due to restricting their labeled fea-ture functions to a single label. Our experimental results show that utilizing these edge-label infor-mation yields a slight improvement of parsing ac-curacy (see Section 3 for details). In this section, we describe our new labeled parsing model that can exploit labeled features involving two edge-labels in a sibling part. To achieve efficient search, we adopt an method characterized by inferring la-bels of outer parts from the labels of inner ones.
Formally, consider the maximization problem in Eq 3. It can be treated as a two-layer maxi-mization: first fixes a split point r and maximizes over all edge-label l , then maximizes over all pos-sible split points. Our approach approximates the maximization in the first layer:
Then the dynamic-programming derivation for each incomplete span is
To compute I ( s,t ) , we need to calculate which is similar to the calculation of l ( s,r,t ) in Model 1. The only difference between them is Thus, their computation costs are almost the same.
The procedure of our algorithm to derivate in-complete spans can be regarded as two steps. At the first step, the algorithm goes through all pos-sible split points (Eq 7). Then at the second step, at each split point r , it calculate the label l ( s,r,t,l  X  ( s,r,t ) and the label l  X  bel for dependency edge ( s,r ) based on incom-plete span I ( s,r ) . The key insight of this algorithm is the inner-to-outer dynamic-programming struc-ture: inner modifiers ( r ) of a head ( s ) and their ones ( t ). Thus, using already computed  X  X est X  la-bels of inner dependency edges makes us get rid of maximizing over two labels, l 0 and l . Moreover, we do not have to extend each incomplete span by the augmentation with a  X  X abel X  index. This makes the space complexity remains O ( n 2 ) , which is im-portant in practice. The graphical specification is provided in Figure 1 (d). 3.1 Setup We conduct our experiments on 14 languages, in-cluding the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 tree-banks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). We train our parser using The k -best version of the Margin Infused Relaxed Algo-rithm (MIRA) (Crammer and Singer, 2003; Cram-mer et al., 2006; McDonald, 2006). In our experi-ments, we set k = 1 and fix the number of iteration to 10, instead of tuning these parameters on devel-opment sets. Following previous work, all exper-iments are evaluated on the metrics of unlabeled attachment score (UAS) and Labeled attachment 2006 shared task. 3.2 Non-Projective Parsing The parsing algorithms described in the paper fall into the category of projective dependency parsers, which exclude crossing dependency edges. Since the treebanks from CoNLL shared tasks con-tain non-projective edges, we use the  X  X ountain-significant improvements with p &lt; 0 . 05 . climbing X  non-projective parsing algorithm pro-posed in McDonald and Pereira (2006). This ap-proximating algorithm first searches the highest scoring projective parse tree and then it rearranges edges in the tree until the rearrangements do not 3.3 Results and Comparison Table 1 illustrates the parsing results our parser with non-projective parsing algorithm, together with three baseline systems X  X he two-stage sys-tem (McDonald, 2006) and the two intermediate models, Model 0 and Model 1 X  X nd the best sys-tems reported in CoNLL shared tasks for each lan-guage. Our parser achieves better parsing perfor-mance on both UAS and LAS than all the three baseline systems for 12 languages. The two ex-ceptions are Portuguese and Turkish, on which our parser achieves better LAS and comparable UAS.
Comparing with the best systems from CoNLL, our parser achieves better performance on both UAS and LAS for 9 languages. Moreover, the av-erage UAS of our parser over the 14 languages is better than that of the best systems in CoNLL. It should be noted that the best results for 14 lan-guages in CoNLL are not from one single system, but different systems that achieved best results for Table 2: Parsing performance on PTB. The re-sults for MaltParser, MSTParser and DNNParser are from table 5 of Chen and Manning (2014). different languages. The system of McDonald et al. (2006) achieved the best average parsing per-formance over 13 languages (excluding English) in CoNLL-2006 shared tasks. Its average UAS and LAS are 87.03% and 80.83%, respectively, while our average UAS and LAS excluding English are 87.79% and 81.29%. So our parser shows signif-icant improvement over the single best system re-ported in CoNLL-2006 shared task. 3.4 Experiments on PTB To make a thorough empirical comparison with previous studies, we also evaluate our system on the English Penn Treebanks (Marcus et al., 1993) with Stanford Basic Dependencies (De Marn-effe et al., 2006). We compare our parser with three off-the-shelf parsers: MaltParser (Nivre and Scholz, 2004; Zhang and Clark, 2008; Zhang and Nivre, 2011), MSTParser (McDonald et al., 2005), and the parser using Neural Networks Surdeanu et al. (2008). (DNNParser) (Chen and Manning, 2014). The re-sults are listed in Table 2. Clearly, our parser is superior in terms of both UAS and LAS. 3.5 Analysis To better understand the performance of our parser, we analyze the distribution of our parser X  X  UAS and LAS over different dependency labels on the English CoNLL treebank, compared with the ones of the two-stage model. Table 3 lists the top 10 dependency labels on which our algorithm achieves most improvements on the F1 score of UAS, together with the corresponding improve-ments of LAS.

From Table 3 we can see among the 10 labels, there are 5 labels  X   X  X NR X ,  X  X DV X ,  X  X MP X ,  X  X IR X ,  X  X OC X   X  which are a specific kind of ad-verbials. This illustrates that our parser performs well on the recognition of different kinds of ad-verbials. Moreover, the label  X  X PRD X  and  X  X BJ X  indicate dependency relations between verbs and their modifiers, too. In addition, our parser also significantly improves the accuracy of apposi-tional relations ( X  X PPO X ). We proposed a new dependency parsing algo-rithm which can jointly learn dependency struc-tures and edge labels. Our parser is able to use multiple edge-label features, while maintaining low computational complexity. Experimental re-sults on 14 languages show that our parser sig-nificantly improves the accuracy of both depen-dency structures (UAS) and edge labels (LAS), over three baseline systems and three off-the-shelf parsers. This demonstrates that jointly learning dependency structures and edge labels can bene-fit both performance of tree structures and label-ing accuracy. Moreover, our parser outperforms the best systems of different languages reported in CoNLL shared task for 9 languages.

In future, we are interested in extending our parser to higher-order factorization by increas-ing horizontal context (e.g., from siblings to  X  X ri-siblings X ) and vertical context (e.g., from siblings to  X  X rand-siblings X ) and validating its effective-ness via a wide range of NLP applications.
 This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.

