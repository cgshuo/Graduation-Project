 Information retrieval evaluation most often involves manually as-sessing the relevance of particular query-document pairs. In cases where this is difficult (such as personalized search), interleaved comparison methods are becoming increasingly common. These methods compare pairs of ranking functions based on user clicks on search results, thus better reflecting true user preferences. How-ever, by depending on clicks, there is a potential for bias. For ex-ample, users have been previously shown to be more likely to click on results with attractive titles and snippets. An interleaving eval-uation where one ranker tends to generate results that attract more clicks (without being more relevant) may thus be biased.

We present an approach for detecting and compensating for this type of bias in interleaving evaluations. Introducing a new model of caption bias, we propose features that model bias based on (1) per-document effects, and (2) the (pairwise) relationships between a document and surrounding documents. We show that our model can effectively capture click behavior, with best results achieved by a model that combines both per-document and pairwise features. Applying this model to re-weight observed user clicks, we find a small overall effect on real interleaving comparisons, but also iden-tify a case where initially detected preferences vanish after caption bias re-weighting is applied. Our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias.
 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Keywords: Interleaving, Implicit feedback, Evaluation
Most information retrieval (IR) evaluation methods rely on man-ual relevance assessments of query-document pairs [26, 30]. When used to evaluate Web search retrieval functions, such an evalua-tion assumes that assessors can reliably infer users X  information Most of this research was done while the author was at Microsoft. needs from very limited information (usually only the queries). Accurately making such inferences may be difficult for ambigu-ous queries, and even more so in settings where relevance can vary widely by user, such as personalized or localized search.
In settings where such relevance assessment is difficult, inter-leaved comparison methods are a promising alternative [4, 19]. These methods compare two retrieval functions (also called rankers) by presenting users with a list of results that combines documents returned by the two rankers, inferring preferences based on users X  clicks. Because interleaving can be applied transparently to all re-sults shown to users, and Web search engines can collect click data at low cost, interleaving offers a scalable way of performing evalu-ation that reflects true user needs and preferences.

A drawback of current interleaving methods is that they do not account for potential click bias introduced by effects of result pre-sentation (in particular Web result captions), rather assuming that this bias affects all rankers equally. 1 Users X  click behavior is known to be influenced by e.g., highlighting [32], as well as URL length, the occurrence of the query in result titles, and other aspects of result presentation [5]. Thus, rankers that rank results with these characteristics highly may attract more clicks than warranted by the quality of the ranking, thereby biasing interleaving outcomes.
This is the problem that we address in this paper: How are inter-leaving outcomes affected by differences in result presentation in practice? On the one hand, as previous work has assumed, caption bias may affect rankers equally. This would increase variance in evaluation but not introduce bias. On the other hand, typical ranker optimization changes may affect captions (for example, by favoring titles with more highlighting), thereby creating a systematic bias. When interleaving methods are applied to measure preferences be-tween rankers, it is important to be able to identify both when bias may be occurring, and to be able to avoid this bias. Specifically, our contributions are: 1. We introduce a general probabilistic approach for modeling 2. We propose two types of features to instantiate this model 3. We apply this approach to real interleaving experiments, find-
We use click bias to refer to any characteristic of a search result that systematically influences click behavior in such a way that a result receives more or fewer clicks than would be warranted by the item X  X  content-based relevance to the query alone. We use caption bias to refer to forms of click bias related to the visual presentation of results on a search result page.
The results of our analysis have implications for how and in what cases interleaving methods can be applied in practice. In particular, our work contributes to better understanding IR evaluation using interleaving methods, and making them more reliable and robust. Further, to the best of our knowledge, this is the first work on com-pensating for caption bias in interleaving methods.

The remainder of this paper is organized as follows. We give an overview of related work in Section 2 and detail our approach in Section 3. Our experiments and results are presented in Section 4. In this section we review related work on using click behavior for IR evaluation ( X 2.1), then consider interleaving in particular ( X 2.2). Finally, we review prior work on caption bias ( X 2.3).
User behavior as captured by clicks on Web search results is a rich source of information on user preferences. Clicks are part of the natural interaction between users and Web search engines, and in comparison to other types of feedback can be collected in large quantities at very low cost. Consequently, a large body of work has focused on using click behavior both to improve search result quality (e.g., [1, 2, 8, 19, 28]), and to infer information about users X  satisfaction with the search results (e.g., [3, 21, 25]). An overview of a large variety of other types of implicit feedback, in addition to click behavior, was presented by Kelly and Teevan [22].

It has also long been apparent that click data is strongly biased, in particular by the position of documents in the presented result list (i.e., by position bias, cf., 2.3). For instance, top-ranked Web search results are clicked much more frequently than lower-ranked results, even in the absence of a strong difference in relevance (e.g., [20]). In the context of evaluation, it has also been found that clicks vary considerably between users and between search tasks [27]. This means that clicks cannot be interpreted as absolute judgments beyond certain narrow settings (e.g., [16]).
A solution to the problem of position bias in particular is to in-terpret clicks as relative preferences, either between a pair of doc-uments [19, 24] or a pair of retrieval functions [20, 25, 31]. For example, a higher-ranked document that was not clicked can be in-terpreted as likely less relevant than a lower-ranked document that was clicked by the user. This relative interpretation was found to be much more reliable than absolute interpretations [20].
Based on the relative comparison intuition, interleaving com-bines search results from two competing rankers and presents these to the user. The results are selected so that it is likely that a user who clicks on more relevant results will be interpreted as preferring the ranker that places more relevant ranks highly, while a randomly clicking user will be interpreted as preferring neither. For instance, given two rankings A and B produced by retrieval functions r r
B for a given query, Team Draft interleaving [25] iteratively cre-ates a combined list as follows: Toss a fair coin. If r coin toss, its top result is selected as the first result in the combined list and then the first different result from B is selected as the sec-ond result in the combined list. Conversely, if r B wins the coin toss, r B selects first and then r A . This toss-then-two-selections is repeated (each time finding the first result in each ranker that is dif-ferent from all previously selected results) until sufficiently many results are in the combined list. When users are presented with the combined list, clicks are mapped back to the ranker that selected each clicked result. The ranker that selected the larger number of clicked results is deemed to  X  X in X  the comparison for this query.
Note that interleaving involves showing each user results returned by both retrieval functions, allowing the user X  X  selection process to provide evidence as to which retrieval function, in expectation, re-turns relevant results more often. By doing this direct comparison, interleaving has been shown to be more sensitive than alternative approaches. Radlinski and Craswell [23] compared the reliabil-ity and sensitivity of Team Draft interleaving to judgement-based evaluation in a Web search setting, and Chapelle et al. [4] provide a detailed comparison and evaluation of several interleaving ap-proaches. Alternative interleaving approaches have also been pro-posed [20], as well as alternative scoring approaches [15]. How-ever, it has been shown that all these interleaving algorithms may potentially be affected by position bias in subtle cases [17, 25]. These biases are addressed by the Probabilistic Interleave approach, recently developed by Hofmann et al. [17]. Probabilistic Interleave has favorable theoretical properties, but has not yet been evaluated in a real search setting.

Probabilistic Interleave is based on a probabilistic formulation of Team Draft interleaving. First, rather than always allowing both rankers to select a document after a coin toss, only the ranker win-ning the coin toss selects the next document. Second, rather than al-ways selecting the next-highest ranked document from the winning ranker, Probabilistic Interleave selects documents with frequency decaying in the position of the document. Finally, instead of a doc-ument being assigned to the ranker that selected each document shown to users, Probabilistic Interleave computes all possible coin toss sequences that could have generated the list of results shown to users, and computes the preference between rankers as an expec-tation across these possible coin toss sequences.
While clicks are becoming more popular as a source of prefer-ence indications on search results, a number of studies have found that click behavior is affected by bias. In this section we give a brief overview of the types of bias previously found to affect users X  click behavior in Web search.

In the context of interleaved comparisons, position bias has been addressed. Position bias results from the layout of a search result page. Because users generally expect more relevant items to be listed at the top of a page, and because people are used to reading pages from top to bottom, top-ranked results are typically the most likely to be examined. This phenomenon was first confirmed in eye-tracking studies such as [11, 12]. Craswell et al. [6] developed models of user behavior to describe position bias, and described a cascade model to explain this effect. The model was refined in several follow-up studies, e.g., to account for multiple clicks on the same result page [14], and to account for differences in click behavior for different types of queries and search goals [13].
In addition to position bias, which reflects where on the page a result was displayed, click behavior is also affected by caption bias, caused by how the result was displayed. Clarke et al. [5] studied caption bias by comparing features and click behavior on pairs of search results. They found that results were more often clicked on when they had longer snippets, shorter URLs, more query terms matching the caption, matches of the whole query as a phrase, if the caption was more readable, or if it contained the term  X  X fficial X  or terms related to images. Yue et al. [32] compared click data on result documents that were sampled to minimize position bias using the Fair Pairs approach [24] to editorial judgments. They identified a bias towards captions that included more highlighted (bold) terms, i.e., items with more bold terms would be clicked more frequently than similar results with fewer bold terms.
Other factors affecting click behavior include the domain of the search result [18], whether or not search results are grouped (e.g., by intent) [9], page loading time [31], the amount of context shown in the snippet [29], and the relation between task and result cap-tion [7]. In this work we focus on effects of visual factors that are most common in a Web search setting. However, our approach is generally applicable to other sources of click bias.
Our method is based on the following idea: when assigning credit for clicks to rankers in an interleaving experiment, the credit can be re-weighted to reflect a likelihood of the user clicking on the result based on just caption bias. This is similar to the ideas pre-sented in [4, 32], although here we focus on improving the fidelity rather than the sensitivity of interleaving experiments.

We will re-weight clicks by the inverse of their caption-based click probability. Results that are very likely to be clicked due simply to visual characteristics receive a low weight, while higher weights are assigned to results whose representation is less likely to attract clicks. Thus, clicks on relatively less  X  X lickable X  results are taken to provide a more reliable indication of relevance, while more  X  X lickable X  results are considered prone to attracting clicks unwar-ranted by their relevance and receive a lower weight. This princi-ple is implemented in the following 3-step approach: (1) model the probability of a click as a combination of position, relevance and caption bias, (2) learn the weights of this model using observations of past user behavior, and then (3) factor out the caption bias com-ponent from interleaving evaluations to make clicks better reflect relevance. We now detail each of these three steps.
The first step is to model caption bias. To obtain an interpretable model, we choose to model the probability of a click using a simple model of the form P ( C | X r ,X p ,X c ) = [1 + exp (  X  W r X r  X  W p X p  X  W Here, P ( C | X r ,X p ,X c ) denotes the probability of a click on a re-sult document, that is characterized by relevance features X tion features X p , and caption features X c . We train the parameters of this model, W r , W p , and W c , using logistic regression. We note that while a model that takes into account non-linear combinations of bias features may produce somewhat more accurate results, this model is easy to interpret, less prone to overfitting than more com-plex models, and was found to perform well when validated on the task of predicting clicks (cf., Section 4.2).

Note that only caption features X c are used when applying the model to compensate for caption bias (cf., Section 3.4). The re-maining features are included during model training only, to re-move effects of document relevance and position when training the components of weight vector W c . In this way we obtain a model of a document X  X  click likelihood given its presentation. The caption features used in this study are detailed in Section 3.2 below.
For model training, relevance and position features are formu-lated as follows: X r ( q,d ) models the relevance of a document d to a query q as a vector of five binary features. They represent stan-dard 5-point relevance judgments that range from  X  X ot relevant X  to  X  X ighly relevant X . Our position features X p follow the formulation in [32]. Specifically, we use six binary indicator features that indi-cate whether each document was presented at rank 1, 2, 3, 4 to 5, 6 to 9, or 10 and below.

In a preliminary study, we also considered two alternative ap-proaches. First, we assessed document-wise models that do not take into account relevance information, but simply model caption bias using visual and position features. However, we found that models that do take relevance into account model click behavior more accurately. Second, we evaluated a pairwise model that pre-dicted which of two documents was more likely to be clicked, based on features that captured visual differences between them (i.e., pre-dicting P ( Click A &gt; Click B ) for two candidate documents A and B , similarly to Clarke et al. [5] and Yue et al. [32]). Here, we found the per-document formulation in Equation 1 to be much more ef-fective in explaining click behavior, and therefore focus on this model in the rest of this work. Nevertheless, we found that pair-wise features, that capture the relationship between the document for which clicks are predicted and its neighboring documents, can be combined with our per-document model to further improve per-formance (implemented as pairwise features, cf., Section 3.2).
We use two types of visual features to model caption bias in X per-document features and pairwise features. We now introduce each in turn. For all features, we assume a standard result page of a Web search engine. Results are displayed with title, URL, and a snippet that shows how each document relates to the user X  X  query. Our per-document features are designed to capture characteristics of individual search result captions, and model aspects that may make them likely to attract (or discourage) user clicks. We started with the features investigated in Clarke et al. [5], such as short snip-pet , term matches in the title , and URL length .

From the initial set of features, we restricted our features to those that we believed may capture visual characteristics relevant to our task, yet not likely to be strongly affected by document relevance. In an initial study we found a statistically significant effect of e.g., the number of query term matches with the document title, and the number of phrase matches with the snippet on click behavior. How-ever, we were not confident that these effects were not strongly af-fected by the rankers used to collect our training data. E.g., a ranker may over-or under-emphasize the importance of matches in the document title, while the 5-point relevance judgments (cf.,  X 3.1) used to remove major effects of relevance may not be sufficiently fine-grained to compensate for these ranker effects. To avoid con-tamination of our caption bias model with such ranker effects, we decided to remove features for which these were a concern.
We binarized all per-document features to avoid cases where the model would be dominated by individual unbounded values. For each  X  X aw X  feature (e.g., title length), we started with natural thresholds, such as the first and third quartile, the mean, and points identified by visual inspection of the feature X  X  histogram. Next, bi-nary features representing these bins were added to a model of doc-ument relevance and position, which was then trained using logistic regression. The thresholds were then manually tuned to maximize the model X  X  fit to the training data (i.e., thresholds were increased and decreased and the model re-trained, until the magnitude of the residuals from the fitted model did not decrease further).
Finally, all constructed binary features were combined in one model, and features that did not have a significant effect on the models X  prediction (measured using a  X  2 test, and p &lt; 0 . 001 ) were removed from the model. In this step we reduced the number of features from 25 to the final set of 10 per-document features.
Our per-document features are presented in Table 1. The fea-ture deep links refers to links to sub-sections of a website that are grouped under a main title result as illustrated in Figure 1. We included this feature because a strong relation with click behavior was found, and this type of presentation is common to most major Figure 1: Example search results of two commercial Web search engines, with deep links included in addition to the ti-tle link.
 Web search engines. The length-related features short URL , short title , long title , short snippet , and long snippet were converted to binary values as described above. For longer URLs, the number of slashes was found more informative than the number of characters. Similarly, for title length, the number of words was more informa-tive than the number of characters. For snippet length, we note that the threshold for short snippets corresponds to roughly half a line of text, while the threshold for long snippets corresponds to a length where the text would flow onto a third line.

Here, we only list the final features and exclude features where no significant effect on click behavior was detected (e.g., highlight-ing in the snippet). In addition to those listed, we initially tested the following features proposed in earlier studies [5]: the number of query term matches with the title, snippet, and URL respectively, and the respective number of phrase matches. However, because our models were developed to specifically capture changes in click behavior due to visual characteristics, we removed these features that may be more strongly affected by document content.
 Our second set of features considers not individual documents, but pairs of documents. The intuition behind these features is that doc-uments presented in response to a query attract clicks not only based on their own representations, but also depending on other, surrounding documents. For example, a somewhat attractive result may attract clicks when placed next to a poorly presented result, but may not get much attention when placed next to a result with a better presentation. Although we found per-document models to perform better individually, we hypothesized that click behavior could best be captured by a combination of characteristics of a doc-ument X  X  own representation, and those of surrounding documents.
As for our per-document features, we avoided unbounded fea-tures to prevent individual features from dominating the model (this may happen when e.g., directly including the difference in URL length). We achieved this by encoding all pairwise features as ternary values (i.e., with possible values (  X  1 , 0 , 1 ). Thus, e.g., the feature  X  URL length above would be  X  1 if the URL length of the current document is less than that of the document ranked immedi-ately above it, 0 if there was no difference, and 1 if the URL was longer than that of the document ranked above it.

A complete list of the pairwise features investigated is provided in Table 2. The features  X  title bold above / below (in words) and  X  snippet bold above / below (in words) are designed to capture relationships between neighboring documents that are as close as possible to those explored in [32], so that effects found here can be compared to this earlier work. In addition, we include features that capture differences in highlighting of the URL, and consider the number of highlighted sections (e.g., phrases) in addition to that of individual words. Finally, we add features capturing the length differences of URL, title, and snippet. As for the document-wise features, we exclude features based on term matches, to focus on visual aspects of the search result captions.
We apply our caption bias models to Team Draft [23, 25] and a variant of Probabilistic Interleave [17]. We describe both below, focusing on how each method aggregates clicks observed within a search result page to infer a comparison outcome, as this is the component that is affected by our caption re-weighting mechanism.
Our implementation of the Team Draft method follows [4]. As explained above, for each observed query an interleaved result list is generated and presented to the user (cf., Section 2.2). User clicks on result documents are observed and then mapped back to the ranker that contributed the corresponding documents. Then, given n observed samples ( q,C A ,C B ) , where q  X  Q is a query, and C and C B are the sets of documents clicked that were contributed by ranker A and B respectively, our first scoring function for compar-ing ranked lists is defined as the mean of observed click differences: This scoring function is equivalent to the aggregation method called  X  click in [4]. Note that in earlier work, click preferences were re-centered to let a value of 0 . 5 denote  X  X o preference X  (i.e., the rankers are inferred to perform equally well). Then, a win for ranker A is detected if the observed score is statistically signifi-cantly higher than 0 . 5 . Here we follow the same convention, and report all scores centered around 0 . 5 . Finally, we ignore clicks on documents above the first document for which the ranked lists pro-duced by A and B differ, to reduce variance [4, 23].

Our implementation of Probabilistic Interleave is a variant of the method originally proposed in [17]. In particular, we use the same interleaving process as Team Draft (that is, we do not interleave probabilistically) to minimize effects on user experience. However, we compute comparison outcomes as if Probabilistic Interleave had been used. This results in comparisons that weight clicks by the magnitude of the difference in position between rankers. For exam-ple, a ranker would gain a small win (in terms of weighted click) for moving a document up by one rank, and a large win when the clicked document was moved up from a much lower rank.
 Next, we define the scoring function for Probabilistic Interleave. In contrast to the Team Draft method, we now do not observe the number of clicks attributed to each ranker, but instead observe a vector C , that indicates which documents of the interleaved list were clicked. This is done so that click counts can be obtained for all possible combinations in which rankers could have contributed documents to the interleaved list. Individually, such a combina-tion of contributing rankers is called an assignment g  X  G . For example, an assignment could be [ A,B,B,A ] , denoting that, for a ranked list of length 4, the first and last documents were contributed by ranker A, and the remaining documents were contributed by B. Further, we use C A g to denote the number of clicks in C that would be obtained by ranker A when assuming assignment g .

We can then define a scoring function for Probabilistic Interleave that marginalizes over all possible assignments for n observed sam-ples consisting each of a query q , an interleaved list l , and a click vector c , and weights the score for each assignment by its probabil-ity P ( g | l i ,q i ) : As in [17], we compute this quantity using P ( G | L,Q ) = P ( L | G , Q ) P ( G | Q ) /P ( L | Q ) , where P ( G | Q ) = P ( G ) = 1 / | G | , and is the probability of observing document L [ r ] at rank r of an inter-leaved list L for query Q , given that it was contributed by the ranker specified in the assignment, i.e., G [ r ] ; and documents L [1 ,r  X  1] have already been placed in L . Finally, we define the probability of a document inversely proportional to its cubed rank, as in [17].
We now come to the key innovation of our approach, namely correcting the outcome of interleaving experiments for the effect of caption bias.

Specifically, for each click by a user, rather than summing up all clicks for each query giving equal weight to each click, we credit each ranker inversely proportionally to the estimated probability of a click based just on the caption shown: where X c is the vector of all visual features of the document (i.e., rank and relevance are ignored), and W c is the weight vector as obtained by the trained regression model. This weighting scheme is a direct application of regression models, where weights for each feature reflect the ratio of the click likelihood for a document given the feature value as opposed to a document without that feature. For example, consider a result with a very short URL (i.e., short URL is true), and 3 highlighted sections in the displayed result title ( title bold ). Also, assume that the estimated weights for these fea-tures, obtained from the model, are 0 . 4 and 0 . 7 . Then, the weight for this document is  X  = 1 /exp (1 . 1) = 0 . 3329 . This low weight indicates that the result was much more likely to be clicked than other documents (as the weight is much lower than 1 ), hence should contribute little to the outcome of the interleaving experiment.
Given the caption bias weights predicted by our model, we adjust our scoring functions to apply re-weighting as follows. Thus, this weighted Team Draft scoring function applies the cap-tion bias click weight to each observed click before aggregating over those clicks.

For Probabilistic Interleave, the scoring function becomes:  X 
Our method for modeling caption bias and applying the resulting model to obtain re-weighted interleaving scores is experimentally validated in the next section.
In this section we detail our experimental setup and results in three parts. First, we focus on training caption bias models (4.1). Results of this step give insights into the features that were found to be useful for explaining click behavior and their relative impor-tance. Second, we assess the quality of the trained models, by com-paring their predictions to observed click behavior (4.2). Third, we show how our caption bias models can be applied to infer user pref-erences from clicks (4.3). Finally, we apply our best-performing caption-bias model to interleaving experiments and analyze its ef-fect on experiment outcomes (4.4).
In our first experiment we train several instantiations of our cap-tion bias model as defined in Equation 1. Analyzing which visual features contribute to explaining click behavior, and what their rel-ative importance is in each of the models, allows us to better under-stand the observed caption bias. In this experiment we consider the following instantiations of our model:
All four models were trained on a sample of log data obtained from a large commercial Web search engine. The data is a random sample of queries and result pages collected on February 16, 2012. To account for the effect of relevance, the log data was intersected with a large set of previously collected relevance judgments (stan-dard 5-point scale). This intersection resulted in approximately 420,000 (non-unique) query-URL pairs.

Note that, due to the intersection with relevance judgments, our training data set does not constitute a random sample. Rather, the use of previously collected judgments introduced bias in the data set, as more judgments were available for e.g., frequent queries, and for documents that were previously ranked highly by the search engine. However, this bias only affects our training data, and not the data sets used for evaluation and analysis.

The weights for our trained models are shown in Table 3. Recall that when we apply our model of caption features to re-weighting clicks, we only use the weights of the caption features to determine the relative change in click attractiveness, and ignore position and relevance features (cf., Eq. 4). Therefore, we only report and ana-lyze the regression weights for these visual features.

For the highlighting model, we find a relatively weak effect for all included features. URL bolding has a positive effect (i.e., more bold increases the click likelihood of a result), but only when com-pared to the document ranked below. Increased highlighting in the title always increases click probabilities, and this effect is stronger than that of highlighting in the snippet (in agreement with [32]). For increased highlighting in the snippet, a small negative effect is detected, which may be caused by easily identifiable caption spam.
For the document-wise model we identify several features that have a strong correlation with click behavior. The highest regres-sion weight is obtained for our deep links feature. This result matches our observation that results with deep links tend to attract more clicks (even on the title link), perhaps because they take up more space on the result page. For highlighting in the result title, a much stronger effect is observed than for the pairwise version of this feature. Finally, click probability is found to decrease for URLs with many slashes, and for short snippets, as expected.
Results for the pairwise model are similar, although the trained weights are larger in magnitude. For URL length, a negative im-pact is detected when the current result has a longer URL than the document above, however this effect is reversed when the URL is compared to the result below.

Finally, in our combined model we find that all per-document features found to be statistically significant previously again have a statistically significant impact, even when combined with pairwise features. However, significant effects are also found for additional pairwise features, suggesting that the click behavior observed in our training data can best be explained when document-wise and pairwise features are combined. The pairwise features that had a statistically significant effect when included in the combined model are  X  URL slashes below,  X  title length below, as well as all high-lighting features for result title and snippet.

To summarize, the visual features that have the largest effect are for models that take per-document features into account. Less im-pact was identified for pairwise features, but a combined model best explains observed click behavior.
Above, we presented four models of caption bias given visual features. Here, we compare the performance of these models by applying the models to predict user clicks on a new data set.
The data for this experiment was again obtained from the com-mercial Web search engine, but was collected several days after the training data. We obtained three non-overlapping random samples (by user), from February 23 to 26, 2012. Each data set consists of queries, the presented search results, and the observed clicks. Below, we refer to these evaluation sets as A , B , and C .
The task on which we evaluate the trained models of caption bias is to predict whether a given document will be clicked or not, based on its visual characteristics and its position in the result list. As ground truth, we use the actually observed clicks.

We measure performance in terms of perplexity, a measure that is typically used to compare the quality of the predictions of a prob-abilistic model to observed outcomes, e.g., to evaluate click predic-tion methods [ 10]. It is formulated as 2  X  P N i =1 1 N x are observed events (here, whether a document was clicked or not), q ( x i ) is the probability of an observed event predicted by our model, and N is the number of observations. Intuitively, perplex-ity captures the degree of  X  X urprise X  that remains after a predictive model is applied. When applying a perfect model that can accu-rately predict all observed events, no surprise remains, and per-plexity is 1. A uniformly random model would obtain a perplexity of 2 , indicating that the model would not provide any information as to whether or not a result document is clicked.

In addition to the four models discussed in the previous section, we add a baseline model, that does not take any visual features into account (but is trained using relevance and position features, and predicts click behavior using document position alone).
Our results are presented in Table 4. Surprisingly, we find that the least predictive model is not the baseline (without any visual features), but the highlighting model. It performs worse than the baseline in all cases, even though it better explained click behavior on the training data. This suggests that the highlighting model may be overfitting the training data. The pairwise model is little bet-ter than the baseline, suggesting that pairwise features alone may not be able to accurately represent users X  click decisions. Better performance is achieved by our document-wise model. The best performance (lowest perplexity) over all data sets is obtained by our combined model. Apart from data set C, where the document-wise mode is not statistically different from the combined model, all other models on all other data sets perform significantly worse than our best-performing model. Our results suggest that a com-bined model of visual features, that takes both document-wise and pairwise features into account, may be most successful at modeling users X  click decisions.

To better understand how well our combined model captures cap-tion bias, we conduct a more detailed analysis of its performance on different segments of queries drawn from data set B. We analyze prediction performance by (1) query frequency, and (2) the type of the information need expressed by the query.

Table 5 shows the perplexity of the baseline and combined mod-els, split by query frequency. We divide queries into three groups: head , which consists of the 20% most frequent queries, body , which consists of queries between the 20th and 80th frequency percentile, and tail , which consists of the 20% least frequent queries.
We find that on head queries, perplexity is best for both models, and that the performance of the combined model is lower than that of the baseline. The reason is that these very frequent queries are typically  X  X asy X , because many users search for the same things, usually with a clearly identifiable goal. For this type of query, users
Per-document Deep links 1.041 (  X  0.021) 1.048 (  X  0.021) features Short URL 0.470 (  X  0.022) 0.403 (  X  0.022)
Pairwise  X  URL slashes above -0.184 (  X  0.020) features  X  URL slashes below 0.160 (  X  0.015) 0.109 (  X  0.015) click predictions are included (with p &lt; 0 . 001 ). Table 5: Perplexity of the baseline and combined models, split by query frequency segment. Best performance per segment is highlighted in bold.  X * X  indicates significant differences be-tween the models ( p = 0 . 01 ). are likely to recognize target result pages, e.g., by the URL. Thus caption bias is low for this type of query, as reflected in our results. For the less frequent body queries, perplexity is higher for both models. Here, the performance of the combined model is signifi-cantly better than that of the baseline model. The trend continues for the tail query segment.

Table 6 shows the results obtained when splitting queries by the type of information need. Here, we use two sources of information to categorize queries. First, we use information provided by the search engine used for data collection. For queries with one pre-dominantly clicked result, this top result is given more space on the result page. Our log data provides information on which queries were treated with such  X  X nhanced navigational X  results, and we use queries marked as such as our first category. For the remaining queries, a simple click-entropy based classifier divides queries into navigational and non-navigational.
 Table 6: Perplexity of the baseline and combined models, split by the type of information need. Best performance per type is highlighted in bold.  X * X  indicates significant differences be-tween the models ( p = 0 . 01 ).

We observe a similar pattern to that obtained using query fre-quency. For enhanced navigational queries, perplexity is lowest, and the baseline model performs better than the combined model. As the head queries above, results for these queries are the least likely to suffer from caption bias. These queries have one main target result, and this result is easy to identify on the result page, so caption bias is expected to play a small role here. For both navigational and non-navigational queries perplexity is higher for both models, but the combined model performs significantly bet-ter in both cases. Again, the performance improvement of the combined over the baseline model is largest for non-navigational queries, where caption bias is expected to be strongest.

To summarize, we validated our models of caption bias on the task of predicting clicks on interleaved result lists. Our combined model, that includes both document-wise and pairwise features to model caption bias, performed best overall. We also found that our are marked with  X * X  and corresponding result titles are highlighted in purple. model worked best on queries where caption bias is expected to affect clicks the most, namely infrequent, non-navigational queries.
In the previous subsections we presented our results for training and evaluating caption bias models, and found that our combined model best predicted observed click behavior. In this section we show how applying this model to re-weight clicks affects interleav-ing scores on individual results, and show how such a re-weighting can be used to predict user preferences.

To show how compensating for caption bias affects interleaved comparisons in detail, consider Table 7. For the query  X  X oday in two minutes X , four search results are shown. The first has a visual representation that results in a weight of 1 . 003 , which is close to the average (i.e., the result is about as likely to be clicked as a result for which all visual features are false / zero). Click probability in-creases e.g., due to the short title, and highlighting in the URL, but decreases due to the missing snippet and the lack of highlighting in the title. Overall, the result is relatively unlikely to be clicked based on attractiveness alone. In contrast, the lower-ranked results look more attractive, and consequently receive lower click weights. In this result list, the two results with the lowest weights (i.e., the most attractive visually) were clicked by the user. This suggests that click behavior may have been affected by caption bias.
We can now compare the outcomes that would be obtained in the above example when inferring a Team Draft outcome with and without applying our model of caption bias. We observe that both clicked results were contributed by ranker A, leading to a win of 2 clicks over B. When applying the caption bias model, the low click weights of the clicked results are taken into account, leading to a much smaller win of 0 . 617 . This example shows how the caption bias model decreases the impact of clicks that may have been biased towards more attractive results.

As an additional proof-of-concept, we applied our caption-bias weighting scheme to a small sample of search result impressions for which two different URLs had been clicked by different users. When a model predicted a lower weight for one of the clicked URLs, this URL would be inferred to be more likely to be clicked due to its presentation, and the URL with the higher click weight would be inferred to be preferred due to its content. For this small data set, we asked human annotators to judge which of two landing baseline highlighting document-wise pairwise combined 0 / 86 / 0 44 / 1 / 41 43 / 11 / 32 43 / 0 / 43 47 / 0 / 39 Table 8: Preference predictions by caption bias models. For each model we include the number of correct / no preference / incorrect predictions (the best result is marked in bold). pages (for the two competing URLs) they would prefer for a given query.

Table 8 shows how often our model predictions agree with the human preference judgments. For the baseline model, no prefer-ences can be inferred, as both URLs were clicked for the given query. Prediction quality of the pairwise model is the same as a ran-dom model would achieve, while accuracy for the document-wise and highlighting models are slightly higher. The best preference predictions are obtained by our combined model.
In this section we address our central research question: How are interleaving outcomes affected by differences in result presen-tation in practice? To better understand this effect, we apply our caption bias model to interleaved comparisons conducted on live Web search traffic, and analyze how results are affected.
We conducted six interleaving experiments, selecting experiments that represented small changes in ranking quality that are typical of incremental ranker improvements at major Web search engines. We also selected pairs such that the competing rankers used methods for applying previously collected clickthrough data, and different weights to make the influence of clicks weaker or stronger. Se-lecting ranker pairs in this way increased our chances of detecting changes in interleaving outcomes due to caption bias. Below, we refer to the interleaving experiments as E1 -E6 .

The interleaving experiments were run on a sample of live traf-fic from the same Web search engine as used in the previous two sections. These experiments were run within three weeks of col-lecting the data sets for model training and evaluation, so that no major changes in click behavior are expected. Interleaved result lists were generated using the Team Draft method. After observing user clicks, four different scoring methods were applied to com-pute interleaved comparison outcomes as defined in Equations 2 X 3 (without compensating for caption bias) and 5 X 6 (with the com-bined caption bias model). Note that the final scores are re-centered and re-normalized to conform to scoring schemes used in previous work, as explained in Section 3.3.

Table 9 gives an overview of the interleaved comparison out-comes obtained with Team Draft and Probabilistic Interleave, be-fore and after caption bias re-weighting. We can see that scores are generally close to 0 . 5 . These scores reflect the incremental ranker changes typically tested at major Web search engines (e.g., changes that affect a small percentage of queries). Scores above 0 . 5 mean that the experimental ranker was found to perform better than the baseline ranker. We also find that, despite the relatively small changes in ranking, most experiments detect a statistically significant difference between the rankers.

We first compare the outcomes obtained under the two baseline methods, i.e., without re-weighting. Outcomes for all experiments agree in direction and in whether the difference is statistically sig-nificant. However, the magnitude of outcomes differs, with e.g., a relatively big difference for E4, and a relatively small difference for E1. The reason is that under the Team Draft method, individual comparison outcomes are binary, independently of the magnitude of the difference between rankers. Under Probabilistic Interleave, outcomes are weighted by the rank distance of clicked documents. E.g., if a click was observed on a document placed at ranks 3 and 4 by the competing rankers, the magnitude of the interleaving out-come would be much smaller than if one ranker had placed the document at rank 1 and the other at rank 10. Thus, for experiments where there is a large absolute difference between Team Draft and Probabilistic Interleave outcomes, a large portion of the differences detected under Team Draft is expected to be caused by relatively small differences between rankings (e.g., E4).

After applying re-weighting, we find that the detected interleav-ing gains are generally smaller than under the original scoring meth-ods. This suggests that a portion of the observed clicks was on results with low weights (i.e., with high click probability). How-ever, in most cases, experiments for which significant differences between rankers were detected before re-weighting, are still signif-icantly different after re-weighting (E1, E2, E3, E5).

One example where comparison outcomes appear to be strongly affected by caption bias is ranker pair E4. For this pair, the original comparison using Team Draft results in a statistically significant gain for one of the rankers ( 0 . 5031 , a relatively big difference in typical ranker evaluations). After re-weighting, a much smaller (but still significant) improvement of 0 . 5005 is observed, indicating that most of the gain observed under Team Draft may be due to caption bias. Comparing to the outcome obtained under the probabilistic method, we find that another portion of observed improvements is due to only small ranking changes. After our model of caption bias is applied to Probabilistic Interleave, no difference between rankers can be detected. This finding suggests that the initially detected im-provement was due to small ranking changes and caption bias, and that there is no true improvement in ranker quality. After analyzing ranker pair E4 in more detail, we found that the competing ranker in this experiment relied particularly heavily on click data. Thus, this experiment is highly likely to be affected by caption bias.
For experiment E6, the comparison outcome changes from non-significant to significantly worse when caption bias re-weighting is applied. For this experiment, the original comparison would support the conclusion that the compared rankers are equivalent. However, the re-weighted outcome indicates that the experimental ranker was really significantly worse than the baseline ranker.
To summarize, our final evaluation applied our best-performing caption bias model to interleaving experiments conducted on search engine traffic. We found small effects of caption bias on all experi-ments. For an experiment that was particularly strongly affected by caption bias, an initially detected interleaving preference vanished after applying caption bias re-weighting to Probabilistic Interleave. Our results suggest that our approach to modeling and compensat-ing for caption bias can successfully identify interleaving experi-ments where caption bias may lead to unreliable outcomes.
In this paper we addressed the problem of caption bias in inter-leaving experiments. Interleaving experiments compare rankers by generating result lists that combine results from two rankers in a way that minimizes position bias, and infer differences in ranking quality from observed user clicks on result documents. In such an evaluation setup, bias introduced by differences in result presenta-tion may affect inferred comparison outcomes. In extreme cases, this could lead to inferring a preference for the wrong ranker.
We started our investigation of caption bias by introducing a set of models designed to model bias using features that capture visual characteristics of individual result documents, and pairwise fea-tures that capture relationships with neighboring documents. We evaluated these models by using them to predict clicks. We found that overall, per-document features were more successful in cap-turing click behavior than pairwise features. However, best results were achieved using a combined model, that uses both feature sets. We also found that a combined caption bias model was the most successful at predicting click behavior, especially when caption bias is expected to be strongest (such as non-navigational queries).
Finally, we applied our best model to six interleaving experi-ments conducted on a small portion of live search traffic of a major commercial web search engine. We found that compensating for caption bias led to small changes on experiment outcomes over-all, although in one case the outcome for a ranker tuned to click-through data was nullified. These results indicate that the proposed approach can successfully model caption bias and identify inter-leaving outcomes that may be unreliable due to caption bias.
This work constitutes a first step towards understanding how cap-tion bias affects interleaving experiments, and how such experi-ments can be made more reliable by compensating for it. One di-rection for future work is to consider more complex models of click behavior. Here, we preferred relatively simple logistic regression models because they are relatively resistant to over-fitting and be-cause of their interpretability. However, modeling click behavior is an active research topic in itself [10]. Although most click models to date model user behavior on individual queries, extensions that model factors across queries have been proposed [34]. Particularly, models that separate perceived relevance from judged relevance, and models that take additional user aspects of user behavior into account [33] are promising in this context.
 The authors would like to thank Sreekar Krishna, Nick Craswell, Tapas Kanungo, and Kuansan Wang for their valuable feedback and discussion throughout this project. This research was partially supported by the Netherlands Organisation for Scientific Research (NWO) under project nr 612.061.814.
 [1] E. Agichtein, E. Brill, and S. Dumais. Improving web search [2] J. Boyan, D. Freitag, and T. Joachims. A machine learning [3] B. Carterette and R. Jones. Evaluating search engines by [4] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue.
 [5] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] E. Cutrell and Z. Guan. What are you looking for?: an [8] Z. Dou, R. Song, X. Yuan, and J. R. Wen. Are click-through [9] S. Dumais and E. Cutrell. Optimizing search by showing [10] G. E. Dupret and B. Piwowarski. A user browsing model to [11] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [12] Z. Guan and E. Cutrell. An eye tracking study of the effect of [13] F. Guo, L. Li, and C. Faloutsos. Tailoring click models to [14] F. Guo, C. Liu, and Y. M. Wang. Efficient multiple-click [15] J. He, C. Zhai, and X. Li. Evaluation of methods for relative [16] K. Hofmann, B. Huurnink, M. Bron, and M. de Rijke.
 [17] K. Hofmann, S. Whiteson, and M. de Rijke. A Probabilistic [18] S. Ieong, N. Mishra, E. Sadikov, and L. Zhang. Domain bias [19] T. Joachims. Optimizing search engines using clickthrough [20] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, [21] J. Kamps, M. Koolen, and A. Trotman. Comparative analysis [22] D. Kelly and J. Teevan. Implicit feedback for inferring user [23] F. Radlinski and N. Craswell. Comparing the sensitivity of [24] F. Radlinski and T. Joachims. Minimally invasive [25] F. Radlinski, M. Kurup, and T. Joachims. How does [26] M. Sanderson. Test collection based evaluation of [27] F. Scholer, M. Shokouhi, B. Billerbeck, and A. Turpin. [28] X. Shen, B. Tan, and C. Zhai. Context-sensitive information [29] A. Tombros and M. Sanderson. Advantages of query biased [30] E. M. Voorhees and D. K. Harman. TREC: Experiment and [31] K. Wang, T. Walker, and Z. Zheng. PSkip: estimating [32] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias: [33] F. Zhong, D. Wang, G. Wang, W. Chen, Y. Zhang, Z. Chen, [34] Z. A. Zhu, W. Chen, T. Minka, C. Zhu, and Z. Chen. A novel
