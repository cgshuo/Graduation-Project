 1. Introduction
The classic measures of information retrieval performance are precision and recall. Precision is the number of retrieved and relevant documents divided by the number of retrieved documents. Recall is the number of retrieved and relevant documents divided by the number of relevant documents. Both ratios are used jointly because they capture two complementary aspects of the retrieval process. Precision tells us how good the sys-tem is at filtering out non-relevant documents. Recall tells us how good the system is at finding relevant documents.

Other measures have been proposed that aim to summarize information retrieval performance in a single number. These include the average precision at seen document, the R-precision, the E-measure, van Rijsber-gen X  X  F and the average precision over all documents. All of these numbers are directly based on the concepts of precision and recall. In fact, they are precise mathematical functions of precision and recall ratios.
Despite numerous critiques of these measures, they remain the most widely deployed in  X  X  X arge X  X  information retrieval problems. In such  X  X  X arge X  X  problems there is a large set of documents, typically so large that the one-by-one examination of each document is not realistic. Then, an information retrieval system has the task of retrieving a set of documents that corresponds to the information need. The user only sees the set of retrieved documents.

This paper is motivated by my concern for a special information retrieval problem. We can call this prob-lem a  X  X  X mall X  X  information retrieval problem. The service helps a user to find the relevant documents out of a small collection of documents. The collection is small enough that the user can examine each document one by one. The purpose of the information retrieval system is to make it easier for users to reach decisions.
One example of such a  X  X  X mall X  X  information retrieval system has been the motivation for this paper. I cre-ated the  X  X  X EP: New Economics Papers X  X  service at http://nep.repec.org and I am involved in running it. NEP is a current awareness service of the RePEc digital library, see http://repec.org . It filters new additions to
RePEc into weekly subject-specific reports. Each report is edited by a volunteer editor. Each week editors are given a list of new additions to RePEc. From that list, they select the documents that are relevant to via email.

The NEP service has been running since 1998. During that time RePEc has grown. So has the list of new additions that appear each week. The median number of new documents per week, over the entire life of the service, is 300. But in recent months, bumper crops of over 600 new documents are not uncommon. With that sort of numbers, we cannot require volunteer editors to ponder over each single one for much more than a few seconds. At present, most editors, when composing the report issue, look first at document titles. If a title looks appealing, they may look at the abstract. But sometimes a non-appealing title may hide a relevant doc-It inevitably leads to editorial mistakes.

To make life easier for the editors, it would be useful to sort the list of new additions in order to show edi-tors up-front those documents that are most likely to be included. This system can bring two benefits to edi-tors. First they do not have to labor through the whole of the new additions list. If the algorithm works well, they can skip the tail end. Second, they can increase attention to the documents that the computer has put to the top of the list. This has the potential to eliminate oversight of documents with imaginative titles.
To evaluate such a sorting system, we need some measures. Within the specific context, precision and recall do not appear to be useful. There are three basic ways in which one can interpret precision and recall within the NEP context.

One approach is to say that they remain constant. Recall is always 100%, and precision is always equal to we take that view, then precision and recall do not depend on the sorting process.

Another approach looks at precision and recall at the level of the last relevant document. Thus, if the infor-mation retrieval system sorts all the relevant documents to the front, then precision and recall are 100%. If there are some non-relevant documents that are found before the last document that is relevant, precision is the number of relevant documents divided by the position of the last relevant document. But recall is still measure that I discuss in Section 4.4 .

A third approach is to think of the output of the information retrieval system as sets of documents. We distinguish the documents that the information retrieval system has predicted as relevant versus those that it has predicted as non-relevant. We can then calculate precision and recall figures as intended by comparing the sets returned by the information retrieval system with the sets assembled by the editor. The latter are assumed to be correct. There are two problems with this approach. First, all computer-based information retrieval systems rank documents by the likelihood that they are relevant. It is in the very nature of the type of calculations done that such a ranking is produced. Therefore, if we conceive the information retrieval sys-tem output as a couple of sets, we exclude additional information that the system has produced. This is incon-sistent with an assumption of rational behavior of users. Second, it should matter a lot in what order the documents appear in. This second problem is best illustrated by an example. For the sake of illustration, let numbers denote documents that, according to the editor, are relevant, and letters denote documents that are not relevant. Assume that the information retrieval system finds that documents 1 and 2 are relevant.
It will sort those to the front. Then [1,2,3,4,a,b,c,d] and [1,2,a,b,c,d,3,4] have the same precision (100%) the relevant documents are at the front, the other half is at the rear. The editor has to still work through the entire list of new additions to find the all relevant documents.

We hope to have convinced the reader of the need for alternative measures. The remainder of the analysis therefore does not start with the analysis of precision and recall values, as Van Rijsbergen (1974) or Shaw (1986) have done. Instead we use an informal utility maximization approach, as, for example, Cooper (1973) . In addition, we will consider rationality arguments, an approach that is more often found in economics rather than in information science. Before we review criteria, we devote two sections to further thinking about the problem. In Section 2 we set out the general framework. In Section 3 we look more closely at the problem that editors face. We try to establish what order a rational editor may place on the outcomes. In Section 4 we present alternative measures. In Section 5 we test these measures on the NEP report data using support vector machines. In Section 6 we offer conclusions. 2. The problem
Let there be a vector x called the outcome vector or simply the outcome. It has n elements, r of which take the value 1, i.e. they represent relevant documents, and n r take the value 0, i.e. they represent non-relevant the set of all possible outcomes. For a fixed r and a fixed n , this is a set with n !/( r !( n r )!) members.
Loosely speaking, we are looking for a measure of how much the 1s are at the front of the vector and the 0s at the end. The best outcome is and the worst outcome is
Let f ( x ) denote a measure of the quality of the outcome. There are many measures that one may define. Each of these measures is subjective in a way. It captures a desired property of the outcome. We can find four requirements that hopefully most people can agree with.
 to explain to people what the measure is.

Requirement 1 is in some ways a corollary of requirement 0. People are used to reason that a better out-come means a higher f (  X  ). Thus Requirement 2 again is a corollary of requirement 0. People are used to reasoning in term of percentages.
Therefore it seems adequate to require
Apart from the best outcome another benchmark is important. That is the case where x is picked randomly out of O  X  r ; n  X  . In that case, the outcomes are of no use. This leads to Requirement 3 where E stands for the expected value operator. One technical constraint that comes out of this requirement is that there has to be a closed form for the expected value. Of course, the expected value of any measure f ( x ) puter technology is simply not powerful enough to accomplish the calculations in reasonable time.
Finally, we have an additional desired feature. We refer to this as the scaling property. With a constant assume n =3, r = 1 and x = [0,1,0]. If t = 2 we can construct x ment in xt times. It is obvious that we can always make such a scaling transformation and that this transfor-mation is unique. Let t x denote the scaled outcome vector. If we make such a transformation, it appears natural to wish that f ( x ) does not change, i.e.
If f (  X  ) satisfies to that property, we will say that it scales. 3. Subject editor behavior modeling
The proposed system helps editors of a current awareness service such as NEP. The ultimate judge of per-editor behavior. While a full mathematical model would be outside the scope of this paper, we hope to estab-process.

An editor faces a list of documents. A document is metadata about a paper plus a link to the full text of that document is difficult to model. Therefore we will not look at it here. In other words, we assume that the exam-ination of a document is a discreet process i.e., the document is examined or it is not examined. Requirement 1 is in some ways a corollary of requirement 0. People are used to reason that a better outcome mean a higher f (  X  ).

After examining a document, the editor knows whether that document is relevant or not. We further assume that the decision to include a document or to exclude it only depends on the contents of that docu-ment. It is independent from the contents of other documents. This reasoning assumes away any learning that may take place while the list of documents is examined.

As the editor works through the list of documents, she faces two types of costs. First there is the cost c examining the next document. Without loosing much generality we can assume that c the report issue composition process. Second, there is the cost of missing relevant documents. Let us loosely call this c 2 , though it is clear that c 2 somehow depends on actual number of documents missed. As the editor moves along the list, she faces an optimal stopping problem. If she stops to examine documents, she no longer suffers the penalty c 1 from examining all the following documents. But she faces the penalty c relevant papers. If c 1 c 2 , the editor will not examine any documents at all, and if c examine all documents. In less extreme cases, there is a balancing act. This balancing act is complicated because of the uncertainty surrounding c 2 .

To make further progress in our reasoning, we need to simplify the problem. Let us assume that a magical interface could be built that would remove the uncertainty regarding c the editor X  X  interface. It would show green while there is at least one more relevant document, and it would show red if there is no more relevant documents. Such a scenario is unrealistic of course, but for the moment just imagine it could be achieved. Clearly when the traffic light turns red, the editor will stop examining new documents. Now let us in addition assume that the editor is a conscientious person. By this we mean that while the traffic light is green, she will continue to examine new documents, until the traffic light is red.
However unrealistic this scenario of the traffic light scenario is, it can teach us one insight. With a traffic light, the editor will, when presented with two outcomes x and x x c = 0. However the examination cost c 1 will be lower the lower i * is. This reasoning establishes a weak order-ing over all outcomes. Comparing two outcomes, an editor will prefer the one with the last relevant document at an earlier position. The editor will be indifferent between two outcomes that have the last relevant docu-ments at the same position. This reasoning requires under certainty, and that the editor is conscientious.
We can hope that it will also hold under some uncertainty, provided that the uncertainty is not too large, and for less then full conscientious editors, provided they are not reckless.

Now imagine that the traffic signal cannot be completely trusted. It would get it most of the time right but not always. Let us assume, to simplify, that the uncertainty would only hold between the second-to-last and simply because the uncertainty is marginally small. Let i * be the position of the last relevant document. Com-it is followed by zeros only.

I claim that when the editor compares the two outcomes, she will prefer x x , but two relevant documents under x 1 . If a wrong signal is received at any earlier position than i * 1 the loss of the number of relevant documents is the same. Therefore, when presented with two outcomes that have the same position for the last relevant document, the editor will prefer the one with the second-to-last docu-ment at the earlier position. This reasoning can be repeated for the third-to-last document, etc. We obtain a complete order over the outcomes. Let us call it the natural order.

We can conjecture that it is possible to find a class of functional specifications for the loss function, and a class of distribution functions of documents in the included/excluded domain such that, when editors mini-mize total loss knowing the distribution function, they prefer outcomes in natural order. Developing such a generic class would, however, go beyond the scope of this paper. 4. Various measures
In this section we address different measures for the literature or purpose-built for this paper. 4.1. The Swets/Brookes measure
One interesting measure was proposed by Swets (1963) . He assumes that  X  X  X hen a search query is submitted where r 2 denotes the variance. According to Swets (1963) , as long as both conditional distributions of z are normal, the measure has desirable properties. But Brookes (1968) suggests that a better measure would be
This measure truly expresses the discriminating power of the underlying information system. The z data that it uses is much richer than the positional data used by other measures proposed in the following here. On the other hand, its precise values are dependent on the technical characteristics of the information retrieval sys-tem. Therefore, its best use is as a technical tool to compare different parameters within the same information retrieval methodology.

An important problem is that the measure violates requirement two. on the numerator. This causes a problem that we discuss in the next subsection. 4.2. The Aselt measure
Although Swets/Brookes measure is rarely used at present, linear measures have not disappeared. For example, Losee (1998) considers the  X  X  X verage search length X  X  as a candidate measure for information retrieval sum of the positions i where x i = 1, divided by n .If a ( x ) is the average search length, we have The expected value for the average search length 2 can be readily found as which, interestingly, does not depend on r . Having found the expected value, we can construct a measure that satisfies requirements 2 and 3. We define the Aselt X  X n acronym for  X  X  X verage search length transformed X  X  X  measure of an outcome x , a ( x ), by cause it is based on an average, which itself is a linear function.

Table 1 carries a numeric illustration of the Aselt measure. Looking at it we have two remarks. First, there seems to be a concentration of points towards the middle of the distribution. Second, not every different out-come has a different Aselt measure. In particular, the measure does not penalize two relevant documents in the middle of the vector any different than one relevant document in the end. From a managerial point of view, this is a problem if we are much concerned about having a measure that penalizes heavily a single document that is left out at the end.

In Table 1 outcomes are sorted by the natural order. The table illustrates that the Aselt measure violates the natural order. That is, there are some couples of outcomes x and x sures. It therefore also affects the Swets and Brookes measures. 4.3. Lofop measure
One idea to combat natural order reversion is to use the natural logarithm, in order to reduce the high num-increment a total quality indicator of the outcome by ln i . Let l ( x ) be this measure. Let x document is at position i , or 0 otherwise. We get which is well defined. A simple calculation shows that
This motivates the definition of the Lofop X  X n acronym for logarithm of found position X  X  X  X easure m ( x )as
Note that the l ( x ) could also be defined for the logarithm to another base. Normalization leaves the actual m ( x ) unchanged for any base. Table 2 suggests that the Lofop measure does have desirable properties. It spreads outcome values more evenly than the Aselt measure and, according to the table, it obeys the natural order. Unfortunately, the respect for the natural order of the Lofop measure in the r =2, n = 5 case is not a general rule. We do not have to look far before reversion on the natural order raises its ugly head again. We will leave it as an exercise for the reader to show that m (1,1,0,0,0,0,0,1) = 2.64% but that m (0,0,1,0,0,1,1,0) = 15.64%. This is a clear violation of the natural order. 4.4. The Nosel measure
Cooper (1968) looked at a more general model than we do here. He assumed that the information system would result in a weak ordering of documents. That is, some documents would be ranked exactly as relevant as some others. In that situation, he assumes that the user will look at these equally ranked documents in a random order. He calls the measure that he proposes the  X  X  X xpected search length X  X . It is the number of non-relevant documents that a user would find until she finds a target number of relevant documents. In our setting, his  X  X  X xpected search length X  X  reduces to the search length, because there is no uncertainty about how the order in which the documents are examined. A further simplification in our case is that we can con-sider that the target number of relevant documents is the true number of relevant documents r . Let k denote the search length, then The expected value of the search length 3 over all outcomes in the outcome set is gests that the distribution of values is highly skewed.

In order to comply with requirements 1 X 3, we define the Nosel X  X n acronym for  X  X  X ormalized search length X  X easure of the outcome l ( x )as
As seen in Table 3 , many different outcomes receive the same Nosel measure. The Nosel measure essentially expresses how low the last document found was. It is not interested in the position of any other document. This can be a strength of the measure, because it makes it easier to explain to people what has been measured.
On the other hand it can also appear as weakness of the measure. While the position of the last document should be X  X ccording to our reasoning in Section 3  X  X he most important aspect of editor satisfaction, it is doubtful that it should be the only one. In particular, an outcome that has all the relevant documents next to each other but away from the top should, be counted as worse than an outcome that has all the relevant scenarios. This idea is, of course, embodied in the natural order.

The Nosel measure does not scale. To see this, it is sufficient to look at a counter example. c (0,0,0,1,1) = .5, but c (0,0,0,0,0,0,1,1,1,1) = .25, which is not even close. 4.5. The Ponori measure
Instead of looking at various measures and examining them if they fit the natural order, a more fruitful approach may be to build measures that directly impose the natural order by construction.

One idea is that we can consider the sequence of 1s and 0s in the outcome vector as a binary number. This binary number can be converted to decimal in order to capture its position in the natural order. Converting x to a decimal form leads to the highest number, and converting x ber. However, consider versus
The difference in the decimal measure between x 1 and x 2 if one does wish to penalize late occurring relevant documents significantly. Therefore, rather than measuring the quality of the result by assigning high powers to the first outcomes, we invert the outcome vector. Thus we give high powers to the lower outcomes and call the resulting number a loss. Of course, we are not limited to considering powers of 2 as the binary-number interpretation suggests. Any power of y &gt; 1 will be able to accomplish the purpose of implementing the natural order. This motivates the following definition.
Let x  X  X  x 1 ; ... ; x n 2 O  X  r ; n  X  . Then the y -Ponori X  X n acronym for  X  X  X olynomial natural order imposi-tion X  X  X  X enalty of x , x ( x , y )is
We find The expected value is
We can substitute for (4) and (5) to obtain a measure that satisfies (1) and (2) . This motivates the following definition. The Ponori measure of an outcome x at the power y is
As y !1 the Ponori measure becomes as indicator if the last relevant document is at the last position. As 4.6. The Copnori measure
The dependency of the Ponori measure on y is inconvenient. It is not clear what y to choose. While the ordering of outcomes is not sensitive to the choice of any y &gt; 1, the numbers coming out of the evaluation definitely are. Thus a more fundamental measure is called for.

One very simple way to achieve this is to count through the elements in the outcome set in the natural order, assigning each worse outcome an incremental penalty of 1. Computing the sequence is reasonably straightfor-ward. 4 If we start with counting at 0, we get the counts j ( x )as The expected value is readily found as
Copnori X  X n acronym for  X  X  X onstant penalty natural order imposition X  X  X  X easure k ( x ) of an outcome x 2 O  X  r ; n  X  is
Unfortunately, the Copnori measure does not scale. This is seen with an example
Both numbers are not even close to each other. But, the measure has three strong points. First, if one under-stands the natural order, it is a very intuitive measure. Second, it does not depend on an arbitrary parameter. Third, given the algorithm that we developed, the Copnori measure is easy to compute even for large n and r .
Table 5 illustrates the Copnori measure. 5. Test
Our aim is to develop a sorted list of new additions to RePEc for editors of NEP. In NEP each subject issue additions to RePEc. Thus sorting the list of new addition is like sorting the nep-all report issue. Each time a subject report. The second is the set of documents that have not been included in the report. The membership of the latter set is somewhat more difficult to determine than the former. Sometimes, an editor may not have
Therefore we restrict membership of the second set to all those documents in the nep-all issues for which at least one document of the nep-all issue has been included from. Thus documents in nep-all issues in which no document appeared in the subject issue have been ignored. While this may be an oversight of negative learning examples, there are still plenty of negative example left, because the generality of subject reports is small. We treat the occurrence of documents in different reports as independent events. Data in Barrueco Cruz, it would be cumbersome to rerank a nep-all report for a certain subject when it becomes known that the editor of another subject report has included that document in her report issue, based on that new information, because editors make their decisions independently from each other but typically within a short time frame after the nep-all report has been issued. Thus, by ignoring co-occurrence of documents altogether, we are working under realistic operating conditions.

We keep feature extraction very simple. From each document, we use the author names, title, abstract, clas-sification codes, and the serial in which the paper has appeared. We concatenate the resulting string. We remove all punctuation, transliterate to lowercase and collapse whitespace. Each whitespace-separated com-ponent of the resulting string is a feature. For each document, we count the occurrence of the feature f as t . The weight of the feature f in the document, w f is then given as
Note that it is not necessary to take document frequency into account here, because to form the ranking, we use support vector machines (SVM). This technique goes back to Vapnik (1995) . It is now a widely used text classification technique. Ginsparg, Houle, Joachims, and Sul (2004) provide one example in a similar context to ours. The svm_light software of Joachims (1999) runs all the calculations. According to Krichel and Bak-kalbasi (2005) the median nep-all report has 300 documents. Therefore we set aside 300 randomly selected documents for testing. The rest we use for training the SVM. We conduct at least 10 runs for each report. For some reports, where the generality is low, some selected testing dataset contains no relevant document. In that case, we repeat runs until at least one among the 300 randomly selected testing documents is relevant.
The results are so bulky that we have confined them to an Appendix A . The Aselt measure gives a reasonable range of results, and shows, by its numerical values, that the performance of the SVM is really quite good. But we have rejected the measure on theoretical grounds. The same holds for the Lofop measure. We still include them in Table A.1 for the sake of completeness.

While the Ponori measure has desirable theoretical properties, there is a bad problem in tests where the set sure that is close to 1, or even equal to 1 after rounding. As we increase the value of y , we are converging toward a situation where the outcome is 100.00% as soon as the last document is not relevant, and a negative number if it is. This clearly is not what we want.

A similar problem affects the Copnori measure. Recall that the Copnori measure gives each outcome its own entire number. Outcomes where a relevant document appears late are penalized very heavily. Therefore, does so even more than the Ponori measure. The alert reader will note that there are a number of maxima in the table are 100% for the Copnori measure but less than 100% for the others. In the theoretical framework that we use, 100% is the value reserved for the optimal outcome x outcome to be evaluated 100% by one measure and less than 100% by another measure. The explanation for this apparent error is the table is rounding. The computer says its 100% when in fact is cannot see any more the real value that is a tiny bit below 100%.

The Nosel measure somehow has the opposite problem. It only looks at the last position of the last doc-ument, it takes no account of the ability of the information retrieval system to put relevant documents to the front. However as we noted in the introduction, the feature is also important, because it allows the editor to spend extra efforts on the documents, reading more than the title.

Thus we can say that in principle, the Nosel measure  X  X  X nderestimates X  X  the success of the system, whereas the Copnori measure  X  X  X verestimates X  X  the success of the system. However, this general statement only holds when the system is a success. When the result is lousy, Copnori exaggerates the bad performance. This is simple the reverse of the fact that the Copnori measure gives very good results for a large span of top-measures. Since both Copnori and Nosel have an expected value of zero at the random results, the Copnori compensates with more lower values at the tail end. Related to this, the variance of the Copnori measure is higher than the variance of other measures. Generally, when the results are quite good, the Nosel measure has the higher variance. This comes as no surprise since it only looks at one single element of the outcome vector, the one that comes lowest. 6. Conclusions
We think of precision and recall as set-based measures. Indeed, they are based on the idea that the total set of documents contains two complementary subsets, the subset of relevant documents and the subset of non-relevant documents. A query creates two other complementary subsets, the subset of retrieved and the subset of non-retrieved documents.

In this paper, we discuss a different class of information retrieval performance measures we call vector-based measures. Vector-based measures start with a different way of thinking about what is happening at query time. We think of the set of documents as a vector. Indeed, from a computational point of view, it is a vector, because all documents are in some order in the information retrieval system. The task of the infor-mation retrieval system is to sort all the documents that are relevant to the beginning of the vector, and sort the non-relevant documents to the end of the vector.

From our setup we have a theoretical ordering of outcomes we call the natural order. Therefore to evaluate the information system, we prefer measures that respect the natural order. It turns out that the Nosel and Cop-nori measure complement each other to provide a reasonable approximation of what the editors should want.
The Nosel measure only weakly enforces the natural order. A very large number of outcomes receive an identical Nosel measure despite the fact that the arrive at different positions in the natural order. From the point of view of the editors, the Nosel measure only penalizes an outcome when the editor has to look at an additional document. It does not take into account, that, for a given value of the position of the last rel-evant document, the editor may abandon the search for relevant documents before the last relevant document is reached, and may, as a consequence of this action, have a varying number of documents lost in different outcomes that receive the same Nosel penalty.

Such differentiation is provided by the Copnori measure. It strictly enforces the natural order. Each out-comes is assigned a different number and the next worse outcome has a constant additional penalty of one.
Moving the last position one further imposes no special additional penalty. But from the point of view of the editors, examining a new document does carry something additional to just one step down in the natural order. From their point of view it has to be a special penalty. Such a extra penalty is provided by the Nosel measure. Therefore it appears best to take a linear combination of the two measures, such as say penalty at each extra document that has to examined in order to find all the relevant documents. We suggest m = 10%, but other values are just as acceptable. All they change is the numeric value of the measure. They have no impact on the actual ordering of outcomes.
 Acknowledgement
I am grateful for comments by Christian Calme ` s, William S. Cooper, Robert M. Losee, Amanda Z. Xu and two referees of  X  X  X nformation Processing and Management X  X . I am also grateful to all the volunteers of NEP for the work they have been putting into running the service. The hospitality of Tatyana I. Yakovleva provided a congenial setting for the work on this paper.
 Appendix A. Test results In Table A.1 , we report, for a selection of NEP reports, the summary statistics for each measure. a is the Aselt measure, m the Lofop measure, l the Nosel measure, k the Copnori measure, and o the Ponori measure.
For each report and each measure, we see the mean in the line  X  X  X ean X  X , the minimum in the line  X  X  X in X  X , the maximum in the line  X  X  X ax X  X , and the standard deviation in the line  X  X  X ev X  X .
 References
