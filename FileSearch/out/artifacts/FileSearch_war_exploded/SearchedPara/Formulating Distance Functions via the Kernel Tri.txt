 Tasks of data mining and information retrieval depend on a go od distance function for measuring similarity between data in stances. The most effective distance function must be formulated in a context-dependent (also application-, data-, and user-dependent) way. In this paper, we propose to learn a distance function by captur ing the nonlinear relationships among contextual information pro vided by the application, data, or user. We show that through a proces s called the  X  X ernel trick, X  such nonlinear relationships can be lea rned ef-ficiently in a projected space. Theoretically, we substanti ate that our method is both sound and optimal. Empirically, using sev eral datasets and applications, we demonstrate that our method i s effec-tive and useful.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Distance function, kernel trick
At the heart of data-mining and information-retrieval task s is a distance function that measures similarity between data instances. To date, most applications employ a variant of the Euclidean dis-tance for measuring similarity. However, to measure similarity meaningfully, an effective distance function ought to cons ider the idiosyncrasies of the application, data, and user (hereaft er we refer to these factors as contextual information). The quality of the dis-tance function significantly affects the success in organiz ing data or finding meaningful results [1, 2, 5, 9, 11].

How do we consider contextual information in formulating a good distance function? One extension of the popular Euclid ean Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. distance (or more generally, the L p -norm) is to weight the data at-9, 18]. For example, for answering an ocean image-query, color features should be weighted higher. For answering an architec-ture image-query, shape and texture features may be more impor-tant. Weighting these features is equivalent to performing a linear transformation in the space formed by the features. Althoug h lin-ear models enjoy the twin advantages of simplicity of descri ption and efficiency of computation, this same simplicity is insuf ficient to model similarity for many real-world datasets. For example , it has been widely acknowledged in the image/video retrieval doma in that a query concept is typically a nonlinear combination of perc eptual features (color, texture, and shape) [16]. In this paper we p ropose performing a nonlinear transformation on the feature space to gain greater flexibility for mapping features to semantics.

We name our method distance-function alignment ( DA lign short). The inputs to DA lign are a prior distance function, and con-textual information . Contextual information can be conveyed in the stance, in the information-retrieval domain, Web users can convey information via relevance feedback showing which document s are relevant to their queries. In the biomedical domain, physic ians can indicate which pairs of proteins may have similar functions . transforms the prior function to capture the nonlinear rela tionships among the contextual information . The similarity scores of unseen data-pairs can then be measured by the transformed function to bet-ter reflect the idiosyncrasies of the application, data, and user.
At first it might seem that capturing nonlinear relationship s among contextual information can suffer from high computational com-plexity. DA lign avoids this concern by employing the kernel trick [3]. The kernel trick lets us generalize distance-based alg orithms to operate in the projected space (defined next), usually nonlinearly re-lated to the input space . The input space (denoted as I ) is the origi-nal space in which data vectors are located (e.g., in Figure 1 (a)), and the projected space (denoted as P ) is that space to which the data vectors are projected, linearly or nonlinearly, (e.g., in F igure 1(b)). The advantage of using the kernel trick is that, instead of explic-itly determining the coordinates of the data vectors in the p rojected space, the distance computation in P can be efficiently performed in
I through a kernel function. Specifically, given two vectors x and x j from I to P . The inner product between two vectors can be thought of as a measure of their similarity. Therefore, returns the similarity of x i and x j in P . The distance between and x j in terms of the kernel is defined as Since a kernel function can be either linear or nonlinear, th e tra-ditional feature-weighting approach (e.g., [2, 18]) is jus t a special case of DA lign .
 We use an example in Figure 1 to illustrate the effectiveness of DA lign . The figure shows how DA lign learns a nonlinear distance function using projection. Figure 1(a) shows two clusters o f data, one in circles and the other in crosses. These two clusters of data obviously are not linearly separable in the two-dimensiona l input space I . After we have used the kernel trick to implicitly project the data onto a three-dimensional projected space P (shown in Fig-ure 1(b)), the two clusters can be separated by a linear hyper plane in the projected space. What DA lign accomplishes is to learn the distance function in the projected space based on contextua l infor-mation. From the perspective of the input space, I distance function captures the nonlinear relationships am ong the training data.

In summary, we address in this paper a core problem of data min -ing and information retrieval: formulating a context-base d distance function to improve the accuracy of similarity measurement . In Section 2, we propose DA lign , an efficient method for adapting a similarity measure to contextual information, and also pro vide the proof of optimality of the DA lign algorithm. We empirically demon-strate the effectiveness of DA lign on clustering and classification in Section 3. Finally, we offer our concluding remarks and sugg es-tions for future work in Section 4. Given the prior kernel function K and contextual information, DA lign transforms K . Kernel function K ( x i , x j ) can be considered as a similarity measure between instances x i and x j . We assume 0  X  K ( x i , x j )  X  1 . A value of 1 indicates that the instances are identical while a value of 0 means that they are completely dissimi-lar. Commonly used kernels like the Gaussian and the Laplaci an are normalized to produce a similarity measure between 0 and 1 polynomial kernel, though not necessarily normalized, can easily be normalized by using
The contextual information is represented by sets S and D S denotes the set of similar pairs of instances, and dissimilar pairs of instances. Sets S and D can be constructed ei-ther directly or indirectly. Directly, users can return the information about whether two instances x i and x j are similar or dissimilar. In such cases, the similar set S can be written as { ( x i , x know only the class-label of instance x i as y i . In this case, we can consider x i and x j to be a similar pair if y i = y j , and a dissimilar pair otherwise.

In the remainder of this section, we first propose a transform a-tion model to formulate the contextual information in terms of the prior kernel K (Section 2.1). Next, we discuss methods to gener-alize the model to compute the distance between unseen insta nces (Section 2.2).
The goal of our transformation is to increase the kernel valu e for the similar pairs, but decrease the kernel value for the diss imilar pairs. DA lign performs transformation in P , to modify the kernel from K to  X  K . Let  X  1 and  X  2 denote the slopes of transformation curves for dissimilar and similar pairs, respectively. For a given and D , the kernel matrix K , corresponding to the kernel modified as follows where 0  X   X  1 ,  X  2  X  1 and  X  k ij is the ij th component of the new kernel matrix  X  K .

In what follows, we prove two important theorems. Theorem 1 demonstrates that under some constraints on  X  1 and  X  2 , our pro-posed similarity transformation model in Eqn. 3 ensures a va lid kernel. Theorem 2 mathematically demonstrates that under t he con-straints from Theorem 1, the transformed  X  K in Eqn. 3 guarantees a better alignment to the ideal K  X  in [8] than the K to the
T HE ORE M 1. Under the assumption that 0  X   X  1  X   X  2  X  1 kernel K is positive (semi-) definite. (The assumption  X  means that we place more emphasis on the decreasing similari ty value ( K ) for dissimilar instance-pairs.)
P ROOF . The transformed kernel  X  K can be written as follows: which corresponds to the kernel matrix  X  K , associated with the train-ing set X , in Eqn. 3. If the prior K is positive (semi-) definite, us-ing the fact that the ideal kernel K  X  is positive (semi-) definite [7], we can derive that  X  K in Eqn. 4 is also positive (semi-) definite if 0  X   X  1  X   X  2  X  1 . Here, we use the closure properties of kernels, namely that the product and summation of valid kernels also g ive a valid kernel [15].

T HE ORE M 2. The kernel matrix  X  K of the transformed kernel  X  K obtains a better alignment than the prior kernel matrix K ideal kernel matrix K  X  , if 0  X   X  1  X   X  2  X  1 . Moreover, a smaller  X  or  X  2 would induce a higher alignment score.

P ROOF . In [14], it has been proven that a kernel with the follow-ing form has a higher alignment score with the ideal kernel the original kernel K , where we use K to distinguish with our  X  K defined in Eqn. 3. Ac-cording to the definition of kernel target alignment [7], we h ave where the common item  X  &lt; K  X  , K  X  &gt; is omitted at both sides of inequality, and the Frobenius norm of two matrices, say M = [ m and N = [ n ij ] , is defined as &lt; M , N &gt; = P Cristianini et al. [7] proposed the notion of  X  X deal kernel X  ( Suppose y ( x i )  X  { 1 ,  X  1 } is the class label of x i which is the target kernel that a given kernel is supposed to a lign with. Employing Eqn. 5 and Eqn. 7, we expand (6) as follows we then rewrite the right side in (8) as follows where we apply the assumption of  X  1  X   X  2 from (9) to (10), and employ Eqn. 3 in the last step (11). Combining (6) and (11), we obtain
Therefore, the transformed kernel  X  K in Eqn. 3 can achieve a bet-ter alignment than the original kernel K under the assumption of 0  X   X  1  X   X  2  X  1 . Moreover, a greater  X  in Eqn. 5 will have a higher alignment score [14]. Recall that  X  2 = 1 smaller  X  2 will have a higher alignment. On the other hand, from (10) and (11), we can see that the alignment score of  X  creasing function w.r.t.  X  1 . Therefore, a smaller  X  1 or in a higher alignment score.

For a prior kernel K , the inner product of two instances is defined as  X  ( x i ) T  X  ( x j ) in P . For simplicity, we denote as  X  i . The distance between x i and x j in P can thus be computed as transformed kernel  X  K in Eqn. 3, the corresponding distance  X   X  k ii +  X  k jj  X  2  X  k ij in P can be written in terms of d  X  d
Since K has been normalized as in Eqn. 2, we have the distance d ij = 2  X  2 k ij
Using the property 0  X  d ij  X  2 and the condition 0  X   X  1  X  and  X  d 2 the transformed distance metric in Eqn. 13 decreases the int ra-class pairwise distance in P , and increases the inter-class pairwise dis-tance in P . The developed distance metric (Eqn. 13) is a valid distance metric (non-negativity, symmetry, and triangle i nequality) since the transformed kernel in Eqn. 3 is a valid kernel, acco rding to Theorem 1.
In this subsection, we show how to generalize the model in Eqn . 13 to unseen instances without overfitting the contextual info rmation. We use the feature-weighting method [12] by modifying the in ner product k ij =  X  T weighting matrix and m 0 is the dimension of P . Based on the idea of feature reduction [12], we aim to achieve a small rank of A , which means that the dimensionality of feature vector  X  small in projected space P . The corresponding distance function thus becomes  X  d 2 distance in Eqn. 13, we formulate the problem as a convex op-timization problem whose objective function is to minimize the rank of the weighting matrix A . However, it induces an NP-Hard problem by directly minimizing rank( A ) , the so-called zero-norm problem in [4]. Since minimizing the trace of a matrix tends t o give a low-rank solution when the matrix is symmetric [10], w e approximate the rank of a matrix by its trace. Moreover, sinc e rank( AA T ) = rank( AA T AA T )  X  trace( AA T AA T ) = k AA we approximate the problem of minimizing the rank of A by min-imizing its Frobenius norm k AA T k 2 problem is formulated as s . t .  X  2 d 2 ij =  X  d 2 ij , ( x i , x j )  X  S where C S and C D are two non-negative hyper-parameters. Theo-rem 2 shows that a large  X  1 or  X  2 will induce a lower alignment score. However, on the contrary,  X  1 =  X  2 = 0 would overfit the training dataset. We hence add two penalty terms C D  X  1 and to control the alignment degree. This strategy is similar to that used in Support Vector Machines [17], which limits the length of w eight vector k w k 2 in projected space P to combat the overfitting prob-lem.

The constrained optimization problem above can be solved by considering the corresponding Lagrangian formulation L ( AA T ,  X  1 ,  X  2 ,  X  ,  X ,  X ,  X  ) (16) = 1  X  X  X  X  X   X  X  1  X   X  (  X  2  X   X  1 )  X   X  (1  X   X  2 ) , where the Lagrangian multipliers (dual variables) are  X  i 0 . This function has to be minimized w.r.t. the primal variabl es AA T ,  X  1 ,  X  2 , and maximized w.r.t. the dual variables To eliminate the primal variables, we set the corresponding partial derivatives to be zero, obtaining the following conditions :
Substituting the conditions of (18) and (19) into (16), we ob tain the following dual formulation which has to be maximized w.r.t.  X  ij  X  X  and  X   X  0 . Actually, which means that W (  X  ,  X  ) is a decreasing function w.r.t W (  X  ,  X  ) is maximal at  X  = 0 .

Now, the dual formulation (20) becomes a convex quadratic fu nc-tion w.r.t. only  X  ij . Next, we examine the constraints of dual for-mulation on  X  ij . According to the KKT theorem [13], we have the following conditions
The constraint (15) requires  X  2  X   X  1 . In the case of  X  0 , the training dataset would be overfitted. In addition, in th e case of  X  1 =  X  2 = 1 ,  X  d 2 ij is exactly equal to the original distance metric d , and we do not get any improvement. To avoid these cases, we then change (15) to be a strict inequality constraint of  X  Therefore, we have  X  = 0 from (23). Using the properties of and  X  = 0 , we can then change the dual formulation (20) and its constraints of (18) and (19) by substituting the expansion f orm of s . t . where  X  T lation (26) is very similar to that of C -SVMs [17]. It is a standard convex quadratic programming, which can result in a global o pti-mal solution without any local minima.

After solving the convex quadratic optimization problem in (26), we can then generalize our distance-metric model defined in E qn. 17 to the unseen test instances. Suppose x and x 0 are two test instances with unknown class labels. Their pairwise distance  X  d 2 feature weighting is computed as Substituting (17) into the above equation, we obtain Remark. We note that here our learned distance function  X  is expressed in terms of the prior kernel K which has been cho-sen before we apply the algorithm. For example, such a prior k er-nel could be chosen as a Gaussian RBF function exp  X   X  k x  X  x 0 k K xx i and K xx j in Eqn. 27 can thus be computed as exp even in the case where both x and x 0 are unseen test instances, their pairwise distance can still be calculated from Eqn. 27. More over, when a linear kernel, K ( x i , x j ) = &lt; x i , x j &gt; projected space P is exactly equivalent to the original input space DA lign then becomes a distance-function-learning in I . Therefore, DA lign is a general algorithm which can learn a distance function in both P and I .
We conducted an extensive empirical study to examine the eff ec-tiveness of our context-based distance-function learning algorithm in two aspects. 1 . Contextual information . We compared the quality of our learned distance function when given quantitatively and qualitati vely dif-ferent contextual information for learning. 2 . Learning effectiveness . We compared our DA lign with the regular Euclidean metric, Kwok et al. [14], and Xing et al. X  X  [20] on classification and clustering applications .
To conduct our experiments, we used five datasets: one toy dat aset, and four UCI benchmark datasets.

One toy dataset The toy dataset was first introduced in [14]. We used it to compare the effectiveness of our method to other me th-ods. The toy dataset has two classes and eleven features. The first feature of the first class was generated by using the Gaus -sian distribution N (3 , 1) , and the first feature of the second class by N (  X  3 , 1) ; the other ten features were generated by
Each feature was generated independently. The first row of Ta -ble 1 provides the detailed composition of the toy dataset.
Four UCI benchmark datasets The four UCI datasets we exper-imented with are soybean , wine , glass , and segmentation (abbre-viated as seg ). The first three UCI datasets are multi-dimensional.
The last dataset, seg , is processed as a binary-class dataset by choosing its first class as the target class, and all the other classes as the non-target class. Table 1 presents the detailed descr iption of these four UCI datasets.

The contextual-information sets S (the similar set) and D dissimilar set) were constructed by defining two instances a s simi-lar if they had the same class label 1 , and dissimilar otherwise. We compared four distance functions in the experiments: our di stance-function-alignment algorithm ( DA lign ), the Euclidean distance func-tion ( Euclidean ), the method developed by Kwok et al. [14] ( and the method developed by Xing et al. [20]. The latter two me th-ods are presented in [19]. We chose the methods of Kwok et al. and Xing et al. for two reasons. First, both are based on conte x-tual information to learn a distance function, as is used in Second, the method of Xing et al. is a typical distance-funct ion-learning algorithm in input space I that been seen as a yardstick method for learning distance functions. The method of Kwok e t al. is a new distance-function-learning algorithm in projecte d space developed recently [14]. We compared DA lign to both methods to test its effectiveness on learning a distance function.

Our evaluation procedure was as follows: First, we chose a pr ior kernel and derived a distance function via the kernel trick. We then new distance function. Finally, we ran k -NN and k -means using the prior and the learned distance functions, and compared thei r results. Three prior kernels we used in the experiments are linear ( For each dataset, we carefully tuned kernel parameters incl uding  X  ,
C S and C D via cross-validation. All measurements were aver-aged over 20 runs.
We chose two datasets, toy and glass , to examine the perfor-mance of our learned distance function when given a differen t qual-ity or quantity of contextual information.
We examined two different schemes for choosing contextual i n-formation for the k -NN classifier. The first scheme, denoted as ran-dom , randomly samples a subset of contextual information from tained from the class-label information. How to construct has been explained in the beginning of Section 2. very high or when nonlinear kernels, such as Gaussian and Lap la-cian, are employed. This is because its computational time d oes not scale well with the high dimensionality of input space an d non-linear kernels [14]. We thus did not report the correspondin g results in these cases. and D sets. The second scheme chooses the most uncertain bound-ary instances as contextual information. Those instances a re the hardest to classify as similar or dissimilar. Without any pr ior knowl-edge or any help provided by the user, we can consider those bo und-ary instances to be the most informative. One way to achieve s uch uncertain instances is to run Support Vector Machines (SVMs ) to identify the instances along the class boundary (support ve ctors), and samples these boundary instances to construct contextu al infor-mation. Some other strategies can also be employed to help se lect the boundary instances. In this paper, we denote such a schem e as SV . We chose SVMs because it is easy to identify the boundary instances (support vectors).

We tested both contextual information selection schemes us ing our distance-learning algorithm on three prior kernels X  X i near, Gaus-sian, and Laplacian. For each scheme, we sampled 5% contextual information from S and D . Figures 2(a) and (b) present the classifi-cation errors ( y -axis) of k -NN using the both schemes ( ure 2(a) shows the result on the toy dataset, and Figure 2(b) shows on the glass dataset. We can see that scheme SV yielded lower error rates than scheme random on both datasets and on all three prior kernels. This shows that choosing the informative contextu al infor-mation is very useful for learning a good distance function. In the remaining experiments, we employed the SV scheme.
We tested the performance of our learned distance function u sing different amounts of contextual information. Figures 3 (a) and (b) show the classification error rates ( y -axis) with different amounts of contextual information ( x -axis) available for learning. We ran the k -NN algorithm using the distance metric learned from our alg o-rithm based on different prior kernels. We see that for both toy and glass datasets, more contextual information is always helpful. H ow-ever, the improvement on the glass dataset did level off after more than about 5  X  10% contextual information was used. Therefore, in the rest of our experiments, we used 5% contextual information.
We used the k -means and k -NN algorithms to examine the effec-tiveness of our distance-learning algorithm on clustering and classi-fication problems. In the following, we first report k -means results, then k -NN results. For clustering experiment, we used the toy dataset and the fo ur UCI datasets. The size of the contextual information was cho sen as roughly 5% of the total number of all possible pairs. DA lign the contextual information to modify three prior distance f unctions: linear, Gaussian, and Laplacain. The value of k for k -means was set to be the number of classes for each dataset. To measure the qu ality of clustering, we used the clustering error rate defined in [2 0] as follows: where { c i } n denotes the cluster predicted by a clustering algorithm, number of instances in the dataset, and 1 { X } the indicator function ( 1 { true } = 1 , and 1 { false } = 0 ).

Table 2, we report the k -means clustering results for the five datasets. From Table 2, we can see that DA lign achieves the best combinations of the soybean and Gaussian, the wine and Linear, and the glass and Gaussian. DA lign performs better than Xing cases where the linear kernel is used. Table 2: Clustering Error Rates on Five Datasets. No results reported for Xing on Gaussian and Laplacian kernels since this algorithm can only work in input space I .
For classification experiment, we used the toy dataset and th e four UCI datasets. The size of the contextual information ch osen was again about 5% of the total number of all possible pairs.
When performing classification, we employed different dist ance functions: linear, Gaussian, and Laplacian. We compared th e per-formance of using these distance functions before and after apply-Table 3: Classification Error Rates on Five Datasets. No resu lts reported for Xing on Gaussian and Laplacian kernels since this algorithm can only work in input space I . ing DA lign , and competing methods. For k -NN, we randomly ex-tracted 80% of the dataset as training data, and used the remaining data 20% as testing data. (Notice that the 80% training data here is for training k -NN, not for modifying distance function.) Such a training/testing ratio was empirically chosen via cross-v alidation so that the classifier using the regular Euclidean metric perfo rmed best for a fair comparison. We set k in the k -NN algorithm to be setting is empirically validated as the optimal one for the c lassifier using the regular Euclidean metric.

We first report the k -NN classification results using five datasets X  error rates (in percentages). We compared our DA lign with three other metrics: Euclidean , Kwok , and Xing . For each dataset, we used three different prior kernels X  X inear, Gaussian, and L aplacian X  and then experimented with the four metric candidates. In th e third column of the table we report the error rates using the Euclidean distance. The last three columns report the results of using Kwok , and Xing , respectively. The best results achieved are shown in bold. First, compared to Euclidean, DA lign performs better on almost all datasets, with only one exception being tying on seg with Laplacian (both achieved zero classification error rate). S econd, DA lign outperforms Kwok on all datasets, improving the classifi-cation error rates by an average of 0 . 40% , 0 . 45% , 0 . 12% and 1 . 06% on the five datasets, respectively. Finally, DA lign better results than Xing X  X  in all testing scenarios. From the experimental results, we make the three observatio ns.
Quality of contextual information counts . Choosing contextual information with good quality can be very useful for learnin g a good distance metric, as shown in Figure 2. The best con-textual information is that which can provide maximal infor ma-tion to depict the context. As illustrated in our experiment , the boundary instances tend to be most useful, since when they ar e disambiguated through additional contextual information , we can achieve the best context-based alignment on the boundary (o n the function).

Quantity matters little . Choosing more contextual information can be useful for learning a good distance metric. However, a s shown in Figure 3, the cost of increasing quantity can outwei gh the benefit, once the quality of information starts to deteri orate.
DA lign helps . DA lign improves upon prior functions in almost all of our test scenarios. Occasionally, DA lign performs slightly worse than the prior function. We conjecture that this may be caused by overfitting in certain circumstances: specificall y, when some chosen prior kernels may not be the best model for the datasets, further aligning these kernels could be counter-productive.
We will further investigate related issues in our future wor k.
In this paper, we have reported our study of an important data base issue X  X ormulating a context-based distance function for m easur-ing similarity. Our proposed DA lign method learns a distance func-tion by considering the nonlinear relationships among the c ontex-tual information, without incurring high computational co st. Theo-retically, we substantiated that our method achieves optim al align-ment toward the ideal kernel. Empirically, we demonstrated that DA lign improves similarity measures and hence leads to improved performance in clustering and classification applications . We plan to apply DA lign on image classification and image-query concept learning [6] to test out its effectiveness on real-world dat asets. [1] C. Aggarwal, A. Hinneburg, and D. Keim. On the surprising [2] C. C. Aggarwal. Towards systematic design of distance [3] M. A. Aizerman, E. M. Braverman, and L. I. Rozonoer. [4] E. Amaldi and V. Kann. On the approximability of [5] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. Wh en [6] E. Y. Chang, S. Tong, K. Goh, and C. Chang. Support vector [7] N. Cristianini, J. Kandola, A. Elisseeff, and J. Shawe-T aylor. [8] N. Cristianini, J. Shawe-Taylor, and A. Elisseeff. On [9] R. Fagin, R. Kumar, and D. Sivakumar. Efficient similarit y [10] M. Fazel. Matrix rank minimization with applications. Ph.D. [11] A. Gionis, P. Indyk, and R. Motwani. Similarity search i n [12] Y. Grandvalet and S. Canu. Adaptive scaling for feature [13] H. W. Kuhn and A. W. Tucker. Nonlinear programming. In [14] J. T. Kwok and I. W. Tsang. Learning with idealized kerne ls. [15] B. Sch  X olkopf and A. Smola. Learning with Kernels: Support [16] S. Tong and E. Chang. Support vector machine active [17] V. Vapnik. Statistical Learning Theroy . John Wiley and Sons, [18] T. Wang, Y. Rui, S.-M. Hu, and J.-Q. Sun. Adaptive tree [19] G. Wu, E. Y. Chang, and N. Panda. Formulating distances [20] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metri c
