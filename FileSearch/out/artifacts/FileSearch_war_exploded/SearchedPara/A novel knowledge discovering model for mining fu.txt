 1. Introduction
Data mining extracts implicit, previously unknown, and potentially useful information from databases. The discovered and business management. Many approaches have been proposed to extract information, with mining sequential patterns as hierarchy. Based on the hierarchy, sequential patterns can be found not only at the leaf nodes of the hierarchy, but also carry more specific and concrete information, and those at higher concept levels carry more general information. sonal History in the Children X  X  Literature category, the sequential pattern mentioned above may appear as follows:
After buying a book from the Children X  X  Literature category, a customer at an on-line bookstore will return to buy two books from the Mathematical Analysis and Children X  X  Literature categories.

Before applying algorithms to discover multi-level sequential patterns, we need to specify a concept hierarchy over the items, which can be constructed by either automatic or manual approaches. The automatic approach is referred to in [11] and experts or users can provide the manual approach.

Amazon Books website for book classification). To solve this problem, we apply fuzzy set techniques to the concept taxono-can be treated as a special case of fuzzy set.
 No previous research, to our knowledge, has ever addressed the problem of mining fuzzy multi-level sequential patterns. ( FMSM ). The FMSM algorithm is developed by extending the well-known generalized sequential pattern (GSP) algorithm, which uses a stage-by-stage process for generating frequent patterns [21] . The benefits of this new model are as follows: 1. By including the concept hierarchy, we can discover more general and compact knowledge from the data. 2. By applying fuzzy concepts to the hierarchy, we can represent the relationships among items in a more complete and natural manner. In turn, the generalized knowledge discovered by the algorithm would be closer to users X  actual semantics.

The remainder of this paper is organized as follows. Section 2 reviews related works. Section 3 formally defines the prob-lem of mining fuzzy multi-level sequential patterns. Section 4 not only presents our algorithm, which is developed by extending the GSP algorithm, but also details the development of a new algorithm, the CROSS-FMSM algorithm, to discover drawn in Section 6 . 2. Related works
Taxonomy is a scheme that partitions a body of knowledge and defines the relationships among the pieces. Scholars use taxonomy to classify and better understand a body of knowledge. A number of researchers have studied taxonomy in rela-when investigating computational linguistics and artificial intelligence. Chuang and Chien [6] proposed categorizing query terms from on-line search engine logs to construct web taxonomies. In the business field, Cho and Kim [7] proposed using product hierarchies to improve the search for collaborative recommendations in e-commerce. Ryu [20] proposed a mecha-nism for electronic shopping supports based on dynamic product taxonomy hierarchies.
 In the data mining field, there are some studies that have employed the taxonomy concept in mining patterns. Srikant and
Agrawal [22] and Han and Fu [14] were the first to propose mining association patterns using the taxonomy concept. The work from Srikant and Agrawal [22] discovered relationships between items at any level of a given hierarchy and con-strained pattern generation using the minimum support and confidence thresholds. Their algorithm used the same support threshold across all levels, however, leading to some uninteresting patterns. Therefore, Han and Fu [14] adopted different minimum support thresholds for different levels, and employed a strategy to constrain pattern generation in order to in-crease mining performance. Furthermore, Srikant and Agrawal [21] proposed the GSP algorithm to discover sequential pat-hierarchy where cross-level patterns may be of interest. For example, Leung et al. [17] addressed cross-level mining in the context of a different type of hierarchy.

All of the above-referenced studies apply the crisp taxonomy concept, whose hierarchical relationship is either 1 or 0 in agement [3,18,23,24] and fall under a classification system called fuzzy taxonomy. Chen and Wei [8] employed fuzzy taxo-nomic structures to discover fuzzy association rules. Following Srikant and Agrawal [21] , Hong et al. [9] discovered interesting generalized association rules with the fuzzy taxonomy concept. Subsequently, Hong et al. [10] proposed an ap-concept with quantitative data.
As shown in the above review, although crisp taxonomy has been applied in discovering association rules and sequential including fuzzy taxonomy in the problem of mining sequential patterns. We propose a new, fuzzy approach to define and discover multi-level sequential patterns. 3. Problem definition In this section, we define the problem of fuzzy multi-level sequential pattern mining in sequence databases. itemset are ordered alphabetically by name.
 For example, a is an item and ( abc ) is an itemset.

Definition 2. A sequence s is an ordered list of itemsets, which is represented as h s 1 6 multiple times in different elements of a sequence.

For example, s = h ( abc ) he i is a sequence with three elements, where s omitted if an element only has one item. Therefore, s 2 =( h ) and s
The taxonomy structure in Fig. 1 is a tree depicting the relationship of concepts from the most generalized categories to degree 0.4 and the Study &amp; Teaching category with degree 0.9. The following formally defines the taxonomy structure: 0&lt; h &lt; l , and is the leaf node level when h = l . At level h (0 originating from a label at level h 1 enters it with a degree between 0 and 1. strings,  X 124 X  and  X 234 X , represented as 124|234.
 encoded method to Fig. 1 , we redraw the taxonomy structure in Fig. 2 .
 occur multiple times in different elabelsets. We omit the brackets if an elabelset has only one elabel. For example, 111,
To discover fuzzy multi-level sequential patterns, we compute the degrees of items, itemsets, and sequences using a tax-onomy structure TS . In the following, we define how the degrees can be formally computed. Definition 4 ( Matching operator ). Assume an encoded label el that is formed from m digits, and an item it with codes conditions are met. Let el i be the i th digit of el and c el = X  *  X  for i =0,1, ... , m 1, then el ? it holds. If el i
Example 1. Suppose we have an encoded label el = 124 and an item it = X  d  X  with codes 124|234 (we assume c el matches it ( el ? it ) because el 0 = c 01 =  X 4 X , el 1 because el 0 = X  *  X , el 1 = c 11 =  X 2 X , and el 2 = c 21 =  X 1 X . Also, when el =1 ** , we say el ? it because el (denoted as x 0 y ), given as follows: degree of edge p 0 in TS . When x 0 ? y is a complete matching, sup _ i ( x 0 , y ) is equal to 1. three different levels: (1) At level 1, x 0 =1 ** is contained in y with degree sup _ i ( x 0 , y ) = max{min(1, 0.55), min(1, 0.8)} = 0.8. (2) At level 2, x 0 =11 * is contained in y with degree sup _ i ( x 0 , y ) = min{0.55} = 0.55. (3) At level 3, x 0 = 112 is contained in y with degree sup _ i ( x 0 , y )=1.

Definition 6.1. For an encoded labelset s 0 i  X  X  x 0 1 x 0 to which s 0 i is contained in a j (denoted as s 0 i a j ). The function can then be defined as follows: sup is s 0 h ; y k h g ; if there exist integers 1 6 k 1 &lt; k 2 &lt; &lt; k Definition 6.1 would become ambiguous if there were multiple combinations of indexes that satisfied the requirements. To remedy this, we provide the following definition.

Definition 6.2. Suppose there are C combinations of 1 6 k
Let sup _ is c denote the degree to which encoded labelset s 0 where c =1,2, ... , C .

Example 3.1. Suppose we have an encoded labelset s 0 i  X  X  111 235 246  X  and an itemset a 235|245|255 246 247). Then s 0 i is contained in a j , because there exist distinct integers k (2) the degree sup is  X  s 0 i ; a j  X  X  min  X  1 ; 1 ; 1  X  X  1.

Example 3.2. Following Example 3.1 , we modify the encoded labelset as s 0 tained in a j . We represent these two mappings by underlining the corresponding codes: (1) (111 235|245|255 246 247) and (2) (111 235|245|255 246 247 ).

These two degrees can be computed as: (1) sup is  X  s 0 i ; a j  X  1  X  min f min  X  1  X  ; min  X  0 : 3  X  ; min  X  1  X g  X  0 : 3 and (2) sup is  X  s 0 i ; a j  X  2  X  min f min  X  1  X  ; min  X  0 : 3  X  ; min  X  1  X g  X  0 : 3.

Finally, the degree sup is  X  s 0 i ; a j  X  is max f sup is  X  s 0
Example 3.3. Following Example 3.1 , we modify the encoded labelset as s 0 contained in a j : (1) (111 235|245|255 246 247) with sup is  X  s 0 i ; a j  X  (2) (111 235|245|255 246 247 ) with sup is  X  s 0 i ; a j  X  (3) (111 235|245|255 246 247 ) with sup is  X  s 0 i ; a j  X  Finally, sup is  X  s 0 i ; a j  X  is max{0.3, 0.3, 1} = 1.

Definition 7.1. Given an encoded sequence s 0  X h s 0 1 s 0 sup _ seq ( s 0 , a ) be the degree to which s 0 is contained in a (denoted as s 0 a ). The function can be defined as follows: sup seq  X  s 0 ; a  X  X  min h f sup is  X  s 0 h ; a w h =1,2, ... , m .
 Definition 7.1 , however, also has the multiple combinations problem. To remedy this, we provide Definition 7.2 .
Definition 7.2. Suppose there are C combinations of 1 6 w max C { sup _ seq c }, where c =1,2, ... , C .
 h (111 235|245|255 246 247) 247 123 246 i . Then s 0 is contained in a , because there exist distinct integers w that: (2) the degree sup _ seq ( s 0 , a ) is min(1, 1) = 1.
 contained in a . We represent these four mappings by underlining the corresponding elabels: (1) h (111 235|245|255 246 247) 247 123 246 i with sup _ seq ( s 0 , a ) (2) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq (s 0 , a ) (3) h (111 235|245|255 246 247) 247 123 246 i with sup _ seq ( s 0 , a ) (4) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq ( s 0 , a )
Finally, the degree sup _ seq ( s 0 , a ) = max{ sup _ seq ( s 0 , a ) 0.3} = 0.3.
 (1) h (111 235|245|255 246 247) 247 123 246 i with sup _ seq ( s 0 , a ) (2) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq ( s 0 , a ) (3) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq ( s 0 , a ) (4) h (111 235|245|255 246 247) 247 123 246 i with sup _ seq ( s 0 , a ) (5) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq ( s 0 , a ) (6) h (111 235|245|255 246 247 ) 247 123 246 i with sup _ seq ( s 0 , a ) Aggregating the six degrees, we obtain degree sup _ seq ( s 0 , a )=max{0.3,0.3,1,0.3,0.3,1} = 1. coded) sequence is the length of the (encoded) sequence. An encoded sequence whose length is k is referred to as a k -se-quence. A fuzzy multi-level sequential pattern whose length is k is referred to as a k -sequential pattern. 4. Algorithm for mining fuzzy multi-and cross-level sequential patterns
In this section, we propose the fuzzy multi-level sequential mining ( FMSM ) algorithm and the CROSS-FMSM algorithm to discover fuzzy multi-level sequential patterns and fuzzy cross-level sequential patterns, respectively. An overview of the two algorithms is shown in Fig. 3 .
 to be discovered. The next step is to generate all frequent length k patterns, L patterns. We present the details of the FMSM algorithm in Section 4.1 and the CROSS-FMSM algorithm in Section 4.2 . 4.1. Fuzzy multi-level sequential mining algorithm node. We employ different minimum support thresholds at different levels to avoid generating numerous uninteresting pat-terns, which occurs if the minimum support is too small at higher levels. Therefore, we progressively reduce the minimum support thresholds at lower levels.

The FMSM algorithm is developed by extending the GSP algorithm [21] , which is based on the well-known Apriori algo-finds patterns at higher levels and lower levels. Compared with the GSP, the FMSM algorithm has four distinct features: 1. The GSP algorithm discovers frequent patterns in the original sequence database. FMSM, however, maps the items in each transaction to encoded labels and generates a new encoded database. The algorithm then discovers patterns at each level by accessing this new encoded database. quent patterns of length 1 for all levels simultaneously. at level 1. If the support of x 0 does not reach the minimum support threshold at the bottom level, we remove those encoded labels matching x 0 from the database. 4. The FMSM algorithm generates candidate patterns and progressively determines frequent patterns from higher levels to lower levels, while the GSP algorithm only finds patterns at a single level.

The FMSM algorithm consists of three phases. The first phase generates candidate and frequent encoded labels at all lev-
C , from the set of frequent encoded sequences of length k -1, denoted by L all patterns in C k , l .

In the following, we discuss how to execute the first phase: (1) Let n denote the number of levels in the hierarchy. The set of candidate patterns of length 1, C (2) Then, the set of frequent patterns of length 1 at level l , L how to execute the second phase for k P 2: (1) For k = 2 and level l =1,2, ... , n 1. C 2, l is obtained by directly joining L (2) For k &gt; 2 and level l =1,2, ... , n 1. We generate the set of candidate encoded sequences C
Finally, we discuss how to execute the third phase, which determines the supports of all patterns in C research [1,16] . Suppose we are given a candidate set C k , l occur at the same time. Otherwise, we set con = 1. After all the patterns in C in the corresponding leaf node in the tree. Therefore, we can determine which patterns are frequent and which are not.
Traversing the tree for every transaction in order to determine pattern supports, however, is not so straightforward. For computation. An inexperienced approach would be to generate a different encoded database to compute the supports of pat-terns at every level. Unfortunately, this method would not only be time-consuming, but space-consuming as well. Therefore, this paper only generates one copy of the encoded database at the bottom level, such as the one shown in Fig. 5 b. When we current level. As a result, we can match patterns at all levels using one copy of the encoded label database. The steps to the FMSM algorithm are shown in Fig. 4 . For clarity, we omit the detailed functions and steps.
Example 5. The following two steps describe how the FMSM algorithm finds the patterns from the sequence database in Fig. 5 a.
 imum support threshold at level 1 is min _ sup [1] = 0.6 and min _ sup [2] = min _ sup [3] = 0.5 at levels 2 and 3. Then C
C 1,3 will be generated as follows ( Pattern : Support ): 0.4, 124|234: 0.7, 235|245|255: 0.8, 246: 0.4, 247: 0.2, 258: 0.3}.

The frequent patterns of length 1 are L 1,1 ={1 ** ,2 ** }, L their supports are larger than or equal to min _ sup [1], min _ sup [2], and min _ sup [3].
Step 2: Execute the second and third phases repeatedly. We discover patterns of length k P 2 from the top levels to the Table 2 .
 land , a customer will return to buy a book named 3000 Solved Problems in Calculus .

Example 6. Given an encoded sequence h (111 112|122 123) 258 235|245|255 i , we will explain how the supports of patterns matches by underlining the corresponding elabels.
 (3) We find h (111 112|122 123) 258 235|245|255 i matches pattern h 112|122 235|245|255 i for all digits. 4.2. Fuzzy cross-level sequential patterns not restricted to one level, but crosses different levels. A fuzzy cross-level sequential pattern could look like this: After buying a book, Alice X  X  Adventures in Wonderland , a customer at an on-line bookstore will return to buy books in the Mathematical Analysis and Children X  X  Literature categories.
 Fuzzy cross-level patterns provide more information than same level patterns.

Slightly modifying the FMSM algorithm to discover fuzzy cross-level sequential patterns creates the CROSS-FMSM algo-rithm . The FMSM algorithm employs various minimum support thresholds for the different levels; however, the CROSS-FMSM algorithm only utilizes one minimum support threshold for all levels.

The matching processes of the CROSS-FMSM algorithm and the FMSM algorithm differ in that an encoded sequence needs can still use the encoded label database at the lowest level to compute the supports of cross-level patterns. sequence h (111 112|122 123) 258 235|245|255 i (the corresponding elabels are underlined). The matching steps are as follows: (1) The third digit of 111 is equal to that of 1 ** . The remaining digits of 111 can be ignored. (2) The second and third digits of 122 are equal to that of 12 * . The first digit of 122 can be ignored. (3) The encoded label 235|245|255 completely matches the third encoded label of the cross-level pattern.
For brevity, we only describe the major concepts of the CROSS-FMSM algorithm. Basically, the CROSS-FMSM algorithm is a mixture of the FMSM algorithm and the Apriori algorithm. The CROSS-FMSM algorithm begins by performing the following operations: (1) generate the encoded label database, (2) generate the frequent patterns of length 1 at all levels, L ations are also executed at the beginning of the FMSM algorithm. After that, however, the CROSS-FMSM becomes more like the well-known Apriori or GSP algorithm. That is, it first uses L levels. Then, following the Apriori algorithm X  X  repetition strategy, it repeatedly generates C scanning the encoded database until the candidate set is empty. Note that when scanning the encoded database to determine
L , the computation of supports is no longer the one used in the Apriori or GSP algorithm, but rather the custom-designed method used in this paper. The steps to the CROSS-FMSM algorithm are shown in Fig. 6 .

Example 8. Following Example 5 and setting the minimum support min _ sup = 0.5, the frequent encoded labels of length 1 for all levels are L 1,1 ={1 ** ,2 ** }, L 1,2 = {12 * ,24 * }, and L from L 2 , and finally determine L 3 from C 3 . For brevity, Table 3 only lists the frequent patterns. 5. Experimental results and performance study
In this section, we use both synthetic and real datasets to study the performances of the following algorithms: the gen-(CROSS-GSP), and the CROSS-FMSM for fuzzy cross-level patterns.

According to the idea of fuzzy taxonomies, the FMSM and the CROSS-FMSM algorithms face an intrinsic problem in com-putation. The problem is when items are organized as a fuzzy hierarchy, the number of candidate patterns increases dramat-ically. To remedy the problem, we designed a preprocessing strategy to improve the performance of the both algorithms. The
Then we store all of them in a hash table before executing the main body of the algorithm. When the main program calcu-them from the scratch.
 These algorithms were implemented using Sun Java language (J2SDK 1.4.2_05) and tested on a PC with an Intel Pentium
IV 1.8 GHz processor and 1.25 GB main memory using the Windows Server 2003 operating system. Neither multithreading sults of the experiments using the synthetic dataset in Section 5.1 and then using the real dataset in Section 5.2 . 5.1. Synthetic dataset
Synthetic datasets were generated by applying the famous synthetic data generation algorithm in [21] and the taxonomy generation algorithm in [22] . The parameters of this experiment are shown in Table 4 .
The first seven parameters are similar to those of the data generation algorithm for traditional sequential patterns. To to any three of them. Therefore, in the scale-up experimental section, we take care of three parameters, N , R , and F .
We extended the transaction data, however, so that the items in different itemsets have different time values and items in the same itemset have the same time values. A value w is drawn from a Poisson distribution with mean T The drawn value w represents the average time interval between successive itemsets in this particular customer X  X  sequence.
Poisson distribution with mean w . Finally, we employ parameter O to simulate the overlapping circumstance in a fuzzy tax-onomy, where multiple nodes at the same level may have the same child nodes at the next level. Therefore, we use O to con-trol the number of common child nodes a node may have with other nodes at the same level. Moreover, we assign the membership degrees between a node and its child nodes using a range in the continuum [0,1]. In summary, the first 11 parameters are the classic parameters used in previous research but the last two, T new parameters created for the problem considered here. Some parameters are fixed: | D | = 100,000, N N = 10,000, R = 250, F =5, T I = 10, and O =2.

The first comparison considers the run times of the four algorithms with different minimum supports. The comparison was conducted based on the six datasets shown in Table 5 , where min _ sup varied from 10% to 2.5%. The results are shown algorithms change as we vary the values of | C |, | T |, | S |, and | I |.

The results all indicate that the GSP and CROSS-GSP algorithms are faster than the FMSM and CROSS-FMSM algorithms. As expected, discovering fuzzy patterns requires more time because more combinations of candidate patterns need to be con-sidered, resulting in a much larger search space. The FMSM and CROSS-FMSM algorithms, however, can still be run in a rea-sonable amount of time. We also found that the performance of the CROSS-FMSM algorithm is usually worse than that of the result, the search space of the candidate set greatly increases and it becomes more complicated to match transactions with patterns. These are some reasons why the FMSM algorithm is faster than the CROSS-FMSM algorithm. Nevertheless, we ob-patterns is the main reason why the FMSM and CROSS-FMSM algorithms X  performances decline.

Next, we studied the scalabilities of these four algorithms. Seven tests were performed using the dataset C4-T2-S4-I1.25 and fixing the minimum support threshold at 2.5%. For each test, we varied the value of a selected parameter, but kept all ing than the other two algorithms. This is because the algorithms generate many more candidate patterns due to fuzzy taxo-number of candidate patterns increases even more.
 remain stable with F , R , and N , and the run times of the FMSM and CROSS-FMSM algorithms decrease with F and R, but grow linearly with N and O . The influences of the four parameters on the FMSM and CROSS-FMSM algorithms are as follows: (1)
For F : Increasing the number of fanouts decreases the number of levels in fuzzy taxonomies. Therefore, the number of can-the number of levels in taxonomies, which reduces the number of items in taxonomies. Therefore, the number of candidate terns increase, as does the run time. (4) For O : When O is larger, we will have more overlapping nodes in taxonomies and, therefore, more candidate patterns will be generated. As a result, the run time will increase.
Summary : To summarize the results of both parts, the FMSM and CROSS-FMSM algorithms grow linearly with all param-
Additionally, the memory usage in the FMSM and CROSS-FMSM algorithms never exceeded 800 MB. This relatively low level supports. Thus, the memory is only used for storing temporary candidate trees during execution.
Finally, we studied the number of patterns generated by the four algorithms. Table 6 shows the datasets used in this experiment and the total number of generated patterns at all levels in each scenario. An index, EnRiching Percentage all levels. All results indicate that the FMSM and CROSS-FMSM algorithms can discover more interesting patterns than the
GSP and CROSS-GSP algorithms. The average ERP for level-by-level is 7.64% and for cross-level is 25.28%. 5.2. Real dataset
This section evaluates the efficiency of the FMSM and the CROSS-FMSM algorithms using real sequential data. We used a real dataset from five branches of the Songchine supermarket in Taiwan, which sold groceries for daily use from 2001/12/28 to 2002/11/28. A series of data preprocessing and cleaning tasks were performed, including combining transaction data from nodes of taxonomies. There were 15,798 members considered, and 386,848 transactions, with an average of 3.0 items per transaction. There were a total of 37,570 items. The taxonomy had 4 levels with 12 roots.

To convert the original crisp taxonomies into fuzzy taxonomies, we requested a marketing expert to adjust and append items changed to generate the novel fuzzy taxonomies.

Categories , is added to demonstrate the semantics-enriched patterns that can be discovered from fuzzy model. The minimum terns generated in these four dimensions in Section 5.2.5 . 5.2.1. The crisp/level-by-level dimension
In the crisp/level-by-level dimension, we found that supermarkets may have two kinds of customers, health-conscious Organic Vegetable) i , meaning customers buying from the Leafy Vegetable category will also buy from the Milk or Organic
Vegetable category. As for the second group, we found that many other customers buy from the Instant Noodles or Cola cat-ered using the traditional one-level mining approach. 5.2.2. The fuzzy/level-by-level dimension
The fuzzy/level-by-level dimension represents the rules that are not discovered by the GSP algorithm. The first rule is mands. The second rule in this dimension is h Fruit Vegetable, Leafy Vegetable i , meaning that customers buying from the egories. This knowledge can be used to improve shelf arrangement, product recommendation, or cross-selling. As for the third rule, we found that customers buying from the Health-conscious Gift category will also buy from the Organic
Health-conscious Drink category. This rule reveals a pattern that supermarket managers could utilize by putting related products from two categories together for increased sales.

To demonstrate our model X  X  capability, we added a new experiment, Virtual Linguistic Term Categories , to discover seman-tics-enriched patterns from the database. The linguistic terms could be high-price, high-quality, medium-price, medium-dependent on user cognition. Two rules, h (Organic Vegetable, High-price Edible Oils) i and h (Low-price Meat, Low-price abases could be more natural, meaningful, and understandable to managers. In summary, these kinds of rules in this dimen-sion could not be discovered by the traditional mining model. 5.2.3. The crisp/cross-level dimension
We show three rules in the crisp/cross-level dimension. All three rules come from the Taiwanese Vegetable category, which is at level 2 of crisp taxonomies. The rules involve Vacuum Golden Mushroom, Water Doofu, and Gao-Nong Great
Vegetable category, the supermarket manager could decrease the prices of the three items. In summary, a good marketing strategy would be to apply the discovered cross-level rules to design a successful product/category mix. 5.2.4. The fuzzy/cross-level dimension
For the last dimension, fuzzy/cross-level, we only show one rule, h (Health-conscious Gift, Vegetable) i . The Health-con-their supports are not greater than the minimum support threshold. We found that some categories at level 3, however, Health-conscious Gift category using fuzzy degrees. According to this rule, we realized that customers buying from the
Health-conscious Gift category will also buy from the Vegetable category. Therefore, the supermarket manager could design a product/category mix corresponding to this rule. Also, products from the Vegetable category could be developed into new products related to the Health-conscious Gift category.
 experiment.Theruleindicatesthat low-price, health-consciousgifts arefrequently purchased withvegetables. Infact,this rule connection between the Health-conscious Gift category and Vegetable category is much stronger when gifts X  prices are low. 5.2.5. The number of patterns
Finally, we studied the number of patterns for the Songchine real dataset by setting the minimum support threshold to 2.5%. The results in Table 8 indicate that the FMSM and CROSS-FMSM algorithms can discover more interesting patterns than the GSP and CROSS-GSP algorithms. Their ERP s are 7.79% and 11.89%, respectively. 6. Conclusions
Sequential pattern mining is a useful method for discovering customer purchasing patterns through time from transac-tional databases. Since the method was first proposed by Agrawal and Srikant [2] in 1995, it has become an established and more interesting and accurate information for decision making.
 Two algorithms for mining fuzzy multi-and cross-level sequential patterns were presented: the FMSM algorithm and the CROSS-FMSM algorithm. Although the performance analyses of the two algorithms were worse than that of the GSP and
CROSS-GSP algorithms, they are still serviceable and can be applied to discover patterns. Tests involving a real dataset proved that the novel model could discover more interesting rules than the traditional model. Moreover, those rules are valuable to decision makers who are looking for decision strategies.

Fuzzy multi-level sequential pattern mining represents a new and promising research area in data mining. The perfor-should be used to test the model X  X  performance.
 Acknowledgements
The authors gratefully acknowledge the Editor and anonymous reviewers for their valuable comments and constructive suggestions. This research was supported by the National Science Council of the Republic of China under the Grant NSC 96-2416-H-143-001.

References
