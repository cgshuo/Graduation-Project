 Query reformulation techniques based on query logs have re-cently proven to be effective for web queries. However, when initial queries have reasonably good quality, these technique s are often not reliable enough to identify the helpful reform u-lations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of re-formulated queries, or expanded queries to be specific, gen-erated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first two posi-tions in the new ranked list than there were initially, which leads to statistically significant improvements in retriev al ef-fectiveness.
 H.3.3 [ Information Search and Retrieval ]: Query For-mulation Algorithms, Measurement, Performance, Experimentation. Query reformulation, query expansion, query log, query per-formance predictor, learning to rank.
Query logs have become an important resource for many tasks including query reformulation [3, 6]. Most log-based reformulation techniques, however, are evaluated using non -standard approaches and proprietary query logs, making it hard to compare one to another. A more recent study [2] compares different techniques using TREC collections and finds that when intial queries have relatively high quality, query expansion is much more reliable than substitution.
Although the log-based expansion technique [2] can gener-ate some good reformulations for high-quality TREC queries, it also produces many bad reformulations and it does not generate a reliable ranking of the reformulations by quality.
In this paper, we show that we can effectively rerank the list of reformulated queries obtained with this log-based ex-pansion approach. By using as few as two features, SCQ (Similarity Collection Query) [8] and query clarity [1], we can substantially improve the ranking of reformulated querie s in terms of the quality of the reformulations in the top two ranks (measured by NDCG@2 ), which then leads to signifi-cant improvements in retrieval effectiveness.
The log-based query expansion method [2] (referred to as LQE ) is a slight modification of the query substitution method proposed by Wang and Zhai [6]. It first estimates a context distribution for terms occuring in a query log. It then constructs a translation model that can suggest simila r words based on their distributional similarity. Given any query, the expansion model will try to expand it with can-didates suggested by the translation model for each query term. The model decides whether to expand the query based on how similar the candidate is to the query term and how appropriate it is to the context of the query. For more de-tails, see [2].
Query quality predictors aim to predict a query X  X  quality without explicit relevance judgements. Thus, given a ranked list of reformulated queries, it is intuitive to think about reorganizing this list based on the  X  X uality X  score given by some predictor.

We tried some of the top-performing predictors that Ku-maran and Carvalho [4] used in a similar task and found that SCQ [8] and clarity score [1] are the most effective for our problem. Therefore, we rerank the list of expanded queries by where 1 and 2 are weight of the two predictors.
 Table 1: Statistics of queries used for reformulation Title Q. 133 133 200 66 119
Desc. Q. 150 150 246 94 134 Table 3: Evaluation of retrieval effectiveness in terms of MAP . Table 2: Our approach ( X  Rerank  X ) consistently out-performs LQE in NDCG@2 . All differences are signif-icant at p &lt; 0 . 05
In this section, we evaluate the performance of our rerank-ing technique. Evaluation is done on five TREC collections: AP, WSJ, Robust-04, WT10G and Gov-2, with both title and description queries. We use the language modeling framework and remove all stop words at indexing time. We adopt the parameter settings for LQE from the authors [2].
Due to the limited coverage of the available query log [5], we use only a subset of TREC queries where the LQE can generate at least one reformulation. Information about the se subsets is given in Table 1.
 On each collection, we first use LQE to generate a list of K expanded queries ( K = 30) for each original query. We append to this list the original query -in the case when all generated reformulations are bad, the reranking approach has a chance to choose not to reformulate. We then use our approach to rerank this list and compare its performance with that of the intial list as well as original query.
We run LQE with the MSN log to obtain a list of refor-mulations for each original query. We use all these queries to do retrieval and record their MAP and use them to cre-ate our dataset. Training and testing are done using 5-fold cross validation on this dataset. 1 and 2 are learned us-ing AdaRank [7] to maximize the average NDCG@2 . The algorithm ends up choosing either ( 1 = 1 , 2 = 0) or ( 1 = 0 , 2 = 1) depending on the collection.
We use NDCG@2 to measure the quality of the ranked list of reformulations given by our approach. Reformulatio ns are graded on a scale from zero to four with respect to the improvement m they provide over the original query. In particular, improvement larger than 0 . 03 corresponds to a 4, or ( m &gt; 0 . 03)  X  4. Similarly, (0 . 01 &lt; m  X  0 . 03)  X  3, (0 &lt; m  X  0 . 01)  X  2, ( m = 0)  X  1 and ( m &lt; 0)  X  0.
Table 2 summarizes the result: the list of reformulations ranked by our approach has a much higher average NDCG@2 than the initial list. All improvements are statistically s ig-nificant at p &lt; 0 . 05 using a two-tailed t-test.
We define the MAP of a ranked list of reformulations as the best MAP observed among its top two queries. In this section, we compare the MAP obtained by (i) the original query, (ii) the list of reformulations generated by LQE , and (iii) the list reranked by our method.

As can be seen in Table 3, the best of the top two refor-mulated queries ranked by our approach is almost always significantly better than the original query. This is not the case in LQE . In many cases, our method also provides signif-icant improvements over LQE . This result suggests that the reranking can push better reformulations to the first two positions in the ranked list.
In this paper, we have shown that by reranking the list of reformulations generated by the log-based query expansion technique [2] with only two features, we can push more good reformulations into the first two positions in the list. This is reflected in the huge gain of NDCG@2 and statistically sig-nificant improvement in retrieval effectiveness. In the futu re, we will investigate more features. We hope this will lead to greater improvement in NDCG@1 , helping retrieval systems to reformulate queries implicitly without user involvement .
This work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF grant #IIS-0711348, and in part by ARRA NSF IIS-9014442. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsor.
