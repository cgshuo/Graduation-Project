 The task of finding people who are experts on a topic has recently received increased attention. We introduce a different expert find-ing task for which a small number of example experts is given (in-stead of a natural language query), and the system X  X  task is to re-turn similar experts . We define, compare, and evaluate a number of ways of representing experts, and investigate how the size of the initial example set affects performance. We show that more fine-grained representations of candidates result in higher performance, and larger sample sets as input lead to improved precision. H.3 [ Information Storage and Retrieval ]: H.3.1 Content Anal-ysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; H.4.m Miscellaneous Algorithms, Measurement, Performance, Experimentation Expert finding, Similar experts, Expert representation
Increasingly, commerical systems and the information retrieval community pay attention to retrieving entities and not just docu-ments. Web search engines offer facilities for searching specific types of entities, such as books, CDs, restaurants. In 2005 and 2006 the TREC Enterprise track provided another example with the expert finding task, where systems return a ranked list of person names in response to a query. Here, people are being sought that are knowledgeable about a given topic, described in natural language.
We address a different expert finding task: we do not assume that the person seeking for experts supplies an explicit description of the area in which she seeks expertise (she might simply not be suf-ficiently knowledgeable). Instead, our user provides a small num-ber of example experts X  X eople that she knows personally or by reputation X , and the system has to return similar experts . Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Finding similar experts (or, more generally, similar people), dif-fers from finding similar documents in a number of ways. Most im-portantly, experts are not represented directly (as retrievable units such as documents), and we need to identify them indirectly through occurrences in documents. This gives rise to our main research question: what are effective ways of representing candidate experts for our finding similar experts task? We define, compare, and evalu-ate four ways of representing experts: through their collaborations, through the documents they are associated with, and through the terms they are associated with (as a set of discriminative terms or vector of weighted terms). Our second research question concerns the number of example experts provided by the user: how does the size of the sample set affect end-to-end performance?
An additional contribution consists of a method for generating example sets and the corresponding  X  X omplete X  sets, against which our results are evaluated. We use the TREC 2006 expert finding qrels as evidence of a person X  X  expertise, and use this evidence to create sets of people that are all experts on the same topics.
In Section 2 we point to related work; in Sections 3 and 4 we describe the expert representations and notions of similarity used. Results are presented in Section 5 and conclusions in Section 6.
As a research area, automatic approaches to expert finding re-ceived a big boost from the introduction of the expert finding task at TREC 2005 [6]. Many appproaches fall into one of two types: cre-ate a textual representation of the experts and estimate how prob-able a given expertise area is, or create a textual representation of the given expertise area and estimate how probable a candidate ex-pert is. Various variations on the expert finding task have since been proposed, including expert profiling  X  X iven a person, return a ranked list of topics in which she is an expert [1].

The finding similar experts task that we address may be viewed as a list completion task. List queries are common types of web queries [5]. Their importance has been recognized by the TREC Question Answering track [7] (where systems return two or more instances of the class of entities that match a description) and by commercial parties (e.g., Google Sets allows users to retrieve en-tities that resemble the examples provided [4]). Ghahramani and Heller [3] developed an algorithm for completing a list based on examples using Bayesian inference techniques. A proposed list completion task will likely be run at INEX 2007 [2].
We introduce four ways of representing a candidate ca : (WG) As a set of people that ca is working with. We use organiza-(DOC) As a set of documents associated with ca . DOC ( ca ) (TERM) As a set of terms extracted from DOC ( ca ) . TERM ( ca ) (TERMVECT) As a vector of term frequencies, extracted from
Let S =  X  ca 1 , . . . , ca n  X  be the sequence of examples provided by the user. Given S , the score of candidate ca is computed using where sim ( ca, ca 0 ) reflects the degree of similarity between can-didates ca and ca 0 . The m candidates with the highest score are returned as output. Using the representations described above we compute similarity scores as follows. For the set-based represen-tations (WG, DOC, TERM) we compute the Jaccard coefficient. E.g., similarity based on the (DOC) representation boils down to Similarity between vectors of term frequencies (TERMVECT) is estimated using the cosine distance: sim ( ca, ca 0 ) = cos( ~ t ( ca ) , ~ t ( ca 0 )) = where ~ t ( ca ) and ~ t ( ca 0 ) denote the term frequency vectors repre-senting candidate ca and ca 0 , respectively. Experimental design. For evaluation we use the TREC Enterprise test collections. The document collection is the W3C corpus (appr. 330.000 documents, 5.7GB). Names and e-mail addresses of 1092 expert candidates (W3C members) are given. Working group mem-bership information (needed for WG ) is provided by the TREC 2005 expert finding topics and qrels.

To simulate the user X  X  input (a set of example experts) and to generate the corresponding  X  X omplete X  set of similar experts that can be used as ground truth, we used the following algorithm. The algorithm generates random sets of experts, with size  X  n + m where n is the size of the example set, and m is the minimal num-ber of additional experts that belong to the same set. We write expert ( ca, t ) to denote that ca is an expert on topic t 2006 topics and qrels are used to define expert ( ca, t ) 1. Select n candidates at random (the sample set S ), and put 2. CA is the set of additional candidates who are experts on 3. The sample set S is valid, if | CA |  X  m We conducted experiments for various input sizes ( n = 1 , . . . , 5 Our system is expected to complete the list with 15 additional can-didates ( m = 15 ). For each n , we generated 1 , 000 (except for n = 1 , where the number of valid sets is only
We measured the mean reciprocal rank of the first retrieved result (MRR), as well as precision at 5, 10, and 15.
 Results. In Table 1 we report on the results of our experiments. We have two important findings. First, more fine-grained repre-sentations of candidates consequently result in higher performance (for all mesaures). Second, concerning the size of the example set, we conclude that larger input samples lead to higher scores (for all measures). Our best representation (TERMVECT) delivers excel-lent performance, achieving MRR=0.853, P@5=0.703 (for n = 5
Interestingly, the (DOC) representation is similar in performance to (TERM) for very small example sets, but looses out on larger sets. Also, for (WG), (DOC), (TERM) the P@5, P@10, P@15 scores tend to be very similar, while for (TERMVECT) we clearly have P@5 &gt; P@10 &gt; P@15.
We introduced an expert finding task for which a small num-ber of example experts is given, and the system X  X  task is to return similar experts . We defined, compared, and evaluated four ways of representing experts: through their collaborations, through the documents they are associated with, and through the terms they are associated with (either as a set of discriminative terms or as a vector of term weights). Moreover, we introduced a method that generates and validates random example sets, and determines the  X  X omplete X  set, against which our results are evaluated. We found that more fine-grained representations of candidates result in higher perfor-mance; a vector of weighted term frequencies, extracted from the documents associated with the person, is proven to be the most ef-fective way of representing candidate experts. Finally, larger sam-ple sets as input lead to better overall performance. This research was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.
