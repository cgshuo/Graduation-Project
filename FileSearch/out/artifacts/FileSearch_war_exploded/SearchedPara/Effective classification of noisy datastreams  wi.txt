 REGULAR PAPER Xingquan Zhu  X  Xindong Wu  X  Ying Yang Abstract Recently, mining from data streams has become an important and chal-lenging task for many real-world applications such as credit card fraud protec-tion and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classi-fiers for effective classification. In this paper, we propose a new dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single  X  X est X  classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accu-racy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and compar-ative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence.
 Keywords Stream data mining  X  Classification  X  Dynamic classifier selection  X  Classifier ensemble  X  Multiple classifier systems  X  Class noise 1 Introduction The ultimate goal of effective mining from data streams (from the classification perspective) is to achieve the best possible classification performance for the task at hand. This objective has traditionally led to an intuitive solution: separate stream data into chunks, and then integrate the classifiers learned from each chunk for a final decision [ 9 , 16 , 34 ]. Given a huge volume of data, such an intuitive solu-tion can easily result in a large number of base classifiers, where the techniques from Multiple Classifier Systems (MCS) [ 1 , 12 ] are involved to integrate base classifiers. The fact behind the merit of MCS is from the following underlying assumption: Each participating classifier in the MCS has a merit that deserves ex-ploitation [ 32 ], i.e., each base classifier has a particular subdomain from which it is most reliable, especially when different classifiers are built using different subsets of features, different subsets of the data, and/or different mining algorithms. grated. Existing integration techniques can be distinguished into two categories: 1. Combine base classifiers for the final decision. When classifying a test in-ing on the level of information being exploited. Type 1 makes use of class labels. Type 2 uses class labels plus a priority ranking assigned to each class. Finally, Type 3 exploits the measurements of each classifier and provides each classifier with some measure of support for the classifier X  X  decision. The CS takes the oppo-site direction. Instead of adopting the combining techniques, it selects the  X  X est X  classifier to classify a test instance. Two types of techniques are usually adopted: 1. Static Classifier Selection (SCS). The selection of the best classifier is speci-2. Dynamic Classifier Selection (DCS). The choice of a classifier is made during bination techniques [ 9 , 16 , 23 , 34 ], and as they have demonstrated, a significant amount of improvement could be achieved through the ensemble classifiers, as long as the accuracy of the individual base classifier is greater than 50%. How-ever, given a data stream, it usually results in a large number of base classifiers, where the classifiers from the historical data may not support (or even conflict with) the learner from the current data. This situation is compounded when the underlying concept of the data stream experiences dramatic changes or evolving, or when the data suffers from a significant amount of noise, because the classi-fiers learned from the data may vary dramatically in accuracy or in their domain of expertise (i.e., they appear to be conflictive). In these situations, choosing the most reliable one becomes more reasonable than relying on a whole bunch of likely contradictive base classifiers. The underlying hypothesis on using individ-ual classifier behavior in the MCS design is that every classifier has characteristics justifying its participation in the MCS. In the case that a very  X  X ood X  classifier is applied in combination with a  X  X ediocre X  classifier, the MCS might be able to im-prove the results in comparison with the result from the single  X  X ood  X  classifier, if both classifiers make independent errors [ 30 ]. If the condition of independence is not verified, the majority vote technique may be biased and is not guaranteed to improve the individual classifiers. The research work with [ 18 ]and[ 24 ] has indi-cated the relationship between the majority vote accuracy and dependency among classifiers, and they concluded that in multiple classifier systems, the independent classifiers are more beneficial than dependent classifiers.
 noisy data streams. Our intuitive assumption is that stream data at hand suffers from dramatic concept drifting, or a significant amount of noise, so the existing CC techniques become less effective. We will first review related work in Sect. 2; and then propose our new method in Sect. 3. In Sect. 4, we discuss about applying the proposed DCS scheme in noisy datasets. Our experimental results and com-parative studies in Sect. 5 indicate that the proposed DCS scheme outperforms most CC or CS methods in many situations and appears to be a good solution for mining real-world data. 2 Related work The two main reasons of employing multiple classifiers for stream data mining are efficiency and accuracy. Although the efficiency could be the main reason for adopting multiple classifiers, because a data stream can always involve a huge volume of data which turns to be a nightmare for any single learner. The accuracy of MCS in handling stream data is also remarkable: especially when the concept in the data stream is subject to evolving, changing or drifting [ 16 , 34 ]. Like many partitioning-based or scale-up learning algorithms (e.g., Bagging [ 4 ], Boosting [ 28 ] and Meta learning [ 7 ]) have demonstrated, by partitioning the whole dataset into subsets, the system efficiency can be dramatically improved, with a limited sacrifice of the accuracy.
 simplest) scheme is CC based simple voting which is also called Select All Major-ity (SAM) [ 21 ], where the prediction from each base classifier is equally weighted to vote for the final prediction. Although simple, SAM has been proved to be ef-fective to integrate multiple classifiers, and many revised versions [ 9 , 16 , 23 , 34 ] have been successfully developed to handle data streams. In comparison with CC based schemes, the CS schemes select one classifier for the final decision, and intuitively, the differences between these two mechanisms are explicit and dis-tinct because the latter uses only one classifier in the final decision. Nevertheless, different research efforts have come up with many solutions in which instead of selecting or combing base classifiers, the decision is made by selecting the  X  X est X  class for a given test instance. To this end, the Behavior Knowledge (BSK) [ 36 ]or Bayesian Belief Network [ 12 ] are usually involved. The behavior-knowledge indi-cates the classification results of the evaluation set assigned by the individual base classifiers. Every possible classification combination is an index regarded as a cell in a look-up table (BSK table). The table is designed using the evaluation set Z and the classification of all instances from L base classifiers. Each Z j is placed in the cell indexed by C 1 ( Z j ),... , C L ( Z j ) . The elements in each cell are tallied and the most representative class label is selected for this cell. Ties are broken randomly. The decision of the  X  X est X  class for a test example  X  I x is made according to the information, which is ignored by the BSK scheme, from the confusion matrices (the BSK table), various Bayesian schemes have been proposed [ 36 ] where belief networks become popular in selecting the  X  X est X  class. Given an evaluation set Z , we assume the possible class values of the instances are  X  1 ,... , X  m ,... , X  T , then P ( X  =  X  m | C j ( I k ), I k  X  Z indicates that given the outcome of the base classifier C j , the probability of the true class label of these instances  X  is  X  m .This probability is estimated by using the error matrix of each base classifier with the evaluation set Z. Then the belief function for class label  X  m is denoted by Eq. (1). where  X  is a normalizing constant to ensure that the sum of the beliefs from all class labels equals to one. I k denotes an instance in the evaluation set Z . Finally the  X  X est X  class of  X  I x is the class with the highest belief, as denoted by Eq. (2). and the confusion matrix are exploited to the full, making them a better alterna-tive to the majority-vote technique. However, this scheme heavily relies on the confusion matrix that is constructed from the classification results of the whole evaluation set, and it may still fall into the category of Classifier Combination rather than Selection.
 selection. Their major difference is that CSC selects the  X  X est X  classifier before a test instances is evaluated, and all test instances are classified by the same single  X  X est X  classifier; and DSC on the other hand, won X  X  determine the  X  X est X  classifier until a test instance is specified, and different test instances can be classified by different  X  X est X  classifiers. A SCS system is based on regions of competences that are predefined to classifying an unlabeled vector, as opposed to a dynamic classi-fier selection system, which determines these regions on the fly. In both cases, the regions are determined by using an evaluation set, which is usually derived from the whole or a portion of the training set.
 (CVM) [ 29 ]. In CVM, cross-validation is adopted and the base classifier with the highest classification accuracy from the cross-validation is selected to classify all test instances. With CVM, the evaluation set is used to explore the merit of each base classifier, which consequently ignores (or contradicts) the assumption that each base classifier has its own subdomain in which it is most reliable. Accord-ingly, instead of using the whole evaluation set, an intuitive substitution is to split the feature space into  X  X ubsets X  or regions of competence, regardless of the clas-sification boundaries. Each base classifier is then assigned to a region and has to classify all unlabeled feature vectors that fall into its region of competence. Two approaches of static classifier selection have been reported in [ 18 ]and[ 33 ]: The first specifies the regions and assigns a responsible classifier for each region, and the second finds the regions and the best classifier for each region based on the ac-curacy results of the classifier. The specification of regions in the former approach can be done either by subdividing the feature space into Q coarse bins, or by clus-tering the training data set into Q number of regions, disregarding the class labels. Then, estimate the classification accuracy of the L classifiers C 1 ,... , C L on each region R r , 1  X  r  X  Q . The base classifier C j is selected as the best classifier for region R r once it has acquired the highest accuracy among all base classifiers. However, these approaches are problematic in that the definitions of the number and the shape of regions are arbitrary and influence the success of these methods. the testing phase, DCS dynamically selects the  X  X est X  classifier for each test in-stance. Among different DCS schemes, the most representative one is Dynamic Classifier Selection by Local Accuracy (DCS-LA) [ 35 ] which explores a local community for each test instance to evaluate the base classifiers, where the local community is characterized as the k Nearest Neighbors ( k NN) of the test instance in the evaluation set Z . The intuitive assumption behind DCS-LA is quite straight-forward: Given a test instance  X  I x , we find its neighborhood  X (  X  I x )in Z (using the Euclidean distance), and the base classifier that has the highest accuracy in classi-fying the instances in  X (  X  I x ) should also have the highest confidence in classifying  X  I .Let C j ( j = 1 ,... , L ) be a classifier, and an unknown test instance. We first label with all individual classifiers ( C j , j = 1 ,... , L ) and acquire L class labels C (  X  I mated for each classifier. Given Z , the local accuracy of classifier C j with instance  X  I , LocC j (  X  I x ) , is determined by the number of local evaluation instances for clas-sifier C j that have the same class label as the classifier X  X  classification, over the total number of instances considered, as defined by Eq. (3).
 where  X (  X  I x ) indicates the k nearest neighborhood of  X  I x in Z ,  X ( I k ) denotes the class label (true class) of instance I k , k is a user-specified positive integer which indicates the number of neighborhood, and ... denotes the number of instances in the set ... . The final decision for  X  I x isgivenbyEq.(4) uation set, DCS-LA uses a local neighborhood of the given test instance to ex-plore the reliability of the base classifier. DCS-LA is an efficient mechanism in selecting the  X  X est X  classifier. Meanwhile, modifications have been made on lo-cal class accuracy, local accuracy using distance-weighted kNN [ 26 ], to improve the performance of the original DCS-LA scheme. In Tsymbol and Puuronen [ 31 ], a weighted nearest neighbor scheme was integrated with bagging and boosting for dynamic classifier selection. Another modified DCS-LA scheme [ 19 ] assigns Nevertheless, although DCS-LA has been widely integrated in many systems, it suffers from the following three disadvantages: 1. The selection of k and the adopted distance function critically affect the system 2. The speed factor. Given a test instance, DCS-LA has to go through the whole 3. Sensitive to noise. Usually, the number of instances in a local community is munity, a new method has been suggested by [ 21 , 22 ] where the correspondence (or behavior) of the base classifiers is used to find the locality of the current test instance  X  I x .Given L base classifiers and N training instances (the whole train-ing set is used as the evaluation set), the algorithm will construct two matrices. The first is the response matrix that contains the predictions of the base classifiers built from the training data on the training data. I.e., the ( i , j ) th cell contains the prediction of base classifier C j for training instance I i . The second matrix, the performance matrix, contains the number of times each base classifier is correct when that training example appears as a test example in a cross-validation run on the training partition. Given a test instance I k , it is first classified by the base clas-sifiers. Comparing each row of the response matrix, the set of row vectors with the most similar response patterns (in term of the number of matched responses) represent a region defined from the perspective of the base classifiers. The corre-sponding rows in the performance matrix are then used to determine the accuracy of each of the base classifiers for the region defined by the examples with simi-lar response patterns. However, the drawback of this scheme is that the locality explored from this scheme critically depends on the performance of each base classifier, and the existences of class noise and poor base classifiers will affect the system performance.
 in classification. Instead of using all attributes to evaluate the base classifiers, a more reasonable way may consider the important attributes of each base classifier. Accordingly, a referee based dynamic classifier selection scheme was proposed in [ 25 ] where referees, in the form of decision trees, partition the whole evalu-ation set into subsets, and each base classifier is evaluated with these subsets to explore its domain of expertise. The advantage of the Referee is that it partitions the evaluation set into subsets by joining the features of the base classifiers. The less important attributes won X  X  be used in partitioning the evaluation set, and the partitioned subsets tend to be more reasonable in exploring the domain expertise. This strategy works as follows: 1. Given a training set X , learn a set of base classifiers, C 1 ,... , C L . 2. For each base classifier C i , a set of features are selected to train a decision tree 3. For each referee R i , all its leaf nodes partition the evaluation set into disjoint 4. Given a test instance  X  I x , the path which can classify  X  I x is first selected from set into subsets, the referee scheme works somewhat similar to the MCS scheme proposed in [ 5 ], where a hybrid decision tree (HBT) was used to determine the usage of various classification algorithm in different scenarios. With the scheme in [ 5 ], some background knowledge has to be integrated. This knowledge defines a set of heuristic rules to perform automatic algorithm selection, depending on the properties of the data. These rules are applied recursively to the data set in question to partition it within the framework of a HBT. The resultant decision tree therefore represents a classification structure where the data is successively partitioned into smaller subdivisions, and where different classification algorithms may be applied to each subset [ 6 ]. The advantage of the referee is that it partitions the evaluation set into subsets by joining the features of the base classifiers. The less important attributes won X  X  be used in partitioning the evaluation set, and the partitioned subsets tend to be more reasonable in exploring the domain expertise. However, the drawbacks are threefold: (1) to learn each referee, one has to explore the features of each base classifier; (2) it uses the learned decision trees to partition the original training set into subsets, and the system performance will critically depend on the quality of learned decision trees; and (3) because each classifier has its own referee, and the reliabilities from different referees are evaluated from different subsets. Without the same measurements, the selected classifier might not work well.
 base classifier, the DCS schemes have to evaluate the classifiers from either an test instance at the current stage. Such a mechanism inherently provides an adap-tive scheme that is most suitable for mining data streams with dramatic changes, where most existing mining efforts appear to be less effective. However, all exist-ing DCS approaches use either distance-based schemes or classification trees (or classification rules) to partition the evaluation set into subsets, where the quality of the partitioned subsets critically depends on the performance of adopted distance functions or classification trees. In this paper, we present a new DCS scheme that evaluates base classifiers with subsets of the evaluation set, where the subsets are constructed with statistical information of attribute values. We believe such a par-titioning scheme is more natural and intuitive in exploring the domain expertise of each base classifier for effective data stream mining. 3 AO-DCS: attribute-oriented dynamic classifier selection In this section, we present a new dynamic classifier selection scheme, called AO-DCS (Attribute-Oriented Dynamic Classifier Selection). We use attribute values of instances to partition the evaluation set into subsets for evaluation purposes. If the instances in a dataset have only one attribute, and we use this attribute to partition the evaluation set into disjoint subsets with each subset corresponding to one value of this attribute, the classification accuracy of each base classifier with these subsets is the performance of the classifier with the instances characterized by each attribute value. Then each base classifier X  X  performance will reflect its domain of expertise. Our AO-DCS takes the following three steps. 1. Statically partition the evaluation set into subsets by using the attribute values 2. Evaluate the classification accuracy of each base classifier on all subsets in JI . 3. For a test instance, use its attribute values to select the corresponding subsets 3.1 Constructing subsets Given a dataset D ,let X , Y and Z be the training, test and evaluation set, with the numbers of instances in X , Y and Z denoted by N X , N Y and N Z respectively, and C 1 , C 2 ,... , C L be the L base classifiers from X . The objective of AO-DCS is to select the  X  X est X  classifier C  X  to classify each instance  X  I x in Y . Our algorithm first acquires statistical attribute information of all instances in D . Assuming the instances in D have M attributes A 1 , A 2 ,... , A M , and each attribute A i contains n i values (we will discretize numerical attributes into discrete values). For an attribute A i , we use its values to partition the evaluation set Z into n i subsets S in Fig. 1 .
 values to partition the evaluation set Z into n i disjoint subsets, and instances with the same values on A i are put in the same subset. We partition the evaluation set Z by each attribute, and construct M i n i subsets from all attribute values. An example of evaluation set partitioning from a dataset containing only two attributes A 1 and A 2 is pictorially depicted in Fig. 2 , where the x -axis denotes the values of attribute A 1 ,andthe y -axis represents the values of attribute A 2 (assuming A 1 and A 2 contain 3 and 4 values respectively). The x -axis and y -axis V S A 2 are also depicted in Fig. 2 . Any two subsets from A 1 and A 2 , e.g., S S 3 , have a small portion of overlapping, and the overlapping region indicates the instances that have the same attribute values on A 1 and A 2 .
 classification accuracy of each base classifier C 1 ,... , C j ,... , C L on each of these M i ni subsets, and we evaluate the classification accuracy of each base classifier on each subset with the procedure in Fig. 3 . We denote the acquired accuracy matrix by Acy S subsets ( M i n i ) can be worked out. An example accuracy matrix is given in Fig. 4 , where the first column denotes the base classifiers, and the first row repre-sents the subsets constructed from all attribute values. Each ( i , j ) th cell indicates the classification accuracy of classifier C i on subset S j .
 base classifiers. Our analysis in Sect. 3.4 will indicate that although the subset constructed from one attribute looks insufficient in this regard, for many applica-tions the class of an instance can be accurately determined by using a small subset of all available attributes. Accordingly, our method takes this advantage and may possibly provide a  X  X atural X  way in guiding the classifier selection. 3.2 Dynamic classifier selection With the acquired classification matrix, Acy S 1 ,... , n selection with the procedure below: on all attributes  X  I A i x , i = 1 ,... , M . These attribute values can be used to find specific subsets that were constructed by similar instances in the evaluation set. The classifier that receives the highest average accuracy with all selected subsets is identified as the  X  X est X  classifier for  X  I x .
 3.3 Missing values and numerical attributes AO-DCS can accommodate both missing values and numerical attributes. A miss-ing value is treated as a specific additional value. Numerical attributes can be converted into nominal ones using discretization techniques. In our system, we adopt the k-means clustering algorithm [ 14 ] to convert numerical attributes into nominal ones. It takes the following steps. 1. Equally partition the numerical attribute values into K discrete regions (clus-2. Repeat: 3. Assign each acquired cluster center a symbol, and the instances in each cluster tive, e.g., Vector Quantization [ 20 ], class dependent or independent discretization [ 8 , 10 ]. 3.4 Remarks on relevant research efforts Given a dataset D, not all attributes have the same importance in classifying the dataset. The importance of each attribute is determined by both the learning algo-rithm and the dataset itself. Instead of pruning out some less important attributes, we use all attribute values to partition the evaluation set. In the case that the impor-tance of each attribute is available, or dimension reduction techniques are adopted [ 15 , 26 ], we can use selected attributes and their values to partition the evaluation set.
 AO-DCS works similar to the static dynamic classifier selection in [ 17 ]whereit also partitions the evaluation set into subsets to evaluate the performance of base classifiers. However, there are two key distinctions:  X  The method in [ 17 ] directly assigns the best classifier for each predefined re- X  Instead of adopting a clustering technique that has to use distance functions lection, where a small region of each instance is determined by going through the evaluation set to find the nearest neighbors, we use attribute values to determine the evaluation subsets for each test instance. The time complexity of our method is far less than the schemes in [ 25 ]and[ 35 ].
 method is also somewhat similar to the rudimentary rule induction algorithm  X  1R [ 11 ]. This method generates a one-level decision tree, which is expressed in the form of a set of rules that all test on one particular attribute. 1R is a simple, cheap method that often comes up with quite good rules for characterizing the structure in data. Perhaps this is because the structure underlying many real-world datasets is rudimentary, and just one attribute is sufficient to determine the class of an instance accurately. With the surprising results from the 1R algorithm, we can find that using a single attribute to explore the domain of expertise might be more reasonable to some degree, especially in noisy environments. 4 Applying AO-DCS in data stream mining In many applications, data is not static but arrives in data streams, and the stream data is also characterized by drifting concepts. In other words, the underlying data generation models, or the concepts that we try to learn from the data, are con-stantly evolving, even dramatically. Meanwhile, real-world stream data is never perfect and can often suffer from corruptions (noise) that may impact interpre-tations of the data, models created from the data and decisions made based on to apply general data cleansing mechanisms [ 37 ] to stream data for better data quality. Therefore, the above two facts (concept drifting and noise) imply that the classifiers learned from a small portion of the stream may vary significantly in performance, which makes DCS a promising solution for effective mining from a real-world data stream. In this section, we propose a framework to apply AO-DCS to mine noisy data streams. This framework is general enough to allow any exiting learning algorithms to be incorporated to handle any real-world data streams. S , S tion algorithm at one time. Then we learn a base classifier C i from each chunk S . To evaluate all base classifiers (in the case that the number of base classifiers is too large, we can keep only the most recent K classifiers) and determine the  X  X est X  one for each test instance, we will dynamically construct an evaluation set Z (using the most recent instances, because they are likely consistent with the cur-rent test instances). When classifying a test instance, I k , we will employ AO-DCS (and the evaluation set Z ) to integrate existing base classifiers and select the  X  X est X  classifier to classify I k .
 There are two common types of noise: attribute noise and class noise [ 37 ]. In this paper, we assume the data stream suffers from a certain level of class noise (which means the errors are introduced in the class labels), and will extensively evaluate the performance of different DCS schemes in handling noisy data, where various levels of class noise are manually introduced (before the data partitioning) to sim-ulate real-world scenarios. This should provide interested readers with valuable knowledge about mining from real-world noisy data streams. 5 Experimental results and comparisons In this section, we design two sets of experiments, (1) DCS from classifiers vary significantly in performance; and (2) DCS in classifying noisy datasets, to evaluate the performance of our method. We take 8 datasets (including synthetic [ 13 ]and real-world data from the UCI database repository [ 2 ]) as benchmark data streams (by assuming the data comes in a time series manner) to evaluate the system per-formance.
 streams with dramatic concept drifting. Although mining data streams with con-cept drifting has been addressed by many research efforts, we cannot find any benchmark dataset with dramatic concept changes (most existing efforts evaluate their algorithms with synthetic data). So we use the following design to simulate the scenarios in this regard. Given a dataset X , we first execute c4.5 rules [ 27 ] on X to learn a classifier C 0 , and then split X into 2  X  chunks (using proportional partitioning [ 7 ]), and randomly select one chunk to induce a base classifier C  X  ,we repeat this procedure until we get  X  base classifiers C 1 , C 2 ,... , C  X  . Normally, given a dataset X , the less the instances are used for training, the worse is the learned classifier in addressing the genuine concept of X . Therefore, from C 1 to C  X  , the classifier becomes weaker and weaker in performance (we assume it is the result of the dramatic change of the underlying concept). We use C 1 , C 2 ,... , C  X  to evaluate the proposed DCS algorithms. In the second set of experiments, DCS schemes are used to integrate classifiers learned from noisy datasets.
 SAM [ 21 ], CVM [ 29 ], DCS-LA [ 35 ] and Referee [ 25 ]. For DCS-LA, we use the overall accuracy from the k nearest neighbors to evaluate the local accuracy. Meanwhile, we set k = 10 for all experiments (as recommended by the original authors). For Referee, we use decision rules [ 27 ] as the referee, because decision rules are easier to manage. To compare the results of DCS schemes in classifying noisy data streams, we implement the Arbiter scheme proposed by [ 7 ].
 accuracy as the final result. The classification accuracy in all tables and figures below denotes the accuracy evaluated from the test sets.
 (  X  to be corrupted and mislabeled as  X  y , so does an instance of class  X  y .Weuse this method because in realistic situations, only certain types of classes are likely to be mislabeled. With this scheme, the percentage of the entire training set that is corrupted will be less than  X   X  100% because only some pairs of classes are considered problematic. In experiments below, we corrupt only one pair of classes (usually the pair of classes with the highest proportion of instances) in each dataset and only report the value in all tables and figures. 5.1 DCS from classifiers vary significantly in performance 5.1.1 Classification accuracy of base classifiers To evaluate whether base classifiers actually vary significantly in performance, we add different levels of noise into the data and generate 5 ( X  = 5 ) base classifiers. We then compare the accuracy of each base classifier with SAM, CVM and AO-DCS, and demonstrate the results in Table 1 , where the first column indicates the noise level, columns 2 to 6 represent the accuracy of each base classifier, and column 7 is the average accuracy of all base classifiers.
 getting worse. However, in a single run, there may have exceptions, Fig. 7 shows the accuracy of the base classifiers from 10 runs (with 15% noise). One can find that from C 1 to C 5 , the overall accuracy is getting worse, but in a single run, the accuracy of the base classifiers can be different from this trend. E.g., in the third run of Fig. 7 , the accuracy of C 3 is better than C 1 and C 2 . The same phenomenon has been found from some of the other datasets. From Fig. 7 , one can find that by dynamic selecting the  X  X est X  classifier, the AO-DCS outperforms all base classi-fiers in any single run.
 curacy from the classifier selection schemes (CVM and AO-DCS) is better than the combination scheme (SAM). However, when the noise becomes extremely serious, the results from SAM become closer to (or even better than) CVM and AO-DCS, as demonstrated on the eighth to tenth columns of Table 1 . There are two possible reasons: (1) when the noise level increases, each base classifier be-comes weaker. In the case that all base classifiers have a low confidence with test instances, combing results from base classifiers becomes more reasonable; and (2) as the noise increases, the constituent models have less correlated errors, which also makes using SAM become more reasonable. Comparing the results from CVM and AO-DCS, one can find that the latter outperforms the former at almost every noise level. As we analyzed before, CVM statistically selects the single best classifier by evaluating the overall performance of the base classifiers. And by integrating the proposed DCS scheme, we can explore the merit of each base classifier and improve the system performance. 5.1.2 Comparative studies on accuracy In this subsection, we compare AO-DCS with three CS algorithms (DCS-LA, Ref-eree, and CVM) and one CC scheme (SAM). We add various levels of noise into original data (and the evaluation set) to evaluate the performance of various algo-rithms in noisy environments. Tables 3 and 4 show the results from the Car and Krvskp datasets, where the first column indicates the noise level and the other columns denote the accuracy from different methods.
 degrees of accuracy, the CS schemes achieve better performances than the CC scheme (SAM) at most noise levels. When the noise level is low, all CS meth-ods outperform SAM with DCS-LA attaining the highest performance. However, with the increase of the noise level, DCS-LA receives the most dramatic decline in comparison with Referee and AO-DCS. Meanwhile, although the accuracy of Referee is usually lower than AO-DCS and DCS-LA, it was found to be the most noise level. In Table 2 , when noise increases from 0% to 40%, the performance of DCS-LA and AO-DCS drops 30.57% (from 90.65% to 62.94%) and 29.58% respectively. But, Referee receives only 21.14% decrease. Actually, our analysis in Sect. 5.2 will indicate that Referee is more sensitive to the increase of the chunk number, and DCS-LA is more sensitive to noise. 5.1.3 Comparative studies on efficiency The time complexity of DCS schemes comes from two phases: 1) evaluate the classification accuracy of base classifiers with predefined subsets, TS1; and 2) se-lect the  X  X est X  classifier for test instances, TS2. For DCS-LA, TS1 is zero, and TS2 is the time to find the nearest neighbors for the test instance. For Referee, TS1 is the time to construct the referee (we use c4.5 rules to construct referee rules in our experiments) for each base classifier, and TS2 is the time to pass all referees and select the  X  X est X  classifier. For AO-DCS, TS1 is the time to evaluate the clas-sification accuracy of each base classifier on the subsets constructed by attributes, and TS2 is the time to select the  X  X est X  classifier that has the highest accuracy with the subsets determined by the test instance. DCS-LA spends all its time on TS2, AO-DCS and Referee spend most time on TS1. Given a dataset D with M at-tributes, assume the numbers of instances in the evaluation and test sets are NZ and NY respectively, and the number of induced base classifier is L . Assume further that DCS-LA selects k nearest neighbors for each test instance, and no indexing structure is adopted to facilitate the kNN search. For each test instance, TS1 of DCS-LA is zero, and TS2 is the time to go through all NZ evaluation instances to find the k nearest neighbors and rank L base classifiers accordingly, which is O ( N Eq. (5) sifier by using instances in the evaluation set, and we assume a quadratic complexity for such a procedure (Although other better (less that quadratic) learn-ing algorithms are available, we assume the quadratic complexity for the worst case), where the complexity to construct L referees is O ( L  X  ( N Z ) 2 ) TS2 is the time to pass all referees and select the  X  X est X  classifier, which is O ( L log L ) . Therefore for NY test instances, the total complexity is denoted by Eq. (6).
 fier with the subsets constructed by attributes, where each classifier needs to go through the evaluation set for M times (where M is the number of attributes). So TS1 of AO-DCS is O ( LMN Z ) TS2 for AO-DCS is the time to select the  X  X est X  classifier that has the highest accuracy with the subsets determined by the test instance, which is O ( M log L ) For NY test instances the total complexity of AO-DCS is denoted by Eq. (7).
 plexity of AO-DCS is certainly the lowest one.
 analysis. Table 5 shows the execution times of these three schemes from the Mushroom dataset , where we present the actual execution time (in seconds) of each method and their ratio values (We used a PC with Intel Pentium 4 with 2 GHz speed and 512 MB memory). In Table 5 , the first column indicates the per-centage of the size of the evaluation set in comparison with the size of the whole dataset, and TS is the sum of TS1 and TS2. RatioA and RatioB are given in Eq. (8). proves the system performance in terms of time efficiency. When the size of the evaluation set increases, both DCS-LA and Referee suffer from spending a lot of time on finding the nearest neighborhood or inducing referee rules from the eval-uation set, which is obviously a nonlinear increase with the size of the evaluation set. But increase the size of the evaluation set has less influence on AO-DCS. 5.2 DCS in classifying noisy data datasets 5.2.1 Experiments with noise levels To evaluate the performances of DCS schemes in classifying partition-based noisy data datasets, we equally split the dataset into 9 non-overlapping chunks (using proportional sampling). We use c4.5rules to construct a base classifier from each chunk, and apply DCS schemes to integrate these base classifiers. In addition, we also add certain levels of class noise into the original data. The experimental re-sults are shown in Tables 6  X  8 , where the first column indicates the noise level, the second to seventh columns represent the classification accuracy of each scheme, and the eighth and ninth columns give the average and variance of the 9 base classifiers X  accuracy.
 schemes with SAM, CVM and Arbitrator, we can find that DCS schemes receive relatively better performances at various noise levels. If we compare the second and eighth columns, we can find that SAM X  X  accuracy is higher than the average accuracy of all base classifiers, where the improvement from SAM can be 2% to 10% compared to the average accuracy of all base classifiers. This confirms that SAM works surprisingly (or embarrassingly) well in many circumstances. From the ninth column, one can find that when the noise level increases, the variance of the base classifiers X  accuracy becomes large, which indicates that the perfor-mance of the base classifier varies significantly. Usually, if the variance of the base classifiers X  accuracy becomes significant, DCS can receive relatively large improvement.
 sensitive to noise. When the noise level increases, the performance of DCS-LA decreases dramatically. The reason is that DCS-LA uses the nearest neighbors to evaluate base classifiers, and in high noise-level environments, the selected neigh-bors may be seriously corrupted by noise. Both Referee and AO-DCS use statisti-cal information of the evaluation set, so noise has less negative impact on them. umn), one can find that Arbiter generally outperforms SAM in most situations. However, when the noise level reaches a relatively high level (30%  X  40%), the improvement from Arbiter disappears. The reason is that when the noise level goes higher, more noisy instances are used to train the Arbiter. Consequently, the ability of the learned Arbiter becomes weaker. 5.2.2 Experiments with variable number of chunks In this subsection, we address the impact of the number of chunks on the perfor-mance of the proposed algorithms. We partition the dataset into different number of chunks (from 3 to up to 63). Meanwhile, we run the experiments at two noise levels (0% and 25%), and show the results in Figs. 9 to 14 .FromFigs. 9 to 14 ,we can find that when the number of chunks increases, the overall accuracy from all schemes decreases. However, in a certain range, the chunk number may have less impact on the classification accuracy (and the increase of the number of chunks can even increase the classification accuracy). This phenomenon might come from the intrinsic characteristics of each dataset, e.g., the level of redundancy in the dataset.
 However, in noisy environments, DCS-LA receives less improvement, especially when the number of chunks is relatively large. As shown in Fig. 9 b, where the noise level is 25%, while the number of chunks increases, the performance of DCS-LA receives less improvement than AO-DCS. The same phenomenon has been found from most other datasets (except from Krvskp, in Fig. 11 b, where Referee receives the highest classification accuracy).
 number of chunks. Take Fig. 9 as an example. When the chunk number increases from 3 to 63, the performance of Referee drops 19.79% (from 90.24% to 72.38%) which is the largest among all three DCS schemes (the drop of DCS-LA and AO-DCS in the same range are 3.93% and 10.17% respectively). The reason behind this phenomenon is that with the increase of the chunk number, the number of instances in each chunk is decreased. Each learned base classifier then tends to bias to only one or two attributes. Consequently, the learned referee of each classifier cannot comprehensively partition the evaluation set into small subsets to explore the domain expertise of each base classifier. Moreover, as we have mentioned in Sect. 2, Referee uses different subsets to evaluate different base classifiers (each classifier has its own referee), and without the same measurements, the selected  X  X est X  classifier might not work well.
 of chunks, the Arbiter scheme likely receives the same classification accuracy as SAM. The reason is that with the increase of the number of chunks (the number of base classifiers), the learned Arbiter will have less influence on the classification accuracy, because the arbitration rules take effect only if the base classifiers cannot have a majority classification. Meanwhile, the existence of noise could also be fatal to Arbiter, because the arbiter is learned from uncertain data collected from different chunks. In noisy datasets, there is no doubt that uncertain data collected from different chunks usually contain significant noise. Then, the learned arbiter has a very limited ability to improve the system performance. (0% and 25%). One can go through all figures to get detailed comparisons. Clearly, these results indicate that DCS acquire much better performance than simple clas-sifier combining, especially when a large number of base classifiers is adopted. This conclusion supports our initial motivation that each base classifier has its own merit that deserves exploitation, and finding the domain of expertise of each classifier supplies a new way to improve the system performance.
 mance on 6 datasets, where (a) is tested on noise-free data; and (b) is tested with 25% class noise. In Figs. 9 to 14 ,the x -axis denotes the number of chunks and the y -axis represents the classification accuracy. 6 Conclusions Traditional data mining algorithms are challenged by two most important fea-tures of data streams: huge volumes of data and the underlying concept drifting. These two challenges raise the need for incorporating classifier ensemble in exist-ing stream mining efforts. This common practice, however, ignores the fact that the base classifier learned from a portion of a data stream carries two important features: (1) weak (even conflictive with others) in overall performance; but (2) still reliable in a specific domain. This fact becomes especially clear if the data stream suffers from dramatic concept drifting or a significant amount of noise. Consequently, instead of adopting any combination scheme, we have presented a new DCS algorithm that selects the  X  X est X  classifier once a time for each test instance in the data stream. We use each attribute to partition the evaluation set into disjoint subsets. We evaluate the classification accuracy of each base classi-fier on each subset. This accuracy indicates the ability of the base classifiers in classifying the instances characterized by each attribute, and hopefully, helps us explore the merit of each base classifier. Given a test instance, its attribute val-ues will determine the subsets that have been constructed by similar instances in the evaluation set. The base classifier that has the highest accuracy with all these determined subsets is selected to classify the test instance.
 combination schemes, the DCS algorithms can possibly acquire more accuracy improvement, especially when the performances of the base classifiers vary sig-nificantly. When the real-world data suffers from dramatic concept drifting or a certain level of noise, AO-DCS turns to be a better choice in integrating multiple classifiers to enhance the system performance.
 References
