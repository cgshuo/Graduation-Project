
Department of Computer Engineering, Sejong University, Seoul, Korea Department of Computer Science, Chungbuk National University, Cheongju, Korea 1. Introduction
Mining of meaningful patterns has become an important research issue in the data-mining field. In many cases, the item in a transaction can have different levels of importance (weight). For instance, in a real market business, the prices (profits) of products are as important as frequencies. Expensive products may reflect large portions of overall revenue, even though they do not have high frequencies. In other application domains, such as financial data analysis, telecommunications industry, biomedical and retail industries, weighted frequent pattern (WFP) mining can be used to discover knowledge. The focus of WFP mining concerns the anti-monotone property, since the anti-monotone property is usually broken when different weights are applied to items between patterns. That is, although a pattern is weighted infrequent, its super pattern can be weighted frequent. Weighted association rule mining (WARM) [23] used a weighted support and developed a weighted downward closure property to resolve this issue. However, WARM is based on that Apriori algorithm [1] that needs all candidate generations and tests with multiple scans of the original database. Thus, weighted frequent pattern mining algorithms based on the pattern growth approach [10] have been developed. They reduce the number of candidate patterns and scan the original database only twice. In the algorithms, the maximum weight of items of (conditional) database is used to compute the maximum weighted support that can be used without breaking the anti-monotone property. In addition to WFP mining, correlated weight frequent pattern mining [30] is also a useful research issue in data mining. In weighted interesting pattern (WIP) mining [30], a new measure, w-confidence (weight-confidence ) was proposed to mine highly correlated weight patterns. W-confidence prevents the generation of patterns with substantially different weight levels. For example, in market basket data, patterns such as {silver ring, bronze necklace} can be pruned, but correlated weight patterns such as {gold ring, gold necklace} are generated by WIP mining. Furthermore, Correlated weight frequent pattern (or WIP) mining is also useful in real application domains, such as financial analysis, retail industry, stock market, trend prediction and telecommunications.

The concept of MFP (Maximal Frequent Pattern) was proposed to avoid mining very large frequent patterns [7]. A frequent pattern is denoted by MFP if it has no super pattern that is frequent. Thus, mining frequent patterns can be reduced to mining a  X  X order X  in the pattern lattice. The entire patterns above the border are infrequent while the others, below the border, are all frequent. However, no methods have been developed to mine maximal correlated weight frequent patterns (MCWP). Therefore, in this paper, we propose two MCWP mining algorithms, termed MCWP(WA) (Weight Ascending order) and MCWP(SD) (Support Descending order). MCWP(WA) takes advantage of candidate pattern generation by obtaining the highest weighted item in the bottom of the FP (Frequent Pattern)-tree based on weight ascending order. However, it has poor prefix sharing; thus, too many conditional databases are created during the mining operation. Thus, we propose the second algorithm, MCWP (SD, Maximal Correlated Weight frequent Pattern mining based on weight ascending order) to resolve this issue. Our proposed second algorithm, MCWP(SD) uses the support descending tree structure, so it speeds up the prefix tree and conditional tree construction during the mining operation. However, MCWP(SD) cannot obtain the highest weighted item in the bottom of the FP-tree based on support descending order. The highest (maximum) weight of a conditional database is needed to prune non-candidate correlated weight pat-terns. MCWP(WA) can calculate it directly, but MCWP(SD) does not. Thus, MCWP(SD) cannot prune non-candidate correlated weight patterns directly, so the pruning process of MCWP(SD) is not effective. For this reason, we suggest a technique that uses additional pruning conditions to prune non-candidate correlated weight patterns to resolve this issue in MCWP(SD). The main contributions of this paper are as follows:  X  Suggestion of new pruning techniques with the support descending ordered tree structure by the  X  Presentation of the maximal correlated weight frequent pattern mining.  X  Illustration of how to push w-confidence deeply into the pattern growth approach in maximal  X  Development of two MCWP algorithms and extensive experimental study.
 The remainder of this paper is organized as follows. Background and related work are presented in Section 2. In Section 3, we develop our proposed maximal correlated weight frequent pattern (MCWP) mining processes. In Section 4, we describe our two MCWP algorithms. Section 5 reveals experimental results. Finally, Section 6 concludes the paper. 2. Background and related work
Let I = { i 1 ,i 2 ,...,i n } be a unique set of items. A transaction database, TDB is a set of transac-tions where each transaction includes a unique transaction identifier, tid, and a set of items, denoted as pattern in the transaction database. The problem of frequent pattern mining is to find the complete set of patterns satisfying a minimum support in the transaction database. Frequent pattern mining usually uses the anti-monotone property [1] to prune infrequent patterns. That is, if a pattern is an infrequent pattern, all super patterns of the pattern have to be infrequent patterns. The frequent pattern mining that uses the anti-monotone property can prune infrequent patterns rapidly. Several efficient pattern mining algorithms, such as weighted frequent pattern mining [31], constraint-based pattern mining [11], closed frequent pattern mining [25,26] and other pattern mining approaches without using support thresh-olds [25], have been developed. These approaches reduce the number of patterns but correlated patterns cannot be discovered in the result sets. Interesting measures [13,17], such as all-confidence [13], and h-confidence [39], have been proposed to discover correlated patterns. However, these measures only focus on the supports of patterns, so only correlated weight frequent pattern mining [30] could mine correlated patterns in terms of the importance (weight). 2.1. Closed and maximal frequent pattern mining
The supports of all frequent patterns are needed to generate association rules. However, when very long patterns are present in the data, it is often impractical to generate the entire set of frequent patterns or closed patterns. A frequent pattern is a closed frequent pattern (CFP) if it has no proper super pattern (superset) with the same support. The entire set of original frequent patterns can be recovered by its corresponding set of CFPs; so representing the set of frequent patterns by its corresponding set of CFPs is lossless compression. However, the rate of compression is quite restricted, since the supports of most similar patterns may be a little different. A pattern X is a maximal frequent pattern (MFP) if X is frequent and every proper super pattern of X is infrequent. In brief, we state that X is maximal. The set of maximal frequent patterns is the smallest possible expression of the data that can still be used to extract the set of frequent patterns. Once the set of frequent patterns is generated, the support information can be easily recomputed from the transaction database.
 CLOSET [18] was the first CFP mining algorithm based on the pattern growth method.
 CLOSET + [26] is an improved version that applies efficient pruning techniques. FPClose* [9] is a suggested CFP mining algorithm that outperforms other CFP algorithms. FPClose* uses an array-based structure to mine frequent patterns and multiple conditional FP-trees to perform subset checking. Bur-dick [4] proposed a novel MFP mining algorithm, MAFIA (MAximal Frequent Itemset Algorithms). This algorithm used a vertical bitmap representation, where the count of a pattern is based on the col-umn in the bitmap. The FPmax* [9] algorithm uses a horizontal format. From the performance test [9], FPmax* outperforms MAFIA for many cases. It is also a depth-first MFP mining algorithm, but the algorithm uses the FP-tree structure based on FP-growth. MFI (Maximal Frequent Itemset)-tree that re-sembles a FP-tree is used to check the subsets in FPmax*. It uses the FP-array technique to reduce the FP-tree traversals. Only two database scans (FP-tree traversals) are needed to construct a new condi-tional FP-tree. The FP-array allows the first scan to be omitted. In addition to those algorithms, there are numerous approaches such as mining MFPs [19,20,38,40,41], sequential MFPs [6,14,15], MFPs over data streams [16,21,29], and so on. 2.2. Weighted frequent pattern mining
Weighted frequent pattern mining aims to find the complete set of patterns satisfying a support and a weight constraint in the transaction database. Weights mean importance for each object in the real world. For example, the weights can indicate profits or prices of certain items. Weighted frequent pat-tern mining has an issue that the anti-monotone property is usually broken when different weights are applied to the items and patterns. That is, the super pattern of a weighted infrequent pattern can be weighted frequent. Weighted association rule mining (WARM) [23] solved the problem of breaking the anti-monotone property by using a weighted support and developing a weighted downward closure property. However, a weighted support of any pattern, AB in WARM is the ratio of the weight of the transactions containing both A and B to the weight of all transactions, so WARM does not consider the support measure. The main limitation of the above algorithm is that they are based on the Apriori algorithm [1] that requires all candidate generation and examining with multiple scans of the original transaction database. Thus, weighted frequent pattern mining algorithms based on the pattern growth method have been proposed [2,8,12,22,27,33,34]. Moreover, lots of mining methods applied in various areas have been studied, such as mining weighted sequential patterns [5,32,35], weighted patterns over data streams [3,24,28], approximate weighted patterns [36,37], etc. Weighted frequent pattern mining can decrease the number of patterns but correlation patterns are not discovered in the resulting patterns.
Weighted frequent pattern mining not only reduces search space but also mines more patterns that are important. A weight of an item is a non-negative real number assigned to reflect the importance of each item in the transaction database. Given a set of items, I = { i 1 ,i 2 ,...,i n } , the weight of a pattern is formally defined as follows: That is, Weight ( P ) means an average weight value among all of the weights of items contained in P .A weighted support of a pattern is defined as the resultant value of multiplying the pattern X  X  support with the average weight of the pattern. That is, given a pattern P , the weighted support is defined as: WSupport ( P ) =
Weight ( P )* Support ( P ). A pattern is termed a weighted frequent pattern, if the weighted support of the pattern is not smaller than a minimum support threshold. Table 1 shows an example of a retail database. In Table 1, the variation of items X  prices is so large that the prices cannot be directly used as weights. Therefore, the normalization process is needed to adjust differences among data from varying sources and to create a common basis for comparison. According to the normalization process, the final weights of items can be determined within a specific weight range (WR). From this example, weights of items are given between 0.3 and 0.6 and the maximum weight of items is the weight (0.6) of the item  X  X heese X . As shown in Table 1, attribute values, such as prices (profits) of items in market basket data, are used as a weight factor and the prices of items are normalized within a specific weight range. Based on the definition, items, and patterns with the items have their own weights.
 Definition 1. (Maximum weight (MaxW)). Maximum weight (MaxW) is defined as the value of the maximum weight of items of the transaction database or the value of the maximum weight of items within a conditional pattern related to a conditional database.

For example, MaxW of TDB in Table 1 is 0.6. In the table, normalized weight values are to convert items X  prices into a fixed and small weight range reflecting characteristics of the real prices. Generally, real prices of items can vary considerably from extremely small to enormously large values, and thus, multiplying any support by a cer tain real price can be too large to express the result. Therefore, we need to normalize these prices so as to prevent this problem. Thus, if a weight range is from 0.3 to 0.6 as shown in the Table 1, the cheese with the highest price has 0.6 as a normalized weight while the milk and yogurt with the lowest price have 0.3. Normalized weights of the others are proportional to their own prices. If a conditional database (CDB( X  X gg X )) for a conditional pattern  X  X gg X  is {(bread, milk), (salt), (biscuit)}, MaxW of CDB( X  X gg X ) is 0.43. The maximum (highest) weight is used to compute the maximum weighted support in weighted frequent pattern mining based on the pattern growth method. It can be used without breaking the anti-monotone property. 2.3. Correlated weight frequent pattern mining Definition 2. (Weight confidence (w-confidence)). The weight confidence of a pattern P = { i 1 ,i 2 , ...,i m } , denoted as Wconf ( P ), is a measure that reflects the overall weight affinity among items within the pattern. This measure is defined as Wconf ( P ) =0 . 2 / 0 . 8=0 . 25 .
 Definition 3. (Correlated weight frequent pattern). A pattern is a correlated weight pattern if the weight confidence of a pattern is not smaller than a minimum weight confidence (min_wconf). The formal defi-pattern P is a correlated weight pattern if and only if | P | &gt; 0and wconf ( P ) min_wconf ( | P | indicates the length of P or the number of items in P ). Moreover, if P  X  X  weighted support is larger than or equal to a given minimum support, P is defined as a correlated weighted frequent pattern.

The WIP (Weighted interesting pattern) mining algorithm [30] introduced a new measure, weight-confidence, to discover highly correlated weight frequent patterns.
 Property 1. Let, P be any pattern and P be a certain super pattern of P . Then, it is always true that Wconf ( P ) Wconf ( P ) since the measure, Wconf satisfies the anti-monotone property.
 Proof. When the pattern, P is expanded as P , a new item with a weight is added, where we can consider the addition as three cases. First, when the weight becomes a new minimum value among the weights of P , Wconf ( P )islargerthan Wconf ( P ) since a numerator of the equation in the Definition 2 becomes lower. Second, in case the weight is a middle value between the previous minimum and max-imum weights, Wconf ( P ) is equal to Wconf ( P ) . Third, when the weight becomes a new maximum value, Wconf ( P )isalsolargerthan Wconf ( P ) since a denominator of the equation becomes higher. Considering these cases, we can know that the relation, Wconf ( P ) Wconf ( P ) is always true, and therefore, this satisfies the anti-monotone property. 2.4. Motivating example
Let us show a motivating example for our work in a medical history database. Given all items (at-tributes) { X  X ge &lt; 50 X ,  X 50 age &lt; 70 X ,  X 70 age X ,  X  X moking = never X ,  X  X moking = past X ,  X  X mok-ing = current X ,  X  X verweight = no X ,  X  X verweight = yes X ,  X  X lcohol intake = never X ,  X  X lcohol intake = past X ,  X  X lcohol intake = current X ,  X  X lood pressure = normal X ,  X  X lood pressure = low X ,  X  X lood pressure = high X  X  in medical history data, each transaction in the database is a medical history data set of a prospective patient. The items  X 70 age X ,  X  X moking = current X ,  X  X verweight = yes X ,  X  X lcohol intake = current X  and  X  X lood pressure = high X  are more important (critical causes of diseases) than others; thus, they have high weights. The items  X  X ge &lt; 50 X ,  X  X moking = never X ,  X  X verweight = no X ,  X  X lcohol intake = never X  and  X  X lood pressure = normal X  have less important probabilities of being a bad influ-ence; thus, they have relatively low weights. The remaining items,  X 50 age &lt; 70 X ,  X  X moking = past X ,  X  X lcohol intake = past X  and  X  X lood pressure = low X  have normal (middle) weights. A pattern { X 70 age X ,  X  X moking = current X ,  X  X verweight = yes X ,  X  X lcohol intake = current X ,  X  X lood pressure = high X  X  is very meaningful, although it has a low frequency from the database. (We assume that the database has much historical medical data of non-patients.) However, since the entire items of the pattern have high weights, the weight ed support (frequency) of the pattern can be higher than t he minimum support threshold. Thus, the pattern is a weighted frequent pattern. Furthermore, it is important that all weights of items of the pattern are high, without exception. That is, we can say that the pattern is a highly cor-related weight pattern that has similar high weights of items. Consequently, the pattern can be a very meaningful pattern, since weights of its items reflect characteristics of patients that are vulnerable to illness. In addition, frequent patterns that contain low or middle weights of items can also be a result that clusters meaningfully medical history data of patients [17]. If those patterns are also maximal weighted frequent patterns, those compact patterns represent all those sub-patterns. Thus, when we find meaning-ful patterns and need to analyze the characteristic of each pattern, MCWP, which is maximal weighted frequent and correlated weight frequent, can be gainfully utilized. 3. MCWP: Maximal correlated weight frequent pattern mining
In this section, we study the problem of correlated maximal pattern mining with similar (or dissimilar) weight levels. The main approach of MCWP is to detect correlated maximal patterns with high corre-lated weight frequent patterns by pushing the w-confidence (weight-confidence) into the pattern growth method. W-confidence satisfies the anti-monotone property depending on the Property 1. Through the algorithms proposed in this paper, we can efficiently prune weakly correlated patterns in advance, where there is no risk by the pruning operations such as pattern losses and errors since the operations are performed on the basis of that property. 3.1. Preliminaries
MCWP is based on the pattern growth method so FP (Frequent Pattern)-trees are recursively cre-ated. In MCWP(WA, Weight Ascending order), an ascending weight ordered prefix tree and a bottom-up traversal strategy are used to construct FP-trees, since the pattern growth approach based on as-cending weight order guarantees the maintenance of the anti-monotone property of w-confidence. In MCWP(WA), given a conditional pattern P ,andapattern Q in the conditional database with the con-ditional pattern P , weight(P) is not smaller than the weight of a pattern Q within a transaction in the conditional database. Therefore, MaxW(P) is greater than or equal to the weight of any item within the conditional database. Thus, MaxW can be easily calculated by only checking weights of items within the conditional pattern, without examining weights of all items of conditional databases in MCWP(WA). However, in the maximal weighted frequent pattern mining based on pattern growth method, the poor prefix sharing of ascending weight ordered trees places additional strain on the entire mining perfor-mance. Thus, the maximal weighted frequent pattern (MWFP) mining based on pattern growth method with weight ascending order is slower than MWFP mining with support descending order in many cases. Thus, we present MCWP with Support Descending order, MCWP(SD). However, MCWP(SD) cannot obtain the highest weighted item at the bottom of the FP-tree based on support descending order. The highest (maximum) weight of a conditional database is needed to prune non-candidate correlated weight patterns. MCWP(WA) can calculate it directly, but MCWP(SD) does not. Thus, MCWP(SD) cannot prune non-candidate correlated weight patterns directly, so the pruning process of MCWP(SD) is not effective. Therefore, we suggest a technique that uses additional pruning conditions for pruning non-candidate correlated weight patterns to resolve this problem in MCWP(SD).
 Definition 4. (Maximal correlated weight frequent pattern, MCWP). A pattern is a maximal correlated weight frequent pattern if the weight confidence of a weighted frequent pattern is not smaller than a minimum weight confidence ( min_wconf ) and any super pattern of the pattern does not become a correlated weight frequent one.
 Property 2. If the w-confidence of a pattern P is less than a minimum w-confidence, all super patterns of P is also smaller than min_wconf . That is, the anti-monotone property of w-confidence is satisfied. Proof. Depending on the Property 1 and its proof, we already know that Wconf ( P ) Wconf ( P )is true with respect to a pattern, P and a super pattern of P , P . Therefore, if Wconf ( P ) is lower than min_wconf , Wconf ( P ) is naturally less than the min_wconf .
 Lemma 1. If the w-confidence of a given conditional pattern P is less than a minimum w-confidence, any super pattern of P is removed.
 Proof. Given a pattern P and a minimum w-confidence ( min_wconf ), 0.6, if the minimum weight of items of P is 0.4 and the maximum weight of that is 0.7, Wconf ( P ) (0.4/0.7 0.571) is less than min_wconf (0.6). In Fig. 1, new items added into P have the weight either smaller than the minimum weight of P , or between the minimum and maximum weights of P , or higher than the maximum value of P . In the patterns, A, B, C, and D adding new item(s) to P ,their Wconf values are also smaller than min_wconf as well as Wconf ( P ). Consequently, if the w-confidence of a conditional pattern P is less than min_wconf , the pattern growth of P is omitted since all of the super patterns of P always become meaningless ones in any case.

Givenanitem x , all patterns that contain x and at least an item with weight less than t  X  weight( x ) (for 0 &lt;t&lt; 1) are cross-weight patterns and the patterns X  w-confidences are less than t . By Lemma 1, the cross-weight patterns can be directly pruned without calculating the w-confidence. 3.2. Mining process in MCWP(WA)
When a prefix tree is constructed for a particular item in the FP-tree structure based algorithm, then all branches prefixing that item are taken with the frequency value of that item. Then, a conditional FP-tree is constructed from the prefix tree by removing the nodes, including infrequent items. MCWP(WA) is also a FP-tree structure based algorithm. However, the weighted frequency of a pattern does not have the anti-monotone property. To apply this property we must use the maximum weight, denoted by MaxW . First, the global maximum weight is the maximum weight of all the items in the entire transaction database. For example, in Table 2, the item  X  X  X  has the global MaxW of 0.8. The local MaxW is needed when we are performing the mining operation for a particular item X  X  conditional database. In contrast with the global MaxW , local (or conditional) MaxW is set as the maximum weights which participate in the conditional database and tree. Therefore, the local MaxW can be lower than the global MaxW since an item with the weight corresponding to the global MaxW can be excluded in the current condi-tional mining steps. Thus, MaxW means the global MaxW when the current mining step is processing the global database and tree, or it indicates the local MaxW when the mining algorithm is performing its operations in the conditional database and tree. Since MCWP(WA) sorts items in weight ascending order, MCWP(WA) has an advantage in a bottom-up operation. For example, after mining the maximal correlated weight frequent patterns for item  X  X  X , for the next item  X  X  X , item  X  X  X  will never reoccur in its prefix and conditional trees. Thus, now we can easily assume that local MaxW becomes the first added prefix item. MaxW (global) is used as the maximum weight of items within the transaction database in the first step. After the global FP-tree is constructed, we know that super patterns of a weighted infrequent pattern can be weighted frequent. If pattern P  X  X  maximum weighted support ( support(P)  X  MaxW (local)) is less than the minimum support and it is weighted infrequent, all super pattern of the pattern P is also a weighted infrequent pattern so it can be pruned. Nevertheless, the maximum weighted support ( sup-port(P)  X  MaxW (local)) is an approximate value. Therefore, in the final step, we should check if the pattern P satisfies the condition ( support(P)  X  weight(P) min_sup ) to prune weighted infrequent pat-terns that satisfy the condition ( support(P)  X  MaxW (local) min_sup ). In the weight ascending ordered FP-tree, the local MaxW is the weight of the lowest level node item of the path, so that the local MaxW calculation for each item is concise in MCWP(WA). MCWP(WA) uses the pruning conditions sequen-tially to mine maximal correlated weight frequent patterns. For example, given a transaction database in Table 2(a), weights of items from Table 2(b), min_sup of 2 and min_wconf of 0.65, after the first scan
Weighted infrequent items are removed and the remainder is sorted in weight ascending order before items in each transaction are inserted in the global FP-tree. Pruning by the weighted support constraint, item  X  X  X  is pruned, since the weighted support of multiplying the support of item  X  X  X  with the global MaxW(0.8) is less than the minimum support (2). Figure 2 presents the global FP-tree and the related header table (the node-links of the tree are omitted for concise presentation). MCWP(WA) mines the maximal correlated weight frequent patterns from the FP-tree using the divide-and-conquer approach for the mining process after the global FP-tree is constructed from the transaction database.
MCWP(WA) divides mining the global FP-tree into mining smaller FP-trees. From the global FP-tree, MCWP(WA) mines (1) the patterns including item  X  X  X  that has the highest weight, (2) the patterns containing  X  X  X  but no  X  X  X , (3) the patterns including  X  X  X  but no  X  X  X  or  X  X  X , (4) the patterns containing  X  X  X  but no  X  X  X ,  X  X  X  or  X  X  X , (5) the pattern containing  X  X  X  but no  X  X  X ,  X  X  X ,  X  X  X , or  X  X  X , (6) the pattern containing  X  X  X  but no  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  or  X  X  X , (7) the pattern containing  X  X  X  but no  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  or  X  X  X  based on a bottom-up traversal strategy. We construct a conditional database by starting from the head of the item  X  X  X  and following the node-link of the item  X  X  X  for the node with item  X  X  X . The conditional database for the conditional pattern  X  X :5 X  (pattern:support) includes three transactions: {(d:1), (fea:3), (dgh:1)}. The local MaxW of conditional pattern  X  X  X  is 0.8. In MCWP(WA), the items  X  X :2 X ,  X  X :1 X  and  X  X :1 X  are pruned by the weighted supports constraints, since the weighted supports (1.6, 0.8 and 0.8) multiply ing the items supports (2, 1, 1) by the local MaxW (0.8) of the conditional pattern are less than the minimum support (2). In addition, the local item  X  X  X  is pruned by the w-confidence, since the w-confidence (0.625) of the candidate pattern  X  X a X  is less than the min_wconf (0.65). The candidate pattern from the conditional pattern  X  X  X  with the local items  X  X  X  and  X  X  X  are {(bfe)}. The corresponding FP-tree is shown in Fig. 3(a). For the node with the item  X  X  X , MCWP(WA) derives a conditional pattern (d:5) and three branches in the FP-tree: {(gea:1), (gh:2), (h:1)}, where the local MaxW becomes 0.79. Local items  X  X :1 X  and  X  X :1 X  are also pruned by the weighted support constraints. After pruning weighted non-correlated patterns in the conditional database, the conditional FP-tree for the conditional pattern  X  X :5 X  is constructed, as shown in Fig. 3(b). The conditional database for the conditional pattern  X  X :3 X  contains a transaction: (ea:3) for the node with item  X  X  X . The local MaxW is 0.65, and therefore, no item is pruned by weighted non-correlated patterns pruning as shown in Fig. 3(c). In this way, we can generate local conditional FP-trees from the global FP-tree and mine correlated weight frequent patterns from them recursively. The FP-trees for the conditional patterns  X  X :3 X ,  X  X :3 X , and  X  X :3 X  are empty and not shown in Fig. 3. Recall the candidate pattern  X  X fea X  is pruned by the anti-monotone property of the w-confidence. That is, if a pattern  X  X a X  is not a correlated weight frequent pattern, the super pattern  X  X fea X  of the pattern is also not a correlated weight frequent pattern and is pruned. 3.3. Mining process in MCWP(SD)
We design MCWP(SD) based on the support descending order to reduce the huge number of nodes in the prefixes and conditional FP-trees in MCWP(WA). MCWP(SD) uses the same technique to generate prefixes and conditional FP-trees in the mining process. As it is not based on the weight ascending order, MaxW could be anywhere for a particular item, so conditional patterns cannot have MaxW in MCWP(SD). Thus, w-confidences of patterns must be calculated when MaxW or MinW is determined. MaxW or MinW must be calculated first to prune weakly correlated patterns in MCWP(SD). When the conditional pattern has the MaxW , any item that has a weight less than ( MaxW  X  min_wconf ) can be pruned by the cross weight property ( MaxW &gt; 0). Thus if the conditional pattern has the MinW , any item that has a weight less than ( min_wconf / MinW ) can be pruned ( MinW &gt; 0). Conversely, if the conditional pattern has neither MaxW nor MinW , no w-confidence of an item in the conditional pattern base can be calculated while maintaining the w-confidence property. Therefore, in the final step, we should check if the pattern P satisfies the condition ( w-confidence (P) min_wconf ). If P does not satisfy the condition, it is pruned finally.
 Lemma 2. Given a conditional pattern P in MCWP(SD) and a pattern Q in the conditional database with P , weight(P) can be less than the weight of a pattern Q within a transaction in the conditional database, unlike in MCWP(WA).
 Proof. Our approaches use modified FP-trees as a compression technique. Those compute global (or local) weighted frequent items by scanning a transaction database (or conditional databases). In MCWP(WA), FP-tree are constructed by an ascending weight ordered prefix tree; so weights of lower level nodes are always not smaller than those of upper level nodes in each branch of the FP-trees. The FP-trees are traversed using a bottom-up strategy in MCWP(WA) so the conditional pattern is first selected by the direction from the leaf node with the highest weight to upper level of nodes. This guarantees that the weight of a conditional pattern is always larger than or equal to the weight of any item of a transaction within a conditional database in MCWP(WA). In contrast to MCWP(WA), FP-trees of MCWP(SD) are constructed in a support descending ordered prefix tree; so weights of nodes are not sorted. Weights of lower level nodes are not always greater than or equal to those of upper level nodes in each branch of the FP-trees. Finally, this does not guarantee that the weight of a conditional pattern is always not smaller than the weight of any item of a transaction within a conditional database in MCWP(SD). Consider the example of Fig. 4. A global FP-tree is constructed in support descending order. Then, we suppose that a conditional pattern P is set to {milk, yogurt}. A pattern with P is {milk, yogurt, bread, biscuit} (de-noted as Q ) , as shown in Fig. 4. Each item in Q is sorted in support descendi ng order of P X  X  conditional database. Thus, the conditional pattern P does not have MaxW of Q (0.34).
 Lemma 3. In MCWP(SD), given a minimum w-confidence, if the w-confidence has the cross-weight property, for any cross-weight pattern P with regard to the minimum w-confidence, the value of the w-confidence is less than the minimum w-confidence.
 Proof. A cross-weight pattern P contains at least two items x and y , such that weight ({ x }) / weight ({ y }) &lt;t where 0 &lt;t&lt; 1. If P contains an item z that has smaller weight than the weight t . If the minimum w-confidence is less or equal to t , the value of the w-confidence is always less than the minimum w-confidence when the cross-weight property is considered.
 The anti-monotone property and the cross-weight property can be used in the pruning strategy in MCWP. Given a weight list for five items &lt; a:0.2, b:0.6, c:0.4, d:0.3, e:0.5 &gt; ,and min_wconf of 0.8, the w-confidence (0.67) of a pattern  X  X c X  is less than min_wconf , so the pattern is pruned. From the anti-monotone property, we can prune the super patterns, such as  X  X cd X  and  X  X ce X , since these patterns have one subset  X  X c X  that is not a correlated weight frequent pattern. Meanwhile, we can find an item  X  X  X  with weight ( X  X  X ) = 0.3 &lt; weight( X  X  X )  X  0.8 = 0.4. If we split the item list into two group  X  X b X  and  X  X de X , any pattern containing items from both groups is the cross-weight pattern with min_wconf for cross-weight patterns. In this example, the cross-weight patterns, such as  X  X c,  X  X d X ,  X  X e X ,  X  X c X ,  X  X d X , and  X  X e X , have to be generated as candidate patterns and pruned later by computing the w-confidence values of the patterns without applying the cross-weight property. Note that these patterns are not pruned by the anti-monotone property.

MCWP(SD) does not sequentially use all of the pruning conditions to mine maximal correlated weight frequent patterns. The pruning methods by weighed infrequency and non-maximality are used sequen-tially in MCWP(SD). However, w-confidence pruning is only used in the final step, in which MCWP(SD) checks the exactly weighted frequency of patterns. The final step w-confidence pruning is not efficient, since the pruning technique no longer reduces the scales of conditional databases. Instead, we suggest the following pruning condition for MCWP(SD).
 Pruning condition: Correlated weight frequent pattern (w-confidence min_wconf ) with its con-ditional pattern has MaxW or MinW . Given a set of items I = { i 1 ,i 2 ,...,i n } , a pattern P  X  I is not a correlated weight frequent pattern, if the conditional pattern P has MaxW (or MinW )and ( weight ( P ) / MaxW ) min_wconf (or ( MinW / weight (P)) min_wconf ) for the conditional pattern bases.

This new pruning condition is restricted, since pruning is only usable when the conditional patterns have MaxW or MinW of those conditional pattern bases. In Section 5, performance evaluations show how the restricted pruning condition enhances the performance of MCWP(SD).

For instance, given a transaction database in Table 2(a), weights of items from Table 2(b), min_sup of 2 and min_wconf of 0.65, after the first scan of the transaction database, the frequent list is &lt; a:4, h:0.57 &gt; ,andthe MaxW is set to 0.8. By the pruning condition, the weighted infrequent item,  X  X :2 X  is Figure 5 shows how the global header table and its FP-tree are constructed. Then, MCWP(SD) mines the maximal correlated weight frequent pattern from the global FP-tree by the divide-and-conquer approach. MCWP(SD) also divides mining the global FP-tree into mining smaller FP-trees. The conditional FP-trees of  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X  are constructed from the global FP-tree. The conditional database for the conditional pattern  X  X :3 X  includes three transactions: {(gdb:1), (gd:3), (d:1)}. The MaxW of conditional pattern  X  X  X  is 0.8, the weight of item  X  X  X . Item  X  X :2 X  and  X  X :1 X  are pruned by the weighted supports constraints, since the weighted supports (1.6 and 0.8) of multiplying the items supports (2, 1) by MaxW (0.8) of the conditional pattern are less than the minimum support (2). Since the MaxW of conditional pattern  X  X  X  is not in the conditional pattern, MCWP(SD) cannot use this MaxW for pruning the non-correlated weight frequent pattern. However, we can obtain MinW of 0.57 from the conditional pattern  X  X  X . As the ratio of MinW / MaxW (0.57/0.8) is not smaller than min_wconf , any item in {h} X  X  conditional pattern base is not pruned by the cross weight property. The conditional FP-tree of  X  X  X  is constructed; it has only a single path (d:3), as shown in Fig. 6(a), since other items are not weighted frequent items. A candidate pattern (hd:3) is generated from FP-tree. A pattern  X  X d:3 X  does not have any super pattern that is a maximal correlated weight frequent pattern, so  X  X d:3 X  is a maximal pattern. Then, MCWP(SD) checks real weighted frequency and real w-confidence of pattern  X  X d:3 X  in the final step. The weighted support of  X  X d:3 X  is ((0.57 + 0.79) / 2  X  3 = 2.04), which is not smaller than the minimum support (2), so  X  X d:3 X  is a real weighted frequent pattern. Last, the calculated w-confidence of  X  X d:3 X  (0.57/0.79 0.72) is higher than min_wconf (0.65) . Thus, Pattern  X  X d:3 X  is a maximal correlated weight frequent pattern. The remaining conditional FP-trees as shown in Figs 6(b) X (d) are to conduct the same steps in Fig. 6(a). 4. Our MCWP algorithms
In this section, we present and describe the details of our proposed algorithms. 4.1. MCWP(WA) algorithm
Figure 7 shows the pseudo-code of the MCWP(WA) algorithm. Given minimum support threshold ( min_sup ) and a minimum w-confidence threshold ( min_wconf ) are set as parameters of MCWP(WA). First, the MCWP(WA) algorithm scans a given transaction database once, weighted frequent items satisfying the pruning condition are found and these items are sorted in weight ascending order. The MCWP(WA) algorithm then calls the recursive procedure MCWP(WA) (FP-tree, MCWP-tree, MCWP). In procedure MCWP(WA), as shown in Fig. 8, a conditional pattern base is generated by combining a prefix with each item (from bottom-up traversal) of the FP-tree X  X  header table in line 1. Currently, a prefix is null and new prefixes are set from  X  Set Y = FP  X  tree(WA). prefix  X  { i } X  in line 2. In line 3, the maximum weight of items of Y is set to MaxW .Inline5,if Y  X  j ( j  X  i  X  X  conditional database) is a correlated weight frequent pattern, then the item j is inserted into the tail, the conditional database of Y in line 6. If Y  X  tail is not a subset of the already found maximal correlated weight frequent patterns, only one branch, B in line 9, Y  X  B is a candidate maximal correlated weight frequent pattern. MaxW is used to calculate approximate weighted support, so exactly weighted frequent checking is needed. Thus, procedure MCWP(WA) checks if pattern Y  X  B is an exactly correlated weight frequent pattern with its real weighted support in line 10. If Y  X  B is a correlated weight frequent pattern, it is included as the output set, MCWP in line 11. It also inserted into a local MCWP(WA)-tree for later subset checking. Conversely, if Y  X  B is not a correlated weight frequent pattern, all kinds of combinations of items of B are checked for those weight correlations again in line 14 X 15. Only maximal correlated weight frequent patterns are mined in line 16 X 19. Turn to line 9, if T y has multiple paths, a new conditional FP-tree with conditional pattern Y , M y is constructed in line 21 and the procedure MCWP(WA) is called recursively for mining M y in line 22. If the recursive procedure returns any of the maximal correlated weight fre-quent patterns, the conditional MCWP-tree is merged with the upper conditional (or local) MCWP-tree in line 29. Otherwise, conditional pattern Y is checked to see if Y is a maximal correlated weight fre-quent pattern in line 24 X 25. If pattern Y is a maximal correlated weight frequent pattern, it is included as MCWP in line 26 and inserted into its MCWP-tree in line 27. Thus, the local FP-tree is recursively divided and smaller conditional databases are constructed with the prefixes respectively and correlated weight frequent patterns are discovered by calling the procedure MCWP(WA) (FP-tree, MCWP-Tree, MCWP) recursively. 4.2. MCWP(SD/SD2) algorithm
In this section, a technique is implemented, such as conditional sentences underlined in Fig. 10, and aflag X  YhasMW  X  is used for storing information to cope with the support descending order. That is, if there is MaxW or MinW in the prefix pattern, the w-confidence pruning can be utilized by this technique before subset checking, so MCWP(SD) can reduce the runtime to construct conditional FP-tree and call its procedure recursively. If the MCWP(SD) algorithm does not apply this technique, the w-confidence pruning can only be utilized after the subset checking (at the same time when the real weighted infre-quent patterns are pruned), so the pruning performance of MCWP(SD) worsens. We term this improved MCWP algorithm, MCWP(SD2). Conversely, MCWP(SD) does not utilize the technique. We compare the MCWP(SD2) with MCWP(SD), which does not utilize the technique, in Section 5 to analyze the performance of our technique. We present maximal correlated weight frequent pattern mining with the support descending order (MCWP(SD)) algorithm, as above.

Both MCWP(SD) and MCWP(SD2) have the same process of the global transaction database, so we present the MCWP(SD) algorithm in Fig. 10 as a representative. However, the recursive procedure of MCWP(SD) and that of MCWP(SD2) differ. Now, we illustrate the MCWP(SD2) algorithm and its recursive procedure, as follows. (The procedure of MCWP(SD) is the same as that presented in Fig. 10, except for underlined code, so we omit its illustration.) First, min_sup and min_wconf are required as parameters of MCWP(SD2). MCWP(SD2) also scans a given transaction database once, weighted frequent items satisfying the pruning condition are found, and these items are sorted in support descending order. Then, the recursive procedure MCWP(SD2) (FP-tree, MCWP-tree, MCWP) is called. In procedure MCWP(SD2), as shown in Fig. 10, a conditional pattern base is generated by combining a prefix with each item (from bottom-up traversal) of the FP-tree X  X  header table in line 1. Currently, a prefix is null and new prefixes are set from  X  Set Y = FP  X  tree(SD). prefix  X  { i } X  in line 2. In line 3  X  4, the maximum weight of i  X  X  conditional database items is set to MaxW and the minimum weight of i  X  X  conditional database items is set to MinW .If MaxW or MinW is a member of weights of the prefix Y  X  X  item, a flag  X  YhasMW  X  X ssetto X  true  X  in line 6, otherwise, YhasMW is set to  X  false  X . If YhasMW is  X  true  X , the procedure MCWP(SD2) can prune items of i  X  X  conditional pattern base by weighted support or w-confidence checking in line 10. Passed items are inserted into i  X  X   X  tail  X  in line 11. However, if YhasMW is  X  false  X , the procedure MCWP(SD2) only uses weighted infrequent pruning condition in line 13. Only if the tail is not a subset of an already found maximal correlated weight frequent pattern, Y  X  X  conditional FP-tree T y is constructed in line 16. If T y has only one branch ( B ) in line 17, B is a candidate maximal correlated weight frequent pattern. B needs to check its real weighted support ( Y  X  B ) to be an ultimate real MCWP. Moreover, if YhasMW is  X  false  X , w-confidence of Y  X  B also must be checked in line 18. If Y  X  B is a correlated weight frequent pattern, it is included as the output set, MCWP in line 21. It also inserted into a local MCWP(SD2)-tree for later subset checking. Conversely, if Y  X  B is not a correlated weight frequent pattern, all kinds of combinations of items of B are to check those weighted correlation again in line 24 X 25. Only maximal correlated weight frequent patterns are mined in line 26 X 31. Turn to line 17, if T y has multiple paths, a new conditional FP-tree with conditional pattern Y , M y is constructed in line 33 and the procedure MCWP(SD2) is called recursively to mine M the conditional MCWP-tree is merged with the upper conditional (or local) MCWP-tree in line 41. Otherwise, conditional pattern Y is checked to see if Y is a maximal correlated weight frequent pattern in line 36 X 37. If pattern Y is a maximal correlated weight frequent pattern, it is included as MCWP in line 38 and inserted into its MCWP-tree in line 39. 5. Performance evaluation 5.1. Test environment and datasets
In this section, we report experimental results on the performance of MCWP, comparing MCWP(WA) and MCWP(SD). In addition, we compare MCWP(SD2) that applies our conditional pruning technique to increase the w-confidence pruning effect with other MCWP algorithms. Those three MCWP algo-rithms are compared to FPmax* [9], another novel pattern mining algorithm. FPmax* is a state of the art maximal frequent pattern mining algorithm. We used five real datasets and several synthetic datasets for our experiments. Table 3 shows characteristics of the six datasets (Accidents, Pumsb, Retail, BMS-Webview1, Chain-store datasets and T10I4DxK datasets). The Accidents dataset contains anonymous traffic accident data. It is quite dense, so a large number of frequent patterns will be mined, even for very high values of minimum support. Another dense dataset, Pumsb dataset includes census data for population and housing. Retail is sparse and includes a retail supermarket store basket dataset. BMS-Webview1 dataset contains several months X  click-stream data from an e-commerce website that is very sparse. These four real datasets can be obtained from the Frequent Itemset MIning (FIMI) dataset repos-itory (http://fimi.cs.helsinki.fi/data/). The Chain-store dataset was taken from a major chain in California (http://cuc is.ece.northwestern.e du/project s/DMS/MineBenc h. html) and it provides price values that can be used as weight values of items. However, FIMI datasets do not provide weight values of items. Weight values of items or patterns are used to calculate weighted supports of patterns and the weight values of items are randomly generated. The synthetic datasets were generated from the IBM dataset gener-ator (http://www.almaden.ibm.com/software/projects/hdb/resources.shtml). We use synthetic T10I4Dx datasets that are sparse and contain from 100 k to 1000 k transactions. We also use another set of syn-thetic datasets with different parameter settings for the scalability test. Table 4 summarizes the parameter settings, where | T | is the average size of a transaction, | L | is the maximum number of potential frequent patterns, and N is the number of items. Each dataset in Table 4 contains 100,000 transactions with an av-erage frequent pattern length of 4. We will term these synthetic datasets, TaLbNc datasets. MCWP(SD), MCWP(SD2) and MCWP(WA) as well as FPmax* were written in Visual C ++ and experiments were performed on a processor operating at 2.40 GHz with 2048MB of memory using the Microsoft Windows 7 operating system. We assume that noise or missing data do not occur in the used datasets since they are out of interests in this paper. 5.2. Experimental results on execution time Real dataset with real weights: Normalized real weights of items are set to be 0.3 X 0.6, based on the price values of items. The minimum w-confidence is 0.95. As shown in Fig. 11, fewer MCWP patterns are generated (by MCWP(WA), MCWP(SD) and MCWP(SD2)) than for FPmax*. Especially, much fewer patterns are generated as the minimum support is increased. Figure 12 shows that MCWP(SD) and MCWP(SD2) perform faster than the others do. FPmax* is slower than the three MCWP algorithms. The runtime of MCWP(SD) is almost the same as that of MCWP(SD2) on the Chain-store dataset. Real datasets with synthetic weights: We analyze the evaluation results for the Accidents, Pumsb, BMS-Webview1, and Retail datasets from Figs 13 to 20. Normalized weights of items are between 0.3 X  0.6, 0.6 X 0.8, 0.6 X 0.9 and 0.3 X 0.6 respectively. Minimum weight-confidences are 0.8, 0.5, 0.9 and 0.8. MCWP(SD2) achieves the best performance results on Accidents, Pumsb and Retail datasets. Excep-tionally, MCWP(WA) is more efficient than others on the BMS-Webview1 dataset. MCWP(WA) can be faster than MCWP(SD2) or MCWP(SD) on a very sparse dataset, as shown in Fig. 18, since the advan-tage of support descending ordered tree structures of MCWP(SD, SD2), which can make conditional database more compact, becomes weakened. When the minimum support threshold is high, runtime performance of MCWP(SD) and MCWP(WA) can be similar, as shown in Figs 14 and 16. MCWP al-gorithms generate fewer but correlated weight patterns compared to FPmax* in all cases in Figs 13, 15, 17 and 19. In Fig. 14, MCWP(SD2) and MCWP(WA) increase slowly, while the minimum support threshold is decreased but FPmax* increases dramatically, since FPmax* generates much larger patterns than does MCWP(SD, SD2, WA). In Fig. 20, it is observed that all of our three algorithms guarantee outstanding runtime results while FPmax* shows the worst performance. Especially, MCWP(SD2) also resents the fastest runtimes in every case due to its strong pruning effect. 5.3. Scalability test
We performed a scalbility test with regard to the number of transacntions from 100 K to 1000 K on the T10I4Dx datasets in Fig. 21 and the number of attributes from T10N to T40N on the TaLbNc dataset in Fig. 22 in terms of runtime. We set a minimum support as 0.01% and a weight range is set to 0.5 X  0.9. All of MCWP algorithms scale better than does FPmax* in this test. In Fig. 21, three algorithms, MCWP(SD, SD, WA) have similar runtime performance. However, MCWP(WA) and MCWP(SD2) are faster than MCWP(SD). When the number of attributes is 30,000 or 40,000 in Fig. 22, we can see that all MCWP algorithms has much better scalability in terms of base attribute sizes of the TaLbNc dataset. 5.4. Memory consumption
We tested the memory usage in MCWP(SD), MCWP(SD2), MCWP(WA) and FPmax*. From Figs 23 to 26, we can know that MCWP(SD) and MCWP(SD2) use less memory than MCWP(WA) and FPmax*.
 Exceptionally, on the dense dataset, Accidents, MCWP(WA) uses even more memory than FPmax*. However, on the other datasets, such as shown in Figs 23, 25 and 26, MCWP(WA) also uses less memory than FPmax*, since MCWP algorithms generate much fewer than the maximal frequent pattern mining algorithm, such as FPmax*, on non-dense datasets. MCWP(SD2) has the best performance in terms of memory usage in all cases. MCWP(SD2) uses more compact tree structures that generate much less than MCWP(WA) and prunes more non-candidate correlated weight frequent patterns than does MCWP(SD). 5.5. Effect of our w-confidence pruning technique for MCWP(SD2)
As illustrated in Fig. 10, MCWP(SD2) uses the suggested technique to prune non-candidate correlated weight frequent patterns. Conversely, since MCWP(SD) does not use the technique, it only can prune non-candidate correlated weight frequent patterns after the subset checking step terminates. In Figs 14 and 18, the runtime of MCWP(SD2) is much faster than that of MCWP(SD). Figures 20 and 22 show that MCWP(SD2) has also improved runtime performance in these cases. Therefore, the experimental results indicate that the proposed technique effectively enhanced the runtime performance of MCWP mining that uses support descending order. Memory usage in MCWP(SD2) was improved on the sparse dataset as shown in Fig. 26 but it was similar to that of MCWP(SD). 6. Conclusions
In this paper, we proposed maximal correlated weight frequent pattern mining (MCWP) to avoid generating huge sets of frequent patterns. MCWP can prune weakly correlated weight frequent patterns that are not useful. We checked maximum weighted s upports and co rrelated weight affinities of candidate patterns to reduce search space efficiently to maintain anti-monotone property of maximal correlated weight frequent patterns. Then, we performed subset checking with already mined maximal correlated weight frequent patterns. Finally, real weighted supports of candidate patterns were checked to generate the results set. We developed two mining algorithms of maximal correlated weight frequent pattern (MCWP), called MCWP(WA) (based on Weight Ascending order) and MCWP(SD) (based on Support Descending order), respectively, based on the framework. MCWP(SD) gains advantage in conditional database access, but may not obtain the highest weighted item of conditional database to mine highly correlated weight frequent patterns. Thus, we suggested a technique that uses additional conditions to prune lowly correlated weight items before the subset checking process. By virtue of its proposal, the performance of MCWP(SD2) algorithm that applied the technique showed more improvement than the MCWP(SD) algorithm, as shown in our extensive test. Our two proposed algorithms, MCWP(SD2) and MCWP(WA) are more efficient and scalable than the FPmax* algorithm. Especially, MCWP(SD2) is more efficient than MCWP(WA) in many cases. The concept of the maximal correlated weight pattern can be used in real application domains, such as the retail industry, telecommunication data, computer vision and medical history analysis.
 Acknowledgements This research was supported by the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (NRF No. 2013005682 and 20080062611).
 References
