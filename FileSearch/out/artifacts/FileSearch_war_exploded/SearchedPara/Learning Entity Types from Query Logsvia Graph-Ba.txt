 Entities (e.g., person, movie or place) play an important role in real-world applications and learning entity types has at-tracted much attention in recent years. Most conventional automatic techniques use large corpora, such as news arti-cles, to learn types of entities. However, such text corpora focus on general knowledge about entities in an objective way. Hence, it is difficult to satisfy those users with spe-cific and personalized needs for an entity. Recent years have witnessed an explosive expansion in the mining of search query logs, which contain billions of entities. The word pat-terns and click-throughs in search logs are not found in text corpora, thus providing a complemental source for discov-ering entity types based on user behaviors. In this paper, we study the problem of learning entity types from search query logs and address the following challenges: (1) queries are short texts, and information related to entities is usually very sparse; (2) large amounts of irrelevant information ex-ists in search logs, bringing noise in detecting entity types. In this paper, we first model query logs using a bipartite graph with entities and their auxiliary information, such as contextual words and clicked URLs. Then we propose a graph-based framework called ELP ( E nsemble framework based on L able P ropagation) to simultaneously learn the types of both entities and auxiliary signals. In ELP, two separate strategies are designed to fix the problems of spar-sity and noise in query logs. Extensive empirical studies are conducted on real search logs to evaluate the effectiveness of the proposed ELP framework.
 H.2.8 [ Database management ]: Database applications-Data mining Query; Entity; Graph  X 
An entity is something that exists in itself, actually or po-tentially, concretely or abstractly, physically or not 1 . Enti-ties are forming the building block for various web applica-tions. Yelp 2 is building on top of a corpus of local class en-tities ( e . g ., the restaurant entity  X  X he French Laundry X , the Point of Interest entity  X  X olden Gate Bridge X , etc.) associ-ated with user reviews. IMDB 3 has a large corpus of movie and actor class entities. Modern search engines like Bing, Google and Yahoo! start building Knowledge Graph con-taining a large collection of diverse types of entities. When a user issues a question about an entity ( e . g .,  X  X et worth of Bill Gates X  or  X  X hone number of Gary Danko X ), the search en-gine can retrieve results directly from the knowledge graph, satisfying the user X  X  need and providing better user experi-ence. Recent study shows that around 70% of the queries contain entity information [31, 22]. Hence, the coverage of entities is very important for these applications. Moreover, knowing the exact types of entities can help the application decide the best way in presenting results to users.
Various entity repositories, ranging from the more general collaborative knowledge bases such as Wikipedia and Free-base to the domain-specific corpora such as IMDB and Yelp, are widely used to extract entity information and aggre-gate the information into a comprehensive knowledge graph. However, there are several problems with this approach: (a) coverage : it is one of the key metric in measuring the quality of knowledge. Knowledge bases like Wikipedia and Freebase primary focus on popular entities from a few limited types, while other domain-specific corpora are more expensive to obtain. Plus, little information exists in knowledge bases for many less popular entities or newly generated entities, such as a new music title. It is difficult to identify and ex-tract such entities in time; (b) ambiguity : multiple types of entity are often associated with the same string collected from the same or different sources. For example, the token  X  X hicago X  is not only a city entity, but also a movie entity or a rock band entity. How to separate them apart in case little is known about the types of the entities, and how to rank these entities according to the popularity and/or user intent, are both quite challenging; (c) discrepancy : errors may ex-ist due to user-generated contents via crowdsourcing, thus information extracted from these sources may be noisy and inconsistent. http://en.wikipedia.org/wiki/Entity http://www.yelp.com/ http://www.imdb.com/
A lot of research work in the literature tries to overcome the above challenges from different perspectives. The exist-ing knowledge bases could only cover a fraction of the whole entity space. In order to expand the size of knowledge bases, many automatic techniques have been proposed to discover entities and their types from different large corpora, such as news articles and web pages [3, 9, 30]. In addition, disam-biguating entities from news reports is also studied in [14, 20]. Other sources such as search query logs can also be leveraged to extract and disambiguate entities. Since the search engine has become the main information source for most people to look for information, search query logs can be a nice complementary source for extracting new entity information as well as learning entity popularity and dis-ambiguating entities. A few state-of-the-art approaches are proposed to classify and disambiguate entities in query logs [16, 26, 6]. For instance, intent-based Model (IM) [26] pre-dicts entity type distributions by jointly modeling user in-tent and entity types via probabilistic inference in a graph-ical manner. Fast Entity Linker (FEL) is proposed in [6] to disambiguate entities by linking queries to entities in a knowledge base. However, these methods do not fully ex-plore the importance of auxiliary signals in query logs, i.e. the structural language patterns (contextual word patterns) in queries and the clicked domains from relevant web URL results. For example, given the query  X  X enu of Purple Pig X  and a user X  X  clicked domain URL  X  X elp.com X , both the pat-tern  X  X enu of X  and the clicked URL help predicting  X  X ur-ple Pig X  as a local restaurant entity. Therefore, knowing the types of these important signals can help mining entity types from query logs more effectively.

In this paper, we model search query logs into a bipartite graph to encode relations between entities and important signals. Two kinds of nodes, entities and their auxiliary in-formation, are contained in the constructed bipartite graph shown in Figure 1 (b). With such a bipartite graph, we can take advantage of the encoded relations [35] to learn entity types. Moreover, the type information can also be assigned to auxiliary nodes, thereby helping disambiguating entities via user-generated texts ( e . g ., contextual words) and user feedbacks ( e . g ., clicked URLs). In this paper, we apply a graph-based L abel P ropagation (LP) method to simultane-ously learn types of both entities and auxiliary signals. Fig-ure 1 (c) shows the steps of LP in an intuitive way. Given a small number of prior-known entities, the types of these en-tities are first propagated to the connected auxiliary nodes, and then the types are propagated back from auxiliary nodes to unknown entities. Despite the simple idea, mining entity types from the built graph is still a challenging task due to the following reasons: In order to address these two issues, we propose an E nsemble framework based on L abel P ropagation (ELP) to simulta-neously learn types of both entities and auxiliary signals. Specifically, we design two separate strategies to fix the prob-lems of sparsity and noise in query logs, respectively.
In summary, our contributions are as follows:
In this section, we first introduce several related concepts and notations. Then, we will formally define the problem of learning node types from a bipartite entity-auxiliary graph extracted from query logs.

Definition 1. A Bipartite Entity-Auxiliary (EA) Gra-ph : A bipartite entity-auxiliary (EA) graph is represented as an undirected graph G = ( V , E ). V is the set of nodes (objects), including two types of objects, i.e., entities E = { e 1 ,...,e M } and auxiliary signals A = { a 1 ,...,a N } . E  X  E  X  A is the set of links (relations) between the nodes in V , which involves the associatedWith link between entities and auxiliary signals. Let W denote an M  X  N weight matrix, in which element w ij equals the frequency associating e i and a .

Figure 1 (b) shows an example of a bipartite EA graph extracted from the search query logs in Figure 1 (a). Three entities are connected with six auxiliary signals, including four contextual words and two clicked URLs.

In EA graph, each entity has at least one type (label) in reality. We assume there are K labels for entities ( K  X  2) and represent entity types as Y  X  R M  X  K . y ik  X  X  is a non-negative real number indicating the probability that entity e belongs to label k . In practice, a small set of entities (seed entities) in the graph may be manually labeled with their types. We denote the labeled entity set as E L . In Figure 1 (b), both  X  X ew York X  and  X  X aylor Swift X  are considered as seed entities. We use Y 0 to denote an instantiation of Y that is consistent with the seed labels. Given an entity e i  X  E with n labels ( n  X  1), we set y 0 ik as 1 . 0 /n if e i has label k , otherwise 0. Given the entity e i  X  E \ E L without labels, we have y 0 ik = 0 for any label k .

From the existing search logs D , we can extract a bipar-tite graph G = ( V , E ) and get Y 0 according to some seed entities E L . Our goal is to learn entity types Y from G . Since the auxiliary nodes can also carry labels with them to indicate the important interconnections between entities and the auxiliary signals, another goal is to assign labels to those auxiliary nodes in G . We use Z  X  R N  X  K as labels of auxiliary nodes, where the element z jk is a non-negative real number indicating the probability that a j relates to la-bel k . Thus our ultimate goal becomes to estimate Y and Z given G and E L . In order to solve this problem, we apply a graph-based L abel P ropagation (LP) method to leverage these important auxiliary signals via their connections with the target entities as shown in Figure 1 (c).
In this section, we propose an E nsemble framework based on LP (ELP) to simultaneously learn types of both entities and auxiliary signals from query logs. Before proceeding, we first introduce how to build the entity-auxiliary graph from real-world search logs.
Given search logs, we first have to extract entities from queries. Several methods are applied to find entities in this paper. First, we use a part-of-speech tagger [1] to extract contiguous words of proper nouns, common nouns and capitalized words [16, 17] to form noun phrases. Sec-ond, we match the extracted noun phrases according to a dictionary of entities built from knowledge bases, such as Wikipedia, Freebase and Yelp. We do not use the type in-formation in those knowledge bases. We assume that the types of entities are unknown in the experiments. These methods help us detect entities in high precision. Besides, we can use a more complex model in [10] to identify the entity and the background part (i.e., contextual words). In the example of the search logs in Figure 1 (a), we extract  X  X ew York X ,  X  X axwell X  and  X  X aylor Swift X  as entities. Thus,  X  X ome sales X ,  X  X eal estate X ,  X  X lbums X  and  X  X ongs X  are con-sidered as contextual words. In our experiments, we use both the uni-gram and binary-grams of contexts as auxiliary nodes. The stop-word nodes are removed from our graph.
In search logs, clicked URLs are also very important for learning entity types. Since each clicked URL may have several levels of domain names to point to a certain webpage, there will be too many redundant nodes of clicked URLs in the constructed graph. Therefore, we group a set of URLs into a single auxiliary node if they have exactly the same top-and second-level domain names. In Figure 1 (a), we only show the first two domain names for the clicked URLs. We use the frequencies of entities and auxiliary nodes appearing together in the query logs as weights of corresponding edges.
Such a bipartite graph helps encode relations between en-tities and important auxiliary signals from search query logs. We can take advantage of the encoded relations to discover entity types by applying the graph-based LP method. How-ever, directly applying LP may not be satisfying due to the following issues in query logs: 1. Queries are short texts, and information related to entities is usually very sparse. LP may not propagate labels adequately. Therefore, it is necessary to explore the hidden connections in EA graph. 2. Large amounts of irrelevant information exists in search logs, bringing noise in detecting entity types. LP may prop-agate errors out and enlarge the error information due to the noise. Hence, it is imperative to discover and remove such noisy information from the EA graph.

In the following, we first focus our attention on how to apply LP on the built graph to learn types of both entities and auxiliary signals simultaneously. Then we introduce two separate strategies LPA and LPD to address the problem of sparsity and noise in the EA graph respectively. After that, we describe the proposed ELP framework that takes advantage of the LPA and LPD strategies.
The problem of learning with labeled and unlabeled data from graphs has been investigated in [36, 37, 18, 33, 11]. The objective and algorithm of the LP method used in this paper are heavily influenced by the works of [36, 18]. Given the collection of search log data D , we can extract an entity-auxiliary graph G = ( V , E ) with a weight matrix W as intro-duced in Section 2. With a small set of seed entities E L can initialize Y 0 . Our goal is to automatically estimate Y for entities and Z for auxiliary nodes according to W and Y . We define a normalized frequency matrix as follows: where D is a diagonal matrix and each element d ii  X  D is the sum of all the elements in the i th row (or column) of WW &gt; . Intuitively, d ii can be interpreted as the volume of all length-of-two paths that start at e i . The reason we use such a normalization is to guarantee the convergence of LP as shown in [18]. Algorithm 1 The LP algorithm Input: Search query log D , a set of seed entities E L and a Output: Label matrices Y and Z 1: // graph construction step 2: // initialization step 3: Compute the weight matrix W from G 4: Compute N from W according to Equation (1) 5: // iterative computation step 6: while NOT converged do 7: // propagation step from entities to auxiliary nodes 8: // propagation step from auxiliary nodes to entities 9: end while 10: Normalize Y and Z according to Equation (5)
With the above definitions and notations, LP iteratively updates Y and Z . For the t -th iteration, it first propagates the types of entities to the connected auxiliary nodes: Then it propagates the types back to entities from the aux-iliary nodes as follows: where  X  is a parameter to trade off the label consistency between the intrinsic graph structure and the seed entities. It has been shown in [18] that the sequence of Y t asymptot-ically converges to: The time complexity of LP is O ( T |E| ), where T is the itera-tion number and |E| is the number of connections in the EA graph. Through our experiments, the algorithm converges after no more than 20 rounds in most cases. The LP method is summarized in Algorithm 1.

Once Y and Z are obtained, we normalize their elements to get the posterior probabilities p ( k | e i ) for i = 1 ,...,M and p ( k | a j ) for j = 1 ,...,N as follows:
Directly applying LP may not be satisfying because the connections extracted from query logs are very sparse and LP cannot propagate labels adequately. Therefore, we pro-pose a strategy LPA ( L abel P ropagation after A dding more connections) to explore the hidden connections in the EA graph. We take advantage of the word2vec tool 4 to connect entities with more contextual words and help the LP model propagate labels more effectively. https://code.google.com/p/word2vec/ Figure 2: Two seperate intuitive examples of the updated graphs obtained from the LPA and LPD strategies. The dashed black edges in (a) represent the hidden connections explored by LPA. In (b), the dashed box of the auxiliary node means that the node is multi-type, and the dashed orange edges represent the connections we should get rid of ac-cording to LPD.

The intuition behind LPA is that, if one entity e connects with one auxiliary node a 1 and a 1 has a high similarity with another auxiliary node a 2 , we should connect e with a 2 pand the connections in the bipartite EA graph. Hence, we need to measure the similarities among auxiliary nodes first. In this paper, we focus on contextual words and measure their similarities according to the semantic meanings by the word2vec tool. The word2vec tool provides an efficient im-plementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words [23, 24, 25]. By calculating the distance between two vector representations, we can obtain the similarity value for two words. Hence, given the auxiliary node set A = { a we could get a similarity matrix S  X  R N  X  N , where each el-ement s ij  X  S denotes the similarity value between a i and a . The above exploration of connections can be formulated as follows: where W A is the updated weight matrix according to LPA. Intuitively, w ij  X  W A can be interpreted as the weight ag-gregation of all length-of-two paths from e i to a j via every a 0  X  A . Given the search logs in Figure 1 (a), we show an intuitive example of the updated graph according to LPA in Figure 2 (a). We assume that the contextual words  X  X ome sales X  and  X  X eal estate X  are very similar so we connect  X  X ew York X  with  X  X eal estate X  in Figure 2 (a). With such a denser graph, we could run the LP algorithm to propagate labels more effectively. We denote the node types learned from LPA as Y A and Z A for entities and auxiliary nodes, respec-tively.
Another issue of directly applying LP is that noise may exist in the built EA graph so that LP may propagate er-rors out and enlarge the error information. Therefore, we propose a strategy LPD ( L abel P ropagation after D eleting noisy nodes) to discover and get rid of noisy information in the EA graph.

The basic idea of LPD is to discover some multi-type auxiliary nodes and delete them with their corresponding connections in the constructed EA graph. Here multi-type nodes mean contextual words or clicked URLs that cover several types of entities. For example,  X  X icture X  relates to several entity types, such as media, location, and person. Hence, it is not informative to take such contextual word into consideration. We apply a similar approach in [2] to get rid of some multi-type auxiliary nodes and update the graph accordingly. Specifically, we start by calculating the similarity (e.g., cosine similarity) between two entities ac-cording to the bag-of-word representations of their auxiliary nodes. Low similarity pairs are more likely to represent en-tities with different types. Hence, auxiliary nodes involved with such entities are not likely to be very specific. So we can consider low similarity pairs as voters and let the auxiliary nodes be the candidates. Each pair votes for its auxiliary nodes they share. The more votes an auxiliary node gets, the higher probability of multi-type it is. We can then apply a threshold to get rid of some auxiliary nodes and update the EA graph accordingly. Figure 2 (b) gives an intuitive exam-ple of the updated graph according to LPD. In this figure, we assume the clicked URL  X  X ww.en.wikipedia.org/wiki/ X  is a multi-type node and delete it with its corresponding edges from the graph. Then we can run LP on such a cleaner graph so that the error information can be propagated out as little as possible. We denote the node types learned from LPD as Y
D and Z D for entities and auxiliary nodes, respectively.
Given the proposed LPA and LPD strategies, we can sim-ply combine them together to derive another two strategies, LPAD and LPDA. LPAD updates the graph by first adding more connections and then deleting noisy nodes. The node types learned from LPAD are denoted as Y AD and Z AD for entities and auxiliary nodes, respectively. LPDA updates the graph in an opposite way, i.e., first deleting noisy nodes and then adding connections based on the remaining nodes. We denote the node types learned from LPDA as Y DA and Z DA for entities and auxiliary nodes, respectively.

Since each strategy has its advantage, we propose an E nsemble framework based on LP (ELP) to combine them together and maximize the margin [12]. We run each strategy sep-arately and select the best one as the final result for each node as follows:
In practice, we can also use the weighted results of the four strategies as the final solution. Since it would bring several weight parameters for these strategies, we calculate the results of ELP according to Equation (7) for simplicity in the experiments.
In this section, we conduct extensive experiments to eval-uate the proposed ELP framework. After introducing the datasets and the experimental settings, we compare differ-ent baseline methods.
We collect a large set of click-through data (denoted as a system set) over a continuous period of time from a real-world search engine. Then a small number of click-through data are sampled from the system set and denoted as a gold set. We manually labeled entities from queries of the gold set with correct types. The labeled data are only used for seed Table 1: Statistics of the collected query data.
 Table 2: Statistics of the entity-auxiliary graphs.  X  X C selections and performance evaluations in the experiments. The basis statistics of these two datasets are shown in Table 1.

In the experiments, we focus on 3 target types of classes, namely Local, Media and Person. By following the extrac-tion rules in Section 3.1, we get entities and related auxiliary signals belonging to these 3 target types. In order to build a compact and reliable graph, we apply a threshold to get rid of some infrequent nodes. For example, we set the threshold as 1 for the gold dataset and filter out those nodes appear-ing only 1 time. The basic statistics of nodes and links in the entity-auxiliary graphs are represented in Table 2. The distributions of the 3 target labels for entities are shown in Table 3.
In order to show that the LP model fits the constructed graph very well, we compare LP with several traditional classification methods. Given the bipartite EA graph, we consider the connected auxiliary nodes as features for each entity and the frequencies (the edge weights) as feature val-ues. We focus on the following methods:
In addition, in order to show the effectiveness of the pro-posed ELP framework, we compare with different variations of the LP model. Since both contextual words and clicked URLs can be considered as auxiliary information for entities in search query logs, we can construct three different bipar-tite graphs. They are the E ntity-C ontext (EC) graph, the E ntity-clicked U RL (EU) graph and the E ntity-Auxiliary (EA) graph. The EA graph considers both the contexts and clicked URLs as the auxiliary information in search query logs, so it contains more information than the EC and EU graphs. We can apply our proposed strategies on these dif-ferent graphs and we summarize them as follows:
For a fair comparison, we use the same parameter settings for the baselines related to the LP method. Specifically, we test with different  X  values for LP and find that  X   X  (0 . 5 , 0 . 9) yields similar good results. So we set the parameter  X  to be 0.75 as in [18]. In order to get the similarities among con-textual words, we use a pre-trained vectors 5 on about 100 billion words and phrases from various news articles. For the number of auxiliary nodes that should be deleted, we set it to be 10 in the experiments. In addition, we use SVM (RBF) with optimized parameters and other traditional clas-sifiers with default parameters in our experiments. For each node, we can get a list of non-negative real numbers from LP indicating the posterior probabilities that the node relates to a label. We clamp these probabilities to 0/1 values for simplicity.

In order to evaluate the results, we focus on the labeled data and use accuracy and weighted average of the F1 score of each class (abbreviated as  X  X eighted-F1 X ) as the perfor-mance measures for entities. Weighted-F1 means that we calculate the F1 score for each label and find their aver-age value weighted by the number of true instances for each label. This metric takes the label imbalance into consider-ation. For an entity with multi-labels, if the learned label matches with one of its multiple labels, we consider it as a correct prediction. Since we do not have ground truth for the auxiliary information, we will not present the quantitative analysis on the auxiliary information. We only show some qualitative analysis in Section 4.4. In the experiments, we randomly select a certain portion (e.g., 10%) of the entities as seeds for 10 times and report the average performances for models related to LP. We use the same seed entities as the training data for the traditional classification models.
In this subsection, we show the performances of the pro-posed ELP framework. We first demonstrate that how the LP method takes advantage of the constructed bipartite EA graph compared with some traditional classification models. Due to space limit, we only show the performances on the EC graph of the gold dataset in Table 4. Similar perfor-mances can be obtained for other graphs.

It can be observed from Table 4 that LP consistently outperforms other classification methods on accuracy and weighted-F1 scores for different amounts of seed entities (training data). It illustrates that the constructed graph helps the LP method propagate the label information out very well. Since all the other classifiers ignore the graph structure, important information may be missing and the performances are not so well compared with the LP method that takes advantage of the graph structure. In addition, when the amount of seed entities increases, the performances become better for almost all classifiers except the SVM method with the RBF kernel. It seems that more training data does not help SVM (RBF) very much. However, in real-ity, more seed entities means more annotations and human labelings. With large volumes of new queries, extracting such supervised information from search query logs can be very expensive and time consuming. In Table 4, LP can only achieve 45% of accuracy when 1% of data are selected as seeds. The performance should be improved if we explore the hidden connections and get rid of multi-type nodes in the constructed graph as in the proposed ELP framework. So in the following, we focus on the gold dataset with 1% of seed entities to show the effectiveness of ELP. freebase-vectors-skipgram1000-en.bin.gz. It can be down-loaded from https://code.google.com/p/word2vec/ . value the better the performance.

The results of different methods based on LP are shown in Table 5. It can be observed that ELP (A) outperforms other baseline methods on both accuracy and weighted-F1 and ELP (C) can also achieve a very good performance. ELP (A) outperforms ELP (C) with an improvement of 21% on the accuracy. It shows that more auxiliary nodes help ELP achieve a better performance on learning entity types from search query logs.

In particular, due to the noisy information in search query logs, directly applying the LP method on the constructed EA graph may reduce the performance as shown in Table 5. However, the results can be improved if we better build the graph as introduced in LPA and LPD. We can observe that the performance of LPA (LPD) on the EA graph are better than those on the EC and EU graphs. It demonstrates that using more high-quality auxiliary nodes can provide more important information and facilitate the process of LPA or LPD. Moreover, compared with the original LP method, both LPA and LPD can improve the performances for all the constructed bipartite graphs (i.e., EC, EU and EA). For example, LPA (A) significantly outperforms LP (A) with improvements of 45% and 104% on accuracy and weighted-F1, respectively. Furthermore, LPA seems more powerful than LPD on both the EC and EA graphs. It shows that exploring hidden connections among the sparse graph plays a more important role in learning entity types from search query logs.

Though LPA and LPD perform better than LP, our pro-posed ELP framework achieves better results than the LPA and LPD strategies. Specifically, ELP (A) outperforms LPA (A) with an improvement of 22% on the weighted-F1 score. In addition, ELP also performs better than LPAD and LPDA with an average improvement of 18% on the weighted-F1 score as shown in Table 5. It implies that ELP can maximize the effectiveness of combining different strategies together. Simply combining LPA and LPD together (e.g., LPAD and LPDA) may not make full use of the operations of adding more connections and deleting the noisy nodes.

In summary, with the help of exploring hidden connections and getting rid of noisy nodes, the proposed ELP framework can achieve an accuracy of 68% on the EA graph with only 1% of entities as seeds. From Table 4, we can see that the LP method needs around 10% of seed entities to get the same accuracy score. Therefore, ELP can help significantly reduce the cost of human labeling in learning entity types from search query logs.

We further show the effectiveness of the proposed ELP framework on the larger system set. Only 0.1% of seed en-tities are used in the experiments to test the power of ELP. Since we only labeled entities in the gold set and these enti-Table 5: Average performances with 1% of seed en-tities for 10 times on the gold dataset. The results are reported as  X  X verage performance + (rank) X .  X   X   X  indicates that the larger the value the better the performance.
 Table 6: Average performances with 0.1% of seed entities for 10 times on the system dataset. The results are reported as  X  X verage performance + (rank) X .  X   X   X  indicates that the larger the value the better the performance.
 ties are included in the system set, we calculate the accuracy and weighted-F1 scores on the labeled entities in the system set. The performances are presented in Table 6. We can get similar observations for the system set.
In this subsection, we present several case studies to show the effectiveness of the proposed ELP framework. We first show the most popular auxiliary nodes with their labels learned from ELP and explain how such auxiliary informa-tion can help detect new entities from search query logs. Then we give some examples of the hidden connections we explored in LPA. After that, we list several multi-type aux-iliary nodes discovered in LPD. At the end, we will analyze Table 7: The most popular auxiliary information learned from ELP (A) on the gold dataset.
 the potential to disambiguate multi-label entities in the pro-posed ELP framework.
Given the gold dataset, we first randomly select 1% of entities as seeds and run ELP on the EA graph. Then we apply a threshold (e.g., larger than 10) to select the most popular contextual words and clicked URLs separately. Af-ter that, we group the auxiliary nodes according to their learned types and rank nodes in each group by the learned probability value in a decreasing order. Due to space limit, we only show the top 5 related auxiliary information learned from ELP (A) for the gold dataset in Table 7. We can ob-serve that people care about the education, real estate and weather very much when they search about local entities.
With the learned types of auxiliary nodes in Table 7, we can discover new entities easily. For example, if a new TV series is released, we can detect it as a new entity when peo-ple search with the word  X  X pisode X . In our experiments, we consider those entities appearing few times (  X  2) as new en-tities and ELP can learn their types correctly. For instance,  X  X ogo X  (an online game) appears only twice and ELP de-tects it as a media entity because its connected contextual words are  X  X pp X  and  X  X pad X . However,  X  X ogo X  refers to a mu-sical artist and a comic strip in Wikipedia. Therefore, the ELP method helps us discover  X  X ogo X  as a new media entity, and we can add such information to the current knowledge graph.
Now we analyze how the hidden connections we explored help LPA fully propagate labels. We focus on those entities with few connected contextual words and give some exam-ples of the entities with their hidden connections we discov-ered from the gold dataset as shown in Table 8. It can be ob-served that the hidden connections provide complementary and discriminative information for learning entity types. For example, given the entity  X  X ig Brother X  and its connected contextual word  X  X BS X , the word  X  X howtime X  help predict  X  X ig Brother X  as an entity of media with more confidence. In addition, the hidden connections can help learn entity types more correctly. Take the entity  X  X eorge Clooney X  as an instance. The connected contexts  X  X ovie X  and  X  X ew X  are a bit ambiguous and  X  X eorge Clooney X  may be considered as an entity of media because  X  X ovie X  is more related to media. Table 8: Some entities and their connected contex-tual words from query logs. The hidden connected contextual words explored by LPA are also shown in the last column.
 Table 9: The multi-type auxiliary information dis-covered from LPD on the gold dataset.
 However, if we connect  X  X eorge Clooney X  with  X  X edding X , we can easily learn that  X  X eorge Clooney X  is an entity of per-son. Therefore, exploring the hidden connections in query logs can help learn entity types more accurately.
Here we analyze the effectiveness of discovering and re-moving the multi-type auxiliary information from LPD. We list the 10 most ambiguous auxiliary nodes discovered from the gold dataset in Table 9. It can be observed that these auxiliary nodes are related to different types of entities and getting rid of them can help propagate labels more accu-rately in LP. For example, the contextual word  X  X nline X  can refer to the online information of a place, a movie and a per-son. In addition, the clicked URL  X  X n.wikipedia.org/wiki/ X  is related to a navigational website that contains diverse in-formation. It is difficult to detect the type of an entity if such URLs are connected with the target entity. Therefore, we identify the ambiguous auxiliary nodes and remove them from our constructed graph.
According to the proposed ELP framework, we can get a list of non-negative real numbers indicating the probabilities that an entity relates to a type. We clamp these probabilities to 0/1 values for simplicity in the performance evaluation. However, in reality, a lot of entities have more than one type. In this subsection, we analyze the potential of ELP to disambiguate multi-label entities.

We first analyze those entities without ambiguity. From our experimental results, ELP gives high probability values to the types of these entities. For example, given the en-tity  X  X old Digger X , ELP learns a probability value of 0 . 94 for the media type. Similarly,  X  X  X alt Disney World X  has a probability value of 0 . 80 for the local type.

We also focus on those multi-type entities to see whether it is easy to disambiguate them in our experiments. We take the entity  X  X axwell X  as an example. ELP learns it as a local entity with a probability of 0 . 48 and a person entity with a probability of 0 . 37. Table 10 shows some connected Table 10: Some auxiliary nodes for a multi-label en-tity  X  X axwell X . We group them according to their types learned from ELP.
 (a) Effect of seed entites
Figure 3: Parameter analysis on the gold dataset. auxiliary nodes for  X  X axwell X . We group them according to their labels learned from ELP and rank them by their probability values in a decreasing order. It can be observed that the auxiliary nodes for  X  X axwell X  as a local entity and a person entity are very different. Hence, ELP provides the potential for us to further split the entity node  X  X axwell X  into two nodes to better build the entity-auxiliary graph from search query logs.
In this subsection, we assess the benefit of ELP with dif-ferent amounts of seed entities. We focus on the EC and EA graphs extracted from the gold dataset and fix other param-eters. Figure 3 (a) shows the classification accuracies. We find that the performances become better when we increase the seed entities. Moreover, the results stabilize when we use more than 10% of seed entities.

We also demonstrate the effect of the numbers of deleted nodes in Figure 3 (b). Here we fix 1% of seed entities and other parameters, but vary the numbers of deleted nodes. It can be observed that the best accuracy is achieved when we remove 10 multi-type auxiliary nodes. The noise may still exist if we get rid of too few nodes and the performance can-not be improved dramatically. However, information may be imcomplete if we delete too many nodes as shown in Figure 3 (b). So in our experiments, we set the number of deleted nodes as 10.
Entity extraction and classification have rapidly developed over the past few years [1, 5, 26]. Several methods are pro-posed to extract entities from web documents [3, 30] and the disambiguation of entities from news articles is studied in [14, 20]. In recent years, the extraction and classifica-tion of entities over query logs receive a lot of attention [27, 26, 16, 1, 17] and disambiguating entities in queries is also investigated in [6]. However, all these existing methods do not fully explore the importance of the auxiliary information related to entities. Our study is different since we encode en-tities and the important auxiliary information together into a bipartite graph and learn the types of both entities and auxiliary nodes simultaneously.

The graph-based label propagation method is also very popular in information retrieval tasks [18, 33]. Li et al. use click graphs, a bipartite-graph representation of click-through data from search query logs, to improve query in-tent classifiers [18]. Spam webpages are detected using the link structure of the click-through bipartite graph in [33]. It propagates spam scores iteratively between queries and URLs from a few seed pages/sites. In our work, we focus on learning entity types from search query logs, which dif-fers from the task in [18, 33]. In addition, these models do not consider the sparsity and noise issues in search query logs. Our work proposes two separate strategies to explore hidden connections in the constructed bipartite graph and detect noisy information in query logs.

Besides the label propagation, many other learning meth-ods, including Markov random walks [15], learning with lo-cal and global consistency [36, 8] and manifold regulariza-tion [4], are based on graphs. Furthermore, Chang et al. [7] proposed an unsupervised embedding scheme on graphs with heterogeneous components. Their method systemati-cally captures network similarity between pairwise nodes by a deep learning framework. Though these models differ in their optimization objectives, they all share the same under-lying assumption that if two samples are close in the intrinsic geometry of an input space, their conditional distributions will be similar.

The entity-oriented analysis of query data is also related to our work [13, 32]. For example, class attributes are ex-tracted from search query logs for entities in [29, 28] as a complement source for existing knowledge bases. Based on the attributes, synonymous query intent templates are iden-tified in [19]. In addition, entity-related search actions, an-notations, and recommendation systems are studied in [22, 21, 34], respectively. However, our work is different from them since we study the problem of detecting entity types from search query logs.
In this paper, we study the problem of discovering entity types from search query logs. In order to take advantage of word patterns and user feedbacks (e.g., clicked URLs) from query logs, we construct a bipartite graph to encode entities and the important auxiliary information together. Based on this, the framework ELP is proposed to simultaneously learn types of both entities and auxiliary signals. In order to effectively learn node types, two separate strategies LPA and LPD are proposed and incorporated into ELP. Extensive empirical studies are conducted on real-world search logs to evaluate the effectiveness of the proposed ELP framework. There are several interesting directions for future work. Since the constructed bipartite graph from search query logs reflects the most popular trending of entities, it can provide complementary information for the current knowledge bases. One direction of our future work is to explore the possibil-ity of incorporating the built graph to the current knowl-edge bases. Another potential direction is to further dis-ambiguate multi-label entities effectively based on the built bipartite graph, which is a hot but challenging problem in recent years. This work is supported in part by NSF through grant CNS-1115234, Yahoo, and the Pinnacle Lab at Singapore Man-agement University. [1] A. Alasiry, M. Levene, and A. Poulovassilis. Detecting [2] R. Baeza-Yates and A. Tiberi. Extracting semantic [3] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, [4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [5] K. Bellare, C. Curino, A. Machanavajihala, P. Mika, [6] R. Blanco, G. Ottaviano, and E. Meij. Fast and [7] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, [8] S. Chang, G.-J. Qi, C. C. Aggarwal, J. Zhou, [9] S. Chaudhuri, V. Ganti, and D. Xin. Exploiting web [10] N. Dalvi, M. Olteanu, M. Raghavan, and P. Bohannon. [11] C. Ding, T. Li, and D. Wang. Label propagation on [12] A. Grove and D. Schuurmans. Boosting in the limit: [13] D. Hakkani-T  X  ur, A. Celikyilmaz, L. Heck, and G. T  X  ur. [14] J. Hoffart, Y. Altun, and G. Weikum. Discovering [15] M. Jaakkola and M. Szummer. Partially labeled [16] A. Jain and M. Pennacchiotti. Open entity extraction [17] A. Jain and M. Pennacchiotti. Domain-independent [18] X. Li, Y. Wang, and A. Acero. Learning query intent [19] Y. Li, B. Hsu, and C. Zhai. Unsupervised [20] Y. Li, C. Wang, F. Han, J. Han, D. Roth, and X. Yan. [21] G. Limaye, S. Sarawagi, and S. Chakrabarti.
 [22] T. Lin, P. Pantel, M. Gamon, A. Kannan, and [23] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [24] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and [25] T. Mikolov, W. Yih, and G. Zweig. Linguistic [26] P. Pantel, T. Lin, and M. Gamon. Mining entity types [27] M. Pa  X sca. Weakly-supervised discovery of named [28] M. Pa  X sca. Attribute extraction from conjectural [29] M. Pa  X sca and B. V. Durme. What you seek is what [30] M. Pennacchiotti and P. Pantel. Entity extraction via [31] J. Pound, P. Mika, and H. Zaragoza. Ad-hoc object [32] J. Reisinger and M. Pa  X sca. Fine-grained class label [33] C. Wei, Y. Liu, M. Zhang, S. Ma, L. Ru, and [34] X. Yu, H. Ma, B. Hsu, and J. Han. On building entity [35] J. Zhang, X. Kong, L. Jie, Y. Chang, and P. Yu. Ncr: [36] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [37] X. Zhu and Z. Ghahramani. Learning from labeled
