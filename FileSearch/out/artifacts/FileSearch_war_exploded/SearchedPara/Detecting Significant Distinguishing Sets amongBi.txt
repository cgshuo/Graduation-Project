 A fundamental task of data analysis is comprehending what distinguishes clusters found within the data. We present the problem of mining distinguishing sets; which seeks to find sets of objects or attributes that induce the most in-cremental change between adjacent bi-clusters of a binary dataset.Viewing the lattice of bi-clusters formed within a data set as a weighted directed graph, we mine the most significant distinguishing sets by growing a maximal-cost spanning tree of the lattice.
 Categories and Subject Descriptors: I.5.3 [Pattern Recog-nition]: Clustering General Terms: Algorithms
Mining bi-clusters in binary data (closed itemsets, formal concepts, subspace clusters, maximal edge bipartite cliques) is a common data mining task which has proven it X  X  util-ity in bioinformatics, basket data analysis, text mining, web mining, and recommender systems [1, 2]. Typically, the set of all bi-clusters tends to be large, which makes reasoning about the bi-clusters a challenging task. Increasingly, meth-ods for reasoning about the bi-clusters incorporate the lat-tice structure for visualization and rule generation [4, 3]. In this paper we propose the new data mining task of discov-ering distinguishing sets which makes use of the bi-cluster lattice. The goal of this task is to discover sets of attributes and / or objects that most distinguish the neighboring bi-clusters of a dataset from each other. Enumerating and exploring the distinguishing sets may be thought of as a useful way to reason about the relationship that exists be-tween bi-clusters. We define the distinguishing sets as the difference between a bi-cluster and its parent in the lattice. However, the lattice tends to be crowded and several paths exist between a bi-cluster and it X  X  ancestors. This motivates the key question: which distinguishing sets are most signif-icant? We view the bi-cluster lattice as a directed graph in which the edges are weighted by the degree of distinc-tion between a bi-cluster and its upper neighbor. With this formulation we may transform the problem of discovering significant distinguishing sets into the problem of growing a maximum cost spanning tree in the graph. Furthermore we intend for this data analysis task to be performed  X  X n the fly X , implying that this problem is at least as difficult as mining bi-clusters.
A database D = ( O, A, R ) consists of a set of objects O , set of attributes A and a relation R between O and A . For a set X  X  O of objects we define X  X  = { a  X  A | aRx for all x  X  X } , the set of attributes common to the objects in X . For a set Y of attributes we also have Y  X  = { o  X  O | oRy for all y  X  Y } . A concept or bi-cluster of D is a pair &lt; X, Y &gt; such that X  X  O , Y  X  A , X  X  = Y and Y  X  = X . The set of all concepts of a database D form a complete lattice ordered by set inclusion.

Given two concepts C 1 = &lt; X 1 , Y 1 &gt; and C 2 = &lt; X such that C 2 is an upper neighbor of C 1 ,intuition tells us that the larger | X 2  X  X 1 | and | Y 1  X  Y 2 | , the more distinct C C 2 are. However, what is more important at distinguishing concepts, the distinction in terms of objects or in terms of attributes? This may depend on the application, but in gen-eral to quantify the distinction between concepts both need to be considered. Viewing the concepts as maximal rectan-gular sub-matrices of the original data set can help visualize this scenario. Let the number of objects in a concept corre-spond directly to the height of the rectangle and the number of attributes correspond directly to its width.

When both height and width change significantly between a concept and its upper neighbor, is when the distinction between them is most significant. Another interesting ob-servation is that starting at the infimum of the lattice and following any path to the supermum, concepts gradually change shape from elongated rectangles height-wise to elon-gated rectangles width-wise in the concept lattice. Thus by quantifying the degree of  X  X hape change X  between a concept C i and C i +1 along a path P in the concept lattice we can capture the degree of distinction between concepts. The first step in capturing distinction is to define a metric that cap-tures the  X  X hape X  X f a concept. One option is to compute the ratio of width to height (height to width). Given a concept C = &lt; X, Y &gt; it X  X  shape index  X  is: s 1 is maximized when a concept is perfectly square, more-over  X  does not distinguish between concepts that are elon-gated width-wise or elongated height-wise. This property is essential as we wish to capture the change of all shapes and not bias the measure towards any particular shape. An-other option is to simply compute the area of the rectangle. This area will correspond directly to the number of 1 X  X  in the concept.
By computing the magnitude of change of  X  between a concept C i = &lt; X i , Y i &gt; and one of its upper neighbors C i +1 = &lt; X i +1 , Y i +1 &gt; along a path P we capture the intu-ition discussed above. Computing this change corresponds to the magnitude of the gradient. where s j is the chosen shape metric. The partial derivatives in equation 3 capture how s j changes with respect to the change in X and Y along a path P n = C 1 C 2 , . . . , C n we may compute the partial derivatives utilizing the forwar d difference operator as follows:
We now present the MDS algorithm which simultaneously enumerates the concepts of a dataset D , builds the concept lattice and grows a maximum cost tree in the concept lat-tice. Utilizing Prim X  X  algorithm we grow the maximum cost spanning tree by first rooting our tree at a desired concept (usually the infimum). Next we compute the next edge to be added to the tree by generating the upper neighbors of the infimum and computing the distinction between each upper neighbor and the infimum as determined by equations 4 and 5. The tree is then grown by greedily selecting the minimum weight edge and the associated concept. This processes is repeated until all concepts have been visited. Every edge in the spanning tree corresponds to exactly two distinguishin g sets (attribute and object distinguishing set). Computing the upper neighbors is the major computational burden of MDS . Lindig has given the algorithm Neighbors to compute the upper neighbors of a concept [5]. Neighbors computes the upper neighbors of any given concept c based solely on c , without the use of any complicated data structures such as prefix tress. For exact details and proof of correctness refer to [5]. Neighbors follows a generate and test strat-egy and runs in O ( | O | 2  X | A | ) time ( O is the set of objects and A is the set of attributes). We introduce two major op-timizations to Lindig X  X  algorithm that greatly improve the practical running time, but do not impact the theoretical complexity. First, we only wish to consider large bi-cluste rs that satisfy minimum constraints on both the number of objects and attributes they contain. Second, we improve the generate and test strategy by combining the two steps resulting in a reduced number of set intersections and set difference operations.
For performance comparison, we used the original source code of CHARM-L [4] and implemented Neighbors in C++ using the STL libraries and data structures. It must be noted that CHARM-L and MDS perform different mining tasks; however, one approach to growing the distinguishing-set tr ee would be to construct the concept lattice using an algorithm such as CHARM-L followed by a post-processing step. Because CHARM-L enumerates closed itemsets we can only specify the minimum support level which in turn corresponds to the minimum fraction of objects a concept should contain. Thus we set MDS to mine concepts with the same minimum sup-port level, and disabled the minimum attribute constraint to ensure a fair comparison. Three real-world datasets (GO, Pheno, Micro) were chosen from the bioinformatics domain for performance tests (available at www.cs.uc.edu/  X  alqadaf). All three data sets were fairly sparse, and thus low levels of support were of most interest. We observed that on all data sets MDS outperforms CHARM-L at low levels of support. For example CHARM-L was unable to terminate in under 2 hours on the Pheno dataset with support levels below 0.7 %, while MDS terminated in under 15 minutes. Moreover, CHARM-L re-quires access to all concepts in order to update the lattice structure. On the other hand, MDS computes the neighbor-hood of a concept without this requirement. This results in a much reduced memory footprint and allows for better scaling at low levels of support.
In this paper we have introduced the novel idea of mea-suring distinction among adjacent concepts by growing a maximum cost spanning tree in the concept lattice. Our al-gorithm, MDS , simultaneously mines the concept lattice and computes the distinguishing-set tree. Moreover our perfor -mance studies reveal that MDS is especially effective at low levels of support, which is critical when dealing with spars e data. [1] Madeira S.C. and Oliveria A.L. Biclustering [2] Shiram Narayanswami and Raj Bhatnagar. A [3] B. Ganter, and R. Wille. Formal Concept Analysis: [4] Mohammed J. Zaki and Ching-Jui Hsiao. Efficient [5] Christian Lindig. Fast Concept Analysis. 8th
