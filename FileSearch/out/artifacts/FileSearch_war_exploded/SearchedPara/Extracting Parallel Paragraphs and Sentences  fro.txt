 A parallel corpus is made up of sentences of two languages that each sentence from one language is a translation of the sentence in the other one [1]. Sentence aligned corpora are necessary for the task of stat istical machine translation. Furthermore, there are many other applications of parallel corpora such as lexicography and language analysis [2]. The importance of parallel corpus increased since the IBM translation models [3] has been introduced . One of the main steps of building a parallel corpus is sentence alignment [4], so in order to obtain a sentence aligned corpus, it is necessary to perform sentence alignment on bitext documents. Sentence alignment is the task of mapping each sentence in the source language to its corresponding sentence (or sentences) in the target language [5]. In addition, if the before the sentence alignment task, because knowing paragraph boundaries help to reduce candidate space for sentence alignment. There are some ways to collect bilingual texts. Using widely translated books such as holy religious books [6], multi-lingual catalogues [7], or getting parallel texts from web [8] are some of the ways for collecting bitext data. Therefore, building appropriate parallel text is a hardship for under-resourced languages similar to Persian language. Persian is a variation of Arabic-script language that is mostly spoken in Iran, Afghanistan, Tajikistan and some parts of India and Pakistan. One of the most challenging issues of Persian language processing and information retrieval (IR) is the lack of feasible corpus. For example, to our knowledge, there is no syntactically and/or semantically tagged corpus in Persian. There are some bilingual Persian-English corpora. One of them is introduced in [9]. Another Persian-English parallel corpus is introduced in [10] that is made of Persian-English aligned movie subtitles wherein the Persian parts are in colloquial sh ape instead of official Persian language that is used in academic and governmental organizations and medias. Therefore, it is required to have a corpus of official Persian language. The process of manually aligning corpora is a time consuming task, where intelligent computer programs may help reduce the time, so automatically building parallel Persian-English corpora made sentence is a hot topic. 
In this paper, considering the need for a feasible parallel Persian-English corpus, we propose a hybrid approach to extract aligned paragraphs and sentences from translated documents. We used some clues such as paragraph or sentence length, punctuation marks and a small bilingual lexicon of simple one-word nouns. In the following sections, after reviewing some related works on sentence alignment in Section 2, we describe our proposed method in Section 3. We have done two experiments on Persian-English data, one for paragraph alignment and one for sentence alignment that are described in Section 4. Finally, Section 5 concludes the paper. There have been many sentence alignment models in recent years that can be categorized into three model types: 1) length-based approaches, 2) Lexical matching approaches, and 3) Hybrid approaches. Furt hermore, there are some other approaches such as measuring cognate similarity between sentences. Most of the works on sentence alignment models are based on the assumption that paragraph anchors are aligned [11-15]. Even though, in some works, candidate sentences are chosen based on a window size of adjacent sentences in the text without any information about paragraph anchors [16]. The first attempts on sentence alignment were done based on length-based models. In the length based approach, it is assumed that the sentence pairs of source-target languages are similar in their length. For example, the sentences in German-English parallel corpus have a correlation of 0.91 [17]. The first attempts on length-based approaches were in [11, 12, 17]. Not only this model is very simple and language independent, but also it can gain global optimum [18]. On the other hand, small deletions and insertions decrease the accura cy drastically [14] and error propagation may happen [18]. The second approach is based on lexical matching. In this approach, bilingual lexicons are used as guides to alignment. The early works on this approach were done by [14, 15, 19, 20]. 
In the third approach, the combination of statistical and linguistic features (such as bilingual lexical matching and simple linguistic clues) is used as a guide to the alignment task. The main reason to use this approach is that for many languages, simple statistical approaches do not gain enough accuracy and there is a need for new approaches to overcome this problem [21]. Most of the recent works on sentence alignment is based on hybrid models. In [13], three phases were used in order to extract aligned sentences. In the first phase, some aligned sentences were extracted via length based models. In this phase, a threshold was considered on length similarity to select only reliable parallel sentences in the corpora. In the second phase, IBM model 1 was used on the extracted aligned corpora and a bilingual lexicon was built. In the last phase, the program used both the gained lexical information and length similarity to find aligned sentences. This work became state of the art in its time and many other researchers improved this model. In [22], a combination of dynamic programming (DP) and divisive clustering was used to improve Moore's model[13].In this work, DP allows many-to-many alignments and divisive clustering refines those alignments with iterative binary splitting. In [23], a two step clustering approach was used to improve both accuracy and efficiency of Moore's model. In the first step the program finds a model-optimal alignment made up of possible 0/1 to 0/1 alignments and in the second step, it merges those alignments into larger ones. That method was 550 times faster than the work in [22]. 
In [24] cognates similarity (similarity based on transliteration) was used as a measure of similarity. In [25], the order of punctuation marks in bitext and lexical information were combined to achieve aligned sentences. In [26], the combination of punctuation marks, cognates and length similarity was used to find better alignments. In addition, probabilistic neural network (P-NNT) and Gaussian mixture models (GMM) were used to combine those features. In [5], a modification of the Champollion in [27] was proposed. This approach was based on a hybrid model that optimized the process of splitting the bilingual texts into small parts for alignment. In [28], an iterative model was used to improve alignment accuracy. That work was an extension of [29] that was used for aligning OCR generated texts. In that work, a length based approach was used in the initialization phase. In the next iterations, a statistical machine translation (SMT) model was built and based on that model; bleu measure was used in the next iterations to compare the translated text by the translation model to the candidate sentences. In the last iteration, the final SMT model was built from the sentence aligned corpora. In [30], a bootstrapping algorithm was done on bitext based on cosine similarity measure to measure similarity of the documents based on TF-IDF. In [31], Wikipedia was considered as a good source of multilingual data with many noises such as sentences without translations. Some features such as date matching, same pictures in the pages and a little manually aligned train data for building bilingual lexicon was used as guides to alignment. In [32], based on the assumption that parallel web pages have similar page structures and translators respect the original structure of the document, an HTML tree alignment model was proposed using dynamic programming. In [33], a language independent context model based on Zipfian word vector was proposed to improve sentence alignment problem. The Zipfian word vector is a vector of values in the sentences based on the logarithmic division of the word frequency in the sentence context and a threshold. In that work, dynamic programming was used to align sentences. There are also some alignment works on movie subtitles based on simple clues such as sentence length and time overlap that some of these works are proposed in [34, 35]. One of the main problems we faced in Persian-English parallel corpus extraction is the lack of paragraph aligned corpora. Indeed , in this paper we focus on introducing a new model that can be used both for sentence alignment and paragraph alignment. In the study we have done on Persian-English bitext, we found that the most of the paragraph lengths are in the similar length order and using pure length-based models results in unreliable alignment. On the other hand, the methods used in [13] is employed IBM model 1 [3] which is not efficient for long paragraphs. The length of paragraphs in Persian is about 100 words (based on our test bed). The IBM model performance is proportional to the sentence length, where for long sentences the space of candidates and processing time increases. We chose three most efficient ones as: 1) length similarity, 2) punctuation mark similarity, and 3) semantic similarity between words of source and target paragraphs or sentences that is calculated exploiting a bilingual dictionary of nouns. 3.1 Feature Similarities For the length based similarity we used Poisson distribution that is employed in [13, 26, 31]. This distribution has only one parameter and is simpler than Gaussian distribution used in [11]. The Poisson distribution only needs the length rate between the source and target sentences as shown in (1) where l t and l s are sentence length of the target and source languages and r is the sentence length rate. For the punctuation similarity, we chose 11 different punctuation types that are comparable in Persian and English 1 . Equation (2) calculates the punctuation similarity score of each punctuation mark (punc i ), where min(s,t) and max(s,t) are the minimum and maximum number of occurrence of punctuation punc i in the source and target sentence or paragraph respectively. So, the overall punctuation probability will be as equation (3), where np is the number of distinct punctuations. We used a small dictionary of Persian one-word nouns and their English translations. In order to calculate the semantic similarity of sentences or paragraphs, we count the co-sentences or paragraphs as shown in (4) where ) ( i s dic and ) ( i t dic are the number noun in the source sentence or paragraph, if the corresponding translated word exists in the target sentence or paragraph, we count a co-occurrence score for that word. 3.2 Combining Similarities In order to combine similarity scores, we used a linear model similar to the mathematical union, in which each part of the equation is weighted by a coefficient as reason that we used this form is that it is simple to use and with this type of equation all types of linear combinations of variab les is considered, so the learner adapts appropriate weight to each coefficient. With this assumption, it is needed to find 18 unknown weight coefficients in (5). 3.3 Using Genetic Algorithm for Weight Learning We used genetic algorithm to find the unknown weight coefficients. It is worth to say that in[36], genetic algorithm was also used for sentence alignment. Elitism is used in the genetic algorithm in order to keep good chromosome in each generation. The fitness function is shown in (6). The precision of the weights of the chromosome in the training data is used in the equation to find the fitness. We evaluate the effectiveness of our proposed alignment method for both sentence and paragraph alignment. In the experiment 1 , we evaluated our method through paragraph alignment. The length rate for Poisson distribution has been chosen as 1. Uniform distribution was used to choose each chromosome gene for crossover in learning part using genetic algorithm. The uniform probability is chosen as 0.5 and the crossover rate as 0.9. The mutation rate is chosen as 0.01. In the experiment 2 , we used our method for sentence alignment. To stem English words, we used an open-source code of Porter stemmer [37] and for Persian words, we used lemmatization code proposed in [38] 2 . 4.1 Experiment 1: Paragraph Alignment In order to evaluate the effectiveness of semantic similarity and exploiting a bilingual dictionary, we relaxed (5) to (7) and compare the results. In this experiment, we extracted about 400 lecture paragraphs from the Iran supreme translation mode in this website is free and there are many free deletions and insertions in the translations. There are also some Arabic sentences (like holy Quran verses) during and aligned the paragraphs manually. Finally, we prepared a paragraph level aligned corpus with about 300 Persian paragraphs and about 350 corresponding English Paragraphs. The reason of difference between the numbers of Persian and English two or more paragraphs in target language as we have one-to-one, one-to-two, one-to-three, and one-to-four alignment in our corpus. To overcome the small training data issue, we used 6-fold cross-validation. The results of paragraph alignment using our proposed method are shown in Table 1. In order to compare the results with the baseline length-based model, we tested the baseline on this data and did not get alignment precision more than 35% considering only the length of paragraphs, which means that the pure length-based model is not appropriate enough for sentence alignment. 4.2 Experiment 2: Sentence Alignment After experiment 1, we developed a visual user-friendly software for manually binary classifier, all bilingual sentences pairs are labeled by our method as  X  X re-aligned X  or  X  X ot-aligned X . In this way, we are able to calculate both precision and recall. We also evaluate the effectiveness of punctuation similarity relaxing (5) to (8). The dataset of our test is composed of 26,108 aligned sentences (13,054 sentences in each language). The results are shown in Table 2, the punctuation similarity improved the model performance, but the improvement is not significant. The reason is that for sentences, numbers of punctuation marks are very small and it does not guide the model very well. On the other hand, in the paragraph level, there are much more punctuation marks which help the model predict better alignments. extracted by OCR but due to poor performance of Persian OCR software, rate of erroneous recognized words was very high. The problem of copyright in translations, un-uniform Persian characters, the lack of bilingual electronic texts and colloquial language typography in many Persian texts made the task in Persian harder than languages like English. As seen in the experiments, the performance of the model depends on the task. If it used to paragraph alignment, the punctuation mark and bilingual dictionary significantly improve the accuracy of the pure length-based model result about 35%. But in the sentence alignment, the length and number of words are small enough to suppress other features. Furthermore, the cognate similarity is not applicable to Persian-English texts, because the alphabet types of these two languages are different. Acknowledgement. This paper is funded by Computer Research Center of Islamic Sciences (CRCIS). I would also like to thank Dr. Shahram Khadivi, Dr. Heshaam Faili, Maryam Aminian, Sina Iravaninan, Dr. Morteza Analoui, Mehrdad Senobari and other people who help me on this work.

