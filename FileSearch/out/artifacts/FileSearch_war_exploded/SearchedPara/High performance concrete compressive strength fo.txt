 1. Introduction
High performance concrete (HPC) has been improved in recent years. HPC has been used in the construction industry for special structures (nuclear structures, tunnels, ports, bridges and precast units) because of its high strength. The conventional concrete consists of the three basic components (i.e., portland cement, fine and coarse aggregates, and water), however the making of HPC needs to incorporate supplementary cementitious materials, such as fly ash and blast furnace slag, and chemical admixture, such as superplasticizer ( Yeh, 1998a, 1998b ); Chang et al., 1996 ). Success-fully estimation of strength based on mix proportions is an important aim. Since the strength is a commonly used criterion in producing concrete. Most studies independently have shown that concrete strength is researched not only by the water to cement ratio but also by other materials. Kou et al. (2011 ) gave the results of a laboratory study on the performance of natural and recycled aggregate concrete prepared with the incorporation of different mineral admixtures including silica fumes, metakao-lin, fly ash and ground granulated blast slag and the compressive and splitting tensile strength were determined. Nochaiya et al. (2010 ) reported the workability and compressive strength results of portland cement, fly ash and silica fume systems. Kayali et al. (2003 ) investigated the effect of polypropylene and steel fibers on high strength lightweight aggregate concrete. The performance of silica fume, metakaolin, fly ash and ground granulated blast-furnace slag on the setting times of high-strength concrete was investigated using the penetration resistance method ( Brooks et al., 2000 ).

HPC includes of different kinds of complex materials and its modeling of behavior is a difficult task. The predicting concrete strength is an important issue in concrete construction, this study developed intelligent techniques for accurately predicting compres-sive strength of the concrete. Similar studies in the literature employed artificial neural networks (ANNs): ( Yeh, 1998a, 1998b ) and Yeh, 1999 ) aimed at demonstrating the possibilities of adapting
ANNs to predict the compressive s trength of high-performance concrete. A set of trial batches of HPC was produced in the laboratory and demonstrated satisfactory experimental results.
The study demonstrated that the strength model based on ANN was more accurate than a model based on regression analysis. ( Chou et al., 2011 ) developed a data-mining approach and performance measures to predict compressive st rength and assessed the predic-tion reliability for HPC. The proposed approaches were compared for performance outcomes to obtain a comprehensive comparison of the applied predictive techniques.

Artificial neural networks; fuzzy logic and their conjunction models widely used in literature ( Vasant et al., 2007a , 2007b ). Topcu and Sar X demir (2008b ) developed ANNs and fuzzy logic models for predicting the 7, 28 and 90 days compressive strength of concretes containing high-lime and low-lime fly ashes. In the models of the training and testing results demonstrated that
ANNs and fuzzy logic systems had strong potential for predicting 7, 28 and 90 days compressive strength of concretes containing fly ash. ( Seyhan et al., 2005 ) developed a three layer feed forward
ANN model having three input neurons, one output neuron and two hidden neurons to predict the ply-lay-up compressive strength. Gupta et al. (2006 ) used ANN technique to obtain more accurate concrete strength prediction based on parameters like concrete mix design, size and shape of specimen, curing technique and period, environmental conditions, etc. Also an effort to build the expert system for the problem was described in their paper to overcome the bottleneck of intricate knowledge acquisition and the results indicated that ANN was a useful technique for predicting the concrete strength. Trtnik et al. (2009 ) studied the analysis of such factors on the velocity X  X trength relationship. The relationship between ultrasonic pulse velocity, static and dynamic Young X  X  modulus and shear modulus was also analyzed. The multilayer feed-forward neural network was used for this purpose. The paper demonstrated that ANNs could be successfully used in modeling the velocity X  X trength relationship and the model gave easily and reliably estimate the compressive strength of concrete by using only the ultrasonic pulse velocity value and some mix parameters of concrete. Kasperkiewicz et al. (1995 ) applied an ANN of the fuzzy type for predicting strength properties of HPC mixes. Composition of HPC was assumed simplified, as a mixture of six components (cement, silica, super-plasticizer, water, fine aggregate and coarse aggregate). The system was trained randomly from the data set, and then tested using remaining examples. A significant enough correlation between the actual strength values and the values predicted by the ANN was observed in spite of data complexity, incomplete-ness, and incoherence.

Fuzzy neural network conjunction models are known promising methods in literature ( Vasant et al., 2012 ). Zarandi et al. (2008 ) developed fuzzy polynomial neural networks (FPNN) to predict the compressive strength of concrete. Two different architectures of FPNN were addressed (Type1 and Type2) and their training methods were discussed. The proposed FPNN was a combination of fuzzy neural networks (FNNs) and polynomial neural networks (PNNs). The results showed that FPNN-Type1 gave strong poten-tial as a feasible tool for prediction of the compressive strength of concrete mix-design but the FPNN-Type2 was recognized as unfeasible model.

Another important predictive approach is genetic algorithm ( Vasant and Barsoum, 2009 ). Yeh and Lien (2009 ) applied the discovery method, Genetic Operation Tree (GOT), which was composed of operation tree and genetic algorithm, to automati-cally produce self-organized formulas to predict compressive strength of High-Performance Concrete. The results showed that
GOT could produce formulas which were more accurate than nonlinear regression formulas but less accurate than ANN models.
The probabilistic neural network (PNN) was applied to the prediction of concrete compressive strength by Lee et al. (2009 ). PNN had the advantage over the conventional ANNs by utilizing lesser time in determining the network architecture and in training. They proposed the method called adaptive probabilistic neural network (APNN) used the dynamic decay adjustment (DDA) algorithm to automatically calculate the smoothing para-meter. The proposed technique proved to effectively estimate realistic values of concrete compressive strengths better than the conventional PNN. Lee (2003 ) developed the I-PreConS (Intelli-gent PREdiction system of CONcrete Strength) that provided in place strength information of the concrete to facilitate concrete form removal and scheduling for construction. The system was developed with ANNs that could learn cylinder test results as training patterns. The study showed that I-PreConS using ANN was very efficient for predicting the compressive strength devel-opment of concrete. Atici (2011 ) applied multiple regression analysis and ANN in estimating the compressive strength of concrete that contains various amounts of blast furnace slag and fly ash, based on the properties of the additives (blast furnace slag and fly ash in this case). The obtained results revealed that although multiple regression analysis was more accurate than ANN in estimating the compressive strength using values obtained from non-destructive testing, the ANN models per-formed better than did multiple regression analysis models.
The remainder of the paper is organized as follows. In Section 2 , the gradient boosting, bagging, discrete wavelet transform and ANNs methods are presented. In Section 3 , we present the dataset, application details of wavelet ensemble models and performance statistics. Moreover, this section reports the empirical results. In Section 4 , we compare the results with previous works and discuss why ensemble modeling works. Section 5 draws conclusions and future study directions. 2. Methods 2.1. Bagging
One of the most popular ensemble methods is bagging (acronym for bootstrap aggregating) proposed by Breiman (1996 ). Bootstrap resampling method ( Efron, 1979 ) and aggregating are the basis of bagging. It works as follows Ismail and Mutanga (2010 ). A replica dataset of size n is randomly dr awn with replacement from the original dataset of the n patterns. A bootstrap sample D l some in D multiple times, whereas others are not included. The idea of the bootstrap is that sampling from the actual dataset D is the best possible approximation for sampling from the unknown distribution P . Then a model is built by using this so-called bootstrap dataset. This procedure is repeated T timesandthusresultsin T models. Then T models are aggr egated by using the mean for regression problems. A more detailed version of bagging is described in Breiman (1999 ). The wavelet bagging ensemble model structure developed in the present study is shown in Fig. 1 .
 2.2. Gradient boosting
Boosting is an important ensemble machine learning method which slightly differs from bagging. The general proceeding of boosting is to build a sequence of models, where each model is trained on a re-weighted version of the original dataset. It works as follows ( Ismail and Mutanga, 2010 ). If a pattern is misclassi-fied, its weight is increased. Otherwise, the weight is decreased.
This causes the model in the forward iteration to focus on the patterns that were misclassified previously. Afterwards, an ensemble of models is created. The classifications of this ensem-ble are subsequently combined to form the final classifier ( Wezel and Potharst, 2007 ). First boosting algorithm was introduced by Schapire (1990 ). The most popular one is AdaBoost proposed by
Freund and Schapire (1996 ). This study employs the gradient boosting method which was introduced by Friedman ( Friedman, 2002 and Friedman, 2009 )). In each iteration, gradient boosting selects subsample of training data randomly without replace-ment. This subsample is then employed instead of original dataset to fit base predictor and estimate the model update for the new iteration. And the wavelet gradient boosting ensemble model structure built in the present study is shown in Figs. 2 and 3. 2.3. Discrete wavelet transform
Wavelet function C ( t ) called the mother wavelet, can be defined as through compressing and expanding C (t):
C t  X  X  X  a jj 1 = 2 C t b a a A R , b A R  X  1  X 
Subject to a a 0 where C a , b  X  t  X  is successive wavelet; a is scale time series f ( t ) A L 2 ( R ), f ( t ) can be defined as W fa , b  X  X  X  a jj 1 = 2 where C  X  t  X  is the complex conjugate function of C ( t )( Gumus et al., 2010 ). Assuming a  X  a j 0 and b  X  kb 0 a j 0 ; subject to a 4 1; where b 0 A R ; k and j are integer numbers, discrete wavelet trans-form of f ( t ) can be written as: W fj , k  X  X  X  a j = 2 0
The appropriate choices for a 0 and b 0 depend on the wavelet function. A common choice for them is a 0  X  2, b 0  X  1( Kisi and Cimen, 2011 ). Now, Eq. (3) becomes binary wavelet transform: W fj , k  X  X  X  2 j = 2
Assuming a discrete time series f ( t ) , which occurs at different time t , the DWT becomes; W fj , k  X  X  X  2 j = 2 where W C f ( j , k ) is the wavelet coefficient for the discrete wavelet 2.4. Artificial neural networks
An artificial neural network (ANN) is a simplified model of the human nervous system which made up of simple, highly inter-connected processing elements ( Rafiq et al., 2001 ). It has been used for reducing experimental work and time losses. Also it has been applied to many experimental studies ( Topcu and Sar X demir, 2008a ). According to ( Ince, 2004 ) the most important property of
ANN in engineering problems is their capability of learning directly from examples and utilization of experimental and field data directly, without assumptions is considerable advantage of
ANN. It can continuously re-train the new data, so that it can conveniently adapt to new data. ANN has been investigated to deal with the problems involving incomplete or imprecise infor-mation ( Yeh, 1999 ).

ANN does not need such a specific equation form. Instead of that, it needs sufficient input X  X utput data. The basic element of the method is the artificial neuron which are typically organized into layers linked via weights called the Perceptron, which is a mathematical model of a biological neuron and the neural approach consists of learning the relation between input and output data using mathematical training processes ( Dahou et al., 2009 ). Training is the process of adjusting the connection weights by repeatedly exposing the network to known input X  X utput data.
The most popular and successful training technique is error back-propagation learning method ( Chena and Wang, 2004 ). This study uses a conventional back-propagation artificial neural network.
The output signal for the lth neuron in the n th layer is given by y l t  X  X  j t is the time index and C n l  X  w n l network, the synaptic weight w n ji  X  t  X  is given by w ji t  X  1  X  X   X  w n ji  X  t  X  X  D w n ji  X  t  X  X  7  X  subject to l r n r N and it can be revised as given by
D w n ji  X  t  X  X  Z l n j  X  t  X  y n 1 i  X  t  X  X  8  X  subject to 0 o Z o 1 where Z is the learning rate, and l  X  t  X  @ E t =@ u n j is the local error gradient. To improve the back-propagation algorithm, a momentum term a is added
D w n ji  X  t  X  X  Z l n j  X  t  X  y n 1 i  X  t  X  X  a D w n ji t 1  X  X   X  9  X  subject to 0 o a o 1
For the output layer, the local error gradient is given by l  X  t  X  X  X  d j  X  t  X  y N j  X  t  X  j  X  u N j  X  t  X  e j  X  t  X  j where d j ( t ) is the goal output signal, and j ( U ) is the activation function . 3. Application and empirical results 3.1. Dataset and application
The experimental dataset obtained from a University of Cali-fornia, repository of data ( Yeh, 1998a, 1998b ), is used in this study. A final set of 1030 samples of ordinary portland cement containing different additives and cured under normal conditions were evaluated from various university research labs ( Chang et al., 1996 ; Chang, 1997 ; Chung, 1995 ; Giaccio, 1992 ; Gjorv et al., 1990 ; Hwang, 1996 ; Hwang, 1966 ; Langley, 1989 ; Lee, 1994 ; Lessard, 1993 ; Lin, 1994 ; Mo, 1995 ; Chou et al., 2011 ).
Table 1 shows the experimental data set of HPC attributes used in this research.

The experimental dataset is very common and is used and checked by many studies partly or wholly (i.e., Yeh, 1998a , 1998b , 1999 , 2003a , 2003b , 2006 ; Chou et al. 2011 ). Those studies employed various predictive models (i.e., artificial neural net-works, augment-neuron networks, multiple regression, support vector machine, multiple additive regression trees, bagging regression trees). This study used ANN ensembles and it is well known that mentioned previous studies have used different ANN models and two different ensemble learning method. Thus, it has already been proved that the dataset is reliable and single ANN models and ensembles of decision trees have been checked using this dataset.

A database of 1030 records is divided into training and testing groups using random sampling. The numbers of training and testing examples for these experiments are listed in Table 2 . For measuring the prediction accuracy we only consider the test sample because good learning (training) sample measures of the prediction accuracy give no guarantee for good test sample measures of the prediction accuracy.

Decent settings are very important for prediction accuracy of ensemble machines. The bagging parameters are the size of each bag (as a percentage); the number of iterations; and the number of seeds. In this case, the values for these parameters were 100, 10, and 1 for bagging, respectively. The best configuration para-meters for the gradient boosting are; the number of iterations is 10 and the shrinking is 1. In this study, the parameters for ANN are: the number of hidden layers is 5, 10 and 15; the number of hidden units is 10, 20 and 30; the learning rate is 0.2, 0.3 and 0.4; the momentum factor was 0.2, 0.3, and 0.4; and the training epochs are 500, 1000 and 1500. The experiments indicated that the best ANN parameters are as follows: the number of hidden layers is 10; the number of hidden units is 10, the number of the learning rate is 0.4; the momentum factor is 0.2; and the training epochs are 1000. The wavelet-ensemble models are obtained combining discrete wavelet transform (DWT) and ANN ensem-bles. The ensemble models use transformed data components obtained using DWT on original data. For the model inputs, the original data were transformed by Haar DWT algorithm. 3.2. Performance statistics and empirical results
The mean absolute error (MAE), root mean squared error (RMSE) and coefficient of determination R 2 performance statistics are used to evaluate the performance of the ANN, bagged ANN (BANN), gradient boosted ANN (GBANN), wavelet bagged ANN (WBANN) and wavelet gradient boosted ANN (WGBANN) models. Coefficient of determination ( R 2 ): R  X  where y  X  actual value; y 0  X  predicted value; and n  X  number of data samples.
 Root mean squared error (RMSE): RMSE  X  Mean absolute error (MAE): MAE  X  1 n
The statistic results of different predictive models are sum-marized in Tables 3 and 4 . Table 3 indicates the results of first empirical study. It can be noted from the table that the WGBANN model has the best performance with highest R 2 (WGBANN  X  0.9528) for determining R 2 statics during the testing period. The other wavelet ensemble model WBANN is the second best model with R 2  X  0.9397. And the simple ensembles of ANN models,
BANN ( R 2  X  0.9278) and GBANN ( R 2  X  0.9270), have almost same but better performance than the ANN model ( R 2  X  0.9088). MAE and RMSE statics are inconsistent with the determination of correlation statics. The best model for minimizing RMSE (3.30 MPa) and MAE (4.54 MPa) is WBANN, the second best model is BANN (RMSE  X  4.87 MPa, MAE  X  3.60 MPa).

Table 4 shows the results of second empirical study. The wavelet ensemble models, WGBANN ( R 2  X  0.9326) and WBANN ( R 2  X  0.9303), are superior to other predictive models. The GBANN model is the third best model with R 2  X  0.9080. Moreover, the BANN model ( R  X  0.9006) also yields better result than the ANN model ( R  X  0.8921). Table 4 indicates the direct relationship between R
MAEandRMSE.ThebestmodelforminimizingRMSE(4.51MPa) and MAE (3.36 MPa) is WGBANN, and the second best model is WBANN (RMSE  X  4.87 MPa, MAE  X  3.73 MPa)..

The actual and predicted strength distributions of the empirical study 1 and study 2 for testing period are depicted with boxplots presented in Figs. 4 and 5 The boxes indicate the interquartile ranges (5 th and 95 th percentile of actual and predicted data), dots indicate values outside the range and the horizontal line within each boxes indicate the median values. When the statistical distributions of the predicted strength and the actual data were compared, the overall performance of GBANN, BANN and ANN were good when compared to the patterns of the actual strength data, but the WBANN and WGBAN models did a fairly better job at capturing the observed data in both cases. Figs. 6 and 7 depict the distributions of the proposed predictive models and the measured strength data statistically. The figures show that the best results obtained by WBANN and WGBANN give a better fit to a straight line than ANN, BANN and GBANN do, which indicates that wavelet ensemble models are more accurate for forecasting
HPC compressive strength. 4. Discussion 4.1. Comparison with previous works
It is well recognized that prediction of concrete strength is very essential in concrete constructions . Thus, for years, researchers have proposed various models for opti mizing the prediction accuracy of theHPCcompressivestrength. Table 5 shows the selected previous studies in predicting the HPC compressive strength. Chou et al. (2011 ) used tree-based ensemble mod els (bagging regression trees
BRT and multiple additive regression trees MART (a conjunction model of gradient boosting and regression trees)) in HPC compres-sive strength forecasting, first. This study employed bagging and gradient boosting methods in building ANN ensembles, first. They reported that BRT yielded R 2  X  0.8904 (average) and MART yielded
R  X  0.9108 (average). In this study, Bagged ANN yields R 2  X  0.9278 and Gradient Boosted ANN yields R 2  X  0.9270. Our findings approx-imate previously reported results. Moreover, for enhancing predic-tion accuracy of ANN ensembles, the study also incorporates discrete wavelet transform in creating wavelet ANN ensemble models, first. For determining R 2 statics, the study obtains more satisfactory results ( R 2 WBGANN  X  0.9528 &amp; R 2 WBGANN primary previous studies did. 4.2. Why ensemble models work
Ensemble learning is a machine learning procedure where multi-ple learners are trained to solve the same problem. Like many other applications, concrete compres sive strength forecasting suffers from concurrent negative effects by the noise. The noise disrupts the training data in model building process. Thus, it may affect the prediction accuracy. Bagging and boosting introduce certain mechanisms to reduce the influence of the noise ( Wang et al., 2012 ).
Combining multiple instances of the same model type can reduce the variance and drastically improve predictive performance.
The best enhancement by ensem ble machines is when the model instances are very different from each other ( Wang et al., 2009 ).
In contrast to ordinary learning machines that try to learn one hypothesis from the learning data, ensemble machines try to build a set of hypotheses and integrate them to use ( Wang et al., 2011 ).
Ensemble machine learning tends to be a very effective procedure when applied to unstable learning algorithms such as decision trees and neural networks ( Pino-Mejias et al., 2008 ). For example,
Breiman (1996 ) has shown that the bagging predictor variance is smaller than or equal to the variance of a simple predictor, leading to increasing prediction accuracy ( Louzada et al., 2011 ). 5. Conclusions
Until now, various high performance concrete (HPC) compres-sive strength forecasting models based on the machine learning methods have been developed. In this study, the prediction accuracy of ensemble models (bagged ANN and gradient boosted ANN) and wavelet-ensemble models (wavelet-bagged ANN and wavelet-gradient boosted ANN) have been investigated for forecasting HPC compressive strength and the obtained results are compared with the conventional ANN model. The bagging and gradient boosting ensembles of artificial neural networks are built in (HPC) compressive strength forecasting, firstly. The enhance-ment in the empirical results we yield in this paper by the application of the ensemble methods is clearly noticeable, but not overwhelming. We think that this may be due to the high amount of noise in the data we use for our empirical study. Therefore, we use the coupling of discrete wavelet transform (DWT) and two ensemble techniques as effective models for HPC compressive strength forecasting. In application, we reconstruct only the approximations of original attributes by using Haar DWT. Thus, we reduce the noise in the data.

The results from this study show that (i) ensemble models are promising techniques on HPC compressive strength forecasting and yields better results than a conventional ANN model (ii) two ensemble models give nearly same results (iii) wavelet gradient boosted ANN model is better than wavelet bagged ANN model (iv) DWT can significantly increase the accuracy of the ANN ensembles. Finally, the methods we outline in this paper can be easily used in prediction applications of material science. For future work we propose to investigate the potential usage of poly ensemble models for HPC strength forecasting.
 Acknowledgement The authors wish to express their gratitude to Professor
I-Cheng Yeh and UC Irvine Machine Learning Repository ( http:// archive.ics.uci.edu/ml/ ) for sharing the experimental data set and also would like to thank the editor and anonymous referees for their helpful comments.

Appendix A. The pseudo-code of bagging algorithm ( Wang et al., 2012 ).

Input: Training sample  X  {( x 1 , y 1 ),( x 2 , y 2 ), y ,( x Process: Output: H ( x )
Appendix B. The pseudo-code of gradient boosting ( Ridgeway, 1999 ).

Input: Training sample  X  {( x 1 , y 1 ),( x 2 , y 2 ), y ,( x Process: Initialize model with a constant value: Output: F 0 ( x ) References
