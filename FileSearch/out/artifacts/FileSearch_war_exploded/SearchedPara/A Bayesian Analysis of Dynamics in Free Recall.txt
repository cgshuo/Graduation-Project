 Department of Computer Science Modern computational models of verbal memory assume that the recall of items is shaped by their semantic representations. The precise nature of this relationship is an open question. To address it, recent research has used information from diverse sources, such as behavioral data [ 14 ], brain imaging [ 13 ] and text corpora [ 8 ]. However, a principled framework for integrating these different types of information is lacking. To this end, we develop a model of human memory that encodes probabilistic dependencies between multiple information sources and the hidden variables that couple them. Our model lets us combine multiple sources of information and multiple related memory experiments.
 Our model builds on the Temporal Context Model (TCM) of [ 10 , 16 ]. TCM was developed to explain the temporal structure of human behavior in free recall experiments, where subjects are presented with lists of words (presented one at a time) and then asked to recall them in any order. TCM posits a slowly changing mental context vector whose evolution is driven by lexical input. At study, words are bound to context states through learning; during recall, context information is used as a cue to probe for stored words. TCM can account for numerous regularities in free recall data, most prominently the finding that subjects tend to consecutively recall items that were studied close in time to one another. (This effect is called the temporal contiguity effect .) TCM explains this effect by positing that recalling an item also triggers recall of the context state that was present when the item was studied; subjects can use this retrieved context state to access items that were studied close in time to the just-recalled item. The fact that temporal contiguity effects in TCM are mediated indirectly (via item-context associations) rather than directly (via item-item associations) implies that temporal contiguity effects should persist when subjects are prevented from forming direct item-item associations; for evidence consistent with this prediction, see [9].
 Importantly, temporal structure is not the only organizing principle in free recall data: Semantic relatedness between items also influences the probability of recalling them consecutively [ 11 ]. Moreover, subjects often recall semantically-related items that were not presented at study. (These are called extra-list intrusions; see [ 15 ].) To capture this semantic structure, we will draw on probabilistic topic models of text documents, specifically latent Dirichlet allocation (LDA) [ 3 ]. LDA is an unsupervised model of document collections that represents the meaning of documents in terms of a small number of  X  X opics, X  each of which is a distribution over words. When fit to a corpus, the most probable words of these distributions tend to represent the semantic themes (like  X  X ports X  or  X  X hemistry X ) that permeate the collection. LDA has been used successfully as a psychological model of semantic representation [7].
 We model free recall data by combining the underlying assumptions of TCM with the latent semantic space provided by LDA. Specifically, we reinterpret TCM as a dynamic latent variable model where the mental context vector specifies a distribution over topics. In other words, the human memory component of our model represents the drifting mental context as a sequence of mixtures of topics, in the same way that LDA represents documents. With this representation, the dynamics of the mental context are determined by two factors: the posterior probability over topics given a studied or recalled word (semantic inference) and the retrieval of previous contexts (episodic retrieval). These dynamics let us capture both the episodic and semantic structure of human verbal memory.
 The work described here goes beyond prior TCM modeling work in two ways: First, our approach allows us to infer the trajectory of the context vector over time, which (in turn) allows us to predict the item-by-item sequence of word recalls; by contrast, previous work (e.g., [ 10 , 16 ]) has focused on fitting the summary statistics of the data. Second, we model inter-subject variability using a hierarchical model specification; this approach allows us to capture both common and idiosyncratic features of the behavioral data.
 The rest of the paper is organized as follows. In Section 2 we describe LDA and in Section 3 we describe our model, which we refer to as LDA-TCM. In Section 4 we describe a particle filter for performing posterior inference in this model. In Section 5.1 we present simulation results showing how this model reproduces fundamental behavioral effects in free recall experiments. In Section 5.2 we present inference results for a dataset collected by Sederberg and Norman in which subjects performed free recall of words. Our model builds on probabilistic topic models, specifically latent Dirichlet allocation. Latent Dirichlet allocation (LDA) is a probabilistic model of document collections [ 3 ]. LDA posits a set of K topics, each of which is a distribution over a fixed vocabulary, and documents are represented as mixtures over these topics. Thus, each word is assumed to be drawn from a mixture model with corpus-wide components (i.e., the topics) and document-specific mixture proportions. When fit to a collection of documents, the topic distributions often reflect the themes that permeate the document collection.
 More formally, assume that there are K topics  X  k , each of which is a distribution over words. (We will call the K  X  W matrix  X  the word distribution matrix .) For each document, LDA assumes the following generative process: Given a collection of documents, posterior inference in LDA essentially reverses this process to decompose the corpus according to its topics and find the corresponding distributions over words. Posterior inference is intractable, but many approximation algorithms have been developed [ 3 , 7 , 17 ]. In addition to capturing the semantic content of documents, recent psychological work has shown that several aspects of LDA make it attractive as a model of human semantic representation [ 7 ]. In our model of memory, the topic proportions  X  play the role of a  X  X ental context X  that guides memory retrieval by parameterizing a distribution over words to recall. We now turn to a model of human memory that uses the latent representation of LDA to capture the semantic aspects of recall experiments. Our data consist of two types of observations: a corpus of documents from which we have obtained the word distribution matrix, 1 and behavioral data from free recall experiments, which are studied and recalled words from multiple subjects over multiple runs of the experiment. Our goal is to model the psychological process of recall in terms of a drifting mental context.
 The human memory component of our model is based on the Temporal Context Model (TCM). There are two core principles of TCM: (1) Memory retrieval involves reinstating a representation of context that was active at the time of study; and (2) context change is driven by features of the studied stimuli [ 10 , 16 , 14 ]. We capture these principles by representing the mental context drift of each subject with a trajectory of latent variables  X  n . Our use of the same variable name (  X  ) and dimensionality for the context vector and for topics reflects our key assertion: Context and topics reside in the same meaning space.
 The relationship between context and topics is specified in the generative process of the free recall data. The generative process encompasses both the study phase and the recall phase of the memory experiment. During study, the model specifies the distribution of the trajectory of internal mental contexts of the subject. (These variables are important in the next phase when recalling words episodically.) First, the initial mental context is drawn from a Gaussian: where s denotes the study phase and I is a K  X  K identity matrix. 2 Then, for each studied word the mental context drifts according to where This equation identifies the two pulls on mental context drift when the subject is studying words: the current word and the topic distribution matrix. This second term captures the idea that mental context is updated with the meaning of the current word (see also [ 2 ] for a related treatment of topic dynamics in the context of text modeling). For example, if the studied word is  X  X tocks X  then the mental context might drift toward topics that also have words like  X  X usiness X ,  X  X inancial X , and  X  X arket X  with high probability. (Note that this is where the topic model and memory model are coupled.) The parameter  X  controls the rate of drift, while  X  controls its noisiness.
 During recall, the model specifies a distribution over drifting contexts and recalled words. For each time t , the recalled word is assumed to be generated from a mixture of two components. Effectively, there are two  X  X aths X  to recalling a word: a semantic path and an episodic path.
 The semantic path recalls words by  X  X ree associating X  according to the LDA generative process: Using the current context as a distribution over topics, it draws a topic randomly and then draws a word from this topic (this is akin to thinking of a word that is similar in meaning to just-recalled words). Formally, the probability of recalling a word via the semantic path is expressed as the marginal probability of that word induced by the current context: where  X  is a function that maps real-valued vectors onto the simplex (i.e., positive vectors that sum to one) and the index r denotes the recall phase.
 The episodic path recalls words by drawing them exclusively from the set of studied words. This path puts a high probability on words that were studied in a context that resembles the current context (this is akin to remembering words that you studied when you were thinking about things similar to what you are currently thinking about). Formally, the episodic distribution over words is expressed as a weighted sum of delta functions (each corresponding to a word distribution that puts all its mass on a single studied word), where the weight for a particular study word is determined by the similarity of the context at recall to the state of context when the word was studied: where Here d (  X  ,  X  ) is a similarity function between distributions (here we use the negative KL-divergence) and is a parameter controlling the curvature of the similarity function. We define {  X  s,w s,n } N n =1 to be delta functions defined at study words. Because people tend not to repeatedly recall words, we remove the corresponding delta function after a word is recalled.
 Our model assumes that humans use some mixture of these two paths, determined by mixing proportion  X  . Letting w r,t  X  Mult (  X  t ) , we have Intuitively,  X  in Equation 6 controls the balance between semantic influences and episodic influences. When  X  approaches 1 , we obtain a  X  X ure semantic X  model wherein words are recalled essentially by free association (this is similar to the model used by [ 7 ] to model semantically-related intrusions in free recall). When  X  approaches 0 , we obtain a  X  X ure episodic X  model wherein words are recalled exclusively from the study list. An intermediate value of  X  is essential to simultaneously explaining temporal contiguity and semantic effects in memory.
 Finally, the context drifts according to where This is similar to how context drifts in the study phase, except that the context is additionally pushed by the context that was present when the recalled word was studied. This is obtained mathematically by defining n ( w r,t ) to be a mapping from a recalled word to the index of the same word at study. For Figure 2: Simulated and empirical recall data. Data replotted from [ 9 ]. ( Left ) Probability of first recall curve. ( Right ) Conditional response probability curve. example, if the recalled word is  X  X at X  and cat was the sixth studied word then n ( w r,t ) = 6 . If there is a false recall, i.e., the subject recalls a word that was not studied, then  X  s,n ( w vector.
 This generative model is depicted graphically in Figure 1, where  X  = {  X  1:4 , X , X , } represents the set of model parameters and  X  is the set of hyperparameters.
 To model inter-subject variability, we extend our model hierarchically, defining group-level prior distributions from which subject-specific parameters are assumed to be drawn [ 6 ]. This approach allows for inter-subject variability and, at the same time, it allows us to gain statistical strength from the ensemble by coupling subjects in terms of higher-level hyperparameters. We choose our group prior over subject i  X  X  parameters to factorize as follows: Dir (  X  ) , X  i  X  Exp (  X  ) , X  i  X  Beta ( a,b ) , i  X  Gamma (  X  1 , X  2 ) . Except where mentioned otherwise, we used the following hyperparameter values: a = b = c = d = 1 , X  = [1 , 1 , 1] , X  1 = 1 , X  2 = 1 . For some model variants (described in Section 5.2) we set the parameters to a fixed value rather than inferring them.
 Here, we use the model to answer the following questions about behavior in free recall experiments: (1) Do both semantic and temporal factors influence recall, and if so what are their relative contri-butions; (2) What are the relevant dimensions of variation across subjects? In our model, semantic and temporal factors exert their influence via the context vector, while variation across subjects is expressed in the parameters drawn from the group prior. Thus, our goal in inference is to compute the posterior distribution over the context trajectory and subject-specific parameters, given a sequence of studied and recalled words. We can also use this posterior to make predictions about what words will be recalled by a subject at each point during the recall phase. By comparing the predictive performance of different model variants, we can examine what types of model assumptions (like the balance between semantic and temporal factors) best capture human behavior. We now describe an approximate inference algorithm for computing the posterior distribution. Letting P (  X  | W ) = Because computing the posterior exactly is intractable (the denominator involves a high-dimensional integral that cannot be solved exactly), we approximate it with a set of C samples using the particle filter algorithm [4], which can be summarized as follows. At time t &gt; 0 : three successively recalled words influence context. Each column corresponds to a specific recalled word (shown in the top row). The bars in each cell correspond to individual topics (specifically, these are the top ten inferred topics at recall; the center legend shows the top five words associated with each topic). Arrows schematically indicate the flow of influence between the components. The context vector at recall ( Middle Row ) is updated by the posterior over topics given the recalled word ( Top Row ) and also by retrieved study contexts ( Bottom Row ). ( Right ) Plot of the inferred context trajectory at study and recall for a different list, in a 2-dimensional projection of the context space obtained by principal components analysis. Using this sample-based approximation, the posterior is approximated as a sum of the delta functions placed at the samples: We evaluate our model in two ways. First, we generate data from the generative model and record a number of common psychological measurements to assess to what extent the model reproduces qualitative patterns of recall behavior. Second, we perform posterior inference and evaluate the predictive performance of the model on a real dataset gathered by Sederberg and Norman. 5.1 Simulations For the simulations, the following parameters were used:  X  1 = 0 . 2 , X  2 = 0 . 55 , X  3 = 0 . 05 , X  = we are simply trying to reproduce qualitative patterns. These values have been chosen heuristically without a systematic search through the parameter space. The results are averaged over 400 random study lists of 12 words each. In Figure 3, we compare our simulation results to data collected by [ 9 ]. Figure 2 (left) shows the probability of first recall (PFR) curve, which plots the probability of each list position being the first recalled word. This curve illustrates how words in later positions are more likely to be recalled first, a consequence (in our model) of initializing the recall context with the last study context. Figure 2 (right) shows the lag conditional response probability (lag-CRP) curve, which plots the conditional probability of recalling a word given the last recalled word as a function of the lag (measured in terms of serial position) between the two. This curve demonstrates the temporal Figure 4: ( Left ) Box-plot of average predictive log-probability of recalled words under different models. S: pure semantic model; E: pure episodic model. Green line indicates chance. See text for more detailed descriptions of these models. ( Right ) Box-plot of inferred parameter values across subjects. contiguity effect observed in human recall behavior: the increased probability of recalling words that were studied nearby in time to the last-recalled word. As in TCM, this effect is present in our model because items studied close in time to one another have similar context vectors; as such, cuing with contextual information from time t will facilitate recall of other items studied in temporal proximity to time t . 5.2 Modeling psychological data The psychological data modeled here are from a not-previously-published dataset collected by Sederberg and Norman. 30 participants studied 8 lists of words for a delayed free-recall task. Each list was composed of 15 common nouns, chosen at random and without replacement from one of 28 categories, such as Musical Instruments, Sports, or Four-footed Animals. After fitting LDA to the TASA corpus [ 5 ], we ran the particle filter with 1000 particles on the Sederberg and Norman dataset. Our main interest here is comparing our model (which we refer to as the semantic-episodic model) against various special hyperparameter settings that correspond to alternative psychological accounts of verbal memory. The models being compared include: We also compare against a null (chance) model in which all words in the vocabulary have an equal probability of being recalled.
 As a metric of model comparison, we calculate the model X  X  predictive probability for the word recalled at time t given words 1 to t  X  1 , for all t : This metric is proportional to the accumulative prediction error [ 19 ], a variant of cross-validation designed for time series models.
 To assure ourselves that the particle filter we used does not suffer from weight degeneracy, we also calculated the effective sample size , as recommended by [ 4 ]: ESS = P C c =1 v ( c ) 2 Conventionally, it is desirable that the effective sample size is at least half the number of particles. This desideratum was satisfied for all the models we explored. Before we present the quantitative results, it is useful to examine some examples of inferred context change and how it interacts with word recall. Figure 3 shows the different factors at work in generating context change during recall on a single trial, illustrating how semantic inference and retrieved episodic memories combine to drive context change. The legend showing the top words in each topic illustrates how these topics appear to capture some of the semantic structure of the recalled words. On the right of Figure 3, we show another representation of context change (from a different trial), where the context trajectory is projected onto the first two principal components of the context vector. We can see from this figure how recall involves reinstatement of studied contexts: Recalling a word pulls the inferred context vector in the direction of the (inferred) contextual state associated with that word at study.
 Figure 4 (left) shows the average predictive log-probability of recalled words for the models described above. Overall, the semantic-episodic model outperforms the pure episodic and pure semantic models in predictive accuracy (superiority over the closest competitor, the pure episodic model, was confirmed by a paired-sample t-test, with p &lt; 0 . 002 ). To gain deeper insight into this pattern of results, consider the behavior of the different  X  X ure X  models with respect to extra-list intrusions vs. studied list items. The pure episodic model completely fails to predict extra-list intrusions, because it restricts recall to the study list (i.e., it assigns zero predictive probability to extra-list items). Conversely, the pure semantic model does a poor job of predicting recall of studied list items, because it does not scope recall to the study list. Thus, each of these models is hobbled by crucial (but complementary) shortcomings. The semantic-episodic model, by occupying an intermediate position between these two extremes, is able to capture both the semantic and temporal structure in free recall. Our second goal in inference was to examine individual differences in parameter fits. Figure 4 (right) shows box-plots of the different parameters. In some cases there is substantial variability across subjects, such as for the similarity parameter . Another pattern to notice is that the values of the episodic-semantic trade-off parameter  X  tend to cluster close to 0 (the episodic extreme of the spectrum), consistent with the fact that the pure episodic and semantic-episodic models are fairly comparable in predictive accuracy. Future work will assess the extent to which these across-subject differences in parameter fits reflect stable individual differences in memory functioning. We have presented here LDA-TCM, a probabilistic model of memory that integrates semantic and episodic influences on recall behavior. By formalizing this model as a probabilistic graphical model, we have provided a common language for developing and comparing more sophisticated variants. Our simulation and empirical results show that LDA-TCM captures key aspects of the experimental data and provides good accuracy at making item-by-item recall predictions. The source code for learning and inference and the experimental datasets are available at www.cs.princeton.edu/  X  blei . There are a number of advantages to adopting a Bayesian approach to modeling free recall behavior. processes [ 18 ]. Second, hierarchical model specification gives us the power to capture both common and idiosyncratic behavioral patterns across subjects, thereby opening a window onto individual differences in memory. Finally, this approach makes it possible to integrate other sources of data, such as brain imaging data. In keeping with the graphical model formalism, we plan to augment LDA-TCM with additional nodes representing variables measured with functional magnetic resonance imaging (fMRI). Existing studies have used fMRI data to decode semantic states in the brain [ 12 ] and predict recall behavior at the level of semantic categories [ 13 ]. Incorporating fMRI data into the model will have several benefits: The fMRI data will serve as an additional constraint on the inference process, thereby improving our ability to track subjects X  mental states during encoding and recall; fMRI will give us a new way of validating the model  X  we will be able to measure the model X  X  ability to predict both brain states and behavior; also, by examining the relationship between latent context states and fMRI data, we will gain insight into how mental context is instantiated in the brain.
 Acknowledgements RS acknowledges support from the Francis Robbins Upton Fellowship and the ERP Fellowship. This work was done while RS was at Princeton University. PBS acknowledges support from National Institutes of Health research grant MH080526. [1] J. Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical [2] D.M. Blei and J.D. Lafferty. Dynamic topic models. In Proceedings of the 23rd international [3] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent dirichlet allocation. Journal of Machine Learning [4] A. Doucet and N. De Freitas. Sequential Monte Carlo Methods in Practice . Springer, 2001. [5] ST Dumais and TK Landauer. A solution to Platos problem: The latent semantic analysis theory [6] A. Gelman and J. Hill. Data analysis using regression and multilevel/hierarchical models . [7] T.L. Griffiths, M. Steyvers, and J.B. Tenenbaum. Topics in semantic representation. Psychologi-[8] M.W. Howard, B. Jing, K.M. Addis, and M.J. Kahana. Semantic structure and episodic memory. [9] M.W. Howard and M.J. Kahana. Contextual variability and serial position effects in free recall. [10] M.W. Howard and M.J. Kahana. A distributed representation of temporal context. Journal of [11] M.W. Howard and M.J. Kahana. When does semantic similarity help episodic retrieval? Journal [12] T.M. Mitchell, S.V. Shinkareva, A. Carlson, K. Chang, V.L. Malave, R.A. Mason, and M.A. [13] S.M. Polyn, V.S. Natu, J.D. Cohen, and K.A. Norman. Category-specific cortical activity [14] S.M. Polyn, K.A. Norman, and M.J. Kahana. A context maintenance and retrieval model of [15] H.L. Roediger and K.B. McDermott. Creating false memories: Remembering words not [16] P.B. Sederberg, M.W. Howard, and M.J. Kahana. A context-based theory of recency and [17] Y. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm for [18] Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. Hierarchical dirichlet processes. Journal of [19] E.J. Wagenmakers, P. Gr  X  unwald, and M. Steyvers. Accumulative prediction error and the
