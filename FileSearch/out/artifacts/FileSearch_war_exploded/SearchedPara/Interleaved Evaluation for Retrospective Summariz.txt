 We propose and validate a novel interleaved evaluation meth-odology for two complementary information seeking tasks on document streams: retrospective summarization and prospec-tive notification. In the first, the user desires relevant and non-redundant documents that capture important aspects of an information need. In the second, the user wishes to receive timely, relevant, and non-redundant update notifi-cations for a standing information need. Despite superficial similarities, interleaved evaluation methods for web rank-ing cannot be directly applied to these tasks; for example, existing techniques do not account for temporality or re-dundancy. Our proposed evaluation methodology consists of two components: a temporal interleaving strategy and a heuristic for credit assignment to handle redundancy. By simulating user interactions with interleaved results on sub-mitted runs to the TREC 2014 tweet timeline generation (TTG) task and the TREC 2015 real-time filtering task, we demonstrate that our methodology yields system compar-isons that accurately match the result of batch evaluations. Analysis further reveals weaknesses in current batch evalua-tion methodologies to suggest future directions for research.
As primarily an empirical discipline, evaluation method-ologies are vital to ensuring progress in information retrieval. The ability to compare system variants and detect differ-ences in effectiveness allows researchers and practitioners to continually advance the state of the art. One such approach, broadly applicable to any online service, is the traditional A/B test [12]. In its basic setup, users are divided into disjoint  X  X uckets X  and exposed to different treatments (e.g., algorithm variants); user behavior (e.g., clicks) in each of the conditions is measured and compared to assess the relative effectiveness of the treatments. As an alternative, infor-mation retrieval researchers have developed an evaluation methodology for web search based on interleaving results from two different comparison systems into a single ranked list [8, 17, 6, 15, 7, 3, 16, 19], as well as recent extensions to more than two system [20]. Instead of dividing the user population into disjoint segments, all test subjects are ex-posed to these interleaved results. Based on user clicks, it is possible to assess the relative effectiveness of the two in-put systems with greater sensitivity than traditional A/B testing [17, 3], primarily due to the within-subjects design.
This paper explores interleaved evaluation for informa-tion seeking on document streams. Although we focus on a stream of social media updates (tweets), nothing in our for-mulation is specific to tweets. In this context, we tackle two complementary user tasks: In the retrospective summariza-tion scenario, which is operationalized in the tweet time-line generation (TTG) task at TREC 2014 [13], the user desires relevant and non-redundant posts that capture key aspects of an information need. In the prospective notifica-tion scenario, operationalized in the real-time filtering task ( X  X cenario A X ) at TREC 2015 [14], the user wishes to re-ceive timely, relevant, and non-redundant updates (e.g., via a push notification on a mobile phone).

The contribution of this paper is the development and val-idation of an interleaved evaluation methodology for retro-spective summarization and prospective notification on doc-ument streams, consisting of two components: a temporal in-terleaving strategy and a heuristic for credit assignment to handle redundancy. Although we can draw inspiration from the literature on interleaved evaluations for web search, pre-vious techniques are not directly applicable to our tasks. We face a number of challenges: the important role that time plays in organizing and structuring system output, differ-ing volumes in the number of results generated by systems, and notions of redundancy that complicate credit assign-ment. Our evaluation methodology addresses these com-plexities and is validated using data from the TREC 2014 and 2015 Microblog evaluations. Specifically, we simulate user interactions with interleaved results to produce a de-cision on whether system A is better than system B, and correlate these decisions with the results of batch evalua-tions. We find that our methodology yields accurate system comparisons under a variety of settings. Analysis also re-veals weaknesses in current batch evaluation methodologies, which is a secondary contribution of this work.
We begin by describing our task models, which are illus-trated in Figure 1. We assume the existence of a stream of timestamped documents: examples include news articles coming off an RSS feed or social media posts such as tweets. Figure 1: Illustration of our task models. At some point in time ( X  X ow X ), the user develops an informa-tion need: she requests a retrospective summary of what has happened thus far and desires prospective notifications of future updates.
 In this context, we consider a pair of complementary tasks: suppose at some point in time the user develops an informa-tion need, let X  X  say, about an ongoing political scandal. She would like a retrospective summary of what has occurred up until now, which might consist of a list of chronologically-ordered documents that highlight important developments. Once she has  X  X ome up to speed X , the user might wish to re-ceive prospective notifications (on her mobile phone) regard-ing future updates, for example, statements by the involved parties or the emergence of another victim. Retrospective summarization and prospective notification form two com-plementary components of information seeking on document streams. In both cases, users desire relevant and novel (non-redundant) content X  X hey, for example, would not want to see multiple tweets that say essentially the same thing. In the prospective notification case, the user additionally de-sires timely updates X  X s close as possible to the actual oc-currence of the  X  X ew development X . This, however, isn X  X  particularly important for the retrospective case, since the events have already taken place.

In this work, we present and evaluate an interleaved evalu-ation methodology for the retrospective summarization and prospective notification tasks described above. Although there has been substantial work on interleaved evaluation in the context of web search [8, 17, 6, 15, 7, 3, 16, 19], we face three main challenges: 1. Temporality plays an important role in our tasks. In web search, ranked lists from different systems can be arbitrar-ily interleaved (and in some cases the relative ordering of documents swapped) without significantly affecting users X  interpretation of the results. In our task, however, the temporal ordering of documents is critical for the proper interpretation of system output. 2. We need to interleave results of different lengths. In web search, most interleaving strategies assume ranked lists of equal length, while this is not true in our case X  X ome systems are more verbose than others. 3. We need to account for redundancy. In our tasks the notion of novelty is very important and  X  X redit X  is only awarded for returning non-redundant tweets. This cre-ates a coupling effect between two systems where one X  X  result might  X  X ask X  the novelty in the other. That is, a system X  X  output becomes redundant only because the in-terleaving algorithm injected a relevant document before the document in question.
 Nevertheless, there is a rich body of literature from which we can draw inspiration. In particular, we employ a simulation-based approach that is well-established for validating inter-leaved evaluations [6, 7, 16].

Our retrospective summarization and prospective notifi-cation tasks are grounded in the Microblog track evalua-tions at TREC: specifically, the tweet timeline generation (TTG) task at TREC 2014 [13] and the push notification sce-nario ( X  X cenario A X ) in the real-time filtering task at TREC 2015 [14]. Although there has been a substantial amount of work on developing systems that try to accomplish the tasks we study (see the TREC overview papers for point-ers into the literature), our focus is not on the development of algorithms, but rather in evaluating system output. We adopt the framework provided by these tracks: relevance judgments and submitted runs are used in simulation studies to validate our interleaved evaluation methodology.
We begin by describing evaluations from TREC 2014 and 2015 that operationalize our retrospective summarization and prospective notification tasks. Tweet timeline generation (TTG) was introduced at the TREC 2014 Microblog track. The putative user model is as follows:  X  X t time T , I have an information need expressed by query Q , and I would like a summary that captures relevant information. X  The system X  X  task is to produce a summary timeline, operationalized as a list of non-redundant, chrono-logically ordered tweets. It is imagined that the user would consume the entire summary (unlike a ranked list, where the user might stop reading at any time).

Redundancy was operationalized as follows: for every pair of tweets, if the chronologically later tweet contains substan-tive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise, the later tweet is redundant with respect to the earlier one. Thus, redundancy and novelty are antonyms; we use them interchangeably in opposite contexts. Due to the temporal constraint, redun-dancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A , then B is redundant with respect to A , but not the other way around. The task also assumes transitivity. Suppose A precedes B and B precedes C : if B is redundant with respect to A and C is redundant with respect to B , then by definition C is redundant with respect to A .
The TTG assessment task can be viewed as semantic clustering X  X hat is, we wish to group relevant tweets into clusters in which all tweets share substantively similar in-formation. Within each cluster, the earliest tweet is novel; all other tweets in the cluster are redundant with respect to all earlier tweets. The track organizers devised a two-phase assessment workflow that implements this idea. In the first phase, all tweets are pooled and judged for relevance. In the second phase, relevant tweets for each topic are then clustered. We refer the reader to previous papers for more details [13, 22], but the final product of the human annota-tion process is a list of tweet clusters, each containing tweets that represent a semantic equivalence class.

In TREC 2014, TTG systems were evaluated in terms of set-based metrics (precision, recall, and F-score) at the cluster level. Systems only received credit for returning one tweet from each cluster X  X hat is, once a tweet is retrieved, all other tweets in the cluster are automatically considered not relevant. In this study, we performed our correlation analysis against recall, for reasons that will become appar-ent later. The track evaluated recall in two different ways: unweighted and weighted. In the relevance assessment pro-cess, tweets were judged as not relevant, relevant, or highly relevant. For unweighted recall (also called S-recall [23] and I-recall [18]), relevant and highly-relevant tweets were col-lapsed to yield binary judgments and all clusters received equal weight. For weighted recall, each cluster is assigned a weight proportional to the sum of relevance grades from every tweet in the cluster (relevant tweets receive a weight of one and highly-relevant tweets receive a weight of two).
In the real-time filtering task at TREC 2015 [14], the goal is for a system to identify interesting and novel content for a user in a timely fashion, with respect to information needs (called  X  X nterest profiles X  but in actuality quite similar to traditional ad hoc topics). In the push notification variant of the task ( X  X cenario A X ), updates are putatively delivered in real time as notifications to users X  mobile phones. A system was allowed to return a maximum of ten tweets per day per interest profile. The official evaluation took place over a span of ten days during July 2015, where all participating systems  X  X istened X  to Twitter X  X  live tweet sample stream to complete the evaluation task; the interest profiles were made available prior to the evaluation period.

The assessment workflow was the same as the TTG task in TREC 2014 (see Section 3.1): relevance assessment us-ing traditional pooling followed by semantic clustering. The task likewise used three-way judgments: not relevant, rele-vant, and highly relevant. We refer the reader to the TREC overview paper for more details [14].

The two metrics used to evaluate system runs were ex-pected latency-discounted gain (ELG) and normalized cu-mulative gain (nCG). These two metrics are computed for each interest profile for each day in the evaluation period (explained in detail below). The final score of a run is the average of daily scores across all interest profiles.
The expected latency-discounted gain (ELG) metric was adapted from the TREC temporal summarization track [2]: where N is the number of tweets returned and G( t ) is the gain of each tweet: not relevant tweets receive a gain of 0, rel-evant tweets receive a gain of 0.5, and highly-relevant tweets receive a gain of 1.0.

As with the TTG task, redundancy is penalized: a system only receives credit for returning one tweet from each cluster. Furthermore, per the track guidelines, a latency penalty is applied to all tweets, computed as MAX(0 , (100  X  d ) / 100), where the delay d is the time elapsed (in minutes, rounded down) between the tweet creation time and the putative time the tweet was delivered. That is, if the system delivers a relevant tweet within a minute of the tweet being posted, the system receives full credit. Otherwise, credit decays linearly such that after 100 minutes, the system receives no credit even if the tweet was relevant.

The second metric is normalized cumulative gain (nCG): where Z is the maximum possible gain (given the ten tweet per day limit). The gain of each individual tweet is com-puted as above (with the latency penalty). Note that gain is not discounted (as in nDCG) because the notion of docu-ment ranks is not meaningful in this context.

Due to the setup of the task and the nature of interest pro-files, it is possible (and indeed observed empirically) that for some days, no relevant tweets appear in the judgment pool. In terms of evaluation metrics, a system should be rewarded for correctly identifying these cases and not generating any output. We can break down the scoring contingency table as follows: If there are relevant tweets for a particular day, scores are computed per above. If there are no relevant tweets for that day, and the system returns zero tweets, it receives a score of one (i.e., perfect score) for that day; oth-erwise, the system receives a score of zero for that day. This means that an empty run (a system that never returns any-thing) may have a non-zero score.
The development of an interleaved evaluation methodol-ogy requires answering the following questions: 1. How exactly do we interleave the output of two systems into one single output, in light of the challenges discussed in Section 2? 2. How do we assign credit to each of the underlying sys-tems in response to user interactions with the interleaved results?
We begin by explaining why existing interleaving strate-gies for web search cannot be applied to either retrospective summarization or prospective notification. Existing strate-gies attempt to draw results from the test systems in a  X  X air X  way: In balanced interleaving [8], for example, the algorithm maintains two pointers, one to each input list, and draws from the lagging pointer. Team drafting [17], on the other hand, follows the analogy of selecting teams for a friendly team-sports match and proceeds in rounds. Both explicitly assume (1) that the ranked lists from each system are or-dered in decreasing probability of relevance (i.e., following the probability ranking principle) and (2) that the ranked lists are of equal length. Both assumptions are problematic because output in retrospective summarization and prospec-tive notification must be chronologically ordered: a na  X   X ve application of an existing web interleaving strategy in the retrospective case would yield a chronologically jumbled list of tweets that is not interpretable. In the prospective case, we cannot  X  X ime travel X  and push notifications  X  X n the fu-ture X  and then  X  X eturn to the past X . Furthermore, in both our tasks system outputs can vary greatly in verbosity, and hence the length of their results. This is an important as-pect of the evaluation design as systems should learn when to  X  X eep quiet X  (see Section 3.2). Most existing interleaving strategies don X  X  tell us what to do when we run out of re-sults from one system. For these reasons it is necessary to develop a new interleaving strategy.

After preliminary exploration, we developed an interleav-ing strategy, called temporal interleaving , where we simply interleave the two runs by time. The strategy is easy to implement yet effective, as we demonstrate experimentally. Temporal interleaving works in the prospective case because time is always  X  X oving forward X . An example is shown in Figure 2, where we have system A on the left and system B on the right. The subscript of each tweet indicates its times-tamp and the interleaved result is shown in the middle (note that tweet t 28 is returned by both systems). One potential Figure 2: Illustration of temporal interleaving. Note that tweet t 28 is returned by both systems. downside of this strategy is that all retrieved documents from both systems are included in the interleaved results, which increases its length X  X e return to address this issue in Section 5.3.
 Our simple temporal interleaving strategy works as is for TTG runs, since system outputs are ordered lists of tweets. For push notifications, there is one additional wrinkle: which timestamp do we use? Recall that in prospective notifica-tion there is the tweet creation time and the push time (when the system identified the tweet as being relevant). We base interleaving on the push time because it yields a very sim-ple implementation: we watch the output of two prospective notification systems and take the output as soon as a re-sult is emitted by either system. However, we make sure to apply de-duplication: if a tweet is pushed by two systems but at different times, it will only be included once in the interleaved results.
In our interleaved evaluation methodology, output from the two different test systems are combined using the tem-poral interleaving strategy described above and presented to the user. We assume a very simple interaction model in which the user goes through the output (in chronological or-der) from earliest to latest and makes one of three judgments for each tweet: not relevant, relevant, and relevant but re-dundant (i.e., the tweet is relevant but repeats information that is already present in a previously-seen tweet). This ex-tends straightforwardly to cases where we have graded rele-vance judgments: for the relevant and redundant judgments, the user also indicates the relevance grade. In retrospective summarization, the user is interacting with static system output, but in the prospective notification case, output is presented to the user over a period of time. This is called the  X  X imple task X , for reasons that will become clear shortly. We assume that users provide explicit judgments, in con-trast to implicit feedback (i.e., click data) in the case of interleaved evaluations for web search; we return to discuss this issue in Section 5.4.

Based on user interactions with the interleaved results, we must now assign credit to each of the test systems, which is used to determine their relative effectiveness. Credit assign-ment for the relevant label is straightforward: credit accrues to the system that contributed the tweet to the interleaved results (or to both if both systems returned the tweet). How-ever, credit assignment for a tweet marked redundant is more complex X  X e do not know, for example, if the redundancy Figure 3: Example of interleaving credit assignment and redundancy handling. was actually introduced by the interleaving. That is, the interleaving process inserted a tweet (from the other run) before this particular tweet that made it redundant.
We can illustrate this with the diagram in Figure 3. A dot-ted border represents a tweet contributed by system A (on the left) and a solid border represents a tweet contributed by system B (on the right). Suppose the assessor judged the tweets as they are labeled in the figure. The second and fifth tweets are marked relevant, and so system B gets full credit twice. Now let X  X  take a look at the third tweet, con-tributed by system A, which is marked redundant X  X e can confidently conclude in this case that the redundancy was introduced by the interleaving, since there are no relevant tweets above that are contributed by system A. Therefore, we can give system A full credit for the third tweet. Now let X  X  take a look at the sixth tweet: generalizing this line of reasoning, the more that relevant tweets above are from sys-tem B, the more likely that we X  X e encountering a  X  X asking effect X  (all things being equal), where the redundancy is an artifact of the interleaving itself. To capture this, we intro-duce the following heuristic: the amount of credit given to a system for a tweet marked redundant is multiplied by a discount factor equal to the fraction of relevant and redun-dant tweets above that come from the other system. In this case, there are two relevant tweets above, both from system B, and one redundant tweet from system A, so system A receives a credit of 0.66.

More formally, consider an interleaved result S consisting of tweets s 1 . . . s n drawn from system A and system B . We denote S A and S B as those tweets in S that come from sys-tem A and B , respectively. For a tweet s i judged redundant, if s i  X  S A , then we multiple its gain by a discount factor D as follows: where I ( s ) is an indicator function that returns one if the user (previously) judged the tweet to be either relevant or redundant, or zero otherwise. On the other hand, if s i  X  S we apply a discount factor D B that mirrors D A above (i.e., flipping subscripts A and B ). If s i is both in S A and S we apply both equations and give each system a different amount of credit (summing up to one).

We emphasize, of course, that this way of assigning credit for redundant judgments is a heuristic (but effective, from our evaluations). For further validation, we introduce an al-ternative interaction model that we call the  X  X omplex task X : in this model, the user still marks each tweet not relevant, relevant, and redundant, but for each redundant tweet, the user marks the source of the redundancy, i.e., which pre-vious tweet contains the same information. With this ad-ditional source of information, we can pinpoint the exact source of redundancy and assign credit definitively (zero if the source of redundancy was from the same run, and one if from the other run). Of course, such a task would be significantly more onerous (and slower) than just providing three-way judgments, but this  X  X omplex task X  provides an upper bound that allows us to assess the effectiveness of our credit assignment heuristic.

One final detail: In the prospective task, we still apply a latency penalty to the assigned credit, as in ELG. Thus, in the case of a tweet that was pushed by both systems, but at different times, they will receive different amounts of credit. In the interleaved results, of course, the tweet will appear only once X  X rom that single judgment we can compute the credit assigned to each system.

To recap: we have presented a temporal interleaving strat-egy to combine system output, introduced a model for how users interact with the results, and devised a credit assign-ment algorithm (including redundancy handling) that scores the systems based on user interactions. From this, we arrive at a determination of which system is more effective. Do these decisions agree with the results of batch evaluations? We answer this question with simulation studies based on runs submitted to TREC 2014 (for retrospective summa-rization) and TREC 2015 (for prospective notification).
To validate our interleaved evaluation methodology for retrospective summarization, we conducted user simulations using runs from the TREC 2014 TTG task. In total, 13 groups submitted 50 runs to the official evaluation. For each pair of runs, we applied the temporal interleaving strategy described above and simulated user interactions with the  X  X round truth X  cluster annotations. Each simulation experi-ment comprised 67,375 pairwise comparisons, which we fur-ther break down into 63,415 comparisons of runs from dif-ferent groups (inter-group) and 3,960 comparisons between runs from the same group (intra-group). Wang et al. [22] were able to elicit two completely independent sets of clus-ter annotations, which they refer to as the  X  X fficial X  and  X  X l-ternate X  judgments. Thus, we were able to simulate user interactions with both sets of clusters.

First, we ran simulations using binary relevance judg-ments. Results are shown in Table 1. When comparing simulation results (which system is better, based on assigned credit) with the batch evaluation results (unweighted recall), there are four possible cases:  X  The compared runs have different batch evaluation results and the simulation was able to detect those differences; denoted (Agree,  X ).  X  The compared runs have the same batch result and the simulation assigned equal credit to both runs; denoted (Agree,  X   X ).  X  The compared runs have different batch evaluation results but the simulation was not able to detect those differences; denoted (Disagree,  X ).  X  The compared runs have the same batch result and the simulation falsely ascribed differences in effectiveness be-tween those runs; denoted (Disagree,  X   X ).
 In the first two cases, the batch evaluation and interleaved evaluation results are consistent and the interleaving can be said to have given  X  X orrect X  results; this is tallied up as (Agree, Total) in the results table. In the last two cases, the batch evaluation and interleaved evaluation results are inconsistent and the interleaving can be said to have given  X  X ncorrect X  results; this is tallied up as (Disagree, Total) in the results table. 1
With  X  X fficial X  and  X  X lternate X  clusters, there are four ways we can run the simulations: simulate with official judgments, correlate with batch evaluation results using official judg-ments (official, official); simulate with alternate judgments, correlate with batch evaluation results using alternative judg-ments (alternate, alternate); as well as the symmetrical cases where the simulation and batch evaluations are different, i.e., (official, alternate) and (alternate, official). Table 1 shows all four cases, denoted by the first two columns. Finally, the two vertical blocks of the table denote the results of the  X  X imple task X  (simulated user provides three-way judgments) and the  X  X omplex task X  (simulated user additionally marks the source of a redundant tweet).

There is a lot of information to unpack from Table 1. Fo-cusing only on  X  X ll pairs X  with the  X  X imple task X , we see that our simulation results agree with batch evaluation results 92%-93% of the time, which indicates that our interleaved evaluation methodology is effective. The inaccuracies can be attributed to the credit assignment heuristic for redun-dant labels X  X his can be seen from the  X  X omplex task X  block, where accuracy becomes 100% if we ask the (simulated) user to mark the source of the redundancy. Of course, this makes the task unrealistically onerous, so we argue that our credit assignment heuristic strikes the right balance between accu-racy and complexity.

With the (official, official) and the (alternate, alternate) conditions, we are simulating user interactions and comput-ing batch results with the same cluster assignments. With the other two conditions, we simulate with one set of clus-ters and perform batch evaluations with the other X  X he dif-ference between these two sets quantifies inter-assessor dif-ferences. Results suggest that the effect of using different assessors is relatively small X  X his finding is consistent with that of Wang et al. [22], who confirmed the stability of the TTG evaluation with respect to assessor differences.
The inter-group and intra-group comparisons suggest how well our interleaved evaluation methodology would fare un-der slightly different conditions. Runs by the same group (intra-group) often share similar algorithms (perhaps vary-ing in parameters), which often yield runs that are similar in effectiveness (or the same). This makes differences more difficult to detect, and indeed, Table 1 shows this to be the S imulation Ju dgment  X   X   X  T otal  X   X   X  T otal  X   X   X  T otal  X   X   X  T otal
A ll Pairs
In ter-Group Pairs Only
In tra-Group Pairs Only (  X  ), and when the runs don X  X  differ (  X   X  ). Figure 4: Scatterplot showing batch vs. simulation results for topic MB178. case (lower agreement). In contrast, differences in effective-ness in runs between groups (inter-group) are slightly easier to detect, as shown by the slightly higher agreement.
To help further visualize our findings, a scatterplot of sim-ulation results is presented in Figure 4 for a representative topic, MB 178, under the all-pairs, (official, official) condi-tion. Each point represents a trial of the simulation compar-ing a pair of runs: the x coordinate denotes the difference based on the batch evaluation, and the y coordinate denotes the difference in assigned credit based on the simulation. We see that there is a strong correlation between simulation and batch results. Plots from other topics look very similar, except differing in the slope of the trendline (since credit is not normalized, but recall is).

The previous results did not incorporate graded relevance judgments. Our next set of experiments examined this re-finement: relevant tweets receive a credit of one and highly-relevant tweets receive a credit of two. The simulated user now indicates the relevance grade for the relevant and redun-dant cases. There is, however, the question of which batch metric to use: the official TREC evaluation used weighted recall, where the weight of each cluster was proportional to the sum of the relevance grades of tweets in the cluster. This encodes the simple heuristic that  X  X ore discussed facets are more important X , which seems reasonable, but Wang et al. [22] found that this metric correlated poorly with hu-man preferences, suggesting that cluster size is perhaps not a good measure of importance. We ran simulations correlat-ing against official weighted recall: the results were slightly worse than those in Table 1, but still quite good. For ex-ample, we achieved 90% accuracy in the (official, official) condition on the simple task, as opposed to 93%.

However, given the findings of Wang et al., these simula-tions might not be particularly meaningful. As an alterna-tive, we propose a slightly different approach to computing the cluster weights: instead of the sum of relevance grades of tweets in the cluster, we use the highest relevance grade of tweets in the cluster. That is, if a cluster contains a highly-relevant tweet, it receives a weight of two; otherwise, it receives a weight of one. This weighting scheme has the effect that scores are not dominated by huge clusters. The results of these simulations are shown in Table 2.

From these experiments, we see that accuracy remains quite good, suggesting that our interleaved evaluation meth-odology is able to take advantage of graded relevance judg-ments. One important lesson here is that capturing  X  X luster importance X  in TTG is a difficult task, and that it is unclear if present batch evaluations present a reasonable solution. Without a well-justified batch evaluation metric, we lack values against which to correlate our simulation outputs. Thus, these results reveal a weakness in current batch eval-uations (indicating avenues of future inquiry), as opposed to a flaw in our interleaved evaluation methodology.
For prospective notification, we validated our interleaved evaluation methodology using runs submitted to the TREC 2015 real-time filtering task ( X  X cenario A X ). In total, there S imulation Ju dgment  X   X   X  T otal  X   X   X  T otal  X   X   X  T otal  X   X   X  T otal
A ll Pairs
In ter-Group Pairs Only
In tra-Group Pairs Only were 37 runs from 14 groups submitted to the official eval-uation. This yields a total of 33,966 pairwise comparisons; 32,283 inter-group pairs and 1,683 intra-group pairs.
Simulation results (with graded relevance judgments) are shown in Table 3 for correlations against ELG and in Table 4 for correlations against nCG. The table is organized in the same manner as Tables 1 and 2, with the exception that we only have one set of cluster annotations available, so no  X  X fficial X  vs.  X  X lternate X  distinction.

Results of the simulation, shown under the rows marked retaining  X  X uiet days X , are quite poor. Analysis reveals that this is due to the handling of days for which there are no relevant tweets. Note that for days without any relevant tweets, there are only two possible scores: one if the system does not return any results, and zero otherwise. Thus, for interest profiles with few relevant tweets, the score is highly dominated by these  X  X uiet days X . As a result, a system that does not return anything scores quite highly; in fact, bet-ter than most submitted runs [14]. To make matters worse, since 2015 was the first year of this TREC evaluation, sys-tems achieved high scores by simply returning few results, in many cases for totally idiosyncratic reasons X  X or example, the misconfiguration of a score threshold.

This property of the official evaluation is problematic for interleaved evaluations since it is impossible to tell with-out future knowledge whether there are relevant tweets for a particular day. Consider the case when system A returns a tweet for a particular day and system B does not return anything, and let X  X  assume we know (based on an oracle) that there are no relevant tweets for that day: according to our interleaved evaluation methodology, neither system would receive any credit. However, based on the batch eval-uation, system B would receive a score of one for that day. There is, of course, no way to know this at evaluation time when comparing only two systems, and thus the interleaved evaluation results would disagree with the batch evaluation results. The extent of this disagreement depends on the number of days across the topics for which there were no relevant tweets. Since the interest profiles for the TREC 2015 evaluation had several quiet days each, our interleaved evaluation methodology is not particularly accurate.
We argue, however, that this is more an artifact of the cur-rent batch evaluation setup than a flaw in our interleaved evaluation methodology per se; see Tan et al. [21] for fur-ther discussion. As the track organizers themselves concede in the TREC overview paper [14], it is not entirely clear if the current handling of days with no relevant tweets is appropriate. While it is no doubt desirable that systems should learn when to  X  X emain quiet X , the current batch eval-uation methodology yields results that are idiosyncratic in many cases.

To untangle the effect of these  X  X uiet days X  in our inter-leaved evaluation methodology, we conducted experiments where we simply discarded days in which there were no rel-evant tweets. That is, if an interest profile only contained three days (out of ten) that contained relevant tweets, the score of that topic is simply an average of the scores over those three days. We modified the batch evaluation scripts to also take this into account, and then reran our simulation experiments. The results are shown in Table 3 and Table 4 under the rows marked discarding  X  X uiet days X . In this vari-ant, we see that our simulation results are quite accurate, which confirms that the poor accuracy of our initial results is attributable to days where there are no relevant tweets. Once again, this is an issue with the overall TREC evalu-ation methodology, rather than a flaw in our interleaving approach. These findings highlight the need for additional research on metrics that better model sparse topics. In or-der to remove this confound, for the remaining prospective notification experiments, we discarded the  X  X uiet days X .
Our credit assignment algorithm is recall oriented in that it tries to quantify the total amount of relevant informa-tion a user receives, and so it is perhaps not a surprise that credit correlates with nCG. However, experiments show that we also achieve good accuracy correlating with ELG (which is precision oriented). It is observed in the TREC 2015 evaluation [14] that there is reasonable correlation between systems X  nCG and ELG scores. There is no principled ex-planation for this, as prospective notification systems could very well make different precision/recall tradeoffs. However, there is the additional constraint that systems are not al-lowed to push more than ten tweets per day, so that a high-Condition  X   X   X  Total  X   X   X  Total  X   X   X  Total  X   X   X  Total Retaining  X  X uiet days X  Discarding  X  X uiet days X  All Pairs 56.7% 34.8% 91.5% 8.4% 0.1% 8.5% 56.8% 34.8% 91.6% 8.3% 0.1% 8.4% Condition  X   X   X  Total  X   X   X  Total  X   X   X  Total  X   X   X  Total Retaining  X  X uiet days X  Discarding  X  X uiet days X  All Pairs 62.5% 35.2% 97.7% 2.1% 0.2% 2.3% 62.7% 35.3% 98.0% 2.0% 0 2.0% Table 5: Lengths of interleaved results as a percent-age of the sum of the lengths of the individual runs. volume low-precision system would quickly use up its  X  X aily quota X . Additionally, we suspect that since TREC 2015 rep-resented the first large-scale evaluation of this task, teams have not fully explored the design space.
We next turn our attention to two issues related to asses-sor effort: the length of the interleaved system output (this subsection) and the effort involved in providing explicit judg-ments in our interaction model (next subsection).

One downside of our temporal interleaving strategy is that the interleaved results are longer than the individual system outputs. Exactly how much longer is shown in Table 5, where the lengths of the interleaved results are shown as a percentage of the sum of the lengths of the individual runs. The lengths are not 100% because the individual system outputs may contain overlap, and comparisons between runs from the same group contain more overlap. Nevertheless, we can see that temporal interleaving produces output that is substantially longer than each of the individual system outputs. This is problematic for two reasons: first, it means a substantial increase in evaluation effort, and second, the interleaving produces a different user experience in terms of the verbosity of the system.

There is, however, a simple solution to this issue: after temporal interleaving, for each result we flip a biased coin and retain it with probability p . That is, we simply decide to discard some fraction of the results. Figure 5 shows the re-sults of these experiments. On the x axis we sweep across p , the retention probability, and on the y axis we plot the sim-ulation accuracy (i.e., agreement between simulation credit and batch results). The left plot shows the results for ret-rospective summarization using unweighted recall and the (official, official) condition; the rest of the graphs look sim-ilar and so we omit them for brevity. In the middle plot, we show accuracy against ELG for prospective notification and against nCG on the right (both discarding quiet days). Since there is randomness associated with these simulations, the plots represent averages over three trials.

We see that simulation results remain quite accurate even if we discard a relatively large fraction of system output. For the prospective task, accuracy is higher for lower p values because there are many intra-group ties. At p = 0, accuracy is simply the fraction of  X  X o difference X  comparisons. Based on these results, an experiment designer can select a desired tradeoff between accuracy and verbosity. With p around 0.5 to 0.6, we obtain an interleaved result that is roughly the same length as the source systems X  X nd in that region we still achieve good prediction accuracy. It is even possi-ble to generate interleaved results that are shorter than the input runs. Overall, we believe that this simple approach adequately addresses the length issue.
Another potential objection to our interleaved evaluation methodology is that our interaction model depends on ex-plicit judgments for credit assignment, as opposed to implicit judgments (i.e., clicks) in the case of interleaved evaluations for web ranking. This issue warrants some discussion, be-cause the ability to gather implicit judgments based on be-havioral data greatly expands the volume of feedback we can easily obtain (e.g., from log data).

We have two responses: First, it is premature to explore implicit interactions for our tasks. For web search, there is a large body of work spanning over two decades that has validated the interpretation of click data for web ranking preferences X  X ncluding the development of click models [1, 4], eye-tracking studies [5, 9], extensive user studies [10], and much more [11]. In short, we have a pretty good idea of how users interact with web search results, which justifies the interpretation of click data. None of this exists for ret-rospective summarization and prospective notification. Fur-thermore, interactions with tweets in our case are more com-plex: some tweets have embedded videos, images, or links. There are many different types of clicks: the user can  X  X x-pand X  a tweet, thereby showing details of the embedded ob-ject and from there take additional actions, e.g., play the embedded video directly, click on the link to navigate away from the result, etc. Not taking any overt action on a tweet doesn X  X  necessary mean that the tweet is not relevant X  X he succinct nature of tweets means that relevant information can be quickly absorbed, perhaps without leaving any be-havioral trails. Thus, any model of implicit interactions we could develop at this point would lack empirical grounding. More research is necessary to better understand how users interact with retrospective summarization and prospective notification systems. With a better understanding, we can then compare models of implicit feedback with the explicit feedback results presented here.

Our second response argues that in the case of prospective notifications, an explicit feedback model might not actually be unrealistic. Recall that such updates are putatively de-livered via mobile phone notifications, and as such, they are presented one at a time to the user X  X epending on the user X  X  settings, each notification may be accompanied by an audi-tory or physical cue (a chime or a vibration) to attract the user X  X  attention. In most implementations today the noti-fication can be dismissed by the user or the user can take additional action (e.g., click on the notification to open the mobile app). These are already quite explicit actions with relatively clear user intent X  X t is not far-fetched to imag-ine that these interactions can be further refined to provide explicit judgments without degrading the user experience.
Nevertheless, the issue of assessor effort in providing ex-plicit judgments is still a valid concern. However, we can potentially address this issue in the same way as the length issue discussed above. Let us assume that the user provides interaction data with probability r . That is, as we run the simulation, we flip a biased coin and observe each judgment with only probability r . In the prospective notification case, we argue that this is not unrealistic X  X he user  X  X ays atten-tion X  to the notification message with probability r ; the rest of the time, the user ignores the update.

Figure 6 shows the results of these experiments (averaged over three trials). On the x axis we sweep across r , the interaction probability and on the y axis we plot the sim-ulation accuracy. The left plot shows the results for ret-rospective summarization using unweighted recall and the (official, official) condition. In the middle plot, we show ac-curacy against ELG for prospective notification and against nCG on the right (once again, discarding quiet days in both cases). Experiments show that we are able to accurately decide the relative effectiveness of the comparison systems even with limited user interactions.

The next obvious question, of course, is what if we com-bined both the length analysis and interaction probabil-ity analysis? These results are shown in Figure 7, orga-nized in the same manner as the other graphs (also av-eraged over three trials). For clarity, we only show re-sults for all pairs. The interaction probability r is plotted on the x axis, with lines representing retention probabil-to achieve good accuracy, even while randomly discarding system output, and with limited interactions. Given these tradeoff curves, an experiment designer can strike the de-sired balance between accuracy and verbosity.
In this paper, we describe and validate a novel interleaved evaluation methodology for two complementary information seeking tasks on document streams: retrospective summa-rization and prospective notification. We present a tem-poral interleaving strategy and a heuristic credit assignment method based on a user interaction model with explicit judg-ments. Simulations on TREC data demonstrate that our evaluation methodology yields high fidelity comparisons of the relative effectiveness of different systems, compared to the results of batch evaluations.

Although interleaved evaluations for web search are rou-tinely deployed in production environments, we believe that our work is novel in that it tackles two completely different information seeking scenarios. Retrospective summarization and prospective notification are becoming increasingly im-portant as users continue the shift from desktops to mobile devices for information seeking. There remains much more work, starting with a better understanding of user interac-tions so that we can develop models of implicit judgment and thereby greatly expand the scope of our evaluations, but this paper takes an important first step. This work was supported in part by the U.S. National Science Foundation (NSF) under awards IIS-1218043 and CNS-1405688 and the Natural Sciences and Engineering Re-search Council of Canada (NSERC). All views expressed here are solely those of the authors. We X  X  like to thank Charlie Clarke and Luchen Tan for helpful discussions.
