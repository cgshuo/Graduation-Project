 1. Introduction
Keyword spotting is a very futuristic and promising branch in unremitting speech recognition and it is useful to retrieve the speech fi les which enclose the words or phrases associated with an application-speci fi c domain. It resolves the problem of obtain-ing the right word or phrase in the speech fl ow. Keyword spotting technologies are extensively used in the security services, tele-communication companies, radio stations, call-centers, broadcast-ing companies and other organizations that use a large stream or collection of speech information. They are looked-for brisk seek in huge data sets. Keyword detection systems can be utilized not only in telephone conversations, but also in video, audio streams, that greatly accelerate the process of data tracking.

Several approaches to this problem have been proposed in the literature ( Jansen and Niyogi, 2009 ). Investigating the task of spotting prede fi ned keywords in continuous speech has both practical and scienti fi c motivations, even in situations where little access to non-lexical linguistic constraints is provided (e.g. spotting native words in an unfamiliar language). Several computational approaches to this problem have been proposed. One of the keyword spotting strategies, proposed by Bridle (1983) , involved sliding a frame-based keyword template along the speech signal and using a nonlinear dynamic time warping algorithm to pro fi ciently search for a match. While the word models in later approaches changed signi fi cantly, this sliding model strategy was used in other approaches ( Wilpon et al., 1989; Silaghi and Bourlard, 2000 ).

A Japanese spoken term detection method for spoken docu-ments proposed ( Nakagawa et al., 2013 ) that robustly considers out of vocabulary (OOV) words and mis-recognition. To address OOV words, recognition errors and high speed retrieval, a distant n-gram indexing/retrieval method incorporates a distance metric in a syllable lattice. An ef fi cient approach ( Norouzian and Rose, 2012 ) to spoken term detection (STD) from unstructured audio recordings using word lattices generated off-line from an auto-matic speech recognition (ASR) system has been proposed. The approach facilitates open vocabulary STD and focuses speci on reducing the difference between detection performance obtained for with in-vocabulary (IV) and out-of-vocabulary search terms. A hybrid two-pass approach ( Norouzian and Rose, 2014 ) for facilitating fast and ef fi cient open vocabulary spoken term detec-tion (STD) is proposed. A large vocabulary continuous speech recognition (LVCSR) system is deployed for producing word lattices from audio recordings. An index construction technique is used for facilitating very fast search of lattices for occurrences of both in-vocabulary (IV) and out-of-vocabulary query terms.

An unsupervised learning framework was proposed ( Zhang et al., 2009 ) to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posterior gram.
For the given one or more spoken examples of a keyword, they use segmental dynamic time warping to compare the Gaussian poster-ior grams between keyword samples and test utterances. A standard hidden Markov model (HMM) based method is the key word-fi ller model. In this case, an HMM consists of three compo-nents: a keyword model, a background model, and a fi ller model.
The keyword model is tied to the fi ller model, which is typically a phone or a broad class loop, meant to represent the non-keyword portions of the speech signal. Finally, the background model is used to normalize keyword model scores. A Viterbi decode of a speech signal is performed using this keyword-fi ller HMM, produ-cing predictions when the keyword occurs. Variations of this approach are provided by Wilpon et al. (1990) , Hofstetter and
Rose (1992) , Rose and Paul (1990) , and Szoke et al. (2005) . The research effort focused on de fi ning specialized con fi dence mea-sures that maximize performance is provided by James and Young (1994) , Weintraub (1995) , Junkawitsch et al. (1996) , and
Thambiratnam and Sridharan (2005) . While these systems do not require a prede fi ned vocabulary, they rely on language modelling and are thus highly tuned to the training environment.
A further extension of HMM spotter approaches consists of using large vocabulary continuous speech recognition HMMs. This approach can actually be seen as a phonetic-based approach in which the garbage model only allows valid words from the lexicon, except the targeted keyword. This use of additional linguistic constraints is shown to improve the spotting perfor-mance ( Cardillo et al., 2002; Rose and Paul, 1990 ). Such an approach however raises practical concerns: one can wonder whether the design of a keyword spotter should require the expensive collection a large amount of labeled data typically needed to train LVCSR systems, as well as the computational cost implied by large vocabulary decoding ( Manos and Zue, 1997 ).
Over the last years, signi fi cant effort toward discriminative training of HMMs has been proposed as an alternative to like-lihood maximization ( Bahl et al., 1986; Juang et al., 1997; Fu and
Juang, 2009 ). These training approaches aim at both maximizing the probability of the correct transcription given an acoustic sequence and minimizing the probability of the incorrect tran-scriptions given an acoustic sequence. When applied to keyword spotting, none of these approaches closely tie the training objec-tive with a fi nal spotting objective, such as maximizing the area under the Receiver Operating Characteristic (ROC) curve.
A method that has been proposed by Itoh et al. (2012) is to realize pseudo-real-time spoken term detection using pre-retrieval results.
Pre-retrieval results for all combination of syllable bigrams are prepared beforehand. The retrieval time depends on the number of candidate sections of the pre-retrieval results. A few top candidates are obtained in almost real-time by limiting the small number of candidate sections. While a user is con fi rming the candidate sections, the system can conduct the rest of retrieval by increasing the number of candidate sections gradually. A technique for phonetic spoken term detection in large audio archive ( Vavruska et al., 2013 )is designed within the framework of weighted fi nite-state transducers and utilizes the rather recently developed notion of factor automata, whichwehaveenhancedwithascorenormalizationandatechnique for systematic query expansion which allows for phone deletions and substitutions and consequently compensates for frequent pronuncia-tion imperfections and systematic phoneme interchanges occurring during the ASR decoding process. A new approach to spoken keyword detection using Auto-Associative Neural Networks (AANN) has been proposed ( Jothilakshmi, 2014 ) which concerns the use of the distribution capturing ability of the AANN for spoken keyword detection. It is based on the con fi dence score obtained from the normalized squared error of AANN.

The most important involvement of this paper concerns the exploitation of the misclassi fi cation rate obtained from the trained
SVM hyperplane of the two classes ( 1 and  X  1) for spoken keyword detection ( Jothilakshmi et al., 2009 ). The proposed method involves sliding a frame-based keyword pattern along the input audio signal and initially a block of frames belongs to the search keyword is set to ( 1) class and a block of frames such that the number of frames in the block is equal to number of frames of the keyword signals are selected from the input signal starting from the fi rst frame is set to (  X  1) class. Then the SVM is trained using these two classes and hyperplane is obtained between these two classes. By using this hyperplane the frames of these two to pro fi ciently search for a match. This work formulates a new spoken keyword spotting system.

The rest of the paper is organized as follows: a brief description about the method of extracting the features for spoken keyword spotting from the speech signal is described in Section 2 . Support vector machine for spoken keyword spotting is given in Section 3 .
The proposed algorithm for spoken keyword detection is pre-sented in Section 4 . Section 5 presents the performance measures for the proposed spoken keyword spotting system. Section 6 presents the experimental results. Section 7 gives the conclusions and describes the future work. 2. Feature extraction for keyword detection
MFCC has proven to be one of the most successful feature representations in speech related recognition tasks. The mel-cepstrum exploits auditory principles, as well as the decorrelating property of the cepstrum ( Davis and Mermelstein, 1980 ). Fig. 1 illustrates the computation of MFCC features for a segment of speech signal which is described as follows ( HTK book, 2002 ): 1. The speech waveform is fi rst windowed with analysis win-dow and the discrete short time Fourier transform (STFT) is computed. 2. The magnitude is then weighted by a series of fi lter frequency responses whose center frequencies and bandwidths roughly match those of the auditory critical band fi lters. These follow the mel scale whereby band edges and center frequen-cies of the fi lters are linear for low frequency and logarithmi-cally increase with increasing frequency as shown in Fig. 2 .We call these fi lters as mel-scale fi lters and collectively a mel-scale fi lter bank. This fi lter bank, with 24 triangularly shaped frequency responses, is a rough approximation to actual audi-tory critical band fi lters covering a 4000 Hz range. 3. The log energy in the STFT weighted by each mel-scale fi frequency response is computed. 4. Finally discrete cosine transform (DCT) is applied to the bank output to produce the cepstral coef fi cients.
 3. Support vector machine (SVM) for keyword spotting Support vector machine is based on the principle of Structural
Risk Minimization (SRM). Like Radial Basis Function Neural Net-works (RBFNN), support vector machines can be used for pattern classi fi cation and nonlinear regression. SVM constructs a linear model to estimate the decision function using non-linear class boundaries based on support vectors. If the data are linearly separated, SVM trains linear machines for an optimal hyperplane that separates the data without error and into the maximum distance between the hyperplane and the closest training points.
The training points that are closest to the optimal separating hyperplane are called support vectors. SVM maps the input patterns into a higher dimensional feature space through some nonlinear mapping chosen a priori. A linear decision surface is then con-structed in this high dimensional feature space. Thus, SVM is a linear classi fi er in the parameter space, but it becomes a nonlinear classi fi er as a result of the nonlinear mapping of the space of the input patterns into the high dimensional feature space.
The support vector machine ( Vapnik, 1998 ) is a useful statistic machine learning technique that has been successfully applied in the pattern recognition tasks ( Jiang et al., 2005; Guo and Li, 2003;
Ramalingam, 2006; Geetha et al., 2009 ). If the data are linearly nonseparable but nonlinearly separable, the nonlinear support vector classi fi er will be applied. The basic idea is to transform input vectors into a high-dimensional feature space using a non-linear transformation  X  , and then to do a linear separation in feature space as shown in Fig. 3 .

A nonlinear support vector classi fi er implementing the optimal separating hyperplane in the feature space with a kernel function
K ( x i , x new ) is given by f  X  x new  X  X  sgn  X  where SV is the support vectors. The SVM has two layers. During the learning process, the fi rst layer selects the basis K  X  x the second layer constructs a linear function in this space. This is completely equivalent to constructing the optimal hyperplane in the corresponding feature space.

The SVM algorithm can construct a variety of learning machines by the use of different kernel functions. Four kinds of kernel functions are usually used. They are 1. Linear kernel:
K  X  x 1 ; x 2  X  X   X  x 1 ; x 2  X   X  2  X  2. Polynomial kernel of degree d :
K  X  x 1 ; x 2  X  X  X   X   X  x 1 ; x 2  X   X  c 0  X  d  X  3  X  3. Gaussian radial basis function (RBF):
K  X  x 1 ; x 2  X  X  exp  X   X   X  x 1 x 2  X  2  X  X  4  X  4. Sigmoidal kernel:
K  X  x 1 ; x 2  X  X  tan h  X   X   X  x 1 ; x 2  X   X  c 0  X  X  5  X  where kernel parameters  X  : width of RBF coef fi cient in polynomial d : degree of polynomial c 0 : additive constant in polynomial
In Lu et al. (2001) , an SVM classi fi cation based supervised technique was proposed for spoken term detection in which they adopted a bottom up binary tree combining three two class SVM classi fi ers for content based audio segmentation. SVM based supervised technique was proposed in Karthik et al. (2005) by labeling the speech data around the spoken keyword as (  X  1) class and the block of frames in the audio fi les as ( 1) class for training an SVM hyperplane and then classi fi ed each window as (  X  1) or ( 1). 4. Proposed spoken keyword spotting algorithm
The keyword spotting consists of the technical elements pre-sented in the previous sections. It is presumed that the acoustic features have been extracted from the speech signal.

The outline of the algorithm is summarized as follows: fi MFCC features (discussed in Section 2 ) for every single frame of the given search keyword speech signal are obtained. Likewise the speech features are obtained for each frame of the given input signal in which the given keyword should be detected. Initially a block of frames belongs to the search keyword is set to ( 1) class and a block of frames such that the number of frames in the block is equal to number of frames of the keyword signals are selected from the input signal starting from the fi rst frame is set to (  X  1) class. Then the SVM is trained using these two classes and the hyperplane is obtained between these two classes. If the search word corresponding to the block of frames is same as the search keyword then the misclassi fi cation rate will be very high. If the search word corresponding to the block of frames is not same as the search keyword, then feature vectors from the block possibly will not fall into the hyperplane and the model gives low misclassi fi cation (probability) rate.

Similarly, the next possibility is the word corresponding to the block of frames which partly match with the search keyword. If this is the case, the misclassi fi cation rate of the block will be in amidst the above two values. After obtaining the misclassi rate for the current block, the block is shifted by a fi xed number of frames to the right. Then the entire process is reiterated for this fresh block. In the same way the misclassi fi cation rates are measured up to the tail end of the block reaches the last frame of the input speech frames. From the misclassi fi cation rate, the global maximum positions are the locations for the search key-word in the input signal. Fig. 4 shows the steps involved in the proposed algorithm. 4.1. Keyword spotting
Given the speech features of the each input signal S  X f S 1 ; 2 ; ... ; n g where i is the frame index and n is the total number of frames in the input speech signal. As well the speech features of the keyword signal K  X f K j : j  X  1 ; 2 ; ... ; m g where j is the frame index and m is the total number of frames present in the search keyword signal. The proposed algorithm for retrieving the speech fi les for the given search speech keyword is summarized as follows:
Initially a block of frames which belongs to the search keyword is considered as W and it is set to ( 1) class: W  X f S i : i  X  1 ; 2 ; ... ; n g A f 1 g
From n frames of input speech signal, m numbers of frames are carefully chosen, and considered as W  X  and it is set to (  X  1) class :
W  X   X f S l g ; p  X  l o m  X  p A f X  1 g
The SVM is trained using these two classes and the hyperplane is obtained between these two classes.

By using this hyperplane the frames of these two classes are classi fi ed. If the search word corresponding to the block of frames is same as the search keyword then the misclassi fi rate will be very high. If the word corresponding to the block of frames is not same as the search keyword, then the feature vectors from the block possibly will not fall into the hyperplane and the model gives low misclassi fi cation (probability) rate.
From this we know that the SVM training misclassi fi cation rate can be used to decide whether the search keyword occurs in the selected block of frames.

Two types of misclassi fi cation rates are computed namely mcr  X  k  X  and mcr  X   X  k  X  , where mcr  X  k  X  is the rate of the ( 1) class misclassi fi ed as ( 1) and mcr  X   X  k  X  is the rate of the (  X  1) class misclassi fi ed as (  X  1) .

Detect keywords from the misclassi fi cation rate by applying a threshold. The threshold ( t s ) is calculated from the con score as follows: t  X  as max ; 0 : 5 o a o 1  X  6  X  where s max is the global maximum misclassi fi cation rate and a is the adjustable parameter.

Then the window is shifted by a fi xed number of frames to the right of current position and the procedure is repeated. Like-wise the entire speech stream must be examined. 5. Performance measures
The purpose of this research is to identify keywords within audio documents based on the SVM misclassi fi cation rates. Unlike
Automatic Speech Recognition, which typically considers the correct recognition of all words that are equally important, we are interested in the tradeoff of precision and recall. First the performance of the proposed keyword spotting algorithm is assessed in terms of detection rate which is de fi ned as
Detection Rate  X  n c n where n c is the number of correctly classi fi ed keywords, n number of incorrectly classi fi ed keywords, and n r is the number of rejected keywords.

The following metrics are used to evaluate the systems pre-sented in this work. The fi gure of merit (FOM) was originally de fi ned by Rohlicek et al. (1989) for the task of keyword spotting.
By optimizing the FOM ( Wallace et al., 2010, 2011 ) the accuracy of the spoken term detection can be increased. It gives the average detection rate over the range [1, 10] false alarms per hour per keyword. The FOM values for individual keywords can be averaged in order to give an overall fi gure. The NIST STD 2006 evaluation plan de fi ned the metrics Occurrence-Weighted Value (OCC) and Actual Term-Weighted Value (ATWV) and a Maximum Term
Weighted (MTWV). These three metrics have been adopted and their description follows.

For a given set of terms and some speech data, let N correct N
FA  X  t  X  and N true  X  t  X  represents the number of correct, false alarm, and actual occurrences of term t respectively. In addition, we denote the number of non-target terms (which gives the number of possibilities for incorrect detection) as N NT ( t ). We also de
P miss  X  t  X  X  1 P  X  t  X  X  N FA  X  t  X  N
In order to tune the metrics to give a desired balance of precision versus recall, a cost C FA for false alarms was de along with a value V for correct detections. The occurrence-weighted value is computed by accumulating a value for each correct detection and subtracting a cost for false alarms as follows:
OCC  X 
Whilst OCC gives a good indication of overall system perfor-mance, there is an inherent bias towards frequently occurring terms. The second NIST metric, the actual term-weighted value is arrived at by averaging a weighted sum of miss and false alarm probabilities, P miss  X  t  X  and P FA  X  t  X  , over the terms:
ATWV  X  1 1 T  X  set a uniform prior term probability P prior  X  t  X  X  10 4 , and the ratio c =  X  to be 0.1 with the effect that there is an emphasis placed on recall compared to the precision in the ratio 10:1. The third term maximum term-weighted value is over the range of all possible values of threshold. It ranges from 0 to  X  1.

In this work, the results in terms of FOM and OCC are presented. However, rather than giving the ATWV values which give point estimates of the miss and false alarm probabilities, we present these results graphically in order to show the full range of operating points. For all results, tuning for the parameters using the developed algorithm is performed on STD development set according to the metric which is used in evaluation. For all measures, the higher values indicate better performance. The performance is also evaluated by the recall  X  precision curve (ROC curve). 6. Experiments and results 6.1. The databases
The experiments have been conducted over a corpus which is composed of broadcast news and conversations recorded from various channels like BBC, NDTV, Doordarshan News and real time recorded speech. It includes three different source types: two hours of broadcast news (BNEWS), one and a half hour of conversational telephony speech (CTS) and one hour of real time conference room meetings (REALCONFSP). For the experiments, we have processed the query set that includes 350 queries. Each query is a phrase containing between one to fi ve terms, common and rare terms, terms that are in the manual transcripts and those that are not. The dataset is divided into the development corpus and evaluation corpus. The development corpus is utilized for training the struc-ture and fi ne-tuning the parameters which is composed of two hour speech from the above three source types each including 100 search terms. The evaluation corpus is composed of 5 h speech including 250 search terms which is for validation. 6.2. Feature extraction
The mel frequency cepstral coef fi cients with delta and accel-eration coef fi cients are used to evaluate the proposed algorithm. So each feature vector consists of 39 coef fi cients. Cepstral mean subtraction is performed to trim down the channel effects. The preferred properties of the speech signals are a sampling rate of 8 kHz, 16 bit monophonic PCM format. The frame rate is as same as the keyword frames/second, where each frame is 16 ms in dura-tion with an overlap of 50% between adjacent frames. 6.3. Parameter tuning phase
In order to adjust the parameters of the algorithm that defer the preeminent performance, several experiments have been performed on development corpus, whose results are provided in this subsection. The parameters to be tuned are the number of frame shifts and different kernels of SVM. The MFCC feature vectors are extracted for all the speech frames as described in Sections 2 and 6.2 . For the given keyword feature vectors, the misclassi fi cation rates are determined by the hyperplane of the two classes (  X  1 and 1) as described in Section 3 .

The feature vectors of Wp are set as (  X  1) classes and the feature vectors of search keyword are set to ( 1) classes. SVM is trained for these two classes and the hyperplane is obtained. By using this hyperplane the frames of these two classes are classi using the misclassi fi cation rate. Fig. 5 shows the progression of the misclassi fi cation rate when the frame shift is changed from 1/2th, 1/4th, 1/8th, 1/16th, 1/32th and 1/64th. There is no change in the misclassi fi cation rate after the 1/16th even though the frame shift was increased to 1/32th and 1/64th. Hence the SVM models are trained for only 1/16th frameshift. Similarly the classi fi is measured for three different types of SVM kernels and based on the results with the SVM classi fi er the linear kernel function with the upper bound of the Lagrange multiplier C  X  1 has been used.
It is not possible to obtain the same misclassi fi cation rates for the same keyword query every time. To avoid the false keyword spotting the misclassi fi cation rates which are less than the thresh-old value are considered. Hence, after obtaining the global maxima of the con fi dence scores for the entire speech signal, the hypothe-sized keyword is validated by using the threshold. For calculating the threshold, the adjustable parameter is used in this experiment. 7. Evaluation results The experiments were conducted on the database described in
Section 6.1 . A set of 250 search queries was elected depending on their high frequency of occurrence and appropriateness as search terms for spoken keyword spotting, and evaluation (retrieving search terms) is performed on the test set. 7.1. Spoken keyword spotting results
Recognition accuracy: Whilst the detection rate is not the main focus of this work, it is an important factor in STD performance.
In Table 1 , we present the recognition accuracy results after providing the empirical tuned parameter values to the algorithm.

Evaluation in terms of FOM and OCC: Table 2 shows that the evaluation in terms of the FOM, BNEWS renders better perfor-mance than the source types CTS and REALCONSP. Similarly in accordance with the term OCC, again the BNEWS provides the best performance.
 We presented detection error trade off (DET) curves of the
ATWV performance in Fig. 6 which shows the miss against false alarm probability for each of the source types. The DET curves in Fig. 6 show that the performances are almost quite similar for each of the source types BNEWS and CTS and these two are outperformed than the REALCONSP. 7.2. Evaluation in terms of ATWT and MTWV
For each found occurrence of the given query, our system outputs the location of the term in the audio recording (begin time and duration), the score indicating how likely is the occurrence of the query, and a hard decision as to whether the detection is correct. We measure precision and recall by comparing the results obtained over the automatic transcripts (only the results having true hard decision) to the results obtained over the reference manual transcripts. Our aim is to evaluate the ability of the suggested retrieval approach to handle transcribed speech data.
Thus, the closer the automatic results to the manual results are, the better the search effectiveness over the automatic transcripts will be. The results returned from the manual transcription for a given query are considered to be relevant and are expected to be retrieved with the highest scores.

The retrieval performance is evaluated by the recall  X  precision curve (ROC curve). Besides the recall and the precision, we use the evaluation measures de fi ned by NIST for the 2006 STD evaluation the actual term-weighted value and the maximum term-weighted value. The term-weighted value (TWV) is computed by fi rst computing the miss and false alarm probabilities for each query separately, then using these and (arbitrarily chosen) prior prob-ability to compute query-speci fi c values, and fi nally averaging this query-speci fi c values over all query q to produce an overall system value: Table 3 shows the values of the above measures and Fig. 7 shows the ROC curve for the source types. Fig. 7 clearly shows that the BNEWS gives the best performance over the CTN and REALSP.
In order to justify, the overall performance of the proposed work is compared with the existing method based on the Auto-Associative Neural Network algorithm ( Jothilakshmi, 2014 ).
Table 4 depicts that the performance of the proposed method is superior to the existing method ( Jothilakshmi, 2014 ). 8. Conclusion
In this paper, a novel method for spoken keyword spotting is proposed using MFCC features and support vector machine. The misclassi fi cation rate of SVM hyperplane is used for spoken term detection. The proposed method involves sliding a frame-based keyword template alongside the speech signal and by means of misclassi fi cation rate of the SVM hyperplane obtained from the two classes ( 1 and  X  1) competently search for a match. This work formulates a new spoken keyword detection algorithm. This work studies how the spoken keyword spotting is performed ef fi ciently over different data sources. The experiment reveals that all the measures provided the best performance for the source type. It yields the overall performance at around 94.5% of the spoken term detection which is better than the methods in the literature. The future work will be on the track to progress the ef fi cacy of the algorithm by optimizing the time taken to scrutinize the complete audio fi le frame by frame.
 References
