 Peng Sun sunp08@mails.tsinghua.edu.cn Tsinghua University, Beijing 100084, China Mark D. Reid mark.reid@anu.edu.au Jie Zhou jzhou@tsinghua.edu.cn Tsinghua University, Beijing 100084, China Boosting is successful for both binary and multi-class classification ( Freund &amp; Schapire , 1995 ; Schapire &amp; Singer , 1999 ). Among those popular variants, we are particularly focusing on LogitBoost ( Friedman et al. , 1998 ) in this paper. Originally, LogitBoost is motivat-ed by statistical view ( Friedman et al. , 1998 ), where boosting algorithms consists of three key components: the loss, the function model, and the optimization al-gorithm. In the case of LogitBoost, these are the Logit loss, the use of additive tree models, and a stage-wise optimization, respectively. There are two important factors in the LogitBoost setting. Firstly, the posteri-or class probability estimate must be normalised so as to sum to one in order to use the Logit loss. This lead-s to a coupled classifier output, i.e. , the sum-to-zero classifier output. Secondly, a dense Hessian matrix arises when deriving the tree node split gain and node value fitting. It is challenging to design a tractable optimization algorithm that fully handles both these factors. Consequently, some simplification and/or ap-proximation is needed. Friedman et al. ( 1998 ) propos-es a  X  X ne scalar regression tree for one class X  strate-gy. This breaks the coupling in the classifier output so that at each boosting iteration the model updat-ing collapses to K independent regression tree fittings, where K denotes the number of classes. In this way, the sum-to-zero constraint is dropped and the Hessian is approximated diagonally.
 Unfortunately, Friedman X  X  prescription turns out to have some drawbacks. A later improvement, ABC-LogitBoost, is shown to outperform LogitBoost in terms of both classification accuracy and conver-gence rate ( Li , 2008 ; 2010a ). This is due to ABC-LogitBoost X  X  careful handling of the above key prob-lems of the LogitBoost setting. At each iteration, the sum-to-zero constraint is enforced so that only K  X  1 scalar trees are fitted for K  X  1 classes. The remaining class  X  called the base class  X  is selected adaptively per iteration (or every several iterations), hence the acronym ABC (Adaptive Base Class). Also, the Hes-sian matrix is approximated in a more refined manner than the original LogitBoost when computing the tree split gain and fitting node value.
 In this paper, we propose two novel techniques to ad-dress the challenging aspects of the LogitBoost set-ting. In our approach, one vector tree is added per iteration. We allow a K dimensional sum-to-zero vec-tor to be fitted for each tree node. This permits us to explicitly formulate the computation for both node split gain and node value fitting as a K dimensional constrained quadratic optimization, which arises as a subproblem in the inner loop for split seeking when fitting a new tree. To avoid the difficulty of a dense Hessian, we propose that for each of these subprob-lems, only two coordinates ( i.e. , two classes or a class pair) are adaptively selected for updating, hence the name AOSO (Adaptive One vS One) LogitBoost. Fig-ure 1 gives an overview of our approach. In Section 2.5 we show that first order and second order approxima-tion of loss reduction can be a good measure for the quality of selected class pair.
 Following the above formulation, ABC-LogitBoost, al-though derived from a somewhat different framework in ( Li , 2010a ), can thus be shown to be a special case of AOSO-LogitBoost, but with a less flexible tree model. In Section 3.1 we compare the differences between the two approaches in detail and provide some intuition for AOSO X  X  improvement over ABC.
 The rest of this paper is organised as follows: In Sec-tion 2 we first formulate the problem setting for Log-itBoost and then give the details of our approach. In Section 3 we compare our approach to (ABC-)LogitBoost. In Section 4 , experimental results in terms of classification errors and convergence rates are reported on a range of public datasets. We begin with the basic setting for the LogitBoost algorithm. For K -class classification ( K  X  2), consider an N example training set { x i , y i } N i =1 where x i denotes a feature value and y i  X  { 1 , . . . , K } denotes a class label. Class probabilities conditioned on x , denoted by p = ( p 1 , . . . , p K ) T , are learned from the training set. For a test example with known x and unknown y , we predict a class label by using the Bayes rule: y = arg max k p k , k = 1 , . . . , K .
 Instead of learning the class probability directly, one learns its  X  X roxy X  F = ( F 1 , . . . , F K ) T given by the so-called Logit link function: with the constraint 1998 ). For simplicity and without confusion, we here-after omit the dependence on x for F and for other related variables.
 The F is obtained by minimizing a target function on training data: where F i is shorthand for F ( x i ) and L ( y i , F i ) is the Logit loss for a single training example: where r ik = 1 if y i = k and 0 otherwise. The proba-bility p ik is connected to F ik via ( 1 ).
 To make the optimization of ( 2 ) feasible, a model is needed to describe how F depends on x . For exam-ple, linear model F = W T x is used in traditional Logit regression, while Generalized Additive Model is adopted in LogitBoost: where each f m ( x ), a K dimensional sum-to-zero vec-tor, is learned by greedy stage-wise optimization. That is, at each iteration f m ( x ) is added only based on F = This procedure repeats M times with initial condition F = 0. Owing to its iterative nature, we only need to know how to solve ( 5 ) in order to implement the optimization. 2.1. Vector Tree Model The f ( x ) in ( 5 ) is typically represented by K scalar regression trees ( e.g. , in LogitBoost ( Friedman et al. , 1998 ) or the Real AdaBoost.MH implementation in ( Friedman et al. , 1998 )) or a single vector tree ( e.g. , the Real AdaBoost.MH implementation in ( K  X egl &amp; Busa-Fekete , 2009 )). In this paper, we adopt a single vec-tor tree. We further restrict that it is a binary tree ( i.e. , only binary splits on internal node are allowed) and the split must be vertical to coordinate axis, as in ( Friedman et al. , 1998 ) or ( Li , 2010a ). Formally, where { R j } J j =1 describes how the feature space is par-titioned, while t j  X  R K with a sum-to-zero constraint is the node values/vector associated with R j . See Fig-ure 1 for an example. 2.2. Tree Building Solving ( 5 ) with the tree model ( 6 ) is equivalent to determining the parameters { t j , R j } J j =1 at the m -th iteration. In this subsection we will show how this problem reduces to solving a collection of convex op-timization subproblems for which we can use any nu-merical method. Following Friedman X  X  LogitBoost set-tings, here we use Newton descent 1 . Also, we will show how the gradient and Hessian can be computed incre-mentally.
 We begin with some shorthand notation for the node loss : where I denotes the index set of the training examples on some either internal or terminal node ( i.e. , those falling into the corresponding region). Minimizing ( 7 ) is the bridge to { t j , R j } in that: 1. To obtain { t j } with given { R j } , we simply take 2. To obtain { R j } , we recursively perform binary s-The key to the second point is to explicitly give the n-ode split gain. Suppose an internal node with n train-ing examples ( n = N for the root node), we fix on some feature and re-index all the n examples accord-ing to their sorted feature values. Now we need to find the index n  X  with 1 &lt; n  X  &lt; n that maximizes the node gain defined as loss reduction after a division between the n  X  -th and ( n  X  + 1)-th examples: where I = { 1 , . . . , n } , I L = { 1 , . . . , n  X  } and { n  X  +1 , . . . , n } ; t  X  , t  X  with index sets I , I L and I R , respectively. Generally, this searching applies to all features. The best division resulting to largest ( 9 ) is then recorded to perform the actual node split.
 Note that ( 9 ) arises in the context of an O ( N  X  D ) outer loop, where D is number of features. However, a na  X  X ve summing of the losses for ( 7 ) incurs an additional O ( N ) factor in complexity, which finally results in an unacceptable O ( N 2 D ) complexity for a single boosting iteration.
 A workaround is to use a Newton descent method for which both the gradient and Hessian can be incremen-tally computed. Let g , H respectively be the K  X  1 gradient vector and K  X  K Hessian matrix at t = 0 . By dropping the constant N odeLoss ( 0 ; I ) that is ir-relevant to t , the Taylor expansion of ( 7 ) w.r.t. t up to second order is: By noting the additive separability of ( 10 ) and using some matrix derivatives, we have where the diagonal matrix  X  P = diag ( p i 1 , . . . , p iK We then use ( 10 ) to compute the approximated node loss in ( 9 ). Thanks to the additive form, both ( 11 ) and ( 12 ) can be incrementally/decrementally computed in constant time when the split searching proceeds from one training example to the next. Therefore, the computation of ( 10 ) eliminates the O ( N ) complexity in the na  X  X ve summing of losses. 2 2.3. Properties of Approximated Node Loss To minimise ( 10 ), we give some properties for ( 10 ) that should be taken into account when seeking a solver. We begin with the sum-to-zero constraint. The proba-bility estimate p k in the Logit loss ( 3 ) must be non-zero and sum-to-one, which is ensured by the link function ( 1 ). Such a link, in turn, means that p k is unchanged by adding an arbitrary constant to each component in F . As a result, the single example loss ( 3 ) is invariant to moving it along an all-1 vector 1 . That is, where c is an arbitrary real constant (Note that 1 is, coincidentally, the orthogonal complement to the space defined by sum-to-zero constraint). This property also carries over to the approximated node loss ( 10 ): Property 1 loss ( t ; I ) = loss ( t + c 1 ; I ) . This is obvious by noting the additive separability in ( 10 ), as well as that g T i 1 = 0, 1 T H i 1 = 0 holds since p i is sum-to-one.
 For the Hessian, we have rank ( H )  X  rank ( H i ) by noting the additive form in ( 11 ). In ( Li , 2010a ) it is shown that det H i = 0 by brute-force determinant expansion. Here we give a stronger property: Property 2 H i is a positive semi-de nite matrix such that 1) rank ( H i ) =  X   X  1 , where  X  is the number of non-zero elements in p i ; 2) 1 is the eigenvector for eigenvalue 0.
 The proof can be found in this paper X  X  extended ver-sion ( Sun et al. , 2012 ).
 The properties shown above indicate that 1) H is sin-gular, so that unconstrained Newton descent is not applicable here, and 2) rank ( H ) could be as high as K  X  1, which prohibits the application of the standard fast quadratic solver designed for low rank Hessian-s. In the following we propose to address this prob-lem via block coordinate descent, a technique that has been successfully used in training SVMs ( Bottou &amp; Lin , 2007 ). 2.4. Block Coordinate Descent For the variable t in ( 10 ), we only choose two (the least possible number due to the sum-to-zero constraint) co-ordinates, i.e. , a class pair, to update while keeping the others fixed. Suppose we have chosen the r -th and the s -th coordinate (how to do so is deferred to next sub-section). Let t r = t and t s =  X  t be the free variables (such that t r + t s = 0) and t k = 0 for k  X  = r , k  X  = s . Plugging these into ( 10 ) yields an unconstrained one dimensional quadratic problem with regards to the s-calar variable t : where the gradient and Hessian collapse to scalars: h = To this extent, we are able to obtain the analytical expression for the minimizer and minimum of ( 16 ): by noting the non-negativity of ( 18 ).
 Based on ( 19 ), node vector ( 8 ) can be approximated as where g and h are respectively computed by using ( 17 ) and ( 18 ) with index set I mj . Based on ( 20 ), the node gain ( 9 ) can be approximated as where g (or g L , g R ) and h (or h L , h R ) are computed by using ( 17 ) and ( 18 ) with index set I (or I L , I R 2.5. Class Pair Selection In ( Bottou &amp; Lin , 2007 ) two methods for selecting ( r, s ) are proposed. One is based on a first order ap-proximation. Let t r and t s be the free variables and the rest be fixed to 0. For a t with sufficiently small fixed length, let t r =  X  and t s =  X   X  where  X  &gt; 0 is some small enough constant. The first order approximation of ( 10 ) is: loss ( t )  X  loss ( 0 ) + g T t = loss ( 0 )  X   X  (  X  g r  X  It follows that the indices r , s resulting in largest decre-ment to ( 23 ) are: Another method that can be derived in a similar way takes into account the second order information: Both methods are O ( K ) procedures that are better than the K  X  ( K  X  1) / 2 na  X  X ve enumeration. However, in our implementation we find that ( 25 ) achieves better results for AOSO-LogitBoost.
 Pseudocode for AOSO-LogitBoost is given in Algorith-m 1 . In this section we compare the derivations of Logit-Boost and ABC-LogitBoost and provide some intu-ition for observed behaviours in the experiments in Section 4 . 3.1. ABC-LogitBoost To solve ( 5 ) with a sum-to-zero constraint, ABC-LogitBoost uses K  X  1 independent trees: Algorithm 1 AOSO-LogitBoost. v is shrinkage factor that controls learning rate. 1: F ik = 0, k = 1 , . . . , K , i = 1 , . . . , N 2: for m = 1 to M do 4: Obtain { R mj } J j =1 by recursive region partition. 5: Compute { t mj } J j =1 by ( 21 ), where the class pair 6: F i = F i + v 7: end for In ( Li , 2010a ), the so-called base class b is selected by exhaustive search per iteration, i.e. , trying all possible b , which involves growing K ( K  X  1) trees. To reduce the time complexity, Li also proposed other methods. In ( Li , 2010c ), b is selected only every several itera-tions, while in ( Li , 2008 ), b is, intuitively, set to be the class that leads to largest loss reduction at last iteration.
 In ABC-LogitBoost the sum-to-zero constraint is ex-plicitly considered when deriving the node value and the node split gain for the scalar regression tree. In-deed, they are the same as ( 21 ) and ( 22 ) in this paper, although derived using a slightly different motivation. In this sense, ABC-LogitBoost can be seen as a spe-cial form of the AOSO-LogitBoost since: 1) For each tree, the class pair is fixed for every node in ABC, while it is selected adaptively in AOSO, and 2) K  X  1 trees are added per iteration in ABC (using the same set of probability estimates { p i } N i =1 ), while only one tree is added per iteration by AOSO (and { p i } N i =1 are updated as soon as each tree is added).
 Since two changes are made to ABC-LogitBoost, an immediate question is what happens if we only make one? That is, what happens if one vector tree is added per iteration for a single class pair selected only for the root node and shared by all other tree nodes, as in ABC, but the { p i } N i =1 are updated as soon as a tree is added, as in AOSO. This was tried but unfortunately, degraded performance was observed for this com-bination so the results are not reported here. From the above analysis, we believe the more flexi-ble model (as well as the model updating strategy) in AOSO is what contributes to its improvement over ABC, as seen section 4 ).
 3.2. LogitBoost In the original LogitBoost ( Friedman et al. , 1998 ), the Hessian matrix ( 14 ) is approximated diagonally. In this way, the f in ( 5 ) is expressed by K uncoupled scalar tress: with the gradient and Hessian for computing node val-ue and node split gain given by: Here we use the subscript k for g and h to emphasize the k -th tree is built independently to the other K  X  1 trees ( i.e. , the sum-to-zero constraint is dropped). Al-though this simplifies the mathematics, such an ag-gressive approximation turns out to harm both clas-sification accuracy and convergence rate, as shown in Li X  X  experiments ( Li , 2009 ). In this section we compare AOSO-LogitBoost with ABC-LogitBoost, which was shown to outperform o-riginal LogitBoost in Li X  X  experiments ( Li , 2010a ; 2009 ). We test AOSO on all the datasets used in ( Li , 2010a ; 2009 ), as listed in Table 1 . In the top section are UCI datasets and in the bottom are M-nist datasets with many variations (see ( Li , 2010b ) for detailed descriptions). 3 To exhaust the learning ability of (ABC-)LogitBoost, Li let the boosting stop when either the training converges ( i.e. , the loss ( 2 ) approaches 0, implemented as  X  10  X  16 ) or a maxi-mum number of iterations, M , is reached. Test er-rors at last iteration are simply reported since no ob-vious over-fitting is observed. By default, M = 10000, while for those large datasets ( Covertype290k, Pok-er525k, Pokder275k, Poker150k, Poker100k ) M = 5000 ( Li , 2010a ; 2009 ). We adopt the same cri-teria, except that our maximum iterations M AOSO = ( K  X  1)  X  M ABC , where K is the number of classes. Note that only one tree is added at each iteration in AOSO, while K  X  1 are added in ABC. Thus, this cor-rection compares the same maximum number of trees for both AOSO and ABC.
 The most important tuning parameters in LogitBoost are the number of terminal nodes J , and the shrink-age factor v . In ( Li , 2010a ; 2009 ), Li reported results of (ABC-)LogitBoost for a number of J -v combina-tions. We report the corresponding results for AOSO-LogitBoost for the same combinations. In the follow-ing, we intend to show that for nearly all J -v com-binations, AOSO-LogitBoost has lower classi -cation error and faster convergence rates than ABC-LogitBoost . 4.1. Classi cation Errors Table 2 shows results of various J -v combinations for a representative datasets. Results on more datasets can be found in this paper X  X  extended version ( Sun et al. , 2012 ).
 In Table 3 we summarize the results for all dataset-s. In ( Li , 2010a ), Li reported that ABC-LogitBoost is insensitive to J , v on all datasets except for Pok-er25kT1 and Poker25kT2 . Therefore, Li summa-rized classification errors for ABC simply with J = 20 and v = 0 . 1, except that on Poker25kT1 and Pok-er25kT2 errors are reported by using the other X  X  test set as a validation set. Based on the same criteria we summarize AOSO in the middle panel of Table 3 where the test errors as well as the improvement relative to ABC are given. In the right panel of Table 3 we pro-vide the comparison for the best results achieved over all J -v combinations when the corresponding results for ABC are available in ( Li , 2010a ) or ( Li , 2009 ). We also tested the statistical significance between AOSO and ABC. We assume the classification error rate is subject to some Binomial distribution. Let z denote the number of errors and n the number of tests, then the estimate of error rate  X  p = z/n and its vari-ance is  X  p (1  X   X  p ) /n . Subsequently, we approximate the Binomial distribution by a Gaussian distribution and perform a hypothesis test. The p -values are reported in Table 3 .
 For some problems, we note LogitBoost (both ABC and AOSO) outperforms other state-of-the-art classi-fier such as SVM or Deep Learning. ( e.g. , the test error rate on Poker is 40% for SVM and &lt; 10% for both ABC and AOSO (even lower than ABC); on M-Image it is 16 . 15% for DBN-1, 8 . 54% for ABC and 8 . 33% for AOSO). See this paper X  X  extended ver-sion ( Sun et al. , 2012 ) for details. This shows that the AOSO X  X  improvement over ABC does deserve the efforts. 4.2. Convergence Rate Recall that we stop the boosting procedure if either the maximum number of iterations is reached or it converges (i.e. the loss ( 2 )  X  10  X  16 ). The fewer trees added when boosting stops, the faster the convergence and the lower the time cost for either training or test-ing. We compare AOSO with ABC in terms of the number of trees added when boosting stops for the re-sults of ABC available in ( Li , 2010a ; 2009 ). Note that simply comparing number of boosting iterations is un-fair to AOSO, since at each iteration only one tree is added in AOSO and K  X  1 in ABC.
 Results are shown in Table 4 and Table 5 . Excep-t for when J -v is too small, or particularly difficult datasets where both ABC and AOSO reach maximum iterations, we found that trees needed in AOSO are typically only 50% to 80% of those in ABC.
 Figure 2 shows plots for test classification error vs. iterations in both ABC and AOSO and show that AOSO X  X  test error decreases faster. More plots for AOSO can be found in this paper X  X  extended version ( Sun et al. , 2012 ). We present an improved LogitBoost, namely AOSO-LogitBoost, for multi-class classification. Compared with ABC-LogitBoost, our experiments suggest that our adaptive class pair selection technique results in lower classification error and faster convergence rates. Acknowledgments We appreciate Ping Li X  X  inspiring discussion and gen-erous encouragement. Comments from NIPS2011 and ICML2012 anonymous reviewers helped improve the readability of this paper. This work was support-ed by National Natural Science Foundation of Chi-na (61020106004, 61021063, 61005023), The Nation-al Key Technology R&amp;D Program (2009BAH40B03). NICTA is funded by the Australian Government as represented by the Department of Broadband, Com-munications and the Digital Economy and the ARC through the ICT Centre of Excellence program.
