 A user-centric entity detection system is one in which the primary consumer of the detected entities is a person who can perform actions on the detected entities (e.g. perform a search, view a map, shop, etc.). We contrast this with machine-centric detection systems where the primary con-sumer of the detected entities is a machine. Machine-centric detection systems typically focus on the quantity of detected entities, measured by precision and recall metrics, with the goal of correctly identifying every single entity in a docu-ment.

However, the simple precision/recall scores of machine-centric entity detection systems fail to accurately reflect the quality of detected entities in user-centric systems, where users may not necessarily want to  X  X ee X  every possible en-tity. We posit that not all of the detected entities in a given piece of text are necessarily relevant to the main topic of the text, nor are they necessarily interesting enough to the user to warrant further action. In fact, presenting all of the detected entities to a user may annoy the user to the point where he decides to turn this capability off completely, an undesirable outcome. Therefore, we propose to measure the quality and utility of user-centric entity detection systems in three core dimensions: the accuracy, the interestingness, and the relevance of the entities it presents to the user. We show that leveraging surrounding context can greatly improve the performance of such systems in all three dimensions by em-ploying novel algorithms for generating a concept vector and for finding concept extensions using search query logs. We extensively evaluate the proposed algorithms within Contextual Shortcuts  X  a large-scale user-centric entity de-tection platform  X  using 1,586 entities detected over 1,519 documents. The results confirm the importance of using context within user-centric entity detection systems, and validate the usefulness of the proposed algorithms by show-ing how they improve the overall entity detection quality within Contextual Shortcuts.
 Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. H.0 [ Information Systems ]: General Algorithms Entity Detection, Content Syndication, Contextual, Con-text, Contextual Shortcuts, Information Extraction
The field of entity detection is embedded within the bigger area of natural language processing (NLP) and information extraction. Much work has been done in this area over the past 40 years using various techniques, including rules, dic-tionaries, and machine learning algorithms [13].

In the majority of these cases the implicit consumer of the detected entities was a machine, e.g. an algorithm whose goal was to correctly identify every single entity in a docu-ment regardless of the surrounding context . Thus precision and recall were the natural measures in such systems, with maximum precision and maximum recall being the ideal. We refer to these works as machine-centric entity detection systems.

Within the past 5 years we have seen an emerging trend of systems (e.g., IntelliTXT 1 , Vibrant Media 2 , Kontera Ontok 4 , and our own Contextual Shortcuts ) that not only detect entities or concepts within text (e.g., web pages), but also transform those detected entities or concepts into ac-tionable,  X  X ntelligent hyperlinks X . These hyperlinks provide additional relevant information in a single-click, e.g. detect-ing an address and showing a map to it, detecting a product name and showing an ad to it, or detecting a movie title and showing local show times. We refer to these as user-centric entity detection systems, because their intended target au-dience is a user interacting with the detected, actionable entities or concepts.

When it comes to user-centric entity detection systems, we claim that the simple goals of maximum precision and maximum recall are not optimal since they measure only the quantity but not the quality of the detected entities http://www.intellitext.com http://www.vibrantmedia.com http://www.konterea.com http://www.ontok.com or concepts. Quality matters, because users (people) may not necessarily want to see every possible entity in the text  X  X agged X  (converted into an actionable hyperlink) for the following reasons:
To summarize, the consumer of these detected entities or concepts is a human (a user), therefore, these entities need to be of real interest in order for him to take any action. One of the challenges of user-centric entity detection sys-tems is that users typically focus on a specific task at hand (e.g., reading an article), while the system presents action-able entities embedded in the document. Not only do those detected entities need to be interesting, they must also be interesting enough to divert the user from his current task and investigate further.

Given the above issues we propose to evaluate the quality (utility) of user-centric entity detection systems along three core dimensions: Boundary Correctness (Accuracy): Are the boundaries Relevance: Are the presented entities or concepts relevant Interestingness: Are the presented entities or concepts in-
The above three metrics can be seen in the following text snippet taken from a recent news story: INGTON
Anti-war Democrats in the Senate failed in an attempt to cut off funds for the Iraq war on Wednesday, a lopsided bipartisan vote that masked growing impatience within both political parties over President Bush X  X  handling of the four-year conflict.
 The named entities and concepts in this snippet are: David Espo , Washington , Democrats , Senate , Iraq , Wednesday , and President Bush . First,  X  X avid Espo X  ,  X  X ashington X  , and  X  X ednesday X  are probably of little utility to the user, and thus should not be tagged and converted into actionable links. For example, it would be a distraction to users to see Washington turned into a link leading to a map of the city. Second,  X  X raq X  is a relevant and potentially interesting named entity. However, it is contained in a more relevant concept, namely the  X  X raq war X  , which is contained in an even more relevant and potentially very interesting concept, namely the  X  X unds for the Iraq war X  .

In order to improve the overall user experience, we believe that the goals for a high quality user-centric entity detection system are as follows. First, the system should be able to present new, interesting, and relevant entities and concepts to the user, rather than just static named entities. Sec-ond, to avoid potential under-selection issues, the system should also  X  X xtend X  entities to cover the most specific (and presumable most interesting) concepts, a technique we call concept extension .

Under-selection of entities is not only an issue for rele-vance or interestingness  X  it can also lead to gross errors, making for a poor user experience. To illustrate this further consider the case shown in Figure 1 of a popular entity de-tection and content syndication system. The detected entity was  X  X arland X , a clear case of under-selection from  X  X elly Garland X , further exacerbated by the offer of more informa-tion about  X  X arland, Texas X .

The major contributions of this paper are listed below, and address the aforementioned problems by leveraging con-text :
We distinguish between user-centric and machine-centric entity detection systems, and argue that for the former the simple precision/recall scores fail to accurately reflect the quality of detected entities. Therefore we propose to mea-sure the quality of detected entities within user-centric entity detection systems in the three core dimensions mentioned above.

We analyze the global context of the document to generate a concept vector -a list of entities and concepts in the docu-ment, ranked by their relevance to the rest of the document. Candidate concepts are derived from periodic analyses of (time-varying) search query log data from Yahoo! Search 5 The concept vector is another source of potentially interest-ing and relevant concepts that may not have yet been found by the entity detection process. The ranking of the concept vector is used to filter out the noisy and irrelevant concepts. For example, popular queries like  X  X  Love You X  may or may not be relevant -depending on whether the document is about the  X  X  Love You X  computer virus or whether  X  X  Love You X  is a simple greeting.

We present a novel algorithm for finding concept exten-sions which uses the concepts in the concept vector to ad-dress the problem of under-selection . Candidate concepts in the concept vector are compared against the raw detected entities in search of possible extensions.

We demonstrate our contributions by showcasing them in our  X  X ontextual Shortcuts X  platform, an instance of a user-centric entity detection system (which we describe in detail in Section 4). The Contextual Shortcuts platform bridges the gap between entity detection and user-interaction with the detected entities, in a customizable and scalable frame-work. The platform comprises a series of components for various tasks in the entity detection and presentation pipeline : HTML parsing, tokenization, entity detection (series of specialized entity detectors), context processing, entity fil-tering, and output generation of annotated (tagged) action-able entities in a variety of formats (e.g., HTML, XML, JSON).

In Section 2 we define the terminology used throughout the paper. Unless explicitly stated otherwise, when we refer to entities we typically also mean concepts . The remainder http://search.yahoo.com of the paper discusses the related work in this area, provides a system overview of Contextual Shortcuts, and presents the proposed algorithm for finding concept extensions and gen-erating a concept vector in depth. We then present an exten-sive evaluation of the algorithms within Contextual Short-cuts, and show how they improve the overall quality of de-tected entities within our system. We discuss the results and conclude the paper with a summary and an outlook on future work.
This section briefly defines the terminology we are using in this paper.
We categorize the related work in this area into three categories. First, we discuss machine learning approaches, such as named entity detection and keyword extraction al-gorithms, and point out the differences in our system. Sec-ond, we describe other user-centric entity detection systems. Third, we discuss some recent studies, which also leverage search engine query logs.
There is a significant amount of work in the field of named entity recognition (NER), where the goal is to locate and classify parts of free text into a set of predefined categories. The first named entity set had 7 categories: names of per-sons, organizations, locations, date, time, money and per-cent expressions [10]. However, most recent research has focused on recognizing person, organization and location names, since finding expressions for other categories are shown to be much simpler [16]. The approaches include manually generated regular expressions (see e.g., [1, 2]), and machine learning techniques (see e.g., [5, 6, 4, 3]). In gen-eral, the target consumers of the NER systems are machines, and common applications include question answering, sum-marization, and automatic correction of missing case infor-mation or misspellings in text. However, since our main goal is to identify the most  X  X nteresting X  entities or concepts in the given text, rather than detecting all the entities that fall into predefined categories, our problem is rather different than that of traditional NER systems. Therefore we focus on the quality of the detected entities in our system.
Also related to our work are the keyword extraction al-gorithms where the goal is to find the most distinctive or representative terms in text. These algorithms are typically based on machine learning techniques [8, 21, 12, 11]. How-ever, compared to our user-centric entity detection problem, they are more suitable for applications such as text sum-marization, or automatically retrieving relevant documents based on the extracted keywords.
In early user-centric entity detection applications [17, 15], the user selects a piece of text, and the system X  X  goal is to recognize (meaningful) entities within that text to en-able useful operations on them (e.g., adding events to the calendar, or starting a phone book application for phone numbers). These systems rely on regular expressions for the detection, whereas [17] also uses simple lookup tables. One of the main differences in our approach is that we automat-ically create actionable, intelligent hyperlinks in the text without requiring any interaction (i.e. manual selection of text) by the user. Secondly, we utilize context to achieve a higher quality of detection. Thirdly, we do not limit the sys-tem to simple rules or dictionaries, but rather use periodic a priori analyses of massive query logs to identify the entities that may be interesting to users in the context. This gives our system the capability of capturing real-world interests that are sensitive to time, allowing our system to dynami-cally evolve and not be limited to static rules or manually generated dictionaries.

In the area of targeted advertising, there are commercial offerings from the aforementioned companies such as Intel-liTXT, Vibrant Media, Kontera, and Ontok. These solu-tions focus exclusively on the in-text advertising problem, specifically on finding keywords in a web page that best match an advertiser X  X  ad to the content of the page, and then linking those keywords to mini-ads that lead to the advertiser X  X  main site. Although they are similar in spirit to our work from a system perspective, these solutions only address the advertising use case.
 Another example that falls into this category is Microsoft X  X  SmartTags TM technology available in products such as Out-look XP TM for email. This technology offers a similar inter-action model to ours, using the type of a detected entity to trigger different actions, e.g. address book for peoples X  names and map displays for addresses. However, currently this is only a proprietary technology tightly coupled to Mi-crosoft X  X  desktop products, and the details of the system are not publicly available to external publishers.
The idea of leveraging search query logs has been explored by many researchers to obtain improvements in various ap-plications. The most closely related work to our approach is described in [9]. The goal in this work is to find keywords or phrases (implicit queries) in an email message that, once submitted to a search engine, would yield results relevant to the topic of the message. First, the authors investigate a simple tf*idf based approach as a base system, which is also discussed in [7]. Then the authors show that if the im-plicit queries returned by this base system are restricted to those commonly found in the logs, then results improve sig-nificantly. For this restriction, the authors used the top 7.5 million most common queries in MSN Search, and discarded those generated queries that do not appear in this list. In the rest of their work, the authors also investigated machine learning techniques and used this restriction as a feature. In [20], a similar machine learning approach is employed to identify the best set of advertising keywords in web pages, i.e. keywords in a web page that are most interesting to advertisers. One of the main differences in our work is that we use query logs not as simple constraints as above, but instead utilize them to enrich our dictionaries. In addition, we further utilize query logs for concept extensions, which help avoid the under-selection problem.
The Contextual Shortcuts entity detection platform is an example of a user-centric entity detection system which aims to provide a pluggable architecture for entity detection and content syndication. It is designed to be highly scalable to support the detection of hundreds of millions of entities per day, and has already been successfully deployed on various Yahoo! network properties (e.g., Yahoo! Mail 6 , Yahoo! Per-sonal Finance 7 , Yahoo! Travel 8 , Yahoo! News 9 ). Once entities have been detected (e.g., on a web page) Contextual Shortcuts provides a framework that allows pub-lishers to associate relevant content and services to be dis-played in context at the  X  X oint of inspiration X . For example, show a map for a place, add an email address to an address book, or show related information. The paper focuses on the entity detection aspects of Contextual Shortcuts, not the content syndication or presentation part.

The major components of the platform are shown in Fig-ure 2: Pre-processing: HTML parsing, tokenization, sentence, and Entity Detection: Use of specialized detectors for detect-Post-processing Collision detection between overlapping
Contextual Shortcuts uses context to improve its over-all detection quality. We describe two algorithms that are leveraging context: Generating a Concept Vector: This algorithm extracts Finding Concept Extensions: The concept extensions al-
In below sub sections, we describe the major system com-ponents and the algorithms in detail. http://mail.yahoo.com http://finance.yahoo.com/personal-finance http://travel.yahoo.com http://news.yahoo.com
The pre-processing step expects plain text or HTML in-put, which is parsed by a highly optimized HTML parser and tokenized. The expected encoding format is UTF-8. Finally, as part of tokenization, a sentence and paragraph detection step is performed to aid in the downstream detec-tion of entities. The processed text serves as the input for the set of entity detectors.

It can be seen that this pre-processing step provides a foundation for the accuracy of the detection. For exam-ple, HTML parsing problems, incorrect tokenization, or sen-tence/paragraph boundaries result in a severe degradation of detection quality.
The Contextual Shortcuts platform supports a pluggable architecture, allowing a variety of entity detectors to be em-ployed in the back-end. The goal is to allow the easy inte-gration of additional detectors.

The components for detecting particular types of entities are the detectors . Detectors work on the parsed and/or to-kenized input, which allows each detector to be simple and avoids re-parsing and re-tokenization.

In the current implementation we use a few detectors that are based on regular expressions for entities such as US phone numbers, URLs, and email addresses. The regular expression detectors work solely on the parsed input. The tokenized input is used with detectors such as the location detector (e.g., street addresses, places of interest) and the keyword detector (e.g., people, organizations).

Currently, all entity detectors are run consecutively, with each detector adding its entities to a global hit collection. This hit collection represents the input for the post-processing steps (e.g., ranking, filtering). Each entity returned by any detector has several properties, including the text of the en-tity as it appears in the input, the starting and ending offsets (in characters) of the entity in the given input before parsing, the type of the entity (phone number, street address, per-son, organization, etc.), and the surrounding context. The surrounding context has been extracted to help in finding concept extensions -see Section 4.2.2. The amount of ex-tracted context is configurable. We currently set it to a fixed size window of tokens before and after the entity.
By modularizing the detectors we can easily run differ-ent configurations for different clients by building individual chains of detectors. The platform provides a Web Service API that allows publishers to control various aspects of en-tity detection, such as limiting the number and types of entities detected, as well as display and annotation formats.
Some of the detectors used require additional data to help detect entities. In particular, the keyword detector and the location detector use data-packs that are pre-loaded into memory to allow for high-performance entity detection.
The Contextual Shortcuts platform uses its own taxon-omy of entity types. The current system consists of a hand-ful major types, such as people , organizations , places , events , animals , shopping and products , and various kinds of identi-fiers . Each of these major types contains sub-types as shown for places and identifiers .
Some of the types of entities are provided as editorially re-viewed dictionaries, e.g. people and organizations . For key-word detection, we employ a set of dictionaries that contain categorized terms and phrases according to the taxonomy described above. We currently use 265,000 entities across 120 different types. It is possible that a term can be a mem-ber of multiple types, such as the term jaguar which exists in /products/auto and in /flora_fauna/animals/predators .
We use geo-spatial data to detect locations. This al-lows detection of complete addresses, place names like cities, counties, countries and places of interest. For some regions, where available, the data-pack contains enough information to also provide street address validation, i.e., it can detect whether the house number provided is a valid address. Only those portions of the address that can be properly validated are detected, preventing the over-selection of location enti-ties.

The data-packs are used to associate type information and meta-data to detected entities. In the case of keywords, the meta-data contains type information as listed in the entity taxonomy. In the case of locations, an entity can contain meta-data in the form of geo-location information. For example, it contains a breakdown of the address into its individual components and additional information such as longitude-latitude coordinates and aerial information. Pass-ing such data along is beneficial to any service provider, e.g. for locations it frees the service from having to parse an address string again.

Although there are quality issues related to the tuning of regular expressions these detectors typically achieve very high accuracy and overall interestingness. We are therefore focusing our overall efforts on the quality on keyword-based entities.

To detect things of interest that go beyond editorially re-viewed terms and phrases (e.g., peoples names, artists, com-pany names, etc.), we employ terms and concepts derived from Yahoo! search engine query logs. Below, we first de-scribe our concept vector generation algorithm in detail in Section 4.2.1. We then describe how we use the concepts detected in the concept vector in our concept extensions al-gorithm, which is crucial in dealing with under-selection er-rors.
As mentioned previously, our named entity detection uti-lizes dictionaries that are maintained editorially and a data-pack with geo-spatial information. However, a very impor-tant resource for us are query logs, which allow the system to adapt to the current real-world interests in a timely fashion. In this section we describe how we generate a concept vector from query log data, which enhances the naked dictionary entities with new and interesting concepts.

First, given a document, a term vector is generated with a tf*idf score [19] of each term using a term dictionary which contains the term-document frequencies (i.e. the number of documents of a large web corpus containing the dictionary term). Our corpus in this case consists of all the web docu-ments that are indexed by Yahoo! Search. The stop-words are removed and the remaining terms X  weights are normal-ized so that they are between 0 and 1. The weights of terms that fall under a certain threshold are punished (their tf*idf score is decreased), and the resulting tf*idf scores below another threshold are removed from the term vector.
Second, a unit vector is generated of all the units found in the document. A unit, described in [18, 14], is simply a multi-term entity in the query logs which refers to a single concept. Briefly, units are constructed from query logs in an iterative statistical approach using the frequencies of the distinct queries as follows. In the first iteration, all the single terms that appear in queries are considered to be units. In the following iterations, the units that frequently co-occur in queries are combined into larger candidate units. The validation of these units is performed based on statistical measures, including mutual information, whose formal defi-nition is shown below: where p ( x, y ) is the joint probability distribution function of x and y , and p ( x ) and p ( y ) are the marginal probabil-ity distribution functions. Informally, mutual information compares the probability of observing x and y together as a query with the probabilities of observing x and y inde-pendent queries. If there is an association between x and y , then the score I ( x, y ) will be higher.

For units, mutual information helps to identify those terms that frequently co-occur in user queries. Similar to the term vector scores, unit scores are also normalized so that they are between 0 and 1. Again, the weights of units that fall un-der a certain threshold are punished, and low scoring units are removed.

Finally, the term vector is merged with the unit vector to obtain the concept vector. We have the following possible cases: 1. A term appears in the term vector, but not in the unit 2. A term appears in the unit vector, but not in the term 3. A term appears in both term and unit vector: We add
We then inspect the merged concept vector, and perform the following additional step on the multi-term concepts. To the weight of the multi-term concept calculated in step two, we add both the unit vector and term vector scores of each individual term it contains. Thus, the maximum final concept weight possible is equal to two times the number of terms a multi-term concept contains (this would happen if each one of single terms it contains has unit vector and term vector scores of 1, which is not possible in practice). This way more specific concepts eventually bubble up in the overall rank. As an example, we list top five concepts in the news snippet shown in Section 1 with their concept vector scores: &lt;termvector id="concept"&gt; &lt;item term="david espo" weight="1.4403"&gt; &lt;item term="special correspondent" weight="1.2075"&gt; &lt;item term="iraq war" weight="1.1833"&gt; &lt;item term="president bush" weight="1.1549"&gt; &lt;item term="political parties" weight="0.6147"&gt; ... &lt;/termvector&gt;
All steps and thresholds are configurable by setting vari-ous parameter values within a configuration. For example, there exists a threshold for a weak unit score. These thresh-old values are set empirically.

Note how a concept vector captures both the interesting-ness and relevance of the concepts found in the document. First, since unit vector scores are derived from query logs, they are a proxy for interestingness, i.e. a popular query leads to an  X  X nteresting X  (high unit vector weight) concept. Second, term vector scores are based on the all terms in the document with respect to a corpus, so they are a proxy for relevance, i.e. a  X  X elevant X  term has a high term vector weight. A concept vector is a merge between a unit and a term vector, and thus can be thought of measuring both interestingness and relevance.
We argue that the problem of accurate selection of enti-ties is very important in user-centric entity detection sys-tems. In particular, we would like to avoid concept under-selection problems. In the example shown earlier where we have  X  X etty Crocker X  X  famous cheesecake X  we would like to detect that whole concept, not only fragments like  X  X etty Crocker X . To prevent under-selection we present an algo-rithm for finding concept extensions by leveraging the con-cepts contained in the concept vector (see above).
The algorithm works as follows. For each candidate entity detected, we consider the concepts appearing in the local context surrounding the entity. If the entity is contained in a more specific concept (the entity X  X  text boundaries fall inside the boundaries of the concept), then we implicitly favor the more specific concept. In Figure 3, we illustrate the algorithm.
 Figure 3: Algorithm for finding Concept Extensions
After all detectors are finished, the entity boundaries are inspected and collisions are resolved in cases of overlapping entities. Then a filtering step is performed to remove  X  X n-interesting X  concepts. Finally, the original text, annotated with the set of entities, is output in a variety of possible formats.

Collision Detection: The platform allows simple detec-tors to be tailored towards very narrow and precise detec-tion capabilities. Therefore, it is possible to encounter colli-sions between entities of different detectors. Suppose one of the detectors detects the institution  X  X niversity of Rome X , whereas a location detector returns part of it,  X  X ome X , as a place. We consider returning  X  X ome X  alone as a case of under-selection. To avoid such problems, we favor the more specific hit in cases where one of the entities fully contains another. For other cases (e.g. partial overlap), we employ simple heuristics such as making a decision based on the order of the entities in the text.

Filtering: Since we use query log data as a resource, the detected entities may either be junk (uninteresting random concepts) or may contain certain sensitive (e.g., illegal, con-troversial, or adult) terms. The filtering step consists of removing concepts that fall below a certain concept weight -see Section 5.3 -and also of removing blacklisted terms. Sim-ilarly, stop-words and certain ambiguous terms contained in our data-pack are also filtered out.

Annotation: As a final step the remaining entities and concepts are returned as either a result set in XML or JavaScript Object Notation (JSON), or as annotated HTML. Each entity or concept is returned with a weight, a set of types, a context and (if available) meta-data, such as longitude-latitude coordinates for places. The XML out-put format is particularly suitable for applications that only need access to the detected entities.
We describe the methodology used to evaluate the utility of the Contextual Shortcuts platform along the three afore-mentioned criteria  X  accuracy , interestingness , and rel-evance  X  and present the respective results across a sample corpus of documents. Furthermore, we specifically highlight the roles and contributions of the algorithms mentioned in Sections 4.2.2 and 4.2.1 to the overall results.
Our standard evaluation methodology consists of a team of expert judges rating the entities / concepts detected and presented by Contextual Shortcuts in a set of documents in a corpus. Using an interface specifically designed for this task, the processed set of documents is presented to the judges. A judge is asked to select a document from the pool of docu-ments, and once selected, asked to read the document care-fully prior to issuing any judgments. Each document shows the detected entities (through highlighting), along with the surrounding context. The judge is then asked to rate each entity or concept highlighted in the document in terms of its accuracy, interestingness, and relevance.
Question 1 asks if the detected entity or concept is cor-rectly selected in the text in terms of its logical boundaries. We distinguish primarily between the two error cases of under-selection and over-selection . The judges can select from the following choices: Yes  X  The highlighted entity or concept has been selected No  X  The boundaries of the highlighted entity or concept
Question 2 attempts to measure the degree to which the highlighted entity or concept is interesting, useful, or com-pelling enough to tear the reader away from the main thread of the document. Would the reader take time out from read-ing the document to actually click on the entity or concept and explore it further? The judges can select from the following choices: Interesting or Useful in General  X  It is very likely that Interesting or Useful Only in This Context  X  This en-Definitely Not Interesting or Useful  X  There is no plau-
Keep in mind that an interesting entity or concept may not necessarily be relevant , i.e. central to the gist or topic of the document, or vice versa. In our guidelines the in-terestingness of entities or concepts is independent of their relevance to the meaning of the document.
Question 3 attempts to measure the degree to which the highlighted entity or concept is relevant to the main topic of the document. We can think of this as  X  X ummarizing X  the document with the best set of entities, regardless of their interestingness.
 The judges can select from the following choices: Relevant  X  The entity or concept is core to the overall Somewhat Relevant  X  This entity or concept fits into the Not Relevant  X  This entity or concept is certainly not rel-
Each of the above questions also includes a  X  X an X  X  Tell X  choice for those rare case when a judge can X  X  decide.
We experimented with our concept extension algorithm on a corpus consisting of 1,304 documents, randomly selected from various sources (549 from Yahoo! Answers Q &amp; A X  X  , 353 from the Enron mail corpus 11 , and 403 from Yahoo! news stories 12 ). This corpus yielded a total of 2,305 enti-ties, 376 of which were extensions. The accuracy scores and http://answer.yahoo.com http://www.cs.cmu.edu/  X enron/ http://news.yahoo.com Table 1: Accuracy results for dictionary entities and concept extensions (OK = correct, US/OS = under/over-selection, BT = bad term). The overall accuracy of the system is 95.8%.
 Without Extensions Qty OK US OS BT Dictionary Entities 2305 1840 405 4 15 Table 2: Accuracy results when concept extensions are not used. All entities are now plain dictionary entities. The number of under-selections increases, and the over-selections from table 1 are now correct. The overall accuracy of the system is 81.3%. the effect of the concept extension technique are reflected in Tables 1 and 2.

As Tables 1 and 2 show, concept extensions play a large role in maintaining high accuracy -without them, overall accuracy drops from 95.8% to 81.3%, driven by the num-ber of under-selections. The reason for this is that the 337 extended entities in Table 1, which were judged to be cor-rect, are reduced to naked entities 2, thus becoming under-selections.
We investigated the relationship between the concept weight derived from the concept vector and the interestingness of the concepts in the vector. This was done using a corpus consisting of 352 Yahoo! Finance web pages, which yielded 2,099 entities (concepts) to be judged. Since interestingness is by nature a subjective measure, we needed to obtain mul-tiple editorial judgments for each concept to gauge the level of agreement among editors. This is in fact what happened, and each of the 2,099 concepts received 2 or more judg-ments, with the majority of cases receiving exactly 2 judg-ments. Out of the 2,099 judgments, the editors disagreed on the interestingness rating in 1,060 instances. These 1,060 cases were not considered, leaving 1,039 judgments for fur-ther analysis. Table 3 shows the interestingness breakdown for these 1,039 cases.

The results in Table 3 show that the overall interesting-ness of concepts detected in this study is approximately 76%. However, this is not ideal, and we can improve on this by only considering concepts that meet a certain cutoff value, and filtering out the rest. The results in Table 3 already Rating Quantity Percentage Interesting in general 313 30.1% Interesting in only in context 478 46.0% Not interesting 248 23.9% Table 3: Interestingness scores for detected con-cepts. All concepts have a minimum concept weight of 1.0. reflect a minimum concept weight (cutoff) of 1.0. Figure 4 shows the effect on the overall interestingness and cover-age of concepts as the minimum concept weight is increased past 1.0. Coverage is calculated as the number of concepts remaining at each cutoff value, as a percentage of the total number of concepts at the initial cutoff value.
 Figure 4: Overall interestingness as a function of concept weight cutoff. As the minimum concept weight is increased, the overall interestingness of the remaining concepts increases. The price for the overall increase in interestingness is a sharp decrease in coverage.
Having tuned the above algorithms with respect to con-cept extensions and minimum concept weight, we now put all the pieces together and evaluate the final system with re-spect to the overall accuracy, interestingness, and relevance. This is done on a fresh corpus, similar in flavor to the pre-vious ones, but with new content. These results, shown in Table 4, are based on a set of 1,586 entities, generated from a corpus of 1,519 documents from various sources ( Yahoo! Answers Q &amp; A X  X , the Enron mail corpus, Yahoo! news sto-ries, newsgroup postings, and user product reviews). As Ta-ble 4 shows, the overall combination of concept extensions, concept vector generation, and concept filtering combine to yield an accuracy score of 97.3%, an interestingness score of 94.6%, and a relevance score of 90.1%.
The use of dynamic search query logs to generate new interesting concepts and concept extensions can be of great value. However, given that search query logs contain many random and noisy entries, care must be taken when using them as a source of entities.

Concept extensions are an excellent tool to combat under-selection. There are cases where they intelligently identify extensions, such as extending the naked entity Paris to the concept Paris fashion week in the context  X ...as designer Marc Jacobs took Paris fashion week to its final phase... X . However, they can also miss, e.g. over-selecting the entity Sinn Fein to the concept included Sinn Fein in the context  X ...support at the last assembly election in 2003 because it included Sinn Fein in the Cabinet X . Mechanisms must ex-Table 4: Overall accuracy, interestingness, and rel-evance scores for Contextual Shortcuts. ist to filter out such incorrect selections, and we use the concept weight derived from the concept vector as one such mechanism. However, this is not without a price, since the concept weight by itself is a very  X  X symmetric X  and sensi-tive filter. To borrow a term from economics, Figure 4 shows that concept weight is very elastic with respect to coverage (small changes in weight lead to large changes in coverage), whereas it is very inelastic with respect to interestingness (large changes in concept weight lead to small changes in interestingness). A deeper analysis of the generated con-cepts is required to maintain a high level of interestingness without greatly sacrificing coverage.

Finally, the goal of many user-centric entity detection sys-tems (including Contextual Shortcuts) is to maximize the number of actions taken by users on the presented enti-ties. For example, one such measure is the click through rate (CTR) , defined as the number of clicks (action) divided by the number of entities presented in a given time period. This measure is driven by the interestingness rather than the rel-evance score (see table 4), since interestingness measures a user X  X  propensity to click on an entity. Thus, in the extreme case, one trivial approach to boost interestingness and click-through-rate would be to simply inject some sensational pieces of text somewhere in each document, e.g.  X  X accine for AIDS Found! X , without regard to their relevance. However, this would be no different than spam, quickly leading users to ignore such  X  X ntities X . Thus the second challenge is to boost interestingness without also sacrificing relevance; one can think of the interestingness/relevance metrics of user-centric entity detection systems as the precision/recall met-rics of machine-centric entity detection systems.
This work addresses the quality issues of detected entities in a user-centric entity detection system such as the Con-textual Shortcuts platform. We show how we can leverage local and global context surrounding the entities to improve the overall quality and utility of the system. Specifically, we present two novel algorithms for using context.
First, we use query logs to discover additional concepts in the given document, thus forming a representation of the document as a ranked list of concepts -the concept vector. We then use the ranking of the concepts in the vector (the concept weight) as a measure of both the interestingness and relevance of the concept, and utilize the weight in a filtering mechanism to remove uninteresting and irrelevant concepts. Second, we use the concepts in the concept vector to avoid under-selecting entities in the text.

We argue that the way to measure the overall utility of de-tected entities in user-centric detection systems, where peo-ple can interact with the detected entities, is via the metrics of accuracy, interestingness, and relevance. We present re-sults showing accuract, interestingness, and relevance scores in the 90%+ ranges, derived from an editorial evaluation of the Contextual Shortcuts platform.

As we move forward, we expect to expand our detection capabilities to cover a broader set of entities and concepts. Increasing the coverage of interesting concepts while main-taining high levels of relevance is one of our greater chal-lenges. We hope to address this issue through a deeper analysis of the candidate concepts, potentially building a concept classifier. Disambiguation will also play a larger role in future versions, since determining the correct type (person, place, organization, event, things) of an entity is important to its overall utility.
We are grateful to Farzin Maghoul for his helpful com-ments and suggestions. Furthermore, we would like to thank J  X org Meyer, Josh Coalson, our editorial team, and everyone else here at Yahoo! who helped developing and building the Contextual Shortcuts platform. [1] D. Appelt, J. Hobbs, J. Bear, D. J. Israel, and [2] D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, [3] S. Baluja, V. Mittal, and R. Sukthankar. Applying [4] O. Bender, F. J. Och, and H. Ney. Maximum entropy [5] D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. [6] A. Borthwick, J. Sterling, E. Agichtein, and [7] S. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. [8] E. Frank, G. Paynter, I. Witten, C. Gutwin, and [9] J. Goodman and V. R. Carvalho. Implicit queries for [10] R. Grishman and B. Sundheim. Design of the MUC-6 [11] M. Henzinger, B.-W. Chang, B. Milch, and S. Brin. [12] A. Hulth. Improved automatic keyword extraction [13] P. Jackson and I. Moulinier. Natural Language [14] S. Kapur and D. Joshi. Systems and methods for [15] B. A. Nardi, J. R. Miller, and D. J. Wright. [16] D. Palmer and D. Day. A statistical profile of the [17] M. Pandit and S. Kalbag. The selection recognition [18] J. Parikh and S. Kapur. Unity: relevance feedback [19] G. Salton and C. Buckley. Term weighting approaches [20] W. tau Yih, J. Goodman, and V. R. Carvalho.
 [21] P. Turney. Learning algorithms for keyphrase
