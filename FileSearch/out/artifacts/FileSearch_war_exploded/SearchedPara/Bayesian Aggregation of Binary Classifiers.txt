 Pohang University of Science and Technology
Multiclass classification is an important supervised learn-ing problem, the goal of which is to assign data points to a finite set of  X  classes. The most common approach to multiclass problem is to decompose it into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Reducing multiclass problems to multiple binary problems can be viewed as encoding , in which several methods are widely used, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC) [1]. Aggregation in-volves combining prediction results determined by binary classifiers into a final answer to the multiclass problem. Aggregation methods can be categorized into two types: hard decoding and probabilistic decoding .

In hard decoding, one seeks a codeword which best matches a collection of labels predicted by binary classifiers, in order to determine a proper label for data  X  . Hamming distance is often used as a discrepancy measure between a codeword and predicted labels, in the case where individual binary classifiers yield binary outputs. Furthermore various loss functions (such as exponential loss and logistic loss) are considered in the case where binary classifiers yield a score whose magnitude is a measure of confidence in the prediction [2].

In probabilistic decoding, we are given probability es-timates (scores over [0,1]) computed by binary classifiers. One couples these probability estimates to determine a set of class membership probabilities. In the case of APs, Hastie and Tibshirani [3] developed a method, pairwise coupling , in which pairwise class membership probability estimates are combined to form a joint probability estimates for all classes, fitting the Bradley-Terry model [4] by minimizing a KL-divergence criterion. This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [5], where a generalized Bradley-Terry model [6] was considered to relate probability estimates obtained by binary classi-fiers to class membership probability estimates. Aggregation weights were assigned to individual binary classifiers and were optimally tuned based on observed data [7]. Both aggregation weights and class membership probabilities are treated as parameters, which requires high dimensional op-timization as the number of classes or the number of data points grows. Moreover, the high dimensional optimization often yields an overfitting problem, which causes the degra-dation of overall classification accuracy.

Most of existing methods in probabilistic decoding treat class membership probabilities as parameters, which are estimated using a model which relates them to probability estimates (obtained by binary classifiers). In contrast, we directly model class membership probabilities as a softmax function whose input argument is a linear combination of discrepancies induced by binary classifiers. In this way, aggregation weights are only parameters to be tuned, giving an influence on a joint probability estimates for all  X  classes. Note that our method requires a fixed number of parameters (equals the number of binary classifiers), whereas the number of parameters grows as the size of training set increases, in the existing optimal aggregation method [7]. We formulate the problem of learning classifier aggregation as Bayesian inference. To this end, we consider a lower bound on the softmax function, which is represented as a product of logistic sigmoids. With this lower bound, we show that the problem of learning classifier aggregation is formulated as variational logistic regression, when a hierarchical Gaussian-gamma prior distribution is placed on aggregation weights. Thus we compute predictive probabilities using variational logistic regression, yielding the class membership probabil-ities. The main contribution of this paper is summarized below.
The rest of this paper is organized as follows. Section II provides a brief overview of several encoding methods which decompose the multiclass classification problem into multi-ple binary problems. In Section III, we present our aggrega-tion modeling where we make use of the softmax function to relate class membership probabilities with multiple binary solutions. Section IV provides a Bayesian treatment for aggregating binary classifiers, where we formulate the prob-lem of learning aggregation weights as variational logistic regression. Inference and prediction are also explained and class membership probabilities are determined by predictive probabilities computed by variational logistic regression. Related work is summarized in Section V, in order to emphasize how our model is different from Bradeley-Terry model-based methods and to describe the existing optimal aggregation method. Numerical experiments on synthetic and real-world datasets are presented in Section VI, with comparison to existing methods. Finally conclusions are drawn in Section VII.

We are given  X  training examples { (  X   X  , X   X  ) }  X   X  =1 , where  X   X  are data vectors and  X   X   X  X  1 ,..., X  } (  X   X  3 ) are class labels associated with  X   X  . Multiclass prediction involves estimating the class membership probabilities of  X   X  ,  X   X , X   X  (  X   X  =  X   X   X   X  ) , for  X  =1 ,..., X  , such that argmax  X  is assigned as a proper class label for  X   X  . Data matrix and class label vector are defined as  X  =[  X  1 ,...,  X   X  ] and  X  =[  X  1 , ...,  X   X  ]  X  , respectively.

Multiclass problems are decomposed into a set of binary problems that are solved by individual binary classifiers. Such decomposition can be viewed as encoding and various methods are widely used:  X  A set of  X  binary functions are learned, each of which  X  A set of  X  (  X   X  1) 2 binary classifiers are learned, each of
Aforementioned encoding methods are represented by a coding matrix  X  =[  X   X , X  ]  X   X   X   X   X  where  X  is the number of binary classifiers involved and  X  is the number of class labels. For instance, Table I shows the 3  X  3 coding matrix for 3-class problem in the case of APs coding. Each column in the coding matrix  X  , denoted by  X   X  , corresponds to codeword . According to the coding matrix, multiclass prob-lem is reduced to a set of binary problems that are solved independently. A final answer to the multiclass problem is determined by combining results computed by individual binary classifiers. This can be viewed as a decoding problem.
Given the coding matrix  X  ,the  X  th binary classifier is independently trained using examples { (  X   X  , X   X , X   X  ) } binary values of target are determined by the coding matrix. For instance, in the case of the 2nd binary classifier in Table I, the binary target value for  X   X  is  X  2 , 2 =1 when  X   X  to  X  X lass 2 X  and is  X  2 , 3 =0 if  X   X  belongs to  X  X lass 3 X .
We assume that each binary classifier yields a probability estimate which ranges between 0 and 1. For example, we can use probabilistic SVM [8]. This assumption allows each binary classifier X  X  output to reside in the same dynamic range. Given  X   X  , we denote the probability estimate obtained by the  X  th binary classifier as Binary classifier output matrix  X   X   X   X   X   X  is constructed by collecting the probability estimates given from  X  binary classifiers: where column vector  X   X  represents probability estimates computed by  X  binary classifiers for data point  X   X  .
In this section we present our aggregation model where we make use of the softmax function to relate class membership probabilities with probability estimates determined by binary classifiers. The softmax function takes a linear combination of the discrepancies between codewords and the probability estimates, as input arguments, to represent class membership probabilities. This approach provides a simple model to evaluate class membership probabilities, involving a small number of parameters, compared to Bradley-Terry model-based methods.

In order to combine binary classifiers in the optimal way, we need to evaluate which codeword  X   X  is closest to  X   X  (a collection of prediction results of binary classifiers, given data  X   X  ) in the sense of a pre-specified discrepancy measure. We define this discrepancy  X  (  X   X  ,  X   X  ) as a linear combination of  X  discrepancies induced by binary classifiers: where is the cross-entropy which is KL-divergence between two binary variables, and  X   X  are aggregation weights .  X   X , X  means don X  X  care , that is, we do not care what probability estimate the corresponding binary classifier yields, so we set  X  (  X  , X   X , X  )=0 .

Denote by  X   X   X   X  the aggregation weight vector. Given input  X   X  and the corresponding  X   X  ,wemakeuseofthe softmax function to model the class membership probability: where  X  (  X   X  ,  X   X  ) is given in (2).

We rearrange the class membership probability (4) by multiplying its numerator and denominator by exp {  X  (  X   X  ,  X   X  ) } : where the 2nd equality is derived by taking (2) into account and  X   X , X   X   X   X   X  are  X  -dimensional vectors, the  X  th entry of which is given by Note that the denominator in (5), called sum-of-exponential , is a convex function [9]. This modification make further computations more simple, which will be explained later.
With the relations (5) and (6), we write the likelihood as where  X  (  X ,  X   X  ) is Kronecker delta which equals 1 if  X  and otherwise 0.

The optimal solution for the aggregation weights can be found by a regularized maximum likelihood where the log-likelihood is given by log  X  (  X   X   X  ,  X  )= and Euclidean norm of  X  is considered as a regularizer, with a regularization parameters  X  .

Note that, as we assume that the prior over  X  is Gaus-sian distribution with an isotropic covariance matrix ( 1 / X  corresponds to the variance), the solution is equivalent to a maximum a posterior (MAP) estimator. Since the objective function of (8) is a convex function (due to the convexity of the sum-of-exponential function), the problem can be effi-ciently solved by any existing convex optimization methods. However the point estimation of the aggregation weights can yield overfitting problems and the regularization parameter  X  is not easily tuned.

In order to solve the limitations of the MAP formu-lation, we consider a full Bayesian inference to estimate the aggregation weights. In this approach we are interested in inferring the posterior distribution over the aggregation weights rather than finding their fixed values. In addition, placing a hierarchial Gaussian-gamma prior distribution on the aggregation weights, we can use more flexible prior distribution than in (8): the covariance matrix forms a diagonal matrix whose elements individually represent the variance of each element of the aggregation weights. Also these variables are automatically learned during inference. The next section describes a full Bayesian treatment of this problem in detail.

In this section we first show how Bayesian inference for aggregation weights is formulated as variational lo-gistic regression. Then, we compute variational posterior distribution over aggregation weights, leading to predictive probabilities to determine class membership probabilities using variational logistic regression. To this end, we consider a bound for the softmax function, leading to a lower bound on the the log-likelihood (9).

As in [10], we first consider an upper bound on the sum-of-exponential function: for any  X  =[  X  1 ,..., X   X  ]  X   X   X   X  and any  X   X   X  . Taking this inequality into account, we compute the lower bound on the likelihood that is written as the product of logistic sigmoids: where  X  (  X  )=1 / (1 + exp { X   X  } ) is the logistic sigmoid function.

It follows from (11) that maximizing the lower bound on the likelihood to estimate aggregation weights is nothing but a set of  X  logistic regression problems. With an appropriate prior distribution over aggregation weights  X  , the problem becomes a simple form of Bayesian logistic regression. We place a hierarchical Gaussian-gamma prior distribution on the aggregation weight vector  X  , which is of form where  X   X   X   X  + and  X   X  = diag (  X  1 ,..., X   X  ) and different hyperparameters  X   X  are associated with different aggregation weights  X   X  . Conjugate hyperprior over  X  is given by the product of independent Gamma distributions where  X  0 and  X  0 are shape and scale parameters, respectively.
The pseudocode for our Bayesian aggregation of binary classifiers is summarized in Algorithm IV, which is mainly based on variational logistic regression [11] with prior distri-butions (12) and (13). Inputs to the algorithm are multiclass labels  X  , corresponding coding matrix  X  for a set of binary classifiers, and prediction results  X  produced by  X  binary classifiers. The algorithm yields posterior mean  X   X  and co-variance  X   X  of aggregation weight vector. Given prediction results  X   X  by binary classifiers for a new test data point, the algorithm computes the class membership probabilities  X   X  (  X  -dimensional vector) which consists of  X  (  X   X  =  X   X   X   X  for test data  X   X  for  X  =1 ,..., X  . Inference and prediction follow variational logistic regression, which is explained in the next section and corresponding updating equations are deferred to Appendix. Readers who are familiar with variational logistic regression [11] can skip Section IV-A and IV-B.

One thing to be noted here is a difference between our method and Bayesian multinomial logistic (or probit) regression which is a method for solving the multiclass classification problem directly (without decomposing it into mutiple binary problems). Bayesian multinomial logistic regression also involves the softmax function in its like-lihood, so various bounds for the softmax function have been considered for approximate Bayesian inference [12], [13], [14], [15], [10]. What X  X  similar between our method and Bayesian multinomial logistic regression is to consider a bound for the softmax since both methods are based on logistic regression. However, our method is very distinct in the sense that we make use of variational logistic regression to infer variational posterior distributions over aggregation weights, while Bayesian multinomial logistic regression di-rectly solves the multiclass classification problem, extending logistic regression to multiclass by employ the softmax function. Thus, our method is flexible in the sense that a variety of binary classifiers are allowed to be aggregated, while Bayesian multinomial logistic regression is restricted to logistic regression.
 A. Variational Inference
Variational inference involves calculating the variational posterior distribution over aggregation weights that is rep-resented by the exponential of a quadratic form. To this end, we seek to maximize a lower bound on the log marginal likelihood, introducing a variational distribution  X  (  X  ,  X  )=  X  (  X  )  X  (  X  ) , which is of the form log  X  (  X   X   X  ) Note that  X  (  X  ,  X  ) eventually approximates the posterior distribution  X  (  X  ,  X   X   X  ,  X  ) .

We consider an approximation of the likelihood  X  (  X   X   X  ,  X  ) because it is not conjugate to Gaussian prior. This approximation is done by considering both the lower bound on the softmax (11) and the local variational approx-imation of logistic sigmoid which is parameterized by a variational parameter  X  [11] parameters by  X  = {  X   X   X  } ,for  X  =1 , ...,  X  and  X   X  X  X  . Then, the lower bound, denoted by log  X  (  X  ,  X  ) , which is Algorithm 1 The pseudocode of Bayesian aggregation of a quadratic form, on the log likelihood is given by
Therefore the variational bound  X  (  X ,  X  ) is given by The variational distributions and variational parameters are estimated by maximizing the variational bound (17). Vari-ational distributions  X  (  X  )=  X  (  X   X   X   X  ,  X   X  ) and  X  (  X  )=  X  priors, according to the quadratic approximation in (16). Thus,  X  (  X  ) and  X  (  X  ) are easily updated, which is known to be mean filed approximation [16]. Variational parameters  X  } are easily updated by solving  X   X  (  X ,  X  ) / X   X  =0 for Detailed updating equations for the posterior mean vector  X   X  , the covariance matrix  X   X  , the shape parameter  X   X  , scale parameters {  X   X   X  }  X   X  =1 , and variational parameters are described in Appendix.
 B. Variational Prediction
Prediction involves calculating class membership proba-bilities for a new test data point  X   X  . As the definition of  X   X   X   X   X   X  1 is predictive probabilities computed by trained  X  binary classifiers for the test data point  X  : where  X   X ,  X  is the probability response of the  X  th binary classifier given the test data point  X   X  .

The class membership probability for the test data point  X   X  is nothing but the predictive distribution of the corre-sponding class label  X   X  , which is lower bounded in varia-tional logistic regression, as shown below  X  (  X   X  =  X   X   X   X  ,  X  ,  X  ) where {  X   X , X   X  }  X   X  =1 are computed using  X   X  and (6). Invoking the optimal variation distribution  X  (  X  )=  X  (  X   X   X   X  ,  X  (that is already computed in inference step), the log predic-tive distribution is written as where  X   X  and  X   X  are predictive posterior mean vector and covariance matrix. This lower bound is maximized with respect to  X   X  ,  X   X  , and variational parameters {  X   X   X  Then the resulting maximum is the predictive distribution that is used as the class membership probability. Parameters involving predictive posterior are updated by Variational parameters {  X   X   X  } are updated by These updating equations are easily derived using the same manner as in Section IV-A.

We provide a brief overview of existing probabilistic decoding methods which are mainly based on (generalized) Bradley-Terry models [3], [5], [17], [6], [7], in order to show how our method is different from them. The main difference lies in how to couple probability estimates determined by binary classifiers to a set of class membership probabilities. Let us give more illustration on the generalized Bradley-Terry model. Suppose that true class membership probabil-ities  X   X , X  =  X  (  X   X  =  X   X   X   X  ) are given for  X  =1 ,..., X  Then the generalized Bradley-Terry model relates these class membership probabilities to the true binary class probability  X   X , X  of binary classifier  X  with respect to  X   X  , where  X  1  X  and  X  0  X  are two disjoint classes, defined as
Class membership probabilities  X   X  =[  X  1 , X  , ...,  X   X , X  the training input point  X   X  are not known in advance. Thus, these are estimated by minimizing a weighted KL-where where the weights are usually set to  X   X   X  =  X   X  , where  X  is the number of data point involved in training the  X  th binary classifier. These weights can be learned from data to improve the overall classification performance [7], in which each weight is assumed to represent the confidence level of the corresponding binary classifier.

We now explain the existing optimal aggregation method, referred to as WMAP [7], in which both aggregation weights and class membership probabilities are treated as parameters. To understand its procedure, we first need to define an optimization problem to estimate the class membership prob-abilities, which is also based on the generalized Bradley-Terry models. Actually its objective function is just the function (25) on which a Dirichlet prior is added, i.e., where  X  (  X   X  )= where  X  0 is a hyperparameter which controls the intensity of the Dirichlet prior. Then, to learn  X   X  = {  X   X   X  } from the training data, we define an object function  X   X  which represents the concordance between the class membership probabilities for the  X  th training point,  X   X  , and the class label  X  [7]: where  X   X  ( {  X   X  } , {  X   X  } )=  X   X , X  isabinaryvariabletobe1if  X  =  X   X  , otherwise 0, and  X  is an inverse temperature parameter specified by the user. The function  X   X  depends on  X   X  indirectly thorough {  X   X  in (26) and this relation enables us to calculate the gradient of (29) with respect to  X   X  . However, calculating this gradi-ent involves the computations for estimating {  X   X   X  }  X   X  =1 of which is a solution of (26) for the  X  th training point [7].
Each iteration for solving the problem (28) involves two inner optimizations which are summarized as bellow.  X  Given {  X   X   X  }  X   X  =1 , find  X   X   X  by solving the constrained  X  Given  X   X   X  , find  X   X   X  for  X  =1 , ...,  X  by solving (26). The gradient-based constrained optimization procedure has been provided to solve this problem [7]. However, since the number of parameters increases with the size of training data, i.e.,  X  +  X  X  X  , WMAP might suffer from problems of heavy computational complexity and overfitting.

We evaluate the classification performance of our method on several multiclass datasets, compared to base method and WMAP described in Section V. In the experiments, we consider three encoding schemes, i.e., APs, OVA and ECOC.
The base method stands for a conventional aggregation method that is a collection of three aggregation strategies. For each encoding scheme, there is one corresponding aggregation rule. The decision rule for class label under each encoding scheme is summarized as follows.  X  (APs) Let  X   X   X , X  =  X  (  X   X  in class  X   X   X   X  in class  X   X  (OVA) Assign  X   X  to the class label corresponding to  X  (ECOC) Assign  X   X  to the class label that has a mini-Note that the base method belongs to the hard decoding.
In contrast to the base method, our method and WMAP provide the unified aggregation model which can handle with the coding matrix generated under all encoding schemes. In implementation we need to specify some hyperparameters for both methods. For our method, we set  X  0 and  X  0 in (13) to 10  X  2 and 10  X  4 , respectively. The performance of our method is not sensitive to these parameters. On the other hands, the values of  X  and  X  0 in WMAP (defined in Section V) should be carefully chosen to guarantee the reasonable performance. In the experiment they were manually set, choosing the values yielding the best performance after several trials were made:  X  =10 and  X  0 =0 . 001 .
The datasets used in the experiments are one synthetic dataset, 2 cancer datasets and 8 UCI datasets [18]. With the synthetic dataset, we discuss the robustness of our method against overfitting, compared to WMAP. Using real-world datasets, we compare the classification performance of each method. We use two linear SVM algorithms for the base binary classifier: LibSVM [19] and Liblinear [20]. According to the size of dataset, LibSVM is used for small datasets and Liblinear is used for large datasets. We use a default setting for both SVM toolboxes. The continuous score from the SVM is transformed into the probability (ranging [0 1] ) by using the sigmoid model, such that where  X   X  (  X  ) is a function of the  X  th SVM, and  X ,  X   X   X  are parameters to be estimated [8].
 As mentioned before, we use three encodings (APs, OVA, ECOC) to generate the coding matrix. Especially, in the case of ECOC the size of coding matrix becomes very large for  X   X  8 , if the complete code is adopted. We used the complete code for  X &lt; 8 , yielding  X  =2  X   X  1  X  1 binary classifiers and generating a binary coding matrix without don X  X  care terms (  X  ). For  X   X  8 we generated a spare random coding matrix as in [2], in which  X  =  X  15log 2  X   X  and entries of the coding matrix are chosen as  X  with probability 1 / 2 and 0 or 1 with probability 1 / 4 for each. A. Synthetic Dataset
In this synthetic example we show that our method outperforms WMAP in the case where the number of binary classifiers is growing while the number of training samples is limited. We draw samples from  X  2-dimensional Gaussian distributions, the mean vectors of which are also randomly generated. In each classification, 1300 samples are drawn from each Gaussian, in which 300 samples are used for training and 1000 samples provide the test set.

Fig. 1 shows the classification accuracy averaged over 5 independent runs (each run involves each test set) when the number of classes,  X  , varies from 10 to 23. In this experiment, APs and ECOC are adopted for encoding: for APs encoding,  X  =  X  (  X   X  1) 2 ; for ECOC encoding,  X  =  X  15log 2  X   X  . As shown in Fig. 1, our method (involving only  X  parameters) always outperforms WMAP which involves  X  +  X  X  X  parameters. In WMAP,  X  number of class probabilities are parameters for each sample (total number of samples is  X  ), leading to  X  X  X  parameters, in addition to  X  aggregation weights. Such a huge number of parameters requires high-dimensional optimization, which degrades the performance due to the overfitting, when the number of samples is limited.
 B. Real-World Datasets
We further examine the performance of each method on two cancer datasets and 8 UCI datasets. Two cancer datasets are acute lymphoblastic leukemia (ALL) [21] 1 and global cancer map (GCM) [22] 2 , which were used to evaluate the performance of WMAP [7]. Their detail descriptions are summarized in Table II. In the cases of two cancer and  X  X solet X  datasets, we first reduce the dimensionality of inputs: for two cancer datasets, we select 10 , 000 attributes (genes) as the input features by using a gene ranking based on the ratio of between-group to within group sum of squares; for  X  X solet X  dataset, we applied PCA to reduce the dimension down to 30. All datasets then are pre-processed such that all attributes are normalized to have unit variance, in order for attributes to reside in similar dynamic ranges.
The performance of each method is evaluated using two measure: classification accuracy and squared error (SE) between true probabilities {  X   X ,  X  }  X   X  =1 and estimated class membership probabilities {  X   X   X ,  X  } , such that Since the true probabilities are not available,  X   X ,  X  is defined to be 1 if  X  is in class  X  , and 0 otherwise. This measure only can be applied to probabilistic decoding methods, that is, our method and WMAP.

The base binary classifier is chosen according to the scale of dataset: LibSVM for the datasets { GCM, ALL, glass, yeast, vowel } and Liblinear for other datasets. Each experiment is repeated 10 times by using the 10-fold cross-validation. The mean and standard deviation are reported.
Table III and IV summarize the average classification accuracy and the average SE respectively, for three dif-ferent methods, including the base method, WMAP, and our method. Our method shows the highest classification accuracy across most of cases. In addition, as shown in Table IV, our method also outperforms WMAP in terms of SE.
We have presented the method for Bayesian aggrega-tion of binary classifiers to solve multiclass classification problems. We modeled class membership probability as a softmax function which takes a linear combination of dis-crepancies between the codeword and probability estimates obtained by the binary classifiers, as input argument. We formulated the problem of learning aggregation weights as a Bayesian inference, placing the hierarchical Gaussian-gamma prior on them. The lower bound on the softmax function was written as a product of logistic sigmoid func-tions, where we made use of variational logistic regression for inference and prediction.
 The main contribution of this paper is to formulate the Bayesian aggregation for general encodings including APs, OVA and ECOC. Compared to WMAP, our method is more flexible in the sense that: (1) the softmax function requires much smaller parameters; (2) it is more robust against overfitting due to the simple model assumption and Bayesian treatment. Numerical results confirm these useful properties of our method. The theoretical analysis of the algorithm, e.g., generalization error bound, is important to prove its generalization performance. At this time, we are not able to provide theoretical analysis, which is left as future work.
The variational distributions are updated by the standard variational method for the factorized distributions. The up-date of  X  (  X  ) is given by log  X  (  X  )=log  X  (  X  ,  X  )+  X   X  [log  X  (  X   X   X  )] + constant where The update of  X  (  X  ) is given by log  X  (  X  )=  X   X  [log  X  (  X   X   X  )] + log  X  (  X  )+ constant where  X   X  =  X  0 + 1 2 and  X   X   X  =  X  0 + 1 2 The variational lower bound is given by
 X  (  X ,  X  )= where  X (  X  ) is a gamma function. Each variational parameter  X   X  is recalculated by [11]
