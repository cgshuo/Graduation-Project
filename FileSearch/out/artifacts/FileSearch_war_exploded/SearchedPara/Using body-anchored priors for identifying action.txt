 { This paper deals with the problem of recognizing transitive actions in single images. A transitive action is often described by a transitive verb and involves a number of components, or thematic the type of grasping, and the presence of the action recipient. In many cases, such actions can be readily identified by human observers from only a single image (see figure 1a). We will consider below the problem of static action recognition (SAR for short) from a single image, without using use of motion information for identifying an action (e.g. talking on the phone) may be limited. studies [3, 4, 5, 6] have shown evidence for the presence of SAR related mechanisms in both the ventral and dorsal areas of the visual cortex, and computational modeling of SAR may shed new light on these mechanisms. Unlike the more common task of detecting individual objects such as faces and cars, SAR depends on detecting object configurations. Different actions may involve the same type of objects (eg. person, phone) but appearing in different configurations (answering, individual object recognition.
 Only a few approaches to date have dealt with the SAR problem. [7] studied the recognition of sports actions using the pose of the actor. [8] used scene interpretation in terms of objects and Figure 1: (a) Examples of similar transitive actions identifiable by humans from single images proposed two-stage approach. In the first stage the parts are detected in the face relative log-posterior estimates for the different actions. sailing. [9] recognized static intransitive actions such as walking and jumping based on a human body pose represented by a variant of the HOG descriptor. [10] discriminated between playing and not playing musical instruments using a star-like model. The most detailed static schemes to date as smoking a cigarette, drinking from a cup, eating from a cup with a spoon, talking on the phone, etc., given only a single image as input. The similarity between the body poses in such actions between similar actions in terms of the actor body configuration can be at a fine level of detail. different actions may be very small, occupying only a few pixels in a low resolution image (brush, the background scene, used by [8, 11] to recognize sports actions and events, is uninformative for many transitive actions of interest, and cannot be directly utilized.
 Since SAR is a version of an object recognition problem, a natural question to ask is whether it can be solved by directly applying state-of-the-art techniques of object recognition. As shown in more standard object recognition applications. The proposed method identifies more accurately the the proposed framework significantly improves their results in the SAR domain.
 The main contribution of this paper is an approach, employing the so-called body anchored strat-hand region are analyzed for action related learning and recognition. During training, this allows the automatic discovery and construction of implicit non-parametric models for different important (a) (b) Figure 2: (a) Examples of the computed binary masks (cyan) for searching for elbow location given the detected hand and face marked by a red-green star and magenta rectangle respectively. The yellow square marks the detected elbow; (b) Graphical representation (in plate notation) of the proposed probabilistic model for action recognition (see section 2.2 for details). The rest of the paper is organized as follows. Section 2 describes the proposed approach and its implementation details. Section 3 describes the experimental validation. Summary and discussion are provided in section 4. the face detection is extended to detect the hands and elbows of the person. This is achieved in a non-parametric manner by following chains of features connecting the face to the part of interest and the relative locations of the hand, face and elbow, are used to model and recognize the static additional implementation details are provided in section 2.3. 2.1 Body parts detection Body parts detection in static images is a challenging problem, which has recently been addressed of the body -the lower arms and the hands. This is due to large pose and appearance variability and the small size typical to these parts. In our approach, we have adopted an extension of the non-parametric method for the detection of parts of deformable objects recently proposed by [14]. This method can operate in two modes. The first mode is used for the independent detection of sufficiently large and rigid objects and object parts, such as the face. The second mode allows propagating from some of the parts, which are independently detected, to additional parts, which are more difficult to detect independently, such as hands and elbows. The method extends the so-mode, these feature chains may start anywhere in the image, whereas in the propagation mode these chains must originate from already detected parts. The method employs a non-parametric parts in the independent detection mode). The details of this model are described in [14]. In our marked by three points. The code for the method of [14] was extended to allow restricted detection the hand, as it has less structure. For each (training or test) image elbow detection by a binary mask of possible elbow locations gathered from training images with the sufficiently similar hand-face offset (within 2a shows some examples of the detected faces, hands and elbows together with the elbow masks derived from the detected face-hand offset. 2.2 Modeling and recognition of static actions Given an image the image, upper indices to parts). Denote the instance of the action contained in for training and unknown for test images). Denote the detected locations of the face by by we will express all size and distance parameters in information about the action resides in regions around specific body parts [19]. Here we focus on can be learned and used. We represent the information from the hand region by a set of rectangular 0 . 75  X  s n features centered at all Canny edge points sub-sampled with a patch features extracted from image the SIFT descriptor [20] of the (in m features extracted from image The probabilistic generative model explaining all the gathered data is defined as follows. The ob-served variables of the model are: the face-hand offset offset model are the action label variable patch feature. The meaning of A , while the meaning of A . Throughout the paper we will use a shorthand form of variable assignments, e.g., instead of the data for image
P A, { B m } ,o fh n ,o he n , { f m n } = P ( A )  X  P o fh n ,o he n  X  Here The and we assume it maintains the following relation: representation of the proposed model.
 As shown in the Appendix A, in order to find the action label assignment to posterior of the proposed probabilistic generative model, it is sufficient to compute: As can be seen from eq. 3, and as shown in the Appendix A, the inference is independent of the exact value of the probabilities Figure 3: Examples of similar static transitive action recognition on our 12-actions / 10-people ( X 12/10 X ) dataset. On all examples, the detected face, hand and elbow are shown by cyan circle, shows the estimated log-posterior of the action variable of the action. Additional examples are provided in supplementary material. 2.3 Model probabilities The model probabilities are estimated from the training data using Kernel Density Estimation (KDE) [21]. Assume we are given a set of samples new sample bility of NN ( Y ) samples proximate Nearest Neighbor (ANN) search (using the implementation of [22]) to compute the KDE. To compute in test image set of row vectors: nh query. Recall that scale factor that we use for the offsets in the query. The query returns a set of and the Gaussian KDE with formed by 10 different people, appearing against different natural backgrounds. The second dataset forming 6 general transitive actions. Although originally designed and used by Gupta et al in [11] static instance of the action. Since successive frames are not independent, the experiments con-ducted on both datasets were all performed in a person-leave-one-out manner, meaning that during the training we completely excluded all the frames of the tested person. Section 3.1 provides more details on the relevant parts (face, hand, and elbow) detection in our experiments complementing section 2.1. Sections 3.2 and 3.3 describe the  X 12/10 X  and the Gupta et al datasets in more detail Figure 4: (a) Average static action confusion matrix obtained by leave-one-out cross validation of of each failure there is a successfully recognized instance of an action with which the method has confused. The meaning of the bar-graph is as in figure 3. Additional failure examples are provided in the supplementary material. together with the respective static action recognition experiments performed on them. All exper-iments were performed on grayscale versions of the images. Figures 3 and 6a illustrate the two 4b shows some interesting failures. 3.1 Part detection details an average of in the general setting experiments, when both the person and the background are unseen during part detector training. However, as shown in [14], average in the human-computer interaction applications. Another setting (denoted environment-trained) is same environment. As demonstrated in the methods comparison experiment in section 3.2, it appears parts detection is well below human performance, but the area is now a focus of active research which is likely to reduce this current performance gap. In our experiments we adopted the more dataset (having each person in different environment) and the environment-trained for the Gupta et al. dataset (having all the people in the same general environment).
 In the 12-10 dataset experiments, the part detection models for the face, hand and elbow described in section 2.1, were trained using 10 additional short movies, one for each person, in which the actors randomly moved their hands. On these 10 movies, face, hand and elbow locations were manually marked. The learned models were then applied to detect their respective parts on the 120 movie sequences of our dataset. The hand detection performance was on a subset of frames). Qualitative examples are provided in the supplementary material. The part detection for the Gupta et al. dataset was performed in a person-leave-one-out manner. For each person the parts (face, hand) were detected using models trained on other people. The mean hand detection performance was in both training and test). 3.2 The  X 12/10 X  dataset experiments The  X 12/10 X  dataset consists of 120 videos of 10 people performing 12 similar transitive actions, ing glasses, eating with a spoon, singing to a microphone, and also drinking without a cup and line is the average ROC of the proposed method. (b) Comparing state-of-the-art object recognition methods on the SAR task with and without  X  X ody anchoring X . All backgrounds were natural indoor / outdoor scenes containing both clutter and people passing by. The drinking and toasting actions were performed with 2-4 different tools, and phone talking was performed with mobile and regular phones. Overall, the dataset contains 44,522 frames. Not / puts down a cup). The ground-truth action labels were manually assigned to the relevant frames. The remaining frames were labeled  X  X o-action X . The average recognition accuracy was the 13 actions (including no-action) and Figure 4a shows the confusion matrix for 13 actions averaged over the 10 test people. As mentioned in the introduction, one of the important questions we wish to answer is the need for deformable parts model [23], Bag-of-Words (BoW) SVM ([24]), and our approach described in as provided by the hand and elbow detection). In the second, body anchored setting, the methods were applied to the hand anchored regions (small regions around the detected hand as described in top scores on recent PASCAL-VOC competitions [25], and BoW SVM is a popular method in obtained by the three methods in the two settings. Figure 5a provides ROC-based comparison of the 3.3 Gupta et al dataset experiments ing 6 distinct transitive actions: drinking, spraying, answering the phone, making a call, pouring from a pitcher and lighting a flashlight. In each movie, we manually assigned action labels to all the frames actually containing the action, labeling the remainder of the frames  X  X o-action X . Since the distinction between  X  X aking a call X  and  X  X nswering phone X  was in the presence or absence of talking X  and  X  X ialing X . The action recognition performance was measured using the person-leave-one-out cross-validation, in the same manner as for our dataset. The average accuracy over the 7 static actions (including no-action) was average 7-action confusion matrix is shown in figure 6b. The presented results are for the static for the dynamic action recognition by [11], who obtained for the participating objects (cup, pitcher, flashlight, spray bottle and phone). We have presented a method for recognizing transitive actions from single images. This task is performed naturally and efficiently by humans, but performance by current recognition methods is severely limited. The proposed method can successfully handle both similar transitive actions (the  X 12/10 X  dataset), and general transitive actions (the Gupta et al dataset). The method uses priors that focus on body part anchored features and relations. It has been shown that most common verbs are associated with specific body parts [19]; the actions considered here were all hand-related in this sense. The detection of hands and elbows therefore provided useful priors in terms of regions and properties likely to contribute to the SAR task in this setting. The proposed approach can be ing the body anchored SAR model described in section 2.2. The comparisons show that without using the body anchored priors there is a highly significant drop in SAR performance even when employing state-of-the-art methods for object recognition. The main reasons for this drop are the Here we derive the equivalent form of log-posterior (eq. 3) of the proposed probabilistic action recognition model defined in eq. 1. In 4, the symbol means equivalent in terms of maximizing over the values of the action variable In eq. 4, action (constant for a given image for [1] Jackendoff, R.: Semantic interpretation in generative grammar. The MIT Press (1972) [2] Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning realistic human actions from [3] Iacoboni, M., Mazziotta, J.C.: Mirror neuron system: basic findings and clinical applications. [4] Kim, J., Biederman, I.: Where do objects become scenes? Journal of Vision (2009) [6] Sakata, H., Taira, M., Kusunoki, M., Murata, A., Tanaka, Y., Tsutsui, K.: Neural coding of [7] Wang, Y., Jiang, H., Drew, M.S., nian Li, Z., Mori, G.: Unsupervised discovery of action [8] Li, L., Fei-Fei, L.: What, where and who? classifying events by scene and object recognition. [9] Thurau, C., Hlavac, V.: Pose primitive based human action recognition in videos or still [10] Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing human and [11] Gupta, A., Kembhavi, A., Davis, L.: Observing human-object interactions: Using spatial and [12] Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-object [15] Ferrari, V., Marin, M., Zisserman, A.: Progressive search space reduction for human pose [16] Andriluka, M., Roth, S., Schiele, B.: Pictorial structures revisited: People detection and [18] Ramanan, D., Forsyth, D.A., Barnard, K.: Building models of animals from video. PAMI [19] Maouene, J., Hidaka, S., Smith, L.B.: Body parts and early-learned verbs. Cognitive Science [20] Lowe, D.: Distinctive image features from scale-invariant keypoints. IJCV (2004) [21] Duda, R., Hart, P.: Pattern classification and scene analysis. Wiley (1973) [22] Mount, D., Arya, S.: Ann: A library for approximate nearest neighbor searching. CGC 2nd [23] Felzenszwalb, P., McAllester, D., Ramanan, D.: A discriminatively trained, multiscale, de-[25] Everingham, M., Van Gool, L., Williams, C., Winn, J., Zisserman, A.: The pascal visual ob-
