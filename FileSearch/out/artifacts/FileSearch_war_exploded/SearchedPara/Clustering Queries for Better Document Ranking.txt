 Different queries require different ranking methods. It is however challenging to determine what queries are similar, and how to rank documents for them. In this paper, we propose a new method to cluster queries according to the similarity determined based on URLs in their answers. We then train specific ranking models for each query cluster. In addition, a cluster-specific measure of authority is defined to favor documents from authoritative websites on the cor-responding topics. The proposed approach is tested using data from a search engine. It turns out that our proposed topic-dependent models can significantly improve the search results of eight most popular categories of queries. Categories and Subject Descriptors: I.5.3 [Cluster-ing]: Similarity measures;H.3.3 [Information Search and Re-trieval]: Retrieval models General Terms: Algorithms, Performance, Experimenta-tion Keywords: Query similarity, Clustering, topic-sensitive rank-ing, website authority
Many users prefer finding information from more author-itative websites. This is particularly the case for popular search topics, such as on movies, software, and celebrities, which represent a significant percentage of the whole query traffic on the Web. Therefore, it is crucial to determine what websites are authoritative for a given query. Many efforts have been devoted to improving document ranking for such popular queries. For example, HITS [7] and PageRank [8] are two typical approaches trying to rank more authoritative or popular web pages higher. However, HITS requires a con-siderable amount of online calculation, which makes it less applicable in practice, and PageRank is query-independent.
In this paper, we assume that users with similar intents would consider similar authoritative websites. Our goal is to develop new methods to determine clusters of similar queries for which the same websites are considered authoritative. An authority score for a website is calculated for each cluster of queries, which is then used as an additional feature to train a specific ranking model for the cluster.

The estimation of query similarity is the key issue to the success of this topic-dependent ranking. If query similar-ity is estimated in such a way that reflects their common perception of authority, the above authority feature could correctly boost the authoritative websites in the ranking. Two families of approaches for similarity estimation have been proposed in the literature: using content words and using click-through data. We intend to determine similar queries beyond those that share common words. This is particularly important for popular search queries, which are often titles of games, movies and software that do not share words among them. However, we cannot rely on the ap-proaches proposed in previous studies using click-through data because of data sparsity. Each URL is considered as a single token, but users click on only a few documents in practice. For example, the queries  X  X ackie Chan X  and  X  X et Li X  X ill result in two different sets of URLs, even though they are similar in the sense that both intend to find documents about movies or movie stars. What we can observe from this example is that these queries on similar topics would likely retrieve documents from the same websites specialized in entertainment or movies. So, by considering a similarity between the retrieved URLs, one could determine a better similarity measure between them.

Experimental results show that our proposed approach improves the retrieval effectiveness by around 20% for eight categories of queries. As the topics involved are most fre-quently searched on the Web, the number of queries con-cerned is as much as 60%.

The rest of this paper is organized as follows. First, we review some related work in Section 2. In Section 3, our proposed approach is described. In Section 4, experimental results are reported. Section 5 concludes the work.
Query similarity has been the object of a large number of studies. It has been estimated according to their con-tent words [9], based upon top search results/snippets [10] or query logs [2, 4, 11, 12]. [11, 12] considers that two queries are similar if they lead to clicks on the same doc-uments. Their experiments show that this similarity mea-sure can help boost the content-based similarity measure. Figure 1: Search results returned for  X  X ackie Chan X  and  X  X et Li X  and their common URL tokens Baeza-Yates et al [1] use terms in the clicked URLs to de-termine similar queries. They store the clusters and the k-most popular URLs for each cluster. The above approaches can only establish similarity relationships between a small number of queries. To deal with this problem, in our ap-proach, we do not rely on clickthrough data, but rather we extend it to the top retrieved URLs. Another limitation of the above approaches is the utilization of a URL as a unit. Similar queries are required to lead to clicking on the same URLs. As we mentioned earlier, this rarely happens for similar queries because different URLs will be retrieved for similar but different queries. In our approach, we fur-ther extend this approach by considering a URL as a set of tokens.

Authority is a factor that affects the ranking of docu-ments. PageRank [8] is a widely used method to deter-mine an authority measure for web pages. However, the method only determines a single authority measure regard-less to queries. In reality, the degree of authority of a website or web page depends on topics. To account for this, Haveli-wala[5] proposed to extend the original PageRank algorithm to a topic biased PageRank vectors. Different from [5], we will consider a common authority measure for a group of similar queries, rather than for individual queries. This al-lows us to better account for the fact that similar queries have the same perspective of authority.
In this section, we describe three key aspects of our ap-proach.
The basic idea we exploit in this section is the following: queries are similar if they retrieve similar URLs. Let us examine two such typical queries: Jackie Chan and Jet Li , who are Kung Fu movie stars. These queries are related and can be considered to be similar, to some extent. The top five returned URLs for their Chinese names from a search engine in China are shown in Figure 1.

We can observe that the top URLs for both queries can be from the same website or contain some common tokens such as  X  X tar X  and  X  X nt X . These tokens are meaningful for these URLs. They often correspond to a specific category of documents that are used to organize documents by web-masters. From these tokens, it is possible for one to guess that the documents are about entertainment or stars, so are the two given queries. From this fact, one can deduce that the two queries are related.

Therefore, we propose the following method to calculate a URL-based query similarity. Given a query q i , a search system would return a list of results, each of which is rep-resented by a title, a snippet, and a URL. In this paper, we focus on the URL part. We extract the top k URLs u i 1 , u i 2 , ..., u ik and consider them as a representation of the query as follows:
Each URL u j is then broken into tokens. First, we split the URL by pre-defined separators, such as slashes (/), dots (.), etc. Second, numbers (except for those in website do-mains) and URL stopwords (e.g.  X  X om X  and  X  X ndex X ) are removed. Finally, the URL is represented by l tokens:
Each query q i is then represented by a vector of tokens as follows: where w ij is the weight of token t j in q i , which is cal-culated in a similar way to traditional TF*IDF measures. The difference is that Document Frequency (DF) is counted upon a large corpus of URLs.

The similarity between two queries is then determined us-ing cosine similarity. Based on the similarity, we apply an agglomerative hierarchical clustering algorithm to group the most similar queries and clusters iteratively until the desired number of clusters is obtained.
In this section, we assume that similar queries have simi-lar authoritative websites. Authoritative websites are recog-nized for each topic area. Once we have created clusters of queries, it is then possible to determine such authoritative websites for the whole query set. In this paper we propose using user clicks for the queries in the cluster to calculate a topic-sensitive authority measure for websites, instead of pages. Given a cluster, the most frequently clicked websites are most likely authoritative ones.

Suppose Q c is the set of queries of a cluster c and s i is one of the websites that users have ever clicked. We sum up the clicks on the website s i over all the queries in Q Count c ( s i ), i.e.

Then, we calculate the authority score of s i by normalizing the count as follows:
To a document d in the category c , we will attribute the authority score of its website, i.e.:
Take the celebrity-related queries as an example. The site ent.sina.com.cn is considered as one of the most popular websites in China on celebrities, and its Authorityc is also as high as 1. Thus, given a celebrity-related query, the pages from ent.sina.com.cn will be assigned the highest SiteImpor-tance value, i.e. 1.
The basic idea behind topic-dependent model is the fol-lowing: similar queries require similar ranking strategies. Different categories of queries may correspond to different search strategies. If we train a specific ranking model for each cluster of queries, the learned model for a specific topic may fit the category of queries more than the general model trained on all mixed queries. In addition, the addition of our proposed topic-sensitive authority measure will further improve the topic-dependent ranking model.

As similar queries have been clustered, we propose learn-ing a topic-dependent ranking model. First, we select the topics that have the potential to have enough data for learn-ing the models offline and influence as many queries as pos-sible online. Then, given the topics, we learn query classi-fiers that will be used to identify the queries on the same topic, and thus topic-dependent models can be learned in training phase and be applied in testing phase. Next, the classifier of a query category is used to identify the train-ing/validation/testing queries of this category to form the datasets for learning a topic-dependent model. Any learn-ing to rank algorithm can be used to train a ranking model for each category of queries. In this paper, we choose to use RankNet[3]. For each query-document pair, we extract a vector of M general features, based on which we can train a general model if all queries are used. In contrast, if only the queries of a particular category are used, the model is a topic-dependent model. We can apply the model for the particular category in testing. Finally, we add the topic-sensitive SiteImportance feature to the M general features. Based on the M+1 features, we can learn a topic-dependent model with topic-sensitive features by applying RankNet. The model is supposed to be more powerful according to our assumptions.
In this section, we conduct two experiments to validate our hypotheses.
In the ranking experiments, we use data obtained from a commercial search engine. The dataset contains 30,000 training queries, 4,000 validation queries and 4,000 testing queries. For queries in the ranking experiment, we have five levels manual relevance judgments for the retrieved docu-ments. The evaluation metrics we used is NDCG[6] based on human relevance judgments. In addition, we extract about 70 general features, including BM25 scores, exact-match scores in anchor text, titles, or URLs, normalized click features mined from logs, etc.
To select topics for topic-dependent ranking experiment, we have conducted the clustering algorithm on 5000 the most frequent queries sampled from the logs of a large commer-cial search engine in December 2007. The clustering algo-Table 1: Clusters in the China Market in Dec. 2007 Online Games World of Warcraft, Audition Online, ... Mini Games Mini game, Fantastic Wonderland, ...
 rithm is described in Section 3.1. We cluster queries into 400 clusters, but will only select ten largest clusters to do our ranking experiment because of two reasons: 1) Learning a ranking model for a specific cluster needs enough train-ing data, and only large clusters can meet the requirements. 2) The more popular the clusters are, the more impact the specific models make for user experiences. The ten largest clusters are shown in Table 1. In the ranking experiments, a classifier which trained on the ten clusters is used to select topic-dependent models for each query.

We compare retrieval performance of three methods: 1) the general model (Baseline), which uses a single ranking model trained on all the queries; 2) the topic-dependent models (T-Model) trained for each query cluster with the general features; 3) the topic-dependent models with the topic sensitive SiteImportance score (T-M+SI) as an addi-tional feature. The last method integrates all the aspects we propose in this paper. The detailed results of validation dataset are shown in Table 2.

We have conducted t-tests to validate whether the im-provement is statistically significant. In the table, * means the improvement is significant (with p-value &lt; 0.05) over the baseline, and ** means the improvement is significant (with p-value &lt; 0.05) over the T-Model method.
 Table 2: Retrieval performance on the category of Software (similar to other seven categories of topics) in the validation set
T-Model (+7.77%) (+5.54%) * (+4.87%) * T-M+SI (+18.9%) * (+17.70%) ** (+20.27%) **
The software category (see Table 2) is a typical one for which our proposed approach can bring improvements. First, when a dedicated model is trained for this category (T-Model), all NDCGs are improved. Second, when the SiteIm-portance feature is used, we can obtain some further im-provements. Third, the T-M-SI model is even significantly better than the topic-dependent model based on the general features (T-Model) in terms of NDCG@3 and NDCG@5. This confirms the effectiveness of our proposed approach.
Similar results are observed on seven other topics: TV , movies , mini games , online games , celebrities , finance and literature . These eight categories of topics form Group 1. Table 3: Query coverage and average improvements of retrieval performance on the test set Topics Improvement C C freq Group 1 (+19.52%) (+20.60%) Others 0 0 83.25% 40.23% Overall +0.0167* +0.0172* 16.75% 59.87%
Opposite results are observed on the category of education and music . Both the T-Model and the T-M+SI model per-form worse than the baseline. The T-Model drops 1.8% in NDCG@1 and the T-M+SI model drops 0.8%. A detailed analysis on the validation set reveals two possible reasons:
First, many queries related to universities are clustered in this category due to the common tokens such as  X  X du X  or  X  X taff X . However, queries in this category are very diverse -they may concern universities, research work, people work-ing in universities, and so on. These queries do not share a common search strategy.

Second, the addition of SiteImportance feature does not help either. This is because no authoritative site exists for this category.

In the testing experiment, we only apply our proposed topic-dependent models to Group 1. Table 3 summarizes the results on the test set. We also show the percentage of the unique queries affected by the models, denoted by C (coverage), as well as that for the whole set of user queries (with frequency) ( C freq ) in the test dataset. Another ad-vantage of our topic-sensitive ranking strategy is that we are able to find out the categories of topics which are im-proved/decreased. As a result, our model could only work for the categories of queries that are suitable.
This experiment compares topic-sensitive SiteImportance with PageRank. We compare three models: 1) the model with PageRank added to the general features, denoted by PR; 2) the model with the topic-sensitive SiteImportance added, denoted by SI; 3) the model with both PageRank and topic-sensitive SiteImportance added, denoted by PR+SI. We conduct the experiments on the eight topics of Group 1. The results are evaluated by NDCG@3. Figure 2 shows the performance for each category of Group 1 as well as the average performance.

As Figure 2 shows, for most of the topic categories, the model with SiteImportance performs much better than the model with PageRank. The average improvement is about 9%. This indicates that although our topic-sensitive SiteIm-portance is based on a simple voting method, it is effective to enhance topic-dependent ranking. In addition, when our SiteImportance feature and PageRank score are combined, we cannot obtain better search results in average. The re-trieval performance even decreases on the category relating to movie , online game , software and finance . This indicates that PageRank cannot provide much complementary infor-mation to our proposed method. Figure 2: Comparing PageRank with that of topic sensitive SiteImportance in terms of NDCG@3
In this paper, we proposed an approach to determine query similarity by considering the similarity between URLs re-trieved for them. Similar queries are clustered and a specific ranking model is trained for each cluster. Furthermore, we proposed a way to determine authority measure of websites for each cluster. Our experiments showed that adding the authority measure as a feature in training a topic-dependent model has turned out to be effective to enhance the retrieval for eight categories. As the coverage of the queries concerned is quite representative, we observed overall significant im-provement.
