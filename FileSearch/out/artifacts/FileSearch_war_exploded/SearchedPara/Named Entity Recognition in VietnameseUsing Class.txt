 PHAM THI XUAN THAO, TRAN QUOC TRI, DINH DIEN University of Natural Sciences, VNU of HCMC and NIGEL COLLIER National Institute of Informatics 1. INTRODUCTION Named entity recognition (NER) is a subtask of information e xtraction (IE) that classifies words in a document into some predefined targe t entity classes including persons , organizations , locations , number expressions , temporal ex-pressions, monetary values , and percentage as well as  X  X one-of-the-above. X  Re-cent state-of-the-art approaches to named entity recognit ion have successfully employed voting as a strategy to combine several classifiers [Sang 2002; Wu et al. 2003; Florian et al. 2003]. There are several voting st rategies, including system combination and a system-internal combination that applies the same algorithm to variants in the data representation.
 knowledge these few have applied a single algorithm to recog nize named en-tities such as Conditional Random Fields [Nguyen et al. 2005 ] and Support Vector Machine [Tran et al. 2007]. In this study, we explore s everal strate-gies for voting by combining those state-of-the-art learne rs using a word-based approach. Furthermore, we have also looked at the effect of r epresentation strategy on learner performance. Previous studies on chunk ing for English and NER for Dutch [Sang 2002] showed that there was a small but notice-able effect, and we wanted to see whether this would be benefic ial for the case of Vietnamese NER. Given also that Vietnamese is a head first l anguage we wanted to explore the effect of left-to-right and right-to-left classifiers as well as their combination in the voting framework.
 infectious-disease tracking to the public X  X  attention. In order to assist in the detection and tracking of diseases, it is becoming increasi ngly necessary to have language processing tools that can mine information fr om the Web in lo-cal languages. Given that this infrastructure is currently lacking, we have been participating in building such a system in four languages (E nglish, Japanese, Thai, and Vietnamese). In this article we report on our exper ience for Viet-namese named entity recognition, which is a key component in our mining system. 1 Vietnamese that have decisively affected our approach. Dat a representations and voting mechanisms are also displayed in this section. Se ction 3 presents experimental results. Our discussions are included in Sect ion 3. Finally, the conclusion and future work appear in Section 4. 2. BACKGROUND 2.1 Related Work A named entity (NE) is a word, or sequence of words, that can be classified as a name of a person, organization, location, etc. Named entit ies can be valuable in several natural-language applications such as automati c text summariza-tion systems, fields of information retrieval, information extraction, question answering, and machine translation.
 eral articles related to the recognition of general named en tities. While date and numeric expressions can be easily detected by automated means, the recognition of three types of NE such as locations, organiza tions, and persons is more complicated and might need the use of natural-langua ge processing techniques. Given their importance, it is clear why some art icles have concen-trated only on the recognition of these three types [Cheng et al. 2006; Zhang and Johnson 2003]. In recent years, there has been growing in terest in entity identification in the molecular biology, bioinformatics, a nd medical natural-language processing communities. The most commonly studie d in those domains have been names of genes and gene products [Yi et al. 2 004; Song et al. 2004].

Systems for NER can be built based on handcrafted rules or on m achine-learning algorithms [Takeuchi and Collier 2002; Cheng et al . 2006] or a com-bination of both [Fang and Sheng 2002]. Voting techniques we re recently con-sidered and applied in chunking [Sang 2000; Shen and Sarkar 2 005]. A few articles on NER tried to use this mechanism and also attained high perfor-mance [Wu et al. 2003; Florian et al. 2003].

Sang [2002] reported that he achieved higher accuracy by app lying a memory-based learner to distinct data representations. Wu et al. [2003] and Florian et al. [2003] reported results by combining diverse classifiers. We start by making each of six classifiers with two directions (forwar d and backward) including support vector machines (SVM) [Kudo and Matsumot o 2001], condi-tional random fields (CRF) [Lafferty et al. 2001], transfoma tion-based learning (TBL) [Brill 1995], Na  X   X ve Bayes and C4.5, process the training data set. We then convert the outputs of these classifiers into open and cl ose brackets and combine them.

Florian et al. [2003] introduced three strategies to combin e various systems called voting techniques. We experimented with these three weighted voting schemes for the open and close brackets and then made pairs of these brackets. Munoz et al. [1999] suggested that a method to choose these pa irs is getting pairs with the highest weight while Sang [2000] and Sang and V eenstra [1999] chose only the shortest phrases between open and close brack ets. Our proce-dure is similar to the first method, except that a weight of eac h tag is the total weight after voting. 2.2 Characteristics of Vietnamese 2.2.1 Words and Morphosyllables. In Vietnamese, the boundaries between words are not always spaces as those in English, and words are usually com-posed of special linguistic units called morphosyllable . A morphosyllable may be a morpheme, or a word, or neither of them [Tran et al. 2007]. For example, there are ten morphosyllables.

In a dictionary, these ten morphosyllables are ten words wit h their own meanings. But in this sentence, some of them are only morphem es. There are many different ways to perform word segmentation, but only t wo of them are grammatically correct, and one of them (#1) is more reasonab le in terms of its c . u  X ,  X  X   X  o  X  i X ,  X  X  `  X nh h `  X nh X ,  X  X i X e . n nay X ).

An NE can consist of either a word, for example,  X  X h  X  u  X  ba X  (Tuesday) or a phrase, for example  X  X  X o . Y t  X   X  e X  (Ministry of Health), and word segmentation is based on the context. So words give more meanings than morp hosyllables and contribute to the easier identification of an NE. The effe ct of a word-based model is shown in the previous study [Tran et al. 2007]. This t ime, therefore, we chose to look at the effects of the word-based approach. come after other words, a Vietnamese word is formed by a head n oun standing before morphosyllables. For example, when indicating a sch ool, in English, the head noun is standing after the name of the school, for exa mple  X  X e Hong Phong high school, X  in which  X  X igh school X  is a head noun and  X  Le Hong Phong X  is a name of that school; in Vietnamese, the head noun  X  X ru  X  ` o  X  ng X  is standing in example is the use of  X  X e X  which indicates a type of vehicle as shown in the examples below: c ` is not necessary. 2.3 Data Representation An NE can be represented with bracket structures, including the open bracket representation (so-called O tag) and the close bracket repr esentation (so-called C tag) that define whether morphosyllables start an NE or not ( open bracket), or whether morphosyllables are at the end of an NE or not (clos e bracket). Alternatively, one can use a tagging representation as IOB n otation.  X  X : Current morphosyllable is inside an NE  X  X : Current morphosyllable is outside an NE  X  X : Current morphosyllable is at an NE boundary There are four variants of this representation.  X  X OB1: Tag B is assigned to the first morphosyllable inside an NE immedi-ately following another NE that belongs to the same class.  X  X OB2: All NE-initial morphosyllables receive a B tag.
  X  X OE1: IOE1 differs from IOB1 in that instead of tag B, a tag E i s used for the final morphosyllable inside an NE immediately preceding ano ther NE that belongs to the same class.  X  X OE2: IOE2 is a variant of IOE1 in which all NE-final morphosy llables re-ceive an E tag.
 resentations; however, each representation differently d isplays an NE. For ex-ample,  X  X rung Qu  X   X  oc X  is an NE of location and displayed as  X  X -LOC I-LOC X  by both IOB1 and IOE1 in which  X  X OC X  is symbolized for LOCATION a nd tag I is assigned to this entity since the entity before and after i t in turn does not belong to the same class. For IOB2, that NE is tagged as  X  X -LOC I-LOC X  as  X  X rung X  is the beginning of the entity and  X  X u  X   X  oc X  is inside the entity. To the contrary,  X  X -LOC E-LOC X  is tagged for IOE2 since  X  X rung X  is i nside the entity and  X  X u  X   X  oc X  is at the end of the entity.
 most chosen; however, a suitable choice of a data representa tion according to each language and model can yield improved results. If we onl y use one classi-fier with one data representation, we cannot do voting since v oting requires at least three outputs so that we can choose which one appears mo st often, and di-verse data representations can be considered as a good solut ion to help provide more outputs.
 and Sarkar 2005]. In this article, we want to see if the repres entation could improve accuracy for a single model and explore the question of whether these different representations can provide information t hat can be exploited using voting.
 2.4 Algorithms This section describes only briefly the classifiers used in co mbination in Section 2.5; a full description of the algorithms and their properti es is beyond the scope of this article since they are presented elsewhere. 2.4.1 Support Vector Machines. Support vector machines (SVMs) were in-troduced by Vapnik [1995]. They provide a new classification method based on the structural risk minimization principle from statist ical learning theory. The idea is simply to maximize the margin between the classes .

Figure 1 shows a linearly separable case. The decision hyper plane defined by g(x)=0 separates positive and negative examples by the la rgest margin. The solid line indicates the decision hyperplane, and two pa rallel dotted lines indicate the margin between positive and negative examples .

We can change the goal of finding the largest margin into findin g a decision function f(x)=sign(g(x)) where The vector w points perpendicular to the separating hyperpl ane. Offset param-eter b helps to increase the margin. z i s are called support vectors and are rep-resentatives of training examples. l is the number of support vectors. K(x,z) is a kernel that implicitly maps vectors into a higher dimens ional space. We can use various kernels, and the design of an appropriate ker nel for a par-ticular application is an important research issue. The fol lowing are some available kernels:  X  X inear kernel : K ( x , y ) = ( xy ) d  X  X olynomial kernel : K ( x , y ) = ( xy + 1) d  X  X BF (Radial Basis Function) kernel : K ( x , y ) = exp(  X  | x  X  y | 2  X  X igmoid kernel : K ( x , y ) = tanh(  X  ( xy ) + c ) There are several SVM-based NER models. Lee et al. [2003] pro posed a two-phase recognition model and Yamamoto et al. [2003] proposed an SVM-based recognition method that uses various morphological inform ation and input fea-tures such as base noun phrase information, the stemmed form of a word, etc.
We used the SVM classifier implemented in the YamCha 2 for our experiments. 2.4.2 Conditional Random Fields. A conditional random field (CRF) is a type of discriminative probabilistic model most often used for the labeling or parsing of sequential data that was introduced by Lafferty e t al. [2001].
The probability of a particular label sequence y given obser vation sequence x is to be a normalized product of potential functions, each o f the form a state feature function of the label at position i and the obs ervation sequence; and  X  j and  X  k are parameters to be estimated from training data.
When defining feature functions, we construct a set of real-v alued features b(x, i) of the observation to express some characteristics o f the empirical dis-tribution of the training data that should also hold for the m odel distribution. Each feature function takes on the value of one of the real-va lued observation features b(x, i) if the current state (in the case of a state fu nction) or previ-ous and current states (in the case of a transition function) take on particular values. All feature functions are therefore real-valued.

Cheng et al. [2006] used CRF to recognize Chinese named entit ies with basic features, word boundaries features, and char features.

We used CRF++ toolkit 3 to implement the CRF model. 2.4.3 Transformation-Based Learning. Transformation-based learning (TBL) is a machine-learning technique, first introduced by E ric Brill [1995], and mainly based on the idea of successively transforming th e data to correct the error that gives the biggest win in terms of error rate. Th e transformation rules obtained are usually few, yet meaningful.

Figure 2 illustrates how transformation-based error-driv en learning applies. First, unannotated text is passed through an initi al-state annota-tor; it is then compared to the truth. A manually annotated co rpus is used as a reference for truth. An ordered list of transformations is learned that can be applied to the output of the initial-state annotator t o make it better resemble the truth. There are two components to a transforma tion: a rewrite rule and a triggering environment. At each iteration of lear ning, the trans-formation is found whose application results in the best sco re according to the objective function being used; that transformation is then added to the ordered transformation list and the training corpus is updated by ap plying the learned transformation. Learning continues until no transformati on can be found whose application results in an improvement to the annotate d corpus. Telecom R&amp;D Beijing. In our study, we applied fnTBL toolkit 4 to train TBL. sifier broadly applied in the machine-learning field. Maron [ 1961] first applied this classifier to the classification.
 where: P(X | Y): the number of elements of X set that is available in Y set. P(Y | X): a number of elements of Y set that belong to X set.
 P(X): the number of elements in X set.
 P(Y): a number of elements in Y set.

The implementation of Na  X   X ve Bayes in the Weka 5 toolkit was used for these experiments. 2.4.5 C4.5. C4.5 is an algorithm introduced by Quinlan [1993] for induci ng classification models, also called decision trees, from dat a. C4.5 is an extension of ID3 that accounts for unavailable values, continuous att ribute value ranges, pruning of decision trees, rule derivation, and so on.
Let us assume a set of records, each of which has the same struc ture consist-ing of a number of attribute/value pairs. One of these attrib utes represents the category of the record. The problem is to determine a decisio n tree that on the basis of answers to questions about the noncategory attribu tes predicts cor-rectly the value of the category attribute. Usually the cate gory attribute takes any case, one of its values will mean failure.
 the information conveyed by this distribution, also called the entropy of P, is If a set T of records is partitioned into disjoint exhaustive classes C 1 , C 2 , .., C k on the basis of the value of the categorical attribute, then t he information needed to identify the class of an element of T is Info(T) = I(P ), where P is the probability distribution of the partition (C 1 , C 2 , ..., C k ): If we first partition T on the basis of the value of a noncategor ical attribute an element of T becomes the weighted average of the informati on needed to identify the class of an element of T i , that is the weighted average of Info(T i ): Consider the quantity Gain(X, T) defined as This represents the difference between the information nee ded to identify an element of T and the information needed to identify an elemen t of T after the value of attribute X has been obtained, that is, this is the ga in in information due to attribute X.

We can use this notion of gain to rank attributes and to build d ecision trees where at each node is located the attribute with greatest gai n among the at-tributes not yet considered in the path from the root.

We used the Weka 6 toolkit to implement C4.5 in our experiments. 2.5 The Data Most machine-learning algorithms require a large corpus to ensure a high de-gree at accuracy on unseen data. Constructing such a corpus b y hand, how-ever, takes lots of time as well as human labor, so we have cons idered that it is necessary to build a tool for supporting corpus creation.

There are several online newspapers in Vietnam; among them, Tuoi Tre 7 and VnExpress 8 are two of the most popular newspapers that cover various topic fields. We mainly extracted thousands of raw articles f rom these two newspapers. We then picked 500 articles in seven fields inclu ding society, health, entertainment, sports, politics, business, and sc ience technology to per-form word segmentation and NE annotation. The process of NE a nnotation is briefly described as follows. First, 100 articles were annot ated by hand. To help speed up the manual annotation, we constructed an NE ann otation tool. We trained an SVM model based on the human-annotated article s and used this model to bootstrap the remaining word-segmented artic les. Articles auto-matically annotated by SVM model were corrected by hand. Whe n completed, our corpus comprised 500 NE annotated articles.
 class has 1,356 entities, the location class has 4,948 entit ies, and so on. The detailed number of named entities in each class is shown in Ta ble III. tities are classified into seven target entity classes: PERS ON, LOCATION, ORGANIZATION, CURRENCY, NUMBER, PERCENT, and TIME. Table I V displays some annotated sentences in our corpus which are fo rmatted in XML. The right column includes sentences in Vietnamese and their correlatively English meaning is in the left column. 2.6 Voting Mechanisms With the same data, different machine learning systems will make different errors. The combined results of these systems is usually bet ter than that of any of the participating system. We can make a combination by assigning some weights to the results of the individual classifier and the fin al possible output is the one with the highest total score. For example, if three classifiers have weights 0.5, 0.2, and 0.3 respectively and they classify the word  X  X ietnam X  as LOCATION, ORGANIZATION, and LOCATION, then the combinatio n method will pick LOCATION since it has a higher total score than ORGA NIZATION. The values of the weights are usually estimated by processin g a part of the training data, the tuning data, which has been kept separate as training data for the combination process. Consequently, voting itself i s a classifier which may be trained.

In this article, we tried three combination methods. The firs t combination method (so-called majority voting ) assigns the same weight to the results of the individual classifiers, and the candidate with the most v otes is chosen as the final result for each position. The second method regards as the weight of each individual classification algorithm its accuracy on some part of the data ( TotPrecision ). The final voting method computes the precision of each assigned tag per classifier and uses this value as a weight for the classifier in those cases in which it chooses the tag ( TagPrecision ). We experimented with these three weighted voting schemes for the open and close br ackets and then made pairs of these brackets by getting pairs with the highes t weight. The weight of each tag is the total weight after voting. 3. EXPERIMENTAL RESULTS AND DISCUSSION There are three different possible processing strategies f or finding an NE in text. The first one predicts NE boundaries and NE classes simu ltaneously. An-other one annotates boundaries first and classes later. The l ast one builds a separate recognizer for each different class. We chose the fi rst processing strat-egy, in which the process of recognition and classification o f an NE happens at the same time.

In this study, we employed a word-based approach as in a previ ous study [Tran et al. 2007]. However, to avoid errors caused by word se gmentation, we converted words into morphosyllables after assigning PO S (part-of-speech) tags. Orthographic features, such as InitCap, AllCaps, etc ., and the gazetteer feature are basic input features in our experiments. These f eatures are com-bined in turn to make diverse sets, and all the classifiers in t his research ac-cess the best combination of all features including morphos yllables, POS tags, orthographic, and gazetteer features. We call this our morp hosyllable-based approach to distinguish it from the approach in the research of Tran et al. [2007]. We used the IOE2 representation in our system.

The algorithm X  X  performance is measured with two standard s cores: preci-sion (P) and recall (R). Precision measures the number of cor rect named enti-ties out of all the named entities that were found by the algor ithm, and recall is the number of retrieved entities out of all the entities in the corpus. The two rates can be combined in one measure as their harmonic mean F= 2PR/( P + R ).
We applied several individual classifiers to each of four dat a representa-tions, and IOE2 appears to be the more effective with most of t he classifiers such as SVM forward, TBL, Na  X   X ve Bayes, and C4.5 while IOB2 is suitable for CRF, and SVM backward performs IOE1 better (see Table V). The result after applying a majority vote to SVM forward (SVMF), SVM backward (SVMB) and CRF also proves the effect of IOE2. We believe that our system will yield the best performance with IOE2 representation.
 racy than SVMB (parsing from right to left), which might be du e to its suit-ability to the data that are in Vietnamese in which head nouns always stand in front, which is contrary to commonly studied languages in which the head is final as in English.
 Table V shows; however, it takes much more time to train than S VM and only gets slightly better than SVMF for IOE2. SVMF is more economi cal and suit-able for our approach.
 converted the outputs into open and close brackets. Of cours e, bracket repre-sentations themselves give better performances than taggi ng representations so we hope the combination of bracket representations can al so yield high re-sults. We then applied the voting mechanism displayed in sec tion 2.1 to all possible combinations. Some performed better than individ ual classifiers, but some did not. Table VI shows the results of a few system combin ations. In most voting cases, TagPrecision usually performs better th an majority voting and TotPrecision, but it seems quite different in our study. That is, this vot-ing technique can be well performed for a combination of thes e systems but not for others, and vice versa. We can see that a combination o f SVM (includ-ing SVM backward and SVM forward), CRF, and TBL shows the best perfor-mance in majority voting as well as in TagPrecision. Otherwi se, in the case of TotPrecision, we receive the best result when combining S VM backward, SVM forward, and CRF. In general, a system combined by SVM for ward, SVM backward, CRF, and TBL gives the highest result of both open a nd close brack-ets. However, open bracket achieves such a good result throu gh majority voting while the best result of close bracket is gained by applying T agPrecision. on the combination technique in Section 2.1. The resulting p erformances are significantly better than any of the six individual systems. We also carry out straightforward votes of the outputs of classifiers. Some pe rform better than individual classifiers. Moreover, all of those results are n ot as good as (O+C) representation. The accuracy measurements for resulting c ombined bracket representations and straightforward votes of the classifie rs are listed in Table VII. Though TotPrecision with four combined systems y ields the best result of an 89.14 F-measure, TBL really has no strong influen ce on the re-sult. Both TotPrecision and TagPrecision of the combinatio n of SVMB, SVMF, and CRF score the same performance of an 89.12 F-measure, and when TBL is additionally combined, the result is only a little higher : 89.14. So the com-bination of three classifiers can be regarded as the best mode l and is what we chose as our voting system.
 ual classifiers and systems combined by different classifier s (see Table VIII). As it turns out, for each of the classifiers, the result gained after a majority vote was often lower than that of each system applied to diverse re presentations as shown in Table V. The resulting performances of the combined systems were better than those of separate classifiers, but not as good as t he combined sys-tems in Table VII which is only for IOE2. This voting techniqu e, therefore, has little positive benefit when applied to data representation s.
 pendent collection of 50 news articles from the World Health Organization and constructed a held-out evaluation set. The 50 news artic les were initially annotated in English and then translated and re-annotated i nto Vietnamese. The two sets were then compared to ensure the accuracy of the t ranslation, which was judged to be over 98% true to the original. The resul t on this held-out collection is rather lower than that achieved on th e cross-fold evalua-tion data in Section 2.5 due perhaps to variations in vocabul ary and reporting structure used by the WHO when compared to Vietnamese newswi re reports. Nevertheless, the result reinforces our view that the combi nation system actu-ally yielded better performance than individual systems.
 were well recognized by CRF while others like LOCATION and OR GANIZA-TION were better recognized by SVM forward. Named entities i n NUMBER and CURRENCY classes were best identified by SVM backward. By using the voting mechanism, errors made by the minority of the clas sifiers can be removed. For instance, when we use majority voting to combin e SVM back-ward and forward and CRF, classes such as LOCATION, ORGANIZA TION, TIME, and NUMBER were better recognized than individual cla ssifiers. In the following example, the phrase  X  X hi `  X  eu ba ba nh X a . p kh ij  X  au X  (many imported tortoises) was annotated by three systems: SVMF, CRF, and TB L.
 some cases, therefore, that this word is recognized as a numb er like the CRF classifier did is exactly correct. However, the meaning in th is case is quite different.  X  X a ba X  is now a tortoise, so  X  X a X  cannot be recogn ized as a number. It must be out of seven predefined target entity classes and an notated as O tag. Majority voting, which chooses the tag appearing most often , helps overcome this error.
 cation performance, we carried out statistical significanc e tests using the t-test in Matlab, which tests the hypothesis that the data in vector x comes from a distribution with mean zero and returns the result in h where h==0 indicates that the null hypothesis (mean is zero) canno t be rejected at the 5% significance level. h==1 indicates that the null hypothesis can be rejected at th e 5% level. The data are assumed to come from a normal distribution with unkn own variance. p is the probability that the value of the t-statistic is equa l to or more extreme than the observed value by chance. ci is a 1-alpha confidence interval for the true mean.

For instance, we generate 100 normal random numbers with the oretical mean zero and standard deviation one. The observed mean and s tandard de-viation are different from their theoretical values, of cou rse, so we test the hypothesis that there is no true difference. And we have h = 0 p = 0.4474 ci = -0.1165 0.2620
The result h = 0 means that we cannot reject the null hypothesi s. The sig-nificance level is 0.4474, which means that by chance we would have observed values of T more extreme than the one in this example in 45 of 10 0 similar experiments. A 95% confidence interval on the mean is [-0.116 5 0.2620], which includes the theoretical (and hypothesized) mean of zero.

We applied statistical significance tests on the results of I OE2 represen-tation in Table V and get all h is 1 and the p value is approximat ely 0. This means we can reject the null hypothesis and our results a re due to real improvements, not random fluctuation. Our significance test s are shown in Table X. 4. CONCLUSION AND FUTURE WORK With the same corpus and machine-learning algorithm, our sy stem yields an F-measure of 88.02, outperforming that in Tran et al. [200 7] for which the F-measure was 87.75 and another word-based experiment w ith an ad-ditional POS feature, the F-measure of which was 87.65. When a POS fea-ture is combined in the system, it makes the performance slig htly lower due to errors of word segmentation, but this was overcome when we applied our morphosyllable-based method.

We have presented a voting mechanism approach that performs better than individual classifiers. In this article, we proposed severa l combinations of different systems as well as data representations. Moreove r, the way we converted POS-assigned words to morphosyllables helps imp rove the perfor-mance. Among four data representations, IOE2 format is the m ost suitable choice for our experiment since it gains better performance than others (see Table V), and we used this format for our whole study. The syst em combined by SVM backward and forward and CRF performed well and attain ed an F-measure of 89.12.

With such a promising result, we believe that in the future, w hen we apply this method to recognize biological named entities in Vietn amese documents, the result will be satisfactory.
 We thank Mr. Taku Kudo for his Support Vector Machines tool, Y amCha, and Conditional Random Fields tool, CRF++. We also thank Radu Fl orian and Grace Ngai for fnTBL++, a Transformation Based Learning too l. We thank the University of Waikato for Weka, a tool for C4.5, and Na  X   X ve Bayes, too. We thank the Global Liaison Office of the National Institute o f Informatics in Tokyo for granting us the travel fund to research this proble m. We also sin-cerely thank colleagues in the Vietnamese Computational Li nguistics Group (VCL) for their invaluable and insightful comments.

