 Information retrieval in search advertising, as in other ad -hoc retrieval tasks, aims to find the most appropriate rank-ing of the ad documents of a corpus for a given query. In addition to ranking the ad documents, we also need to filter or threshold irrelevant ads from participating in the auction to be displayed alongside search results. In this work, we describe our experience in implementing a suc-cessful ad retrieval system for a commercial search engine based on the Language Modeling (LM) framework for re-trieval. The LM demonstrates significant performance im-provements over the baseline vector space model (TF-IDF) system that was in production at the time. From a model-ing perspective, we propose a novel approach to incorporate query segmentation and phrases in the LM framework, dis-cuss impact of score normalization for relevance filtering, and present preliminary results of incorporating query ex-pansions using query rewriting techniques. From an imple-mentation perspective, we also discuss real-time latency c on-straints of a production search engine and how we overcome them by adapting the WAND algorithm to work with lan-guage models. In sum, our LM formulation is considerably better in terms of accuracy metrics such as Precision-Recal l (10% improvement in AUC) and nDCG (8% improvement in nDCG@5) on editorial data and also demonstrates sig-nificant improvements in clicks in live user tests (0.787% improvement in Click Yield, with 8% coverage increase). Finally, we hope that this paper provides the reader with adequate insights into the challenges of building a system that serves millions of users every day.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models, Information filtering; H.3.1 [ Content Analysis and Indexing ]: Indexing methods  X  Work done at Yahoo! Inc Algorithms, Experimentation sponsored search, advertising, language models
The primary source of revenue for major search engines is from advertisements displayed during user searches. A search engine typically displays sponsored text listings o n the top and the right hand side of the web-search results, in response to a user query. The revenue model for these list-ings is  X  X ay-per-click X  where the advertiser pays the searc h engine only if the user clicks on the ad. Sponsored search offers a more targeted and less expensive way of marketing for most advertisers as compared to mass media like TV and newspapers, and online ad spend has gained significant momentum in recent years [1].

Most of the major search engines typically have thousands of advertisers with millions of text ads available to be dis-played. In this paper, we focus on the task of retrieving relevant ads that can be displayed alongside search results given a user query. We describe our experience in imple-menting a successful ad retrieval system for a commercial search engine based on the Language Modeling (LM) frame-work for retrieval. The LM demonstrated significant per-formance improvements over a baseline vectors space model (TF-IDF) implementation. To the best of our knowledge, ours is the first work that discusses first pass retrieval of a commercial advertising system in detail, in addition to be-ing the first (published) work to discuss implementation of language modeling retrieval on a web scale.

The paper is organized as follows. First, we provide an overview of a typical search engine monetization system, de -scribing the system flow starting from a user query to a result page with ads. In Sections 2 -4, we describe the specific component addressed by this paper. In Sections 5 to 6, we dive into the details of implementing a language modeling framework to serve ads for millions of user queries within sub-second latencies. We propose a novel approach to incorporate query segmentation and phrases in the LM framework, discuss impact of score normalization for rele-vance filtering, and present preliminary results of incorpo -rating query expansions using query rewriting techniques. We also discuss real-time latency constraints of a produc-tion search engine and how we overcome them by adapting the WAND search algorithm [5] to work with language mod-els. Section 7 describes our offline and online experimental setup and results on editorial data as well as live user tests . In Section 8, we report on some additional offline experi-ments, and discuss related work in Section 9. Finally, Sec-tion 10 discusses some of the future work that can improve upon the current implementation.
We now describe the search engine monetization (SEM) terminology used in this paper. The terminology is similar across most of the major search engines. An advertiser typ-ically has an account with the search engine, and would run many campaigns . An advertising campaign consists of many ad groups ; each ad group in turn consists of a set of related keywords for a campaign. For each adgroup, there is a set of bidded phrases or keywords that the advertiser bids on. A creative is associated with an ad group and is composed of a title , a description and a display URL . The title is typically 4-5 words in length and the descriptio n has about 10-15 words. Clicking on an ad leads the user to the landing page of the advertiser. In the rest of the paper, an ad document d refers to an ad group, consisting of a title, description, display URL and a set of bid phrases, b ....b m with associated bid amounts.

An advertiser can choose to use standard/exact and/or advanced match for the keywords in an ad group. For example, enabling only standard match for the keyword  X  X ports shoes X , will result in the corresponding creative b eing shown only for that exact query. Whereas, if the keyword is enabled for advanced match, the search engine can show the same ad for the related queries  X  X unning shoes X  or  X  X rack shoes X . A bid is associated with each keyword and a sec-ond price auction model determines how much the advertiser pays the search engine for the click [8].

Search engines typically take a multi-stage approach to selecting and serving advertisements: (1) retrieval: find-ing a candidate set of ads for a query which includes both standard and advanced matches, (2) relevance filtering: a more complex second pass model that filters non-relevant ads for the query, (3) click through rate prediction: es-timating click through rate for the retrieved ads and rank-ing ads on the search page using expected revenue, and (4) a generalized second price auction to place the ad and price a click. An overview of our advertisement retrieval an d relevance/click prediction system is shown in figure 1(a).
The first step in determining what ads to show for a query is retrieval. Finding relevant ads for a query has typically used one of two different approaches: (a) a query rewriting approach (eg., [13, 4]) or a (b) direct query-ad approach [6, 23]. In the query rewriting approach, ads containing the bidded phrase that exactly matches a rewrite of q ( q 1 ...q are retrieved in response to q . In direct query-ad match-ing the ads are treated as documents with multiple sections that correspond to the creative and/or landing page and standard IR and web search techniques are used to match a user query q to an ad d from a reverse index when the two are semantically similar ( d  X  q ). Note that a query annota-tion service may be used to determine query rewrites, detect phrases and named entities and so on in the query before the index is accessed with the richly annotated query.
After the initial first pass retrieval, more computationall y complex models with richer features may be used to score a smaller set of candidate ads and filter irrelevant ads. Note that filtering irrelevant ads is very important in a sponsore d search application. Irrelevant ads not only lead to a bad use r experience [11], but also create a bad advertiser experienc e. An irrelevant ad can cost an advertiser a click that does not convert, and a bad ad with a high bid can impact the price paid by an advertiser whose ad is shown above the said bad ad in the second price auction. The retrieval and relevance passes generate the quality of the final set of candidate ads that can participate in the ad auction.

The final ad ranking is a product of the bid and the pre-dicted Click-Through-Rate (CTR) of the ad ( P ( click | q, d , user )); only the candidate set of ads that survive the first two retrieval passes need to be scored for CTR. Given a set of ads (documents) { d 1 ...d r } shown at ranks 1 ...r for a query q on a search results page, the expected revenue is given as: where cost ( q, d i , i ) is the cost of a click for the ad d position i for the query q . If there isn X  X  significant revenue to be made by displaying ads, the search engine can choose to not show any ads for the query.
In this work, we are focused on the first retrieval pass that generates a candidate set of ads using direct query to ad matching. The objective of the the first pass is to scan the entire ads database using a very fast retrieval paradigm maximizing recall while maintaining a high level of precisi on. Recall in the first pass ensures that we have sufficient and relevant ad coverage for queries, especially tail queries t hat typically have no exact matches. Precision in the first pass is also important since reducing the candidate set of ads reduces the latency for subsequent steps as fewer ads have to be scored; performance constraints are extremely tight i n commercial search engines.

First pass exact match lookup is a database lookup and does not require any similarity scoring; this paper only fo-cuses on advanced matching user queries to ads. Prior to this work, our sponsored search product suffered from poor advanced match recall for tail queries, and the query to ad matching system had high latency and timeouts. In this paper, we address three major issues: (a) find/recall a rel-evant ranked list of advanced match candidate ads for a query, (b) ensure filtering of as many irrelevant ads as possi -ble in the retrieval step, and (c) optimize retrieval to be fa st and within acceptable latency constraints for a commercial search engine.
Unlike web-search where an initial retrieval can be done based on an  X  X ND X  of all the query terms, in sponsored search most relevant ads do not match all words in the query because the query and ad are both short, leading to a large lexical gap. Therefore, in sponsored search, we need a soft scoring technique in the first pass itself which is best imple -mented through the use of an inverted index as follows.
An inverted index is a commonly used data structure in which every index term (or feature) is associated with a postings list [37]  X  a list of entries (or payloads ) for every document id that contains the index term (Fig. 1(b)). The payload contains information required by the scorer, such a s the number of occurrences of the term in the document or a partial score. Our index is a distributed one and is parti-tioned by document id. Results from the different partitions are merged and sent to the click and relevance prediction sys -tems. The index is built on Hadoop TM MapReduce and is refreshed regularly, since advertisers may change their ca m-paigns and creatives several times a day, and expect their changes to take effect almost immediately.

Several techniques for fast index access exist in the lit-erature (eg., [30]). A commonly used document-at-a-time (DAAT) technique that we have found to be effective is WAND [5]. The WAND algorithm uses a two level ap-proach: first, candidate documents are selected using an up-per bound of the score and then the exact scores of only the promising candidates are fully computed. The algorithm X  X  efficiency lies in the ability to cheaply compute the upper bound to restrict candidates for full scoring.

The WAND algorithm assumes that a scorer is additive and of the following functional form: where d k  X  C is a creative in the collection C ,  X  i &gt; 0 is a term dependent constant (like IDF) and f ( T F ( w i , d k is a function of the term frequency (TF) in the document. Variants of the classical TF-IDF algorithm[28] can be easil y fitted into this formulation.

Given such a scorer, the WAND algorithm determines the upper bound of the document score as: where UB i is the upper bound of a term X  X  contribution to any document and is given as and d 1 ...d n are documents in the postings list of w i . If  X  f ( T F ( w i , d j )) is stored in the payload, then UB i maximum value of the payload entry containing this partial score and can be computed and stored at index time rather than runtime.

The WAND algorithm compares the upper bound of a document score to a threshold  X  , which is set dynamically by the algorithm based on the minimum score of the top N results found so far (where N is the number of documents requested). If UB ( d k , q ) &gt;  X  , then the document is fully evaluated. WAND is guaranteed to return the correct top N documents from the collection for a query, if the upper bounds are correct. However, if the upper bounds of a term are too loose, then there will be many false alarms and many documents may be unnecessarily evaluated, increasing over -all latency and CPU usage.

Note that the scorer in Eq. 2 only has term components that are common between the query and the document. In section 5.2 we will show how we estimate upper bounds for a language modeling (LM) scorer that has additional com-ponents. As we will see in section 7.5 the upper bounds are tight enough that our LM implementation is fit to be deployed in a production environment.
For retrieving candidate ads from an ad index for a given query, we developed a Language Modeling (LM) implemen-tation to replace our production TF-IDF system. The base-line TF-IDF model used in our experiments is a well tuned system very similar to the one discussed in [6] and further described in Section 7.1. While we could have further tuned the TF-IDF system, we were motivated to explore a prob-abilistic retrieval system because it allows us a systemati c framework to apply query expansion via relevance models [14], translation models[3], as well as probabilistic quer y segmentation, intent analysis and so on. We will show in Section 7 that for the advertising problem, the language modeling framework significantly outperforms the TF-IDF model. It also performs significantly better than our query rewriting approach, where exact matches to query expan-sions q 1 , q 2 ...q n are retrieved.
In the LM formulation the probability that a document or creative, d k , is relevant to a query ( q ) is given as [22]: In a multinomial approach, the words ( w i ) in q are as-sumed to be i.i.d. Therefore, p ( q | d k ) = Q | q | i p ( w p ( w i | d k ) is a smoothed probability of a word w i being gen-erated by the document d k . In Eq 5, p ( d k ) is a prior prob-ability of relevance, and as we will discuss later, we derive this probability based on an advertiser relevance score. If the focus of retrieval was primarily ranking, one can ignore p ( q ) in Eq 5. However, since we need to estimate a query independent threshold for filtering irrelevant candidates , we need to estimate the complete score. Therefore, we estimate p ( q ) = Q i p ( w i ).
 In log space the score S ( d k | q ) = log ( p ( d k | q )) is:
For the commonly used smoothing techniques[36], the smoo-thed probability of a word w i can be expressed as: where f s and  X  s d nique ( s ) used. For Dirichlet smoothing, f s ( T F ( w i several different smoothing techniques, and found Dirichle t smoothing to work best. We will drop the superscript s in subsequent equations and assume Dirichlet smoothing.
Estimating p ( w i ) : The common way of modeling p ( w i times a word w i appears in the ad corpus C [17]. In our work we estimate p ( w i ) as where p ML ( w i | d j ) is the non-smoothed estimate of document-level term frequency. Using this formulation of p ( w i ) we can write Eq 6 as:
S ( d k | q ) = The primary advantage of this approach is that it plays the document prior into the estimation of p ( w i ), and as we will see next, it also results in a bounded score.

Length Normalization: Length normalization of LM scores by taking the geometric mean of the query-dependent tasks like TDT[29]. Applying such a length normalization, Eq 9 can now be written in log space as:
Using query length normalization basically gives us an average of a per-term classification for the document. Since the per-term score is bounded to 1, the overall score is also bounded, and this helps classification. Note that Eq 10 opens up the possibility of doing first pass retrieval using a per-term offline classification score stored as a payload in the reverse index. We show, in section 7 that length normal-ization significantly helps improve classification accurac y.
Estimating p ( d k ): In many language modeling imple-mentations a uniform prior p ( d k ) is assumed. In advertise-ment retrieval we have found that we can effectively use the quantity p ( d k ) to control for the fact that large advertisers tend to have several orders of magnitude more creatives and bid terms than smaller advertisers do. These advertisers aim for volume and hence bid on thousands of queries. To give smaller advertisers a reasonable chance of participat ing in the auction, we compute the inverse-bid-term frequency (IBF) of an advertiser ( d k ) as: where | B | is the total number of bidded terms in the index and domain ( d k ) is the domain of an ad d k . Note that IBF = 0, if we have only one advertiser with multiple bidded terms. We normalize the IBF over all ads to compute a probabilistic score p ( d ) that is used in the LM equations.
In this section, we will describe how we implement re-trieval using the framework described in the previous sec-tion. Given p ( w i | d k ) =  X  d k p ( w i ) for w i 6 X  d , we can now rewrite Eq. 10 in log space as
S l ( d k | q ) = 1 where q u is the set of unique terms in q , T F ( w i ; q frequency of a unigram w i in q .

In our implementation we store the quantity log ( p ( w i in the payload in the inverted list of feature w i . The score components of our payload is stored in a 16 bit unsigned in-teger, and hence we store the value ( log ( p ( w i | d k in the payload, where the shift K helps to make the pay-load positive and M help store the allowed precision. The quantities log (  X  d k ) and log ( p ( d k )) are stored in a forward index which allows us to store document specific attributes, again after using shift of K and a multiplier of M . We typ-ically use K=24 and M=1000. The payload is bounded at the higher end at 48000.

Now, as described in section 4, we need to be able to ef-ficiently compute the upper bound of the above score for WAND optimization to work. Let us analyze the first com-out to be equal to: log ( contains very few words relative to the general vocabulary space which is much larger, thereby distributing the proba-bility mass for p ( w i ) over a much larger set of terms. There-fore each item in the summation in the first quantity of Eq 13 is typically greater than 1. The first summation resem-bles Eq 2 and satisfies the assumptions made by the WAND scorer. Hence, we can use the same ideas used to estimate the upper bounds of the scorer described in section 4 to estimate the upper bounds of the first component of Eq. 13.
Now, let us take a look at the second quantity in Equation 13. Since all query payloads are available to the WAND optimization we can easily count P i : w The trouble is in estimating an upper bound for log (  X  d A forward index lookup in the first pass of WAND where upper bounds are estimated can be expensive. Alternately, using a fixed value of max ( log (  X  d 1 ) ....log (  X  d | C | not to work very well, because the variance in this factor is very large in the corpus (because of large document length variation). So instead, we chose to estimate the quantity UB [ log (  X  d k )] in the following manner. The posting lists were sorted in decreasing order of log (  X  d k ) at index build time. As a result of this, for every document that is being considered for evaluation, the value of log (  X  d k ) can be no greater than that of a document that was previously fully evaluated. Hence, we assign UB [ log (  X  d k )] to be equal to the value of the last document that was scored fully (i.e., whose Upper Bound exceeded the threshold  X  ).

For the upper bound of log ( p ( d k )) we use the value: not show large variance in values and therefore using a con-stant worked well.

In this way we can estimate the upper bounds of the indi-vidual components with little computational overhead. We will show in Section 7.5 that these upper bounds are tight enough that the resulting speed of the system is faster than our existing TF-IDF scorer.
Until now, we have assumed that the terms w i are uni-grams (words) from the query and/or documents. For ad-hoc information retrieval, such a bag of words approach has typically worked quite well. However, in advertising retri eval we have found that since there are no (or very few) relevant documents with all query terms present, using only word features for retrieval results in issues with precision. Re -trieving Carmen Electra ads for the query  X  X armen flores X  or retrieving baja california vacation ads for the query  X  X aja fresh X  leads to bad user experience. We therefore developed a query segmentation model based on Conditional Random Fields. We now describe the supervised segmentation model, followed by a description of how we use it in LM retrieval. We describe a novel approach of segmenting the ads using the very same CRF trained for queries.
Linear chain CRFs have been used successfully for many sequential labeling tasks like segmentation and part-of-s peech tagging[16]. CRFs are particularly attractive because we can use arbitrary feature functions on the observations. Le t query = q 1 , q 2 ...q 3 denote the input query and seg = y ...y n denote the corresponding state sequence. y i is a bi-nary indicator variable where a value of 1 denotes begin-segment and 0 denotes end-segment . The conditional prob-ability P ( seg | q ) is given as: p ( seg | q ;  X ) = 1 where f k ( y i  X  1 , y i , q, i ) is a feature function and  X  = {  X  the learned feature weights. We have several features based on dictionaries of people names, celebrity names, stopword lists and the top phrases in the query logs. We use the CRF++ toolkit 1 to train the model. The model is trained in a supervised fashion on 6000 queries annotated by hu-mans. The human annotators were given queries that were http://crfpp.sourceforge.net/ pre-segmented by an unsupervised model (similar to [31]). The annotators were asked to correct the segmentations for errors. We have found that agreement rates among anno-tators is typically around 80% for this task. The CRF im-proved the WER over the HMM based approach by 21% and is comparable to our best in-house dictionary based ap-proach; however, we find that it requires much less manual tuning and can generalize better to unseen phrases. In ad-dition, the probability values from the n-best segmentatio ns can be directly used by our model. The following example illustrates the output of the CRF: The segmentations in our system are returned by the Query Processing and Annotation subsystem in Figure 1(a). In our implementation, we always set aside a small probability x for the unigram segmentation and distribute the remaining 1  X  x probability over the top k segmentations that are above a threshold  X  in the proportion of p ( seg i ). x allows us to improve recall via increasing the number of unigrams and  X  allows for the addition of only high confidence phrases.
In our implementation, we also apply a segmentation al-gorithm to the ads at indexing time. The segmenter trained on web-queries had a high error rate on the description sec-tion of the creatives because they are well formed english sentences unlike web queries, and longer as well. We there-fore take the following approach: (1) we segment the title and all the bidded phrases in the ad using the query CRF segmenter as these are short and resemble web queries. (2) We then build a dynamic dictionary from the phrases de-tected in step (1) and then (3) use the dictionary to find a segmentation of the description field finding the longest sub -strings from left to right that exist in the dictionary. Each detected phrase contributes to one possible segmentation o f the creative and the unigram segmentation is also generated as a candidate. The description segmentations are assigned uniform probability. The segmentations are used to gener-ate an expected TF as in Eq. 16. Note that using the query CRF model for both queries and ads also enforces consistent tokenization.

An alternative approach is to have the index store position information in the payloads, which can be computationally expensive. We are currently working on fast implementa-tions of the Markov Random Field Scorer proposed by Met-zler and Croft[18] which combines LM with proximity.
One possible retrieval model using the segmentations ob-tained above is The score of a document p ( d k | q, seg i )  X  S ( d k | q, seg termined as before, with the main difference being that the terms w i are segments as determined by seg i . The primary issue with Equation 15 is that we would need to make a call to the index for each possible segmentation and then com-bine the scores. Multiple index calls per query can be pro-hibitively expensive and we explore an alternative approac h outlined below.
We generate a new bag of words and phrases q  X  = w  X  1 ...w from q where the number of times a term w  X  j is seen is an expectation computed as follows: Documents are now scored using S l ( d k | q  X  ) using the same scoring formula as in Equation 10. Since the score is length normalized (using | q  X  | = P j E ( T F ( w  X  j ; q  X  )) we do not worry about large variations in the size of n  X  . The scorer has the advantage that it can be implemented in one scan of the postings list. The score effectively works out to be where S ( d k | q, seg i ) is as per Eq 9.
In this section we describe experiments on the code de-ployed in production. Standard in-house libraries to do lig ht stemming and stopword removal were used in addition to dropping all tokens of length less than 2 characters. Word and phrase features are extracted from the title, descripti on, display url (after url segmentation), and all the bidded ter ms of a creative.

We now move on to describe the baseline retrieval systems, the data, metrics and the results. We consider two baseline systems for retrieval (a) a TF-IDF  X  X uery to ad X  scoring system and (b) a query rewriting  X  X uery to bidterm X  system, both of which were deployed in production at the time we started this project.

The baseline TF-IDF system used in our production en-vironment was inspired by the framework described in [6], and is well-tuned for ad retrieval. In the TF-IDF system, a document and query are both represented as | V | dimen-sional vectors. The weight of term w i in this vector is given as g ( w i , d k ) = f ( T F ( w i ; d k ))  X  IDF ( w i ) where f ( T F ( w i ; d k ) = log ( T F ( w i ; d k )) + 1 and IDF ( w i ) = | C | /c ( w i ) where | C | is the number of ad docu-ments in the corpus and c ( w i ) is the number of ads contain-ing w i . The query side weight is g ( w i ; q ). The cosine score between a query and a document is simply given by the dot product as follows The TF-IDF scorer is easily implemented in an inverted in-dex with g ( w i ; d k ) as the value in the payload (we store g ( w i ; d k )  X  1000 in the 16-bit payload). S tfidf is linearly combined with the document quality score IBF ( d k )  X  1000 with a weight of 0.2. Note that our baseline implementation of TF-IDF also includes phrases from a large phrase dictio-nary; the unigrams and phrases are weighted by their raw occurrence in the query. In all the experiments reported, phrases are always used in the baseline system.

In the query rewriting baseline, a set of query rewrites ( q ...q n ) are retrieved for the query q . Example rewrites for the query girls baseball gloves are cheap baseball mitts, youth baseball gloves and so on. Rewrites are generated using sev-eral algorithms that run in parallel. One such technique mines the click logs for query reformulations in user sessio ns (as described in [13]). A second technique exploits query-a d co-clicks in a collaborative filtering model described in [2 ]. A third technique mines query and organic search co-clicks using the same collaborative filtering technique. The pool of rewrites from all these methods are scored by a Decision Tree model that uses features which measure lexical overlap as well as the CTR of the query-rewrite pair to generate a similarity score between 0-1 ( s i ) for the rewrite q i . The de-cision tree model has features and training data very simila r to the regression model in [13]. Rewrites that cross a score threshold for relevance are used by our production system.
Given such a system that generates a set of rewrites q 1 , q q q with associated scores s 1 , s 2 ...s n q for query q , a creative can be scored as: where bp ( d k ) are the bidded phrases in d k . This scorer can also be easily realized in an inverted index by storing com-plete bidded phrases as features, and using s i as a query-side payload. Note that the lack of query-dependent length nor-malization means that a query with more rewrites can have a higher score than one with fewer rewrites. This impacts classification accuracy for the query-rewriting baseline, but does not impact ranking.

We also compare with the  X  X elevance model X  or the sec-ond pass model which is a machine learned model that has a rich set of features including click-through-rate featur es. The model is a Gradient Boosted Decision Tree that is al-most identical to the one described in [10]. The model is a classifier trained to predict one of 5 relevance classes  X  Perfect, Excellent, Good, Fair, Bad for a given query-ad in-stance. Note that the first pass LM model has to score the entire corpus and it is not computationally tractable to use the second pass relevance model in the initial retrieval. Ho w-ever, as described earlier, having a good set of candidates sent to the relevance model is of critical importance.
Training, test and evaluation data are obtained by scrap-ing our production system using several different parameter settings. The pooled evaluation data used in experiments in this paper contains 5591 unique queries, which were selecte d based on a stratified sample of search engine traffic that rep-resents all ten search frequency deciles. A total of 39073 ad s were judged by editors on a five point scale (Perfect, Ex-cellent, Good, Fair, Bad). Judgments were performed by professional editors who achieve reasonable consistency. For metrics requiring binary relevance judgments, we consider all judgments better than  X  X ad X  as relevant and the remain-ing  X  X ad X  judgments as irrelevant ads.

We report precision and recall metrics (classification ac-curacy) obtained by sorting all the query-ad pairs by the predicted scores of a given system, and measuring the preci-sion at each recall point. We also report nDCG[12], a pop-ular metric in web-search that measures the quality of the ranked list. We investigated other ranking metrics such as rank correlation and interpolated precision-recall; howe ver the trends were similar to nDCG and we only report nDCG in this paper. We have typically found that optimizing for classification accuracy (precision-recall) while maintai ning nDCG has translated into click yield improvements in on-line tests. Tests on live traffic are discussed separately in Section 7.4.

We used a seperate development set of similar size to tune the Dirichlet parameter  X  and found that a value of 0.5 worked best for our task. Similarly CRF parameters k ,  X  and x were set at 3, 0.5 and 0.5 respectively for the ex-periments in this section. The TF-IDF system had been similarly tuned and different variants of f ( T F ( w i ; d )) and IDF ( w i ) and alternate vector space similarity methods like BM25 scoring were explored; the variant described in Sec-tion 7.1 was found to work best on a development test set. Figure 2: Precision Recall (P-R) Curves: The LM-unigrams+phrases model is significantly better than the TF-IDF model and the query-rewriting baseline at all recall points.
 Figure 3: Number of Ads of each grade type in Dif-ferent Score Bins for LM-unigrams+phrases Scorer
Precision Recall curves for the different systems are shown in Figure 2. We find that the LM scorer (Eq 11) with only unigrams has worse precision than the baselines (TF-IDF and Query Rewriting) at lower recall points, but is much better in the regions of high recall. The Query Rewriting baseline is not as good as the LM on classification, because of a large variance in the number of rewrites for different queries, leading to large variance in scores. However, nDCG as we will see will be quite comparable to LM.

The addition of phrases to the LM scorer (Eq 11 and Eq 20) helps improve precision significantly. The area under th e P-R curve (AUC) for this scorer (LM-unigrams+phrases) is 10% higher than that of TF-IDF. To demonstrate the im-pact of length normalization, we also ran an experiment re-moving query length normalization (LM-unigrams+phrases-noLNorm). Not having query normalization hurts precision around the region of recall 0.3-0.7. We are happy to see that the performance of the LM-unigrams+phrases scorer (also henceforth loosely referred to as the LM scorer) is quite clo se to the relevance model. The relevance model X  X  additional features help improve precision in the low recall regions. Query Rew. Baseline 0.810 0.748 0.733 Table 1: NDCG using baseline techniques (TF-IDF, query rewriting) vs. first pass LM formulation vs. second pass relevance models. LM-uni+phrase is significantly better than TF-IDF.

Table 1 lists the nDCG values for the different methods at ranks 1, 3 and 5. The LM scorer has significantly higher (8% better) nDCG values than the baseline TF-IDF system at all 3 positions. The Query Rewriting baseline performs very well at rank 1, and is comparable to the LM at other ranks. The relevance model is typically the best performing system. Figure 3 shows the number of ads of each grade in each score bin. There is a smooth increase in the number of bad ads as we move towards lower recall bins.

As our approach is ultimately meant to be used for ad-vanced match, we remove the query-ad pairs for which the ad contains an exact match (i.e., one of the bidded terms is identical to the query). The precision recall curves for TF-IDF and LM scorer on this slice of the data are shown in Figure 4. This subset of the data is harder than the complete data set because there is more room for lexical mismatch. The trends are similar in that the LM is significantly better than the TF-IDF baseline. In fact, on this slice of the data the LM is better than TF-IDF even at the low recall points.
Error analysis showed that the LM scorer performed bet-ter than TF-IDF typically because of implicit document length normalization due to Dirichlet smoothing:  X  d k is doc-ument length dependent, making the penalty for a missing term more for longer documents. In our corpus the large variations in document length come from the variance in the #bidded terms per creative; some advertisers have large campaign management tools and experts to automatically discover many bidded terms. Other smaller advertisers on the other hand have 1-2 bidded terms per query.
Rescoring editorially judged data is useful for offline pa-rameter tuning. However, it suffers from the well-known problem that it does not measure the true recall, i.e., it does not indicate the difference in the quality of the ranked lists, when a full retrieval on the entire corpus is performe d. While we could do a retrieval and submit unjudged ads for judgment by humans, we chose a cheaper alternative: viz., bucket testing. Given the large user base of the commer-cial search engine that we had access to, it was possible to expose our algorithm to several million users and reli-ably collect statistics over a sufficiently long period of tim e. Bucket-testing also helps evaluate the monetization capab il-ity of the new algorithm.

A randomized set of users in the experimental bucket are shown candidate ads retrieved by the LM (for advanced match) in addition to other query rewriting (QRW) ad-vanced match approaches that are already in the system. Users in the baseline bucket are shown candidate ads re-trieved by the TF-IDF (for advanced match) in addition to other advance match systems. The operating point of both the algorithms roughly corresponds to the 0.72 recall point on the chart in Figure 4 i.e., we ensure that in both buckets, the two models are returning the same number of ads on average in the first pass. For each ad impressed, the bucket logs indicate which algorithm retrieved it, in addition to i n-formation about where on the page it was shown, whether a click was observed, etc.

The effectiveness of our new methods can be measured by computing click through rate (CTR) statistics on the bucket logs. In our analysis, we consider only those ads that were uniquely recovered by the LM or TF-IDF algorithms. In other words if multiple advanced matching algorithms retrieved an ad in a bucket, the new method (LM or TF-IDF) did not get credit for it.

A 0.787% (statistically significant) Click Yield (# clicks divided by # page  X  views ) gain and 8% coverage increase (% of page-views with ads shown) was observed in the ex-perimental bucket. The LM scorer contributed to 16% of the advanced match impressions in the experimental bucket. If we just compared the set of queries which had ads from the Query-Ad (QuAd) models in each bucket (TF-IDF and LM scorers respectively), we see that LM ads were shown for 10% more queries than TF-IDF ads were. The total num-ber of impressions/ads from the QuAd model were 14.3% more in the experimental bucket than in the baseline. This happens because (a) more ads survived the second pass rel-evance model in the case of the LM scorer (b) there were more new ads discovered by LM that were not retrieved by other advance match algorithms. The Impression-CTR (% clicks observed on ads from the QuAd model on the bucket) remained constant between the baseline and the ex-periment despite the increased coverage. Note that the in-creased depth for a query also makes the marketplace more competitive and improves the price per click on an ad for that query. The QuAd models have higher coverage in the tail, where QRW techniques fail because they typically rely on a query having been observed several times in the logs.
Editorial tests on the bucket scrapes revealed that the % of bad ads unrelated to the query decreased by 13% in the bucket with the LM scorer. We found that LM preferred ads with more word overlap  X  a property favored by human edi-tors and users. Ads for which many query terms were miss-ing were typically not retrieved because of the  X  d k penalty. Anecdotally, this led to fewer false alarms for phone number queries and other non-commercial queries like how to install LG LP conversion kit on a dryer (TF-IDF matched to many  X  X onversion kits X  ads for this query).

Ultimately, it is such a combination of offline and online metrics that gives us confidence in the performance of a new algorithm, and enables its deployment in production. In this work we looked at the core retrieval with just word based features. We can now build on this framework and add richer features. For eg., query categories/intent ( c be incorporated into a score as P p ( c i ) p ( d k | q, c In these experiments we took one partition each of the TF-IDF and LM indices, ensuring that the partitions were identical in terms of the creatives that were indexed in them . The experiment was performed on a 64 bit dual quad core machine with 32G RAM. We queried each index with a 1000 queries, and requested 10 results for each query. The experi -ment was repeated 5 times. The second row of Table 2 shows the average time spent per query for both TF-IDF and LM. We found that the number of times the upper bound was evaluated was 12 times more in the case of TF-IDF scorer, making the TF-IDF scorer much slower. We hypothesize that this may have to do with the large variance associated with the payloads for the TF-IDF scorer (relative to the range of 0-1000) as seen in Table 2 which shows statistics of the payloads for am average of 50 popular query terms. The table also shows the average values of scores in the forward index. The LM payloads exhibit much lower variance (rela-tive to its range of 0-48000) and the average payload in the case of LM is much closer to the max payload, making the approximation based on Eq. 4 much tighter for LM. The variance in  X  d k is much higher than the variance for p ( d and therefore for LM we see gains in speed by sorting the postings list in decreasing order of  X  d k . On the other hand for the TF-IDF payload, since there is only one quantity in the forward index, the payloads are sorted in decreasing or-der of p ( d k ). The overall system being faster, we are able to retrieve more candidate ads with the LM scorer, leading to increased coverage and depth. Heavier X  X oad tests X  involvi ng the complete system also revealed that the LM scorer was in general faster than TF-IDF. The end-to-end latency of the LM scorer at the  X  X etrieval engine X  level (See fig 1(a)) was about 20 ms. According to [20] the average search time on a 2.7G uncompressed index is 19 ms for a probabilistic system like Indri. Our compressed index was about 80G in total (typically partitioned into 40 partitions), making t he scale of our problem much larger.
In this section we report the results of various experiments on our offline prototype system done more recently.
Query Segmentation Alternatives: These experiments use a lower value for  X  on the query side (0.2) for query seg-mentation as opposed to 0.5 in earlier experiments. We ob-served that a lower value of  X  helps improve precision on the
Table 2: Avg. scorer time and Payload Statistics query side; however, on the ad side, lowering  X  unnecessar-ily increases the size of the index without any performance gains ( therefore ad side  X  is still 0.5).

Figure 5 shows a comparison of using multiple segmenta-tions as in Eq 15 vs. the empirical approximation in Eq. 16. We find that the approximation of Eq. 15 is a little better Figure 5: Eq 15 vs Eq. 16. In both cases the query side CRF threshold  X  was lowered to 0.2. The data is post exact match filtering. than the full implementation at the first recall point, but is on the whole quite similar to Eq 16 (as seen in Figure 5).
Comparison to KLD scoring: KL-Divergence (KLD) between the query language model and the document lan-guage model is often used as the probabilistic retrieval fra me-work in IR, and provides a framework in which query expan-sions and term importance can be accommodated[35, 14, 15]. There are a few drawbacks with using the KLD formulation for our task; the score is not bounded or easily compara-ble across queries especially if the queries vary significan tly in length, and secondly, a smoothed query distribution is non-trivial to implement in the first pass.
 Nevertheless, we ran an offline comparison of using the KLD framework and compared it to our LM formulation (both experiments use query side  X  = 0 . 2. The KLD score where V sums over all the terms (unigrams and phrases) in the vocabulary. The query model is given as: p ( w i | q ) =  X p ML ( w i | q ) + (1  X   X  ) p ( w i )
The document model is the same dirichlet smoothed esti-mate used before. Note that p ( w i ) is estimated using Equa-tion 8 and the relevance prior of the document is implicitly used in KLD using this factor; otherwise, we ignore the doc-ument prior in KLD scoring.

Figure 6 reports precision recall comparing the LM frame-work to using KLD scoring with  X  = 0 . 95. There is an im-provement in precision in the first few recall bins, but overa ll, the curves look very similar. NDCG also shows a small 1% improvement over our current LM formulation. The gains we observe or not significant enough to justify the complex-Figure 6: P-R Curves comparing LM formulation to KLD in an offline setting. The data does not include exact matches. ity of implementation issues, unless we want to implement query expansions or term importance using KLD.

Initial Experiments with Query Expansions: Since we had available to us, alternate rewrites q 1 ...q n q for a query q , we also studied whether adding unigrams and phrases from the rewrites to expand q would be beneficial. The simplest way to incorporate the unigrams and phrases from the rewrites into the original query is by forming a new ex-panded query q exp . In our implementation, we modified the expected TF (Eq. 16) value as: E ( T F ( w  X  ; q  X  )) = where q 0 = q and seg ij is a segmentation of q j . p ( q if j = 0 and p ( q j | q ) = (1  X   X  ) s i P n We used this estimate in both scorers: S l and S kld . With S and  X  = 0 . 9 nDCG@3 improves above 0.7%, but precision at the early recall points drops about 1%, probably because length normalization becomes more of an issue. The result with S kld is similar. We are currently investigating alternate models and sources of data to expand the queries, and hope to see improvements.
The information retrieval community has studied the prob-lem of matching queries to relevant documents for several years and has considered a variety of query types and col-lections [33]. While it is not possible to do an exhaustive review of the entire literature, the works that have most in-fluenced ours are those of [27, 22, 36]. However, these works do not consider aspects of a web-scale implementation and the application of the model for advertising. In addition to fast retrieval, some of the modeling ideas used in our work, especially score normalization and probabilistic segment a-tion are novel contributions to the field.

The area of online text advertising is basically focused on two main areas: contextual advertising and sponsored search. Many papers have talked about feature based ma-chine learning models to determine what ads are relevant for a web-page or a search query[7, 32, 10] but few have talked about the first pass retrieval. The most commonly reported advanced match techniques for the first pass are via  X  X uery rewriting X  techniques(e.g., [24, 13]). The tran s-formed query is typically used for retrieval by matching to the bidded term. Models to predict query rewriting tech-niques are typically learned from query logs. One of our baseline systems is such a rewriting system that is in use in a commercial search engine. In the bucket tests, we saw that the LM scorer and rewriting techniques tend to retrieve different sets of candidates, together providing diversity in the set of ads that ultimately participate in the auction.
Query expansion is generally accepted as beneficial for information retrieval specially for matching small pieces of text (e.g., [19]). Expansion may be through pseudo relevanc e feedback [34, 14] or interactive techniques [9]. Query expa n-sion has also been studied in the context of advertising [6, 23, 21, 25]. Since pseudo-relevance feedback (using ads, or web-search results), is not feasible at run-time we explore d query expansions using rewrites in an offline framework. We plan to explore other models with different sources of expan-sion data in the future.

A source of data not used in this paper that has proven useful for sponsored search is the historical CTR of a query-ad pair in predicting its relevance to a query (e.g., [26]). Instead we chose a two-stage approach where the first stage relies less on history and the second stage can use historica l CTR in addition to word-overlap features, as this allows new advertisers who have never been shown for a given query to have a chance to show up on the search results page.
This paper proposed a probabilistic framework for for first pass retrieval in sponsored search and discussed implemen-tation in detail. Our experiments demonstrated significant gains in relevance and clicks, which results in considerabl e impact for commercial search engines. We hope the paper is beneficial to other practitioners of web-scale systems in sponsored search and in other domains.
 We thank several of our colleagues in Labs and Engineer-ing at Yahoo! for making this work possible. In addition we would like to thank our anonymous reviewers for their valuable feedback.
