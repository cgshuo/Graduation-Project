 Abstract Crowdsourcing has emerged as a new method for obtaining annotations for training models for machine learning. While many variants of this process exist, they largely differ in their methods of motivating subjects to contribute and the scale of their applications. To date, there has yet to be a study that helps the practitioner to decide what form an annotation application should take to best reach its objectives within the constraints of a project. To fill this gap, we provide a faceted analysis of crowdsourcing from a practitioner X  X  perspective, and show how our facets apply to existing published crowdsourced annotation applications. We then summarize how the major crowdsourcing genres fill different parts of this multi-dimensional space, which leads to our recommendations on the potential opportunities crowdsourcing offers to future annotation efforts.
 Keywords Human computation Crowdsourcing NLP Wikipedia Mechanical Turk Games with a purpose Annotation 1 Introduction It is an accepted tradition in natural language processing (NLP) to use annotated corpora to train machine-learned models for common tasks such as machine translation, parsing and summarization. Since machine-learned system performance is dependent on the quality of the input annotation, much work in annotation has centered on defining high quality standards that were reliable and reproducible, and finding appropriately trained personnel to carry out such tasks. The Penn Treebank and SemCor are prominent examples in this community. Even now, this high quality route continues to be used in other high-profile annotation projects, such as the Penn Discourse TreeBank (Prasad et al. 2008 ), FrameNet (Baker et al. 1998 ), PropBank (Kingsbury and Palmer 2002 ) and OntoNotes (Pradhan et al. 2007 ), among others.
An alternative to high quality annotation is to make use of quantity: lots and lots of annotations. Redundant annotations in such large datasets would act to filter out noise. The emergence of the Web made this a real possibility, where raw monolingual and parallel corpora, term counts and user generated content enabled the mining of large amounts of statistical data for training models, in both supervised and unsupervised machine learning modes. With the advent of Web 2.0, it is also clear that the Web has also made it possible to take advantage of massive numbers of people . This trend reaches one logical conclusion when the web serves to connect human service providers with those seeking their services. Although described by many different terms, we use the term crowdsourcing throughout this article. Crowdsourcing is a strategy that combines the effort of the public to solve a problem or produce a resource. The term  X  X  X rowdsourcing X  X  has been used in the popular press to emphasize that the workers need not be qualified experts but that useful input can also come from laymen or amateurs. While human subjects can be used to provide data or services in many forms, we limit our attention in this work on annotations for data useful to NLP tasks, and ignore the distributed nature of crowdsourcing.

Crowdsourcing takes many forms, each requiring a different form of motivation to achieve the end goal of annotation. We recognize three major genres of crowdsourcing. In Games with a Purpose (hereafter, GWAP), the main motivator is fun (von Ahn and Dabbish 2008a , b ). Annotation tasks are designed to entertain the annotator during the task. In Amazon Mechanical Turk (MTurk), the main motivator is profit. Practitioners needing annotated data create and list batches of small jobs termed Human Intelligence Tasks (HITs) on Amazon X  X  Mechanical Turk website, which are done by workers from the general public. Workers who fulfil these tasks get credited in micro-payments. While certainly not the only paid labour sourcing environment, Mechanical Turk X  X  current ubiquity makes  X  X  X Turk X  X  a useful label to refer to this and other forms of computer mediated labor. Wisdom of the Crowds (WotC) is another major genre for crowdsourcing. WotC deployments allow members of the general public to collaborate to build a public resource, to predict event outcomes or to estimate difficulty to guess quantities. Wikipedia, the most well-known WotC instance, has different motivators that have changed over time. Initially, altruism and indirect benefit were factors: people contributed articles to Wikipedia not only to help others but also to build a resource that would ultimately help themselves. As Wikipedia matured, the prestige of being a regular contributor or editor became a motivator (Suh et al. 2009 ).

It is important to recognize that the different motivators give each form of crowdsourcing distinct characteristics. Equally important is to note that the space of possible motivations and dimensions of crowdsourcing have not been fully explored. Given raw linguistic data, what vehicle for annotation would be most fruitful to pursue? Thus far, there has been no systematic scheme for potential projects to follow. We attempt to address these issues in depth. In particular, we:  X  deconstruct crowdsourced annotation applications into pertinent dimensions and  X  analyze the characteristics of the three genres of crowdsourcing and make  X  analyze these crowdsourcing instances and propose improvements for crowd- X  discuss the future of crowdsourcing annotations in the conclusion (Sect. 6 ). 2 Related survey work on crowdsourcing Although crowdsourcing is a fairly recent development, it is recognized as a growing and burgeoning research area, as evidenced by this journal X  X  special issue as well as several works that have produced an overview of these methods from different perspectives.

Most related in scope is the work by Quinn and Bederson ( 2009 ) who describe a taxonomy of crowdsourcing (termed  X  X  X istributed Human Computation X  X  in their paper). They divide crowdsourcing into seven genres: GWAP, Mechanized Labor (which we term  X  X  X Turk X  X ), Wisdom of Crowds, Crowdsourcing, Dual-Purpose Work, Grand Search, Human-based Genetic Algorithms and Knowledge Collection from Volunteer Contributors. They deconstruct these genres along six dimensions and also discuss future directions towards utilizing crowdsourcing to solve other computational problems. While certainly an useful starting point, the genres are generally defined and do not specifically address the task of annotation. As such, some of their genres are irrelevant and we feel are better combined from an annotation perspective.

In parallel, Yuen et al. ( 2009 ) also surveyed crowdsourcing applications, categorizing them into five classes: initiatory human computation, distributed human computation, social game-based human computation with volunteers, paid engineers and online players. Their survey analyzes crowdsourcing from a social gaming perspective, differentiating the classes based on game structure and mechanisms. They also touch on the performance aspects in such systems, presenting references to primary studies that describe methods for best measuring the reliability of results coming from crowdsourcing frameworks. Yuen et al. X  X  work, however, do not go beyond its survey to suggest any critiques or analyses of the existing crowdsourcing literature.

Aside from these two surveys that examined crowdsourcing in its wider definition, studies have also analyzed specific theoretical aspects. von Ahn and Dabbish ( 2008a ) present general design principles for GWAPs. They articulate three game classes (or  X  X  X emplates X  X ). Each template defines the game X  X  basic rules and winning conditions such that it is in the players X  interest to perform the intended computation. They also described a set of design principles to complement the templates and proposed metrics to define GWAP computation success in terms of maximizing the utility obtained per player hour spent. In a similar vein, Jain and Parkes ( 2009 ) surveyed existing game-theoretic models for their use in crowd-sourcing models, and outlined challenges towards advancing theory that can enable better design. They believed that game theory has a role in perturbing designs and optimizing from an existing design, instead of being employed to formalize the whole design objective.

Although there has been clear interests in harnessing crowdsourcing, the summative work thus far has concentrated on its mechanisms and design. Surveys have described past work in each crowdsourcing mechanism separately, but have yet to compare application instances using an uniform set of criteria. There has been no work that unifies these frameworks with respect to annotating natural language data, although there clearly is much explicit interest in this area, as is evident by the recent workshops on the MTurk frameworks (Callison-Burch and Dredze 2010 ) and on collaboratively constructed semantic resources (Gurevych and Zesch 2009 , 2010 ), aside from this issue.

To the best of our knowledge, there has been little work that describes which crowdsourcing model is appropriate in which situation or task. It is also not clear when the traditional form of manual annotation by local experts is appropriate X  X re the days of highly-paid expert annotation over? A NLP practitioner who conducts research and development in need of annotations to train models cannot rely on the existing literature to recommend which form of annotation her specific task should take. 3 Comparing crowdsourcing applications Given these limitations of previous work, we first revisit crowdsourcing applications to distil a set of dimensions suitable for characterizing them. Our work differs from Quinn and Bederson ( 2009 ) X  X , as we construct our dimensions from the practitioner X  X  perspective. That is, we distil factors that would influence how a practitioner seeking annotations would design her annotation task. There is certainly overlap with previous work, as some of their dimensions are preserved in our analysis, which we denote with asterisks in subsequent headers. There is also certainly correlation between dimensions. Many of our five dimensions X  X otiva-tion, annotation quality, ease of task setup, human participation and data complexity X  X ave internal facets, which we briefly discuss in turn.
To facilitate subsequent discussion, we also assign a range of possible values for each facet. This allows us to assess published crowdsourcing instances with our subjective opinion, and compare them with traditional, in-house annotation efforts. While opinions will certainly differ on the exact value that should be assigned, the introduction of facet values allows us to compare across applications to uncover trends for success and identify areas that could be better exploited; low scores are not meant to demean an application.

To create the ratings below, each of the authors ( n = 3) independently skimmed and assessed 52 publications that we identified as describing instantiations of crowdsourcing. Using the description of each of the dimensions that follow, we graded each instance on a scale from 0 to 3, in steps of 0.5, where 0 is given to dimensions that are  X  X  X ot applicable X  X  to the instance, and increasingly positive scores indicate increasing favor to the practitioner. We calculated inter-annotator agreement using Fleiss X  Kappa (Fleiss et al. 1971 ), achieving an overall average agreement for all dimensions of 0.75. Due to the few judges involved and potential bias in having like-minded authors perform the ratings, the agreement values are only indicative, but still show a fair amount of agreement, hinting that the trends discussed later, based on this assessment, are generic.

Table 1 shows our classification and scores a few representative crowdsourcing applications based on our facet classification to be discussed below. Higher scores indicate better value for the practitioner. Due to space limitations, we have omitted the full ratings table in this paper, which we have made available online. 1 Rather than discussing all of the instances in the table, we limit our discussion to well-known instances of each crowdsourcing genre: Phrase Detectives (Chamberlain et al. 2009 ), a GWAP created to annotate relationships between words and phrases; the set of 5 NLP tasks performed with MTurk (Snow et al. 2008 ) (specifically, affective text analysis, word similarity, textual entailment, event annotation and word sense disambiguation); and the Open Mind Initiative, a WotC project to collect common-sense facts for automated, real-world reasoning. 3.1 Motivation* The dimension that distinguishes crowdsourcing genres is also a primary issue that the practitioner needs to consider: motivation. Different personas (in the marketing sense) of the public are driven to participate by different objectives. We characterize the approaches systems use to engage participants based on how internally motivated they are.

Fun is a significant motivator, and is heavily aligned with the GWAP form of crowdsourcing. Applications with fun encourage re-playability and when coupled cooperatively with other or competititively against each other). Fun can also hide an ulterior annotation task that could be tedious or complicated (von Ahn and Dabbish 2008b ; Vickrey et al. 2008 ). Profit is also another driver, exemplified best by the MTurk framework. In Amazon X  X  Mechanical Turk, the payment amount is set by practitioner, enabling her to trade-off participation for cost. Since many MTurk workers come from developing countries, the payment per unit annotation can be very low compared to in-house or hired expert annotations (Kittur et al. 2008 ; Callison-Burch and Dredze 2010 ; Snow et al. 2008 ; Ipeirotis 2010 ).

Altruism drives users to annotate for the sake of the system itself. People may annotate or provide information to gain indirect benefit later, as in the case of WotC applications, such as review sites and Wikipedia, where the community benefits as a whole as more users contribute. In some sense, interest and importance can be seen as drivers for the examples in this group. Productively passing time in between television commercial breaks, as noted as a motivator in MTurk (Ipeirotis 2010 ), can also be seen as altruism.

We score motivation independently on each of these three. GWAPs thus tend to score higher on fun, such as the ESPGame (von Ahn and Dabbish 2004 ), which we feel is visually appealing, has the game elements of levels and time pressure, and social elements, all which add to the excitement. MTurk tasks vary in their entertainment per task; (e.g., we rated visual tasks for computer vision (Sorokin and Forsyth 2008 ) higher than textual ones for NLP (Snow et al. 2008 ; Kaisser and Lowe 2008 ). However, in general, MTurk tasks X  interface largely inherit from a common and utilitarian survey-like interface, making them more uniform. Profit is only an element of the MTurk form of crowdsourcing, but we note that it is the primary motivator in traditional annotation efforts. Altruistic motivation, if manifested, usually serves as a secondary incentive, and can replace payment in certain cases (e.g., a small charitable donation will be made by the practitioner if the worker performs the annotation). 3.2 Annotation quality* A practitioner also needs to choose a framework that matches her minimum level of acceptable quality standards. This aspect has been a central concern in crowd-sourcing. As frameworks evolve to become more centrally-mediated by algorithms, data quality has also become a dimension that can be traded off for other factors. For instance, quality can often be traded off by quantity: collecting high-quality data may require a worker to strictly follow complex annotation rules. This dampens the workers X  motivation to do the task, resulting in a smaller number of annotations.
One strategy is to have multiple annotators independently agree on the annotation as measured using standard agreement metrics, assessed post-task itself or in a pilot task, 2 or by asking the crowd to validate the acquired annotations in a separate task (a two-stage annotation process), or adjusting the system X  X  notion of trust of particular workers online (Sheng et al. 2008 ; Feng et al. 2009 ). Different thresholds can be set to determine correctness of the output with an arbitrarily high probability (von Ahn and Dabbish 2004 ; Vickrey et al. 2008 ; Snow et al. 2008 ). Another method is to impose constraints on who may do a task.
A computer mediator (such as those used in GWAP) can be imbued with abilities to track how well an annotator performs, allowing effective pairing between annotators and tasks. In medium-or small-scale tasks, such complexity may not be justified, unless the cost of adoption is minimal; in such cases, a simple thresholding or qualification task (HIT in MTurk) has often sufficed. Small tasks may opt to perform post-processing agreement calculations on a pilot evaluation, to better tune the expected final annotation quality, as is often done in traditional annotation. 3.3 Setup effort This dimension measures the effort in creating the annotation interface. Design should keep in mind the end objective of creating a dataset or learned model. Facets for this dimension involve both the worker and the practitioner.

Usability is viewed from the worker X  X  perspective. A higher level of usability requires more work on the part of the practitioner, but enhances workers X  effectiveness. An annotation task must exhibit a minimum level of usability to be able to collect data transparently, without hampering the worker. These require-ments are highest for the GWAP genre, as most games must be not only eminently usable (von Ahn 2006 ; von Ahn and Dabbish 2008b ) but also entertaining; games that fail on either dimension are simply not played. In contrast, the usability of MTurk tasks are quite uniform, by virtue of their centralization on Amazon. MTurk provides a standardized user interface to potential workers in finding HITs at virtually no cost to the practitioner. While individual HITs can be designed in inspired ways to attract workers, usability enhancements are not immediately visible to workers before starting a HIT, lessening the impact of this factor. Usability impacts WotC and traditional annotation on a case-by-case basis. We do note that WotC applications often require workers to register and log in, which can be a barrier to providing annotations. Allowing anonymous participation or delayed registration (i.e., after a few annotations are given) can mitigate this issue. A substantial annotation environment also aids usability; toolkits and raw HTML provide good support for text annotation, but provide less well-accepted methods for supporting area or free-form annotation.

In all cases, simplifying usability often decreases task completion times and improves overall satisfaction, yielding a synergistic relationship with participation and annotation efficiency. Difficult annotation tasks X  X n terms of lengthy instructions or complex tasks X  X ause dropout and discourage repetition, adversely affecting participation or success rate, as observed by Madnani et al. ( 2010 ), Koller et al. ( 2010 ) and others. Simple tasks do best for both motivators of entertainment (GWAP) and micropayments (MTurk). Complex tasks that can be decomposed into a series of simple tasks, when properly checkpointed between stages, also work well in these frameworks (Le et al. 2010 ; Siorpaes and Hepp 2008 ).

Ease of Task Setup is measured from the practitioner X  X  perspective, and reflects the ease in creating the annotation platform. This includes overhead for technical issues, including server and database maintenance, as well as the software driving the annotation user interface. Easiest to implement are traditional paper-based surveys, common in small-scale tasks. MTurk, with its centralized system, has streamlined the creation of generic surveys and annotation tasks (Sorokin and Forsyth 2008 ; Kittur et al. 2008 ). These can be fulfilled using standard web forms, but are not limited to such. MTurk offers tools and guidelines to help practitioners provide qualification tests and structure HITs appropriately. The MTurk API further allows the practitioner to monitor and manipulate task statistics in their own application.

Standalone web-applications and GWAPs, in contrast, are not easy to create X  X he practitioner must define annotation tasks and tools as well as maintain the framework. In many ways, GWAPs have the greatest cost in both facets: no publicly-available toolkits currently support GWAP creation in both usability/ playability (needed to attract participants (von Ahn 2005 )), nor in implementing its underlying support. WotC and web-based local manual annotations also have high costs, but online tools such as web survey hosts and editor plugins mitigate this somewhat. MTurk benefits from its scale, making it a competitive environment versus traditional one-off web-based annotation tasks. 3.4 Human participation* A larger underlying worker base drives more participation and labelling, and through repetitive annotation, data quality. This dimension assesses the scale of the potential workforce.
 Recognition measures how visible the annotation task is to the general public. While large-impact applications like Wikipedia exist, they are the exception. Using a popular portal as a bridge is eminently helpful for most applications. MTurk wins this dimension as it aggregates thousands of workers and tasks in a single website. However, in MTurk, the practitioner also needs to position the task to attract potential workers; new tasks are easy to find on the front page of MTurk, but quickly are replaced by other tasks. Other frameworks are recognized less; GWAP instances by von Ahn, a prime contributor, are grouped in a single website, but does not yet integrate GWAPs from other scholars. Because a worker base is needed for success, this dimension measures the public relation effort necessary to convert the members of the public into workers. Instances of smaller applications, in GWAPs, web-based or manual annotations depend critically on the recruiting techniques of the practitioner.
Worker Base measures the size and qualifications of the annotator base. The main potential worker base as well. As an upper bound, Wikipedia has over 150,000 active users, users that have edited a page within the last 30 days. 3 Among GWAPs, the largest worker base is the ESP Game, which had approximately 23,000 players (von Ahn and Dabbish 2008a ); in contrast, for NLP tasks, Phrase Detectives reports over 500 users (Chamberlain et al. 2009 ). For MTurk, there are more than 500,000 workers in over one hundred countries. 4 , 5 A large-scale worker base makes it possible to connect specialized workers to specific tasks. In the NLP domain, only large-scale crowdsourcing solutions like WotC and MTurk allow practitioners the possibility of obtaining data from under-resourced languages (Zesch et al. 2007 ; Loup and Ponterio 2006 ; Irvine and Klementiev 2010 ).

Two studies by Ipeirotis ( 2008 , 2010 ) showed that MTurk workers shared similar demographics of Internet users in general. Demographic studies on Wikipedia users also exist (Voss 2005 ), and show that due to its public nature, users self-select to edit articles, which may be an indication of the quality of its contributors. 3.5 Task character Complementary to the qualities of the workers is the nature of the annotation task. Here, the scale of the data to be annotated and its subject matter are important dimensions for consideration.

Data Complexity measures the size of the data to be annotated, both in terms of items to be annotated and the size of individual items, as well as its characteristics relevant to annotation. Depending on the task X  X  purpose, data may be used for a single study, used by multiple projects and communities, or be Web-scale datasets that are independently visible and useful to the general public. Large scale data requires more effort in planning, to be able to sustain long-term active participation and catch the public X  X  attention. Small datasets usually are better served by using some existing infrastructure to alleviate setup costs.

When individual item sizes are large or in a non-text medium, they may cause difficulties for the worker. In NLP, such has been the case of using audio data, either in terms of serving audio data to be annotated or asking individuals to provide audio sample recordings (Kunath and Weinberger 2010 ; Novotney and Callison-Burch 2010 ). Such specialized data requires careful implementation; otherwise worker participation may be adversely affected. However, once the data has been loaded for viewing, visual tasks are usually quick, while many monolingual NLP tasks that require reading and assimilating meaning take a longer period of time.

Specialization measures the specificity of knowledge that an annotation task requires. General human recognition or knowledge can be used for visual tasks or for reverse Turing tests  X  tests that aim to distinguish humans from automated robots (von Ahn et al. 2008 ). Many NLP annotations (e.g., part of speech tagging) may be acceptable for native speakers of the target language with minimal or no training; while often not acceptable for non-native speakers. The quality and type of annotation may need to consider the target audience in these specialized cases; for example, assessing chunk level dependencies may be plausible for native speakers and to create a parser model returning the same type of output, but the general public is likely not skilled enough to deliver quality Penn Treebank tagging or formal definitions for a dictionary. Specialized tasks such as translation, writing and editing are difficult to fill as they which require bilingual competency, and require a high level of concentration when possible, decomposing these into component tasks that are suitable for a wider audience will help, in order to lower the barrier of entry for annotation. Otherwise, a formal training phase, similar to that proposed by Chamberlain et al. ( 2008 ) can address manpower shortage, by allowing hands-on experience guide potential annotators, rather than require the reading of laborious training materials.

In manual annotation tasks, control over these factors is at the practitioner X  X  sole discretion; whereas in the current GWAP and WotC forms, such controls are largely non-existent. However, being community-driven and shaped by human social activities, large WotC initiatives often self-organize to employ active, trusted workers as editors that enforce may enforce specialization to a degree.

Being profit driven, MTurk allows some flexibility for the practitioner to introduce filters for the qualifications for workers. This also allows a worker to be more certain that their efforts will be paid (a practitioner may veto work, if she finds it unsatisfactory). Qualifications are often in terms of explicit skills or knowledge, but could be intrinsic qualities of the potential worker. Of particular interest to NLP tasks are a worker X  X  native and second languages as well as the geographic location, which can figure greatly in multilingual machine translation and speech processing tasks (e.g., Bloodgood and Callison-Burch 2010 ; Kunath and Weinberger 2010 ).
While most MTurk frameworks profess to have general workers without catering to specific skills, we note that some crowdsourcing sites have become hosts for specific communities of workers, especially programmers 6 and copyediting. 7 In such specialized MTurk frameworks, workers often have a public profile and usually want to maintain good standing and positive recommendation of their work to keep up appearances with other members in their community. 4 Recommendations These dimensions aid in the analysis of the efficacy of crowdsourcing efforts. Our analysis of published instances of crowdsourcing shows that each of the three crowdsourcing genres have some characteristic values and that each genre X  X  instances form clusters of ability. For each genre, we assign specific dimensions a  X  X  X ro X  X  or  X  X  X on X  X  value using our subjective opinion, and then make recommen-dations on suitable NLP tasks. Values are from the perspective of the practitioner, where a  X  X  X ro X  X  value indicates that the methodology is well-suited on this dimension. 4.1 Recommendation on GWAP  X  Pros: Fun, Usability  X  Cons: Ease of Task Setup, Recognition, Worker Base, Specialization Although several particular GWAP games have numerous players, most can benefit from more participation. As GWAPs are implemented by many different research labs and decentralized, recruiting workers-as-players for GWAPs is largely an one-off task, and (currently) difficult. Submitting GWAP to free game sites may help jump-start participation; practitioners could also find seed players through social networks or by recruiting players through MTurk [as was done in (Law et al. 2007 ) X  X  TagATune and (Ho et al. 2009 ) X  X  KissKissBan]. While they compete with normal on-line games for players, the added satisfaction of knowing that your game playing goes to help worthy cause may impact participation to some extent.

For GWAP to pay off, a large-scale annotation task is needed to offset the setup effort , as the overhead in GUI creation and the software infrastructure needed to maintain artifacts for encouraging fun are expensive. These include player profiles, high score listings, invitations to encourage a worker X  X  contacts to participate, and ranks for experienced players. A key missing ingredient to make GWAPs more competitive is the current absence of a free development platform that would cater to many of these mundane tasks. However, UI design and game-play still need to be done individually to give a game its own individuality and its players a sense of ownership.

NLP tasks need to appeal to a general audience to suit the GWAP genre, as the barrier to starting the task must be low and require only a short attention span to complete. We have seen examples of Coreference Annotation by Chamberlain et al. ( 2008 ), Chamberlain et al. ( 2009 ), Paraphrase Corpora Collection by Chklovski ( 2005 ) and Semantic Relations Collection by Vickrey et al. ( 2008 ) that have been moderately successful at this. As GWAPs are primarily motivated by fun, tasks should occasionally contain surprises. We believe that problems whose answers could change with different contextual information may be good candidates (e.g., Named Entity Resolution and Discourse Parsing ), where different contexts could change gameplay and scoring mechanisms.

To encourage a larger degree of participation, GWAPs in other fields have relied in part on the viral nature of getting others to participate. Social networking platforms could play a role in creating useful applications for NLP annotations. Short quizzes such as  X  X  How good is your English grammar/vocabulary?  X  X  could serve to compile statistics on common syntactic errors (for Machine Translation ) and vocabulary familiarity (for Readability Studies ).

GWAPs have been characterized as taking on three different game structures: output-agreement, input-agreement and inversion (von Ahn and Dabbish 2008a ). Output-agreement asks a player and their partner to agree on their produced outputs; input-agreement asks whether the two players received the same stimuli from the system (e.g., in Tag-a-Tune , whether the two players have received the same song). The inversion-problem scenario asks one player (the Guesser ) to reproduce the input of the second player (the Describer ) using the second player X  X  output. It is important to note that these strategies help to uphold annotation quality and can be applied to any annotation task, regardless of crowdsourcing genre.

In demonstrated GWAPs, we observe that inputs may be complex multimedia (such as pictures or songs) that are infeasible for a human to generate as an answer to a task. Outputs, on the other hand, can be quite simple, as they are to be generated by a player X  X .g., a descriptive phrase. This is how a majority of NLP tasks would best be cast as well. In such cases, the standard inversion problem is infeasible. An adapted inversion-problem task may be used instead, in which the task has the guesser choose among a confusion set. However, the input-and output-agreement problems are easily catered to such tasks. In many cases, we recommend using all three scenarios to encourage more styles of playing, which may lead to more entertainment and participation. 4.2 Recommendation on MTurk  X  Pros: Recognition, Ease of Task Setup, Profit, Specialization  X  Cons: Fun, Data Complexity  X  Caveat: Annotation Quality While Amazon X  X  Mechanical Turk is not the only example of the MTurk crowdsourcing framework, its prevalence in research and studies have made it a de facto standard for mechanized labour. MTurk X  X  hallmarks (low setup effort , large worker base , and controllable specialization of a task X  X  workforce) allow it to compete very well with traditional and web-based annotations for many tasks. Other MTurk frameworks that serve specific populations also exhibit these qualities to a lesser degree.

However, tasks requiring true experts or long training periods may not be well-served by such a workforce, and may force a practitioner to go up the continuum of pay to hire contractors at a higher pay rate. There are a growing number of companies  X  InforSense, 8 2PiRad 9 and ifiCLAIMS 10  X  X hat now serve this general outsourcing model. Whether the intended study X  X  funds allow an external party to be paid to broker this task is also a possible issue for public-funded research. Small one-off studies also may still be better served by paper-or web-based surveys.
This leads us to discuss a caveat concerning annotation quality. Being uniquely motivated by profit and a viable substantial source of income for certain demographics of workers, there is the financial incentive to cheat on tasks. Almost every annotation design in the MTurk framework needs controls to ensure annotation quality. This can be achieved in ways already discussed: screening workers using acceptance ratings thresholds, using multiple annotators with agreement threshold based on differing criteria, inserting known gold-standard questions to detect spam workers, and using other workers to rate the quality of initial worker annotation.

Cheating is an especially important factor in certain NLP tasks where freely available programs or services can simulate worker competencies. For example, workers asked to provide sentence translations may simulate competence by using services like Google Translate or translation software, defeating the purpose of the annotation task (Bloodgood and Callison-Burch 2010 ; Ambati and Vogel 2010 ). A rushed practitioner may want to pay higher rates for faster or better annotation, but needs to be aware that it incentivizes cheating (Le et al. 2010 ; Mason and Watts 2009 ). For cheating to have little incentive, doing the task properly and cheating must take close to the same level of effort.

A few companies X  X rowdflower, 11 Samasource, 12 and CloudCrowd 13  X  X ave created a trust layer over MTurk, by incorporating such safeguards into their system so practitioners can concentrate on their task specification. The interfaces of these companies make it easier to assign gold standard answers to tasks and to view and monitor analytics on a task. These services essentially add points in the MTurk service curve, trading monetary cost for implementation cost relief and annotation quality assurance.

With respect to NLP applications, POS Tagging and Parsing problems are short but tedious and difficult tasks perhaps requiring little specialization that could benefit from a MTurk instance. With MTurk X  X  qualification and use of repetition to achieve a base annotation threshold, difficult tasks requiring specialization or specific linguistic expertise are possible. Also possible are user evaluations of NLP system outputs designed for end users, of which we have recently seen in Summarization (Filatova 2009 ) and Machine Translation (Callison-Burch 2009 ). 4.3 Recommendation on WotC  X  Pros: Annotation Quality, Recognition, Altruism, Specialization, Data  X  Cons: Fun, Profit  X  Caveat: Ease of Task Setup While Wikipedia and Wiktionary have figured prominently in NLP research, the majority of these studies have studied how to utilize existing WotC resources rather than creating the annotations themselves. Examined from a use perspective, existing WotC instances are similar to other existing datasets and corpora X  X hey need to be filtered and adapted for use in a target task.

When we focus on WotC creation, we find a strong bias to compiling resources as opposed to annotations. This is because outputs of WotC applications should have direct relevance to its workers. We find WotC scores highly for Recognition and Specialization . This implies the annotation tasks in this category may be able to solve tasks that require intensive effort or expertise. To entice annotators to a task, they must feel the indirect benefit of contributing to community knowledge and learning. Many users of a WotC application learn of its existence while looking for information rather than hoping to contribute, which leads to the decreased enthusiasm for contribution. According to Huberman et al. ( 2009 ), the recognition and visibility of a project are paramount to a WotC application X  X  survival; ones that fail to sustain an active and growing user base can die off quickly, as workers can quickly sense their efforts are not rewarded.
As a result, a key factor in the success of a WotC instance is whether it can maintain and increase its workers X  enthusiasm. This determines the scale of annotation that can be supported. To maximize the chance of success, WotC instances should make the most of attention and competition among its worker base. In WotC, productivity exhibits a strong positive dependence on the attention the public have paid to the contribution or contributors. Moreover, contributors compare themselves to others when having low productivity and to themselves when exceeding a personal milestones. These findings suggest that WotC practitioners should make good use of community prestige, a term that we feel embodies both public attention and competitive attitude. This can be enabled by rich profiles that show how many users have benefited from a worker X  X  annotations, as well as displaying his contribution history and ranking among peers. A promising way is to build a social network among users involved in the same task. When users acclimate to the contribution system, get noticed by others, receive feedback on their contributions and form relationships with others, they may feel more ownership with the WotC resource and increase their participation.

Due to the fact that many users of a WotC application learn of its existence while looking for information rather than hoping to contribute, another way to increase the productivity is to convert as many passive users to active contributors as possible. Although passive users should not be given limited access to the WotC application, measures can be taken to entice them to be a contributor. Searches within the WotC resource for annotations that do not yet exist can prompt users to fill in the missing annotation; and by mining search logs, queries end up with such non-existent resources can be made bona fide tasks for active users.

Currently creating such tools for maintaining contributor prestige are one-off and costly, which will affect the setup effort. As with GWAPs, the creation of common toolkits or social network tools that could address these issues would greatly aid this genre of crowdsourcing.
 From a certain perspective, WotC NLP applications have existed for a while. Informal multilingual and domain-specific dictionaries and thesauri have been compiled by avid amateurs for many years, accepting emailed contributions. With Web 2.0, technology has eliminated the incorporation time for new contributions 14 wherever there may be public and community interest in a resource, a WotC application may be useful. Such annotation tasks should not be time-sensitive, but long-lived, as contributions to WotC often are over a long term, in proportion to the level of specialization and current size of the application X  X  existing annotation. Thus we feel Ontological and Lexical Resource Construction and Domain Specific Machine Translation may be suitable NLP tasks for WotC. 5 Discussion In developing our dimensions, we manually removed dimensions that overlapped, favoring orthogonal axes. For example, we omitted annotation efficiency as it is largely determined by the dimensions of human participation and task character. Even after removing overlaps, there is still considerable correlation. This gives us an opportunity to chart the space that existing crowdsourcing methods form and to uncover generalizations about each genre. Figure 1 plots several of the dimensions against each other for the 52 surveyed crowdsourcing instances. In all of our dimensions, higher scores are better for the practitioner; the plots X  upper right hand corner (1.0, 1.0) represents ideal conditions.

In Fig. 1 a, we see that usability and annotation quality are highly correlated, which we see as a causal relationship. Practitioners need to ensure that the tasks are simple, while ensuring that the annotation framework is easy-to-use and has validation to prevent common errors. The other three plots highlight the particular strengths of the different crowdsourcing methods. In Fig. 1 b, GWAP and WotC tasks have stayed true to the layman-as-crowd property, largely avoiding the need for expertise. MTurk is the exception where qualification tests allow a practitioner to recruit specialists, who can perform difficult tasks. Difficult tasks do correlate with a smaller worker base in general, but in the case of well-known WotC instances, the prestige of contribution may still attract a large worker base to contribute their expert opinion. In both of the bottom plots, we can see the distinct advantage that MTurk has in offering a standardized, centralized framework in distinct advantage that MTurk has in offering a standardized, centralized framework in decreasing the cost for the practitioner. In addition, in Fig. 1 c we also see that (successful) WotC applications can annotate or build as large resources as compared to GWAPs. In Fig. 1 d our study also yields a note of concern: there is no obvious negative correlation between Ease of Task Setup and Annotation Quality. Practitioners need to be aware that costly setup does not guarantee high-quality annotation; on the contrary, a costly implementation may be an artifact of difficult tasks or annotation media, which may result in poor annotation.

Aside from the above plots, we highlight a few other observations from our study. From our judgements, we see that MTurk currently beats out both GWAP and WotC forms in terms of setup costs. Practitioners looking for lightweight, small, one-off annotation tasks should consider MTurk as an alternative to traditional annotation recruitment tasks. Setup costs for the latter two framework can be certainly enhanced with the introduction of better toolkits.

Standardization in MTurk allows for low setup costs, but makes the range of tasks a bit more limiting. Where the MTurk really does win for annotators is in its Worker Base, a factor that lets annotation tasks complete quickly. This favors rapid prototyping: to design a series of pilot annotation tasks before running the final, well-designed and calibrated task at a large scale.

Worker availability and sheer numbers in MTurk allow practitioners to get a large number of items annotated but by non-experts. Can many non-experts can reach the quality level of experts? The answer to this in the MTurk literature is mixed, but most studies have concluded  X  X  X es X  X . After efforts are made to filter out cheaters and obviously noisy annotations, the use of smart, multi-annotator merging strategy such as ROVER (Fiscus 1997 ) can deliver performance close to or exceeding intra-expert agreement levels (Lawson et al. 2010 ; Mellebeek et al. 2010 ).

GWAPs require a large initial effort to create, especially in GUI and game strategy design. Currently, while interesting, this form of crowdsourcing still requires more work for practitioners and we do not consider it to be fully mature. The number of participants also may not be adequate for some tasks, which may be due to their lack of visibility, but also because the games are not as entertaining as compared to their professionally-designed kin. Promotion of GWAPs through social networks (Kuo et al. 2009 ), or via mobile platforms may be viable solutions. Finally, the primary constraints for a practitioner are often time and money. Strong time constraints make both pure GWAP and WotC forms impractical, as there is little direct influence a practitioner can leverage to increase participation. Monetary budget can be used to incentivize these types of annotation tasks, either directly or indirectly by charity or lottery.
 6 Conclusions and outlook We have examined crowdsourcing in its wider meaning, as a vehicle for obtaining annotations from the general public. We have paid particular attention towards understanding these tasks from the perspective of the practitioner who needs to get data annotated, with special attention on natural language processing (NLP) tasks. In particular, we assessed many published crowdsourcing instances in this article and assigned subjective scores along dimensions of import to the practitioner. While the scores are only indicative, 15 it has uncovered particular strengths and weaknesses of the different crowdsourcing methodologies.

In the current state of affairs, Amazon X  X  Mechanical Turk and others of its ilk are strong contenders in performing one-off annotation tasks as well as complex tasks that can be easily decomposed, thanks to their unified presentation, tools and large, centralized worker base. Games with a Purpose (GWAP) and Wisdom of the Crowds (WotC) applications also work for niche applications where it is possible for the annotation task to be entertaining or useful to the community as a whole.
The space of crowdsourcing is no doubt evolving, and our study points out that each framework is distinct in character. A clear trend in the development of crowdsourcing is that the space of possible annotation platforms is expanding to include many more points that allow practitioners to trade off costs in one dimension for another. Such hybrid methods may address weaknesses of individual frameworks while synergistically retaining the advantages. The literature already documents instances where the dimensions of annotation quality, quantity and cost can be traded off. As the community adapts work from other areas where human judgment has played a more central role X  X .g., trust models in collaborative filtering (O X  X onovan and Smyth 2005 ; Massa and Avesani 2007 ) X  X e expect formal models of user annotation to supplant the heuristic methods currently being reported.
Some forms of crowdsourcing have weaknesses that we feel could be addressed in the near future. For example, we believe that a GWAP toolkit may alleviate the current prohibitive cost of entry to the genre. New mediums of interchange and annotation have already started that do not use the static web as the vehicle: using the mobile phone (Eagle 2009 ) and the web browser itself X  X n the form of a browser extension (Griesi et al. 2007 ) X  X re platforms to be utilized.

The space of possible configurations also applies to motivation. Current crowdsourcing frameworks, as we have defined it, largely differentiate by a single motivational factor, but that does not prevent future applications from combining them. A few instances of crowdsourcing have incentivized users by combining two of the three motivational dimensions of fun, profit and altruism. We note that NLP has a unique attribute that lends for the motivation factor of self-enrichment : language learning. Although it is hard for language learner to correctly annotate text in a language they are not native speaker of, novel methods may assist such learners in providing useful annotation or translation. For example, instead of demanding full-text translations or annotations, candidate answers provided by other users or machine translation tools can be improved by a worker who may be a language learner.

The above observation leads us to a conclusion that machine systems and humans can work synergistically on problem areas where systems have general competen-cies in coverage but where performance is lacking in specific sub-portions of the task. Similar to active learning, where data may be abundant but labels are scarce or expensive to obtain (Settles 2009 ), tighter integration between learning and annotation will lead to models where annotation data is specifically chosen to address weaknesses in the learned model. Crowdsourced work explicitly aimed at helping develop machine agents in the guise of the Semantic Web is also beginning to take shape Siorpaes and Hepp ( 2008 ). In the translation scenario above, identifying problematic areas for translation systems could be done by crowdsour-ced inspection of translation output. Reinforcing examples from these areas can then be selected and annotated to fix such errors. Research in this area is in its nascent stage but both toolkits for specific application areas, and integration of crowd-sourcing directly into statistical learning framework seems promising (Chang 2010 ; Quinn et al. 2010 ).

We note in closing that Web 2.0 made the web social, connecting people with people. Current crowdsourcing frameworks play along this line, connecting workers to practitioners. Akkaya et al. ( 2010 ) shows that MTurk workers are anonymous X  coming and going, generally not learning nor contribution beyond their atomic interaction with the tasks. Future crowdsourcing models are also likely to connect workers to workers and practitioners to practitioners, incorporating more robust reputation models. We feel that these social aspects will feature as the focus in the the next generation of crowdsourcing frameworks.
 References
