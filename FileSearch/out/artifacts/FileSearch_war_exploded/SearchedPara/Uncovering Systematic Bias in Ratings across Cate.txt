 Recommender systems are routinely equipped with stan-dardized taxonomy that associates each item with one or more categories or genres. Although such information does not directly imply the quality of an item, the distribution of ratings vary greatly across categories, e.g. animation movies may generally receive higher ratings than action movies. While it is a natural outcome given the diversity and hetero-geneity of both users and items, it makes directly aggregated ratings, which are commonly used to guide users' choice by re ecting the overall quality of an item, incomparable across categories and hence prone to fairness and diversity issues. This paper aims to uncover and calibrate systematic category-wise biases for discrete-valued ratings. We propose a novel Bayesian multiplicative probit model that treats the in ation or de ation of mean rating for a combination of cat-egories as multiplicatively contributed from category-speci c parameters. The posterior distribution of those parameters, as inferred from data, can capture the bias for all possible combinations of categories, thus enabling statistically effi-cient estimation and principled rating calibration. H.2.8 [ Database Applications ]: Data Mining Algorithms, Human Factors Bayesian model; category; genre; taxonomy; bias
Recent studies in recommender systems have primarily focused on personalization, where different sets of items are recommended to users with different tastes and preferences [1]. Yet, a related but seemingly simple task, namely how to ag-gregate ratings to re ect the overall quality of an item, has c  X  been somewhat overlooked [4]. As an indicator believed to re ect the population level perception of an item, aggregated ratings, such as the rating or number of stars accompanying each rated movie or product appearing on IMDB and Ama-zon.com, are commonly adopted to guide users' choice when they browse a variety of items [2, 6]. The impact of these overall evaluations are substantial given that they are part of the rst impression when a user browses an unseen item. It has been shown that the average ratings shown previously to users may in uence their own ratings through a\social in-uence bias", namely inducing users to bias their assessment towards the average ratings given by the bigger community [2, 6]. Aggregated ratings from one platform are also use-ful for personalized recommendation in another platform as an external source of information [9]. For example, a movie recommender may augment its data with aggregated ratings from IMDB to provide more accurate predictions.

Direct or weighted averaging are most commonly used methods to aggregate ratings, but their truthfulness and robustness are shown to be problematic [4]. Alternative ag-gregation strategies have been recently proposed, including using median or mode for aggregation to guard against ma-licious users or users with extreme opinions [4], and other methods based on reputation of users [7, 3].

These issues with aggregated ratings arise from the in-creased heterogeneity of users , with malicious attackers as an extreme case. In this paper, we instead examine the is-sue related to the increased diversity of items , which to our knowledge has received little attention. In particular, we focus on diversity structured by a prede ned taxonomy sys-tem, such as genres for movies and music, and\departments" and\sub-departments"for products on Amazon. We ask the question | are averaged ratings comparable across genres (and combinations of genres)? If not, can we extract the sys-tematic biases and calibrate the ratings accordingly so that they are comparable? Note that we will use\categories"and \genres" interchangeably throughout the paper.

Although the comparison of\overall quality"between prod-ucts from different categories may not always seem well-posed, it is a daily task faced by users. For example, they need to decide between spending time and/or money on a comedy movie versus a documentary. Besides, the websites that run a marketplace for third party sellers (e.g. eBay) face the same task when they judge the performance of sell-ers based on ratings given to different types of products or services. In both cases, adopting aggregated ratings with-out removing systematic category-wise biases may impair the fairness and diversity of the ecosystem.
To motivate our study, in this section, we show how the av-erage rating levels vary across different combinations of cate-gories with empirical observations. We perform our analysis on the MovieLens 1M dataset 1 , which contains about 1 mil-lion ratings from 6,000 users on 4,000 movies. All ratings are on a 1 5 discrete scale. Each movie is categorized into at least one genre from a list of 18 basic genres . Typically, a movie is assigned to 1 3 genres. Fi gure 1: Top: the average rating for each genre; Bottom: the average ratings for the top 20 combi-nations of genres.

Figure 1 shows the average ratings for all 18 genres and for the 20 combinations of genres having the largest number of rated movies in the dataset. It is clear that the average rat-ings do not align across genres, as well as their combinations. Given the large number of ratings, most of the differences are statistically signi cant: among all the pairs of genre combi-nations with at least 5 movies in each, 92.4% of them are found to reject the equal-mean null hypothesis with a t-test at a signi cance level of 0 : 05. The gure also shows that a combination of genres being underrated/overrated is related to one or more of its component genres being marginally underrated/overrated.

Besides, given 18 basic genres, there are an exponential number of possible combinations. We also nd the num-ber of ratings vary greatly among combinations, as shown in Figure 2. Similar observation has been reported for the Net ix dataset [10].
 Fi gure 2: The number of ratings available (plotted in decreasing order) vary greatly across combinations of genres. ht tp://grouplens.org/datasets/movielens/
We brie y summarize the challenges implied by empirical studies before we move on to the modeling part. The esti-mation for systematic bias across genres should address the following issues. 1. Jointly estimating the bias for an exponential number 2. Efficiently borrowing information across combinations 3. Good interpretability that relates a combination to its
Suppose that we have N items and each item i receives n i ratings, which are denoted by R ij ( j = 1 ; 2 ; ; n i ). Note that f R ij g for the same j do not necessarily correspond to the same user. Each rating is on a discrete scale with K levels, i.e. R ij 2 f 1 ; 2 ; ; K g . Suppose we have T basic categories/genres and every item is associated with a com-bination of them. The categorization for item i is encoded by f G it g ( t = 1 ; 2 ; ; T ), where each component is binary: G it = 1 indicates that the item is within category t and G it = 0 otherwise. Our task is to model the distribution of the rating for item i depending on its categorization, namely P ( R ij jf G it g ).

The probit model is widely used for describing ratings and other ordered categorical data [5]. The probit model assumes a continuous latent variable Z ij underlying every discrete-valued R ij . By introducing thresholds 1 = g 0 &lt; g 1 &lt; &lt; g K 1 &lt; g K = + 1 that partition the real line into non-overlapping intervals, R ij is recovered by looking at which interval Z ij falls into, i.e.
 where we use r ( ) to denote the transform from the latent variable to the observed data.

Commonly, the latent Z ij is set to be a Gaussian random variable with mean value linearly related to the features, such as Z ij jf G it g iid N ( i ; 1) with i = is the coefficient that measures the additive effect on the average rating contributed from category t . However, we nd this model unsatisfactory in interpretability. For exam-ple, it would be of interest to know how much higher the users tend to rate for animations. Yet, a coefficient of value 0 : 18, for example, is not so straightforward to interpret.
In the Bayesian multiplicative probit model, Z ij is still a Gaussian latent variable associated with every observed dependence is characterized by modulating i and i ac-cording to categorization given by f G it g . For each cate-gory t = 1 ; 2 ; ; T , we de ne t and t , both being posi-tive parameters around 1, as multiplicative factors adjust-ing the mean and the variance respectively. We have the mean for item i as the baseline mean 0 multiplied by in-ation/de ation factors contributed from the categories it b elongs to; the variance is modeled similarly to allow for bigger exibility: where 2 0 is xed to the empirical variance of f R ij g .
We put a conjugate Gaussian prior on the baseline mean centering around the empirical mean of all ratings, given by We explicitly align the center and spread for Z ij and R ij to increase the interpretability of f t g , so that the in a-tion/de ation of ratings from the t -th category can be di-rectly re ected by t .

The category-speci c parameters f t g and f t g should take positive values centered around 1. We put unit-mean conjugate gamma priors on f t g and truncated conjugate Gaussian priors on f t g , namely ( t j ) / N ( t j 1 ; 2 ) 1 ( t &gt; 0) ( t = 1 ; 2 ; ; T ) : (4) The priors for thresholds f g k g are chosen to be uninforma-tive subject to the ordering constraint, given by
The posterior distribution of the parameters can be ob-tained with Markov Chain Monte Carlo (MCMC). Algo-rithm 1 illustrates a Gibbs sampler for drawing S posterior samples: each round, one parameter is sampled from its full conditional given the latest sample values of all other pa-rameters. The thresholds f g k g are updated directly on the observed data f R ij g by integrating out the latent variables f
Z ij g to improve mixing [5].
To get the calibrated ratings, we can impute the latent variables Z ij from the posterior distribution, rescale its mean value by those f t g involved and nally threshold the rescaled latent variable to get the calibrated rating. To form a single calibrated rating R  X  ij corresponding to each raw rating R we use the posterior mean as a point estimate, i.e. (1) and it depends on the thresholds f g k g . The calibrated ratings given above can be easily approximated with MCMC samples for f Z ij g , f t g and f g k g .
In all the following experiments, the hyperparameters are chosen as = 0 : 05 ; = 0 : 1 and a = 20. We performed the posterior inference using an HMC-based sampler imple-mented with Stan [8]. All the results below are obtained from 10,000 MCMC samples on the MovieLens dataset af-ter rst discarding 10,000 burn-in samples.
Admittedly, we are faced with an obvious obstacle in eval-uation | it is impossible to obtain a \ground truth" rating that consistently maps an item to its \intrinsic quality" re-gardless of categories. Instead, since we explicitly model the way systematic biases distribute, we evaluate our model on its predictive ability , namely how accurately it captures the distribution of ratings corresponding to different combina-tions of categories from data.

Again, the \true" distributions are missing here. There-fore, we compare the prediction of our model tted with a tiny subset to empirical histograms (an inefficient but con-sistent estimator) from the full dataset, assuming that the latter is close enough to the truth for those combinations with large sample size. Our posterior is inferred from a randomly sampled training set consisting of 10,000 ratings, which accounts for 1% in size of the full dataset.
We adopt the KL-divergence to measure the difference be-tween two distributions. Figure 3 plots the metric  X  KL = are the empirical histograms from the full set and train-ing set respectively) for each combination of genres (with at least 5 movies) against the number of samples available in the training set. A bigger positive  X  KL suggests that the predicted distribution is closer to the true distribution compared to the empirical histogram in the training set. Fi gure 3:  X  KL for combination of genres with more than 5 movies in the dataset.
 T able 1: Most heavily under/overrated movies and their aggregated ratings before/after calibration.
The posterior distributions estimated from the MCMC samples for the mean adjusting parameters f t g are plotted in Figure 4. The most overrated genre is \Film Noir", then followed by \Animation", while the most underrated genre is \Fantasy", then followed by \Childrens". t for either the most overrated or the most underrated genre amounts to 10% above or below 1, which roughly means an in ation or de ation of involved ratings by 10%. Therefore, the system-atic bias is not negligible if we use the raw ratings without calibration. We also showcase two most underrated and two most overrated movies in Table 1, along with their aggre-gated ratings before/after calibration. A lgorithm 1 MCMC inference algorithm 1: Rand omly initialize parameters. 2: for each l = 1 ; 2 ; ;S do 8: end for Fi gure 4: The posterior distribution of the category-speci c mean adjusting parameters f t g for all the genres in the MovieLens dataset (dashed line=1).

We also nd these biases highly correlated in the poste-rior, as shown in Figure 5 for a few genres. This justi es the usage of Bayesian models despite its higher computa-tional cost, because merely point estimators of f t g can-not correctly calibrate combinations of genres by account-ing for their correlations. For example, because \Childrens" and\Animation"are negatively correlated, corrections based on the multiplication of their point estimators will tend to \over-correct".
 Fi gure 5: The category-speci c mean adjusting pa-rameters f t g are highly correlated in the posterior.
We present a Bayesian multiplicative probit model to un-cover and calibrate category-wise bias in ratings, by effi-ciently borrowing information across a vast number of com-binations of categories/genres. Future directions include modeling bias in a hierarchical taxonomy and comparing the Bayesian model with matrix-factorization-based methods.
This work was partly supported by grant NIH R01-ES017436 from National Institute of Environmental Health Sciences. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] D. Cosley, S. K. Lam, I. Albert, J. A. Konstan, and [3] F. Garcin, B. Faltings, and R. Jurca. Aggregating [4] F. Garcin, B. Faltings, R. Jurca, and N. Joswig. [5] V. E. Johnson and J. H. Albert. Ordinal data [6] S. Krishnan, J. Patel, M. J. Franklin, and [7] B. Mobasher, R. Burke, R. Bhaumik, and [8] Stan Development Team. Stan: A C++ library for [9] A. Umyarov and A. Tuzhilin. Improving rating [10] S. Vargas, L. Baltrunas, A. Karatzoglou, and
