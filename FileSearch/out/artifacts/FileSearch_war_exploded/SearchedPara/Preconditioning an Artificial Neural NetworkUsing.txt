 Logistic Regression (LR) is a state-of-the-art machine learning classifier and is converges more rapidly when each axis is scaled by the log of the naive Bayes tive parameterization with both naive Bayes parameters (learned generatively) and LR parameters (learned discriminatively). The resulting parameterization of LR is known as WANBIA C CLL and has been shown to be effective for both online and batch gradient based optimization for logistic regression mizes the conditional log-likelihood (CLL) of the data given the model. We conjecture that optimizing the mean square error (MSE) should lead to more a linear model optimizing MSE is an Artificial Neural Network (ANN) with no hidden layer (the structure constitutes only an input layer with multiple nodes and an output layer with multiple nodes).
 This paper investigates the performance of linear classification models that optimize MSE relative to those that optimize CLL and whether NB regulariza-tion is as effective with the MSE objective function as it is with CLL. One can view WANBIA C CLL from two perspectives. 1. From the NB perspective, the parameters learned with discriminative training are only alleviating NB X  X  independence assumption. It is irrelevant whether the weights are optimized by the CLL or by the MSE objective function. 2. From the LR perspective, WANBIA C CLL introduces NB weights that precon-dition the search space. For CLL, which is a convex objective function, this leads to faster convergence. A natural question is: will the same trend hold for other objective functions which are not convex, such as MSE? The contributions of this paper are two-fold: 1. We show that NB preconditioning is applicable and equally useful for learning a linear classification model optimizing the MSE objective function. 2. Optimizing MSE leads to a lower bias classifier than LR optimizing CLL. This leads to lower 0 X 1 loss and RMSE on big datasets.
 The rest of this paper is organized as follows. We discuss LR and WANBIA in Sect. 2 . We will derive NB preconditioning of a linear classification model Sect. 5 with some pointers to future work. objective function: where N is the number of data points. Note, we are constraining ourselves to categorical attributes and multi-class problems only. We write P features and multiple classes as: defined as: One can add weights to NB to alleviate the attribute independence assumption, resulting in the WANBIA C CLL formulation, that can be written as: When conditional log likelihood (CLL) is maximized for LR and weighted NB using Eqs. 2 and 3 respectively, we get an equivalence such that  X  models. While it might seem less efficient to use WANBIA C the number of parameters of LR, the probability estimates are learned very efficiently using maximum likelihood estimation, and provide useful information In this section, we will derive a variant of WANBIA C CLL minimize MSE. But before doing that, we will first derive a variant of LR using the MSE objective function  X  an ANN with no hidden layer.
 ANN. Instead of optimizing the objective function in Eq. 1 , one can optimize the following MSE objective function: MSE(  X  )= 1 N N i =1  X  P( c | x ( i ) )) 2 , where y is the true label and C = | equation slightly: holds and 0 otherwise. Note that unlike the CLL objective function in Eq. 1 , stuck in local minimum and, therefore, local minimum avoidance techniques may be required. We will show in Sect. 4 that in practice one can obtain good results with simple gradient descent based (such as quasi-Newton) optimization algorithms without requiring specific mechanisms to avoid local minima. In the following, we will drop the superscript ( j ) for simplicity. Optimizing Eq. 4 requires us to compute its derivative with respect the parameters  X  .We have the following: where,  X  P( c | x )  X  X  where,  X  ( x i ) is an indicator function if value of x i which we are differentiating. Plugging in Eq. 5 , we get:  X  MSE( w ) Note, the gradients with respect to the class parameters can be calculated sim-propagation training algorithm. In the following, we will formulate WANBIA with MSE objective function.
 WANBIA C MSE . Given Eq. 3 , assuming a Dirichlet prior, a MAP estimate of P( y ) is  X  with class y and N is the total number of instances, and m is the smoothing parameter. We will set m = 1 in this work. Similarly, a MAP estimate of P( x is  X  c which equals: dataset with class y and attribute values x i . Now, we have: Using the above equation, let us optimize the MSE objective function by taking gradients with respect to the parameters w . We write: where w k,i,x i denotes parameter associated with attribute i taking value x and plug it in Eq. 8 : Comparing Eqs. 7 and 9 , the following holds: This shows that when optimizing MSE, just like CLL, naive Bayes precondition-log of the NB probability estimates. Such scaling leads to faster convergence, as is shown in the next section. In this section, we compare the performance of a linear model optimized with the MSE objective function with and without NB preconditioning in terms of 0 X 1 loss, RMSE, bias, variance, training time and the number of iterations it takes each algorithm to converge on 73 natural domains from the UCI repos-itory (Table 1 ). We will also compare performance with LR and WANBIA optimized with the CLL objective function.
 the repeated cross-validation bias-variance estimation method proposed by [ 8 ]. The reason for performing bias/variance estimation is that it provides insights into how the learning algorithm will perform with varying amount of data. We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data [ 9 ]. The experiments are conducted on the datasets described in Table 1 . There with instances between 1000 and 10000, and 12 datasets with more than 10000 instances. The datasets with more than 10000 are shown in bold font in Table 1 . tion. We report Win-Draw-Loss (W-D-L) results when comparing the 0 X 1 loss, RMSE, bias and variance of two models. A two-tail binomial sign test is used p  X  0 . 05.
 constitutes all the datasets. The category is denoted by All second category constitutes only datasets with more than 10000 instances. This is denoted by Big in the results When comparing average results across Big datasets, we normalize the results with respect to one of the comparative technique and present the geometric mean.
 attribute value and taken into account exactly like other values. optimization 2 .
 with or without NB preconditioning as WANBIA C MSE and ANN respectively. 4.1 MSE Vs. CLL A win-draw-loss (W-D-L) comparison of bias, variance, 0 X 1 loss and RMSE of WANBIA C CLL and LR versus WANBIA C MSE and ANN is given Table 2 .Itcan be seen that WANBIA C MSE achieves significantly lower bias than WANBIA whereas ANN has lower bias than LR but this difference does not achieve sta-tistical significance. Both WANBIA C MSE and ANN exhibit higher variance, but both WANBIA C MSE and ANN are well suited for bigger datasets for which lower bias is preferable [ 14 ]. This is also evident from Table 2 where WANBIA has significantly lower 0 X 1 loss than WANBIA C CLL on Big the ANN results (with 9 wins, 1 draw and 2 losses), though not significantly different, are better than LR.
 WANBIA C MSE and ANN are lower-bias and higher-variance models as compared to WANBIA C CLL and LR. The superior performance of WANBIA comes at an extra cost. A comparison of the training and classification time of WANBIA C CLL and WANBIA C MSE is shown in Fig. 1 (e) and (f) respectively. It can be seen that optimizing the MSE objective function, though low biased, is a magnitude of order slower than optimizing the CLL objective function. 4.2 WANBIA C MSE Vs. ANN Now that we have established that optimizing the MSE for LR leads to a lower bias model than that by CLL, in this section, we will compare WANBIA ANN to see the effects of scaling and whether NB preconditioning is as effective with the MSE as with the CLL objective function. We compare the scatter of 0 X 1 loss and RMSE values in Figs. 2 and 3 respectively. It can be seen that both parameterizations lead to a similar scatter of 0 X 1 loss and RMSE. This suggests the equivalence of two models (same model, different parameterizations). The training time and number of iterations to convergence for ANN and WANBIA C MSE is shown in Figs. 4 and 5 respectively. It can be seen that WANBIA C MSE greatly improves the training time of ANN. Note, the plots are on the log scale. It can be seen that WANBIA C MSE on some datasets is an order WANBIA C MSE to converge are an order of magnitude less than for ANN. Finally, let us have a look at the convergence plots of ANN and WANBIA in Fig. 6 on some sample datasets. The variation in mean-square-error is plot-ted with varying number of iterations until convergence. It can be seen that WANBIA C MSE has a much better convergence profile than ANN. It is not only converging in far fewer iterations but asymptoting far more quickly than ANN. This is extremely desirable when learning from few passes through the data. 4.3 WANBIA C MSE Vs. Random Forest In Table 3 , we compare the performance of WANBIA C MSE with Random Forest. It can be seen that though not significantly better, bias of WANBIA than that of Random Forest. The variance of RF is slightly lower than that of WANBIA C MSE . On bigger datasets, RF has lower error than WANBIA slightly more often than WANBIA C MSE (winning on seven and losing on five It can be seen that WANBIA C MSE is an order of magnitude slower than RF on faster than Random Forest.
 In this paper, we showed that a linear classifier optimizing MSE has lower bias than vanilla LR optimizing CLL. We also showed that NB preconditioning, which is very effective for LR, is equally effective for a linear model optimized with MSE. We showed that NB preconditioning can speed-up convergence by many classification of a linear classifier optimized with MSE is competitive to state-of-the-art Random Forest classifier with an added advantage of faster training time. There are many interesting directions following from this work:  X  This paper shows that NB preconditioning is effective for an ANN with no hidden layers. It will be interesting to formulate similar preconditioning for ANNs with hidden layers. WANBIA C CLL provides scaling for the nodes in the input layer, however, for nodes in the hidden layer, what weights one should use is an open question that needs investigation.  X  It will be interesting to run WANBIA C MSE with MSE with stochastic gradient descent (SGD) on very large datasets and compare the performance with WANBIA C CLL . We anticipate that WANBIA C MSE will lead to lower error in fewer iterations.

