 The average of the customer ratings on a product, which we call reputation , is one of the key factors in online pur-chasing decision of a product. There is, however, no guar-antee in the trustworthiness of the reputation since it can be manipulated rather easily. In this paper, we de ne false reputation as the problem of the reputation to be manipu-lated by unfair ratings, and design a general framework that provides trustable reputation. For this purpose, we propose TRUEREPUTATION , an algorithm that iteratively adjusts the reputation based on the con dence of customer ratings. H.2.8 [ DATABASE MANAGEMENT ]: Database appli-cations| Data mining Trust, Robustness, False Reputation, Unfair Ratings
In online shopping malls, prior buyers share their experi-ences on their purchased products with other potential buy-ers via evaluation. The most common is to express the level of satisfaction through a rating system. The overall level of buyers' satisfaction on a product is then quanti ed as the score aggregated over all the buyers' ratings and shown to potential buyers. In this paper, we call this aggregated score for the product as reputation . The reputation of a product plays a fairly important role as a guide to potential buy-ers and signi cantly in uences the nal decision of potential buyers to purchase the product [5]. \Is the reputation on a product really trustable to the pub-lic?"
The reputation is an evaluation score on a product ob-tained through collective intelligence, i.e., the collaboration of many individuals. The trustworthiness of the reputation can be achieved when a large number of buyers take part in ratings with honesty [2].
 Corre sponding author
If some users intentionally give unfair ratings to a product especially when few users have participated, the reputation of the product could be easily manipulated. In this paper, we de ne false reputation as the problem of the reputation to be manipulated by unfair ratings. For example, in case of a newly-launched product, its company may hire people in the early stage to give unfairly high ratings to the product for its promotion. False reputation occurs in this case and adversely affects the decision making of potential buyers on the product.

In this paper, we propose a general framework that solves false reputation in rating systems. Note that the most nat-ural and simplest way of aggregating the ratings is to use the average (i.e., to assign the same weight to each rat-ing). However, this may result in false reputation in case abusers take part in ratings. Instead of simple arithmetic mean, therefore, we evaluate the con dence of each rating and adjust the overall reputation based on the con dence of each rating. To determine the con dence of a rating, we exploit three key factors: activity , objectivity , and consis-tency , which are known to be in social science suitable for determining the con dence [6].

The con dence of a rating is calculated based on two scores, the user's activity and the user's objectivity , and then penalized based on its consistency . The user's activity is quanti ed by the number of his/her ratings on all prod-ucts. The objectivity of the rating on a particular product is measured by the deviation from the mean of the ratings on the product. The objectivity of the user is measured by the normalized average of the objectivities of the ratings by that user. The consistency of the user's rating is computed based on how much the objectivity of the rating is consistent with those of his/her other ratings. If a rating deviates signi -cantly from the user's normal behavior, we call it abnormal rating and penalize it by assigning the low consensus score to the rating when computing its con dence.

Note that the objectivity of a rating can be calculated us-ing the standard score which indicates the deviation of `the rating' from `the reputation' on the product. The difficulty in computing this value lies in the fact that the reputation itself is based on the sum of the ratings weighted by the con-dence after the objectivity of each rating is determined. In other words, the reputation and the con dence of a rating converge toward each other, so we propose TRUEREPUTA-TION, an iterative method to compute them. This idea of mutual reinforcement is similarly employed in other appli-cations [1, 7, 8].
For the computation of the con dence of a rating, we as-sign each user the activity score and the objectivity score, and compute the consensus score for each rating from that user based on the consistency of the user's rating. The nota-tions used in the proposed algorithm, TRUEREPUTATION , are presented in Table 1.

The activity of user u , denoted by a u , is quanti ed by the frequency of his/her ratings. Because the number of rat-ings of each user excessively varies, we derive the normalized version of j R u j by applying a transformation function as follows: where
The function in Equation 1 is the sigmoid function for normalization of j R u j to keep the returned value in the range of [0, 1]. Parameters ( 2 Z ) and ( 2 Z ) determine the slope and adjust the midpoint of the curve of respec-tively. In order that should be closed to 1 when j R u j very large, should be a positive number. In order to dis-tribute j R u j evenly in the range of [0, 1], we use = 0 : 02, determined by our extensive experiments. We assign the average number of ratings per user.
The user's objectivity is measured by the normalized av-erage of the objectivities of the ratings by that user. The objectivity of a rating r for item m ( r 2 R m ) is computed based on how much it deviates from the aggregated score of the other buyers' ratings on the same item (i.e., reputa-tion). The reputation is denoted by r m , and the standard deviation is denoted by s m . Both r m and s m are used in the calculation of the objectivity of a rating. The objectivity of a rating, denoted by o r , is calculated as follows:
Th e user's objectivity, denoted by o u , is calculated by the average of the objectivities of the ratings by that user. As it gets closer to 0, we can say that the user is more objective. We de ne o u as follows:
In order to use o u in the calculation of the con dence of a rating, we need the normalized version of o u to keep the value in the range of [0, 1]. The transformation function for normalizing o u is de ned as follows: o u is normalized by the function with  X  and  X  param-eters in Equation 5. Let  X  be a negative number in order that should be close to 1 when o u gets closer to 0. We set to be -2.5 for the same reason to that of Equation 1. We assign  X  the average of all users' objectivities.
Analyzing the consistency of a rating with respect to his/her other ratings, we nd an abnormal rating r ( r 2 R u ) which deviates from the user's normal behavior. The abnormal rating r will be given a low consensus score c r , while the rating within the user's normal behavior will be assigned a high consensus score. We use the boxplot to analyze the consistency of the user's rating.

First, we sort the objectivities of the ratings ( o r ) of each user in ascending order. Then we select three values: rst quartile (Q1), median (Q2), and third quartile (Q3), respec-tively. In the boxplot analysis, the size of a range between Q3 and Q1 is called the inter-quartile range (IQR), and the values that are less than Q 1 (1 : 5 IQR ) or greater than Q 3+(1 : 5 IQR ) are considered to be outliers. In this paper, we consider the value within IQR as the normal behavior, assigning the consensus score of 1, and consider the outliers as the abnormal behavior, assigning the consensus score of 0. In other words, the rating inside IQR is considered to be consistent with user's other ratings. The rating outside IQR is penalized more if o r gets farther from the median. A rating r will be given a consensus score c r according to Equation 6 as follows:
TRUEREPUTATION adopts an iterative method to com-pute the con dence of a rating and adjust the reputation based on the con dence of the user's ratings. Initially, the reputation is set as the arithmetic mean of all the ratings given to an item (i.e., the same con dence to each rating). At each iteration, TRUEREPUTATION recomputes the con-dence of the ratings and adjusts the reputations on all items based on the recalculated con dence scores of all ratings. It stops when the computation reaches a stable state.
TRUEREPUTATION rst computes a u of all users. It com-putes o r ( r 2 R m ) for all ratings on all items, then computes o for all users and converts o u into o u using the function. It computes the consensus score c r of a rating r ( r 2 R that is computed by analyzing the consistency of the ratings by the user. The con dence of a rating t r is computed with the three values of a u , o u and, c r as follows:
The reputation of each item is then adjusted based on the con dence of ratings on the same item as follows:
A t each iteration, TRUEREPUTATION recomputes o u and c r based on r m adjusted at the previous iteration and re-computes t r using newly computed o u and c r with existing a . The reputations of all items are adjusted using t r . The process of computing t r and adjusting r m goes iteratively. TRUEREPUTATION stops when it reaches a stable state. The stableness of TRUEREPUTATION is measured by the changes in the reputations between iterations. Let the vector  X  X  represent the reputations of all items. The k -th element r in the vector  X  X  represents the reputation of k -th item in M ( j M j = l ). The vector  X  X  is represented as  X  X  = [ r
The stableness of TRUEREPUTATION is measured by the change between the old vector  X  X  and the new vector  X  X  . The change is measured by cosine similarity between  X  X  and  X  X  . If the change is less than some preset value (0.000001 in our experiments), TRUEREPUTATION stops. The overall algorithm is presented in Figure 1.

We call the users who give unfair ratings Unfair Rating attackers ( UR-attackers ). The purpose of UR-attackers is either \push" or \nuke." \Push" means that UR-attackers give unfairly high ratings to promote a speci c product and \nuke" means that UR-attackers give unfairly low ratings to demote a certain product [4].

The purpose of the proposed framework is to provide po-tential buyers with trustable reputation by reducing the in-uence of UR-attackers. To verify the superiority of the proposed framework, we constructed the scenarios 1 where false reputation could occur by inserting UR-attackers into the MovieLens dataset. 2 Movies with the number of rat-ings between 90 and 110 were considered as newly-released movies and selected as the target for UR-attackers. Among them, the movies whose reputation is higher than 3.53 (the average reputation score of all movies in MovieLens) were targeted for \push" and the rest for \nuke."
F or example, a company hires UR-attackers to manipulate its product in the early state of the product
The MovieLens dataset has 100,000 ratings with the scale from 1 to 5 for 1,682 movies by 943 users.

In order to generate various UR-attackers, we adopted various shilling attackers which inject unfair ratings into a recommender system [4]. Although we have examined six different types of UR-attackers, due to space limitations we only show the results by Constant UR-attackers and Average UR-attackers in this paper.

The Constant UR-Attack is the simplest scenario in which an attacker generates unfair (maximum or minimum) ratings in order to manipulate several targeted movies. In Average UR-Attack, an attacker behaves strategically by giving the ratings close to the reputation like an ordinary user except giving out an unfair (maximum or minimum) rating to a single targeted movie.

When evaluating the performance of TRUEREPUTATION , we varied two factors: (1) the number of UR-attackers, and (2) the types of UR-attackers and the frequency of his/her ratings.

First, we varied the number of UR-attackers on the tar-geted movie, ranging from 5% of the total number of ratings to 30% with the increment of 5%. Second, we used two different UR-attackers (Constant UR-attacker and Average UR-attacker) and varied the frequency of his/her ratings.
In the case of Constant UR-Attack, we chose 32 targeted movies and inserted Constant UR-attackers (for push or nuke). The number of ratings by a Constant UR-attacker was set to be 2, 4, 8, 16, and 32, respectively.

In the case of Average UR-Attack, we chose 10 targeted movies and inserted Average UR-attackers (for push or nuke). The number of strategic ratings by the Average UR-attacker was 50, 75, and 100, respectively.

To evaluate the effectiveness of the three key factors, ac-tivity, objectivity, and consistency, we compared four varia-tions of the TRUEREPUTATION algorithm. The difference lies in how to measure the con dence of ratings.
 In addition, we compared the performance of TRUEREP-UTATION with those of two existing algorithms ( iCLUB  X  from the clustering-based algorithm [3] in multi-agent sys-tems, and MOBASHER from the classication-based algorithm [4] in recommendation systems) by modifying them as repu-tation adjustment algorithms. iCLUB  X  adjusts reputations by considering not only the users who take part in rat-ings but also the users similar to them in the same cluster. MOBASHER computes reputations based on the ratings l-tered after detecting manipulated users using the classi er proposed by [4].

The performance of each algorithm was evaluated by the difference between the reputation with UR-attackers and the reputation without UR-attackers, as shown in Equation 9.
Through experiments, we try to answer the following ques-tions:
First of all, by comparing TRUEREPUTATION to the mod-i ed algorithms, we examined the effectiveness of the three key factors. Figure 2 shows the rate of change in repu-tation according to the number of inserted UR-attackers. The reputation change rate of ARITHMETIC-MEAN was re-garded as the baseline to compare those of the other pro-posed algorithms. We found that the reputation change rate of TRUEREPUTATION is remarkably less than that of ARITHMETIC-MEAN , as shown in Figure 2. TRUEREPU-TATION evaluates the con dence of all the ratings by UR-attackers, thereby reducing the in uence of UR-attackers, while ARITHMETIC-MEAN gives the same con dence (i.e., equal weight) to all ratings. In the scenario of Constant UR-Attack, the result by OBJECTIVITY&amp;ACTIVITY is slightly superior to that of TRUEREPUTATION . This is because the ratings of a Constant UR-attacker were constantly unfair (either bad or good), and because of the consistency, they were given the high consensus scores.

When comparing (a) and (c) to (b) and (d) in Figure 2, we verify that the in uence of UR-attackers becomes more powerful with the increase in the frequency of the ratings by the UR-attackers.
Figure 2 con rms that the user's activity and the user's objectivity are useful for reputation adjustment, and espe-cially when used simultaneously. In the following experi-ments, we compared two proposed algorithms, OBJECTIV-ITY&amp;ACTIVITY and TRUEREPUTATION , and the existing algorithms, iCLUB  X  and MOBASHER .

In Figure 3, we show that OBJECTIVITY&amp;ACTIVITY , the most robust algorithm in Constant UR-Attack, was very vul-nerable to Average UR-Attack. OBJECTIVITY&amp;ACTIVITY could not reduce the in uence of the Average UR-attackers because almost all the ratings by the Average UR-attacker, who disguised himself/herself as an ordinary user giving fair ratings, looked very fair. On the other hand, TRUEREP-UTATION , which considered the consistency of the Average UR-attacker's ratings, reduced the in uence of Average UR-attackers regardless of the number of UR-attackers and the frequency of the ratings by a UR-attacker.

Also, note that iCLUB  X  whose result is even inferior to that of baseline, is not suitable to reduce the in uence of UR-Attack. MOBASHER is relatively superior but not as good as TRUEREPUTATION .
In this paper, we have proposed TRUEREPUTATION and veri ed its superiority through extensive experiments. We have shown that TRUEREPUTATION is superior to the al-gorithms based on machine learning, such as clustering or classi cation, in solving the false reputation problem.
