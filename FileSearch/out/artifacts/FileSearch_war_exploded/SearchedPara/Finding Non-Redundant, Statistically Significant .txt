 Projected and subspace clustering algorithms search for clus-ters of points in subsets of attributes. Projected cluster-ing computes several disjoint clusters, plus outliers, so that each cluster exists in its own subset of attributes. Sub-space clustering enumerates clusters of points in all subsets of attributes, typically producing many overlapping clusters. One problem of existing approaches is that their objectives are stated in a way that is not independent of the particular algorithm proposed to detect such clusters. A second prob-lem is the definition of cluster density based on user-defined parameters, which makes it hard to assess whether the re-ported clusters are an artifact of the algorithm or whether they actually stand out in the data in a statistical sense.
We propose a novel problem formulation that aims at ex-tracting axis-parallel regions that stand out in the data in a statistical sense. The set of axis-parallel, statistically sig-nificant regions that exist in a given data set is typically highly redundant. Therefore, we formulate the problem of representing this set through a reduced, non-redundant set of axis-parallel, statistically significant regions as an opti-mization problem. Exhaustive search is not a viable so-lution due to computational infeasibility, and we propose the approximation algorithm STATPC. Our comprehensive experimental evaluation shows that STATPC significantly outperforms existing projected and subspace clustering al-gorithms in terms of accuracy.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Clustering Algorithms Projected clustering, Subspace clustering
Seminal research [9] has shown that increasing data di-mensionality results in the loss of contrast in distances be-tween data points. Thus, clustering algorithms measuring the similarity between points based on all features/attributes of the data tend to break down in high dimensional spaces.
It is hypothesized [20] that data points may form clusters only when a subset of attributes, i.e., a subspace ,isconsid-ered. Furthermore, points may belong to clusters in different subspaces. Global dimensionality reduction techniques clus-ter data only in a particular subspace in which it may not be possible to recover all clusters, and information concerning points clustered differently in different subspaces is lost [20].
Therefore, several algorithm s for discovering clusters of points in subsets of attributes have been proposed in the lit-erature. They can be classified into two categories: subspace clustering algorithms, and projected clustering algorithms.
Subspace clustering algorithms search for all clusters of points in all subspaces of a data set according to their re-spective cluster definition. A large number of overlapping clusters is typically reported. To avoid an exhaustive search through all possible subspaces, the cluster definition is typ-ically based on a global density threshold that ensures anti-monotonic properties necessa ry for an Apriori style search. However, the cluster definition ignores that density decreases with dimensionality. Large values for the global density threshold will result in only low-dimensional clusters, whereas small values for the global density threshold will result in a large number of low-dimensional clusters (many of which are meaningless), in addition to the higher-dimensional clusters.
Projected clustering algorithms define a projected clus-ter as a pair ( X, Y ), where X is a subset of data points, and Y is a subset of data attributes, so that the points in X are  X  X lose X  when projected on the attributes in Y , but they are  X  X ot close X  when projected on the remaining at-tributes. Projected clustering algorithms have an explicit or implicit measure of  X  X loseness X  on relevant attributes (e.g., small range/variance), and a  X  X on-closeness X  measure on irrelevant attributes (e.g., uniform distribution/large vari-ance). A search method will report all projected clusters in the particular search space that an algorithm considers. If only k projected clusters are desired, the algorithms typi-cally use an objective function to define what the optimal set of k projected clusters is.

Based on our analysis, we argue that a first problem for both projected and subspace clustering is that their objec-tives are stated in a way that is not independent of the particular algorithm that is proposed to detect such clusters in the data -often leaving the practical relevance of the de-tected clusters unclear, particularly since their performance also depends critically on difficult to set parameter values.
A second problem for the most previous approaches is that they assume, explicitly or implicitly, that clusters have some point density controlled by user-defined parameters, and they will (in most cases) report some clusters. However, we have to judge if these clusters  X  X tand out X  in the data in some way, or, if, in fact, there are many structures alike in the data. Therefore, a density cr iterion for selecting clusters should be based on statistical principles.

Contributions and Outline of the paper. Motivated by these observations, we propose a novel problem formu-lation that aims at extracting from the data axis-parallel regions that  X  X tand out X  in a statistical sense. Intuitively, a statistically significant region is a region that contains sig-nificantly more points than expected. In this paper, we consider the expectation under uniform distribution. The set of statistically significant regions R that exist in a data set is typically highly redundant in the sense that regions that overlap with, contain, or are contained in other sta-tistically significant regions may themselves be statistically significant. Therefore, we propose to represent the set R through a reduced, non-redundant set of (axis-parallel) sta-tistically significant regions that in a statistically meaningful sense  X  X xplain X  the existence of all the regions in R . We will formalize these notions and formulate the task of finding a minimalse tofstatisticall ysignificant X  X xplaining X  X egio ns as an optimization problem. Exhaustive search is not a viable solution because of computational infeasibility. We propose STATPC -an algorithm for 1) selecting a suitable set R reduced  X  R in which we can efficiently search for 2) a smallest set P  X  that explains (at least) all elements in R reduced . Our comprehensive experimental evaluation shows that STATPC significantly outperforms previously proposed projected and subspace clustering algorithms in the accuracy of both cluster points and relevant attributes found.
The paper is organized as follows. Section 2 surveys re-lated work. Section 3 describes our problem definition. The algorithm STATPC is presented in section 4. Section 5 con-tains an experimental evaluation of STATPC . Conclusions and directions for future work are presented in section 6.
CLIQUE [4], ENCLUS [11], MAFIA [18], nCluster [15] are grid-based subspace clustering algorithms that use global density thresholds in a bottom-up, Apriori style [5] discov-ery of clusters. Grid-based subspace clustering algorithms are sensitive to the resolution of the grid, and they may miss clusters inadequately oriented or shaped due to the position-ing of the grid. SCHISM [22] uses a variable, statistically-aware, density threshold in order to detect dense regions in a grid-based discretization of the data. However, for the largest part of the search space, the variable threshold equals a global density threshold. SUBCLU [13] is a grid-free ap-proach that can detect subspace clusters with more general orientation and shape than grid-based approaches, but it is also based on a global density threshold. DiSH [1] computes hierarchies of subspace clusters in which multiple inheritance is possible. Algorithmically, DiSH resembles both subspace and projected clustering algorithms. DiSH uses a bottom-up search based on a global density threshold to compute a sub-space dimensionality for each data point. These subspace di-mensionalities are used to derive a distance between points, which is then used in a top-down computation of clusters. In DUSC [6], a point is initially considered a core point if its density measure is F times larger than the expected value of the density measure under uniform distribution, which does not have anti-monotonic properties, and thus cannot be used for pruning the search space. As a solution, DUSC modifies the definition of a core point so that it is anti-monotonic, which, however, introduces a global density threshold.
Several subspace clustering algorithms attempt to com-pute a succinct representation of the numerous subspace clusters that they produce, by reporting only the highest dimensional subspace clusters [18], merge similar subspace clusters [22], or organize them hierarchically [1].
Projected clustering algorithms can be classified into 1) k -medoid-like algorithms: PROCLUS [3], SSPC [25]; 2) hyper-cube based approaches: DOC/FASTDOC [21], MINECLUS [26]; 3) hierarchical: HARP [24]; 4) DBSCAN-like approach: PREDECON [10]; and 5) algorithms based on the assump-tion that clusters stand out in low dimensional projections: EPCH [19], FIRES [14], P3C [17]. For the algorithms in cat-egories 1) and 2), the problem is defined using an objective function. However, these objective functions are restrictive and/or require critical parameters that are hard to set. The other algorithms do not define what a projected cluster is independent of the method that finds it. P3C takes into account statistical principles for deciding whether two 1 D projections belong to the same cluster. Many of these al-gorithms show unsatisfactory performance for discovering low-dimensional projected clusters.

Related to our work is also the work on Scan Statistics [2], in which the goal is to detect spatial regions with sig-nificantly higher counts relative to some underlying base-line. The methods in Scan Statistics are applicable to full-dimensional data, whereas our problem formulation con-cerns statistically significant regions in all subspaces of a data set. Also related is the method PRIM [12], which shares some similarities with DOC and its variants, since it computes one dense axis-aligned box at a time, where the density of the box is controlled by a user-defined parameter. PRIM does not take into account the statistical significance of the computed boxes and often reports many redundant boxes for the same high-density region.
Let D = { ( x i 1 ,...,x id ) | 1  X  i  X  n } be a data set of nd -dimensional data points. Let A = { Attr 1 ,...,Attr d be the set of the d attributes of the points in D so that x ij  X  dom ( Attr j ), where dom ( Attr j ) denotes the domain of the attribute Attr j ,1  X  j  X  d . Without restricting the generality, we assume that all attributes have normal-ized domains, i.e., dom ( Attr j )=[0 , 1], and we also refer to projections of a point x i  X  D using dot-notation, i.e., if x
An interval I =[ v l ,v u ]onanattribute a  X  A is defined as all real values x  X  dom ( a )sothat v l  X  x  X  v u .The width of interval I is defined as width ( I ):= v u  X  v l associated attribute of an interval I is denoted by attr ( I ). A subspace S is a non-empty subset of attributes, S  X  A . The dimensionality of S , dim ( S ), is cardinality of S .
A hyper-rectangle H is an axis-aligned box of intervals on different attributes in A , H = I 1  X  ...  X  I p ,1  X  p attr ( I i ) = attr ( I j )for i = j . S = { attr ( I 1 ) ,...,attr ( I the subspace of H , denoted by subspace ( H ).

Let H = I 1  X  ...  X  I p be a hyper-rectangle, 1  X  p  X  d .The volume of H , denoted by vol(H) , is defined as the hyper-volume occupied by H in subspace ( H ), which is computed as vol ( H )= p i =1 width ( I i ). The support set of H ,de-noted by SuppSet(H) , represents the set of database points whose coordinate values fall within the intervals of H for the corresponding attributes in subspace ( H ), i.e., SuppSet ( H ):= { x  X  D | x.attr ( I i )  X  I i ,  X  i :1  X  i  X  p } .The actual support of H , denoted by AS(H) , represents the cardinality of its support set, i.e., AS ( H ):= | SuppSet ( H ) | .
Let H be a hyper-rectangle in a subspace S .Weusethe methodology of statistical hypothesis testing to determine the probability that H contains AS ( H ) data points under the null hypothesis that the n data points are uniformly dis-tributed in subspace S . The distribution of the test statistic, AS ( H ), under the null hypothesis is the Binomial distribu-tion with parameters n and vol ( H )[7] 1 , i.e., The significance level  X  of a statistical hypothesis test is a fixed probability of wrongly rejecting the null hypothesis, when in fact it is true.  X  is also called the rate of false positives or the probability of type I error.

The critical value of a statistical hypothesis test is a thresh-old to which the value of the test statistic is compared to determine whether or not the null hypothesis is rejected. For a one-sided test, the critical value  X   X  is computed based on for a two-sided test, the right critical value  X  R  X  is computed by (2), and the left critical value  X  L  X  is computed based on where the probability is computed in each case using the distribution of the test statistic under the null hypothesis. Definition 1. Let H be a hyper-rectangle in a subspace S .Let  X  0 be a significance level. Let  X   X  0 be the critical value computed at significance level  X  0 basedon(2),wherethe probability is computed using Binomial ( n, vol ( H )). H is a statistically significant hyper-rectangle if AS ( H ) &gt; X 
A statistically significant hyper-rectangle H contains sig-nificantly more points than what is expected under uniform distribution, i.e., the probability of observing AS ( H )many points in H , when the n data points are uniformly dis-tributed in subspace S is less than  X  0 .

Let  X  0 be an initial significance level. A value of  X  0 = 0 . 001 is quite common in statistical tests when a single and typically well-conceived hypothesis (i.e., one that has a high chance of being true) is tested; however, the value should be much smaller if the number of possible tests is very large, and we are actually searching for hypotheses that will pass a test; otherwise, a considerable number of false positives
Note that if the attributes are not normalized to [0 , 1], we have to replace vol ( H )by vol ( H ) /vol ( S ). will be eventually reported. We will test for statistical sig-nificance hyper-rectangles in each subspace of the data set. Thus, the number of false positives increases proportionally to the number of subspaces tested. We can use a conserva-for testing hyper-rectangles in a subspace of dimensionality p by the total number of subspaces of dimensionality p as  X  =  X  0 choose ( d,p ) ,where choose ( d, p ) is the binomial coefficient choose ( d, p )= d ! p !  X  ( d  X  p )! , or we can use the FDR method [8].
An important property of stati stical significance is that it is not anti-monotonic, i.e., if H = I 1  X  ...  X  I p is a sta-tistically significant hyper-rectangle, then a hyper-rectangle H = I i 1  X  ...  X  I i k ,1  X  i j  X  p ,1  X  j  X  k ,formedbya subset of the intervals of H , is not necessarily statistically significant. Therefore, Apriori-like bottom-up constructions of statistically significant hyper-rectangles is not possible.
Let H be a hyper-rectangle in a subspace S . As the di-mensionality of S increases, vol ( H ) decreases towards 0, and, consequently, the critical value  X   X  decreases towards 1. Thus, in high dimensional subspaces, hyper-rectangles H with very few points may be statistically significant.
Also, assume H is a statistically significant hyper-rectangle in a subspace S , and assume that there is another attribute a/  X  S where the coordinates of the points in SuppSet ( H ) are uniformly distributed in dom ( a ). We could then add the smallest interval I =[ l, u ]to H that satisfies attr ( I )= a and SuppSet ( I )= H , i.e., l = min { x.a | x  X  SuppSet ( H ) and u = max { x.a | x  X  SuppSet ( H ) } . The resulting hyper-rectangle H will then be statistically significant in subspace S = S  X  X  a } . This happens roughly speaking because the support stays the same, but the volume does not increase ( AS ( H )= AS ( H ), vol ( H )  X  vol ( H )); see [16] for a for-mal proof. Clearly, reporting statistically significant hyper-rectangles such as H does not add any information, since their existence is  X  X aused X  by the existence of other statis-tically significant hyper-rectangles to which intervals have been added in which the points are uniformly distributed along the whole range of the corresponding attributes.
To deal with these problems, we introduce the concept of  X  X elevant X  attributes versus  X  X rrelevant X  attributes. Definition 2. Let H be a hyper-rectangle in a subspace S . An attribute a  X  S , is called relevant for H if points in SuppSet ( H )are not uniformly distributed in dom ( a ); oth-erwise it is called irrelevant for H .

To test whether p oints in SuppSet ( H ) are uniformly dis-tributed in the whole range of an attribute a we use the Kolmogorov-Smirnov goodness of fit test for the uniform distribution [23] with a significance level of the test of  X 
Given a data set D of nd -dimensional points, we would like to find in each subspace all hyper-rectangles that sat-isfy definitions 1 and 2. The number of hyper-rectangles in a certain subspace can be infinite. However, we consider, for each subspace, all unique Minimum Bounding Rectan-gles (MBRs) formed with data points instead of all possible hyper-rectangles. The reason is that adding empty space to an MBR keeps its support constant, but it increases its volume; thus, it only decreases its statistical significance.
Definition 3. Given a data set D of nd -dimensional points. We define a subspace cluster as an MBR formed with data points in some subspace so that the MBR 1) is statistically significant, and 2) has only relevant attributes.
Redundancy-oblivious problem definition. Find all unique subspace clusters in a set of nd -dimensional points.
For any non-trivial values of n and d ,the size of the search space for the redundancy-oblivious problem definition is ob-viously very large. There are 2 d  X  1 subspaces, and the number of unique MBRs in each subspace S ,thatcontain at least 2 points, assuming all coordinates of n points to be distinct in S ,isatleast choose ( n, 2) and upper bounded by choose ( n, 2) + choose ( n, 3) + ... + choose ( n, 2  X  dim ( S )).
The size of the solution to the redundancy-oblivious prob-lem definition can be quite large as well, even if the overall distribution is generated by only a few  X  X rue X  subspace clus-ters { T 1 ,...,T k } , k  X  1, plus uniform background noise: 1) for each T i ,1  X  i  X  k , other subspace clusters may ex-ist around it in subspace ( T i ), formed by subsets of points in SuppSet ( T i ) plus possibly neighboring points in subspace ( T -Figures 1(a), 1(b) and 1(c) illustrate some of these cases; 2) subspace clusters may also exist in lower or higher-dimensional subspaces of subspace ( T i ) due to the existence of T i 1(d) illustrates for a true 2-dimensional subspace cluster in the xy -plane an induced 3-dimensional subspace cluster and two 1-dimensional subspace clusters; 3) additional subspace clusters may also exist whose points or attributes belong to different T i -Figure 1(e) illustrates a subspace cluster in-duced by two subspace clusters from the same subspace, 1(f) illustrates a subspace cluste r induced by two subspace clus-ters from different subspaces; 4) combinations of all these cases are possible as well, and the number of subspace clus-ters that exist only because of the  X  X rue X  subspace clusters is typically very large. For instance, the total number of sub-space clusters in even the simple data set depicted in Figure 1(g)  X  X ith 50 points and two 2-dimensional subspace clus-ters, which are embedded in a 3-dimensional space X  is 656.
Conceptually, the solution R to the redundancy-oblivious problem definition contains three types of elements: 1) a set of subspace clusters T representing the  X  X rue X  subspace clusters, 2) a set representing the false positives reported by the statistical tests, and 3) a set of subspace clusters F representing subspace clusters that exist only because of the subspace clusters in T and possibly , i.e., We argue that reporting the entire set R is not only com-putationally expensive, but it will also overwhelm the user with a highly redundant amount of information, because of the large number of elements in F .
Our goal is to represent the set R of all subspace clusters in a given data set by a reduced set P opt of subspace clusters such that the existence of each subspace cluster H  X  R can be explained by the existence of the subspace clusters P opt and P opt should be a smallest set of subspace clusters with that property. Ideally, P opt = T  X  .

To achieve this goal, we have to define an appropriate Ex-plain relationship, which is based on the following intuition. We can think of the overall data distribution as being gen-erated by the  X  X rue X  subspace clusters, which we hope to capture in the set P opt , plus background noise. We can say that the actual support AS ( H ) of a subspace cluster H can be explained by a set of subspace clusters P ,if AS ( H )is consistent with the assumption that the data was generated by only the subspace clusters in P and background noise.
More formally, if we have a set P = { P 1 , ..., P K } of sub-space clusters that should explain all subspace clusters in R , we assume that the overall distribution is a distribution mixture of K + 1 components, K components correspond-ing to (derived from) the K elements in P and the K +1 component corresponding to background noise, i.e., where  X  k are the parameters of each component density, and  X  k are the proportions of the mixture.
 Conceptually, to justify that an observed subspace cluster H is explained by P , we have to test that the actual support AS ( H )of H is not significantly larger or smaller than what can be expected under the given model. Again, this can in theory be done using a statistical test, if we can determine left and right critical values for the distribution of the test statistics AS ( H ), given a desired significance level.
Practically, there are limitations to what can be done an-alytically to apply such a statistical test. An analytical so-lution requires to first estimate the parameters and mixing proportions of the model, using the data and information that can be derived from the set P ;andthen,anequationfor the distribution of AS ( H ) has to be derived from the equa-tion for the mixture model. Obviously, this is challenging (if not impossible) for more complex forms of distributions.
In the following, we show how to define the Explain rela-tionship assuming that all component densities are Uniform distributions. Let the K + 1 component be the uniform background noise in the whole space, i.e., For the other components, corresponding to P k  X  P ,we assume that data is generated such that in subspace ( P k 1  X  k  X  K , the points are uniformly distributed in the cor-responding intervals of P k (and uniformly distributed in the whole domain in the remaining attributes, since these are the irrelevant attributes for P k ). Formally, if P k has m k attributes, i.e., P k = I k 1  X  ...  X  I k m k ,andthe d attributes are component density is given by f k ( x )  X  Uniform ( I k 1  X  ...  X  I k m k  X  [0 , 1]  X  ...
To determine whether the existence of a subspace clus-ter H = I H 1  X  ...  X  I H m H is consistent with such a model, we have to estimate the possible contribution of each com-ponent density to H . For a component density f k ,that contribution is proportional to the volume of the intersec-tion between f k and H in the subspace of H , i.e., we have to determine the part of f k that lies in H .Thisintersection is  X  X ike H  X  X n m H -dimensional hyper-rectangle  X  H ( P k that can be computed as following. For f k ,1  X  k  X  K ,let P let P K +1 =[0 , 1]  X  ...  X  [0 , 1]: where
Because f k is a uniform distribution, the number of points in  X  H ( P k ) generated by f k follows a Binomial distribution number of points generated by f k ,and vol (  X  H ( P k )) fraction of P k that intersects H .

The numbers n k can easily (under our assumptions) be estimated using the total number of points n and the infor-mation about the actual supports AS ( P i ) of the subspace clusters P i  X  P in the data set. For any of the compo-nents f i ,1  X  i  X  K + 1, the number of points generated by that component is, according to the data model, equal to the observed number of points in P i , AS ( P i ), minus the contributions n j of the other components f j , j = i ,and P K +1 =[0 , 1]  X  ...  X  [0 , 1] (for the background noise f where  X  P i ( P j ) is the intersection of hyper-rectangle P hyper-rectangle P i as defined in equation (8). The equations in (10) can easily be solved for n i since (10) is a system of K + 1 linear equations in K +1 variables. 2
We want to say that a set of subspace clusters P ,plus background noise, explains a subspace cluster H if the ob-served number of points in H is consistent with this assump-tion and not significantly larger or smaller that expected. From the Binomial distributions (9), we can derive a lower and an upper bound on the number of points in H that could be generated by component density f k , without this num-ber being statistically significant; these are the left  X  respectively right  X  R  X  0 ( k ), critical values of this Binomial dis-tribution, with significance level  X  0 .

By summing up these bounds for each component density, we obtain a range [ ES L H ,ES U H ] of the number of points in H that could be accounted for just by the presence of the
When the solution to (10) is not unique or has negative values for some n i this indicates redundancy in the set P , respectively inconsistency with our assumptions, and we can later use this fact to prune such a candidate set P early. subspace clusters in P , plus background noise, i.e., If AS ( H ) falls into this range, we can say that AS ( H )is consistent with P ,orthat P is in fact sufficient to explain the observed number of points in H .
 Definition 4. Let P  X  X  H } be a set of subspace clusters. P explains H if and only if AS ( H )  X  [ ES L H ,ES U H ]. Property 1. { H } X  P explains H .

Proof. See [16].
The problem of representing R via a smallest (in this sense non-redundant) set of subspace clusters can now be defined.
Redundancy-aware problem definition. Given a data set D of nd -dimensional points. Let R be the set of all sub-space clusters. Find a non-empty set P opt  X  R with smallest cardinality | P opt | so that P opt explains H for all H  X 
Note that the optimization problem has always a solution, since R explains H for all H  X  R , because of Property 1.
We emphasize the fact that the redundancy-aware prob-lem definition avoids shortcomings of existing problem def-initions in the literature. First, our objective is formulated through an optimization problem, which is independent of a particular algorithm used to solve it. Second, our definition of subspace cluster is based on statistical principles; thus, we can trust that P opt stands out in the data in a statistical way, and is not simply an artifact of the method.

Enumerating all elements in R in an exhaustive way is computationally infeasible for larger values of n and d . Find-ing a smallest set of explaining subspace clusters by testing all possible subsets of R has complexity 2 | R | ,whichisinturn computationally infeasible for typical sizes of R . We ran an exhaustive search on several small data sets where some low-dimensional subspace clusters were embedded into higher dimensional spaces, similar to and including the data set depicted in Figure 1(g). The result set P opt found for these data sets was always containing only the embedded subspace clusters (i.e., we did not even have any false positives in these cases); In Figure 1(g), the two depicted 2-dimensional rect-angles indicating the embedded subspace clusters represent in fact the subspace clusters found by the exhaustive search.
In order to find the solution P opt to the redundancy-aware problem definition, we need heuristics to 1) find a good set R reduced  X  R in which we can efficiently search for 2) a smallest set P sol that explains (at least) all elements in R mation algorithm, called STATPC, that follows this schema.
To construct a good set R reduced ,STATPCconstructs subspace clusters by analyzing subspaces and local neigh-borhoods around individual data points Q .Forthisstep we suggest first a method for identifying and refining can-didate subspaces based on fixed neighborhoods around Q , and second a method for finding a locally optimal subspace cluster in the neighborhood of Q , given the constructed can-didate subspaces for Q . To solve the optimization problem on R reduced heuristically, we propose a greedy strategy. For a given data point Q , we want to determine if there is a subspace cluster around Q . The neighborhoods we consider in this stage, are all 2-dimensional rectangles with Q in the center and side length 2  X   X  , i.e., all rectangles [ Q.Attr i  X   X , Q.Attr i +  X  ]  X  [ Q.Attr j  X   X , Q.Attr j for some  X   X  [0 , 0 . 5], 1  X  i&lt;j  X  d .

We propose to rank the 2-dimensional rectangles accord-ing to their actual support and select, based on an analysis of this ranking, a set of attributes, called signaled attributes, which are, with high probability, relevant for one of the true subspace clusters around Q .

When one or more true subspace clusters exist around Q , the actual support of the 2-dimensional projections that in-volve attributes of the true subspace clusters may not be statistically significant, nor higher than the support of some 2-dimensional rectangles formed by uniformly distributed attributes. However, the actual support is likely to be at least in the higher range of possible support values under uniform distribution. This does not mean that the top M pairs consist mostly of relevant attributes, but it means that the frequency with which individual relevant attributes are involved in the top M pairs is likely to be significantly higher than the frequency of a randomly chosen attribute.
To determine if an attribute a  X  A is significantly more frequent than expected in the top M pairs, we compare its frequency to the expected frequency of an attribute in a randomly selected set of M pairs.

The number of times X that an attribute a occurs in a subset of M randomly selected pairs of attributes is a hyper-geometric distributed va riable with parameters d  X  ( d  X  1) ber of all pairs), d  X  1 (number of pairs containing attribute a ), and M (sample size): for 1  X  k  X  M , Pr ( X = k )equals choose ( d  X  1 ,k )  X  choose ( d  X  ( d  X  1) 2  X  ( d  X  1) ,M
We say that an attribute a  X  A occurs more often than expected in the top M pairs of attributes in our ranking, if that number of occurrences is larger than  X   X  H ,where  X   X  H is the right critical value (at significance level  X  H )ofthe hyper-geometric distribution (14).
 Let S 0 be a set of signaled attributes. We observe that if S 0 is only a subset of the relevant attributes for a true subspace cluster around Q , then, by considering the points in ahyper-rectangle W of width 2  X   X  around Q in subspace S 0 we capture a fraction of the true subspace cluster X  X  points, which is often large enough to allow us to determine more of the relevant attributes; these are attributes where the points in SuppSet ( W )are not uniformly distributed.

Based on this observation, we can obtain a candidate sub-space around Q through an iterative refinement of S 0 ,as follows. Let S 1 be the set of relevant attributes for W in subspace ( S 0 ). If S 0 S 1 , return the empty set. If S return S 0 . Otherwise, we repeat with S 1 , selecting the rel-evant attributes of W in subspace ( S 1 ), and so on, until no more attributes can be added.
 Let S be a candidate subspace. To determine if a subspace cluster around Q exists in S , we build a series of MBRs in S , starting from Q , and adding in each step to the current MBR the point that is closest to the current MBR in subspace S . For efficiency reasons, and because a cluster contains typically only a fraction of the total number of points, we only build 0 . 3  X  n MBRs around Q in subspace S .
Let R local be the set of MBRs built in this way that are also subspace clusters. If R local =  X  , no true subspace cluster around Q in S could be found; otherwise, we obtain a set of highly overlapping subspace clusters, and we want to select one of these MBRs that is locally optimal in the sense that it explains better all subspace clusters in R local than any other subspace cluster in R local .

Under the same data model assumptions as for the Explain relationship, we can define the expected support of a subspace cluster H assuming a single subspace cluster P 1 ,as:
ES ( H | P 1 )= n P 1  X  vol ( H where n P 1 is the number of points generated by the density component associated with P 1 , obtained by solving equation (10) for P 1 and background noise.

We measure how well P 1 explains H by comparing ex-pected support ES ( H | P 1 ) and actual support of AS ( H ), using a function QualityExplain : R local  X  R local  X  [0 , 1]; QualityExplain ( P 1 ,H ) to 1, the better the quality of expla-nation. The locally optimal subspace cluster P local  X  R local that is selected, is one that maximizes To determine signaled attributes and candidate subspaces, we have to choose values for  X  and M .

In order to be robust to the value of M ,weconsiderdiffer-ent values for M (in our current implementation 3 values), and define the signaled attributes as the attributes that are detected as occurring more often than expected with respect to the majority of the values of M . For example, if we ob-tain  X  X ignificant X  attributes A 1 = { a 1 ,a 2 ,a 3 } for M 1 =5; [16] for details on how different values for M are chosen.)
Regarding the value for  X  , there is no  X  X est X  value and to improve our chances of detecting a true subspace cluster, we suggest to use several different values. We simply try the 3 values 0 . 05, 0 . 1, 0 . 15 for  X  , resulting in up to three candidate subspaces for each point Q that we consider.

To construct a set R reduced , STATPC tries to find sub-space clusters around data points as described. The first point to consider is selected randomly from the set of all points. Subsequent points are selected randomly from the points that do not belong to detected subspace clusters in previous steps. Building R reduced terminates when no data point can be selected for further subspace cluster search. Although | R reduced | &lt; | R | , solving the optimization problem on R reduced by testing all possible subsets is still computa-tionally too expensive in general. Thus, we construct greed-ily aset P sol that explains all subspace clusters in R reduced but may not be the smallest set with this property.
We build P sol by adding one subspace cluster at a time from R reduced .Ateachstep,let Cand be the set of subspace clusters in R reduced that are not explained by the current P sol . Thus, subspace clusters in Cand canbeusedtoex-tend P sol further, until P sol explains all subspace clusters in R step, we select the subspace cluster H  X  Cand for which P sol  X  X  H } explains more subspace clusters in R reduced than any other choice from the set Cand . H is remove from Cand andaddedto P sol ,andwestopwhen Cand is empty.
The experiments reported in this section were conducted on a Linux machine with 3 GHz CPU and 2 GB RAM.

Synthetic Data. 3 We generated data with n = 300 points, d = 50 attributes, k = 5 subspace clusters (with sizes 60, 50, 40, 40, and 50 points), plus 60 uniformly dis-tributed noise points. The distribution of points in subspace clusters can be 1) uniform or 2) Gaussian. Subspace clusters can have 1) an equal or 2) a different number of relevant at-tributes. Thus, we obtained 4 categories of data for which we generated data sets with average numbers of relevant at-tributes 2, 4, 6, 8, 10, 15, and 20. The width of clusters in relevant attributes was 10%  X  30% of the attribute range. Clusters did not overlap in common relevant attributes.
Real Data. We test the performance of the compared algorithms on the following data sets from the UCI machine learning repository 4 : Pima Indians Diabetes (768 points, 8 attributes, 2 classes); Liver Disorders (345 points, 6 at-tributes, 2 classes); and Wisconsin Breast Cancer Prognostic (WPBC)(198 points, 34 attributes, 2 classes).

Experimental setup. We evaluate STATPC against projected clustering algorithms PROCLUS, ORCLUS, HARP, SSPC, MINECLUS, P3C; subspace clustering algorithm MAFIA; and the method PRIM. For MINECLUS, HARP, SSPC and P3C we used the original implementations. PROCLUS and ORCLUS were provided by the SemBiosphere project 5 .PRIM
Available at http://www.cs.ualberta.ca/~gabi/SData/ http://archive.ics.uci.edu/ml/ yeasthub2.gersteinlab.org/sembiosphere is available as a package 6 for the R statistical software. MAFIA was implemented by us.

On synthetic data, we set the target number of clusters to the number of implanted clusters for PROCLUS, OR-CLUS, HARP, SSPC, and MINECLUS. PROCLUS and OR-CLUS require the average cluster dimensionality, which was set to the known average cluster dimensionality. HARP requires the maximum percentage of outliers, which was set to the known percentage of outliers. For algorithms that require other parameter settings, we set these param-eters as recommended by their authors: for PROCLUS: A = 20, B =5;forORCLUS:  X  =0 . 5, k 0 = 30, srr = 10; for SSPC: m =0 . 5; for MINECLUS: w =0 . 3,  X  =0 . 1,  X  =0 . 25, maxout = 20; for P3C: P oisson threshold = 1 . 0 E  X  5; for MAFIA:  X  =1 . 5,  X  =0 . 35, no tiny bins = 50, no intervals unif distrib =5;forPRIM: peel alpha = 0 . 05, paste alpha =0 . 01, mass min =0 . 1. SSPC was run without the supervision option. Except HARP, P3C, and MAFIA, all algorithms are non-deterministic; thus, each of them is run 5 times, and the re sults are averaged. STATPC, P3C, and MAFIA allow data points to belong to more than one cluster; the other algorithms compute disjoint clusters.
STATPC requires 3 significance levels:  X  0 ,  X  K ,and  X  H After testing the sensitivity of STATPC to these parameters (see [16]), we set  X  0 =1 . 0 E  X  10,  X  K =  X  H =0 . 001.
On real data, we use class labels as cluster labels. We set the target number of clusters to the number of classes. For parameters such as the average cluster dimensionality, whose values are hard to determ ine, several values are tried and the results with best accuracy are reported.

The real data sets used were c ollected for classification purposes. We use such real data sets because a systematic evaluation of the compared algorithms on unlabeled data is cumbersome. However, in such real data sets, most of the attributes were selected in the first place because they were considered potentially relevant for the classification prob-lems. Consequently, the real data sets may contain only full-dimensional subspace clusters or very high-dimensional subspace clusters. To actually verify the capability of the competing algorithms to find subspace clusters, we add 5, 10, 20, respectively 50 attributes to each real data set where the data points are uniformly distributed. Subspace clusters that may exist in the data sets, full-dimensional or not, will be subspace clusters of increasingly lower dimensionality as we add more uniform attributes to the data sets.

Performance measures. We use an F value to measure the clustering accuracy. We refer to implanted clusters as input clusters, and to found clusters as output clusters. For each output cluster i , we determine the input cluster j i which it shares the largest number of points. The precision of output cluster i is defined as the number of points com-mon to i and j i divided by the total number of points in i .The recal l of output cluster i is defined as the number of points common to i and j i divided by the total number of points in j i .The F value of output cluster i is the harmonic mean of its precision and recall. The F value of a clustering solution is obtained by averaging the F values of all its out-put clusters. Similarly, we use an F value to measure the accuracy of found relevant attributes based on the match-ing between output and input clusters (except for ORCLUS, since it generates general sets of orthogonal vectors). http://cran.r-project.org/web/packages/prim/
Statistical significance of results. STATPC computes subspace clusters that are statistically significant. The other algorithms sometimes compute statistically significant sub-space clusters, other times they do not, depending on pa-rameter values and on the density of the implanted clusters (denser clusters are easier to detect). The classes in the real data sets form statistically significant clusters, and these clusters stay statistically significant when adding uniform attributes, as shown in section 3.3.

Accuracy results. Figure 2 shows the accuracy of the compared algorithms as a function of increased average clus-ter dimensionality for the category Uniform Different , where the cluster points are uniformly distributed in their relevant subspace, and the clusters have a different num-ber of relevant attributes. We observe that STATPC sig-nificantly and consistently outperforms the competing algo-rithms, both in terms of clustering accuracy and in terms of accuracy of the found relevant attributes. STATPC obtains an improvement in clustering accuracy and accuracy of the found relevant attributes over the best other competing al-gorithm of up to 30%, respectively 34%. The difference in accuracy between STATPC and previous algorithms is more pronounced for the more difficult case of data sets with low-dimensional subspace clusters. As the number of relevant attributes increases, the accuracy of competing algorithms increases as well, since the clusters become more easily rec-ognizable in full-dimensional space.

Similar results are obtained on the remaining 3 categories of synthetic data sets Uniform Equal , Gaussian Equal , and Gaussian Different . Equal vs. different number of relevant attributes for subspace clusters does not have an impact on the accuracy of STATPC. The accuracy results of STATPC on data sets where cluster points are uniformly distributed in their relevant subspace is slightly higher than when the cluster points are Gaussian distributed in their rel-evant subspace. This is because in the latter case, STATPC may miss some points at the clusters X  boundaries. We also compared STATPC against a representative set of full di-mensional clustering algorithms: KMeans, EM, CLARANS, agglomerative and divisive hierarchical clustering. Full di-mensional clustering algorithms do not perform well for the task of extracting subspace clusters. We have also stud-ied systematically the accuracy of STATPC as a function of different data generation parameters. The accuracy of STATPC is unaffected by increasing d ,or k ; and it increases when n or clusters sizes increase, or when the extent or the overlap in relevant attributes decrease. See [16] for details.
Figure 3 shows the accuracy of the compared algorithms on the 3 real data sets, as a function of increased number of uniform attributes added to the data. The first point in the graphs corresponds to the original data sets with no at-tributes added. STATPC consistently finds 7 8-dimensional subspace clusters, 5 6-dimensional subspace clusters, and 2 33-dimensional subspace clusters on Pima Indians Diabetes, Liver Disorders, respectively WPBC data sets and their ex-tensions, outperforming the other algorithms by at least a margin of 20%, 10%, respectively 10%.

Scalability experiments. In all scalability figures, the time is represented on a log10 scale. Figure 4(a) shows scal-ability results for increasing database sizes on synthetic data sets from category Uniform Equal with d = 10, k =2,2 relevant attributes per cluster. Although STATPC has a larger runtime than previous algorithms, it is still accept-able and, as we believe, worth the trade-off for much bet-ter effectiveness in finding subspace clusters. Figure 4(b) shows scalability results for increasing database dimension-ality on synthetic data sets from category Uniform Equal with n = 300, k = 2, 2 relevant attributes per cluster. The scalability of STATPC is comparable to that of the other algorithms. Figure 4(c) shows that STATPC is unaffected by increasing average cluster dimensionalities on the data sets from category Uniform Equal .
In this work, we identified important shortcomings in the existing projected and subspace clustering literature and proposed a novel problem formulation that ensures that found subspace clusters actually stand out in the data in a statis-tical sense. We also proposed an approximation algorithm STATPC for the problem, which in our experimental evalu-ation clearly outperforms state-of-the art projected and sub-space clustering algorithms.

There are many opportunities for future work such as the study of other approximation algorithms, and the investiga-tion of distribution assumptions other than Uniform.
We thank the authors who provided us the code for some projected clustering algorithms. This research was supported by the Alberta Ingenuity Fund, iCORE, and NSERC. [1] E.Achtert,C.B  X  ohm, H.-P. Kriegel, P. Kr  X  oger, [2] D. Agarwal, A. McGregor, J. Phillips, [3] C. C. Aggarwal, C. Procopiuc, J. L. Wolf, P. S. Yu, [4] R. Agrawal, J. Gehrke, D. Gunopulos, and [5] R. Agrawal and R. Srikan. Fast algorithms for mining [6] I.Assent,R.Krieger,E.M  X  uller, and T. Seidl. DUSC: [7] A. Baddeley. Spatial point processes and their [8] Y. Benjamini and Y. Hochberg. Controlling the false [9] K. Beyer, J. Goldstein, R. Ramakrishnan, and [10] C. B  X  ohm, K. Kailing, H.-P. Kriegel, and P. Kr  X  oger. [11] C. H. Cheng, A. W. Fu, and Y. Zhang. Entropy-based [12] J. Friedman and N. Fisher. Bump hunting in [13] K. Kailing, H. P. Kriegel, and P. Kr  X  oger. [14] H. P. Kriegel, P. Kr  X  oger, M. Renz, and S. Wurst. A [15] G. Liu, J. Li, K. Sim, and L. Wong. Distance based [16] G. Moise and J. Sander. TR08-03. Technical report, [17] G. Moise, J. Sander, and M. Ester. P3C: A robust [18] H. Nagesh, S. Goil, and A. Choudhary. Adaptive grids [19] K. Ng, A. Fu, and C.-W. Wong. Projective clustering [20] L. Parsons, E. Haque, and H. Liu. Subspace clustering [21] C. M. Procopiuc, M. Jones, P. K. Agarwal, and [22] K. Sequeira and M. Zaki. SCHISM: a new approach [23] G. W. Snedecor and W. G. Cochran. Statistical [24] K. Yip, D. Cheung, and M. Ng. HARP: a practical [25] K. Yip, D. Cheung, and M. Ng. On discovery of [26] M. L. Yiu and N. Mamoulis. Iterative projected
