 We consider the impact of inter-assessor disagreement on the maximum performance that a ranker can hope to achieve. We demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor, when evalu-ated with respect to a different assessor, the measured per-formance of the ranker decreases significantly. This decrease in performance may largely account for observed limits on the performance of learning-to-rank algorithms.
 Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 In-formation Search and Retrieval:Retrieval models General Terms: Experimentation, Measurement, Theory Keywords: Inter-assessor Disagreement, Learning-to-Rank, Evaluation
In both Machine Learning and Information Retrieval, it is well known that limitations in the performance of ranking al-gorithms can result from several sources, such as insufficient training data, inherent limitations of the learning/ranking algorithm, poor instance features, and label errors. In this paper we focus on performance limitations due solely to la-bel  X  X rrors X  which arise due to inter-assessor disagreement.
Consider a training assessor A that provides labels for training data and a testing assessor B that provides labels for testing data. Even if a ranker can produce a perfect list as judged by A , its performance will be suboptimal with respect to B , given inevitable inter-assessor disagreement. In effect, no ranking algorithm can simultaneously satisfy two or more disagreeing assessors (or users). Thus, there are inherent limitations in the performance of ranking algo-rithms, independent of the quality of the learning/ranking algorithm, the availability of sufficient training data, the quality of extracted instance features, and so on.
We model inter-assessor disagreement with a confusion matrix C , where c ij corresponds to the (conditional) proba-bility that a document labeled i by testing assessor B will be labeled j by training assessor A , for some given set of label grades such as { 0 , 1 , 2 , 3 , 4 } . Given such a model of inter-assessor disagreement, we ask the question,  X  X hat is the 9  X  This work supported by NSF grant IIS-1017903. expected performance of a ranked list optimized for train-ing assessor A but evaluated with respect to testing assessor B ? X  We approach this question in two ways, via simulation and closed form approximation . In the former case, we use the confusion matrix C to probabilistically generate training labels A from testing labels B , optimally rank documents ac-cording to A , and evaluate with respect to B . In the latter case, we analytically derive a closed-form approximation to this limiting performance, as measured by nDCG.

Given a confusion matrix C modeling inter-assessor dis-agreement, one can apply our results to any learning-to-rank dataset. The limiting nDCG values obtained correspond to reasonable upper bounds on the nDCG performance of any learning-to-rank algorithm, even one given unlimited train-ing data and perfect features. Considering the performance of existing algorithms on these datasets, and comparing with the upper bounds we derive, one can argue that learning-to-rank is approaching reasonable limits on achievable perfor-mance.
Much research in the IR community has focused on ad-dressing the problem of system evaluation in the context of missing, incomplete, or incorrect document judgments [1]. Soboroff and Carterette [4] provide an in-depth analysis of the effect of assessor disagreement on the Million Query Track evaluation techniques. Both assessors and users of-ten disagree on the degree of document relevance to a given query, and we model such disagreement with a confusion matrix C as described above. On data sets with multiple assessments per query-document pair, such as the TREC Enterprise Track [2], these confusion matrices can be di-rectly estimated from data, and they can be obtained from user studies as well [3, 5].

For any ranked list returned by a system, the expected limiting nDCG due to assessor disagreement can be formu-lated as a function of (1) the disagreement model C and (2) the number of assessed documents and their distribution over the label classes. One way to compute this expected nDCG is numerical simulation: For every document d hav-ing testing label i d in the ranked list, we randomly draw an alternative label j d with the probability c ij ; we then sort the ranked list in decreasing order of { j d } and evaluate its nDCG performance with respect to labels { i d } . This simu-lation is repeated multiple times and the results averaged to obtain an accurate estimate of the expected limiting nDCG.
In our first experiment, we test whether the inter-assessor confusion matrix C alone can be used to estimate the lim-iting nDCG value. We do so by considering data sets that have multiple judgments per query-document pair, such as were collected in the TREC Enterprise Track where each Figure 1: Applying C ENT model (left) and C MSR model (right) to TREC enterprise data. X-axis is the simulated nDCG upper bound, while Y-axis is the actual nDCG assessor disagreement measured between 2 TREC assessors; pairs of assessor type ( X  X old-Silver X  as GS) are indicated by colors. topic was judged by three assessors: a gold assessor G (ex-pert on task and topic), a silver assessor S (expert at task but not at topic), and a bronze assessor B (expert at nei-ther). Each G , S , and B set of assessments can take on the role of training or testing assessor, as described above, giving rise to six possible combinations: GS, GB, SG, SB, BG, BS. For each such combination, such as GS, the optimal ranked list can be computed with respect to G and evaluated with respect to S, resulting in a real suboptimal nDCG. The GS confusion matrix can also be computed from the data given and the simulation described above performed, yielding an estimated limiting nDCG. These actual and estimated lim-iting nDCG values can then be compared.

Using the TREC Enterprise data, Figure 1 compares the estimated limiting nDCG obtained through simulation with a confusion matrix ( x -axis) with the real suboptimal nDCG ( y -axis) obtained from different assessors. The left plot uses a confusion matrix C ENT obtained from the TREC Enter-prise data itself, as described above, while the right plot uses a confusion matrix C MSR obtained from a user study conducted by Microsoft Research [3]. Note that the more accurate confusion matrix yields better simulated results, as expected, and that the confusion matrix alone can be used to accurately estimate limiting nDCG values in most cases.
Given that a confusion matrix alone can be used, via simu-lation, to estimate limiting nDCG performance, we next con-sider other data sets and their associated real or estimated confusion matrices. Yandex [5] conducted user studies to ob-tain confusion matrices specific to Russian search ( C Y anR and to Ukrainian search ( C Y anU ), and these were shown to improve learning-to-rank performance if the learner was given such models as input. Table 1 presents the estimated limiting nDCG values when applying three different confu-sion matrices to two learning-to-rank data sets. For com-parison, the last column in the table presents the actual best known performance of a learning algorithm on these data sets. Consider the difference between the estimated limiting nDCG bounds (middle three columns) and known ranking performance (last column): If C MSR is a good model of assessor disagreement for these data sets, then the known learning-to-rank performance is reasonably close to the lim-iting bound, and little improvement is possible. On the other hand, if C YanU is a better model of inter-assessor disagree-ment, then learning algorithms have room for improvement.
Let L = { 0 , 1 , 2 , 3 , 4 } be the set of relevance grades, n number of documents with reference label k  X  X  , n the total Table 1: nDCG upper bounds derived from dis-agreement models C applied to popular learning-to-rank data sets. SIM rows are simulated values; the CFA rows are closed form approx. Last column is best known learning-to-rank performance. number of documents in the rank-list, and P rank ( i, r ) the probability that the rank of a given document with reference label i is r , as ordered by the alternative labels j . One can then show that the expected nDCG as measured by reference labels is and  X  ij ( h, s ) is the probability that other s documents have the same alternative label j , and other h documents have alternative label higher than j , given that a particular doc-ument with reference label i has alternative label j . Com-puting  X  ij ( h, s ) straightforwardly is inefficient for even mod-erately long rank-lists, with a running time of O ( n 2 |L|
We instead employ a closed form approximation (CFA) based on approximating  X , a sum-product of binomial con-ditional distributions, with a Gaussian joint distribution of two variables ( h + s, s ). This approximation becomes more accurate as rank-lists get longer. For a fixed i and j we have (3)  X  s =  X  c ij + P k  X  X  n k  X  c kj , and We can approximate E [ nDCG ] in O ( n 2 ) time given that the  X  X pread X  of the Gaussian grows as O ( The CFA rows of Table 1 show closed form approximations for comparison with simulated nDCG upper bounds.
We present a simple probabilistic model of assessor dis-agreement and results which indicate that the performance of learning-to-rank algorithms may be approaching inherent limits imposed by such disagreement. [1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. [2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. [3] B. Carterette, P. N. Bennett, D. M. Chickering, and [4] B. Carterette and I. Soboroff. The effect of assessor [5] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the
