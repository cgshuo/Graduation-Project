 In modern data mining applications, there often exists no universal feature repre-sentation that can be used to express sim ilarity between all possible objects in a meaningful way. Thus, recent data minin g approaches employ multiple represen-tations to achieve more general results that are based on a variety of aspects. In this paper, we distinguish two types of representations and show how to combine sets of representations containing both types using so-called combination trees. The combination trees are build with respect to domain knowledge and describe multiple semantics. To employ combination trees for clustering, we introduce a multi-represented version of the hierarc hical density-based clustering algorithm OPTICS. OPTICS derives so-called cluster orderings and is quite insensitive to the parameter selection. The introduced version of OPTICS is capable to derive meaningful cluster hierarchies with respect to an arbitrary combination tree. The rest of this paper is organized as follo ws. Section 2 surveys related work. In Section 3, we define combinat ion trees. Section 4 descri bes a multi-represented version of OPTICS which is based on combination trees. In Section 5, we provide encouraging experimental results. In [1] an algorithm for spectral clusterin g of multi-represented objects is pro-posed. [2] introduces Expectation Maximization (EM) clustering and agglomer-ative clustering for multi-represented data. Finally, [3] introduces the framework of reinforcement clustering, which is applicable to multi-represented objects. However, these three approaches do not consider any semantic aspects of the un-derlying data spaces. In [4], DBSCAN [5] has been adapted to multi-represented objects distinguishing two possible sem antics. However, DBSCAN has several drawbacks leading to the development of OPTICS[6] which is the algorithm the method proposed in this paper is based on. In [4], there were two general methods to combine multiple representation for den-sity based clustering, called union and intersection method. The union method states that an object is an union core-object if there are at least k data objects in the union of the local  X  -neighborhoods. The inter section method was defined analogously. However, it is not clear which method is better suited to compare an arbitrary set of representations. In [7], the suitability of representations for one or the other combination method is dis cussed. As a result, two aspects of a data space can be distinguished, the precision space and recall space property. An ex-amples for a good precision space are word vectors because documents containing the same set of words usually describe the same content. An example for a recall space are color histograms because two images having a similar content usually have similar color distributions. Furthermore, we can state that precision spaces should be combined using the union meth od and recall spaces should be combined using the intersection method. The result of combining recall spaces improves the precision and the result of combining pr ecision spaces improves the recall. Thus, we can successively group representation of both types and construct a so-called combination tree according to the following formalization: Definition 1 (Combination Tree). Let R = { R 1 ,...,R m } . A combination tree CT for R is a tree of arbitrary degree fulfilling the following conditions:  X  CT.root denotes the root of the combination tree CT.  X  Let n be a node of CT, then n.label denotes the label of n and n.children  X  The leaves are labeled with representations, i.e. for each leaf n  X  CT :  X  The inner nodes are labeled with either the union or the intersection operator, In order to obtain the comparability of distances, we normalize the distance in rep-The algorithm OPTICS [6] works like an extended DBSCAN algorithm, comput-ing the density-connected clus ters w.r.t. all parameters  X  i that are smaller than a generic value of  X  . OPTICS does not assign cluster memberships, but stores the order in which the objects have been processed and the information can be used to assign cluster memberships. This information consists of two values for each object, its core distance and its reachability distance. To compute these informa-tion during a run of OPTICS on multi-repre sented objects, we must adapt the core distance and reachability distance predicates of OPTICS to our multi-represented approach. In the following, we will show how we can use a combination tree CT for a given set of representations R to cluster multi-represent ed objects. The (global) distance between two objects o, p  X  X  w.r.t. a combination tree CT is defined as the combination of the distances of the nodes of CT .
 Definition 2 (distance w.r.t. CT ). Let o, p  X  X  , R = { R 1 ,...,R m } ,d i be the distance function of R i , CT be a combination tree for R ,andletnbeanode in CT, i.e. n.label  X  X  X  ,  X  ,R 1 ,...,R m } .

The distance between o and p w.r.t. node n  X  CT, denoted by d n ( o, p ) ,is recursively defined by
The distance between o and p w.r.t. CT, denoted by d CT ( o, p ) , is defined by The (global)  X  -neighborhood of an object o  X  X  w.r.t. a combination tree CT is defined as the combination of the  X  -neighborhoods of the nodes of CT . Definition 3 (  X  -neighborhood w.r.t. CT ). Let o  X  X  ,  X   X  I R + , R = { R i.e. n.label  X  X  X  ,  X  ,R 1 ,...,R m } .

The  X  -neighborhood of o w.r.t. node n  X  CT, denoted by N n  X  ( o ) , is recursively defined by The  X  -neighborhood of o w.r.t. CT, denoted by N CT , X  ( o ) , is defined by Since the core distance predicate o f OPTICS is based on the concept of k -nearest neighbor ( k -NN) distances, we have to redefine the k -nearest neighbor distance of an object o w.r.t. a combination tree CT .
 Definition 4 ( k -NN distance w.r.t. CT ). Let o  X  X  ,k  X  I N , |D| X  k, R = { R 1 ,...,R m } , CT be a combination tree for R ,andletnbeanodeinCT, i.e. n.label  X  X  X  ,  X  ,R 1 ,...,R m } .

The k -nearest neighbors of o w.r.t. CT is the smallest set NN CT , k ( o )  X  X  that contains (at least) k objects and for which the following condition holds:
The k -nearest neighbor distance of o w.r.t. CT, denoted by nn-dist CT ,k ( o ) , is defined as follows: Now, we can adopt the core distance definition from OPTICS to our combination approach: If the  X  -neighborhood w.r.t. CT of an object o contains at least k objects, the core distance of o is defined as the k -nearest neighbor distance of o . Otherwise, the core distance is infinity.
 Definition 5 (core distance w.r.t. CT ). Let o  X  X  ,k  X  I N , |D| X  k, R = { R 1 ,...,R m } , CT be a combination tree for R ,andletnbeanodeinCT, i.e. n.label  X  X  X  ,  X  ,R 1 ,...,R m } .

The core distance of o w.r.t. CT ,  X  and k, denoted by Core CT , X ,k ( o ) ,is defined by The reachability distance of an object p  X  X  from o  X  X  w.r.t. CT is an asymmet-ric distance measure that is defined as the maximum value of the core distance of o and the distance between p and o .
 Definition 6 (reachability distance w.r.t. CT ). Let o, p  X  X  ,k  X  I N , |D| X  k, R = { R in CT, i.e. n.label  X  X  X  ,  X  ,R 1 ,...,R m } .
 The reachability distance of o to p w.r.t. CT ,  X  ,and k, denoted by Reach CT , X ,k ( p, o ) , is defined by We implemented the proposed clustering algorithm in Java 1.5 and ran several experiments on a work station with two 1.8 GHz Opteron processors and 8 GB main memory. The experiments were perfo rmed on protein data that is described by text descriptions ( R 1 ) and amino-acid sequences ( R 2 ). We employed entries of the Swissprot protein database 1 belonging to 5 functional groups (cf. Table 1). As reference clustering, we empl oyed the classes of Gene Ontology 2 .Toevaluate the derived cluster structure C , we extracted flat clust ers from OPTICS plots and applied the following quality measure for comparing different clusterings We employed an combination tree describ ing the union of both representations. As first comparison partners, we clustered text and sequences separately using only one of the representations. A second approach combines the features of both representations into a common feature space (CFS) and employs the co-sine distance to relate the resulting feature vectors. Additionally, we compared reinforcement clustering (RCL) using DBSCAN as underlying cluster algorithm. For reinforcement clustering, we ran 10 i terations and tried several values of the weighting parameter  X  .The  X  -parameters were set su fficiently large and we chose k = 2. Table 1 displays the derived quality for our method and the four competitive methods mentioned above . As it can be seen, our method clearly outperforms any of the other algorithms.

Another, set of experiments were performed on a data set of images being described by 4 representations. The O PTICS clustering based on a 2 level com-bination trees achieved encouraging result s as well. More information about these experiments can be found in [7].

