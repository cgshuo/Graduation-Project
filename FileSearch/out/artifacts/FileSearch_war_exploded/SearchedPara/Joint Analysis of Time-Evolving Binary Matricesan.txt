 There has been significant recent research on the analysis of incomplete matrices [10, 15, 1, 12, 13, 18]. Most analyses have been performed under the assumption that the matrix is real. There are interesting problems for which the matrices may be binary; for example, reflecting the pres-ence/absence of links on nodes of a graph, or for analysis of data associated with a series of binary questions. One may connect an underlying real matrix to binary (or, more generally, integer) obser-vations via a probit or logistic link function; for example, such analysis has been performed in the context of analyzing legislative roll-call data [6]. A problem that has received less attention concerns the analysis of time-evolving matrices. The specific motivation of this paper involves binary ques-tions in a legislative setting; we are interested in analyzing such data over many legislative sessions, and since the legislators change over time, it is undesirable to treat the entire set of votes as a single matrix. Each piece of legislation (question) is unique, but it is desirable to infer inter-relationships and commonalities over time. Similar latent groupings and relationships exist for the legislators. This general setting is also of interest for analysis of more-general social networks [8]. A distinct line of research has focused on analysis of documents, with topic modeling constituting a popular framework [4, 2, 17, 3, 11]. Although the analysis of matrices and documents has heretofore been performed independently, there are many problems for which documents and matrices may be coupled. For example, in addition to a matrix of links between websites or email sender/recipient data, one also has access to the associated documents (website and email content). By analyzing the matrices and documents simultaneously, one may infer inter-relationships about each. For example, factors to topics/words, providing insight from the documents about the matrix, and vice versa . To the authors X  knowledge, this paper represents the first joint analysis of time-evolving matrices and associated documents. The analysis is performed using nonparametric Bayesian tools; for example, the truncated Dirichlet process [7] is used to jointly cluster latent topics and matrix features. The framework is demonstrated through analysis of large-scale data sets. Specifically, we consider binary vote matrices from the United States Senate and House of Representatives, from the first congress in 1789 to the present. Documents of the legislation are available for the most recent 20 years, and those are also analyzed jointly with the matrix data. The quantitative predictive performance of this framework is demonstrated, as is the power of this setting for making qualitative assessments of large-scale and complex joint matrix-document data. 2.1 Time-evolving binary matrices of rows and columns, respectively N ( t ) y and N ( t ) x , may vary with time. For example, for the leg-islative roll-call data consider below, time index t corresponds to year and the number of pieces of legislation and legislators changes with time ( e.g. , for the historical data considered for the United States congress, the number of states and hence legislators changes as the country has grown). Using a modeling framework analogous to that in [6], the binary matrix has a probit-model gener-matrix is defined as  X  i  X  N (0 , X   X   X   X  + (1  X   X   X  ) Gamma ( a,b ) ;  X   X  is a point measure at infinity, corresponding to there not being an associated random effect. The probability of whether there is a random effect is controlled by  X   X  and  X   X  , each of which is drawn from a beta distribution.
 Random effect  X  j is motivated by our example application, for which the index j denotes a specific vote. In previous political science Bayesian analysis [6] researchers have simply set  X   X  = 1 and  X   X  = 0 , but here we consider the model in a more-general setting, and infer these relationships. Additionally, in previous Bayesian analysis [6] the dimensionality of y ( t ) i and x ( t ) j has been set trices [15, 12], priors/regularizers are employed to constrain the dimensionality of the latent fea- X  k  X  Beta ( c/K,d ( K  X  1) /K ) , for K set to a large integer. By setting c and d appropriately, this favors that most of the components of b are zero (imposes sparseness). Specifically, by inte-grating out the {  X  k } k =1 ,K , one may readily show that the number of non-zero components in b is a random variable drawn from Binomial ( K,c/ ( c + d ( K  X  1))) , and the expected number of ones in b is cK/ [ c + d ( K  X  1)] . This is related to a draw from a truncated beta-Bernoulli process [16]. person/entity that may be present for matrix t + 1 and matrix t . It is assumed here that each column corresponds to a question (in the examples, a piece of legislation), and each question is unique. Since where  X  denotes the pointwise/Hadamard vector product. If the person/entity associated with the i th row at time t is introduced for the first time, its associated feature vector is similarly drawn y i = b  X   X  y already drawn (person/entity i is active prior to time t + 1 ), then a simple auto-regressive model is prior on  X  is set to favor small/smooth changes in the features of an individual on consecutive years. This model constitutes a relatively direct extension of existing techniques for real matrices [15, 12]. Specifically, we have introduced a probit link function and a simple auto-regression construction to impose statistical correlation in the traits of a person/entity at consecutive times. The introduction of the random effects  X  j and  X  i has also not been considered within much of the machine-learning matrix-analysis literature, but the use of  X  j is standard in political science Bayesian models [6]. The principal modeling contribution of this paper concerns how one may integrate such a time-evolving binary-matrix model with associated documents. 2.2 Topic model The manner in which the topic modeling is performed is a generalization of latent Dirichlet allo-cation (LDA) [4]. Assume that the documents of interest have words drawn from a vocabulary V = { w 1 ,...,w V } . The k th topic is characterized by a distribution p k on words ( X  X ag-of-words X  assumption), where p k  X  Dir (  X  V /V,..., X  V /V ) . The generative model draws { p k } k =1 ,T once for each of the T possible topics.
 Each document is characterized by a probability distribution on topics, where the c l  X  Dir (  X  T /T,..., X  T /T ) corresponds to the distribution across T topics for document l . The gen-erative process for drawing words for document l is to first (and once) draw c l for document l . For word i in document l , we draw a topic z il  X  Mult ( c l ) , and then the specific word is drawn from a multinomial with probability vector p z il .
 The above procedure is like the standard LDA [4], with the difference manifested in how we handle the Dirichlet distributions Dir (  X  V /V,..., X  V /V ) and Dir (  X  T /T,..., X  T /T ) . The Dirichlet dis-tribution draws are constituted via Sethuraman X  X  construction [14]; this allows us to place gamma priors on  X  V and  X  T , while retaining conjugacy, permitting analytic Gibbs X  sampling (we there-fore get a full posterior distribution for all model parameters, while most LDA implementations employ a point estimate for the document-dependent probabilities of topics). Specifically, the fol-lowing hierarchical construction is used for draws from Dir (  X  V /V,..., X  V /V ) (and similarly for Dir (  X  T /T,..., X  T /T ) ): The probability mass a h is associated with component  X  h  X  { 1 ,...,V } of the probability vec-tor. The infinite sum is truncated, analogous to the truncated stick-breaking representation of the Dirichlet process [9]. 2.3 Joint analysis of matrices and documents Section 2.1 discusses how we model time-evolving binary matrices, and Section 2.2 describes our procedure for implementing topic models. We now put these two models together. Specifically, we consider the case for which there is a document D ( t ) j of words associated with the j th column at time t ; in our example below, this will correspond to the j th piece of legislation in year t . It is possible that we may have documents associated with the matrix rows as well ( e.g. , speeches for the i th legislature), but in our model development (and in our examples), documents are only assumed present for the columns.
 For column j at time t , we have both a feature vector x ( t ) j (for the matrix) and a distribution on topics c ( t ) j (for the document D ( t ) j ), and these are now coupled; the remainder of the matrix and drawn from N ( 0 , X   X  1 x I K ) , again with a gamma prior placed on  X  x , and  X   X  m are also drawn from a gamma distribution; the c  X  m are drawn iid from Dir (  X  T /T,..., X  T /T ) , using the Dirichlet distribu-x j = b  X   X  x This construction clusters the columns, with the clustering mechanism defined by a truncated stick-breaking representation of the Dirichlet process [9]. The components {  X   X  m , X   X  m } m =1 ,M define a Gaussian mixture model (GMM) in matrix-column feature space, while the { c  X  m } m =1 ,M define a set of M probability vectors over topics, with one such vector associated with each of the afore-mentioned GMM mixture components. The truncated Dirichlet process infers how many mixture components are needed to represent the data.
 In this construction, each of the matrix columns is associated with a distribution on topics (based upon which mixture component it is drawn from). This provides powerful interpretative insights between the latent features in the matrix model and the words from the associated documents. Fur-ther, since the topic and matrix models are constituted jointly, the topics themselves are defined as to be best matched to the characteristics of the matrix ( vis-a-vis simply modeling the documents in isolation, which may yield topics that are not necessarily well connected to what matters for the matrices). A graphical representation of the model is shown in Figure 1.
 There are several extensions one may consider in future work. For example, for simplicity the GMM in column feature space is assumed time-independent. One may consider having a separate GMM for each time (year) t . Further, we have not explicitly imposed time-dependence in the topic model itself, and this may also be considered [2, 11]. For the examples presented below on real data, despite these simplifications, the model seems to perform well. 2.4 Computations The posterior distribution of all model parameters has been computed using Gibbs sampling; the detailed update equations are provided as supplemental material at http://sites.google. com/site/matrixtopics/ . The first 1000 Gibbs iterations were discarded as burn-in followed by 500 collection iterations.The truncation levels on the model are T = 20 , M = 10 , K = 30 , and the number of words in the vocabulary is V = 5249 . Hyperparameters were set as a = b = e = f = 10  X  6 , c = d = 1 , g = 10 3 , and h = 10  X  3 . None of these parameters have been optimized, and  X  X easonable X  related settings yield very similar results.
 We have performed joint matrix and text analysis considering the United States Congress voting records (and, when available, the document associated with the legislation); we consider both the House of Representatives (House) and Senate, from 1789-2008. Legislation documents and meta-data (bill sponsorship, party affiliation of voters, etc.) are available for sessions 101 X 110 (1989-2008). For the legislation, stop words were removed using a common stopword list (the 514 stop words are posted at http://sites.google.com/site/matrixtopics/ , and the corpus was stemmed using a Porter stemmer). These data are available from www.govtrack.us and from the Library of Congress thomas.loc.gov (votes, text and metadata), while the votes dat-ing from 1789 are at voteview.com . A binary matrix is manifested by mapping all  X  X ffirmative X  vote codes ( e.g. ,  X  X ea X ,  X  X es X ,  X  X resent X ) to one, and  X  X egative X  codes ( e.g. ,  X  X ay X , X  X o X , X  X ot Present X ) to zero. Not all legislatures are present to vote on a given piece of legislation, and there-fore missing data are manifested naturally. It varies from year to year, but typically 4% of the votes are missing in a given year.
 We implemented our proposed model in non-optimized Matlab. Computations were performed on a PC with a 3.6GHz CPU and 4GB memory. A total of 11.5 hours of CPU time are required for analysis of Senate sessions 101-110 (1989-2008), and 34.6 hours for House sessions 101-110; in both cases, this corresponds to joint analysis of both votes and text (legislation). If we only analyze the votes, 15.5 hours of CPU are required for Senate session 1-110 (1789-2008), and 62.1 hours for House 1-110 respectively (the number of legislators in the House is over four times larger than that for the Senate). 3.1 Joint analysis of documents and votes We first consider the joint analysis of the legislation (documents) and votes in the Senate, for 1989-2008. A key aspect of this analysis is the clustering of the legislation, with legislation j at time t mapped to a cluster (mixture component), with each mixture component characterized by a distri-Section 2.3). Five dominant clusters were inferred for these data. Since we are running a Gibbs sampler, and the cluster index changes in general between consecutive iterations (because the index is exchangeable), below we illustrate the nature of the clusters based upon the last Gibbs iteration. The dimensionality of the features was inferred to be k b k 0 = 5 (on average, across the Gibbs collection), but two dimensions dominated for the legislation feature vectors x ( t ) j . In Figure 2 we present the inferred distributions of the five principal mixture components (clusters). The cluster index and the indices of the features are arbitrary; we, for example, number the clusters from 1 to 5 for illustrative simplicity.
 In Figure 2 we depict the distribution of topics c  X  m associated with each of the five clusters, and in Figure 3 we list the ten most probable words associated with each of the topics. By examining the topic characteristics in Figure 3, and the cluster-dependent distribution of topics, we may assign itself. For example, clusters 1 and 4, which are the most separated in latent space (top row in Figure 2), share a very similar support over topics (bottom row in Figure 2). These clusters appear to be as-sociated with highly partisan topics, specifically taxes (topics 11 and 15) and health/Medicare/Social Security (topics 12 and 16), as can be seen by considering the topic-dependent words in Figure 3. Based upon the voting data and the party of the legislation sponsor (bill author), cluster 1 (red) appears to represent a Republican viewpoint on these topics, while cluster 4 (blue) appears to repre-sent a Democratic viewpoint. This distinction will play an important role in predicting the votes on legislation based on the documents, as discussed below in Section 3.2.
 In Figure 4 (last plot) we present the estimated density functions for the random-effect parame-tightly concentrated around zero than p (  X  ) . In the political science literature [6] (in which the leg-islation/documents have not been considered), researchers simply just set  X  = 0 , and therefore only assume random effects on the legislation, but not on the senators/congressman. Our analysis appears to confirm that this simplification is reasonable. 3.2 Matrix prediction based on documents There has been significant recent interest in the analysis of matrices, particularly in predicting matrix entries that are missing at random [10, 15, 1, 12, 13, 18]. In such collaborative-filtering research, the views of a subset of individuals on a movie, for example, help inform predictions on ratings of people who have not seen the movie (but a fraction of the people must have seen every movie). However, in the problem considered here, these previous models are not applicable: prediction of votes on a new legislation L N requires one to relate L N to votes on previous legislation L 1 ,..., L N  X  1 , but in the absence of any prior votes on L N ; this corresponds to estimating an entire column of the vote matrix). The joint analysis of text (legislation) and votes, however, offers the ability to relate L N to L 1 ,..., L N  X  1 , by making connections via the underlying topics of the legislation (documents), even in the absence of any votes for L N .
 To examine this predictive potential, we performed joint analysis on all votes and legislation (doc-uments) in the US Senate from 1989-2007. Through this process, we yielded a model very similar to that summarized in Figures 2-4. Using this model, we predict votes on new legislation in 2008, based on the documents of the associated legislation (but using no vote information on this new legislation). To do this, the mixture of topics learned from 1989-2007 data are assumed fixed (each topic characterized by a distribution over words), and these fixed topics are used in the analysis of the documents from new legislation. In this manner, each of the new documents is mapped to one mapped to cluster m (with mapping based upon the words alone), it is then assumed that the latent matrix feature associated with the legislation is the associated cluster mean  X   X  m (learned via the modeling of 1989-2007 data).
 Once this mapping of legislation to matrix latent space is achieved, and using the senator X  X  latent function the probability of a  X  X es X  vote is quantified, for Senator i on new legislation L N . This is  X  i = 0 is reasonable. The legislation-dependent random effect  X  When testing the predictive quality of the model for the held-out year 2008, we assume  X  ( t ) j = 0 (since this parameter cannot be inferred without modeling the text and votes jointly, while for 2008 we are only modeling the documents); we therefore only test the model on legislation from 2008 for which less than 90% of the senators agreed, such legislation assumed corresponding to small |  X  j | (it is assumed that in practice it would be simple to determine whether a piece of legislation is likely to be near-unanimous  X  X es X  or  X  X o X , and therefore model-based prediction of votes for such legislation is deemed less interesting).
 In Figure 4 we compare the predicted, probit-based probability of a given senator voting  X  X es X  for legislation within clusters 1-4 (see Figure 2); the points in Figure 4 represent the empirical data for each senator, and the curve represents the predictions of the probit link function. These results are deemed to be remarkably good. In Figure 4, the senators along each horizontal axis are ordered according to the probability of voting  X  X es X .
 One interesting issue that arises in this prediction concerns clusters 1 and 4 in Figure 2, and the as-sociated predictions for the held-out year 2008, in Figure 4. Since the distributions of these clusters over topics is very similar, the documents alone cannot distinguish between clusters 1 and 4. How-ever, we also have the sponsor of each piece of legislation, and based upon the data from 1989-2007, if a piece of legislation from 2008 is mapped to either cluster 1 or 4, it is disambiguated based upon the party affiliation of the sponsor (cluster 1 is a Republican viewpoint on these topics, while cluster 4 is a Democratic viewpoint, based upon voting records from 1989-2007). 3.3 Time evolution of congressman and legislation The above joint analysis of text and votes was restricted to 1989-2008, since the documents (leg-islation) were only available for those years. However, the dataset contains votes on all legislation from 1789 to the present, and we now analyze the vote data from 1789-1988. Figure 5 shows snapshots in time of the latent space for voters and legislation, for the House of Representatives (similar results have been computed for the Senate, and are omitted for brevity; as supplemental ma-terial, at http://sites.google.com/site/matrixtopics/ we present movies of how legislation and congressman evolve across all times, for both the House and Senate). Five features were inferred, with the two highest-variance features chosen for the axes. The blue symbols denote Democratic legislators, or legislation sponsored by a Democrat, and the red points correspond to Republicans. Results like these are of interest to political scientists, and allow examination of the degree of partisanship over time, for example. Figure 5: Congressman (top) and legislation (bottom) in latent space for sessions 1 X 98 of the House of Rep-3.4 Additional quantitative tests One may ask how well this model addresses the more-classical problem of estimating the values of matrix data that are missing uniformly at random, in the absence of documents. To examine this question, we considered binary Senate vote data from 1989-2008, and removed a fraction of the votes uniformly at random, and then use the proposed time-evolving matrix model to process the observed data, and to compute the probability of a  X  X es X  vote on all missing data (via the probit link function). If the probability is larger than 0.5 the vote is set to  X  X es X , and otherwise it is set to  X  X o X . We compare our time-evolving model to [12], with the addition of a probit link function; for the latter we processed all 20 years as one large matrix, rather than analyzing time-evolving structure. Up to 40% missingness, the proposed model and a modified version of that in [12] performed almost greater than 40% missingness, the proposed time-evolving model manifested a  X  X hase transition X , and the probability of error increased smoothly up to 0.3, as the fraction of missing data rose to 80%; in contrast, the generalized model in [12] (with probit link) continued to yield a probability of error of about 0.1. The phase transition of the proposed model is likely manifested because the entire matrix is partitioned by year, with a linkage between years manifested via the Markov process between legislators (we don X  X  analyze all data by one contiguous, large matrix). The phase transition is expected based on the theory in [5], when the fraction of missing data gets large enough (since the size of the contiguous matrices analyzed by the time-evolving model is much smaller than that of the entire matrix, such a phase transition is expected with less missingness than via analysis of the entire matrix at once).
 While the above results are of interest and deemed encouraging, such uniformly random missingness on matrix data alone is not the motivation of the proposed model. Rather, traditional matrix-analysis methods [10, 15, 1, 12, 13, 18] are incapable of predicting votes on new legislation based on the words alone (as in Figure 4), and such models do not allow analysis of the time-evolving properties of elements of the matrix, as in Figure 5. A new model has been developed for the joint analysis of time-evolving matrices and associated documents. To the authors X  knowledge, this paper represents the first integration of research hereto-fore performed separately on topic models and on matrix analysis/completion. The model has been implemented efficiently via Gibbs sampling. A unique set of results are presented using data from the US Senate and House of Representatives, demonstrating the ability to predict the votes on new legislation, based only on the associated documents. The legislation data was considered principally because it was readily available and interesting in its own right; however, the proposed framework is of interest for many other problems. For example, the model is applicable to analysis of time-evolving relationships between multiple entities, augmented by the presence of documents ( e.g. , links between websites, and the associated document content).
 The research reported here was supported by the US Army Research Office, under grant W911NF-08-1-0182, and the Office of Naval Research under grant N00014-09-1-0212. [1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative filtering: [2] D. M. Blei and J. D. Lafferty. Dynamic topic models. Proceedings of the 23rd International [3] D. M. Blei and J. D. Lafferty. A correlated topic model of science. The Annals of Applied [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine [5] E.J. Cand ` es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. [6] J. Cinton, S. Jackman, and D. Rivers. The statistical analysis of roll call data. Am. Political Sc. [7] T. S. Ferguson. A bayesian analysis of some nonparametric problems. The Annals of Statistics , [8] P. D. Hoff. Multiplicative latent factor models for description and prediction of social networks. [9] J. Ishwaran and L. James. Gibbs sampling methods for stick-breaking priors. Journal of the [10] E. Meeds, Z. Ghahramani, R. Neal, and S. Roweis. Modeling dyadic data with binary latent [11] I. Pruteanu-Malinici, L. Ren, J. Paisley, E. Wang, and L. Carin. Hierarchical bayesian modeling [12] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization with mcmc. In [13] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in NIPS , 2008. [14] J. Sethuraman. A constructive definition of dirichlet priors. Statistica Sinica , 4:639 X 650, 1994. [15] N. Srebro, J.D.M. Rennie, and T.S. Jaakkola. Maximum-margin matrix factorization. In Ad-[16] R. Thibaux and M.I. Jordan. Hierarchical beta processes and the indian buffet process. In [17] H. M. Wallach. Topic modeling: beyond bag of words. Proceedings of the 23rd International [18] K. Yu, J. Lafferty, S. Zhu, and Y. Gong. Large-scale collaborative prediction using a nonpara-
