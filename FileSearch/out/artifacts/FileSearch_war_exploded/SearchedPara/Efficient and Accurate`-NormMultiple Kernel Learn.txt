 Sparseness is being regarded as one of the key features in machine learning [15] and biology [16]. Sparse models are appealing since they provide an intuitive interpretation of a task at hand by sin-gling out relevant pieces of information. Such automatic complexity reduction facilitates efficient training algorithms, and the resulting models are distinguished by small capacity. The interpretabil-ity is one of the main reasons for the popularity of sparse methods in complex domains such as computational biology, and consequently building sparse models from data has received a signifi-cant amount of recent attention.
 Unfortunately, sparse models do not always perform well in practice [7, 15]. This holds particularly for learning sparse linear combinations of data sources [15], an abstraction of which is known as multiple kernel learning (MKL) [10]. The data sources give rise to a set of (possibly correlated) kernel matrices K 1 ,...,K M , and the task is to learn the optimal mixture K = P m  X  m K m for the problem at hand. Previous MKL research aims at finding sparse mixtures to effectively simplify the underlying data representation. For instance, [10] study semi-definite matrices K 0 inducing sparseness by bounding the trace tr ( K )  X  c ; unfortunately, the resulting semi-definite optimization problems are computationally too expensive for large-scale deployment.
 Recent approaches to MKL promote sparse solutions either by Tikhonov regularization over the mixing coefficients [25] or by incorporating an additional constraint k  X  k  X  1 [18, 27] requiring solutions on the standard simplex, known as Ivanov regularization. Based on the one or the other, efficient optimization strategies have been proposed for solving ` 1 -norm MKL using semi-infinite linear programming [21], second order approaches [6], gradient-based optimization [19], and level-set methods [26]. Other variants of ` 1 -norm MKL have been proposed in subsequent work address-ing practical algorithms for multi-class [18, 27] and multi-label [9] problems. Previous approaches to MKL successfully identify sparse kernel mixtures, however, the solutions unweighted-sum kernels K = P m K m are observed to outperform the sparse mixture [7]. One rea-son for the collapse of ` 1 -norm MKL is that kernels deployed in real-world tasks are usually highly sophisticated and effectively capture relevant aspects of the data. In contrast, sparse approaches to MKL rely on the assumption that some kernels are irrelevant for solving the problem. Enforcing sparse mixtures in these situations may lead to degenerate models. As a remedy, we propose to sacrifice sparseness in these situations and deploy non-sparse mixtures instead. After submission of used [12]. Although non-sparse solutions are not as easy to interpret, they account for (even small) contributions of all available kernels to live up to practical applications.
 [18, 25, 27]. Our theorem allows for a generalized view of recent strands of multiple kernel learn-ing research. Based on the detached view, we extend the MKL framework to arbitrary ` p -norm MKL with p  X  1 . Our approach can either be motivated by additionally regularizing over the mix-alternative optimization strategies based on Newton descent and cutting planes, respectively. Em-pirically, we demonstrate the efficiency and accuracy of none-sparse MKL. Large-scale experiments on gene start detection show a significant improvement of predictive accuracy compared to ` 1 -and `  X  -norm MKL.
 the theoretical analysis of existing approaches to MKL, our ` p -norm MKL generalization with two highly efficient optimization strategies, and relations to ` 1 -norm MKL. We report on our empirical results in Section 3 and Section 4 concludes. 2.1 Preliminaries the x lie in some input space X and y  X  Y  X  R . The goal is to find a hypothesis f  X  H , that generalizes well on new and unseen data. Applying regularized risk minimization returns the minimizer f  X  , R  X Y  X  R , regularizer  X  : H  X  R , and trade-off parameter  X  &gt; 0 . In this paper, we focus on  X ( f ) = 1 2 k  X  w k 2 2 and on linear models of the form together with a (possibly non-linear) mapping  X  : X  X  H to a Hilbert space H [20]. We will later make use of kernel functions K ( x , x 0 ) =  X   X  ( x ) , X  ( x 0 )  X  H to compute inner products in H . 2.2 Learning with Multiple Kernels When learning with multiple kernels, we are given M different feature mappings  X  m : X  X  H m , m = 1 ,...M , each giving rise to a reproducing kernel K m of H m . Approaches to multi-ple kernel learning consider linear kernel mixtures K  X  = P  X  m K m ,  X  m  X  0 . Compared to Eq. (1), the primal model for learning with multiple kernels is extended to where the weight vector  X  w and the composite feature map  X   X  have a block structure  X  w = optimal kernel mixture P  X  m K m in addition to regularizing  X  to avoid overfitting. Hence, in terms of regularized risk minimization, the optimization problem becomes Previous approaches to multiple kernel learning employ regularizers of the form  X   X (  X  ) = ||  X  || 1 to promote sparse kernel mixtures. By contrast, we propose to use smooth convex regularizers of the resulting optimization problem is not inherent and can be resolved by substituting w m  X  Furthermore, regularization parameter and sample size can be decoupled by introducing  X  C = 1 n X  (and adjusting  X   X   X   X   X  ) which has favorable scaling properties in practice. We obtain the following convex optimization problem [5] that has also been considered by [25] for hinge loss and p = 1 , where we use the convention that t 0 = 0 if t = 0 and  X  otherwise. An alternative approach has been studied by [18, 27] (again using hinge loss and p = 1 ). They upper bound the value of the regularizer k  X  k 1  X  1 and incorporate the latter as an additional constraint into the optimization problem. For C &gt; 0 , they arrive at Our first contribution shows that both, the Tikhonov regularization in Eq. (4) and the Ivanov regu-larization in Eq. (5), are equivalent.
 Theorem 1 Let be p  X  1 . For each pair (  X  C, X  ) there exists C &gt; 0 such that for each optimal of Eq. (5) using C , and vice versa, where  X  &gt; 0 is some multiplicative constant. Proof. The proof is shown in the supplementary material for lack of space. Sketch of the proof: We incorporate the regularizer of (4) into the constraints and show that the resulting upper bound is tight. A variable substitution completes the proof. 2 Zien and Ong [27] showed that the MKL optimization problems by Bach et al. [3], Sonnenburg et al. [21], and their own formulation are equivalent. As a main implication of Theorem 1 and by using the result of Zien and Ong it follows that the optimization problem of Varma and Ray [25] and the ones from [3, 18, 21, 27] all are equivalent.
 In addition, our result shows the coupling between trade-off parameter C and the regularization pa-rameter  X  in Eq. (4): tweaking one also changes the other and vice versa. Moreover, Theorem 1 implies that optimizing C in Eq. (5) implicitly searches the regularization path for the parameter  X  of Eq. (4). In the remainder, we will therefore focus on the formulation in Eq. (5), as a single param-eter is preferable in terms of model selection. Furthermore, we will focus on binary classification However note, that all our results can easily be transferred to regression and multi-class settings using appropriate convex loss functions and joint kernel extensions. 2.3 Non-Sparse Multiple Kernel Learning We now extend the existing MKL framework to allow for non-sparse kernel mixtures  X  , see also [13]. Let us begin with rewriting Eq. (5) by expanding the hinge loss into the slack variables as follows Applying Lagrange X  X  theorem incorporates the constraints into the objective by introducing non-negative Lagrangian multipliers  X  ,  X   X  R n ,  X   X  R M , X   X  R (including a pre-factor of 1 p for the  X  -Term). Resubstitution of optimality conditions w.r.t. to w , b ,  X  , and  X  removes the dependency of the Lagrangian on the primal variables. After some additional algebra (e.g., the terms associated with  X  cancel), the Lagrangian can be written as subject to  X  &gt; y = 0 , 0  X   X  i  X  C for 1  X  i  X  n , and  X   X  0 . Let us ignore for a moment the non-negativity  X   X  0 and solve  X  L / X  X  = 0 for the unbounded  X  . Setting the partial derivative to zero yields Interestingly, at optimality, we always have  X   X  0 because the quadratic term in  X  is non-negative. Plugging the optimal  X  into Eq. (7), we arrive at the following optimization problem which solely depends on  X  . In the limit p  X  X  X  , the above problem reduces to the SVM dual (with Q = P m Q m ), while p  X  1 gives rise to a QCQP ` 1 -MKL variant. However, optimizing the dual efficiently is difficult and will cause numerical problems in the limits p  X  1 and p  X  X  X  . 2.4 Two Efficient Second-Order Optimization Strategies Many recent MKL solvers (e.g., [19, 24, 26]) are based on wrapping linear programs around SVMs. From an optimization standpoint, our work is most closely related to the SILP approach [21] and algorithms. The two alternative approaches proposed for ` p -norm MKL proposed in this paper are largely inspired by these methods and extend them in two aspects: customization to arbitrary norms and a tight coupling with minor iterations of an SVM solver, respectively.
 Our first strategy interleaves maximizing the Lagrangian of (6) w.r.t.  X  with minor precision and Newton descent on  X  . For the second strategy, we devise a semi-infinite convex program, which we solve by column generation with nested sequential quadratically constrained linear programming (SQCLP). In both cases, the maximization step w.r.t.  X  is performed by chunking optimization with minor iterations. The Newton approach can be applied without a common purpose QCQP solver, however, convergence can only be guaranteed for the SQCLP [8]. 2.4.1 Newton Descent For a Newton descent on the mixing coefficients, we first compute the partial derivatives of the original Lagrangian. Fortunately, the Hessian H is diagonal, i.e. given by H = diag ( h ) . The m -th element s m of the corresponding Netwon step, defined as s :=  X  H  X  1  X   X  , is thus computed by where  X  is defined in Eq. (8). However, a Newton step  X  t +1 =  X  t + s might lead to non-positive  X  . To avoid this awkward situation, we take the Newton steps in the space of log(  X  ) by adjusting the derivatives according to the chain rule. We obtain which corresponds to multiplicative update of  X  : Furthermore we additionally enhance the Newton step by a line search. 2.4.2 Cutting Planes In order to obtain an alternative optimization strategy, we fix  X  and build the partial Lagrangian for lack of space. The resulting dual problem is a min-max problem of the form The above optimization problem is a saddle point problem and can be solved by alternating  X  and  X  optimization step. While the former can simply be carried out by a support vector machine for a fixed mixture  X  , the latter has been optimized for p = 1 by reduced gradients [18]. We take a different approach and translate the min-max problem into an equivalent semi-infinite program (SIP) as follows. Denote the value of the target function by t (  X  ,  X  ) and suppose  X   X  is  X  . Hence, we can equivalently minimize an upper bound  X  on the optimal value and arrive at for all  X   X  R n with 0  X   X   X  C 1 , and y &gt;  X  = 0 as well as k  X  k p p  X  1 and  X   X  0 . [21] optimize the above SIP for p  X  1 with interleaving cutting plane algorithms. The solution of a quadratic program (here the regular SVM) generates the most strongly violated constraint for the actual mixture  X  . The optimal (  X   X  , X  ) is then identified by solving a linear program with respect to the set of active constraints. The optimal mixture is then used for computing a new constraint and so on.
 unlikely to be found in standard optimization toolboxes that often handle only linear and quadratic constraints. As a remedy, we propose to approximate k  X  k p p  X  1 by sequential second-order Taylor expansion of the form using  X   X  =  X  t . Note that the quadratic term in the approximation is diagonal wherefore the subse-quent quadratically constrained problem can be solved efficiently. Finally note, that this approach can be further sped-up by an additional projection onto the level-sets in the  X  -optimization phase similar to [26]. In our case, the level-set projection is a convex quadratic problem with ` p -norm constraints and can again be approximated by successive second-order Taylor expansions. In this section we study non-sparse MKL in terms of efficiency and accuracy. 1 We apply the method `  X  -norm MKL for a regular SVM with the unweighted-sum kernel K = P m K m . 3.1 Execution Time We demonstrate the efficiency of our implementations of non-sparse MKL. We experiment on the MNIST data set where the task is to separate odd vs. even digits. We compare our ` p -norm MKL with two methods for ` 1 -norm MKL, simpleMKL [19] and SILP-based chunking [21], and to SVMs using the unweighted-sum kernel ( `  X  -norm MKL) as additional baseline. We optimize all methods up to a precision of 10  X  3 for the outer SVM- X  and 10  X  5 for the  X  X nner X  SIP precision and computed relative duality gaps. To provide a fair stopping criterion to simpleMKL, we set the stopping criterion of simpleMKL to the relative duality gap of its ` 1 -norm counterpart. This way, the deviations of relative objective values of ` 1 -norm MKL variants are guaranteed to be smaller than 10  X  4 . SVM trade-off parameters are set to C = 1 for all methods.
 Figure 1 (left) displays the results for varying sample sizes and 50 precomputed Gaussian kernels with different bandwidths. Error bars indicate standard error over 5 repetitions. Unsurprisingly, the SVM with the unweighted-sum kernel is the fastest method. Non-sparse MKL scales similarly as ` 1 -norm chunking; the Newton strategy (Section 2.4.1) is slightly faster than the cutting plane variant (Section 2.4.2) that needs additional Taylor expansions within each  X  -step. SimpleMKL suffers from training an SVM to full precision for each gradient evaluation and performs worst. 2 Figure 1 (right) shows the results for varying the number of precomputed RBF kernels for a fixed sample size of 500. The SVM with the unweighted-sum kernel is hardly affected by this setup and performs constantly. The ` 1 -norm MKL by [21] handles the increasing number of kernels best and is the fastest MKL method. Non-sparse approaches to MKL show reasonable run-times, the Newton-based ` p -norm MKL being again slightly faster than its peer. Simple MKL performs again worst. Overall, our proposed Newton and cutting plane based optimization strategies achieve a speedup of often more than one order of magnitude. 3.2 Protein Subcellular Localization The prediction of the subcellular localization of proteins is one of the rare empirical success stories the unweighted sum of kernels (thereby also improving on established prediction systems for this problem). Here we investigate the performance of non-sparse MKL.
 We download the kernel matrices of the dataset plant 3 and follow the experimental setup of [17] with the following changes: instead of a genuine multiclass SVM, we use the 1-vs-rest decom-position; instead of performing cross-validation for model selection, we report results for the best models, as we are only interested in the relative performance of the MKL regularizers. Specifically, efficient (MCC) on the test data. For each norm, the best average MCC is recorded. Table 1 shows the averages over several splits of the data.
 The results indicate that, indeed, with proper choice of a non-sparse regularizer, the accuracy of ` -norm can be recovered. This is remarkable, as this dataset is particular in that it fullfills the rare condition that ` 1 -norm MKL performs better than `  X  -norm MKL. In other words, selecting these data may imply a bias towards ` 1 -norm. Nevertheless our novel non-sparse MKL can keep up with this, essentially by approximating ` 1 -norm. 3.3 Gene Start Recognition This experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences. Accurate detection of the transcription start site is crucial to identify genes and their promoter regions and can be regarded as a first step in deciphering the key regulatory elements in the promoter region that determine transcription. For our experiments we use the dataset from [22] which contains a curated set of 8,508 TSS annotated genes built from dbTSS version 4 [23] and refseq genes. These are translated into positive training instances by extracting windows of size [  X  1000 , +1000] around the TSS. Similar to [4], 85,042 negative instances are generated from the interior of the gene using the same window size.
 Following [22], we employ five different kernels representing the TSS signal (weighted degree with shift), the promoter (spectrum), the 1st exon (spectrum), angles (linear), and energies (linear). Opti-mal kernel parameters are determined by model selection in [22]. Every kernel is normalized such that all points have unit length in feature space. We reserve 13,000 and 20,000 randomly drawn instances for holdout and test sets, respectively, and use the remaining 60,000 as the training pool. Figure 2 shows test errors for varying training set sizes drawn from the pool; training sets of the same size are disjoint. Error bars indicate standard errors of repetitions for small training set sizes. Regardless of the sample size, ` 1 -MKL is significantly outperformed by the sum-kernel. On the contrary, non-sparse MKL significantly achieves higher AUC values than the `  X  -MKL for sample sizes up to 20k. The scenario is well suited for ` 2 -norm MKL which performs best. Finally, for 60k training instances, all methods but ` 1 -norm MKL yield the same performance. Again, the superior performance of non-sparse MKL is remarkable, and of significance for the application domain: the method using the unweighted sum of kernels [22] has recently been confirmed to be the leading in a comparison of 19 state-of-the-art promoter prediction programs [1], and our experiments suggest that its accuracy can be further elevated by non-sparse MKL. We presented an efficient and accurate approach to non-sparse multiple kernel learning and showed that our ` p -norm MKL can be motivated as Tikhonov and Ivanov regularization of the mixing coef-ficients, respectively. Applied to previous MKL research, our result allows for a unified view as so far seemingly different approaches turned out to be equivalent. Furthermore, we devised two effi-leaved with chunking-based SVM training. Execution times moreover revealed that our interleaved optimization vastly outperforms commonly used wrapper approaches.
 We would like to note that there is a certain preference/obsession for sparse models in the scientific community due to various reasons. The present paper, however, shows clearly that sparsity by itself is not the ultimate virtue to be strived for. Rather on the contrary: non-sparse model may improve quite impressively over sparse ones. The reason for this is less obvious and its theoretical explo-ration goes well beyond the scope of its submissions. We remark nevertheless that some interesting asymptotic results exist that show model selection consistency of sparse MKL (or the closely related group lasso) [2, 14], in other words in the limit n  X  X  X  MKL is guaranteed to find the correct subset of kernels. However, also the rate of convergence to the true estimator needs to be considered, thus we conjecture that the rate slower than of the reasons for finding excellent (nonasymptotic) results in non-sparse MKL. In addition to the convergence rate the variance properties of MKL estimators may play an important role to elucidate the performance seen in our various simulation experiments.
 Intuitively speaking, we observe clearly that in some cases all features even though they may contain redundant information are to be kept, since putting their contributions to zero does not improve prediction. I.e. all of them are informative to our MKL models. Note however that this result is also class specific, i.e. for some classes we may sparsify. Cross-validation based model building that includes the choice of p will however inevitably tell us which classes should be treated sparse and which non-sparse.
 Large-scale experiments on TSS recognition even raised the bar for ` 1 -norm MKL: non-sparse MKL proved consistently better than its sparse counterparts which were outperformed by an unweighted-sum kernel. This exemplifies how the unprecedented combination of accuracy and scalability of our MKL approach and methods paves the way for progress in other real world applications of machine learning.

