 Today X  X  Internet computing environment has changed a lot since its first appearance. It is pervasive and ubiquitous, which means more and more devices such as personal digital assistants (PDAs), Pocket PCs, set-top boxes and cellular phones can access hardware (processing power, memory size, I/O capability, screen size, etc.), software (operating system, browser, audio-visual applications, etc.) and network access methods (communication media and speed). It is inappropriate if content publishers deliver the same content to every client. It will be better if content delivery could be adapted to different client capabilities. governments, or personal users to publish information, to conduct business, or to provide service, user-orientation or personalization has been more and more emphasized. Users are diverse, and thus they have different preferences and requirements on the content they want to receive from the Internet, such as language preference, interest on advertisement, privacy preservation, etc. It will be better if content delivery could be adapted to different client preferences. web, HTML is the primary language used for building the web site and represent 20% of total web traffic [3]. A high percentage of web traffic is coming from multimedia content, which is usually embedded in or linked from a HTML page. Currently there are many adaptive content delivery solutions of dissolving mismatch between content and capability by transcoding multimedia content to a more appropriate representation form. For example, images can be reduced in size to fit in a smaller screen. There are several limitations in these solutions.  X  Many of them just focused on one type of multimedia content, e.g. image, video  X  Some solutions made transcoding decisions in web intermediaries (e.g. proxies)  X  On the contrary, some solutions either did transcoding on server side or relied on all the embedded objects. We work on the container first, which is HTML page itself, and then on all the embedded multimedia objects. We are trying to put up with a solution which is generic and overcomes above-mentioned problems. information in a certain format, while some of them are not. For example, values of  X  X eywords X  and  X  X escription X  attribute in  X  X eta X  tag are used by search engines to identify keywords in the page, while users may not care about them. Edge Side Includes (ESI) [6] language markups may be inserted into HTML pages to facilitate content caching and selection, and they are more for web intermediaries instead of end users. Usually, web clients are only interested in content which could be presented through browsers. So, if HTML content is not designed for presentation purpose, it doesn X  X  have to be delivered to clients. only the content which users want and which devices can handle. The content classification is based on heuristics instead of human-edited descriptions. It is autonomous, and there is no overhead on web publishers, both in server processing power and in manpower. It is especially suitable for those existing HTML documents. important information or losing opportunities for aggressive transcoding. In order to have a more complete solution, we propose a generic framework for content selection and delivery. First, our framework supports our previously proposed Content Markup Language (CML) [5] with which server can control the transformation and delivery process. Second, we implement automatic content classification and selectively deliver content based on its class and user capability or preference. Thus, when server instructions or annotations are not available, adaptive content delivery is still possible. Third, content selection and transformation are performed on proxy server, which has many benefits as claimed in [13]. To enable cooperation with other proxies and address data integrity concerns, our proposed la nguage model also provides a way for proxies to leave their traces. Currently, in our framework, we assume clients express their preferences or device capabilities explicitly using some existing technologies [4] [19]. selection model. The reason for such a focus is that, from experiences we learned from today X  X  Internet, solution which is simple and automatic, usually can be deployed more quickly and by larger user bases. The proposed framework also incorporates the strengths of current solutions. works on adaptive content delivery. In sectio n 3, we describe the overall architecture of our content selection and delivery framework. Section 4 details how the framework can be deployed and some implementation concerns. Finally in section 5, we conclude our work and list several future works. Throughout this paper, we use HTML document to refer to a HTML page and its embedded objects, HTML content to refer to any content included in a HTML document, and HTML object to refer to individual object in a HTML document. With diversified and ever changing client capabilities and preferences, it is a big challenge for web content publishers with one source to offer the most cost-effective, best-fit content to its global users. To enable web content publishers to provide frameworks in which web content can be modified on the way from servers to clients, including Internet Content Adaptation Protocol (ICAP) [11] and Open Pluggable Edge Services (OPES) [15]. ICAP provides a framework in which almost any services for adaptive web content delivery can be implemented. ICAP clients can pass HTTP messages to ICAP servers for content modification. or adaptation. Web content transformation can be performed on web servers, web intermediaries (i.e. proxies), or web clients. Many of the solutions are proxy based [1] [8] [17] and proxy-based content transformation is usually called transcoding. Fox et al. [8] developed a system to distill or transcode images when they pass through a proxy, in order to reduce the bandwidth requirements. Chandra and Ellis [1] presented techniques of quantifying the quality-versus-size tradeoff characteristics for transcoding JPEG images. Spyglass Prism [17] is a commercial transcoding proxy. It compresses JPEG images and changes the size and color depth of images to improve the download time. There are also some a pproaches trying to address the issue of content transformation on the server side . They either provide external content annotations or give guidance on how to pr ocess the web content. Hori et al. [10] proposed to annotate HTML pages to guide the transcoding procedure. Mogul et al. [12] [13] proposed a server directed transcoding scheme. objects without looking into the whole web document. InfoPyramid [14] [16] actually considered the web document as a whole, which is similar to our approach. It is a representation scheme for handling the Internet content (text, image, audio and video) media type) and modality (in different media types). It also provided a model to generate and organize different versions of Internet content and then deliver suitable content based on client capabilities. However, it relied on content descriptions (in XML) to determine their resource requirements, and thus it may not work for those existing HTML documents. The need to generate and store different versions of content could also be a scalability problem. to different clients X  needs. Examples include WebOverDrive [20], HTML Optimizer X [9], etc. They can  X  X ompress X  HTML content by filtering out some parts of content that clients are not interested in. Usually this kind of tools only removes blank spaces, new lines and comments. There is no sophisticated classification method on HTML content, and also they only focus on the HTML page itself. preference and capabilities in the first place. W3C has proposed the Composite Capability and Preference Profile (CC/PP) [4] to achieve this goal. Wireless Application Protocol (WAP) Forum has proposed a similar approach called User Agent Profile (UAProf) [19] to handle clients X  descriptions. Both CC/PP and UAProf are based on Resource Description Framework (RDF) and their goals are describing and managing software and hardware profiles. In this study, we will use CC/PP or UAProf for client side descriptions. The general process of content selection and adaptive delivery is illustrated in Figure 1. When a client wants to access certain HTML content from a server, it sends a request to the server together with client descriptions on its preferences and hardware capability constraints. For example, the client may not be interested in, or not be able to handle images and video, but may be particularly interested in audio. Upon selector, together with client descriptions, and content descriptions if they are provided by content publishers. The cont ent selector then selects HTML content based on both client and content descriptions, transforms content when necessary, and passes the selected and transformed content to the client. within a web server, as a service provided by a third-party, as an add-on function of a proxy server, or even in the form of a personal firewall. Figure 2 illustrates one possibility when it is implemented in a proxy server. In the remaining of the paper, we assume that content selection is implemented in proxy server. We will explain the reason for such a choice in section 6. content selection and delivery process or not. If they want to control the process, they can provide content descriptions of importance level, different attributes and transcoding instructions on each fragment of HTML content. Sometimes, they can kind of flexibilities, content publishers can find a tradeoff between more workload of adding extra descriptions and more control on content selection and transformation. such as CC/PP or UAProf to describe their preferences or device capabilities. When clients send the request, they also attach client descriptions. The proxy-based implementation of our framework as the one shown in Figure 2 can search the cache first trying to find a copy of content with matched client descriptions. If search fails, proxy sends request to web server. When receiving the content from server, it analyzes the content. If content descripti ons are attached, content can be passed to selection module directly. Otherwise, content fragments should be automatically classified into different classes and then be passed to selection module. Depending on client descriptions and content descriptions (or content classes), policy engine directs the process of content selection and transformation. The selected and transformed content can be sent back to client, and in the mean time, also be saved in local caches. 
In the following sections, we give more detailed description on content markup language model, and several concerns in the actual deployment of the framework. More detailed description about the content selection model and its markup language can be found in [5]. 4.1 General Considerations There are several concerns in the implementation of our framework. First is the save network resources. It is one of the important goals of designing this framework. Third is transparent content selection and transformation. Clients only need to express their preferences and device capabilities. No change on the client side software is required. And they don X  X  need to send special request or use special protocols. In our current system, only content selection is implemented. But with the proposed discussion will focus on our current implementation and its underlying reasons. decisions, including web servers, proxies and clients. There are different benefits and constraints for different options. are doing content selection, the benefit is that they don X  X  have to transfer HTML documents with all content descriptions, but only contents after selection need to be delivered to clients. However, they have to get client descriptions (preferences, capabilities) to make a decision on content selection. Clients can be from anywhere in the world and using any kind of devices to access the web site, and their preferences and device capabilities vary a lot. When the number of clients increases, the server has to increase its capability (processing power, network bandwidth) to handle more requests. So this solution has a problem of scalability. network; each of them only handles a small portion of clients accessing the web site. So scalability is not an issue for proxies. Since there are much more clients than proxies, so the saving in network bandwidth from proxies to clients is much bigger than bandwidth required from servers to proxies. Additionally, proxies can cache original HTML documents (with content descriptions) as well as client descriptions to reuse them later, thus further save the time and network resources to transfer all the information to clients. We do not consider the possibility of performing content selection on the client side as it is directly against our design objectives. 
There is another alternative solution, i.e. web servers instruct content selection process and proxies actually make decisions based on server instructions, much like server directed transcoding [13]. This way, server X  X  workload is not that severe, and it First, we need additional support from the web architecture (protocols, bandwidth, deployment. Due to the distributed nature of network, it will make the deployment their instructions. As each proxy has its own configuration and policies, it may or may not follow instructions from servers. selections. Therefore, we use proxy-based approach in our implementation. Before we look into detailed operations, we will make the following assumptions: Data Integrity. Since proxies can modify a message on the way from a server to a client, we assume the client can always trust the received message. In other words, the data is preserved for their intended use, which includes content transformation by the authorized, delegated web proxies. Correct Proxy Delegation. We assume only delegated proxies will perform authorized operations on content and traces of their operations are marked correctly. And no malicious proxies can modify the content by performing unauthorized operations. 4.2 Server Operations We server is only an information provider but not a decision maker. It needs to store descriptions are generated offline by content authors. They can use CML language tool to divide HTML pages into fragments and provide descriptions for these fragments. 
Since content descriptions are embedded into HTML documents, when a request for a HTML document arrives, there are two possibilities  X  request for a pure HTML document, or request for a HTML document with content descriptions. The former understand it. The latter could be from a proxy which has implemented our proposed selection and delivery functionalities. Server has to differentiate between these two possibilities and respond accordingly. We make use of HTTP/1.1  X  X ccept X  header. We invent a new field value  X  X ontent-selection X  for the header. When clients/proxies need content descriptions from the serv er, add  X  X ccept: content-selection X  in the HTTP header field in request. When the server sends response to those who need content descriptions, it includes  X 207 content-selection X  if it is successful. 4.3 Proxy Operations When proxies act as a content selector, they should implement two types of operations: promote cooperation among proxies on content selection, and also for the possible data integrity checking. 4.3.1 Content Selection and Reassembly In order to perform content selection for clients, proxies have to gather information from both web servers and clients. When sending requests to web servers, proxies need to include  X  X ccept: conten t-selection X  in the HTTP header field to indicate it needs content descriptions. For clients X  descriptions (preferences and capabilities), there are number of ways, such as HTTP headers or other markup languages including CC/PP and UAProf. After getting HTML content with descriptions and client descriptions, proxies can start to perform content selections accordingly. All heuristics and rules are implemented in a policy engine. As we discussed before, it only includes rules for selection currently. It will be easy if later we include transformation rules. If client descriptions are missing, all content will be selected. If content descriptions are missing, automatic classification can help make selection decision. complete HTML document and respond to clients with all the traces of operations to notify clients on what operations has been done. 4.3.2 Operation Trace Marking When web content is delivered from server to client, there can be many proxies on the way performing content selections. It is important to know what others have done on the content. This requires proxies to leave traces of their operation in HTML content. Leaving traces of operations has two requirements. First, the proxy doing the operation should declare itself in the trace. Second, it should describe what operations are performed and what property of the (modified) content is. sequence to mark up these operations is the same as the sequence to perform these operations. So when an operation is finished, its traces will always be appended to the end. It can preserve the sequence of operations performed. The operation traces are illustrated in Figure 3. 4.4 Client Operations Clients need to send request for HTML documents and express their preferences and then a normal HTTP request is enough. If a client is both interested in HTML content and content descriptions (possibly with operation traces), it can add the  X  X ccept: content-selection X  header to the HTTP request. Clients can utilize HTTP header fields like  X  X ccept-language X ,  X  X ccept-Encoding X  to express some of their preferences. Or they can rely on existing standards such as CC/PP or UAProf to express their preferences and device capabilities in greater detail. They can also use existing profile repository [18], like some of WAP phones. In this paper, we propose a content selection and adaptive delivery framework for today X  X  pervasive and personalized web environment. The framework incorporates strengths from previous researches (e.g. providing server control on delivery process), and selection model, in which content can be classified based on its functionality, matched with user preferences, and then selected, transformed and delivered to users. It is more accurate than one-for-all mechanism and has less reliance on server instructions. Second, the language model not only supports servers to provide content descriptions or guidance on transformation, but also supports proxies to leave their traces, which improves data integrity and cooperation among multiple proxies. Third, the framework is generic and flexible. It works in many different situations, for existing HTML documents or for new web site, with or without server instructions, with a complicated transformation rule engine or simply making a decision between discarding and retaining. A HTTP header exte nsion is also introduced to keep content selection transparent for parties which do not understand the selection model. could vary from system to system. Currently we haven X  X  implemented the whole framework yet. Automatic content classification and selection has been tested and performance improvement has been verified [5], while content transcoding hasn X  X  been integrated to test the overall performance. That will be one of our future works. we only implement two transformation actions. We will try to build a rule engine with more complicated rules to trigger more transformation actions such as image transcoding. Second, we would like to have a user interface for updating the rule engine. It would be easier to add in new rules with the interface when new client descriptions or new rules or new transformation algorithms come out. It is also easier for deleting, updating and other maintenance. Finally, although we have some mechanism for proxies to leave traces, it is not enough to address issues of data integrity and proxy delegation. Since there are existing solutions [2], they can be added to the framework to provide a more robust framework. 
