 1. Introduction
In the paper, we will be concerned with how to create a condensed version of a sentence to be used in news bulletins, email news alerts, RSS feeds, snippets in search results and so forth, under the discriminative clas-sification paradigm. What we find curious about the past literature on sentence compression ( Clarke &amp;
Lapata, 2006; Jing, 2000; Knight &amp; Marcu, 2002; Nguyen, Shimazu, Horiguchi, Ho, &amp; Hukushi, 2004; Rie-dominated by what might be called the syntax-centric approach, where we confine ourselves to working with syntactic information alone in compressing sentence. 1 Obviously, this need not to be the case. Indeed, in text summarization, to which sentence compression is a close cousin, it is a common practice to bring whatever knowledge source is available to bear on the task.
Another issue we face in the research on sentence compression is the unavailability of large scale data: the past research either had people manually annotating data with whatever information is required for automatic compression, or turned to abstracts as a convenient substitute. In fact, much of the prior work on sentence compression turned to a single corpus, namely, one created by Knight and Marcu (2002) , for training data, as pointed out by Clarke and Lapata (2006) .

In this paper, we bring Conditional Random Fields (CRF), a generic statistical model for sequence learning 2004; Sha &amp; Pereira, 2003 ) to sentence compression and show how it offers a general framework that allows us to integrate various knowledge sources beyond syntactic structure. We also address the issue of data pau-city by creating data from RSS (Really Simple Syndication) contents which are rapidly growing in kind and in quantity on the Internet. In particular we will call upon an alignment technique from biology to preprocess
RSS data so that they permit use in sentence compression. 2. Turning RSS feeds into data with Sequence Alignment
We begin by looking at how RSS feeds can be turned into data for sentence compression. Tables 1 X 3 list some examples from RSS feeds the New York Times 2 provides on its web site (NYTimes.com, hereafter).
Each example consists of what is called an RSS description coming from the NYTimes.com, and its potential source, or a closely related sentence we located in the source article. RSS descriptions (or RSSs for short) could be viewed as a shortened rendition or a summary of the associated source sentences, giving a shorter description of events than sources they come from. A useful feature of RSSs here is that they can be arrived such as words, and phrases. In the paper, we will focus on RSSs of this sort, as they easily lend themselves to treatment with the sequence learning which we adopt here. The job of sentence compression is then reduced to s  X  w 1 ; ... ; w n , hopefully in the style of NYTimes.com.

So how do we find a potential source sentence for a particular RSS summary?
RSS summary v and an article it is associated with, we look for, among sentences in that article, the one that is most similar to v (call it s ), using a string similarity metric known as SoftTFIDF ( Cohen, Ravikumar, &amp;
Fienberg, 2003 ), 4 and then make a word by word alignment between v and s , which we would use for training and testing the models. However, aligning v and s can be tricky, as can readily be seen in examples shown earlier, i.e., Tables 1 X 3 , which apparently involve alignments with gaps.

A particular workaround we adopt here involves the use of the Smith X  X aterman algorithm, which enjoys popularity in the molecule sequence analysis in biology. The Smith X  X aterman algorithm ( Smith &amp; Waterman, 1981 ) or SWA is a pattern search heuristics which aims at finding maximally homologous subsequences among long molecule sequences. What distinguishes it from other prior approaches such as Needleman and Wunsch (1970) is that it is specifically targeted at locating noncontiguous subpatterns like those containing internal gaps or insertions. Suppose that we wish to know how similar two sequences v  X   X  A C D E s  X   X  A B C D E 0 are. SWA begins by creating a 5 by 6 matrix shown in Table 4 . D  X  X  represent possible sequence-initial gaps. We then fill in each cell  X  i ; j  X  in the matrix with a value H
Here, s  X  a i ; b j  X  stands for some similarity function, W k and l represent the number of gaps considered. Let W k  X  0 : 2 k , and analogously for W similarity between a and b by
To get a feel of how SWA works, let us work through the example in Table 4 . We set H 0 6 k 6 6 and 0 6 l 6 5.
 We start with a cell at (1,1), that is, we are at the initial letters of both strings. Then we would have
H
H 1 ; 1  X  1. Analogously, H 3 ; 2  X  max f 0 : 8  X  1 : 0 ; 0 : 4 ; 0 : 4 ; 0 g X  1 : 8, and H 2 : 8, and so forth.

A pair of segments with the maximum similarity can be found by first locating a cell that gives the maxi-mum value for H , and tracing back a sequence of cells that leads to this cell, until we reach a cell whose H is taking note of what symbol matches against what at each cell we visit, which would give us an alignment like the following.
Dash indicates the corresponding element is skipped over. Table 4 tells us that A
H
Now it is quite straightforward to translate the alignment into a sequence of labels that allow treatment with the sequence learning. For instance, we might translate the gapped alignment above into a sequence of 0 X  X  and 1  X  X  where  X 0 X  indicates a word to be deleted and  X 1 X  indicates a word to retained. So for the above SWA alignment, we would have
Table 5 shows a gapped alignment SWA produces for an RSS and its source in Table 3 . In experiments de-
LED is the Levenshtein edit distance. Note that under the assumption that log  X  0  X  X  0, we have s  X  a ; b  X  X  1if a  X  b , and 1 log  X  LED  X  a ; b  X  X  otherwise.

In the following section, we turn to a learning paradigm known as Conditional Random Fields or CRFs and discuss how it could be trained to serve as a model of RSSs represented in the form of (1) above. 3. HMM-CRF In this section, we will give a brief overview of CRF. In the context of sequence learning, we could think of
CRF as a random field defined over the topology of Hidden Markov Model, globally conditioned on an input observation. The goal of CRF is to find the probability distribution of a label (or class) sequence
Y  X  X  Y 1 ; Y 2 ; ... ; Y n  X  given a sequence of observations X  X  X  X where CL is a set of cliques in the model and U k a potential function for k 2 CL . input sequences, and Y is a random variable over corresponding label sequences. Y variables or vertices in the k th clique. Intuitively, we could take X to be a stream of words that form a sen-tence, and Y { k } to be a class label indicating whether or not to delete a k th word X X  X  X  X 1 ; X 2 ; ... ; X n  X  .

Suppose that we have a CRF over an HMM-like structure as in Fig. 1 . Then its cliques include f X 1 ; Y 1 g 1 ; f Y 1 ; Y 2 g 2 ; f X 2 ; Y 2 g 3 ; f Y vation node, we see that a clique f Y i ; Y i  X  1 g corresponds to a transition from state Y to an input observation at Y i .
 Define U k by if Y { k } is an transition clique with y k ; y 0 k 2 Y f k g if Y { k } is an input observing clique with x k ; y k 2 Y f k g and l i associated weights.

By plugging (3) and (4) into (2) , we get a CRF modeled on HMM which has the form
Let h  X  X  k 1 ; ... ; k n ; l 1 ; ... ; l m  X  , where each k what simplify the notation, we refer to each feature, as F f (transition feature), and its associated weight by h j where 1 6 j 6 n  X  m . We consider the log-likelihood of (5) , under h , i.e., which is equal to The normalization constant Z h  X  x  X  has the form: Thus, (7) is equivalent to to each k i , to obtain the gradient for L h which is to say
E p  X  Y j X  X   X  F j is the expectation of F j  X  x ; y ; x  X  with respect to p  X  Y j X  X  . So we seek a value for h expectation of F j into agreement with its empirical observation. To optimize h standard iterative optimization schemes, such as iterative scaling, conjugate gradient, and limited memory quasi-Newton, to improve on h  X  X , and re-estimate E P  X  Y j X  X  rithm, as in HMM, and get back to recomputing the gradient again. We repeat the procedure until we reach a stationary point.

Finally, we run the Viterbi algorithm to find the most plausible label sequence y * for a given input x under the model h such that which simply involves pursuing the most probable transitions for x on the HMM-CRF. 4. Noisy channel model and other notables
We review previous work on sentence compression in this section, though we spend most of time on the noisy channel model, a current state-of-the-art system for sentence compression due to Knight and Marcu on. 7 We start with K&amp;M.
 is that s is a potential compression for l . More precisely, it takes the form: of s under a bigram language model, i.e., P  X  s  X  X  defined to be the probability that s grows into l . For example, suppose that we have a parse tree for you are absolutely right. Details aside, formally this will be written down like: sentence. An obvious question is, how do we find s * , or a short sentence, that gives the maximum joint prob-ability with l ? Note that the number of possible short sentences in l , including itself, would be as large as 2
Since Knight and Marcu (2002) do not provide details of how they had worked out the issue, for our implemen-tation of K&amp;M, we decided to turn to an approach based on dynamic programming, which generally operates along the lines of Langkilde (2000) , which K&amp;M apparently make use of.
 We start by associating with each node v in a parse tree for l , except for the root, the probability by v are eliminated. We then pick up a node v that gives the maximum compression probability, and take it off the tree, along with child nodes and words it dominates. 8 each remaining node in the tree, and again do the picking. We repeat the procedure until we reach the desired compression rate. Graphically, this could be represented like: Indeed there are a number of alternative approaches in the literature addressing compression based on syntax.
Riezler et al. (2003) develop a linguistically inclined approach for sentence compression where one uses man-and select among them through a grammar based filter as well as some stochastic measure. Either approach is reported to have performance comparable to K&amp;M.

In a somewhat different vein, Dorr, Zajic, and Schwartz (2003) explore a rule-based approach inspired in part by linguistics, which they call  X  X edge Trimmer X . It works by taking as input a parse tree of sentence, recursively removing constituents which they regard as extraneous, such as preposed adjuncts, parentheticals, trailing PPs and SBARs. What constituent to remove is left to humans to decide on through  X  X bservations of human produced headlines X .

The work by Turner and Chamiak (2005) consists of some modifications to the K&amp;M allowing it to explic-itly favor shorter sentences and also to be able to work in the absence of supervised data.
More recently, McDonald (2006) presents a Markov random field model for sentence compression. Of par-ticular note is that it makes a factorization over the compressed sentence rather than on the full sentence, allowing it to capitalize on structural and linguistic properties of the compression more directly than when one works with a factorization over the full sentence, which is the case with the present work. 5. Data preparation and evaluation setup
We gathered a number of RSS feeds from NYTimes.com over a period of 4 X 5 months. With each RSS item we collected, we associated its possible source sentence from a corresponding article, using SoftTFIDF dis-cussed earlier. Table 6 lists some statistics about RSS feeds we have collected at NYTimes.com. The columns  X  X efore (after) prep. X  indicate the number of pairs before (and after) preprocessing of the data, which consists of finding syntactically close matches between RSSs and source sentences. We had the total of 10,319 pairs collected from the RSS feeds, and retained 23% of them (2391) for the training corpus. We conducted experi-ments using data sets from the RSS areas listed in Table 6 , which were all  X  X istilled X  into sets of compression/ source pairs which permit sufficiently close alignment. Our goal here is to compare a CRF based compressor against the state-of-the-art K&amp;M system discussed earlier, to see whether they differ in performance on RSSs.
Recall that data sets here are all put through SWA, which means that each source sentence is aligned word by word with a corresponding compression. We label a word or token in the source as  X  X etain X  if it gets aligned each token of the source sentence as to whether it is to be retained or to dropped.

CRF usually comes with a battery of features intended to exploit whatever information is available in the input. While most of the features we make use of are intended to pick up some superficial cues from punctu-ation and the writing system, such as whether a word contains a capital letter, or whether it consists solely of digits, there are some that are somewhat linguistically motivated, such as what we call the word feature f which takes the value 1 if word w has the frequency of more than one (in the training data) and 0 otherwise.
A particular feature, inspired by K&amp;M, which we call the  X  X rop X  feature (denoted by f probability that w is dropped in the compression. To find the  X  X rop X  probability of w , we select from among that we are interested in the drop probability of absolutely. Subtrees relevant to deriving absolutely include
ADJP ! RB JJ, VP ! AUX ADJP, and S ! NP VP. Its drop probability is then given as the follwoing equation:
We introduce another syntactically motivated feature which we call the  X  X hunk X  feature: the chunk feature is intended to represent a syntactic constituency such as verb phrase, and noun phrase, among words. But how do we go about encoding a two-dimensional information in a linear structure that CRF works with? We do this by adopting what is known as the BIO encoding scheme in computational linguistics, which simply con- X  X  X  for  X  X utside of the phrase X . For example, assume that we have a sequence of words w some constituents we call V and N like the following.

We then mark each word in a sequence with  X  X  X , I or  X  X  X , depending on whether it occurs at the beginning, inside, or outside of V or N. Note that each of the marks such as V it does not. In what follows we call a CRF model which makes use of the drop feature  X  X RF2 X . We denote a
CRF with chunk as well as drop features by CRF3, and the one that employs neither feature by CRF1.
What is interesting about features here is that the number of features used in learning varies because those that are not applicable are effectively ignored. We found that the number of active features ran somewhere between 5000 and 10,000. We also turned parts of speech (POS) associated with tokens found in the training data, into corresponding binary features which are all part of CRF1 to CRF3. Experiments were done using a
CRF tool developed by Sunita Sarawagi, which is available at SourceForge (crf.sourceforge.net). For K&amp;M, retained) of around 50%, which is generally comparable to that of the actual data. rate at around 64% on average, so they tend to take out less (see Table 7 ).

We evaluated performance by 10-fold cross validation, using F1, or a harmonic mean of precision  X  P  X  and label each word in sentence. So this would mean that we have Fl both for  X  X etain X  and for  X  X rop X , together with be a per-sentence proportion of the number of tokens, i.e., words, correctly labeled and recall concerns how many are correctly identified out of all the tokens carrying a particular class. One thing to note about F1 here same position as well as in the same form as in the reference (i.e., correct) compression. Consider a string
AABBAA in (X1). Let us assume for the sake of argument that a correct label sequence for the string is 010101, which corresponds to a compression like ABA. Then the compression in (X2), while taking the same form as (X1), ends up with zero for F1, since it is derived via a label sequence 101010, which has nothing in common with the correct labeling given in (XI), which is 010101. Apart from the extreme case discussed above,
F1 here works just like a token based similarity between strings. Take (X1) for a reference compression, and (X3) and (X4) for machine generated compressions. Obviously, BA in (X3) is much closer in form to ABA than A in (X4), which indeed agrees with F1 they receive: that is BA has a higher F1 score than A. Note that  X  F 1 forclass 1+ F 1 forclass 0  X  X  0 : 83. On the other hand, (X4) has avgF1 = 0.63. Finally, we estimated P drop  X  w  X  based on parses generated by running the Charniak parser on the training set.
Table 8 shows results for the experiments we conducted. Each figure represents avgF1 averaged over 10-fold runs. CRF compressors, regardless of model they are based on, show a clear superiority over the
K&amp;M model across the RSS domains considered here. Particularly notable is a large difference in performance between CRFs and K&amp;M on Obituaries, where CRFs X  performance more than doubles that of K&amp;M. To get some idea of why there is such a difference, let us look at some samples from actual runs, which are listed in
Table 9 . A part of the source sentence marked in boldface is what we find in an actual RSS summary.
CRF3 got everything correct, K&amp;M got almost nothing correct, which is somewhat peculiar because the train-ing data contains no instance of compression where the who clause, i.e., the who clause, is removed. The rea-son, we suspect, may have to do with the fact that the language model part of K&amp;M tends to blindly bias the system towards shorter sentences: the best way to get one is to eliminate a segment that corresponds to a larg-est subtree in a parse tree, which may well be the wh-clause for an obituary sentence.

It is also worth noting that structural features such as the chunk feature and the drop feature have notice-able effects on performance of CRF: CRF1 is generally outperformed by CRF2, which employs the drop fea-ture, which CRF3, harnessed with the chunk feature, comfortably outperforms most of the time. 6. Syntax driven CRF: an approach to fluency
So far we have seen how a CRF provides a general framework for sentence compression which allows fea-tures from punctuation to word form to syntactic structure to be incorporated into a single probabilistic model so that they are all exploited towards compressing sentence. Also we learned that the HMM-CRF, a CRF modeled on HMM, gives a clear edge over the K&amp;M system, in terms of avgF1.

However, an informal study we performed on CRF generated compressions found that they fall short of thing that sets sentence compression apart from other sequential labeling tasks such as term extraction, and part-of-speech tagging, in that the latter sorts do not demand that coherency, grammatical or otherwise, obtain over a sequence of labels generated by the algorithm.

Since F1 could be viewed as a measure for token based similarity between a compression and its gold stan-dard, one way of improving the intelligibility is to improve the system performance in F1 beyond the current level, though how to do this is not immediately clear. An alternative approach would be to force the CRF to somehow honor the syntax of sentence it is compressing; indeed, a close examination of results we had for
CRF3 indicated that more often than not, ungrammatical cases involve compression in plain violation of the sentential syntax, such as the removal of constituent heads. As a response, we attempt a simple heuristic, which consists in a recursive application of CRF on a syntactic structure of sentence.

In the heuristic, we pack individual word tokens, with which we normally work under CRF, into compound terms corresponding to major phrases such as noun phrase (NP), prepositional phrase (PP), or relative clause (SBAR), and treat them as atomic terms, so that any part of compounds may not be torn off inadvertently during compression. We leave untouched those that do not go into phrases, and let the CRF work on a mixed sequence of phrases and words, rather than just words. 12 arately, i.e., we treat it as if it were part of a single word which happens to correspond to a phrase. Compres-sion thus works on a mixture of phrases and words (see Fig. 3 ).
 Take for instance a sentence Smith saw a woman with a telescope , whose structure looks something like
Fig. 4 . The sentence consists of an NP a woman and a PP with a telescope (ignore an NP inside PP for the sake of simplicity). We combine words in each phrase to form a single compound term such as a-woman and with-a-telescope . Because they are treated as atomic terms, the compressor either takes them out as a whole or leaves them in its entirety, which would keep us from having undesirable compressions like Smith saw a , Smith saw a woman with ,or Smith saw a with .

In CRF, packing words by phrase means conflating corresponding states into one state that spans whatever word is involved. Graphically, this could be represented as in Fig. 5 , where words w packed into compound terms corresponding to NP and PP. Notice that the conflation of words has no effect on the formal structure of CRF, other than that each state now covers a varying number of words; some may cover one word and some, more than one word. An obvious question now is what to label a newly created compound term with; we know what labels are associated with each of the words that make up a compound term, but not what label to go with the compound itself. As a rule of thumb, we say that a packed compound is idea is to keep the compound if it contains any part that is not to be deleted. The downside of this is that we need to retain those parts that are thrown away otherwise, which could cause a substantial loss in F1. One way to alleviate the loss is by applying CRF recursively on substructures we pack, so that we may deal with finer grained elements.

Consider Fig. 6 , where we have a packed sequence, NNP, SBAR, etc., which may correspond to a sentence like Steve Marcus, who was an early exponent of the style that came to be known as fusion, died on Sunday at his home, New Hope, PA. We start off by running a CRF on the packed sequence made up of NPs, PPs and
SBARs, and make a decision, for each one of the phrasal chunks involved, on whether to retain it. Suppose that we choose to retain the SBAR chunk as in Fig. 7 , marked as  X 1 X  and discard those marked as  X 0 X . Then we unpack the SBAR, which will give us another sequence, i.e., WH ! AUX ! NP, for which we invoke an
SBAR-trained CRF to make a further compression, shedding whatever extraneous parts there are that we need to carry around otherwise. (By  X  X BAR-trained CRF X , we mean a CRF trained on data comprised solely of sequences associated with, or dominated by SBAR.) Fig. 8 gives sequences in Figs. 6 and 7 in a tree format, which shows how CRF operates in compliance with the syntax. 7. Evaluating intelligibility
We now look at how the syntax driven conditional random fields or SDR for short, compares to the CRF and K&amp;M in terms of intelligibility. We asked four people, all of whom are either a native or near-native speaker of English, to make an intuitive judgment on the intelligibility of compressions K&amp;M, CRF, and
SDR made for a set of 40 sentences, which are randomly chosen from a pool of articles gathered from the  X  X nternational X  section of the NYTimes.com. The judges are instructed to rate a compression in accordance with the scheme in Table 10 . We run the three models on the 40 sentences to obtain a total of 120 compres-sions and divided them evenly into four sets, which are sent to judges for evaluation. (Each of the judges were assigned to a set of 30 test compressions.) The evaluation material was presented in such a way as to prevent them from knowing details such as how many systems are involved, and which system generated which com-pression. Also we randomized the order in which test compressions are presented, to avoid unintended bias in order.

In each panel in Fig. 9 , we find the frequency distribution of ratings for compressions created by a partic-ular system. For instance, the K&amp;M model get its compressions rated 2 almost as frequently as 5, while CRF3 is given a rating equal to or above 4 more often than it gets rated 3 or below. SDR gets rated better than or equal to 4 83% percent of the time, indicating that most of the time, people find compressions it generates quite readable.

Table 11 puts the readability results against Fl and compression rate (CR). (RR4 for short) and  X  X R P 5 X  (RR5) indicate the normalized ratio of the frequency of system compression being rated equal to or above 4, and that of being rated 5, respectively. the RR4 scale, which means that its compression gets a rating better than equal to 4, 53% of the time. Inter-estingly, CRF3 fares much better. It scores 0.70 on RR4, indicating that its compression is rated equal to or above 4, 70% of the time. It is interesting because it is not explicitly informed about grammar, i.e., how words need to be combined to form a sentence. However, either system is by far surpassed by SDR, which records 0.83 on the RR4, suggesting that we have a readable compression most of the time. Also on RR5, SDR stands out, marking 0.67, compared to 0.30 by CRF3 and K&amp;M. 15
Table 12 shows performance in Fl of SDR against CRF3 and K&amp;M across various domains. The set up is identical to that discussed there; we used 10-fold cross validation, evaluated performance in avgF1. What we find here is a general degradation of performance, i.e., F1, compared to CRF3. How seriously it is affected varies from domain to domain. In some, it declines by more than 20%, in some the decline is somewhat less severe. Despite the setback, SDR manages to stay ahead of K&amp;M, on every domain except NYRegion. Table 13 presents the results on compression rate of the relevant systems, including human. Again, the compression rate of SDR varies with a domain it works on; it is more successful in domains such as
CRF3 and K&amp;M. Its compressions are generally 50% longer than those of K&amp;M, and 30 X 40% longer than those of CRF3.

We view compromise on F1 and compression rate here to be something of a cost that we need to bear to have an increased readability. As we have mentioned earlier, one way not to compromise on either F1 or com-pression rate is to improve CRF beyond the current level. But the question of how to do this is something we do not have an answer to yet.

Finally, we list in Appendix A some of the compressions generated by the three models, to give the reader a flavor of how they are doing compared to each other.
 8. Conclusions
In the paper, we argued that a CRF provides a general probabilistic framework for modeling sentence com-pressions by humans, and demonstrated how it exploits information from various sources including syntax, surface forms, punctuation, etc., towards compressing sentence, which is a significant departure from the state of the art approach by Knight and Marcu (2002) or K&amp;M, which exclusively relies on syntactic information to do compression. The experiments found that K&amp;M performs poorly in modeling RSSs in terms of readability as well as F1.

An issue of particular importance in using CRF for sentence compression is that a label sequence it outputs tion where we recursively apply CRF on phrasal subsequences. An evaluation we conducted with humans found that it significantly improves the readability of compressions the CRF generates. But this comes at a price. Making the CRF label words en masse rather than individually, caused a huge degradation in its per-formance in F1 and compromise in compression rate.
 Despite the setbacks, the model remains significantly superior in F1 as well as in readability to K&amp;M.
Nonetheless, the degradation issue is something we need to address in the future work, in order to bring machine compressions closer in length as well as in content to those made by humans.
 Acknowledgements
Many thanks go to Anna Ritchie, Ben Medlock, Timothy Baldwin, Kristiina Jokinen, Dragomir R. Radev, and Hiromi Oyama for helping me complete the work with their assistance in evaluation.
 Appendix A. Example compressions
Listed in the following are compressions generated by K&amp;M, CRF3, and SDF for a set of sentences which are found to be closely associated with RSS summaries of articles bfrom the International section of the
NYTimes.com. The articles are randomly selected from those harvested over a period of 4 X 5 months in 2005. In the following,  X  SRC  X  denotes a source sentence associated with an RSS summary (denoted by  X 
SRC 18-Home is hundreds of miles away, but the Uzbek refugees living in a United Nations camp on the
RSS The Uzbek refugees living in a United Nations camp on the edge of town still fear their government X  X 
KM 18-Home is hundreds of miles away, but living in a United Nations camp on the edge of town here still CRF3 Is hundreds of miles the Uzbek refugees living in fear their government X  X  reach
SDF The Uzbek refugees living in a United Nations camp on the edge of town here still fear their govern-
SRC Kabul, Afghanistan, Aug 21  X  Americans came under attack in two separate incidents today that high-
RSS Americans came under attack in two separate incidents today that high-lighted the rising violence in just KM Kabul, Afghanistan, Aug 21  X  Americans came
CRF3 Americans came under attack in two separate incidents violence in Afghanistan just one month ahead of SDF Americans came under attack in two separate incide nts today that highlighted the rising violence in
SRC Jerusalem, Dec 21  X  The Palestinian leadership said today that it might postpone parliamentary elec-RSS The leadership said it might postpone elections if Isr ael insists on barring Palestinian voting in East KM Jerusalem, Dec 21  X  The Palestinian leadership said today CRF3 The Palestinian leadership said today that it might postpone parliamentary elections set for Jan 25 if SDF The Palestinian leadership said today
SRC Jerusalem, Jan 3  X  Campaigning for the Palestinian legislative elections got under way today amid
RSS Campaigning got under way today amid doubts about whether the Jan 25 vote would actually take
KM Jerusalem Jan 3  X  campaigning for the Palestinian legislative elections got under way today amid doubts
CRF3 Campaigning for the Palestinian legislative elections got under doubts about whether the Jan 25 vote
SDF Jan 3-Campaigning for the Palestinian legislative elections got under way today amid doubts about whe-
SRC Baghdad, Iraq, August 26  X  thousands of Sunni arabs rallied in central and northern Iraq today to
RSS Thousands of Sunni arabs rallied in central and northern Iraq today to protest the proposed consti-
KM Baghdad, Iraq, August 26 thousands of Sunni arabs rallied in central and northern Iraq today to protest
CRF3 Thousands of Sunni arabs rallied in central and northern Iraq today to protest the proposed Iraqi cons-
SDF Thousands of Sunni arabs rallied in central and northern Iraq today to protest the proposed Iraqi cons-
SRC Austria X  X  glaciers  X  there are 925 of them-are shrinking fast, and as they shrink, this part of the world is
RSS Austria X  X  glaciers are shrinking fast, and as they shrink, this part of the world is slowly losing one of its
KM Austria X  X  glaciers  X  there are 925 of them-are shrinking fast, and as they shrink, this part of the world is CRF3 Part of the world is slowly losing almost literally, reflect the grandeur of the mountains around them
SDF This part of the world is slowly losing one of its many attractions, those rivers of ice that, figuratively References
