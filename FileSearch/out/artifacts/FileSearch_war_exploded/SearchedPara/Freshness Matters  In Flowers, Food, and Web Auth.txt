 The collective contributions of billions of users across th e globe each day result in an ever-changing web. In verticals like ne ws and real-time search, recency is an obvious significant fact or for ranking. However, traditional link-based web ranking algo rithms typically run on a single web snapshot without concern for us er activities associated with the dynamics of web pages and lin ks. Therefore, a stale page popular many years ago may still achi eve a high authority score due to its accumulated in-links. To re m-edy this situation, we propose a temporal web link-based ran king scheme, which incorporates features from historical autho r activ-ities. We quantify web page freshness over time from page and in-link activity, and design a web surfer model that incorpo rates web freshness, based on a temporal web graph composed of mul-tiple web snapshots at different time points. It includes au thority propagation among snapshots, enabling link structures at d istinct time points to influence each other when estimating web page a u-thority. Experiments on a real-world archival web corpus sh ow our approach improves upon PageRank in both relevance and fresh ness of the search results.
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Performance Keywords: Web search engine, temporal link analysis, web fresh-ness, PageRank In a corpus of documents as large as the Web, relevance alone i s often insufficient to produce good rankings. Thus it is neces sary to differentiate pages, and for more than a decade search eng ines have considered page authority in addition to relevance wit h re-spect to queries (and other factors) in web search. Much prev ious work [10, 23, 25] has been studied to estimate page authority based on different assumptions and successfully generalized ont o multi-ple tasks [5, 9, 32]. However, most of these studies accumula ted the authority contributions only based on the evidence of li nks be-tween pages, without considering the temporal aspects conc ealed in pages and their connections.

Freshness is important to the quality of much in our daily liv es, such as flowers and food. The same is also true for web page au-thority estimation. Pages being fresh tend to be welcome. Ho w-ever, traditional link analysis algorithms such as PageRan k [10] es-timate page authority by simply accumulating contribution s from in-links on a static web link structure, without considerin g whether pages are still fresh when web users search for them. Freshne ss of web links is also important to link-based ranking algorit hms. The web is widely recognized as one of the networks in which the rich get richer as the networks grow, leading to power law effects [12]. Old pages have more time to attract in-links, b ut may contain stale information. For example, as of this writi ng, http://www.sigir2007.org/ has 902 in-links [33] while http://www.sigir2010.org/ only has 208. Assuming the same contribution from each in-link, methods like PageRank would render a higher authority score on the earlier version of the SIGIR conference homepage.

In addition, some web spam research [31, 14] has recognized t hat local link structures with sudden changes might indicate li nk spam. A single web snapshot is unable to detect such changes and fur ther smooth or neutralize the undesirable influence automatical ly.
Motivated by these two points, in this work we propose an prob -abilistic algorithm to estimate web page authority by consi dering two temporal aspects. First, to avoid old pages from dominat ing the authority scores, we keep track of web freshness over tim e from two perspectives: (1) how fresh the page content is, referre d to as page freshness ; and (2) how much other pages care about the tar-get page, referred as in-link freshness . To achieve this, we mine web authors X  maintenance activities on page content, such a s the creation and removal of out-links. Each activity is associa ted with the time at which it occurs. We build up temporal profiles for b oth pages and links. A random walk model is exploited to estimate the two predefined freshness measures. By modeling web freshnes s from these two perspectives, we can bias the authority distr ibution to fresh pages, and so neutralize the unfair preference towa rd old pages by traditional link analysis ranking algorithms.

Second, we use multiple web snapshots at distinct time point s, instead of a single snapshot. To make the link graph more stab le, we connect multiple web snapshots by propagating authority flows among them, and so smooth the impact of sudden changes to par-ticular snapshots on web page authority estimation. We expl oit sev-eral proximity-based density kernel functions to model suc h prop-agation. Combining web freshness measures, we utilize a sem i-Markov process to model a web surfer X  X  behavior in selecting and browsing web pages.

We show the superiority of our proposed approach by conduct-ing experiments to evaluate the ranking performance of seve ral rep-resentative temporal web link-based ranking algorithms on a real-world archival web data set. Experimental results demonstr ate that our method outperforms the original time-agnostic PageRan k by 8% on both relevance and freshness of top 10 search results.
The contributions of this work are as follows.
The remainder of this paper is organized as follows. We revie w prior work in Section 2; introduce how we quantify web freshn ess and how we incorporate it into a web surfer model to estimate t ime-dependent web page authorities in Section 3; present how we s et up experiments in Section 4; and show the evaluation results of our proposed ranking algorithm in Section 5. We conclude and consider future work in Section 6. Link analysis which incorporates temporal aspects into sea rch has been studied in [35, 4, 13, 8, 6, 7, 34, 24, 1]. Yu et al. X  X  work in [35] was among the earliest, which incorporated paper age into quantifying paper authority to improve academic search. In ad-dition to utilizing paper citations, the authors modified Pa geRank by weighting each citation according to the citation date. H ow-ever, their work only associated one type of activity, i.e., link (cita-tion) creation, into link analysis in the scenario of academ ic search. Amitay et al. [4] attached a timestamp to each link, approxim ating the age of the page X  X  content. However, they only gave bonus t o the links from fresh pages, rather than combining the freshn ess of the page itself when estimate web page authority. Berberich et al. X  X  work [8] focused on temporal aspects of both web pages and lin ks in web search via the web dynamics from page and link creation , modification and deletion. They assumed users are equally in ter-ested in recency of information, in addition to the quality. Their work specifically emphasized the freshness and activity of p ages and links. However, activities occurring at different time points are not distinguished as long as they were all in the period of use rs X  temporal interests, which could span wide ranges.

Our work differs from prior work in two ways. First, we model the web freshness from two different perspectives by buildi ng tem-poral link profiles and temporal page profiles from multiple t ypes of activities over time. Second, the influence of activities on web freshness decays over time. We include some of these methods (e.g., [35, 8]) for comparison in Section 5.

Another direction in link analysis which incorporates temp oral factors is to directly utilize or mine trends from multiple s napshots of the archival web [6, 7, 34, 24]. Berberich et al. [6] analyz ed the potential of page authority by fitting an exponential mod el of page authority. The success with which web pages attract in-links from others in a given period becomes an indicator of the page au-thority in the future. However, one problem is how to normali ze page authority at different time points, such that they are c ompara-ble. To solve it, Berberich et al. [7] normalized PageRank sc ores by dividing them by the minimum authority score in the same we b snapshot, so that the minimum normalized PageRank score of t he page in any snapshot equals 1.

Yang et al. [34] proposed a new framework which utilizes a ki-netic model to explain the evolution of page authority over t ime from a physical point of view. The page authorities are viewe d as objects subject to both  X  X riving force X  and  X  X esistance X , a nd so the page authority at any time point can be a combination of the cu r-rent authority score resulting from  X  X riving force X  and the decayed historical authority score from  X  X esistance X . Empirical e xperiments demonstrated that authority estimation can benefit from inc reasing use of archival web content. However, their method did not co n-sider the accumulation of incomparable authority scores ca used by the inconsistent numbers of the pages in distinct snapshots . Other than web search, the idea of propagation of authority flows am ong different snapshots has been found in some other domains, su ch as social network analysis. Li and Tang [24] modeled the decaye d ef-fects of old publications in expertise search by allowing au thority exchange only between successive snapshots of the time-var ying social networks.

Our work differs from these in two ways. First, in our method each page in any snapshot is directly influenced by the same pa ge in all the snapshots in a one-step transition decayed by time differ-ence. This process captures a comprehensive interaction be tween pages at different time points naturally. Second, we propos e and evaluate a series of proximity-based kernel functions to co ntrol the authority propagation among multiple snapshots. Again, we com-pare to some of these approaches (e.g., [6, 34, 24]) in Sectio n 5.
Many link analysis methods compute page authority by a stochastic process via the link structure of the web. Howeve r, Liu et al. [25] utilized users X  browsing behaviors to calculate pa ge author-ity from a continuous-time Markov process which combines bo th how likely a web surfer reaches a page and how long the web surf er stays on a page. Their follow-up work [17] generalizes the pa ge importance framework to be a semi-Markov process in which ho w long a web surfer stays on a page can partially depend on where the surfer comes from in one step transition. Since our work m od-els web freshness from both how fresh a page is and how well oth er pages care about a particular page over time, we incorporate these two aspects into a semi-Markov process, which can model a tem po-ral web surfer behavior in a natural and adaptive way. Other r elated work includes exploring temporal aspects of web behaviors [ 2] and utilizing the evolution patterns of pages and links to benefi t web-based applications [16]. In this section, we propose a temporal ranking model (denote d as T-Fresh) to incorporate web freshness and link structures a t dif-ferent time points into web page authority estimation. The m ain idea of T-Fresh is to utilize authors X  maintenance activiti es on web pages and links to estimate web freshness at different time p oints, and then incorporate them into time-dependent page authori ty es-timation by following proximity-based authority propagat ion rules on the time axis. T-Fresh outputs an authority score for each page at every predefined time point. The authority is estimated in an approximated way, partly depending on the link structure an d web freshness of nearby snapshots, with the ones at farther time points having smaller influence. 3.1 Representing Web Freshness Over Time As introduced in Section 1, web freshness reflects how fresh a web page is at a given time point t i by in-link freshness (InF) and page freshness (PF). The reasons we separate these two web freshn ess measures are: (1) InF and PF depict web freshness from the per -spectives of information recommenders and information pro viders respectively; and (2) it prevents one type of web freshness f rom dominating a single freshness score. Given a web page p , we as-sume that each update on p  X  X  parent page q is a direct validation of the link from q to p , and so an update on q implies that q pays atten-tion to all of its out-linked pages, including p . Hence, we use InF to represent the attention from p  X  X  in-link pages, which is computed from the accumulation of activities on all the p  X  X  parent pages up to t . Unlike InF, PF represents how fresh p is up to t i based on the activities on page p itself. For every page p at t i , it associates with InF and PF, denoted as InF ( p ) t i and P F ( p ) t i . 3.1.1 Building Temporal Page and Link Profiles In order to compute InF and PF, we generate temporal page profi les (TPP) and temporal link profiles (TLP) in a manner inspired by Amitay et al. [4]. TPP and TLP record the web authors X  activit ies on the pages and links over time. Given a page p , each item on its TPP records evidence of some type of activity on p at a specific time point. It is written as a 3-tuple &lt; page ID, activity type, timestamp &gt; , where activity type  X  {creation, update, re-moval}. Given a link l with its associated anchortext, TLP records the evidence of some type of activity on l at a specific time point. Each item on TLP can similarly be represented as the 3-tuple &lt; link ID, activity type, timestamp &gt; , where activity type  X  {creation, update with unchanged anchor, up-date with changed anchor, removal}. In this way, each link an d page is associated with a series of timestamped activities. Table 1 summarizes the influence of these activities on web freshnes s. 3.1.2 Quantifying Web Freshness Based on TPP and TLP, we next quantify web freshness, i.e., In F and PF. In order to simplify analysis, we separate the contin uous time axis into discrete time points, e.g., ( t 0 , t 1 , . . . , t a unit time interval  X  t between successive time points, i.e.,  X  t = t  X  t i  X  1 . Web freshness at any time point t i is dependent on (1) the web freshness at t i  X  1 , and (2) the activities on TPP and TLP, which occur between t i  X  1 and t i . When  X  t is small enough, it is reasonable to assume that any activities in [ t i  X  1 , t this way, we map all the web activities onto discrete time poi nts. For web freshness at t i  X  1 , we assume it decays exponentially over time. Thus, InF ( p ) t i and P F ( p ) t i can be given by: where  X  P F ( p ) | t i t ness scores from the activities in [ t i  X  1 , t i ] , and  X  efficient that controls the decay of historical web freshnes s.
In the next step, we compute the incremental in-link freshne ss  X  InF ( p ) | t i t pends on the activities on TLP, we compute  X  InF ( p ) | t cumulating all the activities on p  X  X  in-links in [ t i  X  1 be the number of the j th type of link activity on link l in [ t Let w j be the unit contribution of the j th type of link activity. The incremental in-link freshness is written as: where LA is the set of link activity types. However, it is not enough to propagate such influence in one step; we additionally prop agate in-link activities in an iterative way, leading to smoother in-link freshness scores. Let  X  InF 0 ( p ) | t i t score. For each iteration, every page receives in-link fres hness scores from its parent pages, and also holds its initial scor e. The process converges and produces a stable score for every page de-termined by both its parents X  scores and its own in-link acti vities. Thus, the incremental in-link freshness is given by:  X  InF ( p ) | t i t where m qp is the weight on the link from q to p . Equation 4 is actually the personalized PageRank (PPR) [19]. We use one-s tep transition probability from q to p based on link structure to repre-sent m qp , where P m q  X  = 1 if q has at least one out-link.
We next compute the incremental page freshness  X  P F ( p ) | Similar to  X  InF ( p ) | t i t pends on both the page itself and its out-linked pages, since the out-linked pages are extensions of the current page. We thus prop-agate page freshness backward through links in an iterative way. For each iteration, every page receives page freshness scor es from its out-linked pages, and also holds its initial score. This process converges finally and generates a stable page freshness scor e on ev-ery page. Let C  X  j ( p ) be the number of the j th type of page activity on p in time period [ t i  X  1 , t i ] . Let w  X  j be the unit contribution of the j th type of page activity. The initial incremental page freshne ss score P F 0 ( p ) | t i t where P A is the set of page activity types. The incremental page freshness is given by: where m  X  qp is the weight on the link from q to p . We use the inverted one-step transition probability to represent m  X  P m  X   X  p = 1 if page p has at least one in-link. Once achiev-ing  X  InF ( p ) | t i t P F ( p ) t i by Equation 1 and 2. 3.2 Temporal Ranking Model Now that we quantify web freshness at distinct time points, t he next problem comes to how to utilize web freshness to control auth ority propagation in an archival link graph, so that we achieve a ti me-dependent authority score for every page.

We start by describing a  X  X emporal random surfer model X  whic h motivates our method T-Fresh. The  X  X emporal random surfer model X  is similar to the  X  X andom surfer model X , which explai ns PageRank [10]. However, our surfer model differs from the tr a-ditional model in two aspects. First, the web surfer has spec ific temporal intent, which controls her choice of target snapsh ot. Note that the surfer X  X  temporal intent varies with her current sn apshot. Second, the web surfer prefers fresh web resources. Figure 1 de-picts one simple example of how the surfer behaves on an archi val web of four snapshots.

Consider a web surfer wandering on an archival web corpus, which includes multiple web snapshots collected at differe nt time points ( t 0 , t 1 , . . . , t n ). For every move, the surfer takes the fol-lowing steps. First, she can choose either to follow one of th e out-linked pages or to randomly jump to any page at the same time point. However, unlike PageRank in which a web surfer has equ al probabilities to follow out-going links, the preference of our surfer choosing out-going links is a function of the page freshness of out-linked pages. Consider the example in Figure 1. Suppose the s urfer is currently on page A at t 2 . She follows the link to B at t link) with probability (1  X  d ) F t 2 ( B, A ) , where F t function which depends on the page freshness of all A  X  X  out-linked pages at t 2 and P P : A  X  P F t 2 ( P, A ) = 1 . The probability that the surfer randomly jumps to any page within snapshot t 2 , such as B , is d/N t 2 , where N t 2 is the total number of pages at t
After the surfer reaches the page chosen in the first step, she next selects the specific snapshot of the page to which to jump base d on her temporal intent, which correlates to the time differe nce be-tween the current snapshot and the target snapshot. This pro cess propagates authority among snapshots and uses the link stru cture at one time point to influence the authority computation at othe r time points. The propagation decays with time difference betwee n snap-shots. For the example in Figure 1 (dashed bi-directed links ), sup-pose the surfer reaches B at t 2 after the first step, she can jump to B at any time point as long as it exists, i.e., t 2 , t 1 , and t the probability that she jumps to B at t 1 is written as P which depends on the time difference between t 1 and t 2 .
Once the surfer reaches the page at the chosen time point, e.g ., page B at t 1 , she browses it with the mean stay time  X  t incorporates B  X  X  in-link freshness at t 1 before the next move.
In this way, the surfer X  X  behavior on the archival web can be s ep-arated as (1) moving from one page to another; and (2) staying on a page and browsing it. It leads to the semi-Markov process [30 ] for page authority estimation.

Definition 1. A semi-Markov process is defined as a process that can be in any one of N states 1 , 2 , . . . , N , and each time it enters a state i it remains there for a random amount of time having mean  X  , and then makes a transition into state j with probability P Suppose the time that the process spends on each state is 1; th en the semi-Markov process leads to a Markov chain. Assuming al l states in such a Markov chain communicate with each other, th e process can generate a stationary probability  X  i for any state i . The long-run proportion of time that the original semi-Markov p rocess is in state i is given by: This solution divides the time-dependent page authority es timation into (1) computing the stationary probability that a surfer reaches every page in the archival corpus; and (2) computing the mean time of a surfer staying on every page. 3.2.1 Estimating Stationary Probability We now introduce the computation of probability  X  p,t i that a web surfer enters a page p at snapshot t i . In the first step of each move, the surfer reaches page p at any time point t j by: (1) following p  X  X  in-link at t j to reach p ; (2) jumping from any page at t
P t j ( F ollow | q ) = (1  X  d ) , P t j ( p | q, F ollow ) = F where d is 0.15 by default. F t j ( p, q ) is the web surfer X  X  preference for following out-linked pages. Intuitively, fresh web res ources are likely to attract surfer X  X  attention. We define F t j ( p, q ) as:
In the second step of each move, the surfer reaches page p at t from page p at t j is given by: where V i and V j are the sets of pages at time point t i tively, and w ( t i , t j ) is the weight that represents the influence be-tween the snapshots at t i and t j . Motivated by previous work [15, 22, 26, 28] which used proximity-based methods, we consider six kernel functions to model the authority propagation betwee n snap-shots: gaussian kernel (equation 12), triangle kernel (equ ation 13), cosine kernel (equation 14), circle kernel (equation 15), p assage kernel (equation 16) and PageRank kernel (equation 17). We f or-mally define them as follows. where | T | is the window size of one step authority propagation between snapshots. Other than Equation 12, all kernels requ ire | t  X  t j | &lt; | T | ; that is, the one step authority propagation proceeds only within a window of a specified size. Larger | T | results in more choices for the web surfer at each move between snapshots, wh ile smaller | T | leads to influence mainly from nearby time points. In this work we set | T | to the total number of snapshots involved in authority propagation by default.

Combining the analysis above, the probability that a web sur fer reaches page p at snapshot t i can be written as:  X  where T i is the set of snapshots which can directly distribute au-thority to t i within one step. Based on the surfer X  X  behavior, this Markov process guarantees all states to communicate with ea ch other, leading to a transition matrix that is irreducible an d aperi-odic [30]. As a result, it converges and generates a stationa ry prob-ability on every page existing in any snapshot. 3.2.2 Estimating Staying Time Pages with more in-link activity are likely to attract a surf er to spend time browsing it. We assume the web surfer prefers fres h web resources, and so the mean time (  X  p,i ) of the surfer staying on page p at t i can be proportional to p  X  X  web freshness at t discussed in Section 3.2.1, the web surfer prefers pages wit h high page freshness when choosing among out-going links; we use i n-link freshness to model the time of a surfer staying on a web pa ge. In this way, pages with both high in-link freshness and page f resh-ness are more likely to be given high authority scores. Speci fically, we utilize a sliding window and compute p  X  X  weighted in-link fresh-ness centroid within it as the estimation of  X  p,i , which is formally given by where T  X  t centered on t i , and P t uate one special case, in which w  X  ( t i , t j ) = 1 | T  X  In this way, the authority score A ( i ) in Equation 7 is determined by both  X  p,i in Equation 18 and  X  p,i in Equation 19. 4.1 Data set and Evaluation Many standard data sets such as TREC [27] usually only contai n one snapshot of a web corpus, and so are not suitable to show th e effectiveness of ranking models utilizing temporal inform ation. To evaluate our proposed method, we utilize a corpus of archiva l web pages in the .ie domain collected by Internet Archive [20] from January 2000 to December 2007. This corpus contains 158 mill ion unique web pages, and approximately 12 billion temporal lin ks. To avoid the influence of transient web pages, we extract one w eb graph for each month from the sub-collection of pages for whi ch we have at least 5 crawled copies. These graphs comprise a colle ction of 3.8M unique pages and 435M temporal links in total.

For ranking evaluation, we choose April 2007 as our time period of interest. Ninety queries are selected from a set of sources, including those frequently used by previous researchers, a nd pop-ular queries from Google Trends [18]. For each query, we have an average of 84.6 URLs judged by at least one worker of Ama-zon X  X  Mechanical Turk [3]. When human editors judge each pai r of &lt; query,URL &gt; , they are required to give a score based on (1) how relevant the page is to the query; and (2) how fresh the page wo uld be as a result for the requested time period. The relevance sc ore is selected from among highly relevant, relevant, borderline, not rel-evant and not related , which is translated to an integer gain from 4 to 0. A page with score higher than 2.5 is marked as relevant. Similar to the relevance judgement, the freshness score is s elected from very fresh, fresh, borderline, stale, and very stale , which we translate into an integer scaled from 4 to 0. A page with a scor e higher than 2.5 is marked as fresh. All human editors were ask ed to give the confidence of their provided judgments, in the sel ection of high, medium and low. Judgements with low confidence are no t included in ranking evaluation. A random sample with 76 &lt; query, URL &gt; pairs judged by 3 editors show that the average standard de-viations of relevance and freshness judgements are 0.88 and 1.02 respectively.

Based on these judgements, we evaluate the ranking quality o f our approach on both relevance and freshness over the Normal ized Discounted Cumulative Gain (NDCG) [21] metric. It penalize s the highly relevant or fresh documents appearing at lower posit ions. Precision@k is also utilized to measure the ranking quality , which calculates the number of relevant or fresh documents within the top k results across all queries. 4.2 Compared Methods To show the effectiveness of T-Fresh, we compare with PageR-ank [10] (the baseline) and several representative link-ba sed rank-ing algorithms, which incorporate temporal information, i nclud-ing TimedPageRank [35], T-Rank [8], BuzzRank [6], Temporal -Rank [34], and T-Random [24]. All these algorithms combine w ith Okapi BM2500 [29] linearly by ranks, defined as: The parameters used in Okapi BM2500 are the same as Cai et al. [11]. The variants of T-Fresh are summarized in Table 2. 4.3 Web Activity Detection While accurate web maintenance activities might be recorde d on Web servers X  logs, we must infer such activities from the com par-ison between successive web snapshots in this work. Specific ally, we assume that each page was created at the time at which it was first crawled, and each link was created when it was first found . Al-though some pages can automatically change a portion of its c on-tent in every crawl, we suppose one page has an update when its content has any difference from the previous version, or its meta-data can show the last-modified time is after the crawling tim e of the previous one. To identify the link update, we simply assu me that once a page has an update, all its out-links are consider ed to be updated. We admit that perfect quantification of link upda te ac-tivity may depend on a variety of factors, including the dist ance to page blocks being changed, page editing concentration in ferred from content maintenance patterns, and so on. We leave the se n-sitivity of web activity detection accuracy on ranking perf ormance to future work. We also assume that a page disappears when its returned HTTP response code is 4xx or 5xx. While the gain as-sociated with each type of link and page activity can influenc e the ranking performance, as a preliminary study, we define these gains in Table 1, and again leave the sensitivity of ranking perfor mance with respect to gains on web activity to future work. In this section, we report the results of our ranking evaluat ion and compare T-Fresh to representative link-based algorithms. Results demonstrate that by incorporating web freshness and propag ating authority among different web snapshots, we can achieve mor e rel-evant and fresh search results. 5.1 Correlation of InF and PF As introduced in Section 3.1.2, each page in the temporal gra ph is associated with InF and PF. A reasonable criteria for the g ood estimation of InF and PF would be their potential capability of pre-dicting future web activities even though the correlation b etween them would be rather small. To better consider this idea, we c om-pute the average correlation between web freshness scores a t t and web activities at future time points, i.e., t + 1 , t + 2 , etc., given by Equation 3 and 5.

From Figure 2(a),  X  P F | t t  X  1 and future in-link activity show positive correlation, with the strength inversely proport ional to the time difference between the incremental page freshness and future in-link activities. In most cases, the correlation is maxim ized when  X 
P F and  X  InF are 0.6. It indicates pages can achieve freshness scores from both activities on themselves and their neighbo r pages via propagation. The correlations between  X  InF | t t  X  1 page activities show similar trends (Figure 2(b)). One may n otice that the average correlation between  X  P F | t t  X  1 and in-link activi-ties at t +1 is 0.0519, which is higher than that between  X  InF | and page activities at t + 1 over 13.5%. One interpretation is that a page with very fresh content tends to attract new in-links or exist-ing in-links to validate in next time periods. From Figure 2( c) and (d), the cumulative web freshness scores can show stronger c orre-lation to future web activities, varying with the decay para meter  X  and  X  4 given  X  1 =  X  3 = 1 constantly. For both P F t and InF the correlations achieve the highest when  X  2 and  X  4 are 1 in most cases. To meet our criteria of good estimation about web fres hness, we set  X  P F =  X  InF = 0 . 6 and  X  2 =  X  4 = 1 in the following ranking evaluation. 5.2 Ranking Performance Figure 3 demonstrates the ranking performance in terms of re le-vance and freshness on metric P@10 over all the compared al-gorithms, under the variance of combination parameter  X  from 0.8 to 1. The variant of T-Fresh we choose for comparison is T-Fresh(1,1,30). For relevance evaluation, PageRank achi eves its highest P@10 at 0.4894 when  X  is 0.97. T-Fresh performs the best among all the algorithms, achieving its highest P@10 at 0.50 51 when  X  is 0.91, exceeding PageRank by 3.2%. The TimedPageR-ank places second on metric P@10, which reaches 0.5031 when  X  is 0.92. Under the combination parameter achieving the best P@10 for every method, we compare the ranking performance over al l metrics in Table 3. T-Fresh performs the best among all the al go-rithms over all the metrics. Specifically, it outperforms Pa geRank over 24.7%, 17.8% and 7.8%, in terms of NDCG@3, NDCG@5 and NDCG@10. Single-tailed student t-tests at a confidence l evel of 95% demonstrate the improvements are statistically sign ificant over PageRank on NDCG@3, NDCG@5 and NDCG@10, with p-values 0.0001, 0.0001, 0.0016 respectively.

For freshness evaluation, Figure 3(b) shows ranking perfor mance on metric P@10, varying with combination parameter  X  . T-Fresh demonstrates a stable trend for P@10, which exceeds PageRan k on all the experimental points. Unlike relevance evaluatio n in which improvements of other temporal link-based algorithm s are not obvious, more methods can produce fresher search result s than PageRank. One reason is that these temporal link-based algo -rithms incorporate diverse temporal factors which favor fr esh web pages. T-Fresh reaches its best P@10 at 0.3412 when  X  is 0.88, which is only inferior to TemporalRank with its highest P@10 at 0.3473 when  X  is 0.98. PageRank has its best P@10 at 0.3325 when  X  is 0.97. With individual best combination parameter  X  on P@10, we compare all the ranking algorithms over other metri cs in Table 3. T-Fresh outperforms PageRank in terms of NDCG@3, NDCG@5 and NDCG@10 over 23.8%, 13.5% and 8.3%, with p-values 0.0090, 0.0260 and 0.0263 respectively. One observa tion is the performance of PageRank on metric NDCG@3 is extremely low while its performance on NDCG@5 and NDCG@10 are not so bad. We infer that stale web pages can achieve high authori ty scores by PageRank, and so dominate top positions in search r e-sults. 5.3 Deeper Analysis We study the effects of propagation kernels and window sizes used in staying time estimation on ranking performance in this se ction. Figures 4(a) and (b) show the best ranking performance of T-Fresh(*,1,*) on metric NDCG@10 for relevance and freshness . For most kernels, the relevance performance improves with the t ime span of the temporal graph, and reaches the highest in [30 , 60] , i.e., from 2.5 to 5 years. The improvements upon using single snap-shot are 4.9%, 4.1%, 4.2%, 4.9%, 5.0% and 2.8% for gaussian, triangle, cosine, circle, passage and PageRank kernels res pectively. Passage kernel renders both a stable and best overall perfor mance, followed by gaussian and circle kernels. Results from trian gle and cosine kernels show larger fluctuations over time span of the tempo-ral graph. Combining with the kernel expressions defined in E qua-tions 12-17, we conclude that the ranking performance with r espect to relevance can take advantage of appropriate emphasis on a uthor-ity propagation among far away snapshots.

The ranking performance on freshness shows similar trends t o relevance, though the variance is typically larger. Except for PageRank kernel, all others achieve their highest performa nce in the time interval [30 , 60] . Passage kernel gets the best performance 0.3171 on metric NDCG@10 by outperforming the baseline (usi ng a single snapshot) by 4.5%. One observation is that the perfo r-mance of PageRank kernel suddenly falls down to around 0.295 when the graph time span is beyond 30 months. One possible reason is that the authority propagation among any distinct web snapshots become very weak in PageRank kernel when the graph time span is large enough, and so historical link structures only have tiny influence on page authority estimation at the curre nt time point. In addition, the freshness performance tends to stab lize when the graph time span is over 70 months, which indicates tempor al web graphs with long time span render more stable ranking per for-mance on freshness, and it reflects the long-term freshness o f web resources.
 Figures 5(a) and (b) show the best ranking performance of T-Fresh(5,*,*) on metric NDCG@10 in terms of relevance and fre sh-ness. For relevance evaluation, our results demonstrate: ( 1) To use the average in-link freshness on several adjacent time poin ts is bet-ter than to use it at a single time point when estimating stayi ng time. We infer that average in-link freshness can render a good est imation about how active the page in-links are during a time period; ( 2) It does harm to ranking performance on relevance when the windo w size is too large; (3) Large window sizes result in large vari ance of ranking performance when varying the number of snapshots in the temporal web graph; (4) The ranking performance improves wi th the increase of graph time span in general for all the window s izes. For freshness evaluation, a clear trend in Figure 5(b) shows that a larger window size used in staying time estimation helps gen erate fresher search results with smaller deviation. Dynamic web resources can reflect how active web pages are ove r time. From the perspectives of in-links and the page itself, we quan-tify web freshness from web creators X  activities. We argue t hat web freshness is an important attribute of web resources and can bene-fit a series of time-sensitive applications including archi val search, news ranking, twitter message recommendation, tag recomme nda-tion and so on.

In this work we propose a temporal web link-based ranking al-gorithm to estimate time-dependent web page authority. It i ncorpo-rates web freshness at multiple time points to bias the web su rfer X  X  behavior on a temporal graph composed of multiple web snapsh ots. Experiments on a real-world archival corpus demonstrate it s superi-ority over PageRank on both relevance and freshness by 17.8% and 13.5% in terms of NDCG@5. Results show ranking performance can benefit more from long-term historical web freshness and link structure. The best period covers the past 2.5 to 5 years.
There are a few interesting extensions. The web surfer X  X  tem -poral interest changes with her position in this work. We cou ld fix her temporal interest, and use the estimated authority sc ore to support archival search, which can select highly authorita tive page instances that best match a specific temporal interest, in ad dition to relevance to a given query. On the other hand, the historical infor-mation at the Internet Archive comes from an external source for commercial search engines, which means a large amount of pag es may lack archival copies. In the future, we hope to find a way to mitigate the gap caused by some pages having archival copies and some without in the searching process, so that the method can be applicable to search engines seamlessly.
 Acknowledgments We thank the anonymous reviewers for their useful comments. We also thank Fernando Diaz for valuable suggestions related t o re-cency ranking evaluation. This work was supported in part by a grant from the National Science Foundation under award IIS -0803605 and an equipment grant from Sun Microsystems.
