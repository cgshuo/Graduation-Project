 The CrowdRec workshop brings together the recommender system community for discussion and exchange of ideas. Its goal is to allow the potential of human computation and crowdsourcing to be exploited fully and sustainably, leading to the development of improved recommendation and infor-mation filtering technologies. Currently, the complete range of possible intelligent contributions that recommender sys-tems could elicit from users is under-explored, and its full extent is unknown. Critical questions addressed in the work-shop include how to: formulate crowdtasks, match tasks with crowdmembers, ensure the quality of crowd input, and integrate feedback from the crowd in an optimal manner to improve recommendation. Further, crowdsourcing can also be exploited for system design and system evaluation. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  X nformation Filtering Algorithms, Design, Experimentation, Human Factors. human computation, human intelligence, crowdsourcing, rec-ommender systems
Recommender systems stand to benefit greatly if they would be able to call actively on intelligent human input specifically at those moments and for those purposes that it is most needed. This is the vision pursued by the CrowdRec workshop. The workshop is dedicated to new approaches to information filtering and recommendation technologies that make use of human intelligence, but also take advantage of human plurality, i.e., contributions from a large number of crowdmembers.

The aim of the CrowdRec workshop is to provide a fo-rum within the recommender system community that can foster the development of new ideas and best practices for making use of crowdsourcing to improve the performance of recommendation technology.

The recommender system community faces a number of key challenges if human intelligence is to be actively ex-ploited by algorithms and systems. These include recruit-ment of crowdmembers (e.g., identifying users with the ap-propriate expertise), providing effective explanations of tasks for the crowd (i.e., good task design), controlling noise and quality of the crowd input, designing incentive structures that encourage serious work, and ethical treatment of crowd-members, including protecting personal or privacy-sensitive information, and appropriate compensation for their efforts. Further, crowdsourcing platforms can also be used to de-sign and evaluate recommender systems. Large-scale com-mercial crowdsourcing platforms make it possible to directly question a large number of people concerning their prefer-ences for system design, and their impressions of system results and result presentation. Finally, recommender sys-tems have an important role within crowdsourcing systems. Crowdsourcing systems can operate efficiently at large scale if they are able to offer tasks to users that fit their interests and are also well suited to their ability.
Increasingly in our digital age, when information-related tasks need to be carried out, we turn to  X  X he Crowd X  rather than to a dedicated pool of specialized employees. In 2006, Jeff Howe introduced the notion of outsourcing tasks to the crowd, with the article  X  X he Rise of Crowdsourcing X  [8]. He defines crowdsourcing as,  X ...the act of taking a job tradition-ally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call X  [7]. Under this defini-tion, crowdsourcing is a useful means of accomplishing a task that would need to be carried out anyway. We push beyond this view and point out that the existence of crowdsourc-ing as a paradigm, and also the growth of social networks and large commercial crowdsourcing platforms, significantly changes the playing field. In particular, these developments open up the possibility for new tasks, that are helpful for recommender systems, but never would have originally been created, were  X  X he Crowd X  not available to tackle them.
Human computation and crowdsourcing are both tech-niques that take advantage of the intelligence of human con-tributors. However, an important distinction exists between the two. According to Quinn and Bederson [9], human com-p utation is human intelligence applied to solve problems that might someday be solvable by computers. This view underlies the concept of  X  X rtificial Artificial Intelligence X , the tagline for the large commercial crowdsourcing platform Amazon Mechanical Turk (https://www.mturk.com). The CrowdRec workshop strives to push beyond this view, to look at systems in which human contributions are valuable exactly because they are uniquely human, e.g., taste deci-sions that are only meaningful because they were made by a human being.

Quinn and Bederson [9] identify computational control of human intelligence as another aspect of human computation. Under this view, human computation occurs when a com-putational system or a process directs human contributions. Human computation in recommender systems represents, in this respect, a closed loop, since ultimately the behavior of a recommender system should be driven by the needs and preferences of its human users.
The CrowdRec Workshop series was established in 2013 with the first edition of the workshop held at ACM RecSys 2013 [6]. Motivation to found the workshop series arose from examples in other research communities, each which derived benefit from using workshops to develop and support their individual disciplines in successfully and sustainably exploit-ing the crowdsourcing paradigm. Particularly relevant to recommender systems are the closely related communities who research and develop systems that allow users to in-teract with information. In this section, we mention some selected examples of the previous workshops that have con-tributed to other communities.

The Information Retrieval community has led the way to applying crowdsourcing techniques and large commer-cial crowdsourcing platforms to the problem of evaluating algorithms and systems. The  X  X rowdsourcing for Search Evaluation Workshop X  [2] was held at the ACM SIGIR Con-ference on Research and Development in Information Re-trieval in 2010. Topics included techniques for the devel-opment of data sets needed for evaluation of information retrieval systems, and for the development of models capa-ble of predicting user interest in specific content. In the years that followed, other workshops expanded the topic to include techniques for directly eliciting contributions from  X  X he Crowd X  for the purposes of data mining and informa-tion retrieval. Specifically, we mention the  X  X rowdsourcing for Search and Data Mining Workshop X  [3] at the 2011 ACM International Conference on Web Search and Data Mining, and the  X  X rowdsourcing Web Search Workshop X  [1] held at the 2012 International World Wide Web Conference. The multimedia community has also used workshops to help to foster the growth of crowdsourcing techniques, for the pur-poses of system design, data set creation, evaluation, and user studies. Workshops were held at ACM Multimedia 2012 and 2013 [4, 5], with another planned for 2014.

These workshops have revealed that it takes a great deal of continuous effort in order to develop the viable concepts and the best practices necessary to ensure that a research area is able to achieve maximum benefit from crowdsourc-ing. Crowdsourcing involves human systems, which are no-toriously complex and delicate. In any given area, the op-portunities to apply crowdsourcing cover a large number of diverse angles, for example, from annotation to evaluation. Workshops like CrowdRec help a community to realize these opportunities in their research and development activities.
The term  X  X rowdsourcing X  is sometimes used in a broad sense when a system exploits information from a group of people large enough to be considered a crowd. Under this view, conventional collaborative filtering recommender sys-tems can be considered crowdsourcing, since they exploit information from a large number of users in the form of rat-ings, views, or other interaction data. The specific focus of the CrowdRec Workshop, however, is going beyond recom-mender systems that passively collect information on user behavior, to active systems that reach out to  X  X he Crowd X , eliciting the intelligent information necessary to improve rec-ommendations.
Thank you to the authors, presenters, and reviewers for making this workshop possible. The organizers would also like to acknowledge the support of European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreement 610594. [1] R. A. Baeza-Yates, S. Ceri, P. Fraternali, and [2] V. Carvalho, M. Lease, and E. Yilmaz. CSE 2010: [3] V. Carvalho, M. Lease, and E. Yilmaz. CDSM 2011: [4] K.-T. Chen, W.-T. Chu, and M. Larson. ACM [5] K.-T. Chen, W.-T. Chu, M. Larson, and W. T. Ooi. [6] K.-T. Chen, I. King, C. M. Au Yeung, and O. Alonso. [7] J. Howe. Crowdsourcing. [8] J. Howe. The rise of crowdsourcing. Wired Magazine , [9] A. J. Quinn and B. B. Bederson. Human computation:
