 We address the task of learning rankings of documents from search engine logs of user behavior. Previous work on this problem has relied on passively collected clickthrough data. In contrast, we show that an active exploration strategy can provide data that leads to much faster learning. Specifically, we develop a Bayesian approach for selecting rankings to present users so that interactions result in more informative training data. Our results using the TREC-10 Web corpus, as well as synthetic data, demonstrate that a directed ex-ploration strategy quickly leads to users being presented im-proved rankings in an online learning setting. We find that active exploration substantially outperforms passive obser-vation and random exploration.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Measurement, Performance Clickthrough data, Web search, Active exploration, Learn-ing to rank
There has recently been an interest in training search en-gines automatically using machine learning (e.g. [20, 4, 24]). The ideal training data would be rankings of documents or-dered by relevance for some set of queries. In some cases it is practical to hire experts to manually provide relevance information for particular queries (as in [4]), but usually data provided by experts is too expensive and is not guar-anteed to agree with the judgments of regular users. This is a particular problem when typical user queries are short and ambiguous, as is often the case in web search. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
As an alternative source of training data, previous work has used clickthrough logs that record user interactions with search engines. However, as far as we are aware, all previous work has only used logs collected passively, simply using the recorded interactions that take place anyway. We instead propose techniques to guide users so as to provide more use-ful training data for a learning search engine.

To see the limitations of passively collected data, consider the typical interactions of search engine users. Usually, a user executes a query then perhaps considers the first two or three results presented [14, 15]. The feedback (clicks) on these results is recorded and used to infer relevance judg-ments. These judgments are then used to train a learning algorithm such as a ranking support vector machine [18]. In particular, users very rarely evaluate results beyond the first page, so the data obtained is strongly biased toward documents already ranked highly. Highly relevant results that are not initially ranked highly may never be observed and evaluated, usually leading to the learned ranking never converging to an optimal ranking.

To avoid this presentation effect, we propose that the ranking presented to users be optimized to obtain useful data, rather than strictly in terms of estimated document relevance. For example, one possibility would be to inten-tionally present unevaluated results in the top few positions, aiming to collect more feedback on them. However, such an ad-hoc approach is unlikely to be useful in the long run and would hurt user satisfaction. We instead introduce princi-pled modifications that can be made to the rankings pre-sented. These changes, which do not substantially reduce the quality of the ranking shown to users, produce much more informative training data and quickly lead to higher quality rankings being shown to users.

The primary contribution of this paper is to present a principled approach to efficiently obtain training data that leads to rankings of higher quality. We start by presenting a summary of observations about user behavior in Section 2, as how real users behave guides our approach. Next, in Sec-tion 3 we formalize the learning problem as an optimization task, present a suitable Bayesian probabilistic model and discuss inference and learning. Following this we present strategies to modify the rankings shown to users so that performance of learned rankings improves rapidly over time in Section 4. We describe our evaluation method in Section 5 and present results on synthetic data and TREC-10 Web data in Section 6. In particular, we see the improvements using our exploration strategies are much faster than with passive or random data collection.

Learning rankings relies on training data collected from users. We will now examine the specific properties of user behavior as they will guide our approach for the remainder of this work. In particular we will see what sort of data can reasonably be collected from clickthrough logs and how we should include user behavior in our learning task.
A number of studies have shown that users tend to click on results ranked highly by search engines much more of-ten than those ranked lower. For example, in recent work Agichtein et al.[1] present a summary distribution of the rel-ative click frequency on web search results for a large search engine as a function of rank for 120,000 searches for 3,500 queries. They show that the relative number of clicks rapidly drops with the rank  X  compared with the top ranked result, they observe approximately 60% as many clicks on the sec-ond result, 50% as many clicks on the third, and 30% as many clicks on the fourth. While we may hypothesize that this is simply because better results tend to be presented higher, Joachims et al.[21] showed that there is an inherent bias to rank in user behavior. They showed that users still click more often on higher ranked results even if presented with rankings reversing the top ten results. In fact, eye tracking studies performed by Granka et al.[15] show that the probability that users even look at a search result decays very quickly with rank.

On the one hand, this explains why most common per-formance measures in information retrieval place greater emphasis on highly ranked results (such as Mean Average Precision, Normalized Discriminative Cumulative Gain and Mean Reciprocal Rank). On the other hand, this observa-tion means that rank strongly influences how many times a document is evaluated by users, and hence how much train-ing data can be collected from clickthrough logs.
Given the recorded clicks, the question also remains how best to interpret the log entries to infer relevance infor-mation about a document collection. Two alternative ap-proaches to interpreting clickthrough logs are (1) consider a click on a document in a result set as an absolute sign of rel-evance, or (2) consider a click on a document as a pairwise judgment comparing the relevance of that document to some other document considered earlier. Specifically, a common approach is to assume that a clicked on result is more rel-evant than a non-clicked higher ranked result. Joachims et al.[21] showed that interpreting clicks as relative relevance judgments is generally reliable, while absolute judgments are not. Such pairwise relevance judgments have been success-fully used to learn improved rankings [20, 24].

A final observation of user behavior is that clickthrough logs are inherently very noisy. Users often click on search results without carefully considering them [15]. In partic-ular this means that any single judgment that states that one document is more relevant than another has a signifi-cant probability of being incorrect. However, previous work has shown that if the difference in relevance between docu-ments is larger, relative relevance judgments are less likely to be noisy [25]. This work also showed that if adjacent pairs of documents in the ranking shown to users are randomly swapped to compensate for presentation bias, provably re-liable pairwise relevance judgments about adjacent pairs of documents can be collected.

We summarize that (1) Clickthrough data is best inter-preted as relative relevance judgments; (2) The relevance judgments are noisy; (3) The top ranked documents are the most important to estimate correctly. Guided by these prop-erties, we now turn to formalizing the learning problem.
Assume we have a document corpus C = { d 1 , . . . , d |C| and some fixed user query q . For this query, we want to estimate the average relevance  X   X  i  X  &lt; of each document d (for some user population). From the previous section, we know that users can provide us with noisy judgments of the form  X   X  i &gt;  X   X  j . We assume that some ranking function can provide initial estimates of  X   X  i . The goal is accurately estimate  X   X  i with as little training data as possible. The estimation task involves a three step iterative process: First, given relevance estimates, we must select a ranking to display to users. Second, given the ranking displayed, users provide relevance feedback. Third, using the relevance feedback, we update the relevance estimates and repeat the process for the next user. In this paper, we focus most on the first step, namely selecting rankings of documents to show users so that the collected judgments allow the relevance estimates to be improved quickly, while at the same time maximizing the quality of the rankings.
Let M  X  = (  X   X  1 , . . . ,  X   X  |C| )  X  X  be the true relevance values of the documents in C . Modeling the problem of finding M given training data D in a Bayesian framework, we want to maintain our knowledge about M  X  in the distribution We assume that P ( M |D ) is a multivariate normal with zero covariance: Graphically, we can draw P ( M |D ) as a set of Gaussians centered at  X  i with variance  X  2 i . For example:
This model is motivated by ability estimates maintained for chess players [11]. In the most closely related previous work, Chu and Ghahramani address a similar problem using Gaussian Processes [8]. However, instead maintaining the distribution P ( M |D ), they directly estimate M  X  given D . This is also true of other related prior work [9, 10, 22]. The key difference in our approach is that we are not simply find-ing the optimizing ranking. Rather, maintaining P ( M |D ) is key as it allows us to optimize for collected training data.
We measure the difference between relevance assignments using a loss function L : M X M  X  &lt; . To find good rele-vance estimates, we want to find an M = (  X  1 , . . . ,  X  such that L ( M, M  X  ) is small. Noting that M  X  is unknown, we want to find the ranking that minimizes the expected loss given what we know about M  X  , namely P ( M |D ): where M  X  is drawn from the probability distribution P ( M |D ).
Suppose the loss function L can be decomposed over pairs of documents in C . We can then decompose the expected loss into a form easier to work with: where P ( M  X  |D ) is shorthand for M  X   X  P ( M |D ).
We will now show that the mode of P ( M |D ), namely  X  M = (  X  1 , . . . ,  X  |C| ), is often the solution to Equation 2. Con-sider solving Equation 2 for a loss function that counts the number of misordered pairs of documents. The assignment with minimum expected loss is the mode of P ( M |D ).
Lemma 1. Let P ( M |D ) = N (  X  1 , . . . ,  X  |C| ;  X  1 , . . . ,  X  a distribution over models. Assume L ( M, M  X  ) counts the number of differently ordered pairs of documents, when they are sorted by  X  i and  X   X  i respectively. A solution of is  X  M = (  X  1 , . . . ,  X  |C| ) .
 relevance assignment, and has lower loss than  X  M . There must exist two documents d i and d j that are ranked adja-cently when documents are ordered by M opt yet are ordered differently by  X  M , i.e.  X  opt i &gt;  X  opt j and  X  i &lt;  X  be the ranking obtained by reversing d i and d j . Let these rankings have expected loss E flip and E opt .

As the documents are adjacent, the loss of M opt and M flip only differs in the contribution of the pair ( d i , d j ) . Plugging in the loss function we get E where  X  is the cumulative distribution function of the stan-dard normal distribution, since  X  i &lt;  X  j . Hence we have a contradiction as M opt is not the minimizing ranking. We see a similar result for the loss function that penal-izes any error in the difference of document relevances, i.e. L pair ( M, M  X  , i, j ) = ((  X  i  X   X  j )  X  (  X   X  i  X   X   X  j
Lemma 2. Let P ( M |D ) = N (  X  1 , . . . ,  X  |C| ;  X  1 , . . . ,  X  a distribution over models. Assume L pair ( M, M  X  , i, j ) = ((  X  i  X   X  j )  X  (  X   X  i  X   X   X  j )) 2 . A solution of is  X  M = (  X  1 , . . . ,  X  |C| ) .

Proof. Let M opt be the minimizing model. Let  X  opt ij =  X  i  X   X  d according to M opt , and  X   X  ij and  X   X  ij be defined equivalently for  X  M and M  X  respectively. Let  X  ij = (  X  2 i +  X  2 j contribution to the expected loss for the pair of documents ( d , d j ) is which is minimized if  X  opt ij =  X   X  ij . Hence M opt =  X  mizes all terms in the sum in Equation 3 simultaneously and thus minimizes the expected loss.
 We see that the mode of the distribution P ( M |D ) minimizes the expected loss for two reasonable loss functions. As it can also be obtained very efficiently given P ( M |D ), for the remainder of this work we will assume that the mode is, or is close to, the minimizer of the expected loss. We will refer to the ranking obtained by sorting documents by their relevance according to the mode of P ( M |D ) as the mode ranking .
Given our analysis of real user behavior in Section 2, we see that the loss functions discussed above are too simple. Specifically, two properties to expect of an appropriate loss function are (1) The loss for ranking a less relevant docu-ment above a more relevant document should be larger if the documents are presented higher in the ranking (i.e. where users are more likely to observe them); (2) The loss should be larger if the difference in relevance is larger. To the best of our knowledge there is no common pairwise decomposable loss function with these properties so we propose a quadratic hinge-loss function with cost of misordering decaying expo-nentially with rank: L pair ( M, M  X  , i, j ) = e  X  r ij ` (  X  i  X   X  j )  X  (  X   X  With r ij we denote the minimum rank of d i or d j when all documents are ordered by M (i.e. the relevance assignments used to present results to users) divided by 10, and 1 is the indicator function. A pair of documents is considered misordered if the relative ranking according to M does not agree with that according to M  X  . Making use of the pairwise form of the loss function and plugging in the mode ranking  X  M , the inner term of Equation 3 can now be written as E = = 1  X  that the difference of two normally distributed variables is also normally distributed. Plugging in the loss, and choos-ing to sum over the pairs such that  X   X  ij is always negative, Equation 4 becomes: where erf() is the error function. Substituting this into Equation 3 gives an easy to compute closed form expres-sion for the expected loss.
As discussed in Section 2, clickthrough data is best inter-preted as relative relevance judgments. We can write them in the form d i d j , indicating that d i was judged more relevant that d j . A standard approach to modeling noise in pairwise comparisons is to assume that the probability of an outcome is determined by the Bradley-Terry model [2]: where rel ( d i ) is the relevance of d i . The Bradley-Terry model can be reparameterized setting rel ( d i ) = 10  X  i  X  is a known, global and fixed parameter. Assuming the pairwise judgments are independent (as can be reasonably expected with clickthrough data from multiple users),
P ( D| M = (  X  1 , . . . ,  X  |C| )) = Y Given this likelihood model and a Gaussian prior, we can ap-ply an off-the-shelf algorithm to maintain P ( M |D ), namely the glicko rating system commonly used for rating chess players [11]. Given an estimate of player ability (document relevance)  X  i and error in the estimate  X  i , this algorithm provides a set of approximate online update equations for maintaining the estimated relevance and error as data is col-lected. The update to the estimates for d i following a single comparison to d j (where s i is 1 if d i wins and 0 otherwise) is presented in Table 1.

While it would also be interesting to compare alternative ways of maintaining P ( M |D ) (e.g. [17]), or using a batch algorithm (see [19] for a discussion of alternatives), the sim-plicity and online aspects of the glicko system are appealing. In particular, in real world settings where large amounts of data are collected for large document collections with a large number of queries, a global optimization is likely to be slow and thus infeasible.
As we have seen that users are much more likely to pro-vide feedback on highly ranked documents, we turn to the question of optimizing the data collection process to most quickly minimize the loss. In particular, by selecting which documents to present at high rank, we influence the pairs of documents for which we obtain relevance judgments. In this paper, we will consider only modifications that change two documents in a ranking, limiting ourselves to the top two most of the time. We will see that despite the simplicity of this approach, substantial improvements in performance can be obtained at small cost in presented ranking quality.
We consider the following algorithms for determining which ranking to present users.
 sorting documents by  X  M = (  X  1 , . . . ,  X  |C| ).
The algorithm Top2 assumes no changes are made to the mode ranking, ignoring bias in data collection. This is the where approach used in all previous work in learning to rank, and would be effective if users provided feedback about results throughout the ranking. In some settings this may be the case, for example in search engines for academic articles where many users thoroughly consider all retrieved results. However in general web search settings, as discussed above, users focus their attention on the highest ranked results. documents and present them first and second. Then rank the remaining documents according to  X  M .

This algorithm is a naive modification to the mode rank-ing. Two random documents are picked uniformly and in-serted at the top of the ranking presented to users. Given the uniform distribution, this perturbation is likely to often pick documents that have a low prior expectation of be-ing relevant, thus likely presents users with poorer results. However, it benefits from the potential for feedback on all documents regardless of rank, even in the presence of signif-icant user bias. A similar method was proposed by Pandey et al. [23] in the context of identifying new web pages that would soon become popular, suggesting to randomly insert new documents into web search results.
 documents d i and d j that have the largest pairwise expected loss contribution, and present these first and second. Rank the remaining documents according to  X  M . Formally, this means we select the pair d i and d j that satisfies: LELpair selects the pair of documents with largest pairwise contribution to the expected loss out of all pairs of docu-ments. By presenting these documents at a high rank, the feedback given on them will reduce the uncertainty in the relative relevance of the documents. This will, in the long run, drive the expected loss contribution of the pair of doc-uments down. Given the glicko update rules, the pairwise contribution of all other pairs of documents will not increase. Hence this method will eventually drive the expected loss down. Additionally, due to the exponential decay in the loss of misordered pairs as the rank increases, LELpair tends to select pairs of documents where at least one has a high esti-mated relevance. Lower ranked documents are also eventu-ally selected, but only after high rank documents have been evaluated and their expected loss contribution is reduced. If we ignore the effect of rank in the loss function, this ap-proach is similar to previous work in active learning where users are asked to label items where the predicted label is most uncertain (e.g. [3, 27]). In our setting, document pairs with high pairwise contribution tend to be those with large estimated error in relevance.
 compute the expected pairwise loss and the expected pair-wise loss after a comparison based on the Bradley-Terry model (using  X  M to estimate the probability of possible out-comes) and glicko updates. Select the pair of documents with the largest expected reduction in the pairwise loss and present these first and second. Rank the remaining doc-uments according to  X  M . Formally, if  X  M 0 is the mode of P ( M |D ) after updating it given the outcome of a compari-son of d i and d j , we select the pair d i and d j that satisfies: Intuitively, this algorithm performs approximate gradient descent on the loss function. OSL finds the pair whose con-tribution to the expected loss is likely to decrease most fol-lowing a pairwise comparison. The expected contribution of the pair after a comparison is a weighted sum of the expected loss contribution for the two possible outcomes (either d wins the comparison or d j wins). In this computation, the effect of possible rank changes is ignored for efficiency rea-sons. This method is also related to an approach proposed by Chajewska et al. in the context of utility estimation where they found that the true utility of many different outcomes can be quickly discovered by maximizing the reduction in expected loss given new data [6].
 document d i , compute the total contribution of all pairs including d i to the expected loss of the ranking. Present the two documents with highest total contributions first and second, and rank the remainder according to  X  M . Formally, this method selects the pair d i and d j that satisfies: This method addresses a potential limitation of LELpair and OSL: They only consider individual pairwise document con-tributions to the expected loss despite the contributions of pairs not being independent. LELdoc addresses this by com-puting the total contribution of each document by summing over all pairs including that document. For example, if some document d is ranked third, it X  X  total contribution is that from d and the top ranked document, plus the contribution from d and the second document, plus that from d and the fourth document and so forth. LELdoc selects the two doc-uments with highest total contributions and presents them first and second. By comparing these two documents and reducing the uncertainty in their relevances, we are likely to reduce the contributions to the risk of all pairs including the documents.

An alternative selection algorithm proposed in previous work (e.g. [13, 7]) is to compare pairs of items such that the probability distribution over models changes most in terms of KL-divergence or entropy. We do not pursue this alternative as it does not take into account the loss function being optimized.

Finally, we note that explorations strategies for rankings are related to the opponent assignment problem in sports tournaments. However, there are two key differences. First, a tournament has a different concept of loss. A criterion often optimized is the probability of the true best player winning the final game (e.g.[12, 26]). Second, pairwise com-parisons in a tournament have no cost. In most sports, a common constraint is that all teams or players must com-pete for at least n rounds. This means that each  X  X tem X  must be compared with some other item every round and the optimization problem is to select which pairs are com-pared such that the loss is eventually minimized rather than aiming to minimize the loss as quickly as possible.
We now have a number of strategies for eliciting useful training data from users of a search system, and have a method to estimate the relevance of the documents using our probabilistic model. In this section, we will describe how these strategies were evaluated. In particular, we will com-pare how effective each strategy is at improving the quality of the rankings shown to users.

We evaluate as follows: Given an initial ranking of one thousand documents as returned by a search engine in re-sponse to a query, we derive a prior P ( M ). This prior initial-izes P ( M |D ). For a particular exploration strategy, we next select a ranking to present to users. We evaluate the loss of the presented ranking and of the mode ranking derived from  X  M . Next, we simulate user behavior on the presented ranking, using a simple behavioral model, and collect train-ing data that is used to update the model parameters. We repeat this process 3,000 times for each initial ranking. This experimental setup is formalized in Algorithm 1.

The behavioral model we use to simulate clickthrough data is detailed in Algorithm 2. By using a simulation, it is possible to evaluate the exploration strategies in detail without needing large numbers of test subjects, and avoid effects that may be unique to specific users (e.g. to academic users). Our model simplifies real behavior by assuming that users only click on top two results, and do so with probabil-ity specified by the Bradley-Terry model. This is motivated by the fast decay observed in the number of clicks as rank in-creases in real search systems. Clearly, in a real setting some additional data would be collected from lower ranks, making the results we report conservative in this respect. However, the amount of data collected about results at lower ranks would be significantly smaller.

We repeated each experiment with either 30 or 100 ini-tial rankings, each giving a different initial set of relevance estimates. We report the mean loss across all runs (nor-malized such that the initial loss is 1), or the mean average precision (MAP). In some results we present a single final Algorithm 1 Evaluation Setup 1: Input: Estimated relevances {  X  i } for d i  X  X  2:  X  i  X   X  0 for d i  X  X  3: for iteration 1 through 3,000 do 4: Pick two documents d i , d j to rank 1 st and 2 nd 5: Randomly swap d i and d j (see Section 2) 6: Show the selected ranking to user 7: Record training data given user feedback 8: Update  X  i ,  X  j ,  X  i ,  X  j per Equations 7 and 8 9: end for Algorithm 2 User Behavioral Model 1: Input: Ranking of documents ( d 1 , . . . , d |C| ) 2: Input: True relevances of documents (  X   X  1 , . . . ,  X  3: if U nif ormRandom (0 , 1) &lt; 1 4: Winner is d 1 : s 1  X  1; s 2  X  0 5: else 6: Winner is d 2 : s 1  X  0; s 2  X  1 7: end if performance, i.e. the loss or MAP after 3,000 pairwise com-parisons and model updates. Note that as our rankings are of 1,000 documents, 3,000 comparisons is on average just six noisy pairwise comparisons involving each document.
We start by evaluating the exploration strategies on syn-thetic data, where we evaluate their effectiveness if the as-sumed prior distribution matches the true generating model. This will be followed by an evaluation using TREC-10 data.
We randomly generated a corpus of 1000 documents with expected relevances  X   X  i drawn from N (1500 , 147 2 ). We chose this scale as it is comparable to typical chess scores. We then drew ten independent initial models, drawing  X  i from N (  X   X  i , 147 2 ) and initializing  X  i = 147  X   X  0 . We repeated this process three times, giving 30 initial rankings over three different random corpora.

For each initial ranking, we ran each strategy for 3,000 iterations. Figure 1 shows the loss of the mode ranking at each iteration. Along the horizontal axis is the number of pairwise comparisons. After each pairwise comparison, P ( M |D ) is updated and a new pair to compare is selected. The vertical axis is the average loss of the mode ranking relative to the initial average loss. The error bars indicate one standard error in the mean scaled loss.
 The first question to answer is which strategy learns fastest. The passive Top2 approach does not lead to a meaningful overall reduction in the loss. This is because our user model assumes that users only provide feedback on the top two documents. After a few comparisons, the top few documents have their relative position correctly established and no new documents are ever compared again. Our other baseline algorithm, Random, sees the loss decrease slowly.
We see that the other exploration strategies all perform substantially better than the baselines. Both LELpair and OSL quickly reduce the total loss by selecting pairs of doc-Figure 1: Change in loss as a function of the number of pairwise comparisons for each exploration strat-egy on synthetic data Figure 2: Effect of weight of prior on the final loss evaluated on synthetic data (true noise is  X  0 = 147 ) uments with high contributions to the expected loss, and high expected reductions in it. We see this improvement continues for a large number of comparisons. In contrast, LELdoc appears to asymptote more quickly. This is be-cause the documents selected continue to be those at high ranks even after many comparisons. In effect, LELdoc is too biased toward highly ranked documents. Comparing with LELpair and OSL, we see that while lower ranked doc-uments may have lower total contribution to expected loss, they often have higher individual pairwise contributions. The second natural question to ask is how robust the results are to the weight given to the initial ranking, as in the case of real data the correct weight is likely to be unknown. This weight is encoded by the initial values of  X  i , i.e.  X  0 explore the effect of changing that value. This experiment is possible because we know the level of noise used when generating synthetic data.

Figure 2 shows the effect of selecting an assumed noise level that differs from the true noise level. With our default setting, and that used to generate our synthetic datasets, the probability of a document in the top 10 according to the prior not being in the top 100 according to true relevance is Figure 3: Change in loss as a function of the number of pairwise comparisons for each selection algorithm on TREC-10 data about 8%. We can see that selecting a suboptimal  X  0 does not drastically reduce performance after a fixed number of pairwise comparisons. Apart from LELdoc, the best perfor-mance is achieved when the actual noise is correctly known. Interestingly, LELdoc performs best when the error in the prior is underestimated as it then selects documents further down the ranking for comparison.
In addition to the synthetic data, we also evaluated the exploration strategies using the TREC-10 web track queries (topics 501 through 550) in the WT10g document corpus. This subset of the corpus includes 50 topics and topic de-scriptions, run as queries on documents that are part of the corpus. As part of the 10th Text REtrieval Confer-ence (TREC-10) [16], 18 teams submitted a ranking of doc-uments for each topic. Then, for each topic, documents ranked highly by the teams were manually judged to be ei-ther highly relevant (relevance score of 2), relevant (score of 1) or non-relevant (score of 0). All other documents in the corpus were assumed non-relevant (score of 0). The discretized nature of these relevance judgments is unrealis-tic, as few documents are likely to have precisely the same relevance in the real world. To compensate and make the learning problem more realistic, we added uniform random noise in the range [  X  0 . 5 , 0 . 5] to the true relevance judgments, preserving the relative order of highly relevant, relevant and non-relevant documents.

For each of the 50 TREC-10 topics, we randomly selected two submissions and used the submitted scores to initialize our model. We then repeated the evaluation described for synthetic data. Each submission includes a ranking of typi-cally 1000 documents, with a score given to each document. The scores are arbitrary and unnormalized. Clearly they can be interpreted as a prior in any number of ways. As each ranking typically contains both highly relevant docu-ments and non-relevant documents, we chose to normalize the scores to a linear interval [1500 +  X  0 , 1500  X   X  0 ] with  X  0 = 147. The resulting scores were used to set the initial  X  . The initial estimated error  X  i were set to  X  0 . This means that a document with a score near the maximum score is es-timated to have approximately a 30% percent chance of in fact being in the lower half of the ranking. Figure 4: Change in MAP score as a function of the number of pairwise comparisons for each selection algorithm on TREC-10 data Figure 3 shows the performance of the exploration strate-gies. We see that the loss improves rapidly with LEL-pair, OSL and LELdoc. We also see that Top2 performs as it did on the synthetic data. One the other hand, Ran-dom performs differently: The loss of the mode ranking ini-tially increases, then improves slightly but remains high. We believe that this is due to the mismatch between the prior and model. When two documents are compared, if the outcome is not the expected one then the update to the rel-evance estimates can be large. Sometimes, the lower ranked document moves to a much higher rank, and the loss does not quickly recover due to few comparisons per document. Interestingly, performance of Random depends on the loss of the initial ranking. When the initial loss is high, after 3,000 pairwise comparisons the loss tends to be reduced. The op-posite is true when we start with a very good ranking. The loss function presented earlier is not one commonly used to evaluate rankings. A measure much more widely used by the research community is the mean average precision (MAP). To compute the MAP, we consider each document with true relevance  X   X  i above some threshold as relevant and others as irrelevant. The average precision of a ranking is the average of the precision measured at each relevant docu-ment 1 . The MAP score is the mean of the average precisions across all 100 experiments. We used a threshold of 0.5 scaled in the same way the scores were scaled.

Figure 4 shows how the MAP of the ranking changes as more pairwise comparisons are performed. MAP visibly be-haves very similarly to our loss function. We see that LEL-pair and OSL result in the largest MAP improvement, and appear likely to continue to improve further with more com-parisons. As before, LELdoc performance plateaus quickly and Top2 sees almost no improvement. Random performs poorly, with an initial drop in MAP although after 2,000 pairwise comparisons the MAP is above baseline. Figure 5: Effect of different noise levels in pair-wise preferences on final MAP score, evaluated on TREC-10 data.
 So far, our simulation has assumed a particular amount of noise in user clicks. Given two documents that differ in true relevance by one TREC relevance level, we have assumed that the user will click on the more relevant one 70% of the time. While this level of noise is realistic [21], it is of in-terest to observe what would happen to the difficulty of the learning task if the noise level were different. We modified our user model to change the level of noise, and changed the glicko update equations equivalently. Figure 5 shows the fi-nal MAP score after 3,000 pairwise comparisons as the level of noise changes. First, we see that irrespective of the noise level, the best exploration strategies remain LELpair and OSL. On the left of the figure we see that if there is a lot of noise in the preferences, all the algorithms perform more poorly. This is to be expected, given that the amount of in-formation contained in 3,000 pairwise preferences decreases as the amount of noise in user clicks increases. On the other end of the scale, we see that even if users select the more relevant document almost 100% of the time, the final MAP score decreases somewhat. This is a side-effect of the normal approximation implicit in the glicko updates [11]. The above results assume the noise level is known in ad-vance. Figure 6 shows the effect of a mismatch between the assumed noise level (used to scale the initial scores, and in the glicko updates) and the true level of noise in pairwise preferences (in the user model). Along the horizontal axis we have the probability a user selects the correct document in a pair as more relevant, if the true relevances of the pair differ by one TREC relevance level. The figure shows how the MAP of the mode ranking after 3,000 pairwise compar-isons is affected by different estimates of the noise in pairwise clicks. Each line corresponds to a different assumed noise level. The figure shows that if the amount of noise is under-estimated, performance is poorer although not drastically so (unless the noise is underestimated substantially). On the other hand, we see that if the pairwise preferences are less noisy than assumed, the final performance does not suffer.
Of particular interest, these results tell us that the best strategy is to assume the level of noise conservatively in or-Figure 6: Effect of incorrect assumptions about the noise level in relevance judgments on final MAP score, evaluated on TREC-10 data using OSL.
 e e e Table 2: Mean Average Precision after 3000 itera-tions of optimizing different loss variants using OSL. der to see the best performance improvements. Finally, it is worth noting that in the case of extremely noisy clicks it may be beneficial to aggregate user clicks, i.e. collect a number of pairwise judgments for each pair of documents, then count this as a single pairwise judgment for the docu-ment with the most preference votes. This would reduce the effective level of noise in pairwise judgments at the expense of necessitating more data.
 So far, our experimental results show that minimizing the expected loss also improves the MAP. We now demonstrate that the loss function presented in Section 2 leads to par-ticularly good MAP performance. Table 2 shows the MAP performance after 3,000 pairwise comparisons if we optimize OSL to different variants of the loss function presented, by ignoring individual properties suggested in Section 3.3.
We see that using a loss function without exponential de-cay, without a distance penalty or without a hinge leads to substantial and significant reductions in the final MAP scores. In particular, optimizing a loss function that sim-ply depends on document ranks, rather than on the actual relevance estimates (the fourth line of the table) leads to poorer MAP performance. This shows that despite MAP only being sensitive to document order, minimizing error in relevance estimates leads to better MAP performance. The figures in the previous two sections show the loss and MAP of the mode ranking as pairwise preference data is collected. However, to collect the data, the ranking of doc-uments that is presented to users is not the mode ranking. Rather, users see rankings with a pair of results inserted at Figure 7: MAP scores of the mode rankings, and the presented rankings as a function of number of pairwise comparisons for OSL and Random. the top. This means that usually the presented ranking has a higher loss and a lower MAP score than the mode rank-ing. The difference between the MAP of the presented and mode rankings is shown in Figure 7. The top two lines are for OSL. The top line shows the MAP of the mode ranking. The second line shows the MAP of the ranking shown to users. We see that while the presented ranking is worse than the mode ranking, it is almost immediately above the initial MAP, and improves quickly as data is collected. The sep-aration between the two rankings increases because as the relevance of higher ranked documents is established, lower ranked documents are shown more often. We see a sim-ilar effect comparing the presented and mode rankings of the Random strategy in the lower two lines, although the presented ranking does not have a higher MAP than the initial ranking for all 3,000 pairwise comparisons. We also note that the MAP of the presented ranking using OSL is substantially better than the mode ranking when the Ran-dom exploration strategy is used.

An interesting final experiment is to consider the trade-off between the quality of the presented ranking and the quality of the mode ranking. One possibility to reduce the impact of data collection would be to present selected pairs at lower ranks instead of at the top two positions. With real users, this would lead to a reduction in the amount of data collected, but may improve the MAP of the presented rank-ing by reducing the performance gap. Figure 8 shows how the MAP score of the presented ranking after 3,000 pairwise comparisons would change if the selected pair was presented at a lower rank. The reduction in the amount of data col-lected as a function of rank is shown along the horizontal axis. For example, if for some user population moving the selected pair down by one rank reduces the number of pair-wise preferences collected by 60%, the correct point for an evaluation would be 0.4 on the horizontal axis. This would mean that by presenting a pair at rank 2 and 3 rather than 1 and 2, we would only receive 40% as many clicks. Pre-senting selected pairs at ranks 3 and 4 would receive 16% as many clicks. At 0.4, we see that due to the reduction in the amount of data collected, the MAP of the presented ranking would be lower if the selected pairs were at ranks 3 and 4 rather than at ranks 1 and 2. Figure 8: MAP scores of the presented ranking after 3000 pairwise comparisons assuming pairs presented at different ranks. Figure 9: MAP scores of the mode and presented rankings after 3000 pairwise comparisons.
 Taking the same lines as in Figure 8 and overlaying the MAP of the mode rankings for all three ranks, we get Figure 9. It shows that the separation between the mode ranking and the presented ranking decreases as the rank of the se-lected pair is decreased, as we may expect. However, note that as less data is collected, the MAP of the mode rank-ing after 3,000 pairwise comparisons also decreases. This is especially the case if much less data is collected when pairs are presented at lower ranks.
In this paper, we have formalized the ranked relevance elicitation problem, and demonstrated that by using active exploration the quality of a ranking can be improved faster than by collecting pairwise training data passively or naively. We presented a number of strategies to minimize the ex-pected loss and showed that two in particular perform well. Our experiments showed a significant level of robustness to noise in the clickthrough data and to prior assumptions. We also demonstrated how presentation loss and quality of learned ranking can be traded off.

A natural extension of this work would be to find a global optimization approach for this problem, so as to derive a provably optimal algorithm to collect training data for learn-ing to rank using a regret minimization approach. There has also recently been interest in minimizing the number of documents that need to be evaluated by human judges to evaluate performance of ranking algorithms on a document collection (e.g. [5, 28]). While such human judgments are absolute, i.e. a human is asked to rate each document on a fixed scale, it would be interesting to adapt our approach to a setting that minimizes the number of human evaluations necessary to find the correct ranking with an appropriate prior and loss function. The first author was partly supported by a Microsoft Ph.D. Student Fellowship. This work was also supported by NSF Career Award number 0237381 and a gift from Google. [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] R. A. Bradley and M. E. Terry. The rank analysis of [3] K. Brinker. Active learning of label ranking functions. [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [5] B. Carterette, J. Allan, and R. Sitaraman. Minimal [6] U. Chajewska, D. Koller, and R. Parr. Making [7] W. Chu and Z. Ghahramani. Extensions of gaussian [8] W. Chu and Z. Ghahramani. Preference learning with [9] O. Dekel, C. D. Manning, and Y. Singer. Log-linear [10] N. Fuhr. Optimum polynomial retrieval functions [11] M. E. Glickman. Parameter estimation in large [12] M. E. Glickman. Bayesian optimal design of knockout [13] M. E. Glickman and S. T. Jensen. Adaptive paired [14] L. Granka. Eye tracking analysis of user behaviors in [15] L. Granka, T. Joachims, and G. Gay. Eye-tracking [16] D. Hawking and N. Craswell. Overview of the [17] R. Herbrich and T. Graepel. Trueskill T M : A bayesian [18] R. Herbrich, T. Graepel, and K. Obermayer. Large [19] D. R. Hunter. MM algorithms for generalized [20] T. Joachims. Optimizing search engines using [21] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [22] R. Lin, T. A. Louis, S. M. Paddock, and G. Ridgeway. [23] S. Pandey, S. Roy, C. Olston, J. Cho, and [24] F. Radlinski and T. Joachims. Query chains: Learning [25] F. Radlinski and T. Joachims. Minimally invasive [26] D. Ryvkin. The predictive power of noisy elimination [27] M. Saar-Tsechansky and F. Provost. Active sampling [28] L. Torrey. An active learning approach to efficiently
