 We propose to enhance proximity-based probabilistic re-trieval models with more contextual information. A term pair with higher contextual relevance of term proximity is assigned a higher weight. Several measures are proposed to estimate the contextual relevance of term proximity 1 . We assume the top ranked documents from a basic weight-ing model are more relevant to the query, and calculate the contextual relevance of term proximity using the top ranked documents. We propose a context-sensitive 2 proxim-ity model, and the experimental results on standard TREC data sets show the effectiveness of our proposed model. Context-Sensitive IR, Measure, Proximity, Probabilistic Model
The study of how to integrate the context information of queries and documents into retrieval process draw a lot of attention in recent years [3]. More specifically, many term proximity approaches [2, 9, 10, 11], which reward the docu-ments where the query terms occurring closer to each other, show significant improvements over basic Information Re-trieval (IR) models. In these proximity-based approaches, all the query term pairs are usually treated equally and the difference among various query pairs are not considered, al-though there is a need to distinguish the importance of term proximities. For example, given a query  X  X ecycle automobile tires X , there is a stronger association between  X  X utomobile X  and  X  X ire X  than the association between  X  X ecycle X  and  X  X u-tomobile X . In the top ranked documents,  X  X utomobile X  and  X  X ire X  are expected to occur close to each other, while  X  X ecy-cle X  and  X  X utomobile X  do not necessarily have to occur close.
In this paper, we focus on the problem of differentiating the influence of associated query term pairs. We propose
The contextual relevance of term proximity is the relevancy between the query term proximity and the topic of the query.
Context-sensitive means that the contextual relevance of term proximity is considered.
 a proximity enhancement approach to integrate the con-textual relevance of term proximity into the retrieval pro-cess. We also propose four measures for estimating the con-textual relevance of term proximity, and reward the query term pairs according to both the proximity and the con-textual relevance of proximity. There are several studies that boost proximity retrieval models. A machine learning method is proposed to determine the X  X oodness X  X f a span in [8]. [1] learns the concept importance from several sources (e.g. google n-gram corpus, query logs and wikipedia titles). SVM is used to learn different weights for various term de-pendencies in [6]. The importance of the global statistics is examined for proximity weighting [4]. Phrases are treated as providing a context for the component query terms in [7]. The contribution of this paper is that we propose the contextual relevance of term proximity, which represents to what extent the corresponding term pair should be related to the topic of the query. The contextual relevancy of term proximity is combined with the value of term proximity to characterize how much a document should be boosted. The remainder of this paper is organized as follows. In Section 2, we introduce four measures for estimating the con-textual relevance of term proximity. In Section 3, we pro-pose an enhanced context-sensitive proximity model using the proposed measures. Section 4 presents the experimental results and parameter sensitivities. Section 5 concludes the findings and discusses possible future directions.
In this section, we propose how to estimate the contextual relevance of term proximity. The contextual relevance of term proximity is defined as how much the corresponding term pair should be related to the topic of the query in the context. The notations used in this paper are shown as follows.
We measure the contextual relevance of term proximity base on the assumption that distributions of q i and q j in a relevant documents can represent the association between q and q j .If q i and q j occur closely in relevant documents, ,q the contextual relevance of term proximity between q i and q j is high. On the contrary, if q i and q j do not co-occur or occur far away to each other, the contextual relevance of term proximity between q i and q j is low. Therefore we propose the following four methods for estimating the con-textual relevance of term proximity between q i and q j in a relevant document D . For the extreme case when q i and q do not co-occur in D , we consider the contextual relevance of term proximity equals 0. Otherwise, we define the follow-ing measures to generate a positive value for the contextual relevance of term proximity.

Definition 1. Rel CoOccur ( q i ,q j ,D ) is defined to be 1, if q and q j both occur in D.

Definition 2. The Rel SqRecip is defined as the sum of squared reciprocal distances between q i and q j .

Definition 3. The Rel MinDist is a defined as the following function of the minimum distance between q i and q j . where  X  is a parameter, and MinDist ( q i ,q j ,D )isthemin-imum distance between all co-occurring q i and q j in D .
MinDist ( q i ,q j ,D )
Definition 4. The Rel Kernel is defined as the sum of the kernel functions of distances between q i and q j . where Kernel (  X  ) is kernel function. Here we use the triangle kernel function.
 where u is an input value, and  X  is the kernel parameter.
These functions measure the contextual relevance from different perspectives. Rel CoOccur measures whether q i and q are co-occurring in D . Rel SqRecip , Rel MinDist and Rel considers the positions of q i and q j in D .In Rel SqRecip generate a squared reciprocal function for the distances be-tween all the occurrences of q i and q j , and accumulate the values over D . Then the query term pairs with terms occur-ring closer to each other and/or occurring more frequently will have higher contextual relevance. Rel MinDist is modi-fied from [9], where the minimum distance is shown to be more effective than the other distance-based and span-based proximity approaches. Rel Kernel utilizes the term proximity approach proposed in [10], where a query term is simulated by the kernel function, where the triangle kernel function is recognized to be the most effective. Different types of information are incorporated in these measures.

To better analyze the contextual relevance measurements defined above, we present an example for a given query Q = { q 1 ,q 2 ,q 3 ,q 4 ,q 5 } and a relevant document D . where x represents a non-query term. By observing the query and the document, we find that the term q 5 does not present in D ,whichmeansitdoesnotrelatedto D ,and therefore do not have an association with other query terms. Since q 3 and q 4 are adjacent to each other and far apart from other query terms, q 3 and q 4 aremorelikelytohavea stronger association than the combination of q 2 and q 4 .We calculate the contextual relevance of term proximity between q and q 4 as an instance, and the procedure will be the same for the rest of the query term pairs. The term frequency of terms q 2 and q 4 are tf ( q 2 ,D )=2and tf ( q 4 ,D )=1. The positions of q 2 and q 4 in D are pos ( q 2 ,D )= { 4 , 18 pos ( q 4 ,D )= { 10 } correspondingly. Therefore there are 2 co-occurrences of q 2 and q 4 , and the corresponding distances between these co-occurrences are { 6 , 8 } .Thenwecancal-culate Rel ( q 2 ,q 4 ,D ) with these distances by formulae (1-4). Table 1 shows the values of the contextual relevance of term proximity in this example.

We can see that the contextual relevance measures de-fined above show different characteristics. Rel CoOccur de-tects term pairs with or without an association. For ex-ample, q 5 and other query terms do not have an associ-ation. The term pairs containing q 5 are distinguished by Rel CoOccur . On the other hand, Rel CoOccur does not con-sider the term distributions in D . Rel MinDist takes into ac-count the closest occurrences between a pair of quay terms, and does not consider the frequency of occurrences. Rel MinDist and Rel Kernel accumulates over all the occurrences of two query terms, with different functions and therefore generates different values.
In this section, we propose a context-sensitive proximity retrieval model, by integrating the proposed measures for contextual relevance of term proximity into retrieval pro-cess. Naturally we treat the values of contextual relevance as weights to reward the query term pairs with higher con-textual relevance and to penalize the query term pairs with lower contextual relevance. In practice, we assume the top ranked documents returned by a basic retrieval model (for example, BM25) are more relevant than the rest of the docu-ments. The averaged contextual relevance of term proximity over the top ranked documents is multiplied by the proxim-ity part in the weighting function. A general form of the context-sensitive proximity model is where D is a given document, w ( q i ,D )istheweightof q i D by a basic probabilistic weighting function, Prox ( q i is a bigram proximity weighting approach,  X  is a balancing parameter, topDoc is the number of top ranked documents, AR ( q i ,q j , topDoc ) is the average contextual relevance value of term proximity between q i and q j over the top ranked documents where Rel ( q i ,q j ,D ) is one of the measures defined in Section 2. Please note that AR ( q i ,q j , topDoc ), Prox ( q i ,q w ( q i ,D ) need to be normalized to the same scale.
In formula (6), we use the probabilistic BM25 [5] as the basic weighting function. We adopt the proximity approach used in CRTER [10], since it is an effective pairwise proxim-ity model for probabilistic IR. The BM25 weighting function has the following form. where N is the number of documents in the collection, n ( q is the number of documents containing q i , qtf ( q i )isthe within-query term frequency, dl ( D ) is the length D , avdl is the average document length, the k i s are tuning constants, K equals k 1  X  ((1  X  b )+ b  X  dl ( D ) /avdl ). The proximity part of CRTER is shown as follows.
 where q i,j represents the association between query terms q and q j , w ( q i,j ,D ) is the BM25 weighting function with the following features of q i,j [10] tf ( q i,j ,D )= where Kernel (  X  ) is a kernel function, and Occur ( q i,j We evaluate the proposed approach on three standard TREC data sets. They are AP88-89 with topics 51-100, Web2G with topics 401-450, and TREC8 with topics 401-450. AP88-89 contains articles published by Association Press from the year of 1988 to 1989. The WT2G collec-tion is a 2G size crawl of Web documents. The TREC8 contains newswire articles from various sources, such as Fi-nancial Times (FT), the Federal Register (FR) etc. For all the data sets used, each term is stemmed using Porter X  X  En-glish stemmer, and standard English stopwords are removed. We have three baseline models, BM25, Dirichlet Language Improvement over BM25 1.329% 5.166% 5.634% Improvement over BM25 2.216% 7.621% 6.283% Improvement over CRTER 0.875% 2.335% 0.614% Improvement over BM25 3.840% 9.821% 6.729% Improvement over CRTER 2.478% 4.427% 1.036% Improvement over BM25 3.397% 9.821% 5.999% Improvement over CRTER 2.041% 4.427% 0.345% Improvement over BM25 3.840% 9.216% 6.405% Improvement over CRTER 2.478% 3.851% 0.729% Table 2: Overall MAP Performance ( X * X  indicates significant improvement over BM25, and  X   X   X  indi-cates significant improvement over CRTER) Model (LM) and CRTER. The best parameters are chosen in the baseline models for fair comparisons. In BM25, the values of k 1 , k 2 , k 3 and b are set to be 1.2, 0, 8 and 0.35 respectively, since they are recognized with a good perfor-mance. In CRTER model, we use the recommended settings [10]., which are  X  = 25,  X  =0 . 2, and triangle kernel function. In our proposed context-sensitive proximity model, we use the same parameters in the basic weighting model part (e.g. BM25) and the proximity part (e.g. CRTER). In Rel Kernel , we set the kernel parameter  X  = 25. In Rel MinDist ,weset  X  = 1, which has the best performance in [9]. We normalize AR ( q i ,q j , topDoc ), Prox ( q i ,q j ,D )and w ( q i (6) to the scale of [0,1]. We use the Mean Average Precision (MAP) as our evaluation metric.

Table 2 shows the overall MAP performance. The pro-posed context-sensitive proximity model outperforms BM25, Language Model (LM) and CRTER with all of the contex-tual relevance measuring approaches on all the data sets. For the space limitation, we only include these comparisons. It shows that using the contextual relevance of term prox-imity can further boost the retrieval performance. We can see that the Rel CoOccur , which measures whether two query terms are co-occurring in the relevant documents, reaches the lowest MAP among the contextual relevance measures, which indicates the necessity of considering the term loca-tion information in the term pair contextual relevance defi-nition. Rel SqRecip has the highest MAP over the other ap-proaches on all the data sets. In general, considering both the closeness and frequency of two query terms in the con-textual relevance definition benefits the contextual relevance estimation.

In Table 3, we investigate how the number of top relevant documents affects the retrieval performance. We take the topDoc = 5, 10, 20, 30, 40, 50 60, 70, and 80 documents as relevant, and calculate the average contextual relevance obtained from these documents. The bolded values are the best performance among different topDoc values. We can see that the best topDoc will be around 5 to 40. It means that selecting too many top documents as relevant will introduce noises to the model.
 Figure ?? shows the sensitivity of  X  on all the data sets. We can see that with the growth of  X  , MAP first increases and then decreases. Please note that when  X  =0,thereis no proximity utilized, which is our baseline BM25. When  X  = 1, only term proximity and the contextual relevance of term proximity are considered. In CRTER, the recom-mended setting for the balancing parameter is 0.2. After in-troducing the contextual relevance of term proximity, we can see that the balancing parameter  X  with a value of around 0.3 or 0.4 is better. The reason is that the contextual rele-vance is normalized to [0,1]. The value for the second part of formula (6) becomes smaller, therefore it requires a larger balancing parameter.
We propose a new approach to integrate the contextual relevance of term proximity in retrieval. The contextual relevance of term proximity evaluates how much we should focus on a pair of query terms. In particular, we propose four measures to estimate the contextual relevance of term Rel Kernel . They incorporate different types of information utilized in the contextual relevance of term proximity. We further propose a context-sensitive proximity model via mul-tiplying the contextual relevance of term proximity by the proximity part in a retrieval model.

We evaluate our proposed context-sensitive proximity model on several TREC standard data sets, and the experimental results show the effectiveness of our proposed model over three baselines BM25, Dirichlet LM and CRTER with op-timal parameter settings. In more detail, we discuss how many top documents should be selected for calculating the proximity contextual relevance, and how the balancing pa-rameter  X  affects the retrieval performance.

In the future, we can extend the contextual relevance of term proximity to more query terms. In addition, the con-textual relevance of term proximity can be adopted in other basic weighting models (e.g. Language Model) and/or other proximity approaches. We can also apply the proposed model into relevance feedback. This research is supported by the research grant from the NSERC of Canada and the Early Researcher Award. We thank four anonymous reviewers for their thorough review comments on this paper.
