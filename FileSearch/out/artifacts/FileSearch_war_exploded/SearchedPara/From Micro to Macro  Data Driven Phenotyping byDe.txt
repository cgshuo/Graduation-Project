 Inferring phenotypic patterns from population-scale clinical data is a core computational task in the development of per-sonalized medicine. One important source of data on which to conduct this type of research is patient Electronic Med-ical Records (EMR). However, the patient EMRs are typi-cally sparse and noisy, which creates significant challenges if we use them directly to represent patient phenotypes. In this paper, we propose a data driven phenotyping frame-work called Pacifier (PAtient reCord densIFIER), where we interpret the longitudinal EMR data of each patient as a sparse matrix with a feature dimension and a time dimen-sion, and derive more robust patient phenotypes by explor-ing the latent structure of those matrices. Specifically, we assume that each derived phenotype is composed of a subset of the medical features contained in original patient EMR, whose value evolves smoothly over time. We propose two formulations to achieve such goal. One is Individual Basis Approach ( Iba ), which assumes the phenotypes are differ-ent for every patient. The other is Shared Basis Approach ( Sba ), which assumes the patient population shares a com-mon set of phenotypes. We develop an efficient optimiza-tion algorithm that is capable of resolving both problems efficiently. Finally we validate Pacifier on two real world EMR cohorts for the tasks of early prediction of Congestive Heart Failure (CHF) and End Stage Renal Disease (ESRD). Our results show that the predictive performance in both tasks can be improved significantly by the proposed algo-rithms (average AUC score improved from 0.689 to 0.816 on CHF, and from 0.756 to 0.838 on ESRD respectively, on diagnosis group granularity). We also illustrate some inter-esting phenotypes derived from our data.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; J.3 [ Life and Medical Sciences ]: Health, Medical information systems Medical informatics; phenotyping; sparse learning; matrix completion; densification
Patient Electronic Medical Records (EMR) are systematic collections of longitudinal patient health information gen-erated from one or more encounters in any care delivery setting. Typical information contained in EMR includes pa-tient demographics, encounter records, progress notes, prob-lems, medications, vital signs, immunizations, laboratory data and radiology reports, and etc. Effective utilization of EMR is the key to many medical informatics research problems, such as predictive modeling [36], disease early de-tection [30], comparative effectiveness research [17] and risk stratification [19].

Working directly with raw EMR is very challenging be-cause it is usually sparse, noisy and irregular. Deriving better and more robust representation of the patients, or phenotyping, is very important in many medical informatics applications [13, 35]. One significant challenge for pheno-typing with longitudinal EMR is data sparsity . To illustrate this, we show the EMR of a Congestive Heart Failure (CHF) patient in Fig.1, which is represented as a matrix. The hori-zontal axis is time with the granularity of days. The vertical axis is a set of medical events, which in this example is a set of diagnosis codes. Each dot in a matrix indicates that the corresponding diagnosis is observed for this patient at the corresponding day. From the figure we can see that there are only 37 nonzero entries within a 90-day window.
With those sparse matrices, many existing works just treat those zero values as actual zeros [30, 27, 25], and construct feature vectors from them with some summary statistics, then feed those feature vectors into computational models (e.g., classification, regression and clustering) for specific tasks. However, this may not be appropriate because many of those zero entries are not actual zeros but missing (the patient did not pay a visit and thus there is no correspond-ing record). Thus, the feature vectors constructed in this way are not accurate. As a consequence, the performance of the computational models will be compromised.

To handle the sparsity problem, we propose a general framework, Pacifier (PAtient reCord densIFIER), for phe-notyping patients with their EMRs, which imputes the val-ues of those missing entries by exploring the latent struc-tures on both feature and time dimensions. Specifically, we assume those observed medical features in EMR (micro-phenotypes) can be mapped to some latent medical con-Figure 1: An example of the patient X  X  EMR. The h orizontal axis represents the number of days since the patient has records. The vertical axis corre-sponds to different diagnosis codes. A green dia-mond indicates the corresponding code is diagnosed for this patient at the corresponding day. cept space with a much lower dimensionality, such that each medical concept can be viewed as a combination of several observed medical features (macro-phenotypes). In this way, we expect to discover a much denser representation of the patient EMR in the latent space, and the values of those medical concepts evolve smoothly over time. We develop the following two specific formulations to achieve such goal:
When formulating Pacifier , we enforce sparsity on the la-tent medical concept mapping matrix to encourage represen-tative and interpretable medical concepts. We also enforce temporal smoothness on the concept value evolution matrix that captures the continuous nature of the patients. We de-velop an efficient Block Coordinate Descent (BCD) scheme for both formulations, that has the capability of process-ing large-scale datasets. We validate the effectiveness of our method in two real world case studies on predicting the on-set risk of Congestive Heart Failure (CHF) patients and End State Renal Disease (ESRD) patients. Our results show that the average prediction AUC in both tasks can be improved significantly (from 0.689 to 0.816 on CHF prediction, and from 0.756 to 0.838 on ESRD respectively) with Pacifier .
The rest of this paper is organized as follows: Section 2 presents the general representation of EMR and the problem of patient risk prediction which is one important problem that patient phenotyping will be applied to. In Section 3 we introduce the details of Pacifier . The experimental results are presented in Section 4. In Section 5 we discuss the connection of the proposed approaches to related work and insights for future works. Section 6 concludes the paper. Figure 2: Granularity of medical features. For diag-n osis events, features can be constructed at differ-ent levels of granularity: ICD9 code, diagnosis code (DxGroup) and HCC code.
Risk prediction is among the most important applications in clinical decision support systems and care management systems, where it often requires building predictive mod-els for a specific disease condition. As Electronic Medical Records (EMR) data becomes widely available, informative features for risk prediction can be constructed from EMR. Based on the EMR data, for example, care providers usually want to assess the risk scores of a patient developing differ-ent disease conditions, such as congestive heart failure [30, 6], diabetes [24], and end stage renal disease [1]. Once the risk of a patient is predicted, proper intervention and care plan can be designed accordingly.

The detailed EMR data documents the patient events in time, which typically includes diagnosis, medication, and clinical notes. The diagnosis events are among the most structured, feasible and informative events, and are prime candidates for constructing features for risk prediction [20, 26]. The diagnosis events, often in the form of International Classification of Diseases 9 (ICD9) codes, also come with well-defined feature groups at various levels of granularity such as diagnosis group (DxGroup) and higher-level hierar-chical condition categories (HCC). For example, the code 401.1 Benign Hypertension belongs to DxGroup 401 Essen-tial Hypertension , which is a subcategory in HCC 091 Hy-pertension .

One of the key steps of risk prediction from EMR is to construct features vectors from EMR events, which are used as inputs for classifiers. The goal of feature construction is to capture sufficient clinical nuances that are informative to a specific risk prediction task. Traditionally the feature vec-tors are directly derived from the raw EMR records [30, 27, 25]. In this paper for each patient we first construct a longi-tudinal patient matrix , with a feature dimension and a time dimension [27]. Maintaining the time dimension enables us to leverage the temporal information of the patients dur-ing feature construction. We present the procedure of con-structing feature vectors via longitudinal patient matrices as follows.

In a cohort for a disease study, each patient is also as-sociated with a disease status date called operation criteria date (OCD), on which the disease is diagnosed. A typical risk prediction task is to predict the disease status of the pa-tients at a certain time point in the future (e.g., half a year). We call this period as the prediction window . To build use-ful predictive models, a prediction window before the OCD is usually specified, and the records before the prediction window are used to train the models, i.e., all records within the prediction window before the OCD are considered to be Figure 3: Construction of the longitudinal pa-t ient matrix [27] from Electronic Medical Records (EMR). The goal is to predict disease status of a patient at the operation criteria date (OCD), given the past medical information before the prediction window. For each patient, we construct a longitudi-nal patient matrix, using medical features at a spe-cific granularity. For each patient, the feature vector for classification/regression is finally generated by extracting summary statistics from the longitudinal matrix within the observation window. invisible. Figure 3 illustrates the raw EMR data, OCD, and prediction window.

The next step is to construct a longitudinal patient ma-trix for each patient from the available EMR events, which consists of two dimensions: the feature dimension and the time dimension. One straightforward way to construct such matrices is to use the finest granularity in both dimensions: use the types of medical events as the feature space for the feature dimension and use day as the basic unit for time dimension. Unfortunately the patient matrices constructed in this way are too sparse to be useful. As a remedy, we use weekly aggregated time, and the value of each medical feature at one time point is given by the counts of the corre-sponding medical events within that week. Recall that the medical features can be retrieved at different levels of gran-ularity, which also moderately reduces some sparsity in the data. The choice of feature granularity should not be too coarse, otherwise predictive information within features at a finer level may be lost during the retrieval, as we will show in the experiments. Note that after these preprocessing steps, the constructed patient matrices are still very sparse.
Finally we need to extract summary statistics from the longitudinal patient matrices as the feature vectors for clas-sifiers. Since patients have different lengths of records, typ-ically an observation window of interest is defined and the summary statistics (e.g., mean, standard deviation) are ex-tracted within the observation window for all patients. The overall process is given in Figure 3.
During the aforementioned feature construction process, there are many zeros in the longitudinal patient matrices due to the extreme sparsity in the raw EMR data. How-ever, many of these zeros are not real zeros and instead, they indicate missing information (i.e, no visit). Treated as infor-mative values in the feature extraction process, these values are likely to bias the training of classifiers and yield subop-timal performance. In this paper we propose to treat the zeros in the longitudinal patient matrices as missing values, Figure 4: Illustration of the Pacifier framework. W e treat a longitudinal patient matrix as a par-tially observed matrix from a complete patient ma-trix . We assume the medical features can be mapped to some latent medical concepts with a much lower dimensionality such that each medical concept can be viewed as a combination of several observed med-ical features. For each patient, the values of those medical concepts evolve smoothly over time. Thus the complete patient matrix for each patient can be factorized into a latent medical concept mapping matrix and a concept value evolution matrix. and we densify the sparse matrices before extracting fea-tures to reduce the bias introduced by the sparsity, in hopes of that, the densified matrices provide better phenotyping of patients. We propose novel frameworks of densifying the partially observed longitudinal patient matrices, leveraging their observed medical histories. The proposed framework explores the latent structures on both feature and time di-mensions and encourages the temporal smoothness of each patient.

Let there be n patients with EMR records available in the cohort, and there be in total p medical features. After the feature construction process we obtain n longitudinal patient matrices with missing entries, one for each patient. For the i th patient, its time dimension is denoted by t i i.e., there are medical event records covering a time span of t i before the prediction window. We denote the ground truth matrix of the i th patient as X ( i )  X  R p  X  t i , and in our medical records we only have a partial observation of the matrix at some locations, whose indices are given by a set  X  ( i ) . According to the macro phenotype assumption, we assume the medical features can be mapped to some latent medical concepts space with a much lower latent dimension of size k , such that each medical concept can be viewed as a combination of several observed medical features.
Specifically, we assume that the full longitudinal patient matrix can be approximated by a low rank matrix X ( i )  X  U ( i ) V ( i ) , which can be factorized into a sparse matrix U R p  X  k whose columns provide mappings from medical fea-tures to medical concepts, and a dense matrix V ( i )  X  R whose rows indicates the temporal evolution of these med-ical concepts acting on the patient over time. We call U ( i ) the latent medical concept mapping matrix (abbr. latent mapping matrix ) and V ( i ) the concept value evolution ma-trix (abbr. evolution matrix ). For each patient we assume that the values of those medical concepts evolve smoothly over time. Given the values and locations of observed el-ements in the longitudinal patient matrices, our proposed densification method learns their latent mapping matrices and evolution matrices. We call this densification framework Pacifier , which stands for PAtient reCord densIFIER. The idea of Pacifier is illustrated in Figure 4.

Based on different natures of the medical cohorts, homo-geneous or heterogeneous, we propose two densification for-mulations: an individual basis approach for heterogeneous patients and a shared basis approach for homogeneous pa-tients, and then we provide an efficient optimization algo-rithm for Pacifier that can be used to solve large-scale problems. Here and later we abuse the word basis to denote the columns of a concept mapping matrix, while we don X  X  require them to be orthonormal. Note that the real basis of the space spanned by the columns of the latent mapping ma-trix can always be obtained by performing QR factorization on this basis matrix U i .
In the heterogeneous cohort where patients are very dif-ferent from each other in nature, the medical concepts for each patient may also be different from one patient to an-other. In the individual basis approach ( Pacifier -Iba ), we allow patients to have different latent medical concepts.
Let  X  c ( i ) denote the complement of  X  ( i ) . We adopt the pro-jection operator P  X  ( i ) ( X ( i ) ) used in matrix completion [2]: P otherwise. An intuitive approach for formulating Pacifier -Iba is to solve the following problem for each patient: where R ( U ( i ) , V ( i ) ) denotes the regularization terms that en-code our assumptions and prevent overfitting. We also im-pose a non-negative constraint on the medical concept U ( i ) because most medical events and measurements in EMR are non-negative, and meaningful medical concepts consist of these medical events should also be non-negative. We now discuss how to design proper terms in R ( U ( i ) , V ( i ) to some desired properties: 1) Sparsity. We want only a few significant medical features to be involved in each medical concept so that the concepts can be interpretable. Therefore, we introduce sparsity in the latent mapping matrix U ( i ) via sparse inducing  X  1 -norm on U ( i ) . Indeed the non-negativity constraint may have already brought a certain amount of sparsity, and it has been shown that for non-negative matrix factorization, the sparsity reg-ularization can further improve the decomposition [10]. 2) Overfitting. To overcome overfitting we introduce an  X  regularization on the concept value evolution matrix V ( i ) can be shown that this term also improves the numerical condition of computing a matrix inversion in our algorithm. 3) Temporal smoothness. A patient matrix describes the continuous evolution of medical features for a patient over time. Thus, along the time dimension it makes intuitive sense to impose the temporal smoothness, such that the value of one column of a longitudinal patient matrix is close to those of its previous and next columns. To this end, we introduce the temporal smoothness regularization on the columns of the concept value evolution, which describes the smooth evolution on the medical concepts. One commonly used strategy to enforce temporal smoothness is via penal-izing pairwise difference [37, 34]: where R ( i )  X  R t i  X  t i +1 is the temporal smoothness coupling matrix defined as follows: R ( i ) ( j, k ) = 1 if i = j , R  X  1 if i = j + 1, and R ( i ) ( j, k ) = 0 otherwise.
In the loss function of Eq. (1) we want the values of the low-rank matrix to be close to X ( i ) at the observed locations, directly solving which may lead to complex algorithms. An alternative way is to introduce an intermediate matrix S i such that P  X  i ( S i ) = P  X  i ( X i ), and we want U ( i ) close to S ( i ) . An immediate advantage of propagating the observed information from X ( i ) to U ( i ) V ( i ) indirectly is that we can derive very efficient algorithms and data structures, which give the capability of solving large-scale problems, as we will show later. To this end, we propose the following Pacifier -Iba learning model for each patient:
In homogeneous cohorts where the medical concepts of patients are very similar to each other, we can assume that all patients share the same medical concept mapping U  X  R p  X  k . We propose the following Pacifier -Sba formulation: Since the densification of all patients are now coupled via the shared concept mapping, an immediate benefit of the Paci-fier -Sba formulation is that, we can transfer some knowl-edge among the patients, which is attractive especially when the available information for each patient is very limited and the patients are homogeneous in nature. We demonstrate in the experiments that the Pacifier -Sba performs better than Iba when patients are homogeneous.

On a separate note, considering densification of each pa-tient as a learning task, the Sba approach performs induc-tive transfer learning among the tasks via a shared repre-sentation of U and thus belongs to the multi-task learning paradigm [7, 8]. As such, the Sba in nature is a multi-task matrix completion problem.
The formulations of Pacifier are non-convex and we present a block coordinate descent (BCD) optimization algorithm to obtain a local solution. Note that for each patient the sub-problem of Pacifier -Iba in Eq. (2) is a special case of the problem of Pacifier -Sba in Eq. (3) given n = 1. Therefore in this section we present the algorithm for Eq. (3). This is a standard non-negative  X  1 -norm regularized problem and can be solved efficiently using scalable first order meth-ods such as spectral projected gradient [29] and proximal Quasi-Newton method [14]. N ote that the terms are decoupled for each patient, resulting in a set of minimization problems: T he problem in (6) can be solved using existing optimization solvers. Moreover, since the problem is smooth, it admits a simple analytical solution [37].

Lemma 1. Let Q 1  X  1 Q T 1 = U T U +  X  2 I and Q 2  X  2 Q T  X 
R ( i ) R T ( i ) be eigen-decompositions, and let D = Q T the problem (6) admits an analytical solution:
Note that the parameter  X  2 improves the stability of the  X  X nversion X  in V j,k so that the denominator is guaranteed to be a positive number. Excluding the time of the two QR factorizations, the cost of computing the analytical form so-lution for each sample is given by O ( k 2 pt ). The computation can be greatly accelerated as shown in the next section. In-cluding the time of QR factorizations, obtaining the results from the analytical form is typically 100 times faster than that of solving (5) using optimization solvers. The problem is a constrained Euclidean projection, and is decoupled for each S + ( i ) . The subproblem for each one admits a closed-form solution: S + ( i ) = P  X  c Algorithm 1 T he BCD algorithm for solving the Pacifier -We summarize the BCD algorithm of P acifier -Sba in Algorithm 1. In our implementation, we randomly gener-ate the initial concept evolution matrix V 0 ( i ) , and set U (0). Therefore the initial value of S  X  ( i ) is given by S P of Pacifier is non-convex, and thus it is easy to fall into a local minimum. One way to escape from local minimum is to  X  X estart X  the algorithm by slightly perturbing V i after the algorithm converges, and compute a new solution. Among the many solutions, we use the one with the lowest function value. In the following section we discuss how to accelerate the algorithm to solve large-scale problems. For large scale problems, the storage of the matrix S i and O ( d 2 )-level computations are prohibitive. However, we no-tice that in each iteration, we have that S + ( i ) = P  X  c P + sparse X  structure of S + ( i ) indicates that there is no need to store the full matrices. Instead we only need to store two smaller matrices depending on k and a sparse residual matrix P  X  i ( X ( i )  X  U + V + ( i ) ). This structure can be used to greatly accelerate the computation of Eqs. (4) and (5). In the following discussion we denote S ( i ) = U S ( i ) V S 1) Solve U. The major computational cost of Eq. (4) lies on the evaluation of the loss function and the gradient of the smooth part. Taking advantage of the structure of S i We show that all prohibitive O ( d 2 ) level operations can be avoided given the special structures of S + ( i ) . Gradient Evaluation: = X n O bjective Evaluation: = X n = X n For the evaluation of the loss function, it can be shown that the complexity is O ( k 2 npt ) if all patients have t time slices. Similarly the complexity of computing the gradient is also given by O ( k 2 npt ). Therefore in the optimization, the com-putational cost in each iteration is linear with respect to n , p and t . Thus the algorithm is scalable to large data. 2) Solve V. The term U T S ( i ) can again be computed ef-ficiently using the similar strategy as above. Recall that in solving V + ( i ) we need to perform eigen-decomposition on two matrices: a R k  X  k matrix U T U and a R t  X  t tridiagonal ma-trix R ( i ) R T ( i ) . The two matrices are equipped with special structures: the matrix U T U is a low-rank matrix, and the matrix R ( i ) R T ( i ) is a tridiagonal matrix (a very sparse ma-trix), whose eigen-decomposition can be solved efficiently.
Note that the complexity of time dimension is less critical, b ecause that in most EMR cohorts, the time dimension of the patients are often less than 1000. Recall that the finest time unit of the EMR records is day. Using weekly granular-ity, 1000 time dimension covers up to 20 years of records. In our implementation we use the built-in eigen-decomposition of Matlab, which typically takes less than 1 sec for a matrix with a time dimension of 1000 on regular desktop computers.
In the formulations in Eq. (2) and Eq. (3), we need to es-timate the latent dimension of the patient matrices. Indeed, we can choose the latent dimension via validation methods, as done for other regularization parameters. As an alterna-tive, we can use the rank estimation heuristic to adaptively set the latent dimension of the matrices by inspecting the information in the QR decomposition of the latent concept mapping matrix U , assuming that the latent dimension in-formation of all patients is collectively accumulated in U after a few iterations of updates. The idea was originally proposed in [28, 23] to estimate the rank during the matrix completion of a single matrix.

In order to be self-contained we briefly summarize the algorithm as follows. After a specified iterations of updates, we perform the economic QR factorization on U E = Q U R U where E is a permutation matrix such that | diag( R U ) | := [ r 1 . . . r k ] is non-increasing after the permutation. Denote Q p = r p /r p +1 , and Q max = max( Q p ), and the location is given by p max . We compute the following ratio: A large  X  indicates a large drop in the magnitude of Q i after p max elements, and we thus reduce the latent factor k to p max , retaining only the first p max columns of U and the corresponding rows of the evolution matrices { V ( i ) our implementation we only perform the estimation once. Empirically as shown in Section 4.2, the latent dimension estimation works well when the Pacifier -Sba works, i.e., patients are homogeneous, sharing a few latent concepts.
In the Iba approach the completion of patients are inde-pendent. If we apply latent dimension estimation on each patient, then each patient matrix may have a latent dimen-sion different from others. This imposes difficulties when it comes to analyze the patients, and thus the estimation is not used in Iba .
In this section we present the experimental results to demon-strate the performance of the proposed Pacifier methods Iba and Sba . We then study the scalability of the proposed algorithm with varying feature dimensions, time dimensions, sample sizes, latent dimensions, and ratios of the observed entries. We then apply the proposed Pacifier framework on two real clinical cohorts to demonstrate the improvement on predictive performance achieved by our approaches. The code for the proposed algorithm is available in [33].
In this section we study the scalability of the proposed algorithm using synthetic datasets. In each of the following studies, we generate random datasets with a specified sam-ple n , feature dimension p , average time dimension t , latent dimension k , and observation density k  X  i k . For simplicity we let all samples have the same time dimension. We re-port the average time cost over 50 iterations. For the two algorithms we set all parameters to be 1 e  X  8 in all studies. Sample Size. We fix p = 100, t = 100, r = 10, k  X  i k = 0 . 01, and vary the sample size n = 200 : 200 : 1800. The results are given in Figure 5(a). We observe that for both methods the time costs increase linearly with respect to the sample size. The cost of Iba grows faster than the Sba version, which is expected because in Iba the computation costs of the loss and the gradients are more than those of Sba . Feature Dimension. We fix n = 100, t = 100, r = 10, use k  X  i k = 0 . 01, and vary the feature dimension p = 200 : 200 : 1800. The results are given in Figure 5(b). We see that the time costs for both methods increase linearly with respect to feature dimension, which is consistent with our complexity analysis. The linear complexity of feature dimension is de-sired in clinical applications, since one might want to use as much information available as possible, resulting in a large feature space.
 Time Dimension. We fix n = 100, p = 100, r = 10, k  X  i k = 0 . 01, and vary the time dimension t = 100 : 100 : 900. The results are given in Figure 5(c). We find superlin-ear complexity on the time dimension for both methods, which mainly comes from the eigen decomposition. The complexity on time dimension is less critical in the sense that for most medical records and longitudinal study, the time dimension is very limited. For example, if the time granularity is weekly, then we have 52 time dimensions each year. If 20-year records are available for one patient, then it yields only 1040 time dimensions. Besides, the eigen de-composition can be implemented in the way that utilizes the extreme sparsity of the temporal smoothness coupling matrix.
 Latent Dimension. We fix n = 100, p = 500, t = 500, k  X  i k = 0 . 01, and vary the latent dimension input of the algorithms r = 20 : 20 : 160. The results are given in Fig-ure 5(d). We find that the time costs increase superlinearly with respect to latent dimension for both methods, and the complexity of Sba is close to be linear.
 Observed Entries. We fix n = 100, p = 1000, t = 500, r = 10, and vary the percentage of the observed entries k  X  i k = 0 . 05 : 0 . 05 : 0 . 45. The results are given in Fig-ure 5(d). We see that the time costs increase only sub-linearly with respect to the set of observed entries.
We note that the complexity of Pacifier -Iba is of the same order as that of Sba . The difference between the two methods comes from the computation of the objective value and gradient in the U step. It is obvious that the IBA methods can be parallelized because the computation of all samples are decoupled. Similarly, the major computa-tional complexity of SBA comes from the computation of U in the optimization and eigen-decomposition of V ( i ) , which can also be parallelized by segmenting the computation of each patient.
To gauge the performance of the proposed Pacifier frame-work we apply the two formulations on two real EMR co-horts from one of our clinical partners. In one cohort we dimension; sublinear with respect to the number of observed entries. study the predictive modeling of congestive heart failure (CHF), and in the other cohort we study end stage renal disease (ESRD). In both EMR cohorts we are given a set of patients associated with their outpatient diagnosis events in ICD9 codes and the corresponding timestamps. In our ex-periments we use the prediction windows lengths suggested by physicians (180 days for CHF and 90 days for ESRD), and we remove all events within the prediction window be-fore the operation criteria date.

To construct the longitudinal patient matrices to be im-puted, we use EMR data at the weekly granularity as dis-cussed in Section 2. We select the patients with more than 100 events. Note that we are working on a large feature di-mension, and thus for a patient with 100 EMR events the longitudinal patient matrix is still extremely sparse. Note that in our cohorts the number of case patients is much smaller than control patients, which is very common in most clinical studies. To avoid the effects of biased samples, we perform random under-sampling on the control patients so that we have the equal number of case and control patients in our datasets. To this end, we have constructed two datasets: 1) CHF dataset with 249 patients in each class; 2) ESRD dataset with 187 patients in each class.
 The raw feature space in the low-level ICD9 codes is 14313. Because the matrix constructed using the low-level ICD9 codes is too sparse, we retrieve the medical features at coarser granularities. In order to study the effects of features at different granularities, we compare the medical features at ICD9 diagnosis group level (DxGroup) and HCC level. At DxGroup level there are 1368 features and at HCC level there are 252 features. In the two studies we consider the following commonly-used baselines methods:  X  Zero Imputation (RAW). An intuitive way to impute miss-ing values, which is equivalent to mean value imputation when the data set is first normalized (zero mean and unit standard deviation). This method is standard in the cur-rent medical literature for clinical studies [25, 27, 30].  X  Row Average (AVG). In this baseline approach we fill the missing value using the average value of the observed val-ues of the feature over time.  X  Interpolation (INT) [5]. We use the next observation and previous observation along the timeline to interpolate the missing elements.  X  Next Observation Carry Backward (NOCB) [5]. Missing values are filled using the next observation of this medical feature along the timeline.  X  Last Observation Carry Forward (LOCF) [5]. Missing val-ues are filled using the previous observation of this medical feature along the timeline.
 We compare the baseline methods with the following com-peting methods:  X  Individual Basis Pacifier ( Iba ). Each patient is densified using Algorithm 1.  X  Iba without temporal smoothness ( Iba -NT). This variant of Pacifier -IBA sets the temporal regularization  X  3 to 0.  X  Shared Basis Pacifier ( Sba ) using Algorithm 1.  X  Sba without temporal smoothness ( Sba NT). This variant of Pacifier -Sba sets the temporal regularization  X  3 to 0.  X  Sba with Latent Dimension Estimation ( Sba -E). The la-tent dimension estimation is described in Section 3.5, and only used once during the algorithm.  X  Sba without Temporal Smoothness and with Latent Di-mension Estimation ( Sba NT-E). This variant of Paci-fier -Sba sets the temporal regularization  X  3 to 0 and uses latent dimension estimation once.
 Note that for the extremely sparse matrix as the clinical data in our studies, classical imputation methods such as those based on k-nearest neighbor [9] and expectation maxi-mization [22] do not work. The methods Iba NT and Sba NT are included in the study to explore the effectiveness of the proposed temporal smoothness. For the parameter estima-tion we have separated an independent set of samples for validation, and we select the parameters that give the low-est recovery error on the validation set. In Iba , Sba , and Sba NT, the latent dimension k is also determined via the validation set.

We finally test the predictive performance on the com-pleted datasets using sparse logistic regression classifier (we use the SLEP implementation [15]). From the completed datasets, we derive features by averaging the features along the time dimension within a given observation window (52 weeks). To this end, each patient is represented as a vector of the same dimension as the feature dimension. We then ran-domly split the samples into 90% training and 10% testing, and train the classifier on the training data. The classifier parameter is tuned using standard 10 fold cross validation. We repeat the random splitting for 20 iterations, and report the average performance over all iterations. In order to be comparable, the splitting is the same for all methods in each iteration.
 CHF Cohort. The predictive performance of competing methods is presented in Table 1. We find that in the CHF co-hort: 1) most of the proposed Pacifier approaches and their variants significantly improve the predictive performance as Table 1: Predictive performance on the CHF cohort u sing DxGroup and HCC features.
 compared to the baseline RAW approach. The best AUC o btained by Pacifier -IBA dataset is 0 . 816 while the base-line is only 0 . 689 (a gain of 0 . 127); 2) the individual basis approaches outperform shared based ones; 3) temporal regu-larization significantly improves the predictive performance for all methods; 4) the methods with latent dimension es-timation perform worse than those that do not use latent dimension estimation on this cohorts; 5) the features at Dx-Group level outperform HCC level, which might be due to that in this predictive task, a fine granularity is likely to maintain more predictive information, than a coarse one. ESRD Cohort. The predictive performance on ESRD cohort is given in Table 2. For the DxGroup features we observe similar patterns that is, Iba outperforms all other methods, which achieves an AUC of 0 . 828, compared to the baseline RAW method that achieves 0 . 756 (a gain of 0 . 072). The variants with temporal smoothness perform much better than the ones without temporal smoothness. For the HCC features we see that: 1) the shared basis ap-proaches perform as well as the independent basis, where Sba -E achieves an AUC of 0.827. 2) again the temporal smoothness significantly improves the performance. 3) la-tent dimension estimation works well and outperforms the ones without latent dimension estimation.

As a summary, the experimental results have demonstrated the effectiveness of the proposed methods on real clinical data, and the temporal smoothness regularization brings sig-nificant improvements on predictive performance. In real clinical data, the samples tend to be heterogeneous and therefore the independent basis approaches perform better. However, using the HCC features of the two datasets, shared basis approaches perform better than using the DxGroup features. One potential explanation is that, using HCC fea-tures where the features space is smaller and features them-selves are coarser (in terms of clinical concepts), the patients tend to be more homogeneous. We also notice that the la-tent dimension estimation only works well when shared basis works well. Recall that the idea of latent dimension estima-tion is to detect the jumps in the diagonal elements from the R U factor of QR factorization. This is expected because Table 2: Predictive performance on the ESRD co-hort with DxGroup and HCC features.
 if the patients are homogeneous and share only a few basis, t hen obviously there are such natural jumps.
In this section we show some meaningful medical con-cepts learned by the proposed Pacifier -Sba method. In the latent medical concept mapping matrix U , we are able to obtain feature groups from data, because of the sparsity on the matrix. We first normalize weights of the columns such that the sum of each column is equal to 1. The nor-malized weights indicate the percentages of medical features contributing to the medical concept. We rank the medical features according to their contributions and find that in most of the medical concepts the top medical features are typically related and are comorbidities of a certain disease. In Figure 3, we show a list of medical concepts obtained from our CHF cohort. For example, in the first medical concept, the highly ranked diagnosis groups are all related to Car-diovascular Disease , e.g., Heart failure (428), Hypertension (401) and Dysrhythmias (427), and the second medical con-cepts include features that are typical related to Diabetes and its related comorbidities such as Hypertension (401), Chronic renal failure (585). In the CHF cohort, we have also found very similar medical concepts.
In this paper we treat the zeros in the longitudinal patient matrices as missing values, and proposed a novel framework Pacifier to perform temporal matrix completion via low-rank factorization. To the best of our knowledge, there are no prior work that applies matrix completion techniques to solve the data sparsity in EMR data. The proposed Paci-fier framework aims at densifying the extremely sparse EMR data by performing factorization based matrix com-pletion. The differences between the proposed completion method and existing works are that: instead of treating each patient as vectors and forming a single matrix, we treat each patient as a matrix with missing entries and consider a set of related matrix completion problems. We further propose to Table 3: Medical concepts discovered by the Paci-fi er-Sba in our CHF cohort. In each medical con-cept, we firstly normalize the weights of the medical features in the medical concepts learned and rank the features. For each medical concept we list top 10 medical features and their diagnosis group codes. incorporate the temporal smoothness to utilize the hidden t emporal information of each patient.

The problem of imputation via matrix completion prob-lem is one of the hottest topics in data mining and ma-chine learning. In many areas such as information retrieval and social network, the data matrix is so sparse that clas-sical imputation methods does not work well. The basic problem setting of the matrix completion is to recover the unknown data from only a few observed entries, imposing certain types of assumptions on the matrix to be recovered. The most popular assumption is to assume that the matrix has a low rank structure [2, 18, 28, 31]. There are two types of matrix completion in terms of the assumption on the ob-served entries: The first type assumes that the observation has no noise, and the goal is to find a low rank matrix whose values at the observed locations are exactly the same as the given ones [2, 3, 11]. In real world applications, however, noise is ubiquitous and thus the rigid constraint on the ob-served locations may result in overfitting. In contrast, the noisy matrix completion methods only require the values at the observed locations to be close to the given data [18, 28]. Directly dealing with the rank function in objectives are shown to be NP-Hard. Therefore many approaches seek to use the trace norm which is the convex envelope of the rank function [2, 11, 18]. Most of these approaches, how-ever, require singular value decomposition (SVD) on large matrices, the complexity of which is prohibitive for large scale problems. Recent years have witnessed surging inter-ests on the local search methods, which seek a local solution with extremely efficient algorithms [21, 28]. The Pacifier framework is among these efficient local approaches, which does not require SVD and can be applied to solving large scale problems.

The completed data for each patient has the factorization form of X ( i ) = U ( i ) V ( i ) , and for Sba all patients have the same U ( i ) . Clearly, one advantage of Sba is that we have si-multaneously learned a shared low-dimensional feature space for all patients, and their coordinates that can be used as new (and reduced) features. To see this, let U = Q U R U be the QR factorization of U , then for each patient we have that X can be considered as coordinates on the low dimensional space whose bases are given by columns of Q U . One issue brought by the shared mapping is that the latent dimen-sion is limited by the lowest time dimension of the patient, i.e., min i t i &gt; k . One solution is that we can extend the time dimension of the patients with non-informative time dimensions of all zeros.

We have shown in the experiments that a shared concept mapping works better on homogeneous samples while indi-vidual mappings work better on heterogeneous samples. In reality the samples may form some groups such that within the groups the patients are homogeneous and patients from different groups may be heterogeneous. The degree of homo-geneous/heterogeneous is also affected by feature granularity as shown in our real clinical experiments, where in finer fea-ture level the patients appear to be more heterogeneous. It is thus interesting to explore how to simultaneously identify feature groups and patient groups to further improve the quality of matrix completion. To do so, we can incorporate group learning into the objective as done in [32]: where G is the patient group assignment matrix, and pa-tients within each group G j share the same basis U j . We leave this interesting study to our future works. One final note  X  the proposed Pacifier framework proposed in this paper is not limited to healthcare domain, they can also be applied to temporal collaborative filtering [12, 16, 31], where each user has a rating preference that changes overtime.
In this paper, we propose a data driven phenotyping frame-work called Pacifier (PAtient reCord densIFIER) to den-sify the sparse EMR data. The P acifier interprets the lon-gitudinal EMR of each patient as a sparse matrix with a feature dimension and a time dimension, and estimates the missing entries in those matrices by leveraging the latent structures on both time and feature dimensions. We propose two formulations: Individual Basis Approach ( Iba ), which densifies the matrices patient by patient, and Shared Ba-sis Approach ( Sba ), which densifies the matrices of a group of patients jointly. We develop an efficient optimization al-gorithm to solve the framework, which scales to large-size datasets. We have performed extensive empirical evalua-tions on both synthetic and real datasets, including two real world clinical datasets. Our results show that the predictive performance in both tasks can be improved significantly af-ter the densification by the proposed methods.

