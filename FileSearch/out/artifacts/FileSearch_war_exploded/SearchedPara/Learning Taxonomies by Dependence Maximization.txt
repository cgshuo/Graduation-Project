 We address the problem of finding taxonomies in data: that is, to cluster the data, and to specify in a systematic way how the clusters relate. This problem is widely encountered in biology, when group-ing different species; and in computer science, when summarizing and searching over documents and images. One of the simpler methods that has been used extensively is agglomerative clustering [18]. One specifies a distance metric and a linkage function that encodes the cost of merging two clusters, and the algorithm greedily agglomerates clusters, forming a hierarchy until at last the final two clusters are merged into the tree root. A related alternate approach is divisive clustering, in which clusters are split at each level, beginning with a partition of all the data, e.g. [19]. Unfortu-nately, this is also a greedy technique and we generally have no approximation guarantees. More recently, hierarchical topic models [7, 23] have been proposed to model the hierarchical cluster struc-ture of data. These models often rely on the data being representable by multinomial distributions over bags of words, making them suitable for many problems, but their application to arbitrarily structured data is in no way straightforward. Inference in these models often relies on sampling techniques that can affect their practical computational efficiency.
 On the other hand, many kinds of data can be easily compared using a kernel function, which encodes the measure of similarity between objects based on their features. Spectral clustering al-gorithms represent one important subset of clustering techniques based on kernels [24, 21]: the spectrum of an appropriately normalized similarity matrix is used as a relaxed solution to a partition problem. Spectral techniques have the advantage of capturing global cluster structure of the data, but generally do not give a global solution to the problem of discovering taxonomic structure. In the present work, we propose a novel unsupervised clustering algorithm, numerical taxonomy clustering , which both clusters the data and learns a taxonomy relating the clusters. Our method works by maximizing a kernel measure of dependence between the observed data, and a product of the partition matrix that defines the clusters with a structure matrix that defines the relationship between individual clusters. This leads to a constrained maximization problem that is in general NP hard, but that can be approximated very efficiently using results in spectral clustering and numerical taxonomy (the latter field addresses the problem fitting taxonomies to pairwise distance data [1, 2, 4, 8, 11, 15, 25], and contains techniques that allow us to efficiently fit a tree structure to our data with tight approximation guarantees). Aside from its simplicity and computational efficiency, our method has two important advantages over previous clustering approaches. First, it represents a more informative visualization of the data than simple clustering, since the relationship between the clusters is also represented. Second, we find the clustering performance is improved over methods structure rather than learning it.
 Several objectives that have been used for clustering are related to the objective employed here. Bach and Jordan [3] proposed a modified spectral clustering objective that they then maximize either with respect to the kernel parameters or the data partition. Christianini et al. [10] proposed a normalized inner product between a kernel matrix and a matrix constructed from the labels, which can be used to learn kernel parameters. The objective we use here is also a normalized inner product between a similarity matrix and a matrix constructed from the partition, but importantly, we include a structure matrix that represents the relationship between clusters. Our work is most closely related to that of Song et al. [22], who used an objective that includes a fixed structure matrix and an objective based on the Hilbert-Schmidt Independence Criterion. Their objective is not normalized, however, and they do not maximize with respect to the structure matrix.
 The paper is organized as follows. In Section 2, we introduce a family of dependence measures with which one can interpret the objective function of the clustering approach. The dependence algorithms is explained in Section 3.1. Important results for the optimization of the objective are presented in Sections 3.2 and 3.3. The problem of numerical taxonomy and its relation to the pro-posed objective function is presented in Section 4, as well as the numerical taxonomy clustering algorithm. Experimental results are given in Section 5. In this section, we give a brief introduction to the Hilbert-Schmidt Independence Criterion (HSIC), which is a measure of the strength of dependence between two variables (in our case, following [22], these are the data before and after clustering). We begin with some basic terminology in kernel methods. Let F be a reproducing kernel Hilbert space of functions from X to R , where X is a separable metric space (our input domain). To each point x  X  X , there corresponds an element is a unique positive definite kernel. We also define a second RKHS G with respect to the separable metric space Y , with feature map  X  and kernel  X   X  ( y ) , X  ( y 0 )  X  G = l ( y,y 0 ) . Let ( X,Y ) be random variables on X  X Y with joint distribution Pr X,Y , and associated marginals Pr
X and Pr Y . Then following [5, 12], the covariance operator C xy : G  X  F is defined such that for all f  X  X  and g  X  X  , A measure of dependence is then the Hilbert-Schmidt norm of this operator (the sum of the squared are independent. It is shown in [13] that the Gaussian and Laplace kernels are characteristic on R . Given a sample of size n from Pr X,Y , the Hilbert-Schmidt Independence Criterion (HSIC) is defined by [14] to be a (slightly biased) empirical estimate of k C xy k 2 HS , 1 n is the n  X  1 vector of ones, K is the Gram matrix for samples from Pr X with ( i,j ) th entry k ( x i ,x j ) , and L is the Gram matrix with kernel l ( y i ,y j ) . We now specify how the dependence criteria introduced in the previous section can be used in clustering. We represent our data via an n  X  n Gram matrix M 0 : in the simplest case, this is the centered kernel matrix ( M = H n KH n ), but we also consider a Gram matrix corresponding to normalized cuts clustering (see Section 3.1). Following [22], we define our output Gram matrix to be L =  X  Y  X  T , where  X  is an n  X  k partition matrix, k is the number of clusters, and Y is a positive definite matrix that encodes the relationship between clusters (e.g. a taxonomic structure). Our clustering quality is measured according to In terms of the covariance operators introduced earlier, we are optimizing HSIC, this being an em-for use in kernel target alignment [10], the difference being the addition of centering matrices, H n , as required by definition of the covariance. We remark that the normalizing term H n  X  Y  X  T H n HS was not needed in the structured clustering objective of [22]. This is because Song et al. were in-terested only in solving for the partition matrix,  X  , whereas we also wish to solve for Y : without normalization, the objective can always be improved by scaling Y arbitrarily. In the remainder of this section, we address the maximization of Equation (1) under various simplifying assumptions: these results will then be used in our main algorithm in Section 4. 3.1 Relation to Spectral Clustering Maximizing Equation (1) is quite difficult given that the entries of  X  can only take on values in { 0 , 1 } , and that the row sums have to be equal to 1 . In order to more efficiently solve this difficult combinatorial problem, we make use of a spectral relaxation. Consider the case that  X  is a column vector and Y is the identity matrix. Equation (1) becomes Setting the derivative with respect to  X  to zero and rearranging, we obtain Using the normalization  X  T H n  X  = 1 , we obtain the generalized eigenvalue problem For  X   X  { 0 , 1 } n  X  k where k &gt; 1 , we can recover  X  by extracting the k eigenvectors associated with the largest eigenvalues. As discussed in [24, 21], the relaxed solution will contain an arbitrary rotation which can be recovered using a reclustering step.
 If we choose M = D  X  1 2 AD  X  1 2 where A is a similarity matrix, and D is the diagonal matrix such that D ii = P j A ij , we can recover a centered version of the spectral clustering of [21]. In fact, we wish to ignore the eigenvector with constant entries [24], so the centering matrix H n does not alter the clustering solution. 3.2 Solving for Optimal Y 0 Given  X  We now address the subproblem of solving for the optimal structure matrix, Y , subject only to positive semi-definiteness, for any  X  . We note that the maximization of Equation (1) is equivalent to the constrained optimization problem We write the Lagrangian take the derivative with respect to Y , and set to zero, to obtain which together with the constraint in Equation (5) yields where  X  indicates the Moore-Penrose generalized inverse [17, p. 421].
 Because  X  T H n  X   X   X  T H n = H k  X  T  X   X  1  X  T H n (see [6, 20]), we note that Equation (8) com-putes a normalized set kernel between the elements in each cluster. Up to a constant normalization factor, Y  X  is equivalent to H k  X  Y  X  H k where N i is the number of elements in cluster i , C i is the set of indices of samples assigned to cluster i , and  X  M = H n MH n . This is a standard set kernel as defined in [16]. 3.3 Solving for  X  with the Optimal Y 0 As we have solved for Y  X  in closed form in Equation (8), we can plug this result into Equation (1) to obtain a formulation of the problem of optimizing  X   X  that does not require a simultaneous opti-mization over Y . Under these conditions, Equation (1) is equivalent to By evaluating the first order conditions on Equation (10), we can see that the relaxed solution,  X   X  , to Equation (10) must lie in the principal subspace of H n MH n . 1 Therefore, for the problem of simultaneously optimizing the structure matrix, Y 0 , and the partition matrix, one can use the same spectral relaxation as in Equation (4), and use the resulting partition matrix to solve for the optimal assignment for Y using Equation (8). This indicates that the optimal partition of the data is the same for Y given by Equation (8) and for Y = I . We show in the next section how we can add additional constraints on Y to not only aid in interpretation, but to actually improve the optimal clustering. In this section, we consolidate the results developed in Section 3 and introduce the numerical tax-onomy clustering algorithm. The algorithm allows us to simultaneously cluster data and learn a tree structure that relates the clusters. The tree structure imposes constraints on the solution, which in turn affect the data partition selected by the clustering algorithm. The data are only assumed to be well represented by some taxonomy, but not any particular topology or structure.
 In Section 3 we introduced techniques for solving for Y and  X  that depend only on Y being con-influence clustering solutions by prior knowledge, we wish to explore the problem where additional constraints are imposed on the structure of Y . In particular, we consider the case that Y is con-clusters is consistent with the path length along some fixed tree whose leaves are identified with the clusters. For any positive semi-definite matrix Y , we can compute the distance matrix, D , given by the norm implied by the inner product that computes Y , by assigning D ij = p Y ii + Y jj  X  2 Y ij . It is sufficient, then, to reformulate the optimization problem given in Equation (1) to add the following constraints that characterize distances generated by a tree metric where D is the distance matrix generated from Y . The constraints in Equation (11) are known as the 4-point condition, and were proven in [8] to be necessary and sufficient for D to be a tree metric. Optimization problems incorporating these constraints are combinatorial and generally difficult to solve. The problem of numerical taxonomy , or fitting additive trees, is as follows: given a fixed distance matrix, D , that fulfills metric constraints, find the solution to with respect to some norm (e.g. L 1 , L 2 , or L  X  ), where D T is subject to the 4-point condition. While numerical taxonomy is in general NP hard, a great variety of approximation algorithms with feasible computational complexity have been developed [1, 2, 11, 15]. Given a distance matrix that satisfies the 4-point condition, the associated unrooted tree that generated the matrix can be found in O ( k 2 ) time, where k is equal to the number of clusters [25].
 We propose the following iterative algorithm to incorporate the 4-point condition into the optimiza-tion of Equation (1): Require: M 0 Ensure: ( X  ,Y )  X  ( X   X  ,Y  X  ) that solve Equation (1) with the constraints given in Equation (11) Initialize Y = I
Initialize  X  using the relaxation in Section 3.1 while Convergence has not been reached do end while One can view this optimization as solving the relaxed version of the problem such that Y is only constrained to be positive definite, and then projecting the solution onto the feasible set by requiring Y to be constructed from a tree metric. By iterating the procedure, we can allow  X  to reflect the fact that it should best fit the current estimate of the tree metric. To illustrate the effectiveness of the proposed algorithm, we have performed clustering on two benchmark datasets. The face dataset presented in [22] consists of 185 images of three different people, each with three different facial expressions. The authors posited that this would be best represented by a ternary tree structure, where the first level would decide which subject was repre-sented, and the second level would be based on facial expression. In fact, their clustering algorithm roughly partitioned the data in this way when the appropriate structure matrix was imposed. We will show that our algorithm is able to find a similar structure without supervision, which better represents the empirical structure of the data.
 We have also included results for the NIPS 1-12 dataset, 2 which consists of binarized histograms of the first 12 years of NIPS papers, with a vocabulary size of 13649 and a corpus size of 1740 . A Gaussian kernel was used with the normalization parameter set to the median squared distance between points in input space. 5.1 Performance Evaluation on the Face Dataset Section 4 (where M = H n KH n is assigned as in a HSIC objective). We considered two alternative approaches: a classic spectral clustering algorithm [21], and the dependence maximization approach of Song et al. [22]. Because the approach in [22] is not able to learn the structure of Y from the data, we have optimized the partition matrix for 8 different plausible hierarchical structures (Figure 1). These have been constructed by truncating n -ary trees to the appropriate number of leaf nodes. For the evaluation, we have made use of the fact that the desired partition of the data is known for the face dataset, which allows us to compare the predicted clusters to the ground truth labels. For each partition matrix, we compute the conditional entropy of the true labels, l , given the cluster ids, c , that the clusters are pure [9]. Table 1 shows the learned structure and proper normalization of our algorithm results in a partition of the images that much more closely matches the true identities and expressions of the faces, as evidenced by a much lower conditional entropy score than either the spectral clustering approach of [21] or the dependence maximization approach of [22].
 proportional to the distance in the tree metric (thus, in interpreting the graph, it is important to take into account both the nodes at which particular clusters are connected, and the distance between these nodes; this is by contrast with Figure 1, which only gives the hierarchical cluster structure and does not represent distance). Our results show we have indeed recovered an appropriate tree structure without having to pre-specify the cluster similarity relations. Figure 1: Structures used in the optimization of [22]. The clusters are identified with leaf nodes, and distances between the clusters are given by the minimum path length from one leaf to another. Each edge in the graph has equal cost. Table 1: Conditional entropy scores for spectral clustering [21], the clustering algorithm of [22], and the method presented here (last column). The structures for columns a-h are shown in Figure 1, while the learned structure is shown in Figure 2. The structure for spectral clustering is implicitly equivalent to that in Figure 1(h), as is apparent from the analysis in Section 3.1. Our method exceeds the performance of [21] and [22] for all the structures. 5.2 NIPS Paper Dataset For the NIPS dataset, we partitioned the documents into k = 8 clusters using the numerical tax-onomy clustering algorithm. Results are given in Figure 3. To allow us to verify the clustering performance, we labeled each cluster using twenty informative words, as listed in Table 2. The most representative words were selected for a given cluster according to a heuristic score  X   X   X   X   X  , where  X  is the number of times the word occurs in the cluster,  X  is the number of times the word occurs outside the cluster,  X  is the number of documents in the cluster, and  X  is the number of documents outside the cluster. We observe that not only are the clusters themselves well defined (e.g cluster a contains neuroscience papers, cluster g covers discriminative learning, and cluster h Bayesian learning), but the similarity structure is also reasonable: clusters d and e , which respectively cover training and applications of neural networks, are considered close, but distant from g and h ; these are themselves distant from the neuroscience cluster at a and the hardware papers in b ; reinforcement learning gets a cluster at f distant from the remaining topics. Only cluster c appears to be indistinct, and shows no clear theme. Given its placement, we anticipate that it would merge with the remaining clusters for smaller k . We have introduced a new algorithm, numerical taxonomy clustering, for simultaneously clustering data and discovering a taxonomy that relates the clusters. The algorithm is based on a dependence
Figure 2: Face dataset and the resulting taxon-omy that was discovered by the algorithm maximization approach, with the Hilbert-Schmidt Independence Criterion as our measure of depen-dence. We have shown several interesting theoretical results regarding dependence maximization clustering. First, we established the relationship between dependence maximization and spectral clustering. Second, we showed the optimal positive definite structure matrix takes the form of a set kernel, where sets are defined by cluster membership. This result applied to the original dependence maximization objective indicates that the inclusion of an unconstrained structure matrix does not affect the optimal partition matrix. In order to remedy this, we proposed to include constraints that guarantee Y to be generated from an additive metric. Numerical taxonomy clustering allows us to optimize the constrained problem efficiently.
 In our experiments on grouping facial expressions, numerical taxonomy clustering is more accurate than the existing approaches of spectral clustering and clustering with a fixed predefined structure. We were also able to fit a taxonomy to NIPS papers that resulted in a reasonable and interpretable clustering by subject matter. In both the facial expression and NIPS datasets, similar clusters are close together on the resulting tree.We conclude that numerical taxonomy clustering is a useful tool both for improving the accuracy of clusterings and for the visualization of complex data. Our approach presently relies on the combinatorial optimization introduced in [22] in order to op-timize  X  given a fixed estimate of Y . We believe that this step may be improved by relaxing the problem similar to Section 3.1. Likewise, automatic selection of the number of clusters is an inter-esting area of future work. We cannot expect to use the criterion in Equation (1) to select the number of clusters because increasing the size of  X  and Y can never decrease the objective. However, the elbow heuristic can be applied to the optimal value of Equation (1), which is closely related to the eigengap approach. Another interesting line of work is to consider optimizing a clustering objective derived from the Hilbert-Schmidt Normalized Independence Criterion (HSNIC) [13].
 Acknowledgments This work is funded by the EC projects CLASS, IST 027978, PerAct, EST 504321, and by the Pascal Network, IST 2002-506778. We would also like to thank Christoph Lampert for simplifying the Moore-Penrose generalized inverse.

