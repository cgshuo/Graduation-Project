 In this work, a benchmark to evaluate the retrieval perfor-mance of soundtrack recommendation systems is proposed. Such systems aim at finding songs that are played as back-ground music for a given set of images. The proposed bench-mark is based on preference judgments, where relevance is considered a continuous ordinal variable and judgments are collected for pairs of songs with respect to a query (i.e., set of images). To capture a wide variety of songs and im-ages, we use a large space of possible music genres, different emotions expressed through music, and various query-image themes. The benchmark consists of two types of relevance assessments: (i) judgments obtained from a user study, that serve as a  X  X old standard X  for (ii) relevance judgments gath-ered through Amazon X  X  Mechanical Turk. We report on the performance of two state-of-the-art soundtrack recommen-dation systems using the proposed benchmark.
 H.5.1 [ Multimedia Information Systems ]: Evaluation; H.3.4 [ Systems and Software ]: Performance evaluation benchmark; soundtrack recommendation; evaluation
With the increase of available multimedia content, search-ing for information contained in images, speech, music, or videos became an integral part in the information retrieval field. Evaluating the quality of such systems introduces new challenges as interpreting abstract associations X  X uch as sim-ilarity between images X  X s complex and can be done in var-ious ways. Similar to the text retrieval evaluations consid-ered in TREC [30], evaluating music, image, and video re-trieval systems has been the main concern of venues such as MIREX [19], TRECVID [31], and ImageCLEF [10].

One of the largest contributions made by these venues is the proposal and standardization of a retrieval corpus, i.e., the standardization of a document and a query collection. In addition to this, the defined benchmarks contain human relevance judgments that assess the quality of query results with respect to the specified information need.

In this work, we consider the problem of creating a bench-mark dataset that is used to assess the quality of soundtrack recommendation systems. These systems recommend songs to act as a soundtracks for a given set of images. We cre-ate the benchmark using the notion of preference judgments, where human judges indicate the preferred song out of a pair of songs with respect to a query. The judgments are gathered using Amazon X  X  Mechanical Turk services with the  X  X old standard X  judgments formed in a user study beforehand. More detailed statistics about the proposed benchmark is presented in [27], while the benchmark itself is publicly avail-able at https://sites.google.com/site/srbench/ .

As of now, the soundtrack recommendation state-of-the-art consists of only two approaches, namely an approach by Li and Shan [17] and our own Picasso approach [26]. We investigate and report on the performance of these two ap-proaches using the proposed benchmark. However, the ben-efit of a re-usable benchmark is far greater as it can improve the efficiency of continuous re-evaluations and comparisons of the existing approaches in the future.
A soundtrack recommendation system, over a set of in-dexed songs S = { s 1 ,s 2 , ... } , takes as input/query a set of images q = { img 1 ,img 2 , ... } and the size of the result set K . It returns a subset of the indexed songs S r  X  S ,with |
S r | = K , ordered with respect to their relevance to act as background music for a slideshow that features the given query images.

We address the problem of creating a benchmark to eval-uate the retrieval performance of a given soundtrack recom-mendation system. The proposed benchmark B =( Q, S, R ) contains a set of queries Q = { q 1 ,q 2 , ... } , a set of songs S = { s 1 ,s 2 , ... } , and a set of human relevance judgments R = { r 1 ,r 2 , ... } , with each query q i defined as a set of im-ages q i = { img 1 ,img 2 , ... } . The proposed benchmark fulfills the following important requirements:
A suitable evaluation dataset has to provide a wide cov-erage of both documents and queries. A common approach in traditional text retrieval is to use a large number of doc-uments (e.g., obtained by crawling parts of the Web) and to perform an initial filtering of documents based on existing approaches, commonly known as pooling . Due to the small number of existing soundtrack recommendation approaches, pooling would result in a highly biased dataset. Hence, we have to assemble the set of queries (images) and documents (songs) independently from existing approaches.
While building the song collection, we focus on popular music and try to achieve high coverage through understand-ing common music aspects. There are two major aspects that people refer to when talking about music: the feelings induced by the music and the genre it belongs to. We use Wikipedia [9] to obtain a hierarchy of modern popular music genres and focus on the genres that appear in the top level of the hierarchy.

Additionally, in order to avoid the complexity of working with a large number of nation-specific music styles, we elim-inate genres specific to the origin of music, such as  X  X rasil-ian music X  and  X  X aribbean music X . The resulting genres are showninTable1.
 Music Genres Blues Classical Country Easy listening Electronic Hip Hop and Rap Jazz Metal Folk Pop Rock Ska
Next, we collect a set of feelings and organize them in two high-level groups: positive and negative feelings. We obtained an exhaustive list of fine-grained feelings from Psychpage [8]. As the obtained list contains generic feelings, some are rarely conveyed by music, such as admiration and satisfaction. To identify feelings expressed through music we used the data from the last.fm [16] music portal. For each general feeling, we check how frequently an artist or a song is annotated with the tag (term) that describes a feeling, for instance,  X  X ad X . This  X  wisdom of crowds  X  is gathered using last.fm X  X  search capabilities that retrieves all artists and songs annotated with a specific tag. While building the list, we consider a feeling not related to music if there are less than 500 users who used this feeling as a tag. As the result, we get 7 positive and 7 negative feelings conveyed by music, shown in Table 2.
For each of the genres and feelings in the lists, we retrieve the top-10 played (listened to) artists. Again the last.fm por-tal is used for this task. For each artist, we acquire two rep-resentative songs, and automatically cut them to 30 seconds length X  X rom minute 1:00 to 1:30. As some artists appear in multiple groups, (e.g., in the  X  X asy listening X  genre and in the  X  X ptimistic X  feeling), the document collection consists of 470 songs in total.
In the addressed soundtrack recommendation scenario, a query is represented by a set of images. We create a list of 25 queries, each containing 5 images, such that all images of a query follow a specific image theme. The initial list of image themes is retrieved from a list of photography forms, specified on Wikipedia [11]. For each of these themes, we re-trieve images that are annotated with the theme, using the search functionality of Google X  X  Picasa [21] photo sharing portal. We manually inspect the returned results and use only themes that provided at least 5 coherent and mean-ingful images. This filtering step results in the final list of 25 image themes shown in Table 3. As we can see, image themes vary from photos taken underwater, over photos of people playing sports, to photos of special cloud forms. For each theme, a query is formed by manually selecting 5 pub-licly available images from Picasa, again keeping in mind the coherence and the meaningfulness of the image theme. Image Themes Aviation Architectural Cloudscape Conservation Cosplay Digiscoping Fashion Fine art Fire Food Glamour Landscape Miksang Nature Old-time Portrait Sports Still-life Street Underwater Vernacular Panorama War Wedding Wildlife
Estimating the effectiveness of a retrieval engine is based on measuring the relevance of the returned results with re-spect to the given query. In traditional text retrieval, rel-evance is represented by absolute judgments that usually make use of a binary variable indicating that a document is either relevant or not relevant to a given query.
In the task of soundtrack recommendation, there is no such a notion of fulfilling a particular information need ex-pressed by the query. This renders the assessment less strict in the sense that in general all songs can be used as back-ground music. Thus, we define the relevance R ( s | S, q )ofthe song s ,givenasongcollection S , and a query q  X  Q ,as the rank of that document in the perfect ranking .Witha  X  X erfect ranking X  we denote the full ranking that would be created by  X  X xpert X  users.

A similar measurement is proposed for the task of simi-larity search in sheet music [32], with expert users providing a full ranking of the documents. In contrast to our setup, there, it can indeed be decided if two music sheets are com-pletely not related (relevant to each other).
What remains is the problem of obtaining the full rele-vance ranking, for each benchmark query. Doing this in an exhaustive way is prohibitively expensive, though. Instead, the idea is to let users evaluate a large number of song pairs, foreachbenchmarkquery.

In addition to selecting the best out of two proposed songs, each judge is asked to assess how much better the selected song fits to the query compared to the other song, on the scale from 1 to 5. A rating of 1 means  X  X lmost the same X  while 5 means  X  X arge difference X . The result of one human assessment is given in the following form r =( q,s 1 ,s 2 where q is the image theme query, s 1 and s 2 are songs, p is the preferred song and d is the difference between the songs.
The task at hand is, however, often influenced by the in-dividual taste of the human judges X  X or some queries more than for others. To capture this factor, we ask multiple as-sessors to judge the same song pair and use only the ones that show a high level of agreement.

We formulate questions for human assessment by first cre-ating song pairs in four different categories: genre, positive, negative, and positive-negative. The pairs in one category are all pairs of songs from that category. Only, the positive-negative category consists of pairs where one song is selected from the positive-feeling group and the second song is se-lected from the negative-feeling group. Final questions are all possible triples where one element is an image-theme query and the other two are songs coming from the pre-viously created song pairs. All question triples are stored and the next question to be assessed by judges is selected randomly among all non-assessed questions.
While collecting the assessments we had each question, i.e., song pair for a certain query, answered by 6 assessors. Hence, the individual preference judgments need to be rec-onciled. To achieve this, we compute the majority vote for each of the different agreement levels (four out of six (4/6), 5/6, and 6/6). Note that for the agreement level 3/6 there is no majority vote, so we leave this level out. For a certain agreement level, we then obtain a set of relevance judgments R with each r  X  R of the form r =( q,s 1 ,s 2 ,p,d ), where q is an image query, s 1 and s 2 are songs, p is the indication of the song preferred by the majority of users, and d is the dif-ference between songs averaged over multiple users assessing thesamepair.

Then, the quality (goodness) G of a ranking can be com-puted using preference precision [4] defined as: where the pair of songs is correctly ordered if the song pre-ferred by most assessors is located higher in the ranking compared to the other song, and the evaluated pairs are all song pairs that are assessed by the judges and are contained in the top-K ranking. A pair of songs is contained in the fi-nal top-K ranking if at least one of the songs appears in the top-K results. If only one song is in the top-K results the rank of the second song is considered to be K +1. Intuitively, this measure rewards a system if its ranking agrees with a user X  X  perceived preference, resulting in a higher value with a higher agreement between the two.
 The specified difference between the songs, denoted as d ,can be considered as the strength of preference and, hence, can be taken into account when assessing the quality of a system. As multiple judges are evaluating the same pair of songs for a given query, the final value of the difference between songs is taken as the average of the single evaluations.
The obvious way to extend the preference precision mea-sure using the preference strength is as follows: where p s is the preference strength, having higher value if the preference is stronger. For instance, the preference is strong toward one song if the difference between the two songs is large. Thus, we can use this difference between songs directly as preference strength. Clearly, this measure gives more weight to the preference judgments which were obvious for humans, and dampens the effect of judgments for which even the assessors were not sure about their preference.
Processing large amounts of human-involved tasks can be efficiently addressed using Amazon X  X  Mechanical Turk [20]. This service represents a mediator between the requester X  X  person or an organization posting tasks to be done X  X nd a number of workers X  X eople willing to perform these tasks, while getting paid for it.
 There are studies [1, 2, 24] on the usage of the Mechanical Turk service to collect relevance assessments. All of these studies face the same problem: determining whether the worker really prefers a selected document (song), or if the selection is done randomly to simply gain money, without spending sufficient effort on the assessment task.
To remove assessments of such  X  X heaters X , a certain set of question with known answers is inserted in the evaluation task. These questions are referred to as  X  X rap questions X ,  X  X oney pots X , or  X  X old standard X  questions. Cheating evalu-ator are then identified by the percentage of times the wrong answer is given to a trap question.

For this reason, we collect judgments in two phases. In the first phase, we build a set of  X  X old standard X  questions by collecting judgments from students on our campus, in the controlled environment of our offices. These  X  X old standard X  questions are used as trap questions for the second phase of acquiring assessments, using Mechanical Turk workers.
Each question (song pair and query image theme) is an-swered by six assessors. Only the questions with an agree-ment level of X  X ix out of six X  X re used to create trap questions for the next phase X  X onsidering only the preferred song, not the level of difference d . The probability of achieving this level of agreement randomly is quiet low, with a p-value of 0 . 03125, which makes it safe to use these questions as trap questions.

Note that at the evaluation phase, we can choose to per-form the quality assessment using only the second phase as-sessments, obtained from Mechanical Turk, to indisputably avoid a potential student population bias.
 Collecting a larger amount of assessments is achieved in the second phase by gathering assessments from Mechanical Turk workers. To obtain a robust benchmark, again, each question is answered by six workers. This enables a later evaluation based on different levels of agreement.
Cheating workers are identified as the ones that have per-formed a large number of questions X  X xpecting high money reward X  X hile choosing answers at random. Due to the bi-nary nature of the questions, cheaters answer approximately only 50% of all trap questions correctly. We used a threshold of at least 100 answered questions and less than 65% correct trap questions to reject aworkofa cheating worker. We used one trap question per five regular questions.

Because workers prefer small tasks [1], we created one HIT (Human Intelligent Task) for each question. We set the re-ward to $0 . 02 for each performed task, as the reward per task has only a small impact on the quality but rather in-fluence the quantity of the performed tasks [18].
To obtain trap questions for the Mechanical Turk workers, we collected assessments from 30 students. Students were able to choose whether they want to participate in the study for one hour or two hours, while being payed on an hourly basis. 29 out of 30 students participated in the study for two hours, resulting in the total of 665 questions, each question assessed by 6 students.

Figure 1 shows the number of assessments per student, sorted in descending order. We observe a high variance in assessment performance, even if we exclude the student that assessed songs for only one hour. Pearson X  X  correlation coeffi-cient shows that there is no correlation between the number of performed assessments and the agreement with other as-sessors (0 . 04141), indicating that the quality is not dictated bythenumberofassessmentsperstudent.

Table 4 shows the percentages of questions with different agreement levels for student evaluations. Agreement level  X  X /y X  means that x out of y assessors agreed on one song. The observed values in Table 4 suggest that we can re-ject the null hypothesis that student answers were done by randomly choosing songs, supported by the Chi-square test  X  2 = 1586 . 86, df =3, p&lt; 0 . 0001. This level of significance shows that such a high level of agreement between assessors is almost impossible to achieve by pure chance, but that the task at hand is reasonable and meaningful for the assessors. As we can see, 29.32% (i.e., 195) of all questions have agree-ment level of 6/6, making them applicable as trap questions for the second phase. The averaged difference between songs is also reported for student evaluations in Table 4. We see that the average difference is smallest, 2 . 75, for the questions with low agreement level and gradually increases to 3 . 65 for the questions with the six out of six agreement level.
In the second phase, we used Amazon Mechanical Turk to collect a larger number of assessments. Our aim was to col-lect enough assessments such that each song for each image query had a chance of being judged once. This required us to have more than 5875 questions evaluated, each question as-sessed by six assessors. In the end, we collected assessments evaluating 5990 questions in total.
 Overall, we had 269 assessors participating in the study. On average, each of them performed 138 . 69 assessments. As there was no time limit for each assessor, the skew in the number of performed assessments is much larger than for the students in phase one, ranging from one evaluation up to 3845 evaluations per assessor. Gold standard questions enabled us to detect 15 cheating workers and to reject their work, being replaced by other workers X  assessments. Table 5: Agreement levels for Mechanical Turk workers
The percentage of questions with respect to agreement levels for Mechanical Turk workers is shown in Table 5. As we can see, the percentages of questions with high-agreement levels are lower than for student assessments. Still, we can safely reject the hypothesis of randomly provided answers, with  X  2 = 6605 . 18, df =3, p&lt; 0 . 0001. The reduction in the agreement level might also be an effect of the more di-verse population of workers compared to the population of students.

We see that the percentage of questions with agreement level of X 5/6 X  X nd X 6/6 X  X s close to 50%, which renders almost half of the evaluated questions usable with high confidence. Again we see that there is a correlation between average difference between the songs and the agreement level.
To go beyond the proposal of a benchmark, we now present its application to the evaluation of the two state-of-the-art approaches in the area of soundtrack recommendation. First, an approach by Li and Shan [17] that is based on emotion detection in images and music X  X e refer to this approach as the emotion-based approach . Second, our approach coined Picasso [26], that extracts information from publicly avail-able movies and uses that information to create a match between images and music.
 Emotion-based Approach: The approach by Li and Shan [17], was originally developed to recommend music for impressionism paintings, but can in general be applied to arbitrary sets of images. The key idea is to detect emotions in both images and music and to employ this information for the match making. The detection of emotions and the recommendation of music is done through methods based on a graph representation of multimedia objects, called the mixed media graph .

Picasso: The recommendation process in Picasso [26] is based on information extracted from publicly available movies. This extraction is done in a preprocessing phase which re-sultsinanindexthatcontains &lt; movie screenshot, sound-track part &gt; -pairs. When an image is submitted to the sys-tem, Picasso finds the K most similar movie screenshots to the given query image. It then retrieves the K most sim-ilar songs to the soundtrack parts that corresponds to the retrieved screenshots and after the smoothing procedure rec-ommends the top-K songs as a result.
For the emotion-based approach to operate we need two training datasets, that is, a set of images and a set of songs with labeled emotions. As part of the songs in the benchmark were acquired based on their emotion labels, we already have a training dataset for the songs.
 As a training dataset for images we use the International Affective Picture System [15] dataset. It contains 1196 im-ages, each placed on the three dimensional space of emo-tions it evokes [23]. To create a match between music and images, we need a unified representation of emotions. This is achieved by mapping emotions, used to label music, into the three dimensional space of emotions, used to label images. Each image is labeled by one emotion, where the emotion la-bel corresponds to the area of the space indicated by the two primary dimensions valence and arousal. The used mapping isshowninFigure2.

To create the index for Picasso, we extracted information from 50 publicly available movies. All the movies originate from Hollywood production but cover a wide variety in gen-res and styles. In total, the final index contains 10454 snap-shots taken and the same number of corresponding sound-track parts.
 System 6/6 +5/6 +4/6 Emotion-based 0.658 0.595 0.559 Picasso 0.782 0.690 0.614 Fisher X  X  exact test (two-tailed) 0.0530 0.0249 0.0938
We execute both systems for each of the 25 queries from the benchmark requesting the top-20 songs as a recommen-dation result. The preference precision for both systems is shown in Table 6. The first column contains the preference precision measures when the systems are evaluated using only the questions with six out of six level of agreement. Fur-ther, adding questions with five out of six agreement level to the evaluation results in precision shown in the second col-umn, and finally, the evaluation with four out of six agree-ment level questions added is shown in the third column.
Fisher X  X  exact test is used to examine the probability of achieving these differences in precisions in case the results come from the same system (hypothetically). The contin-gency tables for the Fisher X  X  exact test are created by count-ing the number of correctly and incorrectly ordered pairs for both approaches.

We see that both systems perform best when the ques-tions used for evaluation are the ones for which assessors agreed on the answers. The performance of both systems drops when questions, for which uses did not easily agree on the answers, are added to the evaluation. The achieved pre-cision numbers indicate that Picasso performs better with regard to questions at all levels of agreement. Fisher X  X  exact test shows that it is not likely that this difference in precision is achieved by chance.
 System 6/6 +5/6 +4/6 Emotion-based 0.667 0.607 0.570 Picasso 0.818 0.728 0.645 Student X  X  t-test (two-tailed) 0.0148 0.0042 0.0197
The weighted preference precision of both systems is shown in Table 7. As we can see, the weighted precision for both systems is higher than the preference precision. This shows that incorrectly ordered song pairs were the ones with a small difference between the songs. Again, the best preci-sion is achieved for high agreeing questions as the number of correctly ordered song pairs is higher. We also see that Picasso performs better than the emotion-based approach. By calculating student X  X  t-test, also shown in Table 7, with positive differences for correctly ordered pairs and negative for incorrectly ordered ones, we can reject the hypothesis the means for the two systems are the same.
Our approach is based on the notion of pairwise compar-isons, first mentioned and analyzed by Fechner [7] and made popular later by Thurstone [29]. Thurstone [29] used them to determine the scale of perceived stimuli and referred to it as the law of comparative judgment .Alargebodyofre-search exists on reconstructing the final ranking from a set of pairwise comparisons, e.g., [6, 12].

For information retrieval tasks, Thomas and Hawking [28] use pairwise comparisons in order to compare systems in real settings. They show that click-through data highly cor-relates with perceived preference judgments. Sanderson et al. [24] employ pairwise comparisons with Amazon X  X  Me-chanical Turk [20] to obtain the correlation between user preference for text retrieval results and the effectiveness mea-sures computed from a test collection. The result of their study shows that Normalized Discounted Cumulative Gain (NDCG) [13] is the measure that correlates best with the user perceived quality. Using preference-based test collec-tions is introduced by Rorvig [22] and later developed for text retrieval by Carterette et al. [5]. In this work, the au-thors show that preference judgments are faster to collect and provide higher levels of agreement, compared to ab-solute relevance judgments. Preference-based effectiveness measures are proposed by Carterette and Bennett in [4], showing that they are stable and adhere to the measure-ments based on absolute relevance judgments. Preference judgments between blocks of results are used by Arguello et al. [3] to evaluate aggregated search results. In this work, the small number of such blocks enabled the collection of preferences between all pairs of blocks.

For music similarity, Typke et al. [32] conclude that coarse levels of relevance measure, usually used in text retrieval, are not applicable . Instead, they use a large number of rel-evance levels created from partially ordered lists. The work by Urbano et al. [34] addresses some limitations of this ap-proach by proposing different measures of similarity between groups of retrieved documents. Measuring retrieval effective-ness with these large number of levels can be achieved using the Average Dynamic Recall [33].

Due to its low price and high scalability, crowd sourcing is a popular technique to obtain relevance assessments for information retrieval tasks [1, 2, 24, 14]. The work by Alonso and Baeza-Yates [1] addresses the design and implementa-tion of assessments tasks in a crowd-sourcing setting, indi-cating that workers perform as good as experts at TREC [30] tasks. Similar results have also been achieved by Alonso et al. [2] in the context of XML retrieval. Snow et al. [25] show that Mechanical Turk workers were successful in annotat-ing data for various natural language processing tasks, even correcting the gold standard data for specific tasks.
In this work, we addressed the problem of building a com-prehensive and reusable benchmark for soundtrack recom-mendation systems. We formally defined the task of sound-track recommendation and the format of the evaluation bench-mark. Assessments were collected in form of preferences judg-ments: In the first phase from the students at the university and in the second phase through Amazon X  X  Mechanical Turk. We presented detailed statistics for collected assessments with respect to the agreement levels between assessors and different query types. We showed how the obtained judg-ments can be used to evaluate the quality of the soundtrack recommendation engines and reported on the performance of the state-of-the-art approaches. [1] O. Alonso and R. A. Baeza-Yates. Design and [2] O. Alonso, R. Schenkel, and M. Theobald.
 [3] J. Arguello, F. Diaz, J. Callan, and B. Carterette. A [4] B. Carterette and P. N. Bennett. Evaluation measures [5] B. Carterette, P. N. Bennett, D. M. Chickering, and [6] B. Carterette and D. Petkova. Learning a ranking [7] G. Fechner. Elemente der Psychophysik . Breitkopf und [8] Psychpage -General list of feelings. [9] Wikipedia -List of music genres. [10] ImageCLEF -Image Retrieval in CLEF. [11] Wikipedia -List of photograpy forms. [12] R. Janicki. Ranking with partial orders and pairwise [13] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [14] G. Kazai, N. Milic-Frayling, and J. Costello. Towards [15] P. J. Lang, M. M. Bradley, and B. N. Cuthbert. [16] Last.Fm -Music portal. http://www.last.fm/ . [17] C.-T. Li and M.-K. Shan. Emotion-based [18] W. A. Mason and D. J. Watts. Financial incentives [19] MIREX -The Music Information Retrieval Evaluation [20] Amazon Mechanical Turk. [21] Picasa -Photo sharing portal. [22] M. E. Rorvig. The simple scalability of documents. [23] J. A. Russell. A circumplex model of affect. Journal of [24] M. Sanderson, M. L. Paramita, P. Clough, and [25] R. Snow, B. O X  X onnor, D. Jurafsky, and A. Y. Ng. [26] A. Stupar and S. Michel. Picasso -to sing, you must [27] A. Stupar and S. Michel. Benchmarking Soundtrack [28] P. Thomas and D. Hawking. Evaluation by comparing [29] L. Thurstone. A law of comparative judgments. [30] TREC -Text REtrieval Conference. [31] TRECVID -TREC Video Retrieval Evaluation. [32] R. Typke, M. den Hoed, J. de Nooijer, F. Wiering, [33] R. Typke, R. C. Veltkamp, and F. Wiering. A measure [34] J. Urbano, M. Marrero, D. Mart  X  X n, and J. Llor  X  ens.
