 Collaborative recommendation algorithms are typically evaluated on a static matrix of user rating data. However, when users expe-rience a recommender system, it is dynamic, constantly evolving as new items and new users arrive. The dynamic properties of col-laborative recommendation have become important as prediction algorithms based on the interactions of rating histories have been proposed, and as researchers seek to understand problems of ro-bustness and maintenance in rating databases.

This paper proposes a new evaluation method for the dynamic aspects of collaborative algorithms, the  X  X emporal leave-one-out" approach, which can provide insight into both user-specific and system-level evolution of recommendation behavior. As a case study, the methodology is applied to the Influence Limiter algo-rithm [12], showing that its robustness to attack comes at a high accuracy cost.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Experimentation recommender systems, evaluation, temporal properties Collaborative recommender systems are essential tools for web per-sonalization and e-commerce, and they can be found throughout the information ecology from the World-Wide Web to mobile en-vironments and consumer devices [13]. The user experience of a recommender system is inherently dynamic. New items arrive con-stantly, possibly triggering waves of user activity, as in a popular new movie. In a thriving recommender system, new users are also adding themselves to the mix, uploading new opinions that may re-shape the peer neighborhoods on which recommendations rest. In addition, there is a strongly mixed-i nitiative flavor to the interplay between ratings users provide and the recommendations systems give. A user will be more likely to experience and thus rate an item that the system has recommended [2].

However, the dynamic properties of recommender systems have received scant notice in the res earch literature and are poorly cap-tured by the evaluation methodologies typical of research practice. The standard research methodology coalesces the rating stream into a ratings matrix, which is further decomposed into training and test sets, and algorithmic performance in this static environment has become the gold standard for collaborative recommendation algo-rithms [4]. Even researchers who are making use of temporal as-pects of the rating database [5, 7] are not evaluating the properties of their algorithms over time.

Researchers interested in  X  X old-start X  phenomena, such as [11, 14], approach the problem by selecting some users or items with shorter profiles and looking at system performance on this subset, or by artificially degrading profiles by eliminating ratings. Apply-ing accuracy or ranking metrics then gives a picture of the perfor-mance of the system on some users o r items with sparse histories, but it does not give an overview of how performance changes over the history of a user and, depending on how ratings are eliminated, may not give a valid picture of the evolution of an algorithm X  X  per-formance in the wild.

Two exceptions are evident. One is the  X  X couts, promoters, and connectors" work by Mohan, Keller and Ramakrishnan [9] that ex-amines the roles that ratings play over time in a graph-based model of recommendation. Another example can be found in the recent work of Lathia, Hailes and Capra [6] in which a variant of kNN with adaptive neighborhoods is proposed. Such an algorithm can only be evaluated in a dynamic manner, and Lathia et al. use a more coarse-grained version of the technique proposed here.

The limitations of the static methodology have come to the fore in work on recommender systems robustness. In particular, some researchers have proposed maintenance processes for ratings data-bases, in which the collection of user profiles is periodically swept for evidence of attackers [1]. Others have proposed dynamic al-gorithms based on the evolution and propagation of trust relation-ships. In particular, the Influence Limiter algorithm of Resnick and Sami [12] is a collaborative technique with provable limits on the amount of power that a potential attacker can exert to influence oth-ers.
To assess algorithms like the Influence Limiter or Lathia X  X  Tem-poral Collaborative Filtering, to model the interplay of attack and reaction for recommender system robustness, and to capture other aspects of recommender system dynamics, what is needed is a shift in the way that recommender systems and their properties are stud-ied. Instead of aggregating user opinions into a user-item matrix and performing experiments on the matrix as a whole, this paper proposes a methodology in which evaluations are made periodi-cally over the course of a user X  X  experience with the recommender using only that information that would have been available at the time. For example, if user u gets a recommendation for item time t , the system X  X  prediction will only consider a rating expressed by a peer v if that rating was entered prior to t .

Obviously, there are some drawbacks to this approach, notably, that the data is increased in its sparsity. There will be fewer ratings on which to base a given prediction than if we waited for the full data set to be available. On the other hand, such conditions of spar-sity are precisely what the actual users of the system encounter, and their experience is useful to evaluate.

I take as a starting point the  X  X eave one out X  methodology, in which a single rating is omitted from the database and the rest of the data used to estimate it. The mean absolute error of such estimates can be the evaluation metric [4]. This technique is here extended to leave out not just the single rating, but also all information entered into the system after the rating was encountered. More formally, for a given rating r u,i,t given by a user u for an item i denote the contents of the rating database immediately prior to time t as E ( &lt;t ) .The temporal leave-one-out technique requires that We can measure the errors in such predictions and have a metric for prediction accuracy over time, temporal MAE .

Temporal MAE gives us a systemic measure of accuracy. How-ever, what may matter most to an individual user is not the overall MAE achieved by the system, but the performance of the system over the course of his or her interactions with it. For example, we can imagine a system that returns poor predictions for the first 100 user interactions and then pe rfect recommendations thereafter. Such a system might have an overall MAE equal to that of a system with more middling performance throughout. However, a system with a very long ramp-up curve would not be preferred in an e-commerce setting because users X  confidence in the system would be undermined by their initial interactions.

A user-focused measure of evolving accuracy can be computed by examining how MAE varies as a user X  X  profile is accumulated. We would like to know for a user with a profile of a given size k , what reduction in MAE is obtained on average by rating one additional item? The ProfileMAE algorithm attempts to answer this question. See Algorithm 1. This algorithm assumes a database E , where each entry e is a tuple t, u, i, r where e.t is the time the rating was made, e.u is a user, e.i is an item, and e.r is the rating given by the user. The array stores the error at each profile size. The resulting array of average values shows the evolution of average prediction error over the course of profile entry. 1
In this preliminary study, the temporal leave-one-out technique is applied to two algorithms: the well-studied user-based collabora-tive filtering technique [3] and the Influence Limiter algorithm [12] proposed by Resnick and Sahmi. The version of the user-based al-gorithm here uses a co-rate weighting of 50, a neighborhood size of 20 and a similarity threshold of 0.0.
Of course, the number of users at each size k is different since not all profiles are of the same length, a fact which must be taken into account in analyzing the results.

The Influence Limiter is included as an example of an algorithm that can only be evaluated using a temporal technique. To date, no empirical evaluation of it has been published. In the Influence Lim-iter algorithm, each user has an associated reputation that is used to weight their contribution to each prediction. The reputation is updated as each prediction is compared with the actual rating when the user inputs it. Note that a temporal approach is essential for understanding the performance of this algorithm since the predic-tion for a given user can only be computed knowing the reputations of his or her peers, and those reputation values can only be com-puted by making predictions and updating them in the appropriate sequence. Indeed, altering the sequence in which ratings are en-tered will change the rating behavior of the system significantly. See [12] for details.
 The MovieLens 1M dataset was used to perform experiments. It contains approximately 1 millio n ratings drawn from 6040 users rating 3883 movies within the MovieLens recommender system. These users are only those who joined the system during the year 2000, but it contains ratings that extend beyond that year. This creates a problem for temporal evaluation because the user base does not change after the end of 2000, even though new ratings appear. For this reason, the data set is truncated at 12/31/2000, reducing it by about 6%. Figure 1 shows the temporal MAE per day averaged over a 7 day window. The data is fairly chaotic, but a regression analysis shows two segments. The start up period ends around day 70 with MAE dropping by 0.0011 per day. After that, the MAE drops much more slowly, showing an average decrease around 0.000052 per day. 2
The ProfileMAE algorithm lets us examine how the error experi-enced by a user changes as more ratings are added to his or her pro-file. Figure 2 shows these error values averaged over each profile size. (Note that the number of profiles decreases as size increases, requiring a weighted analysis.) In this figure, we only consider pro-files between sizes 15 and 650. This captures 95% of the profiles and avoids the noisiest data points found in the rare longer pro-files. The figure includes segmented regression lines with a break around 260 ratings. For the initial set of ratings, error drops with each added rating with a slope of -0.00020. Afterwards, however, the line is essentially flat (the slight upward slope is not significant), and the data show a marked increase in variance. Clearly, the initial ratings added by a user are the most powerful in terms of reliably decreasing error.

One natural question then arises: do longer profiles fail to de-
Regressions calculated using R 2.8 linear model linking MAE to profile size weighted by profile counts. Segmented regressions computed with the  X  X egmented X  package.
Figure 1: Daily temporal MAE, 7-day window (1M dataset) crease error because users provide reliable ratings first? This does not appear to be the case, as studies using only the initial 250 rat-ings provided by each user do not show comparable performance to the full profile. A full investigation of this phenomenon will be explored in future work. However, one interesting finding is that average rating decreases with profile length. It appears that users initially tend to enter positive ratings and only later add negative ones.
Collaborative systems by design are supposed to aggregate in-dividual user opinions, but the lack of authentication in Web and e-commerce applications means that it is possible for a malicious user to duplicate his or her opinion by creating multiple profiles, creating a so-called profile injection attack. Typically, the aim of this activity would be to promote a particular item, making it more likely to be recommended. Researchers in collaborative recom-mendation have sought to characterize this problem, its impact on standard recommendation algorithms, and possible defenses [10, 8].

Resnick and Sami X  X  Influence Limiter algorithm [12] is designed to control the impact of a profile injection attack by taking the dy-namic properties of the rating stream into account. In the algorithm, a reputation value is maintained for each user. The reputation for each user j is initialized at a very small value: e  X   X  experiments below, a  X  of 5 was used. 3
When making predictions, users in the peer neighborhood are or-dered temporally. That is, the first user to rate an item is considered first and the others in turn. The prediction from a given neighbor is a weighted function of j  X  X  rating for the item and the prior neigh-bors taken together. The current neighbor j has weight equal to his minus this value. 4
Predictions are stored so that reputations can be updated. When the true value of a user X  X  rating becomes known, the reputation of each user is adjusted in proportion to the loss incurred by the dif-ference between the prior peers X  estimate and the given user X  X  con-tribution.

The Influence Limiter provides a way to organize the recommen-dation process so that each user X  X  contribution to a given prediction can be accounted for and credit (and blame) can be assigned. Users that give good estimates to their peers (in the sense that those es-timates are proved correct later) become more powerful contribu-tors to the recommender system. Raters can only gain influence in the system by contributing  X  X ood" ratings, and those who do not quickly lose the ability to have an impact. This design greatly in-creases the effort required to mount a profile injection attack.
As can be seen from the description, there is no way to evaluate this algorithm purely from a static matrix data set. The algorithm is indeed a good fit to the temporal leave-one-out methodology. In a real system, it is likely that many of the predictions to which a given user profile contributes will never be verified one way or another  X  they will essentially be left as unanswered questions. With the temporal leave-one-out methodology, we make predictions right at the moment before a user supplies a rating, so all predictions are immediately confirmed or denied and reputations immediately up-dated. We would expect this to be something of a best case for the Influence Limiter.
Neither smaller nor larger  X  values (from 1 to 20) produced sub-stantially different performance.
For the purposes of formal analysis, the algorithm as originally described has no notion of selecting neighbors by similarity  X  es-sentially every user who has rated an item contributes equally to the outcome. This approach works poorly, and was replaced in this work by a better-performing variant that is more of an analog to the user-based algorithm in that it selects peers based on correlation and considers only these peers in building its weighted prediction. In addition, peers are weighted both by similarity and by reputation. Figure 3: Daily temporal MAE, 7-day window (Influence Lim-iter algorithm)
The robustness of the algorithm is well-established, but it proves to be very weak in terms of accuracy. Compare Figure 3 with its counterpart Figure 1. First of all, note the scale. The Y intercept of the regression line is almost 0.3 higher in error, representing a ma-jor loss in accuracy through influence limitation and the slope is flat (weak positive slope not significant), indicating that the algorithm is not improving with the addition of more data.

The key to this phenomenon can be seen in Figure 4, a histogram of the reputation values on a logarithmic scale. Almost 50% of the users (2,813) end up with reputations of zero and do not appear on this figure. Of those with a reputation greater than zero, the vast majority (2,864) have the default reputation value of e  X  5 of the users have managed to enhance their reputation beyond this default value. This would seem to indicate that the requirement that a user be consistently a  X  X ood neighbor" in order to build a reputation is simply too stringent.
Researchers have been accustomed to a static view of collabora-tive recommender systems, a by-product of evaluation methodolo-gies that use a static rating matrix as their starting point. However, recent research has resulted in the development of algorithms that cannot be evaluated in this static way, but depend rather on the dy-namic properties of the rating database.

This paper has put forward an evaluation methodology suited to the exploration of the temporal aspects of collaborative recommen-dation. Application of this methodology reveals phenomena not previously observed in the well-known MovieLens data set, in par-ticular the diminishing returns to a user for adding ratings. In addi-tion, this work has enabled the evaluation of the Influence Limiter algorithm, demonstrating that its robustness to attack comes at a severe cost to prediction accuracy and showing that its requirement of consistency in neighbors is too strict.

This work represents an initial foray into the application of these ideas and as such leaves many questions unanswered. There are of course many proposed algorithms for collaborative recommen-dation and while some have been extensively evaluated statically, their dynamic properties are largely unknown. Thanks to JJ Sandvig for implementation work and to Michael O X  X ahony, Rachel Rafter, Barry Smyth, Neil Hurley and other UCD colleagues for valuable discussions. This work was supported
Figure 4: Histogram of final reputation values (log scale) in part by the US-Irish Fulbright Commission, by University Col-lege Dublin, and by DePaul University. [1] R. Burke, B. Mobasher, C. Williams, and R. Bhaumik. [2] D. Cosley, S. K. Lam, I. Albert, J. Konstan, and J. Riedl. Is [3] J. Herlocker, J. Konstan, A. Borchers, and J. Riedl. An [4] J. Herlocker, J. Konstan, L. G. Tervin, and J. Riedl. [5] Y. Koren. Collaborative filtering with temporal dynamics. In [6] N. Lathia, S. Hailes, and L. Capra. Temporal collaborative [7] Z. Lu, D. Agarwal, and I. S. Dhillon. A spatio-temporal [8] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. [9] B. K. Mohan, B. J. Keller, and N. Ramakrishnan. Scouts, [10] M. O X  X ahony, N. Hurley, N. Kushmerick, and G. Silvestre. [11] S.-T. Park and W. Chu. Pairwise preference regression for [12] P. Resnick and R. Sami. The influence limiter: provably [13] B. M. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Analysis [14] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock.
