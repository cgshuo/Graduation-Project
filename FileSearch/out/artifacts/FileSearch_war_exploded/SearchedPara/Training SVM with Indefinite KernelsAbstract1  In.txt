 Kernel methods work by embedding the data into a high-dimensional (possibly infinite-dimensional) feature spac e, where the embedding is defined implicitly through a ker-nel function. Evaluating the kernel function on all pairs of data points produces a symmetric and positive semidefi-nite (PSD) kernel matrix. Support Vector Machine (SVM) with a positive semidefinite kernel matrix has been applied successfully in numerous classification tasks including fa ce recognition, image retrieval, and micro-array gene ex-pression data analysis (Cristianini &amp; Shawe-Taylor, 2000; Sch  X  olkopf &amp; Smola, 2001; Tong &amp; Chang, 2001). The PSD property of the kernel matrix ensures the existence of a Re-producing Kernel Hilbert Space (RKHS) and results in a convex formulation for SVM. Thus, a global optimal solu-tion exists.
 In practice, however, similarity matrices generated from many applications may not be PSD (Qamra et al., 2005; Roth et al., 2003a; Shimodaira et al., 2001). The problem of learning with a non-PSD similarity matrix (indefinite kernel) has been addressed by many researchers (Wu et al., 2005; Haasdonk, 2005; Lin &amp; Lin, 2003). One simple and popular approach is to generate a PSD kernel matrix by transforming the spectrum of the indefinite kernel ma-trix (Wu et al., 2005). Several representative transforma-tion methods include denoise which neglects the negative eigenvalues (Graepel et al., 1998; Pekalska et al., 2002), flip which flips the sign of the negative eigenvalues (Grae-pel et al., 1998), diffusion which applies matrix diffusion on the indefinite kernel (Kondor &amp; Lafferty, 2002), and shift which shifts all the eigenvalues by a positive constant (Rot h et al., 2003b). One common limitation of these approaches is that the transformation may lead to the loss of valuable information in the data.
 Several other works use the non-PSD similarity matrix as a kernel, but they change the formulation of SVM. In (Lin &amp; Lin, 2003), an SMO-type method is proposed to find sta-tionary points for the non-convex dual formulation of SVM with a non-PSD sigmoid kernel. However, this method is based on the assumption that a corresponding RKHS still exists such that SVM formulations are valid. Haasdonk (2005) interprets learning with an indefinite kernel as the minimization of distance between two convex hulls in some pseudo-Euclidean (pE) space. However, it assumes that the representer theorem holds in such a pE space. Ong et al. (2004) associate the indefinite kernels with a Reproducing Kernel Kre  X   X n Space (RKKS), in which a general represen-ter theorem exists and a regularized risk functional can be defined.
 Recently, Luss and d X  X spremont (2007) propose a regu-larized SVM formulation, in which the indefinite kernel matrix is considered as a noisy observation of some un-known PSD one (proxy kernel). One attractive property the proxy kernel can be found simultaneously. However, the convex reformulation in (Luss &amp; d X  X spremont, 2007) involves a nondifferentiable objective function. To facil i-tate the calculation of the gradient, Luss and d X  X spremont (2007) quadratically smoothed the objective function, re-sulting in two algorithms including the projected gradient method and the analytic center cutting plan method. In this paper, we study the problem of training SVM with an indefinite kernel matrix following the formulation in (Luss &amp; d X  X spremont, 2007). We show that this problem can be reformulated as a semi-infinite quadratically con-strained linear program (SIQCLP), which includes a finite number of optimization variables with an infinite number of constraints. We then propose an iterative algorithm to solve this SIQCLP problem, which consists of two key steps: computing an intermediate SVM solution by solving a quadratically constraint linear program with a restricte d subset of constraints, and updating the subset of constrain ts based on the obtained intermediate SVM solution. We fur-ther show the convergence property of the proposed itera-tive algorithm.
 One limitation of the proposed algorithm is that the com-putational cost for solving the quadratically constraint l in-ear program depends on the number of constraints, which gradually increases during the iteration. We propose to improve the efficiency of the iterative algorithm by prun-ing inactive constraints at each iteration. We show that such pruning will not affect the convergence property of the algorithm. In addition, we show the close relationship between the proposed SIQCLP formulation and multiple kernel learning (MKL). More specifically, the intermedi-ate quadratically constraint linear program with a restric ted subset of constraints is shown to be equivalent to a regu-larized version of the multiple kernel learning formulatio n in (Lanckriet et al., 2004). Thus, efficient algorithms for MKL (Lanckriet et al., 2004; Rakotomamonjy et al., 2007; Sonnenburg et al., 2006) can be applied to solve the SIQ-CLP problem. We have performed experiments on a col-lection of benchmark data sets. The presented experimen-tal results demonstrate the efficiency and effectiveness of the proposed algorithm. positive semidefinite (PSD). Let y = [ y be the vector of class labels, where y dual formulation of 1-norm soft margin SVM classification is given by (Sch  X  olkopf &amp; Smola, 2001): where  X  is the vector of Lagrange dual variables, Y = diag ( y ) , C is a pre-specified parameter, and e is a vector of all ones of length n .
 Since K is PSD, the optimization problem in Eq. (1) is a convex Quadratic Program (QP) (Boyd &amp; Vandenberghe, 2004); hence a global optimal solution can be found via standard optimization techniques such as primal-dual inte -rior point methods (Nocedal &amp; Wright, 1999). In practice, however, many similarity matrices may be non-PSD (in-definite kernels), including sigmoid kernels (Vapnik, 1995 ) for various values of its parameters and hyperbolic tangent kernels (Smola et al., 2000). Additional examples include the protein sequence similarity measures based on Smith-Waterman and BLAST scores.
 In (Luss &amp; d X  X spremont, 2007), the indefinite kernel is considered as a noisy observation of some unknown PSD kernel (proxy kernel), and the following max-min opti-mization problem is proposed for simultaneous proxy ker-nel learning and SVM classification: max where K the unknown proxy kernel matrix, and  X  &gt; 0 is the pre-specified parameter, and |||| of a matrix (Golub &amp; Van Loan, 1996).
 The objective function in Eq. (2) is convex in K and con-cave in  X  , thus a global optimal solution exists. How-ever, direct optimization of Eq. (2) in terms of both and K leads to a complex optimization problem involving nondifferentiable objective function (Luss &amp; d X  X spremont , 2007). To facilitate the calculation of the gradient, Luss a nd d X  X spremont (2007) quadratically smoothed the objective function. Two algorithms including the projected gradient method and the analytic center cutting plan method are pro-posed for the proposed formulation. We propose to solve the optimization problem in Eq. (2) by first reformulating it as a semi-infinite program (SIP) (Het-tich &amp; Kortanek, 1993a). The SIP problem refers to opti-mization problems that maximizes the functional F ( z ) sub-ject to a system of constraints on z , expressed as g ( z, t )  X  0 for all t in some set B . When the objective is linear and the constraints are quadratic, the optimization proble m is known as semi-infinite quadratically constrained linear program (SIQCLP).
 For notational simplicity, we denote the objective functio n in Eq. (2) as:
S (  X , K ) =  X  T e  X  The optimal solution to the max-min problem in Eq. (2) is a saddle-point for the function S (  X , K ) subject to the constraints in Eq. (2). Let (  X   X  , K  X  ) be optimal to Eq. (2). For any feasible  X  and K in Eq. (2), we have moreover, it can be verified that By adding an additional variable t  X  R , the max-min op-timization problem in Eq. (2) can be reformulated into a SIQCLP problem as follows: The optimization problem in Eq. (6) has two optimization variables ( t and  X  ) with an infinite number of (quadratic) constraints, i.e., one quadratic constraint t  X  S (  X , K ) each kernel matrix K . When there is only one (fixed) ker-nel matrix K involved in Eq. (6), this optimization problem reduces to a standard SVM problem. We propose an iterative algorithm to solve Eq. (6), which is guaranteed to converge to a global optimum. The algorithm is closely related to the bundle method (Hiriart-Urruty &amp; Lemarechal, 1993; Teo et al., 2007).
 The optimization problem in Eq. (6) maximizes its objec-tive function with respect to two variables t and  X  with an infinite number of (quadratic) constraints. We approach the optimum by optimizing the variables t and  X  with a restricted subset of the infinite number of constraints, and then updating the constraint subset based on the obtained suboptimal t and  X  in an iterative manner. It is similar to the strategy presented in (Sonnenburg et al., 2006). The al-gorithm belongs to a family of algorithms for solving gen-eral SIP problems called the exchange methods , in which the constraints are exchanged at each iteration. The global optimality property of the final solution after convergence is guaranteed (Hettich &amp; Kortanek, 1993b).
 For a restricted subset of constraints, called a localization set of kernel matrices K = { K optimal t and  X  can be computed by solving the following optimization problem: This corresponds to a quadratically constrained linear pro -gram (QCLP) with p quadratic constraints. The optimiza-tion problem is often called the restricted master problem , and the obtained suboptimal solution pair (  X , t ) is called in-termediate solution . Note that this QCLP problem can be solved efficiently using general optimization solvers. To approach the optimum of the SIQCLP problem from a given intermediate solution pair ( t,  X  ), we find the next con-straint with the maximum violation, i.e., the kernel matrix K that minimizes S (  X , K ) . The optimal K can be com-puted by solving the following minimization problem: min If the optimal K  X  to Eq. (8) satisfies t  X  S (  X , K  X  ) , then the current intermediate solution pair ( t,  X  ) is optimal for the optimization problem in Eq. (6). Otherwise, K  X  is added into the localization set K . The intermediate solu-tion pair ( t,  X  ) is updated by solving the restricted master problem based on the updated K . We repeat this iterative process until convergence. The final solution is guaranteed to be globally optimal (Sonnenburg et al., 2006). It can be shown (Luss &amp; d X  X spremont, 2007) that the opti- X  is given by: Here X X , i.e., X the i -th eigenvalue and eigenvector of X .
 Based on the discussions above, we propose an iterative algorithm to solve the optimization in Eq. (6). The pseudo-code is presented in Algorithm 1. Note that the algorithm searches for a quadratic constraint (specified by K  X  ) with the maximum violation in step 1 , then updates the inter-mediate solution ( t,  X  ), which is repeated iteratively until convergence. When no more constraints with violation can be found, i.e., all constraints are satisfied, Algorithm 1 co n-verges. In practice, we determine the convergence by com-paring an upper and lower bound of the objective as de-scribed in the next section. 4.1. Convergence Analysis We analyze the convergence property of Algorithm 1. Re-call that Algorithm 1 alternates between the updating of the Algorithm 1 Proposed Algorithm
Input: Indefinite kernel K Output:  X  and K ;
Initialization: ( t Initialization: i  X  1 , K =  X  ; Do until convergence localization set K (step 1 ) and the updating of the interme-new kernel matrix K  X  With the addition of K K = { K j } i j =1 . We denote Let ( t in Eq. (7) after the i -th iteration. Denote u + u i  X  t i = min K  X  K S (  X  i , K ) = max  X  min K  X  K S (  X , K ) , where K is the updated restricted localization set. The following theorem shows that Algorithm 1 makes con-tinuous progress towards the optimal solution: Theorem 4.1. Let l  X  Eq. (13), respectively. Let (  X   X  , t  X  ) be the optimal solution pair to the optimization problem in Eq. (6). Then Moreover, the sequence { u + and the sequences { l  X  Proof. For any feasible  X  in Eq. (6), we have It follows that the inequality above also holds for their cor -responding pointwise maximum with respect to  X  . From Eq. (13) and the equality below we have u + Eq. (10) that Thus { (  X  sible solution pairs to the optimization problem in Eq. (6). Since (  X   X  , t  X  ) is the optimal solution pair to Eq. (6), we have t  X   X  S (  X  Eq. (11) that t  X   X  l  X  From Eq. (13), we have Thus, the sequence { u + the size of localization set K monotonically increases. It follows from Eq. (11) that { l  X  ing. This completes the proof of the theorem.
 Based on the result in Theorem 4.1, we can use the gap be-tween u + When this gap is smaller that a pre-specified tolerance, we stop the algorithm. 4.2. Pruning Inactive Constraints In Algorithm 1, a quadratically constraint linear program (QCLP) is involved at each iteration (step 2 ). The com-putational cost for solving QCLP grows with the number of quadratic constraints, which increases by one after each iteration. We show that at each iteration, many (inactive) quadratic constraints can be pruned, while retaining the convergence property of the algorithm.
 Assume that (  X  iteration with K i = { K further partition K i into two subsets as K i = K i such that and where the equalities in Eq. (19) and inequalities in Eq. (20) are called active and inactive constraints (Nocedal &amp; Wright, 1999), respectively. Let K  X  be the optimal ma-trix given in Eq. (9) with  X  =  X  K the efficiency by removing the inactive constraints from the optimization and updating the new localization set K i +1 as K tion pair at the ( i + 1) -th iteration with the updated K as the localization set. To show the convergence, we need to prove t Lemma 4.1. Let t t Proof. Prove by contradiction. Assume that t doesn X  X  hold, i.e., t Let (  X   X ,  X  t ) be the optimal solution pair to the optimization problem in Eq. (7) with K i clear that t  X  For any K  X  K i the following holds for any K  X  K i For any  X   X  (0 , 1) , let  X  =  X   X   X  + (1  X   X  )  X  is concave on  X  and t Eq. (19), the following holds for any K  X  K i Recall that for any K  X  K i Since S (  X , K ) is continuous on  X  , and K i there exists an  X   X  (0 , 1) sufficiently close to zero such that where  X  It follows from Eqs. (22) and (23) that Since (  X  the localization set, Eq. (24) contradicts with our assump-tion that (  X  completes the proof of the lemma.
 Lemma 4.1 shows that the upper bound defined in Eq. (13) with the inactive constraints pruned as above decreases monotonically. As the lower bound defined in Eq. (11) al-ways increases monotonically, the proposed pruning strat-egy retains the convergence property in Theorem 4.1. We show the close relationship between the proposed SIQ-CLP formulation in Eq. (6) and the multiple kernel learning formulation in (Lanckriet et al., 2004).
 For a given set of kernel matrices { K vector y , Lanckriet et al. (2004) propose to learn an optimal convex combination of the p pre-specified kernel matrices by solving the following optimization problem: where Y = diag ( y ) , and C is the pre-specified parameter. Recall that K given PSD kernel matrix K where terms of Frobenius norm. Consider a regularized version of the optimization problem in Eq. (25) given by: min subject to where  X  is the pre-specified parameter as in Eq. (6). The optimization problem in Eq. (27) computes an optimal lin-ear combination of the p pre-specified kernel matrices by maximizing the margin for SVM classification, while pe-nalizing kernels with a large deviation from K The following theorem shows the equivalence relationship between the regularized MKL problem in Eq. (27) and the SIQCLP formulation in Eq. (7).
 Theorem 5.1. Let { K kernel matrices. Then the optimization problem in Eq. (7) is equivalent to the one in Eq. (27).
 Proof. Since all constraints in Eq. (27) are linear and the objective is convex on {  X  imization and the maximization in Eq. (27) can be ex-changes. This leads to the following optimization problem: = max = max where t From Eq. (28) and Eq. (29), the optimization problem in Eq. (27) can be reformulated as: which is equivalent to the SIQCLP formulation given in Eq. (7). We complete the proof of this theorem.
 The equivalent result in Theorem 5.1 implies that our pro-posed SIQCLP formulation in Eq. (6) can be solved by recycling existing efficient MKL implementations (Rako-tomamonjy et al., 2007; Sonnenburg et al., 2006). We experimentally evaluate the convergence property of the proposed algorithms. We also compare the proposed al-gorithms with other representative ones using a collection of benchmark data sets. 6.1. Experimental Setup We use several benchmark data sets from the UCI repos-itory (Newman et al., 1998) including Sonar, Ionosphere, Breast Cancer, and Diabetes, as well as USPS (Hull, 1994) ing to two digits 3 and 5 , and randomly select 600 samples for each digit.
 In our simulation study, we first generate Gaussian ker-nels from the data with the parameter value estimated via cross-validation and then construct indefinite kernels through perturbation. More specifically, we randomly gen-erate a matrix E with zero mean and identity covariance matrix, and then apply  X   X  E as the perturbation, where  X  ( E + E T ) / 2 and  X  &gt; 0 is small constant. We set C = 1 SVM. The value of  X  is estimated via cross-validation. 6.2. Convergence In this experiment, we empirically evaluate the conver-gence property of the proposed algorithms with and with-out pruning. We also investigate the number of kernel ma-trices involved when the pruning strategy is employed. We use the sonar data set for this study, and the perturbation matrix is set to be 0 . 1  X  E .
 The results are presented in Figure 1. The top graph in Fig-ure 1 shows the convergence of the upper bound as well as the lower bound of the objective value when the algorithm without the pruning strategy is applied. The bottom graph shows the convergence result for the case when the pruning strategy is applied. We can observe from the figure that the upper bound and lower bound curves approach each other gradually during the iteration. More specifically, the up-per bound monotonically decreases, while the lower bound monotonically increases, both approaching the optimal ob-jective value. This is consistent with our convergence re-sults in Section 4.1. Interestingly, our results show that t he proposed algorithms with or without the pruning strategy applied result in a similar convergent rate. We further ob-serve that the gap between the upper and lower bound is less that 10  X  2 after about 150 iterations, and it takes about 600 iterations to attain a gap smaller than 10  X  5 . Figure 2 shows the number of kernels (the size of the lo-calization set) involved at each iteration. We can observe from the figure that with the pruning strategy, the number of kernels involved in the algorithm stabilizes around a small constant. In contrast, this number increases gradually whe n the pruning strategy is not applied. These results demon-strate the advantage of the proposed pruning strategy. We show in Figure 3 the generalization performance (mea-sured by classification accuracy) of the proposed algorithm with pruning at each of the first 70 iterations. We observe a large variation at the first few iterations, while the accura cy becomes more stable after about 40 iterations. We further run the algorithm until convergence and the resulting accu-racy is about 76%. We obtain a similar observation from other data sets. This implies that an early-stopping strate gy could be employed for the proposed algorithm. 6.3. Classification Performance In this experiment, we compare our proposed algo-rithms (Indefinite SVM) with other representative ones in-cluding Denoise, Flip, Shift, and SVM using indefinite ker-nels in terms of classification accuracy. The presented ex-perimental results are averaged over 10 random partitions of the data into a training and a test set with a ratio 4 : 1 The experimental results are summarized in Table 1. We also report the maximum and minimum eigenvalues of the indefinite kernel matrix in the table. We can observe from the table that Indefinite SVM is competitive with all other algorithms in most cases. It outperforms all other algo-rithms on the Sonar, Ionosphere, Heart, and Diabetes data sets, where the perturbed kernel matrix has a relatively small ratio |  X  cluding Breast Cancer and USPS-3 -5 , where the perturbed kernel matrix has a relatively large ratio |  X  definite SVM is comparable to the best among all other al-gorithms. These results demonstrate the effectiveness of the proposed learning algorithm, especially when the in-definite kernel matrix is highly non-PSD. A similar trend has been observed in (Luss &amp; d X  X spremont, 2007). In this paper, we study the problem of training SVM with an indefinite kernel matrix following the formula-tion in (Luss &amp; d X  X spremont, 2007). We propose a semi-infinite quadratically constrained linear program formula -tion, which can be solved iteratively. The algorithm alter-nates between the computation of an intermediate SVM so-lution by solving a quadratically constraint linear progra m with a subset of constraints, and the computation of the new constraint set based on the obtained intermediate SVM solution. We further propose to improve the efficiency of the iterative algorithm by pruning inactive constraints at each iteration. We show that such pruning will not affect the convergence property of the algorithm. In addition, we show the close relationship between the proposed SIQCLP formulation and multiple kernel learning. The presented analysis provides new insights into the nature of this learn -ing formulation.
 We have performed a simulation study using a collection of benchmark data sets. Our results verify the conver-gence property of the proposed algorithms. Our empiri-cal results show that the proposed algorithms with or with-out the pruning strategy applied result in a similar conver-gent rate, while a much smaller number of kernel matri-ces are involved when the pruning strategy is applied. Our results also demonstrate the favorable performance of the proposed algorithms in terms of classification accuracy in comparison with several other representative algorithms. Our future works include the analysis of the convergence rate of the proposed algorithms similar to the analysis con-ducted in (Teo et al., 2007), the estimation of the regulariz a-tion parameter  X  , and the application of the proposed algo-rithms to real-world applications involving indefinite ker -nels such as protein sequence and structure analysis based on various sequence/structure alignment measures. This research is sponsored in part by funds from the Ari-zona State University and the National Science Foundation under Grant No. IIS-0612069.

