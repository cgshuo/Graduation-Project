 With the globalisation of the world X  X  economies and ever-evolving financial structures, fraud has become one of the main dissipaters of government wealth and perhaps even a major contributor in the slowing down of economies in gen-eral. Although corporate residence fraud is known to be a major factor, data availability and high sensitivity have caused this domain to be largely untouched by academia. The current Belgian government has pledged to tackle this issue at large by using a variety of in-house approaches and cooperations with institutions such as academia, the ulti-mate goal being a fair and efficient taxation system. This is the first data mining application specifically aimed at find-ing corporate residence fraud, where we show the predic-tive value of using both structured and fine-grained invoicing data. We further describe the problems involved in building such a fraud detection system, which are mainly data-related (e.g. data asymmetry, quality, volume, variety and velocity) and deployment-related (e.g. the need for explanations of the predictions made).
 fraud detection; corporate residence fraud; transactional data; structured data
The social contract [30] between governments, citizens and corporations comprises the mutual agreement between these parties on how to allocate resources for common expenses such as road construction, hospitals and the environment.  X 
Enric and Marija contributed equally to this work. Most democratic societies have implemented this social con-tract in the form of a taxation system in which each party agrees to contribute to the total expenditure of the country. Needless to say, the success of such a system depends on the fairness and efficiency and thus the compliance of all actors to the system in place. Falsifying or withholding informa-tion in order to limit the amount of tax liability is therefore against the law and constitutes fiscal (or tax) fraud. This is a large-scale problem that affects a multitude of entities: the public sector, the private sector and citizens [25]. Fiscal fraud exists in several forms, which can broadly be catego-rized as evasion of direct (income and corporate tax) and indirect (VAT) taxes. Governments are a frequent target of fraudsters, who undermine the system and abuse its bene-fits, grants and tax programs.

In Belgium, fiscal fraud is acknowledged as a significant problem. The State Secretary for Fraud in Belgium even stated that  X  X raud is as Belgian as beer and fries X  [9]. Esti-mations by the European Commission show that the Belgian government loses about e 30 billion annually due to fiscal fraud, which corresponds to 6% of its GDP [9], placing Bel-gium X  X  among the highest fraud rates in Western Europe. On a larger level the overall European losses due to tax evasion and avoidance are estimated to be an astonishing e 1 trillion [12]. These numbers show that the fight against fraud is a crucial aspect of any fiscal system. Not only does fraud cause serious damage to society, it also has a direct fi-nancial impact on individuals. The relevance of fraud detec-tion in the current climate of severe fiscal consolidation and social hardship is motivated in the declaration of the G20 leaders of September 2013. In this statement, they empha-size the importance of ensuring that all taxpayers pay their fair share of taxes as well as the need to tackle tax avoidance, harmful practices and aggressive tax planning [27].
Since most tax systems use audits to ensure compliance with tax laws, an accurate selection of the most likely fraud-ulent cases is crucial to maintain an efficient tax inspec-tion. Given this urgent need to identify specifically the most suspicious cases, the Belgian government joined forces with academia to work on automated data mining systems that look for fraud patterns in large amounts of data to detect corporate residence fraud. This type of fraud occurs when companies deceitfully attempt to place their residency in a low-tax country in order to avoid paying the higher taxes of their real location. The data consists of two types of records: on the one hand we have structured data on the Belgian companies (sector, city, etc), on the other hand we have transactional data (invoicing logs) between Belgian and foreign companies. Although using this fine-grained trans-actional data can be tricky, the information that could be retrieved from it is very valuable in order to detect residence fraud. Consider the following (fictitious) example: let X  X  say we see that a foreign company receives invoices from a golf club in Brussels. This could be an indication that the com-pany and its owner(s) likely reside in Belgium. If this is indeed so, other foreign companies that also receive invoices from this specific golf club make for interesting suspects. Working at such a fine-grained identifier level makes avail-able very informative data [28].

The potential of data mining techniques has also been ac-knowledged by governmental entities, including the Belgian government. In their action plan to strengthen the fight against tax fraud, the European Commission articulates it as follows:  X  X or tax administrations, the development and full use of automated tools and risk management techniques would release human and budgetary resources to concentrate on achieving targeted objectives. X  [11] The rest of the paper is structured as follows: In the next section, a literature overview is given on the importance of fraud detection, the different types of fraud, and the the main domain challenges. Section 3 looks deeper into the type and size of the data and Section 4 describes the spe-cific methods that were used. Section 5 shows the results and the deployment, with concluding remarks in Section 6.
As discussed above, the Belgian government is a frequent target of fraudsters. Abuse of the tax system is a very costly fraud type [25], with estimates of losses going into the bil-lions of euros (dollars, pounds) for the EU, US and UK gov-ernments. Translating these numbers to impact on members of society is an easy exercise. For instance, Belgian estimates reveal that fraud against the public sector is estimated to be e 30 billion per year and thus directly costs every adult in the country about e 2,700 annually.

As mentioned before, the elementary form of damage from fraud in government-allotted resources is an immediate fi-nancial loss and thus the unfair redistribution of wealth. Note, however, that the consequences can be much broader. Fraud losses could result in cuts to thinly spread government-budgets, tax increases, less investment in the public sector (such as new roads, hospitals, schools, etc.) and eventu-ally a slower economy altogether. Effective fraud detection, on the other hand, can lead to many benefits. Not only is there the direct impact of recovering parts of the loss of cap-ital, increased effectiveness can also lead to enhanced deter-rence [1]. That is, the increased likelihood of being captured, causes the net expected benefit from the fraudulent activ-ities to be outweighed by their (proportionally increased) expected costs, thus decreasing the appeal of such fraud. Needless to say, governments try hard to cope with ever-more creative fraud-schemes such as the ones addressed in this project.
In the literature, data mining has been applied to many domains for fraud detection. Some of them include the bank-ing sector for discovering fraudulent credit card transactions or card applications [3, 6, 19, 33, 39], identifying fraudulent service subscriptions or calls in the telecommunications do-main [8, 14, 15, 17], detecting false insurance claims [29], re-vealing websites with high level of non-intentional traffic for online advertising [35] or uncovering tax evasions in the pub-lic sector [2, 16, 41] and etc. A comprehensive overview of the complete fraud detection literature is beyond the scope of this paper (see [4, 26, 29]).

Many of the fraud detection studies need to deal, similarly to our work, with heterogeneous types of data and especially large amounts of transactional data. The applications are mainly in the banking and the telecommunications sectors, where the companies keep logs of card transactions and calls. Due to the high dimensionality of the transactional data, a very common approach in the literature is to perform some type of aggregation over the transactional data. One way to do so is to create transaction aggregates for each user account that characterize the typical legitimate behaviour of the user [5, 15]. Any new transaction that deviates from the typical behaviour of that user would be suspected as fraudulent. Other studies [3, 19, 39], take the approach of deriving RFM (Recency, Frequency, Monetary Value) at-tributes from the original features over a period of time. The RFM attributes are then used as inputs for a classifi-cation technique. Aggregating the transactions creates new structured data and loses the fine-grained information that is included in the transactions (cf., the golf club example from Section 1).

To our knowledge, there have been only few studies in the prior literature that take into account the information from very high-dimensional categorical attributes, especially the identifier attributes described by Perlich and Provost [28]. These attributes can represent particular identifiers as the companies accounts in our case, particular names of locations or persons and etc. The work of Fawcett and Provost [14, 15] incorporates such attributes by first search-ing for individual classification rules based on the transaction-level data (such as location in cell phone calls), and then building higher-level features based on these rules. The studies by Brause [6] and Sanchez [33] include these at-tributes by using classification based on association rules on transactional level applicable to smaller datasets. Cortes et al. [8] and Stitelman et al. [35] both employ a graph rep-resentation and apply relational inference on the networks defined among persons connected if they call each other [8] and among browsers connected if they visit the same web-site [35]. Our study explores and combines fraud data on both levels: we apply scalable algorithms to extract fine-grained knowledge from huge amount of transactional data and also consider the structured data. By doing so, we are able to harness the predictive power of both types of data, as well as the added value of combining them.

For the purpose of tax evasion, data mining has been ap-plied to the problem of corporate fraud [7, 20], where compa-nies falsify their financial statements, as well as Value Added Tax (VAT) evasion [2, 16, 41], solely on structured data.
Typical challenges encountered when applying data min-ing techniques in the domain of corporate residence fraud detection relate to positive label scarcity and quality. Addi-tionally, due to the way in which the data is generated nowa-days, we also encounter problems related to Big Data with respect to size (volume), type (variety) and speed of data generation and stationarity-violation (velocity). Further-more, the acceptance by stakeholders of the resulting models is highly dependent on their comprehensibility, which needs to be taken into account both during and after the modeling phase.

Data scarcity: Fraud data are usually highly unbalanced , as there are many more non-fraudulent instances than the number of fraudulent ones. Furthermore, limited resources and the very expensive labeling procedure (auditing) fur-ther bias the class balance. Moreover, one often encounters pollution of the data labels: data instances can have wrong labels if a fraudulent instance has not yet been discovered and therefore is marked as a legitimate one. Additionally, very little structured data is available on the foreign com-panies (except for the country where they are located).
Volume, variety and velocity: Every quarter, the gov-ernment receives millions of tax data entries containing hun-dreds or even thousands of transactions as well as structured data on each of the companies involved in these entries. As such, not only do the datasets have very large volume ,the size also continues to increase. Even so, this is not the only issue related to velocity . Fraudsters are known to change the way in which they commit fraud in progressively more creative and covert ways to evade the detection systems in place. This adversary effect requires continuous back-testing and updating of the models because stationarity assump-tions might be violated. Needless to say, when taken as a whole, the datasets coming from our domain need fast algo-rithms that can cope with these challenges.

As mentioned before, the government receives both tax declarations as well as transactional data. Furthermore, the government has a database with additional information on each of these companies. Ideally, one wants to connect all these various bits of information in order to obtain the best predictions. Unfortunately, it is not trivial to do so in a sensible way. For instance, how could one combine transac-tional logs (e.g., foreign company FC 1 transferred money to a golf club) with a geographical location? Possible answers include hierarchical modeling, ensemble methods and stack-ing; clearly, this situation opens up many possible paths of model combination and design. To the best of our knowl-edge, we are the first to propose a solution for this corporate governance problem.

Comprehensibility: The success of a tax fraud detec-tion system depends on more than accurately flagging sus-pected cases. Each suspected case is sent to an investiga-tor who determines whether it is indeed fraudulent and col-lects evidence. As each investigator develops his/her own expertise on tax fraud, this expertise can conflict with the predictions. If investigators receive many cases they see as clearly non-suspect, they might reject the prediction system altogether. When the system however explains why a case is flagged as suspect, investigators can quickly determine whether this is in line with their experience or not. Further, in a confirmed match situation, the explanation provided by the system can serve as a starting point for the actual Figure 1: The structure of the invoicing net-work based on the incoming and outgoing transac-tions between the fraudulent foreign companies (red squares) and the Belgian companies they interact with (grey nodes). As can be seen from the big cluster, many of the fraudulent foreign companies are connected to the same Belgian companies. investigation. Thus a model that is comprehensible at the instance/decision level is critical both to get user acceptance and to speed up the manual investigation.
Before we can dig into the modelling approaches for this domain, we must first discuss the exact data available to us. Although we received data from various sources, we can discern two main types of records. First we have invoicing records between 2,745,478 Belgian companies and 873,640 foreign companies ( transactional data, T ). Second, we also have structured information on each of the Belgian compa-nies ( structured data, S ).

Transactional data: In terms of transactional invoic-ing data, we can distinguish between two types of invoices: incoming invoices from foreign companies to Belgian com-panies, and outgoing invoices from Belgian to foreign com-panies. We engineered three different datasets from these invoice logs: a dataset of incoming invoices, a dataset from outgoing invoices and a third dataset where we merged both the incoming and outgoing invoices. Additional statistics for the datasets are shown in Table 1. There can be multiple Belgian companies.
 Incoming Outgoing Incoming and Outgoing transactions between two companies, on different dates or with different amounts of money. Hence in Table 1, both the total number of transactions and the number of unique transactions between the companies are shown. The latter counts only the transactions where the sender/recipient pair is unique. Note that this transactional data can be repre-sented both as a matrix and as a bipartite graph.
In the matrix representation each row i corresponds to a foreign company; column-values indicate whether the for-eign company made a connection to resident company j , with entry x i,j equalto1and0otherwise.A bipartite graph (bigraph) is a graph that has two types of nodes and edges exist only between nodes of different type. A subset of the bigraph containing all of the fraudulent nodes is visualized in Figure 1 with red squares representing the fraudulent for-eign nodes and the grey nodes representing Belgian com-panies they interact with. Figure 2 shows the degree dis-tributions (number of transactions) of the Belgian-foreign bipartite company networks.

These graphs help us to understand the power of the fine-grained data in the modeling results presented below. Al-though keeping the full fine-grained data instead of aggre-gate values can be tricky to work with, previous studies have shown fine-grained transaction data to enhance the predic-tive power of models [18, 24, 28]. This is partly due to the fat tail in the degree distribution we see in Figure 2: many companies appear related to only very few other com-panies, but these low-connectivity companies make up the vast majority of the companies. Thus, it is relatively dif-ficult to compress the company-related information into a small number of simple aggregate variables (that do not ob-scure the fine-grained connectivity information). Figure 1 in turn shows that the fraudulent foreign companies indeed do seem to interact with the same Belgian companies, as illustrated by the big cluster in which most fraudulent com-panies are found. Thus, it makes sense to intelligently X  X .e., in a supervised fashion X  X xamine the specific companies in the predictive modeling (note that it is informative both to be connected to one or more suspicion-inducing companies as well as to be connected to one or many suspicion-reducing companies; cf., [28]).
Structured data: Most of the available structured infor-mation is on residential companies because, to date, there is still no sharing of information between governments. This asymmetry in data is one of the challenges to overcome on the level of policy making. For each of the Belgian com-panies we have information on their geographical location, industry type, start-up date, etc. For foreign companies, we only know in which country they are located as well as the target label. As shown in Figure 3, we can infer certain aggregate characteristics for the foreign companies, based on what the average Belgian company that connects to it looks like. 1 For the particular foreign company shown in the figure, we can deduce that its average transaction value is a certain amount and that its usual geographical corre-spondence location is located in Brussels (median region in Belgium). These characteristics can be added into the input vector in order to augment the prediction information. This set-up leads to a total of 31 features per foreign company.
An important problem that arises in our scenario, due to limited resources and the very expensive labeling proce-dure, is skewness in the distribution of the target variable. Out of the total 873,640 available foreign companies, only 62 are marked as positive cases. Because of this skewness, we makeuseofAUCandliftcurvesandwerepeateachofthe experiments 10 times on different out-of-sample selections to ensure robustness and the external validity of the results.
Given the variety and volume of the data, different feature engineering and modeling techniques are first applied and subsequently combined. In this section we first describe each of the different methods briefly after which we discuss their combination, displayed in Figure 4.
In the structured learning scenario, we are interested in predicting whether or not a foreign company is fraudulent, based on the aggregate, structured information of the asso-ciated resident companies. This turns out to be a classical predictive modeling set-up in which we predict target vari-able y based on vectors of structured data x , one for each for-eign company. To deal with the many-to-one variables, such as location, which appear in the transaction data, we en-code them in the structured data via a  X  X eight-of-evidence X  encoding; this is a logarithmic transformation that allows one to transform a categorical variable into a variable that is monotonically related to the target variable [36, 37]. For example, if most of the Belgian companies connected to a foreign company are located in Brussels, this will be encoded as a one in the position of the dummy-encoded X  X russels X  X o-cation variable. Examples of structured variables include the location of the linked company (up to town level), the main activity code of the linked company, and the legal construct typeofthelinkedcompany(withatotalof31suchvari-ables). Once the features have been engineered into a struc-tured input vector, we train an SVM with a linear kernel. SVMs are known to work well with these kinds of data [32] and the choice of kernel is motivated by the need for com-prehensibility of the model (more on this later).
Due to the sensitivity of the data, all of the examples given in the figures are only illustrative; aggregate results and statistics are of course computed on the true data.
The transactional data can also be represented by vectors as follows: for each of the n foreign companies we look up its previous associations with companies in Belgium. Each of the m Belgian companies is represented by a feature and the value of this feature in the foreign company X  X  m -dimensional vector x will be equal to one if such a connection was made and zero otherwise. By aggregating all of these vectors we end up with a very high-dimensional, but highly sparse, ma-trix. There are two main approaches of handling this kind of data: (a) applying propositional learners (such as SVMs and Naive Bayes) on the huge, sparse matrix representation and (b) using relational learning/inference on the graph rep-resentation.

A first approach is to gather all of the data in a big matrix and apply SVM (using the LibLinear package [13]). Clearly, due to the size of the data, this will take quite a while on a standard desktop computing set-up. Further, it likely will not perform very well due to class imbalance, as explained by Wallace et al. [38]. Indeed, poor performance is revealed in the very low AUC and lift values of this approach (SVM Table 1). As a first improvement, we train the SVM on a balanced subset of the data. By equally weighing the num-ber of positive and negative examples, the SVM learns to put equal importance on each of the classes and performs much better (SVM T (50-50)). Other improvements toward this end, could be to directly optimize for a different loss function [31]. An in-depth discussion on this matter is be-yond the scope of this application-focused paper. In a similar vein, we also apply a binary Bernoulli Naive Bayes (NB), specifically tailored for massive, sparse, binary data [18]; let X  X  call that  X  X ig Bayes. X  This classifier uses the same input vectors x , but makes an estimate based on the MAP likelihood estimation of a probability parameter for each of the features. These are gathered in a vector with elements  X  j = P ( X j = x i,j | C = c ) and used in a  X  X aive X  model, where all features are assumed to be conditionally independent of each other, given the class, resulting in the following probability estimate for each class (i.e., fraudulent or not): In this formulation, fraud is encoded by class label C ,and the x i,j indicate whether a transaction was made from for-eign company j to resident company i . A decision is made by comparing the probability estimate for the fraudulence of the company ( C = 1) and the non-fraudulence ( C =0). The NB modeling procedure does not suffer from the class skew problems of the SVM. The Big Bayes modifications for massive, sparse data involve only having to process the non-zero elements of the huge matrix [18]; NB does not need any further modifications to be run efficiently on the fine-grained data.

Intuitively, it makes sense to apply a learner that is specif-ically tailored for the networks resulting from transactions like the ones described in Section 3. In order to do so, we must first realize that such transactional logs between two final step as well into a stacked model. entities (foreign and resident companies) can be represented as a bipartite graph. The visualization already suggests ( X  X wo-hop X ) assortativity in fraud status in the network of foreign companies.

Numerous relational learners exist for graphs with only onetypeofnodes. Inordertomakeuseofthem,wecan apply the three-step framework for classification within bi-graphs proposed in [34]. The idea is to project the bigraph into a unigraph (graph with only one type of nodes) in which foreign companies are connected, based on shared Belgian company connections and then apply a relational learner. By additionally assigning weights to the edges in the pro-jection, more information from the underlying bigraph can be preserved [34]. The resulting classification decision is then based on the posterior probability, defined as: Equation (1) presents the weighted-voted Relational Neigh-bor (wvRN) inference method [21]; with wvRN, the class probability of a node in the graph (a foreign company) is equal to the weighted average probabilities of all of its neigh-bors ( j  X  N ( x i )). A neighbor is defined as a node that is linked to the node that is being investigated in the projected graph (in this case identified by a one in the x vector). As mentioned before, such a connection is made only if two foreign companies share a resident company. Equation (2) shows that the weighting (the similarity between two top-nodes) was chosen as a sum over the tanh of the inverse of the degrees of the shared nodes. That is, if say a Belgian company has connections with all of the foreign companies, this company will define a relatively low weight in the to-tal probability calculation. If it does not, it will likely be more informative and thus should be weighted accordingly. These design choices are based on the results of the extensive experimental study on a wide variety of publicly available transactional datasets conducted by Stankova et al. [34].
Ideally, we want to build a model that incorporates all of the available information. As one can see from the previ-ous sections, it is not trivial to combine these heterogeneous types of data because they require different sorts of models. One way to cope with this problem, while still preserving the variety of modeling approaches is to combine the mod-els in a stacked model . The expected efficacy of such a model is explained by the fact that we are incorporating more in-formation into one model than we did before, which should result in a lower modeling bias [40].

In our scenario, as the stacked model we use a linear SVM to produce a final model that is a linear combination of the output scores of the transactional classifiers and the struc-tured model. An important reason for this particular design is that we do want to keep a maximum level of comprehensi-bility without sacrificing too much predictive performance. Specifically, the 31 variables of structured data are man-ageable to a human observer, but the millions of transac-tions clearly are not. It is much more informative to have the scores of these models encoded as variables X  X  human inspector can assess the contribution of the network-data component. Should this be high enough, specialized tech-niques can be used to inspect the underlying reasons for the predictions of the network-data component (as discussed in the next section).

Figure 4 summarizes all of the steps required to generate the complete, stacked model. First, the data is converted to (a) transactional (graph) data and (b) structured data. Next, predictive models are built on top of these data, each specifically tailored to cope with the particular aspects of the corresponding data (as explained in the previous section). Lastly, the scores of the graph models are combined with the structured data as input to the final stacked model.
The results of all of the previously explained methods in terms of predictive power (AUC) are shown in Table 2. The best performance for each dataset is denoted in boldface and underlined. Performances that are not significantly dif-ferent at a 5% confidence level (according to a Wilcoxon signed rank test [10]) are tabulated in bold face. Significant differences at the 1% level are emphasized in italics, and dif-ferences at the 5% but not at the 1% level are reported in normal script. A first observation that one can make from this table is that our best models achieve very high AUC val-ues (up to 96.22%). The somewhat high standard deviations on these percentages can be explained by the class imbalance (detecting one more or one fewer example can result in a percentage change of about 10%). Nevertheless, our results do show that our best model (the stacked combination of structured and relational models; SV M S + T ) performs sig-nificantly better than all of the transactional methods for the incoming and the outgoing data. Although it is still the best performing model for the combination of both data types, the variance is too large to conclude statistical significance at the 5% level using the Wilcoxon test.

Although these results are certainly interesting in terms of global predictive power and ranking ability, we should note that in the specific context of detecting fraudulent compa-nies we are more interested in the lift (how much better than random) when targeting the highest ranking members of the dataset. This is because the fraud analysts investi-gate the companies deemed to be most suspicious. The lift curves (Figure 5) show the clear superiority at the highest percentiles of the models built on transactional data, where they are able to perform up to a few hundred times better as opposed to random targeting. The traditional, structured-data model and the stacked model deliver clear improve-ments as well, but at the highest percentiles the lifts are Table 2: Results of different techniques in terms of AUC. Subscript S refers to models based on struc-tured data. Subscript T refers to models based on fine-grained transaction data. Subscript S + T refers tomodelsbasedonbothstructureddataandtrans-action data.
 not nearly as strong as those resulting from using the fine-grained transaction-based models.

As we motivated previously, we can now observe empiri-cally that the fine-grained information included in the trans-actional data provides substantial gains for detecting fraud-ulent companies. Referring back to our example, the other fraudulent companies that transact with the Brussels golf club receive high transactional fraud scores, and rightly so apparently X  X s demonstrated by the very high lifts. Once these other foreign companies that transact with these suspicion-conferring Belgian companies 2 are investigated, structured data may still help to find other suspects.

In conclusion, we can say that if one is interested in a global ranking method, the stacked model would be the best design choice in our scenario, whereas the models based on transactional data are better suited for detecting the most likely frauds. The latter result highlights the importance of keeping the fine-grained information as a whole as opposed to only aggregating it into summary variables.
In the actual deployment of our model, we have been made aware of the tremendous importance of being able to explain
The Belgian companies themselves are not suspicious per se, but the foreign companies that transact with them are. the decisions made. Specifically, the auditors need to un-derstand the exact reasons why classification models make particular decisions. Cases (even if they be few) where the model makes an obvious wrong decision can create disillu-sionment with the system and reluctance to use it, unless the reasons behind the decision appear to be sound. There-fore, it is essential that the decisions made by the predictive model can be explained; the auditor can decide to over-rule a specific suggestion and confidently move on to the next one. Going back to our running example of a company that hasreceivedaninvoicefromagolfclubinBrussels: Al-though it might be the case that most foreign companies that receive invoices from that entity are indeed fraudsters, a foreign company such as Rolex that has sponsored a golf tournament at this specific golf club (and therefore has also received an invoice) is likely not fraudulently located abroad. So if the explanation for the classification is given (i.e., re-ceiving an invoice from the specific golf club), an auditor can quickly see why it is or is not valid in the context of the particular focal company.

To our knowledge, the distinction between different types of comprehensibility has received relatively little attention in the data mining literature, even though it often is a cru-cial criterion for final acceptance and increased use of the predictive models. At least two types of explanations exist. Global explanations provide improved understanding of the complete model, and its performance over the entire space of possible instances. Instance-level explanations on the other hand provide explanations for the model X  X  prediction regard-ing a particular instance. When using transactional data, the total number of variables and/or data values considered by the model (in our case, millions) is much larger than for the typical structured data. Global explanation methods, such as examining the coefficients of a linear model or us-ing a rule-based model, are simply not applicable in such a high-dimensional context. However, an instance-level ap-proach used for document classification [23], which faces a similar challenge with a large vocabulary, can also be used in this transactional setting: an explanation is defined as the minimal set of entities one received/sent an invoice to, such that removing all the invoices to/from this set changes the predicted class from the class of interest. For our run-ning example, an explanation could be:  X  X f this company did not receive an invoice from golf club XYZ in Brussels, the predicted class would change to non-fraudulent X  . As such, in-stance based explanations provide an excellent tool for mod-els that use the fine-grained invoicing data. For more on how explanations can be used both to improve acceptance and also to improve the model itself, as well as further references to related work, we refer to [23].

Global explanations do still have value, but in a different way. Decision makers need insight into the general methods used by fraudsters and their evolution. One way to do so is to list all variables of the stacked model in order, ranked according to the size of the coefficients in the linear model. Then we could see for example that the country dummies for certain countries are very high on the list, as well as the scores from the transactional models, and certain activity codes. A rule-based model could provide similar insights. These insights may then lead to different sorts of cases being discovered, which then would prime the network models to find similar instances. We are not able to show the actual global explanations, as they involve confidential information. In reference to this project, State Secretary for Fraud John Crombez reported:  X  X he interaction between the two worlds [academia and government] has proven very valuable. Other countries are now visiting Belgium to see how the Social In-telligence and Investigation Service and the Special Tax In-spection service apply this technique. That is why we need to continue to invest in this technology. X  Not only is the pre-dictive performance of our models appreciated, but also con-sidered to be important to success is the fact that in general use this data mining technique can operate on anonymized data, whereby each company is encoded as a X  X andom X  X um-ber. A company X  X  identity only then needs to be revealed in the context of a particular investigation of a top-suspicion instance. Further, the emphasis on the comprehensibility of the results is deemed essential.

During deployment, the system has to deal with large vol-umes of heterogeneous data and with new data arriving ev-ery quarter, where the underlying data generating process is non-stationary due to the problem being adversarial. The stacked model approach specifically deals with the variety of the data by combining the transactional data from invoices with structural data from tax declarations. The need to re-train the model frequently is facilitated by the scalability of the underlying (naive Bayes and wvRN) methods. They can be run (on a desktop) on the complete data and produce resultsinamatterofminutes.
In this paper we have described what to our knowledge is the first data-mining-based method for building a system for detecting corporate residence fraud. The system is based on transactional and structured data, which is gathered by the Belgian government. The success of such a detection system in practice depends on a combination of factors, in-cluding efficiency, efficacy and comprehensibility. As such, an important part of our research was to evaluate how one can cope with these conflicting requirements. When used for targeting new fraudsters, a combination of the fine-grained transactional data model and instance-based explanations results in a good trade-off between the needs of an audi-tor. On the other hand, combining both structured data and fine-grained data in a stacked model is more suited when the main goal is to gain macro-level insights and policy guidance.
Given the success of this pilot study, we believe further re-search into this application to be a logical next step. There are still many opportunities for improvement. Besides sim-ply improving the modeling methods, one particular aspect that we did not touch upon yet is the pro-active gathering of data with active data-acquisition techniques (see e.g., [22] for a suspicion-scoring application).

It is important to continue to stress the importance of de-ploying counter-fraud measures for the social good of coun-tries. Although our experiments focus on data from the Belgian government, we hope that researchers from other countries are motivated by our results to apply such meth-ods to or to find better methods for their own countries X  data, and/or to convince their governments to do so. It is important for us to understand whether and how data min-ing indeed can improve government fraud detection efficacy and perhaps even policy making. Once we are convinced, then we can work to remove any lingering doubt or scepti-cism among decision makers.
We are very grateful to the Belgian government for allow-ing us to write about our work on this problem and espe-cially for their engagement in the project and their useful feedback. David Martens also thanks the Flemish Research Council (FWO) for financial support (Grant G.0827.12N) and Foster Provost thanks NEC for a Faculty Fellowship. Note that these exact models and techniques are not neces-sarily those used in production. [1] M. H. Baer. Linkage and the Deterrence of Corporate [2] S. Basta, F. Fassetti, M. Guarascio, G. Manco, [3] S. Bhattacharyya, S. Jha, K. Tharakunnel, and J. C. [4] R. J. Bolton and D. J. Hand. Statistical fraud [5] R. J. Bolton, D. J. Hand, et al. Unsupervised profiling [6] R. Brause, T. Langsdorf, and M. Hepp. Neural data [7] M. Cecchini, H. Aytug, G. J. Koehler, and P. Pathak. [8] C. Cortes, D. Pregibon, and C. Volinsky. Communities [9] J. Crombez. Zwart en wit . De Bezige Bij, 2013. [10] J. Dem X  sar. Statistical Comparisons of Classifiers over [11] EUR-LEX. Communication from the commission to [12] European Commission. Fight against tax fraud and [13] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [14] T. Fawcett and F. Provost. Combining data mining [15] T. Fawcett and F. Provost. Adaptive fraud detection. [16] P. C. Gonz  X alez and J. D. Vel  X  asquez. Characterization [17] C. S. Hilas and P. A. Mastorocostas. An application of [18] E. Junqu  X  edeFortuny,D.Martens,andF.Provost. [19] P. Juszczak, N. M. Adams, D. J. Hand, C. Whitrow, [20] E. Kirkos, C. Spathis, and Y. Manolopoulos. Data [21] S. A. Macskassy and F. Provost. A simple relational [22] S. A. Macskassy and F. Provost. Suspicion scoring [23] D. Martens and F. Provost. Explaining data-driven [24] D. Martens, F. Provost, J. Clark, and E. Junqu  X  ede [25] National Fraud Authority. Annual fraud indicator [26] E. Ngai, Y. Hu, Y. Wong, Y. Chen, and X. Sun. The [27] Organisation for Economic Co-operation and [28] C. Perlich and F. Provost. Distribution-based [29] C.Phua,V.Lee,K.Smith,andR.Gayler.A [30] J.-J. Rousseau. The Social Contract, Or Principles of [31] C. Rudin. The p-norm push: A simple convex ranking [32] Y. Sahin and E. Duman. Detecting credit card fraud [33] D. S  X  anchez, M. Vila, L. Cerda, and J.-M. Serrano. [34] M. Stankova, D. Martens, and F. Provost.
 [35] O. Stitelman, C. Perlich, B. Dalessandro, R. Hook, [36] L. C. Thomas. Consumer Credit Models: Pricing, [37] L. C. Thomas, D. B. Edelman, and J. N. Crook. [38] B. C. Wallace, K. Small, C. E. Brodley, and T. A. [39] C. Whitrow, D. J. Hand, P. Juszczak, D. Weston, and [40] D. Wolpert. Stacked generalization. Neural networks , [41] R.-S. Wu, C.-S. Ou, H.-Y. Lin, S.-I. Chang, and D. C.
