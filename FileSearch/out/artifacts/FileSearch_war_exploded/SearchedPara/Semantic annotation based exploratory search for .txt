 1. Introduction
A range of modern semantic annotation approaches makes it possible to annotate documents with higher-level semantic features from ontological concepts to named entities (names of people, places, organizations, etc.). Many researchers argue that semantic features are able to better model essential document content, and that their application can improve the user X  X  ability to find and access the right information at the right time. A number of projects confirmed the potential of semantic annotations, applying them at different stages of the information processing and retrieval mechanisms ( Demner-Fushman &amp;
Oard, 2003; Mihalcea &amp; Moldovan, 2001; Wu, He, Ji, &amp; Grishman, 2008 ). The work presented in this paper follows the re-search stream on improving information access through the use of semantic annotation, yet it attempts to reach the same goal from an alternative direction: empowering the user of an information access system through an innovative named en-tity-based user interface for exploratory search.

Exploratory search is described by Marchionini as a type of search  X  X  X eyond lookup X , such as search to learn and search to investigate . Exploratory search assumes that the user has some broader information need that cannot be simply solved by a single  X  X  X elevant X  Web page, but requires multiple iterations of search/analysis interleaved with browsing and analyzing the retrieved information. The research on supporting exploratory search attracts more and more attention every year for two reasons. On one hand, the number of users engaged in exploratory search activities is growing ( Marchionini, 2006 ). With the exponential growth of information available on the Web, almost any user performs searches  X  X  X eyond lookup X  even to plan a vacation or choose the  X  X  X est X  digital camera. Moreover, some classes of users, such as intelligence analysts, perform multiple exploratory searches every day as a part of their job. On the other hand, traditional search systems and engines working in a more simple mode of  X  X  X uery ? list of results X  provide very poor support for exploratory search tasks (Marchionini, 2006 ).
Users have great difficulty formulating effective queries when they are unsure of their information needs. The challenge is compounded when the user is trying to make sense of search results presented only as a linear list.
 Our team investigated the issue of exploratory search in the context of the DARPA GALE (Global Autonomous Language Exploitation) project. Our goal was to develop a more effective information distillation interface for intelligence analysis.
We initially focused on personalized search, expecting that adaptation to an analyst X  X  global task (beyond a single query) would enable our system to produce and bring better results to the analyst X  X  attention. However, user studies performed by our team to evaluate personalized search interfaces (Ahn, Brusilovsky, He, Grady, &amp; Li, 2008 ) convinced us that tradi-tional personalized search is not sufficient to provide the proper level of support in an information exploration context.
First, an extensive analysis of search logs produced by intelligence analysts revealed that query formulation is a major problem. The analysts struggled to bring hidden relevant documents to the surface by repeating various combinations of just a few of the most obvious query terms, while more powerful and less evident terms were never discovered. Sec-ond, on several occasions the analysts asked for an interface that provides  X  X  X ore transparency X  and  X  X  X ore control X  over the search process. Unfortunately, traditional personalized search offers no support for query formulation and no user control over the process. Personalization starts with an already submitted query and works as a black box, which pro-duces a user-adapted list of results without direct user involvement. Inside this black box, the personalization engine ap-plies a user profile either to generate query expansion or to reorder search results (Micarelli, Gasparetti, Sciarrone, &amp; Gauch, 2007 ).

The work presented in this paper attempted to address these problems by exploring an alternative approach to support we attempted to build an information exploration interface that enhances the user X  X  own abilities in all three tasks: query formulation, query expansion, and re-ranking of the results. The key idea of the proposed approach is the application of named entities (NEs), a popular kind of semantic annotation, to present the aboutness of the search results to the users and to allow them to manipulate and explore these results.

The proposed information exploration approach was implemented in NameSieve, an information exploration interface for intelligence analysts and evaluated in a controlled user study. The following sections of this paper present a description of the NameSieve interface along with a detailed account of how it was built and the results of the user studies. We also review similar work and discuss the potential of integrating the new information exploration interface with our other personalized search approaches. 2. Named entities in information retrieval
As a semantic category, named entities (NEs) act as pointers to real world entities such as locations, organizations, people, or events ( Petkova &amp; Croft, 2007 ). Because NEs can provide much richer semantic content than most vocabulary words, they have been studied extensively in various language processing and information access tasks. NEs have been viewed as alter-native information for indexing. Mihalcea and Moldovan (2001) discussed the idea of using NEs for indexing document con-tent, and they found that the size of the index could be greatly reduced while relevant documents still can be retrieved. As the most common type of out-of-vocabulary terms that do not have translations in the dictionary, the translation of NEs have been treated as a serious problem in dictionary-based Cross-Language Information Retrieval (Oard, 2002 ). Demner-
Fushman and Oard (2003) examined the effect of out-of-vocabulary terms, where the majority are NEs, in CLIR through arti-ficial degradation of the dictionary coverage. They find that the performance can decrease by as much as 60% when NEs are removed from the translations. Through review of the search topics and retrieval systems in several years of Cross-Language
Evaluation Forum (CLEF) experiments, Mandl and Womser-Hacker (2005) evaluated the NEs in those topics and their effects on CLIR. They found that the majority of CLEF topics contain at least one NE, and NEs often make retrieval topics relatively easier to obtain than those topics that do not have any NEs. Of course, their assumption is that reasonable translations can be found for these NEs. Wu and others (2008) further examined the effect of special handling of NEs and their translations using
IE technology in task-based multilingual information exploration, and found that significant impact on retrieval effectiveness can be achieved with high quality translations of NEs.

As the research and practice in information retrieval moved from classic ad hoc retrieval scenarios to new challenges and applications, the roles of NEs have been considered more often for specific tasks. For experiments on topic detection and tracking, NEs have been used extensively for modeling the essential features of seminal events and for differentiating be-tween new events and existing ones (Kumaran &amp; Allan, 2004 ), as well as for detecting novelty in documents and events (Yang, Zhang, Carbonell, &amp; Jin, 2002 ). In terms of question answering and multilingual question answering, NEs also are the essential information for representing the needs behind the questions. Pablo-Sanchez, Mart X nez-Fernandez, and
Mart X nez (2005) reported on multilingual NE processing in cross-lingual question answering and in web cross-language information retrieval. Pizzato, Molla, and Paris (2006) proposed using the extracted NE in pseudo relevance feedback for question answering. Although they did not obtain significant improvement by using NEs, they found that the causes are more related to the retrieval measures used in question answering. Khalid, Jijkoun, and Rijke (2008) talked about the effect of normalizing NEs in question answering, and found that even very simple normalization of NEs have a clear impact on the retrieval and answering tasks.

Compared to the related work in the literature, our work is based on the insight that NEs are semantically richer compo-nents for modeling than keywords. Therefore, our NameSieve system extensively uses NEs to represent the content of re-turned documents. However, our research focuses not on indexing or ranking algorithms, but on the support that NEs can provide in the users X  sense-making process. In NameSieve, automatically extracted NEs, categorized into who (people), where (location), when (time) and what (things), are displayed along with the returned documents so that the essence of those documents can be quickly and flexibly explored by the users. 3. NameSieve: named entity-based information exploration system
The key idea behind NameSieve, our NE-based information exploration interface, is to extract NEs from the documents become more transparent to the user: the most critical information (in the form of NEs) contained in hundreds of retrieved documents is brought to light. This helps users to make sense of the search results. Second, by showing the main NEs related to the user X  X  original search terms, the system uncovers critical people, locations, and organizations relevant to the users X  tasks. Visualization allows users to immediately take the main NEs into account for query expansion and formulate new queries.

The second important idea is to complement the transparency achieved by NE extraction with user control. The list of extracted NEs in our system is not just a passive display, but an interface for instant query expansion and re-ranking of the retrieved results. The workflow supported by NameSieve is the following: (1) User starts a new search by entering an initial query. (2) The system retrieves documents using a traditional ad hoc retrieval engine. (3) The system processes the set of retrieved documents, extracts any NEs, and organizes them by their prominence in the (4) The system displays the list of retrieved documents along with the organized list of extracted NEs. (5) The user explores the presented documents and NEs. During this process, the user can select one or more interesting (6) Selected NEs can be instantly added to the original query for a new search. In this case, the process begins again from (7) Given the selected NEs and search terms, the system updates the current list leaving only those of the originally
We used Indri for the baseline search engine in step (2) and implemented our own transparent Boolean filtering on step (7). We also used an advanced NE extractor mechanism developed at the IBM TJ Watson Research Center. Our experience demonstrated that both the quality of NE extraction and the organization of the interface are critical to making this idea work (see Section 4 for more details).

Fig. 1 shows our second-generation NameSieve X  X  interface with an example taken from one of the study tasks (train fire at a ski resort). The user starts with a query  X  X  X rain fire X . The system retrieves a large number of documents and immediately applies the default Boolean post-filtering, returning 254 documents containing both  X  X  X rain X  and  X  X  X ire X . The matching docu-ments are presented in a traditional style: 10 documents per page with document titles and surrogates generated using the sentences containing the user X  X  query terms. Each term in the surrogates is highlighted, acting as a clue to help users under-stand why the corresponding document was retrieved by the baseline search system.

The user can operate with these results using the control area on the right hand side of the screen, which contains three panels: Query Term Panel, Named Entity Panel, and Notebook Panel. The Query Term Panel shows each term in the current query accompanied by the number of documents in the result list containing the respective term. Users can turn a filter on is updated to filter out all documents not containing the term. When a term filter is turned off, all relevant documents will be shown whether or not the term exists in a document. For example, if a user turns off the filter for the query term  X  X  X ire X , the new result list increases to 643 documents. The number of documents increases because the Boolean post-filtering was re-duced from two terms ( X  X  X rain AND  X  X  X ire X ) to one ( X  X  X rain X ). The updated number of documents is displayed again, next to the term in the Query Term Panel.

The Named Entity Panel shown in Fig. 2 is the core feature of the system. The system extracts and displays NEs from the and color of the displayed NEs are determined by their frequency. More frequently occurring NEs in the retrieved documents vated by default, NEs remain unselected waiting for the users to examine and select them based on the user X  X  preference.
When the NE filter selection is complete, the user clicks the  X  X  X pply Filter X  button, and the system returns an updated doc-ument list. The updated list is post-filtered from the original list and includes only the documents that contain all of the se-lected names. This post-filtering process is done immediately on the entire list of documents retrieved from the previous session.

Fig. 2 shows an example of NE manipulation. Starting from the situation displayed in Fig. 1 , the user examines the NE list, selects the important location name  X  X  X alzburg X  and clicks  X  X  X pply Filter X  to narrow down the currently-retrieved list. When the filter is applied with  X  X  X alzburg X , the number of documents in the list is reduced to 27, and the list of NEs is updated accordingly. The user examines the updated NE list and decides to learn about the connection between the Salzburg gover-only 3 documents remain in the list to be examined in details by the user. The selected filters can be turned off again any-time, so that the search process using the NE filters is as flexible as possible.

To help users remember which NE filters are turned on within the four tabs, the number of selected NEs is displayed and the tab background changes to yellow. The label of the active tab, Who ,in Fig. 2 (left) is rendered in red dark yellow (background), because the user selected the NE  X  X  X ranz Schausberger X . On the Where tab label, we can see that there is another selected name, a location name  X  X  X alzburg X . In order to distinguish itself from the active tab, the background is rendered in light yellow. Below the box, all selected NEs are displayed in a smaller font size followed by the count, giving the user an overview of the exploration process outcomes. 4. Named entity extraction and processing
As we found out during our work on the project, both the power of the mention detection stage and the quality of the post-processing stage are vital to the success of the overall approach presented in this paper. While the intent of our ap-proach was clear from the very beginning, we had to explore several detection mechanisms, go through several major refine-ments of the post-processing pipeline, and run two user studies to achieve a quality of NEs which was meaningful for the users and which can significantly impact their work.

The most recent version of NameSieve and the study presented in this paper used a powerful mention detection anism developed by IBM ( Florian et al., 2004 ). It is based on a statistical maximum-entropy model that recognizes 32 types of named, nominal and pronominal entities (such as PERSON, ORGANIZATION, FACILITY, LOCATION, OCCUPATION), and 13 types of events (such as EVENT_VIOLENCE, EVENT_COMMUNICATION). This mechanism was used to annotate every docu-ment in the TDT4 corpus 3 loaded into NameSieve. After that, we post-processed the annotation results to select the most useful entities and to reorganize them into four groups corresponding to four of the five  X  X  X s of journalism X  ( Who, Where,
When, and What )(Wikipedia, 2009 ), which are also frequently used in intelligence analysis. The following subsections pro-vide a summary of the mention detection approach and the post-processing used in the presented version of NameSieve. 4.1. Mention detection
The goal of the mention detection task is to identify and characterize the main actors in a document: the people, the loca-mining who did what where to whom. Its applications are widespread, from information extraction and template filling, to search and information retrieval, to machine translation and data mining.

Given a sentence, our goal is to identify spans of text (words) that refer to a set of pre-defined types such as persons, orga-nizations, locations, dates, or countries. The identification of these non-overlapping and contiguous chunks of text converts into an equivalent problem of labeling each word in a sentence with a tag corresponding to the mention it belongs to (if any), as follows: The token is not part of any mention  X  outside of any mention (usually O).
 The token begins a mention type X (B-X).
 The token is properly inside a mention of type X (I-X).

The B-X label type is necessary only to separate adjacent mentions of the same type, such as the case presented in Fig. 3  X  where several different mentions of type PERSON are directly adjacent. Such mention encoding is called the IOB represen-go back and forth between the two representations. Historically, tagging models are preferred to the chunking type of mod-els, mainly due to their relatively straightforward and efficient search procedure  X  the Viterbi dynamic-programming search, to be briefly described later. The first instance of such transformation was presented by Ramshaw and Marcus (1994) , where the authors applied the IOB transformation procedure to the task of base-noun phrase chunking. Later, this method was ap-2002).

When detecting mentions, as is also true for many other natural language processing (NLP) tasks, there are many contex-tual, lexical and semantic clues that help in making the classification. Besides the obvious lexical dependency (e.g., John will most likely be a person, while the pronoun we will tend to refer to multiple people or organizations), other decision factors include: part-of-speech information, text chunking information (whether the token is part of a noun phrase, etc.), whether labeled by other slightly different classifiers, etc. In fact, a successful mention detection system will integrate information coming from various and diverse sources; the system described here uses more than six streams of information. Because of our interest in using many knowledge-lean sources, we are examining those statistical systems that can easily and seam-lessly integrate such information. One way to attain this goal is through the use of exponential models; in particular models trained using the maximum-entropy principle.

We are stating the sequence classification described above as the following problem: given a sequence of n words x (a sentence), find the classification sequence y 1 ... n which maximizes the probability P  X  y can be computed by using the chain rule:
In the approximation above, we have made the regular Markov assumption that the classification y sification y i depends only on the classification at the previous step y tuple (y i-k ... i 1 )by z i 1 and use this new variable instead).

Following the preference of allowing the modeling probability P(y here an exponential model: and y i (for instance, x i is John and y i is B-PER, or x a normalizing factor that ensures that the above equation defines a proper probability. The parameters  X  k ciated with the model features; higher values should be associated with the better features. These parameters can be trained using the Maximum-Entropy principle  X  the description of the method is beyond the scope of this article, but the interested reader can read more about the training procedure (Zitouni, Luo, &amp; Florian, 2008 ).

Once the probabilities P(y i |x 1 ... n ,y i 1 ) have been computed, one can use dynamic programming to compute the best se-quence of tags by observing that
Indeed, if we use the notation a i ( y j )= P(y 1 ... i,j
The computation requirement for this matrix is linear in n |Y| , where n is the number of words in the sentence and Y is the which each of the max in Eq. (2) happens and chaining them.

The mention detection system used in this article predicts 32 types of mentions, including persons, organizations, locations, substances, and geological objects, and 13 types of events, including business, communication, disaster, and sport events. Of these, only nine types are used (Table 1 ). It also predicts for each mention whether it is a named (e.g. John Meyer), nominal (e.g. company) or pronominal (e.g. he) mention. The features used by the system to predict mention types include part-of-speech tags, text chunks (whether the word is part of noun phrase, verb phrase, prepositional phrase, etc.), and whether the word is included in precompiled dictionaries of people, organizations, locations, etc. While the features themselves are language-specific, the model infrastructure is not, and models using the same framework have been built for English, Chinese, Arabic, Spanish, and Italian ( Florian et al., 2004 ). Table 2 shows a break-down of the performance of the English system for persons, organizations, locations and overall per-formance across all recognized types, as typical for the task  X  showing precision, recall, and their harmonic mean, the F-measure. 4.2. Mention processing
The mention detection mechanism presented in Section 4.1 was applied to all documents in the TDT4 corpus to produce what can be called  X  X  X aw annotations X . Appendix A and B each show an example of the original text and the raw annotation for document ZBN20001113.0400.0019, which is ranked first in the example in Fig. 1 . Each line under the h ENT i tag of this example represents a single entity found in the document text. It includes information such as entity type, location in the text, co-reference information, and textual representation found in the document. Out of 11 fields returned by the mention detection, NameSieve uses Entity Type (column 1), Co-Reference Information (column 8), and Entity Text (column 10 and 11). The first column (Entity Type) identifies the category of each mention such as PERSON, LOCATION, and ORGANIZATION.
For example, Salzburg is a  X  X  X OCATION X , United Kingdom is a  X  X  X OUNTRY X  name, and  X  X  X ki_lovers X  are  X  X  X EOPLE X . While the tagger supported numerous kinds of mention types, their distribution is not even. As Table 1 shows, 16 mention types occu-pied over 90% of the entity instances in TDT4 corpus. Therefore, we decided to use those 16 top entities only. Among them, seven mention types were excluded because of low relevance for the name based browsing in NameSieve. The excluded en-tity types are depicted in italic font in Table 1 . For example, CARDINAL types are just some casual numbers found in news articles and EVENT_COMMUNICATION types are verbs used for communications, such as  X  X  X aid X .

We then assembled these nine remaining entity types into four Ws ( Who, Where, When, and What ) and present them to the user via the tabbed browsing interface (Fig. 2) so that users can work with the entities at a higher semantic level (e.g. WHO ) and do not have to worry about minor differences among entity types (e.g. PERSON or PEOPLE).
 The mapping from the entity type to the 4 W X  X  was as in Table 3 .

The mention detection mechanism also performs co-reference resolution for the identified entities, linking pronominal and nominal instances with their named antecedents (if they have one), and identifies and classifies relations between the discovered entities. This co-reference information (column eight in Appendix B ) could be used for name disambiguation.
For example, the entities  X  X  X ki-lovers X  and  X  X  X ho X  (line 127 X 130) were annotated the same in the co-reference information column (ZBN20001113.0400.0019-E75).  X  X  X ho X  is a relative pronoun that refers to the  X  X  X ki_lovers X  and we could see that this annotation made sense in that it referred to the same group of people. As in this example, if the tagger was able to iden-tify the different textual expressions as identical named entities, they are given the same co-reference information in column eight. Therefore, we could use that as a unique ID for the semantically unique entities (for example, we could treat  X  X  X ki_lov-ers X  and  X  X  X ho X  as unique entities with an identical entity ID ZBN20001113.0400.0019-E75).

In addition to this  X  X  X ithin-document X  co-reference, the mention detection also supports  X  X  X ross-document X  entity refer-ence, which was annotated as  X  X  X DC X  in the same column. For example, XDC:Cntry:United_Kingdom (line 6) can be under-stood as a unique entity meaning  X  X  X nited Kingdom X  regardless of its form (United Kingdom, UK, or She) across all documents, because it was consistently represented as XDC:Cntry:United_Kindom in the whole corpus. Another example is the person name  X  X  X olfgang Schussel X  which appeared four times in Appendix B (line 148 X 151) with four different forms:  X  X  X chussel X ,  X  X  X irector X ,  X  X  X hancellor X , and  X  X  X im X  (last two columns) but with the same entity representation  X  X  X DC:Per: wolf-gang_schussel X . We could disambiguate these three different textual representations as a unique person X  X  name even across multiple documents, thanks to the cross-document reference information. The cross-document reference information here was also used as entity IDs as in the within-document co-reference information.

NE co-references within-and cross-document provided by an advanced mention detection mechanism allow NameSieve to merge various textual representations of the same NE and support browsing on a semantic level (i.e., the level of concepts meaningful for the user). Technically, document number, entity ID (co-reference information), and their frequencies are stored in the NameSieve database after the disambiguation process. Using this information, NameSieve generates its tab-based faceted browsing interface with proper NE names and counters. The only part of this process that deserves separate explanation is the selection of the best human-readable mention for each NE when presenting it to the users (e.g.  X  X  X ki Lov-ers X  instead of  X  X  X ho X ). While the cross document reference IDs are ready to be presented to the users after a minor heuristic-based post-processing (e.g. XDC:Cntry:Germany to Germany), the within-document entity reference IDs are not in human-readable forms at all (e.g. ZBN20001113.0400.0019-E75). We can look up the original text (column 10), such as  X  X  X ki_lovers X , but the problem is that they are mixed up with nouns, pronouns, relative nouns, and we have no further clue to select the optimal text representation from among them. Our first idea was to simply select the longest mention, but this method per-formed inconsistently. We then decided to use an external resource for this stage of disambiguation and chose Wikipedia as a dictionary. Because we are able to understand each Wikipedia entry title as a  X  X  X oncept X , we compared every possible tex-tual representation of a single entity with the Wikipedia titles and picked one if any of them matched one of the titles. By using this method, we were able to remove noisy textual representations of the entities that do not appear as Wikipedia en-try titles.

Following is the Wikipedia-based algorithm used in the presented version of NameSieve: (1) List the candidate entity variants from the annotated entity text (column 10). (2) Remove stop words from the candidates. (3) If Where, When, What types (4) If Who entity, choose the longest one among the candidates.

We did not use Wikipedia for Who entity types because many non-celebrity person names frequently found in news arti-cles cannot be found in the encyclopedia (and instead could be confused with names of irrelevant celebrities); on the other hand, we expect to find place or organization names in Wikipedia.

For the When tab, we used a different process to normalize the entities, which are useless in the context of NE browsing each news article in the TDT4 corpus, we could simply convert these relative time entities to fixed dates using simple heu-ristics (for example,  X  X  X oday X  to  X  X  X ov 12, 2000 X  or  X  X  X his year X  to  X  X 2000 X ). 5. The study of NameSieve
To assess the usefulness and the value of a NE-based exploratory search interface, we ran a user study of NameSieve. In our study, we wanted to assess two aspects of the approach. First, it is important to determine the usability of the approach.
While potentially powerful, NameSieve X  X  additions make the search interface more complicated and may discourage users from applying the approach. Therefore, the first group of questions we wanted the study to answer is  X  X  X ill the users apply
NameSieve X  X  functionality when faced with an information exploration task? X  and  X  X  X ill the users appreciate NameSieve fea-tures and the whole experience of searching with an extended system? X  Analysis of logs and user X  X  subjective feedback pro-vided the answers to these questions.

Second, it is important to know the effectiveness of the approach. Thus, we needed to answer such questions as  X  X  X ill the extended system provide better ranking bringing relevant documents closer to the user attention? X  and  X  X  X ill the extended system help users find higher quality results and be more productive, as measured by users X  selections and annotations? X 
These questions were harder to answer since performance evaluation requires a controlled study and an evaluation frame-work with a set of information exploration tasks and ground truth (i.e., information on which documents and their fragments contain content relevant to each task.)
The need to answer these two groups of questions defined our selection of the study format: a controlled user study eval-uating NameSieve X  X  impact on user performance and attitude against a baseline system without NameSieve X  X  functionality.
We used the same evaluation framework (He et al., 2008 ) and the same kind of users  X  students in the Information Sciences who had solid search experience. Note that the selection of students as study subjects is an inherent limitation of our study: it does not allow us to generalize the findings to both professional users (such as intelligence analysts) and inexperienced users ( X  X  X a X ve searchers X ). However, we believe that our subjects provide a good representation of non-professional, yet expe-rienced Web searchers who (along with target users such as intelligence analysts) might also benefit from information exploration interfaces such as NameSieve. 5.1. Hypotheses and measures The goals of our study could be formalized as the following set of hypotheses.
 H1: At the objective level, the experimental system (NameSieve) performs better.
 H2: At the subjective level, users prefer the experimental system over the baseline system.

With these hypotheses in mind, we organized a study (within subject) as a comparison between the experimental system , which included a full-fledged version of the NameSieve interface as presented in Section 2 against the baseline system : a dis-abled version of NameSieve without filtering functionality or a NE viewer (Fig. 4 ). This version simply performs the base search function triggered by user queries and has no support for the query reformulation, or NE and query-based filtering.
This organization allowed us to not only ensure that the NE-based interface is used and appreciated by the user, but also to uncover any differences in the value of the new interface on several levels, such as system performance, user performance, and user subjective feedback.

We adopted several tools and measures to support the hypotheses. We mainly analyzed the log data collected during the experimental sessions in order to support the first hypothesis (H1). In order to test the hypothesis H1-1, we counted the number of times the NE features were used by the subjects. This side of the analysis helped us to understand whether the NE features were favored by the users before analyzing the real advantages provided by NameSieve. For the hypothesis
H1-2, we evaluated the precision of the retrieved lists returned by the systems (system precision). By comparing this mea-sure between the baseline and the experimental systems, we evaluated the quality of the ranked lists generated during the interaction between the users and the systems. We assume that the users can help the experimental system to generate bet-ter ranked lists by using the NE filters.
 Hypothesis H1-3 addresses the precision of the notes annotated by the subjects during the experiment (user precision).
The subjects were asked to select relevant passages or sentences they found and to save them to the Notebook (or a shoe-ument surrogates of the retrieved lists or from the full-text documents. This feature was supported by both NameSieve and the baseline system. Because the notes are saved as passages, we defined a passage level precision measure and used it for comparing the precision of passages selected by the subjects. The detailed descriptions of the measures are provided in Sec-tion 5.2. For the second hypotheses (H2-1 and H2-2), we used questionnaires and gathered the subjects X  subjective opinions.
All the evaluation results are presented from Sections 5.5 X 5.8 . 5.2. Materials and procedures
Our study design follows the methodology developed as a part of the task-based information exploration (TBIE) evalua-tion framework. The framework is constructed to examine systems in task-based information exploration ( He et al., 2008 ). It shares ideas with human-centered system design in the literature (Borlund, 2003; White, Kules, Drucker, &amp; schraefel, 2006 ), where supporting human users in their tasks is both the focus and the criterion for examining the usefulness of the systems.
The TBIE evaluation framework utilizes task scenarios that simulate the actual tasks of analysts. Under the overall umbrella of task-based information exploration, users X  exploration behaviors can be categorized as first information foraging  X  then sense-making  X  for collecting useful information as part of a complex and evolving task.

The TBIE framework provides a test reference collection that was developed from the topic detection and tracking collec-tion (TDT4). It contains 28,390 English documents and 18 task scenarios that were expanded upon from available TDT topics.
To assess retrieval effectiveness with the help of TDT4 X  X  original relevant document set, we had two human annotators go through the collection to markup passages inside each document for a given topic. The passages were annotated based on how relevant they were to the topic. In total, 1916 documents were examined with respect to their relevance. The relevance annotation produced, on average, 644.4 highly relevant passages, and 230.5 slightly relevant passages per topic. The novelty annotation produced on average 82.4 highly novel passages, and 118.3 slightly novel passages per topic. We obtained mod-erate inter-annotator agreement in Cohen X  X  Kappa coefficient. We think that this is because annotations at the passage level are extremely difficult ( Allan, 2003 ). The annotation files are independent from the source data (TDT4 collection) and can be used by anyone interested in running similar studies. In this study, we only used the relevance part of the ground truth.
The framework recommends some evaluation metrics, which includes performance-oriented measures like passage pre-cision of selected passages, and the usability measures about the systems X  support, particularly those examining the inter-actions between the users and the systems. Example measures include the efficiency of selecting useful information and users X  subjective comments.

In this study, we adopted two measures for evaluation: system precision and user annotation precision, and they were used to test the system performance and the user performance (H1-2 and H1-3, respectively). System precision represents the ability of the system to present relevant documents in a returned ranked list during the interaction between the system and a user. Here the system precision is calculated on a ranked list at the document level. Since we are interested in how well the system pushes relevant documents to the top of the ranked list, the calculation of precision is at rank five and rank 10.
For example, if a returned ranked list has four relevant documents in the top five and six relevant documents in the top 10, the precision at five is 4/5 = 0.8, whereas precision at 10 is 6/10 = 0.6.

The user precision was calculated at the passage level because the user X  X  task is to select passages. Passage precision is calculated using formula (3), which is derived from a passage precision calculation (Allan, 2003 ). In formula (3), oll character length of the common text chunk between the snippet i and the corresponding ground truth; w of the ground truth combining the two annotators X  mark-ups and the weight could be one of five ad hoc assigned levels: Here the 0.5 associated with nml i is the penalty weight.

Two topics were selected from the 18 task scenarios that the TBIE framework provides: 40001 (Galapagos Oil Spill) and 41012 (Trouble in the Ivory Coast). As an example, the details of 40001 can be found in Appendix C . 5.3. Data collection
Ten subjects recruited from the University of Pittsburgh X  X  School of Information Sciences (SIS) participated in the exper-iment between December 14, 2007 and January 21, 2008. To ensure that the subjects can serve as surrogate information ana-lysts in the study, they were required to be native English speakers with professional training in information science and advance knowledge of information retrieval (i.e. at least a 3-credit course in the subject). Eight of the ten subjects were in graduate-level programs at SIS, while the remaining two were in the undergraduate program. Four of the ten subjects were female and the age range of all subjects was 20 X 36. To further recreate the information overload situation faced by professional analysts in real life, the subjects had to perform search tasks under considerable time constraints.
The experiment was conducted in one 90 min session, consisting of a 15 min training on the experimental and baseline systems, two 20 min search tasks, 20 min for completing snippet annotation and post-task questionnaires, 10 min for breaks, and 5 min for a post-session interview. The 15 min training included demonstrations of both versions of NameSieve (5 min) and a practice search task using the experimental system (10 min.) While the subjects were already familiar with the base-line system, they were not familiar with the features of the experimental system. Thus, the practice task ensured that sub-jects had some level of proficiency with the experimental system X  X  features before working on their two search tasks.
During each search task, subjects were given a one-page task description providing a brief background to the topic sce-nario and a list of questions to answer. They were instructed to search for relevant articles in the collection, analyze them, and select useful passages that provided answers to the questions in the task description. At the end of each search task, subjects annotated each snippet with the number(s) of the question(s) to which the snippet provided useful information.
Subjects then completed a post-task questionnaire to assess their level of satisfaction using NameSieve for the task. Finally, after both tasks were completed, subjects filled out a brief exit questionnaire assessing their interactions with the experi-mental system X  X  query and named entity filtering features vs. the baseline system. The order of the two systems and the two topics were randomized among subjects to control possible learning effects. 5.4. User activities analysis
Before going into the main analysis and the hypothesis testing, we examined basic descriptive statistics about user activ-ities. Tables 4 and 5 show the average number of queries and notes made by the subjects, and compare them by the system and the topic. On average, 12.05 queries were issued and 16.15 notes were saved by the subjects. There was almost no dif-ference between the systems in terms of the query and the note count. However, we can observe that the subjects issued a higher average number of queries with the topic 41012 than 40001 (13.6 vs. 10.5). There is a similar tendency with the num-ber of notes made by the subjects. The subjects saved a higher average number of notes with the topic 40001 than 41012 (17.9 vs. 14.4). This data gives an interesting hint about the topic complexity. We could easily imagine that the users might have issued more queries and saved fewer notes when they were working on the more complex topic, rather than the sim-pler one. This tendency is repeated in the performance analysis, in Sections 5.6 and 5.7 . 5.5. Named entity filter usage
The first question of our study was whether NameSieve X  X  named entity exploration functionality was appealing enough to the subjects to be used for their exploratory searches. The answers to this question were quite positive. While NameSieve X  X  interface was reasonably complicated and new to all subjects, they used post-filtering 42 times in total (5 times on average among the subjects who used the NE filtering feature at least once). Among 10 users, five used the filters more than 5 times during the search sessions, three used it less than 5 times, and two users did not use the filters at all.
The division of the Named Entity Panel into four tabs helped us to collect usage data for each NE type. We were able to count how many times the subjects switched these tabs (Table 6 ) and how many entities were activated per each tab when frequently clicked tab was What (31) and Where (24). Even though Who was used least frequently (10), this tab was dis-played initially by default; therefore, 10 actually indicates the number of times the subjects returned to the Who tab after using some other tab. The number of entities applied during the filtering (second row) coincides with this observation.
The most frequently used entities were from the Who tab (30) and the least from the When tab. This data may be understood as evidence of users X  interest in the NE feature provided by NameSieve and supports the hypothesis H1-1. 5.6. System performance analysis
The second question of this study is whether a search system equipped with named entity exploration functionality could better support users in finding relevant information (H1-2). For search systems, system performance is traditionally assessed by its ability to place relevant documents high in the ranked list of search results. Thus, to compare the performances of experimental and baseline systems we need to consider ranked lists of results obtained by the user when working on the same exploration task in both systems and check which system is better able to  X  X  X ush X  task-relevant documents to the among the top 5 and top 10 documents. As discussed in Section 5.2, we have ground truth information on the topics used in the experiments and were able to easily calculate the task-level precision of each ranked document list returned by the sys-tems. Note, however, that a comparison of the NameSieve interface with a baseline interface is not as straightforward as a comparison between two regular search systems. The problem is that NameSieve changes the nature of the retrieval and ranking process, turning it from a traditional one-stage to a two-stage process. In the first stage, the user issues a regular query and observes the list of results and extracted NEs. In the second stage, the user selects one or more NEs to post-filter and re-rank the original set of results. Thus, to examine the effect of NameSieve, we need to distinguish between ranked lists pected that NameSieve would deliver better results after post-filtering, it is hard to expect that its performance on the first stage would be better than the performance of a baseline system since the query formulation and search stage in NameSieve does not differ from baseline system. Moreover, we might expect that the first stage performance of NameSieve would be worse, since the users  X  enabled with powerful post-filtering  X  would become less careful when formulating the original query. Therefore, we separated calculated NameSieve performance for first-stage lists (experimental without NE filters en-gaged) from the second-stage lists generated using the experimental system with the NE filters.

Fig. 5 shows the comparison of the system performance in terms of document level precision at rank 5 and 10 during the experiment. The results confirm our expectations. The experimental system with engaged NE filters (rightmost column) demonstrated nearly perfect performance with average precision 1.0 at rank 5 and 0.99 at rank 10. This precision was sig-nificantly higher than the average precision achieved without post-filtering by both the baseline system and the experimen-tal system without post-filters engaged (Wilcoxon rank sum test, p &lt; 0.01 and p = 0.02 for rank 5 and 10, respectively). It supports our hypothesis that the use of NameSieve X  X  visualization and post-filtering interface significantly improves system performance for information exploration tasks. As we expected, we also observed a slight decline in the experimental sys-tem X  X  precision on stage one in comparison with the baseline system. While this difference appeared to be insignificant, it could hint that users gradually become  X  X  X ess careful X  with their first-stage query formulation in NameSieve. While we found no formal evidence in favor of this hypothesis, we think that this issue needs further exploration.

The differences in the experimental system X  X  performance with and without NE filters are reminders that the mere pres-ence of new features in a system does not automatically make the system more efficient. The user needs to actively imple-ment the advanced features to obtain better system performance. The voluntary nature of the NE interface (it is left to the user to use filters or not) may limit the impact of the system in a practical context, since system performance for the users who choose not to use the NE interface (2 out of 10 users in our study) will hardly be improved.

We also analyzed how the performance changed during the interaction between the subjects and the system. Each ses-sion lasted 20 min, and we could record the changes in the system X  X  performance as the session progressed. Fig. 6 shows the changes in the system X  X  performance (precision at rank 10) between the first and last half of the sessions (10 min each). Both the baseline and the experimental (without NE) systems (black and gray columns) show that performance decreased in the last half of the session (the likely cause is that it was getting incrementally harder to discover new relevant documents using queries), while the experimental system X  X  performance (white columns) improved (probably as a result of uses gaining expe-rience in using NameSieve). As discussed before, the experimental system without using NE filters behaved identically to the baseline and the pattern of the performance change was the same as that of the baseline system. The experimental system (white columns) with NE filters showed improved performance over the baseline (black and gray columns); the difference was statistically significant in the last half of the session (Wilcoxon rank sum test, p = 0.01).
 5.7. User performance analysis
In addition to the system level performance analysis, we were also able to calculate the precision of the users X  annotations during the experimental sessions and to test our hypothesis H1-3. The passage precision score here was calculated against the ground truth, and the detail was described in Section 5.2. Fig. 7 compares user performances by topic (40001 and 41012)
Even though the overall precision in user annotations may look slightly better for the experimental system than the baseline, the difference was not statistically significant. However, when we examined this statistic separately by topic, we found that subjects working on the more difficult topic, 41012 (Table 7 ), created better annotations with the experimental system (0.75 vs. 1.0) during their 0 X 10 min periods (first half session), and this difference was statistically significant (Exact Wilcoxon rank sum test, p = 0.01). While the average data make it appear that they performed slightly worse using the experimental system for the simpler topic, 40001, this difference was not statistically significant.
 This fact was encouraging because the subjects showed significantly improved performance with the more complex topic.
In addition, they were able to create better annotations in their first half sessions, which means their annotating behavior was very efficient. The reason why the overall user performance using the experimental system was not as good as the sys-tem X  X  performance might be described as follows. The system supports NE filter-based exploration and provides improved retrieval lists as revealed by the system performance analysis. However, the document surrogates and the news text pro-vided by NameSieve from which users were asked to make annotations were not different from those provided by the base-line. As the previous section shows, the current version of NameSieve can increase the user X  X  ability to locate relevant documents, but provides no advantage in comparison with the baseline in locating the right fragments inside these docu-ments. Therefore, the difference in user performance between the two systems may have decreased despite the initial sup-port of the improved ranked lists provided by NameSieve. 5.8. User feedback analysis Following each search task, subjects were given a post-questionnaire to assess their satisfaction with the version of
NameSieve assigned to them for that task. For all questions, subjects were asked to rate their level of agreement from one (not at all) to five (extremely). At the end of the questionnaire, subjects were given the opportunity to write any addi-tional comments they had about NameSieve or the preceding search task, in general.

For the experimental version only, subjects were asked to rate the utility of the features related to filtering and NE view-ing: the ability to filter search results by query terms; display of the document counts for each query term; displaying the to display higher-ranked NEs; relevancy of NEs to results of queries; and display of the ranks of NEs via scroll-over pop-up text. For both systems, subjects were asked to rate their familiarity with the assigned topic; the sufficiency of news provided; utility of the document summaries in the search results; their ability to find useful passages; the system X  X  ease of use; and overall satisfaction with the system. After both search tasks were completed, subjects filled out an exit questionnaire asking them to compare the utility of the experimental version X  X  filtering tools vs. the baseline system, and were interviewed for 5 X  10 min to further explain their impressions of NameSieve.

Based upon their questionnaire responses (Table 8 ) as well as oral and written comments, subjects had an overall positive opinion of the experimental system X  X  NE features. Six of the 10 subjects noted that larger font sizes for higher-ranked named entities, grouping named entities by Who/What/When/Where , and query-term filtering were all very helpful in locating important information. Four of the subjects also noted that simply viewing the NEs gave them a better sense of the unfamil-iar topic assigned to them, helping them to construct queries that yielded relevant snippets. Three subjects also liked the highlighting of query terms in the result snippets, and asked that the same highlighting be applied to full articles.
Three of the five subjects who completed their first search task on the experimental system also said that they wished that NE features had been available to them for their second task. This feedback helps to explain the mean post-task ques-tionnaire responses to questions pertaining to the utility of the NE features shown in Table 8 . While subjects X  question re-sponses were not as positive as their oral and written comments, there was a noticeable pattern in responses by subjects assigned to the experimental system for the first task (Sequence 1) vs. those for their second task (Sequence 2). Table 8 shows that subjects who tried the experimental system after the baseline (i.e., Sequence 2), expressed much more positive opinions about the NE faceted browsing interface main features, especially for its key functionality, NE filtering. Although
Chi-squared tests indicated the differences in responses were not significant, it hints that the subjects were not able to fully appreciate faceted browsing with NE until they could compare their experiences solving realistic tasks with and without this interface.

Chi-square tests were performed on the questionnaire data to determine if there were any significant differences in sub-ject responses between the two versions of NameSieve. Table 9 shows the mean post-questionnaire responses by system to questions applicable to both systems. While there were no significant differences between users X  subjective ratings of the baseline and experimental systems, the consistently positive ratings for the experimental system suggest that NE features were helpful additions to the baseline search system, despite the features X  relative novelty and the sequence effect shown in Table 8 .

This sequence effect is also confirmed by answers to the exit questionnaire, which was administered after the subjects had worked with both systems. The assessments of the NE features in this questionnaire ( Table 10 ) were higher, on average, than those in the post-task questionnaire about the experimental system (Table 8), mainly because Sequence 1 subjects pro-vided a much more positive opinion about NE features after trying to work with the second topic without them. In addition, we can hypothesize that more experience in working with the kinds of tasks we used in the study helped the subject to better appreciate more advanced NE features. This indicates that better training should have been provided on both the base-line and experimental systems, so subjects could become familiar with the capabilities of both prior to the start of the exper-iment. A longer user study with multiple tasks on each system could also have allowed subjects to use the NE features more often and to appreciate them more. 6. Related studies
By the nature of the application area and the key technology used in NameSieve, the system belongs to two intersecting areas: information systems for intelligence analysis and exploratory search systems. This section provides a brief review of the most similar works in these two areas.

Intelligence Analysis is one of the most challenging human information processing activities and requires an analyst to provide extensive information support efficiently in a given context under time constraints. A range of information systems have been developed to support intelligence analysts over the last decade. The best framework for understanding and com-paring a multitude of existing systems was suggested by Pirolli and Card (2005) and its use for a systematic review of tools for intelligence analysis was demonstrated by Card (2007) . The Pirolli-Card framework recognizes two major stages in the work of intelligence analysis  X  information foraging and sense-making  X  and several smaller overlapping subprocesses. The goal of the foraging stage is to assemble a rough collection of resources focusing on recall rather than precision (not to miss important things). The goal of the sense-making stage is to  X  X  X ake sense X  of the collected information; extracting facts, reg-ularities, and forming ideas and theories. Since early foundational work on sense-making ( Russell, Stefik, Pirolli, &amp; Card, the context of the Pirolli-Card framework, NameSieve can be classified as an information foraging tool, which uses informa-tion visualization to help analysts assemble this rough collection (sometimes called a  X  X  X hoebox X ). We can name a few similar tools that use information visualization and were specifically developed for intelligence analysis (Card, 2007; Luo, Fan, Yang, brief review below). NameSieve is different from all these tools in its NE-based approach to visualize and explore a set of documents. While the use of NEs in intelligence analysis has been explored by a few other projects (Bier, Card, &amp; Bodnar, 2008; Gersh, Lewis, Montemayor, Piatko, &amp; Turner, 2006 ), these projects use NEs for sense-making, while NameSieve uses it for information foraging.

As an exploratory search tool, NameSieve combines the ideas of two research streams. One aspect of NameSieve is the visualization of the conceptual content of a retrieved set of documents. In this aspect, it is similar to other systems that at-tempt to visualize search results based on keyword-level content, including such classic systems as Tilebars ( Hearst, 1995 ) sidered as an expansion of the idea of result clustering allowing multiple clustering by NE. Each NE serves as a cluster label and can instantly call up a cluster of documents related to this NE.

The idea of extracting and visualizing NEs in the retrieved set of documents is an extension of our own work on making which was influenced by the modern approach to present tag clouds in social tagging systems. While working on this project, we discovered a few other approaches driven by the same idea: extracting and visualizing information from the list of search results. Kuo, Hentrich, Good, and Wilkinson (2007) suggested extracting keywords from the returned documents and pre-senting it in the form of a tag cloud. WordBars3 system (Hoeber, 2007; Hoeber &amp; Yang, 2008 ) extracts the top 20 keywords from retrieved snippets and allows the user to specify the importance of these keywords and re-filter the results. The project presented in this paper differs from the works mentioned above in several aspects: the breadth and depth of information extraction, the opportunities to use the extracted information for interactive exploration of the results, and  X  most impor-tantly  X  our attempt to move from keyword representation to the semantic level by using NEs.

Another aspect of NameSieve is the use of recognition-based browsing (rather than recall-based search) to allow users to explore a collection of documents. The idea of repeated narrowing of the filtering of retrieved documents by clicking on ex-tracted NEs was inspired to some extent by the modern stream of work on faceted interfaces. In that sense, extracted NEs can be considered as replacements for facet labels when no metadata is available. Since the early work on faceted search ( Yee, schraefel, 2008 ) and faceted web search ( Kules &amp; Shneiderman, 2008 ) were recognized and explored as effective tools for exploratory search. Within the area of faceted interfaces, NameSieve belongs to a small group of systems that attempts to
Ailamaki, &amp; Lohman, 2008 ). In this group, NameSieve is distinguished by its use of NE categories for facet organization. 7. Conclusions and future work
In this paper, we presented a non-traditional approach to building a better information access system using named enti-ties, a popular type of semantic annotation. The proposed approach was implemented in the NameSieve system, which at-tempted to support the information exploration work of an intelligence analyst. NameSieve transparently presents a summary of search results in the form of an NE  X  X  X loud, X  while allowing the analyst to further explore the results using this cloud as a faceted browsing interface. The goal of NameSieve was to help the user in sense-making, query formulation, and manipulating search results. Our study demonstrated that we achieved some of our goals. The new interface was actively used and positively evaluated by the subjects. It enabled them to bring most relevant documents closer to the surface and achieve better performance working with a more difficult topic.

While the study provides some strong support in favor of NE-based exploratory search, its findings should not be general-ized beyond its limitations. Most importantly, our study used surrogate intelligence analysts as subjects. While we make all attempts to recruit users as close to the target users as possible and to place the users in a comparable information overload context, we cannot make any claims about professional intelligence analysts X  performance or attitude to the presented inter-face. Another limitation of the study is that subjects had a relatively short time to master a relatively sophisticated NE-based exploratory search interface. This was our concern before the start of the study and we attempted to address it by providing some training to help the subjects to familiarize themselves with the new features. Yet, the apparently more positive user feedback about NameSieve when used for the second task hints that users need more experience with both the system and the kind of tasks used in the study to fully appreciate and exploit the innovative interface. While we can speculate that the benefits of the NE interface will increase as users gain more experience using it, a much longer user study (which we are planning to perform in the future) is required to state this reliably.

Finally, the search task performed by the users in our study cannot be considered a fully exploratory search task due to the presence of reasonably clearly-defined questions to answer. Evaluating an exploratory search interface using this kind of task limited our ability to explore the true value of this interface. However, as we explained above, the choice of task was motivated by the presence of detailed ground truth data, which allowed us to compare NameSieve X  X  interface with a tradi-tional search interface on a fine-grained, reliable basis. We consider this choice a compromise between realism and exper-imental control. For a good discussion of the tasks that can be used to properly evaluate exploratory search interfaces, we refer the reader to Kules and Capra (2008) .

While the focus of this paper is the innovative interface for NE-based exploratory search, we want to stress that the use of both an advanced mention detection mechanism (one that is able to detect and co-reference multiple mentions of the same
As we mentioned above, an earlier experiment with the NameSieve interface built upon a simpler NE extractor without co-referencing failed to show the benefit of NE browsing. It demonstrated that unresolved NEs are more confusing than helpful to users.

The NameSieve interface presented in this paper was based on a specific kind of semantic annotation; however, we be-concepts. In our future work, we hope to explore this opportunity as well.

In a broader context, it is important to observe again that in this work we switched from AI to HCI techniques to provide improved support for information exploration tasks. However, our long-term goal is to combine AI and HCI approaches to get
In our future work, we intend to combine the ideas of user-controlled personalized search explored earlier ( Ahn et al., 2007, 2008) with NE-based information exploration. Personalization should extend the power of an NE-based exploration inter-face. In turn, this interface could extend the bandwidth of user modeling, enabling us to maintain better knowledge and interest models of the users.
 Appendix A. Full text example (document ZBN20001113.0400.0019)
Salzburg, Austria, Burg introduce the state governor said yesterday that alpine tunnel fire a total of 165 train passengers, driver also died. Shiu, at a press conference Burg said that there were 122 passengers from the bodies had been confirmed dead, including 52 members of the Austrian, 42 Germans, 17 Japanese and eight Americans. In addition, a spokesman for the city the Deli Kaprun when fire Saturday in 3-kilometre long tunnel burning, another train from Tanzania Kitts Horn down-ward movement on the Peak, the train is in the delivery to participate in the competition ski hill returned to the peak, inside the empty. Death toll was more than 154 young ski lovers who he said that as a driver were killed in fire, bringing the total number of deaths to 154 people. For the Austrian national mourning day the biggest disaster. Tunnel fire extinguished, Aus-tria rescue personnel entered the tunnel began yesterday in the burning of the train to find the bodies of victims. These are young ski lovers. Relief workers fear that train was already damaged cable snapped and to train 3,200 metres from the mountains fall down, so that hampered the search for victims. The car caught fire in the skiing and ski lovers, many people only more than 10 years old, is now known only 12 individuals from fled in the vehicle and another 9 people in the tunnel on the peak population of the deadly gas inhalation and were injured. Some of whom 14 are Germans, the Austrian. Kaprun city residents about incidents of shock, they wonder what caused the fire, and that the cycle of train cars on why there was no fire-fighting equipment. Rescue officials said Miller, is to determine the cause of the accident is too early, but he acknowl-edged that train managers are suffering from the fire occurred in a surprise. He said:  X  X  X e do not think that there be such, we are very alarming. X  Experts also to the cause of the accident puzzled because train to the peak of the engine Dai Donggang rope, towed the base X  X anzania Huo Enshan, cars were no engines. Miller 456, electrical faults may be the cause of fire, smoke or vehicles used for cooking gas cylinders goes wrong. He said:  X  X  X e must await the results of the investigation  X  X . Tan-zania Kitts drive the train is Huo Enshan built in 1974, six years ago had a better and bring the two new types of cars and technology. Trains to transport 1,500 passengers per hour from valley station to peak. According to Kaprun Area Tourism director, said Kosovo, the train had never been an accident. When built, this is the first train was pulling for wearing a moun-tain road tunnel. Experts in an interview with television also said that over the past has been assumed that tracks and train system is suffering from the fire could not have occurred. Austrian Chancellor Schuessel on Saturday night phone call to Brit-ish Prime Minister Tony Blair, told him that the victims were tourists from the United Kingdom and Germany. Many victims who might come from the ski Kaprun, perennial Skiing tourism here has become an important local economic sources.
Appendix B. Named entity annotation example (document ZBN20001113.0400.0019) 1 &lt;DOC&gt; 2 &lt;DOCNO&gt;ZBN20001113.0400.0019&lt;/DOCNO&gt; 3 &lt;ENT&gt; 4 LOCATION NAM 107 114 107 114 FALSE ZBN20001113.0400.0019-E0 Mention-0 Salzburg Salzburg 5 ORGANIZATION NAM 126 129 126 129 FALSE ZBN20001113.0400.0019-E1 Mention-2 Burg Burg 6 COUNTRY NAM 3208 3221 3208 3221 FALSE XDC:Cntry:United_Kingdom Mention-156 United_Kingdom 7 COUNTRY NAM 3227 3233 3227 3233 FALSE XDC:Cntry:Germany Mention-157 Germany Germany 127 PEOPLE NOM 912 921 912 921 FALSE ZBN20001113.0400.0019-E75 Mention-50 ski_lovers ski_lovers 128 PEOPLE NOM 1251 1260 1251 1260 FALSE ZBN20001113.0400.0019-E75 Mention-66 ski_lovers ski_lovers 129 PEOPLE NOM 1466 1475 1466 1475 FALSE ZBN20001113.0400.0019-E75 Mention-74 ski_lovers ski_lovers 130 PEOPLE PRO 923 925 923 925 FALSE ZBN20001113.0400.0019-E75 Mention-51 who who 131 PERSON NAM 624 634 624 634 FALSE XDC:Per:_deli_kaprun_ Mention-39 Deli_Kaprun Deli_Kaprun 132 PERSON NAM 3278 3283 3278 3283 FALSE XDC:Per:_deli_kaprun_ Mention-161 Kaprun Kaprun 133 OCCUPATION NOM 597 605 597 605 FALSE XDC:Per:_deli_kaprun_ Mention-37 spokesman spokesman 134 PERSON PRO 927 928 927 928 FALSE XDC:Per:_deli_kaprun_ Mention-52 he he 135 PEOPLE PRO 1693 1696 1693 1696 FALSE ZBN20001113.0400.0019-E77 Mention-85 whom whom 136 PEOPLE NOM 1741 1749 1741 1749 FALSE ZBN20001113.0400.0019-E78 Mention-90 residents residents 137 PEOPLE PRO 1778 1781 1778 1781 FALSE ZBN20001113.0400.0019-E78 Mention-91 they they 148 PERSON NAM 3080 3088 3080 3088 FALSE XDC:Per:wolfgang_schussel Mention-147 Schuessel Schuessel 149 OCCUPATION NOM 2743 2750 2743 2750 FALSE XDC:Per:wolfgang_schussel Mention-131 director director 150 OCCUPATION NOM 3069 3078 3069 3078 FALSE XDC:Per:wolfgang_schussel Mention-146 Chancellor Chancellor 151 PERSON PRO 3163 3165 3163 3165 FALSE XDC:Per:wolfgang_schussel Mention-153 him him 166 &lt;/ENT&gt; 167 &lt;/DOC&gt; Appendix C. An example task scenario: topic G40001 C.1. Galapagos oil spill Background: San Cristobal is a place near Galapagos Islands in the Pacific Ocean.

Short description of the task:
An Ecuadorian oil tanker has spilled fuel near the Galapagos Islands, affecting the local fishing industry and threatening local beaches and wildlife. Your tasks are to suggest (1) how and where ocean currents may spread the oil, and (2) what ac-tions should to be taken after the oil leak event caused by the oil tanker Jessica near the Galapagos Islands. You should inves-tigate possible support that can be provided by the US, as well as potential legal action against the responsible persons.
From the documents, find snippets of text that contain answers to each of the following questions: 1. On what date did the oil spill occur? 2. Where is the location of the oil spill? 3. What type of vehicle was involved in the event? 4. How many gallons of fuel were spilled? 5. What is the cause of the spill? 6. Who is responsible for the spill? 7. What animals have been affected by the leaked fuel? 8. Who put efforts in relieving the pollution? 9. Any support from other countries? 10. What is the impact on the local people, environment, animals and plant life? 11. What are the penalties/lawsuits for those responsible? References
