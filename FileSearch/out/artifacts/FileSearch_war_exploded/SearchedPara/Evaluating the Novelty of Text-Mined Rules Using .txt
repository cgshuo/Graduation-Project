 In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic dis-tance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule -the more the average distance, more is the novelty of the rule. The nov-elty of rules extracted by the DiscoTEX text-mining sys-tem on Amazon.corn book descriptions were evaluated by both human subjects and by our algorithm. By comput-ing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. Text mining, interesting rules, novelty, semantic distance, knowledge hierarchy, WordNet 
A data-mining system may discover a large body of rules; however, relatively few of these may convey useful new knowl-edge to the user. Several metrics for evaluating the "inter-estingness" of mined rules have been proposed [1, 7]. These metrics can be used to filter out a large percentage of the less interesting rules, thus yielding a more manageable number of higher quality rules to be presented to the user. However, most of these measure simplicity (e.g. rule size), certainty (e.g. confidence), or utility (e.g. support). Another im-portant aspect of interestingness is novelty: does the rule represent an association that is currently unknown. For ex-ample, a text-mining system we developed that discovers rules from computer-science job announcements posted to a local newsgroup [18] induced the rule: "SQL --+ database". requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 A knowledgeable computer scientist may find this rule unin-teresting because it conveys a known association. Evaluat-ing the novelty of a rule requires comparing it to an existing body of knowledge the user is assumed to already possess. 
For text mining [8, 3, 17], in which rules consist of words in natural language, a relevant body of common knowledge is basic lexical sen~antics, i.e. the meanings of words and the semantic relationships between them. A number of lexical knowledge bases are now available. WordNet [5] is a seman-tic network of about 130,000 English words linked to about 100,000 lexical senses (synsets) that are interconnected by relations such as antonym, generalization (hypernym), and part-of (holonym). We present and evaluate a method for measuring the novelty of text-mined rules using such lexical knowledge. 
We define a measure of the semantic distance, d(wi, wj), between two words based on the length of the shortest path connecting wi and wj in WordNet. The novelty of a rule is then defined as the average value of d(wi,wj) pairs of words (wi,wj) where wl is in the antecedent and wj is in the consequent of the rule. Intuitively, the seman-tic dissimilarity of the terms in a rule's antecedent and in its consequent is an indication of the rule's novelty. For example, "beer --r diapers" would be considered more novel than "beer ~ pretzels" since beer and pretzels are both food products and therefore closer in WordNet. 
We present an experimental evaluation of this novelty metric by applying it to rules mined from book descriptions extracted from Amazon.com. Since novelty is fundamen-tally subjective, we compared the metric to human judg-ments. We have developed a web-based tool that allows hu-man subjects to enter estimates of the novelty of rules. We asked multiple human subjects to score random selections of mined rules and compared the results to those obtained by applying our metric to the same rules. We found that the average correlation between the scoring of our algorithm and that of the human users, using both raw score correla-tion (Pearson's metric) and rank correlation (Spearman's metric), was comparable to the average score correlation between the human users. This suggests that the algorithm has a rule scoring judgment similar to that of human users. 
Traditional data mining algorithms are generally applied to structured databases, but text mining algorithms try to 
Figure 1: DiscoTEX rule mined from Amazon.corn "romance" book descriptions discover knowledge from unstructured or semi-structured textual data, e.g. web-pages. Text mining is a relatively new research area at the intersection of natural language pro-cessing, machine learning and information retrieval. Various new useful techniques are being developed by researchers for discovering knowledge from large text corpora, by appropri-ately integrating methods from these different disciplines. 
DiscoTEX [18] is one such system, that discovers prediction rules from natural language corpora using a combination of information extraction and data mining. It learns all information extraction system to transform text into more structured data, and this structured data is then mined for interesting relationships. For our experiments, we have used rules mined by Disco-
TEX from book descriptions extracted from Amazon.corn, in the "science", "romance" and "literature" categories. Dis-eoTEX first extracts a structured template from the Ama-zon.corn book description web-pages. It constructs a tem-plate for each book description, with pre-defined slots (e.g. title, author, subject, etc.) that are filled with words ex-tracted from the text. DiscoTEX then uses a rule min-ing technique to extract prediction rules from this template database [19]. An example extracted rule is shown in Fig-ure 1, where the &lt;comments&gt; slot is predicted from the other slots. For our purpose, we only use the filler words in the slot, ignoring the slotnames --in our algorithm, the rule in Figure 1 would be used in the form "daring love woman romance historical fiction -4 story read wonderful". WordNet [5] is an online lexical knowledge-base of 130,000 English words, developed at Princeton University. In Word-
Net, English nouns, adjectives, verbs and adverbs are or-ganized into synonym sets or synsets, each representing an underlying lexical concept. A synset contains words of simi-lar meaning pertaining to a common semantic concept. But since a word can have different meanings in different con-texts, a word can be present in multiple synsets. A synset contains associated pointers representing its relation to other synsets. WordNet supports many pointer types. The ones we used in our algorithm are: Synonym (life --4 existence), Antonym (front--+ back), Attribute (benevolence -4 good), 
Pertainym i.e. morphological relation (alphabetical -4 al-phabet), Similar (unquestioning -4 absolute), Cause (kill -+ die), Entailment (breathe -4 inhale), Holonym i.e. part-of (chapter -4 text), Meronym i.e. whole-of (computer -4 epu), 
Hyponym i.e. specification (plant -4 fungus), and Hypernym i.e. generalization (apple -4 fruit). 
Several measures of semantic similarity based on distance between words in WordNet have been used by different re-searchers. Leacock and Chodorow [13] have used the neg-ative logarithm of the normalized shortest path length as a measure of similarity between two words, where the path length is measured as the number of nodes in the path be-tween the two words and the normalizing factor is the max-imum depth in the taxonomy. In this metric, the greater the semantic distance between two words in the WordNet hierarchy, the less is their semantic similarity. Lee et al. [14] and Rada et al. [21] have used conceptual distance, based on an edge counting metric, to measure similarity of a query to documents. Resnick [22] observed that two words deep in the WordNet are more closely related than two words higher up in the hierarchy, both pairs having the same path length (number of nodes) between them. Sussna [26] took this into account in his semantic distance measure that uses depth-relative scaling. Hirst et al. [9] classified the rela-tions of WordNet into the three broad directional categories and used a distance measure where they took into account not only the path length but also the number of direction changes in the semantic relations along the path. Resnick [23] has used an information-based measure instead of path length to measure the similarity, where the similarity of two words is estimated from the information content of the least prob-able class to which both words belong. 
We have defined the semantic distance between two words wi and wj as: where P(wl,wj) is the weighted shortest path between wl and wj, Dist(p) is the distance along path p according to our weighting scheme, Dir(p) is the number of direction changes of relations along path p, and K is a suitably chosen constant. 
The second component of the formula is derived from the definition of Hirst et al. [9], where the relations of Word-Net are divided into three direction classes --"up", "down" and "horizontal", depending on how the two words in the relation are lexically related. Table 1 summarizes the direc-tion information for the relation types we use. The more direction changes in the path from one word to another, the greater the semantic distance between the words, since changes of direction along the path reflect large changes in semantic context. 
The path distance component of the above formula is based on the semantic distance definition of Sussna [261. It is defined as the shortest weighted path between wi and wj, where every edge in the path is weighted according to the WordNet relation corresponding to that edge, and is nor-malized by the depth in the WordNet hierarchy where the edge occurs. We have used 15 different WordNet relations in our framework, and we have assigned different weights to different link types, e.g. hypernym represents a larger semantic change than synonym, so hypernym has a higher weight than synonym. The weights chosen for the different relations are given in Table 1. 
One point to note here is that Sussna's definition of se-mantic distance calculated the weight of an edge between two nouns wi and wj as the average of the two relations wi -4r wj and wj -4r' wi corresponding to the edge, re-lation r' being the inverse of relation r. This made the 
Table 1: Direction and weight information for the semantic distance between two words a symmetric measure. 
He had considered the noun hierarchy, where every relation between nouns has an inverse relation. But in our frame-work, where we have considered all the four types of words in WordNet (nouns, adverbs, adjectives and verbs) and 15 different relation types between these words, all of these re-lations do not have inverses, e.g. the entailment relation has no direct inverse. So, we have used only the weight of the relation wi --}r wj as a measure of the weight of the edge between wi and wj. This gives a directionality to our se-mantic measure, which is also conceptually compatible with the fact that wl is a word in the antecedent of the rule and wj is a word in the consequent of the rule. lined in Figure 2. The algorithm calculates the semantic distance d(wl, wj) between two words w~ and wj, where wl is in the antecedent and wj is in the consequent of the rule, based on the length of the shortest path connecting wi and wj in WordNet. The novelty of a rule is then calculated as the average value of d(wi,wj) across all pairs of words (wi, w~) . there are 11 separate hierarchies with distinct root nodes. to their corresponding nouns. In this composite connected hierarchy derived from the WordNet hierarchy, we find the shortest weighted path between two words by performing a branch and bound search. nected by a path. However, we have used 15 different Word-
Net relations while searching for the path between two words --this creates a combinatorial explosion while performing the branch and bound search on the composite hierarchy. time-limit (set to 3 seconds in our experiments) within which we try to find the shortest path between the words wl and wj. If the shortest path cannot be found within the time-limit, the algorithm finds a default path between wl and wj by going up the hierarchy from both w~ and w j, using hypernym links, till a common root node is reached. tance of the default path. For nouns and verbs, the PathVia-
Root function calculates the distance of the path between the two words as the sum of the path distances of each word to its root. If the Rnou, or the P~rb node are a part of this path, it adds a penalty term POSRootPenalty = 3.0 to the path distance. If the Rtop node is a part of this path, it adds a larger penalty TopRootPenalty = 4.0 to the path dis-tance. These penalty terms reflect the large semantic jumps in paths which go through the root nodes/~o~n, Rverb and shortest path method does not terminate within the speci-fied time-limit, then the algorithm finds the path from the adjective or adverb to the nearest noun, through relations like "pertainym', "attribute", etc. It then finds the default path up the noun hierarchy, and the PathViaRoot function incorporates the distance from the adjective/adverb to the noun into the path distance measurement. words in WordNet e.g. abbreviations, names like Philip, do-main specific terms like booknews, etc. We assigned such words the average depth of a word (d~ in Figure 2) in the WordNet hierarchy, which was estimated by sampling techniques to be about 6, and then estimated its path dis-tance to the root of the combined hierarchy by using the 
PathViaRoot function. 4. EXPERIMENTAL RESULTS periments to compare the novelty judgment of human users to the automatic ratings produced by our algorithm. The objective here is that if the automatic ratings correlate with human judgments about as well as human judgments corre-late with each other, then the novelty metric can be consid-ered successful. 235 
High score (&gt;8): 
Medium score (4-6): 
Low score(&lt;2): geography -&gt; nature world 
Figure 3: Examples of rules scored by our novelty measure 
For the purpose of our experiments, we took rules that were generated by DiscoTEX from 9,000 Amazon book de-scriptions, obtained by spidering the w~m. amazon, cam web-site -2,000 in the "literature" category, 3,000 in the "sci-ence" category and 4,000 in the "romance" category. From the total set of rules, we selected a subset of rules that had less than a total of 10 words in the antecedent and conse-quent of the rule --this was done so that the rules were not too large for human users to rank. Further pruning was performed to remove duplicate words from the rules. For the Amazon.corn book description domain, we also created a stoplist of commonly occurring words, e.g. book, table, in-dex, content, etc., and removed them from the rules. There were 1,258 rules in the final pruned rule-set. 
We sampled this pruned rule-set to create 4 sets of random rules, each containing 25 rules. 48 subjects were selected for this experiment, most of them being students from Com-puter Science or Computer Engineering background. The subjects were randomly divided into 4 groups of 12 each. Each group was asked to score one of the rule-sets, using novelty as the scoring criterion. We created a web-interface for the subjects to rank these rules with scores in the range from 0.0 (least novel) to 10.0 (most novel), according to their judgment. 
One of the rule-sets was used as a training set, to tune the parameters of the algorithm. The 3 other rule-sets were used as test sets for our experiment. For each of the rule-sets, two types of average correlation were calculated. The first average correlation was measured between the human subjects, to find the correlation in the judgment of novelty between human users. The second average correlation mea-sure was measured between the algorithm and the users in each group, to find the correlation between the novelty scor-Groupl Group2 Group3 "Groupl Group2 Group3 ing of the algorithm and that of the human subjects. We used both Pearson's raw score correlation metric and Spear-man's rank correlation metric to compute the correlation measures. We also ran correlation test after removing obvi-ous outliers (i.e. subjects who were negatively correlated to the majority of the other subjects) from the data. Some of the rules scored by our algorithm are shown in Figure 3. Table 2 shows results using the rankings of all the subjects. Table 3 shows results after removing one outlier from Group1, one from Group2 and two from Group3. 
The correlation between the human subjects and the algo-rithm was low for the first rule-set. For the second and the third rule sets, the algorithm-human correlations axe com-parable to the human-human correlations. From the results, considering both the raw and the rank correlation measures, we see that the correlation between the human subjects and the algorithm is on the average comparable to that between the human subjects. From Tables 2 and 3, we can see that removing the obvious outliers improves the correlation values. However, the correlation values among the human subjects and between the human subjects and the algorithm are both not very high, even after outlier removal. This is because for some rules, the human subjects differed a lot in their novelty assessment. This is also due to the fact that these are initial experiments, and we are working on im-proving the methodology. In later experiments, we intend to apply our method to more specific domains, where we can expect human users to agree more in their novelty judgment of rules. 
However, it is important to note that it is very unlikely that these correlations are due to random chance --ex-cept for the algorithm-human correlation values for Groupl, the correlation values considering all subjects are above the minimum significant r at the p &lt; 0.1 level of significance, while the correlation values after removing the outliers are all above the minimum significant r at the p &lt; 0.05 level of significance, by the t-test. 
On closer analysis of the results of Groupl, we noticed that this set contained many rules involving proper nouns. Our algorithm currently uses only semantic information from WordNet, so it's scoring on these rules differed from that of human subjects. For example, one rule many users scored as uninteresting was "ieee society --+ science mathematics", but since WordNet does not have an entry for "ieee", our algorithm gave the overall rule a high score. Another rule to which some users gave a low score was "physics science nature --+ john wiley publisher sons", presumably based on their background knowledge about publishing houses. In this case, our algorithm found the name John in the Word-Net hierarchy (synset lemma: disciple of Jesus), but there was no short path between John and the words in the an-tecedent of the rule. As a result, the algorithm gave this rule a high score. A point to note here is that some names like Jesus, John, James, etc. have entries in WordNet, but others like Sandra, Robert, etc. do not --this makes it dif-ficult to use any kind of consistent handling of names using filters like name lists. In the future, we plan to resolve this problem by using multiple hierarchies with weighting. 
In the training rule-set, we had also noticed that the rule "sea --&gt; oceanography" had been given a large score by our algorithm, while most subjects in that group had rated that rule as uninteresting. This happened because there is no short path between sea and oceanography in WordNet these two words axe related thematically, and WordNet does not have thematic connections, an issue which is discussed in detail in Section 6. 
Much effort has gone into reducing mined rule-sets by ap-plying both objective and subjective criteria of interesting-ness. Klemettinen et al. [11] proposed the use of rule tem-plates to describe the structure of relevant rules and con-strain the search space. Another notable attempt in using objective measures was by Bayaxdo and Agrawal [1], who defined a partial order, in terms of both support and con-fidence, to identify a smaller set of rules that were more interesting than the rest. Sahax [24] proposed an iterative elimination of uninteresting rules, limiting user interaction to a few simple classification questions. Hussain et al. [10] developed a method for identifying exception rules, with the interestingness of a rule being estimated relative to common sense rules and reference rules. In a series of papers, Tuzhilin and his co-reseaxchers [25, 20] argued the need for subjec-tive measures for the interestingness of rules. Rules that were not only actionable but also unexpected in that they conflicted with the existing system of beliefs of the user, were preferred. Liu et al. [15] have further built on this theme, implementing it as an interactive, post-processing routine. They have also analyzed classification rules, such as those extracted from C4.5, defining a measure of rule interesting-ness in terms of the syntactic distance between a rule and a belief. A rule and a belief are "different" if either the consequents of the rule and the belief axe "similar" but the antecedents are far apart, or vice versa. 
In contrast, in this paper we have analyzed information extracted from unstructured or semi-structured data such as web-pages, and extracted rules depicting important re-lations and regularities in such data. The nature of rules as well as of prior domain knowledge is quite different from those extracted, say, from market baskets. We have pro-posed an innovative use of WordNet to estimate the seman-tic distance between the antecedents and consequents of a rule, which is used as an indication of the novelty of the rule. Domain-specific concept hierarchies have previously been used to filter redundant mined rules [6, 4]; however, to our knowledge they have not been used to evaluate novelty quantitatively, or applied to rules extracted from text data. 
An important issue for future research is learning the pa-rameters of the algorithm, e.g. the weights of the WordNet relations, and values of K, POSRootPenalty and TopRoot-Penalty. These constants axe now chosen experimentally. We would like to learn these parameters automatically from training data, by using machine learning techniques. The novelty score could then be adaptively learnt for a paxticu-lax user and tailored to suit the user's expectations. 
Unfortunately, WordNet fails to capture all semantic re-lationships between words, such as general thematic con-nections like that between "pencil" and "paper". However, other approaches to lexical semantic similarity, such as sta-tistical methods based on word co-occurrence, can capture such relationships. In these methods, a word is typically represented by a vector in which each component is the num-ber of times the word co-occurs with another specified word within a particular corpus. Co-occurrence can be based on appearing within a fixed-size window of words, or in the same sentence, paragraph, or document. The similarity of two words is then determined by a vector-space metric such as the cosine of the angle between their corresponding vec-tors [16]. In techniques such as Latent Semantic Analysis (LSA) [2], the dimensionality of word vectors is first re-duced using singular value decomposition (SVD) in order to produce lexical representations with a small number of highly-relevant dimensions. Such methods have been shown to accurately model human lexical-similaxity judgments [12]. By utilizing a co-occurrence-based metric for d(wi, wj), rules could be ranked by novelty using statistical lexical knowl-edge. 
In the end, some mathematical combination of WordNet and co-occurrence based metrics may be the best approach to measuring lexical semantic distance. To the extent that the names of relations, attributes, and values in a tradi-tional database are natural-language words (or can be seg-mented into words), our approach could also be applied to traditional data mining as well as text mining. Finally, the overall interestingness of a rule might be best computed as a suitable mathematical combination of novelty and more traditional metrics such as confidence and support. Some of the problems we faced while using a "generic" ontology like WordNet can be solved by using multiple domain-specific concept hierarchies, if available. 
The main contribution of this paper is that we have in-troduced a new approach for measuring the novelty of rules mined from text data, based on the lexical knowledge in WordNet. We have also introduced a novel method of quan-titatively assessing interestingness measures for rules, based on average correlation statistics, and have successfully shown that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with each other. We would like to thank Un Yong Nahm for giving us the DiscoTEX rules sets on which we ran our experiments. We axe grateful to John Didion for providing the JWNL Java interface to WordNet, which we used to develop the software, and for giving us useful feedback about the package. We are also grateful to all the people who volunteered to take part in our experiments. The first author was supported by the MCD Fellowship, awarded by the University of Texas at Austin, while doing this research. [1] 1%. J. Bayardo Jr. and R. Agrawal. Mining the most [2] S. C. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [3] 1%. Feldman, edRor. Proceedings of the Sixteenth [4] 1%. Feldman and I. Dagan. Knowledge discovery in [5] C. D. Fellbaum. WordNet: An Electronic Lexical [6] J. Han and Y. Fu. Discovery of multiple-level [7] J. Han and M. Kamber. Data Mining: Concepts and [8] M. Hearst. Untangling text data mining. In [9] G. Hirst and D. St-Onge. Lexical chains as [10] F. Hussain, H. Liu, E. Suzuki, and H. Lu. Exception [11] M. Klemettinen, H. Mannila, P. Ronkainen, [12] T. K. Landauer and S. T. Dumais. A solution to [13] C. Leacock and M. Chodorow. Combining local [14] J. H. Lee, M. H. Kim, and Y. J. Lee. Information [15] B. Liu, W. Hsu, L.-F. Mun, and H. Lee. Finding [16] C. D. Manning and H. Schfitze. Foundations of [17] D. Mladenid, editor. Proceedings of the Sixth [18] U. Y. Nahm and R. J. Mooney. A mutually beneficial [19] U. Y. Nahm and R. J. Mooney. Mining soft-matching [20] B. Padmanabhan and A. Tuzhilin. A belief-driven [21] R. Rada, H. Mili, E. Bicknell, and M. Blettner. [22] P. Resnick. WordNet and distribution analysis: A [23] P. Resnick. Using information content to evaluate [24] S. Sahar. Interestingness via what is not interesting. [25] A. Silberschatz and A. Tuzhilin. What makes patterns [26] M. Sussna. Word sense disambiguation for free-text 
