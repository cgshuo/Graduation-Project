 As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and can promote that con-tent by sponsoring it to appear as an advertisement in the users X  live feeds. In this paper, we present a framework that enabled two of the key targeted advertising components for Tumblr, gender and interest targeting. We describe the main challenges encountered during the development of the frame-work, which include the creation of a ground truth for train-ing gender prediction models, as well as mapping Tumblr content to a predefined interest taxonomy. For purposes of inferring user interests, we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6 . 8 billion user posts, with a very limited amount of categorized keywords, and was shown to have superior performance over the baseline approaches. We successfully deployed gender and interest targeting capability in Yahoo production systems, deliver-ing inference for users that covers more than 90% of daily activities on Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% increase in user engagement with sponsored posts in comparison to untargeted campaigns.
 H.2.8 [ Database applications ]: Data Mining Information Systems, Algorithms, Experimentation Data mining; computational advertising; audience modeling c  X 
In recent years, online social networks have evolved to be-come an important part of life for online users of all de-mographic and socio-economic backgrounds. They allow users to easily stay in touch with their friends and family, discuss everyday events, or share their interests with other users with the click of a button. Tumblr is one such social network, representing one of the most popular and fastest growing networks on the web. Hundreds of millions of peo-ple around the world come every month to Tumblr to find, follow, and share what they love. Consequently, the Tum-blr network represents a gold mine of content, comprising around 200 million blogs on different topics such as travel, sports, or music, with 85 million user posts being published on a daily basis. This wealth of user-generated data opens a great opportunity for advertisers, allowing them to promote their products through high-quality targeting campaigns to both blog visitors and blog owners [19].

The prevalent form of advertising on Tumblr is through sponsored posts that appear alongside regular posts in the user X  X  dashboard , the central page for a Tumblr user, dis-playing the most recent posts of followed blogs in the form of a stream. This form of advertising, where advertisements resemble native content in the stream, is often referred to as native advertising . Native advertisements are usually aesthetically beautiful and highly engaging, which typically makes them more enjoyable than regular display ads [4]. Tumblr launched its native advertising product in May of 2012. Since then, the number of advertisers (or brands) on the platform has grown steadily and reached a milestone of 100 advertisers in April of 2013. Moreover, 8 of the 10 most valuable brands are advertising on Tumblr 1 , while sponsored posts have generated more than 3 billion paid ad impressions since the launch of the Tumblr advertising product 2 . How-ever, a huge marketing potential of Tumblr [19] has not been fully exploited, due to the fact that targeting against specific interest and demographic audiences, a targeting component that Tumblr was missing, has become an industry standard and many advertisers are in need of such a solution.
Building interest targeting products on social and mi-croblogging platforms is an important research topic, dis-cussed previously by several researchers [14]. However, due to its distinct characteristics, Tumblr poses novel challenges, which we explain in detail in this paper. In particular, the marketr.tumblr.com , accessed June 2015 www.comscore.com , accessed June 2015 content and language used on Tumblr have distinct charac-teristics that needed to be accounted for during the model-ing. For instance, users often use tags to summarize the text in their posts. However, the language styles used in the tags and post text are different (e.g., the tag  X  X p X  and the word  X  X p X  found in posts have different meanings,  X  X arry Potter X  and  X  X ewlett-Packard X , respectively). Moreover, unlike the popular social platform Facebook, which contains a large amount of social interactions but a limited amount of con-tent, or the microblogging platform Twitter, which contains an intermediate amount of social interaction and content, Tumblr represents a unique combination of a rich and di-verse content platform and a dynamic social network. To make use of this vast advertising potential, in this paper we propose to classify user-generated Tumblr content into a standard multi-level general-interest taxonomy 3 that adver-tisers commonly use for defining their targeting campaigns, opening doors to high-quality audience segmentation and modeling for purposes of ad targeting. However, inferring categories of user posts is a challenging task, given huge quantities of unlabeled data being posted every day and very limited amount of labeled data, typically obtained by edito-rial efforts. To this end, we propose a novel semi-supervised neural language model, capable of jointly learning embed-dings of post keywords, post tags, and category represen-tations in a common feature space. The neural model was trained on a large-scale data set comprising 6 . 8 billion posts, with only a fraction of manually categorized content.
Targeting pipelines described in this paper are being used to show ads to millions of users daily, and have substantially improved Tumblr X  X  business metrics following the launch. We detail our path to developing targeting capabilities for Tumblr, where one of the key steps was creating user profiles, based on users X  activities that include publishing blog posts, following other blogs, or liking posts. Then, we describe how both demographic and interest predictive models were built based on the created profiles.

Lastly, we emphasize that the privacy of users is of crit-ical importance to Yahoo. Therefore, we were constrained in regards to what data we can use. Specifically, user pro-files were created solely from data which users share pub-licly with others, including contents of blog posts, blog titles and descriptions, as well as follow, like, and reblog actions. http://www.iab.net/QAGInitiative/overview/ taxonomy , accessed June 2015 This data is publicly available through Tumblr Firehose data source 4 . Other user activities, such as user searches on Tum-blr, which blogs they visited or where they clicked, are all considered to be sensitive data and were not used in any way for the development of the presented ad targeting models.
Personalization is defined as  X  X he ability to proactively tai-lor products and product purchasing experiences to tastes of individual consumers based upon their personal and prefer-ence information X  [7], and it has become an important and very lucrative topic in the recent years. Personalization of online content may lead to improved user experience and di-rectly translate into financial gains for online businesses [17]. In addition, personalization fosters a stronger bond between customers and companies, and can help in increasing user loyalty and retention [2]. For these reasons it has been rec-ognized as a strategic goal and is the focus of significant research efforts of major internet companies [8, 15].
We consider personalization through the prism of ad tar-geting [11], where the task is to find the best matching ads to be displayed for each individual user. This improves the user X  X  online experience (as only relevant and interesting ads are shown) and can lead to increased revenue for the adver-tisers (as users are more likely to click on the ad and make a purchase). Due to its large impact and many open research questions, targeted advertising has garnered significant in-terest from the community, as witnessed by a large number of recent workshops 5 and publications [5, 9, 14].

One of the basic approaches in ad targeting is to target users with ads based on their demographics, such as age or gender. Historically, this method has proven to work better than targeting random users. However, while for some prod-ucts this type of targeting may be sufficient (e.g., women X  X  makeup, women X  X  clothing, man X  X  razors, man X  X  clothing), for others it is not effective enough and more involved pro-filing of users is required. A popular targeting approach that addresses this issue is known as interest targeting, in which users are assigned interest categories based on their histor-ical behavior, such as  X  X ports X  or  X  X ravel X  [1]. Typically, a taxonomy is used to decide on the targeting categories, and a model is learned to categorize user activities and estimate their interest in each category. Interest targeting is known to gnip.com/sources/tumblr , accessed June 2015 www.targetad-workshop.net , accessed June 2015 build good brand awareness with relevant audience, which has already shown interest in the corresponding category. In this paper we follow this targeting approach. Alternatively, advertisers may be interested to go a step further and opti-mize for current intent as opposed to long-term interest of users, typically done by assigning categories to actual ads, and training a machine learning model to estimate the prob-ability of an ad click in that category [10, 20]. For each ad category a separate predictive model can be trained and evaluated on the entire user population, with N users with the highest score selected for ad exposure.

To the best of our knowledge, the Tumblr social network has been considered by only a handful of scientific studies. In [3, 18] the problem of blog recommendation is discussed, while in [6] the authors explore social norms on the social network. However, our work is the first that addresses an important problem of ad targeting on Tumblr.
Tumblr 6 is one of the most popular social blogging plat-forms on the web today, where users can create and share posts with the followers of their blogs. According to the data from January 2015 7 , there is a total of 221 . 6 million blogs on Tumblr, which jointly produced over 102 . 7 billion blog posts. With a large number of new users signing up every day, it is currently the fastest growing social platform 8
To register a Tumblr account, a valid e-mail address is required, along with a primary username (which becomes a part of the blog URL) and a confirmation of age. Once created, a Tumblr blog contains a profile picture, blog title, and blog description appearing at the top (see Figure 1), followed by a stream of blog posts bellow. The first blog created by a registered user is considered their primary blog. In addition, very small portion of users maintains one or more secondary blogs. A Tumblr user is uniquely identified www.tumblr.com , accessed June 2015 www.tumblr.com/about , accessed June 2015 http://t.co/3txHFRJreJ , accessed June 2015 by a blog ID of their primary blog, and throughout the paper we will use terms  X  X log X  and  X  X ser X  interchangeably.
Common user activities of Tumblr users include the fol-lowing actions: 1) creating a post on one X  X  blog; 2) sharing a post created by another blog, called reblogging (a reblogged post will appear on the user X  X  blog); 3) liking a post by another blog; and 4) following another blog. Similarly to Twitter, follow connections on Tumblr are unidirectional. However, unlike Twitter, users can create longer and richer content in a form of several post types, such as text, photo, quote, link, chat, audio, and video. The posts are shown in user X  X  dashboard, ordered such that more recent posts appear closer to the top. The most popular types of blog posts are photo and text posts, which, based on the analysis published in [21], together cover more than 92% of all con-tent on Tumblr (see Figure 2 for detailed distribution of post types). In addition, any post type can be annotated with words starting with the  X # X  sign (called tags ) that concisely describe a post and allow for easier browsing and searching. Additional metadata that describes a post includes photo captions in photo posts, post titles in text posts, and artists names in audio posts. An example photo post is shown in Figure 3. Tags are displayed bellow the photo caption (e.g., #gadgets and #tech ), while buttons for reblog and like ac-tions are located in the bottom right corner.
Advertising on Tumblr is implemented through the mech-anism of sponsored (or promoted) posts shown in user X  X  dashboard. This is similar to how advertising works on Twit-ter and Facebook. A sponsored post can be a video, an im-age, or simply a textual post containing an advertising mes-sage. In Figure 4, we show an example of a sponsored post and how it appears on desktop and mobile dashboards. Sim-ilarly to organic (or non-promoted) posts, sponsored posts can propagate from user to user in the network by means of reblogs, and users can also  X  X ike X  the promoted post. Both likes and reblogs can be seen as an implicit form of accep-tance or endorsement of the advertising message. Moreover, just like other posts, sponsored posts are supplemented with notes on who liked and reblogged them.
Interestingly, while user-generated, organic posts are re-blogged 14 times on average, sponsored posts are reblogged 10 , 000 times on average 9 . We have observed that 40% of engagements with sponsored posts are reblogs, likes, and follows. Moreover, every four reblogs of a sponsored post result in 6 downstream reblogs from followers, leading to content longevity, while one third of reblogs of sponsored posts are present for 30 days or more after the initial post.
In this section we describe data sets comprising user ac-tivities and post contents, which were utilized to create user profiles. In particular, user activities include actions such as posts, likes, follows, and reblogs, while post contents include tags, title and body for text posts, artist names from audio posts, as well as tags and captions for photo posts.
Once signed into Tumblr, a user can follow other users X  blogs. The follow action is one-directional as it does not require the followed user to follow back. For the purpose of this study, we extracted a sub-graph which contained 96 . 9 million unique nodes (i.e., users) and 5 . 1 billion edges (i.e., follows), out of which 36 . 4 million are bidirectional (18 . 2 million pairs of users that follow each other). The data set included more than 26 . 1 billion activities on Tumblr. As mentioned earlier, an entire activity log is publicly available through a data feed called Firehose.

To create user profiles for targeting, textual contents of all posts were collected, including photo captions, tags, titles, and bodies. In addition, every time a user performs a post or reblog activity, Firehose lists the user X  X  blog title and blog description, which were also employed to represent a user. As we can see in Figure 1, a blog title and description often provide useful information with respect to targeting, such as the user X  X  first name, age, and even declared interests (e.g., statements such as  X  X ashion addict X  or  X  X  love football X ). http://yhoo.it/1vFfIAc , accessed June 2015
In order to obtain useful representations of user profiles, we propose to extract keywords from available blog informa-tion, which requires data preparation and processing. Given the extracted blog data, including title, content, and tags, we first removed all HTML tags, followed by the extraction of bigrams and the removal of common English stopwords.
In particular, it is common for certain words to appear together more often than some others (e.g., words  X  X redit X  and  X  X ard X ), and we aim to capture those bigrams and use them in keyword-based user profiles. To detect bigrams, we use a procedure that counts the unigram and bigram appearances, and for each combination of words w i and w j calculates the following score, Finally, bigrams with a score above a certain threshold were extracted from the blog contents, along with the remain-ing unigrams. On the other hand, post tags are originally formed as n-grams by the users (e.g., #chess rules ), and were extracted in their original form.
Available data sources were used to create user profiles. In particular, we extracted three distinct groups of user-related data: 1) declared; 2) content of posts; and 3) actions. The specific components included in each of the data groups are listed in Table 1. From each group we extracted features to represent the users as described below.

Declared data consists of information provided during sign-up, including keywords from the blog title and blog description extracted using the method described in Sec-tion 4.2. We counted the keyword frequency in a user X  X  blog title and description, and stored the counts along with a timestamp of the latest log-in as a part of user X  X  profile.
Content features were formed from the textual contents of posts which a user either created or reblogged. The main content feature types include: 1) post tags; 2) keywords from the post title and body; 3) keywords from the captions of photo post; and 4) artist names from audio posts. In this way we collected several millions of distinct keywords that were used to obtain rich representation of user profiles. To illustrate content keyword extraction from the Firehose, consider that user u i at timestamp t used tag #hp five times and tag #nba eight times, keyword football two times in post titles, and posted ten times an audio post with a song from artist Shakira . Then, the resulting user profile would be u i = { tag : { # hp,t : 5; # nba,t : 8 } ,title : { football,t : 2 } ,artist : { shakira,t : 10 }} .

Action features include follows, likes, and reblogs. If user u i follows user u j at timestamp t , we create an indicator feature follows : { j,t : 1 } and add it to the u i user X  X  profile. Similarly, if user u i likes user X  X  post, we create a feature that keeps record of the number of likes m , as likes : { j,t : m } , and update the user X  X  profile accordingly.

The timestamps used in feature engineering represent the day on which the activity happened. For the experiments presented in this paper we subsampled Tumblr users to ob-tain 80 million user profiles. The total number of unique features was 1 . 4 million, and on average a user had 380 non-zero features.
The goal of our work is to identify user groups with in-terests in certain topics, such as music, travel, cooking, or books, in order to allow advertisers to target segmented Tumblr audiences, as well as to infer user demographics (dis-cussed in Section 6). As the topic interests may be defined at various levels of granularity, to avoid sparsity problems while still providing useful and actionable interest categories, user interests are often classified into a pre-determined hi-erarchical interest taxonomy that the advertisers commonly use. However, to be able to create effective user interest classifiers, a modeler requires a sufficient amount of labeled data. Yet, for the problem of the scale of Tumblr interest prediction, this can be a daunting task for human editors. For that reason we propose to use a novel semi-supervised classification approach [12] based on the recently proposed word2vec model [16], which efficiently and seamlessly makes use of large amounts of unlabeled and a limited amount of labeled data for learning effective content classifiers. We decided to classify keywords into the General Interest Taxonomy (GIT), used by the Yahoo Gemini advertising platform for native advertising 10 . The GIT is carefully de-rived based on Interactive Advertising Bureau (IAB) taxon-omy recommendations, in order to meet advertiser needs and protect Yahoo X  X  interests. The GIT has a two-level hierarchi-cal structure, such that advertisers can adjust the audience reach by utilizing broader or narrower interest categories. The top level of the taxonomy contains 23 nodes (e.g.,  X  X u-tomotive X ,  X  X ets X ,  X  X ravel X ), while the second level contains 130 nodes which represent more focused interests (e.g.,  X  X u-tomotive/SUV X ,  X  X utomotive/Luxury X ,  X  X ets/Dogs X ). http://gemini.yahoo.com , accessed June 2015
In this section, we describe a recently proposed classifica-tion approach [12] based on the skip-gram model [16], which is used to categorize keywords into the GIT taxonomy. For conciseness, we describe the proposed model on the assump-tion that it is applied to tag categorization. However, it is straightforward to use the same methodology for categoriza-tion of keywords originating from blog titles and descrip-tions, as well as from text, audio, and image posts. Thus, we consider the task of tag classification, where the goal is to classify tags into one or more interest categories. In or-der to address this problem, we learn tag representation in a low-dimensional vector space using neural language models that are applied to historical Tumblr posts.
 More specifically, let us assume that we are given N posts. In the post logs found in Firehose, each post p is recorded along with the tags g j , j = 1 ...M , where M represents the number of tags in the post. Given the data set D of all posts, the objective is to find a vector representation of tags in which semantically similar tags are nearby in the vector space. For this purpose, we extend ideas originating from recently proposed language models, as described in the re-mainder of this section.

The skip-gram (SG) model involves learning represen-tations of tags in a low-dimensional space from post logs in an unsupervised fashion, by using the notion of a blog post as a  X  X entence X  and the tags within the post as  X  X ords X , borrowing the terminology from the Natural Language Pro-cessing (NLP) domain (see Figure 5). Tag representations using the skip-gram model [16] are learned by maximizing the objective function over the entire D set of blog posts defined as follows, Probability P ( g j + m | g j ) of observing a neighboring tag g given the current tag g j is defined using the soft-max, where v g and v 0 g are the input and output vector represen-tations of tag g of user-specified dimensionality d , n defines the length of the context for tag sequences, and G is the number of unique tags in the vocabulary. Following training of the skip-gram model, tags that co-occur often and tags with similar contexts (i.e., with similar neighboring tags) will have similar vector representations.

The semi-supervised skip-gram (SS-SG) model as-sumes that some tags are labeled with categories from the GIT taxonomy. Then, we introduce a dummy category vec-tor for each node of the taxonomy, and leverage the tag con-texts in blog posts to jointly learn tag vectors and category vectors in the same feature space [12]. Given such setup, after learning the representations every tag from the vocab-ulary can be categorized by simply looking up the closest category vector in the joint embedding space.

Given a set of categorized tags, we extend D to obtain data set D ss where available categories are imputed into post  X  X entences X  p . In particular, labeled tags are accom-panied by assigned categories, and every time a vector of a labeled central tag t j is updated to predict the surround-ing tags, vectors of categories assigned to g j are updated as well. More formally, assuming the central tag g beled with C j of C categories in total,  X  j = { c 1 ,...,c the semi-supervised skip-gram learns tag and category rep-resentations by maximizing the following objective function,
X Probability P ( g j + m | c ) of observing tag g j + m , given label c of the current tag g j , is defined using the soft-max, This procedure allows us to seamlessly incorporate labeled and unlabeled data, and learn tag and category vectors in the common embedding space. Then, classification of tags amounts to a simple nearest-neighbor search among the cat-egory vectors. In Figure 6 we show graphical representation of the semi-supervised skip-gram model.
The models are optimized using stochastic gradient as-cent, suitable for large-scale problems. However, computa-tion of gradients  X  X  in (5.1) and (5.3) is proportional to the vocabulary size G , which may be expensive in practice as G could easily reach several million tags. As an alternative, we used a negative sampling approach [16], which significantly reduces the computational complexity.

The data set used during the model training comprised 6 . 8 billion posts that contained tags. To collect category labels F igure 7: Nearest neighbors of tags: a) #makeup ; b) #dress for some of the tags, we sorted the tags in a decreasing order of popularity and through editorial efforts labeled the top ones with one or more categories. This resulted in a total of 8 , 400 categorized tags used during semi-supervised training. We show examples of categorized tags in Table 2.

The representations were trained using a machine with 96GB of RAM memory and 24 cores. Dimensionality of the embedding space was set to d = 300, and the neighborhood size was set to n = 5. Finally, we used 10 negative samples in each vector update. Similarly to [16], most frequent tags were subsampled during training.
When the vector representations of all tags are learned, we can find similar tags for a given tag by straightforward k -nearest neighbor ( k -NN) searches in the representation space. We use cosine distance [16] as a measure of similarity. To illustrate the usefulness of our approach, word clouds of neighboring tags to the tags #makeup and #dress are shown in Figure 7, where we see that semantically similar tags are grouped in same parts of the embedding space.

Similarly, we can find the most likely category for any tag by finding the nearest category in the vector space. To pro-duce a high-confidence set of categorized tags, we retrieved only tags with a cosine distance of 0 . 7 or higher to the cor-responding category vectors. This threshold was obtained Campaign Control Targeted Home &amp; Garden  X  +9 . 71% Style &amp; Fashion  X  +42 . 53% Sports/Outdoor Sports  X  +19 . 86% Arts &amp; Enter./Television  X  +24 . 37% Arts &amp; Enter./Video Games  X  +19 . 02% Pets/Dogs  X  +27 . 21% Arts &amp; Enter. (campaign no. 1)  X  +9 . 08%
Arts &amp; Enter. (campaign no. 2)  X  +6 . 54% eral examples of qualified user profiles are given in Table 5. Note that a user may be qualified into more than one interest category. When the system was deployed in the production, each user was assigned to 13 categories on average.
In order to target Tumblr users who do not create much content, but actively follow and engage with other blogs, we leverage the follower graph to create additional catego-rized features. In particular, using equation (5.5) we identify users with high value of u t i,cat = k (which we term influencers ). Then, following and liking posts created by influencers in the k -th category can serve as additional evidence of one X  X  interest in that category.

To implement this idea, we labeled 5% of users with the highest interest score in a certain category as influencers, and categorized  X  X ollow X , and  X  X ike X  actions directed towards such users into that category. Then, we recompute equation (5.5) with an extended set of categorized activities that in-cludes the categorized actions. This effectively expands the interest segments with users that are not content producers, but mostly act as consumers of content.
In order to evaluate the generated user interest segments, we performed online A/B testing and worked with several advertisers who ran concurrent interest-targeted and untar-geted campaigns. We tracked user engagement with their ads in terms of sponsored post likes, reblogs, and follows, and present the results for 8 targeting campaigns in Table 6. We observed an average increase of 20% in user engage-ment with sponsored posts in comparison to untargeted cam-paigns (aggregated over 3 metrics), representing a significant improvement over the baseline approach.
In this section, we explain the details of our gender predic-tion model, based on the user profiles described in the pre-vious sections. We first describe the generation process of a golden set of labeled users, which is used to train a predic-tive model that generalizes well on the remaining unlabeled users. This is followed by a description of the classification model and a discussion of the empirical results.
In order to train machine learning method for gender pre-diction, in addition to user profiles we also require labels that present the ground truth (i.e.,  X  X ale X  or  X  X emale X ). However, Tumblr does not collect gender information during sign-up, leaving open the question of how to obtain such data.
To address this problem, we propose to leverage highly in-formative blog description data in order to infer user gender. In particular, users often declare their names in their blog descriptions, as illustrated in Figure 1. To extract the de-clared names, we used several regular expression rules that we found to result in high precision. The obtained results from a large set of name-matching regular expressions were editorially tested for quality. It was found that regular ex-pressions reported in Table 7 yielded the most reliable ex-tracted names (valid names were extracted in more than 95% of the cases).

Next, in order to generate the ground truth, we used US census data of popular baby names 12 from year 1880 to 2013 to create  X  X ame  X  gender X  mapping. More specifically, we used male/female empirical ratios as soft labels, with 1 in-dicating 100% confidence in male and 0 indicating 100% confidence in female name. This approach resulted in 564 thousand female and 395 thousand male users found.
Let D g = { ( x i ,y i ) ,i = 1 ,...,N } denote our gender data set, where N is the total number of labeled users, x K -dimensional user feature vector, and y i  X  [0 , 1] is a soft gender label. The feature vectors were generated from the www.ssa.gov/oact/babynames/limits.html , 06/15 user profiles as described in Section 5.3 by setting  X  = 1, which turns off the time-decay of feature counts (due to the fact that, unlike interest, gender does not fluctuate). To handle large feature counts, we normalized the values by applying log transformation: assuming that the count is x , we replaced feature value with log(1 + x ).

Our goal is to learn a gender predictor, f : x  X  y . As a classification model, we used logistic regression, parame-terized by weight vector w . We assume that the posterior gender probabilities can be estimated as a linear function of input x , passed through a sigmoidal function, and P ( y = 0 | x ) = 1  X  P ( y = 1 | x ). To estimate the parame-ters w , we minimize the following loss function, where hyper-parameter  X  controls the ` 1 -regularization, in-troduced to induce sparsity in the parameter vector and re-duce the feature space to a subset of features that are most predictive. In addition, we experimentally observed that the model generalizes better when we trained an initial model with ` 1 -regularization to find which features have non-zero weight, and then do another round of training without ` 1 regularization, by only using features with non-zero weights from the first round to learn a better classifier.

Given a trained LR model, the posterior class probabilities are estimated as f ( x i , w )  X  [0 , 1]. Then, the predictions are made by thresholding, as  X  y i = sign( f ( x i , w )  X   X  ), where threshold  X   X  (0 , 1) is set to ensure desired precision and recall according to advertiser X  X  specific requirements.
To evaluate accuracy of our gender prediction framework, we trained a logistic regression model on 70% of the golden set and tested on the remaining 30%. We used Vowpal Wab-bit [13] implementation on Hadoop to train the model. To illustrate the performance of our gender classifier, the per-formance results in terms of precision and recall measures Table 9: Editorial evaluation of random user predictions
Prediction # correct # wrong # not sure are presented in Table 8. The threshold value  X  was set to a value which ensured precision of 0 . 8.

In addition to evaluation on the hold-out set, we editori-ally evaluated gender predictions on the unlabeled data set of user profiles. We randomly picked 1 , 007 gender predic-tions from the population of 64 . 1 million users and asked editors to visit their profiles and verify their gender. They were instructed to mark our predictions as  X  X orrect X ,  X  X ncor-rect X , or  X  X ot sure X . The  X  X ot sure X  grade is to be used when the visual inspection of a profile is inconclusive, as we found was often the case. The editorial judgment came back with 573  X  X orrect X  (429 females and 144 males), 9  X  X ncorrect X , and 425  X  X ot sure X  grades (see Table 9). The fact that there are so many  X  X ot sure X  grades indicates that in many cases it is hard to infer the gender even after manual efforts, fur-ther indicating the benefits of the proposed approach and its superior performance in comparison to humans. Finally, we retrained the model with 100% of the golden set and de-ployed it in Yahoo production systems. A demonstration video of gender predictive tags is available online 13 .
To keep up with large number of daily activities, we imple-mented daily scoring of users on Yahoo production servers. We store the activity raw counts as well as decayed counts in Hive tables 14 for efficient retrieval. The decayed counts used in interest prediction are updated on a daily basis by multiplying the old feature values by the decay factor  X  and adding new activities. In order to infer the gender of new users we implemented daily scoring by leveraging MapRe-duce on Hadoop 15 . Both interest and gender models are retrained on a regular basis.

After thorough editorial evaluation of the inferred gender and interest targeting, both targeting frameworks were en-abled through Gemini self-serve tool. Advertisers can choose to use gender and/or interest targeting with custom segment sizes, allowing for effective targeting campaigns. We presented the steps in the development of a large-scale Tumblr gender and interest targeting framework, where we used historical Tumblr activities to create rich user profiles. We described the methodology, including a recently pro-posed semi-supervised neural language model, as well as the high-level implementation details behind the deployed sys-tem. Currently, our gender and interest predictions cover users that generate more than 90% of overall daily activi-ties on Tumblr, and are heavily leveraged by advertisers. In our ongoing work, we are concentrating on creating custom keyword-targeted advertising segments specifically tailored for a particular advertiser, which includes work on address-ing the problems of keyword discovery and expansion. https://youtu.be/jXGJ0TpOlhg , accessed June 2015 https://hive.apache.org , accessed June 2015 https://hadoop.apache.org , accessed June 2015 [1] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. [2] J. Alba, J. Lynch, B. Weitz, C. Janiszewski, R. Lutz, [3] N. Barbieri, F. Bonchi, and G. Manco. Who to follow [4] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and [5] A. Z. Broder. Computational advertising and [6] Y. Chang, L. Tang, Y. Inagaki, and Y. Liu. What is [7] R. K. Chellappa and R. G. Sin. Personalization versus [8] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google [9] N. Djuric, M. Grbovic, V. Radosavljevic, [10] N. Djuric, V. Radosavljevic, M. Grbovic, and [11] D. Essex. Matchmaker, matchmaker. Communications [12] M. Grbovic, N. Djuric, V. Radosavljevic, [13] J. Langford, L. Li, and T. Zhang. Sparse online [14] A. Majumder and N. Shrivastava. Know your [15] U. Manber, A. Patel, and J. Robison. Experience with [16] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [17] D. Riecken. Personalized views of personalization. [18] D. Shin, S. Cetintas, and K.-C. Lee. Recommending [19] T. Singh, L. Veron-Jackson, and J. Cullinane. [20] S. K. Tyler, S. Pandey, E. Gabrilovich, and [21] C. Yi, T. Lei, I. Yoshiyuki, and L. Yan. What is
