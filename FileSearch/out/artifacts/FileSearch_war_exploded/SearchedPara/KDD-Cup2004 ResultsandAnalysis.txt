 This paper summarizes and analyzes the results of the 2004 KDD-Cup. The comp etition consisted of two tasks from the areas of particle physics and protein homology detection. It focused on the problem of optimizing supervised learning to di X eren t performance measures (accuracy , cross-en tropy, ROC area, SLAC-Q, squared error, average precision, top 1, and rank of last). A total of 102 groups participated in the comp etition, 6 of which receiv ed awards or honorable mentions. Their approac hes are describ ed in other papers in this issue of SIGKDD Explorations. In this paper we do not analyze any particular approac h, but give insigh t into the performance of the  X eld of comp etitors as a whole. We study what fraction of the participan ts found good solutions, how well participan ts were able to optimize to di X eren t per-formance measures, how homogeneous their submitted pre-dictions are, and if the best submissions represen t the max-imal performances that could reasonably be achieved. We are keeping the KDD-Cup 2004 WWW site open and have added an automatic scoring system for new submissions in order to encourage further researc h in this area. Real-w orld applications of data mining typically require op-timizing to non-standard performance measures that depend on the speci X c application. For example, in direct mark et-ing, accuracy is a poor indicator of performance, since there is a strong imbalance in cost between missing a customer and making the advertisemen t e X ort a little too broad. Even for the same dataset, we often want to have di X eren t classi X ca-tion rules that optimize di X eren t criteria. For example, in information retriev al, we sometimes want to optimize pre-cision, at other times want to optimize recall, and at other times need to optimize to a measure that balances both pre-cision and recall (e.g. F-Score). The need for data min-ing metho ds that allow optimizing to di X eren t performance measures inspired the tasks of this year's KDD-Cup. In particular, this year's KDD-Cup focused on optimizing pre-dictions to a variety of performance measures in supervised classi X cation.
 The 2004 KDD-Cup comp etition included two datasets | a binary classi X cation task for a quan tum physics problem, and a protein homology prediction task. We provided a supervised training set and an unlab eled test set for each task. Participan ts were asked to submit 4 sets of predic-tions for each test set, each set of predictions maximizing performance according to a particular measure. The four performance metrics for the physics problem were accuracy , cross-en tropy, ROC area, and SLAC Q-Score. The four met-rics for the protein problem were squared error, average pre-cision, top1, and rank of last. These eight metrics are de-scrib ed in Sections 2.1 and 2.2. We provided software 1 to the participan ts to standardize how the performance measures were computed. The same software was used to determine the winners of the comp etition.
 In this report we describ e the two KDD-Cup 2004 tasks in detail 2 , provide participation statistics, and announce the winners and how they were determined. We also analyze the results of the comp etition. The submissions of more than 100 participan ts are an interesting dataset to mine for patterns of how the  X eld as a whole performed on the two data-mining tasks. For example, what percen tage of the  X eld provided reasonable solutions? Did the winners per-form signi X can tly better than the rest of the  X eld? Did the participan ts impro ve their performance by optimizing to particular performance measures? Did all groups that performed well  X nd essen tially the same solutions? These are some of the questions we address in this report. The  X rst of the two KDD-Cup 2004 tasks is a particle physics classi X cation problem. At the Stanford Linear Accelerator Center (SLA C), high energy particle beams are collided to generate subatomic particles. A major challenge in some of these experimen ts is to correctly classify the particle tracks. In the Physics Task, the goal is to learn a classi X cation rule that di X eren tiates between two types of particles generated in collider experimen ts based on 78 attributes. The train-ing set has 50,000 examples, and the test set has 100,000 examples. The data set was contributed by Charles Young et al. from SLAC. The source of the data set, the identity of the two particles, and the de X nitions of the 78 attributes were hidden from participan ts to prevent comp etitors from trying to gain an advantage by studying the physics of the problem. 3 http://kodiak.cs.cornell.edu/kddcup/software.html Additional information can be found on the KDD-Cup 2004 WWW-site at http://kodiak.cs.cornell.edu/kddcup
One migh t argue that in data mining one should take ad-vantage of background knowledge and any other domain-speci X c information that is available, which argues against In the comp etition, we measure performance on the particle physics problem using four metrics: ACC (maximize) We use the usual de X nition of accuracy { AUC (maximize) We use the usual de X nition for area under CXE (minimize) We use the usual de X nition for cross-en tropy, us hiding the identity of the problem. However, we decided to not provide participan ts with the identity of the task and the contributor because we though t few participan ts would be able to spend the time to learn enough particle physics to understand the problem, and because we did not want to burden our collab orators at SLAC with requests from participan ts for additional information. SLQ (maximize) The Slac Q-Score (SLQ) is a domain-sp eci X c On both the Physics and Protein tasks contestan ts are al-lowed to optimize their learning metho ds for each metric and submit di X eren t predictions for the test set for each of the four metrics on each task. Unlik e the Physics problem where each training or test case is indep enden t, this task has more complex structure. The goal in this task, contributed by Ron Elber, is to predict which proteins are homologous to a nativ e sequence. The data is group ed into blocks around each nativ e sequence. We provided 153 nativ e sequences as the training set, and 150 nativ e sequences as the test set. For each nativ e sequence, there is a set of appro ximately 1000 protein sequences for which homology predictions are needed. Homologous se-quences are mark ed as positiv e examples, non-homologous sequences (also called "deco ys") are the negativ e examples. On average, each nativ e sequence contains over 100 decoys per homologous sequence, making this a very unbalanced problem. The goal is to predict which of the 1000 proteins are homologous to the nativ e protein based on 74 attributes. These 74 attributes are a variety of scores that describ e the matc h between two proteins. These scores include, for ex-ample, the length of the longest local alignmen t, the per-centage sequence identity after alignmen t, or the z-score of the global alignmen t 4 .
 Evaluation measures are applied to each block corresp ond-ing to a nativ e sequence, and then averaged over all blocks. Most of the measures are rank-based and assume that model predictions provide a ranking within each block from \most likely homologous" to \least likely homologous". The task is to provide 4 sets of predictions for the test set, each of which optimizes one of the following 4 performance measures. TOP1 (maximize) This measure is de X ned as the fraction RKL (minimize) This score is the average over the blocks RMS (minimize) RMS measures the root mean squared er-APR (maximize) This score is de X ned as the average of Note that three of the measures (TOP1, RLK, and APR) depend only on the relativ e ordering of the matc hes within
To conceal the identity of the proteins and prevent partici-pants from looking up known homologs in public databases, no description of the proteins and features was given to the participan ts during the comp etition. each block, not on the predicted values themselv es. Only RMS measures the accuracy of the predicted values beyond the ordering that they induce. The comp etition started on April 28 th with the publication of the datasets and the task descriptions on the WWW. The datasets each included a labeled training set as well as a test set from which we withheld the labels. To down-load the data, groups had to register and were assigned an anon ymous ID under which they could later submit their results. The anon ymous ID was also used to publish results, so that groups which did not wish to be identi X ed could remain anon ymous. Groups that did want to be identi X ed were allowed to replace their anon ymous ID with a group name.
 The contest was open to any party planning to attend the SIGKDD 2004 Conference. Since the two tasks were evalu-ated separately , each group could enter in both tasks, in only one task, or in only one particular performance measure on one task. Each person was allowed to participate in only one comp eting group per task. Each group was allowed to make multiple submissions for each task and performance mea-sure. We did not provide any feedbac k about performance when submissions were made. Only the last submission be-fore the deadline was evaluated for the comp etition. All previous submissions from the same group were discarded. Submissions were made via a WWW interface. The web interface performed tests on submissions to make sure that the formatting of the submission was correct and that pre-dictions were submitted for all test cases. To help detect formatting problems in the submissions, each test example was assigned a unique identi X er. Groups had to submit their predictions including those case identi X ers. This allowed the submission interface to give immediate feedbac k on the in-tegrit y of the predictions (e.g. wrong number of lines, du-plicate or missing example id's, etc.).
 By the deadline for the submission of predictions on July 14 th , more than 500 groups had registered to download the data. The cumulativ e number of downloads over time is depicted in Figure 1. We suspect that some of the last-minute registrations just before the July 14 deadline are from groups that registered a second time after forgetting their assigned login information. The registration interface remained open after the submission deadline, so that it is still possible to download the data. Since the end of the comp etition, more than 100 new users have registered. By the July 14 th deadline, 102 groups had submitted predic-tions. Of these, 65 groups participated in the Physics task and 59 groups participated in the Protein task. 22 groups participated in both tasks. An analysis of the email ad-dresses revealed a broad international participation. We re-ceived submission from 49 coun try su X xes (including .com, .edu, etc.). The broad international participation is also re X  X cted in the distribution of winners. As describ ed be-low, the winners and honorable mentions went to groups from China, German y, India, New Zealand, and the USA. Roughly half of the winners are primarily a X liated with commercial companies, the other half are from academia. At the opening day of the SIGKDD Conference, the win-ners were o X cially announced. There were two main prizes: \Overall Winner of the Quan tum Physics Task" and \Over-all Winner of the Protein Homology Prediction Task". These overall winners were determined according to the following metho d. All participan ts were ranked according to their performance on the test set for each task and performance measure (four measures per task). Not submitting predic-tions for a performance measure resulted in being ranked last for that measure. The overall winner of a task was the participan t who had the best average rank over the four performance measures for that task.
 In addition to the overall winners, we also awarded eight honorable mentions, one for each metric on each task. Hon-orable mentions were given to the group who ranked highest for that particular task and performance measure. Tables 1 and 2 show the test-set performance of the sub-missions made by each group on each task and metric. The tables also include the rank for each of these scores. In ad-dition, the  X rst column in each table is the overall rank of each group on the whole task. This rank is based on the average rank of each group across the four tasks (last col-umn), as well as on a statistical analysis of the ranks that is describ ed later in this section. Entries in bold in the tables placed either 1 st , 2 nd , or 3 rd overall (based on average rank), or receiv ed an honorable mention for one or more metrics. While the rules outlined in the previous section provide clear guidelines for determining winners, we also were interested in the signi X cance of the performance di X erences between the top comp etitors. Let's begin with a qualitativ e overview of the performances before jumping into a detailed statisti-cal analysis of the observ ed di X erences in performance. Fig-ures 2 and 3 show the performances for each task and per-formance measure plotted in increasing order, so that the leftmost point of each graph represen ts the group with the best performance.
 Interestingly , most plots have roughly the same shap e. The middle of the plots are rather  X  X t, indicating that a broad fraction of the participan ts achieved roughly the same per-formance. At around rank 40 the performance starts drop-ping rapidly on most measures 5 . This means that roughly 65% of the participan ts managed to achieve good perfor-mance on each of the tasks and metrics. The leftmost end of the graphs is particularly interesting. For many measures the graphs show a change in slope that indicates that the top performers pulled away from the pack and did substan tially better than the majority of participan ts in mid X eld. To quan tify the statistical signi X cance of the performance gap among the winning predictions, we conducted a boot-strap analysis. This analysis allows us to evaluate how much the overall ranking of the groups on each of the tasks de-pends on our particular (random) choice of test sets. More precisely , it allows us to estimate the probabilit y that a group would place di X eren tly in the comp etition if we re-peated the comp etition with di X eren t randomly drawn test sets.
 The setup of our bootstrap experimen t was as follows. For r repetitions we drew a bootstrap sample from the original test set. For a test set of size k , this bootstrap sample is gen-erated by drawing k examples from the original test set with replacemen t. We then evaluated the submitted predictions
An exception is the RKL measure, where the steep decrease in performance starts at around rank 20-25. on the bootstrap sample and ranked all groups by overall performance (average rank across the four metrics for each task). Tables 3 and 4 show how often the 5 groups with the highest performance on the original test sets achieved a par-ticular rank on bootstrap samples. For the Physics task we used k = 100 ; 000 and r = 1000 repetitions. For the Protein task we took bootstrap samples over the k = 150 blocks in the test set with r = 10 ; 000 repetitions.
 As Table 3 shows, on the Physics task, the participan t order-ing deriv ed from the original 100k test set is very likely to be the correct ordering. With high probabilit y, the groups that placed  X rst, second, and third on the original test set also placed  X rst, second, and third, respectiv ely, on bootstrap samples. We conclude that there is little uncertain ty on the Physics task about which groups won the comp etition and we can be 95% con X den t that we have assigned  X rst, second, and third place to the correct groups.
 The results are very di X eren t for the Protein task. The  X rst column of Table 4 shows that all three groups that scored highest on the original test set have a signi X can t chance of winning  X rst place on bootstrap samples. Surprisingly , the bootstrap analysis suggests that the group that placed second has the highest probabilit y of being in  X rst place. If we interpret the bootstrap results as a signi X cance test, only once we go down to the fourth ranked group, can we conclude with 95% con X dence that they did not win the comp etition indep enden t of a particular test set. Based on this analysis, we declared a three-w ay tie for  X rst place on MEDai Inc. / Univ. of Central Florida David Inductis Inc. Arpita Chowdhury, Dinesh Bharule,
Golden Helix Inc. Christophe Lambert Figure 4: Winners and honorable mentions for Physics task. the Protein task instead of declaring separate  X rst, second, and third place winners as we were able to do on the Physics task.
 All winners and honorable mentions are listed in Figures 4 and 5. A description of the approac hes they used for the comp etition can be found in other papers of this issue of SIGKDD Explorations. Most reasonable performance metrics are strongly corre-lated: predictions that yield good performance on one met-ric often yield good performance on other metrics as well. However, because di X eren t metrics re X  X ct di X eren t tradeo X s between the predictions and ground truth, a prediction rule that is optimal for one measure is not necessarily optimal for a di X eren t measure. This was part of the motiv ation for this year's KDD-Cup. We wondered how much teams who submitted di X eren t sets of predictions for di X eren t metrics bene X tted by optimizing to each metric. Did the winners win because they understo od the data better and were able to train models that would have performed well on any met-ric, or were they able to gain additional bene X t by separately optimizing their models for each metric? About half of the comp etitors took advantage of the op-portunit y to submit di X eren t predictions for di X eren t per-formance measures. In the following we analyze their sub-missions to examine the exten t to which teams impro ved performance by optimizing to speci X c performance metrics. Speci X cally , we evaluate whether the teams' e X orts to op-timize to individual performance metrics gave them higher scores or not. Clearly , if a team submitted the same predic-
Univ. of Waikato Bernhard Pfahringer Chinese Academ y of Sciences Yan Fu, RuiXiang MEDai Inc. / Univ. of Central Florida David Univ. of Dortm und Dirk Dach, Holger Flick, Figure 5: Winners and honorable mentions for Protein task. tions for all four metrics on one task, we do not gain any insigh t into whether that team could have bene X tted from optimizing to the metrics. However, if we receiv ed 2 or more di X eren t sets of predictions from one team, we can study which set performs best on which performance metric. In particular, does the submission for a particular metric really outp erform the predictions from the same team submitted for other metrics? We  X rst evaluate the team's performance on some metric, A, with the set of predictions that team gave us for metric A. Next, we take one of the other sets of predictions that the same team gave us for a di X eren t metric, and evaluate those predictions on metric A. If the team did better with the predictions submitted for metric A, this indicates that their optimization for metric A was e X ectiv e. If, however, swapping the prediction sets impro ves performance on the metric, then clearly the team was less e X ectiv e at optimizing to each metric and in fact would have done better if they had submitted their predictions for a di X eren t metric. Tables 5 and 6 show how often swapping sets of predictions between pairs of metrics helps or hurts performance on those metrics. The columns in the tables show what metric the predictions were originally submitted for. The rows in the tables are the new metrics those predictions are used for. Each entry is the table is a pair of numbers. The positiv e number is the number of times swapping metrics helped per-formance. The negativ e entry is the number of times swap-ping performance hurt performance. If comp etitors always achieved the best performance on each metric using the pre-dictions they submitted for that metric, all entries would be negativ e.
 From the tables we can see that it is fairly common that a team would have achieved better performance on some metric by using predictions they had submitted for a dif-feren t metric for that metric instead. In fact, the predic-tions groups submitted for rank last (RKL) more often than not would have been better predictions for average precision (APR) and TOP1 than the predictions submitted for those metrics. On average, however, swapping hurts performance more than it helps. On the Physics task, swapping submis-sions helped 67 times but hurt 123 times. On the Protein task, swapping helped 82 times but hurt 206 times. Unfortunately , swapping sets of predictions is not always sensible: for many pairs of metrics, a good submission for one metric migh t simply be inappropriate for another met-ric. For example, a good set of predictions for TOP1 migh t have a single prediction of class 1 for one case in each block, while the rest of the cases in that block migh t be predicted as 0. This is a perfectly reasonable set of predictions for TOP1. But these same predictions are unlik ely to give a good RMSE score. An even more catastrophic example comes from ordering metrics such as APR and AUC. For these metrics, only the ordering induced by the predictions matters. The predictions can be any numbers (real or in-teger) on any scale, as long as they provide a well-de X ned ordering. Since predictions for RMSE, CXE, and SLQ must be between 0 and 1, some APR and AUC predictions can not be used for RMSE, CXE, or SLQ. There are additional instances of predictions that make sense for one metric, but are incompatible with another metric.
 Luckily, most teams submitted predictions between 0 and 1 that were more likely to be compatible across performance measures. In order to eliminate invalid swaps, we emplo y the following test. We de X ne as  X  the change in rank result-ing from substituting some other set of predictions for the intended set. If the rank of the team on the metric we were testing increased or decreased by more than  X  max when we used the predictions from a di X eren t metric, then we consid-ered the swap to be invalid. For example, with a  X  max of 10, if we found that using a team's APR predictions for RMSE Figure 6: Percen tage of swaps that increase performance vs.  X  max for Physics. Figure 7: Percen tage of swaps that increase performance vs.  X  max for Protein. APR. caused that team's RMSE rank to drop from 5th to 30th, we assumed that those APR predictions were not acceptable as RMSE predictions. This approac h assumes that when a set of predictions is incompatible with a performance metric for which it was not submitted, large drops in performance will be observ ed, enabling us to  X lter out the incompatibilities. To avoid bias, if a team's rank on a metric increased by a large amoun t when using predictions for another metric, we supp osed that the predictions the team originally gave us for that metric were someho w  X  X wed, and so we did not include those cases either. For example, most teams pre-vented predictions from reaching 0 or 1 for the CXE metric because even a single misclassi X cation would then cause a near-in X nite loss in cross-en tropy. But a number of teams did submit predictions for CXE that were 0 or 1. For these teams, the predictions they submitted for AUC migh t ac-tually yield better CXE than their CXE predictions. We exclude these cases because clearly they have made major mistak es in the predictions they submitted for one or more metrics.
 Finally , as we saw in the previous section, the teams towards the bottom third of the rankings did much worse than the top two-thirds of the teams. We are mainly interested in the e X ect of optimizing to a metric for groups that were able to achieve good performance on these problems, so we did not include in our analysis those predictions that performed very poorly.
 Once we had determined which sets of predictions were rea-sonable to swap, we did a swap analysis for each problem. The number of valid swaps varies with  X  max , but there were over 100 on each problem even for moderate values of  X  max For each swap we determined if the swap gave a better per-formance or a worse performance than the original predic-tions on that metric. We coun ted the total number of score increases, inc , and the total number of score decreases, dec , and then computed the fraction of times that swapping im-proved performance as opposed to hurt performance. Fig-ure 6 and Figure 7 show the probabilit y that swapping im-proved performance plotted against  X  max , using the top 40 predictions on each metric.
 On the physics problem, the probabilit y that swapping pre-diction sets helps performance is relativ ely constan t at about p = 0 : 35. This is well below 0.5, and the error bars do not include 0.5. This indicates that regardless of  X  max , swap-ping caused decreases in performance signi X can tly more of-ten than swapping caused increases. This means that on the physics problem, groups that optimized to each performance metric impro ved their performance on the physics metrics at least 65% of the time by doing this optimization. The protein problem has a slightly di X eren t story . For small values of  X  max , the graph is actually above 0.5, which indi-cates that using predictions for an alternate metric tended to help a little in cases where the change in rank (delta) was very small. Note, however, that the error bars for all points above 0.5 include 0.5, so this migh t be just statistical  X  X ctuation. Also, the swapping score is not as informativ e for small values of  X  max . For example, when  X  max is 0, it means that the rank remains the same when using alternate predictions. For the rank to remain the same, the change in score must have been very small. As  X  max increases from 0, the fraction of cases for which swapping impro ves per-formance quickly drops to 0.4, and for moderate values of  X  max , the probabilit y that swapping helps is similar to that of the physics problem, about 0.35. We conclude that most participan ts who submitted di X eren t predictions for di X er-ent metrics on the Protein task were able to achieve better performance on each metric by optimizing to each metric. Up to this point, we have only coun ted the number of swaps that cause scores to increase or decrease. We showed that making a swap does tend to decrease performance, suggest-ing that the groups were e X ectiv e at optimizing to each met-ric. But how big is the di X erence? Unfortunately , many of the metrics are non-linear and on di X eren t scales, so a simple average of the change in score is not informativ e. Instead, we will look at the mean rank change caused by swapping. Figure 8 and Figure 9 show the mean rank change as a func-tion of  X  max . The plots suggest that, while optimizing to a particular metric does give some bene X t, this bene X t is typ-ically modest. On the Physics task, swapping submissions between metrics on average lowers a group's rank on that metric only about 1-2 places. On the Protein Task, swapping predictions between metrics would cause a group to rank about 2-7 places lower on each metric. These di X erences in performance can be substan tial in a comp etition where ranking a few positions lower can be the di X erence between coming in  X rst and not even being in the top three, but the changes in performance that yield a decrease in rank of 1-7 places typically is rather small 6 . Furthermore, roughly half of the highest scoring comp etitors did not submit multiple sets of predictions, indicating that optimizing to the partic-ular performance measures was not essen tial for performing well. Do good predictions for a particular metric tend to be sim-ilar? That is, given two high-scoring sets of predictions for a problem and metric, are the predictions very similar? To answ er this question, we looked at the top 30 submissions on 4 di X eren t performance metrics. We treated each sub-mission as a vector in Euclidean space, and determined the Euclidean distance between the two vectors of predictions for each pair of submissions. To normalize predictions for the rank based measures (e.g. APR and AUC), we sorted the predictions, and converted each prediction to be its rank, divided by the total number of predictions. After calculat-ing the matrix of all pairwise distances between submissions for a metric, we examined the distance matrix. To visualize the distances, we use Multi-Dimension Scaling (MDS). Pro-jecting the data down to two dimensions with MDS reduces
See Tables 1 and 2 for an idea of how large a di X erence in performance must be to move several positions in the rankings. normalized stress to below 0.1, suggesting that the predic-tions submitted by the top 30 comp etitors for a metric vary along a low-dimensional manifold.
 Figure 10 shows the MDS plots for Physics AUC and Protein APR. Surprisingly , they show that good predictions need not be very similar to each other. Moreo ver, predictions that are somewhat similar to each other can have surprisingly di X er-ent performance. For example, in the APR plot, the top 3 sets of predictions are relativ ely close to each other near the center of the plot. But the distance between the 25 th submission and the 2 nd submission is less than the distance between the 1 st submission and the 2 nd submission 7 . Fur-thermore, notice that the 4 th place predictions are all the way up in the upper right corner, far away from the 1 st , 2 and 3 rd place predictions. This shows that models with ex-cellen t performance do not always achieve that performance by making similar predictions, and models with fairly similar predictions do not always achieve comparable performance. The previous section showed that the predictions of the best performing groups were not homogeneous. In this section we examine how these di X erences are distributed among the test examples. Figure 11 plots the fraction of test examples that a particular number of groups predicted correctly for the Physics accuracy task. The left-hand plot includes the 10 groups with the highest accuracy on the Physics task, the right-hand plot includes the 30 groups with the highest accuracy .
 Most groups agree on the classi X cation of a large fraction of the examples. On roughly 55% of the test examples, all top 10 groups make the same prediction and classify the exam-ple correctly (righ t-most point of the left-hand graph). Even when considering the top 30 groups, the fraction remains high with 45% of the test cases being correctly classi X ed (and therefore classi X ed the same) by all groups. Interest-ingly , the predictions also are consisten t on many incorrectly classi X ed examples. On about 12% of the test examples the
We have veri X ed by looking at the raw Euclidean distances that this is real and not just an artifact of the 2-d MDS projection.
 top 10 groups all predict the wrong class (left-most point of the left-hand graph). Again, this  X gure changes little when including the top 30 groups.
 In summary , the graphs show that there is a large fraction of test examples that all groups agree upon in their predic-tions. The top 10 groups agree on the prediction for 67% of the test cases, and even the top 30 groups agree on more than 50% of the test cases. However, 8% to 12% of the test cases are consisten tly misclassi X ed. Between the two extremes, the graph is rather  X  X t. This means that only a fairly small fraction of the test cases accoun t for the di X er-ences in prediction that were observ ed for di X eren t groups in previous sections. Because the Physics and Protein tasks are real-w orld prob-lems (as opposed to synthetic problems), we do not know what performance ultimately is achievable on each task and metric. On most metrics, the top models have performance close to each other. This migh t suggest that the best mod-els are at or near the ceiling and better performance is not possible. In this section we try to determine if better per-formance can be achieved on these problems, or if the best models are already near the ceiling.
 An ensem ble is a collection of models whose predictions are combined by weighted averaging or voting. Dietteric h[3] states that \A necessary and su X cien t condition for an en-semble of classi X ers to be more accurate than any of its indi-vidual mem bers is if the classi X ers are accurate and diverse." Recen t work shows that models trained with di X eren t learn-ing algorithms often make uncorrelated errors. When this is true, an ensem ble of good models trained with di X er-ent learning algorithms often outp erforms the best model trained by one of the learning algorithms.[2] We wondered if ensem bles of the best models submitted for each task and metric would impro ve upon the performance of these best models. To answ er this question, we created ensem bles that average the predictions of the best N models for each task and metric. To focus our atten tion on the best models, we vary N from 1 to 20. We then evaluate each of these ensem bles on the  X nal test set using the appropriate performance metric. 8 Figure 12 shows the performance of ensem bles formed by averaging the predictions of the best N models for the four Physics metrics. Figure 13 shows the performance of ensem-bles that combine the best N models submitted for the Pro-tein metrics. The x-axis is the number of models included in the ensem ble. The  X rst ensem ble ( N = 1) in each of the eight plots contains only the single best model submitted for that task and metric. Thus N = 1 is the performance of the winning model submitted in the comp etition for each metric. At N = 2, the ensem bles average the predictions of
Because we use the same test sets to  X nd the best N models, and then to evaluate the performance of ensem bles contain-ing these best N models, we do not have truly indep enden t test sets.
 the best two models submitted for that task and metric. In seven of the eight plots, better performance is achieved for N greater than one. On most problems and metrics, better performance can be achieved by combining the pre-dictions of a few of the best models. For example, on the physics problem, ensem bles that average the top 2-9 mod-els all achieve higher accuracy than the best single model. The best accuracy is achieved for an ensem ble that com-bines the top two models. This ensem ble yields an accuracy of 73.56%, up 0.30% from the best submitted model which has accuracy 73.26%. While this increase is modest, it is four times larger than the 0.07% di X erence in performance between the top two submitted models.
 Similarly , the best AUC and the best SLAC-Q Score are achieved by averaging the best three models submitted for these metrics. On the cross-en tropy metric, however, the best submitted model outp erforms any of the ensem bles, presumably because there is a large gap in cross-en tropy performance between the best model and all other models. On the protein problem, an ensem ble outp erforms the best submitted models on each of the four metrics. An ensem-ble of the best six models increases Top 1 performance from 0.920 to 0.927. An ensem ble of 5 models lowers RMS from 0.0350 to 0.0342. An ensem ble of the best 13 models lowers Rank of Last from 45.6 to 44.5. And an ensem ble contain-ing the top 4 models increases average precision from 0.841 to 0.849. While all of these increases in performance are modest, for three of the four metrics they are equal to or larger than the di X erences in performance between the best two models submitted for that metric. On Rank of Last, the best model ( RKL = 45 : 62) is dramatically better than the second best model ( RKL = 52 : 42), and we were surprised to see that ensem bles combining the best 2-13 models all impro ve upon the best single model.
 In summary , on 7 of 8 metrics, ensem bles that average the predictions of the best N models outp erform the best sub-mitted models by margins comparable to the di X erences in performance we see between the best two submitted mod-els. The best number of models to include in each ensem ble depends on the task, metric, and submissions. To be fair, the decision about the best N should not be made using the  X nal test set as we have done here, so the results we presen t must be taken with a grain of salt. But the graphs do seem to suggest that on both problems and most metrics there still is room for impro vemen t, and the submissions for the comp etition have not hit the ceiling. As further evidence that we have not achieved peak performance on these problems and metrics, we have receiv ed several sub-missions since the comp etition closed that impro ve upon the best performances observ ed during the comp etition. See http://kodiak.cs.cornell.edu/kddcup for the latest re-sults on each problem. We would like to thank the contributors of the datasets, Ron Elber (Cornell CS) and Charles Young (SLA C), for the time and e X ort they invested in creating the data and helping us prepare it for the KDD-Cup 2004. We also thank Johannes Gehrk e (Cornell CS) and Mirek Riedew ald (Cornell CS) for help with the Physics dataset and SLQ metric. We thank Alex Niculescu, Filip Radlinski, and Claire Cardie for their help with the PERF evaluation software, and also thank the participan ts from the Univ ersity of Dortm und and the Chi-nese Academ y of Science who detected bugs in early releases of PERF. This work was partially supp orted by NSF grants IIS-0412894 and IIS-0412930. We presen ted the tasks and the winners of the 2004 KDD-Cup comp etition. Our analysis of the results revealed that roughly two thirds of the participating groups found good solutions. While there was evidence that some participan ts did bene X t by optimizing to the di X eren t performance mea-sures, the bene X ts typically were modest, and on average would change their ranks only a few places. Comparing submissions from di X eren t groups, we found a substan tial amoun t of diversity in the predictions from di X eren t groups, yet determined that much of this diversity occurs on less than 50% of the test cases. The results of an ensem ble learning experimen t con X rms that there is useful diversity among the top comp etitors, and gives some evidence that it is possible to achieve somewhat better performance than the winning submissions.
 While the original KDD-Cup 2004 comp etition is o X cially closed, the datasets and a new submission interface remain available on the KDD-Cup WWW site: New submissions will be scored immediately after submis-sion and the results are inserted into an expanding table of post KDD-Cup results. A coun t of the total number of new submissions a group makes for each task and metric is dis-played in this table to help prevent groups from over X tting to the test sets by testing too many models. We encour-age further participation and researc h on the tasks of the KDD-Cup 2004. [1] R. Baeza-Y ates and B. Ribeiro-Neto. Modern Informa-[2] R. Caruana and A. Niculescu-Mizil. Ensem ble selection [3] T. G. Dietteric h. Ensem ble metho ds in machine learn-Rich Caruana is an Assistan t Professor in the Departmen t of Computer Science at Cornell Univ ersity. He receiv ed a Ph.D. in CS at Carnegie Mellon Univ ersity in 1997 where he was advised by Tom Mitc hell and Herb Simon. Most of Caruana's researc h is in machine learning and data min-ing. His main focus is on developing new learning metho ds for problems in medical informatics, though he is interested in most real-w orld problems that stretc h the capabilities of curren t machine learning metho ds. A recen t focus of his work is optimizing supervised learning metho ds for di X eren t performance criteria.
 Thorsten Joachims is an Assistan t Professor in the De-partmen t of Computer Science at Cornell Univ ersity. In 2001, he  X nished his dissertation on maxim um-margin ap-proac hes to learning text classi X ers, advised by Prof. Katha-rina Morik at the Univ ersity of Dortm und. From there he also receiv ed his Diploma in Computer Science in 1997 with a thesis on WebWatcher, a browsing assistan t for the Web. His researc h interests center on a synthesis of theory and system building in the  X eld of machine learning, with a fo-cus on Supp ort Vector Machines and machine learning with text. He authored the SVM-Ligh t algorithm and software for supp ort vector learning. From 1994 to 1996 he was a visiting scien tist at Carnegie Mellon Univ ersity with Prof. Tom Mitc hell.
 Lars Backstrom is a recen t graduate of Cornell Univ er-sity's computer science departmen t. He is curren tly re-searc hing feature selection for supervised learning with Pro-fessor Caruana at Cornell Univ ersity. Additionally , he is the problem coordinator for TopCo der, Inc., a compan y provd-ing programming comp etitions as a recruiting tool. He is in the process of applying to Ph.D. programs in computer science for the fall of 2005.

