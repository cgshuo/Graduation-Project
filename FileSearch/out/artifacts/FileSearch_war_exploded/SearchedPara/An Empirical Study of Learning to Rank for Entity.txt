 This work investigates the effectiveness of learning to rank methods for entity search. Entities are represented by multi-field documents constructed from their RDF triples, and field-based text similarity features are extracted for query-entity pairs. State-of-the-art learning to rank methods learn models for ad-hoc entity search. Our experiments on an en-tity search test collection based on DBpedia confirm that learning to rank methods are as powerful for ranking entities as for ranking documents, and establish a new state-of-the-art for accuracy on this benchmark dataset.
 Entity Search, Learning to rank, Knowledge Base, DBpedia
Publicly available knowledge bases such as DBpedia, Free-base, and Wikipedia are beginning to be used for tasks such as document ranking, card retrieval, and question answer-ing [3, 4, 5]. A key component in many such systems is ad-hoc entity retrieval -using a query to retrieve one or more entities that satisfy some underlying information need. Sev-eral methods of representing and ranking entities have been developed recently, however the problem is far from solved. Recent knowledge bases store information in RDF triples. A triple is a piece of information about the entity, for ex-ample its name, alias, category, description or relationship to another entity. Some prior research represents each en-tity as a structured document by grouping RDF triples into fields [1, 6] or a tree [2]. This allows entities to be retrieved by typical document retrieval algorithms such as BM25, query likelihood, or sequential dependency models.

This work investigates the effectiveness of learning to rank (LeToR) models, the state-of-the-art in ranking documents, for entity search. It represents entities following the previ-ous state-of-the-art X  X  multi-field representations [6], extracts text similarity features for query-entity pairs, and studies the effectiveness of state-of-the-art learning to rank models for ranking entities. Experimental results on an entity search test collection based on DBpedia [1] confirm that learning to rank is as powerful for entity ranking as for document ranking, and significantly improves the previous state-of-the-art. Results also indicate that learning to rank mod-els with text similarity features are especially effective on keyword queries.

Our analysis further shows the influence of query types on learning to rank models. Different types of queries use dif-ferent parts of an entity X  X  representation. It is more effective to learn different models for different types of queries than to use a single model for all types of queries.
The first question in entity search is how to represent en-tities. We follow Zhiltsov et al. [6] and group RDF triples into five fields: Name , which contains the entity X  X  names; Cat , which contains its categories; Attr , which contains all attributes except name; RelEn , which includes the names of its neighbor entities; and SimEn , which contains its aliases. We include all the RDF triples in DBpedia in the fields.
In state-of-the-art learning to rank systems for document ranking, most features are the scores of common unsuper-vised ranking algorithms applied to different document rep-resentations (fields). The different ranking algorithms and representations provide different views of the relevance of the document to the query. The multiple perspectives rep-resented by these features are the backbone of any learning to rank system.

This approach can be applied to entity search by extract-ing features for query-entity pairs. We use the following ranking algorithms on each of an entity X  X  five fields: Lan-guage model with Dirichlet smoothing (  X  = 2500), BM25 (default parameters), coordinate match , cosine similar-ity , and sequential dependency model (SDM) . We also in-clude Zhiltsov et al X  X . [6] fielded sequential dependency model (FSDM) score for the full document as a feature. As a result, there are in total 26 features as listed in Table 2.
With features extracted for all query-entity pairs, all learning to rank models developed for ranking documents can be used to rank entities. We use two widely-used LeToR models: RankSVM , which is a SVM-based pairwise method, and Coordinate Accent , which is a gradient-based listwise method that directly optimizes mean average pre-cision (MAP). Both of these LeToR algorithms are robust and effective on a variety of datasets. Query Set Queries Search Task SemSearch ES 130 Retrieve one entity ListSearch 115 Retrieve a list of entities Table 2: Query-entity features used in learning to rank.
This section describes our experiment methodology in studying the effectiveness of learning to rank in entity search.
Dataset: Our experiments are conducted on the en-tity search test collection provided by Balog and Neu-mayer [1], which others also have used for research on en-tity retrieval [2, 6]. The dataset has 485 queries with rel-evance judgments on entities from DBpedia version 3.7. These queries come from seven previous competitions and are merged into four groups based on their search tasks [6]. Table 1 lists the four query groups used in our experiments.
Base Retrieval Model: We use the fielded sequen-tial dependency model ( FSDM ) as the base retrieval model to enable direct comparison with prior work [6]. All learning to rank methods are used to rerank the top 100 entities per query retrieved by FSDM .

Ranking Models: RankSVM implementation is provided by SVMLight toolkit 1 . Coordinate Ascent implementation is provided by RankLib 2 . Both methods are trained and tested using five fold cross validation. We use linear kernel in RankSVM . For each fold, hyper-parameters are selected by another five fold cross validation on the training partitions only. The  X  X  X  of RankSVM is selected from the range 1  X  100 using a step size of 1. The number of random restarts and iterations of Coordinate Ascent are selected from the ranges 1  X  10 and 10  X  50 respectively using a step size of 1.
Baselines: The main baseline is FSDM , the previous state-of-the-art for the benchmark [6]. We also include SDM-CA and MLM-CA [6] results as they perform well in the test collection. Evaluation Metrics: All methods are evaluated by MAP@100, P@10 and P@20 following previous work [6]. We also report NDCG@20. Statistical significance tests are per-formed by Fisher Randomization (permutation) tests with p &lt; 0 . 05.
We first present experimental results for learning to rank on entity search. Then we provide analysis of the importance of features and fields, and the influence of different query types on LeToR models. https://www.cs.cornell.edu/people/tj/svm light/svm rank.html http://sourceforge.net/p/lemur/wiki/RankLib/
The ranking performances of learning to rank models are listed in Table 3a. We present results separately for each query group and also combine the query groups together, shown in the All section of Table 3a. Relative perfor-mances over FSDM are shown in parenthesis.  X  ,  X  ,  X  indicate statistical significance over SDM-CA , MLM-CA , and FSDM re-spectively. The best performing method for each metric is marked bold . Win/Tie/Loss are the number of queries im-proved, unchanged and hurt, also compared with FSDM .
The results demonstrate the power of learning to rank for entity search. On all query sets and all evaluation met-rics, both learning methods outperform FSDM , defining a new state-of-the-art in entity search. The overall improvements on all queries can be as large as 8%. On SemSearch ES , ListSearch and INEX-LD , where the queries are keyword queries like  X  X harles Darwin X , LeToR methods show signif-icant improvements over FSDM . However, on QALD-2 , whose queries are questions such as  X  X ho created Wikipedia X , sim-ple text similarity features are not as strong.

Similar trends are also found in individual query per-formances. Figure 1 compares the best learning method, RankSVM , with FSDM at each query. The x-axis lists all queries, ordered by relative performance. The y-axis is the relative performance of RankSVM over FSDM on NDCG@20. On keyword queries more than half of the queries are im-proved while only about a quarter of the queries are hurt. On questions ( QALD-2 ), about the same number of queries are improved and hurt. A more effective method of han-dling natural question queries was developed recently by Lu et al. in which queries are parsed using question-answering techniques [2]. That method achieves 0 . 25 in P@10, but performs worse than FSDM on keyword queries. Section 4.3 further studies the influence of query types on entity-ranking accuracy.
The second experiment studies the contribution of fields and feature groups to learning to rank models. For each field or feature group, we compare the accuracy of models when used without field or features from that group to those with all features. The change in accuracy indicates the contribu-tion of the corresponding field or feature group. The field and feature studies for RankSVM are shown in Figures 2a and 2b respectively. The x-axis is the field or feature group stud-ied. The y-axis is the performance difference between the two conditions (All versus held out). Larger values indicate greater contributions. Figure 2a organizes features by the fields they are extracted from, including Name , Cat , Attr , RelEn , and SimEn . Figure 2b organizes features into five groups, with one retrieval model per group. SDM related contains FSDM and SDM scores as they are very correlated.
Figure 2a shows that RankSVM favors different fields for dif-ferent query sets. The Name field is useful for ListSearch and QALD-2 , but does not contribute much to the other two query sets. RelEn provides the most gain to keyword queries, but is not useful at all for the natural language question queries in QALD-2 . For feature groups we find that SDM related fea-tures are extremely important and provide the most gains across all query sets. This result is expected because all of the queries are relatively long queries and often contain phrases, which is where SDM is the most useful. SemSearch ES
Coordinate Ascent 0 . 396  X  X  (+2.6%) 0 . 295  X  X  (+3.2%) 0 . 206 ListSearch
Coordinate Ascent 0 . 225  X  X  X  (+10.5%) 0 . 300  X  X  X  (+17.3%) 0 . 229
Coordinate Ascent 0 . 121  X  X  (+8.9%) 0 . 275  X  (+4.6%) 0 . 224
Coordinate Ascent 0 . 208  X  (+6.6%) 0 . 141  X  X  (+3.2%) 0 . 115
Coordinate Ascent 0 . 245  X  X  X  (+5.8%) 0 . 248  X  X  X  (+7.2%) 0 . 189
Previous experiments found that when different models are trained for different types of queries, each model favors different types of evidence. However, in live query traffic dif-ferent types of queries are mixed together. The third exper-iment investigates the accuracy of a learning to rank entities system in a more realistic setting. The four query sets are combined into one query set, and new models are trained and tested using five fold cross validation as before.
Table 3a All shows the average accuracy when different models are trained for each of the four types of query. Ta-ble 3b shows the accuracy when a single model is trained for all types of queries. Despite being trained with more data, both learning to rank algorithms produce less effective models for the diverse query set than for the four smaller, focused query sets. Nonetheless, a single learned model is as accurate as the average accuracy of four carefully-tuned, query-set-specific FSDM models.

This results suggests that diverse query streams may ben-efit from query classification and type-specific entity ranking models. They may also benefit from new types of features or more sophisticated ranking models. and a negative value indicates a loss.
This paper uses learning to rank models, the state-of-the-art in document ranking, for more accurate ad-hoc entity ranking. Entities are represented by multi-field documents constructed from RDF triples. How well a query matches an entity document is estimated by text similarity features and a learned model. Experiments on an entity-oriented test collection reveal the power of learning to rank for entity retrieval. Moreover, statistically significant improvements over the previous state-of-the-art are observed on all evalu-ation metrics.

Further analysis reveals that query types play an im-portant role in the effectiveness of learned models. Text similarity features are very helpful for keyword queries, but less effective with longer natural language question queries. Learned models for different query types favor different entity fields because each query type targets different RDF predicates. This difference between query types is a new challenge to the use of entities in diverse search environments, because currently a single learned model does not provide much gain on average. An interesting future research direction is to automatically detect the type of entity search required for a query or task, and then use a model adapted for that type or task.
This research was supported by National Science Founda-tion (NSF) grant IIS-1422676. Any opinions, findings and conclusions expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors.
