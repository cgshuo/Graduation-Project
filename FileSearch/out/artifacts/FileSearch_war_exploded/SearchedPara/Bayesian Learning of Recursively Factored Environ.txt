 Marc Bellemare mg17@cs.ualberta.ca Joel Veness veness@cs.ualberta.ca Michael Bowling bowling@cs.ualberta.ca University of Alberta, Edmonton, Canada, T6G 2E8 The ability of a reinforcement learning agent to ac-curately model its environment can be leveraged in a number of useful ways. In simulation-based search, UCT (Kocsis &amp; Szepesv  X ari, 2006), POMCP (Silver &amp; Veness, 2010) and FSSS (Walsh et al., 2010) can all be used to produce refined value function estimates tailored to the current situation facing the agent. Within the context of Bayesian reinforcement learn-ing, forward search can provide a principled means to explore the environment (Asmuth &amp; Littman, 2011; Guez et al., 2012). Having access to a model also al-lows for hybrid techniques such as Dyna (Sutton, 1991; Silver et al., 2008) and TD-Search (Silver et al., 2012), which use a model of the environment to enhance the performance of more traditional model-free reinforce-ment learning techniques.
 Often, the environment dynamics are unknown a priori and model-based agents must learn their model from experience. This has recently motivated the develop-ment of a number of promising approaches; of note, Doshi-Velez (2009), Walsh et al. (2010), Veness et al. (2010), Veness et al. (2011), Nguyen et al. (2012), and Guez et al. (2012) have collectively demonstrated that it is feasible to learn good probabilistic models of small but challenging domains containing various degrees of partial observability and stochasticity.
 More often than not, however, the evaluation of model learning algorithms is performed over synthetic do-mains designed to illustrate particular algorithmic challenges. Such domains typically feature intricate environment dynamics over small, constrained obser-vation spaces, making the model learning problem per-haps unnecessarily difficult. In this work, we focus on the reverse situation: domains with high-dimensional observation spaces but fundamentally simple dynam-ics, such as the Arcade Learning Environment (Belle-mare et al., 2013), a collection of reinforcement learn-ing domains based on Atari 2600 games. Our aim in this paper is to provide algorithmic scaffolding that lets us scale existing, successful model learning algo-rithms to larger observation spaces.
 One promising approach for scaling model-based re-inforcement learning to larger observation spaces is to consider various kinds of factored models (Ross &amp; Pineau, 2008; Poupart, 2008; Diuk et al., 2009). A good choice of factorization can make the model learning task much easier by decomposing it into a number of more manageable sub-problems; the re-sultant performance then depends on the particu-lar choice of factorization. In this paper we intro-duce a meta-algorithm that efficiently performs exact Bayesian model averaging over a large class of recur-sively decomposable factorizations, allowing us to pro-vide strong competitive guarantees with respect to the best factorization within our class.
 Our approach becomes particularly meaningful in the context of developing domain-independent agents, i.e. agents that attempt to perform well over a large set of domains (Bellemare et al., 2013; Hausknecht et al., 2012). In particular, we apply our model averaging techniques to a recursive decomposition of the Atari 2600 image space to obtain efficient and accurate for-ward models on twenty different games. To the best of our knowledge, our model learning algorithm is the first to successfully handle observation spaces of this magnitude in a model-based reinforcement learning setting. Before we describe our model averaging technique, we first need to introduce some notation for probabilistic agents. The framework used in this paper was intro-duced by Hutter (2005), and can be seen as a natural generalization of a coding distribution , a mathemat-ical object from information theory that is used to assign probabilities to sequential data. This gener-alization works similarly to how (PO)MDPs (Puter-man, 1994; Kaelbling et al., 1998) generalize Markov Models and Hidden Markov Models to the control set-ting. This general presentation allows us to present our techniques in a way that is applicable to many differ-ent kinds of model-based reinforcement learning algo-rithms, including techniques for discrete (PO)MDPs. 2.1. Probabilistic Agent Setting This section describes our probabilistic agent setting, how factored models can be specified within it, and how Bayesian model averaging can be used over a class of factored models to predict nearly as well as the best factorization in a given class.
 We begin with some notation. A string x 1 x 2 ...x of length n is denoted by x 1: n . The prefix x 1: j x 1: n , j  X  n , is denoted by x  X  j or x &lt;j +1 . The no-tation generalizes to blocks of symbols: e.g. ax 1: n de-notes a 1 x 1 a 2 x 2 ...a n x n and ax &lt;j denotes the string a x 1 a 2 x 2 ...a j  X  1 x j  X  1 . The empty string is denoted by . The concatenation of two strings s and r is de-noted by sr . The finite action, observation, and reward spaces are denoted by A , O , and R respectively. We will also use X to denote the joint perception space O X R .
 We now define an environment to be a probability dis-tribution, parametrized by a sequence of actions, over possible observation-reward sequences. More formally, an environment  X  is a sequence of parametrized prob-ability mass functions {  X  0 , X  1 , X  2 ,... } , where that satisfies, for all n  X  N , for all a 1: n  X  A n , for all x with the base case defined as  X  0 ( | ) = 1. Equation 1 captures the intuitive constraint that an action per-formed at time n should have no effect on earlier per-ceptions x &lt;n . Where possible, from here onwards we will drop the index n in  X  n when the meaning is clear. Now, given an environment  X  , the predictive probabil-ity of a percept x n can be defined as  X  a version of the familiar product rule, i.e. Our notion of environment is used in two distinct ways. The first is as a means of describing the true underly-ing environment, which may be unknown to the agent. The second is to describe an agent X  X  model of the en-vironment. This model is typically adaptive, and will often only be an approximation to the true environ-ment. To make the distinction clear, we will refer to an agent X  X  environment model when talking about the agent X  X  model of the environment. Additionally, we will also introduce the notion of an -positive environ-ment model. This is defined as an environment model  X  satisfying  X  ( x n | ax &lt;n a n )  X  for some real &gt; 0, for all n  X  N , for all x 1: n  X  X n and for all a 1: n  X  A n . From here onwards, we assume that all environment models are -positive. 2.2. Factored Environment Models We now introduce some notation for environments whose percept spaces X can be expressed as X 1  X X 2  X   X  X  X  X X k , the Cartesian product of k  X  N subspaces. An example of such a factored space is depicted in Fig-ure 1. First, let X &lt;i := X 1  X  X  X  X  X X i  X  1 for 1  X  i  X  k . This notation will also generalize to n -dimensional Furthermore, given a string x 1: n  X  ( X 1  X  X  X  X  X X k ) n , we introduce the notation x i t  X  X i for 1  X  i  X  k and 1  X  t  X  n , to denote the i th component of x t . This will again generalize to the multi-dimensional case, with x Now, given an action space A and a factored percept space X := X 1  X  X  X  X  X X k , a k -factored environment is of the tuple is an environment model factor  X  := n  X  i n : ( A X X ) n  X  1  X A X X &lt;i  X  ( X i  X  [0 , 1]) o for 1  X  i  X  k , with each  X  i n defining a parametrized probability mass function. Using the chain rule, this naturally induces a factored environment given by where the final line uses the notation One can easily verify that a k -factored environment satisfies Equation 1, and is therefore a valid environ-ment. 2.3. Model Averaging over Factorizations Next we consider a Bayesian approach to learning a factorization online. Given a finite class M of candi-date factorizations, we can define a mixture environ-ment model that averages over the set of factorizations in M by where each real weight w  X  0 &gt; 0 and P  X   X  X  w  X  0 = 1. This is a direct application of Bayesian model aver-aging for our kind of probabilistic agent; see (Veness et al., 2011) for a more detailed discussion of this ap-proach. We can show that this method is justified whenever there exists a factorization  X   X   X  X  that pre-dicts well, since  X  log  X  ( x 1: n | a 1: n ) =  X  log X which implies that we only suffer a constant penalty when using  X  in place of  X   X  . This can be interpreted as a kind of regret bound, where the loss function is given by the code length (Gr  X unwald, 2007), i.e. the number of bits needed to encode the sequence of percepts x 1: n under the environment model  X  (  X | a 1: n ). By taking expectations on both sides with respect to the true environment  X  and rearranging, we see that this result implies This can be rewritten as which upon rearranging gives with D 1: n (  X  k  X  ) := P x denoting the KL divergence between the distributions highlights the per-step penalty  X  1  X  , rather than  X   X  , to predict x 1: n . As w  X   X  0 depend on n , this penalty vanishes asymptotically. Intuitively, the bound in Equation 5 implies that the environment model  X  is in some sense close, as n gets large, to the environment model that uses the best factorization in M .
 The main drawback with Bayesian model averaging is that it can be computationally expensive. Ideally we would like to weight over many possible candidate factorizations, but this typically isn X  X  possible if Equa-tion 4 is implemented directly. Our main contribution in this paper is to show that, by carefully choosing both the prior and the class of possible factorizations, model averaging can be used to derive a practical tech-nique that allows us to build probabilistic agents that do not need to commit to a particular factorization in advance. We now introduce our efficiently computable class of structured factorizations. Here we focus on a domain-independent presentation and defer to Section 4 a more concrete instantiation tailored to the Atari 2600 platform. We restrict our attention to factoring the observation space, noting that this imposes no restrictions as the reward can always be modelled by a separate environment model factor.
 Definition 1. A recursively decomposable space of nesting depth d = 0 is a set. When d  X  N , a recur-sively decomposable space is the set formed from the Cartesian product of two or more recursively decom-posable spaces of nesting depth d  X  1 . The number of factors in the Cartesian product defining a recursively decomposable space F at a particular nesting depth is denoted by dim( F ) , and is defined to be 1 if the nesting depth is 0 .
 For instance, a recursively decomposable space O with nesting depth 2 could be defined as: O := O 1  X O 2 ; O O 2 , 1 and O 2 , 2 being (arbitrary) sets. The recursive structure in O has a natural representation as a tree: where the set appearing at each non-leaf node is formed from the Cartesian product of its children. In this example, dim( O ) = dim( O 1 ) = dim( O 2 ) = 2 and dim( O 1 , 1 ) = dim( O 1 , 2 ) = dim( O 2 , 1 ) = dim( O In what follows, we will continue to use the notation O k to denote the k th factor of the Cartesian product defining the recursively decomposable space O . A re-cursively decomposable space can clearly be factored in many possible ways. In preparation for defining a prior over factorizations, we now define the set of all possible factorizations that can be applied to a partic-ular choice of recursively decomposable space. Definition 2. Given a recursively decomposable space F with nesting depth at least d  X  N , the set C d ( F ) of all recursive factorizations of F is defined by with C 0 ( F ) := F .
 Returning to our example, this gives the following set of factorizations: C 2 ( O ) = {O , O 1  X O 2 , O 1 , 1  X O O Notice that although the number of factorizations for the above example is small, in general the number of possible factorizations can grow super-exponentially in the nesting depth. 3.1. A Prior over Recursive Factorizations Now we describe a prior on C d ( F ) that will support efficient computation, as well as being biased towards factorizations containing less nested substructure. First note that in the recursive construction of C d ( F ) in Equation 6, there are essentially two cases to con-sider: either we stop decomposing the space F (corre-sponding to the {F} term in Equation 6) or we con-tinue to split it further. This observation naturally suggests the use of a hierarchical prior, which recur-sively subdivides the remaining prior weight amongst each of the two possible choices. If we use a uniform weighting for each possibility, this gives a prior weight-of stop/split decisions needed to describe the factor-ization f  X  C d ( F ), with the base case of  X  0 ( f ) := 0 (since when d = 0, no stop or split decision needs to be made). This prior weighting is identical to how the Context Tree Weighting method (Willems et al., 1995) weights over tree structures, and is an applica-tion of the general technique used by the class of Tree Experts described in Section 5 . 3 of (Cesa-Bianchi &amp; Lugosi, 2006). It is a valid prior, as one can show P One appealing side-effect of this recursive construction is that it assigns more prior weight towards factoriza-tions containing smaller amounts of nested substruc-ture. For instance, returning again to our example, 2 less obvious property is that the prior implicitly con-tains some structure that will make model averaging easier. How this works in our reinforcement learning setting will become clear in the next section. 3.2. Recursively Factored Mixture Now that we have our prior over factorizations, we pro-ceed by combining it with the model averaging tech-nique described in Section 2.3 to define the class of recursively factored mixture environment models. The first step is to commit to a set of base environment model factors from which each factored environment is formed. More precisely, this requires specifying an environment model factor for each element of the set of possible stopping points with S 0 ( F ) := {F} , within a recursively decomposable space F of nesting depth d . In our previous example, S means that we would need to specify 7 environment model factors. Each environment model factor needs to have an appropriate type signature that depends on both the history and the parts of the percept space preceding it. For instance, an environment model fac-tor processing O 2 at time n in our previous example would depend on a history string that was an element of the set ( A X O ) n  X  1  X A X O 1 . Similarly, an environ-ment model factor for O 2 , 2 would depend on a history given a history ax 1: n and a d  X  0 times recursively decomposable space F , a recursively factored mixture environment is defined as where each factored model is defined by a product of environment model factors Here T ( X ) := {X i } k i =1 denotes the set of subspaces of a given factored percept space X and x X i 1: n  X  X n denotes the component of x 1: n corresponding to X i . The base case of Equation 7, when d = 0, yields  X  Notice that there exists a significant amount of shared structure in Equation 7, since each environment model factor can appear multiple times in the definition of each factored environment model. This property, along with the recursive definition of our prior in Sec-tion 3.1, allows us to derive the identity 2  X  This identity, whose proof we defer to the appendix, constitutes the core theoretical result of this work: it allows Equation 7 to be computed efficiently.
 Equation 8 is essentially an application of the Gener-alized Distributive Law (Aji &amp; McEliece, 2000), a key computational concept underlying many efficient algo-rithms. By using dynamic programming to compute each  X  d F term only once, the time overhead of perform-ing exact model averaging over C d ( F ) is reduced to just O ( n |S d ( F ) | ). Furthermore, provided each base envi-ronment model factor can be updated online and the can be processed online in time O ( |S d ( F ) | ). A concrete application of this method will be given in Section 4. Finally, as our technique performs exact model averaging, it is straightforward to provide theoretical performance guarantees along the lines of Section 2.3. Theorem 1. Given a recursively decomposable space F with nesting depth d  X  N , for all a 1: n  X  A n , for all x 1: n  X  X n , for all f  X  X  d ( F ) , we have and for any environment  X  , Hence our technique is asymptotically competitive with the best factorization in C d ( F ). Section 3 provides a general framework for specify-ing recursively factored mixture environment models. The aim of this current section is to give an example of how this framework can extend existing model learning algorithms to domains with large observation spaces, such as Atari 2600 games. The quad-tree factoriza-tion (QTF) technique we now describe is particularly suited to image-based observation spaces. Although our factorization is presented in Atari 2600 terms, it is easily extended to other domains whose observation space exhibits two-dimensional structure.
 Following the notation used by Bellemare et al. (2012a) to describe Atari 2600 observation spaces, let D x and D y denote totally ordered sets of row and column indices, with the joint index space given by D := D x  X  D y . In our case, D x = { 1 , 2 ,..., 160 } and D y = { 1 , 2 ,..., 210 } . Denote by C a finite set of possible pixel colours. We define a pixel as a tuple ( x,y,c )  X  X  x  X D y  X C , where x and y denote the pixel X  X  row and column location and c describes its colour. An observation o is defined as a set of |D| pixels, with each location ( x,y )  X  X  uniquely corresponding to a single pixel ( x,y,c ). The set of all possible observations is denoted by O .
 We now describe a natural way to recursively de-compose O by dividing each region into four equal parts; an example of such a decomposition is shown in Figure 2. For a given D 0 x  X  D x whose ordered elements are x 1 ,x 2 ,x 3 ,...,x n , denote by l( D 0 x ) := x 1 ,x 2 ,...,x b n/ 2 c and by h( D x halves of D 0 x ; similarly let l( D 0 y ) and h( D note the two halves of D 0 y  X  D y . Let O D 0 ( x,y,c ) : x  X  X  0 x ,y  X  X  0 y , ( x,y,c )  X  o : o  X  X  be the set of image patches that can occur in the region defined by D 0 x and D 0 y , such that O = O D suming for now that |D x | and |D y | are both divisible O is then recursively defined as F The base case corresponds to indivisible image patches used to form larger image patches. Note that there is a one-to-one correspondence between elements of
D x , D y and O . Whenever |D x | or |D y | is not divis-simple solution is to enlarge D x and D y appropriately and insert pixels whose colour is a special out-of-screen 4.1. Algorithm Algorithm 1 provides pseudocode for an online im-plementation of QTF. The algorithm is invoked once Algorithm 1 Online Quad-Tree Factorization Require: A quad-tree decomposable space F Require: A nesting depth d  X  X  0 } X  N Require: A percept x t at time t  X  N qtf ( F ,d,x t )
Update  X  F with x F t if d = 0 then else per time step; its arguments are the top-level space
D x , D y , its nesting depth d , and the current per-cept. The algorithm recursively updates the base en-vironment model factor  X  F corresponding to F as well as its factors {F i } 4 i =1 . The  X  F and  X  d ables respectively store the values  X  F ( x F 1: n | a 1: n  X  ( x 1: n | a 1: n ); both sets of variables are initialized to 1. As QTF is a meta-algorithm, any suitable set of base environment model factors may be used. In our implementation we avoid numerical issues such as un-derflow by storing and manipulating probabilities in the logarithmic domain. We evaluated our quad-tree factorization on the Ar-cade Learning Environment (ALE) (Bellemare et al., 2013), a reinforcement learning interface to more than sixty Atari 2600 games. The observation space we consider is the 160  X  210-pixel game screen, with each pixel taking on one of 128 colours. When emulating in real-time, game screens are generated at the rate of 60 frames per second; a full five-minute episode lasts 18,000 frames. The action space is the set of joy-stick motions and button presses, giving a total of 18 distinct actions. Similar to previous work using ALE (Bellemare et al., 2013; Naddaf, 2010; Bellemare et al., 2012a;b), we designed and optimized our algorithm us-ing five training games and subsequently evaluated it on a hold-out set of fifteen testing games. 5.1. Experimental Setup For practical reasons, we used a quad-tree factoriza-tion that considered image patches whose size ranged from 32  X  32 down to 4  X  4; this corresponds to a nest-ing depth of 3. Empirically, we found that larger patch sizes generalized poorly and smaller patches performed worse due to limited contextual information. Each patch was predicted using a context tree switching (CTS) model (Veness et al., 2012) based on ten neigh-bouring patches from the current and previous time steps, similar to the P -context trees of Veness et al. (2011). We also encoded the most recent action as an input feature to the CTS model. To handle the large image patch alphabets, we used the recently developed Sparse Sequential Dirichlet (SSD) estimator (Veness &amp; Hutter, 2012) at the internal nodes of our CTS model. To curb memory usage and improve sample efficiency, we incorporated a set of well-established techniques into our implementation. For each patch size, a single CTS model was shared across patch locations, giving a limited form of translation invariance. Finally, each CTS model was implemented using hashing (Willems &amp; Tjalkens, 1997) so as to better control memory us-age. In practice, we found that larger hash tables al-ways resulted in better results.
 We compared QTF with factored models that used a fixed patch size (4  X  4, 8  X  8, 16  X  16, 32  X  32). Each model was trained on each game for 10 million frames, using a policy that selected actions uniformly at ran-dom and then executed them for k frames, where k was also chosen uniformly at random from the set K := { 4 , 8 , 12 , 16 } . This policy was designed to visit more interesting parts of the state space and thus gen-erate more complex trajectories. The models received a new game screen every fourth frame, yielding 15 time steps per second of play. Predicting at higher frame rates was found to be easier, but led to qualitatively similar results while requiring more wall-clock time per experiment. 5.2. Results After training, we evaluated the models X  ability to pre-dict the future conditional on action sequences. This evaluation phase took place over an additional 8000 frames, with action sequences again drawn by select-ing actions uniformly at random and executing them for k  X  K frames. At each time step t , each model was asked to predict 90 steps ahead and recorded how many frames were perfectly generated. Predictions at t + k were thus made using the true history up to time t and the k  X  1 already sampled frames.
 Table 1 summarizes the result of this evaluation. Which patch size best predicts game screens depends on a number of factors, such as the size of in-game objects, their frame to frame velocity and the pres-ence of animated backgrounds. QTF achieves a level of performance reasonably close to the best patch size, above 95% in 13 out of 20 games. In some games, QTF even improves on the performance of the best fixed-size model. Ultimately, Theorem 1 ensures that QTF will asymptotically achieve a prediction accuracy comparable to the best decomposition available to it. Sequential, probabilistic prediction algorithms such as QTF are often evaluated based on their n -step average logarithmic loss, defined as  X  . This measure has a natural information theoretic interpretation: on average, losslessly encoding each percept requires this many bits. While counting the number of correct future frames is a conservative measure and is heavily influenced by the sampling process, the logarithmic loss offers a more fine-grained notion of predictive accuracy. As shown in Table 2, our quad-tree factorization achieves significantly lower per-frame loss than any fixed-size model. In view that each frame contains 160  X  210  X  7 = 235 , 200 bits of data, our gains in Pong and Freeway  X  about 3 bits per frame  X  are particularly important. The benefits of predicting observations using envi-ronment model factors corresponding to larger image patches are twofold. Large regular patterns, such as the invaders in Space Invaders , can easily be repre-sented as a single symbol. When using a base model such as CTS, which treats symbols atomically, sam-pling from QTF is often faster as fewer symbols need to be generated. Thus our quad-tree factorization pro-duces a forward model of Atari 2600 games which is both efficient and accurate.
 In our experiments, we used Context Tree Switching (CTS) models as our environment model factors. One limitation of this approach is that CTS has no provi-sion for partial symbol matches: altering a single pixel within an image patch yields a completely new symbol. This presents a significant difficulty, as the size of the alphabet corresponding to F grows exponentially with its nesting depth d . The results of Table 2 are symp-tomatic of this issue: larger patch size models tend to suffer higher loss. As our meta-algorithm is indepen-dent of the choice of environment model factors, other base models perhaps better suited to noisy inputs may improve predictive accuracy, for example locally linear models (Farahmand et al., 2009), dynamic bayes net-works (Walsh et al., 2010), and neural network archi-tectures (Sutskever et al., 2008). In this paper, we introduced the class of recursively de-composable factorizations and described an algorithm that performs efficient Bayesian model averaging in a reinforcement learning setting. We instantiated our technique into a recursive quad-tree factorization of two-dimensional image spaces which we used to learn forward models of 160  X  210-pixel Atari 2600 games. To the best of our knowledge, our QTF model is the first to tackle observation spaces of this magnitude within the model-based reinforcement learning setting. We also expect that many existing algorithms can be im-proved by an appropriate combination with QTF. While our results show that QTF allows us to obvi-ate choosing a factorization a priori, there remains the question of how to best act given such a model of the environment. Recently, Joseph et al. (2013) argued that, when no member of the model class can repre-sent the true environment, a mismatch arises between model accuracy and model usefulness: the best policy may not rely on the most accurate model. In the fu-ture, we will be integrating our modeling techniques with simulation-based search to further explore this issue in Atari 2600 games.
 A.1. Proof of Equation 8 Proof. Along the lines of the argument used to prove Lemma 2 in (Willems et al., 1995), we can rewrite  X  = X = = = =
