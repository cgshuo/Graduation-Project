 Due to the large amount of uncertain data emerging from a variety of application domains, probabilistic databases have received a lot of attentions recently [3,1]. For example, the rent of an automatically extracted apartment from the web may be missing and is assigned a range of possible prices [ low, upper ]withthe uniform distribution. In a sensor network, the measurement may be inaccurate due to noises and is usually presented as a continuous probability distribution on possible values. Typically, to handle such uncertain data, an uncertain table is used to store a set of tuples each of which comes with a score value represented as a continuous probability density function (pdf).

On the continuous uncertain data, ranking groups of records by their aggre-gate values and returning the K groups with the highest aggregates arises natu-rally in many application domains. In the context of web, it may be required to group the extracted objects into categories and find the top-K categories based on aggregates on continuous uncertain attributes. In a sensor network, it may be required to group sensor readings by location to find the top-K locations based on average temperature where each temperature is presented as a contin-uous probability distribution. All these applications motivate the need for formal formulation and efficient processing techniques for top-K aggregate queries on continuous uncertain data, which is the aim of this paper.

Figure 1 illustrates a top-K aggregate query issued on a table with 10 tuples t , t 2 , ... , t 10 each of which has a score attribute represented as a continuous probability density function (pdf). The semantic of the top-K aggregate query is first dividing all the tuples into four groups according to attribute A , resulting returning the 2 groups with the largest sum aggregates. Since the scores of the tuples in each group are continuous random variables described as pdfs, the sum aggregate on them also produces a continuous random variable described as a pdf (called group aggregate pdf or aggregat e pdf in the following). Suppose that the sum aggregate on groups g 1 , g 2 , g 3 ,and g 4 leads to group aggregate pdfs  X  and  X  4 and the top-2 largest groups are returned to the user.

In this paper, the groups are ranked according to the parameterized ranking function ( PRF ) proposed in [7], which can simulate or approximate a variety of ranking functions with approximate parameter choices. In [7], a generating functions framework is used to compute the PRF values. This produces a cubic algorithm. Their work aims to rank tuples according to their scores, where the pdfs representing the scores are given d irectly. However, in our work, the pdfs which are used to compute the PRF values are derived from the aggregations of the tuples in each group, and thus the computing of the top-K aggregate query requires more time cost. In this paper, w e present another equivalent method for computing the PRF values, which produces a quadratic algorithm. To further improve the performance, we develop pruning techniques and adaptive strategy that avoid computing the exact PRF values of some groups that are guaranteed not to be in top-K . Our experimental study shows the efficiency of our techniques over several datasets with continuous attribute uncertainty.

This paper is organized as follows. Section 2 reviews the related work on rank-ing uncertain data. Section 3 formally formulates the top-K aggregate queries on continuous uncertain data. Section 4 presents the algorithms, including the basic algorithm and optimizations. Section 5 presents experimental results, and Section 6 concludes the paper. Recently, ranking uncerta in data has drawn many research interests, with dif-ferent ranking semantics proposed, such as tuple probability based rank [10], U-Top and U-Rank [13], probabilistic threshold top-K [5], expected rank [6], and typical top-K queries [4]. There are also other research work aiming to improve the ranking performance [16,14].

However, the aforementioned research work mainly focused on discrete uncer-tainty. Soliman and Ilyas [12] considered the problem of ranking continuous un-certain data, considering UTop-Rank , UTop-Prefix , UTop-Set ,and Rank-Agg semantics. Probabilistic ranked ( PRank ) query and probabilistic inverse ranking ( PIR ) query are formulated and tackled in [15] and [8] respectively. Jian Li and Amol Deshpande [7] further presented a PRF based framework for ranking continuous uncertain data, which can simulate or approximate a variety of ranking functions with appropriate parameter choices. Cheng et al. [2] stud-ied aggregate queries on continuous uncertain data, . However, top-K aggregate queires were not considered in their work.

Soliman and Ilyas [11] considered the top-K aggregate queries on discrete uncertain data. In this paper we focus on the top-K aggregate queries on con-tinuous uncertain data. Note that the number of possible worlds corresponding to the discrete uncertainty is finite, while the number of possible worlds corre-sponding to the continuous uncertainty is uncountable. This makes the semantic formulation and efficient ranking techniques such as approximation and pruning for the top-K aggregate queries on discrete uncertain data cannot apply to the continuous uncertain data directly. In this section, we formally formulate the top-K aggregate query when each tuple is associated with a score described as a continuous probability distribution. has a corresponding score s i described as a pdf (probability density function) f i . All the scores are considered as independent continuous random variables. Given a set of grouping attributes A and a group aggregate function Agg ( sum , count , avg , max , min ), a top-K aggregate query first divides T into m groups G = { g aggregate values for each group, and returns the K groups with the largest aggregate values.
 for sum , avg , max , min are derived as probability density functions (called sum: The pdf of the sum of s 1 and s 2 can be computed as: inductively. as follows: max: The pdf of the max of s 1 , s 2 , ... , s | g | can be computed as: min: The pdf of the min of s 1 , s 2 , ... , s | g | can be computed as: The count aggregate has the same semantic as in the traditional database and simply returns the number of tuples in group g . Hence, in the following we will only focus on the above four aggregates.

After computing the aggregate for each group, the next step is to find the K groups with the largest aggregates and return to the user. Since the aggregate of each group is corresponding to a random variable described as a probability density function, there are a variety of different approaches to ranking the groups. In this paper, we adopt the parameterized ranking function ( PRF )proposed in [7] to rank the groups.

The parameterized ranking function ( PRF ),  X  : G  X  R is defined to be: non-increasing real numbers that weight the probabilities that g is ranked at the Atop-K aggregate query returns the K groups with the highest PRF values. PRF s can approximate a variety of different ranking functions through appro-priate choices of the weight param eter values. For example, when  X  1 =1,  X  2 =0, ... ,  X  m = 0, all the groups are ranked by the probabilities that they have the largest aggregate; when  X  1 =1,  X  2 =1, ... ,  X  h =1,  X  h +1 =0, ... ,  X  m =0, all the groups are ranked by the probabilities that they are ranked in the first h places; when  X  1 = m  X  1,  X  2 = m  X  2, ... ,  X  m =0,the PRF values will produce the same ranking as the expected ranks [6].

In [7], a generating functions framework is proposed to compute the PRF value for ranking uncertain tuples. This produces a cubic algorithm. The real line is partitioned into small intervals such that the pdf of each tuple can be approximated as a single polynomial in each small interval. For each small in-terval I j ,let M j be the set of tuples whose pdf support (the support of pdf f is computing the PRF valueforeachtupleis O ( j m 3 j ).

Next, we shall describe another equivalent computing process which produces a quadratic algorithm and further inspires our pruning approach.

For groups G = { g 1 ,g 2 ,...,g m } ,weuse  X  ( g 1 ) as an example to illustrate the process of computing the PRF values. In the following, we use  X  1 ,  X  2 , ... ,  X  m to denote the pdfs of the aggregates of g 1 , g 2 , ... , g m respectively.  X  ( g 1 ) can be computed as =  X  1 P ( r ( g 1 )=1)+  X  2 P ( r ( g 1 )=2)+ ... +  X  m P ( r ( g 1 )= m ) =  X  1 where F ( m ) i ( x )(1  X  i  X  m ) can be computed recursively as follows: and For any other group g i in G ,  X  ( g i ) can be computed similarly by considering g as the first group, and the other groups are sorted arbitrarily. The order of the other groups does not affect the computing result for  X  ( g i ). However, taking different orders may require different time costs. We shall describe the order of the groups adopted in our pruning based optimization in the next section. In the next section, we shall show that the time cost of computing the PRF value for each group g i is O ( N  X  i m 2 )where N  X  i denotes the number of subintervals of the support of  X  i divided by  X  in the Simpson method. We also show that the computing process can be further simp lified and the time costs are reduced to O ( N X  i | G 2 | 2 )where G 2 is the number of groups whose aggregate pdf support overlap with the aggregate pdf support of g i .

In this paper, we assume that all the pdfs have bounded support. Hence, all the integrals in the above formulas defined with infinite integral limits can be transformed into equivalent integrals with finite limits. For example, suppose variable has a pdf with unbounded support, we truncate the distribution and ignore the tail with minuscule probability. Suppose X is a random variable with mean  X  and variance  X  2 ,wehave P ( | X  X   X  | X  t X  )  X  1 t 2 which can be derived from the Chebychev X  X  Inequality. Hence, by selecting appropriate t , a bounded dominating range of the support can be decided. The general framework f or implementing a top-K aggregate query is first divid-ing all the tuples into groups, computing the aggregate for each group, and then returning the groups with the K largest PRF values. The process of dividing all the tuples into groups is the same as in the traditional database. In the following, we shall focus on computing the aggregates and selecting the K groups with the largest PRF values. 4.1 Basic Algorithm Suppose that all the tuples in T = { t 1 ,t 2 ,...,t n } have been divided into groups G = { g 1 ,g 2 ,...,g m } . The basic algorithm first computes the aggregates for each group, i.e.,  X  1 ,  X  2 , ... ,  X  m ,accordingtoEquation (1),(2),(3),or(4),computes and returns the corresponding groups.
 All the integrals are computed by the Simpson method. For a function f : R X  R on an interval [ a, b ], the integral of f can be computed as follows according to the Simpson method: where [ a, b ] is divided into n subintervals with equal length  X  , and in each subin-terval [ x i ,x i +1 ], Note that all the integral limits  X  X  X  and +  X  are transformed to the left end point and right end point of the support of the integrand. According to the defi-nitions of the aggregates in Section 3, the supports of  X  i for the sum , avg , max , and min aggregates are shown in Table 1. Here, we suppose that g i contains tu-We use supp ( f j ) to denote the support of f j and L supp ( f
Inthefollowing,weuse N f i and N  X  i to denote the number of subintervals of the supports of f i and  X  i divided by  X  in the Simpson method. For the sum is the number of tuples in group g i and N avg is the average number of subin-tervals of the supports of f i s divided by  X  in the Simpson method. For the max and min aggregates, the time cost for computing  X  i is O ( | g i | N max ), where N O ( N  X  i m 2 ), where m is the number of groups. Hence, the total cost of the basic and O ( i ( | g i | N max + N  X  i m 2 )) when the aggregates are max and min ,where the summations are over all the groups. 4.2 Optimization 1: Pruning K largest. In this section, we present pruning method which can decide whether agroup g i is in the top K largest without computing the exact value of  X  ( g i ) and hence reduces the overall time cost.

We use the following three pruning heuristics: (H1). For groups G = { g 1 ,g 2 ,...,g m } ,each  X  ( g i ) can be computed as follows: g is considered as the first group, and each other group is considered successively in an arbitrary order. For symbolic simplicity, we write g i as g 1 .Wefirstconsider g
When only g 1 and g 2 are considered,  X  (2) ( g 1 )=  X  1 P ( r ( g 1 )=1)+  X  2 P ( r ( g 1 )=2) pdf of g 1 and the probability that the aggregate of g 2 is less than x ; F 2 ( x )=  X  the probability that the aggregate of g 2 is greater than x .
 as When g u +1 is added,  X  ( u +1) ( g 1 ) can be computed as = From (13), we can further get  X   X  largest groups retrieved so far. We use L to store the smallest PRF value of the top K largest groups retrieved so far. For each other group g i , we compute  X  ( u ) ( g i )from u =2to u = m successively. If  X  ( u ) ( g i ) &lt;L for some u ,then g i cannot be in the top K aggregates and further computations are avoided. Otherwise, we compute the exact value of  X  ( g i ), and insert it into the top-K largest groups, and the group with the smallest PRF value in the original top-K sequence is thrown away. three categories: (i)groups G 1 = { g  X  G | L supp (  X  ) &gt;R supp (  X  the aggregate pdf of g and L supp (  X  ) denotes the left end point of the support G of g and supp (  X  )and supp (  X  i ) denote the supports of  X  and  X  i respectively, and  X  denotes the empty set; (iii) groups G 3 = { g  X  G | R supp (  X  ) &lt;L supp (  X  where  X  denotes the aggregate pdf of g and R supp (  X  ) denotes the right end point of the support of  X  and L supp (  X  of  X  i . When computing  X  ( g i ), only the groups in G 2 need to be considered. We ... , P ( r ( g i )= | G 2 | ) have been computed based on the groups in G 2 ,then computing  X  ( g i )from O ( N  X  i | G | 2 )to O ( N X  i | G 2 | 2 ).

When computing  X  ( g i ), we can combine (H1) and (H2), only consider the groups in G 2 , and replace  X  1 , X  2 ,..., X  u with  X  | G the corresponding computing equations (11), (12), (13) and (14) of (H1). (H3). For two groups g and g in G ,weuse  X  and  X  to denote their aggregate pdfs respectively. If the right end point of the support of  X  is less than the left end point of  X  ,then g must be ranked after g . Hence, for a group g in G ,if the support of its aggregate pdf has a right end point less than the K th smallest left end point of the supports of the aggregate pdfs of the groups in G ,then g must not be in the top K largest groups and can be neglected. After finding the K th smallest left end point  X  of the aggregate pdfs of the groups in G ,thetime cost of computing  X  ( g i ) can be avoided if the overall support of  X  i falls to the left of  X  .

Note that when applying (H3), if we can decide that group g i is not in the top-K only according to its aggregate pdf support which can be computed as illustrated in Table 1, then the computing of the aggregate pdf  X  i can be avoided and the corresponding time cost is saved.

Based on the above three heuristics, we develop our pruning as follows: We first find the K th smallest left end point  X  of the aggregate pdf supports of the groups in G , and neglect all the groups whose overall aggregate pdf supports fall to the left of  X  . All the remaining groups are sorted according to their left end points of aggregate pdf supports, and stored in a list L . Next, we construct aheap H from the first K groups in L according to their PRF values. Then, we retrieve a new group g new from L and compute its upper bound by using heuristics H1 and H2. If the upper bound is less than the lowest PRF value in
H ,then g stopped. Otherwise, the computing process in heuristic H1 and H2 is continued until we can decide that g new is not a top K largest group or the exact value of  X  ( g new ) is computed. If  X  ( g new ) is larger than the lowest PRF value in H ,the group with the lowest PRF value is deleted from H and g new is inserted into H . This process is repeate d until all the groups in L have been processed. Finally, all the groups in H are sorted and returned to the user. 4.3 Optimization 2: Adaptive Strategy In the basic algorithm and the pruning based optimization discussed until now, a user specified small positive real number  X  is used to divide the integral inter-val into subintervals when computing the corresponding integrals. Smaller  X  will make the algorithm more accurate and more time consuming; larger  X  will make the algorithm less accurate and less time consuming. We further take an adaptive strategy which can choose appropriate value for  X  automatically. We first estimate of all the tuple pdfs and aggregate pdfs) in the process of computing the integrals, upper i &lt;lower Kth ,where lower Kth is the lower bound for the Kth largest group retrieved so far, then group g i must not be in the top-K list and can be neglected; (ii) if lower i &gt; upper Kth ,where upper Kth is the upper bound for the Kth largest group retrieved so fa r, then the original Kth largest group must not be in the top-K list and can be neglected, and group g i is inserted into the top-K largest groups lap, we cannot decide whether g i should be in the top-K list. To insert g i into the correct position in top-K for case (ii), or to distinguish g i from the Kth largest group in case (iii), the interval which produces the largest error in the estimation of  X  ( g i ) is divided into two subintervals and the corresponding integrals are re-computed in that corresponding interv als and a more accurate estimate for  X  ( g i ) is produced. This process is repeated until g i is inserted into the correct position or can be distinguished from the Kth largest group.

Note that in the process of the estimate of  X  ( g i )as[ lower i , upper i ], we adopt the adaptive Simpson method [9] where [ a, b ] is an interval with midpoint c , is the computing error for the son X  X  rule on the corresponding intervals. In Equation (12), if +  X   X  X  X  F 1 ( x ) dx , [ A u ,B u ] respectively, then  X  ( u ) ( g i ) can be estimated as [ cording to Equations (7) and (8). In this section, we present the experimental evaluation of the basic algorithm and optimizations for computing top-K aggregate queries on continuous uncertain data. Our experiments are conducted on a PC with double 2.20 GHz CPUs and 2GB RAM. The following five algorithms (abbreviated as (G), (B), (P), (A), (PA) respectively) are implemented in C++: (G): first computing all the group aggregates and then computing all the PRF values by using the generating functions framework proposed in [7]; (B): the basic algorithm presented in Section 4.1; (P): the algorithm after combining the pruning heuristics presented in 4.2; (A): the algorithm after combining the adaptive strategy presented in 4.3; (PA): the algorithm after combining the pruning heuristics and the adaptive strategy.
 For algorithm (G), (B), and (P), we use the smallest  X  produced by the adaptive strategy (A). The weights we use for computing the PRF values are  X  i =1 /i (1  X  i  X  m ). 5.1 Datasets We mainly use several synthetic datasets with various continuous distributions to study our algorithms.

Dataset-1-m -c : The aim of this dataset is to investigate the time costs of the various algorithms on the randomly generated dataset with continuous uncer-tainty. We synthesize a dataset containing 10000 groups with 100 tuples in each group. For each tuple, its score pdf is randomly selected from the four com-monly used distributions: Uniform Distribution, Normal Distribution, Gamma Distribution and Beta distribution.

Dataset-2-m-r : The aim of this dataset is to investigate the time costs of the various algorithms on the continuous data set with different maximum group aggregate pdf support overlap rates (the ratio of the maximum number of group aggregate pdfs which have overlapped supports with respect to the overall num-ber of group aggregate pdfs, abbreviated as MGAPSOR ).We synthesize 50 datasets each of which contains m (= 1000, 2000, ... , 10000) group aggregate pdfs with 5 different MGAPSORs r (= 10% , 30% , 50% , 70% , 90%). The group aggregate pdfs are randomly selected from the commonly used Uniform Distribu-tion, Normal Distribution, Gamma Distribution and Beta distribution. Since the MGAPSOR does not affect the time costs of the part of computing aggregate pdfs, we assume the aggregate pdfs for all the groups have been computed and synthesize them here for comparing the part of computing group PRF values. This is convenient for us to control the MGAPSOR .

Note that when a distribution is selected, its parameters (for example, the mean and variance of the normal distribution) are all generated uniformly from [0 , 1000]. The supports of the Uniform Distribution and the Beta Distribution are bounded naturally. The supports of all the Normal Distributions are truncated to S =[  X   X  r,  X  + r ] such that P ( S )=0 . 9999, and the supports of all the Gamma Distributions are truncated to S =[0 ,r ] such that P ( S )=0 . 9999. 5.2 Results In the figures included in this section, each graph shows the running times of the following five algorithms: (G), (B), (P), (A) and (PA). The y axis adopts the logarithmic coordinates.

Figure 2 shows the running times of the algorithms with respect to different number of groups. Graphs (a), (b), (c) and (d), correspond to the sum , avg , max ,and min aggregate respectively. We set K = 20. Each group has a fixed number of 100 tuples. We see that the time cost of Algorithm (G) is higher than Algorithm (B). The algorithm (PA) has a much lower time cost than the basic algorithm (B). As the number of groups increases, the pruning based opti-mization and the adaptive strategy are more effective for computing the top-K aggregate query than the basic algorithm.

We also run the algorithms on the datasets with different number of tuples in each group, and with different number of K . We all get the same conclusion that on our randomly generated datasets with continuous uncertainty, the basic algorithm (B) has a lower time cost than algorithm (G), and the algorithm after combining the pruning heuristics and the adaptive strategy (PA) has a much lower time cost than the basic algorithm (B).

Figure 3 shows the running times of the algorithms on Dataset-2-m-r. Graphs (a), (b), (c), (d), and (e) correspond to the MGAPSOR 0.1, 0.3, 0.5, 0.7, 0.9 respectively. We see that the time cost of Algorithm (B) is lower than Algorithm (G) when m increases to some fixed number. After adopting the pruning method and the adaptive strategy (PA), the time cost is lower than (G) and (B) for all m . For higher MGAPSOR , the pruning method and the adaptive strategy (PA) are more effective than (G) and (B). We formally formulated the top-K aggregate queries on continuous uncertain data, and developed computing algorithms and optimizations (pruning heuristics and adaptive strategy). We mainly focus on continuous attribute uncertainty in this paper and assume that all the attribute values are independent of each other. The extension to support tuple uncertainty and incorporating correlations are left as future work. Applying our algorithms to real world applications is also another important task for future work.
 Acknowledgement. The work is supported by National Natural Science Foun-dation of China (60773156, 61073004), Chinese Major State Basic Research Development 973 Program (2011CB302203-2), Important National Science &amp; Technology Specific Program (2011ZX01042-001-002-2), and research fund of Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology.
