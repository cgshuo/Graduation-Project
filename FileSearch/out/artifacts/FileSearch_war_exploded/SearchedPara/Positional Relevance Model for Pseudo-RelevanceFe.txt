 Pseudo-relevance feedback is an effective technique for im-proving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to ef-fectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilis-tic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to es-timate PRM based on different sampling processes. Exper-iment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback. H.3.3 [ Information Search and Retrieval ]: Retrieval models, Relevance feedback, Query formulation Algorithms Positional relevance model, pseudo-relevance feedback, po-sitional language model, proximity, passage-based feedback, query expansion
Pseudo-relevance feedback (or blind feedback) is an im-portant general technique for improving retrieval accuracy [26, 24, 27, 3, 25, 16, 32]. The basic idea of pseudo-relevance feedback is to assume that a small number of top-ranked doc-uments in the initial retrieval results are relevant and select from these documents related terms to the query to improve the query representation through query expansion, which generally leads to improvement of retrieval performance.
Most existing feedback algorithms (e.g., [26, 24, 27, 3, 25, 16, 32]) use a whole feedback document as a unit for select-ing expansion terms, which, however, is non-optimal when the content of a document is incoherent (i.e., covering sev-eral different topics) and thus may contain much irrelevant information as often happens in Web search. The existence of multiple topics and irrelevant information would lead to a noisy feedback model as potentially harmful terms from non-relevant topics may be picked up to include in the feed-back model. As a result, the use of pseudo feedback may not improve or even decrease the retrieval performance. Thus a critical challenge in improving all feedback methods is to effectively select from feedback documents those terms that are most likely relevant to the query topic.

In this paper, we solve this challenge by exploiting the po-sition and proximity information of terms as cues to assess if a term is related to the query topic. Since topically re-lated content is usually grouped together in text documents, terms closer to the occurrences of query words are, in gen-eral, more likely relevant to the query topic, thus a good feedback model should intuitively place higher weights on such terms.

Based on this intuition, we propose a novel positional rel-evance model (PRM) to incorporate the cues of term posi-tions and term proximity in a probabilistic feedback model based on statistical language modeling. The key idea is to extend the relevance model [16] to aggregate the associations between a term and query words at the position level via the positional language model (PLM) [19]. An important advan-tage of estimating a relevance model based on PLM is that it can model the  X  X elevant positions X  in a feedback document with probabilistic models so as to assign more weights to terms at more relevant positions in a principled way, thus leading naturally to selection of expansion terms more likely relevant to the query topic.

Since PRM estimates a relevance model at the level of term positions, it incorporates individual term positions di-rectly into a probabilistic model. This is in contrast with virtually all the existing pseudo feedback techniques which have only made use of term statistics at the document level [26, 24, 27, 3, 25, 16, 32, 18], or at the best, at the level of passages [2, 30, 17, 21] without distinguishing every different position.
Analogously to the two methods proposed for estimat-ing the relevance model [16], we also derive two methods for estimating PRM, leading to two different ways to aggre-gate term information based on positions. We evaluate the proposed PRM on two large TREC datasets. Experimental results demonstrate that PRM is effective in exploiting term proximity for pseudo feedback and significantly outperforms the relevance model in both document-based feedback and passage-based feedback.
Pseudo-relevance feedback has been shown to be effective with various retrieval models [26, 24, 27, 3, 25, 16, 32, 18].
In the vector space model, feedback is usually done by using the Rocchio algorithm, which forms a new query vec-tor by maximizing its similarity to pseudo-relevant docu-ments [26]. The feedback method in classical probabilis-tic models is to select expansion terms primarily based on Robertson/Sparck-Jones weight [24].

Several query expansion techniques have been developed in the language modeling framework, including, e.g., the mixture-model feedback method [32] and the relevance model [16]. The basic idea is to use feedback documents to estimate a better query language model. Both the mixture model and relevance model have been shown to be very effective, but the relevance model appears to be more robust [18].
In the mixture-model feedback [32], the words in feed-back documents are assumed to be drawn from two models: (1) background model and (2) topic model. The mixture-model feedback finds the topic model that best describes the feedback documents by separating the topic model from the background model. The topic model is then interpolated with the original query model to form the expanded query.
Much like mixture-model feedback, the relevance model also estimates an improved query language model. Given a query Q , a relevance model is a multinomial distribution P ( w | Q ) that encodes the likelihood of each term w given the query as evidence. To estimate the relevance model, the authors first compute the joint probability of observing a word together with the query words in each feedback doc-ument and then aggregate the evidence by summing over all the documents. It essentially uses the query likelihood P ( Q | D ) as the weight for document D and takes an aver-age of the probability of word w given by each document language model.

All these pseudo feedback algorithms use a whole feedback document as a unit, and thus term position and proximity evidences are largely ignored. Our work is an extension of the relevance model to estimate a feedback model based on individual term positions.
There have been several studies to exploit passage-level evidence of documents for feedback, e.g., [2, 30, 17], which can potentially address the heterogeneous topical structure of documents to some degree. However, these approaches usually take a traditional feedback model as a black box to handle sub-document units as if they were regular docu-ments. For example, Liu and Croft X  X  work [17] estimates a relevance model based on the best matching passage of each feedback document, where fixed-length arbitrary passages that resemble overlapped windows but with an arbitrary starting point [12] can often be used due to its effective-ness and efficiency [12, 17]. A limitation of this approach is that term positions are not directly incorporated into the feedback model. As we will show later in the paper, the pro-posed PRM outperforms such a passage feedback approach.
Some other approaches, e.g., [31, 4], make use of visual cues or eye tracker to improve passage feedback for web search: on the server side of a search engine, documents can be decomposed into topically different components via visual cues [31], while on the client side of users, gaze-based attention feedback [4] can go down to the sub-document level by exploiting evidence about which document parts the user looks at. However, such approaches face the same problems as general passage feedback without being able to model each individual position.

In fact, these passage-based or sub-document level feed-back models are orthogonal to the proposed PRM in the sense that PRM can be applied to passages to model prox-imity inside a passages in the same way as it can be applied to whole documents. Moreover, the underlying positional language model [19], which can capture passage-level evi-dence in a soft way in model estimation, has been shown to work better than imposing a  X  X ard X  boundary of passages. Recently, Metzler and Croft X  X  work on Latent Concept Expansion [21] also indirectly captures term position and proximity evidence through the use of appropriate passages. Their work provides a more general model which is com-plementary with our ideas in that we can use PRM as an effective feature defined on their graph, so our PRM scores can then be combined with other features explored in [21] to further improve its performance.
The term proximity heuristic, which rewards a document where the matched query terms occur close to each other, has been previously studied in [13, 14, 11, 9, 23, 20, 5, 6, 28, 19, 10, 34]. Keen X  X  work [13, 14] is among the earliest efforts, in which, a  X  X EAR X  operator was introduced to ad-dress proximity in Boolean retrieval model. The shortest interval containing a match set was first used as a mea-sure of proximity in [9, 11]. Recent work has attempted to heuristically incorporate proximity into an existing retrieval model (often through score combinations) [22, 23, 5, 6, 28]. For example, a variety of proximity measures were proposed and explored in [28]. Another work [10] used a learning approach to combine various proximity measures to obtain an effective proximity-based retrieval function. Recently in our previous work [19], we proposed a positional language model (PLM) for information retrieval, which not only cap-tured term proximity information but also covered passage retrieval in a unified language modeling approach. However, in all these studies, term proximity has been solely used for ranking documents in response to a given query rather than improving pseudo feedback.

There has been relatively little work done in the area of formally modeling term proximity heuristic in the context of pseudo feedback. However, there have been several at-tempts to simply combine term proximity with other feed-back heuristics to select good expansion terms. In [29], sev-eral distance functions were evaluated for selecting query ex-pansion terms from windows or passages surrounding query term occurrences; however, no improvement was observed as compared to existing feedback methods. Cao et al. [7] used a supervised method to classify whether an individual ex-pansion term is good or not, in which term proximity is one of their features. Their method only loosely combined term proximity with traditional feedback heuristics; in contrast, we incorporate term position and proximity into a proba-bilistic feedback model with more meaningful parameters.
In this section, we describe the proposed positional rele-vance model (PRM) which incorporates term position infor-mation into the estimation of feedback models so that we can naturally reward terms close to query terms in the feed-back documents and avoid including irrelevant terms in the feedback model.

The proposed PRM can be regarded as an extension to the relevance model (RM) [16]. We thus first give a brief introduction to the relevance model.
As a pseudo feedback method, the relevance model [16] has proven to be not only effective, but also robust in a re-cent study [18]. The basic idea is to use the query likelihood score of a feedback document as the weight and estimate a query language model (for feedback) based on weighted aggregation of term counts in the feedback documents.
Formally, let Q = { q 1 , q 2 ,  X  X  X  , q m } be a query and  X  rep-resent the set of smoothed document models for the pseudo feedback documents. One of the most robust variants of the relevance model (RM1) [18] is computed as follows [16]: where p (  X  D ) is a prior on documents and is often assumed to be uniform without any additional prior knowledge about document D . Thus, the estimated relevance model is essen-tially a weighted combination of individual feedback docu-ment model with the query likelihood score of a document as its weight.

After the relevance model is estimated, the estimated P ( w | Q ) can then be interpolated with the original query model  X  Q to improve performance [1].
 where  X  is a parameter to control the amount of feedback. In the rest of the paper, we will refer to this instantiation of relevance model as RM3 .
In the relevance model, the count of a term is computed over an entire feedback document. The main idea of the proposed positional relevance model (PRM) is to further distinguish different positions of a term and discount the occurrences of a term at positions that are far away from a query term in a feedback document.
 Similarly to RM, a PRM is also a multinomial distribution P ( w | Q ) that attempts to capture the probability that term w is seen in a relevant document. However, PRM goes be-yond RM to estimate the conditional probability P ( w | Q ) in terms of the joint probability of observing w with the query Figure 1: Dependence networks for two methods, i.e., method 1 (left) and method 2 (right), of esti-mating positional relevance models.
 Q at every position in every feedback document. Formally, where i indicates a position in document D , and F is the set of feedback documents (assumed to be relevant).
The challenge now lies in estimating the joint probabil-ity P ( w, Q, D, i ). Inspired by the two estimation methods proposed in [16] for estimating relevance models, we derive two methods similarly for estimating P ( w, Q, D, i ). The first method assumes that w is sampled in the same way as Q , while the second method assumes that w and Q are sampled using two different mechanisms.
In this method, we first compute the joint probability of observing a word together with the query words at each position and then aggregate the evidence by summing over all the possible positions. Specifically, we factor the joint probability P ( w, Q, D, i ) for each pseudo-relevant document D as follows:
Intuitively, we have assumed a generative model in which we would first pick a document according to P ( D ), then choose a position i in document D with probability P ( i | D ), and finally generate word w and query Q conditioned on D and i , with probability P ( w, Q | D, i ).

P ( D ) can be interpreted as a document prior and set to a uniform distribution with no prior knowledge about doc-ument D . While it is possible to estimate P ( i | D ) based on document structures, here we assume that every position is equally likely, i.e., P ( i | D ) = 1 | D | . Improving the estimation of p ( D ) and P ( i | D ) would be an interesting future work. An illustration of the dependencies between the variables involved in the derivation is shown on Figure 1 (left side).
After making these assumptions and a further assump-tion that the generation of word w and that of query Q are independent, we have
Plugging Equation 5 into Equation 3, we obtain the fol-lowing estimate of the PRM:
In the above equation, P ( w | D, i ) is the probability of sam-pling word w at position i in document D . To improve the efficiency of PRM, we simplify P ( w | D, i ) as:
The term P ( Q | D, i ) in Equation 6 is the key component in estimating the positional relevance model. It is the query likelihood at position i of document D , and we will discuss how to estimate it based on the positional language model [19] in Section 3.2.3. Additionally, there is a third term | D | in the equation, which penalizes long documents to prevent them from dominating the feedback model (long documents naturally have more positions).

Thus, Equation 6 essentially combines all terms in feed-back documents by assigning different weights to each term: (1) P ( Q | D, i ) serves as a relevance-based weight for each position in each document so that a position with many query terms nearby would have a higher weight. Thus as an intra-document weight, P ( Q | D, i ) can measure the relative weights of positions within a document: a position closer to query words would more likely generate the query, and as a result a term that occurs at this position would naturally receive a higher weight. (2) | D | comes into the formula be-cause of the assumption about uniform distribution over all the positions in a document and can be interpreted as an inter-document weight: it penalizes a long document which is reasonable since a longer document by nature has more positions and more occurrences of terms to contribute.
In this method, we consider the following different way to decompose the joint probability distribution: The assumed generative model is as follows. We first pick a query according to some prior P ( Q ). We then generate a document D with probability P ( D | Q ). Finally, we select a position i in D with probability P ( i | Q, D ) and generate word w according to P ( w | D, i ). An illustration of this sampling process is given on the right side of Figure 1.

For the purpose of estimating P ( w | Q ), we can clearly ig-nore the term P ( Q ) as it is a query-specific constant. Using Bayes Rule and assuming both P ( D ) and P ( i | D ) to be uni-form (as we have assumed in the first estimation method), we have
Plugging Equations 8, 9, and 10 into Equation 3, we ob-tain the following estimate of PRM: where P ( Q | D ) is the query likelihood score of document D , which can be computed using either the positional language models [19] or the standard document language model. In our experiments, we use the latter, i.e., P ( Q | D ) = Q
As in the first estimation method, we compute P ( w | D, i ) using Equation 7.

Similarly to the first estimation method, this second esti-mate of PRM also essentially combines all terms in feedback documents by assigning different weights to each term: the first weighting term in Equation 11 is seen to be the normal-ized query likelihood score of the document, which assigns more weights to documents that are more likely to be rel-evant, while the second weighting term is the normalized query likelihood of each positional language model, which assigns more weights to terms that are closer to query words.
Compared to the first estimation method, the document length normalizer | D | is missing, but a comparable effect is now achieved by normalizing the query likelihood of each positional language model P ( Q | D, i ). Indeed, the effect of intra-document weighting and inter-document weighting can now be seen even more clearly, i.e., the normalized P ( Q | D ) can be interpreted as the inter-document weight favoring a document matching the query well, while the normalized P ( Q | D, i ) clearly achieves intra-document weighting to place more weight on terms closer to query terms in document D .
This section provides the final estimation details for our positional relevance model (Equation 6 and 11), i.e., how to estimate P ( Q | D, i ). We adapt the positional language model [19] to do that.

The key idea of PLM is to estimate a language model for each position of a document. Specifically, we let each word at each position of a document propagate the evidence of its occurrence to all other positions in the document so that positions closer to the word would get more share of the evidence than those far away. The PLM at each position can then be estimated based on all the propagated counts of all the words to the position as if all the words had appeared actually at the position with discounted counts. This new family of language models is intended to capture the content of the document at a position level, which is roughly like a  X  X oft passage X  centered at this position but can potentially cover all the words in the document with less weight on words far away from the position.

Formally, the PLM at position i of document D can be estimated as: where c 0 ( w, i ) is the total propagated count of term w at position i from the occurrences of w in all the positions. Following [19], we estimate c 0 ( w, i ) using the Gaussian kernel function: where i and j are absolute positions of the corresponding terms in the document, and | D | is the length of the docu-ment; c ( w, j ) is the real count of term w at position j . With the approximation method proposed in [19], the following estimation of P ( w | D, i ) is obtained: where  X (  X  ) is the cumulative normal distribution and the denominator is essentially the length of the  X  X oft X  passage centered at position i .

However, there is one issue with the above estimation: the length of  X  X oft X  passages around the boundaries of a docu-ment would be smaller than that in the middle of the docu-ment; as a result, boundary positions tend to unfairly receive more weights. This may not raise problems in PLMs for re-trieval [19], but it is a more serious concern for PRM, where the relative weights of terms are more important. So we decide to use a fixed length for all  X  X oft X  passages in feed-back documents to estimate their corresponding positional language models as follows: This strategy has shown to be better than the original im-plementation in [19] for estimating PRM.

The distribution P (  X | D, i ) needs to be smoothed. Now that all  X  X oft X  passages have equal length, we use Jelinek-Mercer smoothing method to smooth PLM, which is shown to work as well as the Dirichlet prior smoothing method and is relatively insensitive to the setting of  X  in our experiments. where  X   X  [0 , 1] is a smoothing parameter and p ( w | C ) is the collection language model. Now we can compute the positional query likelihood score P ( Q | D, i ) for position i .
Plugging Equation 17 into Equations 6 and 11, we would be able to compute the two estimation methods directly. Interestingly, if we set  X  = 1 or  X  =  X  , Method 2 will degenerate to the general relevance model (see Equation 1).
The computation of positional query likelihood is the most time-consuming part in estimating PRM. Fortunately, there is no serious efficiency concern even with an unoptimized implementation. The reason is because we only need to tra-verse each position of a document twice: during the first pass, the positions of query terms are recorded; in the sec-ond, we compute a positional query likelihood for each po-sition directly based on the position information of query terms collected in the first pass. Therefore, the efficiency is comparable to the estimation of the relevance model.
Finally, the estimated positional relevance model P ( w | Q ) will also be interpolated with the original query model  X  using Equation 2 to improve performance with a similar pa-rameter  X  to that used in the mixture-model feedback [32] and RM3 [1].
We used two standard TREC datasets in our study: Ter-abyte (i.e., the Gov2 collection) and ClueWeb09 Category B. They represent two very large web text collections in En-glish. Queries were taken from the title field of the TREC topics. We used the Lemur toolkit (version 4.10) and In-dri search engine (version 2.10) 1 to implement our algo-rithms. For both datasets, the preprocessing of documents http://www.lemurproject.org/ and queries included stemming with the Porter stemmer and stopwords removing using a total of 418 stopwords from the standard InQuery stoplist. Table 1 shows some basic statis-tics about the datasets.

We evaluated seven methods. (1) The basic retrieval model is the KL-divergence retrieval model [15], and we chose the Dirichlet smoothing method [33] for smoothing document language models, where the smoothing parameter  X  was set empirically to 1500 . This method was labeled as  X  X oFB X . (2) The baseline pseudo feedback method is the relevance model  X  X M3 X  described in Section 3.1 [1], which is one of the most effective and robust pseudo feedback methods un-der language modeling framework [18]. (3) Another baseline pseudo feedback method is a standard passage-based feed-back model, labeled as  X  X M3-p X , which estimates the RM3 relevance model based on the best matching passage of each feedback document [17]. (4) We have two variations of PRM, i.e.,  X  X RM1 X  and  X  X RM2 X , which are based on the two es-timation methods described in Section 3.2, respectively. (5) In addition, we also used PRM1 and PRM2 for passage feed-back in a way as RM3-p does. Specifically, we first computed a PLM for each position of the document, and then we esti-mate a PRM based on a passage of size 2  X  centered at the position with the maximum positional query likelihood score (see Equation 17). These two runs are labeled as  X  X RM1-p X  and  X  X RM2-p X  respectively.

There are several parameters in these pseudo feedback al-gorithms. We fixed the number of feedback documents to 20 and the number of terms in feedback model to 30. Other parameters, including the feedback interpolation coefficient  X  , the two additional parameters  X  and  X  in PRM, the pas-sage size, and the passage smoothing parameter in RM3-p, were all tuned on Terabyte05 dataset.

We used Terabyte06 and ClueWeb09 for testing. The top-ranked 1000 documents for all runs were compared in terms of their mean average precisions (MAP) (for Terabyte06) or eMAP [8] (for ClueWeb09). In addition, other performance measures, such as Pr@10, Pr@30 and Pr@100 for Terabyte06 and eP@10, eP@30 and eP@100 for ClueWeb09, were also considered in our evaluation.
We first examine the overall retrieval precision of the pseudo feedback models for document-based feedback. The results are summarized in Table 2, where the best result for each row is highlighted. As we see, both PRM1 and PRM2 signif-icantly outperform the basic KL-divergence retrieval model in terms of MAP. In addition, PRM1 and PRM2 are also significantly better than RM3 across data sets. For ex-ample, the relative improvements of PRM1 over NoFB are 9 . 0% on Terabyte06 and 13 . 5% on ClueWeb09 in terms of average precision, which are much larger than the corre-sponding improvements achieved by RM3 (only 2 . 8% and 5 . 9% respectively). RM3 improves Pr@10 over NoFB in nei-ther dataset; however both PRM1 and PRM2 often improve Pr@10, though not significantly. Besides, comparing PRM1 Table 3: MAP/eMAP comparison of passage-based feedback methods.  X * X  means the corresponding im-provement over RM3-p is significant. and PRM2, we find that PRM1 is slightly more effective than PRM2.

We are also interested in evaluating if a heuristic passage-based feedback (i.e., RM3-p) can work as well as PRM, since both PRM1 and PRM2 essentially can be regarded as achieving a soft effect of passage feedback. Moreover, we can also use PRM1 and PRM2 for  X  X ard X  passage feed-back in a way as RM3-p does, which leads to PRM1-p and PRM2-p respectively. So we further compare the average precision of PRM1, PRM2, PRM1-p, PRM2-p, and RM3-p in Table 3. From the table, it is clear that PRM1, PRM2, PRM1-p and PRM2-p all outperform RM3-p significantly in most cases, suggesting that our model does not only have sound statistical foundation but also works effectively. In addition, we also observe that RM3-p behaves quite differ-ently in two datasets: it beats RM3 on ClueWeb09 but loses to RM3 on Terabyte06. However, all the four variations of PRM perform better than RM3 consistently. Finally, it is also interesting to see that PRM1 and PRM2 work similarly to PRM1-p and PRM2-p respectively, which may mean that PRM1 and PRM2 have already achieved successfully an ef-fect of passage-based feedback by assigning weights to dif-ferent positions, so it does not bring too much additional benefit to apply PRM to passages explicitly.

Next we examine the robustness to the parameter setting in PRM on the Terabyte06 collection.
In PRM1 and PRM2, there is a parameter  X  inherited from the positional language model to control the propaga-tion range, which would influence the effect of term position and term proximity. Specifically, if we increase  X  to infinity, the effect of term position and proximity will be disabled. However, if we decrease this parameter to a finite value, term position and proximity will play an important role in PRM. We fix other parameters to their default values as trained on Terabyte05 and focus on understanding how  X  affects the retrieval performance of PRM1 and PRM2. From Fig-ure 2 (left), we can see that, as long as  X  is in the range of [100, 1000], both PRM1 and PRM2 outperform RM3 clearly. Indeed, by setting  X  around 200, we can often obtain the op-timal performance for both PRM1 and PRM2. This result confirms the observation in previous work [19]. In addition, comparing PRM1 and PRM2, PRM2 seems to be less sen-sitive to  X  .
 Next, the positional language model is smoothed using Jelinek-Mercer method to estimate PRM. The smoothing is controlled by a parameter  X  . When  X  = 0 , we are us-ing the pure positional language model, while if  X  = 1 , we completely ignore the position and proximity evidence so that every position will receive the same weight. Again, we fix other parameters and show in Figure 2 (right) how the average precision changes under different  X  . The ex-periment results indicate that when  X  is set to around 0 . 1, both PRM1 and PRM2 achieve their optimal performance. However, PRM1 and PRM2 always outperform RM3 with  X  &lt; 1. Comparing PRM1 and PRM2, we see again that PRM2 seems to be more robust.

Recall that we interpolate the feedback model with the original query model. The interpolation is controlled by a coefficient  X  . When  X  = 0 , we are only using the origi-nal query model (i.e., no feedback), while if  X  = 1 . 0, we completely ignore the original query model and use only the estimated feedback model. We fix other parameters and show in Figure 3 (left) how the average precision changes according to the value of  X  . We can see that both PRM1 and PRM2 are clearly better than RM3 with different  X  val-ues. And the optimal  X  for all the methods seems to be in a range around 0 . 5. Besides, it is also interesting to observe that the pure feedback model results (  X  = 1 . 0) of PRM1 and PRM2 are much better than that of RM3, suggesting that the positional relevance model can lead to a more ac-curate query model. Finally, comparing PRM1 and PRM2, the former seems to be slightly more effective.

We further compare the robustness of different methods w.r.t. the number of feedback documents. We change the number of feedback documents from 1 to 200. The MAP results are shown in Figure 3 (middle). We notice that PRM1 and PRM2 are more robust to the number of feed-back documents as compared to RM3. It is also interesting to see there is almost no performance decrease of PRM1 and PRM2 even when we set the parameter to 200, suggesting that the proposed positional relevance model works better in tolerating noisy information. Moreover, with only 1 feed-back document, PRM1 and PRM2 have already been able to outperform RM3, no matter how many feedback docu-ments RM3 uses, which may indicate that our methods can identify good feedback terms more accurately by assigning position-dependent weights.

Additionally, we also compare the sensitivity of differ-ent methods to the number of expansion terms in Figure 3 (right). We vary the number of terms from 5 to 100, and observe that both PRM1 and PRM2 can achieve a very effective performance with only 10 expansion terms, while Figure 4: MAP Plot of PRM1 (left) and PRM2 (right) as compared to RM3 on Terabyte06 RM3 needs 70 terms, but even so, its performance is still not as good as our methods with 10 terms. This would be an-other advantage of our methods since fewer expansion terms mean higher efficiency, which is very important for retrieval systems.

To further see the robustness of our methods on individual queries, we plot the MAP of PRM1 versus RM3 and PRM2 versus RM3 on Terabyte06 in Figure 4. It is interesting that the proposed methods, particularly PRM2, are quite robust; they improve most of the queries clearly with only a small number of queries decreased slightly.
We proposed a novel positional relevance model (PRM) for pseudo-relevance feedback. The PRM exploits term posi-tion and proximity evidence to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be consistent with the query topic. Specifically, PRM generalizes the relevance model to aggregate the associations between a word and query words at the position-level in a probabilistic way. We also developed two methods to estimate the PRM based on different generative models.

Experiment results on two large web data sets show that the proposed PRM is quite effective and robust and per-forms significantly better than the state of the art relevance model in both document-based feedback and passage-based feedback. Compared to the relevance model, the proposed models are also less sensitive to the setting of various param-eters, such as feedback coefficient, number of feedback docu-ments, and number of expansion terms. Comparing the two estimation methods of PRM, the first method (PRM1) ap-pears to be more effective, while the second (PRM2) tends to be more robust. Both methods achieve its optimal retrieval performance when setting the  X  value in a range around 200 and  X  to around 0 . 1.

There are many interesting future research directions to explore. One of the most interesting directions is to further study whether setting a term-specific and/or query-specific  X  can further improve performance. Another interesting di-rection is to study how to optimize  X  automatically based on the layout of web pages. Improving the estimate of other components in PRM (e.g., the probability of choosing a po-sition in a document) would also be interesting.
We thank the anonymous reviewers for their useful com-ments. We also thank Wan Chen for helping improve the English in this paper. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, IIS-0713571, and CNS-0834709. [1] Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, [2] James Allan. Relevance feedback with too much data. [3] Chris Buckley, Gerard Salton, James Allan, and Amit [4] Georg Buscher, Andreas Dengel, and Ludger van Elst. [5] Stefan Buttcher and Charles L. A. Clarke. Efficiency [6] Stefan Buttcher, Charles L. A. Clarke, and Brad [7] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and [8] Ben Carterette, James Allan, and Ramesh Sitaraman. [9] Charles L. A. Clarke, Gordon V. Cormack, and [10] Ronan Cummins and Colm O X  X iordan. Learning in a [11] David Hawking and Paul B. Thistlewaite. Proximity [12] Marcin Kaszkiel and Justin Zobel. Effective ranking [13] E. Michael Keen. The use of term position devices in [14] E. Michael Keen. Some aspects of proximity searching [15] John D. Lafferty and Chengxiang Zhai. Document [16] Victor Lavrenko and W. Bruce Croft. Relevance-based [17] Xiaoyong Liu and W. Bruce Croft. Passage retrieval [18] Yuanhua Lv and ChengXiang Zhai. A comparative [19] Yuanhua Lv and ChengXiang Zhai. Positional [20] Donald Metzler and W. Bruce Croft. A markov [21] Donald Metzler and W. Bruce Croft. Latent concept [22] Christof Monz. Minimal span weighting retrieval for [23] Yves Rasolofo and Jacques Savoy. Term proximity [24] Stephen E. Robertson and Karen Sparck Jones. [25] Stephen E. Robertson, Steve Walker, Susan Jones, [26] J. J. Rocchio. Relevance feedback in information [27] Gerard Salton and Chris Buckley. Improving retrieval [28] Tao Tao and ChengXiang Zhai. An exploration of [29] Olga Vechtomova and Ying Wang. A study of the [30] Jinxi Xu and W. Bruce Croft. Query expansion using [31] Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-Ying [32] ChengXiang Zhai and John D. Lafferty. Model-based [33] ChengXiang Zhai and John D. Lafferty. A study of [34] Jinglei Zhao and Yeogirl Yun. A proximity language
