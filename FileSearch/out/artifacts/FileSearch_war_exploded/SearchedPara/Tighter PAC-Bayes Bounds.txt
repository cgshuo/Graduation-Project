 amiran.ambroladze@math.lth.se Support vector machines (SVM) implement linear classifiers in a high-dimensional feature space using the kernel trick to enable a dual representation and efficient computation. The danger of overfitting in such high-dimensional spaces is countered by maximising the margin of the classifier on the training examples. For this reason there has been considerable interest in bounds on the generalisation in terms of the margin.
 Early bounds have relied on covering number computations [7], while later bounds have considered Rademacher complexity. The tightest bounds for practical applications appear to be the PAC-Bayes bound [4, 5]. In particular the form given in [3] is specially attractive for margin classifiers, like SVM. The PAC-Bayesian bounds are also present in other Machine Learning models such as Gaus-sian Processes [6].
 The aim of this paper is to consider a refinement of the PAC-Bayes approach and investigate whether it can improve on the original PAC-Bayes bound and uphold its capabilities of delivering reliable model selection.
 The standard PAC-Bayes bound uses a Gaussian prior centred at the origin in weight space. The key to the new bound is to use part of the training set to compute a more informative prior and then compute the bound on the remainder of the examples relative to this prior. The bounds are tested experimentally in several classification tasks, including the model selection, on common benchmark datasets.
 The rest of the document is organised as follows. Section 2 briefly reviews the PAC-Bayes bound for SVMs obtained in [3]. The new bound obtained by means of the refinement of the prior is presented in Section 3. The experimental work, included in Section 4, compares the tightness of the new bound with the original one and indicates about its usability in a model selection task. Finally, the main conclusions of this work are outlined in Section 5. This section is devoted to a brief review of the PAC-Bayes Bound Theorem of [3]. Let us consider a distribution D of patterns x lying in a certain input space X , with their corresponding output labels classifier c , the following two error measures are defined: fying a pair pattern-label ( x , y ) selected at random from D Definition (Empirical error) The empirical error  X  c S of a classifier c on a sample S of size m is defined as the rate of errors on a set S where I (  X  ) is a function equal to 1 if the argument is true and equal to 0 if the argument is false. Now we can define two error measures on the distribution of classifiers: the true error, Q D  X  E c  X  Q c D , as the probability of misclassifying an instance x chosen from D with a classifier c chosen according to Q misclassifying an instance x chosen from a sample S .
 For these two quantities we can derive the PAC-Bayes Bound on the true error of the distribution of classifiers: Theorem 2.1 (PAC-Bayes Bound) For all prior distributions P ( c ) over the classifiers c , and for any  X   X  (0 , 1] The proof of the theorem can be found in [3].
 This bound can be particularised for the case of linear classifiers in the following way. The m training patterns define a linear classifier that can be represented by the following equation 1 : takes place, and w is a vector from that feature space that determines the separating plane. For any vector w we can define a stochastic classifier in the following way: we choose the dis-tribution Q = Q ( w ,  X  ) to be a spherical Gaussian with identity covariance matrix centred on the direction given by w at a distance  X  from the origin. Moreover, we can choose the prior P ( c ) to be a spherical Gaussian with identity covariance matrix centred on the origin. Then, for classifiers of the form in equation (1) performance can be bounded by Corollary 2.2 (PAC-Bayes Bound for margin classifiers [3]) For all distributions D , for all classi-fiers given by w and  X  &gt; 0 , for all  X   X  (0 , 1] , we have It can be shown (see [3]) that where E m is the average over the m training examples,  X  ( x , y ) is the normalised margin of the training patterns and  X  F = 1  X  F , where F is the cumulative normal distribution Note that the SVM is a thresholded linear classifier expressed as (1) computed by means of the kernel trick [2]. The generalisation error of such a classifier can be bounded by at most twice the true (stochastic) error Q D ( w ,  X  ) in Corollary 2.2, (see [4]); for all  X  . Our first contribution is motivated by the fact that the PAC-Bayes bound allows us to choose the prior distribution, P ( c ) . In the standard application of the bound this is chosen to be a Gaussian centred at the origin. We now consider learning a different prior based on training an SVM on a subset R of the training set comprising r training patterns and labels. In the experiments this is taken as a random subset but for simplicity of the presentation we will assume these to be the last r examples { x k , y k } m k = m  X  r +1 in the description below.
 With these r examples we can determine an SVM classifier, w r and form a prior P ( w | w r ) consist-ing of a Gaussian distribution with identity covariance matrix centred on w r .
 The introduction of this prior P ( w | w r ) in Theorem 2.1 results in the following new bound. Corollary 3.1 (Single Prior based PAC-Bayes Bound for margin classifiers) Let us consider a prior on the distribution of classifiers consisting in a spherical Gaussian with identity covariance centred classifiers w m and  X  &gt; 0 , for all  X   X  (0 , 1] , we have where  X  Q S \ R is a stochastic measure of the error of the classifier on the m  X  r samples not used to learn the prior. This stochastic error is computed as indicated in equation (2) averaged over S \ R . Proof Since we separate r instances to learn the prior, the actual size of the training set to which we apply the bound is m  X  r . In addition, the stochastic error must be computed only on the instances not used to learn the prior, i.e. the subset S \ R .
 The KL divergence between prior and posterior is computed as follows: Taking expectations using E w  X  Q w =  X  w m we arrive at Corollary 2.2 when applied to the SVM weight vector on the whole training set. It is perhaps worth stressing that the bound holds for all w m and so can be applied to the SVM trained on the whole set. S \ R not involved in generating the prior. The experimental work illustrates how in fact this bound can be tighter than the standard PAC-Bayes bound.
 Moreover, the selection of the prior may be further refined in exchange for a very small increase in the penalty term. This can be achieved with the application of the following result. for all posterior distributions Q ( c ) , for all  X   X  (0 , 1] , Proof The bound in Theorem 2.1 can be particularised for a certain P j ( c ) with associated weight  X  j and with confidence  X  X  j fact that P ( a  X  b )  X  P ( a ) + P ( b ) ).
 Finally, let us take the negation of (5) to arrive at the final result.
 by allocating Gaussian distributions with identity covariance matrix along the direction given by w r Corollary 3.3 (Multiple Prior PAC-Bayes Bound for linear classifiers) Let us consider a set { P all distributions D , for all classifiers w , for all  X  &gt; 0 , for all  X   X  (0 , 1] , we have Proof The proof is straightforward, substituting  X  j = 1 J for all j in Theorem 3.2 and computing the KL divergence between prior and posterior as in the proof of Corollary 3.1.
 holds for all  X  . Therefore, a linear search can be implemented for the value of  X  that leads to the tightest bound. In the case of several priors, the search is repeated for every prior and the reported value of the bound is the tightest. In Section 4 we present experimental results comparing this new bound to the standard PAC-Bayes bound and using it to guide model selection. The tightness of the new bound is evaluated in a model selection and classification task using some UCI [1] datasets (see their description in terms of number of instances, input dimension and number of positive/negative examples in Table 1).
 Table 1: Description of the datasets: for every set we give the number of patterns, number of input variables and number of positive/negative examples.
 For every dataset, we obtain 50 different training/test set partitions with 80% of the samples forming the training set and the remaining 20% forming the test set.
 With each of the partitions we learn a SVM classifier with Gaussian RBF kernel preceded by a model selection. The model selection consists in the determination of an optimal pair of hyperparameters ( C,  X  ) . C is the SVM trade-off between the maximisation of the margin and the minimisation of the hinge loss of the training samples, while  X  is the width of the Gaussian kernel. The best pair is 200 , 500 , 1000 } and  X   X  X  1 8 6  X  For completeness, this model selection is guided by the PAC-Bayes bound: we select the model corresponding to the pair that yields a lower value of Q D in the bound. Table 2 shows the value of the PAC-Bayes Bound averaged over the 50 training/test partitions. For every partition we use the minimum value of the bound resulting from all the pairs ( C,  X  ) of the grid. Note that this procedure is computationally less costly than the commonly used N -fold cross validation model selection, since it saves the training of N classifiers (one for each fold) for each parameter combination. Table 2: Averaged PAC-Bayes Bound and Test Error Rate obtained by the model that yielded the lowest bound in each of the 50 training/test partitions.
 We repeated this experiment using the Prior PAC-Bayes Bound with different configurations for learning the prior distribution of classifiers. These configurations are defined by variations on the percentage of training patterns separated to compute the prior and on the number of scalings of the magnitude of that prior. The scalings represent different lengths  X  of || w r || equally spaced between  X  = 1 and  X  = 100 . To summarize, for every training/test partition and for every pair (% patterns, # of scalings) we look at the pair ( C,  X  ) that outputs the smaller value of Q D . In this case, the use of the Prior PAC-Bayes Bound to perform the model selection increases the computational burden of using the PAC-Bayes one in the training of one classifier (the one used to learn the prior), in comparison to the extra N classifiers needed by N -fold cross validation. Table 3 displays both the average value and the sample standard deviation over the 50 realisations. It seems that ten scalings of the prior are enough to obtain tighter bounds, since the use of 100 or 500 scalings does not improve the best results. With respect to the percentage of training instances left out to learn the prior, something close to 50% of the training set works well in the considered problems. It is worth mentioning that we treat each position in the Table as a separate experiment.
Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior Table 3: Averaged Prior PAC-Bayes bound for different settings of percentage of training instances reserved to compute the prior and of number of scalings of the normalised prior.
 However, one could have included the tuning of the pair (% patterns, # of scalings) in the model selection. This would have involved a further application of the union bound with the 20 entries of the Table for each problem, at the cost of adding an extra ln(20) /m (0.0053 for Wdbc and less for the other datasets) in the right part of Theorem 3.2. We decided to fix the number of scalings and the amount of training patterns to compute the prior since to perform all of the different options would augment the computational burden of the model selection.
 In order to evaluate the predictive capabilities of the Prior PAC-Bayes bound as a means to select models with low test error rate, Table 4 displays the averaged test error corresponding to the mod-els selected in the previous experiment (note that in this case the computational burden involved in determining the model is increased by the training of the SVM that learns the prior w r ). Table 5 displays the test error rate obtained by SVMs with their hyperparameters tuned on the above men-tioned grid by means of ten-fold cross-validation, that serves as a baseline method for comparison purposes.
 According to the values shown in the tables, the Prior PAC-Bayes bound achieves tighter predictions of the generalization error of the randomized classifier in almost all cases.
 Notice how the length of the prior is not so critical in comparison with its direction. The goodness Moreover it has to be remarked that this tightening of the bound does not appear to deliver any reduction in the capabilities to select a good model (such a case would imply that we can predict more accurately a bigger error rate, but our bound is able to predict accurately the same error rate as the PAC-Bayes Bound).

Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior
Scalings Percentage of training set used to compute the prior Table 4: Averaged Test Error Rate corresponding to the model determined by the bound for the different settings of Table 3.
 Table 5: Averaged test error rate. For every partition we select the test error rate corresponding to the model reporting the smaller cross-validation error.
 However, the comparison with Table 5 points out that the PAC-Bayes bound is not as accurate as Ten Fold cross-validation when it comes to selecting a model that yields a low test error rate. Nevertheless, in two out of the four problems ( waveform , and wdbc ) the bound provided a model as good as the one found by cross-validation, added to the fact that in ringnorm the error bars overlap. We conclude the discussion by pointing that the Cross-validation error rate cannot be used directly as a prediction on the expected test error rate in the sense of worse case performances. Of to predict how close they are going to be. In this paper we have presented a version of the PAC-Bayes bound for linear classifiers that intro-with identity covariance matrix. The mean weight vector is learnt in the following way: its direction is determined from a separate subset of the training examples, while its length has to be chosen from an a priori fixed set of lengths.
 The experimental work shows that this new version of the bound achieves tighter predictions of the generalization error of the stochastic classifier, compared to the original PAC-Bayes bound predic-tions. Moreover, if the model selection is driven by the bound, the Prior PAC-Bayes does not degrade the quality of the model selected by the original bound. Nevertheless, it has to be said that in some of our experiments the model selected by the bounds resulted as accurate as the ones selected by ten-fold cross-validation in terms of test error rate on a separate test. This fact is remarkable since to include the model selection in the training of the classifier roughly multiplies by ten the computa-tional burden of the training when using ten-fold cross-validation but roughly by two when using the prior PAC-Bayes bound. Of course the original PAC-Bayes provides with a cheaper model selection, but its predictions about the generalization capabilities are more pessimistic.
 The amount of training patterns used to learn the prior seems to be a key aspect in the goodness of this prior and thus in the tightness of the bound. Therefore, ongoing research includes methods to systematically determine an amount of patterns that provides with suitable priors. Another line of such as sparsity. Finally, a deeper study about which dataset structure causes differences among the performances of cross-validation and bound-driven model selections is also being carried out. Acknowledgments This work has been supported by the IST Programme of the European Community under the PAS-CAL Network of Excellence IST2002-506788. E. P-H. acknowledges support from Spain CICYT grant TEC2005-04264/TCM.
 [1] C L Blake and C J Merz. UCI Repository of machine learning databases . [2] Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal [3] J Langford. Tutorial on practical prediction theory for classification. Journal of Machine Learn-[4] J Langford and J Shawe-Taylor. PAC-Bayes &amp; Margins. In Advances in Neural Information [5] D McAllester. Pac-bayesian stochastic model selection. Machine Learning , 51(1):5 X 21, 2003. [6] M Seeger. PAC-Bayesian Generalization Error Bounds for Gaussian Process Classification. [7] J Shawe-Taylor, P L Bartlett, R C Williamson, and M Anthony. Structural risk minimization
