 When it comes to POS tagging, two things must be checked. First, a POS tagger needs to be tested for its robustness in handling heterogeneous data. 1 Sta-tistical POS taggers perform very well when their training and testing data are from the same source, achieving over 97% tagging accuracy (Toutanova et al., 2003; Gim  X  enez and M ` arquez, 2004; Shen et al., 2007). However, the performance degrades in-creasingly as the discrepancy between the training and testing data gets larger. Thus, to ensure robust-ness, a tagger needs to be evaluated on several dif-ferent kinds of data. Second, a POS tagger should be tested for its speed. P OS tagging is often performed as a pre-processing step to other tasks (e.g., pars-ing, chunking) and it should not be a bottleneck for those tasks. Moreover, recent NLP tasks deal with very large-scale data where tagging speed is critical.
To improve robustness, we first train two separate models; one is optimized for a general domain and the other is optimized for a domain specific to the training data. During decoding, we dynamically se-lect one of the models by measuring similarities be-tween input sentences and the training data. Our hy-pothesis is that the domain-specific and generalized models perform better for sentences similar and not similar to the training data, respectively. In this pa-per, we describe how to build both models using the same training data and select an appropriate model given input sentences during decoding. Each model uses a one-pass, left-to-right POS tagging algorithm. Even with the simple tagging algorithm, our system gives results that are comparable to two other state-of-the-art systems when coupled with this dynamic model selection approach. Furthermore, our system shows noticeably faster tagging speed compared to the other two systems.

For our experiments, we use corpora from seven different genres (Weischedel et al., 2011; Nielsen et al., 2010). This allows us to check the performance of each system on different kinds of data when run individually or selectively. To the best of our knowl-edge, this is the first time that a POS tagger has been evaluated on such a wide variety of data in English. 2.1 Training generalized and domain-specific Consider training data as a collection of documents where each document contains sentences focusing on a similar topic. For instance, in the Wall Street Journal corpus, a document can be an individual file or all files within each section. 2 To build a gener-alized model, lexical features (e.g., n -gram word-forms) that are too specific to individual documents should be avoided so that a classifier can place more weight on features common to all documents.

To filter out these document-specific features, a threshold is set for the document frequency of each lowercase simplified word-form ( LSW ) in the train-ing data. A simplified word-form ( SW ) is derived by applying the following regular expressions sequen-tially to the original word-form, w .  X  X eplaceAll X  is a function that replaces all matches of the regular ex-pression in w (the 1st parameter) with the specific string (the 2nd parameter). In a simplified word, all numerical expressions are replaced with 0. A
LSW is a decapitalized SW . Given a set of LSW  X  X  whose document frequencies are greater than a cer-tain threshold, a model is trained by using only lexi-cal features associated with these LSW  X  X . For a gen-eralized model, we use a threshold of 2, meaning that only lexical features whose LSW  X  X  occur in at least 3 documents of the training data are used. For a domain-specific model, we use a threshold of 1.
The generalized and domain-specific models are trained separately; their learning parameters are op-timized by running n -fold cross-validation where n is the total number of documents in the training data and grid search on Liblinear parameters c and B (see Section 2.4 for more details about the parameters). 2.2 Dynamic model selection during decoding Once both generalized and domain-specific models are trained, alternative approaches can be adapted for decoding. One is to run both models and merge their outputs. This approach can produce output that is potentially more accurate than output from either model, but takes longer to decode because the merg-ing cannot be processed until both models are fin-ished. Instead, we take an alternative approach, that is to select one of the models dynamically given the input sentence. If the model selection is done ef-ficiently, this approach runs as fast as running just one model, yet can give more robust performance.
The premise of this dynamic model selection is that the domain-specific model performs better for input sentences similar to its training space, whereas the generalized model performs better for ones that are dissimilar. To measure similarity, a set of SW  X  X , say T , used for training the domain-specific model is collected. During decoding, a set of SW  X  X  in each sentence, say S , is collected. If the cosine similarity between T and S is greater than a certain threshold, the domain-specific model is selected for decoding; otherwise, the generalized model is selected. The threshold is derived automatically by running cross-validation; for each fold, both models are run simultaneously and cosine similarities of sentences on which the domain-specific model performs bet-ter are extracted. Figure 1 shows the distribution of cosine similarities extracted during our cross-validation. Given the cosine similarity distribution, the similarity at the first 5% area (in this case, 0 . 025 ) is taken as the threshold. 2.3 Tagging algorithm and features Each model uses a one-pass, left-to-right POS tag-ging algorithm. The motivation is to analyze how dynamic model selection works with a simple algo-rithm first and then apply it to more sophisticated ones later (e.g., bidirectional tagging algorithm).
Our feature set (Table 1) is inspired by Gim  X  enez and M ` arquez (2004) although ambiguity classes are derived selectively for our case. Given a word-form, we count how often each POS tag is used with the form and keep only ones above a certain threshold. For both generalized and domain-specific models, a threshold of 0.7 is used, which keeps only POS tags used with their forms over 70% of the time. From our experiments, we find this to be more useful than expanding ambiguity classes with lower thresholds. 2.4 Machine learning Liblinear L2-regularization, L1-loss support vector classification is used for our experiments (Hsieh et al., 2008). From several rounds of cross-validation, learning parameters of ( c = 0.2, e = 0.1, B = 0.4) and ( c = 0.1, e = 0.1, B = 0.9) are found for the gener-alized and domain-specific models, respectively ( c : cost, e : termination criterion, B : bias). Toutanova et al. (2003) introduced a POS tagging algorithm using bidirectional dependency networks, and showed the best contemporary results. Gim  X  enez and M ` arquez (2004) used one-pass, left-to-right and right-to-left combined tagging algorithm and achieved near state-of-the-art results. Shen et al. (2007) presented a tagging approach using guided learning for bidirectional sequence classification and showed current state-of-the-art results. 3
Our individual models (generalized and domain-specific) are similar to Gim  X  enez and M ` arquez (2004) in that we use a subset of their features and take one-pass, left-to-right tagging approach, which is a sim-pler version of theirs. However, we use Liblinear for learning, which trains much faster than their classi-fier, Support Vector Machines. 4.1 Corpora For training, sections 2-21 of the Wall Street Jour-nal (WSJ) from OntoNotes v4.0 (Weischedel et al., 2011) are used. The entire training data consists of 30,060 sentences with 731,677 tokens. For evalua-tion, corpora from seven different genres are used: the MSNBC broadcasting conversation (BC), the CNN broadcasting news (BN), the Sinorama news magazine (MZ), the WSJ newswire (NW), and the G
ALE web-text (WB), all from OntoNotes v4.0. Ad-ditionally, the Mipacq clinical notes (CN) and the Medpedia articles (MD) are used for evaluation of medical domains (Nielsen et al., 2010). Table 2 shows distributions of these evaluation sets. 4.2 Accuracy comparisons Our models are compared with two other state-of-the-art systems, the Stanford tagger (Toutanova et al., 2003) and the SVMTool (Gim  X  enez and M ` arquez, 2004). Both systems are trained with the same train-ing data and use configurations optimized for their best reported results. Tables 3 and 4 show tagging accuracies of all tokens and unknown tokens, re-spectively. Our individual models (Models D and G) give comparable results to the other systems. Model G performs better than Model D for BC, CN, and MD, which are very different from the WSJ. This implies that the generalized model shows its strength in tagging data that differs from the train-ing data. The dynamic model selection approach (Model S) shows the most robust results across gen-res, although Models D and G still can perform better for individual genres (except for NW, where Model S performs better than any other model). For both all and unknown token experiments, Model S performs better than the other systems when evaluated on a mixture of the data (the Total column). The differences are statistically significant for both experiments (McNemar X  X  test, p &lt; . 0001 ). The Stanford tagger gives significantly better results for unknown tokens in BN; we suspect that this is where their bidirectional tagging algorithm has an advantage over our simple left-to-right algorithm. 4.3 Speed comparisons Tagging speeds are measured by running each sys-tem on the mixture of all data. Our system and the Stanford system are both written in Java; the Stan-ford tagger provides APIs that allow us to make fair comparisons between the two systems. The SVM-Tool is written in Perl, so there is a systematic dif-ference between the SVMTool and our system.

Table 5 shows speed comparisons between these systems. All experiments are evaluated on an In-tel Xeon 2.57GHz machine. Our system tags about 32K tokens per second (0.03 milliseconds per to-ken), which includes run-time for both POS tagging and model selection.
 We present a dynamic model selection approach that improves the robustness of POS tagging on hetero-geneous data. We believe that this approach can be applied to more sophisticated algorithms and im-prove their robustness even further. Our system also shows noticeably faster tagging speed against two other state-of-the-art systems. For future work, we will experiment with more diverse training and test-ing data and also more sophisticated algorithms. This work was supported by the S HARP program funded by ONC: 90TR0002/01. The content is solely the responsibility of the authors and does not necessarily represent the official views of the ONC.
