 Mining on streaming data draws more and m ore attentions in research community in recent years. One of the most challenge issu es for data stream mining is classifica-tion analysis with concept drift [4,5,6,7,8]. The currently available algorithms for data stream classification are all dedicated to ha ndle precise data, while data with uncertainty is quite common in real-life applications. Uncertainty can appear in attributes in some applications. In a sensor network system, th e information such as humidity, temperature and weather usually contains massive uncerta inty during the processes of data collect-ing and transmitting [2]. Uncertainty can also appear in class values. For example, in medical diagnosis, it X  X  hard for the doctor to decide the exact disease of a patient. It X  X  wise to predict all the possibilities of each candidate diseases rather than give a crisp result.

In this paper, we study the data stream classi fication tasks with uncertain class val-ues. Firstly, we extend the NS-PDT algorithm [1], a decision tree algorithm dealing with categorical attributes and uncertainty in class values, to handle continuous attributes fol-lowing the scheme of C4.5 [10]. Secondly, we propose two ensemble based algorithms, Static Classifier Ensemble (SCE) and Dynamic Classifier Ensemble (DCE), to classify uncertain data streams with concept drift. Experiments on both synthetic data and real-life data set are made to validate our proposed methods. The experimental results show that DCE outperforms SCE.

This paper is organized as following: Section 2 reviews the related work on data streams and uncertainty. Section 3 describes NS-PDT algorithm for data with uncer-tainty in class values. Section 4 presents our ensemble based algorithms for classifying uncertain data streams. Section 5 shows our experimental results on both synthetic and real-life data set. And Section 6 concl udes this paper and gives our future work. To the best of our knowledge, there is no work so far on classifying uncertain data streams, while both studies on mining of data stream and uncertain data have been well investigated in recent years.

At present, the classification algorithms fo r the data stream scenario are all dedicated to precise data only. A classical approach for d ata stream classification follows ensem-ble based scheme [4,5,6,7,8], which usually learns an ensemble of classifiers from the streaming data, and then uses all the classifiers to predict the newly coming instances. The way to integrate classifiers can be distinguished into two categories:  X  Static classifier ensemble. The weight of each base classifier is decided before the  X  Dynamic classifier ensemble. The weight of each base classifier is decided by the It is concluded in [7] that dynamic methods outperform the static methods [4].
For the classification task on uncertain data, a few decision tree algorithms have been proposed [1,2,3]. Both DTU algorithm [2] and UDT algorithm [3] model the uncertainty in attribute values by probability distribution functions (pdf). This is different from our algorithm, for our goal is to deal with data uncertainty in class values. NS-PDT [1] is a decision tree algorithm to handle data with uncertain class values. However, it can only handle categorical attributes. Hence, in this paper, we firstly extend the initial NS-PDT algorithm [1] to handle continuous attributes and then use it as a base classifier for ensemble classification of data streams. We briefly describe NS-PDT algorithm [1] for uncertainty here. Given an uncertain data set with M classes, the possibility distribution  X  = {  X  ( L 1 ) , X  ( L 2 ) ,..., X  ( L M ) } rep-resents how likely an instance belongs to each class. More specifically,  X  ( L i ) accesses the possibility that an in stance belongs to class L i ,i  X  [1 ,M ] .

Compared with the state-of-the-art tree building algorithm C4.5 [10], NS-PDT re-places the Information Gain by the Non-Specificity Gain ( NSG ), and uses the maxi-mum Non-Specificity Gain Ratio ( NSGr ) to decide which attribute to be selected as the next splitting attribute. NSG is defined as [1]: Here, NSG ( T,A k ) represents the amount of information precision obtained after split-ting the data set T according to attribute A k ; U (  X  ) accesses the information precision of the data set. Please refer to [1] for more details.

As NS-PDT can only deal with categorical a ttributes, here we simply follow the schemes utilized in C4.5 algorithm [10] to e xtend NS-PDT to handle continuous at-tributes. The detailed algorithm is omitted here for lack of space. In this paper, we follow the assumption that in data stream scenario, data arrives as batches with variable length [11]: Here, d i,j represents the j -th instance in the i -th batch. In our paper, we learn a NS-PDT classifier from each batch of uncertain data , and then the base classifiers are combined to form an ensemble. To keep the population capacity of the ensemble, following [6], we delete the oldest classifier when the num ber of classifiers in the ensemble exceeds a predefined parameter EnsembleSize . In the testing phase, our goal is to output a possibility distribution of test instance x over all possible classes. 4.1 Static Classifier Ensemble We proposed a static classifier ensemble algorithm to classify the uncertain data streams, which is illustrated in algorithm 1.

In algorithm 1, the function C i .dis ( x ) in step 3 returns a possibility distribution over different classes of x . In steps 4-6, the possibility for each class is accumulated. In step 8, we normalize the possibility by formula  X  ( L i )=  X  ( L i ) maximum possibility in  X  is 1.

Note that in algorithm 1, wei i represents the weight of classifier C i , and it is decided in the most up-to-date data batch. Let X  X  write | NUM | for the total number of testing instances; D ( x i ) for the accuracy while predicting instance x i ;  X  res ( L j ( x i )) for the real possibility distribution of instance x i on category j ;  X  ( L j ( x i )) for the predicted resulting possibility distribution of instance x i on category j . Then the weight wei i is computed by the PCC dist metric given by [1]: Algorithm 1. Classifying Uncertain Data Streams by Static Classifier Ensemble Here, PCC dist criterion takes into account the average of the distances between the resulting possibility distribution and the real possibility distribution. Note that high value of the PCC dist criterion implies not only that the algorithm is accurate, but also that the possibility distributions outputted by the algorithm is of high quality and faithful to the original possibility distribution [1]. 4.2 Dynamic Classifier Ensemble In our DCE algorithm, the weight of each base classifier is decided by test instance x . The weighted Euclidean distance between two instances with m attributes is given by : ical attribute A i ,wehave dif f ( x A i 1 ,x A i 2 )= the weight of attribute A i , and it is decided by the non-specificity gain ( NSG ) [1] of the attribute A i . Algorithm 2 gives the details of our DCE algorithm.
 In algorithm 2, in step 2 we retrieve K nei neighbors of x from validation data set V according to formula (5). And the most up-to-date data batch from the stream is used as V . In steps 3-9, the weight of each base cl assifier is determined. The function C .Acc ( nei j ) in step 6 returns the accuracy of classifier C i while predicting the instance nei j , and the accuracy is decided accord ing to formula (4). The function d ( nei j ,x ) in step 7 returns the distance following formula (5). After predicting an instance, the weight of the corresponding ba se classifier is accumulated together. Accuracy and dis-tance are the main factors that affect wei i . In steps 10-14, some base classifiers with lower weight are ignored following [6]. The function predict ( E, x, wei ) in step 14 Algorithm 2. Classifying Uncertain Data Streams by Dynamic Classifier Ensemble returns the predicted result of x by a weighted voting scheme. The weighted voting scheme is the same as in algorithm 1. In this section, we report our experimental results. We deal with the data streams with uncertain class information and concept drift , which is regarded as an important issue in mining data streams [4,5,6,7]. Since no benchmark uncertain data set can be found in the literature, we simulate the uncertain data on synthetic data and real-life data set. Here, moving hyperplane concept [4,7] was used as the synthetic data set with continuous attributes, and RCV1-v2 1 was used as the real-life data set with categorical attributes in our experiments.

The uncertain data set was generated by using the approach described in [1]. We simulated various levels of uncertainty ( X % ) when generating possibilistic training data. Firstly, we randomly selected X % of instances in each data batch as uncertain set. Secondly, for each instance in the un certain set, if the instance belongs to the i -th class, we set  X  (  X  i )=1 ,and  X  (  X  j )=  X ,  X  j = i . Here,  X  is a random value distributed uniformly in range of [0, 1]. Thirdly, for each instance of the remaining (100  X  X )% instances, we assigned a certainly sure possi bility distribution (if the instance belongs to the i -th class, we set  X  (  X  i )=1 ,and  X  (  X  j )=0 ,  X  j = i ). The data set used for test in our experiments was also with a certainly sure possibility distribution, following [1].
PCC dist is used as the evaluation metric in our experiments for measuring the classification performance of classifiers on uncertain data set. Note that high value of PCC dist implies not only that the algorithm is accu rate, but also that the distribution possibility is faithful to the original possibility distribution [1] .
 In our experiments, we set EnsembleSize =25, following [7]; and we set K nei =5. Meanwhile, we set ChunkSize , the number of instances in each batch, to 500. Our experiments were made on a PC with Pentium 4 3.0 GHz CPU and 1G memory. All of the algorithms were implemented in Java with help of WEKA 2 software package. 5.1 Moving Hyperplane Concep t with Gradual Concept Drift Moving hyperplanes have been widely used to simulate time-changing concepts [4]. In this subsection, we report our experiments on moving hyperplane concept. A moving hyperplane in d -dimensional space is denoted by: d i =1 a i x i = a 0 .

We followed the same procedure in [4] to simulate concept drift. We used K for the total number of dimensions whose weights are changing; S for the magnitude of the change (every N instances) for weights a 1 ,  X  X  X  ,a k . For more detailed descriptions of moving hyperplanes, please refer to [4].

We s e t d =10, N =1000 in our experiments. For each pa rameter setting, we generated 100 batches of data. The averaged results on the 100 batches are reported in Table 1.
From Table 1, it could be concluded that, DCE outperforms SCE in all scenarios, with averaged improvement being about 2%. It could be also seen that, with the in-creasing of X , the performances of both SCE and DCE decline. It is shown that the higher the level of uncertainty in the training set is, the more imprecise the training set will be, and therefore the more difficult fo r the classifier to learn an accurate model.
We also study the impact of the parameter K nei . Here, we set S =0.1, X =20 and K was selected from [1,10] randomly. We conducted 10 trails and the averaged result is showninFig.1.ItcanbeshowninFig.1that K nei can not be neither too small nor too large. We set K nei =5 in other experiments as it usually gives us good results. 5.2 Experiments on RCV1-v2 Text Data Set In this section, we report our experimental results on a real-life text dataset. RCV1-v2 is a new benchmark collection for text categorization [9]. The news stories in RCV1-v2 were divided into two datasets, training set and testing set. The training dataset was used and four largest categories, CCAT, ECAT, GCAT, and MCAT were considered as the main topics in our experiments to simul ate concept drift. After preprocessing, each document was presented by a binary vector, and information gain algorithm was used to select 100 most predictive features.

The data set in our experiments was decomposed into 46 batches. We vary uncer-tainty X % as follow: 20, 40, 60. In each scenario of our experiments, we selected one category as TopicA and others as TopicB . Concept drift occurs between TopicA and Top-icB . Therefore, there are 12 possible combination for 4 categories. We experiment 12 trails and the averaged result is reported.

We simulate abrupt drift in our experiments as follows: In the batches 0-24, all the instances of TopicA were labeled as positive, others were labeled as negative. In batches 25-46, all the instances of TopicB were labeled as positive, others were labeled as neg-ative. The experimental results for abr upt concept drift are given in Fig. 2.
As expected, from Fig. 2, it could be seem that in batch 25 when abrupt drift occurs, both DCE and SCE have a dramatic decline in PCC dist . However, DCE recovers much faster than SCE in the later batches. In all levels of uncertainty, DCE outper-forms SCE. It reveals that DCE outperforms SCE in handling abrupt concept drift with uncertainty. 5.3 Time Analysis Both SCE and DCE consume the same time in training, because the ways to construct the ensemble is the same. Here we compare the testing time. We generated data streams with varied ChunkSize on hyperplane concept ( K =10, S =1). Consider 100 batches of data with X =20. Fig. 3 shows that large ChunkSize offers better performances. The DCE also performs better than SCE. However, the testing time of the two methods on the whole streams is significantly different. From Fig. 4 we can see that with a large ChunkSize , DCE consumes much more testing time than SCE. It is because when predicting an instance, DCE needs to traverse the whole validation set to get some nearest neighbors. So it X  X  a tradeoff between good performance and faster responding action.
 In this paper, we propose two types of ensemble based approaches to classify data streams with uncertainty in class values. We also extend the decision tree (NS-PDT) approach to handle data with continuous attributes. Experiments on both synthetic and real-life datasets are made to validate our proposed methods. Our experimental results show that the dynamic classifier ensemble (DCE) for uncertain data stream outperforms the static classifier ensemble (SCE).

Attribute uncertainty is also natural and prevalent in many real-life applications, we plan to study this kind of uncertain data streams in future work.

