 This paper addresses the problem of joint modeling of multime-dia components in different media forms. We consider the in-formation retrieval task across both text and image documents, construction approach is introduced for learning an isomorphic data smoothness is guaranteed. The proposed objective function and a Maximum Mean Discrepancy (MMD) term that measures the cross-modality discrepancy. Optimization of the reconstruc-tion terms and the MMD term yields a compact and modality-optimization problem by maximizing variance reduction over a optimization problem can be solved by a highly efficient greedy algorithm, and is guaranteed to be at least a ( e  X  1 ) / approximation to the optimum. The proposed method achieves state-of-the-art performance on the Wikipedia dataset. Information Systems; I.2.6 [ Computing Methodologies ] : [ cial Intelligence-Learning ] Design, Experimentation, Theory learning; Sparse representations ployed to multimedia data for obtaining collections of informa-[ 34 ing and testing, which, however, cannot well address the prob-lem when training and query instances come from different me-dia forms. Thus, as a sub-topic of transfer learning [ 35 abling the machine to allocate a description of a few sentences knowledge from relevant data in a different media form. Thus, by utilizing the mined information, which can be presented in a can be utilized for enhancing an existing learning system. When comparing with the regular cross-modality problem, the  X  X ame-new scenario. Thus, we distinguish the new scenario from the pervised cross-modality problem and the semi-supervised cross-modality problem respectively.

We consider both the supervised and semi-supervised cross-with one-to-one image / text correspondence from the Wikipedia image introduces Michael Jordan X  X  career achievements; (b) the paragraph attached beside the image introduces Chris Morris X  X  career achievements; (c) the last image contains the upper body part of the American football player Merio Danelo, and the at-fer is achieved through building semantic connections upon vi-image domain can be linked to the word  X  X BA X  in the text do-main. By exploiting these semantic connections in the new fea-main can be equivalent to introducing noise data if we are not careful. On the other hand, assuming the auxiliary domain data are more discriminative towards some categories, both the pri-mary domain data and the auxiliary domain data can be adapted to a new feature space, which can be more discriminative than the primary domain alone. It can be observed that the three im-resentative of  X  X ports X .

In this work, we propose a cross-modality submodular dictio-media domains. We employ Maximum Mean Discrepancy (MMD) [ 4 ] to quantitatively measure the degree of miss-matches between dition to the semi-supervised cross-modality scenario, we con-sider a supervised cross-modality scenario, where the unlabeled same-distribution can be utilized during training. The CmSDL approach is employed on the supervised cross-modality scenario can be summarized as follows:
Cross-modality information retrieval: Content-based infor-mation retrieval has been an important subject in multimedia, vious content-based retrieval techniques are based on unimodal data, which can be images [ 10 ] , [ 46 ] , texts [ 33 ] data and the training data are matched, thus unimodal informa-data come from different modalities.

In this paper, we consider two sub-categories of the cross-mo-relation matching (CM), semantic matching (SM) and semantic ing, the abstraction method and the joint working mode of both in approaches GMLDA and GMMFA respectively. A similar sce-nario to the supervised cross-modality problem was proposed in [ 47 and Shao [ 47 ] achieved such a transformation through a cross-to possess the same representations after being projected onto the learned dictionary pair. In the works of both [ 31 ] modality adaptation techniques aim at finding the linear combi-nations of the data from both the source modality and the tar-get modality that possess maximum correlation with each other through CCA. The proposed CmSDL approach is theoretically su-estimator-based measurement CCA, the MMD measurement, w-ror when learning the dictionary.

Submodularity: Submodulairty has recently been applied to selection [ 16 ] and segmentation [ 13 ] . Liu et al. [ 19 method that uses the entropy rate of a random walk on a graph team. for compact and homogeneous clustering. Andreas and Volkan [ 16 ] employed submodularity to perform an iterative dictionary atom selection from a candidate set. Jiang and Davis [ 13 mizing a submodular objective function. The proposed approach candidate set for sparse representations.
Though, the cross-modality information retrieval framework we restrict our discussion on the information retrieval problem across text documents and image documents in this paper. We consider the information retrieval problem from a database Y { Y vides information from both media domains, i.e., Y i Classical information retrieval systems considered the uni-mod-by feature vectors in a uniform media domain, i.e., Y  X  R R [ 30 tion includes retrieving relevant text documents in response to image references ( Y = { I 1 ,  X  X  X  , I m , T m + 1 ,  X  X  X  , T supervised cross-modality scenario, i.e., Y = {{ I 1 , T 1 , I m + 1 ,  X  X  X  , I m + n } or Y = {{ I 1 , T 1 } ,  X  X  X  , { I the top of the above scenarios, we introduce a novel framework which aims at enhancing the classical unimodal information re-dia form. When retrieving either image documents or text doc-modal data, and it is applicable to many existing cross-domain / cross-modality learning systems.
Sparse coding represents an input signal y as a linear combi-R matrix, where k denotes the dictionary size and each column b denotes a t -dimensional dictionary basis. Let X = { x 1 , x  X  compact dictionary for sparse representations can be formulated as: where these two terms denote the reconstruction error and the l each dictionary basis or add a upper bound to the l then extend the dictionary learning function to the more gener-alized cross-modality scenario. natural one-to-one correspondences across both modalities are ing data. Thus, we formulate above notations as Y I  X  struction can be formulated as follows for both domains: such an isomorphic feature space based on dictionary construc-joint optimization function, and minimize over the distribution discrepancy  X ( X I , X T ) of X I and X T : The Maximum Mean Discrepancy (MMD) [ 12 ] , [ 26 ] , [ 27 achieved by embedding distributions into a reproducing kernel MMD estimation as follows: D EF INIT IO N 1. Let F be a class of functions f : X  X  empirical estimate of the Maximum Mean Discrepancy (MMD) is defined as: norm.

The distance between distributions of X I and X T is equivalent to the distance between the two mean elements in a RKHS space H . Given the continuous bounded function C ( X ) and a univer-distributions of X I and X T are identical.

We set F = { f } in which f is an identity function and [ X I , X T ] . Thus,  X ( X I , X T ) is computed as: where A 0 represents the transpose of A and measurement in Equation (4): The objective function in Equation (6) includes two reconstruc-image and text modalities, a cross-modality term that measures the distribution discrepancy through MMD and two sparsity con-and B T .
A popular solution to learning a compact dictionary for sparse sparse codes through Singular Value Decomposition [ 1 ] and Or-the cross-modality term, such a solution becomes no longer ap-satisfy smoothness with sparse representations in the other me-dia domain, which consequently determines the construction of the dictionary in the other media domain.
 ation on the proposed problem by replacing the l 0 sparse norm function with multiple objectives.
The dictionary construction can also be formulated as select-ing a subset of dictionary atoms from an over-complete candi-mulated as a discrete optimization problem. The challenges lie in the computational complexity. Obtaining two subsets of dic-tions. Thus, brutally searching over all candidates is NP-hard.
We formulate the dictionary construction as a dictionary se-ularity in CmSDL, the discrete dictionary selection problem can be solved by an efficient greedy approach. F (
A  X  X  a 1 } )  X  F ( A )  X  F ( A  X  X  a 1 , a 2 } )  X  F ( A  X  X  a a stating that adding an element to a smaller set helps more than adding it to a larger one [ 25 ] .
In order to determine the number of selected elements in the subset B , previous work [ 6 ] , [ 18 ] , [ 13 ] , [ 14 ] of atoms. Thus, we fix the number of selected dictionary atoms for both dictionaries instead of penalizing on each selection.
We sparsify the projection coefficients X with the local greedy greedily adds dictionary atoms to approximate original signals. note
For l -sparse representation, where l is the user-defined spar-let where L 1 ( ; ) = k Y I k 2 F and L 2 ( ; ) = k Y T k 2 F similar proof.
 approximately submodular function if there exists constant any B  X  B 0  X  X  and b /  X  B 0 , the following holds
For approximate submodularity, [ 17 ] shows the following in-output a near-maximum solution: where B n is a set with | B n | = n .
 is defined as Then for the l -sparse problem, we define we assume that each pair of vectors in D I is almost orthogonal (incoherent), i.e., they satisfy following theorem: where  X   X  4 k  X  .
 G ( B ) = L i ( ; )  X  L i ( B ) , then we have  X  F i ( D ) =  X  B x i + e i , b j  X  = x i j +  X  e i , b j  X  , where x i j in x i . And if B is near-orthogonal basis, then  X  Y e , b j  X  = x i j + P b G ( B )= L i ( ; )  X  L i ( B ) where | X  b i , b j  X  X  X   X  . For v /  X  B , terms in G i ( B  X  v are bounded and higher-order infinitesimal on  X  . Then we have
G Define For each i ,  X  F i ( D ) is monotonic, i.e.,  X  F i ( D )= Algorithm 1 Greedy Solution to CmSDL 1: Input: D I , D T , Y I , Y T , f (  X  ) , M ,  X  , l , k . 2: Output: B I , B T , X  X  I , X  X  T , p . 3: Initialization: B I  X ; , B T  X ; , p 0  X  0. 4: loop u = 1 : k 6: end for 10: end loop 11: X  X  I = arg min X I k Y I  X  B I X I k 2 F , s . t . k x 12: X  X  T = arg min X I k Y T  X  B T X T k 2 F , s . t . k x And for v /  X  X  0 we can also obtain which means  X  F i ( D ) is submodular and so is  X  F ( with Equation (12), we have Therefore, where  X   X  4 k  X  .
Utilizing submodularity, we provide a greedy algorithm to it-Previous submodular optimization approaches [ 13 ] , [ 16 mulated the objective functions as discrete maximization prob-with these methods, our problem is more complex due to the ex-have to consider each possible combination of B I i or B T Algorithm 2 Lazy Greedy Solution to CmSDL 1: Input: D I , D T , Y I , Y T , f (  X  ) , l , M ,  X  , k . 2: Output: B I , B T , X I , X T , p . 3: Initialization: B I  X ; , B T  X ; . 4: for each d I i in D I , each d T j in D T compute 5: end for 6: loop u = 1 : k 8: Update the top element in p u with 12: end loop 13: X  X  I = arg min X I k Y I  X  B I X I k 2 F , s . t . k x 14: X  X  T = arg min X I k Y T  X  B T X T k 2 F , s . t . k x where u denotes the number of selected elements in each dic-is initialized with ; , and the combination B I i and B T The pseudocode of the greedy solution to CmSDL is given in Al-gorithm 1.

Due to the exhaustive computation of p in each iteration, the computing the gain for every possible combination of B I i B I and B T j  X  X  T \ B T , which requires ( |D I | X  X  B I | realize that the gain for each combination of B I i and B the top element in each iteration, which reduces the computa-tional complexity from O ( k ( log |D| ) 2 ) to O ( k + |D| 2 rithm 2. [ 15 longer maintained. We evaluate the propose CmSDL approach sponding image document. There are 10 semantic categories in
P recision derived from the latent Dirichlet allocation (LDA) model [ Words (BoW ) representation using 128 codewords. The strategy in results are obtained by averaging over 10 classes.

We address four scenarios for both the supervised and semi-data vs. text queries, (4) labeled text training data + image training data vs. image queries.
We compare the proposed CmSDL approach with state-of-the-pared with the non-knowledge transfer method (PCA), correla-tion matching (CM) [ 31 ] , semantic matching (SM) [ 31 ] tic correlation matching (SCM) [ 7 ] , a bilinear model (BLM) generalized multiview linear discriminant analysis (GMLDA) and generalized multiview marginal fisher analysis (GMMFA) The non-knowledge transfer approach is achieved by directly ap-scenario, CmSDL is compared with classical methods based on a uni-modality (UNI) and an exhaustive search of mixing data from both media forms for training. The brutal approach is realized
P recision through enforcing dimensionality reduction (PCA) on the image representations. We compare the performance of different ap-under curve area of PR curves, are given in TABLE 2 and TABLE 3. It can be observed that the proposed CmSDL method consis-tently outperforms other methods. From the results in the su-pervised cross-modality scenario, we can conclude that brutally introducing miss-matched data to a target domain can break the data smoothness in the original domain, and thus lead to weak performance.
 The cross-modality per-category MAP performance for PCA, CM, SM, SCM and CmSDL methods is illustrated in Figure 5, and the supervised cross-modality per-category MAP performance for UNI, PCA and CmSDL methods is illustrated in Figure 6. It can be observed that the text modality has much better performance than the image modality. Such a fact can be explained with the ther be present in the  X  X rt X  category or the  X  X port X  category. Table 1: Semi-Supervised Cross-Modality Retrieval Performance (MAP scores).
 ation.
 Table 2: Supervised Cross-Modality Retrieval Performance (MAP scores).

In this work, we have proposed a greedy dictionary construc-compact and modality-adaptive dictionary pair. The compact-ness and modality-adaptivity are preserved by including the re-space. The CmSDL approach is evaluated on both the classical cross-modality scenario and the new supervised cross-modality kipedia dataset. [ 1 ] M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm [ 2 ] P. N. Belhumeur, J. P. Hespanha, and D. Kriegman. [ 3 ] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [ 4 ] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, [ 5 ] V. Cevher and A. Krause. Greedy dictionary selection for [ 6 ] G. Cornuejols, G. L. Nemhauser, and L. A. Wolsey. The [ 7 ] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, [ 8 ] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, [ 9 ] R. Datta, D. Joshi, J. Li, and J. Z. Wang. Image retrieval: [ 10 ] B. Fernando and T. Tuytelaars. Mining multiple queries for [ 11 ] R. D. Galv X o. Uncapacitated facility location problems: [ 12 ] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch X lkopf, and [ 13 ] Z. Jiang and L. S. Davis. Submodular salient region [ 14 ] Z. Jiang, G. Zhang, and L. S. Davis. Submodular dictionary [ 15 ] I. Khan, A. Saffari, and H. Bischof. Tvgraz: Multi-modal [ 16 ] A. Krause and V. Cevher. Submodular dictionary selection [ 17 ] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor [ 18 ] N. Lazic, I. Givoni, B. Frey, and P. Aarabi. Floss: Facility [ 19 ] M.-Y. Liu, R. Chellappa, O. Tuzel, and S. Ramalingam. [ 20 ] W. Liu and D. Tao. Multiview hessian regularization for [ 21 ] M. Long, G. Ding, J. Wang, J. Sun, Y. Guo, and P. S. Yu. [ 22 ] D. G. Lowe. Distinctive image features from scale-invariant [ 23 ] C. D. Manning, P. Raghavan, and H. Sch X tze. Introduction [ 24 ] M. Minoux. Accelerated greedy algorithms for maximizing [ 25 ] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis [ 26 ] S. J. Pan, J. T. Kwok, and Q. Yang. Transfer learning via [ 27 ] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain [ 28 ] Y. C. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogonal [ 29 ] J. C. Pereira and N. Vasconcelos. On the regularization of [ 30 ] G.-J. Qi, C. Aggarwal, and T. Huang. Towards semantic [ 31 ] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. [ 32 ] N. Rasiwasia, P. J. Moreno, and N. Vasconcelos. Bridging the [ 33 ] G. Salton. The smart retrieval system X   X  A  X  Texperiments in [ 34 ] L. Shao, L. Liu, and X. Li. Feature learning for image [ 35 ] L. Shao, F. Zhu, and X. Li. Transfer learning for visual [ 36 ] A. Sharma, A. Kumar, H. Daume, and D. W. Jacobs.
 [ 37 ] C. G. Snoek and M. Worring. Multimodal video indexing: A [ 38 ] I. Steinwart. On the influence of the kernel on the [ 39 ] D. Tao, X. Li, X. Wu, and S. J. Maybank. General tensor [ 40 ] D. Tao, X. Li, X. Wu, and S. J. Maybank. Geometric mean for [ 41 ] D. Tao, X. Tang, X. Li, and X. Wu. Asymmetric bagging and [ 42 ] J. B. Tenenbaum and W. T. Freeman. Separating style and [ 43 ] K. Wang, R. He, W. Wang, L. Wang, and T. Tan. Learning [ 44 ] C. Xu, D. Tao, and C. Xu. Large-margin multi-view [ 45 ] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. [ 46 ] F. X. Yu, R. Ji, M.-H. Tsai, G. Ye, and S.-F. Chang. Weak [ 47 ] F. Zhu and L. Shao. Weakly-supervised cross-domain
