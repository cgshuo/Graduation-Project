 ORIGINAL PAPER Kazim Fouladi  X  Babak N. Araabi  X  Ehsanollah Kabir Abstract This paper concerns with the recognition of offline Farsi/Arabic handwriting. The overall appearance of each subword in Farsi/Arabic script is described by its shape contour that provides us with a rich set of discriminative char-acteristics. Our approach is writer-dependent; that is, the sys-tem is trained to recognize the subwords written by a partic-ular writer. A fast contour alignment is the central part of the proposed algorithm, where the alignment is performed based on a handful of feature points. An efficient lexicon reduction algorithm based on characteristic loci feature, which works directly on subwords X  binary images, is proposed as well. Fast and precise alignment along with efficient lexicon reduction and appropriate similarity matching yields a high recognition rate while kept the speed high. Our experiment on IBN SINA database shows that the correct classification rate could be as high as 91.08 %. This figure is achieved merely by subword shape matching, without dots and diacritics, and without any statistical language model.
 Keywords Farsi Persian Arabic subword  X  Contour alignment  X  Handwriting recognition  X  Writer-dependent  X  Lexicon reduction  X  Characteristic loci  X  IBN SINA database 1 Introduction Character and word recognition has been widely researched recently, in part due to widespread applications [ 41 ]. These researches are generally classified into typewritten (printed) and handwritten recognition. Due to differences in the nature of scripts in different languages, separate works have been done for various languages [ 60 ]. Handwriting recognition problem is categorized into online and offline recogni-tion [ 50 ]. In online recognition, input data consist of both pixel positions and time information, while in offline recog-nition, the only available data are pixel positions. Offline problems seem to be more complicated because of missing time data.

The earlier works in handwriting recognition are per-formed for Latin languages, where several solutions have been proposed for single letters and cursive scripts [ 50 ]. For other languages, such as Arabic and Farsi, many attempts have been made in the recent years. Good reviews on this subject are [ 24 , 35 , 40 ] and [ 3 ].

The Arabic/Farsi handwriting recognition has been stud-ied using different approaches. Some works are tuned on specific applications, such as  X  X eading amounts of checks X  [ 23 , 26 , 59 ],  X  X eading fields in manually filled forms X  [ 39 ], and  X  X eading city names on postal envelopes X  [ 16 ]. Some other works are designed for general applications, most of which have a limited lexicon.

In the simplest form, many works recognize Arabic/Farsi digits and numbers with a very high accuracy, e.g., [ 38 , 42 , 45 ]. A major reason for this success is the small number of pat-tern classes. Solving this problem would be very helpful for automatic reading of numeric zip codes on postal envelopes and handwritten forms containing numbers [ 13 ].

Another group of the works deals with the recognition of single Arabic/Farsi letters [ 4 , 9 ]. In these works, although the number of pattern classes is low, but fundamental vari-eties in people X  X  scripts make some difficulties in recognition process.

Another path of research deals with handwritten word recognition [ 14 , 17 , 34 ]. Unit of recognition in this group is a complete  X  X ord X . It is obvious that difficulty in this prob-lem is more than the previous ones since the number of word patterns is very high. To handle this problem, two general approaches are followed: analytical recognition and holistic (segmentation-free) recognition. Holistic methods, in Farsi/ Arabic recognition, may refer to a connected part of a word X  i.e., a subword X  X s well.

Those works that do not use segmentation start with the image of the whole word and try to recognize it via extracting suitable features without dividing it into more simple parts. In the analytical recognition approach, the word should be divided into simpler components such as  X  X rimitives X ,  X  X et-ters X , or  X  X ubwords X . Several methods have been proposed for segmentation, such as using neural networks [ 5 ], using region growing [ 54 ], contour, and skeleton processing [ 65 ], using rotation-invariant features [ 2 ], and using cooperative agents and topological rules [ 56 ]. In a recent study [ 49 ], a segmentation-based method using structural and syntactic pattern attributes is introduced for Arabic handwriting word X  X  recognition. The segmentation process designed based on the nature of Arabic writing utilizes a polygonal approximation algorithm. In segmentation-based approach, if segmentation is done correctly, the number of patterns, comparing with the number of words, is reduced and the recognition process would be easier.

However, since correct segmentation is very difficult and incorrect segmentation causes error in subsequent steps of recognition, segmentation-free approaches are suggested, e.g., [ 6 , 34 ] and [ 20 ].

Another group of works use  X  X ubword X  as the unit of recognition, such as [ 7 , 19 , 28 , 52 ] and [ 1 ]. Using subword has some advantages against other methods. First, due to the nature of Farsi/Arabic handwriting [ 19 ], segmentation of a word into subwords is simpler than segmenting it into let-ters or primitives. Second, the number of subword bodies is much less than the number of words, so the number of pat-tern classes will be reduced. Moreover, several words can be constructed using a limited set of existing subwords; thus, a system, which works based on recognizing subword, can also recognize some other words that are missing in lexicon. We believe emphasizing on subwords is very important in Ara-bic/Farsi word recognition process because many subwords have same body and differences come from dots and other marking symbols, for example The choice of an appropriate representation of patterns has a crucial impact on the subsequent recognition steps. Apart from pixel representation, contour and skeleton representa-tions are often used as well. Skeleton representation uses thinning such that resulting image components have one-pixel thickness [ 2 , 8 ], while contour representation causes each subword to be converted into a closed curve [ 25 , 55 ]. At the first glance, the skeleton-based approach seems more natural for subwords, but it is not necessarily the best choice in our case due to discarding plenty of information (such as thickness/thinning information of body parts, distinction between filled regions of body and non-filled regions, etc.), particularly in Arabic/Farsi handwriting. The contour-based approach, on the other hand, seems more appropriate due to preserving body information. Though working with contours is more complicated and it does not seem to be intuitive either, humans do not trace the outline curve of subwords for recog-nition. The advantage of contour in comparison with skeleton is saving the shape information of subwords. On the other hand, reducing the sensitivity of extracted contour to uncer-tainty factors is a major concern in contour representation.
When we deal with a specific writer and we want to recog-nize his/her handwriting, the shapes of the contours have not significant changes and recognition problem is possible by matching the contour curves. Therefore, in this paper, we focus on contour representation for recognizing Farsi/Arabic subwords.

In holistic approaches, there is an important phase called  X  X exicon reduction X . Since the number of patterns is very high, we need a method to reduce the set of assumptions for any input pattern to increase the speed and accuracy of the classification process. Lexicon reduction techniques can be grouped into four main categories: (1) using context knowl-edge, (2) word/ subword length estimation, (3) shape feature matching [ 18 ], and (4) using special features. Various lexicon reduction techniques have been reviewed in [ 19 , 36 , 62 ].
While in many existing works proposed systems are writer-independent, our approach in this paper is writer-dependent similar to [ 19 ] and [ 28 ]. In both papers, Arabic subwords that are written by a single writer are classified. We start with the fact that in handwritten scripts, shapes of letters, subwords, and words could vary a lot among dif-ferent writers. As a result, for more precise recognition, it would be necessary to consider more than one class for most of these patterns. However, when we tune the system for a particular writer, these variations are highly reduced. Few works attempt to use this feature by making their sys-tems writer-adaptable instead of writer-dependent or writer-independent [ 11 , 12 ] and [ 21 ]. While our proposed writer-dependent system can be generalized to a writer-adaptable system X  X fter some modifications X  X e do not explore this possibility in this paper.
 Our problem in this paper is to recognize the body of Farsi subwords that are written by a particular writer. Sub-word means a connected part of a word with joint letters. For example, the following word has two subwords, which after removing dots, diacritics, and other marks, the body of subword is found: Writer-dependent offline handwritten recognition is partic-ularly important in recognition of old and historic docu-ments [ 30 , 61 , 66 ]. It may find application in recognizing notes and manuscripts of any particular writer, which is writ-ten in the past.

This paper has been organized in nine sections. After the introduction, in Sect. 2 , the overall structure of the proposed system is presented. Section 3 explains the preprocessing steps. In Sect. 4 , contour representation for the body of sub-words is explained. Section 5 , which is the most important contribution of this paper, describes the method of contour matching and similarity measuring based on a fast and accu-rate alignment method. Using this similarity measurement helps us to classify the patterns. Section 6 is devoted to the adopted lexicon reduction method. Implementation and experimental results are presented in Sect. 7 . In Sect. 8 , advantages and shortcomings of the proposed method are discussed. Finally, concluding remarks and suggestions for future works are presented in Sect. 9 . 2 An outline of the proposed system The goal of this paper is to develop a method to recognize the body of Farsi handwritten subwords. We assume that when a person writes with his/her normal script, there are little changes in the shapes of similar subwords; however, letters heights, letter teeth, extensions (Kashidas), etc., may change to some extent (Fig. 1 ).

Tuning the subword recognition algorithm for a single writer can increase the rate of recognition. In this work, we give special consideration to the contour shape of the handwritten subwords. In almost all other comparable works (except [ 19 ]), not the contour shape itself, but a set of appro-priate extracted features from the contour have been used.
Developed system is designed for writer-dependent recog-nition. Focusing on single writer is harmonious with the basic assumption of relative invariance of subword bodies X  shape. The system stores a set of important subwords written by a particular writer in its pattern library before the recognition process. In the recognition phase, the input subword matches with one of the existing subwords in the pattern library after lexicon reduction phase. Therefore, we can first recognize subword bodies X  X y removing dots, diacritics, and other marking symbols. We can introduce these and other use-ful information X  X ike adjacency of subwords X  X or the final decision making in postprocessing phase.

As seen in Fig. 2 , in our proposed system, there are two main phases: pattern library construction and subword recog-nition. During the pattern library construction phase, the sys-tem prepares a library of subword contours based on a rich sample of writers X  scripts. This may be achieved either by giving a tabular form to the writer to write a set of prede-fined frequent/important subwords or by gathering the same information manually from a small sample of handwritten documents.

Then, the contour representations of these subwords are extracted by a sequence of preprocessing steps, and the out-come is stored in the writer subwords library. The preprocess-ing phase includes binarization, image labeling for extracting connected components, morphological operations (includ-ing opening, closing, and dilation), image magnification, and edge detection. The contour representation is obtained by a tracing algorithm from the edge image. It should be noted that in contemporary Farsi/Arabic handwritten script, the holes may be filled. Therefore, we do not use them as discriminant features, and internal contours of the subword are discarded.
Now, when an input subword is fed, the system first uses its binary image in lexicon reduction phase. Then, the sys-tem constructs its contour representation. The resulting con-tour is compared with other contours in the reduced lexicon using an appropriate similarity measure. Contours matching in this phase is basically a curve matching. We introduce a fast method for contour alignment, which is the heart of our approach. The most similar pattern with the input subword is returned as output.

It should be mentioned that we assume error-free subword segmentation. Any error in subword segmentation can affect the performance of recognition, which is out of the scope of this paper.

Figure 3 shows data flow for our proposed subword recog-nition system. 3 Preprocessing The quality of handwriting recognition is highly affected by the quality of input images. We work on scanned images of subwords, which make the problem more serious. Words are usually written by a normal pen and their sizes are naturally small. On the other hand, scanning handwritings by a reso-lution higher than 300 dpi is not practical with a reasonable price, due to common scanners X  speed, usual hardware lim-itations, and paper quality. Therefore, several preprocessing steps on binary input images are proposed here to make them more appropriate for contour construction.
At first, connected components of the subword are extracted from its binary image. Those connected compo-nents with a surface area smaller than a predefined thresh-old are excluded. Small components are either small mark-ing symbols X  X uch as dots X  X r noise. Subword splitting may happen in both writing and scanning. We use morphologi-cal dilation on subword image to reduce the effect of split-ting. Now, the connected component with the largest area is labeled as the main component of the subword. After main component extraction, image is resized using image magni-fication, which makes important details of subword such as teeth more clear. 4 Contour representation of subwords bodies Our studies and experiments indicate that the best shape information can be extracted from the contour. In fact, there are certain features of the Farsi/Arabic script, which are best described by the contour not skeleton. These features are mentioned in Sect. 5 .

To extract contour of body, we start with the output image of preprocessing. The method of contour extraction is essen-tially very simple. Edges of the binary image are found by an edge detection operator. Then, an arbitrary point is selected on the resulting edge (we always choose the rightmost point). In each step, an appropriate neighboring point in clockwise direction with ( x , y ) coordinates is added to the contour. This operation is repeated until we return to the starting point. The details of the utilized  X  X oundary Tracing X  algorithm can be found in [ 58 ]. Now, the contour is available as an array of points ( x , y ) : [ ( x 1 , y 1 ), ( x 2 , y 2 ),...,( x N , y N ), ( x 1 , y 1 Sometimes, it is computationally more convenient to repre-sent the contour in the complex space. In this space, we deal with a one-dimensional array of complex numbers: [ z 1 , z 2 ,..., z N , z 1 ] z
This closed curve reflects the pattern of body of sub-word very well. However, since the contour is constructed by boundary tracing of edge pixels, it is not smooth enough for further processing. A smoothing (low-pass) filter miti-gates this problem. One of the best ways for contour smooth-ing is exploiting Fourier descriptors [ 58 ]. In this technique, the discrete Fourier transform of z [ n ] is computed and high-frequency coefficients are excluded: Z [ u ]= The modified smoothed curve is obtained by taking the inverse Fourier transform of the remaining coefficients (first P coefficients) [ 10 ]:  X  z = 1
Our experiments show that keeping 30 X 50 low-frequency coefficients results in a smoothed curve with acceptable qual-ity and sufficient details for the next steps. Figure 4 shows a sample of contour representations for a particular subword.
An interesting feature of contour representation for hand-written Farsi subwords can be seen in magnitude and phase characteristics of z [ n ] . When pen thickness is almost fixed throughout the subword (as in subwords  X   X  X r X   X ), some form of symmetry is observable in magnitude and phase charac-teristics of z [ n ] . This symmetry is due to the fact that the forward and return paths from the starting point to itself in these contours are almost the same. Furthermore, when an asymmetry is found in magnitude and phase characteristics, the variable pen thickness is observable in some parts of the subword body. Using this simple feature, one can discrimi-nate subwords like  X   X  and  X   X , which have similar shapes. Figure 5 shows a sample of this situation. 5 Proposed fast contour matching using contour alignment For contour matching, it is vital to find an appropriate mea-sure of similarity for a pair of contours. This problem is stud-ied extensively in shape recognition literature, where many algorithms have been suggested [ 57 ].

The most important stage in contour matching is to deter-mine the corresponding points on two curves, which is known as  X  X urve alignment X . Let z 1 [ n ] be the first complex curve and z 2 [ m ] be the second one, where n = 1 , 2 ,..., N and m = 1 , 2 ,..., M . Note that z 1 [ n ]= x 1 [ n ]+ jy 1 [ z [ n ]= x z is defined as a mapping g : g :{ 1 , 2 ,..., N } X  X  1 , 2 ,..., M } g ( n ) = m . (3)
The goal of the alignment problem is to find the most appropriate mapping g (Fig. 6 ). Although we deal with con-tours, the problem formulation is not much different with open curves, except for the periodicity of the contours. It is obvious that the number of possible mapping g increases exponentially with M and N , but by imposing appropriate conditions on alignment, the problem becomes numerically tractable.

The first problem that we encounter is that the starting points of two given contours do not necessarily coincide. On the other hand, lengths of contours (number of their sam-ples) are not necessarily equal. Therefore, two contours must be normalized according to their lengths and starting points to simplify and accelerate the matching process. To equate length of contours, the contour with more samples is down-sampled to match the other one. Now, both contours have equal lengths N = M . There are different approaches to the problem of curve alignment. The idea of curve alignment using dynamic time warping (DTW) is introduced in [ 63 ]. DTW with a proper cost function is used for estimating the shift or warping function from one curve to another to align the two functions. This method is used in [ 19 ] and [ 52 ]for subwords shape classification. In [ 47 ], continuous DTW is used for translation invariant curve alignment with applica-tions to signature verification. The concept of  X  X lignment curve X  is employed in [ 57 ] for treating both curves sym-metrically. A similarity metric based on the alignment curve is defined using length and curvature. The optimal corre-spondence is found by an efficient dynamic-programming method. Using geometric transform between two curves is another approach for curve aligning which is utilized in [ 67 ]. In this work, two curves are aligned under projective trans-form. In [ 37 ], curve alignment is performed using a multires-olution method. Curve features are extracted with an empha-sis on salient and general features, which leads to tangible results for human being X  X  perception. A coarse-to-fine align-ing algorithm is introduced to match curves under different resolutions. In [ 43 ], a fuzzy algorithm for the alignment of curves is proposed. To align a pair of shapes, each feature point selected in the first shape is associated with all the points in the second one. A confidence degree is computed for each of these matches. Pose variables and confidence degrees are obtained by minimization of a fuzzy energy. Sev-eral works in the context of curve alignment use statistical approach. In [ 32 ], an alignment method based on equating the  X  X oments X  of a given set of curves is developed. These moments are intended to capture the locations of important features that may represent local behaviors, such as max-ima and minima, or more global characteristics, such as the slope of the curve averaged over time. The matching is per-formed by equating the moments of the curves while shrink-ing toward a common shape. This approach allows to capture the advantages of both landmarks and continuous monotone registration. In [ 53 ], alignment of curves is performed by non-parametric maximum likelihood estimation, when the indi-vidual transformations of the time axis are modeled by unob-served random shifts. The approach of nonparametric curve alignment is used in [ 44 ] by  X  X ongealing X  as a flexible non-parametric data-driven framework for the joint alignment of data. In general, the above methods are designed for general curves; thus, all the points on the curves are considered and the concept of  X  X pecial points X  X  X hich is problem-related X  has not been addressed. This issue affects the accuracy and the speed of those algorithms, since special features of the shape of Farsi/Arabic subwords are not considered. In sum, the common method for aligning two curves is the standard DTW, which is an O ( N 2 ) algorithm using dynamic program-ming [ 48 ]. This is a general algorithm that considers all points on two curves, but for the problem of subword matching, in the sequel, we propose an algorithm that relies on a small number of feature points on two curves with a linear time complexity. 5.1 Finding common starting points of two contours To tackle this problem, we propose peaks of contours in y direction as starting point candidates. The success of this choice depends on particular shapes of contours extracted from objects, which are Farsi/Arabic script subwords in our case. Every closed curve in xy plane has at least one peak in each direction. On both contours, all the peaks in y direction are considered as starting points. At each iteration, a pair of peaks ( r , s ) on two contours is chosen and the similarity of contours is measured by cross-correlation. Let z ( r ) [ n closed curve z [ n ] in which the starting point is shifted to r , of two closed curves z 1 [ n ] and z 2 [ m ] is computed through the following optimization: ( r  X  , s  X  ) = arg max where peaks ( z ) is the set of peaks of z in x direction, z is phase of z , and the cross-correlation of two equal-size curves u and v is computed by correlation ( u ,v) =
We examined different possible choices for correlation computation in ( 3 ) X  X ncluding, magnitude, phase, real part, and imaginary part of z . Our study indicates that choosing phase provides the best overall performance. This is due to the fact that Arabic/Farsi handwriting is very curly. Most of the letters are written in a way that the curvature of subwords changes rapidly and locally. As a result, phase seems to be the more informative characteristic of the contour. Obtained results support this conjecture. Figure 7 shows some samples of finding common starting points of two contours. 5.2 Contour alignment Comparing contours becomes easier after finding starting points, but there are still some difficulties. The problem par-ticularly comes from the shape of handwriting subwords. Although contours of a subword written by the same person in different situations have similar shapes in general, their details are not necessarily the same and s/he may make some variations on subword shape at each instance. To overcome this problem, it is essential to determine some feature points on each contour that are almost invariant in different writing instances of the same writer. Furthermore, the time consumed by the matching process may reduce drastically by using fea-ture points instead of all the curve points.

Peaks and valleys of contours in y direction are proposed here as suitable feature points. The y direction is the natural choice due to the natural direction of handwriting, which is x . The existence and the relative position of the chosen feature points in the xy plane is almost invariant during different instance of writing subwords by the same person. Indeed, in Farsi/Arabic script, chosen feature points are harmonious with ones intuition. It seems that the writer most often attends chosen feature points to specify the desired word. Having starting point and feature points, the next question is the cor-respondence of feature points between the two contours.
In the sequel, at first, we explain how to extract feature points. Then, a simple string representation is extracted from the contour. Then, a modified longest common subsequence (LCS) algorithm, which we call minimum-distance LCS, is proposed and utilized for fast contour alignment. 5.2.1 Contour representation using its feature points string To determine feature points, each contour is divided into an upper and a lower curve (similar to [ 34 ]). To get this division, first, we find those points of contours with maximum and minimum in x direction. If we move over the contour from the point with the minimum x (left) to the point with maximum x (right) in the clockwise direction, the upper curve is extracted. The remaining part of the contour constitutes the lower curve (Fig. 8 ).

Peaks and valleys are found by taking derivative of smoothed contour curve with respect to x . Now, the sequence of extreme is represented as an attributed string, Fig. 9 . Each peak is encoded by  X  X  X  if it is on upper curve and encoded by  X  X  X  if it is on lower curve. Each valley is encoded by  X  X  X  if it is on upper curve and encoded by  X  X  X  if it is on lower curve. Three attributes are assigned to a point: normalized x , normalized y , and index of the point on the contour. To make meaningful comparison of distances between two con-tours, x and y coordinates of both contours are normalized to [ X  1 , 1 ] . Figure 9 shows a sample of marking the contour of a given subword with the quadruple alphabet {M, m, N, n} and resulting string. Strings are built in clockwise direction, by convention. The first extremum point on the lower curve is defined as the starting point of the string, again by conven-tion. The choice of the starting point of string representation is a convention since we work with closed curves, and dur-ing the matching algorithm, strings are shifted to find the best matched starting point. 5.2.2 Minimum-distance longest common subsequence Now that each contour is represented by a string of its feature points, solution of the problem of contour alignment can be sought in the form of string alignment problems. Here, each position in one string corresponds with a position in another string. The  X  X ongest common subsequence X  or LCS algo-rithm [ 15 ] is an effective algorithm that actually aligns two strings. However, since our strings have important geometri-cal attributes, the standard LCS algorithm could not be used directly. To overcome this problem, we modify standard LCS algorithm. In our modified LCS, not only characters (syntax) should be matched, but also geometric distances of corre-sponding points (semantic) are considered in matching and alignment operation. This algorithm uses dynamic program-ming and corresponds to two positions of the two contours if their characters are the same (for example, corresponding an  X  M  X  in one string with an  X  M  X  in another string means that one peak in lower curve of one contour can match only with a peak in lower curve of another contour, and so on) and normal-ized distance of these points are less than a threshold value. The pseudocode of this algorithm that is called minimum-distance longest common subsequence (minimum-distance LCS) algorithm is shown in Fig. 10 . In this algorithm, there is an array, lcstable [ i , j ], to store the number of matched posi-tions until that location. Furthermore, another array, lcspath [ i , j ], is used to determine the corresponding positions by following their containing marks.

The threshold value in this algorithm is computed as fol-lows: Distances of all pairs of points between the two strings over their contours are computed. Since the length of these strings is not necessarily equal (numbers of peaks and valleys in two contours could be different), we assume that the max-imum number of corresponded points is equal to the length of smaller string. If l is the length of smaller string, we keep l smaller distances and define the largest distance among them as the threshold value. Therefore, the algorithm is guided to match points that are as geometrically close to each other as possible. 5.2.3 Fast contour alignment based on minimum-distance The heart of the alignment algorithm in this paper is the proposed minimum-distance LCS algorithm. Indeed, after equating the length of the contours and determining the com-mon starting points, both contours are restarted according to their starting points, and then the coordinates of the points of both contours are normalized X  X s explained in previous subsection. After this step, string representations of the con-tours are computed and given as an input to the minimum-distance LCS algorithm. The output of this algorithm makes the correspondence between some (but not necessarily all) of the feature points. Not-yet-assigned feature points on a contour are assigned to appropriate points on the other con-tour using a linear interpolation. In Fig. 11 ,let b 1 = f and b 2 = f ( a 2 ) .If f is a linear interpolation function which maps every s 1 on the first axis to s 2 on the second axis, we have the following: s = f ( s
A = a 2 B = a It should be emphasized that s 2 , itself, is not a feature point.
This operation is repeated for any points on two contours that have no correspondence. As a result, alignment process is completed. A pseudocode for contour alignment algorithm has been shown in Fig. 12 .

Some samples of alignments are shown in Fig. 13 .The steps to perform an alignment are presented in Fig. 14 .Itis shown that this algorithm has a low computational complex-ity because of relying on feature points of contours instead of all contour points. Actually, the running time of minimum-distance LCS in the worst case is O ( m  X  n ) similar to standard LCS where m and n are the numbers of feature points of two contours. Since the ratio of feature points of a contour to the number of the contour points is small (according to experi-mental results, in almost all cases m &lt; where M and N are lengths of contours), we can claim that it is an experimentally linear time algorithm for aligning two closed curves. Indeed, if we define k = max { n , m } K = max { N , M } ,wehave m  X  n &lt; k 2 &lt;( which indicates a linear time O ( K ) = O ( max { N , M } ) rithm. 5.2.4 Contours similarity measure Similarity of the contours is measured by a simple similarity measure after alignment. Two contours are comparable point by point after alignment. If g is the alignment function, simi-larity of the contours aligned with this function, namely z and z 2 [ n ] , is computed using a cross-correlation measure as follows: s = correlation ( z
In fact, a nonlinear time (space) warping is performed on the second curve according to the alignment function g , and then the cross-correlation of two contours is computed.
Other similarity measures could be used, as well. For example, we could use geodesic distance [ 19 ]: s = 1 where, ., . denoted the inner product of the two equal-length vectors representing the two curves.

Figure 15 a, b shows examples of similarity measuring for two very similar and two not-much similar contours, respec-tively. In both cases, x and y characteristic curves of those contours are drawn over each other. Characteristic curves of the second contour are located on the first using a non-linear space warping according to alignment points. As it is shown, each part of the second contour is stretched or strained between two alignment points. 6 Lexicon reduction using characteristic loci features In the holistic approach for word/subword recognition, the large size of the reference pattern set (lexicon) is an impor-tant problem, which affects the speed and accuracy of the recognition process. Lexicon reduction helps in part to over-come this problem. There are different techniques for lexicon reduction that is briefly reviewed in Sect. 1 . In this research for lexicon reduction, we use a shape feature matching tech-nique called  X  X haracteristic Loci X , which codes the overall shape of the subwords. Characteristic loci have been intro-duced in [ 31 ] for the first time and used for recognizing digits and isolated characters. In [ 10 ] and [ 22 ], this feature has been used to cluster printed Farsi subwords. The good performance of characteristic loci features in printed subwords clustering motivates us to use it in lexicon reduction for handwritten subwords. To the best of our knowledge, characteristic loci have never been used for handwritten Farsi/Arabic lexicon reduction so far.

To compute the characteristic loci feature, we use the binary image of input subword. For every background pixel of the binary image, a locus number is calculated. The locus number is a four-digit radix-4 number, which represents the number of intersections with the subword body in right, upward, left and downward directions (Fig. 16 a). Number of intersection is limited to 3. Since the largest four-digit radix-4 number is 255, the locus numbers are between 0 and 255. For each binary image of input subwords, the locus number is calculated for all background pixels, and the histogram of locus numbers is computed. Consequently, we obtain char-acteristic loci as a feature vector with 256 elements, where each element represents the total number of background pix-els that have locus number corresponding to that element X  X  index. For example, 82th element of this vector represents the number of background pixels with locus number of 82. This feature vector is normalized by dividing by the total number of background pixels (Fig. 16 b). ( a ) (b)
When the query pattern is entered into the system, its char-acteristic loci feature is computed. Then, the k -nearest pattern in the lexicon (according to their characteristic loci features) is kept as the reduced lexicon. Since the vector of character-istic loci features is basically a histogram, we can use those distances that are more meaningful in comparison with two histograms, such as Euclidean distance (eucl), chi-square dis-tance (chi2), and correlation similarity measure (corr) [ 33 ]. In subword shape recognition phase, we should only con-sider references in reduced lexicon as assumptions for query subword. The value of k is determined according to desirable accuracy of reduction  X  (the probability that the query sub-word class is included in the reduced lexicon). The degree of reduction  X  is defined as the relative decrease in the size of the lexicon after pruning.  X  and  X  are used in measuring reduction efficacy. 7 Experimental results 7.1 Database for experiments Since our goal in this work is writer-dependent recognition of Farsi/Arabic subwords, we use IBN SINA Ext database [ 27 ] that consists of over 20,000 Arabic subwords in 1,260 classes. This database is based on a commentary on an impor-tant philosophical work by the famous Persian scholar Ibn Sina (Avicenna), which is written in the Naskh style by a single writer (Fig. 17 ). It consists of 60 pages in that 50 pages of them are published in the database. The document images were binarized with an algorithm proposed in [ 29 ] to preserve the shape X  X  topology. Each page contains approximately 500 subwords. The distribution of the database is highly unbal-anced; some classes have up to 5,000 entries, while others have fewer than 5. The diacritics and marking symbols are ignored. 7.2 Evaluation of lexicon reduction To evaluate lexicon reduction method, the degree of reduction in the shape database,  X  db , and the degree of reduction in the lexicon,  X  lex , are calculated for all query subwords in the database to obtain a given accuracy of reduction,  X  . Similar to [ 19 ], a leave-one-out strategy is used for the evaluation, where one shape is selected as the query and the rest of the shapes are considered as reference shapes. The query shape is replaced with all the database shapes one at a time. The results are averaged over the entire database.

The performance of lexicon reduction using characteristic loci features on IBN SINA database is shown in Fig. 18 for both database and lexicon. Using  X  lex =  X   X   X  lex as the mea-sure of reduction efficacy, one finds that the best value for is 0.8990 for  X  = 0 . 9368 , X  lex = 0 . 9596, and  X  db = 0 The size of the reduced database for this  X  is 9 (Table 1 ). The best results are obtained using chi-square distance measure; therefore, we use this measure in lexicon reduction phase.
The average time of lexicon reduction for each query sub-word is 102.8 ms, which is run over a 2.4 GHz Pentium 4 Quad PC. 7.2.1 Comparison with other methods We compare the proposed lexicon reduction method with three other state-of-the-art approaches. The first approach is  X  X deal diacritics matching X  and the second one is  X  X ia-critics matching X  [ 46 , 64 ]. In these approaches, the lexicon is reduced first based on the number of subwords, and then a dot descriptor string is used. Since the IBN SINA database con-sists of subwords patterns, only the second step is used in our experiments. In  X  X deal diacritics matching X , the perfect per-formance of the dot descriptor extraction from the subword images is assumed. A rule-based method is used to extract the dot descriptor from the images. The lexicon reduction is per-formed using the string edit distance for the ideal dot descrip-tor of the lexicon. In  X  X iacritics matching X , the assump-tion of ideal behavior of dot extraction step is relaxed. The third compared approach is called  X  X ast L-TSV X  introduced in [ 19 ]. In this method, the subword shape is represented by a weighted topological signature vector (W-TSV), which encodes a particular subword graph extracted from subword skeleton into a low-dimensional vector space. Among sev-eral subword graphs, L-DAG shows the better performance. L-DAG is a directed acyclic graph that represents topologi-cal and geometrical features of subword shape. In Table 2 , the detailed comparison of this method with the proposed method is shown. Table 3 shows the comparison with other lexicon reduction methods on the IBN SINA database.
As it can be seen in Tables 2 and 3 , the reduction efficacy of the proposed lexicon reduction method is higher than all other studied methods, significantly. The better performance of the characteristic loci method in comparison with the fast L-TSV method could be due to their differences in encod-ing the shape information. In fast L-TSV method, the shape information encodes as a simple graph based on the skele-ton of subword image, which cannot represent detailed geo-metric relation between important points of each subword with enough accuracy. Moreover, in fast L-TSV method, the extracted features from the graph do not change with relo-cation of nodes; thus, some dissimilar subwords considered as similar ones only due to similar topology. On the other hand, the characteristic loci method associates information of each point of subword shape with all the neighboring points, which results in a more precise description of the over-all shape. Obtained results indicate that the higher accuracy of the characteristic loci method is desired when it comes to Farsi/Arabic subword matching, particularly due to the importance of details in Farsi/Arabic handwritten subwords. 7.3 Evaluation of subword classification Subword classification is performed after lexicon reduction phase using a curve matching method. The first 40 pages of the IBN SINA database are used as the shape refer-ence data and the last 10 pages form the test and evalua-tion data. The lexicon is reduced using characteristic loci method. The results for different contour qualities are shown in Table 4 . The contour quality is a function of (1) the num-ber of Fourier coefficients used in reconstruction and (2) the downsampling rate. The number of Fourier coefficients controls curve smoothness while downsampling rate shows amount of reduction in contour length. Since our proposed algorithm for contour alignment is based on peak and val-ley points, the smoothing process is necessary to find these feature points accurately. Downsampling, on the other hand, reduces the number of contour points and increases the speed of matching process.

Several observations and discussions on the reported results in Table 4 are in order:  X  Classification accuracy, which is measured by the aver- X  The time of classification process for each query subword  X  The best results based on first match are obtained by 35  X  The obtained results with downsampling rate of 80 %  X  Downsampling corresponds with a particular form of  X  At 90 % accuracy of reduction only four candidate sub- X  The classification accuracy is confined by the lexicon 7.3.1 Comparison with other methods We compare our method with other subword classification approaches reported on IBN SINA database. In [ 19 ], the fast L-TSV method is used for lexicon reduction in IBN SINA database and a contour similarity measurement technique is used for subwords body classification. In our compari-son, two alternatives for three important parts of the algo-rithm are considered, namely Contour representation space (normalized x X  X  and SRV), alignment algorithm (proposed method and standard DTW), and similarity measure [cross-correlation Eq. ( 7 ) and geodesic measure ( 8 )]. Table 5 shows average classification time and average correct classification rate for eight possible combinations.

Contour representation in square root velocity (SRV) space uses derivative of the contour q ( t ) =  X  z ( t )/ therefore, it is invariant under translating and scaling of sub-word contours. This representation is used in [ 19 ]. Our repre-sentation (normalized x X  X ) is also invariant under translating and scaling, but there is no overhead for derivatives compu-tation and there is no noise magnification due to derivatives.
Our proposed alignment algorithm has the advantage of linear time complexity, O ( N ) , as compared with O time complexity for standard DTW algorithm, which is used for the proposed alignment algorithm is 3 X 4 times faster than standard DTW algorithm. This is due to the fact that our method, which is designed particularly for Farsi/Arabic subword patterns, relies on peaks and valleys of the contour shape and does not include other points of contour during alignment. The number of contour points, N , in our experi-ments varies from 12 to 410 (average is 83), while the number of feature points varies from 2 to 75 (average is 9).
Our alignment algorithm runs on normalized x X  X  curve of the subword, while in [ 19 ], the alignment is performed on the SRV representation.

Two similarity measures are considered. Based on our obtained results, there is no meaningful difference between cross-correlation (proposed method) and geodesic measure (which is used in [ 19 ]).

It should be mentioned that finding the query subwords even in the Top-30 patterns is acceptable in our experiments, since we have not employed dots and probabilities of sub-word adjacencies yet.
The comparison between the proposed methods (SRV + proposed alignment on xy curve + cross-correlation) with [ 19 ] (SRV + standard DTW on SRV curve + geodesic) is shown in Tables 7 and 8 .

To determine the contribution of our lexicon reduction method in the classification results, we implement the [ 19 ] method with characteristic loci-based lexicon reduction. Obtained results and comparison with our method are shown in Table 7 . It can be seen that characteristic loci-based lexicon reduction outperforms fast L-TSV in the same setting com-parison. On the other hand, our proposed fast contour align-ment outperforms standard DTW on SRV curve when similar lexicon reductions are employed. The time efficiency of the proposed method is evident from Table 8 , where although our lexicon reduction is more time consuming, the total time consumption is much less than the compared method [ 19 ].
Figure 19 shows samples of correct classification results for some query subwords. The visual similarities of the 5 most similar recognized patterns with the query subword as well as their numerical similarities are considerable. In Fig. 20 , some examples of failed classification are shown.
To compare the proposed method with the state-of-the-art contour-based recognition methods for handwritten charac-ters, we use two popular methods: local contour direction histogram (chain-code histogram) and gradient direction his-togram, which has been used in [ 38 ] for Farsi/Bangla numer-ical characters. In both methods, the input subword image is normalized by R = sin (  X  2 R ) , where R and R are the aspect ratios of input image and normalized image, respec-tively. The extracted features are constructed by a concate-nation of localized histograms of orientation/direction ele-ments. We use 8-direction histograms for both chain-code and gradient methods. To compute the gradients, Sobel masks are applied on the edge image. To get the better results, a low-pass (blurring) filter is applied. Features are extracted by computing direction histograms for each zones of blurred image. Number of zones, N z , is used as a tuning parameter of the algorithm. The details of the chain-code and the gra-dient histograms feature extraction methods can be found in [ 18 ]. Obtained feature vectors are essentially histograms, as a result, similar to the lexicon reduction phase, we can compare and classify them by using histogram comparison methods, namely Euclidean, chi-square, and correlation. Table 6 indi-cates the result of applying the aforementioned two algo-rithms using different configurations, where for all experi-ments lexicon reduction by characteristic loci, features along with chi-square measure are utilized.

We u s e 3  X  3 , 4  X  4 , 5  X  5, and 10  X  10 for number of zones. For gradient histogram features, we found that the best results are obtained using 4  X  4 zones, and chi-square distance measure for  X  = 90 %. For  X  = 95 %, the con-figuration (5  X  5 zones, and chi-square distance measure) yields the best result for Top-1 rank. Moreover, for chain-code histogram features, we found that the best results are obtained using 4  X  4 zones, and chi-square distance measure for  X  = 90 %. For  X  = 95 %, the configuration (3  X  3 zones, and chi-square distance measure) yields the best result for Top-1 rank. Table 6 indicates only the best obtained results for different number of zones, where it can be seen that the gradient histogram features outperform the chain-code his-togram features.

In Table 7 , the best results of chain-code/ gradient his-togram methods have been compared with the proposed method on IBN SINA database. It indicates that the perfor-mance of the chain-code/ gradient histogram method depends highly on lexicon reduction. For  X  = 100 % in lexicon reduc-tion, the performance of the chain-code histogram method reaches to 80.31 %, and the performance of the gradient histogram method reaches to 84.76 %, but the performance of our proposed method reaches to 91.08 %. This accounts for the advantages of contour representation over the gra-dient histogram features of the edge image. However, for  X  = 90 % in lexicon reduction, chain-code and gradient histogram method perform much better. In this case, while their performances are still significantly below our proposed method, they outperform Chherawala [ 19 ] method.

The execution time comparison in Table 8 indicates that the proposed method has a total execution time com-parable with the chain-code/gradient histogram methods. Although the classification time in the chain-code/gradient histogram methods is much lower than the proposed method, they become comparable in the overall recognition system, when consumed time by lexicon reduction is considered as well.

Figure 21 provides a comparison of total classification performance of the proposed method and other studied meth-ods. The superior performance of the proposed method can be seen in this figure. 8 Discussion Since the proposed algorithm in this paper is of order O ( where N is the length of contour, it is obvious that our algo-rithm has a considerable advantage in terms of speed. Indeed, our algorithm can run almost real time in many practical applications. Furthermore, experimental results in Table 5 indicate the advantage of the proposed method in recog-nition accuracy. This is mainly due to our efficient align-ment method, which makes a better correspondence between two contours extracted from Farsi/Arabic handwritten sub-words.

The experimental results show efficiency for the proposed method for contours similarity measurement in the context of Farsi/Arabic handwriting subwords. In almost all of the cases, the more intuitively similar pair of contours results in a larger similarity value by the proposed measure. There are, however, examples that expected to be recognized in higher ranks of similarities, but they are recognized in lower ranks. For instance,  X  In Fig. 20 , row 1, the query subword is  X   X , but the  X  Kashida in 20 , row 2, query subword  X   X , results in  X  The incorrect classification could be due to incorrect  X  Lack of enough clearness of teeth is another source of  X  In some cases, query subword has a high visual sim-
In the best case, the expected subword is recognized in the first rank of the list, which is happened in more than 90 % of the samples. However, as shown in Fig. 20 ,this is not always the case. Our study points to various sources for incorrect first rank recognition, including high similarity among contour patterns, partial matching of contours, and writer X  X  error.

For example, in Fig. 20 , row 1, there are high similari-ties among high-ranked patterns of each rows; therefore, dis-crimination among them is difficult even for a human. One of the methods for eliminating this error is subword cluster-ing. In subword clustering, very similar subwords are put in one class. When the class is recognized, the main subword is identified using other discriminating information.
As another example, in Fig. 20 , row 10, the error is due to partial matching between a subword and a part of another subword X  X n this case, the high similarity between  X   X  part of the subwords causes the incorrect classification. This error can be eliminated by considering top-k results X  X nstead of top one X  X long with statistical language model [ 51 ].
Another example of errors is shown in Fig. 20 ,row4, which is a result of careless writing. The writer wants to write  X   X , but puts one additional tooth. As a result, the subword is recognized as  X   X . These errors may be resolved using the subword X  X  context information and statistical language model [ 51 ].

All in all, these kinds of errors may be alleviated by using the well-known postprocessing methods, which is not the focus of this paper. 9 Conclusion and future works In this paper, a new method for recognizing the body of Farsi/Arabic handwritten subwords was proposed. This method relied on representing main body of subwords with contours and contour alignment. The similarity of subwords was measured by matching their aligned contours.

The basic assumption used in our proposed writer-dependent system was that a particular writer does not make significant changes in the shape of subwords, i.e., s/he writes same subwords almost the same way during repeated trials. Our experiments showed that our assumption is reasonable in practice.

Contour representation of subwords had many advantages including direct access to the shape of the subword which had rich information for recognition process. Moreover, contour representation conveyed particularly useful information for Farsi script matching, where thickness variation played an important role in characterizing the shape of the subword.
The proposed curve alignment method is designed for high accuracy along with high speed, where alignment was per-formed in a time duration proportional to the length of the contours. This feature made the proposed method more prac-ticable. The similarity of aligned contours was measured by a fast correlation-based method.

An efficient lexicon reduction method based on character-istic loci features was proposed as well. This method worked directly on subword binary images. While the processing time of the proposed lexicon reduction method was higher than that of fast L-TSV method [ 19 ], the degree of reduction in the former method was considerably better.

Our experiments with IBN SINA database indicated that the proposed method reached high accuracy while kept the recognition speed high. An accuracy of 91.08 % was achieved without lexicon reduction. With lexicon reduction and for an accuracy of reduction of 90 %, which reduces the size of database to 4, an accuracy of 86.76 % for classification was achieved. Both figures improved existing figures obtained by Chherawala and Cheriet [ 19 ]. The overall matching time (for  X  = 90 %) was also bettered by a factor 2,300 / 109 . 7  X 
The performance of the proposed contour matching method was compared with that of two popular and efficient methods, namely local contour direction histogram and gra-dient direction histogram methods. While the overall recog-nition speed obtained by these two methods was comparable with our proposed method, they could not sustain a compa-rable high accuracy, at the same time.

Experiments with an implementation of proposed method in [ 19 ] along with our lexicon reduction indicated that the improved classification accuracy was not just a result of better lexicon reduction. The contribution of contour representation and proposed fast contour alignment was significant in the overall accuracy and speed. All the reported numbers for the accuracy of classification were achieved merely by subword shape matching. These figures may improve by considering dots and diacritics as well as statistical language model.
Following this work, currently, we are trying to extend our writer-dependent recognition system, to a writer-adaptable one. To this end, we try to expand the pattern set of user X  X  subwords in a model-based scheme, where new subwords are built based on existing ones. This may reduce the size of required training sample significantly. The model-based approach facilitates the writer adaptation. Furthermore, with appropriate pre-and postprocessing, it is possible to use holes, dots and other marking symbols in a later phase of this research to obtain a better performance.
 References
