 Named Entity Disambiguation is the task of disambiguating named entity mentions in unstruc tured text and linking them to their corresponding entries in a large knowledge base such as Freebase. Practically, each text match in a given document should be mapped to the correct entity ou t of the corresponding entities in the knowledge base or none of them if no correct entity is found (Empty Entry). The case of an empty entry makes the problem at hand more complex, but by solving it, one can successfully cope with missing and erroneous data as well as unknown entities. In this work we present AOL X  X  Na med Entity Resolver which was designed to handle real life scen arios including empty entries. As part of the automated news anal ysis platform, it processes over 500K news articles a day, entities from each article are extracted and disambiguated. According to our experiments, AOL X  X  resolver shows much better results in disambiguating entities mapped to Wikipedia or Freebase compared to industry leading products. Named entity resolution; Named entity disambiguation. The ability to identify named entities [1] and link them to entries in knowledge bases (KB) [4] is a popular task in Information Retrieval and NLP communities. In this article we will address the final stage in the process of selecting the best fitting entities to the text from a set of given text matche s plus the empty match. In the presence of an entity name in an article, the common practice of entity resolvers is to match the mention to the most probable entity in the KB, even when the en tity in the article is clearly not one of the existing options [2]. Formally, the option of an empty disambiguation definiti on. The extended disambiguation needs to match the most probable entity in th e KB or none of them if it's more probable. Humans are usually much better than machines at tasks of news articles entity disambiguation, probably because humans can exploit a priori domain and world knowledge which helps identifying the correct context at hand. In many cases, the article X  X  author leaves contextual and disambiguation hints aimed to help human readers to quickly identify and understand the article's domain and cont ext (e.g.: The famous anthropologist Michael Jackson.). We believe this ability is very helpful for the human disambiguation process. AOL X  X  Named Entity Resolver tries to mimic the human entity disambiguation process, while leveraging the author X  X  hints and language logic (true ambiguities are dealt by the author, e.g.  X  X ew York City X  and  X  X ew York State X  will be mentioned at least once in full form in order to help readers disambiguate). We rely on a World Knowledge Graph derived from data sources such as Wikipedia to represent readers X  a priori domains knowledge, and a Document Graph to represent the article at hand along with the author's hint. In this section we present our entity resolution approach. We first describe the algorithm X  X  input. Then we lay out the data preparation and data structures built in order to support the disambiguation proce ss. Finally the iterative disambiguation process is described. The data for the resolution proce ss contains the document text, all possible entity matches from our KB where each entity has a known type (Person, Location etc. ) and its known relations to other entities in the KB. In addition, the inferred article categories are given to the resolver as well (Sports, Politics, etc.). A priori global data such as term frequenc ies, lists of given names and surnames are available as well. As a first stage we build a document graph that represents all the possible resolution permutations. This document graph is the corresponding subgraph derived from the knowledge graph incorporating all the possible entities for the given document augmented with additional nodes and edges representing inferred data as described below. At first, all possible entities from the document are added as entity nodes. Categories inferred from the document are added as category nodes. Predefined textual tokens (e.g.  X  X V X ,  X  X ctor X ) appearing in the document are a dded as text nodes if exist. Heuristics nodes such as high fr equency tokens, common names and stop words are added as well. Finally we add nodes for entities that have no match in th e document itself, but are highly connected to several entities in the document as context nodes (e.g.  X  X he Beatles X  when John Lennon, Paul McCartney &amp; Ringo Starr are present). 
DOI: http://dx.doi.org/10.1145/2911451.2926721 Positively weighted edges are drawn between entity nodes according to their relations in the KB. Additional edges are drawn between entity nodes and other node types (categories, heuristic, text) according to predefined weights or correlation with entity types (e.g. the word  X  X uthor X  will have positive weight to the Harry Potter book entity and negative weight to the Harry Potter movie entity). Ad-hoc positive edges are drawn between entities in the article that share several context nodes when the nodes are not connected in the KB. These ad-hoc edges can be thought of as indicating the paired entities as more probable in term of document context. Once the document graph is constr ucted, the entity resolution process commences. As a first stage, graph nodes are weighted according to the sum of their edges weights. Negatively weighted nodes are filtered out at this stage as there are more negative indicators regarding their correctness than positive ones. Nodes with zero weight, usually those nodes without edges in the graph, are put aside at this stage as their polarity is unclear. These will be analyzed at the end of the disambiguation process when the document context is clearer. Solely remaining with positively weighted entity nodes to resolve, two groups of nodes are formed. Th e first  X  X olved X  group consists of all text and category nodes as well as entity nodes without ambiguity (positively weighted entity with no other entity text matches). This group X  X  nodes are considered positively resolved. The second  X  X nsolved X  group consis ts of all entity nodes with some ambiguity. Strongly Connect ed Components (SCCs) [3] are now constructed to identify context regions in each one of the groups. As SCC has paths from each node to another, it represents a common domain. In cases where there is a KB edge between entities with the same text match, ambiguities exist among the SCC X  X  nodes. In order for the process to properly converge, inner SCCs ambiguities must be resolved first. On each SCC with inner ambiguity the entities with the max scores S(n) as depicted in equati on 1 are kept, and their ambiguous nodes (nodes competing on the same text matches) are filtered out. Entities scores are based on their edges to each of the solved group SCCs and their weights  X   X  . Here | X  X  X | represents the connected SCC size, |  X  X  X  X  | is the number of unique solved SCCs connected to the node and |  X  X  X  X  | is the number of all solved SCCs. Correlations of types and categories are used as a tie breaker. After this stage, all ambiguities exist between separate SCCs in the  X  X nsolved X  group. Our goal is to iteratively move entities from the unsolved group to the solved one. Instead of trying to solve ambiguities on the entity level, at this stage we solve them on the SCC level (previous stage assures that this is possible as each SCC is devoid of internal ambiguities). On each iteration, nodes from the SCC with the max score S(scc) are joined to the solved group and their ambiguous nodes are filtered out. Since entity nodes where filtered out, weights of the remaining ones may become negative, and may be filtered out too. Note that after each iteration the node-g roups change and SCCs are recomputed. At this stage all the nodes in the solved group are considered as correct. But a resolution for zero weighted nodes should still take place. Having confidence in the resolved SCCs and their corresponding context regions, nodes correlating to these regions are considered correct, where the rest are filtered out. The correlation to the context regions is based on the entities types and categories. Any node with no signal what so ever is filtered out. During the entity resolution process nodes are filtered out. Usually empty entries are filtered out at the first and last stages as negative or zero weight nodes, due to l ack of corroborative information regarding their inclusion in the document context. In this work we demonstrated the document graph and how to merge a priori knowledge with dynamic data and signals residing on a given document. The construc tion of ad-hoc disambiguating edges between nodes in the document graph, yields denser document context regions. Th ese regions improve the disambiguation accuracy and the ability to handle empty entry cases, even on sparse document graphs. The process was empirically tested on real life scenarios on large scale dynamic news articles sets (not shown) . The process shows significant improvements both on our own entity extractor and on similar state of the art commercial and open source products. [1] Cucerzan, S., 2007, June. Large-Scale Named Entity [2] Dredze, M., McNamee, P., Rao, D., Gerber, A. and Finin, T., [3] Tarjan, R., 1972. Depth-first search and linear graph [4] Zheng, Z., Si, X., Li, F., Ch ang, E.Y. and Zhu, X., 2012, 
