 A distance-mapping algorithm takes a set of objects and a A distanc+mapping algorithm is one that takes a among objects are approximately preserved. In [FL95], Faloutsos and Lin proposed to use such an algorithm as applications. The approach is to first map the objects to points in a k-dimensional target space. Then perform space. The algorithm proposed is called FastMap. Empirical studies indicated that FastMap works well for Euclidean distances. 
In this paper we present a new distance-mapping al-gorithm, called MetricMap, and three hybrid heuris-tics that combine FastMap and MetricMap in dif-ferent ways. We conduct experiments to compare the performance of the five algorithms based on both Eu-clidean distance and general distance metrics in data clustering applications. A general distance metric is a function 6 that takes pairs of objects into real num-bers, satisfying the following properties: for any ob-negative definiteness); 6(z, g) = 6(y, 5) (symmetry); 6(z, y) 5 6(z,z) + 6(z, y) (triangle inequality). Eu-clidean distance satisfies these properties. On the other hand, many general distance metrics of interest are not Euclidean, e.g. string edit distance as used in biology. None of the five algorithms (nor any other algorithm that we know of) give guaranteed performance for gen-eral distance metrics. For this reason, an experimental analysis is worthwhile. Consider a set of objects 2) = {Oc,Oi, . . . , ON-~} and a distance function d where for any two objects Oi, Oj E D, d(Od,Oj) (or di,j for short) represents the distance between Oi and Oj. The function d can be Euclidean or a general distance metric. The algorithms studied here take the set of objects, some inter-object distances and map the objects to a k-d space R X  (k is user-defined), such that the distances among the objects are approximately preserved. The k-d point Pi corresponding to the object Oi is called the image of Oi. The k-d space containing the images is called target spuce. The differences among the algorithms lie in the way they use for mapping and the target space they choose. For example, FastMap embeds the objects in a Euclidean space, whereas MetricMap embeds them in a pseudoEuclidean space [Gre75]. We describe the algorithms in some detail below; related proofs can be found in FL95, YZW98] and in the full paper available from the authors. 2.1 The FastMap Algorithm 
The basic idea of this algorithm is to project objects on a line (0,, Ob) in an n-dimensional (n-d) space Rn for some unknown n, n 2 k. The line is formed by two pivot objects 0,, Oa, chosen as follows. First arbitrarily choose one object and let it be the second pivot object Oa. Let 0, be the object that is farthest apart from 
Ob. Then update Ob to be the object that is farthest apart from 0,. The two resulting objects O,, Ob are pivots. Now consider an object 0; and the triangle formed by Oi, 0, and Ob (Fig. 1). From the cosine law, one can get d&amp; = d2,,+ + d X , ,, -2Xido,b. first coordinate xi of object Oi with respect to the line embed objects in the target space R X  as follows. 
Pretending that the given objects are indeed points in Rn, we consider an (n -1)-d hyper-plane 3t that is perpendicular to the line (Oa,Ob), where 0, and 
Ob are two pivot objects. We then project all the objects onto this hyper-plane. Let Oi, Oj be two objects and let Oi, 0; be their projections on the hyper-plane 3t. It can be shown that the dissimilarity d X  between 
Oi, 0; is (d X (Oi,0i))2 = (d(Oi,Oj)) X  -(xi -xj) X , i,j =0 ,***&gt; N -1. Being able to compute d X  allows one to project on a second line, lying on the hyper-plane 31, and therefore orthogonal to the first line (0,, Ob). We repeat the steps recursively, k times, thus mapping all objects to points in RL. are indeed points in Rn. If the assumption doesn X  X  hold, (d(Oi, Oj))2 -(xi -zj)2 may become negative. 
For this case: the dissimilarity d X  is modified as follows: d X (Oj, 0;) = -J(Xi -Xj)2 -(d(Oi, Oj)) X . 
Let Oi, Oj be two objects in 23 and let Pi = (xi,... ,XF), Pj = (Xi,..., x5) be their images in the target space R X . The dissimilarity between Pi and Pj, denoted df (Pi, Pj), is calculated as dj (Pi, Pj) = points in R X , n &gt; k, and the distance function d is Euclidean, then FastMap guarantees a lower bound on inter-object distances. That is, df (Pi, Pj) 5 d(OiyOj). Let COStfastmap denote the total number of distance calculations required by FastMap. Then costfos~mop = 3Nk where N is the size of the dataset and k is the dimensionality of the target space. 2.2 The MetricMap Algorithm 
The algorithm works by first choosing a small sample A by randomly picking 2k objects from the dataset. The algorithm calculates the pairwise distances among the sampling objects and uses these distances to establish the target space Rk. The algorithm then maps all objects in the dataset to points in Rk. = (00,. . .) 02&amp;i}. We define a mapping cr as follows: (Y : A + R2k- X  such that a(Oe) = as = (0,. . . ,O), 2(a)). Intuitively we map 00 to the origin and map the other sampling objects to vectors (points) {aj}l&lt;j&lt; X  X k-1 --in R2 X -l so that each of the objects corresponds to a base vector in R2 X -l. kg0 + 4,o -~,j)/2, 1 5 i, j 5 2k -1. Define the function $ as follows: II, : R2k-1 x R2k-1 -+ R such that +(s, y) = z?M($&lt;,,)y where xT is the transpose of vector X. Notice that $(ai, oj) = mi,j, 1 5 i, j 5 2k-1. 
The function + is called a symmetric bilinear form of R2k-1 [Gre75]. M($J&lt;~&gt;) is the matrix of y!~ with 
R2 X - X  equipped with the symmetric bilinear form @ is called a pseudo-Euclidean space. For any two points (vectors) x, y E R2k-1, $(x, y) is called the inner product of x and y. The squared distance between x and y, denoted ]]x -y]12, is defined as ]]x -y]] X  = $(x-y, z-9). This squared distance is used to measure the dissimilarity of two points in the pseudo-Euclidean space. Since the matrix M($,,,) is real symmetric, a diagonal matrix D = diag(&amp;)l&lt;i&lt;2k-1 such that QTM(tiCa&gt;)Q = D, where QT is the transpose of Q, 
Ais are eigenvalues of M($J &lt;((,) arranged in some order, and columns of Q are the corresponding eigenvectors [GV96]. Note that if the matrix M($,,&gt;) has negative eigenvalues, the squared distance between two points in the pseudoEuclidean space may be negative. That X  X  why we never say the  X  X istance X  between points in a pseudo-Euclidean space. where (ei, . . . ,ezk--1) = (al,. . . ,ask--1)Q or equiva-lently (al,. . . , ask-i) = (ei, . . . ,@&amp;I)&amp;~. Each oi, 1 &lt; i 5 2k -1, can be represented as a vector in the space spanned by {ej}l&lt;j&lt;zk-1 and the coordinate 
Fig. 2. Illustration of the MetricMap algorithm (k = 2). of aj with respect to {ei}il;ssk-i is the jth row (see Fig. 2(b)). Each ei corresponds to an eigenvector. 
Suppose the eigenvalues are sorted in descending order by their absolute values, followed by the zero eigenvalues. The MetricMap algorithm reduces the dimensionality of RSk-l to obtain the subspace Rk by removing the k -1 dimensions along which the eigenvalues X;s of M($J,,,,) are zero or their absolute values are smallest (see Fig. 2(c)). Notice that among the remaining k-dimensions, some may have negative eigenvalues. The algorithm then chooses k + 1 objects, called the reference objects, that span R X . Once the target space R X  is established, the algorithm maps each object 0, in the dataset to a point (vector) P. in the target space by comparing the object with the reference objects. The coordinate of P, is calculated through matrix multiplication. Here is how. *,j = (d$ + $,o -e,j)/2, 1 &lt; j 5 k. Define where Let Jikl be the kth leading principal submatrix of the be the kth leading principal submatrix of the matrix leading principal submatrix of the orthogonal matrix R X , denoted Coor(P,), can be approximated as follows: COOT(P,) % J~k~C~~ X 2Q&amp;b. Let Oi7 Oj be two objects in 2) and let Pi = (of,. . . ,s$), Pj = (3,. . . ,$) be their images in R X . Let A(Pi, Pj) = CF=, sign(Xl)(di -X$)2. The dissimilarity between Pi and Pj, denoted &amp;(Pi, Pj), is approximated by Note that if the objects are points in R X , n 2 k, and the distance function d is Euclidean, then as in FastMap, MetricMap guarantees a lower bound on inter-object distances. That is, d,(Pi, Pj) 5 d(Oi, Oj). tance calculations required by MetricMap. It can be shown that Costmetricmclp = 4k2 + (IV -2k)(k + 1). Since N 2 k, CoStmetriemap I Co+,tmop. 2.3 Hybrid Algorithms Let Oi, Oj be two objects in 2) and let Pi, Pi be their images in Rk. The dissimilarity between Pi, Pj, embedded by FastMap, is denoted by df(Pi, Pj). The dissimilarity between Pi, Pj, embedded by Met&amp;Map, is denoted by &amp;(Pi, Pj). The three hybrid algorithms, called AvgMap, MinMap and MaxMap, respectively, use the dissimilarities d,, 4, dz, respectively, defined as follows: da(Pi, Pi) = [df(Pi, Pi) + dm(Pi, Pj)]/2, &amp;(Pi,Pj) = min{df(Pi,Pj),d,(Pi,Pj)), ds(Pi,Pj) = ma,x{df(Pi, Pj), &amp;(Pi, Pj)}. We collectively refer to all these algorithms as mappers. It should be pointed out that AvgMap, MinMap and MaxMap are no more expensive to compute than FastMap and Met&amp;Map; the algorithms have the same cost O(Nk). 
We conducted a series of experiments to evaluate the performance of the mappers in data clustering applications. The algorithms were implemented in C and C++ under the UNIX operating system run on a 
SPARC 20. Two datasets were used: RNA secondary structures and n-dimensional vectors. RNA distance was a general distance metric, so satisfied the triangle inequality, but was not Euclidean. the virus database in the National Cancer Institute. 
The RNA secondary structures were created by first choosing two phylogenetically related mRNA sequences, rhino 14 and ~0x5, from GenBank pertaining to the human rhinovirus and coxsackievirus. The 5 X  non-coding region of each sequence was folded and 100 secondary structures of that sequence were collected. 
The structures were then transformed into trees and their pairwise distances were calculated as described in [WSS99]. The trees had between 70 and 180 nodes. The distances for rhino 14 X  X  trees and ~0x5 X  X  trees were in the interval (1..75) and (1..60), respectively. The distances between rhino 14 X  X  trees and ~0x5 X  X  trees were in the interval (43..94). The secondary structures (trees) for each mRNA sequence roughly formed a cluster. in [ZRL96]. Specifically, we generated q groups of n-dimensional vectors from an n-dimensional hypercube. 
Each vector was generated by choosing n real numbers randomly and uniformly from the interval [LB..HB]. 
The pairwise Euclidean distances among the vectors were then calculated. Initially the groups (clusters) might overlap. We considered all the q groups as sitting on the same line and moved them apart along the line by adding a constant (i x c), 1 5 i 5 q, to the 6rst coordinate of all the vectors in the ith group. c was a tunable parameter. We used CURE [GRS98] to adjust the clusters so that they were not too far apart. 
Specifically, c was chosen to be the minimum value, by which CURE can just separate the q clusters. In our case, c = 1.15. Once the first q clusters were generated, we moved to the second line, which was parallel to the first line, and generated another q clusters along the second line. This step w&amp; repeated until all the q lines were generated, each line comprising q clusters. Again we used CURE to adjust the distance between the lines so that they were not too far apart. The parameters and their base values used in the experiments were: k (dimensionality of the target space) = 10, p (number of clusters) = 4, n (dimensionality of synthetic vectors) = 20, C (number of vectors in a cluster) = 100, 
LB (smallest possible value for each coordinate of the synthetic vectors) = 0, HB (largest possible value for each coordinate of the synthetic vectors) = 100. 3.1 Results 
We first evaluated the precision of embedding for the five mappers. It was found that MaxMap achieves the most accurate embedding for the Euclidean data. This is understandable-since both FastMap and MetricMap underestimate inter-object distances, 
MaxMap gives the lowest error in embedding. The per-formance of MaxMap depends on the dimensionality of vectors n (the performance degrades as n increases), but is independent of the dataset size and distance ranges. 
On the other hand, AvgMap achieves the most accurate embedding for the RNA data. The performance of both 
MaxMap and AvgMap improves as the dimensionality of the target space, k, increases. the presence of imprecise embedding. The purpose is twofold. First, this study shows the feasibility of clustering without performing expensive distance calculations. Second, through the study, one can understand how imprecision in the embedding may affect the accuracy of clustering. The clustering algorithm used in our experiments was the well known average-group method pKR90], which works as follows. 
Initially, every object is a cluster. The algorithm merges two nearest clusters to form a new cluster, until there are only K clusters left where K is p for the Euclidean clusters and 2 for the RNA data. The distance between two clusters Cr and CZ is given as h CO,~~~,CJ,~C~ I4% %&gt;I where Icily i = 17% is the size of cluster Ci. The algorithm requires O(N2) distance calculations where N is the total number of objects in the dataset. In principle, one can use any reasonably good distance-based clustering algorithm since our experiments deal with both Euclidean distance and general distance metrics. cluster C created by the average-group method, but its image is not in C X  X  corresponding cluster, which is also created by the average-group method, in the target space. The performance measure we used was the mis-clustering rate (Err,), defined as Errc = (NC/N) x 100% where N, was the number of mis-clustered objects. of the target space, k, for the Euclidean clusters and Fig. 4 shows Err, as a function of k for the RNA data. 
For the Euclidean data, the average-group method successfully found the 4 clusters in the dataset. For the 
RNA data, the average-group method missed 5 objects in the dataset (i.e., the 5 RNA secondary structures were not detected to belong to their corresponding sequence X  X  cluster). The images of these 5 objects were also missed in the target space; they were excluded when calculating Errc. mance improves as the dimensionality of the target space increases, since the embedding becomes more pre-cise. Fig. 3 shows that the ErrCs of all the mappers approach 0 when k = 9. Fig. 4 shows that Met&amp;Map outperforms FastMap; its Err, approaches 0 when k = 80. Overall, MaxMap is best for the Euclidean data and AvgMap is best for the non-Euclidean RNA data. The results indicate that with the two best map-pers, one can perform clustering on the k-dimensional points. Embedding the data objects can be performed in the off-line stage, thus reducing the clustering time significantly. We next examined the scalability of the results using Euclidean clusters. With higher dimen-sional vectors (e.g. 60-dimensions) and more clusters, the average-group method missed several objects in the dataset. However, MaxMup consistently gives the low-est m&amp;-clustering rate. 
We also conducted experiments by replacing the random sampling objects used by MetricMap with the 2k pivot objects found by FastMap. The performance of MetricMap improves for the Euclidean data, but degrades .for the non-Euclidean data. MaxMup and AvgMap remain the best as in the random sampling case. Our approach is related to two groups of work: multi-dimensional scaling (MS) [GCS89] and the linear pro-jection via Principal Components Analysis [F X  X kSO]. In contrast to these methods, which consider Euclidean space, our approach considers both Euclidean and pseudo-Euclidean spaces. Furthermore, our approach requires O(Nk) time. This is faster than the MS meth-ods, which, in the worst case, need O(iV2) time. Cur-rently we are applying the proposed techniques to sim-ilarity retrieval in metric spaces. We thank the SIGKDD reviewers for their constructive criticism and helpful comments. FL951 C. Faloutsos and K.-I. Lin. FastMap: A fast @?uk90] K. Fukunaga. Introduction to Statistical Pat-[GV96] G. H. Golub and C. F. Van Loan. Matrix Com-[GCSSS] P. E. Green, F. J. Carmone, Jr., and S. M. [Gre75] W. Greub. Linear Algebra. Springer-Verlag, [GRS98] S. Guha, R. Rastogi, and K. Shim. CURE: An [KR90] L. Kaufman and P. J. Rousseeuw. Finding [wSS99] J. T. L. Wang, B. A. Shapiro, and D. Shasha, pZW98] Y. Yang, K. Zhang, X. Wang, J. T. L. Wang, [ZRL96] T. Zhang, R. Ramakrishnan, and M. Livny. 
