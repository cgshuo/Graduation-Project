 Web is unanimously the biggest source of structured, semi-structured and unstruc-tured data and automatic acquirement of useful information from the web is interest-advancement of information retrieval technologies from traditional document-level to object-level [14], expert search problem has gained a lot of attention in the web-based research communities. The motivation is to find a person with topic relevant expertise collaborators for the project, choose mavens for consultation about research topics, reviewers matching for research papers, and to invite program committee members and distinguished speakers for the conferences. 
TREC has provided a common platform fo r researchers to empirically assess ap-proaches for maven search. Several approaches have been proposed to handle this problem. In particular, Cao et al. [6] proposed a two-stage language model which combines a co-occurrence model to retrieve documents related to a given query, and a relevance model for maven search in those documents. Balog et al. [3] proposed a model which models candidates using its support documents by using language mod-eling framework. He also proposed several advanced models for maven search specif-ic to sparse data environments [4]. 
Based on the methods employed in previous language models, they can be classi-fied into two categories: composite and hybrid. In composite approach, D m = { d j } de-bined. While hybrid approach is very much similar to the composite model, except that it describes each term t i by using a combination of support documents models and then used a language model to integrate them. Composite approaches suffers from the support documents. 
Generally speaking, language models are lexical-level and ignores semantics-based information present in the documents. Latent semantic structure can be used to cap-ture semantics-based text information for improved knowledge discovery. The idea of using latent semantic structure traces back to Latent Semantic Analysis (LSA) [8], which can map the data using a Singular Value Decomposition (SVD) from high di-semantic space. 
LSA lacks strong statistical foundation; consequently Hofmann [11] proposed probabilistic latent semantic analysis (PLSA). It was based on likelihood principal and defines a proper generative model for the data. By using PLSA, Zhang et al. [20] pro-posed a mixture model for maven search. They used latent topics between query and support documents to model the mavens by considering all documents of one author as one virtual document. So far all approaches only used authors and supported doc-uments information. While, Tang et al. argued the importance of venues by saying that the documents, authors and venues all are dependent on each other and should be modeled together to obtain the combined influence [17]. Based on it they proposed a unified topic modeling approach by extending Latent Dirichlet Allocation (LDA) [5]. However to the limitation of their approach, they considered the venues information just as a stamp and did not utilize semantics-based text and author X  X  correlations present between the venues. 
Previous approaches ignored venues intern al semantic structure, author X  X  correla-likely to be mavens than authors of not renowned venues. Additionally, renowned venues are more dedicated to specific research areas than not renowned venues, e.g. in famous conferences submission are carefully judged for relevance to the conference areas, while in not renowned venues it is usually ignored by saying that the topics are not limited to above mentioned research areas on the call for papers page. Some people may think that one has to use impact factors of venues to influence the ranking knowledge. Secondly, continuous-time effects can be very handy in a case if author data networks until 2004 and published a lot of papers about this topic; afterwards he switched to academics social network mining and not published many papers. He still while it is not an appropriate choice now. The reason for this is occurrence of biologi-cal word many times in 2004 and preceding years. However, by attaching time stamp one can minimize the high rate of occurrence effect of one word (e.g. biological) for all years. 
In this paper, we investigate the problem of maven search by modeling venues in-ternal semantic structure, author X  X  correla tions and time all together. We generalized previous topic modeling approach [17] from a single document to all publications of the venues and added continuous-time considerations, which can provide ranking of mavens in different groups on the basis of semantics. We empirically showed that our approach can clearly produce better results than baseline approach due to topics us is well justified and produced quite promising and functional results. 
The novelty of work described in this paper lies in the; formalization of the maven search problem, generalization of previous topic modeling approach from document level to venue level (STMS approach) with embedded time effects, and experimental verification of the effectiveness of our approach on the real world corpus. To the best of our knowledge, we are the first to deal with the maven search problem by propos-ing a generalized topic modeling approach, which can capture word-author, author-author and author-venue correlations with non-discretized time effects. 
The rest of the paper is organized as follows. In Section 2, we formalize maven search problem. Section 3 provides maven search modeling related models and illu-strates our proposed approach for modeling mavens with its parameter estimation details. In Section 4, corpus, experimental setup, performance measures with empiri-cal studies and discussions about the results are given. Section 5 brings this paper to the conclusions. Maven search addresses the task of finding the right person related to a specific know-ledge domain. It is becoming one of the biggest challenges for information manage-ment in research communities [10]. The question can be like  X  X ho are the mavens on topic Z ? X  A submitted query by user is denoted by q and a maven is denoted by m . In general semantics-based maven finding process, main task is to probabilistically rank discovered mavens for a given query, i.e. p (m/q) where a query is usually comprised of several words or terms. 
Our work is focused on finding mavens by using a generalized topic modeling ap-proach. Each conference accepts many papers every year written by different authors. To our interest, each publication contains some title words and names, which usually spectively. Conferences (or journals) with their accepted papers on the basis of latent based correlations between the authors publishing papers in the specific venues by considering time effects is an appropriat e way to find mavens. Our thinking is potential mavens of different fields are accepte d so venues internal topic-based author related). venue for a specific year y , an author r on the basis of his accepted paper (s), and for-mulize maven search problem as: Given a venue c with N c words having a stamp of main. Formally for finding specific area mavens, we need to calculate the probability P(z|m) and P(w|z) where z is a latent topic, m is maven and w is the words of a venue. In this section, before describing our STMS approach, we will first describe how ma-vens can be modeled by using Language Model (LM) [19], Probabilistic Latent Se-mantic Analysis (PLSA) [11], and Author-Conference-Topic (ACT) Model [17] and why our approach is indispensable. 3.1 Related Models basic idea is to relate a document given to a query by using the probability of generat-ing a query from given document. In eq. 1, w is a query word token, d is a document and P ( q|d ) is the probability of the document model generating a query. P ( w|d ) is the probability of the document model generating a word by using a bag of words assumption. 
In eq. 1 one can simply merge all support documents of one author and treat it as a virtual document r representing that author [3,15]. Mavens discovered and ranked with respect to a specific query can be retrieved by using the following equation. Document and object retrieval with the help of LM has gained a lot of success. But LM faces the inability to exactly match query with the support documents. Hofmann proposed PLSA [11] with a latent layer between query and documents to overcome LM inability by semantically retrieving documents related to a query. The core of PLSA is a statistical model which is called aspect model [12]. The aspect model is a latent variable model for co-occurrence da ta which associates an unobserved class x w is defined by the mixture, where, each pair (d,w) is assumed to be generated inde-pendently, corresponding to bag of words assumption words w are generated indepen-dently for the specific document d conditioned on topic z . In eq. 3 by changing word w with query q and document d with author r, where r can be seen as a composition of all documents of one author as one virtual document [20]. probability of maven and query which can be used to discover and rank mavens. 
Recently, Author-Conference-Topic (ACT) model was proposed for expertise search [17]. In ACT model, each author is represented by the probability distribution over topics and each topic is represented as a probability distribution over words and venues for each word of a document for that topic. Here venue is viewed as a stamp associated with each word with same value. Therefore, the modeling is just based on semantics-based text information and co-authorship of documents, while semantics-based intrinsic structure of words and authors X  correlations with embedding time ef-which became the reason of topic sparseness that resulted in poor retrieval perfor-mance. The generative probability of the word w with venue c for author r of a docu-ment d is given as: 3.2 Semantics and Temporal Information Based Maven Search (STMS) We think it is necessary to model venues internal semantic structure and author corre-lations than only considering venues as a stamp [17] and time factor for maven search. The basic idea presented in Author-Topic model [16], that words and authors of the documents can be modeled by considering latent topics became the intuition of modeling words, authors, venues and years, simultaneously. We generalized the idea presented in [17] from documents level (DL) to venues level (VL) by considering research papers as sub-entities of the venues to model the influence of renowned and not renowned venues on the basis of participation in same venues. Additionally, we considered continuous-time factor to deal with the topic drift in different years. In the proposed approach, we viewed a venue as a composition of documents words and the authors of its accepted publications with y ear as a stamp. Symbolically, for a venue c ( d thor vector of d i and y c is paper publishing year. 
DL approach considers that an author is responsible for generating some latent top-ics of the documents on the basis of semantics-based text information and co-authorship. While, VL approach considers that an author is responsible for generating some latent topics of the venues on the basis of semantics-based text information and authors correlations with time is not-discretized (please see fig. 1). In STMS ap-nomial distribution  X  r over topics and each topic is associated with a multinomial dis-with year y for author r of venue c is given as: 
The generative process of STMS is as follows: 1. For each author r = 1 ,..., K of venue c 2. For each topic z = 1 ,..., T 3. For each word w = 1 ,..., N c of venue c given by: word in the lexicon, y i = n represents i th year of paper publishing attached with the n th the number of years and R is the number of authors.  X . X  Indicates summing over the assigned to topic z respectively, excluding the current instance. 
During parameter estimation, the algorithm needs to keep track of W x Z (word by distribution  X  can be calculated as:
Where,  X   X  X  is the probability of word w in topic z ,  X   X  X  is the probability of year y the predictive distributions over new words w, new years X  y and new topics z condi-respect to their probabilities as: Where, w is words contained in a query q and m denotes a maven. 4.1 Corpus We downloaded five years publication Corpus of venues from DBLP [7], by only we extracted 112,317 authors, 62,563 publications, and combined them into a virtual document separately for 261 conferences each year. We then processed corpus by a) removing stop-words, punctuations and numbers b) down-casing the obtained words of publications, and c) removing words and authors that appear less than three times in the corpus. This led to a vocabulary size of V=10,872, a total of 572,592 words and 26,078 authors in the corpus. 4.2 Parameter Setting In our experiments, for 150 topics Z the hyper-parameters  X  ,  X  and  X  were set at 50/ Z , .01, and 0.1. Topics are set at 150 at a minimized perplexity [5], a standard measure for estimating the performance of probabilistic models with the lower the best, for the estimated topic models. Teh et al. proposed a solution for automatic selection of num-ber of topics, which can also be used for topic optimization [18]. 4.3 Performance Measures Perplexity is usually used to measure the performance of latent-topic based approach-average entropy to measure the quality of discovered topics, which reveals the purity better. Secondly, we used average Symmetric KL (sKL) divergence [17,20] to meas-ure the quality of topics, in terms of inter-topic distance. sKL divergence is used here to measure the relationship between two topics, more inter-topic sKL divergence (distance) is better. 4.4 Baseline Approach We compared proposed STMS approach with ACT approach and used same number of topics for comparability. The number of Gibbs sampler iterations used for ACT is 1000 and parameter values same as the values used in [17]. 4.5 Results and Discussions 4.5.1 Topically Related Mavens We extracted and probabilistically ranked mavens related to a specific area of re-discovered from the 100 th iteration of the particular Gibbs sampler run. 
The words associated with each topic are quite intuitive and precise in the sense of conveying a semantic summary of a specific area of research. For example, topic # 27  X  X ML Databases X  shows quite specific and precise words when a person is searching for databases experts with move from simple databases to XML databases. Other top-typically mavens of that area of research. For example, in case of topic 35  X  X ayesian Learning X  and topic 119  X  X ata Mining X  top ranked mavens are well-known in their respective fields. 
Proposed approach discovered several other topics related to data mining such as neural networks, multi-agent systems and pattern recognition, also other topics that span the full range of areas encompassed in the corpus. 
In addition, by doing analysis of mavens home pages and DBLP [7], we found that 1) all highly ranked mavens have evenly published papers on their relevant topics for all years, no matter they are old or new researchers and 2) all of their papers are usual-ly published in the well-known venues. Both findings provide qualitative supporting evidence for the effectiveness of the proposed approach. 
Fig. 3 provides a quantitative comparison between STMS and ACT models. Fig. 3 using eq. 12. Lower entropy for different number of topics T= 100, 150, 200 proves the effectiveness of proposed approach for obtaining dense (less sparse, clearer) top-ics. Fig. 3 (b) shows the average distance of topic-word distribution between all pairs of the topics calculated by using eq. 13. Higher sKL divergence for different number of topics T= 100, 150, 200 confirms the effectiveness of proposed approach for obtaining dense topics. 
One would like to quickly acquire the topics and mavens for new venues that are not contained in the training corpus. Provided parameter estimation Gibbs sampling algorithm requires significant processing time for large number of dataset. It is com-putationally inefficient to rerun the Gibbs sampling algorithm for every new venue added to the corpus. For this purpose, we can apply eq. 7 only on the word tokens and authors of the new venue each time temporarily updating the count matrices of (word search query related mavens. 4.5.2 Effect of Topic Sparseness on Retrieval Performance Topic by author matrix (influenced by venues and time information) can also be used for automatic correlation discovery between authors more appropriately, than pre-viously used topic by author matrix (not influenced by venues and time information) [16]. To illustrate how it can be used in this respect, distance between authors i and j is calculated by using eq. 13 for topic-author distribution. 
We calculated the dissimilarity between the authors; smaller dissimilarity value means higher correlation between the authors. Tab. 2 shows top 7 semantics-based mavens ids discovered related to the first maven of each topic for STMS and ACT approaches. For example, in case of  X  X ML Databases X  topic 4808, 337, 5194, 4457, 4870, 4775, 640 are top 7 mavens correlated with  X  X ivesh Srivastava X  for STMS ap-proach in terms of sKL divergence and so on. 
The highlighted blocks in tab. 2 shows that similar results are found for discovered topics and sKL divergence. For example, in case of STMS approach top eight mavens shown in tab. 1 for  X  X ML Databases X  topic has three mavens in common, which are 337  X  X harma Chakravarthy X , 4457  X  X ok Wang Ling X , and 4775  X  X urajit Chaudhuri X . From top 7 related mavens for five selected topics (same is the case with non selected topics) shown in the tab. 1 the error rate (ER) for STMS is less than ACT and STMS has 17.14 % less average error rate than ACT. It shows the bad effect of topics sparseness on maven retrieval performance for ACT, and inability of ACT to discover better results in comparison with STMS. This study deals with the problem of maven search through latent topics. Initially we generalized this problem to VL with embedding continuous time effects and discussed the motivation for it. We then introduced STMS approach, which can discover and probabilistically rank experts related to specific knowledge domains (or queries) by modeling semantics-based correlations and temporal information simultaneously. We demonstrated how it can be used to rank experts for unseen data and to find mavens correlations. We studied the effect of generalization on topics denseness when model-ing entities and concluded that more dense topics will results in better performance of the approach. Empirical results show better performance on the basis of compact top-how to use STMS approach for ranking mavens related to a topic for different years mavens are usually different and the expertness of an author can be dynamic over different time span. Acknowledgements. The work is supported by the National Natural Science Founda-tion of China under Grant (90604025, 60703059) and Chinese National Key Foundation Research and Development Plan under Grant (2007CB310803). We are thankful to Jie Tang and Jing Zhang for sharing their codes, valuable discussions and suggestions.
