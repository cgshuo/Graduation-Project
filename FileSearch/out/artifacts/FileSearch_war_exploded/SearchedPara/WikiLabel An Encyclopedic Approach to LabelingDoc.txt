 This paper presents a particular approach to collective labeling of multiple documents, which works by associating the documents with Wikipedia pages and labeling them with headings the pages carry. The approach has an obvious advantage over past approaches in that it is able to produce fluent labels, as they are hand-written by human editors. We carried out some experiments on the TDT5 dataset, which found that the approach works rather robustly for an arbitrary set of documents in the news domain. Comparisons were made with some baselines, including the state of the art, with results strongly in favor of our approach.

Categories and Subject Descriptors : H.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval // General Terms : Algorithms // Keywords :TDT-5, Cluster Labeling, Topic Detection
One curious aspect of the TDT (Topic Detection and Tracking) program organized by NIST [1], which ran from late 1990s to early 2000s is that it had completely shied away from addressing how to name topics or how one would give a verbal expression to what the topic is given a set of related stories, though its name suggests otherwise. One would suspect that the irregularity may have risen from the general difficulty at the time of labeling a set of documents in any systematic way. However, as pointed out in the past literature [6, 7, 2], in order for humans to make sense of document clusters, which usually amount to no more than a bag of words, it is critically important to label clusters with linguistically coherent expressions that would give a general idea of what the content is about. Take for instance, two cluster labels A and B .
 While it would be some challenge to make sense of A unless you are familiar with the European financial situation, B is more straight-forward, quite sensible even if you are not a financial expert. The goal of the paper is to develop a systematic way of creating a read-able label such as B , for a document cluster, through the use of p 1 denotes a subpage consisting of paragraphs appearing in the main section, i.e., a part that appears immediately after the main title. p 2 denotes a subpage consisting of the main section and a part of the page that immediately follows the section heading of B, i.e., a text fragment directly dominated by B. And similarly for p =  X  A C D  X  , p 5 =  X  A C E  X  and p 6 =  X  A C D F  X  .

More precisely, we define the content D of a subpage p i as com-bined contents of sectional nodes found on its corresponding path  X  p  X  , and its title T as combined titles associated with sectional nodes on  X  p  X  where we set and where D ( n ) denotes child nodes immediately dominated by n , and T ext ( n ) a span of text that immediately follows n  X  X  section head-ing. H ( x ) indicates a title (heading) associated with a section node x . The deconstruction of a Wiki page in Figure 1 gives rise to mini Wiki pages, each corresponding to one of the paths p 1 , ..., p 6 , and each consisting of two elements T ( p i ) and D ( p i ) . For the current work, we assume a section node to be that marked by the equal signs, namely, tags that mark section headings in Wikipedia. 2 In what follows, we refer to a deconstructed version of Wikipedia as a  X  X eWiki. X 
Obviously, there are as many DeWikis as there are ways to de-construct Wikipedia: one may create a DeWiki solely made up of sectional nodes one edge away from the root, or one that includes nodes at the full depth. Wikipedia itself is considered a DeWiki which happens to keep the original structure intact (call it the first-order DeWiki). A DeWiki is said to be of second order if it consists of pages made up of the root and its immediate child nodes. We say that a DeWiki is of infinite order if its pages involve the deconstruc-tion to the fullest extent, where each page has its paths expanded to the fullest. We stipulate that the n -th order DeWiki include pages whose path terminates earlier than n , in addition to those of the n -th order. Suppose we have a Wikipedia consisting of a single page shown in Figure 1. Then W 1 = { p 1 } , W 2 = { p 2 , p 3 } , n -th order DeWiki.

Given a set of documents, we are interested in generating their collective label through the deconstruction of Wikipedia. The label generation happens in three steps: (1) select terms representative of a group of documents; and (2) query the DeWiki using the terms; and (3) take titles of pages the DeWiki considers best matches for the query as a label for the document cluster. The advantage of us-ing the DeWiki to generate labels lies in its strong ability to gener-ate fluent and readable labels, though not much of a surprise, given http://en.wikipedia.or g/wiki/Wikipedia:Manual_of_Style and space; Table 1 shows part of what is called a  X  X opic profile, X  an event description intended to help the annotator decide whether a story fits a given topic [4].

Our experiment made use of a particular answer key set called  X  X nk_SR =nwt_TE=mul, eng .key, X  provided as part of TDT-5. The set consists of gold standard  X  X inks, X  each of which consists of a pair of story identifiers and a decision made by humans on whether they are related to a particular topic. Although the file contains links within and across several different languages, we decided to restrict ourselves to English links, i.e., links involving only English stories, since we were interested in using the English Wikipedia as part of WikiLabel (which denotes our approach). We created doc-ument clusters out of TDT-supplied links by grouping documents by their associated topics, and discarded documents not linked to any topic. This had left us with 4,501 documents, which together comprised 100 topic clusters. The clusters varied in length from 3 to 244 documents, containing 45.01 documents on average (Ta-ble 2). Following a labeling practice common in journalism, 3 we take a label for a news event to be a name of person, entity or inci-dent closely associated with, or evocative of the event in question, in particular those that appear in title, who, what, and terminol-ogy sections of the TDT topic profile. Thus possible labels for the topic 55016 in Table 1 would include  X  Gay Bishop , X   X  Jeffrey John , X   X  Jeffrey John is appointed as Bishop of Reading, England , X   X  Reverend Cannon Jeffrey Phillip Hywel John , X  and  X  Archbishop of Canterbury Rowan Williams ,  X  and not  X 05/20/2003 X  or  X  X eading, England, X  as a date or a place name rarely becomes associated with a news event, except perhaps for  X 9/11. X  In what follows, we call topic labels found in the TDT topic profiles  X  X TLs. X 
In order to put things into a perspective, we make empirical com-parisons to some of the approaches either proposed or used as a baseline in the prior literature. In particular, we will look at TFIDF, T-score [2, 6] and an approach based on Kullback-Leibler (KL) divergence [6]. TFIDF represents an approach which consists of choosing k terms with highest scores in TFIDF from a cluster and presenting them in the order of their scores as a possible label for the cluster. On the other hand, T-score works by ranking bigrams found in a cluster, based on t-statistic and choosing as potential labels, bigrams whose internal words are found to be closely asso-ciated [3]. Mei et al. [6] took a step further when they introduced the KL divergence into the picture, providing the way to exploit the distributional similarity between a label and a cluster in cluster la-beling. After some tinkering with their model, they arrived at the following formula (which we simplified for the sake of readability). Score ( l,  X  i ) =  X E  X  i [ PMI ( w, l )]  X   X   X  represents a multinomial topic model, i.e., a probability distribu-tion of words in a cluster, l denotes a bigram label. PMI represents a pointwise mutual information. w stands for a word in a cluster. E j represents an expectation under  X  j .  X  and  X  are mixing pa-rameters. S denotes a a set of clusters and S/i an S with a cluster i removed. In words, what the formula says is that a most likely label for a cluster  X  i is one that has a strong tie with  X  i in terms of PMI, and least tie with the rest of the clusters in S . The ex-periments below will look at how WikiLabel compares against the three baselines, TFIDF, T-score and the KL divergence on TDT-5.
In its weekly report of trends in news coverage, PEJ (PEW Center for Excellence in Journalism) refers to stories covering events re-lated to the Gulf oil disaster in 2010, collectively as  X  X P oil spill, X  or  X  X il spill, X  those on the leak of classified war reports as  X  X ik-iLeaks, X  and those on incidents involving Sarah Palin simply as  X  X arah Palin X  (http://www.journalism.org).
 T ables 5 through 9 represent R scores averaged over 100 topics. Table 3 gives some details on how large DeWikis we used were. The infinite order DeWiki has grown about twice as large as the first order DeWiki. As a seed Wikipedia, we made use of an English Wikipedia dump created on 04/06, 2011.
 Table 5 shows performance of WikiLabel, run with the first-order DeWiki, along with the baselines. Note that labels generated by the baselines are made as long or as short as those generated by Wiki-Label, which may vary in length. The results clearly indicate that WikiLabel is by far the most successful among the competing mod-els. Table 6 shows short label performance with WikiLabel running with the second order DeWiki.  X  X hort label X  means that WikiLabel outputs labels composed entirely of a main page heading. Shown in Table 4 are some headings from a Wiki page on Channel Islands. The main heading is  X  X hannel Islands, X  followed by some minor headings. We refer to a full-length title (which includes all the rel-evant minor headings) as a long label. Of particular note in Table 6 is that WikiLabel produces a ROUGE score of 0.3757 at k = 1 for W 2 , which is much higher than the corresponding score in W 1 (i.e., seed Wiki) of 0.3103, demonstrating the effectiveness of de-construction. As we go further with the deconstruction, however, we gradually lose the lead over WikiLabel with W 1 , though we sustain the edge over W 1 at k = 1 with W  X  . We find that the use of a long label, which retains all headings in a Wiki page, results in performance far worse than short labels. This may be an indication that minor headings, being made up of attributional expressions such as  X  X istory, X   X  X eography, X  are rarely of use in predicting TTLs.
In order to see how well WikiLabel works for live data, we con-ducted a run of  X  X ore informal X  experiments where we applied WikiLabel to 5,759 news stories collected mostly from online me-dia outlets in the US, during July 26 -August 1, 2010. The sources included the New York Times, Yahoo! News, CNN, MSNBC, Fox, Washington Post, ABC, BBC, and Reuters. We randomly created 1,000 clusters, using Apache Mahout/Kmeans. 6 For each of the clusters we created, we ran WikiLabel, harnessed with the second order DeWiki ( W 2 ), and assigned it to a top scoring short label WikiLabel generated for the cluster (i.e., C| 1 ).

For the validity of labels assigned to clusters, we tuned to PEJ, a non-profit media watchdog based in the US, which manually cre-ates on a weekly basis, a quantitative measure of how much the US media covered on particular news topics, known as News Cover-age Index (NCI). 7 We examined how closely labels generated by WikiLabel correspond to news topics NCI identified.

The top panel in Figure 2 shows some of the labels assigned by WikiLabel, ordered in accordance to the size of the associated cluster. The y -axis represents the number of stories that fell un-der a given label. The bottom panel indicates the NCI for July 26 -August 1, 2010. 8 The y -axis represents the proportion of media coverage on a given topic. In its analysis of NCI for the week, PEJ explains that topic  X  X ikiLeaks X  is meant to represent coverage re-lated to the leak of massive US war reports in Afghanistan, while  X  X mmigration X  is about the debate on a new immigration law in the US, aka Arizona SB 1070. The third leading topic,  X  X il spill X  refers to the media coverage on the oil spill caused by an accident at BP X  X  Deepwater Horizon oil rig in the Gulf of Mexico. Tony Hayward was a president of BP at the time. As it turns out, some of the ma-jor topics detected by WikiLabels such as  X  Afghan War documents http://www .mahout.apache.org http://www.journalism.org/about_news_index/methodology
PEJ does not provide NCIs for topics that rank below top 5.
