 to o ff er to his customers. The i th product has price p purchases product i when faced with options M , the seller may solve (1) max simply predicting expected revenues, R ( M )= " marginal information. Again, we will see that this view is consistent with reality. parametric technique on a real-world data set.
 distinct approach to the problem in this paper. purchases argmin Choice Model: We take as our model of customer choice a distribution,  X  : S over all possible permutations (i.e. the set of all permutations S M . Our choice model is thus This model subsumes a vast body of extant parametric choice models. Revenues: We associate every product in N with a retail price p our choice model is thus given by R ( M )= " y concrete examples of data vectors y : For each i, r , y A will thus have A (  X  ) Comparison Data: This data represents the fraction of customers that prefer a given N ; i &amp; = j . For each i, j , y if and only if  X  ( i ) &lt;  X  ( j ) .
 A 2  X  { 0 , 1 } N  X  N ! has A 2 (  X  ) i =1 i ff  X  ( i )=1 .
 we attempt to solve: (2) this question, we attempt to solve : (3) with a high probability.
 support size, (  X  ( 0 . Let  X  1 ,  X  2 , . . . ,  X  for 1  X  i  X  K , and  X  (  X  )=0 for all  X  &amp; =  X  we index its elements by d . The two conditions we impose are as follows: Signature Condition : For every permutation  X  { 1 In other words, for each permutation  X  Linear Independence Condition : " K satisfied with probability 1 if [  X  1  X  2 . . .  X  stated in the following theorem: for recovery. The algorithm takes y and A and as the input and outputs  X  and A (  X  values y Then, the algorithm is as follows: Algorithm: Initialization :  X  0 =0 ,k (0) = 0 and A (  X  for d =1 to m end for Output K = k ( m ) and (  X  Now, we have the following theorem: independence X  conditions. Then, the above described algorithm recovers  X  . Theorem 2 is proved in the appendix. The algorithm we have described either succeeds the conditions with high probability.
 choice model  X  as follows: choose K permutations,  X  1 ,  X  2 , . . . ,  X  numbers so that they sum to 1 , and assign them to the permutations  X  other permutations  X  &amp; =  X  this happens with a vanishing probability.
 in Section 2. The proof may be found in the appendix.
 o and K = o (  X  N ) for the top set data. distribution consistent with the observed data. In preparation for taking the dual, let A S the assortment M . Since S columns of the matrix A . Armed with this notation, the dual of (3) is: (4) Our solution procedure will rely on an e ff ective representation of the sets A 4.1 A Canonical Representation of A We assume that every set S the d th such set by S the convex hull of the set A polytope contained in the m -dimensional unit cube, [0 , 1] m . In other words, (5)  X  A for appropriately defined A jd derstand a partition of S (4). For simplicity of notation, we assume that each of the polytopes  X  A form, i.e.  X  A always optimized at the vertices of a polytope, we know: We have thus reduced (4) to a  X  X obust X  LP. By strong duality we have: (6) max We have thus established the following useful equality: %  X  ,  X  : max in the description of our canonical representation: (7) of S readily available, and then consider the general case.
 Canonical Representation for Ranking Data: Recall the definition of ranking data from Section 2. Consider partitioning S S to the set of all vectors x jd in { 0 , 1 } N satisfying: (8) Our goal is, of course, to find a description for  X  A x e ffi cient algorithm to solve (3) via (7)! 4.2 Computing a Canonical Representation: Comparison Data illustrate a general procedure for computing a canonical representation. Consider S It is not di ffi cult to see that the corresponding set of columns A vectors in { 0 , 1 } ( N  X  1) N satisfying the following constraints: (9)  X  (  X  ( to simply x j Unlike the case of ranking data, however,  X  A o so that  X  A o type of data, it su ffi ced to stop after a single iteration. of data is quite realistic.
 model to the observable data and picks an optimal o ff er set by evaluating R ( M )= " used parametric model that associates with each product i in N a positive scalar w by convention. The model assumes P ( i |M )= w here? We consider an MNL model on N = 25 products. The model and prices were specified of reality. We conduct the following experiments: Quality of Revenue Predictions: For each type of observable data we compute our set of randomly chosen subsets M of the 25 potential DVD X  X  assuming ranking data and MNL model.
 Quality of Optimal Solutions to Revenue Maximization Problems: For each type consumers choose among alternatives using limited observable data and without making include: [8] DL Donoho. Compressed sensing. IEEE Transactions on Information Theory , 52(4): [13] P. Rusmevichientong, ZJ Shen, and D.B. Shmoys. Dynamic Assortment Optimization
