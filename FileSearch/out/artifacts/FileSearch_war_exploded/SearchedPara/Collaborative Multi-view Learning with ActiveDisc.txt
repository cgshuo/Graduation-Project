 With the explosive growth and variety of information available on the Web, the interest in recommender systems has dramatically increased from both research and industrial communities. In this filed, Collaborative Filtering (CF) approaches, especially matrix factorization models, have achieved significant suc-cess [ 11 ], based on users X  previous interest encoded by the rating matrix reflecting the similarities of similar users or items. However, CF performs poorly when lit-tle collaborative information is available. This is referred to as the data sparsity problem [ 10 , 17 ], which is a common problem in many newly launched recom-mender systems.
 mender systems. The first one lies in traditional single task setting, i.e., how to effectively use existing user-item 1 pairs combined with auxiliary information, such as content [ 1 , 14 ] or complex network structure [ 11 ] given in advance. The second one lies in recently developed transfer learning setting [ 9 ], i.e., how to exploit related task-beneficial auxiliary information, learned from source rec-ommender containing dense interactive data, to strengthen target recommender performance. Specifically, 1) the first direction is extensively studied using side information. Although side information is beneficial for improving recommender performance, it is usually restricted by the availability of predictive data repre-sentations or restricted by relying on feature engineering. Thus, this motivates us to seek a general method that can automatically exploit side information for existing CF approaches. 2) The second direction becomes a recently hot research topic [ 17 ], but it needs more resources, i.e., cross-system information. For exam-ple, we need both Twitter and Facebook resources to improve the target system of Facebook, which is not always easy to acquire due to individual privacy or commercial issues. In addition, it usually makes strong assumptions for design-ing learning algorithms. For example, cross-system entity correspondence [ 10 ]is usually a crucial prerequisite. Thus, this motivates us to seek a novel cost-saving way to achieve knowledge transfer, i.e., attempting to employ the idea of transfer learning to the first direction.
 In addition to the issues above for solving sparsity problem, we also consider how to incorporate discriminative power into existing CF algorithms, inspired by Supervised Matrix (or tensor) Factorization (SMF) approach [ 16 ]. The idea of SMF is to faithfully reconstruct the original matrix using discriminative priors (corresponding class labels), i.e., with additional discriminative constraints, such as max-margin criterion for classification [ 16 ] on the basis variables or factorized latent features. However, in our task, the supervised label information for unrated item, i.e., like or dislike for a user is uncertain. Thus, how to actively determine label certainty and use this predicted label information for improving discrimi-native power is a key challenge applying supervised setting to CF task. Due to this obstacle, most of CF algorithms in both mentioned directions act as unsuper-vised manners that failed to exploit the inherent discriminative priors of the data objects. In fact, this knowledge is useful in many real world applications, such as item with tags etc. Ideally, this obstacle and the above challenging issues for solv-ing sparsity problem should be well considered jointly in a unified framework. To solve all the above challenges, in this paper, we propose a unified multi-view framework for collaborative filtering. This framework can be seen as a com-promised approach between the two directions of single task view and transfer learning setting. It combines the merits of these two directions with discrimina-tive power to solve the rating sparsity problem. More specifically, our approach is from multi-view perspective [ 4 , 5 ], which can exploit side information automat-ically for CF task with the ability of knowledge transfer to construct discrimi-native prior from different views. In contrast with the two directions above, our method can work on a very extremely sparse rating matrix 1) without needing the multi-view data representation available in advance, 2) and only maintaining minimal external resources compared with transfer learning approach to achieve knowledge transfer. The following sections will discuss those in detail. In this paper, we address missing rating prediction problem from multi-view perspective. We employ the idea of transfer learning in a multi-view setting with side information 2 , to complete a sparse rating matrix. For simplicity, we focus on the basic recommending case that the value of user rating has only binary states, which also can be extended easily for ordinal case in various applications. Transfer Learning for CF. The idea of transfer learning [ 9 , 17 ] for address-ing the data-sparsity problem in the target recommender system is to use the data from some related recommender systems. In this category, these approaches assume that the knowledge of a source CF model built with rich collaborative data can be extracted as a prior to assist the training of a more precise CF model for the target recommender systems. For example, many commercial Web sites often attract similar users (e.g., Twitter, Facebook, etc.), or provide sim-ilar product items (e.g., Amazon, eBay, etc.), thus we can bridge two related systems by cross-system entity correspondence [ 8 , 10 ] or using the group level similarity [ 7 ] to improve target system performance. However, all the algorithms of transfer learning rely on cross-system resources, which are usually not easy to acquire because of commercial competition or individual privacy. Thus, we extend this good idea to a more cost-saving resource setting, for CF task based on side information in a single system.
 Multi-view Learning for CF. To achieve the goal of extending the idea of transfer learning to a more cost-saving resource setting mentioned above, we propose to use multi-view learning approach to incorporate the ability of knowl-edge transfer for CF. The basic idea of multi-view learning [ 4 , 5 ] is to leverage the redundancy and consistency among distinct views to strengthen the overall performance. We use this idea [ 4 ] originally for clustering problem to deal with data sparsity problem for recommendation. In traditional multi-view learning for classification problem, each view of objective function is assumed to be capa-ble of correctly classifying labeled examples separately. Then, they are smoothed with respect to similarity structures in all views. Similarly, for the CF task in this paper, we also assume that our individual views of user-item rating matrix and side information are complementary with similar latent structure. The difference is that both views are bridged through a bi-directional prior with discriminative power, which extends the idea of transfer learning to multi-view framework. The key idea is that we exploit learning multi-view representations with multiple task oriented objectives (loss functions) in a unified optimization framework, to improve recommendation performance. Our framework is a general solution, which introduces different views of modeling for recommendation via item prior as a bridge.
 The general framework 3 shows the high level generative process of the pro-posed basic model and the extension. The differences lie in that are modeled by different prior modeling approaches for each latent item representation, which will be discussed in the proposed basic model and the extension respectively.  X  For each user i ,  X  draw a user latent vector u i  X  N ( 0 , X   X  1 u I K ), multivariate Gauss distri- X  For each item j ,  X  draw a multi-view representation variable  X  j via representation learning  X  draw an item latent vector v j  X  N (  X  j , X   X  1 v I K ), multivariate Gauss dis- X  For each user-item pair ( i, j ),  X  draw the response r ij  X  N ( u T i v j ,c  X  1 ij ), univariate Gauss distribution, It is noted that the different ways to model prior as view specific representa-tion lead to different recommendation models. The appropriate choice of data representation (or features) plays a key role in acquiring optimal performance of the state of arts machine learning methods. In particular, we use neural net-work approach to learn view specific prior automatically from data, instead of pre-defined fashion by hands in the framework. Our work shares similar intuition of a recent trend [ 14 ] which brings two well-established approaches together, i.e., probabilistic topic modeling and latent factor models. However, previous approach is not from multi-view perspective. These methods [ 3 , 11 , 14 , 15 ] cannot incorporate multi-view loss into a joint opti-mization framework and are not capable of transferring knowledge actively. 4.1 The Proposed Basic Model Model Formulation: To model view specific prior, we incorporate Stacked Denoising Auto-encoder (SDA) [ 13 ] into our optimization framework as initial estimate for our neutral network. SDA is one of building blocks of (deep) rep-resentation learning as an extension of Auto-Encoder (AE). Given the input x representing document as a binary bag-of-words vector, Denoising Auto-encoder (DA) randomly masks 1 with 0 with a pre-defined probability. Since the miss-ing components have to be recovered from partial input, DA has the chances of capturing general concepts and ignoring noise like function words. Then, as the standard AE, it performs encoding process h ( x ) and decoding process g ( h ( x )), minimizing the reconstruction error L ( x, g ( h ( x ))) to retain maximum informa-tion.
 specific representation of each item as prior in a joint optimization framework. The reconstruction criterion is given in view 2 part of Eq. 2.
 Model Learning: For learning the parameters, we develop a coordinate descent optimization algorithm to maximize a posteriori (MAP) estimate of U, V with column vectors u i and v j respectively. It is equivalent to minimizing the complete negative log-likelihood with respect to W, U, V, b , c : where  X  1 ( x )=1 / 1+ exp (  X  x ) as nonlinear mapping and  X  reconstruction function in an element-wise way. W is the weights matrix of Neural Network (NN). Note that the prior  X  j in general framework is modeled as Specifically, we iteratively optimize the collaborative filtering variables U, V and the parameters of representation learning W, b , c . By setting the derivative of L with respect to u i , v j to zero, we obtain the update rule where C i is a diagonal matrix with elements c ij for each j , R with elements r ij for each j . For item j , C j and R j are similarly defined. Then, given U and V , we update the parameters W, b , c via computing the corresponding gradient of L 1 , which is similar to back-propagation in NN but with additional regularization term and sharing weights constraint. Thus, to update W, b, c , we can only modify the existing optimization procedure of autoen-coder (AE) by adding our regularization term. The additional adding gradient for W during each gradient descent iteration in our case is where  X  denotes Hadamard product performing matrix element wise product, H is a matrix with column representations outputted from the hidden layer in NN for each item {  X  1 ( W x ( j ) + b ) } . dH is the matrix with corresponding derivative value of H . X is a data matrix which contains column vectors as bag of word features for each item. M is the total number of items. The adding gradient for b to existing AE optimization is where 1 is a column vector in which all elements are equal to one. 4.2 Extension with Active Discriminative Prior Model Formulation: The proposed approach has three merits. First, we model item prior using a discriminative learning approach rather than a generative fash-ion. Thus, it allows us to flexibly incorporate any multiple task oriented objec-tives instead of a pre-defined generative process with lower bound of objective like CTR [ 14 ], for joint optimization. Second, this discriminative prior modeling naturally offers explicit weighs for mapping new samples out of training data, not needing a re-sampling procedure as in generative models, e.g., LDA [ 2 ]. Third, our method is a general framework which can be easily extended to exploiting other side information, such as social network data for modeling user prior. a) Discriminative Prior Modeling We extend the basic model to multi-view multi-objective setting. Traditionally, collaborative filtering via matrix fac-torization is to solve an unsupervised matrix reconstruction problem under root mean squared error (RMSE) criterion. However, the ultimate goal of any rec-ommender is to generate recommendation lists for users, which is a ranking problem in nature. Thus, we incorporate ranking based loss explicitly, into our joint optimization framework, with representation learning for prior modeling. Any measures for ranking can be incorporated into our framework. For simplic-ity, we define the pairwise ranking criterion as loss function appeared in part of Eq.8, stacking on the output layer of SDA for joint optimization. Dif-ferent from supervised matrix factorization case [ 16 ], in our CF task, accurately acquiring label information of unrated item is non-trivial. To achieve discriminative prior modeling, the main obstacle is that the super-vised information is not available. Here we cannot acquire the true label of unrated item for each user, i.e., like or dislike, because r preted into two ways. One is that user i is not interested in item j ; the other is that user i does not know about item j . Instead, inspired by the work [ 17 ], we actively compute the predicted (label) rating of each unrated item in each optimization iteration, and then use this predicted label as the supervised infor-mation for training discriminative prior. The selection rule for negative sample set is to choice the top-K unrated items for current user in the predicted rating list sorted in ascending order, according to the score of inner product of latent user u i and item v j . bridge between two views (i.e., rating matrix and side information), is not iden-tical for each direction while optimization as shown in Figure 1. The key idea behind this mechanism is that we assume side information for similarity learn-ing is more reliable than for that using a extremely sparse rating matrix. More specifically, while we optimize the variables related to rating matrix view, the variables related to side information are active as a regularization for it. On the contrary, while we optimize the variables related to side information view, the regularization effect of variables related to rating matrix view is not allowed to be active explicitly, but with a way of using active transfer learning approach implicitly. Thus, the most confident knowledge encoded by actively constructed negative samples, learned from rating matrix view, can be utilized for correctly directing the optimization process with corresponding loss.
 Model Learning: For the extended model, similarly, Maximizing A Posteri-ori (MAP) estimate of U, V is equivalent to minimizing the following complete negative log-likelihood with respect to W 1 ,W 2 ,U,V, b , c : where h  X  1 =  X  ( W 1 x (  X  ) + b ), W =[ W 1 ,W 2 ] is the weights matrix in NN with two hidden layers, R i denotes the rated item set of user i , Cn set including the current item j and all other unrated items as negative samples constructed by active knowledge transfer for user i .  X  is a nonlinear sigmoid function as shown in our basic model. Note that the prior is modeled as Similarly, we follow the same strategy used for the basic model to derive the optimization procedure here. It is noted that we only consider two views of each item, ignoring the effect of user regularization through ranking based loss (view 2) when optimizing each latent user representation. Thus, for u derive the similar update rule as shown in basic model to guarantee the closed optimal solution for updating u i and v j respectively, which can also reduce computational cost simultaneously for our extended model. Then, we use the same way discussed in our basic model to modify standard AE optimization procedure by adding additional gradient for regularization of ranking based loss in view 2. Specifically, one modification refers to computing the desired partial derivatives for output layer of NN. We define the output value of NN for each item j , h j 2 =  X  ( W T 2 h j 1 + c ). The partial derivative of respect to h j 2 is The other gradient modification of AE is the consideration for regularization term appeared in bridge part through view 2, which is similar to our basic model but with 2 hidden layers structure. Thus, the similar derivation can be obtained.
 Speeding Up the Optimization: To reduce computational costs when updat-ing u i and v j , we adopt the same strategy of matrix operation shown in [ 6 ]. Specifically, directly computing VC i V T and UC j U T requires time O ( K O ( K
I ) for each user and item, where J and I are the total number of items and users respectively, K is the dimension of latent representation space. Instead, we rewrite Then, bU U T can be pre-computed and C j  X  bI K has only I where I r refers to the number of users who rated item j and empirically I For VC i V T , it is similar. Thus, we can significantly speed up computation by this sparsity property.
 Prediction: Using the learned parameters above, we can make in-matrix and out-of-matrix predictions defined in [ 14 ]. For in-matrix prediction, it refers to the case where those items that have been rated by at least one user in the system. To compute predicted rating, we use r  X  ij  X  ( u  X  prediction, it refers to the case where those items that have never been rated by any user in the system. To compute predicted rating, we use r where the corresponding  X   X  j is defined in Equation 9. 5.1 Data and Metric Datasets 1) CiteULike Dataset: For a fair comparison, we use the same CiteULike dataset 4 as the benchmark, following the prior work in [ 14 ]. This dataset is challenging. Though it contains 204,986 pairs of observed ratings with 5551 users and 16,980 articles, the sparseness is quite low, i.e., merely 0.2175% , which is much lower than that of the well-known Movielens dataset with the sparseness 4.25% . On average, each user has 37 articles in the library, ranging from 10 to 403, and each article appears in 12 users libraries, ranging from 1 to 321. For each article, the title and abstract information are used as the bag-of-word representation. After the text processing by selecting informative words via tf-idf and removing stop words, 8,000 distinct words are remained in the corpus. 2) LastFM Dataset: We further evaluate our proposed method on real life dataset 5 from LastFm 6 . This dataset is also challenging. Though it contains 92,834 pairs of observed ratings with 1892 users and 17,632 items, the sparseness is quite low, i.e., merely 0.2783% , which is also much lower than that of the well-known Movielens dataset with the sparseness 4.25%. On average, each user has 44.21 items in the play list, ranging from 0 to 50, and each item appears in 4.95 users libraries, ranging from 0 to 611. For each item, the tag information is used as bag-of-word representation. After text processing, 11,946 distinct words are remained in the corpus. In addition, we further remove noisy users which have no items.
 Evaluation Metric. Two possible metrics are precision and recall. As discussed in [ 11 , 14 , 15 ], zero ratings are uncertain which may indicate that a user does not like an article or does not know about it. Thus, we use recall as our metric. top-k result and total relevant items respectively. 5.2 Baselines and Settings Baselines  X  CML-ADP-Bi : The proposed extended model with active discriminative  X  CML-ADP : The proposed extended model with active discriminative prior, in which the bi-directional prior modeling mechanism is not used.  X  CML-Basic : The proposed basic model without active discriminative prior.  X  CTR : The model described in [ 14 ], which is the most similar state-of-the-art approach combining the merits of traditional collaborative filtering and probabilistic topic modeling.  X  PMF : The model described in [ 12 ], which is a state-of-the-art matrix fac-torization approach widely applied without using side information.
 Settings. We evaluate our models in three cases. 1) In Matrix and Out-of Matrix Cases (CiteULike Dataset): We use 5-fold cross-validation scheme following [ 14 ] and we use grid search to find corresponding optimal parameters on a small heltout dataset. We found that the common parameters v = 100; u =0 . 01; a =1; b =0 . 01; K = 200 gives good performance for PMF and CTR approach. For CTR, we set additional parameters  X  u =0 . 01; For our model, we set additional parameters  X  w = 10; and vary parameter  X  effect on prediction accuracy. We also select our optimal parameter  X  For DAE, the number of hidden variables K = 300 selected for the optimal performance. Particularly, we use a masking noise probability in 0.7 for the input layer and a Gaussian noise with standard deviation of 0.1 is used for higher output. For parameter analysis with different  X  v and ratio of unrated items for knowledge transfer, we perform this testing for our proposed models for different top items { 50,100,150,200 } with 300 factors. 2) Randomly Split Case (CiteULike Dataset): We randomly split the dataset into two parts, training (90%) and test datasets (10%), with constraint that users in test dataset have more than half of the average number of rated items, i.e., 20. This expands the range of performance analysis for our evaluation compared with [ 11 ]. The optimal parameters are obtained on a small held-out dataset. For PMF, we set  X  v = 100 , X  u =0 . 01 . For all CTR, we set a =1 ,b =0 . 01 , X   X  u =0 . 01 . The remaining setting is the same as that described above. 5.3 Results and Analysis 1) CiteULike Dataset: For in matrix prediction, from Figure 2 (left), we can see that our models consistently outperform CTR model and PMF under recall and achieves considerable improvement. In addition, we study how the con-tent parameter  X  v affect the overall performance of the recommendation system. From Figure 3 (left), we observe that the value of  X  v impacts the recommenda-tion results significantly, which demonstrates that fusing representation learning with PMF improves recommendation accuracy considerably. For out of matrix prediction task, PMF is useless in this problem. Thus, we only compare CTR with the proposed models in this paper. From Figure 2 (right), we can also see that our models consistently outperforms CTR model under recall metric. Similarly, we study how the content  X  v affects the overall performance in out of matrix prediction task setting. We also can find that  X  mendation results significantly from Figure 3 (right). It is noted that although the improvement compared with in matrix predication case is not considerable, it is also much better than that in original CTR which compares with LDA in its original paper [ 14 ]. This could be explained that the task-oriented opti-mization benefits from the discriminative learning approach compared with a generative fashion as in CTR, which makes a strong assumption in generative process. We further exploit how the ratio of unrated items for knowledge trans-fer can influence the recommendation performance in Figure 4. It is shown that the performance is increased with the ratio but the computation costs are also increased. Thus, we choose 20% as our optimal value. With a more larger one, it may introduce more some uncertain negative samples to undermine the per-formance. Moreover, we can see that the ranking based objective as additional optimization view in the extended model (CML-ADP and CML-ADP-Bi), to augment the RSME error criterion, is also a necessary, which is proven by our experiment results in both in-matrix and out-of-matrix tasks. This ability to incorporate multiple task related optimization objectives is a salient advantage in our proposed collaborative multi-view learning framework, which is not easily achieved in the generative approach, e.g., CTR. 2) LastFM Dataset: From Figure 5, we can find the similar results as shown in previous discussion on CiteULike dataset, which further demonstrates the effectiveness of the proposed method in randomly splitting case. Thus, all three cases in various real applications have proved the promising performance of the proposed method. In this paper, we propose a multi-view learning framework with the ability of knowledge transfer for recommendation. We can learn multi-view representation automatically from data, without needing multi-view data representation avail-able in advance. Our method achieves significant improvements on all three cases compared with the state-of-the-arts. In particular, our models achieve such con-siderable improvements only using content information, in contrast with the models relying on more external resources, such as both content and social network information. Thus, the proposed method serves as a fundamental frame-work, which can be further improved by incorporating additional side informa-tion using the same fashion in [ 3 , 11 , 15 ].
 Acknowledgments. This research was partly supported by National Natural Science Foundation of China (No.61370117,61433015) and Major National Social Science Fund of China (No.12&amp;ZD227).

