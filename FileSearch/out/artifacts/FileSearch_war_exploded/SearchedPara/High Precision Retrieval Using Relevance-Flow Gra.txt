 Traditional bag-of-words information retrieval models use aggregated term statistics to measure the relevance of documents, making it difficult to detect non-relevant documents that contain many query terms by chance or in the wrong context. In-depth document analysis is needed to fi lter out these deceptive documents. In this paper, we hypothesize that truly relevant documents have relevant sentences in predictable patterns. Our experimental results show that we can successfully iden tify and exploit these patterns to significantly improve retrieval precision at top ranks. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Measuremen t, Experimentation Keywords : relevant sentence, relevance flow, re-ranking 
To achieve high precision retrieval, filtering out highly ranked non-relevant documents is crucial. These documents look relevant in traditional bag-of-words models because they contain many query terms like relevant documents. In this paper, we show these deceptive, non-relevant documents can be successfully identified and demoted by analyzing the change of relevance scores at the sentence level. We especially focus on the strength and the position of the relevant sentences in the document. 
To analyze the spatial distribution of relevant sentences in a document, we plot relevance scores of sentences versus their locations in the document as shown in Figure 1. We call this graph a relevance-flow graph . The locations of peak s show positions of relevant sentences, and the height of peaks represent the strength of relevance. From the graph, we extract a set of features that can capture various aspect of the graph. Using training data sampled from the TREC collection we learn a probabilistic model that can distinguish relevance-flow patte rns from relevant documents. Top documents returned by the baseline search engine are re-ranked solely using the classifier scores. Our evaluation result shows that this approach significantly improve s precision, especially at top ranks. 
There have been efforts to anal yze the distributional patterns of query terms to measure term proxim ity scores [4]. However, as far as we know, there has been little attempt to infer document relevance based on sentence-level relevance analysis. In this paper, we show this kind of approach is promising. 
To measure relevance scores of sentences, we use normalized query likelihood scores as estimate s. The query likelihood score is the probability of query Q given the Dirichlet-smoothed unigram language model of a sentence S, i.e. P(Q|S) [1]. For a query, we compute the query likelihood scores for all sentences in the top N documents returned from the baseline search engine. Then, each score is normalized by, and score min are the maximum and the minimum relevance scores across the top N documents. We call the normalized query likelihood score the relevance level . If the relevance level of a sentence is greater than 0.5, then we call the sentence a peak . A sentence at the peak can be considered as an estimate for a relevant sentence. In addition, po sitions of sentences are also normalized for comparison across different documen ts: 0 for the first sentence and 1 for the last sentence. 
The relevance-flow graph visual ly shows the fluctuation of relevance level inside of the document. This graph is often useful to understand why one document is more relevant than the other. For example, Figure 1 shows relevance-flow graphs of a relevant document (Figure 1(a)) and a non-relevant document (Figure 1(b)) for a given query. Both documents have similar log likelihood scores from the baseline search engine. Figure 1(a) has an early peak followed by a few smaller peaks while Figure 1(b) has many smaller peaks at the end of the document. Intuitively, having an early peak is a good sign because many writers put key sentences at considered as a positive sign because high peaks mean that the majority of query terms appear in the sentence, that is, the proximity among query terms is well-prese rved. Our retrieval system can successfully identify these differences and demote the non-relevant document in Figure 1(b). Expl oiting these observations is impossible in traditional bag-of-words retrieval models. 
We learn a statistical model which is able to predict the relevance of a document from the relevance-flow graph of the document. The logistic regression model is used with the following six features extracted from the graphs. Mean and Variance of Relevance Level (F1.1 and F1.2) The arithmetic mean of relevan ce levels shows how relevant a document is at large. This can also be interpreted as COMBAVG in rank fusion [2]. High variance values imply many peaks and valleys in the graph. Peak to Sentence Ratio (F2) 
To investigate relations between the number of peaks and relevance, we use (# peak / # sentence ) as a feature. The first peak position (F3) 
If a document is relevant, then relevant sentences often appear in the beginning of the document. Troy et al. exploited this property and reported improved retrieval performance [5]. Xue and Zhou used this property for text classification tasks [6]. We use the position of the first peak in a document as a feature. Mean and Variance of peak positions (F4.1 and F4.2) 
The mean of peak positions roughly shows where relevant sentences appear. The variance s hows how relevant sentences are spatially distributed in a document. 
Table 1 shows learned weights for the features. Not surprisingly, the mean of relevance levels (F1. 1) has a small weight. Since all documents in the initial search result are competitive under the bag-discriminative. On the other hand, the variance of the relevance levels (F1.2) has the largest we ight. This shows that relevant documents are likely to contain a few highly relevant sentences rather than many medium or low relevance level sentences. The peak to sentence ratio (F2) also shows a similar aspect. F2 has a negative relation to relevance. That is, fewer peaks are preferred. This may sound strange. However, because we know that most top ranked documents have similar query likelihood scores, this can be interpreted as a few  X  X igh X  peaks ar e preferred to many  X  X ow X  peaks. Therefore, the larger weights of F1.2 and F2 imply importance of term proximity. There is a negativ e relationship between the first peak position (F3) and relevan ce. In other words, the early appearance of a relevant sentence is preferred as expected. Both the mean (F4.1) and the variance (F4.2) of the peak positions have little impact and removing these features did not hurt our re-ranking performance. Overall, F1.2 is the mo st important feature. F2 and F3 are also useful. 
For evaluation, we used the AP collection and title queries of topics 51-200 in the TREC corpor a. We split the queries by query-(100 queries) and the others into a test set (50 queries). As preparation for plotting relevance-flow graphs, each document was segmented into sentences using the MXTerminator [3]. 
We retrieved the top 15 documents for each query using the bag-of-words model implemented in the Indri 1 search engine, where the unigram language model was used with the Dirichlet smoothing http://www.lemurproject.org/indri/ parameter  X  doc =3600, which produced the best retrieval performance. Unigram  X  X entence X  language models for relevance-flow graphs are smoothed with the collection language model (Dirichlet smoothing parameter  X  sentence =300). Table 2. Retrieval performance. A superscript * indicates a statistically significant improveme nt on the initia l result. (sign test with p -value &lt; 0.05) 
We re-ranked the initial search result according to predicted relevance. Since our purpose is to achieve high precision in the top results, we use precision at 1 (P@1) and precision at 5 (P@5) as evaluation metrics. Table 2 shows th e retrieval performance. The re-ranked results show statistically significant improvements over the initial result for both metrics. 
In this paper, we demonstrated that analyzing spatial distribution patterns of relevant sentences has major potential to improve precision of retrieval systems. This work is just the first step of our efforts to understand document relevance via relevance analysis at the sentence level. We plan to explore different methods to create relevance-flow graphs and more descriptive features that can capture various aspects from the gr aphs. Finally, we will investigate new scoring functions which can seamlessly combine traditional document-level scores with scores based on our sentence-level analysis. This work was support ed in part by the Center for Intelligent Information Retrieval (CIIR) and in part by NSF grant #IIS-0711348. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor. [1] W. B. Croft and J. Lafferty. Language Modeling for Information 
