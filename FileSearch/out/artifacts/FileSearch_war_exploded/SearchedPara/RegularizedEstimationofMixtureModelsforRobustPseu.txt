 Pseudo-rele vance feedback has pro ven to be an effecti ve strate gy for impro ving retrie val accurac y in all retrie val models. Ho we ver the performance of existing pseudo feedback methods is often af-fected signicantly by some parameters, such as the number of feedback documents to use and the relati ve weight of original query terms; these parameters generally have to be set by trial-and-error without any guidance. In this paper , we present a more rob ust method for pseudo feedback based on statistical language models. Our main idea is to inte grate the original query with feedback doc-uments in a single probabilistic mixture model and regularize the estimation of the language model parameters in the model so that the information in the feedback documents can be gradually added to the original query . Unlik e most existing feedback methods, our new method has no parameter to tune. Experiment results on two representati ve data sets sho w that the new method is signicantly more rob ust than a state-of-the-art baseline language modeling ap-proach for feedback with comparable or better retrie val accurac y. H.3.3 [ Inf ormation Sear ch and Retrie val ]: Retrie val models Information retrie val, algorithm Pseudo feedback, mixture model, regulation, EM
Among man y techniques for impro ving the accurac y of ad hoc information retrie val, pseudo feedback is arguably the most effec-tive one and has been sho wn to be effecti ve across all retrie val models. The basic idea of pseudo feedback is to assume a certain number of top-rank ed documents are rele vant and learn from these documents to impro ve the query representation, thus to impro ve re-trie val accurac y [1, 16]. Intuiti vely , since the top rank ed documents Cop yright 2006 ACM 1 X 59593 X 369 X 7/06/0008 ... $ 5.00. are lik ely rele vant or at least similar to rele vant documents, we may exploit these documents to identify useful additional terms to im-pro ve the query representation and/or to assign better weights to the query terms. Such intuition has been implemented in dif ferent ways in dif ferent retrie val models. In the vector space model, a centroid vector is often constructed based on the feedback documents, and a new query vector is then formed by mo ving the original query vector closer to this feedback centroid vector [11]. In the classic probabilistic model, the feedback documents are naturally treated as examples of rele vant documents to impro ve the estimation of model parameters [10], whereas in the recently proposed language modeling approaches, it can be implemented through estimating a query language model [5, 18] or rele vance model [7] through ex-ploiting a set of feedback documents.

Although the idea of pseudo feedback may be implemented dif-ferently , a common observ ation is that it is generally effecti ve in impro ving retrie val accurac y, which means that, on average, it tends to impro ve retrie val accurac y, especially the recall. Ho we ver, it is also a common observ ation across all retrie val models that pseudo feedback is not completely reliable; this is not really surprising given that man y top rank ed documents may actually be non-rele vant and can be misleading. Indeed, the performance of existing pseudo feedback methods is often affected signicantly by some param-eters, such as the number of feedback documents to use and the relati ve weight of original query terms. For example, in a recent study by researchers at the 2003 Reliable Information Access (RIA) Workshop [2, 9], it has been found that nine retrie val systems, rep-resenting quite dif ferent retrie val models, have all sho wn trade-offs in the choices of optimal numbers of documents for pseudo-rele vance feedback; usually some amount of documents for feed-back can be helpful but too man y of them would cause negati ve inuence. The y also sho w that the optimal number of documents used for feedback varies from system to system, and no explicit relationship has been found between either the query length or the number of rele vant documents and the optimal number of docu-ments for feedback. These observ ations clearly suggest that the performance of man y existing pseudo feedback methods is sensi-tive to parameters such as the number of documents for feedback, and it is quite challenging to choose the optimal number of docu-ments for feedback.

There has been some pre vious work on impro ving the rob ustness of pseudo feedback, mostly through heuristics approaches to opti-mize feedback parameters in a query-specic way, to detect cases when pseudo feedback should not be applied, or to select only a sample of documents from the top rank ed documents for pseudo feedback [17, 13]. Unfortunately , these studies are mostly unsuc-cessful. (A comprehensi ve discussion of this pre vious work can be found in [13].) The fundamental reason why the study of rob ust pseudo feedback has so far been unsuccessful is due to the fact that some or even man y top-rank ed documents are indeed non-rele vant. Ho we ver, there are also two other common reasons why most ex-isting pseudo feedback methods are not very rob ust: (1) The com-bination of the original query and the feedback documents has been  X loose X . For example, a common strate gy is to extract some pre-sumably useful information from the feedback documents and then combine it with the original query representation through interpola-tion. This strate gy is adopted in the Rocchio feedback method [12] and the mixture language model method [18]. One decienc y of such a strate gy is that the extraction of information from feedback documents is not guided by the original query terms; as a result, the extracted information can be easily distracted by any dominant distracting theme in the feedback documents. (2) The non-rele vant information contained in the feedback documents is not carefully modeled. The feedback documents would ine vitably contain man y non-rele vant documents, but man y existing methods do not attempt to discriminate documents in the feedback set or do not model the dif ferences of documents adequately . As a result, the performance tends to be very sensiti ve to the number of documents used for feedback.

The recent development in language models for information re-trie val has been able to address both issues to certain extent. For example, the rele vance model approach proposed in [7] and the Mark ov chain approach proposed in [5] combine the original query with the feedback documents in a principled way. The cluster -based query expansion [4] further uses document cluster informa-tion. Mixture models have been explored in [18, 3, 14]. Ho we ver, even in these more principled models, there are still parameters to tune. For example, in the rele vance model [7], interpolation with a collection language model is needed, which introduces an inter -polation parameter that needs to be set manually . Similarly , in the Mark ov chain approach, the probability of stopping  X bro wsing X  is a parameter that needs to be set manually . In the mixture model proposed in [18], a background noise parameter and an interpola-tion parameter are introduced, and both need to be set manually . A problem with such manually set parameters is that we have no guid-ance on how to set them and the optimal setting tends to vary sig-nicantly according to the query and collection. Thus even in the language modeling frame work, it remains a challenge to develop rob ust feedback models  X  models that either have no parameter to tune, or are relati vely insensiti ve to parameter settings.
In this paper , we sho w that it is possible to develop a rob ust pseudo feedback method with no parameter to tune through using appropriate mixture language models and carefully regularizing the estimation of language model parameters. Our work is based on the two-component mixture model proposed in [18], which has been sho wn to perform quite well in several dif ferent tasks as a pseudo feedback method. The basic idea of this model is to rst t a two-component (topic and background) mixture model to the feedback documents and then interpolated the estimated component topic model with the original query model to obtain an impro ved query language model. This model was later extended in [14] to better inte grate the original query model with the feedback documents and to allo w each feedback document to potentially contrib ute dif-ferently to the estimated feedback topic language model. The ex-tended model is sho wn to be relati vely more rob ust than the original model, but the model is still quite sensiti ve to the number of docu-ments used for feedback [14]. Moreo ver, due to the use of several priors, this new model has man y prior parameters that need to be set manually .

We belie ve that the key to impro ve the rob ustness of such mix-ture models lies in careful regularization of the parameter estima-tion process and eliminating as man y parameters as possible; ide-ally , a feedback query model should not rely on any manually as-signed parameter , and all model parameters would be learned from the original query model and the feedback documents.

We propose to mak e two modications to the two-component mixture model proposed in [18] so as to eliminate the two parame-ters that would otherwise need to be set manually: (1) Introduce a document-specic mixing coef cient to model potentially dif ferent amount of rele vance information in each feedback document; the coef cient will be automatically set through statistical estimation. (2) Use the original query model as a prior on the feedback lan-guage model to be estimated, which not only reparameterizes the interpolation with a more meaningful parameter (i.e., condence on the original query model), but also allo ws us to regularize the esti-mation of the feedback language model so that useful information from the feedback documents would be gradually incorporated into the original query language model. The condence parameter is set through stopping the iterati ve estimation process at the  X right X  time to balance the original query model and the amount of rele vance information extracted from the feedback documents.

On the surf ace, these extensions appear to be quite similar to what has been done in our pre vious work [14]. Ho we ver, there are two key dif ferences: (1) The purpose of using the original query model as a prior is dif ferent; here our purpose is mainly to exploit the query model to guide us in distinguishing documents that po-tentially have more rele vance information than those that do not so that the estimated language model would be more inuenced by the documents that are more lik ely to be rele vant; indeed, we will gradually reduce the condence value on the prior to allo w more and more information from feedback documents to be incorporated into the query language model. (2) Unlik e in [14], the mixing coef-cient of the two models does not tak e a prior and will be estimated solely based on the data.

We further propose a regularized EM algorithm for estimating the parameters, which includes the follo wing two regularization strate gies: (1) gradually discounting the prior condence; and (2) stopping the algorithm as we have suf cient amount of rele vance information extracted from feedback documents. Our main idea is to use the original query model to gradually  X attract X  rele vant terms from the feedback documents in a controlled manner . At the same time, the original query model is also naturally mix ed with the new model learned from the feedback documents as a prior .

A major adv antage of this new model over all existing feedback methods is that it has no parameter to tune. We evaluated this new model and the new estimation method on two representati ve data sets. The results sho w that the new method is signicantly more rob ust than a state-of-the-art baseline language modeling approach for feedback with comparable or better accurac y.

In the rest of the paper , we rst present the new mixture model in Section 2. We then present a regularized EM algorithm in Sec-tion 3. In Section 4, we discuss the experiment results. Finally , we conclude in Section 5.
In this section, we extend pre vious work [18, 14] and develop a query-re gularized mixture model for pseudo feedback.
Our basic retrie val model is the Kullback-Leibler (KL) diver-gence retrie val model [5, 19, 6], which assumes that both a query and a document are generated from a unigram language model. To score a document D w.r.t. a query Q , we would rst estimate the query language model and then compute the KL-di vergence D ( where V is the set of all the words in our vocab ulary . For the sak e of efcienc y, we often truncate the query language model a lar ge number of small probability words that would contrib ute to scoring insignicantly anyw ay.

Clearly , the two main tasks are to estimate the query language model model resulting in some form of interpolation of the relati ve frequenc y of a word in the document and the probability of the word given by some background language model [19]. For example, using one of the most effecti ve smoothing methods called Dirichlet prior smoothing method, we would have where c ( w; D ) is the count of word w in D , p ( w j background language model, and is a smoothing parameter often set empirically . The background language model using the whole collection of documents C [19], i.e., p ( w j
The query model intuiti vely captures what the user is interested in, thus would affect retrie val accurac y signicantly . Without feed-back, where c ( w; Q ) is the count of word w in the query Q , and the total number of words in the query . Such a model, howe ver, is not very discriminati ve because a query is typically extremely short. Several dif ferent methods have been proposed to impro ve the estimation of ments that are used for pseudo-rele vance feedback (i.e., the top-rank ed documents from an initial retrie val run) [5, 18, 7]. In [18], it is proposed that pseudo-rele vance feedback can be implemented in the KL-di vergence retrie val model as updating the query model based on the feedback documents. Specically , we can dene a two-component mixture model (i.e., a x ed background language model model to be estimated) and assume that the pseudo feedback docu-ments are generated using such a mixture model. Formally , let be the unkno wn topic language model and F C a set of pseudo feedback documents. The log-lik elihood function of the mixture model is log p ( F j T ) = X where is a parameter indicating how lik ely we would use the topic model (as opposed to the background model) to generate a word in a document in F . In [18], is set to a constant, and the Expectation-Maximization (EM) algorithm is used to estimate which is then interpolated with the empirical query model to obtain an impro ved estimate of the query model p ( w j (1 ) p ( w j Q ) + p ( w j T ) , where is another parameter to be manually set.

Although this model has been sho wn to perform quite well, its performance is reported to be sensiti ve to the setting of both and [18]. This moti vates us to develop the follo wing query-regularized mixture model as an extension of this basic model; the new model can eliminate the need for manually setting these two parameters. Specically , we will estimate a potentially dif ferent for each document, rather than manually set it to a x ed constant, and we will incorporate the empirical query model p ( w j Q ) mixture model as a prior . We will also mak e our tar get ponent in the mixture model so that we can avoid  X post estimation interpolation X . We now present this new mixture model.
Intuiti vely , all pseudo feedback methods would try to learn some useful information from the feedback documents. The two com-ponent mixture model with a background model proposed in [18] would allo w us to learn a unigram component language model (i.e., ) from the feedback documents by factoring out words with high probabilities according to the background model.

One decienc y of this simple mixture model is that the mixing coef cient is x ed across all the documents even though some feedback documents presumably have more noise than others. To model dif ferent amount of rele vance in dif ferent documents, we should allo w each document to have a potentially dif ferent Naturally , we would expect a rele vant document to have a lar ger we have where log-lik elihood for all the feedback documents in F is thus where = f
Another decienc y of the simple mixture model is that it does not involv e the original query in any way. As a result, the esti-mation of we must interpolate the learned p ( w j Q ) in an ad hoc way. In order to inte grate the original query with the learned language model from feedback documents, we use the original (empirical) query model p ( w j Q ) to dene a conjugate Dirichlet prior for In effect, this is to force the estimated as possible. Here is a parameter indicating our condence on the original query model prior . Since the prior is conjugate, can be interpreted as  X equi valent sample size X , i.e., the inuence of the prior would be equi valent to adding p ( w j Q ) pseudo counts for word w when estimating the topic model is, the more constrained how we will use to allo w the original query model p ( w j Q ) dynamically regularize the estimation of We do not put any informati ve prior on all the parameters is With this prior , we may use Bayesian estimation to maximize the posterior probability of parameters, as opposed to maximizing the lik elihood function of the parameters as in [18]. Specically , we can use the follo wing Maximum A Posterior (MAP) estimator: Since we use a conjugate prior , the MAP estimate can also be com-puted by modifying the M-step in the EM algorithm to incorporate extra counts as dened by the prior (see [8] for the deri vation). The updating formulas are as follo ws: E-step : p ( Z w;D = 1) = M-step : P ( n +1) ( w j T ) = P ( w j Q ) + P D 2 F c ( w; D ) p ( Z The hidden variable Z D has been generated using incorporated the original query model p ( w j Q ) as a prior , the esti-mated topic model model based on feedback documents. That is, we no longer need to interpolate
The mixture model is illustrated in Figure 1. On the surf ace, this model is very similar to the one proposed in [14], where each document is also allo wed to have a dif ferent mixing coef cient. Ho we ver, there are a few important dif ferences between them: (1) The model in [14] puts a prior on parameters that need to be manually set, whereas we attempt to es-timate manual parameter setting, but also gives our model more freedom to discriminate feedback documents in terms of the amount of rele-vance information, which is crucial to achie ve rob ustness w.r.t. the number of documents for feedback. (2) The estimation method used in [14] performs a standard EM MAP estimation, while we use a regularized EM algorithm (to be described belo w) that allo ws us to control the parameter estimation process and gradually let the prior query model attract rele vant terms from the feedback docu-ments.
The mixture model presented abo ve gives us a way to extract a query language model from the feedback documents that is close to our original query language model. Ho we ver, if we apply the stan-dard EM algorithm directly to estimate the query language model, we run into two problems: Setting of : We will have to set the prior condence value man-ually . controls the relati ve weight we put on the original query language model, so in this sense, it plays a role similar to the inter -polation parameter in the pre vious work [18]. Ho we ver, since the relati ve weight on the original query language model depends on both and the accumulated counts from the feedback documents during the EM algorithm, it would be non-optimal to set to some predened constant. Intuiti vely , it should be set based on the accu-mulated rele vance counts from the feedback documents.
 Accuracy of D : Although we allo w each document to poten-tially contrib ute dif ferent amount of rele vance information to the estimated query language model through a document-specic how accurately a document highly depends on the accurac y of the tentati vely esti-mated query language model away from the original query language model, inaccurate, and thus after more iterations, the estimated query lan-guage model will be quite biased toward the popular topic in the feedback documents. Thus our key idea to achie ve rob ust feed-back is to ensure that the tentati ve query model at each iteration of EM is reasonably accurate, i.e., the inuence of the original query language model as a prior should be kept at a high level all the time, which can be achie ved by setting to a lar ge value. Unfor -tunately , a lar ge would pre vent query-related words from the feedback documents, because a lar ge would mak e and also an upper bound on the amount of words from the feedback documents.

To solv e these two problems, we propose to start with a very high value for and gradually lower in each EM iteration. The idea is to gradually relax our constraint on assimilate useful words from feedback documents. Because of such regularization, we may expect the tentati vely estimated time to be relati vely reliable.
 As we regularize EM in this way, we would force with very small values since original query model and cannot explain man y words in the doc-uments. As we lower , howe ver, lar ger , causing the expected rele vance count in the second equation of the M-step (i.e., r ( n ) = P 1) ) to increase as well, which allo ws adapted toward the feedback documents. Since at each iteration, is decreasing while the rele vance count r ( n ) is increasing, the y eventually will  X meet X  (i.e., r ( n ) ). At this time, we can stop the EM algorithm since we have now accumulated about the same amount of rele vance information as the equi valent sample size of our prior based on the original query model. In other words, this is the time when we have roughly an equal weight on the original query and the new information extracted from the feedback docu-ments and it is a relati vely  X safe X  stopping point.

Actually we may let r ( n ) gro w more to achie ve more aggres-sive feedback. Ho we ver, the additional words attracted would no longer be so reliable because our constraint of prior on the query model is no longer strong, so the algorithm would lik ely become less rob ust. Indeed, our experiments sho w that when we use few top-rank ed documents for pseudo-rele vance feedback, we can of-ten further impro ve performance by allo wing r ( n ) to gro w more aggressi vely , but when we use a lar ge number of documents for feedback, more conserv ative gro wth of r ( n ) is often better .
Note that although stopping the algorithm when r ( n ) is in effect achie ving a roughly equal weight interpolation between the original query model and the feedback model, the learned feedback model in our method is expected to be more rob ust than one learned from the EM algorithm where This is conrmed in our experiments. Our regularized EM also helps address the multiple local maxima problem by regularizing the path with the original query model and tar get at a specic local maximum that is most consistent with our original query model.
In sum, our main idea for achie ving rob ust pseudo feedback is to start with setting atively discriminate documents based on how well the y match the current documents accordingly . The process stops as we have accumu-lated enough rele vance information from the feedback documents that balances well with the original query model. We illustrate this regularized estimation process in Figure 2. In the early stage of training, the only reliable information we have is the query prior . Thus, we rst assign a lar ge prior condence . This condence is lar ge enough to force corporating much rele vance information from feedback documents (Figure 2 (a)). As this process continues, we would gradually relax the control of prior information by discounting . During each iter -ation, is reduced a little bit so that some additional rele vance in-formation can be added in. We use a discounting factor 2 (0 ; 1) to control the discounting rate: n +1 = n . Our experiments sho w that the performance is insensiti ve when the initial (i.e., ) is suf ciently lar ge and is close to 1 (i.e., when we slo wly discount a lar ge query prior condence value). We tested our new method on two standard TREC data sets [15]: DOE (Department of Ener gy) data, and TREC678 (the ad hoc data used in TREC6, TREC7, and TREC8). DOE has 226,087 doc-uments, and each document is about 117 terms on average. We used 35 queries that have a descent number of rele vant documents, and the total number of rele vant documents for these queries is 2,047. TREC678 has 528,155 documents with an average doc-ument length of about 477 terms. We used TREC 301-450 top-ics, which have a total number of 13,692 rele vant documents. We chose TREC678 for the reason that this data has been used in the RIA Workshop, in which the rob ustness of pseudo-rele vance feed-back was studied [2, 9]. For both data sets, we used only the title elds of the topics to simulate the kind of extremely short keyw ord queries that users often use.

We implemented our new feedback model on top of the Lemur toolkit 1 , and used Lemur for all our experiments. Follo wing the TREC standard, we retrie ve 1,000 documents for each query , and use mean average precision ( MAP ) as the primary performance measure. In all the experiments, we rst use the basic KL-di vergence method without feedback to retrie ve a rank ed list of documents for each query as our feedback documents. Dirichlet Prior smooth-ing is used and the smoothing parameter is set to 2000 as recom-mended in [19]. We then use either our regularized mixture model (RMM) or the simple mixture model (SMM) to perform pseudo-rele vance feedback with a certain number of top-rank ed documents from the initial retrie val results. We truncate the updated query language model to keep 100 highest probability terms and use the KL-di vergence method again to perform a second round retrie val. We compare RMM with SMM on both data sets to study the effec-tiveness of the new model.
We rst compare RMM with SMM in terms of their rob ustness w.r.t. the number of documents used for feedback. We tested RMM and SMM with six dif ferent numbers of feedback documents: 10, 50, 100, 150, 200, 300. The MAP results are sho wn in Table 1, where we compare RMM with (1) the baseline no-feedback run (KL), (2) SMM with x ed parameters tuned to optimize perfor -mance for 50 feedback documents (SMM-x ed), and (3) SMM tuned for each dif ferent number of feedback documents (SMM-opt).

As discussed in Section 2, SMM has two major parameters to tune, one for the background noise in the mixture model and one for the interpolation weight. Without rele vance information about the test queries, these parameters generally need to be set based on
Available at http://www .lemurproject.or g/ some training queries. In our experiments, we give SMM a slightly unf air adv antage by tuning its two parameters to optimize its per -formance on test queries for 50 feedback documents and then x the parameter values in all the experiments with other numbers of feedback documents. This strong baseline run is labeled as  X SMM-x ed X . We also include an optimal run of SMM (labeled as  X SMM-opt X ), in which the two parameters are tuned in each experiment on the test queries. SMM-opt thus represents the upper bound perfor -mance of SMM on the test queries.

Strictly speaking, RMM also has two insensiti ve parameters to set: the initial prior condence ( 0 ) and the discounting factor ( ). We set 0 and to 30,000 and 0.9, respecti vely . As will be sho wn later , the feedback performance is not sensiti ve to these two param-eters as long as we choose a lar ge 0 and a that is close to 1.0. From Table 1, we can mak e the follo wing observ ations:
We notice that RMM is always better than the no-feedback base-line KL, and the impro vement is statistically signicant in most cases, indicating that as a pseudo feedback method, RMM is ef-fecti ve and rob ust w.r.t. using dif ferent number of documents for feedback. In contrast, SMM-x ed is noticeably less rob ust; indeed, although it impro ves over the no-feedback baseline KL when using a relati vely small number of documents for feedback, it actually does not perform as well as the KL baseline when a lar ge number of documents (e.g., 300) are used for feedback. Ev en SMM-opt, with all the parameters optimized specically for the number of docu-ments used for feedback, can barely impro ve over KL. The sensi-tivity of SMM to the number of feedback documents also seems to be more pronounced on TREC678 than on DOE. One possible explanation is that as we use more feedback documents, the noise becomes more harmful in the case of TREC678 because DOE is a relati vely small, homogeneous data set whereas TREC678 is much lar ger and more heterogeneous.

Compared with SMM-x ed, RMM is signicantly more rob ust as sho wn in the last column of the table, where we see that the im-pro vement of RMM over SMM-x ed is often amplied as we use more and more feedback documents, clearly indicating the supe-rior rob ustness of RMM. There is only one case (DOE, 50 docs) when RMM is worse than SMM-x ed, but in this case, SMM-x ed has been tuned on the test queries. In reality , it is unlik ely that we can perform such tuning, thus SMM will most lik ely perform worse than SMM-x ed in real applications. Considering the fact that RMM has no parameter to tune, the impro vement of RMM over SMM-x ed is quite impressi ve.

Ho we ver, the performance of RMM is still affected by the num-ber of feedback documents and the performance seems to decrease monotonically as we use more and more feedback documents, in-dicating that there is still room for further impro vement of our method in terms of rob ustness.

We further compare RMM with SMM-x ed in terms of preci-sions at dif ferent levels of recall in Table 2. We see that once again, RMM outperforms SMM-x ed signicantly in most cases.
A main idea of RMM is to start with a high condence value for the query model prior ( 0 ) and gradually discount by a factor of in each iteration of the EM algorithm. (See Section 3.) Strictly speaking, these are parameters in our method, thus one question is to what extent the performance is affected by the setting of and . In Figure 3, we plot the MAP values for dif ferent values of 0 and for RMM on both DOE (#docs=300) and TREC678 (#docs=10). We see that the performance is insensiti ve to the set-(#docs 0.6 0.1386 0.1486 7.215% *
TREC678 0.4 0.3148 0.3263 3.6531% * (#docs 0.5 0.2662 0.2726 2.4042% * * The impro vement is statistically signicant at the level of 0.05 ting of these parameters as long as the initial condence value is set to a suf ciently lar ge number and the discounting factor is set to a value close to 1.0., i.e., we start with a suf ciently high condence value and discount the condence slowly , which mak es sense intuiti vely and is what we would expect.
One may notice that the proposed estimation process stops im-mediately after the collected rele vance information reaches the equi v-alent sample size of the query prior . Intuiti vely , with slo w discount-ing of the prior condence, this process would result in an impro ved query language model through gradual incorporation of rele vance information from the feedback documents into the original query model. Our experiment results also sho w that the method is effec-tive and rob ust for pseudo-rele vance feedback. Ho we ver, since we did not let the EM algorithm con verge, it is unclear how to interpret the estimated model We thus run some additional experiments, in which we further con-tinue the EM algorithm until it con verges, but without further low-ering the query condence (labeled as RMM-con v). The results are sho wn in Table 3. Comparing RMM-con v with RMM, we see that the y are mostly similar , suggesting that the ping (RMM) may actually be quite close to the local maxima which RMM-con v obtains. Ho we ver, RMM-con v seems to be slightly more sensiti ve to the number of feedback documents than RMM, and it tends to outperform RMM slightly when fewer documents are used for feedback, but would lose to RMM slightly as more documents are used. One possible explanation is that as the EM algorithm continues, the collected amount of rele vance informa-tion from the feedback documents would lik ely increase, causing more aggressi ve feedback, thus higher sensiti vity to the number of feedback documents. Ho we ver, the dif ference is so small that more experiments are clearly needed to further analyze this dif ference. Table 3: EM con vergence and  X  X nside-estimation X  inter pola-tion.

One dif ference between RMM and SMM is that RMM incor -porates the original query model as a prior to achie ve an  X inside-estimation X  interpolation effect, while in SMM, the interpolation is done after the estimation process is nished. Presumably , the  X inside-estimation X  interpolation is better since the estimation can be guided by the original query model. To see whether such inside-estimation interpolation has any empirical adv antage, we also sho w in Table 3 an optimal run of SMM with a x ed value (0.5) for the interpolation parameter (labeled as SMM-0.5), in which we opti-mize the background noise parameter on the test queries. This run is comparable with RMM because when RMM stops, the learned query model would correspond roughly to a (0 : 5 ; 0 : 5) tion of rele vance information learned from the feedback documents with the original query model. Comparing RMM with SMM-0.5, we see that the inside-estimation interpolation is indeed adv anta-geous probably due to the benet of guiding the estimation of with the query model prior .
Since our method stops EM iterations when a prior condence is equal to the rele vance count r ( n ) , an interesting question is whether stopping exactly in the middle is optimal. Indeed, we can introduce an additional parameter to accommodate more aggres-sive feedback i.e. EM would stop when r ( n ) . With this extension of RMM, we can adjust to obtain the best performance for dif ferent number of feedback documents, which in some sense reects an upper bound of the performance for RMM. The results are sho wn in Table 4. It is interesting to see that the optimal is correlated with the number of feedback documents. The optimal tends to be higher (trusting the feedback information more) when using fewer top-rank ed documents for feedback, which intuiti vely mak es sense as there would be more noise when we use more docu-ments for feedback. Ov erall, setting = 1 : 0 as in RMM is mostly close to optimal. This analysis, howe ver, does suggest an interest-ing possibility of automatically setting the parameter based on the number of feedback documents, which may further impro ve performance of RMM.

To better understand the dif ference between our method and the simple mixture model, we further studied in detail two extreme queries (i.e., query 447 and query 312). Query 447 ( X stirling en-gine X ) is an example query on which our model is signicantly better than the simple mixture model, while Query 312 ( X hydro-ponics X ) is one on which our model is signicantly outperformed by the simple mixture model. In Table 5, we sho w the learned query models for both SMM-x ed and RMM.

SMM-nal is the nal model from SMM after interpolation with the original query model with an optimal mixing coef cient (0.1). Since the optimal mixing coef cient has a small value, it is not sur -prising that the original query words are quite dominant in SMM-nal. RMM-nal is the estimated query model using RMM. As expected, the coef cient of the implicit interpolation is around 0.5. We also sho w the initial query model for RMM (RMM-init), which is mostly the original query model. Comparing RMM-nal with SMM-nal for Query 447, we see that RMM-nal has included more discriminati ve words in the model, while SMM-nal tends to be dominated by some general terms, which may explain why RMM outperformed SMM. In the case of Query 312, howe ver, it is unclear why RMM-nal has performed worse than SMM-nal. It seems that neither has pick ed up man y good terms for this query; instead the y both have some distracting terms. Since RMM has assigned higher weights to the expanded terms than SMM, these distracted terms may hurt the performance more than in the case of SMM.
In this paper , we present a rob ust method for pseudo feedback based on statistical language models. The new method uses the original query model as a prior on the feedback language model to be estimated, which not only reparameterizes the interpolation with a more meaningful parameter , but also allo ws us to regularize the estimation of the feedback language model so that useful infor -mation from the feedback documents would be gradually incorpo-rated into the original query language model. We further propose a regularized EM algorithm for estimating the parameters, which includes the follo wing two regularization strate gies: (1) gradual discounting the prior condence, and (2) stopping the algorithm based on the amount of rele vance information extracted from feed-back documents (controlled by a parameter). Experiment results on several representati ve data sets sho w that the new method is more rob ust than a state-of-the-art baseline language modeling approach for feedback with comparable or better retrie val accurac y.
The rob ustness of the proposed model is partly from using a con-serv ative stopping condition. Ho we ver, as indicated in the analysis of the upper bound of the proposed model, when the feedback infor -mation is reliable, it is desirable to allo w more feedback informa-tion to be incorporated into the query model. The right amount of feedback information to incorporate is presumably associated with the precision of the feedback documents, which is further related to query dif culty . A very interesting future research direction is to further study how to measure the quality of feedback documents and allo w some exibility in combining the query model prior with feedback information accordingly .
This material is based in part upon work supported by the Na-tional Science Foundation under award number IIS-0347933. We thank the anon ymous SIGIR 06 revie wers for their useful com-ments.
