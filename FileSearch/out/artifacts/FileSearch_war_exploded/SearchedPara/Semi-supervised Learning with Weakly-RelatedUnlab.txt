
Carnegie Mellon University Semi-supervised Learning ( SSL ) takes advantage of a large amount of unlabeled data to enhan ce classification accuracy. Its application to text categoriz ation is stimulated by the easy availability of an overwhelming number of unannotated web pages, in contras t to the limited number of annotated ones. Intuitively, corpora with different topics may not be content wise related, however, word usage exhibits consistent patterns within a language. Then the qu estion is, what would be an effective SSL strategy to extract these valuable word usage patterns embe dded in the unlabeled corpus? In this paper, we aim to identify a new data representation, that is o n one hand informative to the target class (category), and on the other hand consistent with the f eature coherence patterns exhibiting in the weakly related unlabeled data. We further turn it into a c onvex optimization problem, and solve it efficiently by an approximate approach. In this section, w e first review the two types of semi-supervised learning: transductive SSL and inductive SSL. T hen we state SSL with weakly related unlabeled data as a new challenge. Finally, we provide a stra tegy of how to address this challenge in the domain of text categorization, as well as a brief summary of related work in text categorization. A variety of methods have been developed for transductive SS L [14, 21]. These methods can be grouped as: EM with generative mixture models, bootstrap ping methods (Self-training, Co-training and the Yarowsky Algorithm), discriminative mode ls (Transductive Support Vector Ma-chines (TSVM) [2]) and data based methods, including Manifo ld Regularization [1], Information Regularization [17], and Low Density Separation(LDS) [11] . Specifically, TSVM extends the max-imum margin principle of SVM to unlabeled data. It combines t he regularization of SVMs on the labeled points with the cluster assumption on the unlabeled points, to enforce the decision bound-ary to lie in low density regions. Data based methods discove r an inherent geometry in the data, and exploit it in finding a good classifier, to which additiona l regularization based on unlabeled data is added to avoid overfitting. Manifold Regularization uses the combinatorial Laplacian as a smoothness term. Based on the assumption that different cla sses usually form separate manifolds, it constructs decision functions that vary little along the data manifolds. Information Regulariza-tion seeks a good conditional Pr( y | x ) , assuming that the decision boundary lies in a low density area and Pr( y | x ) only varies a little in the area of high density. Low Density S eparation makes a similar assumption as Manifold Regularization and Informa tion Regularization. In addition, it fur-ther computes a new data representation based on the unlabel ed data, which often results in better classification performance for SSL.
 Not many inductive SSL approaches have been presented. In ge neral, the essential distinction be-tween transductive learning and inductive learning is that transductive learning produces labels only for the available unlabeled data; while inductive learning not only produces labels for the unlabeled data, but also learns a classifier that can be used to predict l abels for new data. In this sense, some SSL algorithms, though named as  X  X ransductive X , have an ind uctive nature. For example, TSVM is an inductive learner, because it learns a classifier from a mixture of labeled and unlabeled data. Similarly, as an inductive component of Low Density Separat ion (LDS) [11],  X  TSVMs learns the SVM classification model in the primal, which can be used for p redicting new data. However, the graph part of LDS is transductive, because the kernel and the graph distances are addressed by a prior eigen-decompostion and re-representation (MDS); th us, it is unclear how to make a prediction of a new test point other than by rebuilding the graph with the new test point. Manifold Regulariza-tion [1] also has an implementation with inductive nature. H armonic Mixtures [22] is a recent work that aims to overcome the limitations of non-inductive infe rence. It models the data by a generative mixture of Gaussians, and adds discriminative regularizat ion using the graph Laplacian. In this paper, we focus on inductive SSL. In contrast to previ ous work in this area, we focus on the following important problem that has been overlooked befor e. As stated in [11], either directly or indirectly, all successful semi-supervised algorithms ty pically make the cluster assumption, which puts the decision boundary in low density areas without cros sing the high density regions. Note that the cluster assumption is only meaningful when the labeled a nd unlabeled data are somehow closely related. When the unlabeled data comes from arbitrary data s ources, their input patterns may not be closely related to that of labeled ones. As a result, the la beled and unlabeled data could be well separated, which makes it difficult, if not impossible, to ex ploit the cluster assumption. Hence, the key challenge is how to leverage the seemingly unrelated unl abeled data to improve the classifica-tion accuracy of the target classes. Analogous to transfer l earning in which information from one category may be generalized to the others, we propose a schem e that helps the categorization of one data source, by making use of information from other unlabel ed data sources with little relevance. Our study stands in contrast to the previous ones in that we ai m to make maximum use of the un-labeled data that is weakly related to the test bed. We refer t o this problem as  X  SSL with weakly related unlabeled data  X , or SSLW for short. We first build a maximum margin framework for SSL with weakly related unlabeled data. We then cast the framework into an Se cond Order Cone Programming (SOCP) problem that can be efficiently solved.
 A typical approach for semi-supervised learning with weakl y related unlabeled data, presented in the recent study [13] is to first derive a new data representat ion from unlabeled data, and then apply supervised learning technique to the derived new data repre sentation. In [13], the authors proposed a SSL scheme termed as self-taught learning, which essentia lly conducts the unsupervised dimen-sion reduction using sparse coding [10]. The new dimensions derived from the unlabeled data can then be used to represent the labeled data points for supervi sed learning. Notably, self-taught learn-ing [13] performs coding and classification in two separate s tages. In contrast, in our method, the construction of a good data representation is combined with the training of a maximum margin clas-sifier under a unified framework. In particular, the data repr esentation generated by our method exploits both labeled and unlabeled data, which differenti ates the proposed framework from self-taught learning.
 In general, SSLW could improve a wide range of classification tasks. However in this study, we focus on text categorization with a small training set. Text categorization has been actively studied in the communities of Web data mining, information retrieva l and statistical learning [9, 20]. A number of statistical learning techniques have been applie d to text categorization [19], including the K Nearest Neighbor approaches, decision trees, Bayesia n classifiers, inductive rule learning, neural networks, support vector machines (SVM), and logist ic regression. Empirical studies [7] have shown that support vector machines (SVM) is the leading technique for text categorization. Given the limited amount of labeled documents, the key of sem i-supervised text categorization is to exploit the unlabeled documents. The popular implementati ons of semi-supervised SVMs in [8, 15] are considered to be state-of-the-art in text categorizati on.
 For text categorization with a small training pool, it is ver y likely that a large portion of words used by the testing documents are unseen in the training set, whic h could lead to a poor estimation of the similarity between documents. If we can identify the cohere nce information of words (e.g., word correlation) from both the labeled and unlabeled documents , we will be able to more accurately estimate the document similarity, particularly for docume nts sharing few or no common words, thus improving the overall classification accuracy. A straightf orward approach is to utilize the word co-occurrence information for computing document similarity . However, this straightforward approach may not serve the best interests of word correlation, becaus e not all of the co-occurrence patterns are useful. Some co-occurrence patterns (e.g., co-occurre nce with common words) do not reflect the semantic relations among words, and some are not related to the target class. Consequently, it is critical to identify a subset of co-occurrence pattern s that are most informative to the target classification problems. To address this problem, SSLW expl icitly estimates the optimal word-correlation matrix for the target document categorization problem. The rest of paper is organized as follows. Section 2 introduces the basic notations and giv es a brief review of the SVM dualism. In Section 3, we propose the framework of SSL with weakly-rel ated unlabeled data, followed by an efficient algorithm for its computation in Section 4. Sectio n 5 evaluates SSLW; and in section 6 we provide some insights into the experimental evidence and di scuss future work. We introduce the notation used throughout this paper and bri efly review the SVM dual formulation. Denote L = { ( x document x problem for multi-labeled documents can be treated as a set o f independent binary classification problems). Let U = { x size of the vocabulary. Importantly, as an SSL task with weak ly-related unlabeled data, U comes from some external resources that are weakly related to the t est domain. To facilitate our discussion, we denote the document-word matrix on L by D = ( d the word-frequency vector for document d all the n documents. Recall the dual formalism for SVM: where  X  = (  X  with all elements being 1 , and the symbol  X  denotes an element-wise product between two vectors. K  X  R n  X  n is the kernel matrix representing the document pairwise sim ilarity and K = D &gt; D . In this section, we present the algorithm of Semi-supervise d Learning with Weakly-Related Unla-beled Data (SSLW). As analysized in Section 1, the kernel sim ilarity measure in the standard SVM dual formalism K = D &gt; D , is problematic in the sense that the similarity between two documents will be zero if they do not share any common words, even if ther e exists a pairwise relationship be-tween the seen words and the unseen ones, from a large collect ion of documents. To solve this prob-lem, we take into account a word-correlation matrix when com puting the kernel similarity matrix, and we search for an optimal word-correlation matrix, towar ds maximizing the categorization mar-gin. Specifically, we define the kernel matrix as K = D &gt; RD , by introducing the word-correlation matrix R  X  R V  X  V , where each element R words. Note G &gt; G is not a desirable solution to R , because it is improper to assign a high corre-lation to two words simply because of their high co-occurren ce; the two words may be not closely related as judged by the maximum-margin criterion. Therefo re, it is important to search for the opti-mal word-correlation matrix R in addition to the maximum discovered in Eqn. (1), to maximiz e the categorization margin. We denote the optimal value of the ob jective function in Eqn. (1) as  X  ( K ) : Given the fact that  X  ( K ) is inversely-related to the categorization margin [4], min imizing  X  ( K ) is equivalent to maximizing the categorization margin.
 Now we consider how to make maximum use of the weakly-related source U . The G matrix is crucial in capturing the word correlation information from the weak ly-related external source U . Thus, to incorporate the external source into the learning of the wor d-correlation matrix R , we regularize R according to G by introducing an internal representation of words W = ( w vector w factorization (NMF) [6]). We expect that W carries an equivalent amount of information as G does, i.e., G and W are roughly equivalent representations of words. As there e xists a matrix U such that the matrix G can be recovered from W by a linear transformation G = U W , the word-correlation matrix can be computed as R = W &gt; W . Further, the constraints G = U W and R = W W &gt; can be combined to obtain the following positive semi-definite con straint where T = U U &gt; [18]. Another strategy we use to involve the unlabeled data i nto the learning of word correlation, is to construct the word correlation matr ix R as a non-negative linear combination of the top p right eigenvectors of G , i.e., where { s of their eigenvalues  X  non-negative combination weights. Note that introducing  X I R , which is important when computing the expression for matri x T ). This simplification of R al-lows us to effectively extract and utilize the word co-occur rence information in the external source U . Additionally, the positive semi-definite constraint R 0 is converted into simple non-negative constraints, i.e.,  X   X  0 and {  X  O ( V 2 ) , is now reduced to p + 1 . A further insight into the combination weights reveals tha t, both the straightforward co-occurrence matrix G &gt; G and Manifold Regulization, give predefined weights for eigenvector combination and thus can be seen as the speci al cases of SSLW. Precisely speak-ing, the straightforward co-occurrence matrix G &gt; G , directly uses the eigenvalues as the weights. Manifold Regularization does a slightly better job by defini ng the weights as a strict function of the eigenvalues. Different from both, we give SSLW the entire fr eedom to learn the weights from data. In this sense, SSLW generalizes these two methods.
 Based on the above analysis, we reformulate an extension of S VM dual in Eqn. (1), to search for an optimal word-correlation matrix R , by exploiting the word co-occurrence information in the ex ternal U , under maximum-margin criterion, i.e., where the word-correlation matrix R is restricted to domain  X  that is defined as if we use (3) for R , and if we use Eqn. (4) for R . Given the definition of  X  in Eqn. (2), Eqn. (5) is the following min-max problem without analytic solution. This section provides a computationally-efficient and scal able algorithm for solving the min-max problem in Eqn. (8), with domain  X  defined in (6). We first rewrite the maximization problem in Eqn. (1) into a minimization problem by computing its dual fo rm: Then, by plugging Eqn. (9) back into Eqn. (5), we transform th e min-max problem in Eqn. (8) into the following minimization problem: Note that as our goal is to compute R and T , thus any valid ( W, U ) is sufficient, and no uniqueness constraints are imposed on W and U .
 In Eqn. (10), C improve the stability of the optimal solution, as well as to f avor a simpler model over sophisticated ones. The parameters C The trace heuristic has been widely used to enforce a low-ran k matrix by minimizing its trace in place of its rank. In the generalization of the trace heurist ic presented by [5], the dual of the spectrum norm is the convex envelope of the rank on the set of matrices w ith norm less than one. The rank objective can be replaced with the dual of the spectral norm, for rank minimization. In other words, the best convex regularizer one can get for rank minimizatio n is the trace function.
 Eqn. (10) is a Semi-Definite Programming (SDP) problem [3], a nd in general can be solved using SDP packages such as SeDuMi [16]. However, solving a SDP prob lem is computationally expensive and does not easily scale to a large number of training exampl es. [18] recently provides an elegant scheme of rewriting a SDP problem into a Second Order Cone Pro gramming (SOCP) problem that can be much more efficiently solved [3]. Technically, we adop t this procedure and rewrite Eqn. (10) into a typical SOCP problem that can be efficiently solved. Gi ven the estimated word-correlation matrix R and K = D &gt; RD , the example weights  X  in SVM model can be estimated using the KKT conditions  X  = ( yy &gt;  X  K )  X  1 ( e +  X   X   X  +  X  y ) . And the threshold b in SVM can be obtained by solving the primal SVM using the linear programming techniq ue. In this section, we evaluate SSLW on text categorization wit h limited training data. The experiment set up is purely inductive, i.e., the testing feature space i s invisible in the training phrase. As an SSL task with weakly-related unlabeled data, the provided unla beled data have little relevance to the test domain. We show that SSLW can achieve noticeable gains over t he state-of-the-art methods in both inductive SSL and text categorization, and we provide insig ht into why this happens. Following [18], our implementation of SSLW selects the top 200 right eigenvectors of the document-word matrix G matrix to construct the R matrix. As defined in Section 3, the G matrix covers both the training sets and the weakly-related external collection.
 Evaluation datasets Two standard datasets for text categorization are used as th e evaluation test bed: the Reuters-21578 dataset and the WebKB dataset. For comput ational simplicity, 1000 documents are randomly selected from the TREC AP88 dataset and are used as an external information source for both datasets. The AP88 dataset includes a collection of news documents reported by Associated Press in 1988. The same pre-processing and indexing procedu re are applied to these three datasets, by using the Lemur Toolkit 1 . For the Reuters-21578 dataset, among the 135 TOPICS categories, the 10 categories with the largest amount of documents are selecte d (see Table 1). This results in a collection of 9 , 400 documents. For the WebKB dataset, which has seven categorie s: student, faculty, staff, department, course, project, and other, we discard the category of  X  X ther X  due to its unclear definition (see Table 2). This results in 4 , 518 data samples in the selected dataset. The Reuters-21578 dataset and the TREC AP88 dataset have very li mited relevance in topic; and the WebKB dataset and the TREC AP88 dataset are even less content -wise related.
 Table 1: The ten categories of the Reuters-21578 dataset wit h the largest amount of documents. Evaluation Methodology We focus on binary classification. For each class, 4 positive samples and 4 negative samples are randomly selected to form the training set; and the rest of the data serve as the testing set. As a rare classification problem, the test ing data is very unbalanced. Therefore, we adopt the area under the ROC curve (AUR) [12] as the quantit ative measurement of the binary classification performance for text categorization. AUR is computed based on the output of real-value scores of the classifiers returned for testing documen ts. Each experiment is repeated ten times, and the AUR averaged over these trials is reported.
 Baseline Methods We use six baseline methods to demonstrate the strength of SS LW from dif-ferent perspectives. The first two baselines are the standar d SVM and the traditional TSVM.The third baseline is  X  TSVM 2 , the inductive component of LDS, which delivers the state-o f-the-art performance of SSL. The fourth baseline Manifold Regulariz ation 3 ( ManifoldR for short) is in-cluded as a state-of-the-art SSL approach with an inductive nature, and more importantly, being able to incorporate word relationship into the regularizat ion. For the fifth baseline, we compare the word-correlation matrix estimated by SSLW, with the trivia l word-correlation matrix G &gt; G ; and we name this baseline as COR . Finally, self-taught learning [13] serves as our sixth bas eline method, named as Self-taught . It uses the unlabeled data to find an low-dimension represen tation, and then conducts standard classification in this new space.
 Text Categorization with Limited Training Data We describe the AUR results of both the Reuters-21578 dataset and the WebKB datset, by using different metho ds. For the Reuters-21578 dataset, Table 3 summarizes the AUR comparison between the six baseli ne methods and SSLW. Both mean and variance of AUR are shown in the table. We observe that SSL W consistently outperforms the six baselines in AUR across most of the ten categories. In genera l, a t-test shows our performance gain is is provided below. First, TSVM and  X  TSVM overall perform even worse than the standard SVM. This observation reveals that if the unlabeled data are only weakly relevant to the target class, it could harm the categorization accuracy by simply pushing the deci sion boundary towards the low density regions, and away from the high density areas of the unlabele d data. It also justifies our intuitive hypothesis that the cluster assumption is not valid in this c ase. Second, the dramatic advantage of SSLW over the COR method confirms our previous analysis  X  le arning a good word-correlation matrix that is jointly determined by the co-occurrence matr ix and the classification margin (as SSLW does), can achieve significant gains over simply using the tr ivial form G &gt; G . Third, we observe that SSLW algorithm consistently improves over Manifold Regula rization, except on  X  X race X  category where ManifoldR has a little advantage. Most noticeably, on  X  X heat X  category and  X  X hip X  category, the AUR is improved by more than 10% , as a result of SSLW. These results demonstrate that SSLW is effective in improving text categorization accuracy wit h a small amount of training data. We also notice that,  X  TSVM outperforms TSVM on some categories, but is slightly wo rse than TSVM on some others. The unstable performance of  X  TSVM can possibly be explained by its gradient descent nature. Finally, our method receives gains against self-ta ught learning [13] on most categories. This proves SSLW is more effective than self-taught learnin g in using unlabeled data to improve classification. The gains can be attributed to the fact that S elf-taught does coding and classification in two separate stages, while SSLW achieves these two purpos es simultaneously.
 A more careful examination indicates that SSLW also reduces the standard deviation in classification accuracy. The standard deviations by SSLW are mostly less th an 2 . 5% ; while those by the baseline methods are mostly above 2 . 5% . Over all the ten categories except the  X  X oney-fix X  category , SSLW always delivers the lowest or the second lowest standard dev iation, among all the six methods. We hypothesize that the large standard deviation by the baseli ne models is mainly due to the small number of training documents. In this situation, many words should only appear in a few training documents. As a result, the association between these words and the class labels can not be reliably established. In extreme cases where these words do not appea r in any of the training documents, no association can be established between these words and the c lass labels. Evidently, test documents related to these unseen words are likely to be classified inco rrectly. By contrast, SSLW can resolve this problem by estimating the word correlation. For a missi ng word, its association with the class label can be reliably estimated through the correlation wit h other words that appear frequently in the training examples.
 Table 4 shows the AUR results of the WebKB dataset, from which we observe the similar trends as described above in the Reuters-21578 dataset. It is shown th at SSLW maintains its clear advantage over the six baseline methods, across all the six categories .
 Table 3: The AUR results ( % ) on the Reuters-21578 dataset with 8 training examples per category. This paper explores a new challenge in semi-supervised lear ning, i.e., how to leverage the unlabeled information that is weakly related to the target classes, to improve classification performance. We propose the algorithm of Semi-supervised Learning with Wea kly-Related Unlabeled Data (SSLW) to address this challenge. SSLW extends the theory of suppor t vector machines to effectively iden-tify those co-occurrence patterns that are most informativ e to the categorization margin and ignore those that are irrelevant to the categorization task. Appli ed to text categorization with limited num-ber of training samples, SSLW automatically estimates the w ord correlation matrix by effectively exploiting the word co-occurrence embedded in the weakly-r elated unlabeled corpus. Empirical studies show that SSLW significantly improves both the accur acy and the reliability of text catego-rization, given a small training pool and the additional unl abeled data that are weakly related to the test bed. Although SSLW is presented in the context of text ca tegorization, it potentially facilitates classification tasks in a variety of domains. In future work, we will evaluate the benefits of SSLW on larger data sets and in other domains. We will also investiga te SSLW X  X  dependencies on the number of eigenvectors used, and its behavior when varying the numb er of labeled training examples. Acknowledgments The work was supported by the National Science Foundation (I IS-0643494) and National Institute of Health (1R01GM079688-01). Any opinions, findings, and co nclusions or recommendations ex-pressed in this material are those of the authors and do not ne cessarily reflect the views of NSF and NIH.

