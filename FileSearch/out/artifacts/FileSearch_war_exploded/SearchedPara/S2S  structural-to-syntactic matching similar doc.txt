 Ramazan S. Ayg X n Abstract Management of large collection of replicated data in centralized or distributed environments is important for many systems that provide data mining, mirroring, storage, and content distribution. In its simplest form, the documents are generated, duplicated and updated by emails and web pages. Although redundancy may increase the reliability at a level, uncontrolled redundancy aggravates the retrieval performance and might be useless if the returned documents are obsolete. Document similarity matching algorithms do not pro-vide the information on the differences of documents, and file synchronization algorithms are usually inefficient and ignore the structural and syntactic organization of documents. In this paper, we propose the S2S matching approach. The S2S matching is composed of structural and syntactic phases to compare documents. Firstly, in the structural phase, documents are decomposed into components by its syntax and compared at the coarse level. The structural mapping processes the decomposed documents based on its syntax without actually mapping at the word level. The structural mapping can be applied in a hierarchical way based on the structural organization of a document. Secondly, the syntactic matching algorithm uses a heuristic look-ahead algorithm for matching consecutive tokens with a verification patch. Our two-phase S2S matching approach provides faster results than currently available string matching algorithms.
 Keywords String matching  X  Information retrieval 1 Introduction With the progress of technology and the cost reduction of storage, update, and transmission of data, the information is available in many ways to the public. People can subscribe to web pages, newsgroups, and email groups at their will with the anticipation of attractive information. If someone uses  X  X oogle X  to search data on the web, it is likely to get almost identical documents for a set of keywords. Information retrieval systems are eager to return all the duplicates and versions of documents whether they are obsolete or not. Nowadays, the number of available documents to the information retrieval (IR) systems has been enormous. It is left to the user to identify and filter the necessary information. 1.1 Motivation The amount of digitally available information has increased with the support of database sys-tems, information retrieval systems, web servers, and email servers. The information can be copied, updated, and transmitted to different resources. Although the systems can synchro-nize with each other (i.e., the files at different systems are synchronized), the meaningful difference between documents are important for users to make critical decisions. We are particularly interested in the following applications: (a) Email and Newsgroups . The users may receive duplicates of messages when there are (b) Replication in P2P systems . The peers in P2P systems [ 17 ] can search data at different (c) Data mining and information retrieval . The performance of data mining and informa-(d) Subscription systems. The browsers such as Internet Explorer allow users to subscribe (e) Web crawling. There has been tremendous effort to synchronize large sets of web pages
The common theme in these applications is the presentation of differences among similar data before proceeding to update, copy, or delete data. Our goal is to provide an efficient preliminary method to present the differences among similar documents. 1.2 Related work The problem that we investigate is close to finding the longest common subsequences between two strings S and T. The longest common subsequence is a state of the art similarity mea-sure for sequences, and is widely used in sequence related tasks [ 26 ]. The traditional string processing algorithms do not assume anything on the similarity of strings. In other words, these algorithms have to consider worst cases where the input strings are dissimilar.
The dynamic time warping is a dynamic programming approach to determine the similar-ity between two time series [ 21 ]. The typical distance between two time series is determined by the following formula: where function d is a distance function that depends on the application for time series S and P ;and D is a distance matrix to be used for dynamic programming. In this paper, our method for structural mapping is also a kind of dynamic time warping algorithm. However, the initial conditions and the distance functions are generated to serve our purposes.

The document similarity methods usually return the similarity of two documents. How-ever, they do not return any information on what is similar and what is not. String distances are usually expensive to compute in large databases where each document may contain thousands of words. The most efficient document retrieval methods use latent semantic indexing (LSI) based on frequency matrices [ 9 , 10 ]. These methods provide fast document similarity mea-sures. In [ 7 ], two measures on the resemblance and containment of documents are proposed that corresponds to  X  X oughly the same X  and  X  X oughly contained X . Since traditional string distance measures (Hamming, Levenstein, etc.) are usually expensive to compute, they use these new measures based on shingles (the length of consecutive tokens). However, their algorithm does not state the actual differences between documents.

The granularity of alphabets affects the performance of the algorithms. Although text documents are composed of characters, characters are not adequate to evaluate to obtain meaningful differences between two strings. The diff application that compares two docu-ments using an efficient longest common subsequence algorithm is an example state-of-art technique for text differencing [ 23 ]. The granularity of diff command in UNIX is a line. Each line is treated as a token (alphabet). Even a word is shifted in each line to the next line, the documents will be totally treated as different documents. The diff command aims to reduce the number of changes to convert one document to another. There are also other comparison algorithms like bdiff and vcdiff [ 13 , 15 ]. The algorithms are usually studied under delta com-pression techniques [ 13 , 15 , 16 ]. The delta coding is used to convert one file to another based on differences and distances between two files.

The rsync algorithm [ 24 , 25 ] is commonly used to synchronize files. It serves to synchro-nize files that may exist in any format by dividing first into blocks and then generating hash keys. The hash keys are matched in the new file to synchronize two files. The update informa-tion is reported in terms of blocks which may not carry semantic information for meaningful documents. The rsync algorithm focuses on the reduction of the number of bits transferred and it does not have any good performance bounds with respect to edit distance [ 22 ]. Even if there is a single mismatch within a block, that is considered as a different block. 1.3 Our approach In this paper, we propose the S2S matching that is composed of structural and syntactic evaluations to compare the similar documents. Our goal is not to give the longest common subsequence that returns the minimum editing distance. Our goal is also not to convert a document to another efficiently. Our goal is to return the meaningful differences between similar documents that can be used in information filtering.

In our algorithm, we do not treat every appearance of a member of the alphabet in the same way. We assume that the strings have structure and closeness of appearances (or existence of an appearance in the same component) may affect the matching process. Since string matching is a costly process, we firstly determine the similar documents by using document similarity measures and then apply our S2S matching algorithm. Our syntactic evaluation algorithm uses a look-ahead heuristics for two consecutive matches. The document is virtu-ally divided into components (sentences in text documents) and the appearance of an alphabet (e.g. words) in different components (e.g. sentences) can be considered as different. Figure 1 depicts the steps of our approach. We have performed our experiments on text documents.
Our contributions can be listed as follows:  X  S2S: Two-phase matching process is composed of structural mapping and syntactic eval- X  Structural mapping algorithm is optimized and has O ( n ) complexity.  X  Syntactic matching avoids the incorrect matching of documents in different components
Since our algorithm is a heuristic algorithm, there might be cases where the output is not semantically correct. In those cases a worst-case algorithm should process the original data or another efficient algorithm including this one can reevaluate the generated output. But this is not discussed in this paper. We make sure that the complexity of the algorithm is O ( n ) by considering similar documents, so it will not deteriorate the overall complexity. This paper is organized as follows. The following section explains the background on string matching. Our S2S approach is summarized in Sect. 3 . Structural evaluation is discussed in Sect. 4 . Syntactic evaluation is explained in Sect. 5 . Section 6 discusses our experiments and gives analysis of our algorithm. Section 7 discusses limitations and future work. The last section concludes our paper. 2 Background on string matching and file synchronization Let S = s 0 s 1 ... s n  X  1 and T = t 0 t 1 ... t m  X  1 be two strings defined over alphabet .The where 0  X  a &lt; b  X  n . We consider three edit operations: deletion, insertion, and substitu-tion. In this paper, when comparing S and T , if a substring of S does not appear in T ,itis shows a sample sequence matching. The longest common subsequence (LCS) is axyz for this example. If we want to convert S to T , remove fbc from S and substitute (second) a with bc . To convert T to S , we would insert fbc between a and x and substitute bc with a . In terms of the number of operations, only two operations are needed to convert S to T and T to S . The edit distance between S and T is 5 (e.g, insert fbc ; substitute a with c ;andinsert c ). The dist (S,T) gives the edit distance between S and T.

The granularity of alphabet is important in assessment of similarities and differences between two strings. If the semantics is not important and if the consideration is to mini-mize the number of operations to convert a text to another, then the alphabet may consist of lines in text documents as in diff program. This will lead to a fast comparison but incorrect interpretation of differences between strings. For example, consider the documents in Fig. 3 . substitute line 1 of S with lines of 1 to 3 of T ; substitute line 3 of S with line 5 of T ;and substitute line 5 of S with lines 7 through 15. The actual semantically correct answer should be to insert first five lines of T into the beginning of S . If the granularity of the alphabet is a character, the program may even consider an extra space or a tab as a mismatch although the rest of the text is the same. This is also an issue with the rysnc [ 24 , 25 ] algorithm.
In our approach, we consider the granularity of an alphabet as a word. From now on, an alphabet corresponds to a word. Throughout paper, we also use the word token to represent the members of . 3S2S The S2S matching approach compares documents in two phases: structural and syntactic. The structural mapping considers the structural organization of documents and does not look into semantics. On the other hand, syntactic evaluation considers matching of the alphabet. In our system, the syntactic matching follows structural mapping. The structural mapping identifies the set of components of the documents where syntactic matching needs to be performed. Since these components are likely to have some similarity, the syntactic matching can be performed very fast.

The structural mapping does not filter out any syntactic meaning. It is just a preprocessing step to map components for syntactic matching. It just reduces the burden on the syntactic matching. For example, assume that there are two versions of a paper: p 1 and p 2 . We would like to see how p 2 is updated from p 1 . In a traditional approach, the complete document is treated as a single string and then compared. However, it is not necessary to compare a word in the abstract with a word in the conclusion of the paper. Our structural mapping indicates that abstract of p 1 should be compared against the abstract of p 1 and the conclusion of p 1 should be compared against the conclusion of p 2 . However, if the documents are similar to each other and moreover, if all the words that are searched exist at a relatively close distance from the beginning of the search point, hierarchical decomposition may not affect (or improve) the performance. For example, if two documents are exactly the same, structural mapping will not improve the performance. However, such cases cannot be determined in advance, and structural mapping improves the performance especially when new words, sentences, and sections are introduced in a new document. If the syntactic mapping corresponds to matching words, word level matching is performed for the corresponding sentences only. Therefore, a word is not searched beyond the end of corresponding sentences. 3.1 Structural mapping Since the text documents are not structured as XML or web documents, the document similar-ity methods such as [ 11 , 14 ] cannot be applied to plain text documents. However, documents can still be divided into smaller components. For a regular document at the high level, a document is composed of sections and sections are composed of subsections. The subsec-tions are composed of paragraphs and paragraphs are composed of sentences. Sentences are composed of words and words are composed of characters. The lowest level for structural mapping is the level of sentences. The actual similarity of sentences can be accomplished by comparing the words in two sentences.

The identification of the lowest level is determined by syntactic matching. For example, if the syntactic equivalence of two paragraphs can be identified by just comparing the sentences (but without looking into words in the sentences), the lowest level for structural mapping is a paragraph. The number of levels of decomposition of a document depends on the type of a document. The structural mapping can also be performed at multiple levels in a hierarchical way.

It should be noted that the identification of structural organization of a document is domain dependent. The domain of a document might be a book, report, journal (or conference) paper, survey, and test. Some documents may use different fonts to identify section headings while some others might use numbering for section titles. However, sentence and word level opera-tions are not domain dependent. Starting from the sentence level is applicable to all domains.
The matching always starts from the structural mapping and then syntactic mapping fol-lows structural mapping. The structural mapping might have different levels such as sections, paragraphs, and sentences. The structural mapping also starts from the highest level possible. If section and sentence are two levels available for structural mapping, the structural mapping starts from section level mapping followed by the sentence level.

In our case, the documents are decomposed into components (in our case, sentences) by using delimiters such as  X . X ,  X ? X , and  X ; X  symbols. For each string S , we keep an array for delimiter characters. The structural mapping requires the mapping of components at the structural level without considering any semantics. If the mapping levels are sentences, the sentences of two documents are mapped. Figure 4 depicts the structural mapping of two documents.

The mapping is not always 1  X  1 mapping. Sometimes, there is no correspondence and this corresponds to insertion or deletion of a component. In some cases, component may be divided into components or a set of components is united as a single component. At the structural mapping level, it is not possible to judge on insertions or deletions. The structural mapping always assigns a corresponding component and it is the responsibility of syntactic matching to determine insertions and deletions.
 3.2 Syntactic matching We realize two issues when we process syntactic matching. Firstly, some sets of characters like white space, tab, and extra lines do not affect the semantics of the document. Secondly, an occurrence of a word in another sentence cannot always be considered as a correct match. Especially consider articles a and the .Anoccurrenceof the in another sentence is probably a different occurrence.

If a word, w , is appearing more than k sentences later where k is a threshold, that appear-ance can be treated as an incorrect match. Normally, it is reasonable to consider only matches within a sentence. However, the cases where a sentence is divided into multiple sentences or multiple sentences are united into a single sentence might occur and these cases might be missed. In the example shown in Fig. 5 , the word nice in t is matching with the word nice in the second sentence of s . Although this match appears in different sentences, this is a correct match. We provide reliable algorithms in Sects. 4 and 5 . 4 Structural mapping Assume the matching of two documents V and W where | V |= n and | W |= m .If n and m are huge numbers, the matching with algorithms having O ( mn ) complexity is costly. We will use the fact that all documents are structured. A document can be considered as a hierarchical organization of syntactic information. At the high level there are sections, subsections, and so on. We know not all documents are properly hierarchically organized. But we definitely know that all documents are composed of sentences. Structural mapping algorithm returns all the mappings to be evaluated by syntactic matching. Since the problem is divided into smaller problems after mapping, parallel processing algorithms can benefit the mappings of structural mapping.

Let V = v 0 v 1 ...v g  X  1 and W = w 0 w 1 ...w h where v i and w i denote the i th components of V and W , respectively (Note: when structural mapping is considered, all indices refer to components rather than alphabets). Let | v i | denote the length (the number of words in v i ). If | v i |=| w j | , we can state that the number of insertions and deletions is equal. The length of a component plays an important role whether they are similar or not. Depending on the length of components an approximate cost function can be determined as follows: where C is a cost matrix and C i , j is obtained by adding the minimum insertion/dele-tion distance for the i th and j th component. The minimum insertion/deletion distance is minI ns Del Dist (v i ,w j ) =|| v i | X  X  w j || . This algorithm forces matching components. In other words, when two components are compared, the distance is the difference in the num-ber of words. This formula resembles dynamic time warping formulas. However, this type of application of dynamic warping is new for string matching, since in string comparison, words are matched; not the length of the components.

If we consider the insertion or deletion of sentences in distance function, then it should be modified as follows:
Please note that initial formulation does not ignore insertions and deletions. Deletions and insertions are detected at the syntactic matching level. In the second formulation, it says that the distance between two sentences whose lengths are 5 and 40 is min ( 5 , 40 , 35 ) = 5that corresponds to deletion/insertion of the smaller sentence.

However, the previous formulation does not indicate the relationship between the pre-vious cost value and the new additional cost. If the diagonal value is the minimum value, then the substitution is preferred. If the upper or left value is the minimum value, there is insertion/deletion. The previous equation is updated as follows: In our experiments, we have used the third equation.

Ta b l e 1 shows a portion of a sample cost matrix. The first row shows the length of sen-tences in W and the first column shows the length of sentences of V . After the cost matrix is generated, the matrix needs to be traced back to identify the components to be mapped. We use two one-dimensional arrays for each string. The bold values in Table 1 show the minimum values. Whenever a value is computed for a matrix location, we also maintain the previous location (i.e. diagonal, left, or upper) in the matrix that leads to this new value.
The TraceMatch algorithm given in Algorithm 4.1 starts from the bottom-right corner and traces for the minimum value in the direction of top-left corner. Since the size of the cost matrix is ( g + 1 , h + 1 ) , the indices of mapping sentences is 1 less than the indices of the cost matrix. The Z values keep the mapping components of two documents. In other words, the component Z 1 ( i ) of W is mapped to Z 2 ( i ) in V . Note that Z ( i )  X  Z ( i  X  1 ) .Table 2 shows the mapping for the cost matrix given in Table 1 . Depending on this table, { v 0 ,w 0 } , { v { v 12 ,w 12 } can be considered as 1  X  1 mapping. The v 7 is mapped to multiple components in W . This is a possible indication of addition of sentences or splitting of a component. In this case, we map { v 6  X  v 8 ,w 6  X  w 9 } . The sentences at the boundaries must also be included in this mapping.
 Algorithm 4.1 The implementation of T raceMatch algorithm.
 Procedure T raceMatch ( C , V , W ) 4.1 Optimization of structural mapping The complexity of determining the mappings is O ( gh ) . If the time complexity is O ( n ) for regular syntactic matching, the time complexity of S2S matching would be O ( gh +  X  n ) where  X  is the number of mappings (for Table 2 ,  X  = 12). Although this is faster than matching word by word, the gh factor may lead to O ( g 2 ) factor if g  X  h . If this is the dominating factor, it will have a quadratic complexity. In Table 1 , bottom-left and upper-right corners have the maximum values (Table 3 ). Those corners state that there are no sentences that could be mapped. We do not need to compute every value of the cost matrix. Especially, if the number of subcomponents (e.g. sentences or words) matches exactly, there is no need to match these components against other components. The components may also match from the end of the document. The components that match from the end of the document is marked and our algorithm is forced to match those components matching exactly at the end. Definition 4.1 P ( C , r , c ) is called partial cost matrix of C where 0  X  i  X  r and 0  X  j  X  c and P ( C , r , c ) i , j = C i , j .

Assume that P ( C , r , c ) is the best match for r and c sentences of V and W .Wewantto insert sentence of V (c) delete sentence of V . The heuristics is as follows: if matching is cor-Otherwise there is no match for these sentences and one of these sentences should be ignored. We estimate the other cell and compute the matrix as follows:
The optimized algorithm is given in Algorithm 4.2 . The computation of P ( C , r + 1 , c + 1 ) is based on P ( C , r , c ) and the assumption is that we do not need to compute each cell since the local minimum will be in the neighborhood of the last minimum cell of P ( C , r , c ) .The heuristics is to follow the local minimum. It is very likely that this local minimum will lead to global minimum. We explain the success of this local minima tracking in the experiments. Algorithm 4.2 The algorithm for the optimized structural mapping.
 Function int OptimizedStructuralMapping ( V , W ) 4.2 How to use structural mapping results. This heuristics depends on the minimum insertion/deletion distance between com-ponents. It is good to have an indicator on how to use this heuristics. Standard deviation of length of sentences is a candidate for such an indicator and
If  X ( V ) is close to 0, structural mapping might fail. It means that any component can map to any component.

Assume that we have three different granularities for a document: word, sentence, and section. If structural mapping for sections might fail, structural mapping is only applied at the sentence level. If structural mapping at the sentence level might fail, structural mapping is skipped and syntactic mapping is applied directly. In practice, most documents have com-ponents at varying lengths. We have not encountered a situation where all mappings would fail, because most documents have components at varying lengths.

Although  X ( V ) is a good indicator for small documents, it may lose its meaning for large documents. It is more reasonable to take windows of the document and apply standard deviation for these windows. and where w is the size of the window. To see the use of windows, consider a document where the sentences in the first half have length 10 and have length 100 in the second half. The initial standard deviation would not be able to detect this problem. The windowed standard deviation would identify this problem. 5 Syntactic matching Given two strings s and t where s  X   X  and t  X   X  . The lengths of s and t are denoted number of consecutive matches, Look ahead heuristics (LAH) algorithm attempts to avoid the matching of s i in t after a number of consecutive matches is found. LAH algorithm that checks i consecutive matches will be represented as LAH ( i ) algorithm. We will now define i consecutive matches more formally.
 Definition 5.1 (Consecutive i-match) Assume the matching of s p ... m and t q ... n and let M M afalsematch. 5.1 LAH(1) algorithm Look ahead heuristics (1) will start from t 0 and search t 0 in S until a match is found. It continues with t 1 and so on. In other words, if the algorithm was finding the LCS, first match would be part of LCS. LAH(1) algorithm is satisfied with a match. Consider the example giveninFig. 6 . In this example, t 2 and s 3 are considered as a match in consecutive 1-match. Actually, this is an incorrect match. Although this matching also gives a difference between S and T , it is semantically incorrect.

The algorithm given in Algorithm 5.1 uses two match pointers, S future and T future where the first mismatch occurred since the last correct match in strings S and T , respectively. The number of comparisons required for LAH(1) algorithm for the example given in Fig. 5 is 17. The number of comparisons required for dynamic programming is 48. The dynamic programming approach correctly finds the difference between two strings.
 Algorithm 5.1 The implementation of the LAH(1) algorithm.
 Procedure Match _ LAH _1 ( s , t ) 5.2 LAH(2) algorithm The major drawback of LAH(1) algorithm is the assumption of the first match as the correct match. This problem can be resolved with LAH(2) algorithm. The line t 2  X  s 3 intersects with the line t 4 -s 2 (Fig. 7 ). The match of t 2 and s 3 was an incorrect match.
We use three counters for each string: S ol d , S curr ent ,and S future for string S and the s curr ent = t curr ent and s future = t future ). If the algorithm is iterating on T ,itmustmake sure that S future &gt; S curr ent . If this condition is false, the match for current counters was wrong and the current counters get the values of future counters for each string.
Once the condition ( S future &gt; S curr ent ) is satisfied, we need to update the old and current future counters. The algorithm is given in Algorithm 5.2 .The findMatch procedure finds the first match starting from T future . The number of comparisons that is required is 21 for the example in Fig. 7 .

Since only one string is used for iteration and the other is used for comparison, every match cannot guarantee that it is a correct match. In some cases, especially when there is insertion or deletion, the match might be incorrect.
 Algorithm 5.2 The implementation of the LAH(2) algorithm.
 Procedure Match _ LAH _2 ( s , t )
These cases need to be verified. We use a verification patch to handle these cases. We inves-tigate and solve one major problem during verification: incorrect substring match . Incorrect substring match usually occurs when a replica of a token is inserted. The distances between counters ( T future and T curr ent or S future and S curr ent ) are more than a threshold  X  1 .Let a t d and d  X  c &gt; X  1 or b  X  a &gt; X  1 . This threshold does not guarantee an incorrect match but a possible indication of an incorrect match. This either indicates insertion of t c .. d , deletion
The original LAH(2) algorithm takes one string and matches the tokens of the string in the other. The problem with LAH(2) is matching with a wrong occurrence of a token. If there finding the first match from T curr ent of T and X cur ( = S curr ent + 1 ) of S and to increment X cur until a match is found. Let X 1 (a possible T future ) be the index of the corresponding match in S . If the original match is correct, X cur should be greater than S future .Wealso search the s b  X  1 in T . If the difference between both matches is the same, this indicates the existence of incorrect substring match. To resolve this problem, S future is reset. The algo-rithm is given in Algorithm 5.3 . In the algorithm, the boundary situations are not considered where the counters may be  X  1 (does not exist a match) to keep the algorithm simple. Algorithm 5.3 The implementation of the verification algorithm.
 Function Boolean v er i f y ( s , t , T curr ent , T future , S curr ent , S future )
IN: s = s i s i + 1 s i + 2 ... s j  X  1 and t = t p t p + 1 t p + 2 ... t q  X  1 5.3 Discussion for LAH(k) algorithm There are two stages in the LAH algorithm: matching and verification. The matching compo-nent indicates whether current matching is likely to be correct or not. If k -consecutive match returns true, this is a very good indication that the current match is a good match. However, one of the most important parts of the LAH algorithm is the verification component. For each k , a verification algorithm needs to be developed. For this type of verification, the cases where LAH ( k  X  1 ) fails have to be detected and LAH ( k ) verification process needs to ana-lyze these cases so that LAH ( k ) will have advantage over LAH ( k  X  1 ) . The performance of LAH ( k ) depends on the power of its verification algorithm. In other words, it depends on the interpretation of the output of k -consecutive match when k -consecutive match fails.
If there is repetition of a word within next k words, k -consecutive match fails. Large k increases the probability of having duplicate words within a window of k words. Now con-sider, out of k matchings, one of them is out of order. How should the current match be considered: good or bad? If k is large, it should be a good match. However, as the number of out-of-order matchings increases the decision on a good or bad match becomes more difficult. Moreover, there is no straightforward strategy to handle that. In addition, we do not want to devote computing time to decide a good or a bad match in a more complex environment.
It is possible to ignore verification component to estimate the performance, and an empir-ical ideal k value might be determined. It would be possible to see which k outperforms the other. However, such a statistics or theoretical analysis on k value is not useful because any LAH with verification would outperform all of them.

Our approach on LAH(2) is actually comprehensive consecutive-match and verification algorithm. If the incorrect substring match is considered, our verification algorithm is check-ing actually a window almost any size. Comparison of our algorithm with another LAH(k) lacking verification is not a fair comparison. 5.4 Reporting The report of differences should be given in an easily understandable way. The differences are given in terms of insertion, deletion, and substitution. Sometimes the difference can only be reported one way (e.g. just insertion). It may also be a permutation of substitution and insertion or a permutation of a substitution and deletion. In cases where a permutation is pos-sible, we ignore deletion and insertion, and report the result as only in terms of substitution (e.g. a long string is substituted with a short string). The algorithm for reports is given in Algorithm 5.4 . Algorithm 5.4 The implementation of the reporting.
 Procedure Report ( S , T , p , q , j , k )
Although it is rare, it is possible that structural mapping is not correct. In other words, a sentence is not mapped to the correct set of sentences. Such cases would output insertion and deletion of the same sequence. For example, s 0 = abcde and s 1 = abcde . Assume that structural mapping produces incorrect mapping by mapping abc in s 0 to ab in s 1 ;and de in s 0 mapping ( de  X  cde ), it is reported that c is inserted. However, an example of correct mappings would be mapping ab to ab , and mapping cde to cde . We overcome this problem by checking end of a string, it is not reported and that difference is moved to the beginning of the next mapping. In the original incorrect mapping, c was expected to be deleted. We do not report it right away, but we start the next matching from this point. In other words, c is assumed to be
After the results are reported, the older document can be replaced with its difference from the new document using delta encoding. Whenever the older document needs to be retrieved, the older document can be generated using the delta coding based on the new document. It is better to maintain the latest document, since the latest document may need to be accessed more than the older document. In this way, numerous document generations using delta coding may be avoided while saving significant space. 6 Experiments and analysis 6.1 Data sets We have conducted our experiments on two different types of datasets: DBWorld [ 8 ]and Wikipedia [ 28 ].
 DBWorld data set We have chosen a subset of the messages that are sent by DBWorld. DBWorld messages form a good set of data with duplicate and update messages. Among 908 messages in chosen DBWorld data set, we have identified the similar messages and present our experiments on 271 comparisons. 19% of these comparisons yield edit distance of 0.
Wikipedia Data Set. There are four major reasons why we have also chosen Wikipedia:  X  It is publicly available.  X  It keeps history of all updates made to a document.  X  It has already the wikimedia diff comparison function [ 27 ] included on their web site.  X  Some documents are organized hierarchically.

We have created nine datasets with different sizes and number of sections. The docu-ments contain information about  X  X nswering machine X ,  X  X ainbow X ,  X  X omputer X ,  X  X eorge W. Bush X ,  X  X ubble X ,  X  X amazan X ,  X  X PEG X ,  X  X emantics X , and  X  X untsville X . Wikipedia allows to download at most 100 documents in XML format per search item.

Wikipedia documents are sometimes composed of sections. This organization allows us to check how S2S performs if sections are allowed. The availability of wikimedia diff provides an opportunity for us to compare our algorithm against a good, new, and in-use wikimedia diff algorithm. Unfortunately, we were not able to find much documentation about wikimedia diff except the comments in the code and help document. 6.2 Preprocessing The first stage is the generation of frequency matrix for DBWorld data set. All the email messages were parsed using Lex and Yacc and created our database for our experiments. During parsing we eliminated all the extra spaces, irrelevant characters (like  X ================== X ) to build a sequence of words for DBWorld data set. We have divided the text using two punctuation marks  X . X  and  X ? X . We treat email addresses (e.g., raygun@cs.uah.edu) or web addresses (e.g., http://www.cs.uah.edu ) as a single word by ignoring the  X . X  in the string. However, since this division algorithm is not accurate, the  X . X  at the end of abbreviations is also considered as an end of sentence. During frequency matrix generation we eliminated the stop words like X  X re, on, an, the, is, a, why, when, who and some more. For our purposes, we assumed that these words do not affect the similarity of two documents. We have not used LSI in our experiments since frequency matrix is enough to serve for our purposes. We have used the Tex t node for Wikipedia documents in XML format. 6.3 Experimental analysis 6.3.1 String comparisons and granularity Number of String Comparisons Each word is searched for a limited number of sentences based on the output of TraceMatch algorithm in Sect. 4 . This avoids the search of a new word till the end of the other document. A word may need to be matched for any word in the other document in traditional matching algorithms.
 Figure 9 display the graph for number of string comparisons on  X  X eorge W. Bush X  data set. This graph shows that the number of string comparisons for our hierarchical and optimized matching algorithms is almost the same for all except at the word level. The number of string comparisons might be more than the number of comparisons in traditional string matching since a word may need to be compared against all the words in the other document. Figure 10 shows the ratio of the number of string comparisons to the number of words on  X  X eorge W. Bush X  data set. The maximum number of words represents max ( | S | , | T | ) whereas minimum number of words represents min ( | S | , | T | ) . The average ratio for maximum number of words is 0.982 whereas it is 1.006 for minimum number of words. The average of these values is 0.994. For similar documents, it can be stated that our algorithm has linear complexity with respect to the number of words. In Fig. 10 , the lower values indicate possible dele-tion/insertion at the end of a string whereas high values indicate the number of comparisons due to verification. The advantage of our algorithm is its limiting the number of unnecessary matching by using structural mapping.

Granularity The  X  X nswering machine X  document had initially one section, and the final number of sections is eight. The section level algorithm attempts to match at the section level. However, if the section level mapping is not successful due to the same number of sentences in consecutive sections, the structural mapping is applied at the sentence level. Figure 11 shows the number of cell computations for sentence and (optimized) section levels. For small number of sections, the section level does not have any advantage over sentence level mapping. However, the advantage of section level mapping becomes obvious at the third quarter of Fig. 11 . The number of computed cells for section level is far more less than the number of computed cells for sentence level. The effect of optimization has also been shown in the figure. The number of cell computations for optimized section level is 48.4 whereas it is 150.5 for non-optimized section level. The number of cell computations for sentence level is 245.1. The average number of sentences in  X  X nswering machine X  data set is 17.5. The number of cell computations for optimized section level is little bit less than three times the average number of sentences. This also shows that the number of cell computations for optimized section level is linear with the number of sentences. Note that to compute the value of a cell, we need the values for three neighboring cells. That is why the number of cell computations is very close to three times the average number of sentences.
LAH(k) without verification We have performed experiments on the  X  X nswering machine X  data set for LAH(1), LAH(2), LAH(3), LAH(4), and LAH(5). We have got the accuracy results 85, 89, 77, 38%, and 9% for LAH(1), LAH(2), LAH(3), LAH(4), and LAH(5), respectively. As we mentioned in Sect. 5.3, the repetitions within a window of k words degrade the per-formance of LAH(k) without verification. We have realized that articles (like a, an, the) are very likely to be repeated within five words in a document. This is the major reason for LAH(5) having low accuracy results. These results also show that LAH(2) performed better than others without any verification. 6.3.2 Comparison with wikimedia diff The wikimedia diff algorithm firstly eliminates common lines from the the beginning and end of two documents. Then it uses MD5 hash function [ 20 ] to index (or to check the existence of a substring in the other). Wikimedia diff starts with a sequence of string comparisons and uses hash data structure for the rest of the operations. In other words, their critical operations are performed in a different domain.

To compare S2S with wikimedia diff, the following steps are taken:  X  Their comparison is in terms of lines while our comparison is in terms of words. When  X  From time to time, their algorithm checks whether a string is empty or not (because of  X  The effect of hash function is interpreted as follows. Sometimes an input for the hash
S2S has word comparisons and cell computations. On the other hand, wikimedia diff has word comparisons and number of words for MD5 hash computation. The total number of operations for S2S is the sum of the number of word comparisons and the number of cell computations. For wikimedia diff, the number of operations is the total number of word comparisons plus the number of words during MD5 hash computation.

Performance Figure 12 displays number of operations on  X  X nswering machine X  and  X  X eorge W. Bush X  data sets. In Fig. 12 , it can be seen that the number of operations for optimized S2S at section level is significantly better than that of wikimedia diff. The average number of operations for optimized S2S (section), non-optimized S2S (section), and wikime-dia diff are 341, 443, and 435 for  X  X nswering machine X  data set, respectively. For  X  X eorge W. Bush X  data set, the average number of operations for optimized S2S (section), non-optimized S2S (section), and wikimedia diff are 1865, 3267, and 5905, respectively. It should be noted that even non-optimized S2S is comparable with wikimedia diff.

Stability To measure the stability of the two algorithms, we have changed the first and last word in the document. Normally, it is expected that such two changes should not affect the number of operations. Figure 13 shows the results of the stability of the algorithms on  X  X nswering machine X  data set. Although S2S is almost not affected at all, there is a significant change in the number of operations for wikimedia diff. The change of average number of operations for optimized S2S at section level is 4.01 whereas it is 662.71 for wikimedia diff. This also shows that the performance of wikimedia diff relies on the similarity of lines at the beginning and the end of the document. However, the performance of S2S is significantly more stable than the performance of wikimedia diff. 6.3.3 Accuracy We would like to note that it is possible to match two strings in multiple ways. Consider two strings from  X  X nswering machine X  data sets: s 1 =  X  An  X  X nswering machine X  is a device for automatically answering [[telephone]] calls and recording messages left by callers.  X  and s 2 =  X  An  X  X nswering machine X  also known as an  X  X nswer machine X  (especially in UK and British commonwealth countries), is a device for automatically answering [[telephone]] calls and recording messages left by callers.  X  The update made from s 1 to s 2 is to insert  X  also known as an  X  X nswer machine X , (especially in UK and British commonwealth countries),  X  between machine X  and is a (i.e., s 2 =  X  An  X  X nswering machine X  also known as an  X  X nswer machine X  (especially in UK and British commonwealth countries), is a device for automati-cally answering [[telephone]] calls and recording messages left by callers. X ). However, it is also possible to match these two strings by first inserting  X  also known as an  X  X nswer  X  between answering X  and machine X  , and then inserting  X  (especially in UK and British commonwealth countries),  X  between  X  machine X   X  X nd X  is a  X  (i.e., s 2 =  X  An  X  X nswering machine X  also known as an  X  X nswer machine  X  (especially in UK and British commonwealth countries), is a device for automatically answering [[telephone]] calls and recording messages left by callers. X ). In the later case, the edit distance is equal to the the the edit distance of the first case. However, the editing in the first matching is the correct editing. We accepted both of them as correct in our experiments. We should also mention that even getting a smaller editing distance does not not necessarily indicate that it is the best match between two strings.

The accuracy of S2S and wikimedia diff is given in Table 4 . We have provided the S2S results for optimized section level when comparing against the wikimedia diff. It can be seen that the accuracy of S2S is better than the accuracy of wikimedia diff. However, it should be noted that the 1% failure of S 2 S does not indicate incorrect matching but indicates the existence of a better match between documents. S2S still produces valid results in those cases.
In our experiments, we have realized that two measures are helpful to determine the accuracy of the algorithm: edit ratio and difference ratio. These values are determined as follows: and
The edit distance checks whether the number of operations are reasonable with respect to the length of strings. The difference ratio checks whether the number of insertion or dele-tion operations are reasonable with respect to the difference in the length of strings. Our mismatch of strings. 6.4 Complexity analysis For optimized structural mapping, we get O ( n ) complexity. The original complexity of the structural matching algorithm is O ( mn ) where m and n represent the number of sentences. The average complexity of optimized structural matching is O ( m + n + 3  X  max ( m , n )) , which is equivalent to O ( n ) when the number of sentences in two strings is close. The cells need to be computed for only parts where there is a change in the size of sentences. In similar documents, a sentence is usually mapped to a sentence that has the same length. That is why the number of cell computations is reduced significantly.

For syntactic matching, if two strings S and T are the same, the time complexity of our algorithm is O ( n ) where n is the lengths of these strings. In our experiments, the length of two strings is very close to each other. The worst case for LAH algorithm occurs, when a word in T does not exist in S . In this case, the string S will be iterated to its end. If two strings S and T are totally different, the complexity of this algorithm is O ( mn ) . Since our algorithm only considers similar documents, the worst case hardly occurs.
 Given two strings S and T where S  X   X  and T  X   X  .Let S = s 0 s 1 s 2 ... s n  X  1 and T = t 0 t 1 t 2 ... t m  X  1 . Since we compare similar messages, assume sim of the words in T exist in S where 0  X  sim  X  1. If sim is 1 all the words of T exist in S . Usually, sim is more than 0 . 9 in our experiments. For LAH(1), the number of comparisons is ( n ) in the best case. In this best case, S and T are exactly the same. Let g be the number of missing words of T in S and g = ( 1  X  sim )  X  m . In the worst case, the words that do not appear in S will appear first in T . This leads to g*n comparisons for missing words. For other words, only m  X  g comparisons are needed. So, the number of comparisons will be
If sim is guaranteed that 1  X  sim  X  ( 1  X  c 1 / n ) , the worst case complexity will be O ( m ) where c 1 is constant greater than or equal to 0. Since structural mapping divides the problem into smaller problems, there is an upper-bound on n . The average length of a sentence is 28 in our experiments. We get O ( m ) complexity during our experiments. In other words, such a constant c 1 exists.

The approximate string matching algorithms like string matching with k mismatches and string matching with k errors do not provide useful information since we do not have any idea on k . Since we are interested in the differences, these algorithms will return the position where the matching starts and leave it to the user to identify the differences or will ask to use string editing distance methods to report the distance. 7 Limitations and future work Our algorithm only considers three operations when matching two strings: insertion, deletion, and substitution. Our algorithm is not handling swap operations. In other words, if there is a swap operation our algorithm reports this case twice as (1) missing of words in one sentence and (2) addition in the other sentence.

For syntactic matching algorithm, we have evaluated every word whether it is a stop or a frequent word or not. In some cases, matching of frequent words causes a problem. For example,  X  X all for Papers X  messages include program committee at the end message which includes the affiliation information. This part includes  X  X niversity of X  sequence for almost each member. This corresponds to a back to back match for two words. We realize that the matching of frequent words within a window should be omitted to find the correct match. Eliminating frequent words in matching solves this problem, but we leave this as a future work. The granularity of what is matched plays an important role. In this version of the algorithm, we did not consider any improvement for this part and leave it as future work.
If there are cases like swapping or frequent sequence repetition, there will be duplicate information in the report. The report can be reevaluated to eliminate these problems. At this level, the results are satisfactory and consider it as a future work.
 We have encountered HTML documents in our data sets. Our algorithm usually failed for HTML documents. But we believe that if the structural mapping is performed considering the structure of HTML documents, our algorithm will also be successful. 8Conclusion In this paper, we have proposed our S2S matching approach that is composed of a heuristic structural mapping and syntactic matching algorithm. The heuristic syntactic matching algo-rithm is an approximation algorithm that considers i consecutive matches between two docu-ments. The structural mapping can be improved by creating more hierarchies and performing mapping by creating a projection of documents (like table of contents) at the high levels. In cases where this algorithm fails, the complex algorithms can be used. As long as there is a way to split the documents into subdocuments, our algorithm will be successful. The structural mapping part also allows and motivates parallel processing. We are planning to conduct more experiments on XML data and genome sequences.
 References Author Biography
