 We consider the problem of optimally allocating a fixed budget to construct a test collection with associated relevance judgements, such that it can ( i ) accurately evaluate the relative performance of the participating systems, and ( ii ) generalize to new, previously unseen systems. We propose a two stage approach. For a given set of queries, we adopt the trad itional pooling method and use a portion of the budget to evaluate a set of documents retrieved by the participating systems. Next, we analyze the relevance judgments to prioritize the queries and remaining pooled documents for further releva nce assessments. The query prioritization is formulated as a convex optimization problem, thereby permitting efficient so lution and providing a flexible framework to incorporate various constraints. Query-document pairs with the highest priority scores are evaluated using the remaining budget. We evalua te our resource optimization approach on the TREC 2004 Robust track collection. We demonstrate that our optimization techniques are cost efficient and yield a significant improvement in the reusability of the test collections. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  selection process A test collection consists of ( i ) a document collection, ( ii ) a set of test queries, and ( iii ) a set of corresponding relevance judgements. Ideally, every document in the collection would be judged relevant or non-relevant with respect to every query in the test set. In practice this is infeasible. Instead, the relevance assessments are typically obtained for a subset of documents retrieved by the set of participating IR systems. Each participating system contributes a fixed number of re trieved documents to the common pool which are then assessed for relevance by expert judges. While this approach increases the chance of identifying relevant documents, economic constraints may still prevent exhaustive judgments of all the documents in the pool. In this paper, we consider how to prioritize query-document pairs for relevance judgments, when budget constraints preclude obtaining relevance judgments fo r all documents. We formulate the question as an optimization problem in which, for a given budget , we seek to identify a set of n query-document pairs that most accurately rank the participating systems and provide the best generalization to yet unseen systems. The unique contributions of th is paper are (i) explicitly incorporating a cost constraint within the optimization, (ii) formulating the optimization prob lem as a convex optimization, leading to computationally e fficient algorithms for finding a globally optimum solution, and (iii) incorporating a generalization constraint based on the estimated number of un-judged relevant documents for each query. In Section 2 we discuss related wo rk. Particular attention is given to the work of Webber and Park [1], to which we compare our algorithm. Section 3 then provides a detailed description of our algorithm, while Section 4 desc ribes specific implementation issues. Section 5 provides experime ntal results on the TREC 2004 Robust test collection. Finally, Section 6 provides a summary of our results and suggestions for future research directions. Document pooling, originally pr oposed by Sp X rck-Jones and Van Rijsbergen [2] has been widely adopted for the construction of test collections. While studies showed that the number of pooled documents in the early TREC experiments was sufficient to rank the systems performance reliably, it also transpired that a considerable number of relevant documents remained undiscovered [3]. Thus, several alte rnative approaches have been suggested in order to judge more relevant documents, for example, Zobel [3] and Cormack et al. [4]. Webber and Park [1] estimated th e bias that the uniform pooling and incomplete judgments intr oduce when un-judged documents are considered as non-relevant and when they are simply omitted from the computation of the performance scores. They also considered a more precise error estimation by considering a set of common topics and newly introduced systems for which they had full assessments. By removing the uncertainty of the un-judged documents they proposed an adjusted estimator that can be extrapolated to new topics and new systems. Their experiments demonstrate the effectiveness of the estimator with different sizes of common topics sets. However, th ey do not provide criteria for topic selection nor prioritization of new documents for relevance assessment when these are required to evaluate new systems. Research presented in this paper addresses these issues and explicitly models the query and document selection process in relation to the fixed budget constraints. Many TREC test collections contain only 50 queries. Using a relatively small query set allows NIST to judge many documents per query and still stay within the available budget for relevance assessments. This increases the reusability of a test collection for other tasks and systems. On average about 2000 documents are judged per query. Sanderson a nd Zobel [5] suggested an alternative and less costly approach. They showed that if NIST evaluated systems by using a signifi cantly larger set of queries, i.e., much larger than sets of 50 queries, and shallower pools of candidate documents, i.e., much smaller than 100 documents per query, then the assessors X  effort would be greatly reduced without compromising the accuracy of evaluation. Carterette and Smucker [6] supported this suggestion by usi ng statistical tests. Evaluation based on a large number of queri es with shallow judgments motivated a variety of approach es for selecting documents for assessments and defining evaluati on metrics for partially judged result sets, such as statAP [7] or MTC [8]. Following the belief that a larger query set is desirable, the TREC 2007 Million Query track [9] was the first to include thousands of queries. The organizers made us e of recent document selection methods to collect few judgments per query. However, due to the small number of documents assesse d per query, the reusability of such a test collection still remains questionable [10]. This raises a fundamental question of how ma ny and which documents should be assessed per query to achieve an optimal trade-off between the evaluation accuracy and the limited budget that is available for document assessments. In our work, we give a mathematical formulation of this problem that is tractable and extendible to include various refinements. Let S denote the population of all IR systems. Although the distribution of S is unknown, we assume that all, past, present and future systems are drawn from this distribution. This is a simplifying assumption but a good starting point for developing the mathematical model. We are given a document corpus D and a set of N test queries  X   X   X  X  X  X   X   X ,  X   X ,...,  X   X  . We assume that there is a set  X  participating systems (  X   X   X  X  ), each of which returns a number of retrieved documents for each of N queries. From the retrieved documents we create a common pool P , ( P  X  D ) of documents to be used for comparative evaluation of the systems. Let  X  denote the desired budget required to build relevance judgments over the pooled documents P . For a given budget B , that is much smaller than  X   X   X  X   X  X  , we seek to collect re levance judgments for a subset of query-document pairs in order to accurately evaluate the performance of the participating systems and reliably estimate the performance of yet unseen systems. We propose a two-stage process to allocate the limited budget B , which we outline next. Stage 1.  X  Acquire relevance judgments for an initial set of documents in P . In the first stage we allocate a portion B 1 of the budget B to assess the relevance of some of the documents in the common pool P . A number of methods have been pr oposed to select documents for relevance assessment, e.g. [8]. Generally, the selection methods assign a priority value w d to each document and process them accordingly. Given a limited budget, the simplest allocation strategy is to divide the budget equally among N queries and, for each query, select a fixed number of documents with the highest priority scores. In the standard pooling technique, the documents are ranked based on the query relevance. Thus one can choose a uniform pool depth across queries to select documents to fit the available budget B 1 . Therefore, the priority score w documents above the cut-off rank and 0 for those below. Stage 2.  X  Selectively expand relevance judgments In the second stage we utilize the remaining budget, B ( B=B 1 +B 2 ), to extend the pool of relevance judgments from Stage 1. The allocation of B 2 is based on a convex optimization of a cost function that seeks to (i) achieve maximum agreement with the evaluation of  X   X  systems using the full set of common documents P and ideal budget  X  , and (ii) generalize to new, unseen systems. Before we describe in detail th e method for prioritizing queries and documents, we first introduce the mathematical notation and formulation of the model. For the population of all IR systems S , we observe the retrieval performance of each participating system over a finite set of N test queries. The performance measurements are represented in the form of a performance matrix X . Each row corresponds to a system and each column to a query 1 . An entry  X  the performance score of the i-th system on the j-th query. We refer to a column of the matrix X as a query-system vector comprising the performance scores of all the systems for a given query. The column vector m is the average of all query-systems vectors across queries. Let  X  denote the average performance of a system X  X  row in the matrix X , then where  X  X   X  1  X   X  X  X  X  is an N -dimensional vector of 1 X  X . By definition, the column vector m comprises system average performance  X .
 We are interested in the expectation and variance of  X  across all the systems. We therefore define  X  X  X   X  X  X  X  to be the vector of average performance scores for an individual query across the systems. Further, let  X  denote the N  X  N covariance matrix of the N query-systems vectors. Then the expectation and the variance of  X  across systems are given by [11], More generally, the performance  X  of a system can be expressed as a weighted combination of the scores  X   X , X  . Let  X 0,1 X  X  X  X  denote the associated weight vector with real values in [0,1]. Then the weighted average is expressed as  X   X   X  X  X  X  and the expectation and the variance of  X   X  across systems are given by We now determine  X  as priority scores of queries in order to expand relevance judgments from Stage 2 under specified conditions. In practice, it has been shown that some documents are more effective than others in discri minating systems X  performance for a given query (e.g., [8] &amp; [12]). Si milarly, some queries are more effective than others [13]. Thus, it is useful to define a query-document priority score  X   X  X  X  as  X   X  X  X   X  X   X   X  X   X  where  X  are weight coefficients for queries and documents, respectively. While there are many ways to prioritize documents [8], for simplicity, we adopt the uniform selection of documents across queries and focus our attention on the query prioritization. We consider a query j as representative of the query set if its performance across systems is similar to the average performances of systems across all the test queries, i.e., the j-th column of X is close to the vector m . Our objective is to determine the most representative subset of queries based on several criteria. We formalize this by defining the vector  X   X  to represent the weighted average performance of the systems across queries with query weights  X  and  X   X   X  X  X  ,  X   X  X   X   X   X   X  X   X  , the weighted average performance of the system i across queries. We introduce an objective function f (  X  ) to define the distance criteria between  X   X  and m . In the context of IR systems evaluation, two criteria naturally present themselves: (i) the similarity in the ranking of the systems and (ii) the similarity in the absolute values of performance, i.e.,  X  The closeness of two rankings is usually measured using Kendall- X  . Unfortunately, using such a m easure leads to computationally inefficient solutions. Consequent ly, we did not consider this similarity measure further. However, we do use Kendall- X  as an evaluation measure in our experiments to assess the quality of the optimization method. There are many ways to characterize similarity in values between  X   X  and  X  such as the mean squared error and correlation. In our experiments we measure and re port on correlation. Experiments with mean squared error are reported in [14]. The linear correlation measure  X   X  , between  X   X  and  X  is given by with the covariance between  X  and  X   X  computed as where  X   X  X   X  X  X  X  represent a system row in the matrix X . We seek a set of  X  coefficients that maximizes  X   X  . Reordering Equation (2) gives Maximizing  X   X  is equivalent to maximizing  X   X  since  X   X  constant. The maximum value of Equation (2) can be approximated by the minimization pr oblem that is expressed in a quadratic programming form 2 : where the  X 0 X  is a regularization parameter to adjust the trade-off between the quadratic part (  X   X   X  X  X  X  and the linear part (  X  part. In our experiments the regularization term  X  is set to  X  We now add constraints to arrive to the final optimization function. We consider two constraints. Th e first establishes the maximum available budget that can be used for additional judgments while the second, referred to as the generalizability constraint , ensures effective evaluation of new, prev iously unseen systems. We note that other constraints could easily be incorporated into this framework. During Stage 2, we assume that a fixed budget B 2 is available for relevance judgments. It is natural to assume that the budget is allocated in proportion to each query X  X  priority. We can, without represent the proportion of the available budget that will be allocated to individual queries. In other words, if query j has a corresponding weight  X  j &gt;0, we will expend a proportion of the for which  X  j &gt;0, is then based on the optimization: If all the relevant documents for each query in the test collection are identified, then the test collection generalizes to any system. While we cannot guarantee that all the relevant documents are detected, it is clear that the fewer unidentified relevant documents there are, the more generalizable the test collection is . Thus, we define a cost function that no t only minimizes the difference between  X   X  and m , but also minimizes the number of un-judged relevant documents. Let r j be the expected number of un-judged relevant documents for query q j . Given that we allocate  X   X   X   X   X  X  X  X   X   X  of the B to a query q j then, at the end of the Stage 2, the number of newly judged relevant documents will be proportional to  X   X   X  total number of relevant document s judged in Stage 2 is simply  X   X   X   X   X   X  X  X  X  , ignoring the constant of proportionality. Clearly, we want to maximize the total number of relevant documents in order to achieve maximum generalizability. Using a Lagrange multiplier  X  we combine the constraint and the objective function  X   X   X   X  to obtain 
QP =  X  X  X  where  X   X   X   X   X  X  X  X  is added as a normalization factor to keep the first and second term in the same scale. The above optimization function is convex and we solve it using a sequential quadratic programming algorithm [16]. Sec tion 4.2 discusses how to estimate the expected number of relevant documents r j. . Before describing the experime nts, we discuss a number of implementation issues. Note, however, that we cover setting of  X  in Section 5. In practice, the mean vector  X  and the covariance matrix  X  are unknown because the complete population of systems S is unknown. Indeed, we have no information about yet unseen systems. Instead, we have a sample of x 1 , ... , x L of scores (  X  row of matrix X ) of L participating systems by which we can estimate  X  and  X  . Assuming that the set of participating systems is uniformly sampled from S , the standard unbiased estimators of  X  and  X , denoted as  X   X  and  X   X  , are given by  X   X   X  X  X   X   X  X   X  X   X   X   X   X   X  X  X  X  ,  X   X   X 1 X  X  X  X  X   X  X   X  X  X   X   X   X  X  X   X   X   X   X   X  If L is large and the sample of participating systems is diverse, we can get reliable estimations of  X  and  X  . In practice, new systems may not be considered as drawn from a random sample. For example, over time, new systems are likely to perform better than participating systems, as system performance improves. In this case, we can use unbiased estimators for weighted systems to approximate  X  and  X  . Unfortunately, space limitations precludes further discu ssion and the interested reader is directed to [17]. It is difficult to determine whet her or not all re levant documents for a query have been judged. However, the prior work of Zobel [3] suggests that some degree of estimation is possible, given an initial set of rele vance judgments. Given a set of initially judged documents, Carterette et al. [18] applied logistic regre ssion to calculate the pr obability of relevance of un-judged documents. We use the same method to partition un-judged documents into relevant and non-relevant. Specifically, given an initial set of judged documents for a query, the relevance of a document d i to query q j is estimated by: where w is the parameter vector of the model and  X  is a feature vector that uses the document similarity features as introduced in [18]. An un-judged document d i , retrieved for a query q labelled as relevant if th e probability of relevance  X  X  X  0.5 ; otherwise d i is labelled as non-relevant. Hence, the total number of relevant unseen documents for a query j is estimated by the number of un-judged documents with  X  X  X   X   X ,  X   X  X 0.5 . In this Section we describe a set of experiments th at we conducted to evaluate the proposed QP method (Equation 6). Evaluations are conducted by comparing the ranking of the systems based on the full set of queries and available relevance judgments, with the ranking based on our method. The comparison uses Kendall- X  . generalization to unseen systems. Thus we define the criteria for identifying markedly different systems. We use the mean average reuse (MAR) to characterize individual systems (see Section 5.3) and select those with low MAR as  X  X ew X , yet unseen systems. Note, in order to avoid bias against any individual system, all participating systems contribut e equally to the pooling of documents in both phases. We consider three baseline me thods for resource allocation in comparison with our resource optimization method ( QP ): (i) Uniform Allocation (UN) , in which the available budget is uniformly allocated across queries. For example, if the budget can cover only 200 new judgments and there are 100 queries, we judge two new documents per query. (ii) Random Allocation (RA) , in which a random set of n queries is selected and the budget B 2 is uniformly allocated across the selected queries. In our experiments we use n that corresponds to the number of queries selected by our optimization method. We repeat the random query sampling for 1000 trials and report the average of the corresponding results. (iii) Score Adjustment (SA) , in which a random set of n queries is selected and the budget B 2 is uniformly allocated across the selected queries. Once the new relevance judgments are acquired, one can compare the difference between the original and new performance scores and use the average bias as a correction term for both the queries and the system s, as proposed by Webber and Park [1]. Note, the original algorithm by We bber and Park [1] assumes that relevance judgments are rendered for documents retrieved by new systems. In our context, the scor e adjustment is applied to the relevance judgments of docum ents within the common pool i.e., contributed by the partic ipating systems. We use the SA method in 1000 trials of random query sampling and report the average of the corresponding results. Our experiments were performed using TREC 2004 Robust track. Normally, organizations particip ating in TREC register as sites and submit a number of experimental runs for evaluation. These runs often represent variations of the same IR system. For our purposes we consider each run as an individual IR system but take special care when considering IR submissions from the same site. In particular, when experiments re quire that we exclude some of the systems in order to treat them as  X  X ew X  systems, we hold out the entire set of runs from the same site. Furthermore, during the computation of performance metric s, we remove documents that are uniquely retrieved by the held-out systems. The TREC 2004 Robust track consists of 249 queries, 14 sites with a total of 110 automatic runs, and 311,410 relevance judgments obtained over documents in TREC Disks 4 &amp; 5 corpora, excluding the Congressi onal Record sub-collection. Comparative evaluation of TREC runs is conducted based on the average precision (AP). However, since we use only a fraction of relevance judgments in Stage 1, many documents remain un-judged. Consequently, the AP scor es measured for participating systems are uncertain and the performance matrix X is noisy. For that reason, in our experiments we use infAP rather than AP to measure systems effectiveness with respect to the initial judgments. The infAP scores provide a better approximation of the true AP scores [19] and, hen ce, a less noisy performance matrix X . In order to test the generalization and robustness of the four resource allocation methods rela tive to the evaluation of new systems, we first divide the TREC runs into participating systems and held-out systems. During Stag e 1, we randomly select a few sites and use their corresponding r uns as participating IR systems. Using the pooling technique we select the set of documents retrieved by these participating systems and compile the corresponding relevance judgments. Th e pool depth is adjusted to fit the budget allocated to Stage 1. For each held-out system, and each query, we compute the average reuse (AR) [18] to measure the overlap between the documents retrieved by a held -out system and the judged documents. We then define the mean average reuse (MAR) as the average of AR values over the full set of queries. Based on the MAR values, we split the held-out systems into two groups. The first group consists of systems with high MAR across runs. This auxiliary group is evaluated with the relevance judgments obtained at Stage 1 and their performance values are added to the matrix X. Note, however, that th e auxiliary group does not contribute to the document pool. The second group, consisting of runs with low MAR values, forms the set of new systems. The full experiment comprises the following steps: 1. Pick s 1 percent of sites at random, these are the held-in sites. 2. For each query, construct the training pool of top k 3. Compute the MAR for the held-out runs. Average the MAR 4. Pick s 2 percent of sites with low MAR scores and treat their 5. Prioritize queries using the QP method, i.e., using the 6. For the RA and SA method, given that n queries are activated 7. Given the budget B 2 , acquire additional relevance judgments The QP formulation of the budget opt imization in (6) requires the computation of the Lagrange multiplier  X  . We determine  X  empirically by systematic explora tion of the range of values for  X  , 0  X  X  X  X  10. This is performed after Stage 1 but before Stage 2. During Stage 1, we have allocated budget B 1 and acquired the same number of relevance judgme nts for all queries. We then simulate the steps 1 through 7 above, where we split the budget B estimated number r j of un-judged relevant documents for a query q is set to the number of rele vant documents identified during Stage 2 of the simulation to determine  X  , no selected query requires more assessments than we have acquired during Stage 1. Thus, we have all the relevance j udgments needed to evaluate the performance of the simulation. For a particular value of  X  within the range 0  X  X  X  X  10 we apply a 10-fold cross-validation technique. In each of the 10 iterations, 10% of participating systems are held out (these become our simulated new systems). Relevant documents that are in the initial document pool but solely retrieved by th e held-out systems are removed from the pool. The QP method, using the reduced set of judgements, produces a set of qu ery-document pairs. We evaluate this solution by computing the Kendall- X  of the systems X  ranks with the corresponding systems X  ranks using all the relevance judgments acquired using budget B 1 and Stage 1. We record the average Kendall- X  for the 10 trials. Finally, we choose the  X  value with the highest average Kendall- X  . For the TREC 2004 Robust collec tion we report experiments using a total budget that covers either 10,000 or 20,000 relevance judgments. This is less than 7% of the collection X  X  assessor budget of 311,410 relevance judgments. We applied the steps 1 through 7 in Section 5.3 across 10 trials and, in each trial we randomly choose s 1 percent of sites and associated runs as participating systems. The remaining runs are evaluated for MAR and the  X   X  percent of sites with the lowest MAR scores are chosen to be new systems. Depending on the average MAR scores,  X   X  varies between 50% and 40% of the total number of sites. We report averages over the 10 trials. These experiments are repeated for 3 different values of  X   X  and  X  different budget allocations, B 1 and B 2 . Table 1 summarizes the results. We report the Kendall- X  statistic between the ranking of the systems induced by a resource al location method and the ranking over the full set of queries and corresponding relevance judgments. We report separate Kendall- X  statistics for participating systems and for ne w systems, which is common in the literature and permits us to separately discuss the accuracy and generalization of the methods. We observe that for all 9 expe rimental configurations, the Kendall- X  scores of the QP method outperform the other three resource allocation methods. Note that the uniform allocation strategy is comparable and often better than the random allocation strategy for both the participatin g and the new systems. The score adjustment ( SA ) method outperforms the uniform allocation when s =10% (rows 1 through 6). However, when for s 1 =20%, the SA method performs no better than the uniform allocation for new systems but remains better for part icipating systems. In contrast, our QP method yields superior results in all cases, except for configuration #1, in which the initial budget B 1 2000 relevance judgments, i.e., only 0.6% of the total judgments. It is important to note that the QP method has signi ficantly better Kendall- X  scores than the random allocation method, for both Table 1. Result for Robust TREC 2004 runs evaluated by 
MAP. The first two columns report experimental parameters. The next columns report the Kendall- X  of (i) participating systems, and (ii) previously unseen systems for each resource allocation. participating and new systems; an indication that the query prioritization achieved both accuracy and generalizability. We note that increasing the number of participating systems s in Kendall- X  of new systems X  ranking than increasing the budgets, i.e., relevance assessments, and keeping the number of participating systems s 1 constant. This can be seen by comparing probably related to observations by Carterette et al. [18] that a higher diversity of participating sy stems results in a better ranking of new systems. In this paper we consider th e problem of prioritizing query-document pairs for relevanc e assessment given a budget constraint, in order to ( i ) improve the accuracy of evaluating participating systems, and ( ii ) ensure that the test collection generalizes to new, previously unseen systems. We propose a two-stage procedure. In Stage 1, we allocate a budget B uniformly across all queries, acquiring a corresponding set of relevance judgments. In Stage 2, we sue information from Stage 1 to prioritize query-document pairs and allocate a budget B accordingly. While we presented a 2-stage process, our method is iterative and can be applied repeat edly to support a growing set of systems and the corresponding set of relevance assessments. The novelty of our work is in ( i ) modeling query and document selection through explicit cost optimization, and ( ii ) formulating the problem as a convex optimizat ion for which computationally efficient algorithms exist for identifying the optimum solution. Our experiments compare the QP method with, uniform, random sampling and a variant of the score adjustment method presented in [1]. They provided strong evidence that the QP method is ( i ) superior to the selected benchmark methods, ( ii ) exhibits good accuracy, i.e., predicts the performance of participating systems, performance of new systems. One of the main advantages of the QP method is its extensibility. We can leverage research on identifying query characteristics that make queries better suited for use in system evaluation and formulate new components and constraints within the optimization framework. Our future work will investigate a richer set of such heuristics, aiming to produce methods for test collection construction that are e fficient, in terms of required resources for relevance assessments, and effective, in terms of accuracy of systems evaluations. Furthermore, our experiment se t up can be expanded to examine the sensitivity of estimation errors such as estimating the number of un-judged relevant documents and errors in the matrix X . Finally, the full potential of th e method would be realized through an effective iterative model of relevance assessment. Thus, it would be beneficial to evaluate the dynamic and real time application of the cost optimization in the context of the emerging practice of crowdsourcing relevance assessments. [1] W. Webber and L. A. F. Park, "Score Adjustment for Correction [2] K. Sparck-Jones and C. J. van Rijsbergen, "Information retrieval [3] J. Zobel, "How reliable are the results of large-scale information [4] G. Cormack, C. Palmer, and C. Clarck, "Efficient Construction [5] M. Sanderson and J. Zobel, "Information retrieval system [6] B. Carterette and M. D. Sm ucker, "Hypothesis testing with [7] J. A. Aslam, V. Pavlu, and E. Yilmaz, "A Statistical Method for [8] B. Carterette, J. Allan, an d R. Sitaraman, "Minimal test [9] J. Allan, J. A. Aslam, V. Pavlu, E. Kanoulas, and B. Carterette, [10] B. Carterette, E. Kanoulas, V. Pavlu, and H. Fang, "Reusable [11] P. J. Huber, Robust Statistics , 2nd ed. Wiley, 2009. [12] E. Yilmaz, E. Kanoulas, and J. A. Aslam, "A Simple and [13] J. Guiver, S. Mizzaro, and R. Stephen, "A few good topics: [14] M. Hosseini, I. Cox, N. Milic-Frayling, V. Vinay, and T. [15] W. Dinkelbach, "On nonlinear fractional programming," [16] R. W. Cottle, J.-S. Pang, and R. E. Stone, The linear [17] M. Hosseini, I. J. Cox, T. Sweeting, N. Milic-Frayling, and V. [18] B. Carterette, E. Gabrilovich, V. Josifovski, and D. Metzler, [19] E. Yilmaz and J. Aslam, "Estimating average precision with 
