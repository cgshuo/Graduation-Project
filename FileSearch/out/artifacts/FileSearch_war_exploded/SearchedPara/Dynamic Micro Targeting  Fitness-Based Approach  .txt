 It is crucial to segment customers intelligently in order to offer more targeted and personalized products and services. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying clustering algorithms. Recent research proposed a direct grouping-based approach that combines customers into segments by optimally combining transactional data of several customers and building a data mining model of customer behavior for each group. This paper proposes a new micro targeting method that builds predictive models of customer behavior not on the segments of customers but rather on the customer-product groups. This micro-targeting method is more general than the previously considered direct grouping method. We empirically show that it significantly outperforms the direct grouping and statistics-based segmentation methods across multiple experimental conditions and that it generates predominately small-sized segments, thus providing additional support for the micro-targeting approach to personalization. 
Customer segmentation, such as customer grouping by the level of family income, education, or any other demographic variable, is considered as one of the standard techniques used by marketers for a long time [20]. Its popularity comes from the fact that segmented models usually outperform aggregated models of customer behavior [21]. More recently, there has been much interest in the marketing and data mining communities in learning individual models of customer behavior within the context of 1-to-1 marketing [18] and personalization [4], when models of customer behavior are learned from the data pertaining only to a particular customer. These learned individualized models of customer behavior are stored as parts of customer profiles and are subsequently used for recommending and delivering personalized products and services to the customers [2]. 
As was shown in [13], it is a non-trivial problem to compare segmented and individual customer models because of the tradeoff between the sparsity of data for individual customer models and customer heterogeneity in aggregate models: individual models may suffer from sparse data, while aggregate models suffer from high levels of customer heterogeneity. 
A typical approach to customer segmentation is based on the statistics-based approach that computes the set of statistics from customer X  X  demographic and transactional data [3, 13, 23], such as the maximal and minimal times taken to buy an online product, RFM statistics [16], etc. After such statistics are computed for each customer, the customer base is partitioned into segments by using various clustering methods on the space of the computed statistics [13]. It was shown in [13] that while the best statistics-based approaches can be effective and even outperform the 1-to-1 case under certain conditions, the approach can also be very ineffective as different customer statistics calculations result in different n -dimensional spaces and various distance metrics or clustering algorithms would yield very different clusters. 
Recent research [12] proposes the direct grouping segmentation approach that partitions the customers not based on computed statistics and particular clustering algorithms, but in terms of directly combining transactional data of several customers, such as Web browsing and purchasing activities, and building a single model of customer behavior on this combined data. This approach avoids the pitfalls of the statistics based-approach in that it does not require selection of arbitrary statistics and grouping customers based on these statistics. Instead, it provides a more direct approach to customer segmentation by combining customers X  data and identifying the groups of customers generating the best models on this data. It was shown in [12] that the direct grouping segmentation approach dominates the statistics-based segmentation and the 1-to-1 approaches. 
In this paper we aim to improve the performance of previous customer segmentation approaches [11-13] via the method of micro targeting , where predictive models of customer behavior are built not on the segments of customers but rather on the customer-product groups. Table 1 shows a Product Type  X  Customer matrix describing which of the L possible products N of the customers have purchased. Given this purchasing information, we could build predictive models of customer purchase behavior over specific regions within this Product Type  X  Customer space. For example, we can build a regression model predicting the purchase volume of jazz CDs that a group of freshman students from University of XYZ majoring in computer science would buy on a monthly basis. Then the research problem is to identify optimal regions in the Product Type  X  Customer space on which the best predictive models of customer purchasing behavior are built . 
Previous research [11, 12] has considered only vertical partitioning of the Product Type  X  Customer matrix that grouped customers into segments by combining all their purchasing transactions. In this paper, we present a micro targeting approach that identifies the most suitable regions in the Product Type  X  Customer space for building the best local predictive models of customer behavior rather than grouping all the customer X  X  transactions into segments. For example, we may want to build a model predicting whether a customer is going to purchase a particular product during a visit to a website, and we may want to build this model for a certain segment of customers (e.g. freshman NYU students) and a certain category of products (e.g., jazz CDs). 
The proposed micro-targeting approach is based on the observation that a customer may possess different underlying utility functions across different types of products, and therefore should be modeled separately for different product types. The advantage of this approach is that the micro-targeting region is a smaller and more flexible unit of analysis than a customer segment. Therefore, identification of the best micro-targeting regions is a more general problem than identification of the best customer segments, and thus should produce superior performance. The problem with the micro-targeting approach is that the identified regions can be too small and not have sufficient data to build any meaningful models of customer behavior, and we address this problem in the paper. 
In this paper, we compare predictive performance of the micro-targeting approach with the previously studied methods, including direct grouping, and demonstrate that the micro-targeting approach significantly outperforms these other methods by a wide margin . We also show that computational performance of our method is comparable to previously studied direct grouping approach, thus achieving significant predictive performance improvements without excessive increases in computational costs. 
The problem of optimal segmentation of a customer base by customer and product type can be formulated as follows. Assume that C = { C 1 ,...,C N ,} is the customer base consisting of N customers, and that there are L product types P = {P P } . Each customer C i is defined by the set of m demographic customer C i performed k ir transactions Trans(C TR ir2 , ..., TR irk ir } , where transaction TR irj is defined by its {t schema T. Finally, we combine the demographic data {A ..., A im ,} of customer C i and his/her set of transactions for product P r , Trans(C i , P r ), into the complete set of customers X  product specific data TA(C i , P r ) = {A i1 , A i2 , ..., A For example, a customer C i can be defined by attributes A= {Name, Age, and other demographic attributes} and by the set of purchasing transactions Trans(C i , P r ) she made at a Web site for a product type  X  X ook X , each transaction defined by such transactional attributes T as book title, when it was purchased, and the price of the book. Note that some customers purchase only subsets of products L . Thus Trans(C i , P r )={} if customer C purchased no products of type P r . 
Given the set of purchasing transactions s i = { TA TA u } performed by customers C 1 ,...,C n , over product categories P , ..., P r from the Product Type  X  Customer space, we want to build a predictive model M i on this set s i and measure its performance using some fitness function f mapping transactions s into reals, i.e., f(s i )  X   X  . For example, model M decision tree built on transactional data s i of freshmen student customers from University XYZ who bought product categories jazz and hip-hop CDs, and the fitness function f measures predictive accuracy of decision tree model M i built on s 10-fold cross-validation. 
Furthermore, we partition the Product Type  X  Customer space into a mutually exclusive collectively exhaustive set of regions S = {s 1 ,...,s k }, build models M i for each region s described above, and compute their fitness scores f(s any of the standard methods (e.g. area under ROC curve or predictive accuracy using 10-fold cross-validation). We weight each region s i , according to its importance  X  i and compute the overall fitness score for the partition S as 
Finally, we want to find the best partition of the overall set of customer transactions into regions S = {s 1 ,...,s k the overall fitness score  X  over all possible partitions. This optimal partitioning problem is easily reducible to the problem of partitioning C into a set of customers, which was proven in [12] to be NP-hard and, thus, is intractable. Therefore, in this paper, we propose a suboptimal polynomial-time micro-targeting method that has substantially better predictive performance when compared to the previous segmentation methods. 
Our proposed micro-targeting method is related to the reduction-based method for providing multi-dimensional recommendations [1], where certain segments of ratings are selected from the multi-dimensional cube of ratings and recommendation algorithms build local recommendation models using these and only these segments of ratings. However, unlike [1], we focus on building general local models of customers in this paper, rather than building multidimensional recommendation problem. 
Our work is also related to the work on segmentation by context [9], where product type can be thought of as a type of context which can be used to build different predictive models of purchase behavior for individual customers. However, rather than using product types as an explicit context constraint, our approach starts with a customer and a single product type segment representing purchasing context. Then it works with multiple customer/product type combinations and builds local models maximizing predictive performance, thus removing contextual constraints. 
Among various segmentation methods studied in the marketing literature, the ones that are most closely related to our work are various clustering techniques, mixture models, (generalized) mixture regression models and continuous mixture distributions [21]. Our work differs from this previous research in marketing in that we use the direct grouping approach [12] to segment customers without deploying statistics-based clustering methods. 
Our micro-targeting approach builds on previous work on direct grouping method [11, 12]. Since we compare performance of these two and the statistics-based approach in the paper, we describe the direct grouping and the statistics-based segmentation methods in more detail below. 
The statistics-based approach to customer segmentation first computes the set of summary statistics from customer X  X  demographic and transactional data [3, 13, 23], such as the maximal and minimal times taken to buy an online product, RFM statistics [16], etc. After such summary statistics are computed for each customer, the customer base is partitioned into customer segments by using various clustering methods on the space of the computed statistics [13]. In particular, in this paper, we use hierarchical clustering method to segment the customers and build predictive models of their behavior on these segments. More specifically, we consider a variant of the hierarchical approach below that is described in [6, 10] and deployed in [13, 17], as well as a variant of the previously proposed affinity propagation ( AP ) clustering algorithm [8] that is presented in [11].

Hierarchical Clustering (HC) : Using the same hierarchical clustering techniques as in [13], we can learn predictive models of customer behavior of the form where X 1 , X 2 , ..., X p are some of the demographic attributes from A and some of the transactional attributes from T (see Section 2), and function f  X  is a model that predicts certain characteristics of customer behavior, such as prediction of the product category or the time spent on a Web site purchasing the product. The correctness measure of this prediction is our fitness function f (defined in Section 2). These models defined by expression (2), are built for the groups of customers that are obtained as follows. We start with a single aggregated grouping of all customers C and compute a set of summary statistics {Z 1 , ..., Z described earlier in this section. Then we use hierarchical clustering methods [6, 10] on the set of these summary statistics to partition the set of N customers (which are viewed as the set of N points in the h-dimensional summary statistics space) by iteratively applying Euclidean distance-based clustering algorithms in the h -dimensional customer summary statistics space. The Hierarchical Clustering ( HC) method generates new levels of segment hierarchy via progressively smaller groupings of customers until the 1-to-1 level is reached and each segment contains a single customer and his/her transactions. The decision to group certain customers together based on customer demographics attributes and summary statistics {A 1 , A 2 , ..., A m , Z 1 , Z 2 , ..., Z FarthestFirst clustering method [10] that is found to perform well in [13] . We compute progressively smaller customer segments for each level of the clustering hierarchy and build predictive models for these segments at each level. Finally, we compute the overall fitness score (1), where the weights are proportional to the sizes of customer segment, and select the segmentation level with the highest overall fitness score as the best possible segmentation of the customer base. 
Affinity Propagation (AP) : Starting with n unique customers, AP [8] identifies a set of training points, exemplars , as cluster centers by recursively propagating  X  X ffinity messages X  among training points. Similar to greedy K-medoids algorithms, AP picks exemplars as cluster centers during every iteration, where each exemplar in our study is a single customer represented by his/her summary statistics vector. Then AP forms clusters by assigning an individual exemplar X  X  group membership based on  X  X ffinities X  that exemplar has with any possible cluster centers. We assume in this paper that affinity is defined as pair-wise Euclidean distance measures an exemplar has with any possible cluster centers. AP runs in O(N 2 ) . It is a good method for segmenting customers because cluster centers are associated with real customers rather than computed  X  X irtual X  customers as in the case of standard clustering algorithms. 
The direct grouping approach, presented in [12], makes decisions on how to group customers into segments by directly combining different customers and their transactions into groups, building models on the customer data for the group, and measuring the overall fitness score as a linear combination of scores of individual groups. [12] describes the Iterative Merge ( IM ) algorithm that works as follows. 
Starting from single-customer segments (of size 1), IM iteratively seeks to merge existing customer segments by combining data from two segments SegA and SegB when (a) the predictive model based on the combined data for segments SegA and SegB performs better than respective models on SegA and SegB and (b) combining SegA with any other existing segments would have resulted in worse performance than the combination of both SegA and SegB. IM deploys a greedy search strategy since it determines the best pair of customer segments at each iteration and merges them together resulting in the best local solution. The algorithm terminates when there are no more improvements to be made from combining customer segments. 
IM runs in O(N 3 ) time in the worst case, where N is the number of customers, because a single merge of two segments takes O(N 2 ) time in the worst case, and there can be up to N of such merges. However, in practice, the search space of IM is not very large because it merges groups, not individual customers, at a time, and the empirical results reported in [12] confirm this observation. 
It was shown in [11, 12] that the direct grouping method IM outperforms the statistics-based approaches, including HC, AP and some other  X  X raditional X  segmentation methods. However, one limitation of the IM method lies in that it vertically partitions the Product Type  X  Customer matrix presented in Table 1, whereas customer purchasing behavior can vary very significantly across different product categories, such as buying CDs versus diapers. Therefore, we propose the micro-targeting approach in this paper that identifies local regions in the Product Type  X  Customer space that exhibit homogeneous behavior and builds local predictive models on these regions. In the next section, we present this approach. 
Similar to direct grouping methods, such as IM , micro targeting method makes locally optimal merging decisions on customer data to improve the overall performance fitness score. Unlike IM , however, this approach goes beyond grouping similar customers. Rather, it tries to identify local regions in the Product Type  X  Customer space exhibiting truly homogeneous behavior and then builds local predictive models on these regions maximizing predictive performance. 
In this paper, we present a specific micro-targeting method, called Iterative Merge Products ( IM_Prod ) that differs from IM in that the unit of analysis is not a single customer but a product category for a customer. The specifics of the IM_Prod algorithm are presented in Figure 1. 1. Let W = {TA(C 1 , P 1 ), TA(C 1 , P 2 ), ..., TA(C TA(C N , P L ) } // FIFO queue 5. CustomerGroup {CG i , P j } = W.pop() 7. {CG s , P h } = {CG k , P h } that yields maximum Starting from a single customer and a product type segment, IM_Prod iteratively seeks to merge existing customer segments by combining data from two segments SegA and SegB similarly to IM . However, for the initial product type and customer specific segments that have very few transactions, where the number of sample points would not be sufficient to build meaningful predictive models, we proceed as follows. We use clustering techniques to group customers X  product-specific transactions based on the Euclidian distances between customer X  X  product type and demographic summary statistics vectors so that each cluster results in a set of at least ten purchase transactions for different customers and product types. Having this minimal threshold number of transactions helps to produce more meaningful initial predictive models. 
Also note that after IM_Prod started to merge existing segments, it does not constrain products of different type from grouping into the same segment, nor does it constrain the same customer from having membership in multiple segments. Therefore, IM_Prod constitutes a generalization of IM in the sense that the unit of analysis is more  X  X ranular X  for IM_Prod than for IM . Thus, we expect IM_Prod to perform at least as well as IM if both methods examine all possible regions in the Product Type  X  Customer space. However, IM_Prod does not necessarily outperform IM in practice since there are possible locally optimal customer combinations that IM examines and forms which IM_Prod does not consider due to its greedy nature. For example, IM may group customers C together, whereas IM_Prod may not produce such group if IM_Prod had grouped C 1  X  X  product P 1 with C 5  X  X  product P C  X  X  product P 3 into one segment during a previous iteration. Thus, while IM_Prod searches over a bigger solution space, it still may not perform better than IM due to this reason. 
As IM , IM_Prod runs in O(N 3 ) time in the worst case assuming that the number of product types tend to be for the case of IM , the search space of IM_Prod is not very large in practice because it merges segments, not individual customers, at a time. Thus the computational performance of IM_Prod really depends on the number of segments being processed for possible grouping rather than the number of customers N . Our empirical results reported in Section 6 also confirm this observation. 
To compare the relative performance of statistics-based, direct grouping, and micro targeting approaches, we conduct pair-wise performance comparisons using a variant of the non-parametric Mann-Whitney rank test [15] to test whether the fitness score distributions of two different methods are statistically different from each other. To ensure robustness of our findings, we set up the pair-wise comparisons across the following four dimensions: 1. Types of datasets. We used the following datasets: (a) Two  X  X eal-world X  marketing datasets containing panel data (data about a pre-selected group of consumers on whom a comprehensive set of demographic information is collected along with the complete set of their purchases data) of on-line browsing and purchasing activities of Web site visitors and of beverage purchasing activities of  X  X rick-and-mortar X  stores. The first dataset contains ComScore data from Media Metrix on Internet browsing and buying behaviors of 100,000 users http://wrds.wharton.upenn.edu/ ). The second dataset contains Nielsen panelist data on beverage shopping behaviors of 1,566 families for a period of one year. 
The ComScore and Nielsen marketing datasets are very different in terms of the type of purchase transactions (Internet vs. physical purchases), variety of product purchases, number of individual families covered, and the variety of demographics. Compared to Nielsen X  X  beverage purchases in local supermarkets, ComScore dataset covers a much wider range of products and demographics. We further split these two datasets into four datasets of ComScore high-and low-volume customers, which represents the top and bottom 2,230 customers in terms of transaction frequencies respectively; similarly, Nielsen high-and low-volume customer data was generated using the top and bottom 156 customers in terms of transaction frequencies respectively. (b) Two simulated datasets representing high-volume ( Syn-High ) and low-volume customers ( Syn-Low ) who performed many and few transactions respectively, where within each dataset, transactions for customer i where generated from the summary statistics vectors S i as follows. A unique customer summary statistics vector S i was generated for each of the 2048 customers by sampling from ComScore customer summary statistics distributions, which is then used to generate the purchase transactions with four transactional variables. The number of transactions per customer is also determined from ComScore customer transaction distributions. Rather than generating artificial datasets with normal data distributions, we feel that synthetic dataset that simulates real world transactional datasets is better suited in testing our approach. 
Since for the ComScore and Nielsen we consider two datasets (each having high-and low-volume customers), this means that we use six datasets in total. Their characteristics are summarized in Table 2. In particular, CustomerType column specifies the transaction frequency of these datasets, High meaning that customers perform many transactions on average, while Low means only few transactions per customer. The columns  X % of Total Population X ,  X  X amilies X , and  X  X otalTransactions X  specify the percentage of total data population, the number of families, and the sample family transactions contained in the sample datasets. 2. Types of predictive models. We build predictive models using two types of classifiers in Weka 3.4 [22]: C4.5 decision tree [19] and Na X ve Bayes [14]. These were chosen because they represent popular and fast-to-generate classifiers. 3. Dependent variables. We built various models to make predictions of transactional variables, TR ij , and compare discussed approaches across different experimental settings. Examples of some of the dependent variables are day of the week, product price, category of website in ComScore datasets, and category of drinks bought, total price, and day of the week in the Neilson datasets. The data we used to train any one model are independent variables X 1 , X 2 ,..., X p Section 2, except previously chosen variable TR ij . 4. Performance measures. We use the following performance measures: percentage of correctly classified instances (CCI), root mean squared error (RME), and relative absolute error (RAE) [22], all measured on the holdout sample as described in Section 2. 
For models  X  and  X  ,  X  is considered  X  X etter X  than  X  provides better classification results and fewer errors: when the fitness function which we use in IM_Prod and IM to select the best possible merge during every iteration. To determine the best segment level in HC , the CCI, RME, and RAE distributions of different segment levels are compared separately in choosing the best performing segment level that has the most right-skewed CCI distribution and left skewed RME and RAE distributions. 
In terms of data pre-processing, we discretized our datasets to improve classification speed and performance [5]. Nominal transaction attributes, such as product categories, were discretized to roughly equal representation in sample data to avoid overly optimistic classification due to highly skewed class priors. We also discretized continuous valued attributes such as price via our implementation of Fayyad X  X  [7] recursive minimal entropy partitioning algorithm. 
We compared statistical, direct grouping, and micro targeting based methods across all three dependent variables, six datasets, two classifiers and three performance measures to determine the best method. The results of these comparisons are reported in the next section. Since we have compared performance of IM with other statistical and direct grouping methods in [11, 12] already, and since IM_Prod dominates the  X  X est-of-breed X  statistical and direct grouping methods, as shown in Section 6, it means that IM_Prod should dominate these other methods considered in [11, 12], and, therefore, we did not perform these additional comparisons in this work. 
In this section, we present our empirical findings. As mentioned in Section 5, we compare the distribution of performance measures generated by the aforementioned predictive models for individual segments across various experimental conditions. Since we make no assumptions about the shape of the generated performance measure distributions, and the number of sample points differ across distributions as a result of different partition schemes, we use a variant of the non-parametric Mann-Whitney rank test [15] to test whether the distribution of performance measures of the one method is statistically different from another method. For example, to compare IM against the IM_Prod method for the CCI measure, assume IM_Prod generated 150 customer segments and IM generated 50 customer segments for the purpose of grouping customers in a locally optimal way to predict a customer X  X  purchase on a given website. We want to test the null hypothesis that the two distributions of CCI measure generated from the predictive models built on the segments generated by the IM_Prod and IM methods are not different. As mentioned before, we apply the Mann-Whitney rank test to statistically compare distributions of the data points generated by IM_Prod vs. the data points generated by IM to determine which method produces a better set of customer segments for predicting customers X  next purchase. 
More generally, the null hypothesis for comparing distributions generated by methods A and B for a performance measure is: (I) H 0 : The distribution of a performance measure generated H 1 +:The distribution of a performance measure generated H 1 -: The distribution of a performance measure generated 
To test these null hypotheses across distributions of performance measures generated by HC , AP , IM , and IM_Prod methods described in Sections 3 and 4, we proceeded as follows. We ran HC , AP , IM , and IM_Prod on the ComScore, Nielsen, and synthetic data and generated sets of customer segments for each of these methods and various predictive models and fitness functions. Furthermore, we generated sets of CCI, RME, and RAE scores from 36 predictive models (the combination of 6 datasets for the 2 types of customers, 2 classifiers, C4.5 and Na X ve Bayes, and 3 dependent variables) for the total of 108 performance distributions. We then compared the sets of CCI, RME, and RAE scores generated from predictive models of the IM_Prod generated segments vs. that of HC , AP , and IM generated segments to see if better performance is achieved with IM_Prod than with other methods. Table 3 lists the number of Mann-Whitney tests rejecting the null hypothesis (I) at 95% significance level for all the pairwise comparisons of HC , AP , and IM methods across 108 statistical distribution comparisons (methods listed in the leftmost column are compared against the method listed across the top row). 
As Table 3 shows, the IM_Prod method overwhelmingly dominates the other three methods in all of the 108 tests. Similarly, IM also dominates HC and AP in all the 108 tests. From the results reported in Table 3, we conclude the following performance relationship among the direct grouping segmentation methods: HC&lt;AP&lt;IM&lt;IM_Prod . Table 3 demonstrates that performance differences between IM_Prod and IM are significant but does not provide quantitative measures of these differences. To show the extent of these differences, we plot some sample CCI distributions of the  X  X ay of the week X  predictions from the segments generated by IM_Prod versus that of IM on the data from both high and low volume ComScore datasets. These are only representative examples, and predictive models for other variables exhibit similar trends that we cannot present here because of the space limitation. Figures 2 and 3 do it for the high-and Figures 4 and 5 for the low-volume ComScore customers. As Figures 2  X  5 demonstrate, these distributions are significantly skewed to the right for IM_Prod in comparison to IM , which demonstrates the extent to which IM_Prod outperforms IM for the CCI measure across both high-and low-volume customers. 
Besides the clear performance dominance of IM_Prod versus that of IM, Figures 2  X  5 also demonstrate that IM_Prod generates significantly more segments than IM , which is not surprising since IM_Prod focuses on micro-targeting and partitions the data based not only on populations of customers but also based on product categories. Figures 2 and 4 also demonstrate that the CCI segment distribution among low volume customers generated by IM_Prod is more positively skewed than for the high volume customers. 
To gain further insight into the nature of the low volume CCI distributions, we present the segment size (i.e., number of customers in a segment) distributions in Figures 6  X  7 across the 6 customer type data sets. We note that, unlike IM generated segment size distributions (Figures 8-9), IM_Prod generated segments are predominately segments of size 4 and smaller. We also note that unlike what we observed with IM [12] where high volume customers have proportionally more smaller segment sizes (Figures 8-9), it is the low volume customers that have proportionally more smaller segment sizes in segments generated by IM_Prod . This formation of smaller and thus more homogeneous segments amongst low volume customer datasets help explain why CCI distribution for the low-volume customer segments in Figure 4 is more right skewed than for the high-volume customer segments in Figure 2. 
We noted in Section 1 that IM_Prod could assign individual customers to multiple customer and product type segments, where the maximum number of segment membership is limited to the total number of product types (in our case, there are eight product types in each of the 6 panel datasets). Therefore, we also studied to how many different segments the customers belong in various datasets, including high-vs. low-volume customers. From frequency distributions of customer segment memberships (Figures 10-11), we observe that high volume customers tend to get assigned to more segments than low volume customers. This makes sense because high volume customers have higher probability to purchase more product types and therefore get assigned to more product type specific segments. Besides CCI distributions, we also plot sample RME and RAE distributions generated from IM and IM_Prod (Figures 12-15) to demonstrate clear dominance of IM_Prod over IM as is evident from Figures 12 vs. 13 and 14 vs. 15. 
IM_Prod  X  X  dominance over IM is the consequence of our focus on finding more homogeneous regions in the Product Type  X  Customer space. To get a better sense of the performance impact in that space, we plot average CCI performance scores across segments of the same size in terms of purchases (as IM and IM_Prod generated segments have different meaning in terms of segment size -customers segmented by IM_Prod do not have the constraint of one customer can only belong to one segment -we cannot compare the segments based on the number of customers assigned to the segments; instead, for the visual analysis of average CCI performance in the Customer  X  Product space, we group segments together based on the number of purchases within segments) along with the distributions of segment sizes in a three-dimensional space (Figures 16-19). 
We observe from Figures 16 -19 that IM_Prod generated far more segments than IM , the segments tend to be smaller in terms of purchase size, and the predictive models build on these smaller segments perform better in terms of average CCI distributions. This supports our claim that individual customers exhibit different kinds of purchasing behavior across different product types and that it is better to build predictive models for customers purchasing certain product types rather than for the customers across all the product types. 
Since we build predictive models for Customer  X  Product combinations with IM_Prod , this raises an important issue of computational performance of the IM_Prod method because the search space increases significantly in this case in comparison to IM . For example, the search space for IM_Prod has increased 8-fold in our experiments given the 8 product categories. Although the performance of both IM and IM_Prod is O(N 3 ) , as shown in Sections 3 and 4, we compared them experimentally to see how much they differe in practice. The results of this comparison are presented in Figure 20 where the distribution of ratios of run times of IM_Prod over that of IM are plotted for all 36 pair-wise comparisons (across 6 datasets, 2 classifiers, and 3 predictive variables per dataset). As Figure 20 demonstrates, the ratio of computational performance of IM_Prod over IM is never more than 5.5, and this particular ratio is reached quite infrequently according to Figure 20. In most of the cases, this ratio is 3 or less. These lower than expected ratios indicate that while the combinatorial explosion in the search space alludes to the significantly higher performance ratios, as was shown above and demonstrated in Figures 6  X  9, IM_Prod tends to generate segments of smaller sizes and converge to the final solution faster than IM . This observation explains why IM_Prod does not incur significantly more computational expenses than IM , as is empirically shown in Figure 20 and argued in Section 4. 
In this paper, we proposed a micro-targeting approach to address the problem of optimal partitioning of customer bases into homogeneous segments for building better predictive models of customer behavior. This approach extends previously proposed direct grouping method by directly combining product-specific transactional data of one or several customers and building a single local model of customer behavior on this combined data. We presented a polynomial-time direct grouping method, IM_Prod that identifies segments of customers and product types and builds predictive models on these local regions in the Product-Type  X  Customer space. We compared performance of IM_Prod against the traditional statistics-based hierarchical, affinity propagation clustering and the direct grouping method IM . We showed that IM_Prod significantly dominated by a wide margin all other methods across all the experimental conditions and did not incur additional substantial computational expenses in comparison to IM . We then examined the segments generated by IM_Prod and observed that there were many small size segments, and that customers were often assigned to multiple segments. Since most of the generated segments are small, this provides strong support for the micro-targeting approach to personalization. By identifying local regions in the Product Type  X  Customer space that exhibit truly homogeneous behavior and building local predictive models on these regions, the micro-targeting approach increases flexibility of the predictive models describing customer behavior, improves the overall predictive performance, and keeps computational costs at the same level as other direct grouping methods. 
As a future research, we would like to test the effectiveness of our segmentation strategies not only in terms of predictive performance but also in terms of the standard marketing oriented performance measures such as customer value, profitability and other economics-based performance measures. 
