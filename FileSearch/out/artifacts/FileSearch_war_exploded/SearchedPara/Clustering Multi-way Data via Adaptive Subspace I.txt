 Clustering multi-way data is a very important research topic due to the intrinsic rich structures in real-world datasets. In this paper, we propose the subspace clusteri ng algorithm on multi-way data, called ASI-T (Adaptive Subspace Iteration on Tensor). ASI-T is a special version of High Order SVD (HOSVD), and it simulta-neously performs subspace identification using 2DSVD and data clustering using K-Means. The experimental results on synthetic data and real-world data demonstrate the effectiveness of ASI-T. Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering-Algorithms General Terms: Algorithms, Experimentation, Performance Keywords: Multi-way data, Tensor, Clustering, Subspace
Multi-way data or tensors can be represented as X  X  d 1 , When m = 3, X is a three-way data with three modes: data units, features, occasions. The three-way data can be matricized to form a flattened matrix. The sum-up matrix is ( X =  X  i ( X i ) is the i -th frontal slice. Multi-way data naturally appear in many applications such as webpage personalization and high-order web link analysis [8]. One way to cluster three-way data is to convert the three-way data into two-way matrices, but this approach may re-sult in information loss and fail to capture the underlying structures in three-way datasets [1]. On the other hand, tensor factorization methods can be used to cluster three-way data by discretizing their component matrices. There are generally two types of tensor de-composition models: Rank-1 Decomposition [10] and Tucker De-composition including HOSVD and 2DSVD [12, 5]. Many multi-way models can be considered as the extensions or modifications of the above two types.

Despite significant progress made on subspace clustering for two-way data, few attempts have been made to develop subspace clus-tering algorithms on three-way data. Most tensor factorization mod-els only deal with the subspace selection (data reduction) prob-lems. In this paper, we propose a subspace clustering algorithm on multi-way data via adaptive subspace iteration, called ASI-T . ASI-T model is a special version of HOSVD model. We show that the clustering algorithm is also equivalent to K-Means clustering and 2DSVD, and it is a subspace clustering extension on three-way data. More specifically, ASI-T consists of two simultaneous steps: select the subspaces using 2DSVD (identifying the subspace structure in mode 2 and mode 3 of the tensor from the current data clusters) and cluster the three-way data units (mode 1) using K-Means clustering. These two tasks are performed alternatively and iteratively to achieve the stable clustering partitions and subspaces.
Notations Scalars are denoted by lowercase letters, e.g. x ,and vectors are denoted by boldface lowercase letters, e.g. x ,wherethe i -th entry is x i . Matrices are denoted by boldface capital letter, e.g. X ,wherethe i -th row of matrix X is x i . ,the j -th column of matrix denoted by boldface underline letters X ,andthe ( i , j , x . X n 1 , n 2 n 3 is denoted as the flattened matrix by matricizing X in the first mode. The Kronecker product  X  is the operation between two matrices such that Kronecker product of the matrix A  X  a and the matrix B  X  c  X  d is the matrix C  X  ac  X  bd , where each entry is the product of two entries from A and B respectively. We present a brief overview of various tensor factorization models.
Rank-1 Decomposition: The objective function for Rank-1 de-composition can be written as where U  X  n 1  X  K , V  X  n 2  X  K , W  X  n 3  X  K . Rank-1 decomposition is also called Parafac [10] w ith orthogona lity constraints. 2DSVD: 2DSVD is an extension of SVD that it approximates each frontal slices of the three-way data X by minimizing
J HOSVD: HOSVD (High Order SVD) minimizes ASI-T Model: ASI-T model minimizes G D is a binary and row-stochastic matrix, i.e., there is only one entry with value 1 in each row, and other entries are all 0s. ASI-T is equivalent to simultaneous K-Means clustering and 2DSVD. The objective function in Eq.(4) of ASI-T can be transformed into Note that Eq.(6) is the same as the K-Means objective function where D is the cluster indication matrix (binary and row-stochastic) and Z k 1 , n 2 n 3 is the cluster centroid matrix. Moreover, note z  X  , q y ipq f jp g lq . This leads to a formulation of 2DSVD approxima-tion as shown in Eq.(2). Thus ASI-T clusters data units using K-Means and compresses data using 2DSVD simultaneously. ASI-T can also be viewed as an extension of two-way subspace clustering on three-way data [4, 9]. The derivation is omitted.

We use the alternating least square algorithm to optimize D , F and G by updating one while fixing the others iteratively until con-vergence. Let M = D ( D T D )  X  1 D T . Then the objective function can that we need to maximize the second part.
 Update F : We maximize The optimal F can be obtained by taking the first k 2 eigenvectors of X Update G : G can be optimized by maximizing
Update D : D is updated by assigning each data unit to a cluster.
Five Synthetic data are generated by using the algorithm pro-posed by Milligan [7] with different configurations. The real datasets are extracted from the DBLP computer science bibliography that can be downloaded at http://www.informatik. uni-trier.de/ The data are three-way arrays with the author, term, and year modes. We conduct experiments on two such three-way datasets, one of which is DBLP1000 (1000 authors  X  1000 terms  X  20 years), the other of which is DBLP100 (100 authors  X  200 terms  X  20 years). Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and Accuracy (ACC) are used as our performance measures. Gen-erally, the larger the values of these measures, the better the clus-tering performance. The clustering performance of ASI-T is com-pared with a wide range of clustering algorithms: 2 Tensor factor-ization methods: (1) Rank-1 approximation method and (2) HOSVD ; and 7 Two-way data clustering methods: (3) KMeans(sum) :K-Means on the sum-up matrix (authors  X  terms); (4) KMeans(ext) : K-Means on the unfolded matrix in the first mode of the three-way array; (5) KMeans(pca) : Perform PCA first on the unfolded ma-trix in the first mode and then use K-Means algorithm; (6) InfoCo : Run information theoretic co-clustering algorithm [3] on the sum-up matrix; (7) EuclCo : Run Euclidean co-clustering algorithm [2] on the sum-up matrix; (8) MinSqCo : Performs minimum squared residue co-clustering algorithm [2] on the sum-up matrix. (9) Clus-terAgg : Run K-Means clustering on each frontal slice of the three-way array, and combine them using clustering aggregation [11]. The clustering results are computed by averaging ten runs. Table 1: The clustering performance comparison on 2 DBLP datasets among 10 clustering methods.
 Methods DBLP100 DBLP1000 KMeans(sum) 0.523 0.675 0.479 0.157 0.388 0.319 KMeans(ext) 0.106 0.400 0.228 0.004 0.250 0.014 KMeans(pca) 0.312 0.550 0.330 0.199 0.424 0.342 Rank-1 0.664 0.800 0.612 0.154 0.386 0.189 HOSVD 0.416 0.700 0.535 0.187 0.408 0.204 ClusterAgg 0.654 0.815 0.653 0.099 0.305 0.187 InfoCo 0.510 0.775 0.510 0.253 0.415 0.245 EuclCo 0.506 0.675 0.551 0.129 0.361 0.207 MinSqCo 0.351 0.600 0.410 0.218 0.406 0.319 ASI-T 0.657 0.825 0.672 0.415 0.480 0.464
The clustering performance on five synthetic datasets are omit-ted due to space limitation. We observe that the best performance values are achieved by ASI-T among all these clustering methods with all measures on all synthetic datasets. The experimental re-sults on DBLP datasets are presented in Table 1. We observe that clustering performance of ASI-T on DBLP100 and DBLP1000 is the best. In higher dimensions the advantages of ASI-T become more evident because the three-way subspace clustering of ASI-T can overcome the curse of dimensionality . For the same reason, KMeans(pca) and three co-cluster ing algorithms perform relatively better on DBLP1000 compared to their clustering performance on DBLP100. KMeans(ext) is the worst clustering methods for the curse of dimensionality. Rank-1 approximation and ClusterAgg as-sume that frontal slices should share the similar patterns or similar clustering results, thus achieve relatively better clustering results on DBLP100 than they are on DBLP1000.
 Acknowledgments: The work of T. Li is partially supported by NSF under IIS-0546280, HRD-0317692, and IIP-0450552.
