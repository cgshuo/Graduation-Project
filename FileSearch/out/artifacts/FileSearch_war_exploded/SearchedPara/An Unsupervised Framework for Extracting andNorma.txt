 We have developed an unsupervised framework for simul-taneously extracting and normalizing attributes of products from multiple Web pages originated from different sites. Our framework is designed based on a probabilistic graphical model that can model the page-independent content infor-mation and the page-dependent layout information of the text fragments in Web pages. One characteristic of our framework is that previously unseen attributes can be dis-covered from the clue contained in the layout format of the text fragments. Our framework tackles both extraction and normalization tasks by jointly considering the relationship between the content and layout information. Dirichlet pro-cess prior is employed leading to another advantage that the number of discovered product attributes is unlimited. An unsupervised inference algorithm based on variational method is presented. The semantics of the normalized at-tributes can be visualized by examining the term weights in the model. Our framework can be applied to a wide range of Web mining applications such as product matching and re-trieval. We have conducted extensive experiments from four different domains consisting of over 300 Web pages from over 150 different Web sites, demonstrating the robustness and effectiveness of our framework.
 I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms  X 
The work described in this paper is substantially sup-ported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Nos: CUHK4193/04E and CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Com-puting and Interface Technologies.
 Web mining, attribute extraction, attribute normalization
The World Wide Web (WWW) contains a huge number of online stores selling million of different kinds of products. While online stores can reduce the geographical barrier and the time constraint for shopping, it becomes problematic for a user to retrieve, analyze, and compare products. For ex-ample, Figure 1 shows a sample of a portion of a Web page about some product information of a digital camera which consists of several product attributes such as resolution, flash mode, etc. Traditional search engines whose retrieval methods treat every term in a Web document in a uniform fashion often result in ineffective product attribute informa-tion extraction and analysis. For example, in the digital camera domain, if we supply the query terms  X  X uto white balance X , existing search engines may just match the terms in Web pages and return products without attribute content  X  X uto white balance X  but with attribute content  X  X uto ISO X . In fact,  X  X uto white balance X  and  X  X uto ISO X  refer to the values of the attribute  X  X hite balance X  and  X  X ight sensitiv-ity X , which should be regarded as two different attributes of a digital camera, and they should be properly differentiated. Manually identifying product attributes is tedious and time consuming, and practically infeasible for the massive amount of Web sites. As a result, it raises the need for automated methods which can identify the attributes of products effec-tively from Web pages. If the product attribute information is extracted from multiple Web sites, another desirable task is that the product attributes can be automatically normal-ized and preferably the semantic meaning of normalized at-tributes can be obtained. This can improve the indexing of product Web pages, and support intelligent tasks such as attribute search or product matching.

Existing information extraction approaches aim at extract-ing precise text fragments from documents [11]. In partic-ular wrapper learning techniques have been developed to extract information from semi-structured documents such as Web pages [18]. For example, by collecting training ex-amples, which consists of certain product attributes, from someWebpagesinthesiteshowninFigure1,onecanlearn a wrapper for automatically extracting information from the remaining pages in the same Web site. However, one ma-jor limitation of existing wrapper learning methods is that they are supervised methods and hence they require manual effort in preparing training examples for every product at-tribute. Moreover, the learned wrapper can only be applied to the Web site where the training examples come from. For instance, the learned wrapper for Figure 1 cannot be applied to the Web site site shown in Figure 2 because of the differ-ence between the layout formats of the two sites. A separate human work is needed to prepare a new set of training ex-amples. As a consequence, existing wrapper construction or learning methods are not scalable if we wish to extract information from numerous different Web sites.

Recently, Zhu et al. have developed a system which can segment Web pages and label the elements of Web pages from different sources [19]. Their method analyzes the lay-out format of Web pages and employs an integrated ap-proach of Hierarchical Conditional Random Fields (HCRF) and Semi-Conditional Random Fields (Semi-CRF) for seg-menting and labeling the text fragments in a single frame-work. Their approach is template independent. Though this integrated method can partially solve the problem of wrap-per learning, one limitation is that it is a supervised learning method and one has to define the set of possible product at-tributes in advance and provide training examples for each attribute. In other words, it cannot handle the discovery of previously unseen attributes.

To reduce the human work involved, several unsupervised wrapper learning techniques have been proposed [5] by mak-ing use of the layout format of Web pages which are gen-erated by templates. Since the extraction is template de-pendent, the data extracted from different sites, even in the same domain, may not be synchronized. For example, a field extracted from a particular site may contain both book title and author, whereas in another site, book title and author correspond to two different extracted fields. Chuang et al. proposed an unsupervised wrapper learning technique which can construct wrappers to extract synchronized data from multiple sources [4]. The objective is to identify the opti-mal segmentation of the text in Web pages. For example, the field containing both book title and author can be au-tomatically segmented into two separate fields or attributes However, their method requires to train a field model for each field. For example, there are two different field models for book title and author. These field models are required to be trained from manually prepared training examples, or developed by human experts in advance costing substantial human effort. Moreover, it cannot handle previously unseen fields of records. They proposed a heuristic methodology for training the field models for previously unseen fields in an unsupervised manner. The idea is to consider each group of aligned segments created by an unsupervised wrapper as a single field, and train a field model for each group us-ing HMM with a predefined labeling rule. However, such method can only apply to a Web page that contains multi-ple records. For Web pages with a single record, such as the ones in Figures 1 and 2, there exists neither group of aligned segments, nor a single group in which the aligned segments refer to different fields.

Another limitation of existing unsupervised wrapper learn-ing methods is that the extracted fields from different Web sites are not normalized, and hence requiring human work to judge whether two extracted fields refer to the same at-tribute. For example, one may not know that the extracted text fragments  X  X ireworks X  and  X  portrait X  refer to two differ-ent attribute values of the same attribute  X  X hooting mode X  in the digital camera domain. Attribute normalization is defined as clustering attribute values with similar semantic meaning. It is useful for many applications such as storing attribute values of product records into structured database, retrieving and matching of products, etc. Chuang et al. proposed a clustering method to match the extracted data based on the tokens of the data in a separate step [4]. How-ever, since their method mainly considers the tokens in text fragments, it is not able to normalize the text fragments  X  X ireworks X  and  X  X ortrait X  to t he same attribute. Moreover, it requires to fix the number of clusters in advance. In prac-tice, the number of attributes in a domain is unknown, and new features are found in a domain from time to time re-sulting in an unlimited number of attributes.

The requirement of training examples, the incapability of discovering unseen attributes, and the lack of normalization of extracted attributes with similar semantic meaning are the limitations of existing approaches. In this paper, we aim at addressing these problems by developing an unsupervised learning framework for jointly extracting and normalizing product attributes from multiple Web sites.
Consider the Web pages shown in Figures 1 and 2. These two Web pages are collected from two different Web sites in the digital camera domain describing two different digi-tal cameras. Naturally, they have different layout formats because they come from different Web sites. We define at-tribute and attribute value as a field of a product and a value for a particular field respectively. For example one attribute of a digital camera is  X  X ensor resolution X  and the attribute values are  X  X ffective sensor resolution 10,100,000 pixels X  and  X  X ensor resolution 10 megapixels X  for the products in Fig-ures 1 and 2 respectively. To extract the product attribute values from these two pages, one can make use of two wrap-pers, which must be previously learned for each individual Web site, to accomplish the task. However, as described before, human work is needed to prepare training examples for wrapper learning and the attributes to be extracted are required to be defined.

Very often, users may have some prior knowledge about the content of some attributes of interest in the domain. For example, users may know that some terms such as  X  X  X egapixel X  and  X  X SO X  are frequently used to describe a digital camera. Such prior knowledge can be easily collected, for example, by scanning one Web page about digital cameras and collecting a few terms in a list. We can utilize the prior knowledge and infer from the content of the text fragments in Web pages that the text fragment  X  X ensor resolution 10 megapixels X  in Figure 2 likely corresponds to an attribute value. However, there may be some previously unseen attributes. For exam-ple, from the layout format of the Web page in Figure 2, it can be inferred that the text fragment  X  X hite balance auto, daylight, cloudy, tungsten, fluorescent, fluorescent H, cus-tom X  should be an attribute value because the layout format of this text fragment share certain similarity to the extracted text fragment  X  X ensor resolution 10 megapixels X . It likely corresponds to an attribute value of a previously unseen at-tribute  X  X hite balance X . Similarly, more attribute values, which correspond to some previously unseen attributes such as  X  X hutter speed X , can be discovered from both Figures 1 and 2. This shows that there is mutual influence between the content and layout format of product attributes in Web pages. This provides useful clues for extracting attribute values of previously unseen attributes.

The above scenario demonstrates the possibility of mak-ing use of the layout format of text fragments for extrac-tion. The next issue is the requirement of human effort to interpret the semantic meanings of the attribute values of the previously unseen attributes. For example, suppose there is an extracted text fragment  X  X luorescent X  in other Web pages. The text fragment  X  X hite balance auto, day-light, cloudy, tungsten, fluorescent, fluorescent H, custom X  is extracted from the Web page shown in Figure 2. Suppose that these two text fragments are automatically clustered to the same group representing an attribute. One can easily observe that they refer to the same attribute corresponding to  X  X hite balance X . This allows better understanding and interpretation of the semantic meaning of the normalized attribute because of some indicative terms such as  X  X hite balance X  appeared in the majority of the text fragments in the same group.

After product attributes are extracted and normalized, a product can be effectively represented. It is useful for indexing the terms and conduc ting other intelligent tasks such as product matching and comparison.
We have developed an unsupervised learning framework for jointly extracting and normalizing product attributes from multiple Web sites. For example, the text fragments  X  X ireworks X  are  X  X ortrait X  are samples of extracted and nor-malized text fragments in the digital camera domain using our method. These two fragments do not have words in common, but actually they refer to the product attribute  X  X hooting mode X  in the digital camera domain. Unlike exist-ing methods which conduct the extraction and normalization tasks in separate steps unavoidably leading to the accumu-lation of errors, we propose a single framework which can conduct extraction and normalization tasks simultaneously in an unsupervised manner. We also demonstrate in our mathematical formulation that considering both the content information and the layout information can resolve the con-flict between the two tasks.

Our framework considers the page-independent content information and the page-dependent layout information in a single framework. As illustrated in the above motivat-ing example, the mutual influence between the content and the layout format of text fragments provides useful clues for attribute extraction and normalization tasks. We de-sign a probabilistic graphical model to model the relation-ship between the content and layout information for solving the two tasks simultaneously. We employ Dirichlet process prior leading to another characteristic that the number of attributes to be discovered needs not to be fixed and can be unlimited. This can handle product attributes not known in advance and new attributes can be discovered.

The semantic meaning of the extracted and normalized attributes can be visualized by a set of weighted terms in the model. This can significantly help users understand and interpret the attributes. Our framework can be applied to applications such as improving product searching based on attributes and Web online product matching. We have con-ducted extensive experiments from four different domains consisting of over 300 Web pages from over 150 Web sites. The experimental results show that our framework is robust and effective.
In a product domain D ,wehaveasetof reference at-tributes , denoted by A , to describe the products. Let a the i -th attribute in A . For example, in the digital camera domain, reference attributes of digital camera may include  X  X esolution X ,  X  X hite balance X ,  X  X ight sensitivity X , etc. There exists a special element denoted by a representing  X  X ot-an-attribute X . Since the number of attributes is unknown and hence the size of A denoted by | A | is between 0 and  X  .Each product r in D is then characterized by the attribute values of the reference attributes. Let v i ( r ) be the attribute value of the reference attribute a i for product r . For instance, the attribute value of the reference attribute  X  X esolution X  shown in Figure 1 is  X 10.0 Megapixel X .

Given a collection of product Web pages C collected from a set of Web sites S .Let c i ( s )be i -th page collected from the site s . Each page contains a single product p .Within the Web page c i ( s ), we can collect a set of text fragments X ( c i ( s )). For example,  X  X esolution 10,100,000 pixels X  and  X  X ptical sensor type CCD X  are samples of text fragments collected from the page shown in Figure 1. Let x j ( c i ( the j -th text fragment in the Web page c i ( s ). Essentially, each x in X ( c i ( s )) can be represented by a four-field tu-ple ( C, L, T, A ). C refers to the content information of the text fragment such as the tokens contained in  X  X ptical sensor type CCD X . L refers to the layout information of the text fragment. For example, the text fragment  X  X eature X  is grey and in larger font size. T , defined as the target information , is a binary variable which is equal to 1 if the underlying text fragment is an attribute value, and 0 otherwise. For example, the values of T for the text fragments  X  X eature X  and  X  X ptical sensor type CCD X  are 0 and 1 respectively. A defined as the attribute information , refers to the reference attribute to which the underlying text fragment belongs. It is a realization of A and hence it must be equal to one of the elements in A . For example, the values of A for the text fragments  X  X esolution 10,100,000 pixels X  and  X  X hite balance auto, daylight, cloudy, tungsten, fluorescent, fluorescent H, customoptical X  should be equal to the reference attributes  X  X esolution X  and  X  X ptical sensor X  included in A respectively.
In practice, the content information C and the layout in-formation L of a text fragment can be observed from Web pages. However, the target information T and the attribute information A cannot be observed. As a result, given the observation of C and L , product attribute extraction can be formulated as the prediction for the value of T for each text fragment in Web pages aiming at discovering all text frag-ments corresponding to certain attribute values. Formally, for each text fragment, we aim at finding T = t  X  such that =argmax be defined as the prediction of the value of A for each text fragment, so that one can understand the reference attribute to which the underlying text fragment refers. Formally, for each text fragment, we aim at finding A = a  X  such that a  X  argmax for some a  X  A \{ a } and P ( A = a | C, L )=0. When T =0, P (
A = a | C, L ) = 1. Obviously, P ( T | C, L )and P ( A | C, L are dependent since P ( A | T =0 ,C,L ) = P ( A | C, L ). As a result, conducting product attribute extraction and nor-malization separately may lead to conflicting solutions de-grading the performance of both tasks. In our framework, we aim at predicting the values of T and A such that the joint probability P ( T,A | C, L ) can be maximized leading to a solution optimizing both tasks.
Our model can be regarded as an extension of Dirichlet mixture model. Each mixture component, which refers to a reference attribute in our framework, consists of its own distribution about text fragments. Dirichlet process prior is employed so that our framework can handle unlimited num-ber of reference attributes. Figure 3 shows the plate diagram representation of our model. Shaded nodes and unshaded nodes represent the observable and unobservable variables respectively. The edges represent the dependence between variables and the plates represent the repetition of variables. We adopt the stick breaking construction representation of Dirichlet process [15] in our presentation.

Suppose we have a collection of N different text frag-ments collected from S different Web pages. Each gen-eration of a text fragment is modeled as an independent and identical event. The n -th text fragment x n consists of an unobservable variable Z n depending on the variables  X  = {  X  1 , X  2 ,... } . Z n represents the index of the mixture component from which the underlying text fragment is gen-erated. Essentially, we use Z n to replace A n for clarity and A n = a z n where a i  X  A . Next, the content information of the text fragment, denoted as C n , is generated according to about the content C given the variable  X  C k ;and k refers to the k -th mixture component. The target information T n is distribution about the target information T given the vari-able  X  T k . Since the layout format of the text fragments in a Web page is page-dependent, we have a set of layout distri-butions, namely,  X  L s , for generating the layout format of the text fragments in page s . As shown in the running example in Section 1.1, there is mutual influence between the layout information and the target information of a text fragment. T n together with  X  L s will generate the layout information of the n -th text fragment according to P L ( L n | T n , X  L where P L (  X | T n , X  L s ) is the probability distribution about the layout information L given the variables T n and  X  L s ;and s ( x n ) denotes the Web page from which x n is collected.
Unlike ordinary Dirichlet mixture models in literature, in which they consider only one distribution in each mixture component, our framework consists of two different distri-butions characterized by  X  C k and  X  T k for the k -th component. k and tions G C 0 and G T 0 respectively in the Dirichlet process. and G T 0 act as the prior distributions of the content infor-mation and the target information respectively. For exam-ple, suppose we model the content of the text fragments by a mixture model of tokens, G C 0 can be a Dirichlet distribution which is the conjugate prior of a mixture model, P C (  X |  X  C a multinomial distribution, and  X  C k is the set of parameters of multinomial distribution in component k . Similarly, Since T is a binary variable, it can be modeled as a Bernoulli trial. Therefore, P T (  X |  X  T k ) can be a binomial distribution with pa-rameter  X  T k and G T 0 can be a Beta distribution, which is the conjugate prior of a binomial distribution.

Recall that the Dirichlet process is represented by the stick breaking construction in the graphical model depicted in Figure 3. In the stick breaking construction, we have a one-unit length stick and we break a  X  k portion from the re-maining portion of the stick according to Beta (1 , X  )inthe k -th break, where Beta (  X  1 , X  2 ) is the Beta distribution, with parameters  X  1 and  X  2 . The process repeats for infi-nite times and hence the k -thpieceofthebrokenstickscan represent the proportion of k -th component in the mixture. Therefore, Dirichlet process prior can support an infinite number of mixture components, which refer to the product attributes in our framework. Z n is then drawn from the distribution  X  . In summary, the generation process can be described as follows: The joint probability for generating a particular text frag-ment x n given the parameters  X  , G C 0 , G T 0 ,and  X  L s be expressed as follows: where  X  { Z n = i } =1if Z n = i and 0 otherwise. For simplic-ity, we let O , U ,and  X  be the set of observable variables, which include all C n and L n , the set of unobservable vari-ables, which include all T n , Z n ,  X  C k ,  X  T k ,and  X  set of model parameters, which include  X , G C 0 , G T 0 , X  L spectively. Given a set of N text fragments X and the pa-rameters  X  , the inference problem is then defined as follows: Since the computation of log P ( U | O ,  X  )=log involves the marginalization of P ( U , O |  X  ), that is defined in Equation 1, over the unobservable variables, exactly solving Equation 2 is intractable. As a result, approximation meth-ods such as Markov Chain Monte Carlo (MCMC) algorithm are required. In this paper, we develop a variational method to tackle this problem. Recall that the objective of the inference is to compute P (
U | O ,  X  ), however, it is intractable. The main idea of our method is to design a tractable distribution Q ( U |  X  ), which is called the variational distribution of U characterized by a set of variational parameters denoted as  X  . The designed Q (
U |  X  ) should be as close to P ( U | O ,  X  ) as possible. The distance between any two probability distributions P and Q can be measured by Kullback-Leibler(KL) divergence de-fined as as D ( Q || P )= possible events. Therefore, our method aims at minimizing the following KL-divergence by altering the set of variational parameters: Since D ( Q || P )  X  0, we have: The left-hand-side (LHS) is the log likelihood of the obser-vation of all text fragments X given the model parameters; the right-hand-side (RHS) is the lower bound of the likeli-hood function. The minimization of D(Q( U |  X  ) || P( U | O  X  ) ) becomes the maximization of the bound on the RHS given the model parameters.

Using the original variable notation used in Figure 3, the bound can be expanded as follows: where T , Z ,  X  C ,  X  T ,and  X  represent the collection of vari-ables T n , Z n ,  X  C k ,  X  T k ,and  X  k respectively.
As described in Section 2.2, P (  X  k |  X  )canbesettotheBeta distribution Beta (1 , X  ). In our framework, we model the content information of text fragments as a mixture model of tokens in the set of vocabularies V .Hence P C (  X |  X  C is a multinomial distribution and G C 0 can be defined as the Dirichlet distribution G C 0 (  X |  X  C )where  X  C is the set of hyper-parameters. T n follows the binomial distribution where  X  T is the hyper-parameter. The layout information is modeled by a set of Bernoulli trials, denoted as F s .The outcome of each Bernoulli trial is whether the underlying text fragment possesses the f -th formatting feature in page s depending on the value of  X  L s and T n ,where1  X  f  X | F s Therefore, P ( L n | T n , X  L s ) is represented by a set of binomial distributions. According to stick breaking process, we can express: P ( Z n |  X  1 , X  2 ,... )= Next, we make use of the truncated stick-breaking process [6] and define Q ( T , Z ,  X  C ,  X  T ,  X  ) as follows: where K is the truncation level; Q  X  (  X |  X  k, 0 , X  k, 1 )istheBeta tribution with parameter set  X  ; Q T ( T n |  X  n ) is the binomial distribution with parameter  X  n ;and Q Z ( Z n |  X  n, 1 ,..., X  is the multinomial distribution with parameter set  X  n, 1 In the truncated stick breaking process, Q Z ( Z n |  X  n )=0for Z n &gt;K . Under this setting, all the terms in Equation 5 can be expressed in explicit form.

To maximize Equation 5, we can take the first derivative with respect to each of the variational variables and set to 0. Next we obtain the following optimal conditions: where w n,j = 1 if text fragment x n contains j -th token in the vocabulary V ,and0otherwise; X (  X  ), which is called the digamma function, is the first derivative of the log Gamma function; where and u n,f =1if x n contains the f -th layout format in F s page s = s ( x n ), and 0 otherwise;  X  L s,f is the parameter for the binomial distribution representing how likely a text fragment corresponding to a product attribute value contains the f th layout format in F s . Given the model parameters, one can then apply the steepest ascent algorithm, which is an iterative algorithm to update each variable at a time, until convergence.

Essentially, the attribute that x n belongs to can be de-cided by the values of  X  n,k for k =1 , 2 ,... ,eachofwhich represents how likely that x n is generated from the k -th mix-ture component. It can be observed that the value of  X  n,k depends on three different aspects in Equation 10. The first aspect is the prior proportion of the k -th component, which is characterized by the value of the variational parameters  X  k, 0 and  X  k, 1 . The second aspect is the content of x n is denoted by w n,j , and the token distribution in k -th com-ponent, which is characterized by  X  k,j .Thethirdaspect is the likelihood that x n belongs to an attribute, which is characterized by  X  n , and the prior distribution that a text fragment belongs to an attribute value in the k -th compo-nent, which is characterized by  X  k, 0 and  X  k, 1 . On the other hand, the probability that x n is an attribute value is rep-resented by the value of  X  n , which depends on other three aspects. The first aspect is  X  n,k which is the probability that x n belongs to the k -th component. The second aspect is the prior information about how likely a text fragment in the k -th component is an attribute value, characterized by the factor  X (  X  k, 0 )  X   X (  X  k, 1 ). The third aspect is the layout of x n , which is characterized by u n,f , and the factor about the layout format of an attribute, which is characterized by (log  X  L s,f  X  log (1  X   X  L s,f )). Interestingly, it shows that both normalization and extraction decision have mutual influence in the optimal condition according to Equations 10 and 11. In particular, Equation 11 is in the form of logistic regres-sion, which is discriminative in nature, considering a factor related to the k -th component in the mixture, as well as the layout format of the text fragment. Consequently, our model can resolve the conflict between the extraction and normalization tasks and achieve an optimal solution.
As described in the previous section, our model can tackle the tasks of extraction and normalization of product at-tributes by satisfying the optimal condition stated in Equa-tions 7-11 given the model parameters. We have developed a method which can automatically determine the model pa-rameters and initialize a steepest ascent algorithm, achieving unsupervised extraction and normalization.

As exemplified in Section 1.1, our framework can consider the page-dependent layout format of text fragments to en-hance extraction. However, the layout information of an unseen Web page is unknown and hence we cannot prede-fine or estimate the values of  X  L s,f . As a result, we develop an Expectation-Maximization (EM) algorithm based on our variational method to estimate the values of all  X  L s,f in page s . By taking the first derivative of Equation 5 with respect to each  X  L s,f and set it to zero, we can obtain the following formula: This is the optimal value of each  X  L s,f given other model parameters and the current set o f variational parameters. The E-step and M-step are defined as follows: E-step:
Apply steepest ascent algorithm until convergence to achieve the optimal conditions depicted in Equations 7-11. M-step: Calculate each  X  L s,f using Equation 12.

To initialize the EM algorithm, we are required to esti-mate  X  n,k and  X  n for the n -th text fragment. To achieve this, we make use of the prior knowledge, which is in the form of a list of a few terms, denoted as  X  , related to prod-uct attributes. Let  X  i be the i -th term in the list. Notice that the terms are not required to be categorized into differ-ent attributes. In practice, this list can be easily obtained, for example, by scanning just one Web page containing a product in the underlying domain and highlighting the text fragments related to product attributes. For each  X  i ,wese-lect the i -th component in our model and set a higher value of  X  i,j if  X  i is equal to the v j  X  V , and zero otherwise. In particular, we set to 10 for such  X  k,j .Next,forthesecompo-nents, we set  X  i, 0 =6and  X  i, 1 = 4 which essentially means that 6 out of 10 text fragments in this component will be the text fragments related to attribute values.  X  k, 0 and  X  k, 1 set to 4 and 6 respectively for other components.  X  n can then be calculated according to Equation 11. Notice that these values are used in initialization only. Updated values will be automatically calculated via the EM algorithm.
For the model parameters,  X  is the scaling parameter be-tween 0 and 1 in the Dirichlet process, which essentially affects the number of normalized attributes in the normal-ization process. Since we apply our framework to the do-mains, for example, digital camera, in which each product contains a number of attributes, we set  X  to a value that favors large number of normalized attributes. In particular we set  X  to 0.1.  X  T j refers to the prior knowledge about how likely a text fragment will be an attribute value. We treat it as an uninformative prior and set  X  T 0 =  X  T 1 = 1. Simi-larly,  X  C j are treated as uninformative as all  X  C j are set to 1. The truncation level K of truncated stick breaking process is set to, 500, a relatively large value so that attributes are normalized in a finer granularity manner.
We have conducted extensive experiments on four dif-ferent domains, namely, digital camera, MP3 player, cam-corder, and restaurant domains to evaluate our framework. We have collected 85 Web pages from 41 sites, 96 Web pages from 62 sites, and 111 Web pages from 61 sites in the digital camera, MP3 player, and camcorder domains respectively. In these three domains, each Web page contains one prod-uct and a number of product attributes. The dataset of the restaurant domain contains 29 Web pages from the LA-weekly Restaurant Guide 1 . Each page contains attributes including names, addresses, phone numbers, and reviews from customers for one or more restaurants. We use a sim-ple method which considers the line separators such as the from all Web pages. The layout format of each text frag-ment is automatically recorded during the collection process. For evaluation purpose, two human accessors were invited to annotate each text fragment by indicating whether it is a valid product attribute value and normalize all identified at-tribute values to appropriate reference attributes. If there is a disagreement on the judgment of the two human accessors, it is resolved by a discussion among them.
 In each domain, we conducted 10 runs of experiments. For each run, we randomly selected one page in the domain and selected the text fragments corresponding to attribute values. The tokens in these text fragments were used to ini-tialize our algorithm as stated in Section 3.2. In practice, this could simply be done in Web browser by highlighting the portion containing attribute values. Next, we applied our framework to all other remaining pages to simultane-ously extract and normalize product attributes. The per-formance of both extraction and normalization in each run were recorded to evaluate our framework.
We evaluate the performance of product attribute normal-ization. The attribute normalization results are compared with the manually annotated answers. We adopt the pair-wise precision and recall, which are commonly used in clus-tering, as the evaluation metric. Pairwise recall is defined as the number of pairs of text fragments, which are cor-rectly predicted as referring to the same reference attribute by the system, divided by the actual number of pairs of text fragments referring to the same reference attribute. Pairwise precision is defined as the number of pairs of text fragments, which are correctly predicted as referring to the same refer-ence attribute by the system, divided by the total number of pairs of text fragments, which are predicted as referring to the same reference attribute. Pairwise F 1 -measure is de-fined as the harmonic mean of equal weighting of pairwise recall and precision.

We conducted the evaluation for the digital camera, MP3 player, and camcorder domains because the products in these domain contain a large number of attributes, and some of these attributes are previously unseen. The restaurant do-main is not evaluated since each restaurant only consists of a few attributes including names, phone number, addresses, customer reviewers, and credit card information which can be easily predefined. We design a baseline approach for com-parison. For each pair of the text fragments correctly ex-tracted by our framework, we compute the edit-distance as described in [2]. Attribute normalization is then conducted by invoking the agglomerative clustering. This baseline ap-proach only considers the text content of text fragments.
Table 1 shows the attribute normalization performance of our framework and the baseline approach. Each row of the table corresponds to a run of the experiment and the last row is the average performance. Each cell records the perfor-mance of our framework and the performance of the baseline approach is shown in brackets. Each column refers to the ex-traction performance in a domain. Our framework achieves better results compared with the baseline approach. In par-ticular, the average F 1 -measure are 0.78, 0.68, and 0.81 in the digital camera, MP3 player, and camcorder domains re-spectively. It shows that our framework can effectively nor-malize text fragments with similar semantic meaning to the same reference product attribute. The baseline approach has a relatively low recall since it can only consider the to-ken content of the text fragments. In contrast, each mix-ture component of our framework has its own distribution of terms, so that tokens related t o the reference attributes will also be considered. Therefore, our framework can normalize attributes with no common token between text fragments such as  X  X ight Portrait X  and  X  X andle Light X , which refers to the reference attribute  X  X hooting mode X  of a digital camera. Table 2 shows the top 5 weighted terms in 10 largest nor-malized attributes in the digital camera domain. It can be observed that the semantic meaning of the attributes can be easily interpreted from the terms. The output of attribute normalization can be very useful for supporting other intel-ligent applications such as product attribute indexing and product retrieval.
We evaluate the extraction performance of our framework in the digital camera, MP3 player, camcorder, and restau-rant domains. The system extracted attributes are com-pared with the attributes extracted by human as described above. We adopt the commonly used recall and precision as the evaluation metrics. Recall is defined as the num-ber of correctly extracted text fragments corresponding to attribute values divided by the actual number of text frag-ments corresponding to attribute values. Precision is defined as the number of correctly extracted text fragments corre-sponding to attribute values divided by the total number of text fragments extracted by the system. F 1 -measure defined as the harmonic mean of recall and precision is also used.
Table 3 shows the attribute extraction performance of our framework. Each row of the table depicts the extraction performance in a run. The last row shows the average ex-traction performance. Our approach obtains promising re-sults in the four domains. The average F 1 -measure are 0.95, 0.69, 0.60, and 0.58 in the restaurant, digital camera, MP3 player, and camcorder domains respectively. Notice that our framework is an unsupervised approach and does not require human effort to prepare training examples for ev-ery Web site. Surprisingly, in the restaurant domain, our framework achieves a performance which is comparable to thesupervisedmethodstatedin[9]. Moreover,ourframe-work can extract product attributes reasonably well from over 300 Web pages which are originated from over 150 dif-ferent Web sites in the other three domains.
Various information extraction techniques have been pro-posed to extract attributes from semi-structured documents including Web pages [11, 18]. For example, Conditional Random Fields (CRF) [7] have been applied to extract in-formation from Web documents achieving the state-of-the-art performance. Sarawagi and Cohen developed a semi-Markov CRF model which can assign labels to segments of a sequence [12]. Sutton et al. proposed a dynamic CRF models for labeling sequence data [14]. Zhu proposed an in-tegrated model based on hierarchical CRF and semi-CRF for detecting records and extracting attributes from raw Web pages [19]. However, one shortcoming of these supervised methods is that human effort is needed to prepare train-ing examples. Moreover, the attributes to be extracted are pre-defined and hence it cannot discover unseen attributes. Wong and Lam aimed at reducing the human work of prepar-ing training examples by automatically adapting extraction knowledge learned from a source Web site to new unseen sites and discovering new attributes [17]. Probst et al. [10] proposed a semi-supervised algorithm to extract attribute value pairs from text description. Their approach aims at handling free text descriptions by making use of natural lan-guage processing techniques. Hence, it cannot be applied to Web documents which are composed of mixing HTML tags and free texts.

The objective of entity resolution shares certain resem-blances with our goal of product attribute normalization. It aims at classifying whether two references refer to the same entity. Singla and Domingos developed an approach to en-tity resolution based on Markov Logic Network [13]. Bhat-tacharya and Getoor proposed an unsupervised approach for entity resolution based on Latent Dirichlet Allocation (LDA) [1]. One major limitation of these approaches is that the entities are required to be extracted in advance and can-not be applied to raw data.

A common drawback of existing methods is that the ex-traction and normalization tasks are conducted in two sep-arate steps, leading to conflicting solutions and degrading overall performance. Approaches based on CRF have been proposed to collectively conduct information extraction and mining [8, 16]. However, the attributes to be extracted have to be known in advance and previously unseen attributes cannot be handled.

Dirichlet process mixtures have been studied and applied in image analysis and language modeling [3, 15]. Our frame-work extends the Dirichlet process mixture model and shows that the mutual influence between the content and layout information of text fragments can be considered jointly to achieve product attribute extraction and normalization in an unsupervised learning manner.
We have developed an unsupervised framework which aims at simultaneously extracting and normalizing product at-tributes from Web pages collected from different sites. Our method can effectively consider the page-independent con-tent information and the page-dependent layout information of the text fragments of Web pages. We have developed a graphical model, which employs Dirichlet process prior, to model the generation of text fragments in Web pages. An unsupervised inference algorithm based on variational method is derived. We formally show that content and lay-out information can collaborate and improve both extraction and normalization performance. Extensive experiments on four different domains have been conducted to show the ro-bustness and effectiveness of our approach.
