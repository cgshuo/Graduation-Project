 In this paper we study approximate landmark-based meth-ods for point-to-point distance estimation in very large net-works. These methods involve selecting a subset of nodes as landmarks and computing offline the distances from each node in the graph to those landmarks. At runtime, when the distance between a pair of nodes is needed, it can be estimated quickly by combining the precomputed distances.
We prove that selecting the optimal set of landmarks is an NP -hard problem, and thus heuristic solutions need to be employed. We therefore explore theoretical insights to devise a variety of simple methods that scale well in very large networks. The efficiency of the suggested techniques is tested experimentally using five real-world graphs having millions of edges.

While theoretical bounds support the claim that random landmarks work well in practice, our extensive experimenta-tion shows that smart landmark selection can yield dramat-ically more accurate results: for a given target accuracy, our methods require as much as 250 times less space than se-lecting landmarks at random. In addition, we demonstrate that at a very small accuracy loss our techniques are several orders of magnitude faster than the state-of-the-art exact methods. Finally, we study an application of our methods to the task of social search in large graphs.
 Categories and Subject Descriptors H.4.3 [ Informa-tion Systems Applications ]: Communications Applica-tions General Terms Algorithms Keywords Graphs, shortest-paths, landmarks methods.
Understanding the mechanisms underlying the character-istics and the evolution of complex networks is an impor-tant task, which has received interest by various disciplines including sociology, biology, and physics. In the last years we have witnessed a continuously increasing availability of very large networks; blogs, sites with user-generated con-tent, social networks, and instant messaging systems nowa-days count hundreds of millions of users that are active on a daily basis. For graphs of this size, even seemingly simple algorithmic problems become challenging tasks.

One basic operation in networks is to measure how close one node is to another, and one intuitive network distance is the geodesic distance or shortest-path distance . Comput-ing shortest-path distances among nodes in a graph is an important primitive in a variety of applications including, among many others, social-network analysis, VLSI design in electronics, protein interaction networks in biology and route computation in transportation.

Recently, new motivating applications have arisen in the context of web search and social networks. In web search, the distance of a query X  X  initiation point (the query context) to the relevant web-pages could be an important aspect in the ranking of the results [34]. In social networks, a user may be interested in finding other users, or in finding content from users that are close to her in the social graph [30]. This socially sensitive search model has been suggested as part of the social network search experience [2, 36]. Using the shortest path distance as a primitive in ranking functions for search tasks is the main motivation of our work.
Although computing shortest paths is a well studied prob-lem, exact algorithms can not be adopted for nowadays real-world massive networks, especially in online applica-tions where the distance must be provided in the order of a few milliseconds. A full breadth-first search (BFS) traversal of a web-graph of 4 M nodes and 50 M edges takes roughly the same graph the best known point-to-point shortest path algorithms that combine Dijkstra with A* and landmarks, require to access an average of 20 K nodes in order to de-termine the shortest path between two nodes. On the other hand, precomputing all the shortest paths and storing them explicitly is infeasible: one would need to store a matrix of approximately 12 trillion elements.

The methods described in this paper use precomputed in-formation to provide fast estimates of the actual distance in very short time. The offline step consists of choosing a sub-set of nodes as landmarks (or reference objects ) and comput-ing distances from every node to them. Such precomputed distance information is often referred to as an embedding . Our contribution. In this paper we present an extensive analysis of various strategies for selecting landmarks. We devise and experimentally compare more than 30 strategies that scale well to very large networks. For presentation sake, we report the best ones; in case of ties we report the simplest. Our experimentation shows that for a given target accuracy, our techniques require orders of magnitude less space than random landmark selection, allowing an efficient approxi-mate computation of shortest path distances among nodes in very large graphs. To the best of our knowledge, this is the first systematic analysis of scalable landmarks selection strategies for shortest-path computation in large networks. Our main contributions are summarized as follows:
In our experimental evaluation we use real world networks: social graphs with explicit or implicit links from Flickr, a graph based on the communication network of the Yahoo! Instant Messenger service, and the coauthorship graph from the DBLP records. We also use a web-graph defined by the Wikipedia pages and their hyperlinks.
 Paper structure. In Section 2 we outline the related work, and in Section 3 we introduce our notation and our algo-rithmic framework. Section 4 presents a series of landmark section strategies which are experimentally evaluated in Sec-tion 5. Section 6 evaluates an application of our methods for fast social-network-aware search. Finally, Section 7 presents some concluding remarks. Exact shortest-path distances. Dijkstra described the algorithm to compute single source shortest paths (SSSP) in weighted graphs with n nodes and m edges from a node to all others [11]. The cost is O ( n 2 ) in general and can be reduced to O ( m + n log n ) for sparse graphs. For unweighted graphs, shortest paths can be computed using Breadth First Search (BFS) in time O ( m + n ). Floyd-Warshall algorithm employs dynamic programming to solve the all-pairs shortest paths (APSP) problem in an elegant and intuitive way [13] in time O ( n 3 ). Still, the complexity of computing APSP by invoking n Dijkstra/BFS computations is asymptotically faster, since it costs O ( nm + n 2 log n ) and O ( nm ) respectively.
The state of the art in point to point shortest path (PPSP) queries involves combining bidirectional Dijkstra with A* and lower bounds (ALT algorithms) [16, 17, 37, 27, 22]. ALT algorithms employ landmarks in order to prune the search space of the shortest path computation. Their land-marks are similar to the ones we experimented with for the lower-bound estimation (see Section 4.4); instead, in this paper, we use heuristics for selecting landmarks that work well with upper-bound estimates. Our paper addresses a different problem than the one in [16, 17, 37, 27, 22] since we are interested only on the length of a shortest path, not the path itself. Thus, we avoid any kind of online Dijk-stra/BFS traversals of the graph. However, to demonstrate the savings that one can obtain if the path itself is not of interest, and if only the distance length between two nodes is important, in Section 5 we compare our method to these state-of-the-art techniques and demonstrate that orders of magnitude in efficiency can be gained with a very small loss in accuracy.
 Indexing for approximate shortest-paths. We are in-terested in preprocessing a graph so that PPSP queries can be answered approximately and quickly at runtime. Thorup and Zwick [32] observe that this problem is probably the most natural formulation of the APSP. In their paper they obtain the result that for any integer k  X  1 a graph can be preprocessed in O ( kmn 1 k ) expected time, using a data struc-ture of size O ( kn 1+ 1 k ), and a PPSP query can be processed in time O ( k ). The quotient of the division of the estimated distance and the exact is guaranteed to lie within [1 , 2 k  X  1]. For k = 1 we get the exact solution of computing all shortest paths and storing them, which is prohibitively expensive.
For k = 2 the estimate may be three times larger than the actual distance. In large real-world graphs this bound is already problematic because distances are short due to the small-world phenomenon . In a small-world network, such as the Flickr-contacts graph described in Section 5, for an esti-mated distance of 6, the exact distance is only guaranteed to lie within the interval [2 , 6], along with almost every pairwise distance in this graph. A survey on exact and approximate distances in graphs can be found in [38].
 Embedding methods. Our work is related to general em-bedding methods. In domains with a computationally ex-pensive distance function, significant speed-ups can be ob-tained by embedding objects into another space and using a more efficient distance function, such as an L p norm. Several methods have been proposed to embed a space into a Eu-clidean space [6, 20]. There have been attempts to optimize the selection of reference objects for such a setting [3, 35]. Other dimensionality reduction techniques are also widely studied especially in theory and machine learning.
Landmarks have already been used for graph measure-ments in many applications [10, 26, 31, 28] such as round-trip propagation and transmission delay in networks: how-ever, how to optimally select the location of landmarks has not been extensively studied.

Kleinberg et al. [24] discuss the problem of approximating network distances in real networks via embeddings using a small set of beacons (i.e., landmarks). Of most interest is the fact that they introduce in their analysis the notion of slack , as a fraction of pairs in the network for which the algorithm provides no guarantees. Their analysis considers choosing beacons randomly. In this paper we show that in practice, simple intuitive strategies work much better than the random. Abraham et al. [1] generalize the metric embed-ding with slack. On another perspective, computing shortest paths in spatial networks has also attracted interest recently [25, 29]; our work is different since we focus on graphs that exhibit complex social network or web-graph behavior. Applications. Our work is also tightly connected to the various notions that have been introduced to measure the centrality of a vertex. Betweenness centrality measures the amount of shortest paths passing from a vertex while close-ness centrality measures the average distance of a vertex to all other vertices in the network [15]. Brandes [7] gave the best known algorithm to compute the exact between-ness centrality of all vertices by adapting the APSP Dijk-stra algorithm. The algorithm runs in O ( nm + n 2 logn ) time, which is prohibitive for large graphs. Bader et al. [4] gave a sampling-based approximation algorithm and showed that centrality is easier to approximate for central nodes. In our work, we use closeness centrality as a strategy in choosing central points as landmarks in the graph.

Fast PPSP computation is becoming very relevant for In-formation Retrieval. Socially sensitive search in social net-works and location-aware search are attracting a substantial interest in the information retrieval community [5]. For in-stance, it has been found that people who chat with each other are more likely to share interests [30]. An experi-ment discussed in Section 5 considers ranking search results in social networks based on shortest path distances. This problem has also been studied recently by Vieira et al. [36]. Their work is also based on landmarks, but their landmarks are chosen randomly. Since our work has been inspired by the task of social search we revisit it in Section 6 and show that our techniques outperform the random landmark selec-tion [36] by very large margins.

Approximation methods for computing other graph prox-imity functions that are based on random walks, such as personalized pagerank and random walk with restart, have also been studied recently [14, 33]. The growing interest in involving context and/or social connections in search tasks, suggests that distance computation will soon be a primitive of ranking functions. The restriction is that the ranking functions of search engines have hard computational dead-lines to meet, in the order of hundreds of milliseconds. Our methods can provide accurate results within these deadlines.
In this section we introduce the notation that we use in the rest of the paper. We then describe how to index distances very efficiently using landmarks. We formally define the landmark-selection problem that we consider in this paper, and prove that it is an NP -hard problem. Consider a graph G ( V, E ) with n vertices and m edges. Given two vertices s, t  X  V , define  X  s,t = h s, u 1 , u to be a path of length |  X  s,t | =  X  between s and t , if { u the set of all paths from s to t . Accordingly, let d G ( s, t ) be the length of the shortest path between any two vertices s, t  X  V , we refer to this as the geodesic distance, or dis-tance, between such vertices. In other words, d G ( s, t ) = Figure 1: Illustration of the cases for obtaining tight upper bounds (left) and tight lower bounds (right) as provided by Observations 1 and 2 |  X  s,t |  X  |  X  s,t | for all paths  X  s,t  X   X  s,t . Let SP s,t of paths whose length is equal to d G ( s, t ).

For simplicity we consider unweighted, undirected graphs, but all the ideas in our paper can be easily applied to weighted and/or directed graphs.

Consider an ordered set of d vertices D = h u 1 , u 2 , . . . , u of the graph G , which we call landmarks . The main idea is to represent each other vertex in the graph as a vector of shortest path distances to the set of landmarks. This is also called an embedding of the graph. In particular, each vertex v  X  V is represented as a d -dimensional vector  X  ( v ): For ease of presentation, from now on we will denote the i -th coordinate of  X  ( v ) by v i , i.e., v i = d G ( v, u
The shortest-path distance in graphs is a metric, and therefore it satisfies the triangle inequality. That is, given any three nodes s , u , and t , the following inequalities hold. An important observation that we will use to formulate the landmark-selection problem is that if u belongs to one of the shortest paths from s to t , then the inequality (2) holds with equality.

Observation 1. Let s, t, u be vertices of G . If there ex-ists a path  X  s,t  X  SP s,t so that u  X   X  s,t then d G ( s, t ) = d ( s, u ) + d G ( u, t ) .
 A similar condition exists for the inequality (3) to be tight, but in this case, it is required that either s or t are the  X  X iddle X  nodes.

Observation 2. Let s, t, u be vertices of G . If there ex-ists a path  X  s,u  X  SP s,u so that t  X   X  s,u , or there ex-ists a path  X  t,u  X  SP t,u so that s  X   X  t,u , then d G ( s, t ) = | d G ( s, u )  X  d G ( u, t ) | .
 The situation described in Observations 1 and 2 is shown in Figure 1.
Given a graph G with n vertices and m edges, and a set of d landmarks D , we precompute the distances between each vertex in G and each landmark. The cost of this offline computation is d BFS traversals of the graph: O ( md ).
Recall that our task is to compute d G ( s, t ) for any two vertices s, t  X  V . Due to Inequalities (2) and (3), we have In other words, the true distance d G ( s, t ) lies in the range [ L, U ], where L = max i | t i  X  s i | and U = min j { s Notice that one landmark may provide the best lower bound and another the best upper bound. Any value in the range [ L, U ] can be used as an estimate  X  d ( s, t ) for the real value of d ( s, t ). Some choices include using the upper bound using the lower bound the middle point or the geometric mean Notice that in all cases the estimation is very fast, as only O ( d ) operations need to be performed, and d can be thought of as being a constant, or a logarithmic function of the size of the graph.

Our experiments indicate that the  X  X pper bound X  esti-mates  X  d u ( s, t ) = U work much better than the other types of estimates, so, in the rest of the paper, we focus on the upper-bound estimates. We only comment briefly on lower-bound estimates later in Section 4.4.

As follows by Observation 1, the approximation  X  d u ( s, t ) is exact if there exists a landmark in D , which is also in a shortest path from s to t . This motivates the definition of coverage :
Definition 1. We say that a set of landmarks D covers landmark in D that lies in one shortest path from s to t . Our landmark-selection problem is formulated as follows.
Problem 1 (landmarks d ). Given a graph G = ( V, E ) select a set of d landmarks D  X  V so that the number of pairs of vertices ( s, t )  X  V  X  V covered by D is maximized. A related problem is the following
Problem 2 (landmarks-cover). Given a graph G = ( V, E ) select the minimum number of landmarks D  X  V so that all pairs of vertices ( s, t )  X  V  X  V are covered.
To obtain some intuition about landmark selection, con-sider the landmarks d problem for d = 1. The best land-mark to select, is a vertex that it is very central in the graph, and many shortest paths pass through it. In fact, selecting the best landmark is related to finding the vertex with the highest betweenness centrality [15].

To remind the reader the definition of betweenness cen-trality, given two vertices s and t , let  X  st denote the number of shortest paths from s to t . Also let  X  st ( u ) denote the number of shortest paths from s to t that some u  X  V lies on. The betweenness centrality of the vertex u is defined as The fastest known algorithms to compute betweenness cen-trality exactly are described by Brandes [7]. They extend well-known all-pairs-shortest-paths algorithms [9]. The time cost O ( nm ) for unweighted graphs and O ( nm + n 2 log n ) for weighted graphs. Additionally, Bader et al. [4] discuss how to approximate betweenness centrality by random sampling.
For our problem, consider a modified definition of be-tweenness centrality according to which we define I st ( u ) to be 1 if u lies on at least one shortest path from s to t , and 0 otherwise. We then define It follows immediately that the optimal landmark for the landmarks d problem with d = 1 is the vertex that maxi-mizes C ( u ). Our modified version C ( u ) can be computed as efficiently as C B ( u ) by modifying Brandes X  algorithm [7].
Both of the problems landmarks d and landmarks-cover are NP -hard. An easy reduction for the landmarks-cover problem can be obtained from the vertex-cover problem. Theorem 1. landmarks-cover is NP -hard.

Proof. We consider the decision version of the problems landmarks-cover and vertex-cover . The latter prob-lem is defined as follows: given a graph G , and an integer k , decide if there is a subset of vertices V  X   X  V of size at most k so that for all edges ( u, v )  X  E either u  X  V  X  or v  X  V Transform an instance of vertex-cover to an instance of landmarks-cover . Consider a solution D for landmarks-cover . Consider now the set of all 1-hop neighbors and ob-serve that each pair is connected by a unique shortest path of length 1 (i.e. an edge). Since all pairs of vertices are cov-ered, so are 1-hop neighbors, therefore the edges of E are also covered by D , therefore, D is a solution to vertex-cover . Conversely, consider a solution V  X  for vertex-cover . Con-sider a pair of vertices ( s, t )  X  V  X  V , and any shortest path  X  s,t between them. Some vertices of V  X  should be on the edges of the path  X  s,t , and therefore V  X  is also a solution to landmarks-cover .
 As a consequence, landmarks d is also NP -hard.

Next we describe a polynomial-time approximation solu-tion to the landmark-selection problem. The main idea is to map the problem to set-cover problem. Given the graph G = ( V, E ), we consider a set of elements U = V  X  V and a collection of sets S , so that each set S v  X  S corresponds to a vertex v  X  V . A set S v contains an element ( s, t )  X  U if v lies on a shortest path from s to v . Thus, solving the set-cover problem on ( U, S ) with the greedy algorithm [8], we obtain a O (log n )-approximation to landmarks-cover problem and a (1  X  1 /e )-approximation to landmarks d problem.
However, the running time of the above approximation algorithm is O ( n 3 ), which is unacceptable for the size of graphs that we consider in this paper.

The suggested strategies of the next section are motivated by the observations made in this section regarding properties of good landmarks. This section describes our landmark-selection strategies.
The baseline scalable strategy is to select landmarks at random [36, 24, 31]. The strategies we propose are moti-vated by the discussion in the previous section. On a high level, the idea is to select as landmarks X  X entral X  X odes of the graph, so that many shortest paths are passing through. We use two proxies for selecting central nodes: ( i ) high-degree nodes and ( ii ) nodes with low closeness centrality , where the closeness centrality of a node u is defined as the average distance 1 n P v d G ( u, v ) of u to other nodes in the graph.
In order to cover many different pairs of nodes, we need to spread the landmarks throughout the graph. In accordance, we propose two improvements for our strategies: ( i ) a con-strained variant, where we do not select landmarks that are too close to each other, and ( ii ) a partitioning variant, where we first partition the graph and then select landmarks from different partitions. Random : The baseline landmark-selection strategy consists of sampling a set of d nodes uniformly at random from the graph.
 Degree : We sort the nodes of the graph by decreasing de-gree and we choose the top d nodes. Intuitively, the more connected a node is, the higher the chance that it partici-pates in many shortest paths.
 Centrality : We select as landmarks the d nodes with the lowest closeness centrality. The intuition is that the closer a node appears to the rest of the nodes the bigger the chance that it is part of many shortest paths.

Computing the closeness centrality for all nodes in a graph is an expensive task, so in order to make this strategy scal-able to very large graphs, we resort to computing centrali-ties approximately. Our approximation works by selecting a sample of random seed nodes, performing a BFS com-putation from each of those seed nodes, and recording the distance of each node to the seed nodes. Since the seeds are selected uniformly at random and assuming that graph distances are bounded by a small number (which is true since real graphs typically have small diameter), we can use the Hoeffding inequality [21] to show that we can obtain arbitrarily good approximation to centrality by sampling a constant number of seeds.
Our goal is to cover as many pairs as possible. Using a basic strategy such as the ones described above, it may occur that the second landmark we choose covers a set of pairs that is similar to the one covered by the first, and thus its contribution to the cover is small.

The constrained variant of our strategies depends on a depth parameter h . We first rank the nodes according to some strategy (e.g., highest degree or lowest closeness cen-trality). We then select landmarks iteratively according to their rank. For each landmark l selected, we discard from consideration all nodes that are at distance h or less from l . The process is repeated until we select d landmarks. We denote our modified strategies by Degree / h and Centrality / h .

For the experiments reported in the next section we use h = 1 , 2 , 3 and we obtain the best results for h = 1. So, in the rest of the paper, we only consider this latter case.
In order to spread the landmarks across different parts of the graph, we also suggest partitioning the graph using a fast graph-partitioning algorithm (such as Metis [23]) and then select landmarks from the different partitions. We suggest the following partitioning-based strategies.
 Degree/P : Pick the node with the highest degree in each partition.
 Centrality/P : Pick the node with the lowest centrality in each partition.
 Border/P : Pick nodes close to the border of each partition. We do so by picking the node u with the largest b ( u ) in each partition, according to the following formula: where P is the set of all partitions, p is the partition that node u belongs to, and d i ( u ) is the degree of u with respect to partition i (i.e., the number of neighbors of u that lie in partition i ). The intuition of the above formula is that if a term d i ( u )  X  d p ( u ) is large, then node u lies potentially among many paths from nodes s in partition i to nodes t in partition p : such ( s, t ) pairs of nodes have distance at most 2, and since they belong to different clusters most likely there are not direct edges for most of them. For completeness, we remark that our experiments in all graphs, indicate that no significant improvement can be obtained by more complex strategies that combine both partitioning and constrained strategies.
As mentioned in Section 3.3, values  X  d l ( s, t ),  X  d  X  d ( s, t ) can also be used for obtaining estimates for the short-est path length d G ( s, t ).

Following Observation 2, landmarks that give good lower-bound estimates  X  d l ( s, t ) are nodes on the  X  X eriphery X  of the graph, so that many graph nodes are on a shortest path be-tween those landmarks and other nodes. Most of the strate-gies we discuss above are optimized to give good upper-bound landmarks, by selecting central nodes in the graph, and they perform poorly for lower-bound landmarks.
In fact, random landmarks perform better than any of the above methods with respect to lower bounds. With the in-tuition to select landmarks on the periphery of the graph, we also tried variations of the following algorithm (also de-scribed in [17]): ( i ) select the first landmark at random ( ii ) iteratively perform a BFS from the last selected landmark and select the next landmark that is the farthest away from all selected landmarks so far (e.g., maximizing the minimum distance to a selected landmark). This algorithm performs better than selecting landmarks at random, but overall the performance is still much worse than any of the methods for upper-bound landmarks.
We present experimental results in terms of efficiency, ac-curacy and comparison to existing work for five datasets.
In order to demonstrate the robustness of our methods and to show their performance in practice, we present ex-periments with five real-world datasets. The first four are anonymized datasets obtained from various sources, namely Flickr, Yahoo! Instant Messenger (Y!IM), and DBLP. The last one is a document graph from the Wikipedia (nodes are articles, edges are hyperlinks among them). Figure 2 il-lustrates the distance distributions. Next, we provide more details and statistics about the datasets.
 Flickr-E: Explicit contacts in Flickr. Flickr is a popular online-community for sharing photos, with millions of users. The first graph we construct is representative of its social network, in which the node set V represent users, an the edges set E is such that ( u, v )  X  E if and only if a user u has added user v as his/her contact.

We use a sample of Flickr which with 25M users and 71M relationships. In order to create a sub-graph suitable for our experimentation we perform the following steps. First, we create a graph from Flickr by taking all the contact relation-ships that are reciprocal. Then, we keep all the users in the US, UK, and Canada. For all of our datasets, we take the largest connected component of the final graph.
 Flickr-I: Implicit contacts in Flickr. This graph infers user relationships by observing user behavior. Reciprocal comments are used as a proxy for shared interest. In this graph an edge ( u, v )  X  E exists if and only if a user u has commented on a photo of v , and v has commented on a photo by u .
 DBLP coauthors graph. We extract the DBLP coauthors graph from a recent snapshot of the DBLP database that considers only the journal publications. There is an undi-rected edge between two authors if they have coauthored a journal paper.
 Yahoo! IM graph. We use a subgraph of the Yahoo! In-stant Messenger contact graph, containing only users who are active also in Yahoo! Movies. This makes this graph much sparser than the others. Goyal et al. describe this dataset in detail [18].
 Wikipedia hyperlinks. Apart from the previous four datasets, which are social and coauthorship graphs, we con-sider an example of a web graph, the Wikipedia link graph. This graph represents Wikipedia pages that link to one an-other. We consider all hyperlinks as undirected edges. We remove pages having more than 500 hyperlinks, as they are mostly lists.
 Summary statistics about these datasets are presented in Table 1. The statistics include the effective diameter  X  0 . 9 the median diameter  X  0 . 5 , which are the minimum shortest-path distances at which 90% and 50% of the nodes are found respectively, and the clustering coefficient c . In these graphs the degree follows a Zipf distribution in which the probabil-ity of having degree x is proportional to x  X   X  ; the parameter  X  fitted using Hill X  X  estimator [19] is also shown in the table.
We measure the accuracy of our methods in calculating shortest paths between pairs of nodes. We randomly choose 500 random pairs of nodes. In the case of the Random selection strategy we average the results over 10 runs for each landmark set size. We report for each method and dataset the average of the approximation error: |  X   X   X   X  | / X  where  X  is the actual distance and  X   X  the approximation. Figure 3 shows representative results for three datasets. Observe that using two landmarks chosen with the Cen-trality strategy in the Flickr-E dataset yields an approx-imation equal to the one provided by using 500 landmarks selected by Random . In terms of space and query-time this results in savings of a factor of 250. For the DBLP dataset the respective savings are of a factor greater than 25.
Table 2 summarizes the approximation error of the strate-gies across all 5 datasets studied here. We are using two landmark sizes: 20 and 100 landmarks; with 100 landmarks we see error rates of 10% or less across most datasets.
By examining Table 2 one can conclude that even sim-ple strategies are much better than random landmark selec-tion. Selecting landmarks by Degree is a good strategy, but sometimes does not perform well, as in the case of the Y!IM graph in which it can be worse than random. The strategies based on Centrality yield good results across all datasets.
We implemented our methods in C++ using the Boost and STL Libraries.All the experiments are run on a Linux server with 8 1.86GHz Intel Xeon processors and 16GB of memory.
 Regarding the online step the tradeoffs are remarkable: The online step is constant, O ( d ) per pair where d is the number of landmarks. In practice it takes less than a mil-lisecond to answer a query with error less than 0.1.
The offline computation time depends on the strategy. We break down the various steps of the offline computation in Table 3. Observe that in some cases the computation time depends on the time it takes to perform a BFS in each dataset, shown in Table 1. The offline computation may include as much as four phases: Table 1: Summary characteristics of the collections.  X  0 . 9 : effective diameter,  X  0 . 5 : median diameter,  X  : power-law coefficient, c : clustering coefficient, t BF S cpu-time in seconds of a breadth-first search. Table 2: Summary of approximation error across datasets, using 20 landmarks (top) and 100 land-marks (bottom). 1. A centrality computation is required for the methods 2. A partition of the graph is required for the meth-3. The selection of the landmarks depends on the strat-4. The embedding implies labelling each node in the graph
The computational time during the indexing is dominated by the BFS traversals of the graph. S such traversals are necessary for performing centrality estimations in the algo-rithms, basically by picking S seed nodes uniformly at ran-dom and then doing a BFS from those nodes; the centrality of a node is then estimated as its average distance to the S seeds. The embedding computation always takes d BFSs. Note that this step may be parallelized.
 Table 3: Indexing time. Partitioning and selecting times are expressed in wallclock seconds for d = 100 landmarks in the Flickr-E dataset
With respect to the memory requirements to store the index, in all of our datasets more than 99% of the pairs are at a distance of less than 63, meaning that we can safely use six bits per landmark and node to store the embeddings. With 20 landmarks in large a graph of 100 M nodes, we would use 120 bits (15 bytes) per node. Thus, around 1.5 GB of memory would be required to store all the embeddings, which is a very small memory footprint for this application. For the case that either the landmarks or the nodes are more and the index needs to resort in the disk, we store each node X  X  embedding in a single page. Thus, we perform one page access at query-time.
The triangulation performance of random landmarks has been theoretically analyzed by Kleinberg et al. [24]. In their paper they prove bounds for the performance of the bound that support the claim that random landmarks work well in practice. In this section we provide experimental evidence that even though selecting landmarks at random makes it possible to prove bounds on the triangulation task, in all our datasets the methods we propose outperform Random .
Our methods produce a smaller range within which the actual distance certainly lies in. For any pair of vertices one can measure the ratio between the best lower bound and the best upper bound that can be achieved given a set of landmarks.

Recall from Section 3.3 that using the landmarks we can provide both an upper ( U ) and a lower bound ( L ) for the actual shortest path distance. We measure the average ra-Table 4: Summary of average ratio between L and U across datasets, using 20 landmarks (top) and 100 landmarks (bottom) tio ( L/U ) over all queries and present the results in Table 4. Observe that Border/P and Degree/P outperform Ran-dom in all datasets. They also outperform the rest of the strategies with smaller margins. We note that one needs to use one or two orders of magnitude more landmarks in Ran-dom to achieve the accuracy of Border and Degree/P .
A more detailed view is presented in Figure 4. We plot histograms that reflect the distribution of the L/U ratio for 20 seeds in DBLP and for 100 seeds in Y!IM. Observe that the volume of the distributions of methods Border and Degree/P are similar and that they are both clearly closer to the unit than the respective distribution of Random .
We compare our method with state-of-the-art algorithms (ALT) for computing exact point-to-point shortest paths, as described by Goldberg and Harrelson [16, 17]. We note that our methods can be considered as fair competitors with ALT, since the ALT methods not only compute exact dis-tances, but they also compute the shortest path itself and not only its length. Nevertheless, our comparison can be seen as an illustration on how much one can gain if only the length of the shortest path is needed and if one is willing to tolerate small approximation errors.

In brief, ALT methods combine bidirectional Dijkstra traver-sal, with A* and landmark-based lower bounds. We imple-were chosen using Goldberg X  X  farthest heuristic [17]. Below we report results only for Ikeda X  X  algorithm, because it out-performed Pohl X  X  algorithm in all of our datasets; the same result is reported in [16].

The results are summarized in Table 5. For our methods, we use as many landmarks are needed to bring the error below 0.1. For ALT, since it is an exact method, we use the number of landmarks that gives the best performance. In the rows labeled Operations in Table 5 we report the number of arithmetic operations during the LB/UB compu-tations. This measure gives a large advantage to ALT meth-ods since it disregards costs involving hashing visited nodes, priority queue maintenance and most importantly random accesses to nodes. We also present the average number of CPU ticks, noting that this measure is implementation de-pendent: our implementation is main memory based and uses the C++/STL priority queue template.

Table 5 confirms that our techniques outperform the state-of-the-art for exact Point-to-point SP by several orders of magnitude according to all measures while suffering a very small loss in accuracy in all five datasets.
In this section we describe an application of our method to network-aware search. In network-aware search the results of a search are nodes in a graph, and the originating query is represented by a context node in the graph. The context node may represent the user issuing the query, or a document the user was browsing when she issued the query. Nodes that match the query are ranked using a ranking function that considers, among other factors, their connection with the context node; for instance, the ranking function may favor the results that are topologically close to the context node.
There are a number of use cases where social search may be helpful. For instance, a user may be searching on a so-cial networking site for a person which she remembers only by the first name. There might be potentially thousands of matching people, but friends-of-friends would be ranked higher since they are more likely to be acquaintances of the query-issuer. As another application consider a user search-ing for books, music or movies of a certain genre: items favorited by her friends or friends-of-friends are more likely to be interesting for her. Yet another application is context-aware search, where a user is reading a page with a search form on it (e.g. Wikipedia) and enters a query. Pages linked to by the original page (or close by in terms of clicks) should be presented first.
 The problem we consider was defined by Vieira et al. [36]. Given a source vertex and a set of matching-vertices that satisfy the query (which are provided by an inverted index), rank the matching vertices according to their shortest path distance from the source vertex.
To evaluate the effectiveness of our methods for search, we need a method for generating search tasks, as well as a performance metric. For the latter we use precision@k de-noted by p@k; this is the size of the intersection between the top k elements returned by our method using approxi-mate distances, and the top k elements in the result set X , normalized by k . Given that there might be elements of X tied by distance, we extend X to include the elements at the same distance as the k -th element on X , as any of those elements can be considered a top-k result to the query.
We point out that while this evaluation method emulates a hypothetical ranking function that uses only the distances, in most practical settings the distance between two items should be a component of the ranking function, not a re-placement for it.

To generate queries and select the matching results we consider that each element in the graphs has a set of tags or keywords associated to it. In the case of the Flickr datasets (both explicit and implicit graphs) tags are naturally pro-vided by users; a user has a tag if she has tagged at least one photo with that tag . In the case of the Yahoo! Instant Messaging graph, we cross the information with the items users have rated in Yahoo! Movies (we have been provided with an anonymized dataset in which this information is al-ready joined), so the tags of a user are the movies she has rated. In the case of the Wikipedia dataset, the tags are the words the pages contain. We select words that are neither uncommon nor trivially common: we pick words that have document frequency between 1 K and 10 K . In the case of DBLP, we use artificial tags. We create random tags and assign them to 100 random users in the graph.
Table 6 summarizes the precision at 5 of the strategies for 20 and 100 landmarks. The Random technique is outper-formed in all datasets, by large margins.

Figure 5 shows how the precision increases by adding more landmarks. The x -axis is logarithmic so it is clear that re-turns are diminishing. In any case a few tens of landmarks are enough for Flickr and a few hundred landmarks for the other datasets. Results in Table 6 agree with those in Ta-ble 2 and similar conclusions hold. The constrained Cen-tral/1 and the partitioning-based Border/P yield con-sistently good results across all datasets. We note that the results for other precision measures, such as p@1, p@2,p@10 and p@20, are qualitatively similar to p@5.

As a general observation, the number of landmarks nec-essary for a good approximation depends much more on the graph structure than on the graph size. For instance, despite Y!IM being the smallest graph, it is also the one that ex-hibits the larger range of distances, as shown in Figure 2. As expected, on this graph the search task requires more land-marks to obtain a high precision than on the other graphs. Figure 5: Precision at 5 for the social search task Table 6: Summary of precision at 5 across datasets, using 20 and 100 landmarks
In this section we consider the case that the distances of the graph nodes to the landmarks do not fit in the main memory and need to be stored on disk. Recall that the query returns an answer set of relevant nodes. These nodes are only known at query-time. Depending on the query se-lectivity s (i.e., the size of the answer set) we may follow different strategies: if s is small, the methodology of Sec-tion 5.3 applies; if the index is stored in external memory, we need to perform s page accesses per query.

If s is large then we follow a different strategy. First, ob-serve that for large s it is meaningful to retrieve the top-k for some k  X  s . To that end, we use ideas similar to the Thresh-old Algorithm (TA) algorithm, introduced in the seminal work of Fagin et al. [12]. The application of TA algorithms is as follows: nodes and their distance to a given landmark are stored in ascending order. There is one such list per landmark. We visit the lists sequentially using only sorted access until we get the top-k relevant answers. There are two important observations. First, during query-processing, a list (i.e., landmark) may be pruned as a whole using the greatest distance in the current top-k set as a threshold. Second, this approach employs mainly sequential access in disk which is much faster than random access.

As a final remark, consider a system that implements the social search task. Recall that Dijkstra/BFS will very quickly retrieve nodes with small distance from the query-node, but its efficiency degrades very fast in social-network like graphs. Contrary to that, embedding based methods are very efficient, especially for low selectivity queries. Thus a hybrid approach which combines both would optimize ef-ficiency; its details are not in the scope of this work.
Motivated by applications such as context-aware web search and socially-sensitive search in social networks, we studied how to do fast and accurate distance estimation on real-world massive graphs using landmarks. We characterized the problem of optimal landmark selection and proved that it is NP -hard. We described several strategies for landmark selection which outperform the current approximate stan-dard Random and the state-of-the-art exact techniques by large margins according to our extensive experimentation. In the simplest class of strategies, Centrality appears to be much more robust than Degree . Among the more elabo-rate strategies, the ones based on partitioning, in particular Border/P , exhibit consistent savings in computational cost across all datasets. When applied to the task of context-aware search these methods yield results of high precision.
In our on-going work we are studying methods for an ef-fective synergy of upper and lower bounds. We also plan to investigate the issue of dynamic data structures and how to deal with frequent network updates. A key aspect is to in-vestigate methods to provide estimates in grams for distance functions other than the shortest path distance, which could also be used as primitives in context and/or social aware search tasks.
