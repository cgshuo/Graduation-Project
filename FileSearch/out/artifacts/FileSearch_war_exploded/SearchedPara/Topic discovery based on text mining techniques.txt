 1. Introduction
The popularity of Internet has caused an ever-increasing amount of textual documents (Web pages, news, scientific papers, etc.). This information explosion has led to a growing challenge for Information Retrieval systems to efficiently and effectively manage and retrieve this information. The standard formulation of the information access problem presumes a query, the user X  X  expression of an information need. However, many what events happened during a given month and/or at a given place. In this case, the information need is too vague to be described as a single topic and the user does not know the topics of her interest. Indeed, the user may wish to browse over a set of discovered topics extracted from the document collection at hand.
Text Mining, also known as Intelligent Text Analysis, Text Data Mining or Knowledge Discovery in Text and knowledge from unstructured text collections. Clustering is a useful technique in Text Mining for discov-tures or clusters can be found directly from the data without relying upon any background knowledge. Current clustering techniques can be broadly classified into two categories: partitional and hierarchical.
Pedersen, Karger, &amp; Tukey, 1992 ) firstly introduced document clustering as a document browsing method, which is also the focus of our work.

There is no doubt that a document collection organized into a hierarchy is very helpful for users. However, it is not enough. Users also need to determine at a glance whether clusters are of their interest by means of some kind of summary. In the literature, the problem of summarizing a set of input documents, called mul-&amp; McKeown, 2002 ). Basically, a multidocument summarization system tries to determine which sentences must be included in the summary, and then how to organize them to make the summary comprehensible.
In this context, our research focuses on two issues: (1) how to discover the structure of topics reported in a properly summarize these topics and their subtopics.

For example, Fig. 1 presents part of the structure of the  X  X  X urrent conflict with Iraq X  X  topic from the TDT2 collection as users would desire to obtain. In the first level of the hierarchy, incoming documents are grouped into small subtopics. In the upper levels, composite topics are successively built upon them. Furthermore, a brief description of each topic is included to help users to browse on the hierarchy. An example of the hier-archy produced by our system is shown in Fig. 6 .

In this paper, we present a new incremental hierarchical clustering algorithm. It uses a multilayered clus-tine over the cluster representatives of the previous level.

Moreover, we propose a novel and effective algorithm that provides a summary for each hierarchy topic. It
Pattern Recognition. Contrary to other multidocument summarization methods, the selection of sentences is based on the frequent discriminating terms of each cluster, which can be efficiently computed.
It is worth mentioning that, in our method, we assume that no previous knowledge about the collection sequence, all the processes involved in the proposed method (i.e. topic discovery and summary construction) must be completely unsupervised. Additionally, as our main focus is on web-based applications, we also require incremental clustering so that whenever a new document arrives to the collection it is not necessary to start from scratch.

The proposed method can be also useful for the analysis of large document collections, mainly in the fol-lowing tasks:
Exploration of document contents . Starting from the top level of the hierarchy, the user can browse and rent contents. Such a global view can change along time as new documents are included in the collection.
Topic detection . The topic detection problem ( Allan, Carbonell, Doddington, Yamron, &amp; Yang, 1998 ) consists of determining for each incoming document, whether it reports on a new topic, or it belongs to some previously detected topic. A topic detection system forms topic-based clusters of documents. The clusters in our top level of the hierarchy can be seen as detected topics in a stream of news. Also, the lower levels of the hierarchy can present topics with different detail levels.

Multidocument summarization . Each topic of the hierarchy is summarized taking into account the discrim-inating terms. These summaries can help users to find relevant documents effortless. They can be also used for Information Retrieval purposes (e.g. find the clusters related to a user query).

Dimensionality reduction . The relevant terms extracted with our summarization method can be used to rep-resent the documents in a vector space with less number of dimensions without affecting the clustering quality.
The remainder of the paper is organized as follows: Section 2 introduces some related approaches. Section 3 presents the global architecture of our method, and describes both the representation of documents taking into account their spatio-temporal components, and a document similarity measure for them. Section 4 describes an incremental hierarchical clustering algorithm to discover topics and their subtopics. Section 5 presents the cluster description method to build topic summaries, and Section 6 shows the experiments carried out. Finally, conclusions are presented in Section 7 . 2. Related work
This section describes the main works in the literature that concern with hierarchical and incremental clus-tering algorithms, but that also provide some mechanism to obtain summaries from the generated cluster hierarchies.

Hierarchical clustering algorithms produce a nested sequence of partitions, with a single all-inclusive cluster at the top and singleton clusters of individual documents at the bottom. The result of these algorithms can be viewed as a tree, called a dendrogram. The hierarchical methods can be further divided into agglomerative or divisive algorithms. Hierarchical agglomerative clustering (HAC) methods (e.g. single-link, complete-link, average-link) start with each document in a cluster of its own, iterate by merging the two closest clusters at each step, and finish when some halting criterion is reached. HAC methods differ from the procedure to com-cluster is selected to be further bisected. This process is repeated until all clusters are singleton.
One of the advantages of partitional clustering algorithms is that they use global information of the collec-clustering algorithms can generate small and reasonably cohesive clusters, a task in which partitional algorithms may fail. However, the agglomerative approaches may take wrong decisions at earlier stages of the clustering process, which tend to be multiplied as the agglomeration of clusters progresses.
In this context, hybrid approaches attempt to combine both the global view used in partitional clustering algorithms and the local view used in agglomerative algorithms. Among them, Scatter/Gather ( Cutting et al., 1992 ) and constrained agglomerative algorithms ( Zhao &amp; Karypis, 2005 ) are two remarkable examples.
Scatter/Gather is a well-known algorithm which has been proposed for a document browsing system based mine an initial clustering which is then refined using the k -means algorithm. The algorithm works in a local manner on small groups of documents rather than trying to deal with the entire corpus globally to reduce the time complexity. This algorithm puts limits on how many clusters can be formed and it is not deterministic, that is, repeated calls to this algorithm on the same corpus may produce different partitions. Scatter/Gather summarizes document clusters by meta-documents containing profiles of topical words and the most typical est similarity to the cluster X  X  centroid. Together, the topical words and typical titles form a cluster digest.
In Zhao and Karypis (2005) constrained agglomerative algorithms are introduced. In this approach, a partitional clustering algorithm is used to compute a k -way clustering solution. Then, each of these clusters is treated as a separate collection and an agglomerative algorithm is used to build a tree for each of them.
Finally, the k different trees are merged by using an agglomerative algorithm that treats the documents of each subtree as an already formed cluster. Its time complexity is O( n
The two algorithms mentioned above are non-incremental methods. There are some incremental algorithms that update the cluster hierarchy when new documents arrive, such as Suffix Tree Clustering ( Zamir, Etzioni, Madani, &amp; Karp, 1997 ) and DC-tree ( Wong &amp; Fu, 2000 ).

The Suffix Tree Clustering (STC) algorithm does not treat a document as a set of words but rather as a string, making use of proximity information between words. STC relies on a suffix tree to identify sets of doc-uments that share common phrases and uses this information to create clusters and to summarize their con-tents for users. However, the time complexity of STC is quite high with respect to the number of terms. The tree also suffers a great degree of redundancy.
 DC-Tree is an incremental hierarchical algorithm for Web document clustering. Unlike the STC algorithm,
DC-Tree is based on the vector X  X pace representation of documents. Starting from the root, each new docu-old, in which case a new cluster node is created. The algorithm defines several parameters and thresholds, thus the parameter tuning is problematic. The main drawback of this algorithm is that document assignments to clusters are irrevocable.

As mentioned in the introduction, the topic detection problem also makes use of text clustering. The topics of a collection are unknown a priori and no previous knowledge about them is available. Recently, in the last topic detection and Tracking (TDT) competition, TDT 2004 ( National Institute of Standards &amp; Technology, 2004 ) hierarchical topic detection task was introduced, which is mainly intended to evaluate topic detection tom comprises the singleton clusters. For evaluating these systems, it has been proposed several measures that combine both normalized detection cost and a lattice travelling cost. However, the set of manually identified topics employed to evaluate them is not hierarchically organized, although it contains topics with different cannot be properly applied to evaluate our system.

ICT ( Yu et al., 2004 ) and UMASS ( Cornell et al., 2004 ) are two algorithms that have been proposed for the uses an agglomerative hierarchical clustering method in each bucket to produce micro-clusters. Finally, the hierarchy is built, by applying successively the Single-Pass clustering. UMASS system proposes two steps, bounded 1-NN for event formation and bounded agglomerative clustering for building the hierarchy. The algorithm ends when the number of clusters is smaller than a given value. The generated clusters by both methods depend on the document arrival order and they do not provide any summary for the detected topics.
To sum up, we compare the hierarchical clustering algorithms mentioned above considering if they are incremental, if they provide mechanisms to obtain summaries, if they depend on the document arrival order, and finally its complexity. As Table 1 shows, our method is incremental, order-independent and provides sum-subquadratic.
 3. Global architecture
The global architecture of our method is presented in Fig. 2 . Starting from a collection of documents, the system first processes them to get a proper representation in order to cluster them accordingly to their seman-collection. Finally, the summarization method is applied to obtain the summaries of the discovered topics. 3.1. Document processing
The steps included in this pre-processing are the following ones: document texts are tokenised, abbrevia-tions and proper names are recognised, stop-words are removed using a standard stop-word list, part-of-speech tags are added and common words are replaced by their lemmas. As a tool for annotating text with part-of-speech and lemma information we used the Tree-Tagger parser, developed by the University of Stutt-gart. All these elements are called terms. Additionally, we also extract some useful meta-data such as the pub-lication dates and the places mentioned in the texts. 3.2. Document representation
From a journalist X  X  perspective, a news story describing a topic will typically specify the following informa-tion: when it occurred, who was involved, where it took place and how it happened. Therefore, we think that these properties are very useful to describe and summarize the topics.

Most approaches do not consider these properties, representing the whole document as a bag of textual terms. As a consequence, with this representation it is hard to detect for example whether two different train accidents are two different topics for terms occurring in the involved documents are very similar. There are some systems such as Kumaran and Allan (2004) that use named entities to create three vector representations for each document (all terms, only named entities and non-named entity terms). However, the score of each determine the simple rules for each category. Stokes and Carthy (2001) define a composite document repre-sentation based on the lexical chains derived from a text and traditional keyword index terms. Makkonen,
Ahonen-Myka, and Salmenkivi (2004) has recently presented a topic detection approach that employs seman-tic classes in the representation of documents. They split the term space into four semantic classes: places, names, temporal expressions and general terms, and they use class-wise similarity measures. Since these of each semantic class.

In this paper, we propose a specific representation for each semantic property of news: the actions and their main protagonists (what and who?), their time properties (when?) and their space properties (where?). The main aim of this representation is not only to compare news taking into account these properties, but also to provide a useful way of describing and summarizing the topics. Thus, our system builds three feature vectors to represent each document d i , namely: the content of the document, n is the total number of terms and w i
Salton, Allan, &amp; Singhal, 1995 ) of the term t k in d
A vector of publication dates. A document only contains its publication date, whereas cluster representatives have a selection of publication dates taken from their members. In our previous work ( Pons-Porrata,
Berlanga-Llavori, &amp; Ruiz-Shulcloper, 2002b ) we use a vector of weighted time entities instead a vector of publication dates. These time entities are either dates or date intervals that are automatically extracted from us that using a vector of publication dates not only reduces the computational cost but also produces better results for our clustering algorithm.

A vector of weighted places  X  TF p i k =1, ... , l i . Place names are automatically detected from the document X  X  contents by using a thesaurus built from the National Imagery and Mapping Agency files (NIMA). represented as a path of codes with the different geographic regions where the place is included (country, administrative region, etc.). For ambiguous place names, the detection algorithm selects those paths involv-ing the most frequently mentioned countries in the document. 3.3. Document similarity measure
Automatic document clustering is based on a similarity measure and a clustering criterion. The widest adopted document similarity measure has been the well-known cosine measure. In our case, we also introduce three similarity comparison criteria, each one corresponding to the components of the proposed document representation.
 To compare the term vectors of two documents d i and d j we use the traditional cosine measure:
To compare the time vectors of two documents we propose the following function: D  X  d dates f i of the document d i . This function is based on the traditional distance between sets. To compare the place vectors of two documents we propose the following Boolean function:
For example,  X  X  X osovo X  X  represented by 5G.02, is similar to  X  X  X ugoslavia X  X  represented by 5G, because both have a common prefix. As it can be noticed, we consider that two documents are similar with respect to its place vectors if both contain at least one place belonging to the same country. However, if one of the docu-ments have no place references in their contents (absence of information), they are considered similar (rather not dissimilar). Finally, the global similarity measure can be defined as follows: where b time is the maximum number of days that are required to determine whether two articles refer to the same or to different topics. This measure is intended to capture the idea that two documents reporting a same topic should have a high semantic similarity, time proximity and place coincidence. Moreover, time and space components are used as filters in the global measure, which can reduce notably the complexity of the clustering algorithm.
 It worth mentioning that more complex similarity measures for time and space components can be defined.
However, our previous experiments demonstrated that this simpler measure works as well as the complex ones for our clustering algorithm ( Pons-Porrata, Berlanga-Llavori, &amp; Ruiz-Shulcloper, 2002a ). 4. Document clustering algorithm
Traditional hierarchical clustering algorithms (e.g. complete-link, single-link, average-link, etc.) are intended to build a hierarchy of document classes. However, the levels of the generated hierarchies correspond of a set of clusters of abstract documents, which summarize those generated in the previous level.
In the following section we describe how cluster representatives are obtained, and afterwards we present the proposed clustering algorithm. 4.1. Cluster representatives
The representative of a cluster c , denoted as c , is a tuple  X  T nents, respectively. In this work, it is calculated as the union of the cluster X  X  documents:
T c  X  X  T c l ; ... ; T c n  X  , where T c j is the weight average of term t documents in the cluster containing this date, and s is the total number of publication dates that describe the documents of this cluster.
 places mentioned in the documents of this cluster.

In order to reduce the length of the cluster representatives, their vectors are truncated by removing the terms (dates, places) whose frequency is smaller than the 10th part of the vector maximum frequency. 4.2. Incremental hierarchical clustering algorithm
Similarly to constrained agglomerative algorithms ( Zhao &amp; Karypis, 2005 ), our hierarchical algorithm combines features from both partitional and agglomerative approaches. The proposed algorithm aims at hierarchy level. In this way, the higher the hierarchy level is, the less details in the identified topics are obtained. The lowest level of the hierarchy is intended to allocate the smallest identifiable topics, which are groups of tightly related documents. The rest of the levels contain cluster representatives.
The incremental hierarchical algorithm we propose (see Fig. 3 ) uses a clustering routine that must fulfill the following requirements: It must be incremental.

It should not depend on the order of the incoming documents.
It must use a similarity measure that takes into account the temporal-spatial proximity of documents and representatives.

Since the proposed hierarchical algorithm is incremental, it is assumed that it deals with an existing hier-archy of documents (initially empty), where the first level consists of a partition of the document collection, and the next levels contain the cluster representatives of each previous level. When a new document arrives at cess can create new clusters and remove existing ones.

When clusters are removed from a level of the hierarchy, their representatives must be removed from the tatives must be calculated. The members of the clusters where changes took place (that is, some cluster rep-mentioning that in each level of the hierarchy different parameters of the similarity measure and of the clus-tering routine, and even different clustering routines can be used.
 by the representatives of the clusters 3, 4 and 5 of the first level. When the new document d arrives, a new cluster (11) is created with some documents of the cluster 2 and all the documents of the cluster 3. The
Fig. 4 (b) shows the changes that take place in the second level of the hierarchy. Thus, the representatives c 2 and c 3 are removed and two new representatives c 0 2 and c that the clusters formed by the representatives c 6 ; c 7
In this paper, we use the compact clustering algorithm as the basic clustering routine. It is based on the incremental construction of all b 0 -compact sets. Basically, two documents are b greater or equal than a user-defined threshold b 0 . We call b whose vertices are the documents of the collection and there is an arc from the vertex d the most b 0 -similar document to d i . The b 0 -compact sets are equivalent to the connected components in the b -maximum similarity graph without taking into account the orientation of its arcs (see Fig. 5 ). More details of this incremental clustering algorithm and the mathematical properties in which is based can be seen in Pons-Porrata et al. (2002b) .

The time complexity of the compact clustering algorithm is O( n similarity measure, the complexity of the compact clustering algorithm is linear. The need for exhaustive pair-wise comparisons of documents is avoided by using a time window. This window contains the documents in the time period defined by b time days. Thus, each document is only compared with the documents in the time window, since the similarities with other documents are zero. 5. Cluster description method
As mentioned in Section 2 , current clustering-based systems do not provide information enough so that users become aware of the cluster contents. In this section we propose a summarization method to provide informative cluster descriptions.

Basically, a multidocument summarization system tries to determine which sentences must be included in the summary, and then how to organize them to make the summary comprehensible. Many of these approaches are based on a sentence weight function that takes into account the position of the sentences in the documents, the length of the sentences, and the number of frequent keywords of the set of documents they appropriate ones for the summary. 5.1. Summarization method
The temporal entities of a document cluster can be summarized as a date interval from the time vector of its cluster representative. Places can be easily summarized by taken a representative place from the cluster documents. Thus, in this section we only focus on the term vector to extract the cluster summaries.
One critical point in summary generation is the selection of  X  X  X ood X  X  terms for extracting the most infor-mative sentences. Current approaches usually take a list of terms from the (multi)document ranked by the
TF-IDF weights. In these systems, IDF scores are taken from external sources, which can be not appropriate as they may not reflect properly the vocabulary frequencies of the collection at hand. On the other hand,
TF-IDF does not take into account interesting word co-occurrences containing terms with low IDF (i.e. they phrases (e.g. at hand, hand in hand, etc.), and therefore it presents a low IDF. However, in some news documents this word is very relevant to select the topic informative sentences, as for example in the
TDT2 topic  X  X  X hoplifter X  X  Hand Amputated X  X . Similarly, some significant words of very large topics (e.g.  X  X  X onica Lewinsky Case X  X ) also have low IDF weights, and therefore they may be not selected for the summary.

In order to determine at a glance whether the contents of a topic are of user interest or not, we propose a new summarization method based on Testor Theory ( Lazo-Corte  X  s et al., 2001 ). Starting from a set of docu-ter. Once these terms have been selected, the system extracts all the sentences containing the selected terms.
In order to avoid redundant sentences in the summary, each selected sentence is compared with the others to detect high word overlapping. In this case, only the largest sentence is included in the summary. Finally, in order to improve the coherence and organization of the summaries, we sort the selected sentences according to the publication date of the news documents and their position in the text. The next section describes with detail the application of the Testor Theory for the selection of candidate terms. 5.2. Applying the Testor Theory for the selection of terms
In the Testor Theory, the set s  X f x i of a matrix M is called a testor , if after deleting from M all columns except { i is a testor ( Lazo-Corte  X  s et al., 2001 ).

For each cluster c , we construct a matrix MR ( c ), whose columns are the most frequent terms in the repre-and the second one is formed by the other cluster representatives. Notice that our goal is to distinguish the cluster c from the other clusters.

For the calculus of the typical testors of a matrix M , the key concept is the comparison criterion of the values of each feature. In our case, the features that describe the documents are the terms and its values are the frequency of terms. The comparison criterion applied to all the features is where v i respectively, and d is a user-defined parameter. As it can be noticed, this criterion considers the two values In order to determine all typical testors of a matrix we use the algorithm LEX ( Santiesteban &amp; Pons-Porrata, 2003 ), which computes efficiently the typical testors of a data collection.

A summary of a cluster c consists of a set of sentences extracted from the documents in c in which the high-est quantity of terms that belong to the typical testors of the matrix MR ( c ) occurs. Moreover, the sentences that cover the calculated typical testor set are also added to the summary. More details of this method can be seen in Pons-Porrata, Ruiz-Shulcloper, and Berlanga-Llavori (2003) .

For example, let suppose that we have three clusters about (1) the economic sanctions on India X  X  govern-ment for detonating three underground nuclear explosions, imposed by Bill Clinton, (2) the declarations of the president Clinton related to the situation of the human rights in China, and (3) the medallists in women X  X  500-meter short track speed skating in the Olympic games. We want to obtain the summary of the first cluster c . As mentioned before, we first obtain the most frequent terms in the representative of c we construct the matrix MR ( c 1 ) as follows:
Notice that most terms are not present in the representative of the cluster 3. From this matrix, we construct the difference matrix in order to calculate the typical testors, by applying the comparison criterion mentioned above. In this case, we use d = 0.02. Thus, the difference matrix is
The typical testors of this matrix are: {president_Bill_Clinton,Indian}, {sanction}, {nuclear} and {United_States,Indian}. All these terms are used to extract the sentences that will form the summary of c 6. Experiments and results
The effectiveness of our method has been evaluated using the TDT2 dataset, version 4.0. This corpus con-sists of 6 months of news stories from January to June 1998. The news stories were collected from six different sources. Human annotators identified a total of 192 topics in the TDT2 dataset. Nine thousand eight hundred three sub-collections, namely: training, development test and evaluation test, which are summarized in Table 2 . Unfortunately the TDT collections are not annotated at different abstraction levels, and therefore, we can-not evaluate properly the composition of topics and subtopics. Instead, we evaluate each level of the generated hierarchy against all the TDT2 topics to observe their impact in different tasks: topic detection and topic retrieval.

We use two quality measures of the literature that compare the system-generated clusters with the manually 1998 ). The F 1-measure is widely applied in Information Retrieval systems, and it combines the precision and recall factors. In our case, the F 1-measure of the system-generated cluster number j with respect to the man-ually labeled topic number i can be evaluated as follows: where n ij is the number of common members in the topic i and the cluster j , n and n j is the cardinality of the cluster j .
 To define a global measure, first each event must be mapped to the cluster that produces the maximum
F 1-measure: c 1 0.12 0.09 0.08 0.15 0.11 0.07 c 2 0.1 0 0.09 0 0 0.06 c 3 0 0 0 0 0.09 0.08 c ; c 2 010110 c ; c 3 111100 Hence, the macro F 1-measure is calculated as follows:
The detection cost is a measure that combines both the miss and false alarm errors between a topic i and a system-generated cluster j : false alarm, respectively, P topic is the a priori probability of a document belonging to a given topic, C
C vention in the TDT evaluations, we assign C false_alarm = 0.1, C
Again, to define the final measure, each topic must be mapped to the cluster that produces the minimum detection cost: The macro-average of this measure (also called Topic Weighted ) is then defined as follows:
Finally, the normalized detection cost is calculated as follows:
Table 3 shows the maximum F 1-measure obtained by our method on the TDT2 dataset and its sub-collec-tions. The first row shows results for our method using only the cosine measure, whereas the second and third rows show results when using our similarity measure instead. The second row only considers terms and the time component, whereas the third one considers terms, time and space components. Each cell contains the
F 1 values considering a hierarchy of one level, two levels and so on. For the evaluation, we consider the flat partition produced by each level of the hierarchy and we compare this partition with the manually identified topics. The entries that are boldfaced correspond to the level that performed the best in each document collection.

In our experiments we use the same b 0 value in all levels of the hierarchy, but b and 50 days in the bottom and the remaining levels, respectively. To statistically compare the obtained results we used the paired t -test ( Devore &amp; Peck, 1997 ), with 95% of confidence.

Several observations can be made by analyzing the results in Table 3 . First, the performance difference between the cosine measure and our similarity function is not statistically significant. Also, the performance difference between using only the temporal component and the spatio-temporal one is not statistically signif-icant either. Taking into account that the system effectiveness does not diminish when introducing the place filter, we can conclude that extracted places are valid to describe the topics. In general, regarding time and ness to reduce the time complexity of the compact clustering algorithm from O( n of documents ( Pons-Porrata et al., 2002b ).

TDT2 topics. Moreover, F 1 values in smaller document collections still improve if more levels of the hierarchy are added. It is worth mentioning that F 1 values are very good, being the proposed system a good tool for retrieval purposes. Moreover, even being an unsupervised system, its results are comparable with supervised classification systems.

With regard to the detection cost, Table 4 shows the obtained results following the same structure. This table includes the values of topic weighted detection cost and normalized detection cost obtained in each col-significant according to the paired t -test.

Looking at the results of one-level and two-level hierarchies, we can see that they are in agreement with the results presented earlier in Table 3 . We consider that the detection cost values obtained by our method are good enough if we compare them with those obtained by the existing detection systems (see evaluation results in Fiscus, Doddington, Garofolo, &amp; Martin, 1999 ).

Table 5 shows the best quality results obtained for each document collection using the base cosine with the hierarchy allows users to retrieve topics, and the third level can be used for topic detection tasks.
Table 6 shows the F 1 results for 20 TDT target topics randomly selected. Last column contains the hier-
Fig. 6 shows a snippet of the topic hierarchy created by our method for the TDT2 collection. This hierarchy is represented as an XML file containing for each topic its summary and the subtopics it comprises. The summaries capture the main ideas about each topic and an appreciable reduction of words is also achieved (superior to 70% in most of the topics). The whole hierarchy can be seen in http://krono.act.uji.es/ Experiments .

Our last set of experiments was focused on evaluating the impact of typical testors in the construction of summaries. In the experiments we want to demonstrate if the terms selected with typical testors are represen-tative enough of the topics they describe. With this purpose, we conducted an experiment where all documents representation is reduced to those terms, we perform again all the hierarchical clustering process. If the selected terms are representative enough, then the obtained clusters should present a similar or better quality with respect to using the original document representation.

Table 7 shows the reduction in the representation space when considering only typical testor terms, and the best values for detection cost and F 1 using the reduced representation space. As it can be noticed, the dimen-according to the paired t -test. 7. Conclusions
In this paper we propose a new hierarchical clustering algorithm that combines partitional and agglomer-ative approaches to produce topic hierarchies. This algorithm obtains cohesive clusters with arbitrary shapes incremental, being suited for web-based applications, where continuously new documents arrive. Moreover, the generated set of clusters is unique and independent of the document arrival order, making it insensitive to the particular way documents arrive to the system.

The proposed similarity measure between documents considers their place names, time references and tex-tual terms. Such a measure does not always improve quality results but it reduces the time complexity of the clustering algorithm from quadratic to linear.

Additionally, we propose a multidocument summarization method. It employs the calculus of typical tes-tors as its primary operation and from them it constructs the summaries of each cluster. The most important better summaries.

These Text Mining techniques are combined together to build a topic discovery system. The hierarchy obtained by our clustering algorithm allows users to discover the structure of topics and subtopics of a doc-
The multidocument summarization method provides a summary for each system-generated topic at any level of the hierarchy. This method is helpful to a user in order to determine at a glance whether a topic is of its interest. This approach avoids the definition of what a topic is, since is the user who decides in which level or undesirable to specify a query formally.

Experiments demonstrate the validity of our algorithms for both Information Retrieval and Topic Detec-additional information to users: the hierarchy of topics and subtopics along with their description. Retrieval level to topic detection tasks.

We believed that the accuracy of our method could be improved by optimizing the parameter values ( b b study of new ways of introducing the spatial information in the global similarity measure to improve the clus-tering and summary qualities. Also, one key issue we face is the use of overlapped clustering routines in our hierarchical algorithm instead the compact clustering routine to allow for multitopic documents. Future work also includes enhancing the summary coherence by using, for example, anaphora resolution techniques. References
