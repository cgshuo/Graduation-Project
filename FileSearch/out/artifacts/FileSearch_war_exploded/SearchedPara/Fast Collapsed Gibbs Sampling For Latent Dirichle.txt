 In this paper we introduce a novel collapsed Gibbs sam-pling method for the widely used latent Dirichlet alloca-tion (LDA) model. Our new method results in significant speedups on real world text corpora. Conventional Gibbs sampling schemes for LDA require O(K) operations per sam-ple where K is the number of topics in the model. Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample. On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA. No ap-proximations are necessary, and we show that our fast sam-pling scheme produces exactly the same results as the stan-dard (but slower) sampling scheme. Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes. For the PubMed collection of over 8 mil-lion documents with a required computation time of 6 CPU months for LDA, our speedup of 5.7 can save 5 CPU months of computation.
 G.3 [ Probabilistic algorithms ]: Miscellaneous Experimentation, Performance, Algorithms Latent Dirichlet Allocation, Sampling
The latent Dirichlet allocation (LDA) model (or  X  X opic model X ) is a general probabilistic framework for modeling sparse vectors of count data, such as bags of words for text, bags of features for images, or ratings of items by customers. The key idea behind the LDA model (for text data for ex-ample) is to assume that the words in each document were generated by a mixture of topics, where a topic is repre-sented as a multinomial probability distribution over words. The mixing coefficients for each document and the word-topic distributions are unobserved (hidden) and are learned from data using unsupervised learning methods. Blei et al [3] introduced the LDA model within a general Bayesian framework and developed a variational algorithm for learn-ing the model from data. Griffiths and Steyvers [6] subse-quently proposed a learning algorithm based on collapsed Gibbs sampling. Both the variational and Gibbs sampling approaches have their advantages: the variational approach is arguably faster computationally, but the Gibbs sampling approach is in principal more accurate since it asymptoti-cally approaches the correct distribution.

Since the original introduction of the LDA model, the technique has been broadly applied in machine learning and data mining, particularly in text analysis and computer vi-sion, with the Gibbs sampling algorithm in common use. For example, Wei and Croft [19] and Chemudugunta, Smyth, and Steyvers [5] have successfully applied the LDA model to information retrieval and shown that it can significantly outperform  X  in terms of precision-recall  X  alternative meth-ods such as latent semantic analysis. LDA models have also been increasingly applied to problems involving very large text corpora: Buntine [4], Mimno and McCallum [12] and Newman et al [15] have all used the LDA model to automat-ically generate topic models for millions of documents and used these models as the basis for automated indexing and faceted Web browsing.

In this general context there is significant motivation to speed-up the learning of topic models, both to reduce the time taken to learn topic models for very large text collec-tions, as well as moving towards  X  X eal-time X  topic modeling (e.g., for a few thousand documents returned by a search en-gine). The collapsed Gibbs sampling algorithm of Griffiths and Steyvers involves repeatedly sampling a topic assign-ment for each word in the corpus, where a single iteration of the Gibbs sampler consists of sampling a topic for each word. Each sampled topic assignment is generated from a condi-tional multinomial distribution over the K topics, which in turn requires the computation of K conditional probabili-ties. As an example, consider learning a topic model with one million documents, each with 1000 words on average, K = 1000 topics, and performing 500 Gibbs iterations (a typical number in practice). This would require generating a total of 5  X  10 11 word-topic assignments via sampling, where each sampling operation itself involves K = 1000 computa-tions.

The key idea of our paper is to reduce the time taken for the  X  X nner-loop X  sampling operation, reducing it from K to significantly less then K on average; we observe speedups up to a factor of 8 in our experiments. Furthermore, the speedup usually increases as K increases. In our proposed approach we exploit the fact that, for any particular word and document, the sampling distributions of interest are fre-quently skewed such that most of the probability mass is concentrated on a small fraction of the total number of top-ics K . This allows us to order the sampling operations such that on average only a fraction of the K topic probabilities need to be calculated. Our proposed algorithm is exact, i.e., no approximation is made and the fast algorithm correctly and exactly samples from the same true posterior distribu-tion as the slower standard Gibbs sampling algorithm.
The problem of rapidly evaluating or approximating prob-abilities and drawing samples arises in a great many do-mains. However, most existing solutions are characterized by the data being embedded in a metric space, so that ge-ometric relationships can be exploited to rapidly evaluate the total probability of large sets of potential states. Mix-ture modeling problems provide a typical example: a data structure which clusters data by spatial similarity, such as a KD-tree [2], stores statistics of the data in a hierarchical fashion and uses these statistics to compute upper and lower bounds on the association probabilities for any data within those sets. Using these bounds, one may determine whether the current estimates are sufficiently accurate, or whether they need to be improved by refining the clusters further (moving to the next level of the data structure).
Accelerated algorithms of this type exist for many com-mon probabilistic models. In some cases, such as k-means, it is possible to accelerate the computation of an exact solu-tion [1, 16, 17]. For other algorithms, such as expectation X  maximization for Gaussian mixtures, the evaluations are only approximate but can be controlled by tuning a qual-ity parameter [13, 10, 9]. In [8], a similar branch-and-bound method is used to compute approximate probabilities and draw approximate samples from the product of several Gaus-sian mixture distributions.

Unfortunately, the categorical nature of LDA makes it difficult to apply any of these techniques directly. Instead, although we apply a similar  X  X ound and refine X  procedure, both the bound and the sequence of refinement operations must be matched to the expected behavior of the data in topic modeling. We describe the details of this bound along with our algorithm in Section 4, after first reviewing the standard LDA model and Gibbs sampling.
LDAmodelseachof D documents as a mixture over K latent topics, each of which describes a multinomial distribu-tion over a W word vocabulary. Figure 1 shows the graphical model representation of the LDA model.

The LDA model is equivalent to the following generative process for words and documents:
For each of N j words in document j 1. sample a topic z ij  X  Multinomial(  X  j ) 2. sample a word x ij  X  Multinomial(  X  z ij ) where the parameters of the multinomials for topics in a document  X  j and words in a topic  X  k have Dirichlet priors. Intuitively we can interpret the multinomial parameter  X  k as indicating which words are important in topic k and the parameter  X  j as indicating which topics appear in document j [6]. Given the observed words x = { x ij } , the task of Bayesian inference is to compute the posterior distribution over the latent topic indices z = { z ij } , the mixing propor-is to use collapsed Gibbs sampling [6], where  X  and  X  are marginalized out, and only the latent variables z are sam-pled. After the sampler has burned-in we can calculate an estimate of  X  and  X  given z .

We define summations of the data by N wkj =# { i : x ij = w, z ij = k } , and use the convention that missing indices are summed out, so that N kj = In words, N wk is the number of times the word w is assigned to the topic k and N kj is the number of times a word in document j has been assigned to topic k . Given the current state of all but one variable z ij , the conditional probability of z ij is then where Z is the normalization constant and the superscript  X  ij indicates that the corresponding da-tum has been excluded in the count summations N wkj . Algorithm 3.1: LDA Gibbs Sampling ( z , x ) for i  X  1 to N do 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :
An iteration of Gibbs sampling proceeds by drawing a sample for z ij according to (1) for each word i in each doc-ument j . A sample is typically accomplished by first cal-culating the normalization constant Z , then sampling z ij according to its normalized probability; see Algorithm 3.1. Given the value sampled for z ij the counts N kj ,N k ,N wk updated. The time complexity for each iteration of Gibbs sampling is then O ( NK ).

Given a sample we can then get an estimate for  X   X  j and  X  given z :
For most real data sets after several iterations of the Gibbs sampler, the probability mass of the distribution p ( z ij k | of the topics as in Figure 4. FastLDA takes advantage of this concentration of probability mass by only checking a subset of topics before drawing a correct sample. After calculating the unnormalized probability in (1) of a sub-set of topics, FastLDA determines that the sampled value does not depend on the probability of the remaining top-ics. To describe how FastLDA works, it is useful to in-troduce a graphical depiction of how a sample for z ij conventionally drawn. We begin by segmenting a line of unit length into K sections, with the k th section having sample for z ij by drawing a value uniformly from the inter-val, u  X  Uniform[0,1], and selecting the value of z ij based on the segment into which u falls; see Figure 2.
As an alternative, suppose that we have a sequence of bounds on the normalization constant Z , denoted Z 1 ...Z such that Z 1  X  Z 2  X  ...  X  Z K = Z . Then, we can graphi-cally depict the sampling procedure for FastLDA in a similar way, seen in Figure 3. Instead of having a single segment for have several segments s k l ...s k K associated with each topic. The first segment for a topic k , s k k , describes a conserva-tive estimate of the probability of the topic given the upper bound Z k on the true normalization factor Z .Eachofthe subsequent segments associated with topic k , namely s k l l&gt;k , are the corrections for the missing probability mass for topic k given the improved bound Z l . Mathematically, Figure 2: Draw from p ( z ij = k | z  X  ij , x , X , X  ) . p a jk b wk . u  X  Uniform [0 , 1] . A topic k is sampled by finding which segment ( p k ) contains the draw u . Here the total number of topics K =6 . the lengths of these segments are given by Since the final bound Z K = Z , the total sum of the seg-ment lengths for topic k is equal to the true, normalized probability of that topic: Therefore, as in the conventional sampling method, we can draw z ij from the correct distribution by first drawing u Uniform[0 , 1], then determining the segment in which it falls.
By organizing the segments in this way, we can obtain a substantial advantage: we can check each segments in or-der, knowing only p 1 ...p k and Z k , and if we find that u falls within a particular segment s k l , the remaining segments are irrelevant. Importantly, if for our sequence of bounds Z ...Z K , an intermediate bound Z l depends only on the values of a jk and b jk for k  X  l , then we may be able to draw the sample after only examining topics 1 ...l .Giventhat in LDA, the probability mass is typically concentrated on a small subset of topics for a given word and document, in practice we may have to do far fewer operations per sample on average.
FastLDA depends on finding a sequence of improving bounds on the normalization constant, Z 1  X  Z 2  X  ...  X  Z K = Z . Figure 3: Draw from p ( z ij = k | z  X  ij , x , X , X  ) . p a jk b wk . u  X  Uniform [0 , 1] . A topic k is sampled by finding which segment ( s k j ) contains the draw u . Here the total number of topics K =4 .
 We first define Z in terms of component vectors a, b, c : Then, the normalization constant is given by
To construct an initial upper bound Z 0 on Z ,weturn to the generalized version of H  X  older X  X  inequality [7], which states Notice that, as we examine topics in order, we learn the lations to improve the bound at each step. We then have a bound at step l given by: This sequence of bounds satisfies our requirements: it is decreasing, and if l = K we recover the exact value of Z .In this way the bound improves incrementally at each iteration until we eventually obtain the correct normalization factor.
H  X  older X  X  inequality describes a class of bounds, for any valid choice of ( p, q, r ); these values are a design choice of the algorithm. A critical aspect in the choice of bounds is that it must be computationally efficient to maintain. In particular we want to be able to calculate Z 0 and update in constant time. We focus our attention on two natural choices of values which lead to computationally efficient im- X  , the norms can be updated in constant time, while for r =  X  ,wehave c l +1: K r =min k N k which is also rela-tively efficient to maintain. Section 4.4 provides more detail on how these values are maintained. Empirically, we found that the first choice typically results in a better bound (see Figure 10 in Section 6.4).

FastLDA maintains the norms a p , b q , c r separately and then uses their product to bound Z . One might con-sider maintaining the norm a b , b c or even Z instead, improving on or eliminating the bound for Z . The problem with maintaining any combination of the vectors a, b or c is that the update of one z ij will cause many separate norms to change because they depend on the counts of z ij variables, N wkj . For example, if we maintain d wk = b wk c k ,thena change of the value of z ij from k to k requires changes to d wk ,d wk  X  w resulting in O (2 W ) operations. However with-out d ,only b wk ,b wk ,c k ,c k change.
 Algorithm 4.1: FastLDA ( a, b, c ) sump k  X  0 u  X  X  X  Uniform[0 , 1] for k  X  1 to K do 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :
Finally, we must also consider the order in which the top-ics are evaluated. Execution time improves as the number of topics considered before we find the segment s k l containing u decreases. We thus would like the algorithm to consider the longest segments first, and only check the short segments if necessary. Two factors affect the segment length: p k ,the unnormalized probability, and Z l , the bound on Z at step l . Specifically, we want to check the topics with the largest p early, and similarly the topics which will improve (decrease) the bound Z l .

Those topics which fall into the former category are those with (relatively) large values for the product a k b k c k those falling into the latter category are those with large to seek out those topics k which have large values in one or more of these vectors.

Another factor which must be balanced is the computa-tional effort to find and maintain an order for refinement. Clearly, to be useful a method must be faster than a di-rect search over topics. To greedily select a good refinement order while ensuring that we maintain computational effi-ciency, we consider topics in descending order of N kj ,the frequency of word assignments to a topic in the current doc-ument (equivalent to descending order on the elements of b ). This order is both efficient to maintain (see Section 4.4) and appears effective in practice.
The sampling step for FastLDA begins with a sorted list of topics in descending order by N jk , the most popular topic for a document to the least popular. A random value u is sampled u  X  Uniform[0 , 1]. The algorithm then considers topics in order, calculating the length of segments s k l goes. Each time the next topic is considered the bound Z is improved. As soon as the sum of segments calculated so far is greater then u , the algorithm can stop and return the topic associated with the segment u falls on. Graphically, the algorithm scans down the line in Figure 3 calculating only s k l and Z k for the k topics visited so far. When the algorithm finds a segment whose end point is past u it stops and returns the associated topic. By intelligently ordering the comparisons as to whether u is within a segment, we need to do 2 K comparisons in the worst case.
To improve over the conventional algorithm, FastLDA must maintain the sorted order of N kj and the norms of each component: min k Nk , a l : K and b l : K ,moreeffi-ciently then the K steps required for the calculation of Z . The strategy used is to calculate the values initially and then update only the affected values after each sample of z . Maintaining the descending sort order of N kj or the minimum element of N k can be done inexpensively, and in practice much faster than the worst case O (log K )required for a delete/insert operation into a sorted array. We start by performing an initial sort of these integer arrays, which takes O ( K log K ) time. During an update, one element of N kj is incremented by one, and another element of N kj is decremented by one (likewise for N k ). Given that we have integer arrays, this update will render the array in almost sorted order, and we expect that only a few swaps are re-quired to restore sorted order. Using a simple bubble sort, the amortized time for this maintain-sort operation is very small, and in practice much faster than O (log K ).
Maintaining the value of the finite norms, a and b , from iteration to iteration can be done by calculating the values once during initialization and then updating the value when an associated z ij is sampled. Two norms need to be updated when z ij is updated, the value of a for document j and the value of b for word w ,where x ij = w . These updates can be done in O (1) time.

In addition, we require the incremental improvements at each step of the sampling process, i.e., at topic k  X  1we require a k : K and b k : K , the norms of the remaining topics Figure 4: Average fraction of a document explained by top-20 topics, for NYTimes (K=400 topics) and PubMed (K=2000 topics). We see that, on aver-age, the top-20 topics in any document account for approximately 90% of the words in the document.
 Table 1: Size parameters for the four data sets used in experiments. D is number of documents, N is total number of words in the collection, and W is size of vocabulary. from k to K . (We upper-bound c k : K by its initial value, c .) For finite p -norms, given a k : K p it is an O (1) update
We compared execution times of LDA and FastLDA using four data sets: NIPS full papers (from books.nips.cc), En-ron emails (from www.cs.cmu.edu/  X  enron), NYTimes news articles (from ldc.upenn.edu), and PubMed abstracts (from www.pubmed.gov). These four data sets span a wide range of collection size, content, and average document length. The NYTimes and PubMed collections are relatively large, and therefore useful for demonstrating the potential bene-fits of FastLDA. For each collection, after tokenization and removal of stopwords, the vocabulary of unique words was truncated by only keeping words that occurred more than ten times. The size parameters for these four data sets are shown in Table 1.

While the NIPS and Enron data sets are moderately sized, and thus useful for conducting parameter studies, the NY-Times and PubMed data sets are relatively large. Running LDA on the NYTimes data set using K = 1600 topics can take more than a week on a typical high-end desktop com-puter, and running LDA on the PubMed data set using K = 4000 topics would take months, and would require memory well beyond typical desktop computers. Conse-quently, these larger data sets are ideal candidates for show-ing the reduction in computation time from our FastLDA method, and measuring speedup on real-life large-scale cor-pora.
Before describing and explaining our experiments, we point the reader to the Appendix, which lists the exact parameter specifications used to run our experiments. With the goal of repeatability, we have made our LDA and FastLDA code publicly available at http:// www.ics.uci.edu/  X  iporteou/ fastlda and the four data sets at the UCI Machine Learning Repository, http:// archive.ics.uci.edu/ml/ machine-learning-databases/ bag-of-words/.

The purpose of our experiments was to measure actual reduction in execution time of FastLDA relative to LDA. Consequently, we setup a highly-controlled compute envi-ronment to perform timing tests. All speedup experiments were performed in pairs, with LDA and FastLDA being run on the same computer, compiler and environment to allow a fair comparison of execution times. Most computations were run on workstations with dual Xeon 3.0GHz processors with code compiled by gcc version 3.4 using -O3 optimization.
While equivalence of FastLDA to LDA is guaranteed by construction, we performed additional tests to verify that our implementation of FastLDA produced results identical to LDA. In the first test we verified that the implementa-tions of LDA and FastLDA drew samples for z ij from the same distribution. To do this, we kept the assignment vari-ables z  X  ij constant, and sampled a value for, but did not update, z ij . We did this for 1000 iterations and then ver-ified that the histograms of sampled values were the same between LDA and FastLDA. In the second test, using 100 runs on the NIPS corpus, we confirmed that the perplexity for FastLDA was the same as the perplexity for LDA. This double checking affirmed that FastLDA was indeed correctly coded, and therefore timings produced by FastLDA would be valid and comparable to those produced by LDA.
For the NIPS and Enron data sets, we timed the execution of LDA and FastLDA for 500 iterations of the Gibbs sam-pler, i.e., 500 sweeps through the entire corpus. This number of iterations was chosen to be large enough to guarantee that burn-in had occurred, and that samples were being drawn from the posterior distribution. This number of iterations also meant that the measurement of execution time was rel-atively accurate. Each separate case was run twice using different random initializations to estimate variation in tim-ings. These repeat timings of runs showed that the variation in CPU time for any given run was approximately 1%. We do not show error bars in the figures because this variation in timings was negligible.

For the NYTimes and PubMed data sets, we used a slightly different method to measure speedup, because of the consid-erably larger size of these data sets compared to NIPS and Enron. Instead of measuring CPU time for an entire run, we measured CPU time per iteration. To produce an accurate estimate, we estimated this per-iteration CPU time by tim-ing 20 consecutive iterations. FastLDA was initialized with Figure 5: CPU time for LDA and FastLDA, as a function of the number of topics K for NIPS and Enron data sets. parameters from an already burned-in model for NYTimes and PubMed. The K = 2000 and K = 4000 topic models of PubMed were computed on a supercomputer using 256 processors using the parallel AD-LDA algorithm [14].
Because of its large size, PubMed presented further chal-lenges. Running LDA or FastLDA on PubMed with K = 2000 and K = 4000 topics requires on the order of 100-200 GB of memory, well beyond the limit of typical work-stations. Therefore, we estimated speedup on PubMed us-ing a 250,000 document subset of the entire collection, but running LDA and FastLDA initialized with the parameters from the aforementioned burned-in model that was com-puted using the entire PubMed corpus of 8.2 million docu-ments. While the measured CPU times were for a subset of PubMed, the speedup results we show hold for FastLDA running on the entire collection, since the topics used were those learned for the entire 8.2 million documents.
For all experiments, we set Dirichlet parameter  X  =0 . 01 (prior on word given topic) and Dirichlet parameter  X  = 2 /K (prior on topic given document), except where noted. Setting  X  this way ensured that the total added probability mass was constant. These settings of Dirichlet hyperparam-eters are typical for those used for topic modeling these data sets, and similar to values that one may learn by sampling or optimization. We also investigated the sensitivity of speedup to the Dirichlet parameter  X  .

The bound presented in Section 4.1 was expressed in the more general form of H  X  older X  X  inequality. For all experi-ments, except where noted, we used the general form of H  X  older X  X  inequality with p =2, q =2, r =  X  . Section 6.4 examines the effect of different choices of p, q and r .Asis shown and discussed in that section, the choice of p =2, q =2, r =  X  is the better one to use in practice.
CPU time for LDA increases linearly with the number of topics K (Figure 5), an expected experimental result given Figure 6: Speedup factor over LDA, as a function of the number of topics K for NIPS and Enron data sets. the for loop over K topics in algorithm 3.1. The CPU time for FastLDA is significantly less than the CPU time for LDA for both the NIPS and Enron data sets. Furthermore, we see that FastLDA CPU time increases slower than linearly with increasing topics, indicating a greater speedup with in-creasing number of topics. Figure 6 shows the same results, this time displayed as speedup, i.e. the y-axis is the CPU Time for LDA divided by the CPU Time for FastLDA. For these data sets, we see speedups between 3  X  and 8  X  ,with speedup increasing with higher number of topics. The frac-tion of topics FastLDA must consider on average per sample is related to the fraction of topics used by documents on av-erage. This in turn depends on other factors such as the latent structure of the data and the Dirichlet parameters  X  and  X  . Consequently, in experiments using a reasonable number of topics the speedup of FastLDA increases as the number of topics increase.

Our summary of the speedup results for all four data sets are shown in Figure 7. Each pair of bars shows the speedup of FastLDA relative to LDA, for two different topic settings per corpus. The number of topics are: NIPS K = 400 , 800, Enron K = 400 , 800, NYTimes K = 800 , 1600 and PubMed K = 2000 , 4000, with the speedup for the larger number of topics shown in the black bar on the right of each pair. We see a range of 5  X  to 8  X  speedup for this wide variety of data sets and topic settings. On the two huge data sets, NYTimes and PubMed, FastLDA shows a consistent 5 . 7  X  to 7 . 5  X  speedup. This speedup is non-trivial for these larger computations. For example, FastLDA reduces the compu-tation time for NYTimes from over one week to less than one day, for K = 1600 topics.

The speedup is relatively insensitive to the number of doc-uments in a corpus, assuming that as the number of docu-ments increases the content stays consistent. Figure 8 shows the speedup for the NIPS collection versus number of top-ics. The three different curves respectively show the en-tire NIPS collection of D = 1500 documents, and two sub-collections made up of D = 800 and D = 400 documents (where the sub-collections are made up from random sub-Figure 7: Speedup of FastLDA over LDA for the four corpora. Bars show: NIPS K = 400 , 800 ,Enron K = 400 , 800 ,NYTimes K = 800 , 1600 and PubMed K = 2000 , 4000 .  X  =2 /K for all runs. samples of the full 1500-document collection). The figure shows that speedup is not significantly effected by corpus size, but predominantly dependent on number of topics, as observed earlier. The choice of Dirichlet parameter  X  more directly affects speedup, as shown in Figure 9. This is be-cause using a larger Dirichlet parameter smooths the distri-bution of topics within a document, and gives higher proba-bility to topics that may be irrelevant to any particular doc-ument. The resulting effect of increasing  X  is that FastLDA needs to visit and compute more topics before drawing a sample. Conversely, setting  X  to a low value further concen-trates the topic probabilities, and produces more than an 18  X  speedup on the NIPS corpus using K = 800 topics.
We experimented with two different bounds for Z , cor-responding to particular choices of p, q and r in H  X  older X  X  inequality. The first was setting p = q = 2 and r =  X  , i.e. using min k N k . We also used the symmetric setting of p = q = r = 3. In all comparisons so far we found the p = q = r = 3 setting resulted in slower execution times than p = q = 2 and r =  X  .

Figure 10 shows given two choices for p, q, r , how quickly the bound Z k converges to Z as a function of the number of topics evaluated. This plot shows the average ratio Z k /Z for the k th topic evaluated before drawing a sample. The faster Z /Z converges to 1, the fewer calculations are needed on average. Using the NIPS data set, four runs are compared using the two different choices of p, q, r and K = 400 versus K = 4000 topics. Here as well, we see that the bound pro-duced by p = q = r = 3 tends to give much higher ratios on average, forcing the algorithm to evaluate many topics before the probabilities approach their true values.
Topic modeling of text collections is rapidly gaining im-portance for a wide variety of applications including infor-mation retrieval and automatic subject indexing. Among Figure 8: Speedup over LDA as a function of the number of topics K , for different collection size ( D ) versions of the NIPS data set. these, Latent Dirichlet Allocation and Gibbs sampling are perhaps the most widely used model and inference algo-rithm. However, as the size of both the individual docu-ments and the total corpus grows, it becomes increasingly important to be as computationally efficient as possible.
In this paper, we have described a method for increasing the speed of LDA Gibbs sampling while providing exactly equivalent samples, thus retaining all the optimality guaran-tees associated with the original LDA algorithm. By orga-nizing the computations in a better way, and constructing an adaptive upper bound on the true normalization con-stant, we can take advantage of the sparse and predictable nature of the topic association probabilities. This ensures both rapid improvement of the adaptive bound and that high-probability topics are visited early, allowing the sam-pling process to stop as soon as the sample value is located. We find that this process gives a 3 X 8  X  factor of improvement in speed, with this factor increasing with greater numbers of topics. These speed-ups are in addition to improvements gained through other means (such as the parallelization tech-nique of Newman et al. [14]), and can be used in conjunction to make topic modeling of extremely large corpora practical.
The general method we describe, to avoid having to con-sider all possibilities when sampling from a discrete distribu-tion, should be applicable to other models as well. In partic-ular we expect the method to work well for other varieties of topic model, such as the Hierarchical Dirichlet Process [18] and Pachinko allocation [11], which have a sampling step similar to LDA. However, how to maintain an efficient upper bound for Z , the accuracy of the bound, and an efficient X  X o X  maintain ordering in which to consider topics, remain model specific problems.

Additionally, our bound X  X nd X  X efine algorithm used one refinement schedule based on the document statistics. Whether other choices of bounds or schedules could further improve the performance of FastLDA is an open question. Figure 9: Speedup over LDA as a function of the Dirichlet parameter  X  , using the NIPS data set. Decreasing  X  encourages sparse, concentrated topic probabilities, increasing the speed of our method. We thank John Winn for his discussions on the method. Computations were performed at San Diego Supercomput-ing Center using an MRAC Allocation. This material is based upon work supported by the National Science Foun-dation under Grant No. 0447903 and No. 0535278 and by ONR under Grant No. 00014-06-1-073. The work of PS, AA, and DN is supported in part by the National Science Foundation under Award Number IIS-0083489 and also by a National Science Foundation Graduate Fellowship (AA) and by a Google Research Award (PS). [1] K. Alsabti, S. Ranka, and V. Singh. An efficient [2] J. L. Bentley. Multidimensional binary search trees [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] W. Buntine, J. L  X  ofstr  X  om, J. Perki  X  o, S. Perttu, [5] C. Chemudugunta, P. Smyth, , and M. Steyvers. [6] T. L. Griffiths and M. Steyvers. Finding scientific [7] G. Hardy, J. E. Littlewood, and G. Polya. Inequalities . Figure 10: The average accuracy of the bound Zk/Z , as a function of the number of topics visited, for two possible choices of ( p, q, r ) . The norm choices (2 , 2 , appears to be considerably tighter, on average, than the symmetric choice (3 , 3 , 3) . [8] A. T. Ihler, E. B. Sudderth, W. T. Freeman, and A. S. [9] K. Kurihara and M. Welling. Bayesian k-means as a [10] K. Kurihara, M. Welling, and N. Vlassis. Accelerated [11] W. Li and A. McCallum. Pachinko allocation: [12] D. Mimno and A. McCallum. Organizing the OCA: [13] A. Moore. Very fast EM-based mixture model [14] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [15] D. Newman, K. Hagedorn, C. Chemudugunta, and [16] D. Pelleg and A. Moore. Accelerating exact k-means [17] D. Pelleg and A. Moore. X -means: Extending [18] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [19] X. Wei and W. B. Croft. Lda-based document models We describe here the parameter specifications used to run our experiments. We have made our LDA and FastLDA code publicly available at http:// www.ics.uci.edu/  X  iporteou/ fastlda and the four data sets at the UCI Machine Learn-ing Repository, http:// archive.ics.uci.edu/ ml/ machine-learning-databases/ bag-of-words/.

For all NIPS and Enron runs: 1.  X  =2 /K ,  X  =0 . 01, except for experiments versus  X  2. CPU times measured over 500 iterations, including burn-3. Speedup computed over entire run 4. Runs repeated with different random initializations
For NYTimes and PubMed runs: 1.  X  =2 /K ,  X  =0 . 01 2. CPU times measured over 20 iterations 3. Speedup computed on a per-iteration basis 4. LDA and FastLDA runs initialized with model param-
