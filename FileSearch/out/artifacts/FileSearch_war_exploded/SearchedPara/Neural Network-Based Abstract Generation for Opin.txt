 Collecting opinions from others is an integral part of our daily activities. Discovering what other peo-ple think can help us navigate through different as-pects of life, ranging from making decisions on reg-ular tasks to judging fundamental societal issues and forming personal ideology. To efficiently absorb the massive amount of opinionated information, there is a pressing need for automated systems that can gen-erate concise and fluent opinion summary about an entity or a topic. In spite of substantial researches in opinion summarization, the most prominent ap-proaches mainly rely on extractive summarization methods, where phrases or sentences from the origi-nal documents are selected for inclusion in the sum-mary (Hu and Liu, 2004; Lerman et al., 2009). One of the problems that extractive methods suffer from is that they unavoidably include secondary or redun-dant information. On the contrary, abstractive sum-marization methods, which are able to generate text beyond the original input, can produce more coher-ent and concise summaries.

In this paper, we present an attention-based neu-ral network model for generating abstractive sum-maries of opinionated text . Our system takes as in-put a set of text units containing opinions about the same topic (e.g. reviews for a movie, or arguments for a controversial social issue), and then outputs a one-sentence abstractive summary that describes the opinion consensus of the input.

Specifically, we investigate our abstract genera-tion model on two types of opinionated text: movie reviews and arguments on controversial topics . Ex-amples are displayed in Figure 1. The first exam-ple contains a set of professional reviews (or crit-ics) about movie  X  X he Martian X  and an opinion con-sensus written by an editor. It would be more use-ful to automatically generate fluent opinion consen-sus rather than simply extracting features (e.g. plot, music, etc) and opinion phrases as done in previous summarization work (Zhuang et al., 2006; Li et al., 2010). The second example lists a set of arguments on  X  X eath penalty X , where each argument supports the central claim  X  X eath penalty deters crime X . Ar-guments, as a special type of opinionated text, con-tain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim gener-ation .

Existing abstract generation systems for opinion-ated text mostly take an approach that first identi-fies salient phrases, and then merges them into sen-tences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work re-quires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions.

To address the challenges above, we propose to use an attention-based abstract generation model  X  a data-driven approach trained to generate informa-tive, concise, and fluent opinion summaries. Our method is based on the recently proposed frame-work of neural encoder-decoder models (Kalchbren-ner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous work, our summarization system is designed to sup-port multiple input text units. An attention-based model (Bahdanau et al., 2014) is deployed to al-low the encoder to automatically search for salient information within context. Furthermore, we pro-pose an importance-based sampling method so that the encoder can integrate information from an im-portant subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise preference-based regularizer. With importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input.

We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Pap-ineni et al., 2002) indicates that our system outper-forms the state-of-the-art extract-based and abstract-based methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Toma-toes movie reviews, compared to 19.72 by an ab-stractive opinion summarization system from Gane-san et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems. We collected two datasets for movie reviews and arguments on controversial topics with gold-standard abstracts. 1 Rotten Tomatoes ( www. rottentomatoes.com ) is a movie review web-site that aggregates both professional critics and user-generated reviews (henceforth RottenToma-toes ). For each movie, a one-sentence critic con-sensus is constructed by an editor to summarize the opinions in professional critics. We crawled 246,164 critics and their opinion consensus for 3,731 movies (i.e. around 66 reviews per movie on average). We select 2,458 movies for training, 536 movies for val-idation and 737 movies for testing. The opinion con-sensus is treated as the gold-standard summary.
We also collect an argumentation dataset from idebate.org (henceforth Idebate ), which is a Wikipedia-style website for gathering pro and con arguments on controversial issues. The arguments under each debate (or topic) are organized into dif-ferent  X  X or X  and  X  X gainst X  points. Each point con-tains a one-sentence central claim constructed by the editors to summarize the corresponding arguments, and is treated as the gold-standard. For instance, on a debate about  X  X eath penalty X , one claim is  X  X he death penalty deters crime X  with an argument  X  X n-acting the death penalty may save lives by reducing the rate of violent crime X  (Figure 1). We crawled 676 debates with 2,259 claims. We treat each sen-tence as an argument, which results in 17,359 argu-ments in total. 450 debates are used for training, 67 debates for validation, and 150 debates for testing. In this section, we first define our problem in Sec-tion 3.1, followed by model description. In gen-eral, we utilize a Long Short-Term Memory network for generating abstracts (Section 3.2) from a latent representation computed by an attention-based en-coder (Section 3.3). The encoder is designed to search for relevant information from input to bet-ter inform the abstract generation process. We also discuss an importance-based sampling method to al-low encoder to integrate information from an impor-tant subset of input (Sections 3.4 and 3.5). Post-processing (Section 3.6) is conducted to re-rank the generations and pick the best one as the final sum-mary. 3.1 Problem Formulation In summarization, the goal is to generate a summary y , composed by the sequence of words y 1 ,..., | y | . Unlike previous neural encoder-decoder approaches which decode from only one input, our input con-sists of an arbitrary number of reviews or arguments (henceforth text units wherever there is no ambigu-ity), denoted as x = { x 1 ,...,x M } . Each text unit x k Each word takes the form of a representation vector, which is initialized randomly or by pre-trained em-beddings (Mikolov et al., 2013), and updated during training. The summarization task is defined as find-ing  X  y , which is the most likely sequence of words  X  y ,...,  X  y N such that: where log P ( y | x ) denotes the conditional log-likelihood of the output sequence y , given the input text units x . In the next sections, we describe the attention model used to model log P ( y | x ) . 3.2 Decoder Similar as previous work (Sutskever et al., 2014b; Bahdanau et al., 2014), we decompose log P ( y | x ) into a sequence of word-level predictions: where each word y j is predicted conditional on the previously generated y 1 ,...,y j  X  1 and input x . The probability is estimated by standard word softmax: h j is the Recurrent Neural Networks (RNNs) state variable at timestamp j , which is modeled as:
Here g is a recurrent update function for generating the new state h j from the representation of previ-ously generated word y j  X  1 (obtained from a word lookup table), the previous state h j  X  1 , and the input text representation s (see Section 3.3).
 In this work, we implement g using a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), which has been shown to be ef-fective at capturing long range dependencies. Here we summarize the update rules for LSTM cells, and refer readers to the original work (Hochreiter and Schmidhuber, 1997) for more details. Given an ar-bitrary input vector u j at timestamp j  X  1 and the previous state h j  X  1 , a typical LSTM defines the fol-lowing update rules:  X  is component-wise logistic sigmoid function, and W  X  X  X  and biases b  X  are parameters to be learned dur-ing training.

Long range dependencies are captured by the cell memory c j , which is updated linearly to avoid the vanishing gradient problem. It is accomplished by predicting two vectors i j and f j , which determine what to keep and what to forget from the current timestamp. Vector o j then decides on what infor-mation from the new cell memory c j can be passed to the new state h j . Finally, the model concatenates the representation of previous output word y j  X  1 and the input representation s (see Section 3.3) as u j , which serves as the input at each timestamp. 3.3 Encoder The representation of input text units s is computed using an attention model (Bahdanau et al., 2014). Given a single text unit x 1 ,...,x | x | and the previous state h j , the model generates s as a weighted sum: where a i is the attention coefficient obtained for word x i , and b i is the context dependent repre-sentation of x i . In our work, we construct b i by building a bidirectional LSTM over the whole in-put sequence x 1 ,...,x | x | and then combining the for-ward and backward states. Formally, we use the LSTM formulation from Eq. 5 to generate the for-ward states h f jection word x j using a word lookup table). Like-using a backward LSTM by feeding the input in the reverse order, that is, u j = x | x | X  j +1 . The coeffi-cients a i are computed with a softmax over all input: where function v computes the affinity of each word x i and the current output context h j  X  1  X  how likely the input word is to be used to gener-are parameters to be learned. 3.4 Attention Over Multiple Inputs A key distinction between our model and ex-isting sequence-to-sequence models (Sutskever et al., 2014b; Bahdanau et al., 2014) is that our input consists of multiple separate text units. Given an input of N text units, i.e. to concatenate them into one sequence as z = where SEG is a special token that delimits inputs.
However, there are two problems with this ap-proach. Firstly, the model is sensitive to the order of text units. Moreover, z may contain thousands of words. This will become a bottleneck for our model with a training time of O ( N | z | ) , since attention co-efficients must be computed for all input words to generate each output word.

We address these two problems by sub-sampling from the input. The intuition is that even though the number of input text units is large, many of them are redundant or contain secondary information. As our task is to emphasize the main points made in the input, some of them can be removed without los-ing too much information. Therefore, we define an importance score f ( x k )  X  [0 , 1] for each document x k (see Section 3.5). During training, K candidates are sampled from a multinomial distribution which is constructed by normalizing f ( x k ) for input text units. Notice that the training process goes over the training set multiple times, and our model is still able to learn from more than K text units. For test-ing, top-K candidates with the highest importance scores are collapsed in descending order into z . 3.5 Importance Estimation We now describe the importance estimation model, which outputs importance scores for text units. In general, we start with a ridge regression model, and add a regularizer to enforce the separation of summary-worthy text units from others.

Given a cluster of text units { x 1 ,...,x M } and their summary y , we compute the number of overlapping content words between each text unit and summary y as its gold-standard importance score. The scores are uniformly normalized to [0 , 1] . Each text unit x k is represented as an d  X  dimensional feature vector k  X  R d , with label l k . Text units in the training data are thus denoted with a feature matrix  X  R and a label formulation for ridge regression, and we use fea-tures in Table 1. Furthermore, pairwise preference constraints have been utilized for learning ranking models (Joachims, 2002). We then consider adding a pairwise preference-based regularizing constraint to incorporate a bias towards summary-worthy text units:  X   X  where T is a cluster of text units to be summa-rized. Term ( r p  X  r q )  X  w enforces the separation of summary-worthy text from the others. We further each element as 1 . The objective function becomes:  X  ,  X  are tuned on development set. With  X   X  =  X   X  I d 3.6 Post-processing For testing phase, we re-rank the n -best summaries according to their cosine similarity with the input text units. The one with the highest similarity is in-cluded in the final summary. Uses of more sophis-ticated re-ranking methods (Charniak and Johnson, 2005; Konstas and Lapata, 2012) will be investi-gated in future work. Data Pre-processing. We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and depen-dency relations. For RottenTomatoes dataset, we re-place movie titles with a generic label in training, and substitute it with the movie name if there is any generic label generated in testing.
 Pre-trained Embeddings and Features. The size of word representation is set to 300, both for in-put and output words. These can be initialized randomly or using pre-trained embeddings learned from Google news (Mikolov et al., 2013). We also extend our model with additional features described in Table 2. Discrete features, such as POS tags, are mapped into word representation via lookup tables. For continuous features (e.g TF-IDF scores), they are attached to word vectors as additional values. Hyper-parameters and Stop Criterion. The LSTMs (Equation 5) for the decoder and encoders are defined with states and cells of 150 dimensions. The attention of each input word and state pair is computed by being projected into a vector of 100 dimensions (Equation 6).

Training is performed via Adagrad (Duchi et al., 2011). It terminates when performance does not im-prove on the development set. We use BLEU (up to 4-grams) (Papineni et al., 2002) as evaluation met-ric, which computes the precision of n-grams in gen-erated summaries with gold-standard abstracts as the reference. Finally, the importance-based sampling rate ( K ) is set to 5 for experiments in Sections 5.2 and 5.3.

Decoding is performed by beam search with a beam size of 20, i.e. we keep 20 most probable out-put sequences in stack at each step. Outputs with end of sentence token are also considered for re-ranking. Decoding stops when every beam in stack generates the end of sentence token. 5.1 Importance Estimation Evaluation We first evaluate the importance estimation compo-nent described in Section 3.5. We compare with Support Vector Regression (SVR) (Smola and Vap-nik, 1997) and two baselines: (1) a length baseline that ranks text units based on their length, and (2) a centroid baseline that ranks text units according to their centroidness, which is computed as the co-sine similarity between a text unit and centroid of the cluster to be summarized (Erkan and Radev, 2004).
We evaluate using mean reciprocal rank (MRR), and normalized discounted cumulative gain at top 3 and 5 returned results (NDCG@3). Text units are considered relevant if they have at least one overlap-ping content word with the gold-standard summary. From Figure 2, we can see that our importance es-timation model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as ref-erence. ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases.

For comparisons, we first compare with an ab-stractive summarization method presented in Gane-san et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algo-rithm to remove repetitive information, and merge opinionated expressions based on syntactic struc-sider two extractive summarization approaches: (1) L
EX R ANK (Erkan and Radev, 2004) is an unsuper-vised method that computes text centrality based on PageRank algorithm; (2) Sipos et al. (2012) propose a supervised SUBMODULAR summarization model which is trained with Support Vector Machines. In addition, LONGEST sentence is picked up as a base-line.

Four variations of our system are tested. One uses randomly initialized word embeddings. The rest of them use pre-trained word embeddings, additional features in Table 2, and their combination. For all systems, we generate a one-sentence summary.
Results are displayed in Table 3. Our system with pre-trained word embeddings and additional fea-tures achieves the best BLEU scores on both datasets (in boldface ) with statistical significance (two-tailed Wilcoxon signed rank test, p &lt; 0 . 05 ). Notice that our system summaries are conciser (i.e. shorter on average), which lead to higher scores on precision based-metrics, e.g. BLEU, and lower scores on recall-based metrics, e.g. METEOR and ROUGE. On RottenTomatoes dataset, where summaries gen-erated by different systems are similar in length, our system still outperforms other methods in METEOR and ROUGE in addition to their significantly bet-ter BLEU scores. This is not true on Idebate, since the length of summaries by extract-based systems is significantly longer. But the BLEU scores of our system are considerably higher. Among our four systems, models with pre-trained word embeddings in general achieve better scores. Though additional features do not always improve the performance, we find that they help our systems converge faster. 5.3 Human Evaluation on Summary Quality For human evaluation, we consider three aspects: in-formativeness that indicates how much salient infor-mation is contained in the summary, grammaticality that measures whether a summary is grammatical, and compactness that denotes whether a summary contains unnecessary information. Each aspect is rated on a 1 to 5 scale (5 is the best). The judges are also asked to give a ranking on all summary varia-tions according to their overall quality.
 We randomly sampled 40 movies from Rotten-Tomatoes test set, each of which was evaluated by 5 distinct human judges. We hired 10 proficient En-glish speakers for evaluation. Three system sum-maries (LexRank, Opinosis, and our system) and human-written abstract along with 20 representative reviews were displayed for each movie. Reviews with the highest gold-standard importance scores were selected.

Results are reported in Table 4. As it can be seen, our system outperforms the abstract-based sys-tem O PINOSIS in all aspects, and also achieves bet-ter informativeness and grammaticality scores than L
EX R ANK , which extracts sentences in their origi-nal form. Our system summaries are ranked as the best in 18% of the evaluations, and has an average ranking of 2.3, which is higher than both O PINOSIS and L EX R ANK on average. An inter-rater agree-ment of Krippendorff X  X   X  of 0.71 is achieved for overall ranking. This implies that our attention-based abstract generation model can produce sum-maries of better quality than existing summarization systems. We also find that our system summaries are constructed in a style closer to human abstracts than others. Sample summaries are displayed in Figure 3. 5.4 Sampling Effect We further investigate whether taking inputs sam-pled from distributions estimated by importance scores trains models with better performance than the ones learned from fixed input or uniformly-sampled input. Recall that we sample K text units based on their importance scores ( Importance-Based Sampling ). Here we consider two other setups: one is sampling K text units uniformly from the in-put ( Uniform Sampling ), another is picking K text units with the highest scores ( Top K ). We try vari-ous K values. Results in Figure 4 demonstrates that Importance-Based Sampling can produce compara-ble BLEU scores to Top K methods, while both of them outperform Uniform Sampling. For METEOR score, Importance-Based Sampling uniformly out-5.5 Further Discussion Finally, we discuss some other observations and po-tential improvements. First, applying the re-ranking component after the model generates n -best ab-stracts leads to better performance. Preliminary ex-periments show that simply picking the top-1 gener-ations produces inferior results than re-ranking them with simple heuristics. This suggests that the current models are oblivious to some task specific issues, such as informativeness. Post-processing is needed to make better use of the summary candidates. For example, future work can study other sophisticated re-ranking algorithms (Charniak and Johnson, 2005; Konstas and Lapata, 2012).

Furthermore, we also look at the difficult cases where our summaries are evaluated to have lower in-formativeness. They are often much shorter than the gold-standard human abstracts, thus the information coverage is limited. In other cases, some generations contain incorrect information on domain-dependent facts, e.g. named entities, numbers, etc. For in-stance, a summary  X  X  poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland X  is generated for movie  X  X ore X . This sum-mary contains  X  X ate Shortland X  which is the direc-tor of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted. Our work belongs to the area of opinion summa-rization. Constructing fluent natural language opin-ion summaries has mainly considered product re-views (Hu and Liu, 2004; Lerman et al., 2009), com-munity question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summariza-tion approaches are employed to identify summary-worthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corre-sponding feature. Our model instead utilizes ab-stract generation techniques to construct natural lan-guage summaries. As far as we know, we are also the first to study claim generation for arguments.
Recently, there has been a growing interest in generating abstractive summaries for news arti-cles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014). Most approaches are based on phrase extrac-tion, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains.

Our task is closely related to recent advances in neural machine translation (Kalchbrenner and Blun-som, 2013; Sutskever et al., 2014a). Based on the sequence-to-sequence paradigm, RNNs-based mod-els have been investigated for compression (Filip-pova et al., 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level. Built on the attention-based trans-lation model in Bahdanau et al. (2014), Rush et al. (2015) study the problem of constructing abstract for a single sentence. Our task differs from the mod-els presented above in that our model carries out ab-stractive decoding from multiple sentences instead of a single sentence. In this work, we presented a neural approach to generate abstractive summaries for opinionated text. We employed an attention-based method that finds salient information from different input text units to generate an informative and concise summary. To cope with the large number of input text, we de-ploy an importance-based sampling mechanism for model training. Experiments showed that our sys-tem obtained state-of-the-art results using both au-tomatic evaluation and human evaluation.

