 Text classification techniques rely heavily on the presence of a good feature set , or indexing terms, and the selection of discriminant features with regards to the classes. This task to the  X  X leanliness X  of the documents: the presence of non-relevant or repetitive contents, as is often found on the Web, will degrade their performance. In our work, we are especially interested in a particular kind of Web documents, call for tenders , in which a contracting authority invites contractors to submit a tender for their products and/or services. These documents can be found on the contracted organisation Web site, or on dedicated tendering sites. In earlier work [1] we hypothesized that the noise in such documents was caused by the use of a sublanguage [2,3] that describes the procedural aspects of the tenders submission, rather than their topic.
 classification methods [4], it is not clear whether it is adequate to filter such  X  X ro-cedural X  noise. Indeed in our experiments with call for tenders we have found it difficult to extract either the procedural language (i.e. non-relevant features), or the tenders topic language (i.e. relevan t features). There seems to be a significant overlap between the two vocabularies. However certain patterns or constructs of the procedural language can immediately be seen in the documents, and their presence should be an indication of the relevance of the surrounding context. a contextual approach to filter out words or passages in the documents. That is, we first select some n-grams features or named entities, and accept or reject passages based on their presence or absen ce. Our aim is to improve classification removing the  X  X oise X  from documents.
 our first approach to content filtering based on filtering out a passage around the least relevant n-grams. We obtain a significant increase of the micro-F1 measure by using bigrams and a window passage: +11% on our collection of call for tenders, and +3.6% on the 4 Universities dataset. Our second approach, filtering around named entities, gives a moderate increase (+2.6%). We have also tried combining the two approaches, but with little success to report so far. The search for a better feature set for classification is hardly new. It has been demonstrated that feature selection is central to some algorithms such as Naive Bayes [4], and therefore several techniques have been proposed, the most popular being InfoGain . In early work by Lewis [5] the use of phrases , i.e. terms syntactically connected, was considered as a replacement for single-term features. The results were discouraging, which could be partly explained by the fact that there were too many index terms, with low frequency and high redundancy. Still, the idea was revisited by many. More recently Tan et al. [6] find an improvement on the classification of Web pages, by using a combination of bigrams and unigrams selected on the merits of their InfoGain score. However the same technique applied to the Reuters collection did not yield the same gain, mostly because of its over-emphasis on  X  X ommon concepts X . Since their method favours recall, the authors conclude it was harder to improve Reuters because it already had high recall. of documents relevant to a user profile. There has been much interest lately with spam filtering [7]. Content filtering, such as discussed in this paper, is also not a new idea, although it has not often been linked with classification. Early work with filtering based on character n-grams met with surprising success [8]. In [9] the notion of non-relevant passages in a document is exploited: a document is classified based on the relevance of its passages and their sequence as modeled with Hidden Markov Models. The area of automatic summarisation [10] is also related, since one of its subgoals is also to identify the most meaningful sentences. For example, the relevancy of a sentence can be defined based on its position, length, the frequency of the terms and its similarity with the title [11]. [12] but rarely the other way around. Its use in classification is mostly restricted to replacing common strings such as dates or money amounts with tokens, to increase the ability of the classifier to generalise. 3.1 The MBOI Project This study is part of the MBOI project (Matching Business Opportunities on the Internet), which deals with the discovery of business opportunities on the Internet [13]. The project aims to develop tools for business watch, including spidering, information extraction, classification, and search. The aspect of inter-est here, classification, consists of classifying call for tenders by industry type, according to one of the existing norms: SIC (Standard Industrial Classification), NAICS (North American Industry Classification System), FCS (Federal Supply Codes), CPV (Common Procurement Vocabulary), etc.
 formation amongst submission instructions, rules, requirements, etc. Sometimes the notice posted on the Web will have all relevant information to determine the subject, sometimes very little, and in some extreme cases none. Often the contracting authority will have the applicant pay to get a full description of the call for tender.
 of the documents vary a lot. Although a given organisation will tend to reuse the same patterns, it would not be feasible to manually define filters based on these patterns, without falling into a maintenance nightmare. 3.2 The Test Collection For our experiments, we created a collection of call for tenders documents by downloading the XML daily synopsis from the FedBizOpps Web site (ten-ders solicited by American government agencies, available at http://www. fedbizopps.gov/ ). The XML documents have the same contents as the HTML documents found on the same site. The period downloaded ranged from Septem-ber 2000 to October 2003. We kept only one document per tender, i.e. chose a document amongst pre-solicitations and amendments. Our collection (thereafter called FBO) is available at http://iro.umontreal.ca/ ~ paradifr/fbo/ . such as the date of publication ( X 21 May 2001 X ), classification codes (NAICS  X 424120 X  and FCS  X 75 X ), the contracting authority ( X  X ffice of Environmental Studies X ), etc. The body of the document is composed of the subject line and the description; only these fields will be used for classification. Only a portion of the body is indicative of the tender subject (shown in bold). The rest concerns dates and modalities for submission.
 (although FCS will not be used here). Since the NAICS codes were not tagged in XML at the time (as they now are), they were extracted from the free text description. This resulted in 21945 documents (72Megs), which were splitted 60% for training, and 40% for testing.
 to a level of the hierarchy. For example, for industry code 424120 (Stationery and Office Supplies Merchant Wholesalers) the sector code is 424 (Merchant Wholesalers, Nondurable Goods). Each of the three participating countries, the U.S., Canada and Mexico, have their own version of the standard, which mostly differ at the level of industry codes (5th or 6th digit). We reduced the category space by considering only the first three digits, i.e. the corresponding  X  X ector X . This resulted in 92 categories (vs. 101 for FCS). We did not normalise for the uneven distribution of categories: for NAICS, 34% of documents are in the top two categories, and for FCS, 33% are in the top five.
 tested on the unfiltered documents. Naive Bayes is a common choice in the literature for baseline, and it is known to be sensitive to feature selection, which makes it appropriate to our study. Furthermore, some of the better performing but costlier techniques, such as SVM, do not scale up to our project requirement of handling more than 100K documents.
 following thresholds were applied: a rank cut of 1 ( rcut ), a fixed weight cut of 0.001 ( wcut ), and a category cut learnt after cross-sampling 50% of the test set over 10 iterations ( scut ). More details about these thresholding techniques can be found in [15,16].
 for our baseline classifier are shown in table 1, under the label  X  X nfiltered X . The next line,  X  X nfiltered bigram X , is provided as an indication of the effect of bigrams alone on classification. It is again unfiltered contents, but using only bigrams as features (see our definition of bigram below). Since the feature space is much larger, we have selected 64,000 feat ures this time. Surprisingly, there is a great drop in the micro-F1 measure. A quick look at the bigrams shows that many were actually part of the procedural language. It is difficult to filter them out, because when we select less features, we also remove  X  X ood X  features and decrease the micro-F1 measure further. Two levels of passage filtering will be considered, depending on the unit be-ing filtered: sentences or windows (i.e. sequence of words). Window filtering is appealing on our collection, because sentences can be long, and relevant and non-relevant information is often mixed in a sentence. Also, segmenting into sentences is not trivial in this collectio n, because it is not we ll formatted: for example the end-of-sentence period could be missing, or a space could appear inside an acronym (e.g.  X  X . S. X ). 4.1 Supervised Filtering of Sentences In a first experiment we manually labeled 1000 sentences from 41 documents of FBO. The label was  X  X ositive X  if the sentence was indicative of the tender X  X  subject, or  X  X egative X  if not. Sentences with descriptive contents were labeled positive, while sentences about submission procedure, rules to follow, delivery dates, etc. were labeled negative. In the example of figure 1, only the first sen-tence would be labeled positive. Overall, almost a quarter of the sentences (243) were judged positive.
 i.e. the author would start by introducing the subject of the tender, and then explain the rules and requirements. This is not always the case. In combined tenders, the text often starts with background information, and then define each item. In some cases, the subject is scattered amongst negative sentences. positive and negative classes. The task seems to be relatively simple, since when we tested the classifier on a 40/60 split we obtained a micro-F1 measure of 85%. We thus filtered the whole collection with this classifier, keeping only the positive sentences. The collection size went f rom around 600,000 sentences to 96,811. The new, filtered documents were then classified with another Naive Bayes classifier. micro-F1 measure, 7.6% over the baseline ( X  X nfiltered X ). Although this result in itself is interesting, our real aim is to achieve unsupervised filtering, i.e. not requiring a training collection and labeled sentences. We propose in the next section a technique to select sentences based on the presence of vocabulary. 4.2 Unsupervised Filtering of Sentences Our approach to unsupervised filtering of sentences is to build a list of n-grams from the collection, and then filter out either a sentence or a window of terms around each of their occurences in the documents. We define an n-gram as a consecutive sequence of n words, after r emoval of stop words. For example, we have found the following top 5 n-grams in FBO:  X  unigrams:  X  X ommercial X ,  X  X tems X ,  X  X cquisition X ,  X  X overnment X  and  X  X nfor- X  bigrams:  X  X tems-commercial X ,  X  X usiness-small X ,  X  X onditions-terms X ,  X  trigrams:  X  X ink-fedbizopps-document X ,  X  X upplemented-additional-sure and the frequency of the n-gram. Although one would expect the InfoGain measure to be a better discriminant, it turns out that the high-frequency terms in FBO are uniformely distributed in the classes, so the simpler frequency works as well if not slightly better. Also, since we are trying to select non-relevant features, i.e. features with low InfoGain, we will also capture the unfrequent features, whether they are distributed evenly or not.
 grams (i.e.  X  X ent-unigram X ,  X  X ent-bigram X  and  X  X ent-trigram X ). Only the most frequent 1,500 n-grams in the collection were kept (this parameter was deter-mined manually). The criterion for a sentence to be filtered out was the following: a sentence was rejected if 1/8 of its n-grams were in the reject list (again this parameter was determined empirically).
 our baseline), which is quite similar to the results obtained with the trained classifier in the preceding section. 4.3 Window Filtering As mentioned before, although the sentence seems like a good logical unit to perform filtering, it is a bit problematic in our collection because it is not so well delimited, and it is not guaranteed to have the right granularity (i.e. sentences can contain both relevant and non-relevant information). Another approach is to ignore punctuation and sentence markers, and to filter a window around a term.
 up to m words after the n-gram. Additionaly, two regions to be filtered out are connected if  X  X lose X  enough.
  X  X indow-trigram X , respectively). of window filtering for bigrams and trigrams, using term frequency. A window of size 2 was used, i.e. the region filtered out started with the two terms preceding the n-gram, up to the two succeeding terms. Two regions to be filtered out were  X  X onnected X  if less than 6 terms apart. The window-bigrams filter gives our best results: a micro-F1 of 0.6101 micro-F1 (+11%) and a macro-F1 of .3787 (+14.9%).
 training set only. Slightly higher figures can be reached by calculating term frequency over both the training and the test set (the default in rainbow). This is possible in a real scenario, if we constantly update the term distribution with new documents.
 on non-relevant features. We have also tried the opposite, i.e. selecting relevant features and keeping only those sentences or windows where they appeared. The results are similar. 5.1 Entities as Indicators of Relevance Named entities are expressions containing names of people, organisations, loca-tions, time, etc. These often appear in call for tenders, but are rarely indicative of the subject of the tender. Therefore, we hope that by identifying these ex-pressions, we can either filter out passages that contain them, or reduce their impact on the classifier.
 lowing:  X  geographical location . In a call for tender, this can be an execution or delivery  X  organisation . Most often the organisation will be the contracting authority  X  date . This can be a delivery date or execution date (opening and closing  X  time . A time limit on the delivery date, or business hours for a point of  X  money . The minimum/maximum contract value, or the business size of the  X  URL . The Web site of the contracting authority or a regulatory site (e.g. a  X  email , phone number . The details of a point of contact.
 generic in the sense that they also apply to many other domains. We have also considered the following entities, specific to our collection:  X  FAR (Federal Acquisition Rules). These are tendering rules for U.S. govern- X  CLIN (Contract Line Item Number). The line item define a part or sub- X  dimensions . In the context of a tender, a dimension almost always refers to ence is an indication of a negative passage or sentence, i.e. not relevant to the subject of the tender. CLIN and dimensions on the other hand are positive in-dicators, since they introduce details about the contract or product. and Nstein NFinder , a tool for the extraction of named entities. Table 2 shows the accuracy of the entities as positive/negative indicators on the 1000 training sentences. For example, dates (a negative indicator) appeared in 62 sentences, 59 of which were labeled negative. Dimensions (a positive indicator) appeared in 8 sentences, all of which were labeled positive.
 an accuracy around or lower that of an  X  X lways-negative X  classifier (which would be correct 75.7% of the time on our 1000 sentences). That is partly because they often appear along with the subject in an introductory sentence. For example in figure 1 the first sentence contains an organisation,  X  X ffice of Environmental Studies X , a location,  X  X echanicsburg, PA X , as well as the subject,  X  X oner car-tridges and supplies X . Furthermore, these entities are inherently more difficult to recognise than date and time, which only require a few simple patterns, and can achieve near-perfect recognition accuracy. To make matters more difficult, some documents are all in capital letters, which make the task more difficult because there are no clues to distinguish proper and common nouns. Some examples of errors were:  X  X pace Flight X  as a person,  X  X OB X  as an organisation, or  X 184BW Contracting Office X  as a location. 5.2 Classification with Entities As noted above, a common use of named entities in text categorisation is to replace each instance in the text with a generic token. That strategy does not seem to pay off on FBO, as shown in table 3, under the label  X  X okens X . The micro-F1 measure does not change and the macro-F1 decreases by 1.5%. We have tried different combinations of entities, especially leaving out locations and organisations, all with similar results.
 ject. For example, knowing that the contracting authority is the USDA (U.S. Department of Agriculture) increases the likelihood of a tender to be relevant to agriculture. This information is already taken into account by the classifier if the full name appears in the text. However if the acronym alone appears, only limited inference is possible (unless the acronym systematically appeared in all tenders of its kind).
 training collection. Firstly we have built an acronym list from all organisation entities of the form:  X  X ull name (acronym) X . We thus collected 1068 acronyms, excluding two-letters acronyms, which were deemed too ambiguous, especially since our collection includes many two-letter state abbreviations. We then ex-panded acronyms in the documents (except when they appeared inside brackets), and used the window-bigrams selection. Unfortunately, as shown in the last line of table 3,  X  X cronym X , this approach yielded a micro-F1 of .5265, a decrease of 4.2% over the baseline. One possible exp lanation for this poor performance is the high degree of ambiguity in the acronyms. For example, ISS refers to  X  X nte-grated Security System X  or  X  X nternational Space Station X . In this case we put both expansions in the document.
 from table 2. We have built a sentence filter that rejects a sentence if enough negative indicators are found. For indicators with a 100% accuracy, one instance is enough to reject a sentence. For others, we give a weight to each entity equals to its accuracy minus 75.7% (i.e. the accuracy of the always-negative classifier). We sum up the weights, and reject the sentence if it is above a threshold (which we have set to .40 in this experiment). The results of this filtering, under label  X  X ndicators X , yields a micro-F1 of 0.5640, or +2.6% of the baseline. Each instance was replaced with a generic token, as above, and the bigrams were computed with those tokens. The aim was to find more generic patterns inside the bigrams. For example, the bigrams now included patterns such as  X  X xceed-[money] X  (as in  X  X usiness size should not exceed $10.4M X ). Such a pattern could not be picked up before because money amounts, as other numbers, would be rejected by the tokeniser. Furthermore, using an entity tag increases the frequency of the bigram and therefore its chance to be included in the filter list. Unfortunately, as shown in table 3 under labels  X  X rained X  and  X  X indow-bigram X , this combination does not significantly increase over the sentence and window filtering of the preceding section: the trained method goes from .5918 to .5935, and the window-bigram from .6101 to .6077. We have tried these techniques on the 4-Universities collection, which can be obtained from CMU World Wide Knowledge Base project at URL http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/ .
 It contains 8282 Web pages collected mainly from four American universities, and manually classified in seven categories: student pages, faculty, staff, department, course, project or other.
 2000 features, as suggested by the authors. We have also used their script to replace some numbers with generic tokens. We have not however implemented their cross-tests, i.e. train on three, and test on one university.
 lection. Contrary to FBO, the use of bigrams and named entity tokens (tokens) had a positive impact on the unfiltered collection, with an increase of 8.6 and 2.6%, respectively. We did not test the  X  X ndicators X  filtering, since we did not have information about the accuracy of the entities.
 to the InfoGain measure, and obtained a micro-F1 value of 0.6723, an increase of 3.6% over the baseline. However, the macro-F1 has suffered a hefty drop (-9.7%). We have not yet studied the reasons for this. We have investigated the use of bigrams and named entities to perform content filtering. Our domain of application was the classification of call for tenders. Our findings are that filtering a windows of terms around most frequent bigrams works well for this kind of collection: we could obtain an increase of 11% of micro-F1. We also get a moderate improvement of 2.6% by filtering sentences based on named entities; however this method relies on an accuracy estimate, which is not always practical to get in reality. We have tried combining the two approaches but so far our results are rather inconclusive.
 increase of the micro-F1 using the window-bigram method. This is, we hope, an indication that this method is well-suited for Web collections. We plan to do more tests in the future to verify that claim.
 similarly for trigrams we did not include bigrams. This might explain the lower results for trigrams, because they are too restrictive, and fail to capture common two-terms relationships. An obvious extension, as in [6], is to combine them. and non-relevant sentences in the document. This idea is similar to the HMM proposed in [9].
 This project was financed jointly by Nstein Technologies and NSERC.

