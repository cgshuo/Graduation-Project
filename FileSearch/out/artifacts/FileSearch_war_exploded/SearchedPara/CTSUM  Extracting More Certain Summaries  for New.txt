 People often read summaries of news articles in order to get reliable information about an event or a topic. However, the information expressed in news articles is not always certain, and some sentences contain uncertain information about the event. Existing summarization systems do not consider whether a sentence in news articles is cert ain or not. In this paper, we propose a novel system called CTSUM to incorporate the new factor of information certainty into the summarization task. We first analyze the sentences in news articles and automatically predict the certainty levels of sentences by using the support vector regression method with a few useful features. The predicted certainty scores are then incorporated into a summarization system with a graph-based ranking algorithm. Experimental results on a manual ly labeled dataset verify the effectiveness of the sentence certainty prediction technique, and experimental results on the DUC 2007 dataset shows that our new summarization system cannot only produce summaries with better content quality, but also produce summa ries with higher certainty. H.3.1 [ Information Storage and Retrieval ]: Content Analysis Intelligence ]: Natural Language Processing  X  text analysis Algorithms, Performance, Design, Human Factors. CTSUM; information certainty; multi-document summarization. With the rapid growth of on-line digital content publishing and propagation, there are usually hundreds or thousands of news articles about an event or topic. People often read summaries of news articles in order to obtain useful and certain information about an event or a topic. Existing multi-document summarization systems (e.g. NewsInEssence [28], NewsBlaster [7]) can be used to produce a short summary for a collection of news articles. However, the information expressed in news articles is not always unreliable or uncertain. For ex ample, the following sentence expresses the writer X  X  speculation about an event, and it contains a weasel word  X  seems  X . Therefore, the sentence contain uncertain information.  X  X owever, it seems that Obama will not use the platform to relaunch his stalled drive for Israeli-Palestinian peace. X  Based on our analysis in Section 6.2, there are a considerable portion of sentences containing uncertain information in news articles, while human labeled refe rence summaries usually contain more certain information. Human annotators tend to extract and use certain sentences when they annotate summaries for news documents. Disappointingly, existing summari zation systems do not consider the factor of information certainty, and therefore some uncertain information may be injected into the summaries produced by existing summarization systems, which will hinder users X  acquisition of the genuine information about the news event. In this study, we investigate the factor of information certainty in the task of multi-document summarization, and propose a novel system called CTSUM to incorporate this new factor into the multi-document summarization task. The CTSUM system can produce more certain summaries for news articles. To the best of our knowledge, our proposed system is the first to incorporate the factor of information certainty into the summarization task. After an in-depth investigation of the certain or uncertain information in news articles, we propose a few useful features to distinguish between certain senten ces and uncertain sentences in news articles. Our proposed CTSU M system first estimates the certainty scores of sentences in news articles by using the SVM regression learner and then incorporates the estimated sentence-level certainty scores into a summarization system based on the graph-based ranking algorithm. Experimental results verify the effectiveness of both the sentence-level certainty score estimation technique and the summarization system. Based on experiments on a manually-labeled sentence set, the estimated certainty scores are highly correlated with human labeled scores. Evaluations on the DUC2007 dataset shows that our proposed CTSUM system can extract both high-quality and certain summaries for news articles, and it can significantly outperform the baseline GRSUM system without considering the certainty factor. The contributions of this work are summarized as follows: 1) We originally investigate the information certainty factor in 2) We propose a novel system called CTSUM based on the 3) We propose to assess the se ntence-level information 4) Experimental results on th e DUC2007 dataset verify the The rest of this paper is organized as follows: Section 2 reviews related work. Our proposed system is overviewed in Section 3. Section 4 describes the key techni ques of sentence-level certainty assessment and the evaluation results. Section 5 describes the summarization methods. The summarization evaluation results are presented in Section 6. Lastly, we conclude this paper in Section 7. Multi-document summarization is th e task of producing a concise and fluent summary to deliver th e major information for a given document set (usually news articles) . If the summary is required to topic-focused or query-biased mu lti-document summarization. The methods for multi-document summarization can be coarsely categorized into abstraction-based or extraction-based. In this study, we focus on extraction-based method s, which have been adopted in most existing summarization systems. The extraction-based summariza tion methods usually rank and select a few existing sentences in the documents to form a summary. Sentence extraction methods can be rule-based or learning-based. Rule-based methods make use of heuristic rules to score sentences by considering a few features, such as sentence position, TFIDF, etc. Such summarization methods include the centroid-based method [27] and NeATS [19]. Lear ning based methods have been proposed for optimally combining various sentence features based on supervised learning techniques [25, 33, 35, 51]. In order to capture more reliable semantic relatedness between sentences for document summarization, latent semantic analysis, matrix factorization and deep learning ha ve been explored [21, 45]. Recent advanced methods formulate the summarization problem as various optimization problems, and solve the problems by selecting an optimal subset of sentences from the document set. Such methods include budgeted median [37], minimum dominating set [34], A* search [1], integer linear programming [10, 15], data reconstruction [11], submodular function [17, 18]. Particularly, graph-based ranking algorithms have been widely used for both generic and topic-focused document summarizations in recent years. LexRank [6] and TextRank [24] are two earliest graph-based summarization models, which adopt the basic PageRank style algorithms to rank sentences. Later on, a few models have been proposed to enhance the basic PageRank algorithm. For example, the topic-sensitive PageRank model is applied for topic-focused multi-document summarization [41]. The ClusterCMRW and ClusterHITS models [43] use cluster-based ranking algorithms to compute the saliency scores of sentences by taking into account the cluster-level information in the graph-based ranking algorithm. The DsR model [48] is a document-sensitive graph-based ranking model for multi-document summarization, and it considers the document-level influences in the ranking model. The mutual reinforcement chain model [47] further makes use of three different text granularities, i.e., document, sent ences and terms, to construct a heterogeneous graph and develops a mutual reinforcement learning approach for topic-focused document summarization. The manifold-ranking and multi-modality manifold-ranking models have also been applied for topic-focused multi-document summarization [42, 44], and the basic assumption underlying the algorithms is that similar senten ces are likely have same ranking scores and sentences on the same structure (i.e. cluster) are likely to have the same ranking scores. All existing summarization systems do not consider the factor of information certainty in the su mmarization process, and they assume that all sentences in news articles are of equal certainty and thus any sentence can be selected into the summary if the sentence is highly ranked based on some ev aluation metric, which is not very appropriate. In contrast, our pr oposed CTSUM system will take into account the sentence-level certainty score in the graph-based ranking algorithm in order to improve the summarization performance. The concept of certainty has different dictionary definitions, but they usually revolve around the noti on of  X  X he quality or state of mind of being free from doubt, especially on the basis of evidence X  [23]. There are several related concepts, which have been addressed in previous NLP and linguistic s studies: subjectivity, modality, evidentiality, factuality and hedging. Certainty can be viewed as a type of subjective information available in texts and a form of epistemic modality expressed through explicitly-coded linguistic means [31]. There are usually explicit certainty markers in texts to explicitly signal presence of certainty information that covers a full continuum of writer X  X  confid ence, ranging from uncertain possibility and withholding full commitment to statements to a confident necessity, reassurance, and emphasizing of the full commitment to statements. The certainty markers include such devices as subjectivity expressions, epistemic comments, evidentials, approximators, understatements, te ntatives, intensifiers, emphatics, boosters, and assertives. In the NLP field, text subjectivity analysis, event factuality annotation and hedge detection are the most closely related studies. Subjectivity concerns discourse participants and their stance with respect to what is conveyed by means of the text. Subjectivity encompasses a diverse set of interrelated research lines in the fields of NLP and data mining . For example, some work is devoted to identifying the author X  X  affectual (or emotional) state [5]. Another related area focu ses on opinion identification at different levels of granularity: document-level [26, 38], clausal-and phrasal-level [29, 50] a nd lexical level [30, 49]. Event factuality is defined as the level of information expressing the commitment of relevant sources towards the factual nature of events mentioned in discourse [32]. Events in discourse are FactBank [32] is a corpus of ev ents annotated with factuality information. Each event X in texts is annotated and assigned with one of six committed values or two underspecified values. The three positive committed values include CT+, PR+, PS+: CT+ means according to the source, it is certainly the case that X; PR+ means according to the source, it is probably the case that X; PS+ means according to the source, it is possibly the case that X. For example, the event  X  X eave X  in the following sentence was annotated with a committed value of  X  X S+ X . Hedge detection has been a shared task of the CoNLL conference, which aims to identify sentences which contain uncertain information and recognizing in-sentence spans which are speculative [8]. It has received considerable interest recently in the NLP field. A hand-crafted list of hedge cues has been used to identify speculative sentence in MEDLINE abstracts [16]. The most recent approaches to this task exploit supervised or semi-supervised models and various f eatures have been attempted, including single word features [22], n-gram features [36], syntactic features [13] and Wiki pedia weasel tags [9]. Most researches are based on the BioScope corpus [40], which consists of manually labeled biological texts from full papers and scientific abstracts. Another related but different research area is information credibility, which is a much broader concept than information certainty. Information credibility has been an active research area with the advent of on-line documen ts and social media content (e.g. blogs, comments, microblogs, etc.). Information credibility usually refers to the believabil ity or quality of the information and/or its source. Hilligoss and Rieh [12] present a unifying framework of credibility assessment in which credibility is characterized across a variety of media and resources with respect to diverse information seekin g goals and tasks. Automatic methods for credibility analysis have been performed on web pages, articles, blogs, messages and facts. Bendersky et al. [2] determine the quality of a web document by its readability, layout and ease-of-navigation, and other factors, and then a quality-biased ranking method is presented to promote documents containing high-quality content. Li et al [14] propose a two-step method to determine whether a given statement is truthful, and if statement. Weerkamp and Rijke [46] estimate two groups of credibility indicators for blog posts, and integrate them into the topical blog post retrie val process. Castillo et al. [3] analyze microblog postings related to  X  X rendi ng X  topics, and classify them as credible or not credible, based on a variety of features extracted from users X  posting and re-posting behavior, from the text of the posts, and from citations to external sources. As mentioned earlier, existing summarization systems do not consider the information certainty factor in the summarization process. In order to address this problem, we propose a novel system called CTSUM to produce more certain summaries for news articles. A certain summary must meet the following two goals: 1) Summary content quality : The content quality of a 2) Summary certainty : The sentences in a certain summary Since sentence is the natura l and widely-used unit in the extraction-based summarization sy stems, we focus on sentence-level certainty analysis in this study. Our proposed CTSUM system can ach ieve the above two goals by summarization process. It consis ts of two components: sentence-level certainty assess ment and certain summary extraction. The first component aims to estimat e the certainty score of each sentence in news articles and the second component aims to extract summary sentences by considering the sentence-level certainty scores. The two components will be described in details in next two sections, respectively. A news writer X  X  certainty level may remain constant in a news text or it may fluctuate from statement to statement. In this study, we focus on automatic estimation of sentence-level certainty level in news articles and the task is defi ned as a process of assessing how certain readers are about the statement in a sentence is evident and factual. In this study, we aim to assign a certainty score of each sentence to indicate the sentence X  X  certainty level, rather than coarsely categorize each sentence into  X  X ertain X  or  X  X ncertain X . The task of certainty assessment has traditionally been considered a task for humans. Fortunately, much certainty information comes from linguistic coding in texts an d some explicit markers can be recognized and used for automatic certainty assessment. As far as we know, there exist no publicly available benchmark corpus for this task. Related corpora include FactBank [32] and BioScope [40], but they are not suitable for this task. In the FactBank corpus, only the event-leve l factuality value is annotated, but the sentence-level certainty is different from the event-level factuality. In the BioScope corpus, the text genres are biological texts from full papers and scientif ic abstracts, which are different from new articles. Moreover, the sentences in the BioScope corpus are coarsely labeled as  X  X ertain X  or  X  X ncertain X . Therefore, we annotated our own corpus for this task. We first collected 1000 sentences from th e FactBank corpus, which are from news articles. Two students were employed for certainty level annotation. The two students were firstly asked to read a short annotation guideline, which synthetically considered the categorization dimensions in [31]. Then they were asked to label a score between 1 and 5 for each sentence separately after they carefully read the sentence. Here,  X 5 X  means  X  X ery certain X ;  X 4 X  means  X  X lmost certain X ;  X 3 X  means  X  X oderately certain X ;  X 2 X  means  X  X lmost uncertain X ,  X 1 X  means  X  X ery uncertain X . Note that during the annotation process, the original event-level factuality annotation results were provided to the students for their reference. The raw agreement between their annotations is 0.586, and the annotations X  consistent degree measured by Correlation Coefficient (  X  ) is 0.7424. Finally, the overall certainty score of each sentence is the average of the scores provided by the two annotators. Figure 1 shows the distribution of overall certainty scores of the sentences, and we can see there are a considerable portion of sentences with uncertain information. 
Figure 1. Distribution of overall certainty scores of sentences As mentioned above, sentence-level certainty assessment is a task of mapping each sentence to a numerical value corresponding to the certainty level. The larger the value is, the more certain the sentence is. The task can be considered a regression problem and in this study, we adopt the  X  -support vector regression (  X  -SVR) method [39] for addressing this prediction task. The SVR algorithm is firmly grounded in the framework of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points. More specifically, we use the LIBSVM tool [4] with the RBF kernel for this regression task. We use the following four groups of features for each sentence, which are derived from different dimensions. Explicit certainty markers : Explicit certainty marker words and phrases in a sentence are usually good indicators for its certainty level. For example, if there are some words like  X  X robably X  and  X  X aybe X  in a sentence, the sentence is likely to be uncertain, or the certainty level will be reduced accordingly. We collect a list of 34 explicit certainty markers (e.g.  X  X ikely X ,  X  X ossible X ), and use the presence or absence of the markers as a binary feature. Subjectivity markers : Usually, expressing objectively is more factual than expressing subjectively. Such information as judgments, opinions, attitudes, beliefs and emotions usually reflect an idea that does not represent an external reality, but rather a hypothesized world, ex isting only in someone X  X  mind. Therefore, the use of such subjective information will devalue the certainty level. We collect 88 subjectivity markers (e.g.  X  X ear X ) and use the presence or absence of the markers as a binary feature. future) to the moment when the sentence was written. The future tense is less certain than the past and present tenses, since the future is usually predictions, plans, warnings and suggested actions, which may not come true. We collect 51 markers referring to future time (e.g.  X  X  X hall X ,  X  X ext year X ) and use the presence or absence of the markers as a binary feature. Perspective factor : It refers to the view of writer or reporters. Directly involved state is more cer tain than indirectly one. For example, the information conveyed by indirectly involved third parties (e.g. experts, analysts , or anonymous  X  X omeone X ) is usually less certain than the information conveyed by directly involved ones (e.g. victims and w itnesses). We collect 19 markers referring to indirectly involved states (e.g.  X  X omeone X ,  X  X uoted X ) and use the presence or absence of the markers as a binary feature. We use the open-source OpenNLP toolkit 1 to parse each sentence into a constituency-based parse tree and we obtain the ranges of main clause and subordinate clau ses if existed. We compute the above feature values from the main clause and subordinate clauses separately, and use all the feat ure values for regression. The reason is the above factors have different influences in main clause or subordinate clauses. All the above feature values are scaled by using the provided svm-scale program. For evaluation, we randomly sepa rated the labeled sentence set into ten sets of 100 sentences, a nd selected nine of them as a training set and the remaining one as a test set. We then used the http://opennlp.apache.org/ LIBSVM tool for training and te sting. The process was conducted for 10 times, and finally the results were averaged. Two standard metrics were used for evaluating the prediction results. The two metrics are as follows: Mean Square Error (MSE) : This metric is a measure of how correct each of the prediction values is on average, penalizing more severe errors more heavily. Pearson X  X  Correlation Coefficient (  X  ) : This metric is a measure of whether the trends of predicti on values matched the trends for human-labeled data. Table 1 shows the prediction results with the standard deviation values. We implement a SVR baseline for comparison, which simply uses all words in sentences as features. The results of the SVR method after removing every group of features are also reported. We can see that the overall resu lts of the SVR method with all groups of features are very promising and the performance is much better than the baseline method. We can also see the each feature group is beneficial to the overall prediction performance, because the performance values have a decline after removing any group of features. minus explicit certainty markers 0.4854 Finally, we apply the SVR method to predict the certainty score of each sentence in the document sets to be summarized. The certainty score is then normalized into [0, 1] by dividing by the maximum score. Finall y, each sentence s i in news articles is associated with a normalized certainty score CertainScore(s larger the score is, the more certain the sentence is. In this study, we focus on topic-focused multi-document summarization and adopt the gra ph-based ranking framework for sentence ranking because it has been widely adopted for document summarization in recent years [41]. Existing graph-based ranking algorithms are based on  X  X oting X  or  X  X ecommendation X  between sentences. A link between two sentences is considered as a vote cast from one sentence to the other sentence. The score associated with a sentence is determined by the votes that are cast for it, and the score of the sentences casti ng these votes. For topic-focused multi-document summarization, the relevance between sentences and the given topic is incorporated into the graph-based ranking framework as priors. In order to make use of the certai nty information of each sentence, we assume that sentences with high certainty level should be ranked higher than sentences with lo w certainty level. We alter the transition matrix by considering the sentence-level certainty information for achieving this goal. Formally, given a document set D and a topic or query description between sentences in the document set. V is the set of vertices and each vertex s i in V is a sentence in the document set. E is the set of edges. Each edge e ij in E is associated with an affinity weight f ( s s standard cosine measure between the two sentences as follows: where Here, we have f ( s i , s j )= f ( s j , s i ) and let f ( s self transition. The transition probability from s i to s j normalizing the corresponding affinity weight as follows: Note that p ( s i , s j ) is usually not equal to p ( s normalized matrix M ~ =( corresponding to the transition probability. We can see that the transition probability from s i to s similarity between s i and all other sentences. If sentence s then the transition probability from s i to s j is high, no matter if s certain or not. A high transition probability from s will propagate more of its score to s j . CertainScore(s j ) of each sentence s j , we change Equation (1) into Equation (4) by adding the certainty factor to obtain a new affinity weight f new ( s i , s j ) from sentence s follows: where  X   X  0 is a parameter to control the influence of the certainty score of each sentence and f new ( s i , s j ) is usually not equal to f s ). The new transition probability from s i to s j is then computed as follows: describe G with each entry corres ponding to the new transition probability. We can see that if  X  is set to 0, then we have f new ( s p value, the certainty factor in Equation (4) is dominated by the certainty score of sentence s j . The new transition probability from s i to s j level of s j . Sentences with high cert ainty level are likely to receive more from a source sentence than sentences with low certainty level, and thus highly certain sentences are likely to be ranked higher than less certain se ntences for summary extraction. Note that in Equations (4) and (5), we do not make use of the certainty score of the source sentence s i when computing the transition probability from s i to s j , because the certainty score of s will not affect the transition probability after row normalization. We also compute the relevance score rel(s i , q) of each sentence s to query q by using the standard cosine measure. The relevance score is then normalized to rel  X  (s i , q) as follows in order to make the sum of all re levance values of the sentences equal to 1. Based on matrix M ~ , the topic-biased saliency score InfoScore(s for sentence s i can be deduced from those of all other sentences linked with it, and it can be formulated in a recursive form as follows: Similarity, based on matrix new M ~ , the new topic-biased saliency score InfoScore new (s i ) for sentence s i can be deduced recursively as follows: where  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. After computing the saliency score InfoScore(s InfoScore new (s i ), we can select highly rank ed sentences to form the summary. For the multi-document summarization tasks, some sentences are highly overlapping with each other, and thus we apply the same greedy algorithm as in [41, 44] to penalize the sentences highly overlapping with other highly scor ed sentences. The sentences are firstly ranked by their saliency scores, and then the most highly ranked sentence is selected into the summary, and the saliency scores of the remaining sentences are penalized according to the content overlap (standard cosine similarity) between the sentences and the selected summary sent ence. The above selection is iterated until the summary length reaches the length limit. The details of the algorithm is omitted here. Based on the saliency scores com puted with Equation (9), we can produce summaries without considering the certainty information of sentences, and the system is named GRSUM in this study. Based on the saliency scores co mputed with Equation (10), we can produce summaries with considering the certainty information of sentences, which is named CTSUM in this study. The GRSUM is considered the baseline system, and it is actually a degeneration version of CTSUM when is  X  set to 0. It is worth noting that the certaint y level of sentences can also be easily incorporated into other document summarization methods, which is, however, not the focus of this study. We conducted experiments for topic-focused multi-document summarization, which has been one of the fundamental tasks in the DUC conferences. We used the DUC2007 dataset as test set for evaluation. Forty-five document clusters with topic descriptions were provided. NIST assessors first developed topics/questions of interest to them Reference summaries have been created for all the document clusters by NIST assessors. Note that multiple reference summaries written by different assessors are provided for each document cluster. Given a t opic description and relevant documents, the task aims to create from the documents a brief, well-organized, fluent summary which answers the need for information expressed in the topic. Each topic consisted of a title and a narrative text, and we concatenated the title and narrative text to represent the topic. In addition, we used the DUC2006 dataset as development set for parameter tuning and the value of  X  is set to 1 for CTSUM in the experiments. The two datasets are summarized in Table 2. As a preprocessing step for similarity computation, the stop words in each sentence were removed and the remaining words were stemmed using the Porter X  X  stemmer 2 . In this section we conduct analysis of the certainty level of the evaluation datasets. We used the SVR method to predict the certainty score of each sentence in each document set and obtain an average score for each document set, and then the scores are further averaged across all document sets. We also used the SVR method to predict the certainty score of each sentence in each reference summary and obtain an average score for each reference summary, and then the scores are further averaged across all http://www.tartarus.org/martin/PorterStemmer/ reference summaries for all document sets. The final average scores are presented in Table 3, where  X  X tdev X  means the standard deviation. We can see that the average certainty score in reference summaries are significantly higher than that in documents on both the development set and the test set. That X  X  to say, in the news articles, there are a considerable portion of sentences with relatively low certainty, but in reference summaries, most sentences are highly certain. The results validate our assumption that human annotators tend to select or write certain sentences to produce the summaries, and the uncertain information is seldom used. Therefore, our proposed CTSUM can benefit from the explicit use of the certai nty scores of sentences. 
Table 3. Comparison of average certainty scores of sentences We used the ROUGE-1.5.5 [20] t oolkit for evaluating the content quality of produced summaries by comparing them with the reference summaries, which has been widely adopted by DUC and TAC for automatic summary quality evaluation. It measured summary quality by counting overl apping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summa ry. ROUGE-N is an n-gram based measure and the recall oriented score, the precision oriented score and the F-measure score for ROUGE-N are computed as follows: where n stands for the length of the n-gram, and Count gram) is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries. Count(n-gram) is the number of n-grams in the reference summaries or candidate summary. We showed the popular three ROUGE scores in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based) and ROUGE-SU4 (based on skip bigram with a maximum skip distance of 4). Both F-measure and Recall scores are reported in the experiments. Note that the ROUGE scores are computed for each document set, and then the scores are averaged. When using the ROUGE-1.5.5 toolk it, we used the option  X -l 250 X  in order to truncate the summary longer than the length limit, and used the option  X -m X  for word stemming. Firstly, we evaluate the cont ent quality of our proposed CTSUM system by using the ROUGE metrics. Since our proposed CTSUM system is a direct improvement of the GRSUM system by considering a new factor of sentence-level certainty in the transition probability calculation, we compare CTSUM and GRSUM to show whether the new factor is helpful to the summarization performance. The comparison results over ROUGE F-measure and Recall NIST baseline is also presented and it is the official baseline system established by NIST. We can see from Table 4 that CTSUM outperforms GRSUM over all metrics. In order to show whether the performance differences between CTSUM and GRSUM are statistically significant, we apply the paired t-test (two-tailed) over each metric and the p-values are shown in Table 5. We can see that all the p-values are smaller than 0.01, and the results demonstrate that CTSU M can significantly outperform GRSUM over all ROUGE metrics. Since the differ ence between CTSUM and GRSUM lies only in the use of the sentence-level certainty score for calculating the transition probability between sentences, the superior performance of CTSUM over GRSUM can validate the efficacy of the sentence-level certainty factor. Table 5. P-values of two-tailed t-tests for ROUGE scores of We also compare our proposed CTSUM system with a few advanced baselines, as shown in Table 6. For fair comparison, we only compare CTSUM with the following unsupervised summarization methods: ManifoldRank : This method is proposed in [44], and it makes use of the manifold-ranking algorithm to rank sentences. The ROUGE F-measure scores in the table are di rectly borrowed from [42]. MultiMR : This method is proposed in [42], and it considers the within-document sentence relati onships and the cross-document sentence relationships as two modalities and makes use of the multi-modality manifold-ranking algorithm to rank sentences. Four different ranking schemes (i .e. LIN, COM, SEQ1 and SEQ2) are employed. The ROUGE F-meas ure scores in the table are directly borrowed from [42]. DSDR : This method is proposed in [11], and it extracts summary sentences that can best reconstr uct the original documents. The linear reconstruction model (DSDR-lin) and the nonnegative linear reconstruction model (DSDR-non) are proposed. The ROUGE F-measure scores in the table are di rectly borrowed from [11] ClusterHITS : This method is proposed in [43], and it considers the topic clusters as hubs and sent ences as authorities, then applies the HITS algorithm to rank sentences. The ROUGE F-measure scores in the table are directly borrowed from [11]. SNMF : This method is proposed in [45], and it uses symmetric non-negative matrix factorization for sentence clustering and then select sentences from each cluste r. The ROUGE F-measure scores in the table are directly borrowed from [11]. The ROUGE F-measure scores of the systems are shown in Table 6. We can see that our proposed CTSUM system outperform the baseline methods and the performance of CTSUM is comparable to that of the state-of-the-art methods. Table 6. Comparison with other methods on DUC2007 Secondly, we evaluate the summary certainty of our proposed CTSUM system. Two evaluation methods are employed: automatic evaluation and manual evaluation. Automatic evaluation makes use of the automatically estimated certainty scores. Each summary X  X  certainty score is the average of the certainty scores of all the sentences in the summary, and then the summaries X  certainty scores are averaged across the 45 document sets. The average certainty scores for CTSUM and GRSUM are compared in Table 7, and the detailed certainty scores for all summaries are compared in Figure 2. We can see that the average certainty score for CTSUM is significantly higher than that for GRSUM, which means CTSUM can produce more certain summaries than GRSUM. Manual evaluation relies on subjective assessment of the summaries X  certainty levels. Two students are employed to manually check all the summaries produced by CTSUM and GRSUM, and label for each summary a score between 1 to 5 to indicate the summary X  X  certainty level. We average the labeled scores for each summary across the two students, and then average the scores across the 45 document sets. The comparison results are shown in Table 8 and Figure 3. We can get the same conclusion that CTSUM can indeed produce more certain summaries than GRSUM. Figure 2. Detailed co mparison of estimated certainty scores of 
Figure 3. Detailed comparison of manually labeled certainty We now examine the influence of parameter  X  in our proposed CTSUM system. In the above experiments,  X  is set to 1, and we now vary  X  from 0 to 5 with a step of 0.5. We show the curves of ROUGE-1 F-measure, ROUGE-2 F-measure, ROUGE-SU4 F-measure, and the average certainty score in Figures 4-7, respectively. Note that when  X  is equal to 0, the CTSUM system corresponds to the GRSUM system. We can see from Figures 4-6 that when  X  is equal to 0, the summarization performance is the worst, and the summarization performance rise up with the increase of  X  from 0 to 1. When  X  is larger than 1, the performance almost keeps steady. The reason is that when  X  is set to any large value, the certainty factor in Equation (4) is dominated by the by the specific value of  X  . We can see from Figure 7 that when  X  is equal to 0, the average certaint y score of produced summaries is the lowest, and the average certainty score rises up sharply with the increase of  X  from 0 to 1. When  X  is larger than 1, the average certainty score tends to become steady. The curves in the figures demonstrate that our CTSUM system can always produce summaries with better content quality and higher certainty than GRSUM, and the certainty factor is validated to be very helpful in the summarization process. Finally, we show the example summaries produced by GRSUM and CTSUM for document set D0725 in DUC2007 (Corresponding to document set ID 25 in Figures 2-3). The unique sentences in each summary are highlighted in italics and bold. The estimated certainty score of the summary produced by GRSUM is 3.195, while the estimated cert ainty score of the summary produced by CTSUM is 3.971. We can see that some unique sentences extracted by GRSUM have low certainty scores, e.g., the 2nd sentence and the 11th sentence. In this paper we investigate the certainty factor in the summarization process and propose a new system called CTSUM to extract more certain summaries for news articles. Our CTSUM system first estimates the certainty score of each sentence, and then makes use of the sentence-level certainty score in the graph-based ranking summarization algorith m. Experimental results on the DUC2007 dataset verify the helpfulness of the sentence-level certainty score, and our proposed CTSUM system can significantly outperform the baseline GRSUM system. As mentioned earlier, the sentence-level certainty score can be easily incorporated into other summarization methods, and in future work we will conduct more experiments with other summarization methods to further show the merit of the certainty factor. In this study, we focus on summarization of news articles. With the increase of social media content (e.g. Twitter, blogs), the certainty or credibility problem in social media content is more incorporating the certainty factor into summarization of social media content in future work. This work was supported by NSFC (61170166, 61331011), Beijing Nova Program (2008B03) and National High-Tech R&amp;D Program (2012AA011101). We thank Jianmin Zhang and Bingqing Li for data annotation, a nd Qi Su for earlier discussion of data annotation guideline. We also thank the anonymous reviewers for their helpful comments. [1] A. Aker, T. Cohn, and R. Gaizauskas. Multi-document [2] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of [3] C. Castillo, M. Mendoza, and B. Poblete. Information credibility on [4] C.-C. Chang and C.-J. Lin. LIBSVM : a library for support vector [5] K. Dave, S. Lawrence and D. M. Pennock. Mining the peanut [6] G. Erkan and D. R. Radev. LexPageRank: prestige in multi-[7] D. K. Evans, J. L. Klavans, and K. R. McKeown. Columbia [8] R. Farkas, V. Vincze, G. M X ra, J. Csirik and G. Szarvas. The [9] V. Ganter and M. Strube. Finding hedges by chasing weasels: Hedge [10] D. Gillick, B. Favre and D. Hakkani-Tur. The ICSI summarization [11] Z. He, C. Chen, J. Bu, C. Wang, L. Zhang, D. Cai and X. He. [12] B. Hilligoss and S. Y. Rieh. Developing a unifying framework of [13] H. Kilicoglu and S. Bergler. Reco gnizing speculativ e language in [14] X. Li, W. Meng, and C. Yu. T-verifier: verifying truthfulness of fact [15] C. Li, X. Qian and Y. Liu. Using supervised bigram-based ILP for [16] M. Light, X. Y. Qiu and P. Srinivasan. The language of bioscience: [17] H. Lin and J. Bilmes. Multi-docu ment summarization via budgeted [18] H. Lin and J. Bilmes. A Class of Submodular Functions for [19] C.-Y. Lin and E.. H. Hovy. Fr om single to multi-document [20] C.-Y. Lin and E.H. Hovy. Automatic evaluation of summaries using [21] Y. Liu, S.-H. Zhong and W. Li. Query-Oriented Multi-Document [22] B. Medlock and T. Briscoe. Weakly supervised learning for hedge [23] Merriam-Webster Online Dictionary, http://www.m-w.com/ [24] R. Mihalcea and P. Tarau. A language independent algorithm for [25] Y. Ouyang, S. Li, W. Li. Developing learning strategies for topic-[26] B. Pang, L. Lee and S. Vaithyanathan. Thumbs up? sentiment [27] D. R. Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based [28] D. Radev, J. Otterbacher, A. Winkel, and S. Blair-Goldensohn. [29] J. Read and J. Carroll. Annotating expressions of appraisal in [30] E. Riloff, J. Wiebe and T. Wilson. Learning subjective nouns using [31] V. L. Rubin, E. D. Liddy and N. Kando. Certainty identification in [32] R. Saur X  and J. Pustejovsky. FactBank: a corpus annotated with event [33] F. Schilder and R. Kondadadi. FastSum: fast and accurate query-[34] C. Shen and T. Li. Multi-document summarization via the minimum [35] D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. Document [36] G. Szarvas. Hedge classification in biomedical texts with a weakly [37] H. Takamura and M. Okumura. Text summarization model based on [38] P. D. Turney. Thumbs up or thumbs down? semantic orientation [39] V. Vapnik. The Nature of Statistical Learning Theory . Springer, [40] V. Vincze, G. Szarvas, R. Fark as, G. M X ra and J. Csirik. The [41] X. Wan. Using only cross-document relationships for both generic [42] X. Wan and J. Xiao. Graph-Ba sed Multi-Modality Learning for [43] X. Wan and J. Yang. Multi-document summarization using cluster-[44] X. Wan, J. Yang and J. Xiao. Manifold-ranking based topic-focused [45] D. Wang, T. Li, S. Zhu, C. Ding . Multi-document summarization via [46] W. Weerkamp and M. de Rijke. Credibility improves topical blog [47] F. Wei, W. Li, Q. Lu and Y. He. Query-sensitive mutual [48] F. Wei, W. Li, Q. Lu, and Y. He . A document-sensitive graph model [49] J. Wiebe. Learning subjective adjectives from corpora. In AAAI/IAAI , [50] J. Wiebe, T. Wilson and C. Cardie. Annotating expressions of [51] K.-F. Wong, M. Wu and W. Li. Extractive summarization using 
