 With the ever-increasing number of spatio-textual objects, many applications require to find objects close to a given query point in spatial databases. In this paper, we study the problem of keyword-based k -nearest neighbor search in spatial databases, which, given a query point and a set of keywords, finds k -nearest neighbors of the query point that contain all query keywords. To efficiently answer such queries, we propose a new indexing framework by integrat-ing a spatial component and a textual component, which can efficiently prune search space in terms of both spatial information and textual descriptions. We develop effective index structures and pruning techniques to improve query performance. Experimental results show that our approach significantly outperforms state-of-the-art methods. H.2.8 [ Database Applications ]: Spatial databases Spatio-textual objects, k -nearest neighbors, Space Pruning
Keyword search in spatial databases has received signif-icant attention in recent years. This is mainly driven by the popularity of keyword search and the ever-increasing amount of available spatio-textual data such as Points of In-terest (POIs), geotagged documents and labeled placemarks. Several types of spatial keyword search have been studied, including keyword-based range query [5, 10], top-k spatial keyword search [4, 2, 1, 7], m -closest keywords query [9], and approximate string search in spatial data [8, 3]. In this pa-per, we focus on a fundamental and frequently encountered type of query in geographic, called keyword-based k -nearest neighbor (K 2 N 2 ) query, which, given an integer k ,aquery point and a set of keywords, finds the k -nearest objects to the query point that contain all query keywords.

K 2 N 2 queries have many real applications in our daily lives. Figure 1 shows an example for finding the top-2 n-earest  X  Chinese restaurants  X  to a given location .Inthis running example, objects o 3 and o 4 are top-2 answers. K queries are also very useful even critical during emergency operations, for instance finding the nearest shelter with med-ical facilities to a specific location . Due to the wide range of potential real applications, it calls for efficient methods to support K 2 N 2 queries.
 Figure 1: A K 2 N 2 query for finding the two nearest  X  Chinese restaurants  X  X oagivenlocation.

One straightforward solution is to extend existing incre-mental nearest neighbor search algorithms as follows. It first builds a spatial index structure such as R-tree and finds al-l the candidate nearest neighbors (a spatial-search phase). Then it eliminates the candidates that do not satisfy the textual constraints (a verification phase). We refer to this solution as k NN-first approach. Since this solution com-pletely ignores the possible pruning power from textual con-straints during the search phase, the I/O overhead caused by those unnecessary index accesses and the CPU overhead in the verification phase can be rather expensive. Another straightforward solution is based on text indexing methods such as inverted files and signature files. It first finds al-l the spatial objects which satisfy the textual constraints (a keyword-search phase). Then it sorts the objects based on their distances to the query point and returns the top-k results (a ranking phase). We refer to this solution as keyword-search-first approach. This solution may also suf-fer from high I/O and CPU cost, since it might have to retrieve large numbers of spatial objects and need to sort them in order to return the best k answers.

Apparently the above solutions are very inefficient. The big problem is that they answer queries in an independent two-phase manner, and they are unable to simultaneously do spatial pruning and textual pruning. A more efficient solution is proposed by Felipe et al. [4]. They devised index structures IR 2 -tree and MIR 2 -tree by combining R-tree with signature files. However, this solution has the following lim-itations. Firstly, it cannot achieve high performance for dif-ferent query parameters and data sets. For instance, if there is a small number of objects that contains all query keyword-s, this method performs worse than the keyword-search-first approach. The reason is that it must traverse the R-tree to retrieve the results. Compared with the keyword-search-first approach, the extra R-tree node accesses could be expen-sive. Secondly, signature files may generate large numbers of false positives , which will decrease the pruning power of signatures. Specifically, if the total number of distinct terms in the data set is larger or the length of signatures is shorter, there will be larger numbers of false positives, and thus this method has very low textual pruning ability. Thirdly, since signatures are stored in R-tree nodes, there exists a trade-off between the signature length and the fan-out of the R-tree. For large data sets, the overhead of storing signatures could out-weights the benefits from the textual pruning.
The K 2 N 2 problem is more challenging than the k NN query and keyword search, since it contains both spatial and textual constraints. Using an ad-hoc combination of exist-ing k NN and keyword-search techniques cannot maximize the potential pruning power, and it is hard to obtain the optimal performance for various query parameters and data sets. To address this problem, in this paper we propose a new indexing framework. The key idea is to minimize the search space by simultaneously exploiting both spatial and textual pruning ability. Our contributions are summarized as follows. (1) We propose a new indexing framework to efficiently answer K 2 N 2 queries. By integrating a spatial component and a textual component, the framework can simultaneously exploit both the spatial and textual prun-ing power, and minimize the search space. (2) We devise effective index structures in the framework and propose ef-ficient search algorithms to answer K 2 N 2 queries efficiently. (3) Experimental results show that our method significant-ly outperforms the state-of-the-art method by one or two orders of magnitude.
Consider a set of spatio-textual objects O .Eachobject o  X  X  is denoted by a pair ( o s ,o t ), where o s is a location with latitude and longitude, and o t is a set of terms. A keyword-based k -nearest neighbor (K 2 N 2 )query Q includes three components: the number of expected answers k ,query location q s , and a set of keywords q t . We find the top-k nearest objects to the query location q s that contain all query keywords. Next we formulate the K 2 N 2 problem.
Definition 1 (K 2 N 2 Problem). Consider a K 2 N 2 query q =( k, q s ,q t ) .Let  X  A denote the set of objects containing all keywords in q t 1 .TheK 2 N 2 problem finds a subset A X   X  A with k objects such that  X  o  X  X  ,  X  o  X   X  A X  X  : dist ( q, o ) &lt; dist ( q, o ) , where dist (  X  ,  X  ) is a metric space distance function (e.g., Euclidean distance).

For example, in Figure 1, for the top-2 query with key-words  X  Chinese restaurants  X , top-2 answers are o 3 and o
Without loss of generality, suppose that the size of  X  A smaller than k , i.e., |  X  A|  X  k .
Several types of queries taking account into both the s-patial information and textual description have been stud-ied [5, 10, 9, 8, 4, 2]. The most related studies to our prob-lem are [4, 7, 1]. Felipe et al. [4] proposed index structures by integrating signature files and R-tree and Cary et al. [1] combined R-trees and inverted files. Wu et al. [7] focused on processing multiple queries jointly. Another similar study is L k T query [2], which computed top-k answers by combining textual relevancy and spatial distance. The method com-bined inverted files and R-tree to answer the L k Tquery.
To answer K 2 N 2 queries, we propose a new indexing struc-ture BR-tree in Section 3.1. Then using BR-tree, we intro-duce two algorithms in Section 3.2 and Section 3.3. BR-tree Overview: We propose a new indexing structure with two components: a spatial component for spatial prun-ing, and a textual component for textual pruning, called BR-tree. As an example, in the BR-tree, we use the R-tree to maintain the spatial information of objects using mini-mum bounding rectangle (MBR) and use the B-tree to main the terms in the objects. For each term, BR-tree keeps a sorted list of MBRs which contain objects that include the term. We assign each MBR with an integer in pre-order traversal on the R-tree and MBRs are sorted in numerical order. Notice that in the MBR list, we use the pointers to point to the R-tree nodes, instead of storing the real MBRs.
For example, Figure 2(a) illustrates a sample data set and query point p based on the example in Figure 1. For sim-plicity, we only keep four distinct terms summarized in the vocabulary. Figure 2(b) and Figure 2(c) show the BR-tree. L -L 4 are the four inverted MBR lists. Each list contains a number of MBR identifiers. For instance, id M 1 is the i-dentifier of MBR M 1 , and points to its corresponding R-tree node (denoted by a dashed arrow). Note that we only show the pointers from MBR identifiers of L 1 to R-tree nodes in Figure 2, others are omitted for clarity.
 BR-tree Construction: The BR-tree can be built in two steps. Firstly, we can apply any existing bulk-loading al-gorithm to build an R-tree. At the same time, the inverted MBR lists for all distinct terms are generated. In particular, for each newly generated R-tree leaf node, the node MBR identifer is added to a set of inverted MBR lists. We also maintain a term list for every R-tree node. For a non-leaf node, we obtain the distinct terms in the subtree by calcu-lating the union of term lists belonging to the child nodes, then add the node MBR identifer to the inverted MBR list-s with respect to these terms. We repeat these steps in a bottom-up manner until the R-tree has been built. Second-ly, we construct the textual component B-tree on all the distinct terms. On the leaf node, for each term, we keep its corresponding inverted MBR lists.
 Advantages of BR-tree: BR-tree has the following salient features. Firstly, we can do both spatial and textual prun-ing using the BR-tree, which can significantly improve search performance. Secondly, as the widespread implementation of B-tree and R-tree, our indexing framework can be easi-ly integrated into existing database systems. It provides a loose coupling between the spatial component and the tex-tual component and avoids increasing the complexity of in-dex structures. Thirdly, as each inverted MBR list is stored contiguously, the retrieval of inverted MBR lists is highly ef-ficient. The benefit from the search-space pruning based on inverted MBR lists significantly out-weights the overhead, and the query performance can be greatly improved.
Recall the keyword-search-first approach, which first find-sallthe candidate objects satisfying the textual constraints (the keyword-search phase) using inverted index or signature files and then sorts the objects according to their distances to the query point and returns the top-k results (the ranking phase). This method may have to retrieve large numbers of spatial objects and cannot utilize spatial pruning. To ad-dress this problem, we propose a new algorithm, called kbr which extends keyword-search-first approach by providing effective spatial pruning using the BR-tree.

The main idea of kbr is that we can group the objects based on spatial information, and prune a group if it does not contain all keywords using textual pruning. For each group containing all keywords, if its distance to the query point is larger than a threshold, we can prune all objects in the group using the spatial pruning. Thus we can do both spatial pruning and textual pruning in a combined way.
To achieve our goal, we use each leaf-level MBR as a group. To do the textual pruning, for each query keyword, we first retrieve its leaf-level MBRs. Then we intersect the MRBs for all query keywords. Each MBR in the intersection is a candidate region , and all objects that are not in any can-didate region can be pruned. In order to efficiently retrieve the leaf-level MBR identifers, we store offsets in B-tree n-odes to indicate where the leaf-level MBR identifers start in each inverted MBR list. For example, as shown in Fig-ure 2(c), using the offset stored in the B-tree node, we can easily know that the leaf-level MBR identifers in L 1 begin from id M 4 (denoted by a dot-dashed arrow). Then all the leaf-level MBR identifers in L 1 can be efficiently retrieved, i.e., L 1 = { id M 4 ,id M 8 } .

Next, using the candidate regions, we do not need to enu-merate all candidate objects and compute the distances of all candidate objects to the query point. Instead, for each candidate region, we compute its minimum distance to the query point, and use it as an estimation of the distances of objects in the candidate region to the query point. Then we add it to a priority queue sorted by their distances to the query point in an ascending order. Next for each elemen-t in the priority queue, we pop the one with the minimal distance. If the element is a candidate region, we push the objects in the candidate region into the priority queue; oth-erwise, it must be an object, and if the object contains all keywords, it is an answer. We repeat the above steps until we find k objects.
Recall the kNN-first approach, which first finds all the candidate nearest neighbors using the R-tree (the spatial-search phase) and then verifies the candidates to check whether they satisfy the textual constraints (the verification phase). This solution completely ignores the possible pruning pow-er from textual constraints during the spatial-search phase. To address this problem, we propose a new algorithm, called sbr , which uses BR-tree to do textual pruning.

The basic idea of sbr is that for an MBR, we use the inverted MBR lists to check whether the MBR contains all keywords. If no, we can prune the MRB and all of its de-scendant MBRs. To achieve our goal, given a query, sbr gradually expands the search region centering at the query point as follows. We use a priority queue, which sorts el-ements by their minimum distances to the query point in ascending order, to maintain the search region. Firstly, we push the root into the queue. Then we pop each element from the queue. If the element is an MBR r , we find its child MBRs that contain all query keywords. To this end, we use the inverted MBR lists. A naive method is to com-pute intersection of inverted MBR lists for query keywords. However this method is rather expensive as there may be large number of MBRs in these lists. Instead we only need to compute the intersection of MBRs in the lists which are child MBRs of r . To improve the performance, as shown in Figure 3, for each non-leaf MBR in the inverted MBR lists, we use two pointers to keep its first child MBR (denoted by a solid arrow) and the last child MBR (denoted by a dashed arrow). As the MBRs are sorted, we can easily get all MBRs in a node based on the first MBR and last MBR. Thus given a set of keywords and MBR r , we first retrieve r  X  X  child M-BRs for each keyword based on the inverted MBR lists (and the two pointers), and then compute the intersection.
Take keywords X  Chinese  X  X nd X  Restaurant  X  X ndMBR M 2 as an example. As shown in Figure 3, using the two pointers of id M 2 in L 1 , we can easily retrieve the child MBR iden-tifiers of M 2 for keyword  X  Chinese  X , i.e., { id M 4 } . Similarly we retrieve M 2  X  X  child MBR identifiers in L 3 for keyword  X 
Restaurant  X , i.e., { id M 4 ,id M 6 } . By intersecting these t-wo lists of child MBR identifiers, we know that in all of M 2  X  X  child MBRs, only M 4 contains keywords X  Chinese  X  X nd  X 
Restaurant  X . For each of such child MBRs, we compute its minimum distance to the query point, and put them into the priority queue. If the element popped from the queue is an object and it contains all keywords, then it is an answer. We repeat the above steps until we find k objects.
In this section, we propose a concept term MBR (TMBR) and incorporate it into the BR-tree to improve performance.
Definition 2 (Term MBR). Given an MBR M and a term t ,thetermMBR M M,t is the minimum bounding rect-angle that completely encloses the set of objects containing term t , i.e., O = { o | o ( o s ,o t )  X  M, t  X  o t } .
For instance, for MBR M 3 and term t 3 , the corresponding term MBR M M 3 ,t 3 is shown in Figure 4, which is the MBR of all objects in M 3 that contain term t 3 .

We can generate term MBRs as follows. For a leaf node, its term MBR M M,t is the minimum bounding rectangle of MBRs of every object o i in M . For a non-leaf node, its term MBR M M,t is the minimum bounding rectangle of its child term MBR. In this way, we replace MBR identifiers in inverted MBR lists of BR-tree with term MBRs, and devise an enhanced index structure called TBR-tree. Note that given a region, we need to locate objects in the region, and thus we still keep leaf-level MBR identifers in TBR-tree. Figure 5 gives the inverted MBR lists in TBR-tree of dataset in Figure 2(a). Similar to traditional MBRs, a term MBR canbestoredbymaintainingthelowsideandhighside along each dimension, hence it is not expensive to keep all term MBRs in inverted MBR lists.

Using term MBRs, we propose two optimizations. The first one is term MBR pruning .AsthetermMBRcap-tures how a term is distributed in an MBR, we could reduce search space by checking the term MBRs. If there is no in-tersection of the term MBRs, the node can be pruned. For example in Figure 4, since M M 3 ,t 1 and M M 3 ,t 3 have no in-tersection, there is no need to examine M 3 . This pruning mechanism is more powerful than the MBR pruning. The other optimization is tighter minimal distance bound , which uses the intersection of the term MBRs, instead of the node MBR, to calculate the distance. For instance, in Figure 4, the intersection of M M 3 ,t 2 and M M 3 ,t 3 is the object o 11 . We can compute the distance according to the location of o 11 rather than M 3 . As term MBRs give a more accurate estimation of the minimal distance to the query point, this optimization can enhance the spatial pruning power.
Using TBR-tree, we propose a new search algorithm to improve kbr , called ktbr .Differentfrom kbr , ktbr finds candidate regions by intersecting leaf-level term MBRs, and eliminates those regions whose intersected term MBRs are empty. In addition, the distances to the query point are es-timated according to the intersected term MBRs, which are more accurate than those computed using MBRs. Moreover, only the objects enclosed in intersected term MBRs are pos-sible results, and others can be pruned. In order to check whether an object is contained in intersected term MBRs, we also keep intersected term MBRs in the priority queue.
Using TBR-tree, we propose a new search algorithm to improve sbr , called stbr . Similar to sbr , stbr gradually expands search region by using the priority queue. When visiting a non-leaf node, stbr computes the intersection of term MBRs corresponding to its child nodes. The child n-odes, whose intersected term MBRs are empty, are pruned, as well as their descendant nodes. For a leaf R-tree node, only the objects in the intersected term MBR will be kept. Moreover, stbr uses intersected term MBRs to estimate the distances to the query point, instead of using node MBRs.
We compared with MIR 2 -tree [4]. As MIR 2 -tree generally achieves much higher performance than IR 2 -tree, which was also reported in [6], we do not report results for IR 2 -tree. All the algorithms were implemented in C++. All the ex-periments were performed on a computer with an Intel(R) Core(TM)2 Duo P8600 @ 2 . 40GHz CPU and 2GB RAM.
 Data sets and queries. Weusedtworealdatasetsin our experiments, the California (CA) data set and the Vir-ginia (VA) data set, obtained from the CloudMade project 2 Each spatial object in the two data sets contains the longi-tude and latitude coordinates and several textual attributes. We summarize the properties of data sets in Table 1. We also generated several query sets in our experiments. Every http://cloudmade.com/ query set comprised of 1 , 000 randomly queries. We report the average respond time in each query set.
We compare the query performance of our proposed meth-ods ( sbr , kbr , stbr and ktbr ) with the straightforward solutions, i.e., k NN-first ( k NNF) and keyword-search-first (KSF) methods. Figure 6 shows the results.
We see that the k NN-first method performs much worse than other methods. In particular, when increasing the val-ue of k , its performance degrades rapidly. This is because for larger k values, the k NN-first method needs to access more R-tree nodes to obtain the candidate objects and then verify them. For the keyword-search-first method, since it com-putes all results at once, its performance is independent of k . In general, our methods outperform the straightforward solutions significantly. The reason is that the k NN-first com-pletely ignores the textual pruning power during its spatial-search phase, and the keyword-search-first method cannot utilize spatial pruning during its keyword-search phase. In-stead our methods exploit both the spatial and textual prun-ing ability to reduce search space. We also notice that stbr and ktbr achieve much higher performance than sbr and kbr . The reason is that stbr and ktbr employ term MBRs to obtain more powerful pruning ability and more accurate estimation on the minimal distance to the query point.
We compare our methods BR-tree and TBR-tree (using hybrid ) with state-of-the-art method MIR 2 -tree [4]. Varying the value of k . We compare the algorithms by varying the value of k . Figure 7 shows the results. In gen-eral, BR-tree and TBR-tree significantly outperform MIR 2 tree for all values of k . And this performance gap enlarges when increasing the value of k . NotethatTBR-treeper-forms much better than BR-tree. This is expected since TBR-tree is integrated with term MBR pruning technique, thus it can greatly reduce the search space. Varying the number of keywords. In this set of experi-ments, we use k = 10 as the default parameter. In Figure 8, we can see that BR-tree and TBR-tree achieve much high-er performance than MIR 2 -tree. We also notice that when increasing the number of query keywords, the query perfor-mance of MIR 2 -tree is improved. This is because queries with more keywords usually produce less false positives dur-ing signature matching. Similar observations can be made in other index structures. For e xample, in Figure 8(a), the query performance increases when varying the number of keywords from 3 to 4. The reason is that queries with more keywords generally have higher selectivity, and this favor-able property enhances the pruning ability. As expected, due to the optimizations based on term MBRs, TBR-tree is more efficient than BR-tree. Figure 8: Comparison by varying keyword numbers ( k =10 ).
In this paper we have proposed a new indexing framework for the efficient processing of K 2 N 2 queries, which integrated a spatial component and a textual component to simultane-ously prune search space according to both the spatial infor-mation and textual description. We developed four search algorithms for answering K 2 N 2 queries. We have conducted an extensive set of experiments to evaluate the performance. The results on real datasets experimentally proved the prac-ticality and efficiency of our proposed approaches. Acknowledgement. This work was partly supported by the National Natural Science Foundation of China under Grant No. 61003004, National Gr and Fundament al Research 973 Program of Chin a under Grant No . 2011CB302206, and a Tsinghua project unde r Grant No. 20111081073, an d the  X  X ExT Research Center X  funded by MDA, Singapore, under Grant No. WBS:R-252-300-001-490.
