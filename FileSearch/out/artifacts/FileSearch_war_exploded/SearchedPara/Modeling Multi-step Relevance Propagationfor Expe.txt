 An expert finding system allows a user to type a simple text query and retrieve names and contact information of indi-viduals that possess the expertise expressed in the query. This paper proposes a novel approach to expert finding in large enterprises or intranets by modeling candidate experts (persons), organizational documents and various relations among them with so-called expertise graphs .Asdistinct from the state-of-the-art approaches estimating personal ex-pertise through one-step propagation of relevance probabil-ity from documents to the related candidates, our methods are based on the principle of multi-step relevance propaga-tion in topic-specific expertise graphs. We model the pro-cess of expert finding by probabilistic random walks of three kinds: finite, infinite and absorbing. Experiments on TREC Enterprise Track data originating from two large organi-zations show that our methods using multi-step relevance propagation improve over the baseline one-step propagation based method in almost all cases.
 H.3 [ Information Storage and Retrieval ]: ;; H.3.3 [ In-formation Search and Retrieval ]: ; Algorithms, Measurement, Performance, Experimentation Enterprise search, expertise, expert finding, language mod-els, random walks, Markov processes
In large organizations or in the scope of the global web users often search for persons rather than for relevant doc-uments. The demanded information may be just not pub-lished: not considered important to be released, not avail-able in electronic format, or just hardly expressible in writ-ten language. In these cases asking people becomes the only way to find an answer [15]. Besides being sources of unpub-lished knowledge, the experts on the search topic are often able to explain it in detail and even guide the user further to related areas. Experts can be in demand not only for asking them questions, but also for assigning them to some role or a job. Conference organizers may search for reviewers, re-cruiters for talented employees, even consultants for other consultants to redirect inquiries and not lose clients [25].
The need in finding a well-informed person may be crit-ical for any kind of project. However, any attempts to identify experts by manual browsing through organizational documents or via informal social connections may fail in large enterprises, especially when they are geographically distributed. A standard text search engine may be of great help, but still is not able to automate this task. Usually, a specialized expert finding system is developed to assist in the search for individuals or departments that possess cer-tain knowledge and skills within the enterprise and outside [35]. It allows either to save time and money on hiring a con-sultant when company X  X  own human resources are sufficient, or helps to find an expert at affordable cost and convenient location in another organization. Similarly to a text search engine, an automatic expert finder uses a short user query as an input and returns a list of persons sorted by their level of knowledge on the query topic.

It is common to consider that the more often a person is related to the documents containing many words describing the topic, the more likely we may rely on this person as on an expert. The proof of the relation between a person and a document can be an authorship (e.g. we may consider external publications, descriptions of personal projects, sent emails or answers in message boards), or just the occurrence of personal identifiers (names, email addresses etc.) in the text of a document. Thus, the most successful approaches to expert finding obtain their estimator of personal expertise by summing the relevance scores of documents directly related to a person [5, 34]. Alternative approaches find experts by measuring network centrality of persons in specialized pro-fessional communities [51].

Both aggregated relevance and centrality based expert finding methods still ignore some properties of the data. Methods using aggregated relevance do not reflect relations between experts and do not consider documents that are indirectly related to persons. Whereas the centrality based methods simply model documents as unweighted links be-tween candidates, neglecting their relevance to the query.
Our contributions . In this paper, we advance previous work by combining features of both above-mentioned ap-proaches. We demonstrate that it is beneficial to continue the propagation of document relevance after the first step of its aggregation on the level of directly related persons. Following the principles of spreading activation algorithms [17], we allow the probability of document relevance to flow further through reciprocal connections between persons and documents. Specifically, our contributions are:
The remainder of this paper is organized as follows. The related research on expert finding and link-based analysis is described in detail in the next section. In Section 3 we ex-plainhowthedynamicsofanexpertisedomaincanbemod-eled with graphs, as well as motivate and propose expert finding methods built upon random walks in these graphs. Our experiments with two real-world test collections sup-porting our assumptions are discussed in Section 4. Finally, Section 5 concludes the presented work and outlines the di-rections of future research.
Expert finding started to gain its popularity at the end of  X 90s, when Microsoft, Hewlett-Packard and NASA made their experiences in building such systems public [18, 19, 8]. They were not fully automated and represented reposito-ries of manually created skill descriptions of their employees with simple search functionality. Nowadays these and other companies invest a lot into making their expert search en-gines commercially available and attractive [1, 2, 20]. How-ever, apart from causing the new boom on the growing en-terprise search systems market, expert finding systems also compelled close attention of the IR research community [22]. The expert search task is a part of the Enterprise track of the Text REtrieval Conference (TREC) since its first run in 2005 [14]. The TREC community created experimental data sets consisted of organizational document collections, lists of candidate experts and the set of search topics, each with a list of actual experts.

First, pioneering approaches to expert finding could be classified as profile-centric [15, 32]. This technology was the first step in full automation of expert finding in organizations and generally aimed to avoid manual maintenance of per-sonal profiles. In these approaches, all documents related to a candidate expert are merged into a single personal profile prior to the actual retrieval process. The personal profiles are ranked like documents w.r.t a query using standard re-trieval measures and corresponding best candidate experts are returned to the user. There is also a special class of effective profile-centric methods representing the relevance model as a mixture of personal language models and then measuring the probability that a person would generate the query given the probabilities to generate relevant terms in top documents [43, 42].

The follow-up document-centric approaches analyze the content of each document separately [11, 5, 34, 41]. All of them basically utilize a simple probabilistic model assuming that the probability of expertness of a person is a sum of relevance probabilities of all related documents (see Section 3.2 for details). Some recent works attempt to avoid prop-agation of the relevance of those documents or their parts that are not related strongly enough to the candidate ex-pert. In one approach, only the score of the text window of a fixed size surrounding the person X  X  mention is considered [33]. In the other approaches, either the overall pairwise dis-tance between a candidate X  X  mention and the query terms [39] or their order of occurrence in a document [44] are used to measure the degree of association between the document and the candidate.

All the above-mentioned approaches share the principle claiming that the relevance of the textual context of a person adds up to the evidence of his/her expertness. Furthermore, each next class of approaches appears to be more effective than the previous one [6, 39], seemingly due to the esti-mation of relevance of the textual content related to a per-son on the lower and hence less ambiguous level. However, in their definition of personal context, these approaches re-strict themselves to the scope of documents directly related to a person. They ignore the complexity of link structure among persons and documents and hence do not consider the expertness of directly and indirectly linked persons as well as the relevance of documents found not in the imme-diate neighborhood of a candidate.

In this connection, it is important to mention another line of research that proposed finding experts by calculating their centrality in the organizational social network [10, 27, 51]. However, these approaches ignore the relevance of doc-uments that serve just as the evidence of relation between persons. So, while being effective for query-independent tasks like finding the most authoritative experts in ques-tion/answering portals and forums [4], they are inferior in performance to the state-of-the-art query-dependent expert finding methods [12]. Random walk based models regularly appear in different IR research areas, but first of all known from web retrieval. Among them, Pagerank [38], HITS [28] (its random walk d d d (a) Tagged text fragments (b) Corresponding graph based version is described by Ng et. al. [37]) and SALSA [31] are probably the most popular. Several attempts have been made in the last years to make these models query and content dependent. The Intelligent Surfer [40] walks to linked pages biased by their relevance to the query. Person-alized Pagerank allows to put preferences on certain web-pages, so that the centrality of any document would depend on its proximity to preferred ones [38, 26]. Both ideas were further combined in a unified framework which considered also the bi-directional walk over hyperlinks [46]. Random walks on graphs containing queries and clicked links (or en-tire search trails) were recently utilized for web search result expansion [16, 9]. Searching with graph-based methods for typed entity classes on the Web was explored recently in several publications [50, 49].

There are applications of random walks beyond the bounds of hyperlinked corpora. Pagerank in graphs of terms, doc-uments and document clusters was adapted for ad-hoc text retrieval [30, 29]. Finite random walk over terms through thesaural and syntactic relations is applied to query expan-sion [48, 13] and question answering [21] tasks. It also used as a model of preference flow between users in a recommen-dation system [47].

The presented work, to our knowledge, is the first exten-sive study of relevance propagation with random walks on query-dependent graphs in the field of expert finding.
This section proposes and discusses the modeling of appro-priate graphs that represent the association between experts and documents in a certain domain of expertise. Suppose we have a set of documents associated with scores as the result of an initial standard document retrieval on the given topic. From the ranked documents, a second set of contained can-didate experts is extracted. Their containment relations can be represented in an expertise graph , where both documents and candidate experts become vertices and directed edges symbolize the containment conditions (see Figure 1). The simplest form of expertise graphs is always bipartite, since all edges point from documents to experts only and back. Figure 2 shows a typical expertise graph computed for one of the TREC queries. Let us recall that we are interested in propagation of relevance through the graph network. So, it is further important to exploit all known connections be-tween the entities of the graph.

Including Further Links . In many situations not only containment relations, but also links between documents Figure 2: A fragment of the real expertise graph with links between documents (white nodes) and candidate experts (black nodes) for the query  X  X us-tainable ecosystems X  or organizational connections between possible experts are known. Whereas inter-document links are represented by directed edges following the link, expert-to-expert edges are usually bidirectional. By including such additional edges, the graph gains a higher density and enables more intensive relevance propagation, however by losing its strict bipartite property.

Including Further Entity Types . Experts and documents do not need to be the only entities in the expertise graph. Although the expert finding task is only interested in the ranking of experts, it might still be useful for the relevance propagation to exploit additional connections via nodes of other types, such as dates, locations or events. Persons out-side the company might reveal interesting connections as well, if they are mentioned in the documents together with the candidate experts. We may also incorporate relations and entities extracted from other external global professional networks (e.g. LinkedIn.com ).

Controlling Graph Size and Topical Focus. Apart from the graph modeling itself, the most influential parameter on the graph size and density is the number of retrieved docu-ments taken into account while building the graph. Notice that only the restriction to the top ranked documents makes the expertise graph model query dependent. We can include more lower ranked documents to increase the graph X  X  den-sity, but with the drawback of losing its topical focus.
One of the most theoretically sound and effective repre-sentatives of document-centric expert finding methods (see Section 2), proposed by Balog et al. [5, 6], follows the prob-abilistic language modeling principle of IR [23] and defines the probability of expertness for the candidate expert e with respect to the query Q as: where P ( R | D ) is the probability that the document D is relevant and P ( Q | D ) is the probability of the document D to generate the query Q . The latter probability is the mea-sure of document relevance R accordingtoLM-basedIR [23]. P ( e | D ) is the probability of association between the candidate and the document. Top is the set of documents retrieved and the prior probability P ( D ) is distributed uni-formly over the Top .

If we look at Equations 1 and 2 we may notice that they correspond to a probabilistic process, in which a user se-lects a document among the ones appearing in the initial ranking, looks through the document, enlists all candidate experts mentioned in it and refers with the current infor-mation need to one of them. The probability of selecting a document is its probabilistic relevance score since the user will most probably search for useful information and con-tacts of knowledgeable people in one of the top documents recommended by a search engine. The following selection of a candidate expert depends on the level of its responsibility to the content of the document: e.g. its author will most probably be selected first, but a person mentioned in the acknowledgments will be less likely considered useful. The described process can be interpreted as one-step relevance probability propagation from documents to related candidate experts.

We use the method described by Equation 1 as our base-line. The probability of the query to be generated by the document language model [23] is calculated as: where tf ( q, D ) is the term frequency of q in the document D , | D | is the document length and  X  G is a Jelinek-Mercer smoothing parameter -the probability of a term to be gen-erated from the global language model calculated over the entire collection C . We set it to 0.8 which is the optimal value according to our preliminary experiments.

Here and further on we also use the probabilities of se-lecting a document given a person and of selecting a person given a document: where a ( e, D ) is the non-normalized association score be-tween the candidate e and the document D proportional to their strength of relation. Our way of distributing these scores over candidate experts in a document is described in the experimental part of the paper (see Section 4.1).
If we want to automatically point the user to the most knowledgeable people on the topic, we should imagine how they could be found instead during manual search. The one-step probabilistic process is not quite a realistic model in this case. It is not likely that reading only one document and consulting only one person is enough to completely satisfy a personal information need in the enterprise. The real-world user should realize that the expertise needed is partly contained in several retrieved documents and partly in the personal memory of several experts.

We may imagine that the search for expertise may con-sist of the following repeating stages of gradual knowledge acquisition: 1. At any time: (a) randomly reading a document, or just 2. After reading a document: (a) consulting with a person 3. After consulting with a person: (a) reading other doc-
Note that while modeling expertise gathering process, we apply different techniques to concentrate the random walk around the most relevant documents, since we rely on the assumption that all sources of the same knowledge are lo-cated close to each other in expertise graphs. In our meth-ods described further in this section we try to overcome the limitations of the baseline one-step relevance propagation. We model the expert finding as a K -step, an infinite or an absorbing process of consulting with documents and peo-ple. First we present three mod els considering that exper-tise graphs are bipartite (graphs used for the infinite ran-dom walk are not strictly bipartite due to the probability of a jump to any node), and then we suggest the model taking links among same-type entities into account.
In this approach, we consider that the user makes some predefined number of steps in his/her search for expertise. Since the user walks over a bipartite expertise graph with layers of document and candidate expert nodes, this walk becomes a process of moving to a node from an opposite layer at each step, starting from some node in a document layer.

In order to emphasize the importance of a candidate to be in close proximity to relevant documents, we utilize the probabilities of their relevance in two ways: (1) the proba-bility of selection of the first document as a starting point for the walk is proportional to its probability of relevance, (2) the probability to stay at a document node at any step is also proportional to its probability of relevance. Actually, the non-zero self-transition probability is important for finite random walks, since it allows to diffuse the initial probabil-ity more slowly, smoothly and hence makes the algorithm less sensitive to the setting of number of steps.
Since we consider this walk as finite, we believe that at some point a user is tired/satisfied with some candidate and stops the search process. So, we iteratively calculate the probability that a random surfer will end up with a certain candidate after K steps of a walk started at one of the ini-tially ranked documents:
The probabilities P ( e | D ), P ( D | e )and P ( R | D )usedin above equations are defined in Equations 2 and 5. Finally, we consider that P ( Expert | e )  X  P K ( e ).

It is also possible to estimate the candidate X  X  expertise using several finite walks of different lengths at once. For instance, it can be calculated as a weighted sum of probabil-ities P 1 ( e ) ...P K ( e ). We could also smooth the current node probabilities with probabilities to appear in the same nodes in the past and future. However, all such approaches would significantly increase the size of our parameter set due to introduction of weight coefficients. So, despite our method can be easily utilized in this way, we experiment only with unsmoothed probabilities.
In our second approach, we assume that the walk in search for expertise is a non-stop process. We may imagine that the user visits document and candidate nodes over and over again making a countless number of steps. By analyzing the statistics of this discrete Markov process we may con-clude that persons visited more often during this infinite walk were more beneficial for the user. However, its station-ary distribution does not depend on the initial probability distribution over states. In order to retain the importance for a candidate to stay in proximity to relevant documents and also to assure the existence of a stationary distribution, we introduce jump transitions to the nodes of a graph.
At first, we introduce the possibility to return regularly to the document nodes from any node of the expertise graph and to start the walk through mutual documents-candidates links again. We consider that the probability of jumping to the specific document P J ( D ) equals its probability to be relevant to the query. This assumption makes candidate ex-perts which are situated closer to relevant documents visited more often in total during consecutive walk steps. We also add a probability to jump to candidates P J ( e ). We consider that the more often the candidate appears in top documents, the more likely that it is known to the user sooner or later and hence can be selected for a random jump. So, we make it equal to the probability to find the candi-date in a randomly selected document from the retrieved top. However, it can have other origins and may be propor-tional to the candidate X  X  popularity/authority in the whole organization or inversely proportional to his/her occupancy level.

The following equations are used for iterations until con-vergence: where  X  is the probability that at any step the user decides to make a jump and not to follow outgoing links anymore, cf ( e, T op ) is the number of top documents where the can-didate e appears, | Top | is the size of a result set. The de-scribed Markov process is aperiodic and irreducible (due to introduced jump probabilities), and hence has a stationary distribution. Consequently, we consider that P ( Expert | proportional to the stationary probability P  X  ( e ). Although our method is computationally intensive, it converges very fast (after 200-300 iterations) for typical expertise graphs containing at most 2000 nodes according to our experiments.
In our next approach we represent the search for an expert as an absorbing random walk in a document-candidate graph [45]. We calculate the probability of finding a candidate if consider that this candidate is the required expert. The can-didate node which we want to evaluate is only self-transient, since we assume it to be the final destination of the walk. This means that in contrast to the one-step approach, we calculate the probability of finding a certain candidate ex-pert by making any sufficient number of steps in the graph. Formally speaking, we remove all outgoing edges from the measured candidate, add the self-transition edge to it and use the following equations iteratively:
Finally, we consider that P ( Expert | e ) is proportional to the probability P  X  ( e ). It should be mentioned that in strong-ly connected graphs with an absorbing node the probability of absorption in the infinity is effectively close to 1. How-ever, the graphs we deal with are nearly uncoupled and, de facto, the probability of absorption for a certain node de-pends only on the connectivity of the region it belongs to and on the total probability of relevance of closely connected document nodes.

Making the full run of iterations for each candidate is unnecessary. If we rewrite the above equations in a matrix form, we get p = p 0 A i ,where p 0 is a vector of starting probabilities, the matrix A consists of one-step transition probabilities and A i contains probabilities of transitioning from one node to another in i steps. In our calculations we use matrix B containing probabilities of transitioning from each node to another in the minimum number of steps. We get this matrix by filling it with those elements from A i which become non-zero after some next iteration. When no new element in A i becomes non-zero after some iteration, the filling of B is finished. The vector of probabilities p used for candidate ranking is calculated as p = p 0 B .
The absorbing random walk based method has several the-oretical advantages over the previously presented methods. First, it can be regarded as a generalization of the baseline method described by Equation 1, considering that we regard P ( e | D ) as the probability to transfer from the document D to the candidate e not in one step, but in the minimum suf-ficient number of steps . Second, it does not need a training phase since it is parameter-free.
Usually, for graph-based algorithms, the introduction of new information into the analysis often comes to discovering new links among analyzed entities. The scenario of search-ing for expertise in the enterprise may include not only mov-ing from relevant documents to the candidate experts found in them and vice versa, but also along document-document and candidate-candidate connections. We may find it natu-ral that a user goes over the ranked documents by following hyperlinks. The discovery of new experts may be possible not through documents only, but also with the help of can-didate experts the user is in contact with already. For exam-ple, they can send the user to their colleagues in the same department who expectedly possess similar expertise. This  X  X scalation phase X  of expertise seeking, when people end up with experts not initially recommended by a system, but related to those, even crossing organizational boundaries, is common in enterprises according to recent user studies [3].
We experimented with adding these new transitions to our expertise graph and using them for the Infinite Random Walk method. The iterations specified in Equations 9 and 10 are updated in the following way:
P i ( D )=  X P J ( D )+(1  X   X  )((1  X   X  D ) P i ( e )=  X P J ( e )+(1  X   X  )((1  X   X  e ) where  X  D is the probability of following document-document connections,  X  e is the probability of following candidate-candidate connections. The new transition probabilities are calculated as: where N D is the number of outgoing document links from the document D and N e is the number of outgoing can-didate links from the candidate e . It is, of course, reason-able to differentiate the strength of relation and directions of influence among co-workers or even include entire organi-zational hierarchy into the graphical model, but we do not have this information in our data sets.
We conduct our experiments with two data sets provided by the TREC community. Although both testbeds allow to realistically simulate classic expert finding scenarios, they have some clear distinctions.

W3C data, TREC 2005, 2006. This collection rep-resents the internal documentation of the World Wide Web Consortium (W3C) and was crawled from the public W3C (*.w3.org) sites in June 2004. The data consists of several sub-collections: web pages, source code, mailing lists etc. In our experiments we use the largest (1.85 GB, 198 000 doc-uments), the most clean and structured part of the corpus, containing email discussions within the W3C. This part is rather homogeneous in format (each document is an email of average length 450 words) and hence its features would not significantly vary over different enterprises. It also al-lows the accurate detection of candidate experts in docu-ments just using their unique email addresses. The W3C data is supplemented with the list of 1092 candidate experts represented by their full names and email addresses. We experiment with 49 queries and respective relevance (exper-tise) judgments used for TREC evaluations in 2006, which are more reliable comparing to the queries used for the pilot TREC evaluations in 2005, when candidate experts were not judged manually.

CSIRO data, TREC 2007. The data used in TREC 2007 is a crawl fr om publicly available pages of another or-ganization -Australia X  X  national science agency CSIRO. It includes about 370 000 web documents (4 GB) of various types: personal home pages, announcements of books and presentations, press releases, publications. Instead of a list of candidate experts, only the structure of candidates X  email addresses was provided: firstname.lastname@csiro.au .Us-ing this as a pattern we built our own candidates list by finding about 3500 candi dates in the collection. 50 queries with judgments made by retired CSIRO employees were used for the evaluation.

At the collection preparation stage, we extract associa-tions between candidate experts and documents. For both data sets we use simple recognition by searching for can-didates email addresses and full names in the text of doc-uments. For the CSIRO documents the association scores a ( e, D ) between documents and found candidates are set uniformly to 1.0. In the case of W3C data, we may differ-entiate the type of a candidate-document relation, by look-ing at the email field where the candidate was detected: from , to , cc or body . We use the following association scores: a ( e, D from )=1 . 5, a ( e, D to )=1 . 0, a ( e, D cc )=2 . 5and a ( e, D body )=1 . 0 respectively, which is the most realistic combination according to recent studies of W3C  X  X ists X  sub-collection [7]. For the initial documents ranking as well as for the graph generation the open-source PF/Tijah retrieval system [24] was employed.

The results analysis is based on calculating popular IR performance measures also used in official TREC evalua-tions: Mean Average Precision (MAP), precision at top 5 ranked candidate experts (P@5) and Mean Reciprocal Rank (MRR). MAP shows the overall ability of a system to distin-guish between experts and non-experts. P@5 is considered more significant than precisions at lower ranks since the cost ofanincorrectexpertdetectionisveryhighinanenterprise: the contact with a wrong person may require a mass of time. If we consider that the user can be satisfied with only one expert on the topic (considering that all experts are always available for requests), then the performance of MRR mea-sure becomes crucial.

In our experiments discussed below we compare our meth-ods with a baseline to study the effectiveness of the multi-step relevance propagation approach. However, for the sake of a fair comparison, we also show the performance of the simplest of known methods, called Votes in [34], which ranks candidates just by the number of top documents where they appear.

The evaluation of the following methods is discussed fur-ther:
The first step in any document-based expert finding algo-rithm is a document retrieval run extracting relevant docu-ments for analysis. Both datasets where indexed with the use of Snowball stemmer and standard English stopwords were removed. In our experiments we use the language model based approach to IR for scoring documents (see Equations 3, 4) and retrieve a predefined number of top ranked documents, which we consider sufficient to cover the topic of a query. We retrieved only documents with at least one mention of a candidate. The optimal number of re-trieved documents varied considerably over the data sets. In our preliminary experiments we had to retrieve 1500 doc-uments from the W3C collection and just 50 documents from the CSIRO collection for the maximum performance of the Baseline method. We believe that this difference is caused by two reasons. The average number of experts per query is very small for the CSIRO collection -3, whereas it is 60 for the W3C collection. Since only very authoritative persons were considered experts in the CSIRO, they mostly appear in the top relevant documents on a topic. Moreover, the number of candidate experts is three times higher for the CSIRO data. This means that the number of persons com-peting with each other increases with each next retrieved document faster, what makes the task of finding experts among them harder.

In order to achieve a denser document-candidate graph we experimented not only with persons from the candidates list, but also with other persons found in the collection con-sidering each found email address as an identifier of an indi-vidual. The additional person entities increased the graph-sizes by far, since also documents containing a person but no candidate experts were included into the graph network as well. This graph expansion allowed us to use the non-candidate persons that are not selected for the final ranking as mediators for the relevance transmission from candidate to candidate.
Both the FRW and the IRW methods depend on one pa-rameter. In case of the FRW method this is the number of relevance propagation steps K to be done. Figures 3 and 4 compare the MAP performance of the Baseline method with the performance of the FRW method after from 1 to 43 propagation steps for both data sets.

We see that the maximum MAP is reached after making in average 13 steps for the W3C data and 33 steps for the CSIRO data. This is not very long walk in our graph, so the relevance will not be propagated too far. This is partly true because we have a probability to stay at document nodes, but also because a typical expertise graph (see Figure 2) is nearly uncoupled and the limited scope of its (almost) disjointsubsetsmakesasurfertodoalotofstepstogoout of the bounds of a subset. However, the most important observation is that for both collections the increasing of the number of propagation steps from one to more also leads to MAP increase for both datasets, making this noticeable already after a few steps. Figure 3: MAP for the Finite Random Walk method (FRW) with different numbers of propagations steps taken, W3C (2006) data Figure 4: MAP for the Finite Random Walk method (FRW) with different numbers of propagations steps taken, CSIRO (2007) data
The only parameter to tune for the IRW method is  X  -the probability of a walk restart. Empirically, in Figure 5 we see that any value starting from 0.03 and higher improves over the baseline for both datasets. However, for the W3C collection, 0.1 value gives the best result and 0.05 is the opti-mal setting for the CSIRO collection. These values actually mean that we restart our infinite random walk in average after 10 and 25 steps taken, what appears to be very close to the optimal numbers of steps for the finite random walk for the respective collections. Moreover, this setup of  X  to the value between 0 and 0.15 is also typical for the use of random walks in web retrieval [36].
 To avoid the effects of over-training for the IRW and the FRW methods in our final evaluation, we applied 5-fold cross-validation technique. We divided test queries for both collections in 5 parts and for each part trained our methods on the other 4 parts. We could also consider training on one TREC collection and testing on another. However, since the TREC data used in 2006 and 2007 is quite different (what actually allows us to conduct representative experiments), the structure, the size and the topical diversity of expertise graphs also differs considerably, so that they can not be used to tune parameters for each other. Since the ARW method is parameter-free, it could be directly applied to the entire query set without any training.

The performance of all methods for all measures is pre-sented in Table 1. As hoped, we see that actually both Infi-nite and Finite Random Walk methods are equally effective and outperform the baseline method for all measures. The ARW method also shows the improvement over the base-line for all measures, but still seems inferior to the IRW and the FRW methods. To test the statistical significance of the obtained improvement with respect to the baseline, we cal-culated a paired t-test over both query sets for each method and each measure. Results indicated that the improvement for the MAP measure is significant for all three methods at the p &lt; 0.001 level. For the MRR measure it is significant at the p &lt; 0.01 level for the IRW method and at the p &lt; 0.05 level for the FRW method. For the P@5 measure it is significant at the p &lt; 0.01 level for the IRW and the ARW methods, and at the p &lt; 0.05 level for the FRW method. The improvement we got is also comparable with the advan-tage of the baseline over the simplest Votes method. Having in mind that our baseline is the one of the most effective methods known, we may conclude that the improvements of our methods are significant in the area.

It is also important to mention that both methods that needed training phase showed the improvement over almost all regions of their parameter space (see Figures 3, 4 and 5) and our parameter-free method also showed the compa-rable improvement. Each full expert finding run for each method (including document retrieval and relevance prop-agation stages) can be performed in about 1 second on a desktop computer. This result suggests that the multi-step relevance propagation for expe rt finding is not only advanta-geous, but also practicable technique, which is easy to tune, resource-light and stable in performance. Figure 5: MAP for Infinite Random Walk method (IRW) over different values for jumping probability
So far we considered only bipartite document-candidate graphs without links between nodes of the same type. How-ever, the CSIRO collection allows to also include the ad-ditional information about relations among documents and candidate experts. Documents from *.csiro.au are highly hyperlinked. Candidate experts are professionally interre-lated, if they are employed in the same CSIRO department. Table 1: Performance for all measures, both data sets and all tested methods While inter-document links are easily extracted from docu-ments since they are HTML tagged, a candidate X  X  working department can be inferred only from the candidate X  X  email address: the third level domain name is usually an abbre-viation of a department X  X  name. As an illustrative exam-ple, the candidate X  X  email address David.Dall@ ento .csiro.au shows that David Dall works at the CSIRO Entomology re-search department. We inter-link all candidates experts in the same department and also take into account the hyper-links between documents. The experimental results shown in Figure 6 demonstrate only the benefit from adding orga-nizational links. When we set the probability of relevance propagation between related candidate experts  X  e to 0.25, we get noticeable improvement (significant at the p &lt; 0.05 level). Adding links between documents degrades the per-formance for almost all values of the inter-document propa-gation probability  X  D . Figure 6: MAP for the Infinite Random Walk method (IRW) with additional links, CSIRO (2007) data
Intuitively, the inter-department links between candidate experts can help only within  X  X unctional X  organizations whose employees are highly specialized and separate units are divided by knowledge areas. In this case we may assume that people working in the same department are all experts on similar topics -otherwise, the evidence that the specific person is knowledgeable cannot be propagated to his/her co-workers. Unfortunately, the W3C data set contains nei-ther any information about the distribution of candidate experts across organizational units nor any links between documents. There are few links due to  X  X eply-to X  relations between some emails, but in our preliminary experiments their use did not cause any change in performance. In or-der to prove our intuition, we make a simulation of the case described above. Using provided expertise judgments we in-wikipedia.org/wiki/Organizational_structure terconnect all persons who are experts on the same topic, in order to see whether it helps to rank them higher. In other words, we test the situation when all experts on a specific topic work in the same department in the W3C. We see in Figure 7 that using simulated organizational links increases the performance of the Infinite Random Walk method for all values of  X  e with the maximum at 0.6 value (this result is significant at the p &lt; 0.01 level). This experiment shows the potential advantage of modeling professional connections among employees in the enterprises with the structure sim-ilar to the simulated. Figure 7: MAP for the Infinite Random Walk method (IRW) with simulated organizational links, W3C (2006) data
We have introduced a novel class of expert finding meth-ods founded on the following twofold principle. First, it states that expert finding is a process of walking (consult-ing) in an expertise graph of candidate experts and topical documents. Second, it advocates that the relevance appear-ing from the documents should be propagated not once, but multiple times, further through various connections in such a graph. We showed that one of the most effective among ex-isting methods is a special case of our approach: one-step rel-evance propagation on our expertise graph. Notably, exper-iments conducted on the data crawled from web-sites of two large organizations validated the effectiveness of our meth-ods. We empirically demonstrated that the use of multi-step relevance propagation by different probabilistic random walk based methods sharing the same above-mentioned principle leads to significant improvements over the baseline one-step propagation. We also found the benefit of utilizing direct organizational links among candidate experts.

There are several promising di rections to extend the pre-sented research. First, it is reasonable to continue with mak-ing the random walk more relevance-directed. For example, a surfer could move from persons to documents with a prob-ability proportional not only to their association degree, but also to their relevance. Second, new entities (e.g. dates of mailing lists) can be introduced into expertise graphs to bet-ter model the relevance flow. Furthermore, not only docu-ments, but paragraphs or sentences may serve as sources of flowing out relevance.

Future work in this area should also include a wider use of professional connections between employees or any other knowledge about information flows in the organization. In cases when an appropriate organizational structure is not available, one can try to infer it through hierarchical clus-tering of persons using their pre-computed static profiles or links inferred from their e-mail communications. It is also interesting to study how the propagation of document rel-evance through persons to other documents may affect the performance of the initial document retrieval run. We want to sincerely thank Hugo Zaragoza from Yahoo! Research, Barcelona for useful discussions and extensive com-ments on the first versions of this paper.

