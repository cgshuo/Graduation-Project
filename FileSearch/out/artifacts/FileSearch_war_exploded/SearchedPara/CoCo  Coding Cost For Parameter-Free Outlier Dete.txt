 How can we automatically spot all outstanding observations in a data set? This question arises in a large variety of applications, e.g. in economy, biology and medicine. Exist-ing approaches to outlier detection suffer from one or more of the following drawbacks: The results of many methods strongly depend on suitable parameter settings being very difficult to estimate without background knowledge on the data, e.g. the minimum cluster size or the number of de-sired outliers. Many methods implicitly assume Gaussian or uniformly distributed data, and/or their result is diffi-cult to interpret. To cope with these problems, we pro-pose CoCo, a technique for parameter-free outlier detection. The basic idea of our technique relates outlier detection to data compression: Outliers are objects which can not be effectively compressed given the data set. To avoid the as-sumption of a certain data distribution, CoCo relies on a very general data model combining the Exponential Power Distribution with Independent Components. We define an intuitive outlier factor based on the principle of the Mini-mum Description Length together with an novel algorithm for outlier detection. An extensive experimental evaluation on synthetic and real world data demonstrates the benefits of our technique. Availability: The source code of CoCo and the data sets used in the experiments are available at: http://www.dbs.ifi.lmu.de/Forschung/KDD/Boehm/CoCo . H.2.8 [ Database applications ]: Data mining Algorithms, Design, Reliability Outlier Detection, Coding Costs, Minimum Description Length, Data Compression
Automatic outlier detection in large data sets is often equally or even more important than the detection of regu-larities. In various application fields like economy, biology, or medicine, the detection of extraordinary observations is of great interest. For example, the identification of crimi-nal activities, such as credit card fraud, is crucial in elec-tronic commerce applications [9]. In biology, an automatic detection of outstanding measurements or noise is critical for high-throughput data generated with e.g. mass spec-trometry or gene expression analysis. The wide range of application fields also includes entertainment, sports, e.g. performance analysis of athletes, and many more.

Today, many data mining publications are in the field of clustering or outlier detection. The first field searches for regularities in a data set whereby the second identifies irreg-ular data. Closer inspection of both fields reveals a strong relationship, whereby one goes barely without the other: On one hand, most clustering algorithms are confronted with outliers which deteriorate the cluster quality and/or desta-bilize the algorithm. Thus, the outliers need to be removed beforehand. On the other hand, outlier detection algorithms require a definition of the underlying cluster structure al-though clusters are not explicitly identified. Only if the cluster structure (of the regular data) is known, outliers can be identified without any doubt. Following the definition of Hawkins [5]: To formalize this definition, the ordinary and potentially clustered points as well as the outliers need to be differen-tiated with respect to a well-defined distinction criterion . In existing outlier detection approaches, the distinction cri-terion is quantified by a metric distance function and pa-rameter settings. The results are only meaningful if the distance function is well-characterized with respect to the object similarity and suitable parameter settings. However, Figure 1: Data compression: The principle of MDL is to detect regularities in the data and compress accordingly. these premises assume a prior characterization of the data set.

To cope with the problems of defining a distinct crite-rion and parametrization, we present CoCo, a parameter-free outlier detection method based on the ideas of data compression and coding costs. CoCo is able to identify the outliers in a data set based on a flexible definition of the reg-ular data . The regular data is flexibly defined by a very gen-eral Probability Density Function (PDF), in our case a mix-ture model of the Exponential Power Distribution (EPD). The EPD is a family of distribution functions which con-tains the Gaussian distribution, the uniform distribution, the Laplacian distribution, and a great variety of other dis-tribution functions. Compared to previous outlier detection approaches, the EPD is not restricted to either uniform or Gaussian distribution functions. We demonstrate with our experiments (cf. Section 4) that the EPD is powerful enough to model the regular data in a variety of applications.
CoCo considers a point P as outlier, unless it fits nicely in any of the distribution functions to be estimated of the points in the neighborhood of P , independent of the neigh-borhood size. To measure the quality of the fit of P we adopt the idea of data compression: If a point fits well into a distribution function, it can be compressed efficiently. To connect the data compression efficiency of P with the de-gree of P being an outlier, Figure 1 illustrates an intuitive example: Suppose, we want to transfer data via a commu-nication channel. The sender wants to transfer the string a b m c to the receiver. A naive way would be to trans-fer each single character requiring in total 16,008 bits for m = 1 , 000 and 8 bits per character. To minimize the com-munication costs, a smart sender exploits regularities in the data. A little program could generate the first part of the string by printing 1,000 times the character a followed by 1,000 times b . An efficient coding in an arbitrary language requires e.g. 344 bits. The sender additionally transfers c as single character (8 bits) instead of adding a print state-ment to the little program (which would require 64 bits). Thus, 352 bits are required to transfer the string in total. This clever compression reduces the communication cost to 2.15%. In the example, object c is an outlier generated by a different mechanism than the other objects. The regular objects can be strongly compressed by formulating the un-derlying mechanism with a model (here, the little program) and require only a transfer of 344 / 2 , 000 = 0 . 17 bits, each. Compared to the transfer of 8 bits for object c , the increase in coding costs can deposit c as an outlier. Unlike the char-acter strings in the simple example, the object P is a point, i.e. a d -dimensional vector of continuous values. Inferred from the idea of Huffman coding, we can apply the data compression idea by assigning few bits to frequent values and many bits to rare values of the coordinates of P . Fre-quent and rare values can be clearly distinguished using the above mentioned EPD. This principle is generally called the Minimum Description Length (MDL).

CoCo effectively applies the MDL principle to parameter-free outlier detection. No a-priori information about the data set is required, like the number of clusters and outliers, the cluster size, a distance metric, or the cluster density. Furthermore, we define a CoCo outlier factor with the con-cept of coding costs of an object, given the entire data set. With the outlier factor we can clearly separate the cluster points from the outliers.

The paper is organized as follows: In the next section, we briefly survey the related work. In Section 3 we intro-duce CoCo by elaborating a flexible model for continuous data relying on two major building blocks: The Independent Component Analysis (ICA) and the EPD. Furthermore, we define our CoCo outlier factor. Section 4 provides an ex-tensive experimental evaluation and Section 5 concludes the paper.
The most established approaches to outlier detection in databases can be classified into the two categories of distance-and density-based approaches. Additionally, a brief survey of the application of the information-theoretic MDL prin-ciple in data mining is given. For an extended survey on anomaly detection please refer to [4].
Distance-based outlier detection is among the earliest ap-proaches and has been proposed and further elaborated by E.M. Knorr and R.T. Ng [10, 11, 12]. An object o of a database DB is a distance-based outlier if at least a frac-tion  X  of the objects in DB have a distance greater than a previously specified distance d . This basic approach pro-vides binary flagging of points as outliers or non-outliers. An extension [12] proposes algorithms to support semantic interpretation of distance-based outliers. However, without knowledge of the data distribution, it is difficult to specify suitable values for the parameters  X  and d . In addition, a fixed distance threshold d identifies only global outliers.
Density-based outlier detection introduces an outlier no-tion derived from density-based clustering and, therefore, detects not only global but also local outliers. A point is flagged as an outlier if it does not fit well into the objects neighborhood density.

The local outlier factor LOF [3] formalizes this idea by considering the MinPts nearest neighbors of an object as its neighborhood. The LOF of an object is defined by the ratio of its MinPts -nearest neighbor distance and the mean MinPts -nearest neighbor distance in its neighborhood. How-ever, the global parameter MinPts strongly affects the out-lier detection result: Arbitrary high or low values of MinPts either regard small cluster points as outliers or do not detect outliers, respectively.

LOCI [14] is a density-based multi-granularity outlier fac-tor. Similar to LOF, points are regarded as outliers if the object density in their local neighborhood significantly de-viates from the average object density in the local neighbor-hood. The local neighborhood is specified by two parame-ters, which are called counting and sampling neighborhood. The counting neighborhood specifies some volume of the fea-ture space which is used to estimate the local object den-sity. The sampling neighborhood is larger than the count-ing neighborhood and contains all points which are used to compute the average object density in the neighborhood. LOCI differs from LOF by this decoupling of counting and sampling neighborhoods. It can be demonstrated that with-out this decoupling, density estimation leads to incorrect results in some specific cases. In addition, the decoupling allows for efficient algorithms for approximate computation of LOCI. However, the decoupling requires the specification of additional parameters. Together with the outlier factor, the LOCI approach proposes a visualization, the so-called LOCI plot which displays the LOCI of a point w.r.t. in-creasing sizes of the local neighborhood and, thereby, allows e.g. to identify micro-clusters. However, LOCI as well as LOF apply the Euclidean distance as a global metric dis-tance function. In addition, the LOCI approach proposes to flag points as outliers which deviate in their local object den-sity more than three times of the standard deviation of the overall object density of the sampling neighborhood. This flagging assumes a Gaussian distribution of the object den-sities.
Information-theoretic concepts, especially the MDL prin-ciple and related ideas have been recently successfully ap-plied to clustering [1, 2, 15], and are also established in the areas of regression [17], rule mining [19], classification [8], and anomaly detection [7]. The MDL principle relates learn-ing and data compression, as already illustrated in Figure 1. Learning regularities from data allows to compress the data more efficiently. For model selection in clustering and classification, MDL allows to compare different candidate models achieving a natural balance between goodness of fit and model complexity. To the best of our knowledge, the MDL principle has not been applied to the problem of outlier detection so far.

Regarding the problem specification, clustering is most related to outlier detection. However, outliers are regarded as a problem for clustering, since they can severely affect the result of most algorithms. A parameter-free extension of K-Means clustering is X-Means [15]. However, the X-Means algorithm is restricted to spherical Gaussian clusters and very sensitive to outliers. RIC [1] has been designed as a post-processing step to improve an initial clustering of an ar-bitrary conventional clustering algorithm. After filtering the initial clusters from noise, for each cluster a model is deter-mined. This model comprises a rotation matrix determined by PCA and a PDF assigned to each coordinate selected from a set of predefined PDFs. The recently proposed al-gorithm OCI [2] introduces a very general clustering notion based on the EPD and ICA. Also related are approaches to MDL-based de-noising of signals [16, 18]. However, these approaches are especially designed for time series and their goal is to reconstruct the signal as accurate as possible.
With CoCo, we introduce an entirely parameter-free out-lier detection method based on coding costs. Following Hawkins [5], we adapt the outlier definition to the MDL principle for data compression. A data point is considered as outlier, if its compression rate is unusually high. As refer-ence to define a high compression rate, we consult a compres-sion rate of a cluster point. This approach nicely avoids the definition of a distance metric which would require thresh-olding of an undefined and unknown neighborhood.

Data sets may be rotated or distorted with respect to the Cartesian coordinate system. The ICA enables us to process data sets which are not aligned to the orthogonal axes. However, the idea of an ordinary point needs to be clearly defined. In contrast to currently available outlier detection methods, we expect real life data to underly not only Gaussian distributions. Besides, we want to include several other distributions. A generalization of the Gaussian PDF is the EPD. The EPD includes, among others, the uniform, and the Laplacian PDF. By utilizing an EPD, any a-priori information on the type of distribution is required. Therefore, we do not create a bias towards Gaussian data models. Combining ICA with EPD as the description of a regular subset of the data set, we cover many real-world data sets without taking explicit care of cluster density, shape, and orientation.

Entirely automatic, CoCo detects outliers having high coding costs with respect to the ordinary points which can be effectively compressed. We implemented a bottom-up approach to identify all irregular data points while choosing the best compression model of ordinary points.

For each data point o , we initiate a set of nearest neigh-bors. Without prior knowledge of the underlying cluster shape, we extract a substantial number of nearest neighbors nn o based on their Euclidean distance to o . We reliably center and whiten the set of nearest neighbors with ICA, ica nn o , and fit an EPD, epd nn o . Iteratively, we expand the nearest neighbor set with those remaining data points to be best compressed based on the current epd nn o . After each update of the set of nearest neighbors nn o , we simply ad-just the ica nn o and epd nn o since it is an expensive opera-tion to estimate it anew. For each epd nn o estimate, we can calculate the coding costs cost o as compression rate of the object o under the given cluster description epd nn o . If the data is fully explored for each object o , we extract the most suitable EPD cluster model by selecting the minimum com-pression rate of any object included in cost min nn outlier factor for the data object cost o ( j ) is determined by its corresponding compression excess to cost min nn o ( j ).
The following defines the principles of ICA, EPD, data compression, and their link to the parameter-free outlier de-tection with CoCo.
It was observed that mixtures of signals get best de-mixed when searching for non-Gaussianity. A mixture of several signals originated from any distribution type is always more Gaussian than the originals. The entropy of a Gaussian dis-Algorithm 1 CoCo Input: Database D
OF := {} // Outlier Factors for data object o  X  D do end for
XMeans( OF ) to obtain outlier &amp; cluster points tribution is maximal, whereby, all other distributions have a lower entropy. However, the coding costs, measured by the entropy, need to be minimized in order to guarantee a maximal compression efficiency. Thus, we apply the ICA to maximize non-Gaussianity as a measure of statistical in-dependence. Its algorithm favors the directions in the data which are not similar to the Gaussian distribution.
We assume that most data sets in experimental data usu-ally do not follow equally dense distributions. They are rather distorted data sets with respect to the Cartesian co-ordinate system. The ICA first transforms the data into a so-called white space. Whitening involves de-correlation and normalization of the data to unit variance which enables us to implicitly handle unequally dense clusters.

The Principal Component Analysis (PCA) identifies the directions of maximal variance ~y given a set of coordinates ~x  X  C in a d -dimensional space. First, the data get centered ~c = ~x  X  ~m around the empirical mean of the data set C . Second, the centered data ~c need to be normalized to unit variance in all directions. The eigenvalue decomposition of the covariance matrix  X  is  X  : = V  X   X   X  V
T , where V and  X  are orthogonal matrices containing the eigenvectors and eigenvalues of  X , respectively. Finally, the PCA transform of ~x is determined by Note, that  X  = diag (  X  1 ,..., X  d ) and diag ( p 1 / X  1 ,..., p 1 / X  d ) are both diagonal matrices.
For optimal projection of the data we need to determine the directions of minimal entropy (generated with ICA) rather than the one of maximal variance (created by PCA). After transforming the data to white space, the FastICA al-gorithm [6] determines a weighting matrix W containing the independent components. Regarding the original space, the independent components are not orthonormal in contrast to the principal components. The iterative optimization of W expects the input data to be whitened. The fix point iteration optimizes W = ( ~w 1 ,..., ~w d ), whereby the weight vectors are updated with the following rule: We use tanh( s ) for the non-linear contrast function g ( s ). is the expected value. W is updated until convergence and then orthonormalized. The overall projection of the origi-nal data into the white space of independent components is achieved by the de-mixing matrix M  X  1 . With M = V  X  W and V are orthonormal matrices, thus the determinant is simply det ( M  X  1 ) = Q 1  X  i  X  d p 1 / X  i . Recall that the rota-tion performed in the white space is expressed by W , and whitening is achieved by multiplying the coordinate vector by the scaled Eigenvector matrix.

After the independent components are determined, we can simply project the data ~x into the independent component space with
The EPD is a generalization of the Gaussian distribution in such a way, that it also includes the Laplacian and the uniform distribution, depending on the parameter setting. Its PDF has three different parameters. Beside the location parameter  X  , and the scale parameter  X  , a shape parameter p is introduced [13]. For a random variable X , the EPD is Figure 2: Different shapes of the Exponential Power Distribution for different choices of parameter p . Figure 3: Data set approximated with an EPD and a Gaussian distribution. defined as: Note that  X ( s ) = R  X  0 t s  X  1 exp(  X  t ) dt is the gamma function as an extension of the factorial operator for real numbers.
The shape parameter p determines kurtosis, or the sharp-ness of the distribution. For p &gt; 2, the EPD is platykur-tic, with p  X   X  mimicking a uniform distribution. For p = 2, the EPD corresponds to a Gaussian distribution. If 2 &lt; p &lt; 0, the EPD is leptokurtic, including a Laplacian distribution for p = 1 (Fig. 2).
After projection of the coordinates into the white space and ICA, the data ~z is de-correlated and independent. This allows us to describe each coordinate independently by an own EPD. Typically, a multi-dimensional data space con-tains d different PDF representations f EPD ( z i ;  X  i with 1  X  i  X  d . All d distributions are combined in a mixing matrix M , where the data points ~x correspond to ~x = M  X  ~z + ~m , with ~m being the shifting vector and M determined by PCA, as described above. M allows the in-dependent components vectors to be not orthogonal. The EPD in a d -dimensional space (after ICA) is defined for a point ~x as f EPD ( x ; M  X  1 , ~m, X , X ,p ) = Figure 3 illustrates the effect of the approximation of a data set with an EPD after ICA. While the approximation of the same data with a Gaussian distribution is rather inappro-priate.
The estimation of the three parameters is a non-trivial problem. Although,  X  i = 0 and  X  i = 1 are defined for p=2 (Gaussian distribution) after ICA,  X  i and  X  i are no longer identical to the empirical mean and standard devi-ation, respectively. All three parameters  X  i ,  X  i and p be optimized by estimating the maximum likelihood, given a data set C . Only a simultaneous approximation of all pa-rameters ensures that the derivatives of the likelihood of the EPD vanish with respect to  X  i ,  X  i and p i .

Assuming  X  i and p i to be given, the parameter  X  i can be simply calculated with the derivative of the likelihood func-tion with respect to  X  i of the EPD P ~z  X  C f EPD ( ~z i The parameters  X  i and p i need to be optimized explicitly. We use a nested bisection search as optimization technique to find p i and  X  i in their parameter space. The direction to browse through the space is determined by the derivatives of the log-likelihood function with respect to  X  i df and p i df + with s i = | z i  X   X  i | .  X ( s ) = d ln  X ( s ) ds is the digamma func-tion being the logarithmic derivative of the gamma function. The EPD is estimated by this maximum likelihood approach until convergence of p i .
After we estimated an exact representation f EPD ( x ; M  X  1 ~m, X , X ,p ) of the data ~x with ICA and EPD, we need a re-liable approach to judge the accuracy of the fit. We create the link of the concept of PDFs to the principle of data com-pression with the help of the MDL. Based on the Huffman coding, a number of bits are assigned to each object with the inverse logarithm of the probability of the object. This negative log-likelihood represents the coding costs c PDF an object ~x , given any PDF, and is defined as: In order to represent the coding cost in the number of bits, the logarithm is typically used to a basis of 2. With CoCo, we underly an EPD as PDF. Thus, the relative coding cost of a data point ~x under a given EPD after ICA is: c
EPD ( ~x ) = log 2 ` | det ( M  X  1 ) |  X   X  X We can neglect to determine the absolute coding costs de-pending on different PDFs and the coding of the PDF pa-rameters. It is absolute crucial to determine statistically independent major directions with ICA to guarantee opti-mal data compression. Figure 4 clearly demonstrates that ICA transforms the data in such a way that it removes re-dundancy in the data with respect to the axes for best com-pression.
Putting everything together, for each set of coordinates ~x from the nearest neighbors nn o generated with CoCo, we de-termine the rotation and the cluster description with EPD epd nn o . For each estimate epd nn o , the data compression rate is calculated with c EPD ( ~o ), ~o being the whitened co-ordinates of object o . We determine the efficiency to com-press the data points nn o , with an epd nn o estimate, with any object p  X  nn o having minimal coding cost: We gather information of compression rates for each set of nn increasing size. Ideally we need to know the optimal neigh-borhood cluster size of o to determine the perfect compres-sion of o regarding C . Practically, we only have information for each epd nn o estimate throughout the data set. With it comes the information of any object ( p ) exhibiting the minimal coding cost in nn o . The best compression rate (min( cost min nn o ), throughout all generated nn o sets) rep-resents the best epd nn o estimate for any nn o . In order to obtain the factor of o being an outlier, the CoCo outlier fac-tor is the absolute compression rate increase with respect to a minimal p .
 The structure of a data set is usually unknown. We screen C coming from o iteratively by adding a set of neighbors; its size growing exponentially with respect to the size of C . To guarantee a stable estimate of EPD we initiate nn o with a set of 20 neighbors. This screening approach of CoCo is however quadratic in the number of points n . In addition, the runtime is cubic in the dimensionality d due to PCA and EPD estimation.

After all CoCo outlier factors are obtained, we expect all outliers to exhibit unusually high costs in comparison to the ordinary, perhaps clustered points. The cluster points can be compressed very effectively and show outlier factors around 0. Flagging of outliers is difficult, since it involves to define a suitable threshold, which is a non-trivial task for an unknown data set. Instead, we simply apply an X-Means algorithm to determine the set of clustering points being the cluster closest to 0. Theoretically, we can establish an outlier order by simply organize the other CoCo outlier fac-tor groups in ascending order. In practice, X-Means usually finds two clusters, one containing the clustering points, the other determining all outliers.

CoCo combines ICA with EPD as cluster description to determine outliers entirely parameter-free with the principle of data compression. No a priori knowledge of the number of outliers or the underlying cluster shape or density is re-quired.
In the following we evaluate our outlier factor CoCo in comparison to LOF [3] and LOCI [14] using one synthetic data set as well as NBA data. We implemented CoCo and LOF in Java and obtained the implementation of LOCI from the authors. The synthetic data set was created to exemplify the strength of CoCo.
We detected the outliers of a synthetic data set with our novel algorithm CoCo and compared them with outliers de-tected by LOF and LOCI. Figure 5 provides the results of CoCo, LOF, and LOCI for the synthetic data set. The syn-thetic data set consists of four clusters C1-4 containing 184 (C1), 154 (C2), 52 (C3), and 50 (C4) data points. Each and 26 outliers. Detected outliers are highlighted with red crosses. cluster has different cluster properties and a non-orthogonal major orientation. Cluster C3 underlies a Gaussian PDF. All together 26 noise points were added to the data set.
CoCo correctly detects all 26 outlier points highlighted with red crosses (Fig. 5, left). All belong to one group of outliers, beside the group of cluster points shown in black. Note, that CoCo requires no input parameter in order to identify all noise points. It can handle different types of cluster shapes and orientations without expecting an explicit description of their distributions.

LOF was applied to identify the outliers based on a MinPts neighborhood of 50 determined by the size of the smallest cluster in the set (Fig. 5, middle). We obtain the top 26 outliers (highlighted with red crosses) since we know how many outliers are present in the data set. There are 24 out of the 26 noise points assigned correctly. Two noise points next to cluster C2 (circled in blue as No. 3) are not detected, leading to two falsely identified cluster points as outlier (cir-cled in blue as No.1&amp;2). Note, that we collected the top 26 data points ranked by the LOF score. Setting the pa-rameter MinPts to a value smaller or equal than 10, LOF identifies more cluster points as outliers while leaving many true outliers undetected (data not shown). A MinPts value of 20 to 50 leads to the result shown in Figure 5. If we have no a priori information about the number of outliers, it is only possible to determine an arbitrary number of outliers. In addition, an approximate cluster size needs to be known in advance to set MinPts , in order to get a meaningful out-put. These assumptions make it difficult to apply LOF to real world data.

LOCI was applied to our synthetic data set with  X  = 0 . 5 and r min = 10 (Fig. 5, right) and could identify 43 outlier points based on the suggested outlier flagging criteria. All together 17 true outliers were missed, while two points from within cluster C3 and 27 points from cluster C4 were labeled as outliers. Different parameter settings of r min may detect more true outliers, but at the same time label more cluster points as outliers. Obviously, LOCI is not able to deal with clusters showing low density, like C4. In Figure 7, we have a closer look at the LOCI plot of an outlier point (circled in blue as No. 1 in Figure 5, right) and a cluster point (No. Figure 7: LOCI plot for two points detected as out-liers. (1) True outlier. (2) Falsely labeled cluster point. 2). The LOCI plots look very similar even though they are supposed to emphasize the difference between a cluster point and an outlier. We have to note, that although we applied the algorithm with the suggested parameter settings, the result was difficult to interpret even after correspondence with the authors.
To emphasize the difference and strength of the CoCo out-lier factor in comparison to the LOF score, we introduce a visualization of the  X  X utlierness X  (Fig. 6). A scatter plot of the data in x-y directions is combined with a bar representa-tion of the outlier factors in the z-dimension. We can clearly show that the utilization of data compression is able to sep-arate the outliers from the cluster points in comparison to the outlier factor of LOF. For the majority of the cluster points the CoCo coding costs are close to 0.0 which can be seen by the short, dark blue bars. Outliers are either light blue or even red indicating their extraordinariness, ranging from 6.4 up to 24.2. Due to the large range between clus-ter points and outliers it is possible to clearly differentiate them using CoCo. In contrast, LOF produces values rang-ing from 0.8 up to 2.3 which makes it almost impossible to clearly differentiate cluster points from outliers explicitly.
The visualization of the outlier-factors of LOF demon-strates, that the cluster structure is based on Euclidean dis-crosses and marked with player names. tances: the outlier factors continuously increases circular from the cluster centers to the cluster margins. In contrast to LOF, the CoCo outlier factors are equally low through-out the entire cluster except for the cluster edge points. It is based on the flexible cluster structure description using ICA and EPD.
After extensive evaluation of CoCo on synthetic data sets, we want to apply our novel parameter-free outlier detection method to experimental data. We used the NBA data avail-able at the NBA website http://www.nba.com . In the Sea-son 2007/08, 450 players are described with four attributes: the number of games played (GP), the number of points (PPG), the rebounds (RPG), and assists (APG) per game. CoCo was applied to this NBA data detecting 105 outliers. Figure 8 displays scatter plots of the data. For simplicity reasons, we highlight only the top 10 outliers in red as listed in Table 1. Obviously, the data distribution is non-Gaussian.
The top 10 outliers identified by CoCo, include outstand-ing players like Stephon Marbury with a coding cost of 19.6 being 12 times higher than the average coding costs. Mar-bury is an outstanding player with respect to all attributes. He played only 24 games out of 82 and was still able to achieve 13.9 points and additionally assisted in 4.7 points, resulting in being involved in 18.6 points per game. Jamaal Tinsley, has played 39 games in this season but was still able to assist in 8.4 game points. He was involved in 20.3 points and played more games than Marbury. Gilbert Are-nas exhibits a rare combination of playing 13 games while achieving 19.4 points per game. Jason Kidd is outstanding in the number of rebounds having played in 80 out of 82 games. Elton Brand has played only few games but still was able to achieve an outstanding number of points. As evident from Figure 8, outstanding players such as Kidd or Brand are best characterized with the most general model with only one component.

To put the CoCo outlier detection method into a context, we applied LOF and LOCI to the NBA data set, as well. Table 2 displays the top 10 outliers identified by LOF. High-lighted in bold are all players that were identified as top 10 outlier of CoCo, like Marbury, Arenas, or Brand. Except for one, all players from the top 10 outlier of CoCo are at least under the top 20 of LOF. However, the outstanding player Kidd was missed by LOF ranked at the 50th position with a LOF score of 1.16. In addition, as observed for synthetic data, the result of LOF strongly depends on its parameteri-zation. Only seven players are reproducibly detected as top 10 for a MinPts = 40 (players are marked with an asterix). All five players which were found to be under the top 10 of CoCo were also included in the intersect of MinPts = 40 and MinPts = 50 which strikes that they are strongly out-standing. The top 10 outliers found by LOCI are shown in Table 3. The intersect between LOCI and CoCo is again highlighted in bold.
CoCo o.f. Name GP PPG RPG APG 19.6 Stephon Marbury 24 13.9 2.5 4.7 17.9 Jamaal Tinsley 39 11.9 3.6 8.4 16.1 Gilbert Arenas 13 19.4 3.9 5.1 15.4 Andrew Bynum 35 13.1 10.2 1.7 13.6 Elton Brand 8 17.6 8 2 12.9 Ronald Muray 73 9.1 4.5 1.3 12.8 Jason Kidd 80 10.8 7.5 10.1 12.5 Chris Kaman 56 15.7 12.7 1.9 12.3 Ramon Sessions 17 8.1 3.4 7.5 12.0 Randy Foye 39 13.1 3.3 4.2 Table 1: Top 10 outliers identified with CoCo on NBA data. (o.f. = outlier factor).

LOF Name GP PPG RPG APG 1.43 Elton Brand* 8 17.6 8 2 1.32 Steve Francis 10 5.5 2.3 3 1.31 Kasib Powell 11 7.6 4 1.6 1.28 Gilbert Arenas* 13 19.4 3.9 5.1 1.28 Chris Webber* 9 3.9 3.6 2 1.27 Stephon Marbury* 24 13.9 2.5 4.7 1.26 Dwyane Wade* 51 24.6 4.2 6.9 1.25 LeBron James 75 30 7.9 7.2 1.24 Andrew Bynum* 35 13.1 10.2 1.7 1.24 Chris Kaman* 56 15.7 12.7 1.9 Table 2: Top 10 outliers identified by LOF with MinPts = 50 on NBA data sorted by outlier-factor.
 Players also among the top 10 of CoCo are marked in bold. The asterix indicates players which are also among the top 10 using MinPts = 40 . Note that all players found to be under the top 10 of CoCo and LOF MinPts = 50 are also found using MinPts = 40 .
In this paper, we proposed CoCo, a parameter-free outlier detection. The perspective of data compression in outlier de-tection allows to define a notion of outliers, which is intuitive to interpret and requires no parameter settings. Our exper-iments demonstrate that CoCo is not restricted to Gaussian data but applicable to a wide range of data distributions.
In future work, we will further elaborate techniques to fa-cilitate the interpretation of cost-based outliers. In addition, we will focus on online algorithms for cost-based outlier de-tection in data streams, since online monitoring is essential in many applications involving outlier detection.
 Name GP PPG RPG APG LeBron James 75 30 7.9 7.2 Kobe Bryant 82 28.3 6.3 5.4 Dwyane Wade 51 24.6 4.2 6.9 Chris Kaman 56 15.7 12.7 1.9 Elton Brand 8 17.6 8 2 Andrew Bynum 35 13.1 10.2 1.7 Jamaal Tinsley 39 11.9 3.6 8.4 Mike Bibby 48 13.9 3.3 6 Jermaine O X  X eal 42 13.6 6.7 2.2 Udonis Haslem 49 12 9 1.4 Table 3: Top 10 outliers identified by LOCI on NBA data. Players also among the top 10 of CoCo are marked in bold. [1] C. B  X  ohm, C. Faloutsos, J.-Y. Pan, and C. Plant. [2] C. B  X  ohm, C. Faloutsos, and C. Plant. Outlier-robust [3] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and [4] V. Chandola, A. Banerjee, and V. Kumar. Anomaly [5] D. Hawkins. Identification of Outliers . Chapman and [6] A. Hyv  X  arinen, J. Karhunen, and E. Oja. Independent [7] E. Keogh, S. Lonardi, and C. A. Ratanamahatana. [8] S. Kim and I.-S. Kweon. Simultaneous classification [9] E. M. Knorr. On digital money and card technologies. [10] E. M. Knorr and R. T. Ng. A unified notion of [11] E. M. Knorr and R. T. Ng. Algorithms for mining [12] E. M. Knorr and R. T. Ng. Finding intensional [13] A. Mineo and M. Ruggieri. A software tool for the [14] S. Papadimitriou, H. Kitagawa, P. B. Gibbons, and [15] D. Pelleg and A. Moore. X-means: Extending [16] J. Rissanen. Mdl denoising. IEEE Transactions on [17] M. Robnik-Sikonja and I. Kononenko. Pruning [18] J. Xie, D. Zhang, and W. Xu. Spatially adaptive [19] T. Yoshida, H. Motoda, and T. Washio. Adaptive
