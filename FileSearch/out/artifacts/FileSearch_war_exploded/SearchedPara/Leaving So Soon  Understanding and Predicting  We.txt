 Users of search engines often ab andon their searches. Despite the high frequency of Web search abandonment and its importance to Web search engines, l ittle is known about why searchers abandon beyond that it can be for good or bad reasons. In this paper, we extend previous work by studying search abandonment using both a retrospective survey and an in-situ method that captures aban-donment rationales at abandonment time. We show that although satisfaction is a common motivator for abandonment, one-in-five abandonment instances does not rela te to satisfaction. We also studied the automatic prediction of the underlying reason for ob-served abandonment. We used features of the query and the re-sults, interaction with the result page (e.g., cursor movements, scrolling, clicks), and the full search session. We show that our classifiers can learn to accurately predict the reasons for observed search abandonment. Such accurate predictions help search pro-viders estimate user sa tisfaction for queries wi thout clicks, afford-ing a more complete understanding of search engine performance. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process , selection process .
 Web search abandonment; Abandonment rationales. Search engine result page (SERP) abandonment is a type of search abandonment that occurs frequentl y and happens when users do not click on any of the results returned for a query [22][29][30]. Since clicks are absent in abandoned queries, it is difficult to un-derstand why searchers are abandoning. They may have obtained the information they sought directly on the SERP, they may be dissatisfied with the results and failed to find any results worth clicking, or there may be othe r reasons for abandonment (e.g., accidental closure of the browser window). Since research in this important area is limited, a compre hensive analysis of the reasons behind SERP abandonment is both timely and necessary. To understand the reasons for SE RP abandonment and to enable search engines to estimate search success, we need to be able to automatically determine abandonment rationales. Features of in-teraction mined from large-scale l ogs have been shown to be use-ful in understanding searchers X  sa tisfaction with search results [1][5][11][16]. However when modeling SERP abandonment, the absence of search result clickt hrough data means that this im-portant signal cannot be used to make inferences regarding the cause of abandonment. We therefore need to study whether there is evidence in the query, the results returned, and/or search inter-action behavior beyond hyperlink clicks that might help predict people X  X  motivations for abandonment. We extend previous work in this area in the following ways:  X  We capture the reasons for abandonment in-situ from the  X  We capture all search-related behaviors for a larger number  X  We develop and study classi fiers to predict abandonment We began our studies of abando nment rationales by employing a restrospective survey, and elicited abandonment rationales directly from searchers based on their recall of recent abandonment events. We then used the survey findings to inform the design of a Web browser plugin that captured abandonment rationales in-situ, allowing us to obtain the reas ons for SERP abandonment as it happened and at the same time ga ther data about SERP content and searcher interaction that may be useful for predicting why people abandoned. The plugin was deployed within Microsoft Corporation for a period of four weeks and was adopted by over 2,500 people. Using data collected by the plugin, we show that we can train classifiers capable of reliably predicting why a user abandoned a SERP given their inte raction behavior on the SERPs plus their search flow during the session. The remainder of this paper is structured as follows. Section 2 presents related work on abandonment and the use of interaction features (in particular mouse cursor behavior since it does not involve link clicks) to infer SERP satisfaction. Sect ion 3 describes the findings of a retrospective survey that we used to elicit a range of abandonment rationales from users. Section 4 presents our study methodology including the browser plugin deployed to capture abandonment rationales in-situ and the data collection methods. Section 5 provides an overview of the explanations gathered in-situ, and the predicti on experiments are described in Section 6. In Section 7 we discuss the findings of our study and their implications. We conclude in Section 8. The click-centric nature of Web search has made the use of hyper-link clicks a popular approach for studying user search behavior and search goals. Traditionally, clickthrough data has been inter-preted as implicit user feedback about the re levance of search re-sults. Clicks and dwell times on s earch result pages are often in-terpreted as a positive signal that the user is satisfied X  X o some degree X  X ith the result. Clicks ha ve been used to improve a va-riety of search-related tasks, e.g., to predict search success [11][16], compare alternative search algorithms [5], and learn ranking functions [25]. Conversely, the absence of clicks is inter-preted as a negative signal of the quality of the results, and some efforts have been made to reduce query abandonment [28]. Gaze tracking provides a much richer understanding of patterns and se-quence of user attention on search result pages [8][23], and might be useful in differentiating between good and bad abandonment. However, gaze tracking is difficul t to instrument outside of la-boratory settings and thus is impractical at Web scale. More recently, researchers have used cursor movements as an al-ternative to gaze tracking. Several researchers have examined the relationship between eye gaze and cursor positions during search tasks [13][17][26]. Rodden et al. [26] identified a strong align-ment between cursor and gaze positions. They also observed dif-ferent types of cursor behaviors: (i) neglecting the cursor while reading, (ii) using the cursor as a reading aid to follow text (either horizontally or vertically), and (iii ) using the cursor to mark inter-esting results. Guo and Agichtein [13] examined the relationship between search intent and curs or movements through a browser toolbar. In a follow-up study, they conducted two shopping tasks, and found that interaction featur es improved accuracy in discrimi-nating between research and purchase intents [14]. Li et al. [22] were the first to distinguish between good and bad abandonment in search and the need to augment click behavior to understand abandonment. They defined good abandonment as an abandoned query for which the searcher X  X  information need was successfully addressed by the SERP, without needing to clickthrough to additional pages. This can happen when the an-swer to a user X  X  query is in the snippet text or, increasingly, when search engines provide specific kinds of answers to try to meet the user X  X  information need (e.g., weather, flight status information, stock quotes, etc.). Li et al. co mpared abandonment for desktop and mobile search in three different locales (U.S., Japan, and Chi-na) and developed ground truth data using editorial judg-ments. They defined  X  X otentiall y X  good abandonment as queries that had a dominant information need that could be answered by a SERP, and  X  X ikely X  good abandonm ent by examining such SERPs to see whether the answers were on the page. Likely good aban-donment queries included those like [weather Oregon] or [1 USD in GBP] and had answers in either the result snippet or in a dedi-cated inline answer element on the SERP. Li et al. manually la-beled potential good abandonment in small set of 400-1000 aban-doned queries from the Google search engine. They did so by only considering the query and not the search results returned by Google. From that analysis, they estimated that 19-32% of aban-doned Web searches conducted on desktop computers could be related to satisfaction (i.e., those queries that were classified as  X  X es X  or  X  X aybe X  with respect to potential good abandonment). Stamou and Efthimiadis studied searches without clicks by sur-veying a small sample of volunteer searchers [29][30]. In one study [29], they recruited 38 gra duate students and asked them to complete an external survey (running in a separate Web browser window) for each Web search they performed in a single day. They identified two reasons why queries are abandoned: for inten-tional causes such as spell-checking, finding an answer or keeping abreast of the latest information; and unintentional causes such as irrelevant results, no search results, search was interrupted, and repetition of already-examined results. Stamou and Efthimiadis found that 13% of the study queries had no clicks, split evenly between intentional and unintentional causes. Analysis of partici-pant responses revealed that th e most common reason for uninten-tional abandonment was dissatisfact ion with the search results, whereas the explanations provi ded for intentional abandonment were more evenly distributed between different explanations. However, since participants were not prompted to complete the survey, the distribution of explana tions gathered may not be fully representative of all task types (e.g., participants may be less like-ly to remember the survey when engaged in intensive tasks). In a follow-up study [30], Stamou and Efthimiadis address the lack of searcher prompting by using a browser plugin, deployed to a group of six participants. They studied two types of SERP inac-tivity: pre-determined (user planne d on finding an answer on the SERP without clicking) and post-determined (user planned on vis-iting a result when they queried, but decided not to once the re-sults were returned). Stamou and Efthimiadis found that just over a quarter of the queries (27%) were abandoned because of a pre-determined intention, and that half of the post-determined aban-doned queries were negatively aff ected by the information on the SERP, representing dissatisfi ed or bad abandonment. Castillo et al. [6] highlighted the value of studying search behav-ior as a proxy to understanding how  X  X enacious X  searchers were in finding inline answers for SERPs. Chilton and Teevan [7] studied repeated behaviors to understa nd abandoned SERPs containing inline answers. Huang et al. [17] recently described a scalable method for collecting cursor inter action patterns on SERPs. In one of their experiments they sought to distinguish good from bad abandonment. They examined on e category of abandoned queries, namely short questions that contained answers in the result snip-pets on the SERP (what Li et al. [22] called the  X  X nswer X  catego-ry). For these queries, Huang et al. found differences in cursor trail length, movement time, a nd cursor speed depending on whether the answer was present in the result snippets (good aban-donment) or was not present in the result snippets (bad abandon-ment). Good abandonment was associ ated with shorter trails, less movement and slower cursor speed . However, they did not show whether cursor data can be applied to predicting whether aban-donment is good or bad or applied to a wider range of query types. Beyond the SERP, White and Dumais [32] studied more extreme abandonment, where peopl e switch away from the search engine. We extend the previous work described in this section in a num-ber of ways. First, we capture abandonment rationales in-situ from the abandoning users, rather than retrospectively via third-party judges. Second, we record more extensive log data than other studies across a greater number of users, providing access to broad range of abandonment intent s and affording other analysis such as the nature of abandonment occurrences which have not previously been studied. Finally, we develop classifiers to accu-rately predict the reasons for abandonment and explore features that may be useful in distinguishing betw een abandonment types. We begin by describing our initial explorations of the reasons for SERP abandonment, which informs the design of the in-situ sur-vey and the analysis that we perfor m. Our first step was to distrib-ute a survey asking people to recall their last abandonment epi-sode and provide details of the motivation for it. To obtain an initial set of candidate explanations for why users abandoned their searches we used a retrospective survey. An invitation to complete an online survey was distributed via email to a random sample of 3,000 employees from within Microsoft Corporation X  X  campus in Redmond, WA. The sample comprised employees in a range of technical and non-technical roles. In completing the survey, respondents were asked to recall one recent example of a query they issued to a search engine where they did not click on any SERP hyperlink. Given this point of reference, the survey asked participants about: I. Their motivation for not clicking on any link on the SERP. II. Their level of satisfaction with the search results (five-point 
III. The clarity of their search goal (five-point scale ranging from 
IV. The complexity of their search task (five-point scale ranging V. The kind of task they were performing (based on Kellar et The nature of the search task has also been shown to effect SERP abandonment [30] and how searchers examine the SERP more generally [3][8]. We therefore requested information about task types in question V to better understand the nature of the tasks that people were performing wh en they abandoned SERPs. Overall, we received responses from 186 survey participants. Fig-ure 1 provides an overview of the reasons for abandonment pro-vided by survey respondents. Our analysis of the responses showed that dissatisfaction ( DSAT ) with the search results re-turned by the search engine was the primary reason for SERP abandonment (41%). This was followed by satisfied abandonment ( SAT ) at 32% of responses. Interestingly, there was a large frac-tion of abandonment cases (27%) which the participants neither rated as clear SAT nor clear DSAT. Most of these were cases where the participants abandoned because they decided on a better query before they examined the search results (13%), sometimes they did not pursue the search any further because it was not im-portant enough (3%), and rarely did they state that they got inter-rupted (1%). The Other category (10% of all responses) contained reasons with insufficient frequency to warrant their own category (e.g.,  X  X  figured it out for myself X ). Other was also occasionally used even though there was a re sponse option dedicated to the participants X  explanation. For example, the Accidental category did not appear directly in any of our survey responses, but some of the responses for Other implied that the reason was accidental:  X  X  lost network connection X  or  X  X  unintentionally closed the tab. X  Participants X  responses to ques tions III-V are summarized in the histograms in Figure 2. The survey revealed that 93% of the par-ticipants had a clear understanding of what they were looking for, 74% of the tasks were simple, and most of them (74%) involved information gathering or fact-finding tasks. These statistics align well with previous research on understanding user goals in Web search, independent of abandonment [20][27]. It appears that there is nothing remarkable about the types of tasks for which people abandon SERPs, at least in terms of the questions that we asked. To better understand the relationship between abandonment ra-tionales and participants X  level of satisfaction with the search re-sults provided the search engine we cross-tabulated their respons-es. Figure 3 provides a breakdown of user satisfaction (question II in the bulleted list of survey questions shown above) by the dif-ferent reasons for abandonment. Fr om the figure we can see that while dissatisfaction is most prominent for DSAT and satisfaction is most prominent for SAT, th e remaining abandonment reasons are to a large extent characterized by neutral to positive satisfac-tion. When Interrupted was offered as a reason for abandonment (n=2), the level of satisfaction was entirely neutral, suggesting, as expected, that interruptions we re not caused by the SERP. The retrospective survey provided us with some insight into the reasons for query abandonment and the approximate frequencies with which each explanation happened (or at least could be recol-lected by participants). This information was useful to us in mak-ing decisions about which response options to offer in the plugin that we deployed to monitor ab andonment rationales in-situ. We now describe the methodology that we used to monitor abandon-ment rationales and search inte ractions at abandonment time. In selecting an in-situ methodology for our study, we also consid-ered a log-based analysis or la b-based experiments. The high de-gree of naturalism and ecological validity afforded by the plugin made it more attractive than the other methods. Log-based studies capture the behavior, but not the rationale for it. Lab-based studies may capture rationales, but may also expose people to artificial conditions and may lead to unnatural patterns of search behavior. Since one of the goals of this research is to develop an abandon-ment classifier that could be applied in real settings, capturing ex-planations in-situ in a natural setting was important for us. To this end, we developed and deployed a Web browser plugin called AbandonTracker that surfaces a survey in a popup window to the searcher asking for an explanation whenever SERP aban-donment is detected. In the remainde r of this section, we describe the plugin, including important and transferable design decisions that we made, and its deployment within our organization. The first step is to define precisel y what we regard as abandonment. Intuitively, SERP abandonment occurs when the searcher does not click on any of the links on the SERP. Li et al. [22] provide a def-inition of abandonment by requiring  X  a query that is not followed by any click or any further query within a 24-hour period.  X  We agree broadly with this definition, but refine it in two important ways to consider: (i) the nature of the click and (ii) the nature of the trigger event (used to determine when and how the SERP abandonment occurs). Hence, we define abandonment in Web search as a situation where the following conditions are met: 1. No clicks on results: There are no hyperlink clicks on any re-sults or advertisements, including results returned by the rank-ing algorithm and direct answers inserted into the results for topics such as news and weather. Note that we include adver-tisements in our definition of abandonment since we believe that they may also satisfy searcher needs in a similar way to search results. If a SERP click is on a related search, spelling suggestion, query alteration, or navigational link offered by the search engine (e.g., changing the scope of the search from  X  X eb X  to  X  X mages X ) then we also regard that as abandonment since clicking on these links ta kes the user to another SERP. 2. Trigger event occurs: In addition to defining what counts as abandonment, we also need to define the point in time that the determination of no clicks (condi tion 1) should be made. To do this we define a set of abandonment triggers comprising one of the following actions: When the above two requirements have been met, the AbandonTracker system displays a popup survey, requesting that users indicate why they abandoned the search results. The survey appears on top of the SERP befo re the next Web page begins loading. The AbandonTracker plugin was developed for the Internet Ex-plorer Web browser. To remove the effect of variations in search engine quality and SERP layout s we focused on abandonment on a single search engine. In this subsection we describe the popup survey shown to participants and the data gathered by the plugin. Whenever a query is abandoned per our definition in the previous section a popup is shown on top of the browser window. See Fig-ure 4 for an example of the popup survey for the query [maui weather]. The survey prompts the user to enter the reason for abandoning their query. It presents the abandoned query string to help the user identify the query that is being referred to (particu-larly useful in cases where many SERPs are being viewed in dif-ferent browser tabs). The survey also offers four broad abandon-ment rationales from which the participant can select the appro-priate response based on the findings from the retrospective sur-vey. These are SAT , DSAT , Interrupted or Unimportant , and Oth-er . Participants who selected the Other category could optionally specify their reason in the text area at the bottom of the survey. Since the survey interrupted searchers directly with a popup, we wanted to keep it compact and easy to complete quickly. There-fore we elected not to include the option a better query came to mind , since it could overlap with DSAT and require effort from users to distinguish between the explanations. Since all trigger events were automatically recorded (including manual requery), this explanation could be reconstructed using some combination of the manual requery trigger event, the time between the initial query and the requery, and by analyzing the Other reasons pro-vided by users. Since we believed that it would be useful to know where on the SERP users found th eir information and how often they did so, we also offered multiple explanations for the source of the information lead ing to SAT abandonment. In deploying the plugin to partic ipants, we were concerned that given the frequency of SERP abandonment, the popup might ap-pear too often, interrupting searchers from their primary task, and potentially irritating them to the point where they uninstall the plugin. We addressed this concern in three ways: Firstly, we in-troduced two  X  X gnore X  buttons in the survey, one to ignore the cur-rent instance of SERP abandonment, and another button to ignore all SERP abandonments for the next hour. Secondly, we imple-mented a trigger controller mechanism that suppressed the popup for 50% of all SERP abandonments on a per user basis. Thirdly, the popup survey would show up for a maximum frequency of 10 times per day per user to reduce the overall per-user effort. In addition to participant responses to the questions in the popup survey, AbandonTracker also records Web interaction data. For each user, it records the unique plugin identifier, all URLs that the participant visited, timestamps, unique identifiers for browser tabs and browser instances, and stores this information in a remote da-tabase. We also recorded the titles, the URLs, and the snippets of the top 10 search results, the presence/absence of other SERP fea-tures such as direct answers, and interactions with the SERP, in-cluding hyperlink clic ks as appropriate. In addition to the standard click l ogs of the search engine, we also captured a number of cursor acti ons on the result page using a scalable methodology similar to th at described by Buscher et al. [4]. JavaScript-based logging f unctions were embedded into the HTML source code of the SERP. The  X  -and  X  -coordinates of the mouse were recorded every 250 milliseconds if the mouse had moved at least eight pi xels (approximately half a line of text on the SERP) since it was last polled, non-hyperlink clicks (e.g., clicks in SERP whitespace to hi de popups, right-clicks to print or refresh), scrolling, text selection, focus gain and loss events of the browser window, as well as boundi ng boxes of several areas of interest (AOIs) on the SERP (e.g., top and bottom search boxes, mainline results and its contained result entities) and the brows-er X  X  viewport size. Combining these data sources enabled us to develop a rich picture of how se archers engaged with the SERP. Features were extracted from data gathered by the plugin as well as the search engine to build the predictive models described later. The AbandonTracker was deployed to employees of a large tech-nology company. Participants were recruited via an email invita-tion to a random sample of 5,000 employees. The invitation was sent to employees with a variet y of backgrounds and job roles, ranging from software engineers to patent attorneys and adminis-trators. In total, over 2,500 memb ers of the organization installed the plugin. A number of steps were taken to ensure participants X  privacy such as not storing pers onally identifiable information directly and not recording requests to intranet and secured servers. To motivate our participants to keep the plugin installed for the duration of the study, we randomly selected two participants per week of the study who had the plugin installed and awarded them each a 50 USD gift card. Awards were not tied to the amount of feedback that participants provi ded to avoid unduly biasing their search behavior with monetary incentives. We now present an analysis of the data gathered by AbandonTracker. We discuss the characteristics of the data, de-scribing overall usage statistics and characterizing the motivation behind observed abandonment instan ces and the ways in which people abandon (i.e., the why and the how of SERP abandon-ment). We also describe a comparison of the in-situ data with that gathered via the retrospective survey. No previous study of aban-donment has explored these important issues in this depth or at this scale. The study ran for 30 days in late 2011. We discarded data gath-ered on the first two days from our analysis since these were when we sent out the recruitment ema ils (and when 93% of our users installed the plugin), and we did not want initial testing of the plugin to affect data quality. We report results for 928 participants who provided feedback for at least one abandoned query. The other users either did not abandon, did not provide feedback, or did not use the search engine that we focus on. During the study, those participants visited 739,505 URLs in 172,887 distinct browser tabs, 39,606 of which were queries to the search engine we study. About 22% of these vi sited SERPs were abandoned per our definition of SERP abandonment provided in the previous 
Figure 5. Distributions of the different reasons for query abandonment in retrospective and in-situ survey data. 
Figure 6. Abandonment tri gger conditions showing how section. In half of these abandoned SERPs, the survey popup win-dow was suppressed by the system. Of the remaining 50% of abandonment instances where the popup was shown, 59% of the popups were explicitly ignored by participants. The dataset was further processed to remove queri es related to users testing the plugin functionality (e.g., [test], [h ello world]). In the end, we gathered 1,799 abandonment instances that we analyzed further. Figure 5 (right) shows that there is a fairly even split between the SAT and DSAT reasons for abandonment. Findings showed that when satisfied, the answer to an abandoned query was eight times more likely to come from a dedicated inline answer on the SERP (such as weather, st ock quote, etc.) than from result summaries. However, SAT and DSAT explanations still only occupied around 80% of the reasons for why people abandoned. The other 20% of queries deserves special attention since it has not been considered in previous studies. For simplicity we created a superclass called Unintentional comprising the Interrupted , Unimportant , and Ac-cidental classes. This superclass comprised 7% of abandonment instances. Since we did not record the better query option directly on the in-situ survey, we reviewed the comments provided in re-sponse to Other where the trigger was manual requery and identi-fied those corresponding to our definition of better query (e.g.,  X  X  was dissatisfied with my query X ,  X  X idn X  X  enter the right key-words X ). Overall, 5% of the responses indicated that the aban-donment rationale was related to th e participant deciding to build a better query. The remaining queries were in the Other class and comprised reasons such as being directed to the incorrect engine vertical (e.g.,  X  X  wanted images X ), seeking an intranet site, change of criteria, or undirected searching (e.g.,  X  X ust vaguely browsing X ). For comparison, we also show th e distribution of explanations from the retrospective survey (Figure 5 left). Previous work on search engine switching [15] showed that there can be noteworthy differences in explanations gather ed from in-situ and retrospective methods which might be suggestiv e of cognitive biases in the types of events that people recollect. We wanted to see whether the same trend was observed in our data. From Figure 5 we can see that the distributions of explanations are fairly similar. The main difference is in the fraction of SAT instances of abandon-ment (31% in the retrospective survey versus 38% in the in-situ plugin). Similar reductions in SAT for retrospective versus in-situ analysis have been observed in pr evious work of a similar nature [15] and may in part be related to people X  X  tendency to more read-ily recall negative events in retrospective studies [21]. Understanding how people abandon (the so-called trigger ) is im-portant for applications of ab andonment, since to model aban-donment, we need to know how to capture it. Li et al. [22] set their trigger as no click or query in the 24 hours after the aban-donment. However, given that pe ople use search engines frequent-ly (and often more than once per day), the 24-hour requirement is not sufficient for a complete anal ysis of abandonment; not to men-tion that they did not study how people abandoned SERPs. Figure 6 shows that there are a range of abandonment trigger conditions. Interestingly, the most frequent way in which searchers aban-doned a SERP was by manually entering the new query in its search box or in the Web browse r (45% of abandonments). Clos-ing the Web browser tab in which the SERP was displayed (21%) or manually entering a URL in the Web browser X  X  address bar (17%) were the main two othe r abandonment triggers. Manual URL entry and tab closure are both suggestive of task termination, whereas manual re-query may also reflect task continuation, de-pending on the nature of the query. The other trigger types are less popular (all 7% comprising of instances or less) and cover many different events with a range of possible explanations. To better understand the relations hip between explanations and triggers we study all explanation-trigger pairs. Figure 7 provides a breakdown of abandonment reasons by trigger conditions. Note that by definition, the better qu ery explanation is only available for manual requery. The distribution of reasons differs depending on how the query was abandoned. A number of key insights can be made from these results. First, if the participant closed the Web browser tab, the session timed out, or they entered a URL, they were 2-3 times as likely to be satisfied as when manually re-querying. This suggests a link between behavior and abandonment rationales (e.g., tab closed  X  SAT, manual-requery  X  DSAT). We further explore the behavior-rationa le relationship in the next sec-tion as we turn attention to pr edicting abandonment rationales us-ing only the information a search engine can principally record. Although prior work has explored some characteristics of aban-donment in a limited capacity (and more limited than this study) [22][29][30], to our knowledge there has been no work on predict-ing explanations for observed abandonment instances. Using the 1,799 labeled abandonments from the in-situ study as ground truth, we built and tested cla ssifiers to predict abandonment ra-tionales. This section describes the features that we generated, the classifiers used, the evaluation metrics, the models that were compared in the study, the learning procedure, and the findings. The four explanation labels from earlier were used corresponding to the main answer alternatives from the survey popup (Figure 4): SAT , DSAT , Unintentional , and Other . Although we hand-labeled the better query class, to avoid bias fro m this labeling we only used the original user feedback for prediction. We frame the pre-diction task as binary, predicting a single label vs. all other labels. We focus on binary prediction since it effectively models an ap-plication scenario that is likely to be of great interest to search providers (e.g., generate a list of DSAT abandonments for further inspection). We examine the features that are the most important in predicting abandonment rationales. We also study the features that distinguish SAT and DSAT abandonment since these provide useful insight into behavioral differences between the classes. Our predictions are made retrospectively once an instance of abandonment is observed. Around 2,000 features are generated for the abandoned SERP, the previous and next SERP, and the full session. We limited feature genera tion to the immediately preced-ing and succeeding SERPs both to maintain a manageable feature space and because those SERP interactions may provide the best insight about abandonment rationale for the current SERP (e.g., the next SERP may be for a re fined query, suggesting DSAT). For each abandoned SERP in our data, we extracted many fea-tures of the query, the associated SERP of interest, and the search session for the task of predicting the rationale for the observed SERP abandonment. The features we re divided into five classes: (i) session, (ii) query, (iii) result, (iv) hyperlink-click and dwell, and (v) cursor. We now describe each class in more detail. Search Session Features: Search sessions begin with a query to a search engine, and terminate fo llowing 30 minutes of inactivity between successive actions [31]. Session features for each query include the numbers of abandonmen ts observed in the session, the total number of queries issued, whether the abandoned query was re-issued again in the session, the position of the abandoned query of interest with respect to all queries in the session and the posi-tion of the query of interest w ith respect to all abandonments. Query Features: These include features of the query string itself (length in characters and length in tokens), and historic features such as the query frequency in the logs of a commercial search engine and the average query SERP result clickthrough rate. His-toric feature values were derived from a year of search engine query-click logs from 2010. Other fe atures included the similarity between successive queries in the search session, measured in dif-ferent ways including overlap and cosine similarity. Result Features: We computed features of the SERP returned for the query. These features included the number of results that were returned, the number of adver tisements and their position on the SERP (to help capture whether th e query had commercial intent), whether related queries or spelli ng suggestions were shown, the average length of result URLs, and whether there was any special treatment for the query such as special  X  X irect answers X  for que-ries with directly-serviceable needs (e.g., [weather in maui ha-waii]). Since abandonment may o ccur because searchers find in-formation in the titles and snippets of returned search results, we also computed features of the cosine similarity between the query string and each result title, and the cosine similarity between the query string and each result snippet. In addition to what was shown on the SERP, we also created features reflecting the match between the query and each of the top-ranked search results via the score assigned by the search engine X  X  ranking algorithm. Hyperlink Click and SERP Dw ell Features (Engagement): Although abandoned SERPs by defi nition do not contain clicks on the results or advertisements, th ere may still be clicks on other regions of the page (e.g., relate d searches). We computed around 300 features of user clicks on various SERP components (e.g., the number of clicks on search result s and the number of clicks on the search box) and overall dwell time on the result page. Note that when we compute these features for the previous and next SERPs they may include clicks on search results and advertisements. Cursor Activity Features: Features of cursor interaction with the SERP can reveal patterns and pref erences that cannot be observed through clickthrough behavior and can be captured at scale [17]. As described earlier, we captured cursor movements on the aban-doned pages and the previous and post abandonment SERP. The cursor-related features that we computed included: Trails : These features are derived from the cursor movement trails on the SERP and include trail length, trail speed, trail time, total number of cursor movements, a nd summary values (average, me-dian, standard deviation) for si ngle cursor movement distances and cursor dwell times in the same position, etc. We also created features for the total number of mouse movements and the total number of times that the cursor changed direction. Hovers : We recorded total hover time on the SERP. Since we rec-orded the coordinates of areas of in terest we were also able to as-sociate cursor movements with particular SERP elements. This allowed us to represent the total hover time on inline answers (e.g., stock quotes, headlines, etc.), total time hovering in the low-er and upper search boxes on the result page, in the left rail (where search support such as query s uggestions and search history would usually be shown), in the ri ght rail (where advertisements would usually be shown), and in the algorithmic results. We also computed the total time that the cursor was idle on the SERP. Result Inspection Patterns and Reading : In a similar way to [3], this summarizes how users inspected the results, including the total number of search results th at users hovered over, the average result hover position, and the fraction of the top ten results that were hovered. We also created features of the linearity of scan-ning over results and evidence of the user reading with the mouse. Non-hyperlink Clicks : The total number of non-hyperlink clicks in various AOIs on the SERP, including the number of clicks in the upper and lower search box, the left and right rails, the algorith-mic results, and across all SERP regions. Scrolling : Including the total number of scroll events, the number of times the user scrolled up and down, the total scroll distance (in pixels), the maximum scroll height (in pixels) referring to the  X  -coordinate at the top of the vi ewport relative to the SERP, and time between SERP load and scrolling. Other : Including whether the user clicked on the search box (sug-gesting that they were going to re-query), the number of text se-lections (total and unique results), and the number of hover pre-views (total and unique results). Hover previews are an interface feature that provides more information about a search result when requested by hovering on its caption. We experimented with a variety of algorithms to predict aban-donment rationales, using the featur e sets described in the previ-ous section. We found that multiple additive regression trees (MART) [12] was the best-performing classifier. Both L1 and L2 regularization were used to avoi d overfitting predictions to the training set (90% of the sampled set determined via cross valida-tion). L1 selects only effective features, and L2 penalizes extreme feature weights. The effectiveness of these two regularizations was demonstrated theoretically a nd empirically [24], showing that classification with regularization can be effective even if there are exponentially as many featur es as training examples. In evaluating the performance of our predictions, we measure pre-cision (the fraction of predicted instances that were correctly pre-dicted) and recall (the fraction of all true instances that were cor-rectly predicted). In this paper we report on the  X   X  measure, with  X  set to 0.5 rather than  X  =1.0.  X   X . X  assigns twice as much weight to precision than to recall. High precision is very important in ap-plication scenarios for a predictor of abandonment rationales. In an online scenario, we would want to be highly confident before adapting the search experience based on abandonment rationale predictions. In an offline scenario , such as studying dissatisfaction in log data, we need to obtain a set of dissatisfied abandonment instances for further analysis. Since there are many abandonment events in logs, we do not need to classify all of them (have high recall) as long as we can precisely label some. Note that we exper-imented with  X   X . X  and the trends in results are the same as  X  We compare the effectiveness of different feature sets for per-forming the predictions. We also trained the binary classifiers on varying sets of the features described in Section 6.1. In addition to analyzing the performance of each class individually, we also consider the following three feature combinations:  X  All: Classifiers trained on all features.  X  All.NoInteraction: Binary classifiers trained on all features except those that capture post-qu ery interaction behavior on the 
SERP such as Click+Dwell , or Cursor . This helps us under-stand the importance of SERP interactions in predicting the rea-sons why people abandon. SERP in teractions have been used in previous work to estimate satisfaction [1][10][13].  X  All.NoCursor: Classifiers trained on all features other than 
Cursor . The cursor features repr esent a new source of behav-ioral information and we were particularly interested in their contribution to the overall pred iction performance of the model. We used two baselines in our experiments:  X  Marginal: The marginal distribution across each label.  X  Answer Presence: The presence or absence of direct answers on the result page. Direct answers are special elements more targeted at task completion / answering information needs on the SERP than normal algorithmic results [7]. Examples of such answers are weather reports and stock quotes. Many direct an-swers are designed to encourag e satisfied abandonment, espe-cially when presented at or near the top of the search result list. 
This baseline predicts SAT if the SERP has a direct answer el-ement in the top-three result po sitions, and DSAT otherwise. 
This baseline is much stronger since it is based on an opera-tional system (which others co uld replicate) and it is query-dependent. Note that we do not compute these values for the 
Unintentional or Other classes as there is no clear mapping be-tween the presence or absence of answers and these labels. Each of the models described in the previous section is used to generate a rationale prediction for each of the 1,799 observed abandonment instances in our set. Predictions are made at the end of the search session containing the abandoned SERP. This aligns with our application setting wher e predictions would be made ret-rospectively. This also lets us capitalize on session features. Ten-fold cross validation was performed to increase the reliability of the results over 10 randomized e xperimental runs. We now pre-sent findings on the prediction effectiveness using the different feature classes. We report averages over all runs and folds and present the results of statistical testing as appropriate. We begin by analyzing the performance of the binary predictions for each class of SERP abandonment. The top row of Table 1 pre-sents the  X   X . X  values representing how effectively our binary clas-sifiers can predict SAT , DSAT , Unintentional , and Other using all features. The next to bottom row of the table contains the Margin-al baseline score for each class. The bottom row contains results for the Answer Presence (AP) baseline in the operational search engine. Significance values are also indicated using paired  X  -tests in comparison with the marginal and AP baseline. As can be seen from the table, AP outperforms the marginal for both SAT and DSAT predictions. Since it is stronger than the marginal, we use AP as the preferred baseline in the remainder of our analysis. Turning attention to the performance of the classifiers that use All features, we see that our classi fiers significantly outperform the baselines in predicting SAT , DSAT , and Other . However, there are no significant gains over the baseline Unintentional . One explana-tion is that unintentional abandonments are affected by external factors, such as distractions, loss of interest, or task shifts, which may not be predictable using the f eatures of the SERP or search-ers X  interactions with it. The table also shows that it was easier to predict DSAT abandonment than SAT abandonment, perhaps be-cause it may not be obvious that the user found the sought infor-mation directly on the SERP, especially if they do not interact in any way with it. Another reason is that DSAT abandonment is re-lated to re-querying (as shown in Figure 7), providing a clear sig-nal for prediction. We now study th e impact of the feature classes. Table 1 also shows the breakdown of performance by the different feature classes and the two class combinations defined earlier. There is interesting variance in the feature classes that matter de-pending on the prediction task. For example, using only Query features yields a classifier that does not significantly outperform the AP baseline (Answer Presence: 0.5443 vs. Query: 0.5532, ns ). Since the presence of a direct answer on the SERP is dependent on the query, much of the value from query features may already be encoded in AP. Cursor movements capture aspects of how people examine the SERP [17]. Given the range of possible cursor behaviors it is encouraging that Cursor features alone yield rea-sonable prediction perfo rmance, primarily for DSAT predictions, and for SAT and DSAT , adding Cursor to All helps. 
Table 1. Binary prediction performance (  X   X . X  ) for each feature class and task. Significant diffe rences from the marginal are marked using bold =  X  &lt; .05, underlined bold =  X  &lt; .01. For Presence baseline are marked with  X  =  X  &lt; .05,  X  =  X  &lt; .01. prediction of SAT vs. other classes and DSAT vs. other classes. Importantly, Table 1 shows that both SAT and DSAT can be accu-rately predicted using All.NoInteraction , which is accessible to search engines without needing further instrumentation. Focusing briefly on the Other classification task, we see in the last column of the table that it is features of hyperlink clicks and dwells that were most useful in predicting the Other class. Recall that the Other classification contained the better query subclass and was often used by participants to ca pture query dissatisfaction (e.g., typographical errors). Closer in spection of the key features showed that they were associated with an immediate query refor-mulation, rapid clickthrough on the SERP following the current (abandoned) one, or engagement w ith spelling corrections on the current SERP, both reflecting problems with the abandoned query. In addition to studying performance at the feature class level, it can also be informative to examine the individual features that contributed the most evidential weight to the classifications. We focus on the SAT and DSAT prediction tasks in the remainder of the paper both to simplify our analysis and given that these are important motivations for search providers. In Table 2 we present the top five most important features for the SAT and DSAT predic-tion. Descriptive names are used for each feature and the suffixes  X  X revSERP X  or  X  X extSERP X  are used to represent the features of the previous or next SERP respectively. For each feature we also present the normalized weight with respect to the most predictive feature, and the Pearson correlation coefficient ( r ) between each feature X  X  values and the ground tr uth labels. The feature weights are returned as part of the MART classifier output and reflect the relative importance of the features in the classifications models that were generated. This helps determine the tendency of the fea-ture value, e.g., the similarity to the next query positively corre-lates with DSAT but negatively correlates with SAT . Table 2 shows that the features th at were most important in pre-dicting SAT abandonment were associated with similarity between successive queries, the presence of inline answers on the page (especially an direct answer at the top of the list of search results), the quality of the search results re trieved, and the degree of exam-ination of the SERP, measured in terms of total time spent dwell-ing on AOIs. The features associated with DSAT were also associ-ated with query reformulation, but this time the reformulation was positively related to DSAT . Interestingly, the presence of non-hyperlink clicks was st rongly associated with DSAT abandonment. NonHyperlinkClickCount is the second most important feature for DSAT . Although these clicks can occur in any non-link SERP lo-cation many of the clicks occur in the search box as shown by the importance of the ClickCountInTopSearchBox feature. However, the fact that NonHyperlinkClickCount is more important than ClickCountInTopSearchBox suggests that non-hyperlink clicks in regions of the SERP other than the search box are also important in predicting DSAT . Result quality and the speed with which users clicked the search result on the follow-on SERP (if there was a click) were also strong predictors of DSAT , the latter perhaps sug-gesting that the search task is di fficult with users needing to spend more time examining the search results. So far we have shown that we can predict SAT and DSAT aban-donment with good accuracy. However, since there was some similarity in the top-five features for SAT and DSAT in Table 1 (just different directionality), we wanted to better understand what features distinguished between these two key abandonment types. We used the labeled data and fo cused on the subset of abandon-ment instances that represented a SAT or DSAT , ignoring the oth-ers. In these experiments, SAT is the positive class. Using all fea-tures, we can effectively differe ntiate between the two abandon-ment types. Specifically, we achieved an  X   X . X  score of 0.7847, sig-nificantly more than the AP baseline performance of 0.6133 (t(99)=1.41, p=.016). The features that best distinguish SAT from DSAT abandonment are in Table 3. The positive label was as-signed to SAT , so the tendencies of the correlations are positive if they correlate with satisfaction and negative if they relate more to dissatisfaction. The features that matter most in this task are asso-ciated with the queries (similarity to next query and next query length) and results (result quality, presence of answers, and match between query and result snippets). Only two of the top-ranked features are associated with interaction / cursor movements. One of these are the number of non-hyperlink clicks on the SERP which seem to carry a signal from both search box clicks, i.e., re-formulations, as well as further types of interaction on the SERP (e.g., clicks to restore focus to the page, clicks prior to scrolling). Interestingly, although answers to user requests may be found in captions for many queries, the match between the query and the caption is actually more closely associated with DSAT . Although abandonment occurs often and is critical for measuring satisfaction, there has been very limited study of why it occurs. Our study is the first to address key issues in understanding aban-donment, namely gathering rationa les in-situ, characterizing the reasons that searchers abandon and their abandonment behavior at scale, and predicting abandonmen t rationales from search behav-ior. It is only through extensive st udies and detailed analysis of this nature that we can truly understand search abandonment. Not only did we observe the same satisfaction-oriented rationales identified in previous studies, but we also found that a significant fraction of abandonment (around 20%) does not fit under SAT or DSAT (e.g., formulating and issuing a better query before inspect-ing the results). We need to further analyze these explanations, especially the better query class, which we leave for future work. Participants in both of our studi es were employees at Microsoft and are may therefore be more familiar with technology than the average user. Studies with more general user cohorts are needed. Our classifiers signifi cantly outperformed the marginal and AP baseline for the categories SAT , DSAT , and Other , but not Unin-tentional . One explanation is that unintentional abandonment is either independent of SERP conten t or search behavior, making it difficult to learn with those features. Focusing on particular fea-ture classes, Result and All.NoInteraction classes were found to perform reasonably well, and only slightly worse than All , which include cursor movements, clicks , and dwells. Since the features in Result and All.NoInteraction are available in the search engine logs, extra instrumentation may not be required to achieve strong prediction accuracy. Interestingly, Cursor features also provide reasonable performance on their ow n, and significa ntly impact prediction accuracy when combined with all other features for this task. Cursor features were also prominent in the one versus all predictions (Table 1) . More exploration of that is required. Our findings of positive and negative associations of single fea-tures with abandonment rationales are of particular importance to search engine providers since these facilitate the definition of ro-bust metrics for capturing aspects of perceived search quality. Being able to accurately predic t the reasons for abandonment has many implications for search engine design and evaluation. The frequency of SERP abandonment a nd its predicted rationale can be used as a metric to evaluate engine performance. Such a metric could be used to supplement exis ting click-based metrics by not only assigning an abandonment rate to the query, but also estimat-ing the type of SERP abandonment that occurred. This also has implications for the layout of th e SERP: certain queries could be altered to ensure good abandonment is encouraged, for example by inserting more inline answers, or bad abandonment designed out. This can also be used to reduce instances of bad abandon-ment. For example, Sarma et al. [28] propose algorithms for learn-ing to rank with the goal of minimizing query abandonment, and such algorithms could be trained us ing the output of our classifiers to minimize bad abandonment rather than all abandonment. In addition, search engines that can predict abandonment given only a single SERP can adapt the search experience and/or the ranking algorithm for follow-on queries if DSAT is estimated There is limited knowledge of why people abandon SERPs and only restricted offline analysis can be performed on abandonment in search logs. Better knowledge of the causes of search engine abandonment and methods to accurate ly predict these reasons are critical for understanding and m odeling user satisfaction with search engines. We used data gath ered by retrospective and in-situ surveys to characterize abandonment rationales, showing that one-in-five abandonment instances does not relate to satisfaction. We developed classifiers capable of accurately predicting why search-ers abandon and studied the features that distinguish SAT and DSAT abandonment. Future work will involve further study of the reasons behind abandonment, the application of our models to search logs to develop abandonme nt-based metrics for assessing search engine performance, and training ranking algorithms using log data labeled with our classifi ers to minimize instances of bad abandonment and ultimately improve searcher satisfaction. [1] E. Agichtein et al. (2006). Lear ning user interaction models [2] E. Agichtein, E. Brill, and S.T. Dumais. (2006). Improving [3] G. Buscher, S. Dumais, and E. Cutrell (2010). The good, the [4] G. Buscher et al. (2012). Large-scale analysis of individual [5] B. Carterette and R. Jones. (2007). Evaluating search engines [6] C. Castillo et al. (2010). When no clicks are good news. [7] L. Chilton and J. Teevan. (2011). Addressing people X  X  infor-[8] E. Cutrell and Z. Guan (2007). What are you looking for? An [9] D. Downey et al. (2008). Unde rstanding the relationship be-[10] H. Feild, J. Allan, and R. Jone s. (2010). Predicting searcher [11] S. Fox et al. (2005). Evaluatin g implicit measures to improve [12] J. Friedman, T. Hastie, and R. Tibshirani. (2000). Additive [13] Q. Guo, S. Yuan, and E. Agic htein. (2011) Detecting success [14] Q. Guo and E. Agichtein. (2010). Ready to buy or just [15] Q. Guo et al. (2011). Why searchers switch: understanding [16] A. Hassan, R. Jones, and K. Klinkner. (2010). Beyond DCG: [17] J. Huang, R.W. White, and S.T. Dumais. (2011). No clicks, [18] T. Joachims et al. (2005). Accurately interpreting [19] T. Joachims and F. Radlinski. (2007). Search engines that [20] M. Kellar, C. Watters, and M. Shepherd. (2006). A goal-[21] E.A. Kensinger. (2007). Negativ e emotion enhances memory [22] J. Li, S.B. Huffman, and A. Tokuda (2009). Good abandon-[23] L. Lorigo et al. (2008). Eye tracking and online search: Les-[24] A.Y. Ng. (2004). Feature selectio n, L1 vs. L2 regularization, [25] F. Radlinski, R. Kleinberg, an d T. Joachims. (2008). Learning [26] K. Rodden et al. (2008). Eye-mouse coordination patterns on [27] D. Rose and D. Levinson. (2004 ). Understanding user goals [28] A. Sarma, S. Gollapudi, and S. Ieong. (2008). Bypass rates: [29] S. Stamou and E.N. Efthimia dis. (2009). Queries without [30] S. Stamou and E.N. Efthimiadi s. (2010). Interpreting user [31] R.W. White and S.M. Drucker. (2007). Investigating behav-[32] R.W. White and S.T. Dumais. (2009). Charactering and Pre-
