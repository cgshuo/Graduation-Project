 Chunping Wang CW 36@ EE . DUKE . EDU Ivo Shterev IS 33@ EE . DUKE . EDU Eric Wang EW 28@ EE . DUKE . EDU Lawrence Carin LCARIN @ EE . DUKE . EDU Department of Statistical Science, Duke University, Durham, NC 27708 The segmentation of general imagery is a problem of long-standing interest. There have been numerous techniques developed for this purpose, including K-means and associ-ated vector quantization methods (Ding &amp; He, 2004), sta-tistical mixture models (McLachlan &amp; Basford, 1988), as well as spectral clustering (Ng et al., 2001). This list of existing methods is not exhaustive, although these methods share attributes associated with most existing algorithms. First, the clustering is based on the features of the image, and when clustering these features one typically does not account for their physical location within the image (al-though the location may be appended as a feature compo-nent). Secondly, the segmentation or clustering of images is typically performed one image at a time, and therefore there is no attempt to relate the segments of one image to segments in other images ( i.e. , to learn inter-relationships between multiple images). Finally, in many of the tech-niques cited above one must a priori set the number of an-ticipated segments or clusters. The techniques developed in this paper seek to perform clustering or segmentation in a manner that explicitly accounts for the physical locations of the features within the image, and multiple images are segmented simultaneously (termed  X  X ulti-task learning X ) to infer their inter-relationships. Moreover, the analysis is performed in a semi-parametric manner, in the sense that the number of segments or clusters is not set a priori , and is inferred from the data. There has been recent research wherein spatial information has been exploited when clus-tering (Figueiredo et al., 2007), but that segmentation has been performed one image at a time, and therefore not in a multi-task setting.
 To address the goals elucidated above within a statistical setting, we employ a class of hierarchical models related to the Dirichlet process (DP) (Ferguson, 1973). The Dirichlet process is a statistical prior that may be summarized suc-cinctly as follows. Assume that the n -th patch is repre-sented by feature vector x n , and the total image is com-posed of N such feature vectors { x n } n =1 ,N . The feature vector associated with each patch is assumed drawn from a parametric distribution f (  X  n ) , where  X  n represents the pa-rameters associated with the n -th feature vector. A DP prior can be placed on  X  n , which is characterized by the non-negative parameter  X  and the  X  X ase X  distribution G o . We adopt the stick-breaking construction developed by Sethu-raman (Sethuraman, 1994), and the hierarchical model may be expressed as This is termed a  X  X tick-breaking X  representation of DP be-cause one sequentially breaks off  X  X ticks X  of length  X  from an original stick of unit length ( As a consequence of the properties of the distribution Beta (1 ,  X  ) , for relatively small  X  it is likely that only a relatively small set of sticks  X  h will have appreciable weight/size, and therefore when drawing parameters  X  from the associated G it is probable multiple  X  n will share the same  X  X toms X   X  h (those associated with the large-amplitude sticks). The parameter  X  therefore plays an im-portant role in defining the number of clusters that are con-stituted, and therefore in practice one typically places a non-informative Gamma prior on  X  (Xue et al., 2007). The form of the model in (1) imposes the prior belief that the feature vectors { x n } n =1 ,N associated with an image should cluster, and the data are used to infer the most prob-able clustering distribution, via the posterior distribution on the parameters {  X  n } n =1 ,N . Such semi-parametric cluster-ing has been studied successfully in many settings (Xue et al., 2007; Rasmussen, 2000). However, there are two limitations of such a model, with these defining the focus of this paper. First, while the model in (1) captures our belief that the feature vectors should cluster, it does not im-pose our additional belief that the probability that two fea-ture vectors are in the same cluster should increase as their physical locations within the image become more proxi-mate; this is an important factor when one is interested in segmenting an image into contiguous regions. Secondly, typical semi-parametric clustering has been performed one image or dataset at a time, and here we wish to cluster mul-tiple images simultaneously, to infer the inter-relationships between clusters in different images, thereby inferring the inter-relationships between the associated multiple images themselves.
 As an extension of the DP-based mixture model, we here consider the recently developed kernel stick-breaking pro-cess (KSBP) (Dunson &amp; Park, 2008), introduced by Dun-son and Park. As detailed below, this model is similar to that in (1), but now the stick-breaking process is augmented to employ a kernel function to quantify the prior belief as-sociated with spatially proximate patches. In (Dunson &amp; Park, 2008) a Markov chain Monte Carlo (MCMC) sampler was used to estimate the posterior on the model parameters. In the work considered here we are interested in relatively large data sets, and therefore we develop an inference en-gine that exploits ideas from variational Bayesian analysis (Beal, 2003).
 There are problems for which one may wish to perform segmentation on multiple images simultaneously, with the goal of inferring the inter-relationships between the differ-ent images. This is referred to as multi-task learning (MTL) (Thrun &amp; O X  X ullivan, 1996; Xue et al., 2007), where here each  X  X ask X  corresponds to clustering feature vectors from a particular image. As presented below, it is convenient to simultaneously cluster/segment multiple images by linking the multiple associated KSBP models with an overarching DP. There are at least three applications of MTL in the con-text of image analysis: ( i ) one may have a set of images, some of which are labeled, and others of which are unla-beled, and by performing an MTL analysis on all of the images one may infer labels for the unlabeled image seg-mentation, by drawing upon the relationships to the labeled imagery; ( ii ) by inferring the inter-relationships between the different images, one may sort the images as well as sort components within the images; ( iii ) one may identify abnormal images and locations within an image in an un-supervised manner, by flagging those locations that are al-located to a segmentation component that is locally rare. A similar scenario has been studied in (Sudderth et al., 2006), where the spatial translations are handled with transformed Dirichlet processes. 2.1. KSBP prior for image processing The stick-breaking representation of the Dirichlet process (DP) was summarized in (1), and this has served as the basis of a number of generalizations of the DP. The de-pendent DP (DDP) proposed by MacEachern (MacEach-ern, 1999) assumes a fixed set of weights,  X  , while allow-ing the atoms  X  = {  X  1 ,  X  X  X  ,  X  N } to vary with the predic-tor x according to a stochastic process. Dunson and Park (Dunson &amp; Park, 2008) have proposed the kernel stick-breaking process (KSBP), which is particularly attractive for image-processing applications. Rather than simply con-sidering the feature vector { x n } n =1 ,N , we now consider { x n , r n } n =1 ,N , where r n is tied to the location of the pixel or block of pixels used to constitute feature vector x n . We let K ( r , r 0 ,  X  )  X  [0 , 1] define a bounded kernel function with parameter  X  , where r and r 0 represent general loca-tions in the image of interest. One may choose to place a prior on the kernel parameter  X  ; this issue is revisited be-low. A draw G r from a KSBP prior is a function of position r , and is represented as Dunson and Park (Dunson &amp; Park, 2008) prove the valid-ity of G r as a probability measure. Comparing (1) and (2), both priors take the general form of a stick-breaking representation, while the KSBP prior possesses several interesting properties. For example, the stick weights  X  ( r ; V h ,  X  h ,  X  ) are a function of r . Therefore, although the atoms {  X  h } h =1 ,  X  are the same for all r , the weights effectively shift the probabilities of different  X  h based on r . The basis functions  X  h serve to localize in the space of r regions (clusters) in which the weights  X  h ( r ; V h ,  X  are relatively constant, with the size of these regions tied to the kernel parameter  X  .
 If f (  X  n ) is the parametric model (with parameter  X  n ) re-sponsible for the generation of x n , we now assume that the augmented data { x n , r n } n =1 ,N are generated as The notation G r  X  KSBP ( a, b,  X , G o , H ) is meant to convey that G r is drawn one time from the KSBP, and is a parametric function of location r , and it is evaluated at specific locations { r n } n =1 ,N .
 The generative model in (3) states that two feature vectors that come from the same region in the image (defined via are likely to share the same atoms  X  h . The settings of a and b control how much similarity there will be in drawn atoms for a given spatial cluster centered about a particular  X  . If we set a = 1 and b =  X  , analogous to the DP, small concentration parameter  X  and/or small kernel parameter  X  will impose that  X  h is likely to be near one, and therefore only a relatively small number of atoms  X  h are likely to be dominant for a given cluster spatial center  X  h . On the other hand, if two features are generated from distant parts of a given image, the associated atoms  X  h that may be promi-nent for each feature vector are likely to be different, and therefore it is of relatively low probability that these feature vectors would have been generated via the same parameters  X  . It is possible that the model may infer two distinct and widely separated clusters/segments with similar parameters (atoms); if the G o within the KSBP is itself drawn from a DP, as it will be below when analyzing multiple images, widely separated clusters may share the exact same atoms. For the case a = 1 and b =  X  , which we consider below, we employ the notation G r  X  KSBP (  X ,  X , G o , H ) . Below we will also assume that f (  X  ) corresponds to a multivariate Gaussian distribution. 2.2. Spatial correlation properties As indicated above, the functional form of the kernel function is important and needs to be chosen carefully. A commonly used kernel is given as K ( r ,  X  ,  X  ) = exp (  X   X  k r  X   X  k 2 ) for  X  &gt; 0 , which allows the associated stick weight to change continuously from V h to 0 conditional on the distance between r and  X  . By choosing a kernel we are also implicitly imposing the de-pendency between the priors of two samples, G r and G r 0 Specifically, both priors are encouraged to share the same atoms  X  h if r and r 0 are close, with this discouraged other-wise. Dunson and Park (Dunson &amp; Park, 2008) derive the correlation coefficient between two probability measures G The coefficient approaches unity in the limit as r  X  r 0 . Since the correlation is a strong function of the kernel pa-rameter  X  , below we will consider a distinct  X  h for each stick. This implies that the spatial extent within the image over which a given stick is important will vary as a function of the stick (to accommodate textural regions of different sizes). We now consider the problem for which we wish to jointly segment M images, where each image has an associated set of feature vectors with location informa-tion, in the sense discussed above. Aggregating the data across the M images, we have the set of feature vectors { x ferent, and therefore the number of feature vectors N m may vary between images. The premise of the model discussed below is that the cluster or segment characteristics may be similar between multiple images, and the inference of these inter-relationships may be of value. Note that the assump-tion is that sharing of clusters may be of relevance for the feature vectors, but not for the associated locations. 3.1. Model A relatively simple means of sharing feature-vector clus-ters between the different images is to let each image be processed with a separate KSBP (  X  m ,  X  m , G m , H m ) . To achieve the desired sharing of feature-vector clusters be-tween the different images, we impose that G m  X  G and G is drawn G  X  DP (  X , G o ) . Recalling the stick-breaking form of a draw from DP (  X , G o ) , we have G = P h =1  X  h  X   X  h , in the sense summarized in (1). The discrete form of G is very important, for it implies that the different G r will share the same set of discrete atoms {  X  h } h =1 ,  X  is interesting to note that for the case in which the kernel parameter  X  is set such that K ( r ,  X  h ,  X  )  X  1 , the hierar-chical KSBP (H-KSBP) model reduces to the hierarchical Dirichlet process (HDP) (Teh et al., 2005).
 Therefore, the H-KSBP model is represented as where N (  X  ) is a Gaussian distribution. Assume that G is composed of the atoms {  X  h } h =1 ,  X  , from the perspective of the stick-breaking representation in (1). These same atoms are shared across all { G r the associated KSBPs, but with respective stick weights unique to the different images, and a function of position within a given image. The posterior inference allows one to infer which clusters of features are unique to a particu-lar image, and which clusters are shared between multiple images. The density functions H m are tied to the support of the m -th image, and in practice this is set as uniform across the image extent. The distinct  X  m , for each of which a Gamma hyper-prior may be imposed, encourages that the number of clusters (segments) may vary between the differ-ent images, although one may simply wish to set  X  m =  X  for all M tasks.
 For notational convenience, in (4) it was assumed that the kernel parameter  X  m varied between tasks, but was fixed for all sticks within a given task; this is overly restrictive. In the implementation that follows the parameter  X  hm may vary across tasks and across the task-specific KSBP sticks. 3.2. Posterior inference For inference purposes, we truncate the number of sticks in the KSBP to T , and the number of sticks in the trun-cated DP to K (the truncation properties of the stick-breaking representation of DP are discussed in (Ishwaran &amp; James, 2001), although we emphasize that when trun-cating KSBP one must take into account the draws from the Beta distribution and the properties of the kernel, to assure that the truncated set of sticks sum to one). Due to the discreteness of G = draw of the KSBP, G r ble values {  X  k } k =1 ,K ; when drawing atoms  X  hm from G , the respective probabilities for {  X  k } k =1 ,K are given by {  X  k } k =1 ,K , and for a given r nm the respective prob-{  X  dences between the data and atoms explicitly, we further introduce two auxiliary indicator variables. One is z this indicating which stick of the KSBP the feature vec-tor x nm is associated, and the other is t hm , this indicating which mixing component  X  k the atom  X  hm is associated with.
 With this specification we can represent our H-KSBP mix-ture model via a stick-breaking characterization. A graph-ical representation of the proposed H-KSBP model is pro-vided in Figure 1.
 For the large-scale problems of interest here we employ variational Bayesian (VB) inference, which has proven to be a relatively fast (compared to MCMC) and accurate in-ference tool for many models and applications (Beal, 2003; Blei &amp; Jordan, 2004). To employ VB, a conjugate prior is required for all variables in the model. In the proposed model, we however cannot obtain a closed form for the variational posterior distribution of the node V hm , because of the the kernel function. Alternatively, motivated by the Monte Carlo Expectation Maximization (MCEM) al-gorithm (Wei &amp; Tanner, 1990), we develop a Monte Carlo Variational Bayesian (MCVB) inference algorithm, where the intractable nodes are approximated with Monte Carlo samples from their conditional posterior distributions. The resulting algorithm combines the benefits of both MCMC and VB, and has proven to be effective for the examples we have considered (some of which are presented here). Given the H-KSBP mixture model detailed in Section 3.1, we can follow standard variational Bayesian infer-ence (Beal, 2003) to infer the variables of interests. All the updates are analytical except for V hm , which is es-timated with the samples from its conditional posterior distributions. Due to the limited space, we only con-sider the update for V hm . To obtain the conditional pos-terior distribution of V hm , we rewrite z nm = min { h : A nm,h = B nm,h = 1 } , with two auxiliary variables defined as: A nm,h  X  Bernoulli ( V hm ) and B nm,h  X  Bernoulli ( K ( r nm ,  X  hm ,  X  m )) .
 The conditional posterior distributions of V hm are Beta (1 + where for h = 1 , 2 ,  X  X  X  , z nm  X  1 , and A nm,h = B nm,h = 1 for h = z nm .
 The hyper-parameters  X  ,  X  , and  X  are assumed to be con-stant for inference of the other parameters. However, since the model performance may be sensitive to the settings of those hyper-parameters, we can relax this assumption by placing non-informative priors. The updates are straight-forward (Beal, 2003) and therefore omitted here. 3.3. Convergence To monitor the convergence of our MCVB algorithm, we compute the lower bound of the log model evidence at each iteration. Because of the sampling of some variables, the lower bound does not in general increase monotonically, but we observed in all experiments that the lower bound increases sequentially for the first several iterations, with generally small fluctuations after it has converged to the local optimal solution. We have applied the H-KSBP multi-task image-segmentation algorithm to both synthetic and real images. We first present results on synthesized imagery, wherein we compare KSBP-based clustering of a single image with associated DP-based clustering. We then consider H-KSBP as applied to actual imagery, taken from a widely utilized database. The hyper-priors in the model for the examples are set as follows: Gamma priors, G (  X  10 and G (  X  30 ,  X  40 ) , for  X  and  X  with parameter  X  10 = 1 e  X  2 ,  X  normal-Wishart prior, N (  X  k |  X  0 ,  X  0  X  k ) W (  X  k conjugate to the Gaussian distribution with  X  0 = 0 ,  X  0 = 1 , w  X  = d + 2 ,  X   X  = 5  X  I ; the discrete priors for  X  and  X  with uniform weights over all candidates. The stick-breaking truncations are K = 40 , T = 40 . 4.1. Single image segmentation In this simple illustrative example, each feature vector is associated with a particular pixel, and the feature is simply a real number, corresponding to its intensity; the pixel lo-cation is the auxiliary information within the KSBP, while this information is not employed by the DP-based segmen-tation algorithm. Figure 2 shows the original image and the segmentation results of both algorithms. In Figure 2(a) we note that there are five contiguous regions for which the intensities are similar. There is a background region with a relatively fixed intensity, and within this are four distinct contiguous sub-regions, and of these there are pairs for which the intensities are comparable. The data in Fig-ure 2(a) were generated as follows. Each pixel in each re-gion is generated independently as a draw from a Gaussian distribution; the standard deviation of each of the Gaus-sians is 10, and the background has mean intensity 5, and the two pairs are generated with mean intensities of 40 and 60. The color bar in Figure 2(a) denotes the pixel ampli-tudes. The DP and KSBP segmentation results are shown in Figures 2(b) and 2(c), respectively. A distinct color is as-sociated with distinct cluster parameters. In the DP results we note that the four subregions are generally properly seg-mented, but there is significant speckle in the background region. The KSBP segmentation algorithm is beset by far less speckle. Further, in the KSBP results there are five distinct clusters (dominant KSBP sticks), where in the DP results there are principally three distinct sticks (in the DP, the spatially separated segments with the same features are treated as one cluster, while in the KSBP each contiguous region is represented by its own stick).
 In the next set of results, on real imagery, we employ the H-KSBP algorithm, and therefore at the task level segmen-tation is performed as in Figure 2(c). Alternatively, using the HDP model (Teh et al., 2005), at the task level one em-ploys clustering of the form in Figure 2(b). The relative performance of H-KSBP and HDP is analyzed. 4.2. H-KSBP applied to a set of real images
Within the subsequent image analysis we employ features constituted by the independent feature subspace analy-sis (ISA) technique, developed by Hyv  X  arinen and Hoyer (Hyv  X  arinen &amp; Hoyer, 2000). These features have proven to be relatively shift or translation invariant, which enables them to be widely applicable to many type of images.
We test the H-KSBP model on a subset of images from Microsoft Research Cambridge, available at http://research.microsoft.com/vision/cambridge/recognition/.
There are seven types of images used in this database: buildings, clouds, countryside, faces, fireworks, offices and urban. Twenty images are randomly selected from the database for each type, yielding a total of 140 images.
To capture textural information within the features, we first divided each image into a contiguous 24  X  24 -pixel non-overlapping patches (more than 70,000 patches in total) and then extract ISA features from each patch; color images are considered, and the RGB colors are handled within ISA feature extraction as in (Hoyer &amp; Hyv  X  arinen, 2000). Concerning learning the ISA independent feature subspaces, we randomly select 150 patches out of each of the 140 images from the seven classes, and these 150 image patches are used for basis training. The posterior on the H-KSBP (and HDP) model parameters is inferred based on the proposed MCVB algorithm, processing all 140 images simultaneously; as discussed in Section 2, the HDP analysis is performed by a special setting of the
H-KSBP parameters. To mitigate the influence of random samples and VB initialization, we perform the experiment ten times and report the average results.

Borrowing the successful  X  X ag of words X  assumption in text analysis (Blei &amp; Lafferty, 2005), we assume each im-age is a bag of atoms, which results in a measurable quan-tity of inter-relationship between images, specifically simi-lar images should share similar distribution over those mix-ture components. An important aspect of the H-KSBP al-gorithm is that while in text analysis the  X  X ag of words X  may be set a priori , here the  X  X ag of atoms X  is inferred from the data itself, within the clustering process. Related concepts have been employed previously in image analysis (Quelhas et al., 2007), but in that work one had to set the canonical set of image atoms (shapes) a priori , which is somewhat ad hoc .

As an example, for the data considered, we show one real-ization of H-KSBP in Figure 3. In the figure, we display canonical atom usage across all 140 images. Figure 3 is a count matrix, where each square represents the relative number of counts in a given image for a particular atom (atoms indexed along the vertical axis in Figure 3).
Figure 4 gives a representation of most of the atoms. For example the 4 -th, 31 -st and 39 -th atoms are associated with clouds and sky; the 38 -th atom is principally modeling buildings; and the 11 -th atom is associated with trees and grasses. While performing the experiment, we also noticed it was relatively easy to segment clouds, fireworks, coun-tryside, and urban images while harder to obtain contigu-ous segments within office images (these typically have far more details, and less large regions of smooth texture; this latter issue may be less an issue of the H-KSBP, but rather of the features employed). An example of this difficulty is observable in Figure 5, as office images are composed of many different atoms. Fortunately, the office images still tend to share similar usage of atoms so that they can be grouped together (sorted) when quantifying similarities between images based on the histogram over atoms (dis-cussed next).
 The results in Figure 5, in which both H-KSBP and HDP segmentation results are presented, demonstrate general properties observed when analyzing the images considered here: ( i ) the segmentation characteristics of HDP were gen-erally good, but on some occasions they were markedly worse (less detailed) than those of H-KSBP; and ( ii ) the H-KSBP was generally more sensitive to detailed textu-ral differences in the images, thereby generally inferring a larger number of principal atoms (increased number of large sticks). To demonstrate the image-sorting potential of the H-KSBP, we compute the Kullback-Leibler (KL) divergence on the histogram over atoms between any two images, by aver-aging histograms of the form in Figure 3 over ten random MCVB initializations. For each image, we rank its simi-larity to all other images based on the associated KL diver-gence. Performance is addressed quantitatively as follows. For each of the 140 images, we quantify via KL divergence its similarity to all other 139 images, wherein we achieve in ordered list. In Figure 6 we present a confusion ma-trix, which represents the fraction of the top-ten members of this ordered list that are within the same class (among seven classes) as the image under test. Figure 6. The confusion matrix over image types, generated using H-KSBP.
 As demonstrated in Figure 6, the H-KSBP performs well in distinguishing clouds, faces and fireworks images. The buildings and urban images often share some similar atoms, mainly representing buildings, and therefore these are somewhat confused (reasonably, it is felt). The offices im-ages are often related to other relatively complex scenes. Some typical image ranking results are given in Figure 7. It was found that the HDP produced similar sorting results as produced by H-KSBP ( e.g. , the associated confusion ma-trix for HDP is similar to that in Figure 6), and therefore the HDP sorting results are omitted here for brevity. This indicates that while in some cases the HDP segmentation results are inferior to those of H-KSBP, in general the abil-ity of HDP and H-KSBP to sort images is comparable (at least for the set of images considered).
 The H-KSBP results on the 140-image database were per-formed in non-optimized Matlab TM software, on a PC with 3 GHz CPU and 2 GB memory. It required about 3 hours to compute one run of the MCVB code for 80 iter-ations, with typically 40-50 iterations required to achieve convergence. The H-KSBP and HDP algorithms were run with comparable computation times. The kernel stick-breaking process has been extended for use in image segmentation. The algorithm explicitly im-poses the belief that feature vectors that are generated from proximate locations in an image are more likely to be as-sociated with the same image segment. We have also ex-tended the KSBP algorithm to the MTL setting, exploring the inter-relationship of images by sharing the same mix-ing components. Generally superior segmentation perfor-mance of H-KSBP was observed relative to HDP, when Figure 7. Sample image sorting results, as generated by H-KSBP. The top left image is the original image followed by the five most similar images and then the five most dissimilar images. segmenting multiple images simultaneously. In addition to segmenting multiple images, the H-KSBP and HDP algo-rithms also yield information about the inter-relationships between the images, based on the underlying sharing mech-anisms inferred among the associated clusters. For the im-ages considered, it was found that the H-KSBP and HDP yielded very similar sorting results.
 Beal, M. (2003). Variational algorithms for approximate
Bayesian inference . Doctoral dissertation, Gatsby Com-putational Neuroscience Unit, University College Lon-don.
 Blei, D., &amp; Jordan, M. (2004). Variational methods for the
Dirichlet process. Proc. the 21st International Confer-ence on Machine Learning .
 Blei, D., &amp; Lafferty, J. (2005). Correlated topic models. Advances in Neural Information Processing System . Ding, C., &amp; He, X. (2004). K-means clustering via princi-pal component analysis. Proc. the International Confer-ence on Machine Learning (pp. 225 X 232).
 Dunson, D., &amp; Park, J.-H. (2008). Kernel stick-breaking process. Biometrika .
 Ferguson, T. (1973). A bayesian analysis of some nonpara-metric problems. Annals of Statistics , 1 .
 Figueiredo, M., Cheng, D., &amp; Murino, V. (2007). Clus-tering under prior knowledge with application to image
Hoyer, P., &amp; Hyv  X  arinen, A. (2000). Independent compo-
Hyv  X  arinen, A., &amp; Hoyer, P. (2000). Emergence of phase-
Ishwaran, H., &amp; James, L. (2001). Gibbs sampling meth-MacEachern, S. (1999). Dependent nonparametric process. McLachlan, G., &amp; Basford, K. (1988). Mixture models:
Ng, A., Jordan, M., &amp; Weiss, Y. (2001). On spectral clus-
Quelhas, P., F. Monay, J.-M. O., Gatica-Perez, D., &amp; Tuyte-
Rasmussen, C. (2000). The infinite gaussian mixture
Sethuraman, J. (1994). A constructive definition of dirich-Sudderth, E. B., Torralba, A., Freeman, W. T., &amp; Willsky,
Teh, Y., Jordan, M., Beal, M., &amp; Blei, D. (2005). Hierar-
Thrun, S., &amp; O X  X ullivan, J. (1996). Discovering structure
Wei, G., &amp; Tanner, M. (1990). A monte carlo implemen-Xue, Y., Liao, X., Carin, L., &amp; Krishnapuram, B. (2007).
