 be trusted less than weights of frequent features.
 Confidence-weighted (CW) learning [6], motivated by PA learning, explicitly models classifier between weight uncertainty and prediction uncertainty, which unfortunately cannot be computed exactly. CW learning preserves the convenient computational properties of PA algorithms while weight updates that are more effective in practice [6, 5].
 We begin with a review of the CW approach, then show that the constraint can be expressed in a convex form, and solve it to obtain a new CW algorithm. We also examine a dual representation with a review of related work.  X  y where positive sign corresponds to a correct prediction.
  X  In the CW model, the traditional signed margin is the mean of the induced univariate Gaussian random variable This probabilistic model can be used for prediction in different ways. Here, we use the average covariance  X  is then used just to adjust training updates. The CW update rule of Dredze et al. [6] makes the smallest adjustment to the distribution that divergence to the previous distribution, yielding the following constrained optimization: They rewrite the above optimization in terms of the standard deviation as: min Unfortunately, while the constraint of this problem is linear in  X  , it is not convex in  X  . Dredze et al. [6, eq. (7)] circumvented that lack of convexity by removing the square root from is positive semidefinite (PSD), it can be written as  X  =  X  2 with  X  = Q diag(  X  1 / 2 yields the following convex optimization with a convex constraint in  X  and  X  simultaneously: We call our algorithm CW-Stdev and the original algorithm of Dredze et al. CW-Var. 3.1 Closed-Form Update While standard optimization techniques can solve the convex program (4), we favor a closed-form solution. Omitting the PSD constraint for now, we obtain the Lagrangian for (4), L = Input parameters a&gt; 0 ;  X   X  [0 . 5 , 1] Initialize  X  For i =1 , . . . , n Output Gaussian distribution N `  X  Figure 1: The CW-Stdev algorithm. The numbers in parentheses refer to equations in the text. At the optimum, it must be that where we assumed that  X  i is non-singular (PSD). At the optimum, we must also have, from which we obtain the implicit-form update as assumed above. 3.2 Solving for the Lagrange Multiplier  X  We start by computing the inverse of (9) using the Woodbury identity [14, Eq. 135] to get
 X  Let Multiplying (10) by x # solved for u i to obtain The KKT conditions for the optimization imply that either  X  =0 and no update is needed, or the in  X  :  X  2 v 2 i writing the larger root  X  i :  X  = 1 +  X  2 / 2;  X  = 1 +  X  2 . The larger root is then from (13) we have that  X  i &gt; 0 . If, instead, m i  X  0 , then, again by (13), we have summarize the discussion in the following lemma: of (3) is satisfied before the update with the parameters  X  We obtain the final form of  X  i by simplifying (13) together with Lemma 1, so, it does nothing. Otherwise it performs an update as described above. We initialize  X   X  1 = aI for some a&gt; 0 . The algorithm is summarized in Fig. 1.
  X  1 = aI , so the first-round constraint is y i ( w i  X  x i )  X  a % x i % original PA update.
 Second, the update described above yields full covariance matrices. However, sometimes we may prefer diagonal covariance matrices, which can be achieved by projecting the matrix  X  i +1 that only need to project x i x # as it can be computed directly element-wise. We use CW-Stdev (or CW-Stdev-full) to refer to the full-covariance algorithm, and CW-Stdev-diag to refer to the diagonal-covariance algorithm. Finally, the following property of our algorithm shows that it can be used with Mercer kernels: Theorem 2 (Representer Theorem) The mean  X  depend only on inner products of input vectors: The proof, given in the appendix, is a simple induction. tion and then we compute a bound on the number of mistakes that the algorithm makes. 4.1 Invariance to Initialization v , one may assume that a effects performance. In fact the number of mistakes is independent of a scale parameter a should be calibrated.
 Lemma 3 Fix a sequence of examples ( x 1 , y ( a =1 ). Let also  X   X  i ,  X   X  relations between the two set of quantities hold: these identities also hold for i +1 using Eqs. (9,14,11,12) .
 the choice of a . Therefore, we assume a =1 in what follows. 4.2 Analysis in the Mistake Bound Model The main theorem of the paper bounds the number of mistakes made by CW-Stdev. Theorem 4 Let ( x 1 , y with ( 0 ,I ) , with x i  X  R d and y which the algorithm made an update (  X  i &gt; 0 ), Then the following holds: Figure 2: (a) The average and standard deviation of the cumulative number of mistakes for seven algorithms. (c) Comparison between CW-Stdev-diag and CW-Var-diag on text classification. The proof is given in the appendix.
  X   X  from  X  i +1 * I above it follows that situation, the bound becomes have unit norm, whereas in the perceptron bound, the margin of 1 is for examples with arbitrary norm. This follows from the fact that (4) is invariant to the norm of x i . order perceptron (SOP) [3], CW-Var-diag, CW-Var-full [6], CW-Stdev-diag and CW-Stdev-full. All algorithm parameters were tuned over 1 , 000 runs.
 perform the first-order ones, which made at least 129 mistakes. Additionally, CW-Var makes more mistakes than CW-Stdev: 8% more in the diagonal case and 17% more in the full. The diagonal improved the first-order methods. The second-order methods outperform the first-order methods, feature weights should be near zero but without much confidence.
 NLP Evaluation: We compared CW-Stdev-diag with CW-Var-diag, which beat many state of the each algorithm; points above the line represent improvements of CW-Stdev over CW-Var. Stdev the effectiveness of our algorithm on real world data. Figure 3: Top : Plot of the two in-formative features of the synthetic data. Bottom: Feature weight dis-tributions of CW-Stdev-full after 50 examples.
 with example specific variance. The weighted-majority [12] algorithm and later improvements [2] weight vectors. analysis. Based on both synthetic and NLP experiments, we have shown that our method improves upon recent first and second order methods. Our method also improves on previous CW algorithms. We are now investigating special cases of CW-Stdev for problems with very large numbers of fea-tures, multi-class classification, and batch training.

