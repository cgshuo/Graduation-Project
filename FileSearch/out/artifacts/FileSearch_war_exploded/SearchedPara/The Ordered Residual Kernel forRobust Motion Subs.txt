 Multi-body motion segmentation concerns the separation of motions arising from multiple moving objects in a video sequence. The input data is usually a set of points on the surface of the objects which are tracked throughout the video sequence. Motion seg mentation can serve as a useful pre-processing step for many computer vision applications. In r ecent years the case of rigid (i.e. non-articulated) objects for which the motions could be semi-de pendent on each other has received much attention [18, 14, 19, 21, 22, 17]. Under this domain the affine projection model is usually adopted. Such a model implies that the point trajectories from a parti cular motion lie on a linear subspace of at most four, and trajectories from different motions lie on distinct subspaces. Thus multi-body motion segmentation is reduced to the problem of subspace se gmentation or clustering. To realize practical algorithms, motion segmentation appr oaches should possess four desirable at-tributes: (1) Accuracy in classifying the point trajectori es to the motions they respectively belong to. This is crucial for success in the subsequent vision appl ications, e.g. object recognition, 3D reconstruction. (2) Robustness against inlier noise (e.g. slight localization error) and gross outliers (e.g. mistracks, spurious trajectories), since getting im perfect data is almost always unavoidable in practical circumstances. (3) Ability to automatically deduce the number of motions in the data. This is pivotal to accomplish fully automated vision applicatio ns. (4) Computational efficiency. This is integral for the processing of video sequences which are usu ally large amounts of data. Recent work on multi-body motion segmentation can roughly b e divided into algebraic or factoriza-tion methods [3, 19, 20], statistical methods [17, 7, 14, 6, 1 0] and clustering methods [22, 21, 5]. No-table approaches include Generalized PCA (GPCA) [19, 20], a n algebraic method based on the idea that one can fit a union of m subspaces with a set of polynomials of degree m . Statistical methods of-ten employ concepts such random hypothesis generation [4, 1 7], Expectation-Maximization [14, 6] and geometric model selection [7, 8]. Clustering based meth ods [22, 21, 5] are also gaining atten-tion due to their effectiveness. They usually include a dime nsionality reduction step (e.g. manifold learning [5]) followed by a clustering of the point trajecto ries (e.g. via spectral clustering in [21]). A recent benchmark [18] indicated that Local Subspace Affini ty (LSA) [21] gave the best per-formance in terms of classification accuracy , although their result was subsequently surpassed by [5, 10]. However, we argue that most of the previous approa ches do not simultaneously fulfil the qualities desirable of motion segmentation algorithms . Most notably, although some of the ap-proaches have the means to estimate the number of motions, th ey are generally unreliable in this respect and require manual input of this parameter. In fact t his prior knowledge was given to all the methods compared in [18] 2 . Secondly, most of the methods (e.g. [19, 5]) do not explicit ly deal with outliers. They will almost always breakdown when given corr upted data. These deficiencies reduce the usefulness of available motion segmentation algorithm s in practical circumstances. In this paper we attempt to bridge the gap between experiment al performance and practical usability. Our previous work [2] indicates that robust multi-structur e model fitting can be achieved effectively with statistical learning. Here we extend this concept to mo tion subspace clustering. Drawing inspi-ration from robust statistical model fitting [4], we estimat e random hypotheses of motion subspaces in the data. However, instead of ranking these hypotheses we encapsulate them in a novel Mercer kernel. The kernel can function reliably despite overwhelming sampling imbalance, and it permits the application of non-linear dimensionality reduction te chniques to effectively identify and reject outlying trajectories. This is then followed by Kernel PCA [ 11] to maximize the separation between groups and spectral clustering [13] to recover the number of motions and clustering. Experiments on the Hopkins 155 benchmark dataset [18] show that our metho d is superior to other approaches in terms of the qualities described above, including computat ional efficiency. 1.1 Brief review of affine model multi-body motion segmentat ion multi-body motion segmentation the t fp  X  X  correspond to points on the surface of rigid objects which are moving. The goal is to separate the trajectories into gro ups corresponding to the motion they belong to. In other words, if we arrange the coordinates in th e following data matrix the goal is to find the permutation  X   X  R P  X  P such that the columns of T  X  are arranged according to the respective motions they belong to. It turns out that un der affine projection [1, 16] trajectories from the same motion lie on a distinct subspace in R 2 F , and each of these motion subspaces is of dimensions 2, 3 or 4. Thus motion segmentation can be accompl ished via clustering subspaces in R 2 F . See [1, 16] for more details. Realistically actual motion s equences might contain trajectories which do not correspond to valid objects or motions. These tr ajectories behave as outliers in the data and, if not taken into account, can be seriously detrimental to subspace clustering algorithms. First, we take a statistical model fitting point of view to mot ion segmentation. Let { x i } i =1 ,...,N be the set of N samples on which we want to perform model fitting. We randomly draw p -subsets from the data and use it to fit a hypothesis of the model, where p is the number of parameters that define the model. In motion segmentation, the x i  X  X  are the columns of matrix T , and p = 4 since the model is a four-dimensional subspace 3 . Assume that M of such random hypotheses are drawn. For each data point x i compute its absolute residual set r i = { r i 1 ,...,r i M } as measured to the M hypotheses. For motion segmentation, the residual is the or thogonal distance to a hypothesis subspace. We sort the elements in r i to obtain the sorted residual set  X  r i = { r i  X  i the permutation {  X  i 1 ,..., X  i M } is obtained such that r i  X  i as the sorted hypothesis set of point x i , i.e.  X   X  i depicts the order in which x i becomes the inlier of the M hypotheses as a fictitious inlier threshold is increased from 0 to  X  . We define the Ordered Residual Kernel (ORK) between two data points as where z t = 1 t are the harmonic series and Z = P M/h t =1 z t is the ( M/h ) -th harmonic number. Without lost of generality assume that M is wholly divisible by h . Step size h is used to obtain the Difference of Intersection Kernel (DOIK) where  X  t = t h and  X  t  X  1 = ( t  X  1) h . Symbol  X   X  a : b i indicates the set formed by the a -th to the b -th elements of  X   X  i . Since the contents of the sorted hypotheses set are merely p ermutations of { 1 ...M } , i.e. there are no repeating elements, model fitting problems. However, we concentrate on motion su bspaces in this paper.
 Let  X  be a fictitious inlier threshold. The kernel k  X  r captures the intuition that, if  X  is low, two points arising from the same subspace will have high normali zed intersection since they share many common hypotheses which correspond to that subspace. If  X  is high, implausible hypotheses fitted on outliers start to dominate and decrease the normalized in tersection. Step size h allows us to quantify the rate of change of intersection if  X  is increased from 0 to  X  , and since z t is decreasing, k will evaluate to a high value for two points from the same subs pace. In contrast, k  X  r is always low for points not from the same subspace or that are outliers.
 Proof of satisfying Mercer X  X  condition. Let D be a fixed domain, and P ( D ) be the power set of D , i.e. the set of all subsets of D . Let S  X  X  ( D ) , and p,q  X  S . If  X  is a measure on D , then called the intersection kernel, is provably a valid Mercer k ernel [12]. The DOIK can be rewritten as If we let D = { 1 ...M } be the set of all possible hypothesis indices and  X  be uniform on D , each term in Eq. (7) is simply an intersection kernel multiplied b y | D | /h . Since multiplying a kernel with a positive constant and adding two kernels respectivel y produce valid Mercer kernels [12], the DOIK and ORK are also valid Mercer kernels.  X  Parameter h in k  X  r depends on the number of random hypotheses M , i.e. step size h can be set as a ratio of M . The value of M can be determined based on the size of the p -subset and the size of the data N (e.g. [23, 15]), and thus h is not contingent on knowledge of the true inlier noise scale or threshold . Moreover, our experiments in Sec. 4 show that segmentation performance is relatively insensitive to the settings of h and M . 2.1 Performance under sampling imbalance Methods based on random sampling (e.g. RANSAC [4]) are usual ly affected by unbalanced datasets. The probability of simultaneously retrieving p inliers from a particular structure is tiny if points from that structure represent only a small minority in the da ta. In an unbalanced dataset the  X  X ure X  p -subsets in the M randomly drawn samples will be dominated by points from the m ajority structure in the data. This is a pronounced problem in motion sequences , since there is usually a background  X  X bject X  whose point trajectories form a large majority in t he data. In fact, for motion sequences from the Hopkins 155 dataset [18] with typically about 300 po ints per sequence, M has to be raised to about 20,000 before a pure p -subset from the non-background objects is sampled.
 However, ORK can function reliably despite serious samplin g imbalance. This is because points from the same subspace are roughly equi-distance to the samp led hypotheses in their vicinity, even though these hypotheses might not pass through that subspac e. Moreover, since z t in Eq. (3) is de-creasing only residuals/hypotheses in the vicinity of a poi nt are heavily weighted in the intersection. Fig. 1(a) illustrates this condition. Results in Sec. 4 show that ORK excelled even with M = 1 , 000 . In this section, we describe how ORK is used for multi-body mo tion segmentation. 3.1 Outlier rejection via non-linear dimensionality reduc tion Denote by F k  X  r the Reproducing Kernel Hilbert Space (RKHS) induced by k  X  r . Let matrix A = [  X  ( x 1 ) ... X  ( x N )] contain the input data after it is mapped to F k computed using the kernel function k  X  r as Since k  X  r is a valid Mercer kernel, K is guaranteed to be positive semi-definite [12]. Let K = Q X  X  T be the eigenvalue decomposition (EVD) of K . Then the rank-n Kernel Singular Value Decomposition (Kernel SVD) [12] of A is Via the Matlab notation, Q n = Q : , 1: n and  X  n =  X  1: n, 1: n . The left singular vectors U n is an orthonormal basis for the n -dimensional principal subspace of the whole dataset in F k  X  r . Projecting the data onto the principal subspace yields where B = [ b 1 ...b N ]  X  R n  X  N is the reduced dimension version of A . Directions of the principal subspace are dominated by inlier points, since k  X  r evaluates to a high value generally for them, but always to a low value for gross outliers. Moreover the kernel ensures that points from the same subspace are mapped to the same cluster and vice versa. Fig. 1 (b) illustrates this condition. Fig. 2(a)(left) shows the first frame of sequence  X  X ars10 X  fr om the Hopkins 155 dataset [18] with 100 false trajectories of Brownian motion added to the origi nal data (297 points). The corresponing RKHS norm histogram for n = 3 is displayed in Fig. 2(b). The existence of two distinct mode s, corresponding respectively to inliers and outliers, is evi dent. We exploit this observation for outlier rejection by discarding data with low norms in the principal subspace.
 The cut-off threshold  X  can be determined by analyzing the shape of the distribution . For instance we can fit a 1D Gaussian Mixture Model (GMM) with two component s and set  X  as the point of equal Mahalanobis distance between the two components. How ever, our experimentation shows that an effective threshold can be obtained by simply setting  X  as the average value of all the norms, i.e. This method was applied uniformly on all the sequences in our experiments in Sec. 4. Fig. 2(a)(right) shows an actual result of the method on Fig. 2(a)(left). 3.2 Recovering the number of motions and subspace clusterin g After outlier rejection, we further take advantage of the ma pping induced by ORK for recovering the number of motions and subspace clustering. On the remaining data, we perform Kernel PCA [11] to seek the principal components which maximize the varianc e of the data in the RKHS, as Fig. 1(b) removal, where N  X  &lt; N . Denote by C = [  X  ( y 1 ) ... X  ( y N  X  )] the data matrix after mapping the data The centered kernel matrix  X  K  X  =  X  C T  X  C [11] can be obtained as where K  X  = C T C is the uncentered kernel matrix, I s and 1 s,s are respectively the s  X  s identity matrix and a matrix of ones. If  X  K  X  = R X R T is the EVD of  X  K  X  , then we obtain first-m kernel principal components P m of C as the first-m left singular vectors of  X  C , i.e.
 where R m = R : , 1: m and  X  1: m, 1: m ; see Eq. (9). Projecting the data on the principal component s yields where D  X  R m  X  N  X  . The affine subspace span ( P m ) maximizes the spread of the centered data in the RKHS, and the projection D offers an effective representation for clustering. Fig. 3( a) shows the Kernel PCA projection results for m = 3 on the sequence in Fig. 2(a).
 The number of clusters in D is recovered via spectral clustering. More specifically we a pply the Normalized Cut (Ncut) [13] algorithm. A fully connected gra ph is first derived from the data, where its weighted adjacency matrix W  X  R N  X   X  N  X  is obtained as and  X  is taken as the average nearest neighbour distance in the Euclidean sense among the vectors in D . The Laplacian matrix [13] is then derived from W and eigendecomposed. Under Ncut, the number of clusters is revealed as the number of eigenvalu es of the Laplacian that are zero or numerically insignificant. With this knowledge, a subseque nt k -means step is then performed to cluster the points. Fig. 3(b) shows W for the input data in Fig. 2(a)(left) after outlier removal. It can be seen that strong affinity exists between points from th e same cluster, thus allowing accurate clustering. Figs. 3(a) and 3(c) illustrate the final cluster ing result for the data in Fig. 2(a)(left). There are several reasons why spectral clustering under our framework is more successful than previous methods. Firstly, we perform an effective outlier rejection step that removes bad trajectories that can potentially mislead the clustering. Secondly, the mapping induced by ORK deliberately separates the trajectories based on their cluster membersh ip. Finally, we perform Kernel PCA to maximize the variance of the data. Effectively this also imp roves the separation of clusters, thus facilitating an accurate recovery of the number of clusters and also the subsequent segmentation. This distinguishes our work from previous clustering based methods [21, 5] which tend to operate without maximizing the between-class scatter. Results in S ec. 4 validate our claims. Henceforth we indicate the proposed method as  X  X RK X . We leve rage on a recently published bench-mark on affine model motion segmentation [18] as a basis of com parison. The benchmark was eval-uated on the Hopkins 155 dataset 4 which contains 155 sequences with tracked point trajectori es. A total of 120 sequences have two motions while 35 have three m otions. The sequences contain degenerate and non-degenerate motions, independent and pa rtially dependent motions, articulated motions, nonrigid motions etc. In terms of video content thr ee categories exist: Checkerboard se-quences, traffic sequences (moving cars, trucks) and articu lated motions (moving faces, people). 4.1 Details on benchmarking Four major algorithms were compared in [18]: Generalized PC A (GPCA) [19], Local Subspace Affinity (LSA) [21], Multi-Stage Learning (MSL) [14] and RAN SAC [17]. Here we extend the benchmark with newly reported results from Locally Linear M anifold Clustering (LLMC) [5] and Agglomerative Lossy Compression (ALC) [10, 9]. We also comp are our method against Kanatani and Matsunaga X  X  [8] algorithm (henceforth, the  X  X M X  method ) in estimating the number of indepen-dent motions in the video sequences. Note that KM per se does n ot perform motion segmentation. For the sake of objective comparisons we use only implementa tions available publicly 5 . Following [18], motion segmentation performance is evalua ted in terms of the labelling error of the point trajectories, where each point in a sequence has a grou nd truth label, i.e. Unlike [18], we also emphasize on the ability of the methods i n recovering the number of motions . However, although the methods compared in [18] (except RANS AC) theoretically have the means to do so, their estimation of the number of motions is generally unrealiable and the benchmark results in [18] were obtained by revealing the actual number of motio ns to the algorithms. A similar initial-ization exists in [5, 10] where the results were obtained by g iving LLMC and ALC this knowledge a priori (for LLMC, this was given at least to the variant LLMC 4 m during dimensionality reduc-tion [5], where m is the true number of motions). In the following subsections , where variants exist for the compared algorithms we use results from the best perf orming variant.
 In the following the number of random hypotheses M and step size h for ORK are fixed at 1000 and 300 respectively, and unlike the others, ORK is not given knowle dge of the number of motions. 4.2 Data without gross outliers We apply ORK on the Hopkins 155 dataset. Since ORK uses random sampling we repeat it 100 times for each sequence and average the results. Table 1 depi cts the obtained classification error among those from previously proposed methods. ORK (column 9 ) gives comparable results to the other methods for sequences with 2 motions (mean = 7.83%, med ian = 0.41%). For sequences with 3 motions, ORK (mean = 12.62%, median = 4.75%) outperfor ms GPCA and RANSAC, but is slightly less accurate than the others. However , bear in mind that unlike the other methods ORK is not given prior knowledge of the true number of motions and ha s to estimate this independently. Table 1: Classification error (%) on Hopkins 155 sequences. R EF represents the reference/control method which operates based on knowledge of ground truth seg mentation. Refer to [18] for details. We also separately investigate the accuracy of ORK in estima ting the number of motions, and com-pare it against KM [8] which was proposed for this purpose. No te that such an experiment was not attempted in [18] since approaches compared therein gen erally do not perform reliably in esti-mating the number of motions. The results in Table 2 (columns 1 X 2) show that for sequences with two motions, KM (80.83%) outperforms ORK (67.37%) by  X  15 percentage points. However, for sequences with three motions, ORK (49.66%) vastly outperfo rms KM (14.29%) by more than dou-bling the percentage points of accuracy. The overall accura cy of KM (65.81%) is slightly better than ORK (63.37%), but this is mostly because sequences with two m otions form the majority in the dataset (120 out of 155). This leads us to conclude that ORK is actually the superior method here. Table 2: Accuracy in determining the number of motions in a se quence. Note that in the experiment with outliers (columns 3 X 4), KM returns a constant number of 3 motions for all sequences. We re-evaluate the performance of ORK by considering only re sults on sequences where the number of motions is estimated correctly by ORK (there are about 98  X  63.37% of such cases). The results are tabulated under ORK  X  (column 10) in Table 1. It can be seen that when ORK estimates t he number of motions correctly, it is significantly more accura te than the other methods. Finally, we compare the speed of the methods in Table 3. ORK wa s implemented and run in Matlab on a Dual Core Pentium 3.00GHz machine with 4GB of main memory (this is much less powerful than the 8 Core Xeon 3.66GHz with 32GB memory used in [18] for t he other methods in Table 3). The results show that ORK is comparable to LSA, much faster th an MSL and ALC, but slower than GPCA and RANSAC. Timing results of LLMC are not available in t he literature.
 4.3 Data with gross outliers We next examine the ability of the proposed method in dealing with gross outliers in motion data. For each sequence in Hopkins 155, we add 100 gross outliers by creating trajectories corresponding to mistracks or spuriously occuring points. These are creat ed by randomly initializing 100 locations in the first frame and allowing them to drift throughout the se quence according to Brownian motion. The corrupted sequences are then subjected to the algorithm s for motion segmentation. Since only ORK is capable of rejecting outliers, the classification err or of Eq. (16) is evaluated on the inlier points only. The results in Table 4 illustrate that ORK (colu mn 4) is the most accurate method by a large margin. Despite being given the true number of motions a priori, GPCA, LSA and RANSAC are unable to provide satisfactory segmentation results.
 Table 4: Classification error (%) on Hopkins 155 sequences wi th 100 gross outliers per sequence. In terms of estimating the number of motions, as shown in colu mn 4 in Table 2 the overall accu-racy of ORK is reduced to 48.13%. This is contributed mainly b y the deterioration in accuracy on sequences with two motions (47.58%), although the accuracy on sequences with three motions are maintained (50.00%). This is not a surprising result, since sequences with three motions generally have more (inlying) point trajectories than sequences with two motions, thus the outlier rates for se-quences with three motions are lower (recall that a fixed numb er of 100 false trajectories are added). On the other hand, the KM method (column 3) is completely over whelmed by the outliers X  for all the sequences with outliers it returned a constant  X 3 X  as the number of motions.
 We again re-evaluate ORK by considering results from sequen ces (now with gross outliers) where the number of motions is correctly estimated (there are abou t 75  X  48.13% of such cases). The results tabulated under ORK  X  (column 5) in Table 4 show that the proposed method can accura tely segment the point trajectories without being influenced by t he gross outliers. In this paper we propose a novel and highly effective approac h for multi-body motion segmenta-tion. Our idea is based on encapsulating random hypotheses i n a novel Mercel kernel and statistical learning. We evaluated our method on the Hopkins 155 dataset with results showing that the idea is superior other state-of-the-art approaches. It is by far th e most accurate in terms of estimating the number of motions, and it excels in segmentation accuracy de spite lacking prior knowledge of the number of motions. The proposed idea is also highly robust to wards outliers in the input data. Acknowledgements. We are grateful to the authors of [18] especially Ren  X e Vidal for discussions and insights which have been immensely helpful. [1] T. Boult and L. Brown. Factorization-based segmentatio n of motions. In IEEE Workshop on [2] T.-J. Chin, H. Wang, and D. Suter. Robust fitting of multip le structures: The statistical learning [3] J. Costeira and T. Kanade. A multibody factorization met hod for independently moving ob-[4] M. A. Fischler and R. C. Bolles. Random sample concensus: A paradigm for model fitting with [5] A. Goh and R. Vidal. Segmenting motions of different type s by unsupervised manifold clus-[6] A. Gruber and Y. Weiss. Multibody factorization with unc ertainty and missing data using the [7] K. Kanatani. Motion segmentation by subspace separatio n and model selection. In ICCV , [8] K. Kanatani and C. Matsunaga. Estimating the number of in dependent motions for multibody [9] Y. Ma, H. Derksen, W. Hong, and J. Wright. Segmentation of multivariate mixed data via lossy [10] S. Rao, R. Tron, Y. Ma, and R. Vidal. Motion segmentation via robust subspace separation in [11] B. Sch  X olkopf, A. Smola, and K. R. M  X uller. Nonlinear co mponent analysis as a kernel eigen-[12] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis . Cambridge University [13] J. Shi and J. Malik. Normalized cuts and image segmentat ion. TPAMI , 22(8):888 X 905, 2000. [14] Y. Sugaya and K. Kanatani. Geometric structure of degen eracy for multi-body motion seg-[15] R. Toldo and A. Fusiello. Robust multiple structures es timation with J-Linkage. In ECCV , [16] C. Tomasi and T. Kanade. Shape and motion from image stre ams under orthography. IJCV , [17] P. Torr. Geometric motion segmentation and model selec tion. Phil. Trans. Royal Society of [18] R. Tron and R. Vidal. A benchmark for the comparison of 3-D motion segmentation algo-[19] R. Vidal and R. Hartley. Motion segmentation with missi ng data by PowerFactorization and [20] R. Vidal, Y. Ma, and S. Sastry. Generalized Principal Co mponent Analysis (GPCA). TPAMI , [21] J. Yan and M. Pollefeys. A general framework for motion s egmentation: independent, articu-[22] L. Zelnik-Manor and M. Irani. Degeneracies, dependenc ies and their implications on multi-[23] W. Zhang and J. Koseck  X a. Nonparametric estimation of m ultiple structures with outliers. In
