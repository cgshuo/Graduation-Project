 Knowledge Bases (KBs), such as Wikipedia, have shown great power in many applications such as question answering, entity linking and entity retrieval. How-ever, most KBs are maintained by human editors, which are hard to keep up-to-date because of the wide gap between limit number of editors and millions of entities. As reported in [ 8 ], the median time lag between the emerging date of a cited article and the date of the citation created in Wikipedia is almost one year. Moreover, some less popular entities cannot attract enough attentions from edi-tors, making the maintenance more challenging. These burdens can be released if relevant documents are automatically detected as soon as they emerge and then recommended to the editors with certain relevance estimation. This task is studied as Cumulative Citation Recommendation (CCR) in Text REtrieval Con-ference (TREC) Knowledge Base Acceleration (KBA) 1 Track. CCR is defined as filtering a temporally-ordered stream corpus for documents related to a prede-fined set of entities in KBs and estimating the relevance levels of entity-document pairs. In this paper, we follow the 4-point scale relevance settings of TREC-KBA-2013, as illustrated in Table 1 . We consider two scenarios in CCR: (i) vital + useful : detecting relevant ( vital and useful ) documents, and (ii) vital only : detecting vital documents only.
 There are two challenges in CCR. (1) It is too time-consuming to process all the documents in the stream corpus. To guarantee the coverage and diversity of the recommendation, a real-life CCR system need include as many documents as possible to filter relevant ones for target entities. (2) Besides, according to current definitions, it is difficult to figure out the explicit differences between the four relevance levels, especially for vital and useful . Moreover, in the ground truth of TREC-KBA-2013 dataset, there also exist many inconsistent annotations due to the vague boundaries between relevance levels.
 In this paper, we focus on the aforementioned two challenges. For the former one, we implement an entity-centric query expansion method to filter the corpus and discard irrelevant documents as many as possible beforehand. For the latter one, given an entity E , we resort extra evidences to enrich the definitions of rel-evance levels, including the profile page of E , existing citations for E in KB and some temporal signals. Entities X  profile pages are helpful in entity disambigua-tion since they contain rich information about the target entities. Moreover, most KB entities already possess several citations which are cited into KB because of their relatedness to the target entities. In our opinion, these two evidences are helpful to differentiate relevant and irrelevant documents. However, it is not enough to detect vital documents accurately merely with these two evidences because vital documents are required to contribute some novel knowledge about the target entity. Hence we introduce temporal signals to support the detection of vital documents. For an KB entity, the occurrences of its relevant documents, especially vital documents, are not distributed in the stream corpus evenly along a timeline. When some significant event related to it happens, an entity is spot-lighted in a short time period during which its vital documents emerge heav-ily and exceed the average frequency of occurrence. Therefore, the difficulty of detecting vital documents can be reduced as along as we detect these  X  X ursty X  time periods. Accordingly, we propose the following assumptions as supplements to the definitions of relevance levels. 1. The relevant (i.e., vital and useful ) documents are more related to the profile 2. The relevant documents occurring in the  X  X ursty X  time periods where the Under the guidance of the refined definitions, we then undertake a comparison study on classification and learning to rank approaches to CCR.
 definitions of relevance levels of CCR. Under the refined definitions, all exper-imental approaches outperform their own baselines. Besides, the classification approach performs best and achieve the state-of-the-art performance on TREC-KBA-2013 dataset. The proposed assumptions are proved to be reasonable and effective. In addition, the experimental results show that our filtering method is effective, making the whole procedure more efficient and practical. Entity Linking. Entity linking describes the task of linking a textural name of an entity to a knowledge base entry. In [ 5 ], cosine similarity was utilized to rank candidate entities based on the relatedness of the context of an entity mention to a Wikipedia article. [ 6 ] proposed a large-scale named entity disambiguation approach through maximizing the agreement between the contextual informa-tion from Wikipedia and the context of an entity mention. [ 14 ] implemented a system named WikiFy! to identify the important concepts in a document and automatically linked these concepts to the corresponding Wikipedia pages. Similarly, [ 11 ] parsed the Wikipedia to extract a concept graph, measuring the similarity by means of the distance of co-occurring terms to candidate concepts. Besides identifying entity mentions in documents, CCR is required to evaluate the relevance levels between the document and the mentioned entity.
 Knowledge Base Acceleration. TREC KBA has hosted CCR track since 2012. In the past tracks, there were three mainstream approaches submitted by the par-ticipants: query expansion [ 13 , 17 ], classification, such as SVM [ 12 ] and Random filtering method is implemented through calculating the similarity of word co-occurring graphs between an entity X  X  profile and a document. Transfer learning is employed to transfer the keyword importance learned from training entities to query entities in [ 18 ]. All these work focus on utilizing the training data ade-quately to improve the performance of a CCR system, yet the key contribution of our work is the improvement of the definitions of relevance levels of CCR. With the help of the refined definitions, we can differentiate the relevant and irrelevant documents more accurately. Givenanentity E from a KB (e.g., Wikipedia or Twitter) and a document D , our goal is to generate a confidence score r ( E, D )  X  (0 , 1000] for each document-entity pair, representing the citation-worthiness of D to E . The higher r ( E, D ) is, the more likely D can be considered as a citation for E .
 The TREC-KBA-2013 dataset is utilized as our experimental dataset, which consists of a stream corpus and a predefined target entity set. The stream cor-pus , is a time-ordered document collection and contains nearly 1 billion doc-uments published between Oct. 2011 and Feb. 2013. The target entity set is composed of 141 entities: 121 Wikipedia entities and 20 Twitter entities. These entities consist of 98 persons, 24 facilities and 19 organizations. The documents from Oct. 2011 to Feb. 2012 are annotated as training data and the remain-der from Mar. 2012 to Feb. 2013 as testing data. Each document-entity pair is annotated as one of the four relevance levels. In previous study [ 8 ], most relevant documents mention the target entity explic-itly. A na  X   X ve filtering strategy is to retain the documents mentioning a target entity name at least once and discard those without mentioning any target entity. Nevertheless, this strategy has two disadvantages. (1) It misses lots of relevant documents for entities with aliases, because entities can be referred by different surface forms in documents. (2) It cannot differentiate two entities with the same name. For example, when we filter the stream corpus for  X  Michael Jordan  X , a machine learning scientist, the filtering results also include documents referring to the basketball player  X  Michael Jordan  X .
 An entity-centric query expansion method is proposed to address the above problems. We first expand as many reliable surface forms as possible for each target entity. Then we extract related entities as expansion terms. Finally, we expand the na  X   X ve filtering query (i.e., entity name) with the surface forms and these related entities. Surface Form Expansion. For a Wikipedia entity, we treat the redirect names as its surface forms. For a Twitter entity, we extract its display name as its surface form. Take @AlexJoHamilton as an example, we extract its display name Alexandra Hamilton as a surface form.
 Query Expansion. We first construct a document collection for each target entity E , denoted as C ( E ). C ( E ) is composed of documents including the profile page of E , existing citations for E in KB and the documents labeled as vital and useful for E in training data. Then we perform named entity recognition for each document d  X  C ( E ) and obtain a related entity set R ( E )for E . The related entities in R ( E ) are expansion terms we leveraged to improve the na  X   X ve filtering query. The relevance score between a document d referring to E is estimated as: where occ ( d, e ) is the number of occurrences of related entity e in d . w ( E, e ) serves as the prior weight of e to E .Hereweuse e  X  X  IDF (inverse document frequency) in C ( E ) to estimate its prior importance to E . where | C ( E ) | is the number of documents in C ( E ), and number of documents containing related entity e . Through setting a relevance threshold, the documents with low relevance scores are removed from the stream corpus, and the remaining documents compose a candidate relevant document collection RC ( E ). In the following work, we perform all experiments on RC ( E ) instead of the entire stream corpus for the sake of efficiency. 5.1 Relevance Evidences Profile. Each KB entity possesses a profile page either in Wikipedia or on Twitter. The profile page can be leveraged to differentiate relevant and irrele-vant documents for an entity. We make an assumption that relevant documents are more similar with profile page in comparison to irrelevant ones. We verify the assumption in various approaches. In query expansion, we extract named entities from the profile page as expansion terms for a target entity. In super-vised approaches, we develop some similarity features. In Wikipedia, entities X  profile pages are usually organized as different sections. Each section introduces a specific aspect of the target entity. Relevant documents for a target entity are possibly highly related with a few of these sections rather than all of them. Therefore, we calculate similarities between the documents and different sections respectively instead of the whole profile page. These features are listed in the second block of Table 2 , denoted as profile features. Citations. There usually exists a list of supplementary citations for an entity in Wikipedia. These existing citations are extremely valuable in identifying relevant documents. In the refined definitions of relevance levels, we assume that relevant documents are more similar with existing citations than irrelevant ones. We also verify it in various approaches. In query expansion, named entities are extracted from the existing citations as expansion terms. In supervised approaches, the similarity between each citation and the document is utilized as training fea-tures. These features are listed in the third block of Table 2 , denoted as citation features.
 Temporal Signals. CCR is to filter relevant documents from a temporally-ordered stream corpus and entities are evolving with the passage of time. How-ever, semantic features cannot capture the dynamic characteristics of entities in the stream corpus. Temporal signals have been considered to make up for this deficiency [ 2 , 4 , 17 ].
 The view statistics 4 of an entity X  X  Wikipedia page is adopted as a useful signal to capture if something important to the entity happens during a given time period. Nevertheless, Twitter entities do not have off-the-shelf daily view statistics as Wikipedia entities. Alternatively, we employ Google Trends tics, which shows how often a particular search term is entered relative to the total search-volume. Based on these statistics, we can capture temporal signals for each entity by detecting its bursy activities.
 For an entity E , we have a daily view/search statistics sequence v = ( v ,v (MA) method [ 16 ]. More concretely, for each item v i in v , 1. Calculate moving average sequence of length w as 2. Calculate cutoff c ( i ) based on previous MA sequence Pre ( MA w (1) ,  X  X  X  ,MA w ( i )) as 3. Detect bursty day sequence d , where d = { i | MA w ( i ) &gt;c ( i ) 4. Calculate daily bursty weights The moving average length can be varied to detect long-term or short-term bursts. We set the moving average length as 7 days (i.e., w = 7). The cutoff value is empirically set as 2 times the standard deviation of the MA (i.e.,  X  = 2). Moreover, we compact the consecutive days in d into bursty periods. The bursty weight for each period is calculated as the average weight of all the bursts in this period.
 represent their temporal relation. Let t be the timestamp of D .If t is in one of E  X  X  bursty periods, denoted as [ t start ,t end ], then b ( D, E ) is calculated by Equation 3 . Otherwise, b ( D, E ) would be set as 0. documents appear at the beginning of a burst are more informative than those appear at the end. 5.2 Approaches Query Expansion. We consider query expansion as one baseline, since each document obtains a relevance score (i.e., rel ( D, E )) after the filtering step as described in Section 4 . The relevance score can be converted to the confidence score conveniently.
 Classification. CCR is usually considered as a binary classification task which categories documents into different relevance levels. In our internal test, random forests outperforms other classifiers, such as SVM and logistic regression. So we employ random forests classifier in our experiments. Two classifiers are built with different features, denoted as Class and Class+ respectively. Class is built with the basic features in Table 2 , while Class+ is built with the whole features in Table 2 .
 Learning to Rank. If we consider the different relevance levels as an ordered sequence, i.e., vital &gt; usef ul &gt; neutral &gt; garbage , CCR becomes a learning to rank (LTR) task. Two ranking models are built, denoted as Rank and Rank+ respectively. Rank is a LTR model built with the basic features in Table 2 , while Rank+ is a LTR model built with the whole features in Table 2 . 6.1 Evaluation Metrics A CCR system is fed with documents in a chronological order and outputs a con-fidence score in the range of (0,1000] for each document-entity pair. There are two metrics to evaluate the performance: max( F ( avg ( P ) ,avg ( R ))) and max( SU ). Scaled Utility (SU) is a metric introduced in filtering track to evaluate the abil-ity of a system to separate relevant and irrelevant documents in a stream [ 15 ]. A cutoff value is varied from 0 to 1000 with some step size and the documents with the scores above the cutoff are treated as positive instances. Correspond-ingly, the documents with the scores below the cutoff are negative instances. The primary metric max( F ( avg ( P ) ,avg ( R ))) is calculated as follows: given a cutoff c and an entity E i , P i ( c )and R i ( c ) are calculated respectively, then macro-average them in all entities, avg ( P )= where N represents the quantities of entities in the target entity set. Therefore, F is actually a function of the relevance cutoff c , and we select the maximum F to evaluate the overall performance of a CCR system. In a similar manner, max( SU ) is calculated as an auxiliary metric. 6.2 Filtering Evaluation This section evaluates the filtering performance of our entity-centric QE method in comparison to the official baseline of TREC-KBA-2013 track, which is a base-line that annotators manually generate a list of reliable queries of each entity for filtering. We calculate the best recall of different methods through setting the cutoff as 0, in which case all remaining documents after filtering are considered as positive instances. Meanwhile, corresponding precision, F 1and SU measures are taken into account. The results are reported in Table 3 .
 From the perspective of SU , our entity-centric QE method achieves the best filtering results in both scenarios. Moreover, in vital only scenario, the QE method outperforms the baseline on all metrics. In vital + useful scenario, although recall of our QE method is not best, it achieves the overall best F 1 and SU . This reveals that our filtering method can comprise between precision and recall to reach an overall optimal filtering performance.
 6.3 Relevance Estimation Evaluation Results and Discussion. All the results of our approaches are reported in Table 4 . For reference, the official baseline and two top-ranked approaches in TREC-KBA-2013 track are also included. The official baseline assigns a  X  vita l X  rating to each document that matches a surface form of an entity and estimates a confidence score based on the length of the observed name [ 9 ]. The UMass approach derives a sequential dependent retrieval model which scores documents by frequency of unigrams, bigrams and windowed bigrams of the target entity name [ 7 ]. The UDel approach is a query expansion approach, which tunes the weights of the expansion terms with training data [ 13 ]. Figure 1 illustrates the macro-averaged recall and precision measures of the approaches listed in Table 4 . The parallel curves are contour lines of max( F ( avg ( P ) ,avg ( R ))) measure. The approaches in upper right perform better than the lower left ones.
 approaches cannot perform as well as the official baseline. In vital only , UMass approach beats the official baseline less than 1% on overall max( F ). Neverthe-less, all our approaches outperform the official baseline notably. Moreover, our approaches outperform the others on separate measures for Wikipedia and Twit-ter entities respectively.
 approaches in both scenarios. Even the classification and LTR approaches merely utilizing the basic feature set, i.e., UniClass and UniRank , achieve compara-tive performance with unsupervised approaches. After being augmented with the supplemental features, supervised approaches achieve more promising results. As illustrated in 1(a) and 1(b) , though our QE approach can achieve high recall among all approaches, the precision is not satisfactory. This may be resulted from our equally weighting strategy for expansion terms. UDel approach weights expansion terms with the help of training data and achieves higher pre-cision than our QE approach in both scenarios. We believe our query expansion approach can be improved by introducing better weighting strategies.
 UniClass+ outperforms UniClass on all metrics prominently. Since the only difference is whether to include 3 extra types of features developed from relevance evidences, the new features are indeed helpful in CCR. Furthermore, according to Figure 1 , these features can improve both precision and recall of classification approaches. Similar to classification approaches, in both scenarios, UniRank+ , achieves better results than its baseline UniRank . 6.4 Feature Analysis In this section, we concentrate on UniClass+ and explore the impacts of the proposed features in two scenarios. We perform a fine-grained feature analysis with the help of information gain (IG). Table 5 demonstrates the IG values of the proposed features in different scenarios. The maximum , mean and median IGs achieved by basic features are also reported for reference (i.e., max , mean , min in Table 5 ).
 Generally, the three types of proposed features perform better than the basic features in both scenarios. As we have stated, CCR is not just a text classification task, so basic text features are not so significant as the developed feavotures accord-ing to the refined definitions. The temporal feature, i.e., burst value works best in vital only scenario, verifying our assumption that vital documents are more likely to appear in  X  X ursty X  periods of an entity. The temporal feature also works consid-erably well in vital + useful scenario, revealing that useful documents distribute depending on vital documents to a certain extent. This phenomenon inspires us to explore more precise boundary between vital and useful in future.
 The objective of CCR is filtering vitally relevant documents from a huge time-ordered stream corpus and then separating them into different relevance levels. The key challenge is how to classify candidate documents into different relevance levels accurately due to the unclear definitions of them. Apart from the labeled data, we proposed two assumptions based on three relevance evidences to enrich the definitions. The relevance evidences include the profiles of KB entities, exist-ing citations in KB and temporal signals. Under the guidance of the refined def-initions, we investigated three mainstream CCR approaches in two scenarios. The experimental results validated the effectiveness of our assumptions. In addi-tion, we developed an entity-centric query expansion method to filter the volume stream corpus and reduce the amount of documents into a considerable size.
