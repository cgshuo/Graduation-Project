 Graduate School of Informatics, Kyoto University Department of Statistics, University of Oxford Comparing, summarizing and reducing the dimensionality of empirical probability measures defined on a space  X  are fundamental tasks in statistics and machine learning. Such tasks are usually carried out using pairwise comparisons of measures. Classic information divergences (Amari and Na-gaoka, 2001) are widely used to carry out such comparisons. Unless  X  is finite, these divergences cannot be directly ap-plied to empirical measures, because they are ill-defined for measures that do not have continuous densities. They also fail to incorporate prior knowledge on the geometry of  X  ,whichmightbeavailableif,forinstance,  X  is also aHilbertspace.Bothoftheseissuesareusuallysolvedus-ing Parzen X  X  approach (1962) to smooth empirical measures with smoothing kernels before computing divergences: the Euclidean (Gretton et al., 2007) and  X  2 distances (Harchaoui et al., 2008), the Kullback-Leibler and Pearson divergence s (Kanamori et al., 2012a;b) can all be computed fairly effi-ciently by considering matrices of kernel evaluations. The choice of a divergence defines implicitly the mean ele-ment, or barycenter, of a set of measures, as the particular measure that minimizes the sum of all its divergences to that set of target measures (Veldhuis, 2002; Banerjee et al., 200 5; Teboulle, 2007; Nielsen, 2013). The goal of this paper is to compute efficiently barycenters (possibly in a constrained subset of all probability measures on  X  )definedbythe op-timal transport distance between measures (Villani, 2009,  X  6). We propose to minimize directly the sum of optimal transport distances from one measure (the variable) to a set of fixed measures by gradient descent. These gradients can be computed for a moderate cost by solving smoothed opti-mal transport problems as proposed by Cuturi (2013). Wasserstein distances have many favorable properties, doc -umented both in theory (Villani, 2009) and practice (Rubner et al., 1997; Pele and Werman, 2009). We argue that their versatility extends to the barycenters they define. We illus -trate this intuition in Figure 1, where we consider 30 images of nested ellipses on a 100  X  100 grid. Each image is a dis-crete measure on [0 , 1] 2 with normalized intensities. Com-puting the Euclidean, Gaussian RKHS mean-maps or Jef-frey centroid of these images results in mean measures that hardly make any sense, whereas the 2-Wasserstein mean on that grid (defined in  X  3.1) produced by Algorithm 1 captures perfectly the structure of these images. Note that these re-sults were recovered without any prior knowledge on these images other than that of defining a distance in [0 , 1] 2 the Euclidean distance. Note also that the Gaussian kernel smoothing approach uses the same distance, in addition to a bandwidth parameter  X  which needs to be tuned in practice. This paper is organized as follows: we provide background on optimal transport in  X  2, followed by the definition of Wasserstein barycenters with motivating examples in  X  3. Novel contributions are presented from  X  4: we present two subgradient methods to compute Wasserstein barycenters, one which applies when the support of the mean measure is known in advance and another when that support can be freely chosen in  X  .Thesealgorithmsareverycostly even for measures of small support or histograms of small size. We show in  X  5thatthekeyingredientsofthese approaches X  X he computation of primal and dual optimal transport solutions X  X an be bypassed by solving smoothed optimal transport problems. We conclude with two applica-tions of our algorithms in  X  6. Let  X  be an arbitrary space, D ametriconthatspaceand P (  X  ) the set of Borel probability measures on  X  .Forany point x  X   X  ,  X  x is the Dirac unit mass on x .
 Definition 1 (Wasserstein Distances) . For p  X  [1 ,  X  ) and probability measures  X ,  X  in P (  X  ) ,their p -Wasserstein dis-tance (Villani, 2009,  X  6) is where  X  (  X ,  X  ) is the set of all probability measures on  X  that have marginals  X  and  X  . 2.1. Restriction to Empirical Measures We will only consider empirical measures throughout this paper, that is measures of the form  X  = n is an integer, X =( x 1 ,...,x n )  X   X  n and ( a 1 ,...,a lives in the probability simplex  X  n , Let us introduce additional notations: Measures on a Set X with Constrained Weights. Let  X  be a non-empty closed subset  X  of  X  n .Wewrite Measures supported on up to k points. Given an integer k and a subset  X  of  X  k ,weconsidertheset P k (  X  ,  X  ) of measures of  X  that have discrete support of size up to k and weights in  X  , When no constraints on the weights are considered, namely when the weights are free to be chosen anywhere on the probability simplex, we use the shorter notations P ( X ) P ( X,  X  n ) and P k (  X  ) def = P k (  X  ,  X  k ) . 2.2. Wasserstein &amp; Discrete Optimal Transport Consider two families X =( x 1 ,...,x n ) and Y = ( y 1 ,...,y m ) of points in  X  .When  X  =  X  = tween  X  and  X  is the p th root of the optimum of a network flow problem known as the transportation problem (Bertsi-mas and Tsitsiklis, 1997,  X  7.2). This problem builds upon two elements: the matrix M XY of pairwise distances be-tween elements of X and Y raised to the power p ,which acts as a cost parameter, and the transportation polytope U ( a, b ) of a  X   X  n and b  X   X  m ,whichactsasafeasibleset,definedasthesetof n  X  m nonnegative matrices such that their row and column marginals are equal to a and b respectively. Writing 1 n for the n -dimensional vector of ones, Let  X  A, B  X  def =tr( A T B ) be the Frobenius dot-product of matrices. Combining Eq. (1) &amp; (2), we have that W p (  X ,  X  )  X  X he distance W p (  X ,  X  ) raised to the power p  X  can be written as the optimum of a parametric linear pro-gram p on n  X  m variables, parameterized by the marginals a, b and a (cost) matrix M XY : W p p (  X ,  X  )= p ( a, b, M XY ) def = min We present in this section the Wasserstein barycenter prob-lem, a variational problem involving all Wasserstein dis-tances from one to many measures, and show how it encom-passes known problems in clustering and approximation. 3.1. Definition and Special Cases Definition 2 (Agueh and Carlier, 2011) . AWasserstein barycenter of N measures {  X  1 ,...,  X  N } in P  X  P (  X  ) is aminimizerof f over P ,where Agueh and Carlier consider more generally a non-negative weight  X  i in front of each distance W p p (  X ,  X  i ) .Thealgo-rithms we propose extend trivially to that case but we use uniform weights in this work to keep notations simpler. We highlight a few special cases where minimizing f over aset P is either trivial, relevant to data analysis and/or has been considered in the literature with different tools or un der adifferentname.Inwhatfollows X  X   X  n and Y  X   X  m are arbitrary finite subsets of  X  .  X  N =1 , P = P ( X ) When only one measure  X  ,supported on Y  X   X  m is considered, its closest element  X  in P ( X )  X  X f no constraints on weights a are given X  X an be computed by defining a weight vector a on the elements of X that results from assigning all of the mass b i to the closest neighbor in metric D of y i in X .  X  Centroids of Histograms : N&gt; 1 ,  X  finite, P = P (  X  ) . When  X  is a set of size d and a matrix M  X  R d  X  d + describes the pairwise distances between these d points (usually called in that case bins or features), the 1 -Wasserstein distance is known as the Earth Mover X  X  Distance (EMD) (Rubner et al., 1997). In that context, Wasserstein barycenters have also been called EMD prototypes by Zen and Ricci (2011).  X  Euclidean  X  : N =1 , D ( x, y )= * x  X  y * 2 ,p =2 , P = P (  X  ) .Minimizing f on P k (  X  ) when (  X  ,D ) is a Euclidean metric space and p =2 is equivalent to the k -means problem (Pollard, 1982; Canas and Rosasco, 2012).  X  Constrained k -Means : N =1 , P = P k (  X  , { 1 k /k } ) . Consider a measure  X  with support Y  X   X  m and weights b  X   X  m .Theproblemofapproximatingthismea-sure by a uniform measure with k atoms X  X  measure in P (  X  , { 1 k /k } )  X  X n 2 -Wasserstein sense was to our knowl-edge first considered by Ng (2000), who proposed a variant of Lloyd X  X  algorithm (1982) for that purpose. More recently , Reich (2013) remarked that such an approximation can be used in the resampling step of particle filters and proposed in that context two ensemble methods inspired by optimal transport, one of which reduces to a single iteration of Ng X  X  algorithm. Such approximations can also be obtained with kernel-based approaches, by minimizing an information di-vergence between the (smoothed) target measure  X  and its (smoothed) uniform approximation as proposed recently by Chen et al. (2010) and Sugiyama et al. (2011). 3.2. Recent Work Agueh and Carlier (2011) consider conditions on the  X  i  X  X  for a Wasserstein barycenter in P (  X  ) to be unique using the multi-marginal transportation problem. They provide solu -tions in the cases where either (i)  X  = R ;(ii) N =2 using McCann X  X  interpolant (1997); (iii) all the measures  X  i are Gaussians in  X  = R d ,inwhichcasethebarycenterisa Gaussian with the mean of all means and a variance matrix which is the unique positive definite root of a matrix equa-tion (Agueh and Carlier, 2011, Eq.6.2).
 Rabin et al. (2012) were to our knowledge the first to con-sider practical approaches to compute Wasserstein barycen -ters between point clouds in R d .Todoso,Rabinetal. (2012) propose to approximate the Wasserstein distance be-tween two point clouds by their sliced Wasserstein distance, the expectation of the Wasserstein distance between the pro -jections of these point clouds on lines sampled randomly. Because the optimal transport between two point clouds on the real line can be solved with a simple sort, the sliced Wasserstein barycenter can be computed very efficiently, us -ing gradient descent. Although their approach seems very effective in lower dimensions, it may not work for d  X  4 and does not generalize to non-Euclidean metric spaces. We propose in this section new approaches to compute Wasserstein barycenters when (i) each of the N measures  X  i is an empirical measure, described by a list of atoms Y i  X   X  m i of size m i  X  1 ,andaprobabilityvector b i in the simplex  X  m i ;(ii)thesearchforabarycenterisnotconsid-ered on the whole of P (  X  ) but restricted to either P ( X,  X  ) (the set of measures supported on a predefined finite set X of size n with weights in a subset  X  of  X  n )or P k (  X  ,  X  ) (the set of measures supported on up to k atoms with weights in asubset  X  of  X  k ).
 Looking for a barycenter  X  with atoms X and weights a is equivalent to minimizing f (see Eq. 3 for a definition of p ), over relevant feasible sets for a and X .When X is fixed , we show in  X  4.1 that f is convex w.r.t a regardless of the properties of  X  .Asubgradientfor f w.r.t a can be re-covered through the dual optimal solutions of all problems p ( a, b i ,M XY i ) ,and f can be minimized using a projected subgradient method outlined in  X  4.2. If X is free ,con-strained to be of cardinal k ,and  X  and its metric D are both Euclidean ,weshowin  X  4.4 that f is not convex w.r.t X but we can provide subgradients for f using the primal optimal solutions of all problems p ( a, b i ,M XY i ) .Thisinturnsug-gests an algorithm to reach a local minimum for f w.r.t. a and X in P k (  X  ,  X  ) by combining both approaches. 4.1. Differentiability of p ( a, b, M XY ) w.r.t a Dual transportation problem. Given a matrix M  X  R n  X  m ,theoptimum p ( a, b, M ) admits the following dual Linear Program (LP) form (Bertsimas and Tsitsiklis, 1997,  X  7.6,  X  7.8), known as the dual optimal transport problem: where the polyhedron C M of dual variables is By LP duality, d ( a, b, M )= p ( a, b, M ) .Thedualoptimal solutions X  X hich can be easily recovered from the primal optimal solution (Bertsimas and Tsitsiklis, 1997, Eq.7.10 ) X  define a subgradient for p as a function of a : Proposition 1. Given b  X   X  m and M  X  R n  X  m ,themap a , X  p ( a, b, M ) is a polyhedral convex function. Any optimal dual vector  X  % of d ( a, b, M ) is a subgradient of p ( a, b, M ) with respect to a .
 Proof. These results follow from sensitivity analysis in LP X  X  (Bertsimas and Tsitsiklis, 1997,  X  5.2). d is bounded and is also the maximum of a finite set of linear functions, each indexed by the set of extreme points of C M ,evaluatedat a and is therefore polyhedral convex. When the dual optimal vector is unique,  X  % is a gradient of p at a ,andasubgradient otherwise.
 Because for any real value t the pair (  X  + t 1 n ,  X   X  t 1 is feasible if the pair (  X  ,  X  ) is feasible, and because their objective are identical, any dual optimum (  X  ,  X  ) is deter-mined up to an additive constant. To remove this degree of freedom X  X hich arises from the fact that one among all n + m row/column sum constraints of U ( a, b ) is redundant X  we can either remove a dual variable or normalize any dual optimum  X  % so that it sums to zero, to enforce that it belongs to the tangent space of  X  n .Wefollowthelatterstrategyin the rest of the paper. 4.2. Fixed Support: Minimizing f over P ( X ) Let X  X   X  n be fixed and let  X  be a closed convex subset of  X  n .Theaimofthissectionistocomputeweights a  X   X  such that f ( a, X ) is minimal. Let  X  % i be the optimal dual variable of d ( a, b i ; M XY i ) normalized to sum to 0. f being asumofterms p ( a, b i ,M XY i ) ,wehavethat: Corollary 1. The function a , X  f ( a, X ) is polyhedral con-vex, with subgradient Assuming  X  is closed and convex, we can consider a naive projected subgradient minimization of f .Alternatively, if there exists a Bregman divergence B ( a, b )=  X  ( b )  X   X  ( a )  X  X  X  X   X  ( a ) ,b  X  a  X  for a, b  X   X  defined by a prox-function  X  ,wecandefinetheproximalmapping P a ( b )= argmin c  X   X  (  X  b, c  X  a  X  + B ( a, c )) and consider accelerated gradient approaches (Nesterov, 2005). We summarize this idea in Algorithm 1.
 Algorithm 1 Wasserstein Barycenter in P ( X,  X  ) Inputs : X  X   X  n ,  X   X   X  n .For i  X  N : Y i  X   X  m i ,b i  X  Form all n  X  m i matrices M i = M XY i ,seeEq.(1).
Set  X  a =  X  a = argmin  X   X  . while not converged do end while Notice that when  X  =  X  n and B is the Kullback-Leibler divergence (Beck and Teboulle, 2003), we can initialize  X  a with 1 n /n and use the multiplicative update to realize the proximal update:  X  a  X   X  a  X  e  X  t 0  X  X   X   X  ;  X  a  X   X  a/  X  a  X  is Schur X  X  product. Alternative sets  X  for which this projection can be easily carried out include, for instance, all (convex) level set of the entropy function H ,namely  X  = { a  X   X  n | H ( a )  X   X  } where 0  X   X   X  log n . 4.3. Differentiability of p ( a, b, M XY ) w.r.t X We consider now the case where  X  = R d with d  X  1 , D is the Euclidean distance and p =2 .When  X  = R d , afamilyof n points X and a family of m points Y can be represented respectively as a matrix in R d  X  n and an-other in R d  X  m .Thepairwisesquared-Euclideandistances between points in these sets can be recovered by writing x = diag( X T X ) and y def = diag( Y T Y ) ,andobservingthat Transport Cost as a function of X . Due to the margin con-straints that apply if a matrix T is in the polytope U ( a, b ) , we have: Discarding constant terms in y and b ,wehavethatminimiz-ing p ( a, b, M XY ) with respect to locations X is equivalent to solving As a function of X ,thatobjectiveisthesumofaconvex quadratic function of X with a piecewise linear concave function, since is the minimum of linear functions indexed by the vertices of the polytope U ( a, b ) .Asaconsequence, p ( a, b, M XY is not convex with respect to X .
 Quadratic Approximation. Suppose that T % is optimal for problem p ( a, b, M XY ) .UpdatingEq.(7), x a  X  X  T % ,X T Y  X  = * X diag( a 1 / 2 )  X  YT % T diag( a  X  1 / 2 Minimizing a local quadratic approximation of p at X yields thus the Newton update Asimpleinterpretationofthisupdateisasfollows:thema-trix T % T diag( a  X  1 ) has n column-vectors in the simplex  X  m .Thesuggestedupdatefor X is to replace it by n barycenters of points enumerated in Y with weights defined by the optimal transport T % .Notethat,becausethemini-mization problem we consider in X is not convex to start with, one could be fairly creative when it comes to choosing D and p among other distances and exponents. This sub-stitution would only involve more complicated gradients of M
XY w.r.t. X that would appear in Eq. (7). 4.4. Free Support: Minimizing f over P k ( R d ,  X  ) We now consider, as a natural extension of  X  4.2 when  X  = R ,theproblemofminimizing f over a probability measure  X  that is (i) supported by at most k atoms described in X ,a matrix of size d  X  k ,(ii)withweightsin a  X   X   X   X  k . Alternating Optimization. To obtain an approximate mini-mizer of f ( a, X ) we propose in Algorithm 2 to update alter-natively locations X (with the Newton step defined in Eq. 8) and weights a (with Algorithm 1).
 Algorithm 2 2 -Wasserstein Barycenter in P k ( R d ,  X  ) initialize X  X  R d  X  k and a  X   X  while X and a have not converged do end while Algorithm 2 and Lloyd/Ng Algorithms. As mentioned in  X  2, minimizing f defined in Eq. (5) over P k ( R d ) ,with N =1 , p =2 and no constraints on the weights (  X  =  X  k ), is equivalent to solving the k -means problem applied to the set of points enumerated in  X  1 .Inthatparticularcase,Algo-rithm 2 is also equivalent to Lloyd X  X  algorithm. Indeed, the assignment of the weight of each point to its closest centroi d in Lloyd X  X  algorithm (the maximization step) is equivalent to the computation of a % in ours, whereas the re-centering step (the expectation step) is equivalent to our update for X using the optimal transport, which is in that case the trivial tran s-port that assigns the weight (divided by N )ofeachatom in Y i to its closest neighbor in X .Whentheweightvector a is constrained to be uniform (  X  = { 1 k /k } ), Ng (2000) proposed a heuristic to obtain uniform k -means that is also equivalent to Algorithm 2, and which also relies on the re-peated computation of optimal transports. For more general sets  X  ,Algorithm1ensuresthattheweights a remain in  X  at each iteration of Algorithm 2, which cannot be guaranteed by neither Lloyd X  X  nor Ng X  X  approach.
 Algorithm 2 and Reich X  X  (2013) Transform. Reich (2013) has recently suggested to approximate a weighted measure  X  by a uniform measure supported on as many atoms. This approximation is motivated by optimal transport theory, no -tably asymptotic results by McCann (1995), but does not attempt to minimize, as we do in Algorithm 2, any Wasser-stein distance between that approximation and the original measure. This approach results in one application of the Newton update defined in Eq. (8), when X is first initialized to Y and a = 1 m /m to compute the optimal transport T % . Summary We have proposed two original algorithms to compute Wasserstein barycenters of probability measures: one which applies when the support of the barycenter is fixed and its weights are constrained to lie in a convex subset  X  of the simplex, another which can be used when the sup-port can be chosen freely. These algorithms are relatively simple, yet X  X o the best of our knowledge X  X ovel. We sus-pect these approaches were not considered before because of their prohibitive computational cost: Algorithm 1 compute s at each iteration the dual optima of N transportation prob-lems to form a subgradient, each with n + m i variables and n  X  m i inequality constraints. Algorithm 2 incurs an even higher cost, since it involves running Algorithm 1 at each iteration, in addition to solving N primal optimal transport problems to form a subgradient to update X .Sincebothob-jectives rely on subgradient descent schemes, they are also likely to suffer from a very slow convergence. We propose to solve these issues by following Cuturi X  X  approach (2013) to smooth the objective f and obtain strictly convex objec-tives whose gradients can be computed more efficiently. To circumvent the major computational roadblock posed by the repeated computation of primal and dual optimal trans-ports, we extend Cuturi X  X  approach (2013) to obtain smooth and strictly convex approximations of both primal and dual problems p and d .Thematrixscalingapproachadvocated by Cuturi was motivated by the fact that it provided a fast approximation p  X  to p .Weshowherethatthesameap-proach can be used to smooth the objective f and recover for a cheap computational price its gradients w.r.t. a and X . 5.1. Regularized Primal and Smoothed Dual A n  X  m transport T ,whichisbydefinitioninthe nm -simplex, has entropy h ( T ) def =  X  (2013) has recently proposed to consider, for  X  &gt; 0 ,aregu-larized primal transport problem p  X  as We introduce in this work its dual problem, which is a smoothed version of the original dual transportation prob-lem, where the positivity constraints of each term m ij  X   X   X   X  j have been replaced by penalties 1  X  e  X   X  ( m ij  X   X  d These two problems are related below in the sense that their respective optimal solutions are linked by a unique positiv e vector u  X  R n + : Proposition 2. Let K be the elementwise exponential of tors ( u, v )  X  R n +  X  R m + such that the optimal solutions of p  X  and d  X  are respectively given by T  X  = diag( u ) K diag( v ) ,  X  Proof. The result follows from the Lagrange method of multipliers for the primal as shown by Cuturi (2013, Lemma 2), and a direct application of first-order conditions for th e dual, which is an unconstrained convex problem. The term so that it sums to zero as discussed in the end of  X  4.1. 5.2. Matrix Scaling Computation of ( u, v ) The positive vectors ( u, v ) mentioned in Proposition 2 can be computed through Sinkhorn X  X  matrix scaling algorithm applied to K ,asoutlinedinAlgorithm3: Lemma 1 (Sinkhorn, 1967) . For any positive matrix A in R + and positive probability vectors a  X   X  n and b  X   X  m , there exist positive vectors u  X  R n + and v  X  R m + ,unique up to scalar multiplication, such that diag( u ) A diag( v )  X  U ( a, b ) .Suchapair ( u, v ) can be recovered as a fixed point of the Sinkhorn map The convergence of the algorithm is linear when us-ing Hilbert X  X  projective metric between the scaling factor s (Franklin and Lorenz, 1989,  X  3). Although we use this al-gorithm in our experiments because of its simplicity, other algorithms exist (Knight and Ruiz, 2012) which are known to be more reliable numerically when  X  is large.
 Summary: Given a smoothing parameter  X  &gt; 0 ,using Sinkhorn X  X  algorithm on matrix K ,definedastheelemen-twise exponential of  X   X  M (the pairwise Gaussian kernel matrix between the supports X and Y when p =2 ,using bandwidth  X  =1 /  X   X  and T port problems. To take advantage of this, we simply propose to substitute the smoothed optima  X  %  X  and T %  X  to the original optima  X  % and T % that appear in Algorithms 1 and 2. Algorithm 3 Smoothed Primal T %  X  and Dual  X  %  X  Optima Input M,  X  , a, b
K =exp(  X   X  M ) ; K = diag( a  X  1 ) K %use bsxfun(@rdivide, K , a )
Set u = ones( n ,1)/ n ; while u changes do end while v = b./ ( K T u ) .
T %  X  = diag( u ) K diag( v ) . %use bsxfun ( @times ,v, ( bsxfun ( @times ,K,u )) % ) ; We present two applications, one of Algorithm 1 and one of Algorithm 2, that both rely on the smooth approximations presented in  X  5. The settings we consider involve computing respectively tens of thousands or tens of high-dimensional optimal transport problems X 2.500  X  2.500 for the first ap-plication, 57 . 647  X  48 for the second X  X hich cannot be re-alistically carried out using network flow solvers. Using ne t-work flow solvers, the resolution of a single transport prob-lem of these dimensions could take between several minutes to several hours. We also take advantage in the first appli-cation of the fact that Algorithm 3 can be run efficiently on GPGPUs using vectorized code (Cuturi, 2013, Alg.1). 6.1. Visualization of Perturbed Images We use 50 . 000 images of the MNIST database, with approx-imately 5 . 000 images for each digit from 0 to 9. Each image (originally 20  X  20 pixels) is scaled randomly, uniformly between half-size and double-size, and translated randoml y within a 50  X  50 grid, with a bias towards corners. We dis-play intermediate barycenter solutions for each of these 10 datasets of images for t =1 , 10 , 60 gradient iterations.  X  is set to 60 / median ( M ) ,where M is the squared-Euclidean distance matrix between all 2,500 pixels in the grid. Using aQuadroK5000GPUwithcloseto1500cores,thecom-putation of a single barycenter takes about 2 hours to reach 100 iterations. Because we use warm starts to initialize u in Algorithm 3 at each iteration of Algorithm 1, the first it-erations are typically more computationally intensive tha n those carried out near the end. 6.2. Clustering with Uniform Centroids In practice, the k -means cost function applied to a given em-pirical measure could be minimized with a set of centroids X and weight vector a such that the entropy of a is very small. This can occur when most of the original points in the dataset are attributed to a very small subset of the k cen-troids, and could be undesirable in applications of k -means where a more regular attribution is sought. For instance, in sensor deployment, when each centroid (sensor) is limited in the number of data points (users) it can serve, we would like to ensure that the attributions agree with those limits . Whereas the original k -means cannot take into account such limits, we can ensure them using Algorithm 2. We illus-trate the difference between looking for optimal centroids with  X  X ree X  assignments (  X  =  X  k ), and looking for opti-mal  X  X niform X  centroids with constrained assignments (  X  = { 1 k /k } )usingUScensusdataforincomeandpopulation repartitions across 57.647 spatial locations in the 48 cont igu-ous states. These weighted points can be interpreted as two empirical measures on R 2 with weights directly proportional to these respective quantities. We initialize both  X  X ree X  a nd  X  X niform X  clustering with the actual 48 state capitals. Re-sults displayed in Figure 3 show that by forcing our approx-imation to be uniform, we recover centroids that induce a more balanced clustering. Indeed, each cell of the Voronoi diagram built with these centroids is now constrained to hol d the same aggregate wealth or population. These centroids could form the new state capitals of equally rich or equally populated states. On an algorithmic note, we notice in Fig-ure 4 that Algorithm 2 converges to its (local) optimum at a speed which is directly comparable to that of the k -means in terms of iterations, with a relatively modest computationa l overhead. Unsurprisingly, the Wasserstein distance betwe en the clusters and the original measure is higher when adding uniform constraints on the weights. Conclusion We have proposed in this paper two original algorithms to compute Wasserstein barycenters of empiri-cal measures. Using these algorithms in practice for mea-sures of large support is a daunting task for two reasons: they are inherently slow because they rely on the subgradi-ent method; the computation of these subgradients involves solving optimal and dual optimal transport problems. Both issues can be substantially alleviated by smoothing the pri -mal optimal transport problem with an entropic penalty and considering its dual. Both smoothed problems admit gra-dients which can be computed efficiently using only matrix vector products. Our aim in proposing such algorithms is to demonstrate that Wasserstein barycenters can be used for vi -sualization, constrained clustering, and hopefully as a co re component within more complex data analysis techniques in future applications. We also believe that our smoothing ap-proach can be directly applied to more complex variational problems that involve multiple Wasserstein distances, suc h as Wasserstein propagation (Solomon et al., 2014). Acknowledgements We thank reviewers for their com-ments and Gabriel Peyr  X e for fruitful discussions. MC was supported by grant 26700002 from JSPS. AD was partially supported by EPSRC.

