  X  X valuation as a service X  (EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot be distributed. The basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed. However, this concept violates some of the premises of tra-ditional pool-based collection building and thus calls into question the quality of the resulting test collection. In par-ticular, the service API might restrict the diversity of runs that contribute to the pool: this might hamper innovation by researchers and lead to incomplete judgment pools that affect the reusability of the collection. This paper shows that the distinctiveness of the retrieval runs used to con-struct the first test collection built using EaaS, the TREC 2013 Microblog collection, is not substantially different from that of the TREC-8 ad hoc collection, a high-quality collec-tion built using traditional pooling. Further analysis using the  X  X eave out uniques X  test suggests that pools from the Mi-croblog 2013 collection are less complete than those from TREC-8, although both collections benefit from the pres-ence of distinctive and effective manual runs. Although we cannot yet generalize to all EaaS implementations, our anal-yses reveal no obvious flaws in the test collection built using the methodology in the TREC 2013 Microblog track.
 Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval]: Systems and Software X  X erformance Evaluation Keywords: test collections; reusability; meta-evaluation
Large-scale community-wide evaluations such as TREC operationalize the Cranfield paradigm by building retrieval test collections that support IR research [2]. A test collec-tion consists of a corpus of documents, a set of information needs called topics, and relevance judgments that specify which documents are relevant for which topics. A funda-mental assumption in the paradigm is that researchers can acquire a test collection X  X  document corpus. But what if this is not possible? This was the challenge in the TREC Microblog track evaluation, where the corpus is comprised of tweets posted on Twitter. Since the company X  X  terms of service forbid redistribution of tweets, a test collection built on tweets cannot be distributed in a traditional manner.
The solution developed by the track organizers, termed  X  X valuation as a service X  (EaaS), was to provide an API through which the tweet collection can be accessed for com-pleting the evaluation task [4, 3]. This API, consisting of a basic keyword search interface, was the only method through which track participants could access the collection. The operational deployment of the EaaS model was successful in attracting participation at TREC 2013, which provides en-couraging evidence that EaaS could be generalized to other evaluation scenarios involving sensitive data (e.g., medical or desktop search). Before expanding the use of the approach, however, we need to know whether EaaS has any substantive impact on the quality of the collections produced.

One concern about the EaaS model is a potential lack of diversity in the retrieval runs used to build the test collec-tion. The criticism is this: if everyone must use the API, won X  X  they all end up doing basically the same thing? Di-versity is important from at least two perspectives. Previ-ous work has tied the reusability of a collection built using pooling to the diversity of the retrieval runs that form the pools [1]. Diversity in participant submissions also serves as a proxy indicator that researchers are trying different (novel) retrieval techniques. The contribution of this pa-per is an analysis of the diversity of the pools in the TREC 2013 Microblog track (using EaaS) compared to previous evaluations X  X ncluding the first two years of the Microblog track in TRECs 2011 and 2012 X  X here participants had ac-cess to the entire corpus. We first propose a novel metric, called Run Average Overlap (RAO), that measures the over-all distinctiveness of one retrieval run with respect to a given set of runs. In a second analysis, we apply another test of reusability, the leave out uniques (LOU) test. Analyses show no obvious differences in the characteristics of the pools, sug-gesting that the EaaS model can be used to build reusable collections.
Pooling [8] is a method for building test collections when the target document corpus is too large to make exhaustive relevance judgments for every document for every topic. A pooled collection is built from a set of retrieval runs sub-TREC-8 528 K 50 100 71 40 MB2012 16 M 60 100 121 33
MB2013 243 M 60 90 71 20 mitted by participants of the process, typically as part of a community-wide evaluation such as TREC. 1 A run is the output of a retrieval system for each topic in the collection; documents are assumed to be ordered by decreasing like-lihood of relevance to the topic. The pool for a topic is comprised of the union of the top K (called the pool depth ) documents retrieved for that topic by each run. Only docu-ments in the pool are judged for relevance by a human; all other documents are assumed to be not relevant.

Test collections are research tools that are most useful when they are reusable , that is, when they fairly evaluate retrieval systems that did not contribute to their construc-tion [1]. Thus, a reusable test collection can be used to evaluate systems even after the original evaluation that cre-ated the collection has concluded. For a pooled collection to be reusable, the pool must contain enough of the relevant documents in the collection to be an unbiased sample of the relevant documents. In practice, the completeness of a pool is a function of (at least) the pool depth, the effectiveness of the runs that comprise it, the diversity of the runs, and the true number of relevant documents [10, 9].

Many of the test collections built in TREC, including the ad hoc collections and the Microblog collections, were built through pooling. The TREC ad hoc collections, especially the TREC-8 collection, have been subject to a number of analyses and have been used in hundreds of retrieval exper-iments whose outcomes have proven to generalize. Thus, these collections are generally accepted as high-quality test collections [5]. Our basic premise is that the TREC 2013 Microblog collection is reusable if it displays characteristics similar to the TREC ad hoc collections. The particular char-acteristics of interest are the distinctiveness of the runs over which the pools are formed and the number of relevant doc-uments found by a single participant.

More details about the three collections used in this paper are given in Table 1. (Our experiments also considered the ad hoc test collections from TREC-6 and TREC-7, but they yielded similar results, and so for brevity we only present results from TREC-8 here.) The first column of the table gives the name of the collection as used in the remainder of this paper. The corpus for TREC-8 consists of approxi-mately half a million (mostly) newswire articles. The pools were predominantly constructed from the 71 ad hoc runs that were judged (from a total of 129 submitted runs), but runs from some other tracks contributed small numbers of documents to the pools as well. 2 The other two collections were built as part of the TREC Microblog tracks in 2012 and 2013, whose document corpora consist of tweets posted to Twitter. The document corpus used for the MB2012 track (called Tweets11) contains approximately 16 million tweets, which were distributed by publishing a list of tweet ids that participants then fetched [7, 6]. The EaaS model enabled the MB2013 corpus to be much larger, about 243 million tweets [4]. Relevance judgments for the MB collections were made on a 3-point scale ( X  X ot relevant X ,  X  X elevant X ,  X  X ighly relevant X ), but in this work we ignore the different degrees of relevance and use both higher grades as  X  X elevant X .
The final column of Table 1 gives the number of partici-pants that contributed runs. TREC generally allows partic-ipating groups to submit more than a single run to a given task. In practice, runs submitted by the same participant tend to be much more similar to each other than they are to runs from other participants. Analyses of run diversity need to account for this phenomenon.
Our biggest concern regarding the EaaS model is whether the common API imposed on participants restricts their re-trieval approaches to the extent that different participants are compelled to submit essentially similar runs. In order to answer this question, we introduce a novel metric, Run Average Overlap (RAO), which is a measure of the tendency of a set of runs to retrieve documents in common.

Run Average Overlap is computed for a given run with respect to a specific set of runs. In what follows we compute the RAO score for each run in a pool set with respect to the pool set. Say that a run O retrieves document d for a topic t if d occurs in the top K ranks for t in O . When K is set equal to the pool depth, as done here, O retrieves d if it contributes d to the pool. Let P be the total number of participants that have runs in the run set, and P d be the number of distinct participants that retrieve d . Finally, let T be the total number of topics and n be the number of documents retrieved by O for t . Then: In words, the RAO score for a run is the mean over all topics of the score computed for individual topics, where the per-topic score is the mean over the documents retrieved by O of the reciprocal of the number of participants (including itself) that retrieve that document. The greater the RAO score the more distinctive the result set. The minimum RAO score is which signifies that O retrieved only documents that all other participants also retrieved. The maximum score is 1.0, which means that no other participant retrieved any docu-ment that O retrieved. Note that document distinctiveness for the purposes of computing RAO is computed over partic-ipants , and thus the number of runs submitted by any given participant is not an issue.

While RAO scores close to 1 P for all runs in a pool set indicate that the runs are not diverse, the presence of runs with high scores is not sufficient to conclude that the pool is appropriately diverse. The RAO score is computed over all retrieved documents, not relevant documents. Distinc-tive but ineffective runs are easy to produce (for example, by selecting random documents from the collection) but un-helpful for pool building. Thus, the RAO score must be used in conjunction with an effectiveness score to ensure the presence of distinctive and effective runs in the pool set. We use R-precision as the effectiveness measure here since it was the primary metric for the TREC 2013 Microblog track. Figure 1 shows plots of RAO vs. R-precision for the three collections. Each point in a graph represents one pool run. Manual runs (runs for which there was some human pro-cessing in the construction of the run) are plotted as open as open squares and automatic runs as filled circles. squares while automatic (i.e., non-manual) runs are plotted as filled circles. A highly distinctive and highly effective run would fall in the upper right-hand corner of the graph.
The plots for the MB2013 and the TREC-8 collections are very similar. These collections exhibit a general inverse re-lationship between effectiveness and distinctiveness (points run roughly from upper left to lower right). This is sim-ply a manifestation of the fact that there are many fewer relevant documents than non-relevant documents, and thus many fewer effective retrieved sets than ineffective retrieved sets. But, importantly, we see that both the TREC-8 and MB2013 collections contain outlier runs that are both dis-tinctive and effective X  X hese represent high-quality manual runs that contributed to the pools.

The graph for the MB2012 collection, which was not con-structed using the EaaS model, is different from the others. The pool runs for MB2012 have a much narrower range of effectiveness and have a much greater RAO score on aver-age. Note, however, that this is the one collection that does not have any runs that are outlier effective runs, and overall effectiveness is relatively poor. Thus, the large RAO scores are probably another manifestation of the tendency for in-effective runs to be different.
In our second analysis, we examined uniquely retrieved relevant documents, which are relevant documents that were contributed to the pool by only a single participant. The leave out uniques test (LOU) [1] gauges the reusability of a test collection by examining the effect on evaluation scores of a participant X  X  runs when the participant X  X  uniquely re-trieved relevant documents are removed from the judgment set. The test computes the evaluation scores that would have resulted had the participant not been involved in the collection building process.

Figure 2 plots the results of the LOU test for each of the three collections. Each run is plotted as a point, with manual runs shown as empty boxes and automatic runs as filled circles. The x -axis is the MAP score of the run as computed using the official relevance judgment set for that collection, and the y -axis is the MAP score as computed using the judgment set with the participant X  X  unique rele-vants removed. When both scores are the same, the point falls on the diagonal line. A run whose MAP score decreases Table 2: Collection characteristics on uniquely re-trieved relevant documents and LOU test results.
 when unique relevants are removed falls below the line, and a run whose MAP score improves when unique relevants are removed lies above the line.

The presence of distinctive and effective manual runs in the TREC-8 and MB2013 collections is obvious in the LOU test results. These runs fall well below the line of equal scores, and are the only runs to do so. The degradation in MAP scores is not surprising given that the participants who submitted these runs contributed large numbers of unique relevant documents to the pools. As shown in the second row of Table 2, for these two collections the participant who submitted the most effective manual runs contributed more than 40% of the unique relevant documents. In contrast, the largest percentage of unique relevant documents con-tributed by a single participant in the MB2012 collection is just 20%. The percentage of total relevant documents that are uniquely retrieved relevants is given in the first row of the table. Here the TREC-8 and MB2013 collections differ: the MB2013 collection has a much larger percentage of total relevant documents that are unique.

A large percentage of total relevant documents that are uniquely retrieved relevant documents is one indication that a test collection may be less reusable, and the LOU test results are consistent with this observation. In Figure 2, all of the automatic runs are on the line of equal scores for the TREC-8 collection, but there is more movement away from the line for the two Microblog collections. The final three rows of Table 2 quantify this movement. The table gives statistics regarding the distribution of the percentage difference in MAP scores in the LOU test when considering only automatic runs whose original MAP scores were at least 0.1 (because percentage differences are artificially magnified for ineffective runs). The statistics computed are the mean of the percentage difference over these automatic runs, the maximum percentage difference observed in the automatic runs, and the count of the number of runs that have a per-centage difference greater than 1% (out of the total). No TREC-8 automatic run has a percentage difference greater than 1%, and since differences that small are within the level of evaluation noise [9], the collection is regarded as highly reusable. The MB2013 collection has a mean percentage dif-ference of about 1%, with 15 runs (out of 57) having at least a 1% difference and a maximum difference of about 8%.
Note that this amount of change in LOU test results is not a consequence of using the EaaS model. In fact, the large percentage of relevant documents that are unique is actually confirmation that the evaluation method accommodated dis-tinctive runs (i.e., the API did not hamper researchers X  abil-ity to generate runs that are both effective and distinctive), and the MB2012 collection, which was not built using EaaS, has similar characteristics. The likely explanation is the size of pools relative to the size of the corpus [1]. Corpora used in the two Microblog collections are much larger than the TREC-8 collection, so the judgment pools represent a much smaller percentage of the collection than for the TREC-8 collection. Also note that this level of change is unlikely to have much impact in the utility of the collection as a research tool. As can be seen in Figure 2, the relative or-dering of runs is largely stable even in the presence of MAP score differences. At worst, researchers who encounter many unjudged tweets in top ranks in new runs should be more cautious in their conclusions.
The evaluation as a service model for constructing re-trieval test collections can offer significant advantages over traditional construction techniques, but only if it does not fundamentally hamper innovation and it leads to high-quality resources. This paper examined the one collection that has been built using the EaaS model to date, the TREC 2013 Microblog collection, and found its run diversity to be similar to the high-quality TREC-8 ad hoc collection. The results of the leave out uniques test suggest that pools from the Microblog 2013 collection are less complete than pools from TREC-8, but both collections strongly benefit from the presence of distinctive and effective manual runs.

Of course, these findings are for a single collection and a specific API. This implementation provides a generous num-ber of documents in response to a query and a generous allot-ment of queries per participant. These choices contribute to good design, as a more restrictive API would likely have im-pacted submitted runs and the resulting test collection neg-atively. Although we cannot yet make statements about the EaaS model in general, the TREC 2013 Microblog collection does offer an existence proof that a high-quality retrieval test collection can be constructed using this new method. This work has been supported in part by NSF under awards IIS-1217279 and IIS-1218043. Any opinions, findings, con-clusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsor.
