 Search personalization has been pursued in many ways, in order to provide better result rankings and better overall search experience to individual users [5]. However, blindly applying personalization to all user queries, for example, by a background model derived from the user X  X  long-term query-and-click history, is not always appropriate for aiding the user in accomplishing her actual task. User interests change over time, a user sometimes works on very different categories of tasks within a short timespan, and history-based personalization may impede a user X  X  desire of discov-ering new topics. In this paper we propose a personaliza-tion framework that is selective in a twofold sense. First, it selectively employs personalization techniques for queries that are expected to benefit from prior history information, while refraining from undue actions otherwise. Second, we introduce the notion of tasks representing different gran-ularity levels of a user profile, ranging from very specific search goals to broad topics, and base our reasoning selec-tively on query-relevant user tasks. These considerations are cast into a statistical language model for tasks, queries, and documents, supporting both judicious query expansion and result re-ranking. The effectiveness of our method is demonstrated by an empirical user study.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval -relevance feedback, retrieval models General Terms: Algorithms, Experimentation, Human Factors Personalization does not always improve result quality. Thus, it is crucial to restrict personalization to those queries that benefit from its application. To decide whether a query needs personalization we reason on its ambiguity and rep-resentation strength in the user X  X  profile. Existing measures for predicting the difficulty of a query include query clarity [2], which has also been studied for facilitating an adaptive query expansion in [3]. Chirita et al. [1] consider an adaptive personalized query expansion by determining the number of expansion terms empirically as a function of query scope with respect to the user profile and the general query clar-ity. However, this approach always interprets a user profile in its entirety, ignoring the issue that the same users pur-sue different tasks that require different adaptation strate-gies. We propose a framework based on fine-grained lan-guage models for units of a user X  X  past search and browse behavior, coined tasks . They reflect the non-homogeneous, various aspects of user interests which might, e.g., range from the specific task of searching a hotel in Borneo to the general interest in traveling. These tasks are obtained by means of a hierarchical clustering of the user X  X  profile. The atomic units forming a task are either cohesive query chains, i.e., subsequently posed queries including their result clicks and further browsed documents within the same session, or query-independently browsed documents.

Our approach indirectly determines the ambiguity level of a query, and its presence in the user profile. Person-alization is facilitated by a re-ranking of the original search results (typically the top-50) by means of a client-side proxy. For each query, we perform a hierarchical clustering of the query X  X  result set items (each represented by its title and snippet information) to obtain candidate query facets F ...,F m which represent the different aspects the query might span. Similarly, we retrieve the top-k tasks T 1 ,...,T k similar to the query from the user X  X  profile. To also take the immediate query context into consideration, we include a task T k +1 representing the currently active session. Each obtained query facet and each task is represented by a uni-gram language model. Finally, we determine the facet/task pair ( F  X  i ,T  X  j ) with the lowest Kullback-Leibler (KL) diver-gence. If KL ( F  X  i ,T  X  j ) is larger than a threshold  X  , we con-clude that the current query goes for a previously unexplored task, and thus refrain from biasing the search results. up-date the query representation with terms that best discrim-inate the chosen query facet F  X  i from all other query facets, while being most similar to the chosen task T  X  j , i.e., terms for which holds, and re-rank the original results based on the KL di-vergence between their title/snippet representation and the new query representation. Thus in case of ambiguity, the query is biased towards the facet with the best-matching counterpart in the user X  X  profile. Alternatively, we consider v ( w ) = P ( w | F i  X  T j ), as well as, thresholding the number of terms to be selected.
The language model of a user task is a weighted mixture of its components: queries, result sets, clickstream documents, and browsed documents. Let Q be a query language model, and B a language model of browsed documents. Then the task language model T is obtained as where w denotes a word in the vocabulary. B is the aver-age of the individual browsed documents X  language models, whereas Q is the uniform mixture of the task X  X  query chains (QC denotes a query chain Q 1 ,Q 2 ,...,Q k where Q k is the last query in the chain), and each query chain is modeled by a weighted sum of its constituent queries, weighted in such a manner that queries later in the chain are considered more important than queries early in the chain: P ( w | Q ) = 1 | QC |  X  X A single query is represented by a mixture of its actual query terms (c(w,Q) denotes the frequency of word w in query Q, and | Q | is the length of query Q), its clicked documents ( CR ), its intentionally non-clicked documents ranked above clicked ones ( NR ) , its unseen result documents ranked below all clicked ones ( UR ), and the mixture model over all the documents the user visited starting from clicked result set documents, such that P ( w | Q i ) is computed as  X  Similar to the query chain language model, the clickstream language model is a harmonically weighted combination of the constituent documents X  models to give higher weight to documents visited later in the search session. The ratio-nale is that documents visited later in the session are bet-ter matches to the user X  X  information need, and thus better characterize the query X  X  intention. All constituent language models employ Dirichlet prior smoothing [6].
To evaluate the effectiveness of our approach, we asked 7 volunteers from our lab to install our proxy on their PCs to log their search and browsing activities for a period of 2 months. Table 1 shows the activity levels of these users.
During a one-week evaluation phase, each participant eval-uated self-chosen search tasks, among which at least two were re-finding tasks. These tasks consisted of 2 . 3 queries on average, from which the first, the last, and the median query if available were subject to evaluation. For each task, the participant was presented with the top-50 Google re-sults for the selected queries, merged and placed in random order to avoid a result position bias. We asked users to group the top-10 results of each query by giving labels to them. This was to generate the optimal query facets as perceived by the individual user, and to decouple the eval-uation of our personalization strategy from the quality of the result-set clustering. In total our experiment comprises 59 search tasks, and 98 individual evaluation queries. To measure the ranking quality, we use the normalized Dis-counted Cumulative gain (DCG) [4]. We experimented with different parameter settings for the weights in the task lan-guage model. Table 2 presents the NDCG at top-50 av-eraged over the individual average NDCGs of all partici-pants, with the best performing parameter setting in place for each user individually. For each user the chosen setting is the one that maximizes the improvement in NDCG of enforced personalization (personalization is carried out for each query) over the original results. We compare our se-lective personalization against the original results and the enforced personalization results for both ways of generating query facets, from  X  X uman X  labels or by automatic hierar-chical clustering. In both cases, selective personalization significantly outperforms both the original Google ranking with one-tailed paired t-test p-values of 0 . 038 (human), re-spectively 0 . 07 (automatic), and the enforced personaliza-tion (human: 0 . 013, automatic: 0 . 002), proving automatic clustering competitive. Also, for users that benefit the most from personalization, we find a medium negative correlation between the improvement in NDCG of the enforced person-alization over the original ranking and KL ( F  X  i ,T  X  j ing the validity of our approach for predicting whether a query benefits from personalization.
 Table 2: Average NDCG at top-50 for best parameter
Our next goal is to learn and adjust the optimum values for these parameters automatically from the user X  X  profile by making use of explicitly or even implicitly assessed test queries. In an online system, this process of adjusting pa-rameters should be run periodically in the background.
