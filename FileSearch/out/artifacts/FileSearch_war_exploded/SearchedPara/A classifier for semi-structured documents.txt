 In this pap er, w e describ e a no v el text classi er that can e ectiv ely cop e with structured do cumen ts. W ereport ex-p erimen ts that compare its p erformance with that of a w ell-kno wn probabilistic classi er. Our no v el classi er can tak e adv an tage of the information in the structure of do cumen t that con v en tional, purely term-based classi ers ignore. Con-v en tional classi ers are mostly based on the v ector space mo del of do cumen t, whic h views a do cumen t simply as an n -dimensional v ector of terms. T o retain the information in the structure, w e ha v e dev elop ed a structured v ector mo del, whic h represen ts a do cumen t with a structured v ec-tor, whose elemen ts can b e either terms or other structured v ectors. With this extended mo del, w ealsoha v eimpro v ed the w ell-kno wn probabilistic classi cation metho d based on the Bernoulli do cumen t generation mo del. Our classi er based on these impro v emen ts p erformes signi can tly b etter on pre-classi ed samples from the w eb and the US P aten t database than the usual classi ers.
 H.2.8 [ Information Systems ]: Database Applications| Data Mining Classi cation, Semi-structured do cumen t, Structured v ector As semi-structured texts, suc h as texts in HTML and XML (eXtensible Markup Language) [2], proliferate, there is press-ing need to supp ort e X cien t and e ectiv e information re-triev al (IR), searc h, and ltering on them. T ext classi -cation has been extensiv ely studied in the IR comm unit y [14]. Con v en tional classi ers do not p erform w ell on struc-tured do cumen ts [6, 8]. The con v en tional classi ers p erform p o orly b ecause they are designed for non-structured data. Do cumen t structures p ose new c hallenges to automatic clas-si ers. W e b eliev e do cumen t structures con tain high-qualit y seman tic clues that purely term-based classi ers cannot tak e adv an tage of. The con v en tional text classi ers are designed for self-con tained, at (or non-structured) texts. Markups and formatting cues in the structured do cumen ts can mis-lead the classi ers; but their remo v al means that only partial information fed in to the classi ers.
 In this pap er, w e prop ose a no v el classi er for semi-structured do cumen ts. First, w e extend the con v en tional mo del of do c-umen t. Instead of viewing a do cumen t as a bag of terms, w e view it as a structur e dve ctor whose elemen ts ma y b e either simple terms or other structured v ectors. W e sho w path expressions of terms are equiv alen t to the structur ed ve ctor represen ting the do cumen t. Second, w e extend the proba-bilistic do cumen t classi cation mo del to obtain one suitable for structured v ectors.
 T o our kno wledge, this is the rst classi cation system that feeds b oth textual and structural features in to a general statistical mo del in order to classify semi-structured do c-umen ts. The new classi er signi can tly excel con v en tional text-based classi ers. It cuts do wn the rate of the classi ca-tion error on US P aten t data 1 from 70% to 17%, and the one on the sample data from Y aho o 2 from 67% to 40%. The rest of this pap er is organized as follo ws: in section 2, w e review the basics of the con v en tional text classi ers. In section 3, w e describ e our new classi er. In section 4, w e rep ort the p erformance of the new classi er on test datasets. W e review related w ork in section 5 and pro vide concluding remarks in section 6. Giv en a text do cumen t, a text classi er assigns one of the prede ned class categories to it or determines the lik eliho o d that the do cumen t b elongs to them. A classi er is rst pro vided a topic set with sample training do cumen ts for eac h topic. T raining do cumen ts are supplied with attac hed pre-assigned classes. The classi er dev elops mo dels for the classes, a pro cess also called learning . Later, presen ted with previously unseen do cumen ts, the classi er assigns the b est matc hing classes. This is called testing . In this section, w e review v ector space mo del of do cumen ts and Bernoulli do cumen t generation mo del used b y [4, 5, 6]. In con v en tional text classi cation, the ob ject b eing classi ed is a self-con tained, non-structured (or at) do cumen t. The at text do cumen t is often mo deled as a v ector of terms [11, 12]. Let D b e a set of text do cumen ts, and W be a giv en lexicon of n terms. ~ W = &lt;w 1 ;:::w n &gt; isav ector of terms in W . A text do cumen t, d 2 D , can be view ed as an n -dimensional v ector, ~ d = &lt;d 1 ;:::d n &gt; . If term w d is the n um b er of o ccurrences of the term in the do cumen t; otherwise, d i is zero. A do cumen t d is generated b y rst pic king a class. Eac h class c has an asso ciated m ulti-faced coin. Its face represen ts a term t and has some success probabilit y f ( c; t ), that is the o ccurrence rate of t in c . T erms in d are generated b y ipping the coin a giv en n um b er of times (i.e., the desired n um ber of w ords in d ), and taking terms corresp onding to the face of the coin. W e de ne the follo wing notations: n ( d; t ) = the no. of o ccurrences of term t in do cumen t d n ( d ) = the no. of terms in d n ( c; t ) = n ( c ) = f ( c; t ) = where L ( c ) is the size of the lexicon of class c . If do cumen t d Th us, giv en the assumption that the o ccurrences of terms in a do cumen t are indep enden t, Semi-Structured data is data that do es not ha v e to con-form to a xed sc hema [1]. Semi-Structured do cumen ts are text les that con tain semi-structured data. The examples are BibT ex le, SGML (Standard Generalized Markup Lan-guage) [7], HTML, or XML. In this pap er, w e concen trate on XML do cumen ts. XML do cumen ts di er from t ypical text do cumen ts in the follo wing resp ects: 1. Eac h elemen t of an XML do cumen t is tagged. 2. The tags are usually nested, whic hmak es XML do cu-3. An y elemen t of an XML do cumen t can b e referred to Th us, prop er classi cation of XML do cumen ts requires a sc heme that exploits the ab o v e prop erties of XML. W eex-tend the con v en tional do cumen t mo del in order to incorp o-rate hierarc hical sectioning of text. In the extended mo del, a do cumen t is hierarc hically structured and text is em bed-ded in the structure. F or example, a b o ok consists of man y c hapters that consist of sections. Eac h p ortion of text of a b o ok b elongs to a section, whic h in turn b elongs to a c hap-ter that con tains the section, and to the b o ok at its highest lev el.
 W e describ e structur e d ve ctor mo del and semi-structur e d do cument gener ation mo del in section 3.1 and 3.2, resp ec-tiv ely . W e presen t the tag augmen tation metho d and semi-structured classi er in section 3.3 and 3.4, resp ectiv ely . W eha v e dev elop ed a do cumen t mo del for semi-structured do cumen ts, called structur e dve ctor mo del ,based on the fol-lo wing observ ation: terms from the same XML elemen tha v e to b e group ed together i) to b e treated together and ii) to b e di eren tiated from terms in other XML elemen ts. The reason is that terms from one substructure of a do cumen t ma yha v e a distribution di eren t from that of terms in an-other substructure (or the distribution of terms in the en tire do cumen t). The at v ector space mo del fails to enco de this t yp e of information.
 Before w e presen t the formal de nition of structur e dve ctor mo del , w e in tro duce the concept informally with example. Figure 1 presen ts a tin y XML do cumen t and its do cumen t structure tree. A do cumen t corresp onds to the ro ot ele-men t of its corresp onding do cumen t tree. Eac h tagged ele-men t of ro ot no de corresp onds to a c hild no de of the ro ot no de in the do cumen t tree. In the example, &lt;name&gt; and &lt;contact info&gt; are c hild no des of &lt;person&gt; ro ot elemen t of the do cumen t. Lik ewise, an y tagged elemen tis a c hild of the higher lev el elemen t in the nested structure. In the ex-ample, tagged elemen t &lt;email&gt;jdoe@email.com&lt;/email&gt; is ac hild of &lt;contact info&gt; elemen t. Pure text can not ha v e an yc hild, th us is represen ted as a leaf in the structure tree. Therefore, in terior no des of do cumen t structure tree corre-sp ond to tagged elemen ts of do cumen t, and text is stored in leaf no des of the tree. The lab el of a in terior no de is the tag of the corresp onding do cumen t elemen t.
 F or simplicit y of exp osition, w e restrict atten tion to struc-tures of do cumen ts that can b e mo deled b y a tree, although, in general, they are b etter mo deled b y directed graphs. T a-ble 1 lists the sym b ols used in the formal de nition. Figure 2 sho ws the general structure of a do cumen t, d . e d (0 ; 0) is the ro ot elemen t of the do cumen t in question and corresp onds to ro ot no de of the structure tree. Eac hnode atlev el ( i 1) of the tree is further re ned b ynodes at i th lev el, e d f e are reac hed. m d ( i ) is the n um ber of nodes at lev el i of the do cumen t structure tree. Apathto anin terior no de e d ( i; j ), p ( i; j )=( e d (0 ; 0) :e d (1 ;p 1 ) :e d (2 ;p 2 ) :::e of structure no des that m ust b e visited to reac h to the no de e i;j from the ro ot. The path from the ro ot to the no de e ( i; j ) is unique, as eac hnode has only one paren t no de. Th us, p d ( i; j ) uniquely determines the lo cation of the cor-resp onding elemen t's lo cation in the do cumen t. In the rest of this pap er, sometimes w e use e d ( i; j ) and p d ( i; j )in ter-c hangeably when it do es not cause an y confusion. Eac hleaf no de con tains text terms of the elemen t that immediately con tains the text.
 Alternativ ely , a do cumen t is represen ted b yav ector, ned recursiv ely: ~ e d ( i; j )isa v ector that consists of all sub-v ectors, ~ e d ( i +1 ;k )of its c hildren elemen ts, where 0 k&lt; m d ( i; j ), m d ( i; j ) is the n um ber of c hild no des of e This results in the nested do cumen tv ector sho wn in gure 3 that corresp onds to the do cumen t in gure 2.
 Do cumen ts that b elongs to a class do not ha v e to conform to a sp eci c sc hema. W e de ne the do cument structur e of a class as a sup erset of the structures of all do cumen ts b e-longs to the class. The do cumen t class tree structure, E consists of all do cumen t structures in the class, i.e. E f p c (0 ; 0) ;::p c ( i ; 0) ; ::p c ( i ;m c ( i )) ;::p c ( h where m c ( i ) denotes the n um b er of no des at lev el i of the class do cumen t tree and h c denotes the heigh t of the class do cumen t tree. Let us de ne new notations: n ( d; p d ( i; j ) ;t ) = # of o ccurrences of t in p d ( i; j )of d n ( d; p d ( i; j )) = # of all terms in p d ( i; j )of d n ( c; p d ( i; j ) ;t ) = n ( c; p d ( i; j )) = The follo wing outlines the Bernoulli do cumen t generation mo del for semi-structured do cumen ts : F or an y path of length i ,
P [ p d ( i ;j ) j c ]= P [ p d ( i 1 ;p i 1 ) j c ] P [ e where f ( c; p d ( i; j ) ;t )= L ( c ) is the size of the lexicon of class c . Note that equation 4 requires w eak er indep endence assump-tion than the con v en tional classi er lik e the one in sec-tion 2.2 (equation 1). Equation 1 requires the indep en-dence assumption to be held for the en tire terms of the do cumen t, whereas equation 4 requires the assumption to be held only in the structure no de it b elongs to. This is pragmatically m uc h b etter. Exp erimen tal results in a later scetion sho who w this w eak er indep endence assumption re-quiremen t helps impro ving the classi cation results. Notice that the same term that o ccurs in di eren t XML el-emen ts ma y ha v e di eren t meaning. F or example, in the follo wing do cumen t, the meaning of \course" is di eren tde-p ending on what elemen t the term b elongs to : &lt;resume&gt; &lt;name&gt;John Doe&lt;/name&gt; &lt;education&gt; &lt;/education&gt; &lt;hobby&gt;Taking walk on golf course.&lt;/hobby&gt; &lt;/resume&gt; W etak e the path expression of eac h term in the do cumen tb y augmen ting the tags of the structure elemen ts it b elongs to. F or example, the term \course" in education elemen t yields \resume.education.course", and the one in hobb y yields \re-sume.hobb y .course". T erms in the same elemen t share the same path, whereas the same term in di eren telemen ts ha v e di eren t path.
 The path expression is equiv alen t to the path of pre-order tra v ersal of a do cumen t tree. Eac h sub v ector, v d ( i; j ), of structured v ector of a do cumen t d corresp onds to an ele-men t, e d ( i; j ), of the structure tree. The sub v ector, v is con tained in one-lev el higher sub v ector, v d ( i 1 ;p v ector corresp onding to e d ( i 1 ;p i 1 ), the paren t elemen t of e d ( i; j ). Therefore, the nesting hierarc h y of the struc-tured v ector corresp onds to the pre-order tra v ersal of the tree structure. Th us the path expression and the nesting hierarc h y of the structured v ector are equiv alen t. Based on this prop ert y ,w e can enco de do cumen ts b y path expressions of terms in a single at v ector. Th us w e can en-jo y b oth the b ene t of structured mo del and the fast com-putation of a at v ector. F or classi cation of semi-structured do cumen ts, w ec ho ose the class that maximizes the follo wing a p osteriori class probabilit y:
P [ c j d ]= where ( c ) is the prior distribution of t on the class c . Refer to the de nition of the notations de ned in section 3.2. W e ran exp erimen ts with our classi er on the follo wing t w o datasets: 1. US P aten t Dataset W edo wnloaded 10,705 US paten t do cumen ts in three cate-gories from IBM's P aten tServ er 4 . W e randomly split the do cumen ts in to a training set of 7,136 do cumen ts and a test set of 3,569 do cumen ts. The do cumen ts are w ell categorized and the language is quite consisten t. The XML is generated b y a computer program with a common DTD. Th us, all do cumen ts share the same structure. W edo wnloaded ab out 1700 resumes of 52 topic areas from Y aho o 5 .F rom the resumes in HTML w e eliminated con ten t-irrelev an t formatting directiv es, h yp erlinks, images, and scripts and discarded ab out 450 do cumen ts that are mostly m ulti-media illustration. W e further discarded or merged, if ap-propriate, topic areas with less than 30 resumes. F or exam-ple, Biology , Chemistry , Earth Sciences, Mathematics, and Ph ysics w ere merged to Science all together. Finally ,w ecre-ated a sample set of 890 do cumen ts of 10 topic areas, whic h w as randomly divided in to a training set of 600 do cumen ts and a test set of 290 do cumen ts. W e compare the the p erformance of t w o algorithms: our new algorithm, called STR , and the classi er in tro duced in section 2 for at do cumen t mo del, called FLA T . The classi-cation errors w ere measured in 3 di eren t settings: STR F LAT noT ag s : FLA T without structure tags F LAT wT ag s : FLA T with structure tags The result is sho wn in gure 5: STR , FLAT w T ag s , FLAT ac hiev ed 17%, 24%, and 70% error rate, resp ectiv ely . It is not so clear wh y F LAT noT ag s p erforms so badly , ev en in comparison to F LAT wT ag s . Un til more con vincing explana-tion is found, w eten tativ ely conclude the structure informa-tion itself con tains v aluable information on classi cation. Note that STR did not p erform its full p oten tial due to the c haracteristics of the dataset. As sho wn in the equation 2 in section 3.2, STR tak es in to accoun t b oth the distribution of structure elemen t and the term distribution within the ele-men t. As discussed earlier, all paten t do cumen ts share the same sc hema. Moreo v er, they conform to the sc hema quite rigidly . Since all do cumen ts ha v e the same structure, equa-tion 3 is the same for all do cumen ts. Th us, STR di ers from FLA T only in the computation of term distribution in v ar-ious elemen ts (equation 4). The impro v emen t of error rates from 24% to 17% w as ac hiev ed only b y the di erence in term distribution induced b y do cumen t structure. This implicitly tells us that it w ould b e great b ene t to tak ein to accoun t the distribution of structure when do cumen ts structure in a corpus v aries signi can tly . F or r esume dataset, w e compare classi cation errors of the t w o algorithms in three di eren t settings: ST R F LAT r : FLA T on ra wHTML F LAT c : FLA T on HTML after cleansing As sho wn in gure 5, STR , FLAT c , F LAT r ac hiev ed 40%, 53%, and 67% error rate, resp ectiv ely . Though the error rate of FLAT r seems high, comparable error rate w as al-ready rep orted b y [6]. By using cleaned up HTML do c-umen ts, FLAT c ac hiev ed 14% impro v emen t o v er FLAT STR impro v ed 13% in comparison to FLAT c . In fact, the scale of error reduction b y ST R is m uc h greater consider-ing the signal-to-noise ratio of terms. By tag augmen tation, the n um b er of terms used b y STR is ab out 2.7 times more without adding an y more training set. Therefore, with the same signal-to-noise ratio, this sc heme has higher p oten tial to p erform b etter. The classi cation problem has b een addressed in statistical decision theory ,mac hine learning, and data mining. Deci-sion tree is a w ell-kno wn mac hine-learning tec hnique often used to classify n umerical and categorical data [3, 10]. In general, the classi ers for n umerical and categorical data handle data with m uc hlo w er dimensionalit y in comparison to text. [9] applied decision tree algorithms on text. Neural net w ork approac h is applied to text categorization b y [13]. Probabilistic classi ers are also used in text classi cation. T o mak e the computation simple, Naive Bayes t ypically as-sumes that the w ord o ccurrence is indep enden t. [9] rep orted the ev aluation results of Naive Bayes on Reuters. T APER [4] is a v ariation of Naive Bayes to build a classi er for hi-erarc hical taxonom y .
 All text classi ers discussed ab o v e assume that do cumen ts b eing classi ed are non-structured self-con tained do cumen ts. [6] is a classi er that is designed sp eci cally for h yp ertext suc h as HTML utilizing b oth lo cal and non-lo cal informa-tion (through h yp erlink) to the do cumen t. The pap er sho ws the classi ers designed for self-con tained do cumen ts p erform p o orly on h yp ertext b ecause the w eb do cumen ts are ex-tremely div erse. Ho w ev er, [6] still do es not tak ein to accoun t the structure of the do cumen twhic h also con tains v aluable information for classi cation. W e ha v e dev elop ed a no v el algorithm of classifying semi-structured do cumen ts b y extending the underlying do cu-men t mo del and using structure-based. The new classi er tak es adv an tage of the information laten t in the do cumen t structure as w ell as the text. Our exp erimen ts pro v ethat the metho d impro v es the accuracy of the classi cation. F or a set of US paten t do cumen ts, the new metho d cuts do wn classi cation error from 70% to 17%. As next steps, w e plan to explore con text-sensitiv e feature selection on the basis of do cumen t structure.
