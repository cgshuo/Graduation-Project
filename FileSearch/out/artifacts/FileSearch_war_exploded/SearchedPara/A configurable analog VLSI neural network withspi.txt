 Neuromorphic analog, VLSI devices [12] try to derive organi zational and computational principles from biologically plausible models of neural systems, aimi ng at providing in the long run an elec-tronic substrate for innovative, bio-inspired computatio nal paradigms.
 In line with standard assumptions in computational neurosc ience, neuromorphic devices are en-dowed with adaptive capabilities through various forms of p lasticity in the synapses which connect the neural elements. A widely adopted framework goes under t he name of Hebbian learning, by which the efficacy of a synapse is potentiated (the post-syna ptic effect of a spike is enhanced) if the pre-and post-synaptic neurons are simultaneously active o n a suitable time scale. Different mech-anisms have been proposed, some relying on the average firing rates of the pre-and post-synaptic neurons, (rate-based Hebbian learning), others based on ti ght constraints on the time lags between pre-and post-synaptic spikes ( X  X pike-Timing-Dependent-Plasticity X ).
 The synaptic circuits described in what follows implement a stochastic version of rate-based Heb-bian learning. In the last decade, it has been realized that g eneral constraints plausibly met by any concrete implementation of a synaptic device in a neural net work, bear profound consequences on the capacity of the network as a memory system. Specifically, once one accepts that a synaptic element can neither have an unlimited dynamic range (i.e. sy naptic efficacy is bounded), nor can it undergo arbitrarily small changes (i.e. synaptic efficacy h as a finite analog depth), it has been proven ([1], [7]) that a deterministic learning prescription impl ies an extremely low memory capacity, and a severe  X  X alimpsest X  property: new memories quickly erase t he trace of older ones. It turns out that a stochastic mechanism provides a general, logically appeal ing and very efficient solution: given the pre-and post-synaptic neural activities, the synapse is st ill made eligible for changing its efficacy according to a Hebbian prescription, but it actually change s its state with a given probability. The stochastic element of the learning dynamics would imply ad h oc new elements, were it not for the fact that for a spike-driven implementation of the synapse, the noisy activity of the neurons in the network can provide the needed  X  X oise generator X  [7]. There fore, for an efficient learning electronic network, the implementation of the neuron as a spiking eleme nt is not only a requirement of  X  X iolog-ical plausibility X , but a compelling computational requir ement. Learning in networks of spiking IF neurons with stochastic plastic synapses has been studied t heoretically [7], [10], [2], and stochastic, bi-stable synaptic models have been implemented in silicon [8], [6]. One of the limitations so far, both at the theoretical and the implementation level, has be en the artificially simple statistics of the stimuli to be learnt (e.g., no overlap between their neural r epresentations). Very recently in [4] a modification of the above stochastic, bi-stable synaptic mo del has been proposed, endowed with a regulatory mechanism termed  X  X top learning X  such that syna ptic up or down-regulation depends on the average activity of the postsynaptic neuron in the recen t past; a synapse pointing to a neuron that is found to be highly active, or poorly active, should not be f urther potentiated or depressed, respec-tively. The reason behind the prescription is essentially t hat for correlated patterns to be learnt by the network, a successful strategy should de-emphasize the coherent synaptic Hebbian potentiation that would result for the overlapping part of the synaptic ma trix, and that would ultimately spoil the ability to distinguish the patterns. A detailed learning st rategy along this line was proven in [13] to be appropriate for linearly separable patterns for a Percep tron-like network; the extension to spiking and recurrent networks is currently studied.
 In section 2 we give an overview of the chip architecture and o f the implemented synaptic model. In section 3 we show an example of the measures effectuated on the chip useful to characterize the synaptic and neuronal parameters. In section 4 we report som e characterization results compared with a theoretical prediction obtained from a chip-oriente d simulation. The last paragraph describes chip performances in a simple classification task, and illus trate the improvement brought about by the stop-learning mechanism. The chip, already described in [3] implements a recurrent ne twork of 32 integrate-and-fire neurons with spike-frequency adaptation and bi-stable, stochasti c, Hebbian synapses. A completely recon-figurable synaptic matrix supports up to all-to-all recurre nt connectivity, and AER-based external connectivity. Besides establishing an arbitrary synaptic connectivity, the excitatory/inhibitory na-ture of each synapse can also be set.
 The implemented neuron is the IF neuron with constant leakag e term and a lower bound for the membrane potential V ( t ) introduced in [12] and studied theoretically in [9]. The cir cuit is borrowed neurons can be directly probed (i.e., their  X  X embrane poten tial X  sampled), while for all of them the emitted spikes can be monitored via AER [5]. The dendritic tr ee of each neuron is composed of up to 31 activated recurrent synapses and up to 32 activated e xternal, AER ones. For the recurrent synapses, each impinging spike triggers short-time (and po ssibly long-term) changes in the state of the synapse, as detailed below. Spikes from neurons outside the chip come in the form of AER events, and are targeted to the correct AER synapse by the X-Y Decoder. Synapses which are set to be excitatory, either AER or recurrent are plastic; inhibit ory synapses are fixed. Spikes generated by the neurons in the chip are arbitrated for access to the AER bu s for monitoring and/or mapping to external targets.
 The synaptic circuit described in [3] implements the model p roposed in [4] and briefly motivated in the Introduction. The synapse possesses only two states of e fficacy (a bi-stable device): the internal synaptic dynamics is associated with an internal variable X ; when X &gt;  X  potentiated, otherwise is set to be depressed. X is subjected to short-term, spike-driven dynamics: upon the arrival of an impinging spike, X is candidate for an upward or downward jump, depending on the instantaneous value of the post-synaptic potential V The jump is actually performed or not depending on a further v ariable as explained below. In the absence of intervening spikes X is forced to drift towards a  X  X igh X  or  X  X ow X  value depending o n whether the last jump left it above or below  X  scale.
 A further variable is associated with the post-synaptic neu ron dynamics, which essentially mea-sures the average firing activity. Following [4], by analogy with the role played by the intracellular concentration of calcium ions upon spike emission, we will c all it a  X  X alcium variable X  C ( t ) . C ( t ) undergoes an upward jump when the postsynaptic neuron emits a spike, and linearly decays between two spikes. It therefore integrates the spikes sequence and , when compared to suitable thresholds as detailed below, it determines which candidate synaptic jum ps will be allowed to occur; for example, it can constrain the synapse to stop up-regulating because t he post-synaptic neuron is already very active. C ( t ) acts as a regulatory element of the synaptic dynamics.
 The resulting short-term dynamics for the internal synapti c variable X is described by the following conditions: X ( t )  X  X ( t ) + J if description of circuits implementing these conditions can be found in [3].
 In figure 1 we illustrate the effect of the calcium dynamics on X . Increasing input forces the post-synaptic neuron to fire at increasing frequencies. As long as C ( t ) &lt; V both up and down jumps. When C ( t ) &gt; V towards its lower bound.
 Figure 1: Illustrative example of the stop-learning mechan ism (see text). Top to bottom: post-synaptic neuron potential V ron potential V We report synapse potentiation (LTP) / depression (LTD)fro m the chip and we compare experimental results to simulations.
 For each synapse in a subset of 31, we generate a pre-synaptic poisson spike train at 70 Hz. The post synaptic neuron is forced to fire a poisson spike train by applying an external DC current and a poisson train of inhibitory spikes through AER. Setting to z ero both the potentiated and depressed efficacies, the activity of the post-synaptic neuron can be e asily tuned by varying the amplitude of the DC current and the frequency of the inhibitory AER train. We initialize the 31 (AER) synapses to depressed (potentiated) and we monitor the post-synapti c neuron activity during a stimulation trial lasting 0.5 seconds. At the end of the trial we read the s ynaptic state using an AER protocol developed to this purpose. For each chosen value of the post-synaptic firing rate, we evaluate the probability to find synapses in a potentiated (depressed) st ate repeating the test 50 times. The results reported in figure 2 (solid lines) represent the average LTP a nd LTD probabilities per trail over the 31 synapses. Tests were performed both with active and inactiv e Calcium mechanism. When calcium mechanism is inactive, the LTP is monotonically increasing with the post-synaptic firing rate while when the calcium circuit is activated the LTP probability ha s a max form V Identical tests were also run in simulation (dashed curves i n figure 2). For the purpose of a meaning-ful comparison with the chip behaviour relevant parameter a ffecting neural and synaptic dynamics and their distributions (due to inhomogenities and mismatc hes) are characterized.
 Simulated and measured data are in qualitative agreement. T he parameters we chose for these tests are the same used for the classification task described in the next paragraph. Figure 2: Transition probabilities. Red and blue lines are L TP probabilities with and without cal-cium stop-learning mechanism respectively. Gray lines are LTD probabilities without calcium stop-learning mechanism, the case LTD with Ca mechanism is not sho wn. Error bars are standard devia-tions over the 50 trials We configured the synaptic matrix to have a perceptron like ne twork with 1 output and 32 inputs (32 AER synapses). 31 synapses are set as plastic excitatory one s, the 32nd is set as inhibitory and used to modulate the post-synpatic neuron activity. Our aim is to teach the perceptron to classify two patterns through a semi-supervised learning strategy:  X  X p  X  and  X  X own X . We expect that after learn-ing the perceptron will respond with high output frequency f or pattern  X  X p X  and with low output frequency for pattern  X  X own X . The self regulating Ca mechan ism is exploited to improve perfor-mances when Up and Down patterns have a significant overlap. T he learning is semi-supervised: for each pattern a  X  X eacher X  input is sent to the output neuro n steering its activity to be high or low, as desired. At the end of the learning period the  X  X eacher X  is turned off and the perceptron output is driven only by the input stimuli: in this conditions its clas sification ability is tested. We present learning performances for input patterns with in creasing overlap, and demonstrate the effect of the stop learning mechanism (overlap ranging from 6 to 14 synapses).
 Upon stimulation active pre-synaptic inputs are poisson sp ike trains at 70 Hz, while inactive inputs are poisson spike trains at 10 Hz. Each trial lasts half a seco nd. Up and Down patterns are randomly presented with equal probability. The teaching signal, a co mbination of an excitatory constant cur-rent and of an inhibitory AER spike train, forces the output fi ring rate to 50 or 0.5 Hz. One run lasts for 150 trials which is sufficient for the stabilization of th e output frequencies. At the end of each trial we turn off the teaching signal, we freeze the synaptic dynamics and we read the state of each synapse using an AER protocol developed for this purpose. In these conditions we performed a 5 seconds test ( X  X hecking Phase X ) to measure the perceptron f requencies when pattern Up or pattern Down are presented. Each experiment includes 50 runs. For ea ch run we change: a) the  X  X efinition X  of patterns Up and Down: inputs activated by pattern Up and Do wn are chosen randomly at the beginning of each run; b) the initial synaptic state, with th e constraint that only about 30 % of the synapses are potentiated; c) the stimulation sequence.
 For the first experiment we turned off the stop learning mecha nism and we chose orthogonal patterns. In this case the perceptron was able to correctly classify th e stimuli: after about 50 trials, choosing a suitable threshold, one can discriminate the perceptron ou put to different patterns (lower left panel on figure 4). The output frequency separation slightly incre ases until trial number 100 remaining almost stable after that point.
 We then studied the case of overlapped patterns both with act ive and inactive Calcium mechanism. We repeated the experiment with an increasing overlap: 6, 10 and 14. (implying an increase in the coding level from 0.5 for the orthogonal case to 0.7 for th e overlap equal to 14). Only the threshold K up parameters are tuned so that the Ca variable passes K up synaptic neuron around 80 Hz. We show in figure 3 the distribut ions of the potentiated fraction of the synapses over the 50 runs at different stages along the ru n for overlap 10 with inactive (upper panels) and active (lower panels) calcium mechanism. We div ided synapses in three subgroups: Up (red) synapses with pre-synaptic input activated solely by Up pattern, Down (blue) synapses with pre-synaptic inputs activated only by Down pattern, and Ove rlap (green) synapses with pre-synpatic inputs activated by both pattern Up and Down. The state of the synapses is recorded after every learning step. Accumulating statistics over the 50 runs we o btain the distributions reported in figure 3. The fraction of potentiated synapses is calculated over t he number of synapses belonging to each subgroup. When the stop learning mechanism is inactive, at t he end of the experiment, the green Figure 3: Distribution of the fraction of potentiated synap ses. The number of inputs belonging to both patterns is 10. distribution of overlap synapses is broad, when the Calcium mechanism is active, synapses overlap tend to be depotentiated. This result is the  X  X icroscopic X  e ffect of the stop learning mechanism: once the number of potentiated synapses is sufficient to driv e the perceptron output frequency above 80 Hz, the overlap synapses tend to be depotentiated. Overla p synapses would be pushed half of the times to the potentiated state and half of the times to the dep ressed state, so that it is more likely for the Up synapses to reach earlier the potentiated state. When the stop learning mechanism is active, the potentiated synapses are enogh to drive the output neuro n about 80 Hz, further potentiation is inhibited for all synapses so that overlap synapses get depr essed on average. This happens under the condition that the transition probability are sufficiently small to avoid that at each trial the learning is completely disrupted. The distribution of the output frequ encies for increasing overlap is illustrated in figure 4 (Ca mechanism inactive in the upper panels, active for the lower panels). The frequencies are recorded during the  X  X hecking phase X . In blue the histog rams of the output frequency for the distribution remain well separated even for high overlap wh en the Calcium mechanism is active. A quantitative parameter to describe the distribution sepa ration is  X  values are summarized in table 1.
 Figure 4: Distributions of perceptron frequencies after le arning two overlapped patterns. Blue bars refer to pattern Down stimulation, red bars refers to patter n Up. Each panel refers to overlap. For each run the number of potentiated synapses is different due to the random choices of Up, Down and Overlap synapses for each run and the mismatches affecti ng the behavior of different synapses. The failure of the discrimination for high overlap in the abs ence of this stop learning mechanism is due to the fact that the number of potentiated synapses can overcome the effect of the teaching signal for the down pattern. The Calcium mechanism, defining a maximum number of allowed potentiated synapses, limits this problem. This offer the p ossibility of establishing a priori threshold to discriminate the perceptron outputs on the basis of the fr equency corresponding to the maximum value of the LTP probability curve. We briefly illustrate an analog VLSI chip implementing a netw ork of 32 IF neurons and 2,048 reconfigurable, Hebbian, plastic, stop-learning synapses . Circuit parameters has been measured as well as their dispersion across the chip. Using these data a c hip-oriented simulation was set up and its results, compared to experimental ones, demonstrate th at circuits behavior follow the theoretical predictions. Once configured the network as a perceptron (31 AER synapses and one output neuron), a classification task has been performed. Stimuli with an inc reasing overlap have been used. The results show the ability of the network to efficiently classi fy the presented patterns as well as the improvement of the performances due to the calcium stop-lea rning mechanism.
 [1] D.J. Amit and S. Fusi. Neural Computation , 6:957, 1994. [2] D.J. Amit and G. Mongillo. Neural Computation , 15:565, 2003. [3] D. Badoni, M. Giulioni, V. Dante, and P. Del Giudice. In Proc. IEEE International Symposium [4] J.M. Brader, W. Senn, and S. Fusi. Neural Computation (in press) , 2007. [5] V. Dante, P. Del Giudice, and A. M. Whatley. The neuromorp hic engineer newsletter. 2005. [6] E. Chicca et al. IEEE Transactions on Neural Networks , 14(5):1297, 2003. [7] S. Fusi. Biological Cybernetics , 87:459, 2002. [8] S. Fusi, M. Annunziato, D. Badoni, A. Salamon, and D.J. Am it. Neural Computation , 12:2227, [9] S. Fusi and M. Mattia. Neural Computation , 11:633, 1999. [10] P. Del Giudice, S. Fusi, and M. Mattia. Journal of Physiology Paris , 97:659, 2003. [11] G. Indiveri. In Proc. IEEE International Symposium on Circuits and Systems , 2003. [12] C. Mead. Analog VLSI and neural systems . Addison-Wesley, 1989. [13] W. Senn and S. Fusi. Neural Computation , 17:2106, 2005.
