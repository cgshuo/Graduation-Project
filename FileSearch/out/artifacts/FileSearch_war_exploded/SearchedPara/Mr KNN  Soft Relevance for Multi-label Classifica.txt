 Multi-label classification refers to learning tasks with each instance belonging to one or more classes simultaneously. It arose from real-world applications such as information retrieval, text categorization and functional genom ics. Currently, most of the multi-label learning methods use the strategy called binary relevance, which constructs a cla ssifier for each unique label by grouping data into positives (examples with this label) and negatives (examples without this la bel). With binary relevance, an example with multiple labels is considered as a positive data for each label it belongs to. For some classes, this data point may behave like an outlier confusing classifiers, especially in the cases of well-separated classes. In this paper, we first introduce a new strategy called soft relevance, where each multi-label example is assigned a relevance score to the labels it belongs to. This soft relevance is then employed in a voting function used in a k nearest introduced to the k nearest neighbor classifier for better performance. We compare the proposed method to other multi-label learning methods over three multi-label datasets and demonstrate that the proposed me thod provides an effective way to multi-label learning. I.2.6 [ Artificial Intelligence ]: Learning  X  induction; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  clustering, information filtering Algorithms, Experimentation, Theory Multi-label classification, soft relevance, k-nearest neighbors While most of the research efforts in machine learning have been devoted to single-label classification problems, where each instance is restricted to exactly one class, multi-label classification is drawing increasing interest and emerging as a fast-growing research field. Multi-label lear ning, arising from real-world assigned to one or more classes (labels). For example, in web search, each returned webpage with a given query may be labeled with more than one categories: consider the following webpage http://www.pbs.org/sci ence/science_he alth.html, which may be annotated as  X  X cience X ,  X  X ealth  X ,  X  X ducation X , and  X  X ews and Media X , four out of 14 top-level categories used by Yahoo! Search. Other applications with multi-label classification include automatic text categorization [1-2], where each free-text document may be assigned to multiple predefined categories; scene classification of images and videos [3-7], where an image may have more than one tags; functional genomics [8-12], where a gene can be annotated with a set of functions [13-14]; music categorization into emoti ons [15-17] and directed marketing [18]. Over the years, there have been a variety of methods developed for multi-label classifications. These methods are grouped as either problem transformation methods or algorithm adaptation methods [19]. Problem transformation methods first transform multi-label learning tasks into multiple single-label learning tasks, which are then handled by the standard single-label learning algorithms. The first family of the problem transformation methods is based on  X  X opy X  or  X  X election X  [3, 20]. Copy of one single label (each labe l is a unique element of the k labels). A constant weight of 1/ k may also be assigned to each of the copied instances ( X  X opy-weight X ). Alternatively, one can use a selection strategy by replacing the multiple labels of each instance with a single label that is the most frequent ( X  X elect-max X ) or least frequent ( X  X elect-min X ), or rando mly selected ( X  X elect-random X ). Another simple transformation is to use the data with a single label only ( X  X gnore X ). The second family is called Label Powerset (LP) method and its variants [ 21-22]. LP methods treat each unique set of labels in the training set as a new (and single) label, and reconstruct the training set w ith the newly defined labels. To deal with the small sample pr oblems in LP, the pruned problem transformation method simply dis cards the newly-defined labels with very few training data and reassigns these data into other relevant labels. Alte rnatively, the random k -labelsets method ensembles classifiers constructed from randomly selected LP label sets. The third family is based on binary relevance (BR) [23]. Rather than focusing on reassigning samples with labels, BR uses a popular yet simple strategy commonly-employed in multi-class classification, namely  X  one-versus-rest X . Basica lly, it constructs a binary classifier for each unique label using training data grouped as positives (instances with this label) and negatives (instances without this label). An alternative strategy for multi-class classification is called  X  X ne-versus-one X , which is also explored in multi-label learning. Ranking by pairwise comparison (PRC) methods construct binary classifiers for the  X   X   X  possible class pairings where m is the total number of unique labels and each classifier is trained on the data belonging to the two classes (but not both) [24]. Furthermore, extended from PRC, the calibrated label ranking met hod introduces an artificial label and combines the PRC and BR learning strategies [25]. Algorithm adaption methods modify single-label learning algorithms for multi-label data le arning. McCallum first proposed a mixture model based on na X ve Bayes and EM algorithms for multi-label text classification [1]. About one year later, Schapire and Singer proposed the use of boosting-based systems for text categorization and automatic call-type identification from unconstrained spoken customer re sponses [2]. They developed two boosting algorithms by maintaining a set of weights over training examples and associated labels: AdaBoost.MH minimizing Hamming loss and AdaB oost.MR ranking the labels such that the top-ranked labels are also correct. Ghamrawi and McCallum investigated a multi-label conditional random field classification method that models dependencies between labels [26]. Kernel-based hierarchical classification methods are also popular in multi-label text classification. Cai and Hofmann extended support vector machines (SVMs) learning and integrated discriminant functions for hierar chical classification of document categorization problems [27]. Rous u et al. used a kernel-based algorithm modified from the Maximum Margin Markov Network framework [28]. Another family of learning methods treat multi-label learning problems as ranki ng problems, where a score is assigned to every instance-label pair; traditional learning methods such as SVMs [12], neural networks [29], and online ranking methods [30], were then used fo r classification. Other learning methods such as decision trees [11], probabilistic generative models [31], and Bayesian rule [32] are also adapted for multi-label classifica tion problems. A number of multi-label learning methods are adapted from a case-based learning called k nearest neighbor ( k NN) method [7, 16, 33-34], which is a simple yet powerful learning approach. k NN makes predictions of a test data based on its k nearest neighbors and a majority voting rule. k NN is easy to implement and appropriate by nature for multi-label classifications. With an infinite number of data, k NN is assured of becoming optimal [35]. k NN-based approaches have shown great potential in multi-label learning problems [7, 16, 33-34]. The multi-label k NN learning method (ML-KNN) applies maximu m a posteriori principle for classification and ranking, where th e likelihood is estimated using the k nearest neighbors of a data [7 ]. Brinker and Hullermeier demonstrated that k NN-based learning approach is competitive with or even better than stat e-of-the-art-model-based methods [33]. Similar results are also observed by Dimou et al. [36]. Up to date, all the k NN-based multi-label learning methods use the popular BR strategy [23]. Although effective, BR may performance of classifiers, as we will discuss in Section 3. Furthermore, the estimation of the posteriori in ML-KNN may be inaccurate, due to the facts that the samples with and without a particular label are typically hi ghly imbalanced and also it is highly possible that only few sample s are available for some given number of nearest neighbors with a certain number of labels. To address these problems, we propose a novel k NN-based multi-label learning approach ca lled voting Margin-Ratio k NN (Mr.KNN), which introduces the voting margin-ratio concept and soft relevance in the vote strategy. We test it on three commonly-used datasets and experimental results show a significant improvement in performance compared to the ML-KNN method. The major contribution of this paper is summarized as follows: (1) we propose a novel learning al gorithm that integrates both problem transformation methods and algorithm adaptation methods. (2) A new concept called so ft relevance is introduced for data transformation. Rather than making a hard decision on label assignments, we introduce the use of a modified fuzzy c-means algorithm in a supervised setting, which will provide a relevance score of an instance with respect to a particular label. This score is produced based on real data dist ribution. (3) We introduce a new voting schema in Mr.KNN, whic h is based on the distances relevance of each nearest neighbor s. Furthermore, a margin-ratio metric, which is a critical issue in k NN design. (4) We evaluate the proposed algorithm and provide the comparison with the ML-KNN algorithm. A detailed discu ssion about the newly proposed method is also presented. we briefly review the ML-KNN al gorithm. This is followed by a detailed description of the new al gorithm we propose in Section 3. We compare different algorithms with three multi-label datasets and show the results in Section 4. Finally, we conclude with discussions in Section 5. In this section, we briefly revi ew problem transformation methods and algorithm adaptation methods [19]. We will then discuss the ML-KNN algorithm. Throughout this paper, we will use the following notations. Let  X   X   X  X   X   X  X ,  X   X   X   X  X  X  X   X  multi-label examples with input vectors  X  X   X   X  X   X  and class label vectors  X  X   X   X   X   X , X   X   X  . A  X 1 X  in the j-th component of a class label vector indicates that the associated instance belongs to the j-th class. For each multi-label instance, problem transformation methods convert it into one or multiple instances with a single label. For illustration, we consider a five-label classification problem with a multi-label data set shown in Table 1. This data set consists of two Tables 2 and 3 show the new si ngle-label datasets transformed from the original multi-label data set by simply labeling each multi-label instance with the most frequent (select-max) and the least frequent (select-min) labels, respectively. As can be seen, this strategy will most likely create highly imbalanced datasets. Another popular strategy employ ed in problem transformation method is the so-called binary relevance, which converts the multi-label learning problem to multiple single-label binary classification problems. For example, for the data set shown in Fig. 1, five new data sets will be generated, each corresponding to a particular class label (Table 4). For each data set in Table 4, an instance with the associated labe l is marked as positive (+), or negative (-) otherwise. Standard binary classification algorithms can then be applied to each dataset. While there are some other similar strategies [19], a common problem in problem transformati on methods is that multi-label instances are forced into one single category without considering data distribution. For example, with select-max strategy, many classes would consist of very few positive examples and dominant number of negative examples. In Section 3.1, we will further discuss the potential problems with a binary relevance strategy.  X   X   X  )  X   X   X   X   X   X   X  X   X   X  X   X   X  X   X   X  X   X 
 X  X   X   X   X   X  Algorithm adaptation methods m odify standard single-label learning algorithms for multi-label classification. For example, among many others, decision trees [11], AdaBoost [2], and support vector machines [27] are adapted for multi-label learning. In [11], the C4.5 is adapted by allowing leaves of a tree to represent a set of labels. Furthe rmore, to measure the amount of  X   X   X   X   X   X   X   X  X  X  X   X   X   X   X   X  X   X   X   X   X   X  X  X  X   X   X   X   X   X   X  X  X  X  , where  X   X   X  probability of class  X   X  and  X   X   X   X   X   X  X  X  X  X 1 X   X   X  . AdaBoost.MH is an extension of AdaBoost for multi-label and multi-class learning tasks [2]. It deals with multi-label learning problems with a divide and conque r strategy and maintains a set of weights as a distribution over both training examples and associated labels. To overcome the potential overfitting problems in AdaBoost.MH or other adapted methods, a SVM-like optimization strategy is introduced for multi-label learning [11], where a multi-label learning problem is treated as a ranking problem and a linear model that minimizes a ranking loss and maximizes a margin is developed. Let  X   X  X  X  X   X   X  denote the training data subset consisting of the k nearest neighbors of the example  X  X   X  . Let  X   X   X  expressed as follows.  X   X   X   X   X   X   X  X   X   X  X  X  X   X   X  ML-KNN assigns the j-th label to an instance using the binary relevance strategy. First, k nearest neighbors  X   X   X  X  instance in terms of the probability:  X   X  X  X   X   X   X   X  X 1 X  neighbors belonging to the j -th class. Specifically, let If  X   X  &gt; 1,  X  X then  X   X   X   X   X 1 ; otherwise,  X  X  and prior can be estimated from training data using frequency counting for each  X  X   X   X ,...,0,1  X  (where a smoothing parameter is used to control the strength of uniform prior) [7]. Note that by number of positive and negati ve examples with exactly  X  nearest neighbors belonging to the j -th class, respectively (this ratio is slightly different with a smoothing parameter). With the BR strategy, data distributions for some labels are highly imbalanced (i.e., the number of positive samples is much less than that of negative samples). Conse quently, the ratio estimation may not be accurate. Fig. 1 shows the da ta distribution of each label for the yeast data with 14 labels (mor e information about this data set can be found in Section 4). The y-axis is the ratio between the number of data with a particular label (x-axis) and total number of samples. While more than 70% of samples have labels 12 and 13, only less than 2% of samples have label 14. Mr.KNN consists of two component s: a modified fuzzy c-means (FCM)-based approach to produce soft relevance and a modified k NN for multi-label classification. To see the limitation of the binary relevance strategy, consider an example shown in Fig. 2. In Fig. 2, data points from three classes (represented by triangles, cross sy mbols, and circles) are plotted in a two-dimensional feature space. The instance marked with a cross inside a circle belongs to tw o classes, marked by circle and cross symbols. As can be seen, for the class with circles, this instance looks like a typical data . However, it may be an outlier for the class with cross. In bina ry relevance-based methods, this instance will be used in both classes as positive samples, which may degrade the classification performance. To deal with this problem, we propose the applica tion of an unsupervised learning algorithm in a supervised setting. Specifically, we will adapt the fuzzy c-means (FCM) algorithm [37] to yield a soft relevance value for each instance with respect to each label. This soft relevance indicates the strength of an instance related to a label in the given feature space. To modify FCMs for supervised problems, we treat each class as a cluster and denote  X   X  X  X  the membership (relevance) value of an  X   X ,..., X 1,2  X   X  . Our goal is to find an optimal fuzzy c-partition by minimizing the following cost function:  X   X   X  X  X  X   X   X   X  X  X   X   X   X   X   X  X   X   X  X ,  X   X   X   X  where m is a weighting exponent on each fuzzy membership, called a fuzzifier, and is typically set equal to 2;  X   X   X  X  Minkowski distance defined as  X   X   X  X   X   X  X ,  X   X   X   X   X  X  X  X  X  X  X   X  X  X   X  X   X  X  X   X   X  where f is a positive integer and f  X  1;  X   X  X  X  and  X  Minkowski distance can handle different shape of (classes) clusters. For example, f = 2 corresponds to an Euclidean distance, corresponds to a  X   X  norm targeting clusters with a diamond-like shape [38]. One can also introduce a covariance matrix or a weighting vector in the distance measure to reflect the importance of difference features. However, determining the matrix or weights is another research topic and will not be discussed here. Each membership  X   X  X  X  is between zero and one and satisfies the following equation: Furthermore, unlike an unsupervised learning task where data labels are unknown, the class labels for each training data are known, which can be formulated as follows:  X  X  X   X   X   X   X   X   X  X  X  Eq. (5) reinforce that if an instance does not have a particular label, then the associated membership is zero. To find the membership values, we minimize the cost function  X  respect to all the memberships and the prototypes, subject to the Lagrangian function  X  X  X  X  X  X   X   X   X  X  X   X   X   X   X   X  X   X   X  X ,  X   X   X   X  where  X   X  and  X   X  are Lagrangian coefficients. By minimizing Eq. 6 with respect to  X   X  X  X  and using Eqs. (4) and (5), we obtain the class membership as  X  To find the update for the cluster centers for the fixed  X  need to take the gradient of Eq. 6 with respect to  X  X   X   X  cannot get closed form solutions and iterative techniques will be used. If f is even and finite, we have  X  X   X   X  X   X   X  X ,  X   X   X   X  Consequently, from  X  X  X   X   X   X   X  X  X   X   X  Eq. 9 can be solved by the well-known Gauss-Newton method [39]. In this study, we are also interested in evaluating the  X  norm, where f = 1 by following the nonlinear constrained optimization procedures described in [40] for center updating. The algorithm we just describe a ssigns soft relevance score (the membership) to each instance with respect to a label. Since it is developed for data with known and multiple labels, we name it multi-label FCMs (MLFCM). Like general FCMs, MLFCM consists of two iterative steps: (1) update the class membership  X   X  X  X  based on Eq. 7 for current cent ers; and (2) update the centers  X  X   X   X  for the membership obtained from step 1 using Eq. 9 for even Instead of randomly generating cl ass centers, we take advantage of the known labels by using the sample means of each class as the starting centers, which guarantees that the final membership is not dependent on the initial selection of centers and that the algorithm converges quickly. By running MLFCM, we will obtain voting factors, as described next. A standard k NN method assigns class labe ls to a test instance voting function that relates an instance  X  X   X  to the j-th class label is defined as follows  X  X  X  X   X   X  X   X   X   X   X   X 1  X   X   X  X  X   X   X   X  X  X  X  When applied to our multi-label learning problems, however, two issues need to be addressed. The first issue is the imbalanced data distribution, as seen in Fig. 1. For example, for the yeast dataset, most of the neighbors will have labels 12 and 13. By majority voting, the majority of test data will be assigned to class labels 12 and 13. The second issue is that the voting defined in Eq. 10 does not take into account the distance s between a test instance and its k nearest neighbors. To address these problems, we incorporate a distance weighting method [ 41] and the soft relevance  X  from our MLFCMs. The new voti ng function is defined as where the distance  X   X   X  X   X   X  X ,  X   X  is the Minkowski distance defined in Eq. 3. To determine the optimal values of f in the Minkowski distance and k in the k NN, we introduce a new evaluation function, which is motivated by the well-known ma rgin concept [42]. Consider a five-class learning problem with an instance belonging to two the instance is in the center (a plus symbol inside a circle); a circle represents a voting value for the label marked by the number inside a circle. For example, Fig. 3(a) indicates that the instance receives largest vote from label 2, fo llowed by labels 3, 5, 4 and 1. While both Figs. (a) and (b) have largest votes for the true labels 2 and 3 the instance belongs to, we prefer f and k that produce Fig. 3(b), as it has a larger voting margin (marked by an arrow in Fig. 3), which is defined as the voting difference between the true label with smallest voting (e.g., label 3) and the false label with the largest voting (e.g., label 5). This can be formulated as follows where  X   X  and  X   X  represent the true label set and false label set for instance i , respectively. and Fig. 3(c) shows an example where the vote for the true label 3 is smaller th an that for both labels 5 and 4, so the margin may be negative. Our goal is to seek the combination of f and k that maximizes the average voting margin ratio, which is defined as the average ratio of voting between the true label with smallest voting (e.g., label 3) and the false label with the largest voting (e.g., label 5). The overall learning method for multi-label learning is called voting Margin Ratio k NN, or Mr.KNN. Mr. KNN consists of two steps: tr aining and test. The procedures are summarized in Fig. 4. ______________________________________ Training (offline): Input: training data Output: f, k , soft relevance for each training data (  X   X  X  X  for (each combination of f and k ) do { MLFCM; { Identify k nearest neighbors }; }; Save f, k , and  X   X  X  X  with the largest average voting margin ratio Leave-one-out-cross-validation to determine the threshold ( th ) Test (online): Input: test data Output: labels for each test data for (each test data) do { Label assignment }; ______________________________________ Next, we apply the proposed method to three multi-label classification problems and to evaluate its performance. Experimental results in [7 , 36] showed that ML-KNN outperformed existing multi-label learning methods, such as BoosTexter [2] and Rank-SVM [12]. In this session, we conduct a comparative study betwee n Mr.KNN and ML-KNN. Three commonly-used multi-label datase ts are tested in this study. The first task is to predict gene functions of yeast. Each gene can have several functional categories and is characterized by microarray expression and phylogenetic profiles [12]. The second learning problem is about automa tic detection of emotions in music, where each song is labele d using six categories: amazed-surprised, happy-pleased, relaxing-calm, quiet-still, sad-lonely, and angry-fearful [17]. The third problem is about semantic scene classification, where a natural scene may consist of objects from different labels [3]. Table 5 lists the statistics for the three datasets, where label cardinality is defined as the aver age number of labels per instance and label density is the averag e number of labels per example divided by the total number of unique labels. As seen, the yeast data have the largest label cardinality. To evaluate the performance of learning methods, we choose four criteria commonly-used in multi-label classification: Hamming  X   X   X  X   X   X  X ,  X   X  X ,  X   X   X   X  X  X  X   X  , where a test instance  X  X   X  vector  X  X   X   X   X   X , X   X   X  , and the predicted label vector  X  X  Hamming loss is defined as follows:  X  X  X  X  X  X  where  X  represents an exclusive OR (XOR) operation in Boolean logic. The other three measures are:  X  X  X  X  X  X  X  X  X  X  where  X  X   X   X  X , is the inner product between two vectors u and v .  X   X  X  X  X  X  X  and  X   X  X  X  X  X  X  X  X  where  X  represents a logic OR operation in Boolean logic. In addition, we also introduce a ranking-based measure commonly-used in learning to rank tasks [43]: normalized discounted cumulative gain (NDCG) [44]. In multi-label learning, we use NDCG to evaluate the final ranking of labels for each instance, not the binary label vector. In other words, for each instance, a label will receive a voting score. Ideally, these true labels will rank higher than false labels. The NDCG of a ranking list of labels at position n is defined as where r ( i ) is the ranked relevance of the label at position i (in our application, it is either zero or one) and the normalization constant Z is chosen so that the NDCG of a perfect ranking is 1. For each dataset, we select the f in the Minkowski distance from 1, 2, 4, and 6; and k in the k NN from 10, 15, 20, 25, 30, 35, 40, and 45, which results in 32 combinations of ( f , k ). The average voting margin ratio is used to choose th e optimal parameters as described in Section 3. Fig. 5 shows an example of the average voting margin ratio versus ( f , k ) obtained from training data set of yeast. the optimal parameters for testing. Table 2 shows the test results for yeast. Mr.KNN outperform s ML-KNN in terms of the Hamming loss, accuracy, precision, and recall. For yeast data, since the cardinality is 4.237 (Tab le 5), we calculate the NDCG with n  X  6. Fig. 6 shows the difference of NDCG for Mr.KNN and ML-KNN, which conforms to th e results shown in Table 6. We also run the two KNN-based al gorithms to emotion and scene datasets, both with sma ller cardinality than the yeast data. Tables 7 and 8 show the results for emotion and scene datasets, respectively. Interestingly, both datasets show that ML-KNN produces smaller Hamming loss yet lower accuracy than Mr.KNN. One of the reasons is probably the data distribution. Unlike the yeast data, where few labels are dominant, both emotions and scene data are much balanced and evenly distributed, which is evident from Fi g. 7 and Fig. 8. As such, the data imbalance problem is not se vere with the binary relevance strategy. Even so, Mr.KNN still yi elds better accuracy, precision, and recall than ML-KNN, as shown in Tables 7 and 8. We do not compute the NDCG scores for these two datasets as their cardinalities are around one. To see the effect of number of labels on the performance of learning methods, we use the mediamill data [5], which are extracted from the generic video indexing problem. The original mediamill data set is highly imbalanced (see the distribution of training data in Fig. 9) with more than 40,000 instances and 101 unique labels. We randomly select 1,500 examples as training data and 500 as test data and eval uate learning of the top (most frequent) 10, 20, 30, 40, and 50 la bels. Fig. 10 shows the data distribution of the top 50 labels in the new training set. The cardinality for each dataset is listed in Table 9. Tables 10 and 11 list the classi fication results for ML-KNN and Mr.KNN, respectively. Mr.KNN consistently outperforms ML-KNN. As the number of labels in creases, learning performance tends to decrease. We also plot the NDCG with n = 6 for 10 label cases in Fig. 11. Similar results are observed for 20-50 class labels. As of computational complexity , Mr.KNN requires more time to train the model (mainly, time to compute the margin ratio) than ML-KNN. However, the training can be conducted offline and the online test time needed for both Mr.KNN and ML-KNN are almost the same. Multi-label learning is attracting growing interest in information retrieval and data mini ng societies. Currentl y, most methods use the binary relevance strategy, whic h deals with one class label at a time. As analyzed in our study, an instance with multiple labels may be an outlier for some classes, which will degrade the performance of a classifier. In this paper, we introduce the soft relevance strategy, in which each instance is assigned a relevance score with respect to a label. This relevance score is related to the distance of the instance to center s of classes. Furthermore, it is used as a voting factor in a modified k NN algorithm. Evaluated over three commonly-used multi-label datasets, the proposed method outperforms ML-KNN (in te rms of accuracy, precision, and recall). Another factor that is not well st udied in multi-label learning is the effect of number of unique la bels on learning performance. In this paper, we investigate the learning of mediamill data with different number of unique labels. We show that when the number of unique labels increases, performance of the binary relevance-based ML-KNN method decreases. On the contrary, the soft relevance-based Mr.KNN produces similar results for different number of labels. Thus, the soft re levance strategy is appropriate for multi-label learning problems, especially for problems with a large number of labels and large cardinality. Our future work will explore an efficient way for training with large scale data sets and also evaluate on different distance metrics including these for nominal data. The material is based upon work supported by the US National Science Foundation Award IIS-0644366. [1] McCallum, A. Multi-label text classification with a mixture [2] Schapire, R. and Singer, Y. BoosTexter: A boosting-based [3] Boutell, M., Luo, J., Shen, X. and Brown, C. Learning multi-[4] Qi, G., Hua, X., Rui, Y., Tang, J., Mei, T. and Zhang, H. [5] Snoek, C., Worring, M., Van Gemert, J., Geusebroek, J. and [6] Yang, S., Kim, S. and Ro , Y. Semantic home photo [7] Zhang, M. and Zhou, Z. ML-K NN: A lazy learning approach [8] Barutcuoglu, Z., Schapire, R. and Troyanskaya, O. [9] Blockeel, H., Schietgat, L., Stru yf, J., D X eroski, S. and Clare, [10] Cesa-Bianchi, N., Gent ile, C. and Zaniboni, L. Hierarchical [11] Clare, A. and King, R. Knowledge discovery in multi-label [12] Elisseeff, A. and Weston, J. Kernel methods for multi-[13] Chen, X., Liu, M. and Ward, R. Protein function assignment [14] Liu, M., Chen, X. and Jothi, R. Knowledge-guided inference [15] Li, T. and Ogihara, M. Toward intelligent music information [16] Wieczorkowska, A., Synak, P. and Ra , Z. Multi-label [17] Trohidis, K., Tsoumakas, G., Kalliris, G. and Vlahavas, I. [18] Zhang, Y., Burer, S. and Street, W. Ensemble pruning via [19] Tsoumakas, G., Katakis, I. and Vlahavas, I. Mining multi-[20] Chen, W., Yan, J., Zhang, B., Chen, Z. and Yang, Q. [21] Read, J. A pruned problem transformation method for multi-[22] Tsoumakas, G. and Vl ahavas, I. Random k-labelsets: An [23] Vembu, S. and G X rtner, T. Label ranking algorithms: A [24] H X llermeier, E., F X rnkranz, J., Cheng, W. and Brinker, K. [25] F X rnkranz, J., H X llermeier, E., Loza Menc X a, E. and Brinker, [26] Ghamrawi, N. and McCallum, A. Collective multi-label [27] Cai, L. and Hofmann, T. Hierarchical document [28] Rousu, J., Saunders, C., Szedmak, S. and Shawe-Taylor, J. [29] Zhang, M. and Zhou, Z. Multil abel neural ne tworks with [30] Crammer, K. and Singer, Y. A family of additive online [31] Ueda, N. and Saito, K. Parametric mixture models for multi-[32] Gao, S., Wu, W., Lee, C. and Chua, T. A maximal figure-of-[33] Brinker, K. and H X llermeier, E. Case-based multilabel [34] Spyromitros, E., Ts oumakas, G. and Vlahavas, I. An [35] Duda, R., Hart, P. and Stork, D. Pattern classification . [36] Dimou, A., Tsoumakas, G., Mezaris, V., Kompatsiaris, I. and [37] Bezdek, J. Pattern recognition with fuzzy objective function [38] Groenen, P. and Jaj uga, K. Fuzzy clustering with squared [39] Fletcher, R. Practical Methods of Optimization: Vol. 2: [40] Bobrowski, L. and Bezdek, J. c-means clustering with the l [41] Shepard, R. N. Toward a unive rsal law of generalization for [42] Vapnik, V. The nature of statistical learning theory . Springer [43] Chen, X., Wang, H. and Lin, X. Learning to rank with a [44] J X rvelin, K. and Kek X l X inen, J. Cumulated gain-based 
