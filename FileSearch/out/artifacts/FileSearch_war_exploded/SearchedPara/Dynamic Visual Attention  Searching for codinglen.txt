 Visual attention plays an important role in the human visual system. This voluntary mechanism embedded in the vast amount of incoming visual data. In the pa st decade, we have witnessed the success of a number of computational models on visual attent ion (see [6] for a review). Many of these models analyze static images, and output  X  X aliency ma ps X , which indicate the probability of eye fixations. Models such as [3] and [4] have tremendously bo osted the correlation between eye fixation data and saliency maps.
 However, during the actual continuous perception process, important dynamic behaviors such as the sequential order of attended targets, shifts of attention b y saccades, and the inhibitory mechanism that precludes us from looking at previously observed targe ts, are not thoroughly discussed in the research on visual attention. Rather than contributing to t he accuracy of saliency map generation, characterizes the ebbs and flows of visual attention? Up to the present, this question is not comprehensively answ ered by existing models. Algorithms simulating saccades in some attention systems [23, 7] are de signed for engineering expediency rather behaviors. models who take discrete frames of images as the elementary u nits, our framework is based on con-tinuous sampling of features. Inspired by the principle of p redictive coding [9], we use the concept of energy to explain saliency, feature response intensity, and the appropriation of computational re-sources in one unified framework. The appropriation of energ y is based on the Incremental Coding features will receive the highest energy, and become salien t. Since the proposed model is temporally continuous, we can demonstrate a series of simulations of dy namic attention, and provide plausible explanations of previously unexamined behaviors. 1.1 Space and Feature Based Attention Many of the bottom-up visual attention models follow the Koc h and Ullman framework [10]. By analyzing feature maps that topographically encode the spa tial homogeneity of features, an algo-attention from a one-shot observation of an image. However, several critical issues may be raised when this framework is applied to continuous observations ( e.g. video). First, space-based atten-tion itself cannot interpret ego-motion. Additional coord inate transformation models are required to translate spatial cues between two different frames. Sec ond, there are attention mechanisms that (IOR) [8]. The initial space-based framework is not likely t o provide a convincing explanation to these mechanisms.
 In addition to saliency based on local irregularity, recent investigations in V4 and MT cortical ar-eas demonstrate that attention can also be elicited by parti cular features [13, 18]. In the field of computational models, explorations that are biased by feat ures are also used in task-dependent spa-tial saliency analysis [16]. The emerging evidence in featu re-driven attention has encouraged us to propose a pure feature-based attention model in parallel wi th the space-based feature map paradigm. 1.2 On the Cause of Attention more rigid form, saliency can be defined by the residuals of Di fference of Gaussian filter banks [7], regions with maximal self-information [3], or most discrim inant center-surround composition [4]. However, all of these principles do little to address the cau se of saliency mechanisms in the brain. At the level of computation, we cannot attribute the formati on of attention to functional advantages such as foraging for foods [6]. In this paper, we hypothesize that visual attention is driven by the predictive coding principle, that is, the optimization of m etabolic energy consumption in the brain. In our framework, the behavior of attention is explained as a consequence of an actively-searching observer who seeks a more economical neural code to represen t the surrounding visual environment. space. The activity of the feature ensemble is considered as a probability function. We evaluate each feature with respect to its Incremental Coding Length (ICL). The ICL of i th feature is defined saliency of a region is obtained by summing up the activity of all features at that region. 2.1 Sparse Feature Representation Experimental studies [15] have shown that the receptive fiel ds of simple-cells in the primary visual cortex produce a sparse representation. With standard meth ods [2], we learn a set of basis functions features in the analysis of attention. Specifically, we use 120000 8  X  8 RGB image patches from natural scenes for training. A set of 8  X  8  X  3 = 192 basis functions is obtained. (See Fig. 1). Let A be the sparse basis, where a i is the i th basis function. Let W = A  X  1 be the bank of filter functions, where W = [ w linear filter to the image patch.
 image x , we have s = Wx . Since each basis function represents a structural primiti ve, in the time. Considering the energy consumed by neural activity in the brain, this sparse coding strategy is advantageous [11].
 Figure 1: First 30 components of the basis functions A and the corresponding filter functions W are shown in this figure. 2.2 The Incremental Coding Length the cortex, some previous work has analyzed the information representation and coding in early visual system [20, 21, 1]. Guided by the insights behind pred ictive coding [17], we propose the Incremental Coding Length (ICL) as a computational princip le based on features. This principle aims to optimize the immediate energy distribution in the sy stem in order to achieve an energy-economic representation of its environment.
 The activity ratio p we can compute the activity ratio p Furthermore, we denote p = [ p this structure at the neuronal level goes beyond the scope of this paper. However, studies [13] have suggested evidence of a population of neurons that is capabl e of generating a representation for in-termodal features. In our implementation, the distributio n p addresses the computational properties of this putative center.
 Since the visual information is jointly encoded by all featu res, the most efficient coding strategy should make equal use of all possible feature response levels . To achieve this optimality, the model needs to maximize the entropy H ( p ) . Since p is determined by the samples X , it is possible for a system to actively bias the sampling process in favor of maxi mizing information transmission. i , which will add a variation  X  to p to the feature activity probability increment is: where: Accordingly, we define the Incremental Coding Length (ICL) t o be: 2.3 Energy Redistribution We define the salient feature set S as: S = { i | ICL ( p successive observations of feature i would increase H ( p ) . In the context of visual attention, the activations of that feature can offer entropy gain to the sys tem.
 Within this general framework of feature-level optimizati on, we can redistribute the energy among features. The amount of energy received by each feature is de noted d automatically neglected by setting d [ m 1 , m 2 , . . . , m n ] as: of p , which can be obtained by sampling the environment over spac e and time.
 immediate environment. We proposed a framework that explains dynamic visual attent ion as a process that spends limited available energy preferentially on rarely-seen features. In this section, we examine experimentally the behavior of our attention model. 3.1 Static Saliency Map Generation By sequentially sampling over all possible image patches, w e calculate the feature distribution of a static image and generate the corresponding saliency map. These maps are then compared with records of eye fixations of human subjects. The accuracy of an algorithm is judged by the area under its ROC curve.
 We use the fixation data collected by Bruce et al. [3] as the ben chmark for comparison. This data set contains the eye fixation records from 20 subjects for the full set of 120 images. The images are down-sampled to an appropriate scale ( 86  X  64 , 1 models are indicated below. Due to a difference in the sampli ng density used in drawing the ROC curve, the listed performance is slightly different (about 0.003) from that given in [3] and [4]. The algorithms, however, are all evaluated using the same bench mark and their relative performance should be unaffected. Even though it is not designed for stat ic saliency map generation, our model achieves the best performance among mainstream approaches .
 3.2 Dynamic Saliency on Videos sampling within one 2-D image) as well as over time (when samp ling over a sequence of images). The temporal correlation among frames can be considered as a Laplacian distribution. Accordingly, at the t th frame, the cumulative activity ratio distribution p t yields: where  X  is the half life.  X  p normalization factor that ensures p t is a probability distribution.
 In video saliency analysis, one of the potential challenges comes from simultaneous movements of the targets and self-movements of the observer. Since our mo del is feature-based, spatial movements of an object or changing perspectives will not dramatically affect the generation of saliency maps. In order to evaluate the detection accuracy of our approach und er changing environment, we compare the dynamic visual attention model with models proposed in [ 7] and [5].
 maps to a videoclip is determined by comparing the response i ntensities at saccadic locations and random locations. Ideally, an effective saliency algorith m would have high output at locations gazed by observers, and tend not to response in most of the randomly chosen locations.
 saccadic locations q measure their dissimilarity. Higher the KL divergency is, m ore easily a model can discriminate human saccadic locations in the image.
 Figure 4: The eye-track records and the video is obtained fro m [5]. This video contains both target movements and self-movements. In this video, 137 saccades ( yellow dots in figure A) are collected. Given the sequence of generated saliency maps, we can obtain the saliency distribution at human saccade locations (narrow blue bars), and random locations (wide green bars). The KL-divergency of these two distribution indicates the performance of each model. 3.3 Dynamic Visual Search We are particularly interested in the dynamic behaviors of a ttention. Reported by researchers in neurobiological experiments, an inhibitory effect was aro used after sustained attention [12]. This mechanism is referred as Inhibition of Return (IOR) [8]. Res earch on the cumulative effects of attention [24] has suggested that the dynamics of visual sea rch have broad implications for scene nism that prevents an autonomous system from being permanen tly attracted to certain salient spots and thereby to facilitate productive exploration, the comp utational modeling of IOR is of practical value in AI and robotics. Previous computational models suc h as [22, 7] implemented the IOR in a spatially-organized, top-down manner, whereas our model samples the environment online and is driven by data in a bottom-up manner. Spontaneous shifts of a ttention to new visual cues, as well as the  X  X efusal of perception X  behavior arise naturally as c onsequences of our active search model. Moreover, unlike the spatial  X  X nhibitory masking X  approac h in [7], our model is feature-based and is therefore free from problems caused by spatial coordinat e transformations. 3.3.1 Modeling Sensory Input The sensory structure of the human retina is not uniform. The resolution of perception decreases movement is made so that the desired visual stimuli can be map ped onto the foveal region. Similar to the computational approximations in [14], we consider th e fovea sampling bias as a weighted mask W over the reconstructed saliency map. Let the fovea be locate d at ( x ( x, y ) is weighted by W ( x, y ) : In the experiments, we choose  X  = 1 . 3.3.2 Overt Eye Movements towards Saliency Targets with Inhi bition of Return In the incremental perception of one static image, our dynam ic visual system is guided by two fac-fosters feature preferences in the system. The second facto r is a foveal structure that allows the system to bias its sampling via overt eye movements. The inte rplay of these two factors leads to an active visual search behavior that moves towards a maximum e ntropy equilibrium in the feature dis-within short interval because of the foveated weighting. Th is property of IOR is demonstrated by our experiments.
 An implementation of our dynamic visual search is shown in th e algorithm box.
 It is also worth noting that, when run on the images provided b y [3], our dynamic visual attention algorithm demonstrates especially pronounced saccades wh en multiple salient regions are presented in the same image. Although we have not yet validated these sa ccades against human retinal data, to our knowledge this sort of  X  X ttentional swing X  has never b een reported in other computational systems. A novel dynamic model of visual attention is described in thi s paper. We have proposed Incremental Coding Length as a general principle by which to distribute e nergy in the attention system. In this principle, the salient visual cues correspond to unexpecte d features -according to the definition of ICL, these features may elicit entropy gain in the perceptio n state and are therefore assigned high energy.
 To validate this theoretical framework, we have examined ex perimentally various aspects of visual attention. In experiments comparing with static saliency m aps, our model more accurately predicted saccades than did other mainstream models. Because the mode l updates its state in an online manner, we can consider the statistics of a temporal sequence and our model achieved strong results in video model provides a coherent mechanism for dynamic visual sear ch with inhibition of return. In expectation of further endeavors, we have presented the f ollowing original ideas. 1) In addition to spatial continuity cues, which are demonstrated in other literature, saliency can also be measured using features. 2) By incorporating temporal dynamics, a vi sual attention system can capture a broad range of novel behaviors that have not successfully been exp lained by saliency map analysis. And 3) dynamic attention behaviors might quantitatively be exp lained and simulated by the pursuit of a maximum entropy equilibrium in the state of perception. We thank Neil Bruce, John Tsotsos, and Laurent Itti for shari ng their experimental data. The first author would like to thank Charles Frogner, Yang Cao, Shengp ing Zhang and Libo Ma for their insightful discussions on the paper. The reviewers X  pertin ent comments and suggestions also helped to improve the quality of the paper. The work was supported by the National High-Tech Research Program of China (Grant No. 2006AA01Z125) and the National B asic Research Program of China (Grant No. 2005CB724301)
