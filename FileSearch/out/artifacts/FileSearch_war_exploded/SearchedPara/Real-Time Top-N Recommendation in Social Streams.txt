 The Social Web is successfully established, and steadily grow-ing in terms of users, content and services. People generate and consume data in real-time within social networking ser-vices, such as Twitter, and increasingly rely upon continuous streams of messages for real-time access to fresh knowledge about current affairs. In this paper, we focus on analyzing social streams in real-time for personalized topic recommen-dation and discovery. We consider collaborative filtering as an online ranking problem and present Stream Ranking Ma-trix Factorization  X  RMFX  X , which uses a pairwise approach to matrix factorization in order to optimize the personal-ized ranking of topics. Our novel approach follows a selective sampling strategy to perform online model updates based on active learning principles, that closely simulates the task of identifying relevant items from a pool of mostly uninterest-ing ones. RMFX is particularly suitable for large scale appli-cations and experiments on the 476 million Twitter tweets dataset show that our online approach largely outperforms recommendations based on Twitter X  X  global trend, and it is also able to deliver highly competitive Top-N recommen-dations faster while using less space than Weighted Reg-ularized Matrix Factorization (WRMF), a state-of-the-art matrix factorization technique for Collaborative Filtering, demonstrating the efficacy of our approach.

The amount of user generated content in social media applications and the rate at which such content is made available poses a challenge to state-of-the-art recommender system algorithms. For instance, the number of users of the popular micro-blog service Twitter 1 is estimated to have sur-passed 300 million generating more than 200 million tweets (micro-blog posts) per day [18].

When dealing with user generated content in social me-dia applications, it is crucial that finding good patterns and making inference about them is done in a reasonable time. One scenario where this point becomes particularly critical is that of filtering a continuous stream of incoming tweets in order to recommend topics that match user interests at a specific moment, given the huge scale of this kind of data. The challenge here is to account for dynamic short term information needs of the users. As a surrogate for a user X  X  topic interests one could use hashtags . For example, in Twit-ter, if user Alice often tags her tweets with the hashtag #TechCrunch and never uses the hashtag #fashion , we can exploit this information and use it as a good indicator for her preferences. We can infer that, currently, Alice is more interested in technology news than, for instance, in fashion . Thus the task can be cast as that of recommending hashtags to users.

Collaborative filtering (CF) has been shown to be an effec-tive approach to recommender systems. The essence of CF lies in analyzing past user and item interactions to generate personalized recommendations based on the preferences of other users with similar behavior. One of CF X  X  most suc-cessful techniques are low dimensional linear factor models, that assume user preferences can be modeled by only a small number of latent factors [11].

Although latent factor models are able to generate high quality recommendations, coping with fast changing trends in the presence of large scale data might be a challenge, since retraining such models is costly. One alternative is to learn the parameters online, updating the decision function for each new observation [2]. Unfortunately, the gain in pro-cessing time achieved by online learning algorithms comes at the cost of reduced prediction quality and work has been done on closing this gap. The main issue with online ap-proaches is their short-term  X  X emory X , i.e., since the up-dates based only on the most recent data point do not take into account past observations, the model quickly  X  X orgets X  them. Recently Zhao et al. [23] proposed an online learn-ing algorithm for maximizing Area Under the ROC Curve (AUC), a metric that is widely used for measuring the classi-fication performance for imbalanced data distributions, that addresses this problem by keeping a representative sample of the data set in a reservoir and using only this sample plus the new observation for retraining.
One issue that arises with the reservoir approach is how to perform the model updates. In this paper we propose to se-lectively sample the most informative instances from a reser-voir using personalized small buffers and perform stochastic gradient descent updates based on active learning princi-ples [6, 22]. We elaborate on the notion of  X  X nformative el-ements X  and illustrate the application of our approach to learning factorization models. We demonstrate its useful-ness on the task of recommending hashtags to Twitter users based on real world data.

To summarize, the main contributions of this work are as follows:  X  We introduce a novel framework for online collaborative filtering. The novelty of our approach lies in a selective sampling strategy to update the model based on person-alized small buffers.  X  We propose Stream Ranking Matrix Factorization ( RMFX ), an online leaning algorithm based on a pairwise rank-ing approach for matrix factorization that is intended for streaming data, and is well founded in stochastic gradient descent.  X  For unpersonalized learning to rank, many studies have been made in the field of information retrieval. This paper presents an innovative personalized ranking perspective to matrix factorization for social media streams, which has not been reported before in the literature.  X  Finally, this paper provides an example of integrating large-scale collaborative filtering with the real-time nature of Twitter.
 The reminder of the paper is organized as follows: In Sec-tion 2, we present background material and notation. In Sec-tion 3, we present our Stream Ranking Matrix Factorization approach. Section 4 discusses related work. In Section 5, we show the results of our approach by analyzing real-world data consisting of millions of tweets. Finally, in Section 6, we conclude and present directions for future work.
First we introduce some notation that will be useful in our setting. Let U = { u 1 ,...,u n } and I = { i 1 ,...,i the sets of all users and all items, respectively. We reserve special indexing letters to distinguish users from items: for users u , v , and for items i , j . Suppose we have interactions between these two entities, and for some user u  X  U and item i  X  I , we observe a relational score x ui .

Thus, each instance of the data is a tuple ( u,i,x ui ). For example in the movie recommendation case, the tuple might correspond to an explicit  X  X ating X  given by user u to movie i or, in the case of hashtag/topic recommendation, to a  X  X eight X  that is implicitly derived from user u  X  X  interaction patterns, e.g., how many times the user u has used hashtag i . Typical CFs organize these tuples into a sparse matrix X of size | U | X | I | , using ( u,i ) as index and x ui as entry value. The task of the recommender system is to estimate the score for the missing entries. The relational scores themselves are ordinal and need not be numbers. Thus, we assume a total order between the possible score values. We distinguish pre-dicted scores from the known ones, by using  X  x ui . The set S of all observed scores is defined as follows:
For convenience, we also define for each user the set of all items with an observed score, denoted by B + u :
Low dimensional linear factor modeling are popular col-laborative filtering approaches [11]. These models consider that only a small number of latent factors can influence the preferences. Their prediction is a real number,  X  x ui , per user item pair ( u,i ). Some of the most successful realizations of latent factor models are based on matrix factorization (MF). In its basic form, matrix factorization estimates a matrix X : U  X  I by the product of two low-rank matrices W : | U | X  k and H : | I | X  k : where k is a parameter corresponding to the rank of the approximation.

The factorization process is performed by minimizing a loss function that measures the quality of the reconstruc-tion  X  X . One alternative to learn the optimal parameters of the model is to use a Stochastic Gradient Descent (SGD) approach [2].

Even though the squared loss has been successfully used for MF in the context of rating prediction (e.g., [11]) and item prediction [8], we are interested in a ranking approach to MF, and therefore require an ordinal loss to guide the factorization process. In particular we are interested in a pairwise approach, similar to the one used by RankSVM [9], a popular ranking method in the field of learning to rank. We present the details of our approach in the next section and discuss related work in Section 4.
In the presence of a continuous stream of incoming tweets, arriving at a high rate, our objective is to process the incom-ing data in bounded space and time and recommend a short list of interesting topics that meet users X  individual taste.
The high rate makes it harder to: (i) capture the informa-tion transmitted, (ii) compute sophisticated models on large pieces of the input, and (iii) store the input data, which can be significantly larger than the algorithm X  X  available mem-ory.

This problem setting fits a streaming model of computa-tion by Muthukrishnan [12], which establishes that, by im-posing a space restriction on algorithms that process stream-ing data, we may not be able to store all the data we see. The impact is that the data generated in real-time carries high-dimensional information which is difficult to extract and process. Any time lag in modeling the data could ren-der the outcome of the modeling obsolete and useless.
We assume that topics of interest are captured by the hash-tagging behavior in Twitter. Hashtags are words or phrases prefixed with the symbol #, e.g., #recsys , a form of metadata tag used to mark keywords or topics in a tweet. Hashtags evolve over time, reflecting the dynamics of user preferences in the social stream. Our approach seeks to in-corporate these dynamics to produce a short list of interest-ing recommendations based on a matrix factorization model for CF, which is learned online.

In this section, we formally define the problem and in-troduce our approach Stream Ranking Matrix Factorization or RMFX , and develop our model in steps discussing the ra-tionale behind them. Such steps are illustrated in Figure 1. Figure 1: Main steps of our approach Stream Ranking
We focus on learning a matrix factorization model for col-laborative filtering in presence of streaming data. To this end, we will follow a pairwise approach to minimize an or-dinal loss. Our formalization extends the work of Scully [15] for unpersonalized learning to rank, to an online collabora-tive filtering setting.

With slight abuse of notation, we also use S to represent the input stream s 1 ,s 2 ,... that arrives sequentially, instance by instance. Let p t = (( u,i ) , ( u,j )) t denote a pair of train-ing instances sampled at time t , where ( u,i )  X  S has been observed in the stream and ( u,j ) /  X  S not.

Formally, we define the set P as the set of tuples p = (( u,i ) , ( u,j )) selected from the data stream S , as follows: We require pairs that create a contrast in the preferences for a given user u over items i and j . Since we are dealing with implicit, positive only feedback data (i.e. the user never explicitly states a negative preference for an item) we follow the rationale from Rendle et al. [13] and assume that user u prefers item i over item j . We will restrict the study to a binary set of preferences x ui = { +1 ,  X  1 } , e.g., observed and not-observed , represented numerically with +1 and  X  1, respectively. For example, if a user u in Twitter posts a message containing hashtag i , then we consider it as a pos-itive feedback and assign a score x ui = +1. More formally, x ui = +1  X  X  X  i  X  B + u . In future work we plan to explore how repeated feedback can be exploited to establish a total order for items in B + u .

It is obvious that, in the case of streaming data, we do not compute P explicitly, but instead select pairs from the stream, at each time step, that meet P  X  X  membership re-quirements.

With P defined, we find  X  = ( W , H ) that minimizes the pairwise objective function: In this paper, we explore the use of the SVM loss, or hinge-loss , used by RankSVM for the learning to rank task [9]. RMFX Framework Input: Output:  X  = ( W , H ) 1: initialize W 0 and H 0 2: initialize sample stream S 0  X  X  X  3: counter  X  0 4: for t = 1 to T S do 5: R  X  updateReservoir ( R ) 6: counter  X  counter + 1 7: if c = counter then 8:  X   X  updateModel ( S t , X  W , X  H + , X  H  X  , X , X ,T  X  ) 9: counter  X  0 10: end if 11: end for 12: return  X  T = ( W T , H T ) Given the predicted scores  X  x ui and  X  x uj , the ranking task is reduced to a pairwise classification task by checking whether the model is able to correctly rank a pair p  X  P or not. Thus, L ( P, W , H ) is defined as follows: where h ( z ) = max (0 , 1  X  z ) is the hinge-loss; y uij x uj ) is the sign ( z ) function, which returns +1 if z &gt; 0, i.e., x ui &gt; x uj , and  X  1 if z &lt; 0. The prediction function  X  w u , h i  X  h j  X  =  X  w u , h i  X   X   X  w u , h j  X  corresponds to the difference of predictor values  X  x ui  X   X  x uj .

Please note that in this special case of binary rank values of observed and not-observed , the optimization problem de-fined by Eq. (3) is equivalent to the problem of optimizing area under the ROC curve (AUC) for binary-class data [15].
Other convex loss functions can also be applied, e.g., squared or logistic loss [13, 15], as well as any prediction function be-sides the dot product  X  X  ,  X  X  [14].

To conclude this section, we compute the gradient of the pairwise loss at instance p t  X  P with non-zero loss, and model parameters  X  t = ( w u , h i , h j ), as follows: Our goal is to develop an algorithm to efficiently optimize the objective function (2).

Based on stochastic gradient descent concepts [2], we present the framework of our algorithm in Figure 2. The main com-ponents of this framework are: (i) a sampling procedure done on the streaming data and (ii) a selective model update based on small buffers created per each user.
When processing streams of data, one usually wants to avoid the cost of retraining a model every time new data points arrive; thus online updates are usually used. Unfor-tunately, the gain in processing time and bounded space achieved by this online learning approach comes at the cost of reduced prediction quality, compared to more accurate models that the large training set could allow. The main issue with online approaches is their short-term  X  X emory X , i.e., since the updates based only on the most recent data point do not take into account past observations, the model quickly  X  X orgets X  them. In the presence of an abundant source of training examples, a way to reduce complexity of a learn-ing algorithm consists of picking a random subset of training examples and building a model on this subset. In this phase of our model, we employ the technique of random sampling with a reservoir [20], which is widely used in data streaming, and recently has been proposed for online AUC maximiza-tion in the context of binary classification [23].

A reservoir sampling algorithm incrementally maintains a random sample of fixed size of the incoming stream of tweets. We represent the reservoir as a list R := [ s 1 ,s 2 ...,s  X  X emembers X  | R | random instances from stream S . Instances can occur more than once in the reservoir, reflecting the distribution of the observed data, thus the reservoir captures an accurate  X  X ketch X  of history under the constraint of fixed space.

Let be t the index reflecting the order of arrival of data in the stream, note that until t = | R | all data points enter the reservoir. When t = | R | we have a random sample of size | R | of the stream; indeed the entire dataset so far is in the reservoir. For subsequent t we need to decide whether the newly arrived data should be put in the reservoir and, if so, which data already in the reservoir it should replace.
Vitter shows in [20] that if one includes the t th data in-stance with probability | R | /t and replaces uniformly at ran-dom an instance from the reservoir, the reservoir is a ran-dom sample of the current dataset. This reservoir sampling mechanism is implemented by the procedure updateReser-voir ( R ) in Figure 2.
The random sampling with a reservoir allows us to retain a fixed size of observed instances, bounding the space available for the algorithm to a set of | R | randomly chosen samples from the stream and update the model using this history. Although simply updates of the model based on the reservoir may yield better results than single online updates, it is still far from the accuracy achieved by the offline cases. On top of that, in the reservoir we store only user and item pairs observed in the stream, and the question of how to sample the pairs needed for creating the contrasts P still remains.
In order to address this drawback, we need to exploit as much information as possible from the sampled tweets in the reservoir. In particular we propose to perform model updates and retraining on the most informative examples present in the reservoir, then, the question is how to select such exam-ples from this sketch of the stream. This scenario is similar to the one of active learning , where the system asks the user to evaluate a minimum set of items which will contribute the most to learning his/her preferences (e.g., [10]). Consider the case of binary classification using Support Vector Machines (SVM). SVM attempt to find a hyperplane that divides the two classes with the largest margin. From the theoretical foundations of SVM we know that only the support vectors have an effect on the solution. The support vectors are the points that lie closest to the hyperplane, therefore the most informative training points, and the goal of training is to discover them [19].

Usually, the training set is chosen to be a random sam-pling of instances, for example the tweets in our reservoir. However, in many cases principled criteria can be used to sample the training data with the goal to reduce its need for large quantities of labeled data.

Our scenario of dyadic data, i.e., user-item interactions, differs from the one of SVM in two fundamental ways: (i) since we are learning personalized rankings, there are as many hyperplanes as users, unlike an SVM, (ii) we are not just learning a hyperplane per user, but simultaneously also the item feature vectors, in contrast to SVM where the val-ues of the features vectors, defining the training points, are known and given in advance.

Moreover, remember that we are concerned with learning personalized rankings from pairwise comparisons, hence the most informative instances are the ones that have opposite labels but are close to each other in the ranking induced by the user X  X  hyperplane, intuitively they are more difficult to order than the ones away from each other in the rank-ing [22]. Figure 3 illustrates how user u  X  X  feature vector w induces a particular (personalized) ranking at a given it-eration in a two dimensional example 2 . w u determines the ordering of four item points. For any user weight vector w the items are ordered by the projection onto w u , or equiva-lently, by their signed distance to a hyperplane with normal vector w u . The items in the figure are ordered ( h 1 , h h ). We denote as  X  the distance between two projections of data points with different labels on the induced ranking, the smaller the  X  , the more informative the instances are for training the model. Figure 3: Example of how a user weight vector w u ranks
Finally, to answer the question of how to select such ex-amples from the reservoir, we will use an active learning in-spired approach. In classical active learning [17], the search for the most informative instance is performed over the en-tire dataset, which involves the recomputation of each train-ing example X  X  distance to the new hyperplane. This process is prohibitively expensive for large datasets or unbounded data streams. Therefore, we propose a selection method based on the  X 59 trick X  [16, 6], that establishes that randomly sam-pling only 59 instances, regardless the training set size, is enough to guarantee with 95% probability, that one of them is among the top 5% closest instances to the hyperplane. This approach also simulates the real world scenario of given a pool of items, ranking the positive ones higher than the negatives, modeled into the recommender system evaluation protocol proposed in [3].

At each iteration, we select at random a user-item ( u,i ) interaction from the reservoir, which represents a positive feedback observation. Next, we construct a small buffer for user u by sampling 59 negative items j  X  X , creating the re-quired contrast in the preferences for user u over items i and j  X  X . The user buffer contains exactly 59 pairs of the form p = (( u,i ) , ( u,j b )), b = 1 ... 59. Then, we compute the val-ues  X  uijb between the projections on w u of each instance in the pair p b . Finally, we sample a pair p  X  with probability pro-portional to its informativeness , which is given by 1 / X  and use p  X  to perform the matrix factorization model up-dates. This procedure is shown in Figure 4, which includes three regularization constants:  X  W ,  X  H + , and  X  H  X  , one for the user factors, the other two for the positive and negative item factors updates. Moreover, we include a learning rate, and a learning rate schedule  X  that adjusts the step size of the updates at each iteration.
Learning of large-scale recommender systems for dealing with dynamic and fast changing content has been addressed before, for instance in the context of the Google News sys-tem [4]. However the problem setting in [4] is different from the one addressed here, since their work does not deal with a continuous stream of user generated data, but instead pro-vides recommendations to users based on offline models.
The Fast Online Bilinear Factor Model (FOBFM) [1] ad-dresses the related task of click through rate prediction. They combine offline training with online updates in a prin-cipled framework. While FOBFM addresses a regression task, we are concerned here with a learn to rank problem. Also our approach does not need an explicit dimensionality reduction step for the offline learned features.

Online matrix factorization learning methods have also been investigated by Rendle and Schmidt-Thieme for rat-ing prediction [14]. They propose online update rules on a stochastic gradient descent style based on the last example observed. While those update rules take into account only the last observed data point, RMFX uses a reservoir with a representative set of previously seen data points from the stream. This idea has been previously explored by Zhao et al. [23] in the context of binary classification, in contrast, a novel idea introduced in this work is the selective sampling based on personalized buffers according to the distance of points to the decision boundary which, as shown in our ex-periments, delivers better results than using exclusively the random sampling technique used by Zhao et al. [23]. RMFX Model Update based on SGD for MF using active learning with small buffers Input: Output:  X  = ( W , H ) 1: procedure updateModel ( S t , X  W , X  H + , X  H  X  , X  0 , X ,T 2: for t = 1 to T  X  do 3: Select a user-item pair ( u,i ) from R uniformly at 4: Construct a small buffer for user u by sampling 5: Compute the distances  X  uijb for each pair 6: Sample a pair p  X  = (( u,i ) , ( u,j )) from the buffer 7: y uij  X  sign ( x ui  X  x uj ) 8: w u  X  w u +  X  y uij ( h i  X  h j )  X   X   X  W w u 10: h j  X  h j +  X  y uij (  X  w u )  X   X   X  H  X  h j 11:  X  =  X   X   X  12: end for 13: return  X  = ( W T  X  , H T  X  ) 14: end procedure Figure 4: Matrix factorization model update based on
Yu proposed a selective sampling technique for learning ranking functions in the context of data retrieval applica-tions [22]. Our method, on the other hand, is a learning to rank approach for personalized item prediction.

Since we deal with pairwise classification from positive-only data, negative examples must be sampled. The sampling of the 59 negative examples for each positive one has been pro-posed, discussed and proved in [6]. Whereas they do it for active learning, we adapt it for our online learning to rank scenario.
In this section, we demonstrate our approach by analyzing real-world data consisting of millions of tweets. We present the evaluation protocol and experimental setting, as well as the results of the empirical study. The dataset corresponds to the 476 million Twitter tweets 3 [21], which includes over 476 million Twitter posts from 20 million users, covering a 7 month period from June 1, 2009 to December 31, 2009. The number of hashtags present in the dataset is 49,293,684. It is estimated that this is about 20-30% of all public tweets published on Twit-ter during the particular time frame. For our evaluation we computed a 5-core of the dataset, i.e., every user has used at least 5 different hashtags, and every hashtag has been used at least by 5 different users. The 5-core consists of 35,350,508 tweets, 413,987 users and 37,297 hashtags.
 Evaluation of a recommender in the presence of stream data requires a time sensitive split. We evaluated by splitting the dataset S into two sets: a training set S train and a testing set S test . Consider we make the split at time t split we put into S train the individual training examples (tweets) with timestamps less that t split . Into S test , we put the user rankings with timestamps greater than t split . The recom-menders are trained on S train and then their performance is measured on S test . Note that given the dynamics in Twitter, there might be users in S train not present in S test .
To evaluate the recommenders we used a variant of the all-but-1 protocol, also known as the leave-one-out holdout method. In particular, we follow a similar schema as the one described in [3].

Our goal is to evaluate the system performance when it suggests Top-N topics to a user. For example, recommending the user a few specific hashtags which are supposed to be the most attractive to him. That is, to find the relative position of these interesting items within the total order of items ranked for a specific user.

To this end, for each user u  X  | U test | we aggregate his rankings in the test set S test by accumulating the item fre-quencies across those rankings in order to produce a single total ranking. The items are again sorted in descending order of their accumulated frequencies.

We take one item i at random from the top-10 of the ag-gregated ranking and hide it. The goal of a recommender system is to help users to discover new items of interest, therefore we impose the additional restriction that the hid-den item has to be novel for the user, and therefore we re-move from the training set all occurrences of the pair ( u,i ). In total, we have | U test | = 260 , 246 hidden items.
Then, for each hidden item i , we randomly select 1000 additional items from the test set S test . Notice that most of those items selected are probably not interesting to user u .
We predict the scores for the hidden item i and for the ad-ditional 1000 items, forming a ranking by ordering the 1001 items according to their scores. The best expected result is that the interesting item i u to user u will precede the rest 1000 random items.

Finally, we generate a Top-N recommendation list by se-lecting the N items with the highest score. If the test item i is in the Top-N , then we have a hit , otherwise we have a miss . We measure the quality by looking at the recall metric. Traditionally, collaborative filtering algorithms are evalu-ated by the accuracy of their predicted ratings. One com-monly used performance metric for rating accuracy is the Mean Absolute Error (MAE).

However, we are interested in measuring Top-N recom-mendation performance and not in rating prediction. There-fore, we measure the quality by looking at the recall metric, also known as hit rate , which is widely used for evaluating Top-N recommender systems (e.g., [5, 3]).

In our recommender systems setting, the recall metric is defined as follows: where 1 [ z ] is the indicator function that returns 1 if condition z holds, and 0 otherwise. A recall value of 1.0 indicates that the system was able to always recommend the hidden item, whereas a recall of 0.0 indicates that the system was not able to recommend any of the hidden items. Since the precision is forced by taking into account only a restricted number N of recommendations, there is no need to evaluate precision or F1 measures, i.e., for this kind of scenario, precision is just the same as recall up to a multiplicative constant. We implemented our RMFX , and evaluated them against the following competing models: 1. RMF-RSV: Reservoir Sampling involves retaining a fixed size of observed instances in a reservoir . The reservoir captures an accurate  X  X ketch X  of history under the constraint of fixed space. We randomly choose | R | samples from the stream and update the model using this history, i.e., without performing any selective sampling. 2. RMF-SP: Single Pass takes a single pair from the stream and performs an update of the model every itera-tion. This approach does not  X  X emember X  previously seen instances. That is, we sample a pair p t  X  P at iteration t , and directly execute the model updates described in lines 7 to 11 in Figure 4. 3. Trending Topics (TT) . This model sorts all hash-tags based on their popularity, so that the top recommended hashtags are the most popular ones, which represents the trending topics overall. This naive baseline is surprisingly powerful, as crowds tend to heavily concentrate on few of the many thousands available topics in a given time frame. 4. Weighted Regularized Matrix Factorization (WRMF) . This is a state-of-the-art matrix factorization model for item prediction introduced by Hu et al. [8]. Their method outperforms neighborhood based (item-item) mod-els in the task of item prediction for implicit feedback datasets. The model is computed in batch mode , assuming that the whole stream is stored and available for training. It is ex-pected that the performance of this offline method, with full access to the user-item interactions, will set an upper bound for the online approaches.

We simulate the stream receiving one instance at the time based on the tweets X  publication dates. Tweets without hash-tags were ignored.

For RMFX , RMF-RSV, and RMF-SP we set regularization constants  X  W =  X  H + =  X  H  X  = 0 . 1, learning rate  X  0 = 0 . 1, and a learning rate schedule  X  = 1, and find that the setting gives good performance. We are currently investigating how to efficiently perform a grid search on stream data to tune up the hyperparameters dynamically. Moreover, the number of iterations is set to the size of the reservoir for both RMFX and RMF-RSV.

WRMF setup is as follows:  X  WRMF = 0 . 015, C = 1, epochs = 15, which corresponds to a regularization parame-ter, a confidence weight that is put on positive observations, and to the number of passes over observed data, respec-tively [8] 4 .

We divided the six-month Twitter activity of our dataset, by choosing the first five months (from first of June, 2009 to end of November, 2009) for training. We use the remaining Figure 5: Recommendation performance in terms of recall month, i.e., December, to build 10 independent test sets fol-lowing the evaluation protocol described previously in this section. The models RMFX and RMF-RSV are built on the sketch of the stream available just before the evaluation pe-riod, i.e., end of November, 2009. For TT, we use as pre-dictors the most popular hashtags from the last four weeks before the evaluation, i.e., TT of November, 2009.

We restricted the analysis to short list of recommenda-tions and computed the recall metric for Top-N recommen-dations, where N  X  X  1 , 5 , 10 } . The value of the metric for a particular Top-N is denoted as recall@N . The performance is evaluated on the test set only and the reported results are the average over 5 runs. All the differences reported are statistically significant (two-sample t-test, p &lt; 0.015).
Reproducibility of Experiments. We will provide an anony-mized dataset of the 5-core dataset used in our experiments and a reference implementation of RMFX upon request by email. We used the WRMF implementation provided by My-MediaLite , a free software recommender system library [7] Figure 5 summarizes the recommendation performance for RMFX , the baselines and the upper bound given by WRMF. We can observe that RMFX is superior with respect to the online methods RMF-SP and RMF-RSV, and largely out-performs the trending topics (TT). Please note that the trending topics from the previous four weeks achieve a re-call@10=7.8%, this performance based on the crowd behav-ior in Twitter is much better than a random model, whose recall@10 is under 1%.

Table 1 shows that RMFX achieves the best performance over all online methods evaluated with reservoir sizes 2, 4 and 8 million. As expected, the offline method WRMF sets an upper bound for the online approaches achieving a re-call@5 of 18.96%, but RMFX is still competitive with a re-call@5=16.58% and the advantage of real-time updates.
Finally, with a fixed reservoir size of 8M, we also explored the impact of model dimensionality over the recommenda-tion quality for RMFX . The results are presented in Figure 6 and Table 2. From the figure, we see that RMFX consistently outperforms the baseline TT and the online competitors for 32, 64 and 128 dimensions. Table 1: Recommendation performance for different sizes Table 2: Recommendation performance for 32 and 64 fac-We report in this section the CPU training times and space required for the best performing variation of our online ap-proach: RMFX , and the ones for the strongest baseline: WRMF. We also discuss the trade-off between time and space savings against the recommendation performance.

RMFX is implemented in the Python programming language using SciPy 6 . We ran RMFX on a Intel Xeon 1.87GHz ma-chine. For WRMF, we used the implementation provided by MyMediaLite [7], which is implemented in C#. The baseline WRMF was run on a machine with a slightly faster CPU (In-tel Xeon 2.27GHz). The experiments were conducted using GNU/Linux 64-bit as operating system. None of the meth-ods was parallelized and therefore used one single CPU for computations. Please remember that running times heavily depend on platform and implementation, so they should be only taken as relative indicators.

In Table 3, we can observe the gains in speed of our ap-proach over the baseline for all the evaluated reservoir sizes. We can observe that for all reservoir sizes RMFX is faster and more space efficient than WRMF. For example, RMFX with a Table 3: Time, Space and Recommendation Quality Comparison. reservoir size 8M is approximately 43 times faster and uses 77% less space than WRMF, and yet it delivers a highly competitive recommendation performance corresponding to 86.47% of the state-of-the-art baseline computed offline.
We proposed RMFX , an approach for recommending topics to users in presence of streaming data. Our online setting for collaborative filtering captures:  X  X hat is interesting to me right now within the social media stream X . RMFX represents a novel principled approach for online learning from streams, that selects a subsample of the observed data based on the objective function gradients, and uses it to guide the matrix factorization.

RMFX receives instances from a microblog stream and up-dates a matrix factorization model following a pairwise learn-ing to rank approach for dyadic data. At the core of RMFX is stochastic gradient descent which makes our algorithm easy to implement and efficiently scalable to large-scale datasets. Our empirical study used Twitter as a test bed and showed that models updated using the selective sampling approach proposed here, significantly outperform online methods that use random samples of the data.

In future work, we plan to investigate how the frequency of repeated events (i.e. users using the same hashtag or lis-tening to the same song) can be incorporated to the model to generate more accurate predictions.

