 The task of linking databases is an important step in an increasing number of data mining projects, because linked data can contain information that is not available otherwis e, or that would require time-consuming and expensive collec-tion of specific data. The aim of linking is to match and ag-gregate all records that refer to the same entity. One of the major challenges when linking large databases is the efficien t and accurate classification of record pairs into matches and non-matches. While traditionally classification was based on manually-set thresholds or on statistical procedures, man y of the more recently developed classification methods are based on supervised learning techniques. They therefore re -quire training data, which is often not available in real wor ld situations or has to be prepared manually, an expensive, cumbersome and time-consuming process.

The author has previously presented a novel two-step ap-proach to automatic record pair classification [6, 7]. In the first step of this approach, training examples of high qualit y are automatically selected from the compared record pairs, and used in the second step to train a support vector ma-chine (SVM) classifier. Initial experiments showed the feas i-bility of the approach, achieving results that outperforme d k -means clustering. In this paper, two variations of this approach are presented. The first is based on a nearest-neighbour classifier, while the second improves a SVM clas-sifier by iteratively adding more examples into the training sets. Experimental results show that this two-step approac h can achieve better classification results than other unsupe r-vised approaches.
 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database applications X  Data mining Algorithms, Experimentation, Performance Data matching, data linkage, deduplication, entity resolu -tion, nearest neighbour, support vector machine
As increasingly large amounts of data are being collected by many organisations, techniques that enable efficient min-ing of massive databases have in recent years attracted in-terest from both academia and industry. Sharing of large databases between organisations is also of growing impor-tance in many data mining projects, as data from various sources often has to be linked and aggregated in order to im-prove data quality, or to enrich existing data with addition al information [9, 24]. Similarly, detecting duplicate recor ds that relate to the same entity within one database is com-monly required in the data preparation step of many data mining projects [14]. The aim of such linkages and dedupli-cations is to match all records that relate to the same en-tity. These entities can be, for example, patients, custome rs, businesses, product descriptions, or publications.
Traditionally, record linkage has been employed in the health sector and within statistical agencies [24]. Today, many public and private sector organisations use deduplica -tion and linkage techniques to improve the quality of their databases. Government agencies use record linkage to, for example, identify people who register for assistance multi ple times, or who collect unemployment benefits despite being employed. National security, and crime and fraud detection , are other areas where record linkage is increasingly being e m-ployed. Security agencies often require fast access to files of a particular individual in order to solve crimes or to preven t terror through early intervention [18].

If all databases to be linked contain common entity iden-tifiers (or keys), then the problem of linking at the entity level can be solved by a standard database join. In most situations, however, no such entity identifiers are availab le, and therefore more sophisticated linkage techniques have t o be applied. Broadly, these techniques can be classified into deterministic, probabilistic, and modern approaches [9].
Figure 1 outlines the general record linkage process. An important initial step for successful linkage is data clean ing and standardisation, as in most real-world databases noisy , incomplete and incorrect information is common [10]. A lack of high quality data can be one of the biggest obstacles to successful record linkage. The main tasks of data clean-ing and standardisation are the conversion of the raw input data into well defined, consistent forms, and the resolution of inconsistencies in the way information is represented. Figure 1: The general record linkage process. The output of the blocking step are candidate record pairs, while the field (attribute) comparison step generates weight vectors with matching weights.

When linking two databases, A and B , potentially each record in A should be compared with all records in B . Therefore, the total number of potential record pair com-parisons equals | A | X | B | , with || denoting the number of records in a database. Similarly, when deduplicating a database, A , the total number of potential record pair com-parisons is | A | X  ( | A | X  1) / 2, as each record potentially has to be compared to all others. However, as the performance bottleneck in a record linkage or deduplication system is normally the expensive detailed comparison of field (or at-tribute) values between records [1, 9], it is impossible to c om-pare all pairs when the databases are large. Additionally, assuming there are no duplicate records in the databases to be linked (i.e. one record in A can only be a true match to one record in B , and vice versa), then the maximum num-ber of true matches corresponds to min ( | A | , | B | ). Thus, when linking larger databases the computational efforts po-tentially increase quadratically while the maximum number of true matches only increases linearly. This also holds for deduplication, where the number of true duplicate records is always less than the number of records in a database.
To reduce the potentially very large number of compar-isons to be conducted between pairs of records, some form of indexing or filtering technique, collectively known as block-ing [1], is employed by most record linkage systems. A single record attribute, or a combination of attributes, often cal led the blocking key , is used to split the databases into blocks. All records that have the same value in the blocking key will be inserted into the same block, and candidate record pairs are only generated from records within the same block. Even though blocking will remove many of the record pairs that are obvious non-matches, some true matches will likely also be removed in the blocking process, because of errors or typographical variations in record attribute values [9] .
The two records in a candidate pair are compared us-ing similarity functions applied to selected record attrib utes (fields). These functions can be as simple as an exact string or a numerical comparison, can take typographical varia-tions into account [11], can be specialised for example for date or time values, or they can be as complex as a dis-tance comparison based on look-up tables of geographic lo-cations (longitudes and latitudes). There are also various approaches to learn such similarity functions from trainin g data [3, 11]. Each similarity function returns a numerical Figure 2: Four example records (made of given name and surname; and street number, name and type) and the corresponding weight vectors (WV) result-ing from the comparisons of these records. matching weight that is usually normalised, such that 1 . 0 corresponds to exact similarity and 0 . 0 to total dissimilar-ity, with attribute values that are somewhat similar having a matching weight somewhere in between 0 and 1.

As illustrated in Figure 2, for each compared record pair a weight vector is formed that contains the matching weights calculated for that pair. Using these weight vectors, candi -date pairs are classified into matches , non-matches , and pos-sible matches , depending upon the decision model used [9, 15, 17]. Pairs of records that are not compared due to the blocking process are implicitly assumed to be non-matches. Assuming there are no duplicate records in the databases to be linked, then the majority of candidate pairs are likely non-matches, because the maximum possible number of true matches corresponds to the number of records in the smaller of the databases that are linked. Classifying record pairs i s therefore often a very imbalanced problem [9].

Two records that have equal or very similar values in all their attributes will likely refer to the same entity, as it i s unlikely that two entities have very similar values (or even the same value) in all their record attributes. All matching weights calculated when comparing such a pair of records will be 1, or close to 1. On the other hand, weight vectors that contain matching weights of only 0, or weights close to 0, were with high likelihood calculated when two records that refer to two different entities were compared, as it is unlikely that two records that refer to the same entity have totally different values in all their attributes. For exampl e, when somebody moves, most of that person X  X  address details will change, while the person X  X  name, gender and date of birth will stay the same. Therefore, there would be several record attributes that keep their values.

Based on these observations, it is often easy to accurately classify record pairs as matches when their weight vectors contain only matching weights close to or equal to 1, and as non-matches when their weights are all close to or equal to 0. On the other hand, it is more difficult to correctly clas-sify record pairs that have some similar and some dissimilar attribute values. In Figure 2, for example, records R1 and R2 are very similar to each other, with only two small dif-ferences in their given name and street type attributes, and thus very likely refer to the same person. On the other hand, records R3 and R4 are more different from each other, and it is not obvious if they refer to the same person or not.
It follows that it is possible to automatically select train ing examples (weight vectors) from the set of all weight vectors that with high likelihood correspond to true matches or true non-matches, and to then train a supervised binary classifie r using these training examples as  X  X eeds X . For example, of the weight vectors shown in Figure 2, WV(R1,R2) can be selected as a match training example, while WV(R1,R3) and WV(R2,R3) , possibly even WV(R1,R4) and WV(R2,R4) , can be used as non-match training examples.

This two-step approach to automated record pair classifi-cation has first been proposed by the author in [6], with ini-tial experiments indicating its feasibility. The contribu tion of this paper is the evaluation of two improved classificatio n methods to be used in the second step of the approach. The first method is based on nearest-neighbour classification, a nd the second improves SVM classification by iteratively addin g more weight vectors into the training sets.

The remainder of this paper is structured as follows. An overview of related work is given next. Then, the proposed two-step approach to record pair classification is presente d in detail in Section 3, with the new classification methods discussed in Section 3.2. These two methods are then eval-uated experimentally in Section 4 using both real and syn-thetic data sets, and the paper is concluded in Section 5 with an outlook to future work.
The classic probabilistic record linkage approach, as for-malised in the 1960s by [15], has in recent years been im-proved by applying the expectation-maximisation (EM) al-gorithm for better parameter estimation in record pair clas -sification [23], and by using approximate string comparison s to calculate partial agreement weights when record attribu te (field) values have typographical variations [11, 14].
Since the mid 1990s, researchers have investigated a vari-ety of approaches to record linkage, originating from artifi -cial intelligence, database technology, information retr ieval, machine learning, and data mining [9, 14], with the aims of improving the linkage quality and the scalability of the lin k-age process. Many of these approaches are based on super-vised learning techniques and require training data (recor d pairs with known true match or true non-match status). Such training examples, however, are often not available in real world situations, or they have to be prepared manually. This is a laborious process, and often the training data gen-erated is not 100% accurate, as even humans are not always able to clearly determine weather two records are a true match or not, without having access to further information.
One supervised approach is to learn similarity measures for approximate string comparisons, such as the costs for edit-distance operations [3, 11], with the aim to adapt sim-ilarity calculations to a particular data domain. Decision tree induction [13, 22] and support vector machines [19] are two popular supervised machine learning techniques that have been employed successfully for record pair classifica-tion. As expected, these techniques usually achieve better linkage quality compared to unsupervised approaches.
Three methods for record pair classification have been im-plemented in TAILOR [13]: the first is based on supervised decision tree induction, the second is using unsupervised k -means clustering (with three clusters, one each for matches , possible matches and non-matches), and the third is a hy-brid approach that combines the first two to overcome the problem of lack of training data. It first clusters a sub-set o f the weight vectors (again into matches, possible matches an d non-matches), and then uses the match and non-match clus-ters to train a supervised decision tree classifier. Both the fully supervised and hybrid approach outperformed k -means clustering in experimental studies. In Section 4, a variati on of the hybrid TAILOR approach (employing a SVM instead of a decision tree classifier) will be compared experimental ly to the proposed two-step classification approach.

Active learning is an approach that aims to overcome the problem of lack of training data. A system that presents a difficult to classify record pair to a user for manual classific a-tion is discussed in [21]. After such a pair has been manually classified, it is added to the training data and the classifier is re-trained. This process is repeated until all record pairs are successfully classified. Using this approach, manually cla s-sifying less than 100 training pairs provided better result s than a fully supervised approach that required 7,000 ran-domly selected examples. A similar approach is presented in [22], where a committee of decision trees is used to learn a set of rules that describe linkages.

Unsupervised clustering techniques have been investigate d both to improve blocking [1, 12] and for automatic record pair classification [13, 16, 17]. The clustering techniques k -means and farthest-first were compared in [16] with super-vised decision tree induction using both synthetic and real data. Surprisingly, farthest-first clustering outperform ed k -means and achieved results comparable to decision trees. In [17], the k -means clustering algorithm has been employed to group weight vectors into matches and non-matches. In this approach, a user can also identify a  X  X uzzy X  region half way in between the two cluster centroids where the difficult to classify record pairs are located. These pairs will then be handed to the user for manual clerical review. Using syn-thetic data, it was shown that this approach can significantl y reduce the number of record pairs that have to be reviewed manually, while keeping high linkage quality.

Recently, unsupervised techniques based on relational clu s-tering [2] have been explored for entity resolution of re-lational data. While traditional record linkage technique s assume that only similarities between attribute values are available, in relational data the entities have additional rela-tional information that can be used to improve the quality of entity resolution. Relational information is present, for ex-ample, in census databases that include a family relationsh ip attribute (with values like  X  X arried to X ,  X  X ependent of X , o r  X  X arent of X ); or in bibliographic data where, besides the na me of a paper, a publication record also contains a list of one or more authors that can indicate co-author relationships. Experimental results [2] showed that relational entity res o-lution outperforms traditional record linkage based only o n record attribute similarities. However, non-relational d ata is still available in many real world applications, such as i n databases that contain hospital patient or customer inform a-tion, and this paper concentrates on improving unsupervise d classification of such non-relational data.

The two-step classification approach presented in this pa-per has been inspired by similar methods for text classifica-tion, where commonly only a limited number of positive la-beled examples, besides many unlabeled examples, are avail -able for training. In such situations the aim is to learn a cla s-sifier from these positive and unlabeled examples. In [25], the TC-WON approach is described, which iteratively trains a SVM using the positive and a selected set of strong nega-tive examples. Additional unlabeled examples are included into the training sets as the trained classifier becomes more accurate, until all unlabeled examples are classified.
The idea of seeded record pair classification is based on the following two assumptions. First, weight vectors that contain exact or high similarity values in all their match-ing weights were with high likelihood generated when two records that refer to the same entity were compared. Sec-ond, weight vectors that contain mostly low similarity valu es were with high likelihood generated when two records that refer to different entities were compared. As a result, selec t-ing such weight vectors in a first step as seeds for generat-ing training data, and training a classifier using these seed training examples in a second step, should enable automatic , efficient and accurate record pair classification.
Previously, in [6] and [7], the author has shown the fea-sibility of this proposed approach, and investigated sever al variations of how to select the initial seed training exampl es. This paper concentrates on the second step of the approach, which will be discussed in detail in Section 3.2. First, an overview of the first step of the approach is given.
Let W be the set of weight vectors that were generated in the comparison step (after blocking has been applied to re-duce the total number of detailed record pair comparisons). The aim of the first step of the proposed approach is to select weight vectors from W that with very high likelihood corre-spond to true matches and true non-matches. The selected weight vectors are inserted into the match seed training ex-amples set, W M , and the non-match seed training examples set, W N , respectively (with W M  X  W N =  X  ). There are two main approaches to selecting training examples, either usi ng distance thresholds or nearest-based [6].

The threshold based approach selects weight vectors that have all their matching weights within a certain distance threshold to the exact similarity or total dissimilarity va lues, respectively. For example, using the weight vectors from Figure 2 and a distance threshold of 0 . 2, only WV(R1,R2) will be selected into W M , and WV(R1,R3) and WV(R2,R3) into W N . The remaining three weight vectors will not be selected, as at least one of their matching weights is furthe r than the 0 . 2 distance threshold away from 0 or 1.
In the nearest based approach, on the other hand, weight vectors are sorted according to their distances (using, for ex-ample, Manhattan or Euclidean distance) from the vectors containing only exact similarities and only total dissimil ari-ties, respectively, and the respectively nearest vectors a re se-lected into the training sets. In Figure 2, WV(R1,R2) is clos-followed by WV(R3,R4) ; while WV(R1,R3) and WV(R2,R3) only contain total dissimilarity values, and WV(R1,R4) and WV(R2,R4) are the next vectors closest to them.

Experiments [6] showed that the nearest based approach generally outperforms threshold based selection. One reas on is that nearest based selection allows explicit specificati on of the number of weight vectors to be included into W M and W N . Because weight vector classification is often a very imbalanced problem, the number of true non-matches in W is commonly much larger than the number of true matches [9], and thus more weight vectors should be selected into W N than into W M . An estimation, r , of the ratio of true matches to true non-matches can be calculated using the number of records in the two databases to be linked, A and B , and the number of weight vectors in W : with || denoting the number of elements in a set or a database. The problem with balanced training set sizes is that weight vectors that likely do not correspond to true matches will be selected into W M [6].
Once the seed training example sets for matches, W M , and non-matches, W N , have been generated, they can be used to train any binary classifier. In the following two sec-tions, a nearest-neighbour based classifier and an iterativ e SVM classifier are presented. The set of weight vectors not selected into the seed training example sets will be denoted with W U , with W U = W \ ( W M  X  W N ).
The basic idea of this classifier is to iteratively add unclas -sified weight vectors from W U into the training sets until all weight vectors are classified. In each iteration, the un-classified weight vector closest to k already classified weight vectors is classified according to a majority vote of its clas -sified neighbours (i.e. if the majority is either matches or non-matches). Using the seed training example sets, this nearest-neighbour based classifier can be implemented effi-ciently as illustrated in Figure 3 and detailed in Algorithm 1. Algorithm 1: Seeded k-NN classification Input: -Set of weight vectors generated in comparison step: W -Seed training examples match set: W M -Seed training examples non-match set: W N -Distance function: dist -Number of nearest-neighbours to consider: k Output: -Weight vectors classified as matches: Z M -Weight vectors classified as non-matches: Z N 1: Z M := W M and Z N := W N 2: W T := ( W M  X  W N ) and W U := W \ W T 3: Initialise empty heap H 4: M := [ ] , N := [ ] , U := [ ] 5: for w m  X  W M : 6: M [ w m ] := { ( k + 1) nearest w u  X  W U , 7: end for 8: for w n  X  W N : 9: N [ w n ] := { ( k + 1) nearest w u  X  W U , 10: end for 11: for w u  X  W U : 12: U [ w u ] := { ( k + 1) nearest w t  X  W T , 13: s := P w 14: Insert ( s, w u ) into H 15: end for 16: while W U 6 =  X  : 17: ( s, w t ) := first element in H 18: W U := W U  X  w t 19: if U [ w t ] contains more weight vectors from 20: Z M := Z M + w t 21: else: 22: Z N := Z N + w t 23: end if 24: X u :=  X  w 25: for w u  X  X u : 26: d := dist ( w t , w u ) 27: if w u is one of ( k + 1) nearest to w t : 28: Update w t in U [ w u ] with d 29: s := P w 30: Update ( s, w u ) in H 31: end if 32: end for 33: if w t  X  Z M : 34: M [ w t ] := { ( k + 1) nearest w u  X  X u , 35: else: 36: U [ w t ] := { ( k + 1) nearest w u  X  X u , 37: end if 38: end while
In Algorithm 1, the number of nearest weight vectors to be considered when nearest neighbours are selected is denoted with k (with k  X  1). The function dist calculates the dis-tance between two weight vectors, and can be any distance function such as Euclidean, Manhattan, Canberra, or Cosine distance. In the first line of the algorithm, the output sets of classified match and non-match weight vectors, Z M and Z
N , are initialised to the seed training example sets. Next, in line 2, the set W T of all seed training examples and the set W U of all unclassified weight vectors are generated. An empty heap data structure, H , is then initialised next. A heap has the property that its first element is always the smallest element. It will be used in the second phase of the algorithm to iteratively get the next unclassified weight ve c-tor that has the smallest distance to the training sets. In line 4, three lists, M , N and U , are initialised that will be used to store nearest weight vectors as detailed below. Lines 5 to 15 constitute the first phase of Algorithm 1. In lines 5 and 6, for each weight vector in the match seed training set W M , the closest ( k + 1) not classified weight vectors from W U are stored in the list M . The same is done in lines 8 and 9 for the weight vectors in the non-match seed training set W N , with nearest neighbours from W U stored in list N . These ( k + 1) nearest neighbours (in M and N ) are represented in Figure 3 using dashed arrowed lines. In lines 11 and 12, for each unclassified weight vector in W U , the closest ( k + 1) weight vectors from the overall seed training set W T are stored in the list U . These nearest neighbours (in U ) are represented in Figure 3 using black arrowed lines (with the nearest neighbour indicated using a bold black line). Additionally, in lines 13 and 14, the sum of the distances, s , of the k closest training set neighbours for each w u in W u are calculated and inserted into the heap H . Therefore, at the end of this first phase of the algorithm, the first element of H will be the weight vector from W U with the smallest distance sum to vectors from W T .
Phase two of Algorithm 1 (line 16 onwards) iterates until all weight vectors in W U are classified. In line 17, the first element in H , i.e. the weight vector with the smallest sum of distances to classified weight vectors, is taken from H and removed from W U in line 18. Depending upon if the majority of neighbours of w t are matches or non-matches, it is added to the set of classified matches, Z M , or classified non-matches, Z N , respectively (lines 19 to 22).

In line 24, the set U [ w t ] of nearest training set weight vectors of w t is used to create the set X u of nearest unclas-sified weight vectors. X u is retrieved via the corresponding nearest sets in the lists M and N . Line 25 then loops over each of the unclassified weight vectors w u in X u that are nearest to the newly classified weight vector w t . In line 26, the distance d from w u to w t is calculated, and the list of nearest classified weight vectors for w u , U [ w u ], is updated with this new distance d in line 28 if d is one of the ( k + 1) smallest distances. In this case, the heap element for w also needs to be updated in H with the newly calculated distance sum s (lines 29 and 30). Finally, depending upon if w t was classified as a match or a non-match, the set of nearest unclassified weight vectors for w t is updated in the corresponding list M or N in lines 33 to 36.

This process is illustrated in Figures 3 (b) and (c). The middle upper, unclassified weight vector is classified as a match, because its nearest neighbour is a match. Its list of nearest matches (solid lines) is used to get its nearest clas -sified neighbours (the two seed matches in the top right), which in turn have lists of their nearest unclassified neigh-bours (dotted lines). The union of these lists becomes the new list of unclassified nearest neighbours, represented in Figure 3 (c) with the new dotted lines that point from the newly classified match to its two unclassified neighbours.
Na  X   X ve nearest-neighbour based classification would involve calculating the distances between all pairs of weight vecto rs, and thus have complexity O ( | W | 2 ). This could be improved significantly by employing data reduction or fast search-ing and indexing techniques for nearest-neighbour classifi -cation [20], work that is left for future improvements.
The seeded training sets also allow a reduction of distance calculations to be done through the use of the nearest lists M , N and U . In the first phase of Algorithm 1, distances are only calculated between the weight vectors in the overal l training example set W T and those in W U . If a fraction of t ( t &lt; 1) of all weight vectors is included in W T , then the number of distance calculations in the first phase of the al-gorithm is | W | 2  X  ( t  X  t 2 ). The maximum number of distance calculations will have to be done if half the weight vectors are in W T and half are in W U , i.e. t = 0 . 5: | W | 2  X  0 . 25; while with t = 0 . 1, for example, the number of distance calculations to be done is only | W | 2  X  0 . 09. The second phase of the algorithm involves calculating a maximum of ( k + 1)  X  k distances for each of the weight vectors in W as for each of the k nearest training set weight vectors the ( k + 1) nearest vectors will have to be checked for closeness.
The iterative SVM classifier is similar to the TC-WON [25] approach for text and Web page classification based on only positive labeled training examples. The basic idea is to tra in an initial SVM using the seed training example sets W M and W
N , and to then iteratively add the strongest positive and negative classified weight vectors from W U into the training sets of subsequent SVMs.
 Algorithm 2 details the steps involved in this approach. The input parameter ip determines what percentage of un-classified weight vectors will be added into the training set s in each iteration, and tp determines the total percentage of weight vectors that will be added into the training sets. For example, if tp = 100% then all weight vectors from W will be used in the last iteration to train the final SVM. The algorithm starts with initialising the training sets T for matches and T N for non-matches, and by creating the set W U of all unclassified weight vectors. The initial SVM svm 0 is trained in line 3 using T M and T N . The main loop then starts in line 5 and iterates until tp percent of all weight vectors have been included into the training sets.
Each iteration starts in line 6 by classifying the weight vec -tors in W U using the previously trained SVM svm i . The function svm classify returns two sets, X M and X N , that contain the weight vectors from W U classified as matches and non-matches, respectively. These classified weight vec -tors are sorted in line 7 according to how far away they are from the SVM decision boundary, and in lines 8 and 9 the strongest positive and negative weight vectors are ex-tracted into the sets Y M of new matches, and Y N of new non-matches. The size of these sets is determined by the in-crement percentage parameter ip . For example, if ip = 50%, then in each iteration half of the weight vectors in X M are inserted into Y M and half of X N into Y N . In lines 10 and 11, the new training examples in Y M and Y N are added to the training sets T M and T N , and a new SVM svm i is trained next in line 13 using these expanded training sets. Finally, in the last step within the iteration, in line 14, th e new training examples from Y M and Y N are removed from the set W U of unclassified weight vectors.
 Algorithm 2: Seeded iterative SVM classification Input: -Set of weight vectors generated in comparison step: W -Seed training examples match set: W M -Seed training examples non-match set: W N -Increment percentage: ip -Total training percentage: tp Output: -Weight vectors classified as matches: Z M -Weight vectors classified as non-matches: Z N 1: T M := W M and T N := W N 2: W U := W \ ( W M  X  W N ) 3: svm 0 := train svm ( T M , T N ) 4: i := 0 5: while ( | T M | + | T N | ) &lt; ( | W | X  tp/ 100) : 6: X M , X N := svm classifiy ( svm i , W U ) 7: Sort X M and X N according to distance from 8: Y M := | X M | X  ( ip/ 100) vectors from X M 9: Y N := | X N | X  ( ip/ 100) vectors from X N 10: T M := T M  X  Y M 11: T N := T N  X  Y N 12: i := i + 1 13: svm i := train svm ( T M , T N ) 14: W U := W U \ ( Y M  X  Y N ) 15: end while 16: X M , X N := svm classifiy ( svm i , W U ) 17: Z M := T M  X  X M and Z N := T N  X  X N
The final SVM is then used in line 16 to classify the weight vectors in W U that have not been classified so far (this step is not required if tp = 100%), and in line 17 the final two sets of matches and non-matches, Z M and Z N , are created.
Assuming that training a SVM is of quadratic complexity in the number of training examples [25], then the overall complexity of Algorithm 2 is O ( | W | X  i ), with i being the number of times a SVM is trained. The value of i depends upon the ip and tp parameter values. For example, if ip = 50% and tp = 100% then i = log 2 ( | W U | ), while if ip = 25% and tp = 50% then i = 3, as in each step 25% of weight vectors from W U will be added to the training sets. Training of SVMs will become increasingly time consuming as more weight vectors are added into the training sets.
The two record pair classifiers presented above were eval-uated and compared with two other classification methods. The first is a fully supervised SVM that has access to the true match status of all weight vectors. Nine parameter vari -ations were evaluated: three kernel methods (linear, poly-nomial and RBF), and three values for the cost parameter, C [4] (0 . 1, 1, 10). The second method is based on the hybrid approach implemented in the TAILOR [13] toolbox. It first employs k -means (with one cluster each for matches, possi-ble matches and non-matches), and then uses the match and non-match clusters to train a SVM (in [13] a decision tree classifier has been used instead). Two distance functions (Manhattan and Euclidean) were evaluated for the k -means step, while for the SVM classifier step the same nine param-eter variations as for the supervised SVM were used. Census 449 + 392 Linkage 1.000 0.988 2,093 1 / 4.34 1 / 5.40 Restaurant 864 Deduplication 1.000 0.713 106,875 1 / 122.7 1 / 953.2 Cora 1,295 Deduplication 0.924 0.793 173,769 1 / 133.2 1 / 9.94 DS-Gen-A 1,000 Deduplication 0.957 0.995 2,475 1 / 1.48 1.13 / 1 DS-Gen-B 2,500 Deduplication 0.940 0.997 9,878 1 / 2.95 1 / 2.06 DS-Gen-C 5,000 Deduplication 0.953 0.997 35,491 1 / 6.10 1 / 4.48
DS-Gen-D 10,000 Deduplication 0.948 0.997 132,532 1 / 12.25 1 / 9.32
For the two-step classification approach, the imbalanced nearest based selection method [6] was used, with the num-ber of seed training examples in W N selected as 5% or 10% of all weight vectors in W , respectively, and the number of training examples in W M calculated according to the ratio r as given in Equation (1). For the nearest-neighbour based two-step classifier (as described in Section 3.2.1), again M an-hattan and Euclidean distances were evaluated in combina-tion with k set to 3 and 9. For the iterative SVM based two-step classifier (described in Section 3.2.2), the same n ine parameter variations as for the supervised SVM were evalu-ated, and the parameters ip and tp were set to the pairs (0,0) (no iterative refinement), (25,25), (25,50), and (50,100).
The experiments for the supervised SVM and TAILOR classifiers were conducted using 10-fold cross validation ( 90% used for training and 10% for testing), while this was not possible for the two-step classifier, because the selection of seed training examples requires all weight vectors in W .
All classifiers are implemented in the Febrl [8] record link-age system, which is written in Python. The libsvm library was used for the SVM classifier [4]. All experiments were run on a 2.13 GHz dual-core CPU with 2 GBytes of main memory, running Linux 2.6.20 and using Python 2.5.1.
Experiments were conducted using both real and synthetic data, as summarised in Table 1. Three real data sets from the SecondString toolkit 1 were used, while four synthetic data sets of various sizes were created using the Febrl data set generator [5]. This synthetic data contains name and ad-dress attributes that are based on real-world frequency ta-bles, and includes 60% original and 40% duplicate records. The duplicates were randomly created through modification of record attributes (like inserting, deleting or substitu ting characters, and swapping, removing, inserting, splitting or merging words), again according to real-world error char-acteristics. Up to nine duplicates were generated for one original record, with a maximum of three modifications per attribute and a maximum ten modifications per record. http://secondstring.sourceforge.net
Standard blocking [1] was applied in all experiments to re-duce the number of detailed record pair comparisons, with the blocking keys being combinations of name, address and postcode values. In the record pair comparison step, the Winkler [24] approximate string comparator (commonly used in record linkage) was employed for name, address, paper ti-tle and conference name attribute (field) comparisons. Ad-ditionally, character difference comparisons [8] were used on attributes such as postcode, street number, and publica-tion year. Figure 4 shows histograms based on the summed weight vectors that were generated in the comparison step. The imbalance between the number of matches and non-matches can be seen clearly, especially for the  X  X estaurant  X  data set. The ratios of matches to non-matches (both cal-culated according to Equation (1) and based on the data itself) are shown in the last column of Table 1.

The quality of the compared record pairs is shown in Ta-ble 1 using the pairs completeness measure (which is the number of true matched record pairs generated by blocking divided by the total number of true matched record pairs), and the complexity of the record pair comparison step is shown using the reduction ratio measure (which is one mi-nus the number of record pairs generated by blocking divided by the total possible number of record pairs) [9, 13].
Due to the imbalanced distribution of matches and non-matches in the weight vector set W , the accuracy measure commonly used for evaluating classifier performance is not suitable for assessing the quality of record pair classifica -tion [9]: the large number of non-matches would dominate accuracy, and show results that are too optimistic. In-stead, the F-measure, F , the harmonic mean of precision, P , and recall, R , is used for measuring classifier quality: F = 2( P  X  R ) / ( P + R ), with P = T P/ ( T P + F P ) and R = T P/ ( T P + F N ). T P is the number of true positives (true matched record pairs classified as matches), F N the number of false negatives (true matched record pairs classi -fied as non-matches), and F P the number of false positives (true non-matched record pairs classified as matches). gives the percentage of weight vectors from W selected into W
Table 2 shows the quality of the seed training example sets generated in the first step of the proposed two-step classifi-cation approach (as described in Section 3.1), given as the percentage of correctly selected weight vectors in these se ts, i.e. (100  X | true matches in W M | / | W M | ) and 100  X | true non-matches in W N | / | W N | ). For the second step, Figure 5 shows the average F-measure results, together with the min-imum and maximum F-measure results over all parameter variations discussed above (i.e. 9 supervised SVMs, 18 TAI-LOR classifiers, 8 nearest-neighbour based two-step classi -fiers, and 72 iterative two-step SVM classifiers  X  18 each for the four variations of the ( ip , tp ) parameter pairs). These average results, rather than the  X  X est X  results using a cert ain parameter setting, are presented as they provide a more real -istic picture of the overall performance of a classifier (whi ch likely depends upon the characteristics of a data set).
As can be seen in Table 2, the seed training example sets selected in the first step of the proposed two-step clas-sification approach are mostly of very high quality, with the match training example set W M only containing true matches in all but one case (10% seed size for the  X  X estau-rant X  data set). While overall the 1% training set selection contains the highest quality seed training data, the size of these sets (especially W M ) is very small, and the resulting classifiers based on these 1% seeds generated in the second step were much worse in most experiments compared to the classifiers generated based on 5% or 10% seed sizes. There-fore, the classifiers based on 1% seed size were not used further and are not included in the results presented.
Figure 5 shows the F-measure results for all data sets, with the results for the four synthetic data sets averaged (a s they all had very similar F-measure results). As can be seen, there is a large variety of F-measure result values for certa in classifiers, and very different F-measure result values for t he same classifier on the different data sets.

As expected, the supervised SVM (which can be seen as an  X  X racle X  as it knows the true match status of all record pairs ) performs best on all data sets. For the  X  X ensus X  and  X  X estau-rant X  data sets, there are, however, parameter settings tha t lead to SVM classifiers that perform worse than the best two-step classifier using the iterative SVM approach. The TAILOR hybrid classifier approach has the lowest perfor-mance on most data sets. Only on  X  X ora X  did it achieve better results than most two-step classifier variations.
The nearest-neighbour based two-step classifier performs better than all iterative SVM variations for all synthetic d ata sets, while for the real data sets the iterative SVM generall y achieves better classification results. Looking at the diffe rent values of the parameter pairs ( ip , tp ), a noticeable improve-ment when including more weight vectors into the training sets is only visible for  X  X ora X , while the improvements are very small for the synthetic data sets, and mixed for the  X  X ensus X  data. For the  X  X estaurant X  data set, the results ar e getting worse when more training data is added.

The results for the  X  X estaurant X  data set are very low for all unsupervised classifiers compared to the supervised SVM . One reason for this is that for this data set the weight vector set W only contains 112 matches (duplicates), but 106,763 non-matches (a ratio of 1 to 953), making it very difficult to extract true match examples. Another reason is that the attributes in this data set contain addresses with a large pr o-portion of either abbreviations or completely different val ues (such as  X  X os Angeles X  versus  X  X everly Hills X ) for the same restaurant. Therefore, the weight vectors generated when such attribute values were compared have a very overlapped distribution of matches and non-matches (as can be seen in Figure 4), that are hard to classify without knowing the true match status of these weight vectors. This can also be seen in Table 2, where with the 10% seed size the match training set W M already contains more than 9% non-matches.
The experiments presented in this paper show that the proposed two-step classification approach can achieve resu lts that outperform other unsupervised record pair classifica-tion techniques, such as the hybrid TAILOR approach which previously has shown to be better than k -means cluster-ing [13]. On the other hand, these experiments also showed the limitations of unsupervised classification based on onl y pair-wise record attribute similarities.
This paper presented a novel unsupervised two-step ap-proach to record pair classification that is aimed at automat -ing the record linkage process. This approach combines au-tomatic selection of seed training examples with training o f a binary classifier. The two two-step classifiers discussed achieve improved record pair classification results compar ed to other unsupervised classifiers, such a the hybrid TAI-LOR [13] approach. Thus, the proposed approach can be used in situations where no training data is available.
Future work will include to conduct more experiments using different data sets, including run-time tests on data sets of various sizes in order to experimentally get scal-ability results. Related to this is the implementation of data reduction and fast searching and indexing techniques for the nearest-neighbour based classifier [20], and simila r approaches for the iterative SVM, with the aim to reduce training times while keeping a high record pair classificati on quality. Another area of research will be to investigate ac-tive learning techniques [21, 22] and combine them with the seeded training example selection approach presented here . Active learning can, for example, be used in the iterative two-step SVM classifier to select the hardest to classify ex-amples and hand them to a user for manual classification (assuming additional information might be available).
This work is supported by an Australian Research Council (ARC) Linkage Grant LP0453463 and partially funded by the New South Wales Department of Health, Sydney. [1] R. Baxter, P. Christen, and T. Churches. A [2] I. Bhattacharya and L. Getoor. Collective entity [3] M. Bilenko and R. J. Mooney. Adaptive duplicate [4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [5] P. Christen. Probabilistic data generation for [6] P. Christen. A two-step classification approach to [7] P. Christen. Automatic training example selection for [8] P. Christen. Febrl -A freely available record linkage [9] P. Christen and K. Goiser. Quality and complexity [10] T. Churches, P. Christen, K. Lim, and J. X. Zhu. [11] W. Cohen, P. Ravikumar, and S. Fienberg. A [12] W. Cohen and J. Richman. Learning to match and [13] M. Elfeky, V. Verykios, and A. Elmagarmid. TAILOR: [14] A. Elmagarmid, P. Ipeirotis, and V. Verykios. [15] I. Fellegi and A. Sunter. A theory for record linkage. [16] K. Goiser and P. Christen. Towards automated record [17] L. Gu and R. Baxter. Decision models for record [18] J. Jonas and J. Harper. Effective counterterrorism and [19] U. Y. Nahm, M. Bilenko, and R. J. Mooney. Two [20] J. S. Sanchez, J. M. Sotoca, and F. Pla. Efficient [21] S. Sarawagi and A. Bhamidipaty. Interactive [22] S. Tejada, C. Knoblock, and S. Minton. Learning [23] W. E. Winkler. Using the EM algorithm for weight [24] W. E. Winkler. Methods for evaluating and creating [25] H. Yu, C. X. Zhai, and J. Han. Text classification
