 As a number of social network services appear online recentl y, there have been many attempts to analyze social networks for ex-tracting valuable information. Most existing methods first repre-sent a social network as a quite sparse adjacency matrix, and then analyze it through matrix operations such as matrix multipl ication. Due to the large scale and high complexity, efficient process ing multiplications is an important issue in social network ana lysis. In this paper, we propose a GPU-based method for efficient spars e ma-trix multiplication through the parallel computing paradi gm. The proposed method aims at balancing the amount of workload bot h at fine-and coarse-grained levels for maximizing the degree of paral-lelism in GPU. Through extensive experiments using synthet ic and real-world datasets, we show that the proposed method outpe rforms previous methods by up to three orders-of-magnitude.
 H.4 [ Information Systems Applications ]: Miscellaneous Measurement, Algorithms Sparse matrix multiplication; social network analysis; GP U
Recently, the amount of social networks data is rapidly grow -ing with various kinds of social network services being wide ly used. Moreover, a huge number of mobile devices accelerate t he growth of the volume of social networks data. Due to large sca le and high complexity, there have been a lot of studies to analy ze social networks to extract valuable information. They expl oit the analysis results to address issues in ranking [1], similari ty measure [2][3], and recommendation [4][5]. In general, a social net work G can be represented as a set of nodes V and a set of edges E , i.e.,  X  Corresponding author c G = &lt; V, E &gt; , where each edge can have its own weight. One of typical formats for representing such social networks is th e adja-cency matrix (shortly, matrix), where an element of a matrix means a weight of an edge. Most of social network analyses can be don e through matrix-matrix multiplication or matrix-vector mu ltiplica-tion [1][2][5].

As the scale of social networks increases, the size of matric es increases as well. Thus, in order to compute multiplication s of such big matrices , the following two kinds of approaches have been proposed: (1) a distributed computing approach and (2) a par allel computing approach. The distributed computing approach lo cates social network data over multiple machines and computes the mul-tiplication through communications among machines. For ex am-ple, a MapReduce based method [6] belongs to this approach. O n the other hand, the parallel computing approach locates the data on a single machine and computes the multiplication with a large number of processors inside the machine. For example, a Graphic Processor Unit (GPU) based method [7] belongs to this approach. Between the two approaches, the distributed computing appr oach might be more scalable and flexible than the parallel computi ng one, but it requires high cost and high management overhead o f distributed systems.
 There have been many parallel computing methods based on GPU proposed for matrix multiplication [8][9]. NVIDIA , one of major manufacturers of GPU, has proposed various matrix mul ti-plication methods based on GPU. Those methods can be catego-rized into three types depending on the sparsity of matrices : (1) multiplication for two dense matrices , (2) multiplication for a spare matrix and a dense one , and (3) multiplication for two sparse ma-trices . NVIDIA provides three different implementations (i.e., l i-braries) for those methods, cuBlas, csrMM [8], and csrGEMM [9], respectively. In addition to the NVIDIA X  X  methods, Yang et. al. [7] have proposed a sparse matrix-vector multiplications ( spMV ) method. This method can be easily extended to support sparse ma-trix multiplication by performing spMV iteratively. Herea fter, we call this extended method spMV-E .

GPU has characteristics different from CPU. If a GPU-based method does not consider them, the performance of matrix mul -tiplication using GPU can be significantly degraded [8][9]. First, GPU follows the single instruction multiple threads (SIMT) where multiple threads perform the same instruction concurrently [10]. Second, GPU has various kinds of memories having distinct ch ar-acteristics. In particular, device memory is the largest, b ut the slow-est one among them. It plays a role like main memory in CPU, and thus has all the data before GPU processing.
 Besides, social networks have their own unique characteris tics. If a GPU-based method for social network analysis does not ta ke them into account, the performance of matrix multiplicatio n could be degraded as well. First, social networks are sparse , that is, the number of edges is much smaller than that of all the possi-ble edges (relationships) among nodes [11]. For example, th e den-sity of DBLP data is 0.001% ( | V | =317k, | E | = 1,049k), and that of Youtube data is only 0.0002% ( | V | =1,134k, | E | =2,987k). As a result, most elements of matrices for social networks are ju st zeros (i.e., empty). Second, social networks follow the power-law degree distribution [7][12]. That is, so-called hub nodes have a extremely high out-degree while ordinary nodes a low out-degree. In th e ma-trices for social networks, some rows (or columns) might hav e a large number of non-zero elements, while other rows (or colu mns) have only few non-zero elements. That is, the distribution o f non-zero elements tends to be highly skewed [7].

Existing GPU-based methods for matrix multiplication do no t consider the above characteristics of social networks, lea ding to the following problems. First, it is difficult for the method s using the dense matrices such as cuBlas and csrMM[8] to handle even moderate-scale social networks (e.g., | V | =20,000) due to the lim-ited size of device memory (e.g., 6GB) [13]. Since most of rea l-world social networks are much bigger than that size, they ar e not applicable in the analysis of real-world social networks. S econd, the methods using the matrix in sparse format such as csrGEMM [9] and spMV-E tend to degrade the performance due to poor loa d balancing of computations in either the thread level or the S tream-ing Processor (SM) level. Because the distribution of non-z ero ele-ments tends to be highly skewed in a social network [7], in the inner and row-row products, some non-zero elements in a matrix cou ld be matched, during a matrix multiplication, to a fairly larg e number of non-zero elements in the other matrix. That means some thr ead could spend much longer time to complete its own work than oth er threads. The overall computation time in GPU is determined b y the slowest thread among all running threads in the SIMT model. T he same phenomenon could occur at the SM level as well.

We propose an efficient sparse matrix multiplication method for social network analysis on GPU that improves the performanc e in orders-of-magnitude by good load balancing of computation s at both the thread level and the SM level. The proposed method pe r-forms matrix multiplication based on the outer product scheme, that is, column-row vector multiplication. In the outer pro duct, each element in a column vector is multiplied by all the eleme nts in a row vector. The outer product is much less affected by the distribution of non-zero elements of a matrix, and so, the am ount of workload tends to be evenly distributed over threads. The pro-posed method also rearranges the amount of workload at the SM level such that a heavier amount of workload is performed before a lighter amount of workload, where a heavier (lighter) one m eans a column-row vector multiplication having a more (less) num ber of pairs of non-zero elements matched. This rearrangement c ould reduce the total amount of computation time in a matrix multi pli-cation.

The contributions of this paper can be summarized as follows : The rest of this paper is organized as follows. Section 2 revi ews CUDA and related work for matrix multiplication using GPU. S ec-tion 3 points out the problems that the existing methods suff er from. Section 4 proposes our method to solve these problems. Secti on 5 demonstrates and analyzes the results of experimental eval uation. Finally, Section 6 summarizes and concludes the paper.
In this section, we present the characteristics of compute unified device architecture (CUDA) in accordance with the problem of ma-trix multiplication. CUDA is a technique based on the concep t of general purpose computing on graphic processing unit (GPGPU) that is supported by NVIDIA [14][15]. Figure 1 shows the arch i-tecture of CUDA. CUDA consists of a set of streaming multiproces-sors (SM), each of which is again composed of a set of streaming processors (SP). It follows the single instruction multiple threads (SIMT) model on which multiple threads execute the same inst ruc-tion at one time. An execution of a CUDA program is partitione d into a grid of thread blocks that run logically in parallel. A thread block consists of warps , each of which usually consists of a set of 32 threads. Threads in a warp can run concurrently on an SM [10]. A user can make the scheduler of GPU assign a set of thread bloc ks to a specific SM in a specific order. Just for simplicity in our e xpla-nation (without loss of generality), we assume that a thread block consists of a warp.

There are various kinds of memories on CUDA such as device memory, shared memory, texture memory, constant memory, an d register. Among them, we focus on the device memory and the shared memory, mainly used in a matrix multiplication. The de-vice memory is very large (capacity: 500MB  X  6GB), but relatively slow (access latency: 400  X  600 cycles), and is visible to all threads on the device [14]. The data processed in CUDA should be loade d into the device memory in advance. If the data is scattered ov er the device memory, a lot of accesses to the memory could occur in reading them. A lot of memory accesses lead to performance de gra-dation due to the high latency of the device memory. Thus, it i s preferred to store data consecutively and to access it in a co alesced fashion, which is called coalesced memory access [15]. Each SM is equipped with an on-chip memory, which is called the share d memory [10]. The shared memory is smaller (4KB  X  16KB), but faster (access latency: 20  X  40 cycles) than the device memory. It is only visible to the threads of the same thread block. If mul tiple threads try to access the shared memory at the same time, they need to be serialized to avoid conflicts [16].
There are different sparse formats representing a matrix. A mong them, we present four sparse formats mainly used in this pape r such as the coordinate list (COO) format, the compressed sparse c olumn (CSC) format, the compressed sparse row (CSR) format, and th e ELLPACK format [17][18].

The COO format consists of three arrays: an array storing non-zero element values (VAL), an array storing row indices (RI) , and an array storing col indices (CI). The CSC format consists of three arrays: an array storing non-zero element values (VAL), an a rray storing row indices (RI), an array storing column pointers, each of which refers to the first element of each column (CP) [7]. The CSR format consists of three arrays: an array storing non-zero element values (VAL), an array storing column indices (CI), and an ar ray storing row pointers, each of which refers to the first elemen t of each row (RP) [17]. We can compute the number of non-zero ele-ments in a row (or a column) by a difference between RP [ i + 1] and RP [ i ] (or CP [ i + 1] and CP [ i ] ) in the CSR (or CSC) for-mat. The ELLPACK format stores all elements of the matrix in an n -by-k two-dimensional array if each row of an n -by-n matrix has up to k non-zero elements, where row vectors with fewer than k non-zero elements are zero-padded. A column index and a valu e of each element are stored in index and data arrays, respective ly [18].
Figure 2 shows the COO, CSC, CSR, and ELLPACK formats. In the COO format, V AL normally stores element A (shortly, { A } ), { B } , and { C } in sequence in row major, from X [0][] to X [7][] . RI stores row indeces of elements while CI does column indeces of elements. In the CSC format, V AL stores { A } , { K } , and { D } in column major, from X [][0] to X [][7] . RI stores row indeces of elements. CP [1] stores 2 because CP [1] refers to { D } which is the 2 nd element of X in the order based on column major (when { A } is 0 th ). In the CSR format, V AL stores { A } , { B } , and { C } in row major, from X [0][] to X [7][] . CI stores column indeces of elements. RP [1] stores 3 because RP [1] refer to { D } is the 3 element in X . In the ELLPACK format, 8 -by-8 X is represented by 8 -by-3 X data and 8 -by-3 X index . The first row of X { A } , { B } , and { C } as elements of X [0][] . The first row of X stores 0 , 5 , and 7 as indices of elements in X [0][] .
In this section, we briefly review existing methods of matrix mul-tiplication on device memory in GPU.
We present a multiplication method of dense-dense matrices, called cuBlas , proposed by NVIDIA, which is based on the inner product of Eq. (1) [8].
 cuBlas divides each matrix to multiple sub-matrices. A thre ad block performs the multiplications between a sub-matrix X and a sub-matrix Y s of Y , and each thread performs the multipli-cation of X s [ i ][] with Y s [][ j ] of those sub-matrices. Since cuBlas uses the dense format where a zero element also occupies mem-ory space, cuBlas is not suitable for social networks where m ost of elements are zeros within the dense matrix.

Figure 3 shows an example of cuBlas. For multiplying X and Y , each of which is an 8 -by-8 matrix, each matrix is spilt into multiple 4 -by-4 sub-matrices of X 1  X  4 , Y 1  X  4 , and Z 1  X  4 , respectively. The thread block 1 performs multiplication of X 1 with Y 1 , where thread 1 multiplies X 1 [0][] and Y 1 [][0] and stores the result in Z
We present a multiplication method of sparse-dense matrices, called csrMM , proposed by NVIDIA [8]. It is based on the CSR vector method which is a method for a sparse matrix and a vecto r multiplication (spMV). For multiplication of sparse-dens e matri-ces, csrMM performs the CSR vector method iteratively for ea ch vector of the dense matrix [8]. csrMM represents X as the CSR (sparse) format and other matrices, Y and Z , as the matrix in dense format. In the CSR vector method, a thread block performs the Here, a thread performs the multiplication of one element of X [ i ][] with Y [ i ][ j ] [17].

Figure 4 shows the CSR vector method. Thread block 1 performs multiplications for the elements between RP [0] and RP [1]  X  1 . Thread 1 multiplies X [0][0] with Y [0][0] , then, stores the result in Z [0][0] .

We present a multiplication method of sparse-sparse matrices, called csrGEMM also proposed by NVIDIA [9]. In csrGEMM, all matrices are represented in the CSR format. csrGEMM is based on the row-row product of Eq. (2) .

A thread block performs the multiplication of X [ i ][] and rows of Y , and produces Z [ i ][] . Here, a thread multiplies X [ i ][ j ] and Y [ j ][ k ] , where k means a element in Y[j] [9]. Figure 5 shows an ex-ample of csrGEMM. Thread block 1 multiplies X [0][] with Y [0][] , Y [5][] and Y [7][] . Threads 1 and 2 multiply X [0][0] with Y [0][0] and Y [0][3] , then store the result in Z [0][0] and Z [0][3] , respec-tively. Threads 3, 4 and 5 multiply X [0][5] with Y [5][2] , Y [5][5] , and Y [5][6] , then store the result in Z [0][2] , Z [0][5] , and Z [0][6] , respectively. As well, threads 6, 7, 8, 9 and 10 multiply X [0][7] with non-zero elements of Y [7][1] , Y [7][2] , Y [7][3] , Y [7][4] , and Y [7][7] then store the result in Z [0][1] , Z [0][2] , Z [0][3] , Z [0][4] , and Z [0][7] , respectively.
We present the most efficient spMV method proposed by Yang. et. al. In this method, a matrix is divided into sub-matrices . Here, the CSR vector method is employed when the row vectors of a sub-matrix are long and have similar lengths; Otherwise, th e ELL method is employed [7]. As already mentioned in Section 2.3. 2, the spMV method can be easily extended to support matrix mult i-plication by performing it iteratively. Thus we modify this method for two sparse matrix multiplication, which we call this mod ified one, spMV-E in this paper. In order to utilize the spMV method for two sparse matrices multiplication, we employ the CSC forma t for column vectors of matrix Y because spMV-E accesses these vec-tors in column major. Then, it performs multiplications onl y for a pair of non-zero elements.

We only present the ELL method [17] here because the CSR vector method has already been introduced in Section 2.3.2. In the ELL method, a thread block multiplies a set of rows in X as many as its threads with a column. Here, a thread multiplies X [ i ][] with a column. Figure 6 shows an example of the ELL method. If thread block 1 consists of three threads, it performs the multiplic ations of X [0][] , X [1][] , and X [2][] with Y [][ j ] . Here, thread 1 performs the multiplication of X [0][] with Y [][0] , then stores the result in Z [0][0] [17].

In this section, we present three observations on the proble ms that the existing methods have.

Observation 1: The kernel function having a conditional bra nch for a matrix multiplication causes the poor performance due to the waiting time. All the threads in a warp execute the same in-struction concurrently due to the nature of the SIMT archite cture of GPU. Thus, if a conditional branch appears, some threads i n a warp execute necessary instructions while other threads in the same warp are just waiting (doing nothing).

For example, due to the inner product based on the CSR vector method in spMV-E, there can be no Y [ j ][ i ] multiplied with X [ i ][ j ] . For multiplication of X [ i ][ j ] with Y [ j ][ i ] , there are two steps of computation: confirm presence or absence of a pair of non-zer o elements ( Chk (  X  ) ); multiply them ( M ul (  X  ) ).

Algorithm 1 shows the pseudo code of the two sparse matrix multiplication based on the CSR vector method using only spa rse format. In the CUDA program, the thread block index and dimen -sion ( blockIdx.x and blockDim.x ) and thread index ( threadIdx.x ) are given. Here, num _ thread represents the number of threads in a thread block. In the CSR vector method, a thread in a thread block performs checking the indices of elements (Line 8: Chk (  X  ) ), and then multiplies two elements if row_index is the same as c ol-umn_index (Line 9: M ul (  X  ) ). Otherwise, a thread just waits for other threads to be completed in the same warp. That is, the thread divergence due to the conditional branch causes such waiting time over threads, which is a considerably time-consuming.

Observation 2: The skewed distribution of non-zero element s causes poor load balancing in a thread level. Figure 7 shows Algorithm 1 Two sparse matrix multiplication based on CSR vec-tor method Input: X (CSR format), Y (CSC format) Output: Z (COO format) 1: tid = blockDim.x * blockIdx.x + threadIdx.x; 2: warp_id = tid/ num_thread; 3: row = warp_id; 4: for i  X  the size of Y.CP [] do 5: if row &lt; the size of X.RP [] then 6: for col_index  X  X.RP [ row ] to X.RP [ row + 1] do 7: for row_index  X  Y.CP [ row ] to Y.CP [ row + 1] do 8: if row_index = col_index then 9: val= X.V AL [row_index]* Y.V AL [col_index]; 10: After parallel sum reduction then store values; 11: end if 12: end for 13: end for 14: end if 15: end for the power-law degree distribution found in a real-world soc ial net-work[12]. The x -axis represents the number of nodes that have cor-responding out-degrees, and the y -axis does count (i.e., the number of out-degree). This figure means that some nodes have a large number of edges while many ordinary nodes have a few edges. In other words, some rows (or columns) have a large number of non-zero elements while other rows (or columns) have only a f ew non-zero elements in a matrix. This leads to poor load balanc ing of computations in a thread level in the case of a method that a s-signs a thread to a row (or a column). The amount of workload fo r a matrix multiplication is considerably different with thr eads. The threads having a light amount of workload could wait for a lon g time until other threads having a heavy amount of workload ar e completed due to the nature of the SIMT architecture of GPU. Figure 7: Power-law degree distribution in a social network .
Observation 3: A naive scheduling method for thread blocks incur poor load balancing in a thread block level due to the skewed distribution of non-zero elements. In the case of a method that assigns a thread block to a row (or a column), the amount o f workload can be also significantly different over thread blo cks due to the skewed distribution of social network data. Since a na ive scheduling method would assign thread blocks to each SM in ro w major order, the amount of workload in SM could be different o ver SMs. This naive method might make the total execution time ve ry long in case the thread block having the heavy amount of work-load is assigned to an SM at the last moment. Therefore, it is v ery important to balance the amount of workloads among SMs evenl y. Figure 8(a) shows a case that an SM waits for another SM, where Block denotes a thread block; workload does the amount of work-load assigned to each block. Block 6 having the heaviest Work load is executed on SM1 at the end of the program. In this case, the completion of the program is delayed until block 6 finishes, w hile SM2 keeps waiting after its completion.
We propose the outer product based scheme for matrix multipli-cation on GPU. The the outer product based scheme for matrix m ul-tiplication is defined in Eq.(3) below [19]. All the schemes ( based on the inner, outer, and row-row product) for matrix multipl ica-tion require the exactly same amount of multiplications between elements (elements multiplication) . However, unlike the inner and row-row product based schemes, outer product based scheme a c-cesses only once a column and a row in two matrices X and Y , respectively. In other words, X [][ i ] only is multiplied only with Y [ i ][] for each i . In addition, every element of X [][ i ] has the same amount of elements multiplication because it should be mult iplied by all the elements of Y [ i ][] .
 In order to solve the problems found in Observations 1 and 2 in Section 3, we use the characteristics of the outer product ba sed scheme as follows.

First, the conditional branch over threads does not occur th anks to another characteristic of the outer product based scheme : X [ j ][ i ] is multiplied with all the elements in Y [ i ][] without performing Chk (  X  ) . If a thread performs the multiplications of X [ j ][ i ] with all the elements in Y [ i ][] , it performs only M ul (  X  ) . This solves the problem mentioned in Observation 2. Second, we can solve the poor load balancing in a thread level with the the outer product based scheme. In a thread block, all the threads have the amou nt of workload as much as that proportional to the number of elemen ts in Y [ i ][] . Therefore, each thread block should perform the multipli-cation of X [][ i ] with Y [ i ][] , where each thread multiplies X [ j ][ i ] with all the elements in Y [ i ][] , there by having the same workload. This solves the problem stated in Observation 2.

Here, thread blocks could face the confliction when they save the results of multiplications at the same time. It needs the atomic operation to avoid the confliction, leading to some performance degradation. However, we individually secure the result sp ace of multiplications for thread blocks then merge the result sub -matrices into a result matrix on main memory. Due to the size limitatio n of device memory, we just need to divide matrices to sub-matric es, each of which fits in the device memory. It incurs transferrin g sub-matrices between the main memory and the device memory. This transferring cost is, however, not a big issue, considering we can reduce the computation time greatly.
 Figure 9 shows the proposed method for Z = X [][ i ]  X  Y [ i ][] . Thread block 1 takes responsibility for a part of matrix mult iplica-tion for X [][0] and Y [0][] . As a result, thread 1 yields Z [0][0] and Z [0][3] by multiplying X [0][0] with Y [0][0] and Y [0][3] , respec-tively; thread 2 does Z [6][0] to Z [6][3] by multiplying X [6][0] with Y [6][0] and Y [6][3] , respectively.
As mentioned in Observation 3, the social network data repre -sented by a matrix follows the power-law degree distributio n. In the case of such matrices, the numbers of non-zero elements i n dif-ferent columns or rows could be significantly different [7]. This Figure 9: The outer product based scheme for matrix multipli ca-tion. causes the amount of workload over thread blocks to be differ ent greatly, which makes the amount of workload unevenly distri buted over SMs, if thread blocks are arbitrary assigned to SMs. If a thread block having the largest workload is assigned to an SM at the e nd of the process, all other SMs wait until the thread block is do ne as Figure 8(a).

In order to solve this problem, the amount of workload needs t o be evenly assigned over SMs independently of the number of th read blocks. For making the amount of workload balanced over SMs, we should first compute the number of elements multiplicatio ns occurring in multiplying every pair of a column and a row. Its time complexity is O (1) due to a pointer array in the CSC (or CSR) format. In a thread block, the amount of actual workload is sl ightly different from that of the elements multiplications becaus e multiple threads in a thread block are concurrently processed by diff erent SPs. The amount of workload in the thread block can be compute d as in Eq.(4) where # X [][ i ] (resp. # Y [ i ][] ) means the number of elements in a column in X (resp. a row in Y ), # threads does the number of threads and T B does a thread block.
 W orkload i = (# X [][ i ] / # threads in T B )  X  # Y [ i ][] (4)
Then, we need to assign the amount of workload to SMs as evenly as possible. Making the amount of workload distribut ed over SMs evenly is the task to find combinatorial optimizatio n of given thread blocks. It is similar to the well-known knapsack prob-lem to find combinatorial optimization of a set of items given [20 ]. In particular, this is a 0-1 knapsack problem since a thread b lock cannot be performed on two or more SMs. Since our load balanc-ing problem is a NP-hard problem, it is difficult to find the opt imal solution within a reasonable time. Thus, we propose a heuris tic method based on a greedy algorithm.

The proposed method sorts the pairs of column and row (as-signed to thread blocks) in descending order by the amount of workload, and then assigns the thread block having the heaviest amount of workload first to an SM available. In order to sort the thread blocks according to their workload efficiently, the p roposed method creates a data structure called workload table (WLT). WLT consists of two arrays WLT index and WLT workload having the same size. Each entry in WLT index stores the index of a pair of col-umn and row (to be assigned to a thread block) and each entry in WLT workload stores the amount of workload of the pair.

The time complexity of generating WLT index and WLT workload is O ( n ) because Eq.(4) O (1) complexity needs to be computed n times in n -by-n two matrices multiplications. The space complex-ity of WLT is O ( n ) as well.

Figure 8(b) and 8(d) show WLT generated from X and Y pre-sented in Figure 9. By sorting entries of WLT according to the amount of workload, we no longer need to sort the matrices of much larger sizes. We note that WLT has a much smaller num-ber of elements than n because there are many columns and rows, X and Y , that have no non-zero elements inside. In Figure 8(c), WLT has only six entries because there are no elements in the 2 and 4 th columns of X , likewise, in the 2 nd and 4 th rows of Y .
If thread blocks are assigned to SMs according to the sorted indexes in WLT, some SMs execute the thread blocks having the heavy amount of workload while other SMs execute a more num-ber of remaining (relatively lighter) thread blocks. As a re sult, the proposed method arranges the last thread blocks to complete their execution at the similar times, which alleviates poor load b alancing in a SM level. The thread blocks are assigned to SMs according to the sorted entries in WLT in Figure 8(d). While SM1 perform s the multiplication of block 0 having the heaviest amount of w ork-load, SM2 performs the multiplication of other blocks (bloc k 1  X  5) having relatively smaller amount of workload than block 0. T hese blocks perform computations of multiplication on SM2. By us ing this strategy for load balancing in the SM level, we could imp rove the degree of parallelism of GPU considerably.
In order to further improve the performance of a matrix multi pli-cation in GPU, we adopt two additional techniques: (T1) usin g the coalesced memory access and (T2) utilizing the shared memor y. We adopt T 1 by storing X and Y as the CSC and CSR formats, respectively, for accessing in X in column major and Y in row ma-jor, in the outer product based scheme. Since each thread in t he same thread block performs the multiplication of one elemen t of X [][ i ] with all the element of Y [ i ][] , all the threads share all the elements of Y [ i ][] . Thus, we adopt T 2 by loading Y [ i ][] , which is only accessed by X [][ i ] , to the shared memory .

Algorithm 2 shows the pseudo code of our method called lbGEMM (load balanced spGEMM). The input is a pair of X and Y in the CSC and CSR formats, respectively. The output is Z in the COO format. The matrices X , Y , and Z in these formats are loaded into the main memory, and then the sub-matrices of X  X  , Y  X  , and Z  X  are transferred to device memory. The size of sub-matrices need s to fit in that of device memory. The values of Y  X  [ i ][] assigned to a thread block are loaded into shared memory. Then, row_length is com-puted by the number of elements in Y  X  [ i ][] by referring to Y  X  .RP . Finally, the thread performs the multiplication of each ele ment of the column in X  X  with the row in Y  X  on shared memory, and stores the result.
 Algorithm 2 Fast sparse matrices multiplication on GPU Input: X  X  (CSC format), Y  X  (CSR format) Output: Z  X  (COO format) 1: tid =threadIdx.x; 2: tbid = blockDim.x  X  blockIdx.x + blockDim.y  X  blockIdx.y; 3: row_length = Y  X  .RP [ tbid + 1] -Y  X  .RP [ tbid ] ; 4: load values from Y  X  .V AL [tbid] to Y  X  .V AL [tbid] + 5: for i  X  row_length do 6: val= X  X  .V AL [ tid ]  X  Y  X  .V AL [ i ] ; 7: x = X  X  .CP [ tid ] ; 8: y = Y  X  .RP [ i ] ; 9: Z  X  .RI [] = x; Z  X  .CI [] = y; Z  X  .V AL [] = val; 10: end for
We evaluate the performance of the proposed method compared with previous methods for a matrix multiplication on GPU.
We conducted experiments using the server equipped with Int el i7 3.4GHz, 32GB main memory, and GTX TITAN Black [13] as GPU. This GPU has 2,880 SPs, each of which has the clock of 889MHz.

In our experiments, we used both synthetic and real-world so -cial network datasets, loadable on main memory. Table 1 show s the basic statistics of them [11] where #Mul. indicates the number of elements multiplications occurring in a matrix multipli cation. We observed that, in real-world Stanford social network dat asets, as the number of nodes increases, the density tends to decrea se. Thus, we divided the real-world social network datasets int o two groups, Group S and Group L , according to the number of nodes included. Group S has relatively small networks consisting of less than 100,000 nodes and having relatively high density of mor e than 0.01% while Group L does relatively large networks consisting of more than 100,000 nodes and having the density of less than 0.001%. Since our social networks data consist of different num-bers of nodes, we cannot multiply a pair of matrices for diffe r-ent social networks. So, in our experiments, we performed se lf-multiplications where X and Y are identical (i.e., Z = X  X  Y ). S EmailEnron 36k 367k 2.73e-02 0.05B Slashdot 82k 948k 1.40e-02 0.1B L For a matrix multiplication in GPU, the total size of matrice s X , Y , and Z should be smaller than that of device memory in GPU. However, it often exceeds the size of the device memory in the case of a huge volume of real-world social networks. Thus, we load entire matrices X and Y first to main memory in host and divide them to smaller sub-matrices X  X  and Y  X  in such a way that each pair of sub-matrices can be multiplied on the device memory.
We discuss how to divide a matrix into smaller sub-matrices f or each method in more detail. In the case of dense matrices, we d e-termine the sizes of sub-matrices in the number of nodes (say n ) loadable in the device memory, and then partition the whole m atri-ces into their n -by-n sub-matrices. A matrix in sparse format stores only non-zero elements (indicating edges in a social networ k) in row major (or column major) order. In the case of sparse matri ces, we determine the sizes of sub-matrices in the number of non-z ero elements (say e ) loadable in the device memory, and then partition the whole matrices into smaller sub-matrices having the siz es thus determined.
 and Y  X  to get Z  X  in GPU with each method. In the case of using cuBlas, we first determine the number of nodes (say n ) for X  X  , Y  X  , and Z  X  loadable in the device memory. For multiplication of each pair of sub-matrices in GPU, we call cuBlas implemented in th e NVIDIA Library [8]. of cuBlas. The size of space is dominantly determined by sub-matrices Y  X  and Z  X  since they are much larger than sub-matrix X  X  . Thus we first determine the sizes of Y  X  and Z  X  loadable in device memory, and partition X  X  according to the sizes of Y  X  or Z  X  . For multiplication of each pair of sub-matrices in GPU, we call c srMM implemented in the NVIDIA Library [8].

In multiplications of two sparse matrices (i.e., with csrGE MM, spMV-E, and lbGEMM), we first determine e that makes X  X  , Y  X  , and Z  X  loadable in device memory, partition the three matrices X , Y , and Z into smaller sub-matrices X  X  , Y  X  , and Z  X  according to e , and call each method for multiplying each pair of sub-matric es in GPU.

In csrGEMM of the NVIDIA Library [9], a thread block is in charge of a row of X  X  together with several rows of Y  X  . If non-zero elements within a row of X  X  exceeds the size of shared memory, its corresponding thread block could not run. To address thi s prob-lem, we divide a row having a large number of non-zero element s in X  X  into smaller sub-rows, and make a thread block process the sub-rows serially . This makes the thread block executed multiple times while other concurrently running thread blocks are ex ecuted only once, which leads to poor thread-block-level load balancing in csrGEMM. Since, in real-world social networks, the numbe r of non-zero elements in a row (i.e., number of edges to a node) co uld be significantly large owing to the power-law , this poor load bal-ancing is likely to occur frequently in csrGEMM.
Table 2 shows the execution times for matrix multiplication with seven datasets obtained by different methods. The total exe cution time consists of (1) transmission time ( trans ) for transferring data between main memory in host and device memory in GPU and (2) computation time ( com ) spent for matrix multiplication in GPU. Table 3 indicates the portion of transmission and computati on times to the total execution time.

In the case of dense matrices, the number of nodes is a dominan t factor to determine the execution time because it determine s the size of dense matrices. Thus, a social network with a large nu mber of nodes increases both of the transmission and computation times.
In the case of sparse matrices, the number of non-zero elemen ts (i.e., edges) and also the number of elements multiplicatio ns are two dominant factors to determine the execution time. The tr ans-mission time from main memory to device memory is determined by the number of non-zero elements in X and Y while the trans-mission time from device memory to main memory is determined by the number of non-zero elements in Z . We note that the number of non-zero elements in Z is dependent on the number of multipli-cations. These two factors affect the computation time diff erently depending on multiplication methods.

In cuBlas, the execution time increases as the number of node s increases due to all dense matrices X , Y , and Z . The computation time occupies about 77% of the total execution time in all cas es be-cause cuBlas performs a large number of elements multiplica tions even for zero-elements.

As in cuBlas, the total execution time of csrMM tends to incre ase as the number of nodes grows. The transmission time is affect ed by the number of non-zero elements within X in sparse format and the number of nodes in two dense matrices Y and Z . The computation time is dependent on the number of non-zero elements in X to be multiplied to all the elements in Y . Compared with cuBlas, the computation time occupies a small portion (less than 20%) of the total execution time in csrMM since only the non-zero elemen ts in X are involved in elements multiplications with csrMM. The tr ans-mission time occupies more than 80% of the execution time. Si nce dense matrices Y and Z are much larger than X in sparse format, the number of nodes in a social network dominates the executi on time in csrMM.

The total execution time of spMV-E dramatically increases w ith the number of non-zero elements (i.e., edges in a social netw ork). The transmission time depends on the number of non-zero elem ents within all sparse matrices X , Y and Z . The computation time is also dependent on the number of non-zero elements, rather th an the number of multiplications, in spMV-E. This is because of the thread divergence , which is more likely to occur as the number of non-zero elements increases. The thread divergence leads t o poor load balancing in GPU due to Chk (  X  ) mentioned in Observation 2, thereby requiring much more computation time.

The total execution time of csrGEMM dramatically grows as th e number of elements multiplications increases. this is main ly due to poor load balancing in the thread block level. There is no sig nifi-cant difference between the portions of the transmission an d com-putation times in our datasets, except for the LiveJournal d ataset. The resulting matrix Z normally determines the transmission time because it is much larger than both of X and Y . The computation time is dependent on the number of elements multiplications . There is an exceptional case that the portion of the transmission t ime is larger than that of the computation time in the LiveJournal d ataset, which is the largest in our datasets. the LiveJournal datase t con-tains a large number of non-zero elements (i.e., edges), how ever, does not incur a large number of actual elements multiplicat ions. This causes a large time for data transmission from main memo ry to device memory but a relatively small time for computation s in GPU.

The total execution time of lbGEMM, our method, increases with the number of elements multiplications. We note that th e exe-cution time of lbGEMM does not dramatically increases thank s to good load balancing both in the thread level and the SM level ( but not thread block level) unlike other methods for two sparse m atrices (spMV-E and csrGEMM). There are some differences in ratios o f transmission and computation times depending on social net work data. We conjecture that the differences are due to differen t num-bers of threads concurrently executed in GPU. lbGEMM alloca tes a thread block for elements multiplications between a colum n in for multiplications between an element in the column of X  X  and all the elements in the row of Y  X  . Because different numbers of non-zero elements exist inside columns, the numbers of threads u sed in thread blocks could be different. As a thread block employ s a more number of threads, the computational parallelism in G PU gets higher, leading to shorter computation time. We note th at all the threads in a thread block have an identical number of elem ents multiplications, which guarantees load balancing in a thre ad-level. For example, the number of multiplications for Youtube is tw ice as large as that for Slashdot while the execution time for Youtu be is not twice as large as that for Slashdot. We observed that the n umber of threads in a thread block with Youtube was 32 on average whi le it was 64 with Slashdot.

Let us compare the execution times of all the methods. csrMM outperforms cuBlas five times by reducing the number of unnec -essary multiplications involving zero-elements. Althoug h spMV-E is for two matrices both in sparse format, it shows poor perfo r-mance (even worse than csrMM) due to poor load balancing in the thread level (i.e., thread divergence) within GPU. csrG EMM and lbGEMM, which only adopt sparse format, outperform cuBl as, csrMM, and spMV-E significantly. In particular, our lbGEMM achieves a dramatic improvement in performance by 3 to 145 ti mes, compared with csrGEMM, thanks to good load balancing in both the thread and SM levels. A performance gap in execution time between lbGEMM and csrGEMM increases with the number of el-ements multiplications.
Now, we show the scalability of lbGEMM with synthetic datase ts compared with existing ones. Following the characteristic s of real-world datasets used, we generated the three types of synthet ic datasets ( S1, L1, and L2 ): S 1 for relatively small networks and L 1 &amp; L 2 for relatively large networks. S1 has 20,000 nodes which are all load-able on device memory for running cuBlas. It has various dens ities of 0.1%  X  2%, and is similar to those in Group S in real-world network datasets. Here, we changed the density up to 2% (seam s unrealistic) in order to do in-depth investigation on some c ases not observed in real-world datasets; L1 has 1,000,000 nodes with var-ious densities of 0.001%  X  0.0001%. It imitates Group L of real-world datasets; L2 consists of 1,000,000  X  5,000,000 nodes of a fixed density of 0.0001%, which also imitates Group L of real-world datasets. We generated all the synthetic datasets in s uch a way that it follows the power-law distribution.
Figure 10(a) shows the execution time obtained by changing t he density on S 1 . The x -axis indicates the density of matrices and the y -axis does the execution time in log scale . We note that the execution time of cuBlas is identical regardless of density since it is the method of dense-dense matrix multiplication and th us is not affected by the density. We observe that the performance of lbGEMM becomes worse than that of cuBlas in the density of 2%. We note that this high density of a social network is unrealis tic in practice. However, we show this result just for indicating t he cross-over point; csrMM performs worse than cuBlas from the densit y of 1%. In particular, spMV-E is the worst one except for the dens ity of 0.1%. spMV-E has the overhead of checking whether an ele-ment of X [ i ][] matches with some element of Y [][ i ] (i.e., Chk (  X  ) ). The overhead becomes larger as the number of non-zero elemen ts increases. However, we note that real-world social network s do not have a chance to have such a high density as seen in Table 1.
Figure 10(b) shows the execution time obtained by changing the density on L 1 . The x -axis indicates the density of matrices and the y -axis does the execution time in log scale . Similar to the result on S 1 , the execution time of all the methods except for cuBlas increases with the increase of density. lbGEMM is sho wn to outperform all other methods significantly. The performa nce of csrGEMM becomes worse due to the poor load balancing over threads and/or thread blocks. We observe the execution time of csrMM increases slightly with changing the density. spMV-E is the worst one in this experiment. It spends more execution time t han other methods. So, we only show a part of the result of spMV-E.
Figure 10(c) shows the execution time with different number s of nodes on L 2 . The x -axis represents the number of nodes and the y -axis does the execution time in log scale . The execution time of all the methods increases as the number of nodes gets larger. We see that lbGEMM consistently outperforms all other methods . We know that the performance of spMV-E is greatly affected by th e number of nodes as well as the density.

Summary : Through the experiments with synthetic datasets, we observe lbGEMM, significantly outperforms other methods (b y up to 1,400 times compared with csrGEMM, the second best one) in situations where the datasets have low density and consist o f a large number of nodes, indicating with the characteristics of rea l-world social networks. Therefore, we could expect lbGEMM outperf orms others in real-world datasets. We also observe that the perf ormance of csrGEMM becomes worse in datasets following the power-la w due to its poor load balancing; spMV-E is significantly influe nced by both the density and the number of nodes; csrMM tends to be more influenced by the number of nodes than the density; cuBla s is only influenced by the number of nodes.
SimRank [2] is a well-known algorithm for link-based similar-ity computations. The SimRank computation consists of mult ipli-cations of three matrices and one addition of two matrices as shown in Eq.(5). The multiplications of three matrices can be subd ivided into two separate steps (each of which involves a multiplica tion of two matrices) as in Eq.(6), where S k denotes the similarity matrix obtained for the k th iteration, T does the matrix for temporary stor-age, W does the adjacency matrix, I does the identity matrix, and c represents a damping factor. The result of the SimRank compu ta-tion can be obtained by performing Eq.(6) iteratively until it faces an equilibrium state.

As a case study, we evaluate the performance of lbGEMM in comparison with csrGEMM (best among the existing ones) by ap -plying them into the SimRank computation on the DBLP dataset . We tried to apply spMV-E, csrMM, and cuBlas as well, but faile d to obtain their results because they spent a huge amount of ti me in computations. We observed that lbGEMM outperforms csrGEMM by about 970 times (lbGEMM: 8.8 seconds and csrGEMM: 8,626.2 seconds) in this experiment. This result indicates that lbG EMM is quite beneficial in real-world applications involved wit h matrix multiplication.
This paper proposed an efficient method for sparse matrix mul ti-plications on GPU that is mainly targeted for analyzing larg e social networks.

We observed that real-world social networks usually have th e characteristics of a very low density (e.g., 0.001%) follow ing the power-law degree distribution. For such social networks, t he exist-ing methods based on the inner product or row-row product cou ld degrade the performance due to poor load balancing in either the thread or SM levels.

Our lbGEMM performs sparse matrix multiplication based on the outer product scheme. It distributes the amount of workl oad evenly among threads. In order to fully utilize the ideal com puting power of GPU, we need to balance the amount of workload over SMs. Towards this goal, we proposed a strategy based on a gree dy algorithm by using the WLT data structure. In lbGEMM, while some SMs execute thread blocks having a large amount of work-load, other SMs execute the rest of number of (relatively sma ll) thread blocks. Thus, lbGEMM could make the last thread block s completed at similar times, and so, could achieve good load b al-ancing of computations in the SM level.

Finally, through extensive experiments using both synthet ic and real-world datasets, we showed that lbGEMM dramatically ou tper-forms existing methods based on GPU by up to three orders-of-magnitude. In addition, we evaluated the performance of lbG EMM in a real-world application, SimRank, as a case study and sho wed lbGEMM performs 970 times better than csrGEMM (the best one in existing ones). We believe that our method could be used as a very useful building block for developing a variety of algor ithms to analyze a large real-world social networks.
