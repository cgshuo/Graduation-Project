 property and has inspired a multitude of other models such as Dependancy Graph (DG)[ 8 ], All-K-Order-Markov (AKOM)[ 10 ], Transition Directed Acyclic Graph (TDAG)[ 7 ], Probabilistic Suffix Tree (PST)[ 1 ] and Context Tree Weight-ing (CTW)[ 1 ]. Although, much work has been done to reduce the temporal and spatial complexity of these models (e.g. [ 1 , 3 ]), few work attempted to increase their accuracy. Besides, several compression algorithms have been adapted for sequence predictions such as LZ78 [ 12 ] and Active Lezi [ 4 ]. Moreover, machine learning algorithms such as neural networks and sequential rule mining have been applied to perform sequence prediction [ 6 , 11 ].
 of them assume the Markovian hypothesis that each event solely depends on the previous events. If this hypothesis does not hold, prediction accuracy using these models can severely decrease [ 3 , 5 ]. Second, all these models are built using only part of the information contained in training sequences. Thus, these models do not use all the information contained in training sequences to perform predic-tions, and this can severely reduce their accuracy. For instance, Markov models typically considers only the last k items of training sequences to perform a pre-diction, where k is the order of the model. One may think that a solution to this problem is to increase the order of Markov models. However, increasing the order of Markov models often induces a very high state complexity, thus making them impractical for many real-life applications [ 3 ].
 sequences without information loss by exploiting similarities between subsequences. It has been reported as more accurate than state-of-the-art models PPM, DG, AKOM on various real datasets. However, a drawback of CPT is that it has an impor-tant spatial complexity and a higher prediction time than these models. Therefore, an important research problem is to propose strategies to reduce the size and pre-diction time of CPT. Reducing the spatial complexity is a very challenging task. An effective compression strategy should provide a huge spatial gain while provid-ing a minimum overhead in terms of training time and prediction time. Further-more, it should also preserve CPT X  X  lossless property to avoid a decrease in accuracy. Reducing prediction time complexity is also very challenging. An effective strategy to reduce prediction time should access as little information as possible for mak-ing predictions to increase speed but at the same time it should carefully select this information to avoid decreasing accuracy.
 named FSC (Frequent Subsequence Compression), SBC (Simple Branches Com-pression) and PNR (Prediction with improved Noise Reduction). The two first strategies are compression strategies that reduce CPT size by up to two orders of magnitude while not affecting accuracy. The third strategy reduces the pre-diction time by up to 4.5 times and increases accuracy by up to 5%. This paper is organized as follows. Section 2 introduces CPT. Sections 3 and 4 respectively describes the two compression strategies (FSC and SBC) and the prediction time reduction strategy (FNR). Section 5 presents an experimental evaluation of each strategy on seven real datasets against five state-of-the-art prediction models. Finally, Section 6 draws conclusion.
 Tree is a type of prefix tree (aka trie ). It contains all training sequences. Each tree node represents an item and each training sequence is represented by a path starting from the tree root and ending by an inner node or a leaf. Just like a prefix tree, the prediction tree is a compact representation of the training sequences. Sequences sharing a common prefix share a common path in the tree. The Lookup Table is an associative array which allows to locate any training sequences in the prediction tree with a constant access time. Finally the Inverted Index is a set of bit vectors that indicates for each item i from the alphabet Z , the set of sequences containing i .
 For a sequence s = i 1 ,i 2 , ...i n of n elements, the suffix of s of size y with 1  X  y  X  n is defined as P y ( s )= i n  X  y is performed by identifying the sequences similar to P y ( s ), that is the sequences containing all items in P y ( s ) in any order. The suffix length is a parameter similar to the model X  X  order of All-k-order Markov and DG. Identifying the optimal value is done empirically by starting with a length of 1. CPT uses the consequent of each sequence similar to s to perform the prediction. Let u = j 1 ,j 2 , ...j m be a sequence similar to s .The consequent of u w.r.t to s is the longest subsequence j v ,j v +1 , ...j m of u such that found in the consequent of a similar sequence of s is stored in a data structure called Count Table (CT). The count table stores the support (frequency) of each of these items, which is an estimation of P ( e | P y ( s )). CPT returns the most supported item(s) in the CT as its prediction(s).
 is dynamically loosened to ignore noise. Identifying similar sequences, and more particularly the noise avoidance strategy of CPT, is very time consuming and account for most of CPT X  X  prediction time [ 5 ]. For a given sequence, if CPT cannot find enough similar sequences to generate a prediction, it will implicitly assume the sequence contains some noise. The prediction process is then repeated but with one or more items omitted from the given sequence. CPT X  X  definition of noise is implicit and has for sole purpose to ensure that a prediction can be made every time. CPT has been presented as one of the most accurate sequence prediction model [ 5 ] but its high spatial complexity makes CPT unsuitable for applications where the number of sequences is very large. CPT X  X  size is smaller than All-k-Order Markov and TDAG but a few orders of magnitude larger than popular models such as DG and PPM. CPT X  X  prediction tree is the largest data structure and account for most of its spatial complexity. In this section, we focus on strategies to reduce the prediction tree X  X  size.
 Strategy 1 Frequent Subsequence Compression (FSC). In a set of train-ing sequences, frequently occurring subsequences of items can be found. For some time. The additional cost is the on-the-fly decompression of the prediction tree, which is fast and non intrusive because of the DCF structure.
 Strategy 2: Simple Branches Compression (SBC). Simple Branches Com-pression is an intuitive compression strategy that reduces the size of the predic-tion tree. A simple branch is a branch leading to a single leaf. Thus, each node of a simple branch has between 0 and 1 children. The SBC strategy consists of replacing each simple branch by a single node representing the whole branch. For instance, part (2) of Fig. 2 illustrates the prediction tree obtained by apply-ing the DCF and SBC strategies for the running example. The SBC strategy has respectively replaced the simple branches D, C , B, C and E, x, A by single nodes DC , BC and ExA . Identifying and replacing simple branches is done by traversing the prediction tree from the leafs using the inverted index. Only the nodes with a single child are visited. Since the Inverted Index and Lookup Table are not affected by this strategy, the only change that needs to be done to the prediction process is to dynamically uncompress nodes representing simple branches when needed. Strategy 3: Prediction with Improved Noise Reduction (PNR) .Aspre-viously explained, to predict the next item i n +1 of a sequence s = i 1 ,i 2 , ..., i n , CPT uses the suffix of size y of s denoted as P y ( s ) (the last y items of s ), where y is a parameter that need to be set for each dataset. CPT predicts the next item of s by traversing the sequences that are similar to its suffix P y ( s ). Searching for similar sequences is very fast ( O ( y )). However, the noise reduction mecha-nism used for prediction (described in Section 2) is not. The reason is that it considers not only P y ( s ) to perform a prediction, but also all subsequences of P ( s )havingasize t&gt;k , where k is a parameter. The more y and k are large, the more subsequences need to be considered, and thus the more the prediction time increases.
 if their sole presence negatively impact a prediction X  X  outcome. The PNR strategy is based on the hypothesis that noise in training sequences consists of items having a low frequency, where an item X  X  frequency is defined as the number of training sequences containing the item. For this reason, PNR removes only items having a low frequency during the prediction process. Because the definition of noise used in CPT+ is more restrictive than that of CPT, a smaller number of subsequences are considered. This reduction has a positive and measurable impact on the execution time, as it will be demonstrated in the experimental evaluation (see Section 5). sequence to be predicted s , CPT X  X  structures and the noise ratio TB and a min-imum number of updates, MBR , to be performed on the count table (CT) to perform a prediction. The noise ratio TB is defined as the percentage of items in a sequence that should be considered as noise. For example, a noise ratio of 0 values. PPM and LZ78 do not have parameters. DG and AKOM have respectively a window of four and an order of five. To avoid consuming an excessive amount of memory, TDAG has a maximum depth of 7. CPT has two parameters, splitLength and maxLevel and CPT+ has six parameters; three for the FSC strategy, two for the PNR strategy and splitLength from CPT. The values of these parameters have also been empirically found and are provided in the project source code. Experiment specific parameters are the minimum and maximum length of sequences used, the number of items to be considered to perform a prediction (the suffix length) and the number of items used to verify the prediction (called the consequent length). Let be a sequence s = i 1 ,i 2 , ...i n having a suffix S ( s ) and a consequent C ( s ). Each model takes the suffix as input and outputs a predicted item p . A prediction is deemed successful if p is the first item of C ( s ). Datasets having various charac-teristics have been used (see Table 1 ) such as short/long sequences, sparse/dense sequences, small/large alphabets and various types of data. The BMS, Kosarak, MSNB and FIFA datasets consist of sequences of webpages visited by users on a website. In this scenario, prediction models are applied to predict the next web-page that a user will visit. The SIGN dataset is a set of sentences in sign language transcribed from videos. Bible Word and Bible Char are two datasets originating from the Bible. The former is the set of sentences divided into words. The latter is the set of sentences divided into characters. In both datasets, a sentence represents a sequence.
 prediction is accurate, a failure if the prediction is inaccurate or a no match if the prediction model is unable to perform a prediction. Four performance measures are used in experiments: Coverage is the ratio of sequences without prediction against the total number of test sequences. Accuracy is the number of successful predictions against the total number of test sequences. Training time is the time taken to build a model using the training sequences Testing time is the time taken to make a prediction for all test sequences using a model. Experiment 1: Optimizations Comparison. We have first evaluated strate-gies that aims to reduce the space complexity of CPT (cf. Section 3) by mea-suring the compression rate and the amount of time spent for training. Other measures such as prediction time, coverage and accuracy are not influenced by compression gain, this makes SBS the most profitable strategy. While SFC has a higher impact on the training time, SFC enhances the compression rate for each dataset.
 and accuracy obtained by applying the PNR strategy. Fig. 4 (left) compares the prediction time of CPT+ (with PNR) with that of CPT. The execution time is reduced for most datasets and is up to 4.5 times smaller for SIGN and MSNBC. For Bible Word and FIFA, prediction times have increased but a higher accuracy is obtained, as shown in Fig. 4 (right). The gain in prediction time for CPT+ is dependant on both PNR and CPT parameters. This gain is thus dataset specific and non linear because of the difference in complexity of CPT and CPT+. The influence of PNR on accuracy is positive for all datasets except MSNBC. For Bible Word, the improvement is as high as 5.47%. PNR is thus a very effective strategy to reduce the prediction time while providing an increase in prediction accuracy. Experiment 2: Scalability. In this experiment we compared the spatial com-plexity of CPT+ (with both compression strategies) against CPT, All-K-order Markov, DG, Lz78, PPM and TDAG. Only the FIFA and Kosarak datasets were used in this experiment because of their large number of sequences. In Fig. 5 , the spatial size of each model is evaluated against a quadratically growing set of training sequences -up to 128,000 sequences. Both PPM and DG have a sub linear growth which makes them suitable for large datasets. CPT+ X  X  growth is only an order of magnitude larger than PPM and DG and a few orders less than CPT, TDAG and LZ78. The compression rate of CPT+ tends to slightly dimin-ish as the number of sequences grows. This is due to more branches overlapping in the prediction tree, a phenomenon that can generally be observed in tries. training sequences is smaller than the number of items, CPT+ has a smaller footprint than the other prediction models. For the FIFA dataset, when 1000 sequences are used, CPT+ X  X  node count is 901 compared to the 1847 unique items in the alphabet. Results are similar for Kosarak. Models such as PPM and DG can X  X  achieve such a small footprint in these use cases because they have a least one node per unique item.

