 Usually, we can use a classification or clustering machine learning algorithm to manage knowledge and information retrieval. If we have a small size of known information with a large scale of un-known data, a semi-supervised learning (SSL) algorithm is often preferred. Under the cluster or manifold assumption, usually, the larger amount of unlabeled data are used for learning, the bigger gains of the SSL approaches are achieved. In the paper, we adopt the graph-based SSL algorithm to solve the problem. However, the graph-based SSL algorithms are unable to be learnt with large-scale unlabeled samples and originally can only work in a trans-ductive setting. In the paper, we propose a scalable graph-based SSL algorithm to attack the problems aforementioned by Gaussian mixture model label propagation. Experiments conducted on the real dataset illustrate the effectiveness of the proposed algorithm. I.5.m [ Computing Methodologies ]: PATTERN RECOGNITION X  Miscellaneous Algorithms Semi-supervised learning, Gaussian mixture models, label propa-gation
In real application, such as text categorization, the labeling doc-uments are expensive to obtain, whilst unlabeled ones are often abundant and easy to collect. Semi-supervised learning (SSL) al-gorithms which can use both types of data were proposed to signif-icantly improve the classification generalization performance com-pared to the supervised algorithms which use labeled data alone [1]. The gains of the SSL approaches are achieved by the amount of un-labeled data used in training, which should be relatively large [2]. Therefore, it is necessary to make semi-supervised algorithms scal-able for real applications.

If samples are in a low-dimensional manifold, a similarity graph is usually constructed to approximate it. This results in the graph-based SSL approaches. Here, data are represented by a graph, each vertex corresponding to an example. However, the graph-based SSL algorithms usually do not scale well to a very large amount of (label and unlabeled) data [1, 3].

Recently, a scalable semi-supervised learning has been studied in mainly two different directions: using a smaller weight matrix by constructing a smaller graph or the sparser solution of a sparse graph, and avoiding the matrix inversion. The former includes harmonic mixture [4] to construct a  X  X ackbone" graph, manifold regularization limited to linear models [5], a bipartite graph con-structed by mixture models estimated by labeled and unlabeled ex-amples [6], sparse grid for the construction of a sparse graph [7], transductive Gaussian process [8], sparse solution with  X  -insensitive manifold regularization [9], and an approximation method with subset selection to construct a graph [10]. The latter includes active learning and Gaussian transform for fast computation [11].
In the paper, we focus on using a large amount of unlabeled sam-ples for semi-supervised learning and propose scalable graph-based SSL algorithms, the Mixture Model Label Propagation (MiMoLaP) for short since a graph-based SSL approach propagates the labels to the neighborhood until the global smoothness is achieved on the whole dataset. In particular, both the labeled and unlabeled data are represented by Gaussian mixture models (GMM). Then, a graph is built, with the vertices being the GMM components, and the affin-ity matrix being computed by the probability product kernel [12]. With the graph, we can adopt both of the regularization frameworks recently proposed [13, 14] and the solutions can be obtained in a closed form. Also, an out-of-bag sample can be predicted by combining mixture models weighted by the a posteriori probabil-ity belonging to each component, which is also incapable for most of label propagation SSL approaches. Experiments on the real-world dataset validate the effectiveness of the proposed approach, in which we significantly reduce both computational and spatial complexities while obtaining the comparable accuracies and better robust results.

The outline of the paper is as follows. In Section 2 we describe the proposed scalable MiMoLaP approaches, with details on the construction of graph, the learning framework and the prediction of out-of-bag examples. Then, we carry out experiments to com-pare our results with the state-of-the-art SSL algorithms and the supervised baselines in Section 3. Finally, the conclusion and dis-cussions are given in Section 4.
Let X l = [ x 1 , . . . , x l ]  X  R D  X  l be the data matrix of l la-beled samples, where each sample x i is in a D -dimensional feature space. Let Y = { Y ic } l,C i,c =1 be the class label matrix for X Y ic = 1 if x i is labeled as the class c and 0 otherwise. Finally let X u = [ x l +1 , . . . , x l + u ]  X  R D  X  u be the unlabeled data matrix, consisting of u unlabeled patterns. The total number of training example is l + u in the SSL setting.

In the proposed approaches, local information is first estimated with the Gaussian mixture models (GMM), and the label is grad-ually propagated in components , i.e., through a cluster component to its neighboring ones. This propagation is in the same way as in the plain graph-based SSL algorithm, where propagation happens in terms of points, but not components. This is why the proposed methods are more scalable than the plain propagation. In the fol-lowing we first introduce the basis of a common graph-based SSL, and then present the proposed methods in detail. In this paper, we only consider an undirected graph. The graph G is described by vertices V which consist of labeled and unlabeled samples, and edges E which denote the similarity or dissimilarity among samples in the dataset X . The edges are usually weighted by a weight matrix W = { w ij } . For instance, if we use the Gaussian (or RBF) kernel, and if the distance d ij between vertices x is defined by Euclidean distance d ij = k x i  X  x j k 2 , then we could have the weight value w ij = exp(  X  d ij 2  X  2 ) . The weighted graph can be fully or sparsely connected.

Let D be a diagonal matrix for the graph G , with its ( i, i ) -th di-agonal element being the sum of the i -th row of W , e.g., the degree of a vertex x i is d ii = D  X  W . Usually the normalized Laplacian  X  L = D  X  1 / 2 is desired as a closed-form solution can be obtained.

To obtain the solution, the inverse of an ( l + u )  X  ( l + u ) ma-trix is necessary in most of graph-based algorithms. However, it is difficult to deal with a large amount of unlabeled samples. In the following, the scalable label propagation algorithms are introduced to attack this problem.
The idea of the proposed approaches is to map each data point into a probability space via Gaussian mixture models, and then build an adjacency graph for the mixture components. In this way label propagation happens at the component level, and the compu-tational cost is significantly reduced. After the mapping is done, the inner product between the mapped data points can be calculated us-ing the probability product kernel . We will also see in Section 2.3 that the proposed graph-based algorithms can do an inductive learn-ing.
If data points are represented by a GMM, each datum can take the form: where  X  k = {  X  k ,  X  k ,  X  k } denote the model parameter with the prior  X  k , the mean vector  X  k and the covariance matrix  X  k -th component and models can be inferred from each datum. Ac-cordingly, the data are represented by K Gaussians. In this case, the vertices of a graph are probability distributions instead of data points. Consequently, the edges between vertices are defined be-tween distributions over the probabilistic space.

If N i and N j are the components of the GMM models, the simi-larity between N i and N j is the integral of the product of two mix-ture components over the input space. This is called Probability Product Kernel and is positive definite [12]. Thus, we can obtain the associated edge weight of the graph and it can be analytically calculated as: positive constant. Different values for  X  lead to different metrics.
After obtaining the affinity matrix W for the component graph, we can simply apply one of the recently proposed graph-based SSL algorithms for learning, such as [13, 14]. We refer to the adaption of our method to these two SSL algorithms as MiMoLaP LG (local and global consistency) and MiMoLaP H (harmonic property), re-spectively.
 MiMoLaP LG . In our case, the labeling information for  X  X abeled" cluster components is not deterministic. We assume the cluster-to-label probabilities to be soft, and a good classification function should not change its values from the initial labeling information on the whole dataset (the global property) while gradually chang-ing the soft labeling in neighborhood cluster components (the local property). Hence, we define the regularization framework as fol-lows: where F  X  R K  X  C are the interesting function values for all the K mixture components, and F k  X  R C are the function values on all the C classes for the k -th mixture component. Here the first term is a squared-error loss function, and the second term is a normalized Laplacian regularizer. Differentiating (2) with respect to F , the closed-form solution can be obtained as where  X  = 1 / (1+  X  ) , 0 &lt;  X  &lt; 1 is a model parameter determined by model selection, and the transition matrix S = D  X  1 / 2 is symmetric with each entry s ij = w ij  X  MiMoLaP H . Unlike MiMoLaP LG , we can fix the soft labeled infor-mation of training labeled components for semi-supervised learn-ing using the harmonic property proposed by Zhu et al. [14]. Here, the Laplacian matrix  X  L can be factorized to four submatrices which are formed by labeled components (  X  L mm ), labeled and unlabeled components (  X  L mn ), unlabeled and labeled components (  X  L mn ) and unlabeled components ( obtain a closed-form solution for unlabeled training components by where  X  Y m refers to the class label mixture for the components con-taining labeled images.
The proposed algorithms can work in a semi-supervised induc-tive way. An out-of-bag image can be predicted by combining mix-ture models weighted by the a posteriori probability belonging to each component.

After the SSL learning, we have all the parameters  X  k = {  X  } , and the component-to-label probabilities p ( y | c k ) which can be de-rived from F  X  . Based on the Bayes X  rule, the probability of a new test image x belonging to the component k is Then, the probability of x taking label j is and the final label for x is arg max j p ( y j | x ) .
In order to compare the classification performance of the pro-posed algorithms, we carry out experiments using a large-scale handwritten image dataset using the proposed MiMoLaPs, and the multi-switch semi-supervised linear SVM for large scale problems, short as SVMlin [5] 1 . Based on different regularization frame-works, we consider the proposed scalable label propagation algo-rithms described in Section 2.2.2, MiMoLaP H and MiMoLaP respectively.

In the extended large scale USPS, OCR image data of handwrit-ten digits 2 , there are 266 , 079 training and 75 , 383 test patterns with 676 features for a binary problem ( X  X igit 0 X  and  X  X igit 1 X ). To validate the capability of the proposed approaches, we randomly sub-sampled 12 subsets of l = 10 , 50 , and 100 labeled samples as small-size training labeled sets for semi-supervised learning. To observe the influence of the amount of unlabeled samples on the SSL results, we also randomly sub-sample 6 subsets of unlabeled data by varying the numbers from u = 500 , 1 , 000 , 5 , 000 , 10 , 000 , 50 , 000 to 100 , 000 . Accordingly, keeping u fixed, there are 12 classification accuracies based on the 12 labeled sets containing l images. In the following experiments, all tests are performed using the whole test set with 75 , 383 samples and so the average accura-cies over the 12 subsets of labeled and unlabeled training samples are reported.

Figure 1 shows the average classification accuracies and the stan-dard deviations using SVM, SVMlin, and the proposed MiMoLaPs by fixing l = 10 and by varying u . One can see that all the SSL approaches significantly increase the classification accuracies com-pared to that by SVM. MiMoLaPs obtain the best results including accuracies and robustness. In the details, the classification accu-racies and the robustness of our proposed algorithms can be sig-nificantly improved using more unlabeled training samples. When
Available at: http://vikas.sindhwani.org/svmlin.html.
Available at:www.cse.ust.hk/ ivor/cvm.html. Figure 1: Classification results (i.e., average accuracies and standard deviation) of different approaches. Figure 2: Influence of l and u with MiMoLaP LG and MiMoLaP H . u = 10 , 000 , the accuracies by both the proposed MiMoLaPs con-verge to the best result, i.e., 98 . 5% with the standard deviation 0 . SVMlin usually cannot work well when data are not sparse and nonlinear separable.

To further observe the influence of the numbers of labeled sam-ples l and unlabeled samples u on classification results, we also report average classification accuracies over the 12 subsets by vary-ing both l and u for MiMoLaP H and MiMoLaP LG . From Fig. 2, one can see that the average accuracies increase with the more number of unlabeled samples used for training by keeping l fixed and when u is sufficiently large, the performances of both MiMoLaP MiMoLaP H converge using different number of labeled samples. More unlabeled samples used for training can increase the general-ization capacity of an SSL algorithm. Therefore, in the following experiments, we only consider the datasets when l = 10 .
To compute the PPKs in the proposed MiMoLaPs, the number of mixture components K and the metric parameter  X  should be selected. In order to investigate the influence of the two parame-ters on the classification results, we use all the unlabeled samples, i.e., u = 266 , 069 for semi-supervised training by varying K from 220 to 420 . Besides, we also check the influence of the value  X  , on the classification results. The experimental results are shown in Fig. 3(a). Observed from Fig. 3(a), the accuracies oscillate for different number of K involved, when  X  = 1 . 0 . However, when  X  = 0 . 5 , the classification accuracies are very stable. These results are consistent with those shown in Fig. 2, where the classification accuracies converge to 98 . 5% . Namely, when K = 220 , the mix-ture model could capture enough local information of the dataset, and so varying the number K has a little influence on the classifica-tion accuracies.

To validate the scalability of the proposed approaches, with all unlabeled samples, i.e., u = 266 , 069 , CPU times with the increase of K are reported in Fig. 3(b). All the experiments are conducted on a 3.0GHz CPU with the MATLAB implementations. One can Figure 3: (a) Influence of the component number K and  X  , and (b) CPU times. see that the CPU times of mixture modeling (clustering), training and test with larger K increases linearly. This confirms the scala-bility of the proposed approaches.
In this paper, the scalable graph-based SSL algorithms, the Gaus-sian Mixture Model Label Propagation (MiMoLaP), have been pro-posed for semi-supervised inductive classification. In a common label propagation SSL algorithm, it is necessary to compute the ( l + u )  X  ( l + u ) affinity matrix constructed by all the available l labeled and u unlabeled images and also necessary to compute its inverse with the computational complexity O (( l + u ) 3 ) , where u is usually in millions. In order to solve this problem, in the pro-posed MiMoLaP scalable SSL algorithms, the data are represented by Gaussian mixture models. Then, the affinity matrix is computed in the component level instead of data patterns. The size of the ma-trix can be reduced to K  X  K , where ( K  X  ( l + u ) ) and thus its inverse can be computed in an efficient way with significantly re-duced computational and spatial complexities. Moreover, the graph hyperparameter can be estimated directly from the data, which is difficultly determined by model selection with a small amount of labeled points [13]. With the graph, we can adopt one of the reg-ularization frameworks proposed in [13, 14] in the learning phase and a closed form solution can be obtained. Due to the new rep-resentation with a generative model, an out-of-bag image can be predicted by combining mixture models weighted by the a posteri-ori probability belonging to each component, which usually is un-able for most of label propagation SSL methods [3]. Experiments using real datasets confirm the effectiveness of the proposed algo-rithm with a good scalability, resulting in the better accuracies and generalization.

For the estimation of mixture models, there might exist the prob-lems that in high-dimensional space, the local information cannot be captured very well and so the estimated GMM is not stable due to outliers. However, for the learning as future development we can combine dimensionality reduction, e.g., random projection which is prone to Gaussian in a low-dimensional subspace [16].
This work was partially supported by Natural Science Founda-tion of China (No. 60705008), Shanghai Municipal R&amp;D Founda-tion (No. 10511500703), 973 Program (No. 2010CB327906) and 863 Program (No. 2009AA01A346). [1] O. Chapelle, B. Sch X lkopf, and A. Zien, Semi-Supervised [2] T. Joachims,  X  X ransductive inference for text classification [3] X. Zhu,  X  X emi-supervised learning literature survey, X  [4] X. Zhu and J. Lafferty,  X  X armonic mixtures: combining [5] V. Sindhwani and S. Keerthi,  X  X arge scale semi-supervised [6] K. Yu, S. Yu, and V. Tresp,  X  X lockwise supervised inference [7] J. Garcke and M. Griebel,  X  X emi-supervised learning with [8] T. G X rtner, Q. V. Le, S. Burton, A. Smola, and S. V. N. [9] I. W. Tsang and J. T. Kwok,  X  X arge-scale sparsified manifold [10] O. Delalleau, Y. Bengio, and N. L. Roux,  X  X fficient [11] M. Mahdaviani, N. de Freitas, B. Fraser, and F. Hamze,  X  X ast [12] T. Jebara, R. Kondor, and A. Howard,  X  X robability product [13] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch X lkopf, [14] X. Zhu, Z. Ghahramani, and J. Lafferty,  X  X emi-supervised [15] L. Fei-Fei, R. Fergus, and P. Perona.,  X  X earning generative [16] P. Diaconis and D. Freedman,  X  X symptotics of graphical
