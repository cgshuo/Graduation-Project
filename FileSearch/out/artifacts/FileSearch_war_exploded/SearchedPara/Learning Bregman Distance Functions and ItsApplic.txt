 An effective distance function plays an important role in many machine learning and data mining techniques. For instance, many clustering algorithms depend on distance functions for the pairwise distance measurements; most information retrieval techniques rely on distance functions to identify the data points that are most similar to a given query; k -nearest-neighbor classifier depends on dis-tance functions to identify the nearest neighbors for data classification. In general, learning effective distance functions is a fundamental problem in both data mining and machine learning. Recently, learning distance functions from data has been actively studied in machine learning. In-stead of using a predefined distance function (e.g., Euclidean distance), researchers have attempted to learn distance functions from side information that is often provided in the form of pairwise con-straints, i.e., must-link constraints for pairs of similar data points and cannot-link constraints for pairs of dissimilar data points. Example algorithms include [16, 2, 8, 11, 7, 15].
 Most distance learning methods assume a Mahalanobis distance. Given two data points x and x 0 , tance metric that needs to be learned from the side information. [16] learns a global distance metric (GDM) by minimizing the distance between similar data points while keeping dissimilar data points far apart. It requires solving a Semi-Definite Programming (SDP) problem, which is computation-ally expensive when the dimensionality is high. BarHillel et al [2] proposed the Relevant Compo-nents Analysis (RCA), which is computationally efficient and achieves comparable results as GDM. The main drawback with RCA is that it is unable to handle the cannot-link constraints. This problem was addressed by Discriminative Component Analysis (DCA) in [8], which learns a distance metric by minimizing the distance between similar data points and in the meantime maximizing the distance between dissimilar data points. The authors in [4] proposed an information-theoretic based metric learning approach (ITML) that learns the Mahalanobis distance by minimizing the differential rel-ative entropy between two multivariate Gaussians. Neighborhood Component Analysis (NCA) [5] learns a distance metric by extending the nearest neighbor classifier. The maximum-margin nearest neighbor (LMNN) classifier [14] extends NCA through a maximum margin framework. Yang et al. [17] propose a Local Distance Metric (LDM) that addresses multimodal data distributions. Hoi et al. [7] propose a semi-supervised distance metric learning approach that explores the unlabeled data for metric learning. In addition to learning a distance metric, several studies [12, 6] are devoted to learning a distance function, mostly non-metric, from the side information.
 Despite the success, the existing approaches for distance metric learning are limited in two aspects. First, most existing methods assume a fixed distance metric for the entire input space, which make it difficult for them to handle the heterogeneous data. This issue was already demonstrated in [17] when learning distance metrics from multi-modal data distributions. Second, the existing methods aim to learn a full matrix for the target distance metric that is in the square of the dimensionality, making it computationally unattractive for high dimensional data. Although the computation can be reduced significantly by assuming certain forms of the distance metric (e.g., diagonal matrix), these simplifications often lead to suboptimal solutions. To address these two limitations, we propose a novel scheme that learns Bregman distance functions from the given side information. Bregman distance or Bregman divergence [3] has several salient properties for distance measure. Bregman distance generalizes the class of Mahalanobis distance by deriving a distance function from a given convex function  X  ( x ) . Since the local distance metric can be derived from the local Hessian matrix of  X  ( x ) , Bregman distance function avoids the assumption of fixed distance metric. Recent studies [1] also reveal the connection between Bregman distances and exponential families of distributions. For example, Kullback-Leibler divergence is a special Bregman distance when choosing the negative entropy function for the convex function  X  ( x ) .
 The objective of this work is to design an efficient and effective algorithm that learns a Bregman distance function from pairwise constraints. Although Bregman distance or Bregman divergence has been explored in [1], all these studies assume a predefined Bregman distance function. To the best of our knowledge, this is the first work that addresses the problem of learning Bregman distances from the pairwise constraints. We present a non-parametric framework for Bregman distance learning, together with an efficient learning algorithm. Our empirical study with semi-supervised clustering show that the proposed approach (i) outperforms the state-of-the-art algorithms for distance metric learning, and (ii) is computationally efficient for high dimensional data.
 The rest of the paper is organized as follows. Section 2 presents the proposed framework of learning Bregman distance functions from the pairwise constraints, together with an efficient learning algo-rithm. Section 3 presents the experimental results with semi-supervised clustering by comparing the proposed algorithms with a number of state-of-the-art algorithms for distance metric learning. Section 5 concludes this work. 2.1 Bregman Distance Function Bregman distance function is defined based on a given convex function. Let  X  ( x ) : R d 7 X  R be a strictly convex function that is twice differentiable. Given  X  ( x ) , the Bregman distance function is defined as For the convenience of discussion, we consider a symmetrized version of the Bregman distance function that is defined as follows The following proposition shows the properties of d ( x 1 , x 2 ) .
 Proposition 1. The distance function defined in (1) satisfies the following properties if  X  ( x ) is a Remark To better understand the Bregman distance function, we can rewrite d ( x 1 , x 2 ) in (1) as where  X  x is a point on the line segment between x 1 and x 2 . As indicated by the above expression, the Bregman distance function can be viewed as a general Mahalanobis distance that introduces a local distance metric A =  X  2  X  (  X  x ) . Unlike the conventional Mahalanobis distance where metric A is a constant matrix throughout the entire space, the local distance metric A =  X  2  X  (  X  x ) is introduced via the Hessian matrix of convex function  X  ( x ) and therefore depends on the location of x 1 and x 2 . Although the Bregman distance function defined in (1) does not satisfy the triangle inequality, the following proposition shows the degree of violation could be bounded if the Hessian matrix of  X  ( x ) is bounded.
 Proposition 2. Let  X  be the closed domain for x . If  X  m, M  X  R , M &gt; m &gt; 0 and where I is the identity matrix, we have the following inequality The proof of this proposition can be found in Appendix A. As indicated by Proposition 2, the de-gree of violation of the triangle inequality is essentially controlled by convex function with almost constant Hessian matrix, we would expect that to a large degree, Breg-man distance will satisfy the triangle inequality. In the extreme case when  X  ( x ) = x &gt; Ax/ 2 and  X  2  X  ( x ) = A , we have a constant Hessian matrix, leading to a complete satisfaction of the triangle inequality. 2.2 Problem Formulation To a learn a Bregman distance function, the key is to find the appropriate convex function  X  ( x ) that is consistent with the given pairwise constraints. In order to learn the convex function  X  ( x ) , we take a non-parametric approach by assuming that  X  (  X  ) belongs to a Reproducing Kernel Hilbert Space H  X  . Given a kernel function  X  ( x, x 0 ) : R d  X  R d 7 X  R , our goal is to search for a convex function  X  ( x )  X  H  X  such that the induced Bregman distance function, denoted by d  X  ( x, x 0 ) , minimizes the overall training error with respect to the given pairwise constraints.
 the input patterns of all training instances in D .
 Following the maximum margin framework for classification, we cast the problem of learning a Bregman distance function from pairwise constraints into the following optimization problem, i.e., where  X ( H ) = { f  X  H : f is convex } refers to the subspace of functional space H that only includes convex functions, ` ( z ) = max(0 , 1  X  z ) is a hinge loss, and C is a penalty cost parameter. representer theorem for  X  ( x ) because it is  X   X  ( x ) used in the definition of distance function, not ( d  X  1 ), and h ( z ) = exp( z ) . For the convenience of discussion, we assume h (0) = 0 throughout this paper.
 First, since  X  ( x )  X  X   X  , we have R d into A and  X  A that are defined as We define H k and H  X  as follows The following proposition summarizes an important property of reproducing kernel Hilbert space H  X  when kernel function  X  (  X  ,  X  ) is restricted to the form in Eq. (2.2).
 Proposition 3. If the kernel function  X  (  X  ,  X  ) is written in the form of Equation (2.2) with h (0) = 0 , we have H k and H  X  form a complete partition of H  X  , i.e., H  X  = H k  X  X   X  , and H k  X H  X  . We therefore have the following representer theorem for  X  ( x ) that minimizes (3) Theorem 1. The function  X  ( x ) that minimizes (3) admits the following expression where u  X  R N and X = ( x 1 , . . . , x N ) .
 The proof of the above theorem can be found in Appendix B. 2.3 Algorithm P i =1  X  i  X  ( y  X  x i ) where  X  i  X  0 , i = 1 , . . . , N are non-negative combination weights. This results in  X  ( x ) = Notice that when h ( z ) = z 2 / 2 , we have d ( x a , x b ) expressed as This is a Mahanalobis distance with metric A = X diag (  X  ) X &gt; = longer stationary due to the non-linear function exp( z ) .
 Given the assumption that q ( y ) = Note that the constraint  X  k  X  0 is introduced to ensure  X  ( x ) = function. By defining we simplify the problem in (11) as follows where ` ( z ) = max(0 , 1  X  z ) . We solve the above problem by a simple subgradient descent approach. In particular, at iteration t , given the current solution  X  t and b t , we compute the gradients as which (  X  t , b t ) suffers a non-zeros loss, i.e., We can then express the sub-gradients of L at  X  t and b t as follows: step size that is set to be  X  t = C t by following the Pegasos algorithm [10] for solving SVMs. The pseudo-code of the proposed algorithm is summarized in Algorithm 1.
 Algorithm 1 Algorithm of Learning Bregman Distance Functions Computational complexity One of the major computational costs for Algorithm 1 is the prepa-ration of kernel matrix K and vector { z i } n i =1 , which fortunately can be pre-computed. Each step of the subgradient descent algorithm has a linear complexity, i.e., O (max( N, n )) , which makes it rea-sonable even for large data sets with high dimensionality. The number of iterations for convergence is O (1 / X  2 ) where  X  is the target accuracy. It thus works fine if we are not critical about the accuracy of the solution. We evaluate the proposed distance learning technique by semi-supervised clustering. In particular, we first learn a distance function from the given pairwise constraints and then apply the learned distance function to data clustering. We verify the efficacy and efficiency of the proposed technique by comparing it with a number of state-of-the-art algorithms for distance metric learning. 3.1 Experimental Testbed and Settings We adopt six well-known datasets from UCI machine learning repository, and six popular text bench-icantly in properties such as the number of clusters/classes, the number of features, and the number of instances. The diversity of datasets allows us to examine the effectiveness of the proposed learn-ing technique more comprehensively. The details of the datasets are shown in Table 1. Similar to previous work [16], the pairwise constraints are created by random sampling. More specifically, we randomly sample a subset of pairs from the pool of all possible pairs (every two instances forms a pair). Two instances form a must-link constraint (i.e., y i = +1 ) if they share the same class label, and form a cannot-link constraint (i.e., y i =  X  1 ) if they are assigned to different classes. To calculate the Bregman function, in this experiment, we adopt the non-linear function h ( x ) = (exp( x &gt; x 1 ) , . . . , exp( x &gt; x N )) .
 To perform data clustering, we run the k-means algorithm using the distance function learned from 500 randomly sampled positive constraints 500 random negative constraints. The number of clusters is simply set to the number of classes in the ground truth. The initial cluster centroids are randomly chosen from the dataset. To enable fair comparisons, all comparing algorithms start with the same set of initial centroids. We repeat each clustering experiment for 20 times, and report the final results by averaging over the 20 runs.
 We compare the proposed Bregman distance learning method using the k-means algorithm for semi-supervised clustering, termed Bk-means , with the following approaches: (1) a standard k-means , (2) the constrained k-means [13] ( Ck-means ), (3) Ck-means with distance learned by RCA [2], (4) Ck-means with distance learned by DCA [8], (5) Ck-means with distance learned by the Xing X  X  algorithm [16] ( Xing ), (6) Ck-means with information-theoretic metric learning ( ITML ) [4], and (7) Ck-means with a distance function learned by a boosting algorithm ( DistBoost ) [12]. To evaluate the clustering performance, we use the some standard performance metrics, in-cluding pairwise Precision, pairwise Recall, and pairwise F1 measures [9], which are evalu-over the total number of pairs actually placed in the same cluster, and pairwise F1 equals to 2  X  precision  X  recall/ ( precision + recall ) . 3.2 Performance Evaluation on Low-dimensional Datasets The first set of experiments evaluates the clustering performance on six UCI datasets. Table 2 shows the average precision, recall, and F1 measurements of all the competing algorithms given a set of 1 , 000 random constraints. The top two highest average F1 scores on each dataset were highlighted in bold font. From the results in Table 2, we observe that the proposed Bregman distance based k-means clustering approach (Bk-means) is either the best or the second best for almost all datasets, indicating that the proposed algorithm is in general more effective than the other algorithms for distance metric learning. 3.3 Performance Evaluation on High-dimensional Text Data We evaluate the clustering performance on six text datasets. Since some of the methods are infeasible for text clustering due to the high dimensionality, we only include the results for the methods which are feasible for this experiment (i.e., OOM indicates the method takes more than 10 hours, and OOT indicates the method needs more than 16G REM). Table 3 summarizes the F1 performance of all feasible methods for datasets w1a, w2a, w6a, WebKB, 20newsgroup and reuter. Since cosine similarity is commonly used in textual domain, we use k-means, Ck-means in both Euclidian space and Cosine similarity space as baselines. The best F1 scores are marked in bold in Table 3. The results show that the learned Bregman distance function is applicable for high dimensional data, and it outperforms the other commonly used text clustering methods for four out of six datasets. Table 2: Evaluation of clustering performance (average precision, recall, and F1) on six UCI datasets. The top two F1 scores are highlighted in bold font for each dataset. Table 3: Evaluation of clustering F1 performance on the high dimensional text data. Only applicable methods are shown. OOM indicates  X  X ut of memory X , and OOT indicates  X  X ut of time X . 3.4 Computational Complexity Here, we evaluate the running time of semi-supervised clustering. For a conventional clustering algorithm such as k-means, its computational complexity is determined by both the calculation of distance and the clustering scheme. For a semi-supervised clustering algorithm based on distance learning, the overall computational time include both the time for training an appropriate distance clustering over the six UCI datasets are listed in Table 4. It is clear that the Bregman distance based clustering has comparable efficiency with simple methods like RCA and DCA on low dimensional data, and runs much faster than Xing, ITML, and DistBoost. On the high dimensional text data, it is much faster than other applicable DML methods.
 datasets ( 10% sampling from the datasets in Table 1). In this paper, we propose to learn a Bregman distance function for clustering algorithms using a non-parametric approach. The proposed scheme explicitly address two shortcomings of the existing approaches for distance fuction/metric learning, i.e., assuming a fixed distance metric for the entire input space and high computational cost for high dimensional data. We incorporate the Bregman distance function into the k-means clustering algorithm for semi-supervised data clustering. Exper-iments of semi-supervised clustering with six UCI datasets and six high dimensional text datasets have shown that the Bregman distance function outperforms other distance metric learning algo-rithms in F1 measure. It also verifies that the proposed distance learning algorithm is computation-ally efficient, and is capable of handling high dimensional data.
 This work was done when Mr. Lei Wu was an RA at Nanyang Technological University, Sin-gapore. This work was supported in part by MOE tier-1 Grant (RG67/07), NRF IDM Grant (NRF2008IDM-IDM-004-018), National Science Foundation (IIS-0643494), and US Navy Re-search Office (N00014-09-1-0663).
 Proof. First, let us denote by f as follows: The square of the right side of Eq. (2) is where From the fact that since Proof. We write  X  ( x ) =  X  k ( x ) +  X   X  ( x ) where Thus, the distance function defined in (1) is then expressed as d ( x a , x b ) = ( x a  X  x b ) &gt; Since |  X  ( x ) | 2 H |  X  have  X  ( x ) =  X  k ( x ) , which leads to the result in the theorem. [1] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with bregman divergences. In [2] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning a mahalanobis metric from [3] L. Bregman. The relaxation method of finding the common points of convex sets and its appli-[4] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. [5] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighborhood component analy-[6] T. Hertz, A. B. Hillel, and D. Weinshall. Learning a kernel function for classification with [7] S. C. H. Hoi, W. Liu, and S.-F. Chang. Semi-supervised distance metric learning for collab-[8] S. C. H. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. Learning distance metrics with contextual [9] Y. Liu, R. Jin, and A. K. Jain. Boostcluster: boosting clustering by pairwise constraints. In [10] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver [11] L. Si, R. Jin, S. C. H. Hoi, and M. R. Lyu. Collaborative image retrieval via regularized metric [12] T. H. Tomboy, A. Bar-hillel, and D. Weinshall. Boosting margin based distance functions [13] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl. Constrained k-means clustering with back-[14] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neigh-[15] L. Wu, S. C. H. Hoi, J. Zhu, R. Jin, and N. Yu. Distance metric learning from uncertain side [16] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with application to [17] L. Yang, R. Jin, R. Sukthankar, and Y. Liu. An efficient algorithm for local distance metric
