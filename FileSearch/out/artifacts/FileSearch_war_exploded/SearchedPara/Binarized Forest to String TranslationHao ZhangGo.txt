 In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The uni-fying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the source side or the target side for inference, there are four general categories within the framework:  X  string-to-string (Chiang, 2005; Zollmann and  X  string-to-tree (Galley et al., 2006; Shen et al.,  X  tree-to-string (Lin, 2004; Quirk et al., 2005;  X  tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the sub-space of structures of the source side constrained by an input tree or trees. Hence, tree-to-x mod-els are more constrained but more efficient. Mod-els such as Huang et al. (2006) can match multi-level tree fragments on the source side which means larger contexts are taken into account for transla-tion (Poutsma, 2000), which is a modeling advan-tage. To balance efficiency and accuracy, forest-to-string models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Tradition-ally, such forests are obtained through hyper-edge pruning in the k -best search space of a monolin-gual parser (Huang, 2008). The pruning parameters that control the size of forests are normally hand-tuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we re-fer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. We believe that structural variants which allow more source spans to be explored during translation are more important (DeNeefe et al., 2007), while syn-tactic variants might improve word sense disam-biguation but also introduce more spurious ambi-guities (Chiang, 2005) during decoding. To focus on structural variants, we propose a family of bina-rization algorithms to expand one single constituent tree into a packed forest of binary trees containing combinations of adjacent tree nodes. We control the freedom of tree node binary combination by restrict-ing the distance to the lowest common ancestor of two tree nodes. We show that the best results are achieved when the distance is two, i.e., when com-bining tree nodes sharing a common grand-parent. In contrast to conventional parser-produced-forest-to-string models, in our model:  X  Forests are not generated by a parser but by  X  Instead of using arbitary pruning parameters,  X  There is at most one nonterminal per span so Since GHKM rules (Galley et al., 2004) can cover multi-level tree fragments, a synchronous grammar extracted using the GHKM algorithm can have syn-chronous translation rules with more than two non-terminals regardless of the branching factor of the source trees. For the first time, we show that simi-lar to string-to-tree decoding, synchronous binariza-tion significantly reduces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is re-structured using our binarization algorithm, result-ing in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extrac-tion. Fourth, on the translation forest generated by all applicable translation rules, which is not neces-sarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up de-coding algorithm with intergrated LM intersection using the cube pruning technique (Chiang, 2005). The rest of the paper is organized as follows. In Section 2, we give an overview of the forest-to-string models. In Section 2.1, we introduce a more efficient and flexible algorithm for extracting com-posed GHKM rules based on the same principle as cube pruning (Chiang, 2007). In Section 3, we in-troduce our source tree binarization algorithm for producing binarized forests. In Section 4, we ex-plain how to do synchronous rule factorization in a forest-to-string decoder. Experimental results are in Section 5. Forest-to-string models can be described as where f stands for a source string, e stands for a tar-get string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T , and Y stands for the target side yield of a derivation. The search problem is finding the derivation with the highest probability in the space of all deriva-tions for all parse trees for an input sentence. The log probability of a derivation is normally a lin-ear combination of local features which enables dy-namic programming to find the optimal combination efficiently. In this paper, we focus on the models based on the Synchronous Tree Substitution Gram-mars (STSG) defined by Galley et al. (2004). In con-trast to a tree-to-string model, the introduction of F augments the search space systematically. When the first-best parse is wrong or no good translation rules are applicable to the first-best parse, the model can recover good translations from alternative parses.
In STSG, local features are defined on tree-to-string rules, which are synchronous grammar rules defining how a sequence of terminals and nontermi-nals on the source side translates to a sequence of target terminals and nonterminals. One-to-one map-ping of nonterminals is assumed. But terminals do not necessarily need to be aligned. Figure 1 shows a typical English-Chinese tree-to-string rule with a re-ordering pattern consisting of two nonterminals and different numbers of terminals on the two sides.
Forest-to-string translation has two stages. The first stage is rule extraction on word-aligned parallel texts with source forests. The second stage is rule enumeration and DP decoding on forests of input strings. In both stages, at each tree node, the task on the source side is to generate a list of tree fragments by composing the tree fragments of its children. We propose a cube-pruning style algorithm that is suit-able for both rule extraction during training and rule enumeration during decoding.

At the highest level, our algorithm involves three steps. In the first step, we label each node in the in-put forest by a boolean variable indicating whether it is a site of interest for tree fragment generation. If it is marked true , it is an admissible node. In the case of rule extraction, a node is admissible if and only if it corresponds to a phrase pair according to the un-derlying word alignment. In the case of decoding, every node is admissible for the sake of complete-ness of search. An initial one-node tree fragment is placed at each admissible node for seeding the tree fragment generation process. In the second step, we do cube-pruning style bottom-up combinations to enumerate a pruned list of tree fragments at each tree node. In the third step, we extract or enumerate-and-match tree-to-string rules for the tree fragments at the admissible nodes. 2.1 A Cube-pruning-inspired Algorithm for Galley et al. (2004) defined minimal tree-to-string rules. Galley et al. (2006) showed that tree-to-string rules made by composing smaller ones are impor-tant to translation. It can be understood by the anal-ogy of going from word-based models to phrase-based models. We relate composed rule extraction to cube-pruning (Chiang, 2007). In cube-pruning, the process is to keep track of the k -best sorted lan-guage model states at each node and combine them bottom-up with the help of a priority queue. We can imagine substituting k -best LM states with k composed rules at each node and composing them bottom-up. We can also borrow the cube pruning trick to compose multiple lists of rules using a pri-ority queue to lazily explore the space of combina-tions starting from the top-most element in the cube formed by the lists.

We need to define a ranking function for com-posed rules. To simulate the breadth-first expansion heuristics of Galley et al. (2006), we define the fig-ure of merit of a tree-to-string rule as a tuple m = ( h, s, t ) , where h is the height of a tree fragment, s is the number of frontier nodes, i.e., bottom-level nodes including both terminals and non-terminals, and t is the number of terminals in the set of frontier nodes. We define an additive operator + : and a min operator based on the order &lt; : m 1 &lt; m 2  X  X  X  The + operator corresponds to rule compositions. The &lt; operator corresponds to ranking rules by their sizes. A concrete example is shown in Figure 2, in which case the monotonicity property of (+ , &lt; ) holds: if m a &lt; m b , m a + m c &lt; m b + m c . However, this is not true in general for the operators in our def-inition, which implies that our algorithm is indeed like cube-pruning: an approximate k -shortest-path algorithm. The motivation of tree binarization is to factorize large and rare structures into smaller but frequent ones to improve generalization. For example, Penn Treebank annotations are often flat at the phrase level. Translation rules involving flat phrases are un-likely to generalize. If long sequences are binarized, the commonality of subsequences can be discov-ered. For example, the simplest binarization meth-ods left-to-right , right-to-left , and head-out explore sharing of prefixes or suffixes. Among exponentially many binarization choices, these algorithms pick a single bracketing structure for a sequence of sibling nodes. To explore all possible binarizations, we use a CYK algorithm to produce a packed forest of bi-nary trees for a given sibling sequence.

With CYK binarization, we can explore any span that is nested within the original tree structure, but still miss all cross-bracket spans. For example, translating from English to Chinese, The phrase  X  X here is X  should often be translated into one verb in Chinese. In a correct English parse tree, however, the subject-verb boundary is between  X  X here X  and  X  X s X . As a result, tree-to-string translation based on constituent phrases misses the good translation rule.
The CYK-n binarization algorithm shown in Al-gorithm 1 is a parameterization of the basic CYK binarization algorithm we just outlined. The idea is that binarization can go beyond the scope of parent nodes to more distant ancestors. The CYK-n algo-rithm first annotates each node with its n nearest ancestors in the source tree, then generates a bina-rization forest that allows combining any two nodes with common ancestors. The ancestor chain labeled at each node licenses the node to only combine with nodes having common ancestors in the past n gener-ations.
 The algorithm creates new tree nodes on the fly. New tree nodes need to have their own states in-dicated by a node label representing what is cov-ered internally by the node and an ancestor chain representing which nodes the node attaches to ex-ternally. Line 22 and Line 23 of Algorithm 1 up-date the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Good-man, 1999), the ancestor computation can be sum-marized by the (  X  ,  X  ) pair.  X  produces the ances-tor chain of a hyper-edge.  X  produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate , min) pair. concatenate produces a concatenation of node la-bels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the min-imum length among all sequences covered by the node span. The ancestor chain of a new node is the common ancestors of the nodes in its minimum tree sequence.

For clarity, we do full CYK loops over all O ( | w | 2 ) spans and O ( | w | 3 ) potential hyper-edges, where | w | is the length of a source string. In reality, only de-scendants under a shared ancestor can combine. If we assume trees have a bounded branching factor b , the number of descendants after n generations is still bounded by a constant c = b n . The algorithm is O ( c 3  X | w | ) , which is still linear to the size of input sentence when the parameter n is a constant.
Figure 3 shows some examples of alternative trees generated by the CYK-n algorithm. In this example, standard CYK binarization will not create any new trees since the input is already binary. The CYK-2 and CYK-3 algorithms discover new trees with an increasing degree of freedom. In this section, we deal with binarization of transla-tion forests, also known as translation hypergraphs (Mi et al., 2008). A translation forest is a packed forest representation of all synchronous derivations composed of tree-to-string rules that match the source forest. Tree-to-string decoding algorithms work on a translation forest, rather than a source for-est. A binary source forest does not necessarily al-ways result in a binary translation forest. In the tree-to-string rule in Figure 4, the source tree is already binary with the help of source tree binarization, but the translation rule involves three variables in the set of frontier nodes. If we apply synchronous binariza-tion (Zhang et al., 2006), we can factorize it into two smaller translation rules each having two vari-ables. Obviously, the second rule, which is a com-mon pattern, is likely to be shared by many transla-tion rules in the derivation forest. When beams are fixed, search goes deeper in a factorized translation forest.

The challenge of synchronous binarization for a forest-to-string system is that we need to first match large tree fragments in the input forest as the first step of decoding. Our solution is to do the matching using the original rules and then run synchronous binarization to break matching rules down to factor rules which can be shared in the derivation forest. This is different from the offline binarization scheme described in (Zhang et al., 2006), although the core algorithm stays the same. We ran experiments on public data sets for English to Chinese, Czech, French, German, and Spanish Algorithm 1 The CYK-n Binarization Algorithm translation to evaluate our methods. 5.1 Setup For English-to-Chinese translation, we used all the allowed training sets in the NIST 2008 constrained track. For English to the European languages, we used the training data sets for WMT 2010 (Callison-Burch et al., 2010). For NIST, we filtered out sen-tences exceeding 80 words in the parallel texts. For WMT, the filtering limit is 60 . There is no filtering on the test data set. Table 1 shows the corpus statis-tics of our bilingual training data sets.

At the word alignment step, we did 6 iterations of IBM Model-1 and 6 iterations of HMM. For English-Chinese, we ran 2 iterations of IBM Model-4 in addition to Model-1 and HMM. The word align-ments are symmetrized using the  X  X nion X  heuris-tics. Then, the standard phrase extraction heuristics (Koehn et al., 2003) were applied to extract phrase pairs with a length limit of 6 . We ran the hierar-chical phrase extraction algorithm with the standard heuristics of Chiang (2005). The phrase-length limit is interpreted as the maximum number of symbols on either the source side or the target side of a given rule. On the same aligned data sets, we also ran the tree-to-string rule extraction algorithm described in Section 2.1 with a limit of 16 rules per tree node.
The default parser in the experiments is a shift-reduce dependency parser (Nivre and Scholz, 2004). It achieves 87 . 8% labelled attachment score and 88 . 8% unlabeled attachment score on the standard Penn Treebank test set. We convert dependency parses to constituent trees by propagating the part-of-speech tags of the head words to the correspond-ing phrase structures.

We compare three systems: a phrase-based sys-tem (Och and Ney, 2004), a hierarchical phrase-based system (Chiang, 2005), and our forest-to-string system with different binarization schemes. In the phrase-based decoder, jump width is set to 8 . In the hierarchical decoder, only the glue rule is applied to spans longer than 10 . For the forest-to-string sys-tem, we do not have such length-based reordering constraints.
 We trained two 5 -gram language models with Kneser-Ney smoothing for each of the target lan-guages. One is trained on the target side of the par-allel text, the other is on a corpus provided by the evaluation: the Gigaword corpus for Chinese and news corpora for the others. Besides standard fea-tures (Och and Ney, 2004), the phrase-based decoder also uses a Maximum Entropy phrasal reordering model (Zens and Ney, 2006). Both the hierarchi-cal decoder and the forest-to-string decoder only use the standard features. For feature weight tuning, we do Minimum Error Rate Training (Och, 2003). To explore a larger n -best list more efficiently in train-ing, we adopt the hypergraph-based MERT (Kumar et al., 2009).

To evaluate the translation results, we use BLEU (Papineni et al., 2002). 5.2 Translation Results Table 2 shows the scores of our system with the best binarization scheme compared to the phrase-based system and the hierarchical phrase-based sys-tem. Our system is consistently better than the other two systems in all data sets. On the English-Chinese data set, the improvement over the phrase-based sys-tem is 1 . 3 BLEU points, and 0 . 8 over the hierarchi-cal phrase-based system. In the tasks of translat-ing to European languages, the improvements over the phrase-based baseline are in the range of 0 . 5 to 1 . 0 BLEU points, and 0 . 3 to 0 . 5 over the hierar-chical phrase-based system. All improvements ex-cept the bf2s and hier difference in English-Czech are significant with confidence level above 99% us-ing the bootstrap method (Koehn, 2004). To demon-strate the strength of our systems including the two baseline systems, we also show the reported best re-sults on these data sets from the 2010 WMT work-shop. Our forest-to-string system ( bf2s ) outperforms or ties with the best ones in three out of four lan-guage pairs. 5.3 Different Binarization Methods The translation results for the bf2s system in Ta-ble 2 are based on the cyk binarization algorithm with bracket violation degree 2 . In this section, we vary the degree to generate forests that are incremen-tally augmented from a single tree. Table 3 shows the scores of different tree binarization methods for the English-Chinese task.

It is clear from reading the table that cyk-2 is the optimal binarization parameter. We have verified this is true for other language pairs on non-standard data sets. We can explain it from two angles. At degree 2 , we allow phrases crossing at most one bracket in the original tree. If the parser is reason-ably good, crossing just one bracket is likely to cover most interesting phrases that can be translation units. From another point of view, enlarging the forests entails more parameters in the resulting translation model, making over-fitting likely to happen. 5.4 Binarizer or Parser? A natural question is how the binarizer-generated forests compare with parser-generated forests in translation. To answer this question, we need a parser that can generate a packed forest. Our fast deterministic dependency parser does not generate a packed forest. Instead, we use a CRF constituent parser (Finkel et al., 2008) with state-of-the-art ac-curacy. On the standard Penn Treebank test set, it achieves an F-score of 89 . 5% . It uses a CYK algo-rithm to do full dynamic programming inference, so is much slower. We modified the parser to do hyper-edge pruning based on posterior probabilities. The parser preprocesses the Penn Treebank training data through binarization. So the packed forest it pro-duces is also a binarized forest. We compare two systems: one is using the cyk-2 binarizer to generate forests; the other is using the CRF parser with prun-ing threshold e  X  p , where p = 2 to generate forests. 1 Although the parser outputs binary trees, we found cross-bracket cyk-2 binarization is still helpful.
Table 4 shows the comparison of binarization for-est and parser forest on English-German translation. The results show that cyk-2 forest performs slightly better than the parser forest. We have not done full exploration of forest pruning parameters to fine-tune the parser-forest. The speed of the constituent parser is the efficiency bottleneck. This actually demon-strates the advantage of the binarizer plus forest-to-string scheme. It is flexible, and works with any parser that generates projective parses. It does not require hand-tuning of forest pruning parameters for training. 5.5 Synchronous Binarization In this section, we demonstrate the effect of syn-chronous binarization for both tree-to-string and forest-to-string translation. The experiments are on the English-Chinese data set. The baseline systems use k -way cube pruning, where k is the branching factor, i.e., the maximum number of nonterminals on the right-hand side of any synchronous translation rule in an input grammar. The competing system does online synchronous binarization as described in Section 4 to transform the grammar intersected with the input sentence to the minimum branching factor k  X  ( k  X  &lt; k ), and then applies k  X  -way cube pruning. Typically, k  X  is 2 .

Table 5 shows that synchronous binarization does help reduce search errors and find better translations consistently in all settings. The idea of concatenating adjacent syntactic cate-gories has been explored in various syntax-based models. Zollmann and Venugopal (2006) aug-mented hierarchial phrase based systems with joint syntactic categories. Liu et al. (2007) proposed tree-sequence-to-string translation rules but did not pro-vide a good solution to place joint subtrees into con-nection with the rest of the tree structure. Zhang et al. (2009) is the closest to our work. But their goal was to augment a k -best forest. They did not bina-rize the tree sequences. They also did not put con-straint on the tree-sequence nodes according to how many brackets are crossed.

Wang et al. (2007) used target tree binarization to improve rule extraction for their string-to-tree sys-tem. Their binarization forest is equivalent to our cyk-1 forest. In contrast to theirs, our binarization scheme affects decoding directly because we match tree-to-string rules on a binarized forest.
Different methods of translation rule binarization have been discussed in Huang (2007). Their argu-ment is that for tree-to-string decoding target side binarization is simpler than synchronous binariza-tion and works well because creating discontinous source spans does not explode the state space. The forest-to-string senario is more similar to string-to-tree decoding in which state-sharing is important. Our experiments show that synchronous binariza-tion helps significantly in the forest-to-string case. We have presented a new approach to tree-to-string translation. It involves a source tree binarization step and a standard forest-to-string translation step. The method renders it unnecessary to have a k -best parser to generate a packed forest. We have demon-strated state-of-the-art results using a fast parser and a simple tree binarizer that allows crossing at most one bracket in each binarized node. We have also shown that reducing search errors is important for forest-to-string translation. We adapted the syn-chronous binarization technqiue to improve search and have shown significant gains. In addition, we also presented a new cube-pruning-style algorithm for rule extraction. In the new algorithm, it is easy to adjust the figure-of-merit of rules for extraction. In the future, we plan to improve the learning of trans-lation rules with binarized forests.
 We would like to thank the members of the MT team at Google, especially Ashish Venugopal, Zhifei Li, John DeNero, and Franz Och, for their help and dis-cussions. We would also like to thank Daniel Gildea for his suggestions on improving the paper.
