 Xiangrui Meng ximeng@linkedin.com LinkedIn Corporation, 2029 Stierlin Ct, Mountain View, CA 94043 Michael W. Mahoney mmahoney@cs.stanford.edu Department of Mathematics, Stanford University, Stanford, CA 94305 Statistical analysis of massive data sets presents very substantial challenges both to data infrastructure and to algorithm development. In particular, many popu-lar data analysis and machine learning algorithms that perform well when applied to small-scale and medium-scale data that can be stored in RAM are infeasible when applied to the terabyte-scale and petabyte-scale data sets that are stored in distributed environments and that are increasingly common. In this paper, we develop algorithms for variants of the robust regression problem, and we evaluate implementations of them on data of up to the terabyte scale. In addition to being of interest since ours are the first algorithms for these problems that are appropriate for data of that scale, our results are also of interest since they highlight  X  X lgorithm engineering X  challenges that will become more common as researchers try to scale up small-scale and medium-scale data analysis and ma-chine learning methods. For example, at several points we had to work with variants of more primitive algo-rithms that were worse by traditional complexity mea-sures but that had better communication properties. 1.1 MapReduce and Large-scale Data The MapReduce framework, introduced by (Dean &amp; Ghemawat, 2004) in 2004, has emerged as the de facto standard parallel environment for analyzing mas-sive data sets. Apache Hadoop 1 , an open source software framework inspired by Google X  X  MapReduce, is now extensively used by companies such as Face-book, LinkedIn, Yahoo!, etc. In a typical application, one builds clusters of thousands of nodes containing petabytes of storage in order to process terabytes or even petabytes of daily data. As a parallel computing framework, MapReduce is well-known for its scalabil-ity to massive data. However, the scalability comes at the price of a very restrictive interface: sequential access to data, and functions limited to map and re-duce. Working within this framework demands that traditional algorithms be redesigned to respect this in-terface. For example, the Apache Mahout 2 project is building a collection of scalable machine learning algorithms that includes algorithms for collaborative filtering, clustering, matrix decomposition, etc. Although some algorithms are easily adapted to the MapReduce framework, many algorithms (and in par-ticular many iterative algorithms popular in machine learning, optimization, and linear algebra) are not. When the data are stored on RAM, each iteration is usually very cheap in terms of floating-point opera-tions (FLOPs). However, when the data are stored on secondary storage, or in a distributed environment, each iteration requires at least one pass over the data. Since the cost of communication to and from secondary storage often dominates FLOP count costs, each pass can become very expensive for very large-scale prob-lems. Moreover, there is generally no parallelism be-tween iterations: an iterative algorithm must wait un-til the previous step gets completed before the next step can begin. 1.2 Our Main Results In this work, we are interested in developing algo-rithms for robust regression problems on MapReduce. Of greatest interest will be algorithms for the strongly over-determined ` 1 regression problem, 3 although our method will extend to more general ` p regression. For simplicity of presentation, we will formulate most of our discussion in terms of ` p regression; and toward the end we will describe the results of our implemen-tation of our algorithm for ` 1 regression.
 Recall the strongly over-determined ` p regression prob-lem : given a matrix A  X  R m  X  n , with m n , a vector b  X  R m , a number p  X  [1 ,  X  ), and an error parameter &gt; 0, find a (1 + )-approximate solution  X  x  X  R n to: i.e. , find a vector  X  x such that where the ` p norm is given by k x k p = ( P i | x i | p ) more robust alternative to the widely-used ` 2 regres-sion is obtained by working with ` p regression, with p  X  [1 , 2), where p = 1 is by far the most popular alter-native. This, however, comes at the cost of increased complexity. While ` 2 regression can be solved with, e.g. , a QR decomposition, ` 1 regression problems can be formulated as linear programs, and other ` p regres-sion problems can be formulated as convex programs. In those cases, iterated weighted least-squares meth-ods or simplex methods or interior-point methods are typically used in practice. These algorithms tend to re-quire dot products, orthogonalization, and thus a great deal of communication, rendering them challenging to implement in the MapReduce framework.
 In this paper, we describe an algorithm with better communication properties that is efficient for solv-ing strongly over-determined ` p regression problems to moderate precision on MapReduce. 4 Several aspects of our main algorithm are of particular interest:  X  Single-pass conditioning. We use a recently- X  Single-pass random sampling. By using a con- X  Effective initialization. By using multiple sub- X  Effective iterative solving. By performing in par-In addition to describing the basic algorithm, we also present empirical results from a numerical implemen-tation of our algorithm applied to the ` 1 regression problems on data sets of size up to the terabyte scale. 1.3 Prior Related Work There is a large literature on robust regression, dis-tributed computation, MapReduce, and randomized matrix algorithms that is beyond our scope to review. See, e.g. , (Rousseeuw &amp; Leroy, 1987), (Bertsekas &amp; Tsitsiklis, 1991), (Dean &amp; Ghemawat, 2004), and (Ma-honey, 2011), respectively, for details. Here, we will review only the most recent related work.
 Strongly over-determined ` 1 regression problems were considered by (Portnoy &amp; Koenker, 1997), who used a uniformly-subsampled solution for ` 1 regression to estimate the signs of the optimal residuals in order to reduce the problem size; their sample size is pro-portional to ( mn ) 2 / 3 . (Clarkson, 2005) showed that, with proper conditioning, relative-error approximate solutions can be obtained from row norm-based sam-pling; and (Dasgupta et al., 2009) extended these subspace-preserving sampling schemes to ` p regres-sion, for p  X  [1 ,  X  ), thereby obtaining relative-error approximations. (Sohler &amp; Woodruff, 2011) proved that a Cauchy Transform can be used for ` 1 condi-tioning and thus ` 1 regression in O ( mn 2 log n ) time; this was improved to O ( mn log n ) time with the Fast Cauchy Transform by (Clarkson et al., 2013), who also developed an ellipsoidal rounding algorithm (see Lemma 1 below) and used it and a fast random pro-jection to construct a fast single-pass conditioning al-gorithm (upon which our Algorithm 1 below is based). (Clarkson &amp; Woodruff, 2013) and (Meng &amp; Mahoney, 2013) show that both ` 2 regression and ` p regression can be solved in input-sparsity time via subspace-preserving sampling. The large body of work on fast randomized algorithms for ` 2 regression (and related) problems has been reviewed recently (Mahoney, 2011). To obtain a (1 + )-approximate solution in relative scale, the sample sizes required by all these algorithms are all proportional to 1 / 2 , which limits sampling al-gorithms to  X  X ow precision X , e.g. ,  X  10  X  2 , solutions. By using the output of the sampling/projection step as a preconditioner for a traditional iterative method, thereby leading to an O (log(1 / )) dependence, this problem has been overcome for ` 2 regression (Ma-honey, 2011). For ` 1 regression, the O (1 / 2 ) conver-gence rate of the subgradient method of (Clarkson, 2005) was improved by (Nesterov, 2009), who showed that, with a smoothing technique, the number of it-erations can be reduced to O (1 / ). Interestingly (and as we will return to in Section 3.3), the rarely-used el-lipsoid method (see (Gr  X otschel et al., 1981)) as well as IPCPMs (see (Mitchell, 2003)) can solve general con-vex problems and converge in O (log(1 / )) iterations X  with extra poly( n ) work per iteration.
 More generally, there has been a lot of interest re-cently in distributed machine learning computations. For example, (Daum  X e et al., 2012) describes effi-cient protocols for distributed classification and op-timization; (Balcan et al., 2012) analyzes communi-cation complexity and privacy aspects of distributed learning; (Mackey et al., 2011) adopts a divide-and-conquer approach to matrix factorization such as CUR decompositions; and (Zhang et al., 2012) develop communication-efficient algorithms for statistical opti-mization. Algorithms for these and other problems can be analyzed in models for MapReduce (Karloff et al., 2010; Goodrich, 2010; Feldman et al., 2010); and work on parallel and distributed approaches to scaling up machine learning has been reviewed recently (Bekker-man et al., 2011). In the remainder of this paper, we use the following formulation of the ` p regression problem: This formulation of ` p regression, which consists of a homogeneous objective and an affine constraint, can be shown to be equivalent to the formulation of (1). 5 Denote the feasible region by  X  = { x  X  R n | c T x = 1 } , where recall that we are interested in the case when m n . Let X  X  be the set of all optimal solutions to (3) and x  X  be an arbitrary optimal solution. Then, let f ( x ) = k Ax k p , f  X  = k Ax  X  k p , and let where ([ Ax ] p  X  1 ) i = sign( a T i x ) | a T i x | p  X  1 i -th row of A , i = 1 ,...,m . Note that g ( x ) T x = f ( x ). For simplicity, we assume that A has full column rank and c 6 = 0 . Our assumptions imply that X  X  is a nonempty and bounded convex set and f  X  &gt; 0. Thus, given an &gt; 0, our goal is to find an  X  x  X   X  that is a (1 + )-approximate solution to (3) in relative scale, i.e. , such that f ( X  x ) &lt; (1 + ) f  X  .
 As with ` 2 regression, ` p regression problems are easier to solve when they are well-conditioned. The ` p -norm condition number of A , denoted  X  p ( A ), is defined as: where  X  This implies  X  min p ( A ) k x k 2  X k Ax k p  X   X  max p ( A ) k x k 2 ,  X  x  X  We use  X  p ,  X  min p , and  X  max p for simplicity when the underlying matrix is clear. The element-wise ` p -norm of A is denoted by k A k p . We use E ( d,E ) = { x  X  R n | x = d + Ez, k z k 2 = 1 } to describe an ellipsoid where E  X  R n  X  n is a non-singular matrix. The volume of a full-dimensional ellipsoid E is denoted by |E| . We use S ( S,t ) = { x  X  R n | Sx  X  t } to describe a polytope, where S  X  R s  X  n and t  X  R s for some s  X  n + 1. Given an ` p regression problem, its condition number is generally unknown and can be arbitrarily large; and thus one needs to run a conditioning algorithm before randomly sampling and iteratively solving. Given any non-singular matrix E  X  R n , let y  X  be an optimal solution to the following problem: This problem is equivalent to (3), in that we have x  X  = Ey  X   X  X  X  , but the condition number asso-ciated with (4) is  X  p ( AE ), instead of  X  p ( A ). So, the conditioning algorithm amounts to finding a non-singular matrix E  X  R n such that  X  p ( AE ) is small. One approach to conditioning is via ellipsoidal round-ing. In this paper, we will modify the following result from (Clarkson et al., 2013) to compute a fast ellip-soidal rounding.
 Lemma 1 ((Clarkson et al., 2013)) . Given A  X  R m  X  n with full column rank and p  X  [1 , 2) , it takes at most O ( mn 3 log m ) time to find a non-singular matrix E  X  R Finally, we call a work online if it is executed on MapReduce, and offline otherwise. An online work deals with large-scale data stored on secondary storage but the work can be well distributed on MapReduce; an offline work deals with data stored on RAM. In this section, we will describe our main algorithm for ` p regression on MapReduce. 3.1 Single-pass Conditioning Algorithm The algorithm of Lemma 1 for computing a 2 n -rounding is not immediately-applicable to large-scale ` p regression problems, since each call to the oracle requires a pass to the data. 6 We can group n calls to-gether within a single pass, but we would still need O ( n log m ) passes. Here, we present a determinis-tic single-pass conditioning algorithm that balances the cost-performance trade-off to provide a 2 n 2 /p conditioning of A . See Algorithm 1. Our main result for Algorithm 1 is given in the following lemma. Lemma 2. Algorithm 1 is a 2 n 2 /p -conditioning algo-rithm and it runs in O (( mn 2 + n 4 ) log m ) time. Algorithm 1 A single-pass conditioning algorithm. Input: A  X  R m  X  n with full column rank &amp; p  X  [1 , 2). Output: A non-singular matrix E  X  R n  X  n such that 1: Partition A along its rows into sub-matrices of size 2: For each A i , compute its economy-sized singular 3: Let  X  A i =  X  i V T i for i = 1 ,...,M , 4: Compute  X  A  X  X  SVD:  X  A =  X  U  X   X   X  V T . 6: With the algorithm of Lemma 1, compute an el-7: Return E .
 Proof. The idea is to use block-wise reduction in ` 2 -norm and apply fast rounding to a small problem. The tool we need is simply the equivalence of vector norms. Let C = { x  X  R n |k Ax k p  X  1 } , which is convex, full-dimensional, bounded, and centrally symmetric. Adopting notation from Algorithm 1, we first have because for all x  X  R n , k Ax k p p = and k Ax k p p = Next we prove that E 0 gives an ( Mn 2 ) 1 /p  X  1 / 2 -rounding of  X  C . For all x  X  R n , we have X and for all x  X  R n and hence E 0 gives an ( Mn 2 ) 1 /p  X  1 / 2 any 2 n -rounding of  X  C is a 2 n  X  n 2 /p  X  1 = 2 n 2 /p of C . Therefore, Algorithm 1 computes a 2 n 2 /p -conditioning of A . Note that the rounding procedure is applied to a problem of size Mn  X  n  X  m/n  X  n . There-fore, Algorithm 1 only needs a single pass through the data, with O ( mn 2 ) FLOPs and an offline work of O (( mn 2 + n 4 ) log m ) FLOPs. The offline work re-quires m RAM, which might be too much for large-scale problems. In such cases, we can increase the block size from n 2 to, for example, n 3 . This gives us a 2 n 3 /p  X  1 / 2 -conditioning algorithm that only needs m/n offline RAM and O (( mn + n 4 ) log m ) offline FLOPs. The proof follows similar arguments.
 See Table 1 for a comparison of the results of Algo-rithm 1 and Lemma 2 with prior work on ` 1 norm conditioning (and note that some of these results, e.g. , those of (Clarkson et al., 2013) and (Meng &amp; Mahoney, 2013), have extensions that apply to ` p -norm conditioning). Although the Cauchy Trans-form (Sohler &amp; Woodruff, 2011) and the Fast Cauchy Transform (Clarkson et al., 2013) are independent of A and require little offline work, there are several con-cerns with using them in our application. First, the constants hidden in  X  1 are not explicitly given, and they may be too large for practical use, especially when n is small. Second, although random sampling algo-rithms do not require  X  min p and  X  max p as inputs, some algorithms, e.g. , IPCPMs, need accurate bounds of them. Third, these transforms are randomized algo-rithms that fail with certain probability. Although we can repeat trials to make the failure rate arbitrarily small, we don X  X  have a simple way to check whether or not any given trial succeeds. Finally, although the online work in Algorithm 1 remains O ( mn 2 ), it is em-barrassingly parallel and can be well distributed on MapReduce. For large-scale strongly over-determined problems, Algorithm 1 with block size n 3 seems to be a good compromise in practice. This guarantees 2 n 3 /p  X  1 / 2 -conditioning, and the O ( mn 2 ) online work can be easily distributed on MapReduce. 3.2 Single-pass Random Sampling Here, we describe our method for implementing the subspace-sampling procedure with map and reduce functions. Suppose that after conditioning we have  X  p ( A ) = 1 and  X  p ( A ) = poly( n ). (Here, we use A instead of AE for simplicity.) Then the following method of (Dasgupta et al., 2009) can be used to per-form subspace-preserving sampling.
 Lemma 3 ((Dasgupta et al., 2009)) . Given A  X  R m  X  n that is (  X , X ,p ) -conditioned 7 and an error parameter &lt; 1 7 , let r  X  16(2 p + 2)(  X  X  ) p ( n log 12 + log 2 and let S  X  R m  X  m be a diagonal  X  X ampling matrix, X  with random entries: where the importance sampling probabilities Then, with probability at least 1  X   X  , the following holds for all x  X  R n , This subspace-preserving sampling lemma can be used, with the formulation (3), to obtain a relative-error ap-proximation to the ` p regression problem, the proof of which is immediate.
 Lemma 4 ((Clarkson et al., 2013)) . Let S be con-structed as in Lemma 3, and let  X  x be the optimal so-lution to the subsampled problem: Then with probability at least 1  X   X  ,  X  x is a 1+ 1  X  approximate solution to (3) .
 It is straightforward to implement this algorithm in MapReduce in a single pass. This is presented in Al-gorithm 2. Importantly, note that more than one sub-sampled solution can be obtained in a single pass. This translates to a higher precision or a lower failure rate; and, as described in Section 3.3, it can also be used to construct a better initialization.
 Several practical points are worth noting. First, n X  p p an upper bound of k A k p p , which makes the actual sam-ple size likely to be smaller than r . For better control on the sample size, we can compute k A k p p directly via one pass over A prior to sampling, or we can set a Algorithm 2 A single-pass sampling algorithm.
 Input: A  X  R m  X  n with  X  max p ( A ) =  X  p , c  X  R n , a Output: N approximate solutions:  X  x k , k = 1 ,...,N . 1: function mapper ( a : a row of A ) 2: Let p = min { r k a k p p / ( n X  p p ) , 1 } . 3: Emit ( k,a/p ) with probability p , k = 1 ,...,N . 4: end function 5: function reducer ( k , { a i } ) 6: Assemble A k from { a i } . 8: Emit ( k,  X  x k ). 9: end function big r in mappers and discard rows at random in re-ducers if the actual sample size is too big. Second, in practice, it is hard to accept as an input and deter-mine the sample size r based on Lemma 3. Instead, we choose r directly based on our hardware capacity and running time requirements. For example, suppose we use a standard primal-dual path-following algorithm (see (Nesterov &amp; Nemirovsky, 1994)) to solve subsam-pled problems. Then, since each problem needs O ( rn ) RAM and O ( r 3 / 2 n 2 log r ) running time for a (1 + )-approximate solution, where r is the sample size, this should dictate the choice of r . Similar considerations apply to the use of the ellipsoid method or IPCPMs. 3.3 A Randomized IPCPM Algorithm A problem with a vanilla application of the subspace-preserving random sampling algorithm is accuracy: it is very efficient if we only need one or two accurate digits (see (Clarkson et al., 2013) for details), but if we are looking for  X  X oderate-precision X  solutions, e.g. , those with  X  10  X  5 , then we very quickly be lim-ited by the O (1 / 2 ) sample size required by Lemma 3. For example, setting p = 1, n = 10,  X  X  = 1000, and = 10  X  3 into Lemma 3, we get a sample size of approximately 10 12 , which as a practical matter is certainly intractable for a  X  X ubsampled X  problem. In this section, we will describe an algorithm with a O (log(1 / )) dependence, which is thus appropriate for computing moderate-precision solutions. This algo-rithm will be a randomized IPCPM with several fea-tures specially-designed for MapReduce. In particu-lar, the algorithm will take advantage of the multiple subsampled solutions and the parallelizability of the MapReduce framework.
 As background, recall that IPCPMs are similar to the bisection method but work in a high dimensional space. An IPCPM requires a polytope S 0 that is known to contain a full-dimensional ball B of desired solutions described by a separation oracle. At step k , a query point x k  X  int S k is sent to the oracle. If the query point is not a desired solution, the oracle returns a half space K k which contains B but not x k , and then we set S k +1 = S k  X  K k and continue. If x k is chosen such that |S k +1 | / |S k |  X   X ,  X  k for some  X  &lt; 1, then the IPCPM converges geometrically. Such a choice of x k was first given by (Levin, 1965), who used (but did not provide a way to compute) the cen-ter of gravity of S k . (Tarasov et al., 1988) proved that the center of the maximal-volume inscribed ellipsoid also works; (Vaidya, 1996) showed the volumetric cen-ter works, but he didn X  X  give an explicit bound; and (Bertsimas &amp; Vempala, 2004) suggest approximating the center of gravity by random walks, e.g. , the hit-and-run algorithm (Lov  X asz, 1999). Table 2 compares IPCPMs with other iterative methods on ` p regression problems. Although they require extra work at each iteration, IPCPMs converge in the fewest number of iterations. 8 For completeness, we will first describe a standard IPCPM approach to ` p regression; and then we will describe the modifications we made to make it work in MapReduce. Assume that  X  min p ( A ) = 1 and  X  p ( A ) = poly( n ). Let  X  f always denote the best objective value we have obtained. Then for any x  X  R n , by convexity, This subgradient gives us the separation oracle. Let x 0 be the minimal ` 2 -norm point in  X , in which case and hence x 0 is a  X  p -approximate solution. Moreover, Algorithm 3 A randomized IPCPM Input: A  X  R m  X  n with  X  min p ( A )  X  1, c  X  R n , a set of Output: An approximate solution  X  x . 1: Choose K = O ( n ). 2: Compute ( f ( x ) ,g ( x )) for each initial point x . 3: Let  X  f = f ( X  x ) always denote the best we have. 4: for i=0,. . . ,M-1 do 5: Construct S i from known ( f,g ) pairs and  X  f . 9: end for 10: Return  X  x . which defines the initial polytope S 0 . Given &gt; 0, for any x  X  X  = { x  X   X  |k x  X  x  X  k 2  X  k Ax 0 k p / X  2 p } , k Ax k p  X  X  Ax  X  k p  X k A ( x  X  x  X  ) k p  X   X  p k x  X  x  X  So all points in B are (1 + )-approximate solu-tions. The number of iterations to reach a (1 + )-approximation is This leads to an O (( mn 2 + poly( n )) log( n/ ))-time al-gorithm, which is better than sampling when is very small. Note that we will actually apply the IPCPM in a coordinate system defined on  X , where the mappings from and to the coordinate system of R n are given by Householder transforms; we omit the details.
 Our randomized IPCPM for use on MapReduce, which is given in Algorithm 3, differs from the standard ap-proach just described in two aspects: sampling initial-ization; and multiple queries per iteration. In both cases, we take important advantage of the peculiar properties of the MapReduce framework.
 For the initialization, note that constructing S 0 from x 0 may not be a good choice since we can only guar-antee  X  p = poly( n ). Recall, however, that we actually have N subsampled solutions from Algorithm 2, and all of these solutions can be used to construct a better S . Thus, we first compute  X  f k = f ( X  x k ) and  X  g k = g ( X  x for k = 1 ,...,N in a single pass. For each  X  x k , we de-fine a polytope containing x  X  using (6) and k x  X   X   X  x k k  X   X k A ( x  X   X   X  x k ) k p  X  f  X  +  X  f k  X  We then merge all these polytopes to construct S 0 which is described by 2 n + N constraints. Note also that it would be hard to use all the available approxi-mate solutions if we chose to iterate with a subgradient or gradient method.
 For the iteration, the question is which query point we send at each step. Here, instead of one query, we send multiple queries. Recall that, for a data intensive job, the dominant cost is the cost of input/output, and hence we want to extract as much information as pos-sible for each pass. Take an example of one of our runs on a 10-node Hadoop cluster: with a matrix A of size 10 8  X  50, then a pass with a single query took 282 seconds, while a pass with 100 queries only took 328 seconds X  X o the extra 99 queries come almost  X  X or free. X  To generate these multiple queries, we follow the random walk approach proposed by (Bertsimas &amp; Vempala, 2004). The purpose of the random walk is to generate uniformly distributed points in S k such that we can estimate the center of gravity. Instead of com-puting one estimate, we compute multiple estimates. We conclude our discussion of our randomized IPCPM algorithm with a few comments.  X  The online work of computing ( f,g ) pairs and the  X  The way we choose query points works well in  X  Sending multiple queries makes the number of The computations are performed on a Hadoop cluster with 40 CPU cores. We used the ` 1 regression test problem from (Clarkson et al., 2013). The problem is of size 5 . 24 e 9  X  15, generated in the following way:  X  The true signal x  X  is a standard Gaussian vector.  X  Each row of the design matrix A is a canonical  X  The response vector b is given by b Since the problem is separable, we know that an opti-mal solution is simply given by the median of responses corresponding to each entry. If we use ` 2 regression, the optimal solution is given by the mean values, which is inaccurate due to corrupted measurements.
 We first check the accuracy of subsampled solutions. We implement Algorithm 1 with block size n 3 (ALG1), which gives 2 n 5 / 2 -conditioning; and the Cauchy trans-form (CT) by (Sohler &amp; Woodruff, 2011), which gives asymptotic O ( n 3 / 2 log 3 / 2 n )-conditioning; and then we use Algorithm 2 to compute 100 subsampled solutions in a single pass. We compute k AE k 1 explicitly prior to sampling for a better control on the sample size. We choose r = 100000 in Algorithm 2. We also imple-ment Algorithm 2 without conditioning (NOCD) and uniform sampling (UNIF) for comparison. The 1st and the 3rd quartiles of the relative errors in 1-, 2-, and  X  -norms are shown in Table 3. ALG1 clearly performs the best, achieving 0 . 01 relative error in all the metrics we use. CT has better asymptotic conditioning qual-ity than ALG1 in theory, but it doesn X  X  generate better solutions in this test. This confirms our concerns on the hidden constant in  X  1 and the failure probability. UNIF works but it is about a magnitude worse than ALG1. NOCD generates large errors. So both UNIF and NOCD are not reliable approaches.
 Next we try to iteratively improve the subsampled so-lutions using Algorithm 3. We implement and compare the proposed IPCPM with a standard IPCPM based on random walks with single point initialization and single query per iteration. We set the number of it-erations to 30. The running times for each of them are approximately the same. Figure 1 shows the con-vergence behavior in terms of relative error in objec-tive value. IPCPMs are not monotonically decreasing algorithms. Hence we see even we begin with a 10-approximate solution with the standard IPCPM, the error goes to 10 3 after a few iterations and the ini-tial guess is not improved in 30 iterations. The sam-pling initialization helps create a small initial search region; this makes the proposed IPCPM begin at a 10  X  2 -approximate solution, stay below that level, and reach 10  X  6 in only 30 iterations. Moreover, it is easy to see that the multiple-query strategy improves the rate of convergence, though still at a linear rate. We have proposed an algorithm for solving strongly over-determined ` p regression problems, for p  X  [1 , 2), with an emphasis on its theoretical and empirical prop-erties for p = 1. Although some of the building blocks of our algorithm are not better than state-of-the-art al-gorithms in terms of FLOP counts, we have shown that our algorithm has superior communication properties that permit it to be implemented in MapReduce and applied to terabyte-scale data to obtain a  X  X oderate-precision X  solution in only a few passes. The proposed method can also be extended to solving more general convex problems on MapReduce.
 Most of the work was done while the first author was at ICME, Stanford University supported by NSF DMS-1009005. The authors would like to thank Suresh Venkatasubramanian for helpful discussion and for bringing to our attention several helpful references.
