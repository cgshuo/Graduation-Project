 Will that berry taste good? Is that table strong enough to sit on? Predicting whether an object has an unobserved property is among the most basic of all inductive problems. Many kinds of knowledge appear to be relevant: different researchers emphasize the role of causal knowledge, similarity, cate-gory judgments, associations, analogical mappings, scrip ts, and intuitive theories, and each of these approaches accounts for an important subset of everyday inf erences. Taken in isolation, however, each of these approaches is fundamentally limited. Humans d raw on multiple kinds of knowledge and integrate them flexibly when required, and eventually ou r models should attempt to match this ability [1]. As an initial step towards this goal, we present a model of inductive reasoning that is sensitive both to causal relationships between properti es and to similarity relationships between objects.
 The inductive problem we consider can be formalized as the pr oblem of filling in missing entries in an object-property matrix (Figure 1). Previous accounts of inductive reasoning generally address some version of this problem. Models of causal reasoning [2] usually focus on relationships be-tween properties (Figure 1a): if animal A has wings, for instance, it is likely that animal A can fly. Similarity-based models [3, 4, 5] usually focus on relation ships between objects (Figure 1b): if a duck carries gene X , a goose is probably more likely than a pig to carry the same ge ne. Previous models, however, cannot account for inferences that rely on similarity and causality: if a duck car-ries gene X and gene X causes enzyme Y to be expressed, it is likely that a goose expresses enzyme Y (Figure 1c). We develop a unifying model that handles infere nces like this, and that subsumes previous probabilistic approaches to causal reasoning [2] and similarity-based reasoning [5, 6]. Our formal framework overcomes some serious limitations of the two approaches it subsumes. Ap-proaches that rely on causal graphical models typically ass ume that the feature vectors of any two objects (any two rows of the matrix in Figure 1a) are conditio nally independent given a causal net-work over the features. Suppose, for example, that the rows o f the matrix correspond to people and the causal network states that smoking leads to lung cancer w ith probability 0 . 3 . Suppose that Tim, Tom and Zach are smokers, that Tim and Tom are identical twins , and that Tim has lung cancer. The assumption of conditional independence implies that Tom an d Zach are equally likely to suffer from long cancer, a conclusion that seems unsatisfactory. The as sumption is false because of variables that are unknown but causally relevant X  X ariables capturing unknown biological and environmen-tal factors that mediate the relationship between smoking a nd disease. Dealing with these unknown Figure 1: (a) Models of causal reasoning generally assume th at the rows of an object-feature matrix are conditionally independent given a causal structure ove r the features. These models are often used to make predictions about unobserved features of novel objects. (b) Models of similarity-based reasoning generally assume that columns of the matrix are conditionally independent given a similarity structure over the objects. These models are oft en used to make predictions about novel features. (c) We develop a generative model for object-feat ure matrices that incorporates causal relationships between features and similarity relationsh ips between objects. The model uses both kinds of information to make predictions about matrices wit h missing entries. variables is difficult, but we suggest that knowledge about s imilarity between objects can help. Since Tim is more similar to Tom than Zach, our model correctly pred icts that Tom is more likely to have lung cancer than Zach.
 Previous models of similarity-based reasoning [5, 6] also s uffer from a restrictive assumption of conditional independence. This time the assumption states that features (columns of the matrix in Figure 1b) are conditionally independent given informatio n about the similarity between objects. Empirical tests of similarity-based models often attempt t o satisfy this assumption by using blank properties  X  X ubjects, for example, might be told that coyotes have prope rty P, and asked to judge the probability that foxes have property P [3]. To a first appr oximation, inferences in tasks like this conform to judgments of similarity: subjects conclude, for example, that foxes are more likely to have property P than mice, since coyotes are more similar to f oxes than mice. People, however, find it natural to reason about properties that are linked to f amiliar properties, and that therefore violate the assumption of conditional independence. Suppo se, for instance, you learn that desert foxes have skin that is resistant to sunburn. It now seems tha t desert rats are more likely to share this property than arctic foxes, even though desert foxes are mor e similar in general to arctic foxes than to desert rats. Our model captures inferences like this by in corporating causal relationships between properties: in this case, having sunburn-resistant skin is linked to the property of living in the desert. Limiting assumptions of conditional independence can be av oided by specifying a joint distribution on an entire object-property matrix. Our model uses a distri bution that is sensitive both to causal relationships between properties and to similarity relati onships between objects. We know of no previous models that attempt to combine causality and simil arity, and one set of experiments that has been taken to suggest that people find it difficult to combi ne these sources of information [7]. After introducing our model, we present two experiments des igned to test it. The results suggest that people are able to combine causality with similarity, a nd that our model accounts well for this capacity. Consider first a probabilistic approach to similarity-base d reasoning. Assume that S o is an object structure: a graphical model that captures relationships b etween a known set of objects (Figure 1b). Suppose, for instance, that the objects include a mouse, a ra t, a squirrel and a sheep ( o 1 through o 4 ). S o can be viewed as a graphical model that captures phylogeneti c relationships, or as a formalization of the intuitive similarity between these animals. Given so me feature of interest, the feature values for all objects can be collected into an object vector v o and S o specifies a distribution P ( v o ) on these vectors.
 We work with the case where ( S o ,  X  ) is a tree-structured graphical model of the sort previously used by methods for Bayesian phylogenetics [8] and cognitive mod els of property induction [5, 6]. The objects lie at the leaves of the tree, and we assume that objec t vectors are binary vectors generated by a mutation process over the tree. This process has a parame ter,  X  , that represents the base rate of a novel feature X  X he expected proportion of objects with that feature. For instance, if  X  is low, the model ( S o ,  X  ) will predict that a novel feature will probably not be found i n any of the animals, but if the feature does occur in exactly two of the animals, the mo use and the rat are a more likely pair than the mouse and the sheep.
 The mutation process can be formalized as a continuous-time Markov process with two states (off and on) and with infinitesimal matrix: We can generate object vectors from this model by imagining a binary feature spreading out over the tree from root to leaves. The feature is on at the root with pro bability  X  , and the feature may switch states at any point along any branch. The parameter  X  determines how easy it is to move between the on state and the off state. If  X  is high, it will be easy for the Markov process to enter the on s tate, and difficult for it to leave once it is there.
 Consider now a probabilistic approach to causal reasoning. Assume that S f is a feature structure: a graphical model that captures relationships between a kno wn set of features (Figure 1a). The features, for instance, may correspond to enzymes, and S f may capture the causal relationships between these enzymes. One possible structure states that e nzyme f 1 is involved in the production of enzyme f 2 , which is in turn involved in the production of enzyme f 3 . The feature values for any given object can be collected into a feature vector v f and S f specifies a distribution P ( v f ) on these vectors.
 Suppose now that we are interested in a model that combines th e knowledge represented by S f and S o (Figure 1c). Given that the mouse expresses enzyme f 1 , for instance, a combined model should predict that rats are more likely than squirrels to express e nzyme f 2 . Formally, we seek a distribution P ( M ) , where M is an object-feature matrix, and P ( M ) is sensitive to both the relationships between features and the relationships between animals. Given this distribution, Bayesian inference can be used to make predictions about the missing entries in a parti ally observed matrix.
 If the features in S f happen to be independent (Figure 1b), we can assume that colu mn i of the matrix is generated by ( S o ,  X  i ) , where  X  i is the base rate of f i . Consider then the case where S f captures causal relationships between the features (Figur e 1c). These causal relationships will typically depend on several hidden variables. Causal relat ionships between enzymes, for instance, are likely to depend on other biological variables, and the c ausal link between smoking and lung cancer is mediated by many genetic and environmental variab les. Often little is known about these hidden variables, but to a first approximation we can assume t hat they respect the similarity structure S . In Figure 1c, for example, the unknown variables that media te the relationship between f 1 and f are more likely to take the same value in o 1 and o 2 than in o 1 and o 4 .
 We formalize these intuitions by converting a probabilisti c model S f (Figure 2a) into an equivalent model S D f (Figure 2b) that uses a deterministic combination of indepe ndent random events. These random events will include hidden but causally relevant var iables. In Figure 2b, for example, the model S D f indicates that the effect e is deterministically present if the cause c is present and the transmission mechanism t is active, or if there is a background cause b that activates e . The model S f is equivalent to S f in the sense that both models induce the same distribution ov er the variables that appear in S f . In general there will be many models S D f that meet this condition, and there are algorithms which convert S f into one of these models [9]. For some applications it might b e desirable to integrate over all of these models, but here we a ttempt to choose the simplest X  X he model S D f with the fewest variables.
 Given a commitment to a specific deterministic model, we assu me that the root variables in S D f are independently generated over S o . More precisely, suppose that the base rate of the i th variable in S D f is  X  i . The distribution P ( M ) we seek must meet two conditions (note that each candidate ma trix M now has a column for each variable in S D f ). First, the marginal distribution on each row must match Figure 2: (a) A graphical model S f that captures a probabilistic relationship between a cause c and an effect e . (b) A deterministic model S D f that induces the same joint distribution over c and e . t indicates whether the mechanism of causal transmission bet ween c and e is active, and b indicates whether e is true owing to a background cause independent of c . All of the root variables ( c , t and b ) are independent, and the remaining variables ( e ) are deterministically specified once the root variables are fixed. (c) A graphical model created by com bining S D f with a tree-structured representation of the similarity between three objects. Th e root variables in S D f ( c , t , and b ) are independently generated over the tree. Note that the arrows on the edges of the combined model have been suppressed. column i must match the distribution specified by ( S o ,  X  i ) .
 There is precisely one distribution P ( M ) that satisfies these conditions, and we can represent it using a graphical model that we call the combined model. Supp ose that there are n objects in S o . To create the combined model, we first introduce n copies of S D f . For each root variable i in S D f , we now connect all copies of variable i according to the structure of S o (Figure 2c). The resulting graph provides the topology of the combined model, and the conditi onal probability distributions (CPDs) are inherited from S o and S D f . Each node that belongs to the i th copy of S o inherits a CPD from ( S o ,  X  i ) , and all remaining nodes inherit a (deterministic) CPD from S f . Now that the distribution P ( M ) is represented as a graphical model, standard inference tec hniques can be used to compute the missing entries in a partially-observed matrix M . All results in this paper were computed using the implementation of the junction tree algorithm included in the Bayes Net toolbox [10]. When making inductive inferences, a rational agent should ex ploit all of the information avail-able, including causal relationships between features and similarity relationships between objects. Whether humans are able to meet this normative standard is not clear, and almost certainly varies from task to task. On one hand, there are motivating examples like the case of the three smokers where it seems natural to think about causal relationships a nd similarity relationships at the same time. On the other hand, Rehder [7] argues that causal inform ation tends to overwhelm similarity in-formation, and supports this conclusion with data from seve ral tasks involving artificial categories. To help resolve these competing views, we designed several t asks where subjects were required to simultaneously reason about causal relationships betwe en enzymes and similarity relationships between animals. 3.1 Experiment 1 Materials and Methods. 16 adults participated in this experiment. Subjects were as ked to reason about the presence of enzymes in a set of four animals: a mouse , a rat, a sheep, and a squirrel. Each subject was trained on two causal structures, each of wh ich involved three enzymes. Pseudo-biological names like  X  X exotase X  were used in the experimen t, but here we will call the enzymes f , f 2 and f 3 . In the chain condition, subjects were told that f 3 is known to be produced by several Figure 3: Experiment 1: Behavioral data (column 1) and predi ctions for three models. (a) Results for the chain condition. Known test results are marked with a rrows: in task 1, subjects were told only that the mouse had tested positive for f 1 , and in task 2 they were told in addition that the rat had tested negative for f 2 . Error bars represent the standard error of the mean. (b) Res ults for the common-effect condition. pathways, and that the most common pathway begins with f 1 , which stimulates production of f 2 , which in turn leads to the production of f 3 . In the common-effect condition, subjects were told that f 3 is known to be produced by several pathways, and that one of th e most common pathways involves f 1 and the other involves f 2 .
 To reinforce each causal structure, subjects were shown 20 c ards representing animals from twenty different mammal species (names of the species were not supp lied). The card for each animal represented whether that animal had tested positive for eac h of the three enzymes. The cards were chosen to be representative of the distribution captured by a causal network with known structure (chain or common-effect) and known parameterization. In th e chain condition, for example, the network was a noisy-or network with the form of a chain, where leak probabilities were set to 0.4 ( f 1 ) or 0.3 ( f 2 and f 3 ), and the probability that each causal link was active was se t to 0.7. After subjects had studied the cards for as long as they liked, the c ards were removed and subjects were asked several questions about the enzymes (e.g.  X  X ou learn a bout a new mammal X  X ow likely is it that the mammal produces f 3 ? X ) The questions in this training phase were intended to enc ourage subjects to reflect on the causal relationships between the e nzymes.
 In both conditions, subjects were told that they would be tes ting the four animals (mouse, rat, sheep and squirrel) for each of the three enzymes. Each condition i ncluded two tasks. In the chain condi-tion, subjects were told that the mouse had tested positive f or f 1 , and asked to predict the outcome of each remaining test (Figure 1c). Subjects were then told i n addition that the rat had tested neg-ative for f 2 , and again asked to predict the outcome of each remaining tes t. Note that this second task requires subjects to integrate causal reasoning with s imilarity-based reasoning: causal reason-ing predicts that the mouse has f 2 , and similarity-based reasoning predicts that it does not. In the common-effect condition, subjects were told that the mouse had tested positive for f 3 , then told in addition that the rat had tested negative for f 2 . Ratings were provided on a scale from 0 (very likely to test negative) to 100 (very likely to test positive).
 Results. Subjects used the 100 point scale very differently: in task 1 of each condition, some subjects chose numbers between 80 and 100, and others chose n umbers between 0 and 100. We therefore converted each set of ratings to z-scores. Averag e z-scores are shown in the first column of Figure 3, and the remaining columns show predictions for s everal models. In each case, model predictions have been converted from probabilities to z-sc ores to allow a direct comparison with the human data.
 Our combined model uses a tree over the four animals and a caus al network over the features. We used the tree shown in Figure 1b, where objects o 1 through o 4 correspond to the mouse, the rat, the squirrel and the sheep. The tree component of our model has on e free-parameter X  X he total path length of the tree. The smaller the path length, the more like ly that all four animals have the same feature values, and the greater the path length, the more lik ely that distant animals in the tree (e.g. the mouse and the sheep) will have different feature values. All results reported here use the same value of this parameter X  X he value that maximizes the average correlation achieved by our model across Experiments 1 and 2. The causal component of our model includes no free parameters, since we used the parameters of the network that generated the card s shown to subjects during the training phase. Comparing the first two columns of Figure 3, we see that our combined model accounts well for the human data.
 Columns 3 and 4 of Figure 3 show model predictions when we remo ve the similarity component (column 3) or the causal component (column 4) from our combin ed model. The model that uses the causal network alone is described by [2], among others, and t he model that uses the tree alone is described by [6]. Both of these models miss qualitative tren ds evident in the human data. In task 1 of each condition, the causal model makes identical predic tions about the rat, the squirrel and the sheep: in task 1 of the chain condition, for example, it canno t use the similarity between the mouse and the rat to predict that the rat is also likely to test posit ive for f 1 . In task 1 of each condition the similarity model predicts that the unobserved features ( f 2 and f 3 for the chain condition, and f 1 and f for the common-effect condition) are distributed identica lly across the four animals. In task 1 of the chain condition, for example, the similarity model does not predict that the mouse is more likely than the sheep to test positive for f 2 and f 3 .
 The limitations of the causal and similarity models suggest that some combination of causality and similarity is necessary to account for our data. There are li kely to be approaches other than our combined model that account well for our data, but we suggest that accurate predictions will only be achieved when the causal network and the similarity infor mation are tightly integrated. Simply averaging the predictions for the causal model and the simil arity model will not suffice: in task 1 of the chain condition, for example, both of these models predi ct that the rat and the sheep are equally likely to test positive for f 2 , and computing an average across these models will result in the same prediction. 3.2 Experiment 2 Our working hypothesis is that similarity and causality sho uld be combined in most contexts. An alternative hypothesis X  X he root-variables hypothesis X  X as suggested to us by Bob Rehder, and states that similarity relationships are used only if some o f the root variables in a causal structure S f are unobserved. For instance, similarity might have influen ced inferences in the chain condition of Experiment 1 only because the root variable f 1 was never observed for all four animals. The root-variables hypothesis should be correct in cases wh ere all root variables in the true causal structure are known. In Figure 2c, for instance, similarity no longer plays a role once the root vari-ables are observed, since the remaining variables are deter ministically specified. We are interested, however, in cases where S f may not contain all of the causally relevant variables, and w here simi-larity can help to make predictions about the effects of unob served variables. Consider, for example, the case of the three smokers, where S f states that smoking causes lung cancer. Even though the root variable is observed for Tim, Tom and Zach (all three are smokers), we still believe that Tom is more likely to suffer from lung cancer than Zach having disco vered that Tim has lung cancer. The case of the three smokers therefore provides intuitive evid ence against the root-variables hypothesis, and we designed a related experiment to explore this hypothe sis empirically.
 Materials and Methods. Experiment 2 was similar to Experiment 1, except that the com mon-effect condition was replaced by a common-cause condition. In the fi rst task for each condition, subjects were told only that the mouse had tested positive for f 1 . In the second task, subjects were told in addition that the rat, the squirrel and the sheep had tested p ositive for f 1 , and that the mouse had Figure 4: Experiment 2: Behavioral data and predictions for three models. In task 2 of each condi-tion, the root variable in the causal network ( f 1 ) is observed for all four animals. tested negative for f 2 . Note that in the second task, values for the root variable ( f 1 ) were provided for all animals. 18 adults participated in this experiment.
 Results. Figure 4 shows mean z-scores for the subjects and for the four models described previously. The judgments for the first task in each condition replicate t he finding from Experiment 1 that subjects combine causality and similarity when just one of t he 12 animal-feature pairs is observed. The results for the second task rule out the root-variables h ypothesis. In the chain condition, for example, the causal model predicts that the rat and the sheep are equally likely to test positive for f . Subjects predict that the rat is less likely than the sheep t o test positive for f 2 , and our combined model accounts for this prediction. We developed a model of inductive reasoning that is sensitiv e to causal relationships between fea-tures and similarity relationships between objects, and de monstrated in two experiments that it pro-vides a good account of human reasoning. Our model makes thre e contributions. First, it provides an integrated view of two inductive problems X  X ausal reasoning and similarity-based reasoning X  X hat are usually considered separately. Second, unlike previou s accounts of causal reasoning, it acknowl-edges the importance of unknown but causally relevant varia bles, and uses similarity to constrain inferences about the effects of these variables. Third, unl ike previous models of similarity-based reasoning, our model can handle novel properties that are ca usally linked to known properties. For expository convenience we have emphasized the distinct ion between causality and similarity, but the notion of similarity needed by our approach will ofte n have a causal interpretation. A tree-structured taxonomy, for example, is a simple representati on of the causal process that generated biological species X  X he process of evolution. Our combined m odel can therefore be seen as a causal model that takes both relationships between features and ev olutionary relationships between species into account. More generally, our framework can be seen as a m ethod for building sophisticated causal models, and our experiments suggest that these kinds of models will be needed to account for the complexity and subtlety of human causal reasoning.
 Other researchers have proposed strategies for combining p robabilistic models [11], and some of these methods may account well for our data. In particular, t he product of experts approach [12] should lead to predictions that are qualitatively similar t o the predictions of our combined model. Unlike our approach, a product of experts model is not a direc ted graphical model, and does not support predictions about interventions. Neither of our ex periments explored inferences about inter-ventions, but an adequate causal model should be able to hand le inferences of this sort. Causal knowledge and similarity are just two of the many vari eties of knowledge that support induc-tive reasoning. Any single form of knowledge is a worthy topi c of study, but everyday inferences often draw upon multiple kinds of knowledge. We have not prov ided a recipe for combining arbi-trary forms of knowledge, but our work illustrates two gener al themes that may apply quite broadly. First, different generative models may capture different a spects of human knowledge, but all of these models use a common language: the language of probability. P robabilistic models are modular, and can be composed in many different ways to build integrated mo dels of inductive reasoning. Second, the stochastic component of most generative models is in par t an expression of ignorance. Using one model (e.g. a similarity model) to constrain the stochastic component of another model (e.g. a causal network) may be a relatively general method for combining pr obabilistic knowledge representations. Although we have focused on human reasoning, integrated mod els of induction are needed in many scientific fields. Our combined model may find applications in computational biology: predicting whether an organism expresses a certain gene, for example, s hould rely on phylogenetic relation-ships between organisms and causal relationships between g enes. Related models have already been explored: Engelhardt et al. [13] develop an approach to protein function prediction that com-bines phylogenetic relationships between proteins with re lationships between protein functions, and several authors have explored models that combine phylogen ies with hidden Markov models. Com-bining two models is only a small step towards a fully integra ted approach, but probability theory provides a lingua franca for combining many different representations of the world.

