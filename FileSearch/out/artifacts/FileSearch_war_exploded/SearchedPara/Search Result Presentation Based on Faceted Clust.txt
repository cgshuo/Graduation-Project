 We propose a competence partitioning strategy for Web search re-sult presentation: the unmodified head of a ranked result list is combined with a clustering of documents from the result list tail. We identify two principles to which such a clustering must adhere to improve the user X  X  search experience: (1) Avoid the unwanted effect of query aspect repetition, which is called shadowing here. (2) Avoid extreme clusterings, i.e., neither the number of cluster labels nor the number of documents per cluster should exceed the size of the result list head. We present measures to quantify the shadowing effect, and with Faceted Clustering we introduce an al-gorithm that optimizes the identified principles. The key idea of Faceted Clustering is a dynamic, user-controlled reorganization of a clustering, similar to a faceted navigation system. We report on evaluations using the AMBIENT corpus and demonstrate the po-tential of our approach by a comparison with two well-known clus-tering search engines.
 Categories and Subject Descriptors : H.3.3 [Information Stor-age and Retrieval]: Information Search and Retrieval X  Clustering H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval X  Search process Keywords : search result clustering, cluster labeling
Search result presentation strategies can be characterized as ei-ther being relevance-based or diversity-based [12]. While the ob-jective of the relevance-based strategy is to serve the information need that is most likely associated with a submitted query, the ob-jective of the diversity-based strategy is to serve different infor-mation needs in parallel. In this paper we propose to exploit the strengths of both result presentation strategies by combining the head of a ranked result list with a clustering of documents in the tail. This competence partitioning strategy entails fundamental dif-ferences compared to the current practice of Web search result clus-tering engines: Instead of adding an alternative means for search result inspection, we are aiming at a complementing clustering that takes the result list head into account. Our intuition is that users always have a look at the top results first, and turn to a result clus-tering only to avoid skimming through the result list tail. Taking on this perspective allows us to derive two principles for the search result clustering task.
 Principle 1: Avoid Shadowing. Users resort to a clustering of the tail only if their information need is still unsatisfied after having examined the result list head. I.e., it is unlikely that clusters con-taining documents similar to those in the head will improve user satisfaction. We term the undesired effect of topic repetition as shadowing . The shadowing effect becomes severe if the result list head is dominated by a single query aspect. If the head is rather diverse, users might be happy with a clustering that delivers addi-tional resources for these aspects. With respect to the employed clustering algorithm, avoidance of shadowing means to not blindly cluster the results with the highest ranks, but to carefully select re-sults from the tail.
 Principle 2: Avoid Extreme Clusterings. Users who refuse to search the tail of a ranked result list won X  X  spent time on parsing an overlong list of cluster labels either, nor will they skim through the documents in a very large cluster. We hence constrain both the number of cluster labels and the maximum cluster size of a clus-tering to the size of the result list head. Here we take ten as value for both. I.e., our second principle limits the total number of doc-uments that fit into a flat clustering to 100. This number may in-creased by a hierarchical clustering, which, however, comes at the price that valuable search time is spent in unfolding and exploring the hierarchical structure.

With Faceted Clustering, we introduce a clustering algorithm tai-lored to the competence partitioning strategy and the two princi-ples above. The algorithm generates a so-called monothetic clus-tering for a result list: A cluster C l with label l is comprised of ex-actly those documents of a result list which contain l . Monothetic clusterings are preferred over polythetic clusterings in Web search result presentation because the cluster labels can be easily inter-preted: they behave similar to adding an extra term to the original query. In order to comply with Principle 1, our clustering algorithm prefers to select cluster labels which do not appear in the result list head but which occur frequently in the result list tail. In order to comply with Principle 2, we introduce for a cluster C its visible portion as the ten most relevant documents in C .

To optimize the number of documents that fit into the cluster-ing without violating Principle 2, we propose a cluster layout that can be considered in between the flat and the hierarchical layout paradigm X  X he faceted layout . The faceted layout allows to com-bine a set of cluster labels L = { l 1 , . . . , l k } to form a new cluster C
L that contains the documents of C l 1  X  . . .  X  C l k . Note that the visible portion of this intersection can reveal documents that are not visible in any of the clusters C l 1 , . . . , C l k . A user can explore visible documents by selecting and deselecting cluster labels in an ad-hoc fashion X  X imilar to a faceted navigation system, which is Figure 1: Abstract user interface of a Faceted Clustering search engine. Clicking on a cluster label will toggle the label and indicate its selection (green labels 2, 3) or deselection (la-bels 1, 4). Intersections between clusters are formed intuitively by selecting multiple cluster labels. why we term our approach Faceted Clustering. The faceted na-ture introduces great flexibility and a strong means to individual searches: the number of clusters that can be built from ten clus-ter labels grows from 10 to 2 10 . At the same time, the number of cluster labels to examine does not increase over the flat cluster layout; even the navigational overhead of unfolding and collapsing a hierarchical cluster structure is avoided. Figure 1 sketches the interaction principles of a user interface for Faceted Clustering.
In the following, selected entry points and recent results for Web search result clustering and related research fields are given. Web Search Result Clustering. The existing approaches for Web search result clustering can be distinguished along three dimen-sions [2]: data-centric, description-aware, and description-centric. Data-centric approaches give top priority to clustering, while the formation of cluster labels has no effect on the partitioning of the snippets. The labels are usually derived from a cluster X  X  mathe-matical representation, e.g., a centroid or a medoid under a bag-of-words model. Hence the generated labels form a sequence of probably unrelated words, often lacking understandability. Ex-amples for such approaches are WebCat [6] and AIsearch [13]. Description-aware approaches , by contrast, interweave the pro-cesses of clustering and labeling. In the existing approaches of this type, such as Grouper [17] or SnakeT [4], the labeling pro-cess controls a polythetic cluster analysis. Description-centric ap-proaches , finally, employ cluster analysis solely for the purpose of discovering topics in a collection. Snippet partitioning is achieved by monothetic clustering, where each feature is used in an isolated manner to partition a collection into (overlapping) clusters. This approach leads to an improved understandability, which is bought by an possibly acceptable decrease in the clustering quality. Exam-ples for such approaches are CAARD [7], Lingo [9], DisCover [8], and KeySRC [1]. Also the presented Faceted Clustering algorithm belongs to this category.
 Cluster Labeling. The salient properties of good cluster labels are comprehensibility, descriptiveness, and discriminative power [13]. The basic building blocks of labels are phrases, whereas noun phrases [11], named entities [14], and title phrases [5] are recently discussed alternatives to improve comprehensibility. Descriptive-ness means that a label should speak for each document in a cluster, while discriminative power means that the semantic overlap of two labels should be minimum.
 Faceted Search. Faceted search can be considered as a  X  X ontrolled, multi-view cluster analysis X . Without doubt, facets excel in satis-fying the three label properties (comprehensibility, descriptiveness, Figure 2: Average subtopic recall for the 44 topics of the AM-B IENT corpus. The black curve shows the human assessment, the green curve shows the assessment by Welch X  X  unsupervised algorithm. discriminative power) mentioned above. The automatic and ad-hoc generation of facets is an active research field, but currently this problem must be considered as unsolved [15]. With Faceted Clus-tering, we contribute an innovative approach to automatic but partly restricted facet generation: the semantic relationship between facet values is not considered X  X  criteria that is usually applied in faceted search applications.
The basic hypothesis of our research is that the result list tail is a valuable information source for a significant number of hidden query aspects. Faceted Clustering shall expose this wisdom to the user. This section gives evidence for this hypothesis by demonstrat-ing that more than 20% of all query aspects can be found only in the tail of search result lists.
Let D q = d 1 , . . . , d n be the ranked list of n search results re-trieved for a query q . Each d  X  D q denotes a result snippet, i.e., the concatenation of the search result X  X  title and its short descrip-tion. In addition, for each query q a set A q of query aspects is given. A query aspect is represented either by a short textual de-scription, by a single Web page, or by multiple relevant Web pages obtained from individual searches. We denote the set of all query aspects of q that are covered by d as coverage ( d, A q ) . To mea-sure the query aspect recall for a result list D q at rank R based on coverage ( d, A q ) , we utilize the measure subtopic recall introduced by Zhai et al. [18]. Subtopic recall quantifies the fraction of query aspects covered by the first R search results:
SubTopicRecall @ R =
The definition of coverage is crucial since it has to resemble hu-man relevance judgments. A suitable corpus for subtopic analysis is the AMBIENT corpus [3], which has been used in numerous evaluations. The corpus contains 44 queries created on the basis of Wikipedia disambiguation pages that describe on average 18 di-verse meanings (here: query aspects) of the query term. For each query the corpus provides 100 search results retrieved by Yahoo! in 2008. Each result is manually labeled with the query aspect that it covers. Figure 2 illustrates the average subtopic recall of the Yahoo! retrieval results; the dark curve shows the coverage assess-ments on the basis of human relevance judgments.

We would like to draw attention to two observations. (1) The search results retrieved from Yahoo! in 2008 cover 48.3% of the Figure 3: Average subtopic recall for the queries of the AMBI-E NT corpus. For each query 100 search results are retrieved from the four search engines Bing, Yahoo!, Etools, and Chat-Noir. query aspects listed on Wikipedia X  X  disambiguation pages. If we define the result list head H  X  D q to comprise the first ten search results, it covers 22.9% of all query aspects. Additional 24.8% of the query aspects are covered solely in the result list tail, which sub-stantiates our hypothesis of  X  X he wisdom of the tail X  in search re-sults. (2) Published in 2008, the retrieval results from Yahoo! may be suspected of being outdated. Recently Welch et al. introduced an unsupervised strategy to assess the coverage ( d, A q ) on the ba-sis of the set of Wikipedia articles that are referenced on a disam-biguation page [16]. The authors propose a normalized probability score Pr( a | d ) , which is computed for each query aspect a and doc-which is not further motivated by Welch, then a is said to be cov-ered by d . To actually compute Pr( a | d ) , the authors apply a vector space model and cosine similarity. In Figure 2 the green curve il-lustrates the average subtopic recall of Welch X  X  method computed for the queries from Yahoo!. A robust agreement with the manual relevance judgments can be clearly identified. Based on the impres-sive performance of Welch X  X  method on the AMBIENT corpus, one may well assess the subtopic coverage of the result lists from other search engines.

We apply Welch X  X  method to the search result lists obtained from the four search engines Bing, Yahoo!, Etools, and ChatNoir [10]. In contrast to the well-known search engines Bing and Yahoo!, Etools is a meta search engine and aggregates results of Bing and Yahoo! among others. Furthermore, our research group has devel-oped an index for the ClueWeb09-T09B corpus, called ChatNoir, which utilizes the field-oriented retrieval model BM25F, PageR-ank, and SpamRank. For all queries of the AMBIENT corpus we retrieve the top 100 results of each search engine. The obtained av-erage subtopic recall is illustrated in Figure 3. Like Yahoo! in 2008, also the modern search engines show the importance of the tail as a valuable information source. A tailored clustering approach that organized just the remaining query aspects in the long tail would perfectly complement the result list head and enable a more effi-cient and effective result analysis. The key to such a complement-ing behavior lies in the alleviation of undesired topic repetition, called shadowing. To quantify the shadowing effect in search result lists for the AMBIENT dataset we compare the top-ten results H to the search results in the tail D q \ H . Following Welch X  X  method for query aspect detection, we compute the cosine similarity  X  cos ( d, d Figure 4: Average shadowing for the queries of the AMBIENT c orpus, caused by the search engines Yahoo!, Etools, Bing, and ChatNoir. A bar indicates the shadowing at the result list posi-tions 10. . . 100. The darker a bar the more shadowing occurred at that position. d  X  H and d i  X  D q \ H . A search result d i is said to be shadowed , iff max {  X  cos ( d, d i ) | d  X  H, d i  X  D q \ H } X  0 . 3 .
Figure 4 shows the results of our analysis. At each search result position the shadowing effect is encoded as a gray scale value be-tween 0.0 (no shadowing) and 1.0 (always shadowed). I.e., the gray scale value reflects the relative frequency of shadowing events at a specific rank. Obviously, up to 25.5% of the search results retrieved by Yahoo! are shadowed. Compared to all other search engines, ChatNoir reveals the lowest shadowing of only 14.6%. Nonethe-less, all studied search engines confirm the hypothesis that query aspects covered by the result list head reappear frequently through-out the tail. The Faceted Clustering algorithm, which is introduced next, penalizes documents that cause shadowing and hence is able to effectively alleviate shadowing in the generated clusterings.
Faceted Clustering aims at complementing a result list head with a tailored clustering of the result list tail. The algorithm belongs to the class of description-centric document clustering approaches, whose strategy may be summarized as  X  X escription comes first X : Before the actual cluster formation process starts, a set of candidate labels  X  L is extracted from the document set D . Due to the mono-thetic nature of this formation process, a label l  X   X  L induces X  X n the sense of: determines X  X or a set D a cluster C l  X  D . Similarly, a label set L induces a set of clusters C , also termed a clustering. Note that the extraction of appropriate cluster labels is a non-trivial task that involves the application of sophisticated natural language processing techniques like noun phrase extraction and morpholog-ical generalization. The paper in hand won X  X  focus nor contribute to the label extraction problem; in our experiments we resort to state-of-the-art technology.
Given a set of candidate labels  X  L , cluster formation means to construct a label subset L  X   X  L whose induced clustering C adheres to the two principles of the competence partitioning strategy. Such a clustering minimizes shadowing and avoids extreme clusterings (i.e., a high cluster number as well as large clusters), and, at the same time, maximizes the number of visible documents. Similar to other description-centric document clustering algorithms, Faceted Clustering pursues a greedy strategy for cluster formation. The algorithm is outlined in Algorithm 1.

In each of the k iterations a cluster label l  X   X   X  L is chosen that maximizes candidateScore . The label l  X  is added to the set of already selected labels L , and the intermediate clustering C as well as the document sets N (not yet visible) and U (untouched) are Algorithm 1 F aceted Clustering Input: C andidate labels  X  L , Documents D Parameters: Number of iterations k Output: Labels L , Clustering C
L =  X  , C =  X  , N = D, U = D for r = k downto 1 do end for return ( L, C ) updated with regard to l  X  . Note that by means of set intersections up to 2 k  X  r new clusters could be added to C in each round, but we restrict the number of operands (clusters) within an intersection to three. Alternatively, the update of C can be bound to a constant size. Documents that are assigned to a cluster are marked as touched and removed from the set of untouched documents U . Recall that only the k most relevant documents in a cluster will be visible to a user. In this regard the set N keeps track of the assigned but not yet visible documents. The overall runtime is in O ( |  X  L |  X  k ) , which puts Faceted Clustering on the list of the fastest description-centric algorithms such as DisCover [8] and CAARD [7].

The heart of Faceted Clustering is the heuristic candidateScore , outlined in Algorithm 2. Given an intermediate clustering C the heuristic estimates for a candidate label l (via the induced clus-ter C l ) the potential to boost the number of visible documents in the final clustering. This estimate is computed as a score of two summands whose rationale is as follows.

The first term, | A | , is the number of documents that will become additionally visible if the cluster C l is added to the intermediate clustering C . A is the intersection of N , the set of not yet visi-ble documents, and V , the set of visible documents if C is updated with C l . I.e., the set V is union set of the intersections of C and the clusters in C , completed by C l  X  X  head. In the first iteration, the maximum value of | A | equals k , whereas in the following iter-ations the number of documents in N and their distribution in the intermediate clustering is considered.

The second term quantifies the  X  X otential X  of a candidate la-bel l . It considers the documents which have not yet been touched, but which become a non-visible cluster document after C is up-dated with C l . In this regard the set R contains all documents that the function updateClustering (see Algorithm 1) would remove from U but retain in N . Having many of these documents in differ-ent clusters is important since they entail a high score when forming the intersection sets. Loosely speaking, we are looking for labels that bring many new documents into play. The higher the potential of a label is, the more likely is the event that a future iteration will return a label that entails a high | A | -value. Since the probability for this event decreases with each iteration, the weight of the sec-ond term is reduced in each round. For smoothing purposes, R is normalized by | U \ A | , which bounds the maximum value of the second term to r . This section reports on two experiments that analyze whether the Faceted Clustering algorithm optimizes Web search result cluster-ing according to the competence partitioning strategy. In the first experiment we show that the faceted cluster layout leads to a sig-nificant increase in result coverage compared to a flat cluster layout that tries to avoid extreme clusterings. The second experiment re-Algorithm 2 F aceted Clustering: candidateScore Input: C lustering C , Label l , Output: Candidate score score V = { d | d  X  ( head ( C l )  X  C ) , C  X  X } X  head ( C l ) A = V  X  N /* Additionally visible under l * /
R = ( C l  X  U ) \ A /* Retained in N u nder l */ score = | A | + r  X  | R | / | U \ A | return ( score ) veals that, when being compared to well-known competitors, o nly Faceted Clustering is able to reduce shadowing within its clusters. Both experiments use search result lists that we received for the 44 queries of the AMBIENT from the search engine Yahoo!. The result lists contain 500 result snippets, from which we consider the first ten as the result list head.
A key characteristic of the Faceted Clustering approach is its flexibility to assign documents to clusters, while avoiding extreme clusters and navigational overhead at the same time. However, the question is whether the term distribution of the query results in fact allow an exploitation of the gained space. To evaluate the benefit of Faceted Clustering in this regard, we cluster the Yahoo! result lists with both the Faceted Clustering algorithm and an algorithm variation that generates a flat clustering. If the number of results that are covered by faceted clusterings is significantly higher than for flat clusterings, we have a strong indication for the superiority of Faceted Clustering.

For the Flat Clustering algorithm, the score of a label l is based only on the number of new results the cluster C l would add to an in-termediate clustering C . Intersections with existing clusters are not considered. The quantity k , which controls the maximum number of documents for a cluster, as well as the amount of cluster labels | L | is set to ten for both algorithms. I.e., for Flat Clustering the result coverage is bound to 100.

Before applying the clustering algorithms, the set of candidate cluster labels  X  L has to be determined. This step is independent of the cluster formation step, and many different strategies have been proposed for this purpose in the past (cf. Section 2). Here, set of all unigrams and bigrams which occur in the result snippets and which contain no common stopwords. Compared to labels of more sophisticated approaches, unigrams and bigrams are shorter on average and hence are better suited for the purpose of forming cluster intersection.

The result of this experiment is shown in Figure 5 in the form of two histograms. Note that the histograms do not overlap at all, which is because the Faceted Clustering approach consistently sur-passes the maximum result coverage of plain clusterings. With 130 . 7 results on average, the Faceted Clustering approach is clearly superior to the baseline approach, which yields an average result coverage of 89 . 8 only. According to the Wilcoxon signed-rank test, the performance difference is significant at confidence level 0 . 01 .
The achieved result coverage gain results from the possibility to form cluster intersections: From the 5755 unique results covered in total by all 44 clusterings, 40% are covered only by intersecting two clusters, and 12% are covered only by intersecting three clusters. In 0.2% of all cases, even four intersections are needed. We infer from our result coverage experiment that the faceted clustering layout is highly effective when it comes to generate well-balanced, non-extreme clusterings. Figure 5: The two histograms show the frequency at which t he Flat Clustering algorithm and the Faceted Clustering al-gorithm achieve a certain result coverage. Basis are the search result clusterings for the 44 queries of the AMBIENT corpus.
A second key characteristic of Faceted Clustering is the allevi-ation of the shadowing effect in its generated clusterings. As a baseline for the evaluation we take the average shadowing in the original result lists from Yahoo!, which is 25.5% (cf. Section 3 and Figure 4). We expect the average shadowing under Faceted Clus-tering to be moderately lower than for the baseline. A dramatic de-crease cannot be expected since the Faceted Clustering algorithm enfolds its full power in cases where the result list head is dom-inated by a single query aspect. To further substantiate our eval-uation, we also measure shadowing for the clusterings generated by two other well-known description-centric clustering algorithms. The first algorithm, Lingo, uses singular value decomposition to find topically related terms and groups them under the same cluster label. The second algorithm, STC, uses a suffix tree representation of the documents to find reoccurring phrases that can be used as cluster labels. Both algorithms are contained in the Carrot2 soft-ware library. 1
To apply the shadowing quantification method from Section 3 to a clustering, we take the documents of each cluster and add them to the head of the relevant result list one at a time. To determine the order in which the clusters X  documents are added, we use the relevance scores that are provided by STC and Lingo. For Faceted Clustering, we added the documents in the order of their removal from the set N of not yet visible documents. The results of our evaluation are shown in Figure 6: The Faceted Clustering algorithm successfully reduces shadowing from 25.5% in the original list to 18.9% in the clustering. This decrease is significant according to a proportions test at confidence level 0.01. 2 By contrast, Lingo and STC, which both do not take the shadowing effect into account, cause a slight increase in shadowing.
With Faceted Clustering, we present innovative technology to make the most out of the critical time and size constraints users impose on the analysis on Web search results. Besides its possi-bility to operationalize a competence partitioning strategy for Web Search, we see Faceted Clustering as a general means to effectively explore large text collections. [1] A. Bernardini, C. Carpineto, M. D X  X mico. Full-Subtopic [2] C. Carpineto, S. Osi  X  nski, G. Romano, D. Weiss. A Survey of h ttp://project.carrot2.org/ prop.test from the R stats package. Figure 6: Average shadowing for the queries of the AMBIENT c orpus, caused by the search engine Yahoo! (copied from Fig-ure 4) and the clustering algorithms STC, Lingo, and Faceted Clustering. [3] C. Carpineto, G. Romano. AMBIENT corpus. [4] P. Ferragina, A. Gull X . A Personalized Search Engine Based [5] F. Geraci, M. Pellegrini, M. Maggini, F. Sebastiani. Cluster [6] F. Giannotti, M. Nanni, D. Pedreschi, F. Samaritani. WebCat: [7] K. Krishna, R. Krishnapuram. A Clustering Algorithm for [8] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, [9] S. Osi  X  nski, J. Stefanowski, D. Weiss. Lingo: Search Results [10] M. Potthast, M. Hagen, B. Stein, J. Gra X egger, M. Michel, [11] J. Stefanowski, D. Weiss. Comprehensible and Accurate [12] B. Stein, T. Gollub, D. Hoppe. Beyond Precision@10: [13] B. Stein, S. Meyer zu Ei X en. Topic Identification: Framework [14] H. Toda, R. Kataoka. A Clustering Method for News Articles [15] D. Tunkelang. Faceted Search . Morgan &amp; Claypool [16] M. Welch, J. Cho, C. Olston. Search Result Diversity for [17] O. Zamir, O. Etzioni. Grouper: A Dynamic Clustering [18] C. Zhai, W. Cohen, J. Lafferty. Beyond Independent
