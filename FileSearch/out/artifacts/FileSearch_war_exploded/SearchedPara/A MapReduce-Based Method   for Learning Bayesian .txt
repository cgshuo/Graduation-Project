 With the rapid development of Internet and Web applications, more and more data are computing may generate many queries, each involving access to supercomputer-class computations on gigabytes or terabytes of data [4]. Thus, knowledge discovery from challenging by incorporating the specialties of massive data [1]. al uncertainty knowledge. The most important and popular PGM is Bayesian network tion of random variables and each node has a conditional probability table (CPT) that interdependent parameters [6], and has b een widely used in real applications. expert knowledge and learning from data [5]. The former relies on the knowledge of provides us a feasible way to obtain uncertain knowledge implied in data, frequently This makes us focus on the DAG learning from massive data in this paper. Currently, dependency analysis and scoring &amp; search are the methods for learning BN from data [6]. The typical dependency analysis methods focus on the dependence MMHC [10] and MDL [11]. These methods choose the candidate BN structure based basic ideas can be universally suitable for the dependency analysis as well. However, K2 will have to be extended w.r.t. the storage and retrieval of massive data increased rapidly due to the expensive cost for computing the required parameters. 
The scoring function [8] in K2 is as follows: these two parameters of all records can be summed to obtain the final values required illustrate that both the scoring and search process can be executed in parallel. 
From the parallel point of view, some parallel methods for BN learning [12, 13, 14, processor cooperation, while have not con cerned the throughput computing to access in parallel and efficiently to access massive data. 
Fortunately, MapReduce is a programming model for processing and generating conduct and process massive data. It has been used in data analysis and dependency-analysis-based BN learning [17, 18, 19, 20, 21]. Although MapReduce cannot reveal from massive data. Using MapReduce to extend the current centralized K2 algorithm, we are to explore the parallel and data-int ensive K2 algorithm. To make the MapRe-duce-based K2 fit the demand of massive data definitely is the purpose of our study. follows:  X  We developed the architecture of BN learning system by extending the traditional 
K2 algorithm based on MapReduce.  X  obtain all parameters in parallel. In the map process, the parameters in each record through the map process will be summed to obtain the requited ones.  X 
Thus, we obtained the BN structure from massive data by extending K2 algorithm.  X  We implemented our proposed algorithms for learning BN from massive data. Experimental results show the feasibility of our method. The rest of this paper is organized as follows: Section 2 introduces K2 algorithm and gives experimental results. Section 5 concludes and discusses future work. 2.1 Scoring and Search for BN Learning A BN is a DAG G =( V , E ), in which the following holds [5]: to node Y means that X has a direct influence on Y ( X , Y  X  V and X  X  Y ). node. The parents of node X are all those that have arrows pointing to it. L is instantiated as w ij . Here let 3. The following pseudo-code expresses the heuristic search algorithm, called K2 [8]. Algorithm 1. K2 Input: 1) A set of n nodes, a sequence of the nodes 2) An upper bound u on the number of parent nodes 3) A database D containing m records Output: A printout of the parents for each node Steps: For i :=1 to n Do  X 
OKTo Proceed:=true while OKToProceed and |  X  i |&lt; u Do Let z be a node in Pred( x i )  X   X  i maximizing g ( i ,  X  i  X  { z i }) P
End while output(`Node:', x i , `Parents of this node:',  X  i ) End for 2.2 MapReduce Model MapReduce library express the computation as two functions: Map and Reduce. with the same intermediate key together and passes them to the Reduce function. 
Reduce, also written by users, accepts an intermediate key and a set of values for that key. It merges these values to form a possibly smaller set of values. The intermediate values are supplied to the user X  X  reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory [15]. 
Fig. 1 (a) indicates that we send the in-put data to the Master node, by which the data are split into many maps sent to Slave node, which will return the result to Master node and reduce the map result to the final one for output. The NameNode JobTracker is allocated on Master node to control the whole task and DataNodes, and TaskTracker is allocated on Slave node to communicate with the NameNode JobTracker, shown in Fig.1 (b). The architecture of the MapReduce-based K2 algorithm is shown in Fig. 2:  X 
Preprocessing. The required parameters in Equation (1) are obtained by statistical counting from the given massive sample data in parallel based on MapReduce.  X  the highest score of all nodes will be merged to achieve the global optimal one. 3.1 MapReduce-Based Scoring Function denote Absent and Present respectively. In this example, r i is 2. map process and reduce process respectively for accessing the sample data and sum-and N ijk of each record will be obtained as intermediate results. Algorithm 2. MapReduce-based Scoring Function Input: 1) m records upon n variables 2) record L x (1  X  x  X  m ) including values in { s 1 , s 2 , ..., s n }, where s i (1  X  i  X  n ) is 1 or 0, denoting the i th variable is Present or Absent respectively Output: &lt;key/value&gt; pairs of  X  N ij  X  (or  X  N ijk  X ) and the value Algorithm 2.1. Map (String key , String value ) // key : key of data record, value : content of data record Compute N ij and N ijk from L x (1  X  i  X  n ; 1  X  j  X  q i ; 1  X  k  X  r i ) //by Equation (2) Generate key/value pairs &lt; N ijk , 1&gt; End If Generate key/value pairs &lt; N ij , 1&gt; End If Generate key/value pairs &lt; N ijk , 1&gt; End If Generate key/value pairs &lt; N ij , 1&gt; End If ue of N ij is 0. Similarly, we can obtain for the pairs for other records. r  X  r k &gt; and &lt; N ijk , Algorithm 2.2. Reduce(String key, Iterator values) //key: N ij or N ijk , values: in the Iterator and have the Result:=0 For each key/value pair Do value:=Interator.Next() result:=result+value End For Output&lt;key, result&gt; 3.2 MapReduce-Based Search Function x tively. For node x 2 , the candidate structure ( x 2  X  x 3 ) is shown in Fig. 3 (d). S each map process (Algorithm 3.1) of Algorithm 3, the candidate local structures will Algorithm 3 . MapReduce-based Search Function Input: candidate local structures of each node Output: the local optimal structure of each node Algorithm 3.1. Map (String key, String value) //key: node name x i , value: a candidate structure of x i Generate key/value pairs &lt; i &amp; S ij , score ij &gt; //key consists of node name x i and structure S ij , and value is the score of S ij &lt; x the highest score will be chosen as the local optimal structure of x i . Algorithm 3.2. Reduce(String key, String value) // key: node name x i &amp; structure S ij , value: score of S ij max:=0; For each S ij of x i Do If max&lt; score ij Then max:= score ij Content:=key. S ij Node:=key. x i End If Output &lt;Node,Content&gt; End For ( x ( x 7.22  X  10 -5 . Fig. 4, we can obtain the global optimal structure, shown in Fig. 5. According to the sequence of Seq , which makes the merging result be the global optimal structure. 
If the massive sample data have m records upon n variables. The time complexity time complexity of our method is O ( m * n 2 / t ). easily compute the CPTs for each node in the DAG by the similar MapReduce-based ideas adopted in Algorithm 2 when accessing the massive data. 4.1 Experiment Environment E5700 @3.00GHz @3.01GHz with 2GB main memory. On each machine, the ver-sions of Hadoop, Linux and Java are 0.20.2, Ubuntu 10.04, and JDK 1.6 respectively. 
Chest-clinic [23] is a widely used BN in medical science, including 8 nodes, where ponding BN in the experiments. The initial data set has 1000 records. To achieve the 15.26MB. We generated the test data from 15.62KB to 610.35MB. 4.2 Efficiency Comparison algorithms, and algorithm time is the execution time of Algorithm 2 without including the time of initialization of Hadoop and I/O. First, we recorded TT and AT on differ-ent sized data with the increase of Slave nodes, shown in Fig. 6. Then, we recorded the total time with the increase of data volume with 1, 2 and 3 creased slowly with the increase of data volume under 1, 2 or 3 Slave nodes. Mean-cluster, BN can be constructed in less time. Algorithm 3) with the increase of data volume with 1, 2 and 3 Slave nodes respective-4.3 Structure Comparison Power Constructor [23] is one of the current BN learning systems, by which we applied the chest-clinic sample data with 10000 records to construct BN, shown as Fig. 9. Also we used the same data to construct BN by our method, and we obtained the same structure as that obtained by Power Constructor. Thus, we can confirm that the BN structure learned by our algorithm is correct. In this paper, as the initial exploration of BN learning from massive data, we gave a MapReduce-based method for BN learning from massive data while focusing on the able for data-intensive computing and can be adopted generically for representing and inferring uncertainties implied in massive data. 
In the future, we will improve the environment for real massive data sets. Further, knowledge model can be obtained efficiently even oriented to massive data. Acknowledgement. This paper was supported by the National Natural Science Foun-dation of China (61063009, 61163003, 61232002), the Ph. D Programs Foundation of tion for Leaders of Disciplines in Science and Technology (2012HB004), the Founda-and the Foundation of the Key Laboratory of Software Engineering of Yunnan Prov-ince (2012SE205). 
