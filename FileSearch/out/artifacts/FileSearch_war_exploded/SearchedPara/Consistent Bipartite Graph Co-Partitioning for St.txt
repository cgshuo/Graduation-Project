 Heterogeneous data co-clustering has attracted more and more applications. While the co-clustering algorithms for two types of heterogeneous data (denoted by pair-wise co-clustering), such as documents and terms, have been well studied in the literature, the work on more types of heterogeneous data (denoted by high-order co-clustering) is still very limited. As an attempt in this direction, in this paper, we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships. Actually, this case could be a very good abstract for many real-world applications, such as the co-clustering of categories, documents and terms in text mining. In our philosophy, we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure. Accordingly, we proposed the concept of consistent bipartite graph co-partitioning, and developed an algorithm based on semi-definite programming (SDP) for efficient computation of the clustering results. Experiments on toy problems and real data both verified the effectiveness of our proposed method.
 I.5.3 [ Pattern Recognition ]: Clustering  X  algorithms.
 Algorithms, Performance, Design, Experimentation, Theory. Co-clustering, High-Order Heterogeneous Data, Consistency, Spectral Graph. Clustering is a process that partitions a set of objects into groups or clusters such that objects in the same cluster are similar while objects in different clusters are dissimilar. Homogeneous data clustering has been a well-studied research area in the community of machine learning and data mining. Several algorithms have been developed including k -means [7], maximum likelihood estimation [7], spectral clustering [1][22] and so on. In recent years, more and more data mining applications have asked for the clustering of highly inter-related heterogeneous objects, such as documents and terms in a text corpus, customers and purchasing items in market basket analysis and reviewers and movies in movie recommender systems. In such scenarios, using previous methods to cluster each type of objects independently might not work very well since the similarities among one type of objects sometimes can only be defined by the other type of objects. To tackle this problem, many researchers started to study the co-clustering of two types of heterogeneous data. Dhillon et al [4] and Zha et al [26] extended the traditional spectral clustering algorithms and proposed the bipartite spectral graph partitioning algorithm to co-cluster documents and terms simultaneously. Similar techniques were also applied in biology [14] and image processing [19]. Moreover, Dhillon et al [5] proposed the information theoretic co-clustering method based on mutual information. As can be seen, the co-clustering of two types of heterogeneous objects (denoted by pair-wise co-clustering) has attracted much attention in recent years. However, comparatively speaking, the co-clustering of multiple types of objects (denoted by high-order co-clustering) has not been well studied in the literature. Some limited works include [23], [25] and so on. Take [23] for example, Zeng et al proposed a unified framework named ReCoM to cluster multi-type interrelated Web objects. However, there is no sounded objective function and theoretical proof on the effectiveness of this iterative algorithm. For the above discussion, one may argue that the high-order co-clustering could be solved by trivially extending the pair-wise co-clustering methods. For example, one can use the spectral cut of a k -partite graph 1 to analyze the inter-relationship among the k types of objects. However, as will be seen in this paper later, such a trivial extension is only a pretty trap and we can not really get the desirable co-clustering results in such a way. Therefore, it is necessary to work on some more advanced technologies to handle the high-order co-clustering problems. As a preliminary attempt to the high-order co-clustering problem, in this paper, we will work on a specific case of it in which there form a star structure of the inter-relationships (see Figure 1). Actually, this case could be a very good abstract of many real-world applications, such as Web users, search queries and Web pages in Web search systems (corresponding to Figure 1(a), where the query is the central data type), authors, conferences, papers, and key words in academic publications (corresponding to Figure 1(b), where paper is the central data type); customers, shops, shareholders, suppliers, and advertisement medias (corresponding to Figure 1(c), where shop is the central data type). Co-clustering over such heterogeneous data has its explicit meaning. For example, in an academic publication system, the co-clustering results might indicate that a certain group of authors usually write research papers of a certain series of topics using a certain list of key words, and submitted to a certain kind of conferences. Figure 1. The Star-structured High-order Heterogeneous Data. It is easy to understand that the basic element of the star structure is the triplet as shown in Figure 1(a). If we can successfully co-cluster such triplet data, the corresponding technology would be easily extended to more complicated star structures. Therefore, in the following discussions of our paper, we will focus on the processing of such triplet data, and use X= { x 1 , x 2 y absent between X and Z . Recalling the discussion about the trivial extension of pair-wise co-clustering to the high-order case, although the relations among X , Y and Z could be represented by a tripartite graph, we can not use spectral cut of this graph to get the desirable co-clustering results. Instead, we model this problem as the consistent fusion of two pair-wise co-clustering sub-problems, with the constraint of the triplet structure. That is, we look for such two partitions for the sub-problems of X -Y co-clustering and Y -Z co-clustering, provided that each of them is not locally optimal, but their clustering results on the central type Y are the same and the overall partitioning is globally optimal under a certain objective function. We call such partitions by consistent bipartite graph co-partitions. We proved in this paper that such kind of consistent partitions could be found by semi-definite programming (SDP). Then we tested the above ideas and the corresponding algorithms on both toy and real data. The results showed the feasibility and validity of our methods. The rest of this paper is organized as follows. In Section 2 the background knowledge on graph-based clustering is introduced while the concept of consistency is proposed in Section 3. Then in Section 4 the method to solve the triplet co-clustering is described in details and the experimental results are discussed in Section 5. Concluding remarks and future work directions are listed in the last section. In this section, we will review some research works on spectral clustering, which serves as the foundation of our proposed concept of consistent bipartite graph co-partitioning. Spectral clustering [1][22] is a category of clustering algorithms based on spectral graph partitioning [18], which was proposed and well studied in the literature. To explain how this method works, we need to enumerate some basic knowledge in graph theory first. A graph G= ( V , E ) is composed by a set of vertices V= {1,2,...,| V |} we can further define the adjacency matrix M of the graph by In the spectral graph partitioning methods, the vertices correspond to data objects, the edges correspond to the relationships among objects and the edge weights correspond to the strength of the relationships. Suppose the vertex set V is partitioned into two subsets V 1 and V 2 , then the corresponding cut might be defined as: The above definition can be easily extended to k subsets: Then the clustering is achieved by minimizing the cut . Usually, balanced clusters are more preferred, so some variations of the definition of cut were proposed and therefore different kinds of spectral clustering methods [6][12][22] were derived. For example, Ratio Cut [12] is achieved by balancing cluster sizes, while Normalized Cut [22] is attained by balancing cluster weights. Among these variations, Normalized Cut (or NCut) is one of the most popularly-used spectral clustering methods. Its objective function is shown in (4) where e is the column vector with all its elements equal to 1: Here D is a diagonal matrix with D ii =  X  k E ik , and L=D-M is called Laplacian matrix. q is a column vector with q i = c 1 -c relaxing q i from discrete values to continuous values, it can be proved that the solution for (4) is the eigenvector corresponding to the second smallest eigenvalue  X  2 of the following generalized eigenvalue problem [4][11][22]: Then we can obtain the desired clusters by running some routine clustering algorithms such as k -means [7] on this eigenvector q . In order to use spectral graph partitioning to solve the pair-wise co-clustering problem, Dhillon [4] used the undirected bipartite graph 2 in Figure 2 to represent the relationship between the two types of heterogeneous objects. In this figure, squares and circles represent two types of objects X = { x 1 , x 2 ,..., x y ,..., y n } respectively, and the edges only exist between heterogeneous items. Then the bipartite graph can be represented we further use A to denote the inter-relation matrix in which A written as: where the vertices have been ordered such that the first m vertices index the objects of X while the last n index the objects of Y . Suppose the dashed line in Figure 2 shows the very partition that minimizes (4), we will obtain two subsets { x 1 , x { x into two subsets { y 1 , y 2 , y 3 , y 4 } and { y 5 , y 6 out this very partition, we also need to solve a generalized eigenvalue problem like (5). Due to the bipartite property of the graph, after some trivial deduction, this problem can be converted to a singular value decomposition (SVD) [11] problem, which can be compute more efficiently. For the details of this algorithm, please refer to [4]. After reviewing some background knowledge on spectral clustering, one natural question is whether this technology can be trivially extended to the high-order case. In this section, we will show that such an extension does not work as one expects. And then we will propose our concept of consistency , which models the high-order co-clustering problem as the fusion of pair-wise sub problems. Consider the triplet data mentioned in the introduction. Inheriting the representations in Section 2, this triplet data can be pictured as a tripartite graph as shown in Figure 3 (for this specific example, the reasonable co-clustering results are labeled by the dashed line matrices between X and Y , and between Y and Z respectively, it is easy to derive the adjacency matrix for Figure 3: where  X  is a weighting parameter, and the vertices have been next n index the objects of Y and the last t index the objects of Z . Although it seems natural to partition the graph by working out the generalized eigenvalue problem corresponding to the adjacency matrix (7), we would like to point out that this idea does not always work as it seems. Actually, if we move the original tripartite graph will turn to be a bipartite graph as Figure 4 shows. graph and have to distinguish the loss of cutting an X -Y edge from loss function. However, these two kinds of edges are heterogeneous and might not be comparable. Although we could try to make them comparable by introducing the weighting parameter  X  , however, it is non-trivial to choose a proper value for  X  and we can not avoid the risk of assigning all vertices in Z (or X ) into one subset like the ill partitioning shown in Figure 4. In other words, analyzing the matrix M in (7) by traditional spectral clustering method (we denote this approach by TSC for ease of reference) does not work as it is expected, and it is necessary for us to develop more advanced technology to handle high-order co-clustering. To tackle the aforementioned problem, we propose to treat the tripartite graph in Figure 3 as two bipartite graphs in Figure 2 and Figure 5 respectively, which share the central part of objects in Y . Then we transform the original high-order problem to the fusion of the pair-wise co-clustering problems over these two bipartite graphs. However, if we conduct bipartite spectral graph partitioning on Figure 2 and 5 independently, it will have a great probability that other words, the two locally optimal partitioning schemes in Y do not match in most cases. This is not what we want. Actually, we are looking for such two partitions for Figure 2 and 5, provided that each of them is not locally optimal, but their clustering results on the central type Y are the same, and the overall partitioning is globally optimal under a certain objective function. We call it by consistent bipartite graph co-partitioning (CBGC). So far, the aforementioned concept of CBGC is very generic. To make it computable, we will give a specific objective function and discuss how to optimize it efficiently. Note that, in this paper, we will only focus on a partition of two clusters, where all types of objects will be simultaneously clustered into two groups respectively. We denote q =( x , y ) T and p =( y , z ) vectors for the two local bipartite graphs, and denote D and L (2) as the diagonal matrices and Laplacian matrices for the adjacent matrices A and B . Then we mathematically model the consistent co-partitioning problem as follows, where  X  is a weighting parameter to balance which local graph we trust more. We can see that the above additive objective function commendably reflects the concept of consistency, and the two constraints might avoid the awkward situation in Figure 4. In this section we will propose an algorithm to compute the solution of the optimization problem defined in Section 3.2. The core idea is to convert it to a semi-definite programming (SDP) problem so that it can be computed efficiently. L (1) , L (2) , D (1) and D (2) to adapt the length of  X  : where the 0 X  s are matrix blocks with all the elements equal to zero. Then accordingly (8) can be rewritten as: Problem (11) is a typical sum-of-ratios quadratic fractional programming problem [8], which is hard and complicated to solve although there has been some branch-and-bound algorithms [2]. To avoid solving this fractional programming problem, we use a familiar skill in spectral clustering to simplify it: by fixing the values of the denominators in (11) to e T  X  respectively, we have: where 1 0 , Optimization problem (12) turns to be a quadratically constrained quadratic programming (QCQP) [3] problem and it is not difficult to verify that the constraints are all convex because matrices  X  and  X  2 are both positive semi-definite. As we know, convex QCQP problem can be cast in the form of a semi-definite programming problem (SDP) [3] for efficient computation. SDP is an optimization problem with the form as below: where C is a symmetric coefficient matrix and W is a symmetric parameter matrix; the symbol 0 f means positive semi-definite; A constraints; the matrix inner-product is defined as: As it turns out, QCQP can be reformed as a SDP by relaxing the product terms  X  i  X  j to an element  X  ij of a symmetric matrix  X  . To show this, we begin with the following elementary proposition. P
ROPOSITION . Given a vector k R  X   X  and a matrix then  X  is positive semi-definite. Proof. This is a standard result from linear algebra [11].  X  Using this proposition, it is straightforward to show that the QCQP in (12) is equivalent to the following SDP: As it has been proved that the SDP relaxation of a QCQP may produce an approximation to the original problem with a good error bound [21], we further ignore the constraints of  X  =  X  get the following relaxation: where E is a matrix block with all the elements equal to one; the constraint constrains are bound controllers with some constants  X  1 Up to now, we have got a standard form of SDP. The first column of W (except W 11 ) can be regarded as the representation of  X  . SDP is a hot research field [21] in recent years, and many fast iterative algorithms have been designed to solve it [13][16][17]. For example, an interior-point method SDPA [20] was implemented in [9] for solving the standard form SDP and its dual problem. We could use it to compute an efficient solution to the optimization problem (17). To summarize, our algorithm to solve the co-clustering of triplet data can be listed as below. For ease of reference, we also use CBGC to abbreviate it in the future discussions. The CBGC Algorithm 1. Set the parameters  X  ,  X  1 and  X  2 . 2. Given the inter-relation matrices A and B , form the 3. Extend D (1) , D (2) , L (1) and L (2) to  X  4. Solve (17) by a certain iterative algorithm such as SDPA. 6. Run the k -means algorithm on  X  to obtain the desired Although when explaining the concept and the algorithm of CBGC, we take the triplet heterogeneous data co-clustering for example, here we want to point out that they can be easily generalized to solve the co-clustering of star-structured k -partite heterogeneous objects. In such case, we only need to refine the optimization problem (8) to the following form: graphs. Once again, (18) can be re-organized as a QCQP and further solved as a SDP. In this section, we evaluated the effectiveness of our proposed consistency concept and the corresponding SDP-based CBGC algorithm. For this purpose, we conducted three experiments. The first one was a toy problem to show that the proposed CBGC algorithm could avoid the ill partitioning situation in Figure 4 to many extents. The second experiment was another toy to show that our method could get a trade off partitioning on the central type of objects under the concept of consistency. The last experiment was implemented on a real data set to confirm our conclusions. In the first toy problem, the set sizes of X , Y and Z were 3, 8 and 6 respectively, and the inner-relationships among objects were shown in Figure 6. Suppose the edge weights were all assigned illuminated by the dashed lines in Figure 6 might be what we wanted. On this toy data, we compared the clustering results produced by the traditional spectral clustering method (TSC) and our CBGC algorithm. In TSC, we treated Figure 6 as an ordinary graph and worked out the generalized eigenvalue problem corresponding to the adjacency matrix like (7). We tuned  X  in a large range and plotted the embeddings when  X  = 0.01, 1, and 100 in Figure 7, 8 and 9. The vertical axis reflected the corresponding embedding values, and the colors of the points showed the k -means clustering results. embedding values of X and Z were mixed together in the results. For example, the first 3 points in Figure 7(a) represented X (from x to x 3 ), while the next 6 points represented Z (from z embedding of Y (from y 1 to y 8 ) was shown in Figure 7(b). From Figure 7 to 9 and many results that were not listed in this paper, we could find that TSC failed in partitioning X no matter what value  X  took: all vertices in X and a part of vertices in Z were always clustered together while the rest vertices in Z made another cluster. This told us that in some cases, the ill partitioning results as mentioned in Section 3.1 could hardly be avoided if we treat the relationship among heterogeneous data as a homogeneous graph. In this subsection, we ran CBGC also on the toy data as shown in Figure 6. Here, we use A to denote the adjacency matrix of the X -graph. As the embedding values of X , Y and Z were extracted from the resulting matrix W of SDP, they were mixed together in one vector  X  . Specifically, when we set  X  =0.3 and  X  1 =  X  embeddings in this vector as shown in Figure 10. As can be seen, CBGC successfully avoided the ill partitioning and the co-clustering results of the three heterogeneous objects were quite accordant with our desire. Then we turned the value of  X  in the interval [0, 1]. We found that the clustering results changed along with the changing of  X  . For example, when setting  X  =0.1, we got the results in Figure 11, in which the partitioning on X was different from in Figure 10 because matrix B had much more influence than A . When setting  X  =0.9, matrix A became dominant so that y 7 and z out from the whole set (see Figure 12). This is also reasonable because the X-Y bipartite graph was actually unconnected. If we further change the value of  X  to very close to 0 or 1, the degrade to be working on only one bipartite graph. Figure 10. The Embeddings Produced by CBGC (  X  =0.3). Figure 11. The Embeddings Produced by CBGC (  X  =0.1).
 Figure 12. The Embeddings Produced by CBGC (  X  =0.9). To summarize, this experiment showed that CBGC can avoid the case of ill partitioning to many extents, and tuning  X  is an effective way to trade-off between the two bipartite graphs. In the second toy problem, the set sizes of X , Y and Z were 16, 20 and 21 respectively, and the two inter-relationship matrices A and B were shown in Figure 13 and 14, where the size of the circle at ( i,j ) illuminated the corresponding edge weight (the quantization range is 0~9). Figure 13. The Inter-relation Matrix A for Toy Problem II.
 Figure 14. The Inter-relation Matrix B for Toy Problem II.
 From matrix A , we could find that the reasonable co-clustering results of the X -Y bipartite graph should be { x { x ,..., x 16 } and { y 1 ,... y 12 } vs { y 13 ,..., y 20 could find that the co-clustering results of the Y -Z bipartite graph should be { y 1 ,..., y 8 } vs { y 9 ,... y 20 } and { z It was clear that the partitioning scheme for Y was different in the two bipartite graphs. In such a situation, it is an interesting question what kind of co-clustering results our CBGC algorithm will produce. To get a comprehensive answer to this question, we tuned the parameter  X  in CBGC from 0 to 1 with a step of 0.2, and got the embedding values as shown in Figure 15 to 20. Here the could find that when  X  =0, matrix A was suppressed to zero and the partitioning on Y and Z were completely accordant with what matrix B showed. When  X  =0.6, the partitioning scheme on Y was a tradeoff between the two locally optimal partitioning schemes of the two bipartite graphs. This well showed the core idea of our consistency concept. When  X  went up to 1, matrix B vanished and the partitioning on X and Y were in accordance with what matrix A showed. Besides these three typical cases,  X  =0.2, 0.4, 0.8 just showed how the partitioning schemes transited from one case to another. Figure 15. The Embeddings Produced by CBGC (  X  =0.0). Figure 16. The Embeddings Produced by CBGC (  X  =0.2). Figure 17. The Embeddings Produced by CBGC (  X  =0.4). Figure 18. The Embeddings Produced by CBGC (  X  =0.6). Figure 19. The Embeddings Produced by CBGC (  X  =0.8). 
Figure 20. The Embeddings Produced by CBGC (  X  =1.0). In the field of text categorization, hierarchical taxonomy classification is widely used to get better trade-off between effectiveness and efficiency compared with flat taxonomy classification. Unfortunately, many data sets are not explicitly organized in hierarchical forms. To take advantages of hierarchical classification, people have to mine a hierarchical taxonomy from the data set. We can see that the relationship between categories, documents and terms is just a star structure, so we modeled it by a tripartite graph and solved it by the CBGC algorithm. Then the category clusters could be used to generate a hierarchical taxonomy. In this section, we would like to show that the proposed algorithm could output reasonable category clusters for hierarchical taxonomy building, while the building process is omitted, for the details of which, please refer to [10]. The real data used in our experiments were sampled from the dataset of 20-newsgroups 3 which contains about 20,000 articles from 20 newsgroups. We picked five categories (see Table 1) and randomly select 30 articles for each category. Then we used the technique as described in the Appendix to build the category-by-document matrix A and used term frequency to build the document-by-term matrix B . Here we carried out feature selection according to [24] for terms so that only 533 terms were reserved. It could be easily seen from the category names listed in Table 1 that the expected co-clustering results on categories should be {C C } and {C 3 , C 4 , C 5 }, and the documents should be clustered according to the categories they belong to. We tuned different values of  X  to see the performance of the TSC algorithm. From Table 2 we can see that the results were disappointing no matter how this parameter was set. In fact, when  X  was small, the algorithm degenerated to co-cluster on a category-document bipartite graph which is unconnected in this data set. Otherwise, all categories were assigned to one cluster. http://people.csail.mit.edu/~jrennie/20Newsgroups Then we ran CBGC on the same real data with  X  1 results under different settings of  X  were shown in Figure 21 to 23. As the total number of categories, documents and terms was too large, we plotted the embeddings of terms in a separate sub-figure to show the results distinctly, although the embedding values of the three heterogeneous objects were co-clustered together. From Figure 21, we found that when  X  = 0.5, the first two categories (C 1 and C 2 ) were clustered together with their documents (the first 60 documents) and a part of terms (the dark-colored points), while the rest categories (C documents (the last 90 documents) and terms (the white-colored points) were grouped into another cluster. When  X  was set to 0 and 1, the results (Figure 22 and 23) looked similar to the degraded cases as shown in Figure 15 and 20. These results confirmed from a practical point of view the advantages of our consistent bipartite graph co-partitioning over analyzing the category-document and document-term bipartite graphs separately. Figure 21. The Embeddings Produced by CBGC (  X  =0.5). Figure 22. The Embeddings Produced by CBGC (  X  =0.0). Figure 23. The Embeddings Produced by CBGC (  X  =1.0). To summarize, our experiments on both toy and real data showed that our concept of consistent bipartite graph co-partitioning can well handle the star-structured high-order heterogeneous data co-clustering, and greatly outperform the traditional spectral clustering either on separate bipartite graphs or on the tripartite graph. In this paper, we used a k -partite graph to represent the star-structured inter-relationships among high-order heterogeneous objects, and proposed the concept of consistent bipartite graph co-partitioning to get the co-clustering of these objects simultaneously. Then we proved our desired consistent co-clustering can be achieved by optimizing a certain objective function based on semi-definite programming. Experiments on both toy and real data showed that our approach worked effectively and efficiently. For the future work, we plan to investigate the following issues: 1. So far, in this paper we only discussed the case of a partition 2. We will put effort on developing a fast and simple technique 3. We are willing to implement the proposed algorithm on 4. We will study how the CBGC algorithm will perform if we 5. We will further explore whether there are any more 6. We also plan to apply our method to extend other algorithms We would like to thank Professor Kurt M. Anstreicher for his great generosity and enthusiasm in helping us nail down some facts on semi-definite programming. We should also thank Tao Qin, Guang Feng and Huai-Yuan Yang for their constructive suggestions on this work. Dr. Tie-Yan Liu was the corresponding author of this paper. [1] Bach, F.R., and Jordan, M.I. Learning spectral clustering. [2] Benson, H.P. Global Optimization Algorithm for the [3] Boyd, S., and Vandenberghe, L. Convex Optimization. [4] Dhillon, I.S. Co-clustering documents and words using [5] Dhillon, I.S., Mallela, S., and Modha, D.S. Information-[6] Ding, C., He, X., Zha, H., Gu, M., and Simon, H. A min-max [7] Duda, R.O., Hart, P.E., and Stork, D.G. Pattern [8] Frenk, J.B.G., and Schaible, S. Fractional Programming. [9] Fujisawa, K., Fukuda, M., Kojima, M., and Nakata, K. [10] Gao, B., Liu, T., Cheng, Q., Feng, G., Qin, T., and Ma, W. [11] Golub, G.H., and Loan, C.F.V. Matrix computations. Johns [12] Hagen, L., and Kahng, A.B. New spectral methods for ratio [13] Klerk, E. Aspects of Semidefinite Programming: Interior [14] Kluger, Y., Basri, R., Chang, J.T., and Gerstein, M. Spectral [15] Modha, D.S., and Spangler, W.S. Feature Weighting in k -[16] Monteiro, R.D.C. First-and Second-Order Methods for [17] Pardalos, P.M. and Wolkowicz, H. Topics in Semidefinite [18] Pothen, A., Simon, H.D., and Liou, K.P. Partitioning sparse [19] Qiu, G. Image and Feature Co-clustering. ICPR (4) 2004: [20] SDPA Online for your future. http://grid.r.dendai.ac.jp/sdpa/. [23] Wang, J., Zeng, H., Chen, Z., Lu, H., Tao, L., and Ma, W. [24] Yang, Y., and Pedersen J.P. A Comparative Study on Feature [25] Zeng, H., Chen, Z., and Ma, W. A Unified Framework for [26] Zha, H., Ding, C., and Gu, M. Bipartite graph partitioning In this appendix we would like to illuminate how we built the category-by-document matrix A in Section 5.3. Suppose D= { d 1 , d 2 ,..., d n } denotes the documents in the dataset for each document d i in D can be represented by a t -dimensional in document d i . Furthermore, each document is assigned a category label from the set C= { c 1 , c 2 ,..., c m number of categories. The category-by-document matrix A can be easily built according to the information from the corpus. In this matrix, rows correspond to categories and columns to documents. Each element A indicates the correlation between document d j and category c If document d j belongs to k categories c 1 , c A column of matrix A are set to zero. After a series of processing as described above, the category-by-document matrix A can be easily obtained.  X 
