 A.1. Proof of Corollary 1 Restatement of Corollary 1: Assume we have a family of functions F  X  , a KWIK-learning algorithm KWIK for F  X  , and a fixed-state opti-mization algorithm FixedStateOpt . Then there exists a no-regret algorithm for the MAB problem on F  X  . Proof. Let A ( , X  ) denote Algorithm 1 when parame-terized by and  X  . We construct a no-regret algorithm A  X  for the MAB problem on F  X  that operates over a series of epochs. On the start of epoch i , A  X  simply rounds. We will describe how i , X  i , X  i are chosen. First let e ( T ) denote the number of epochs that A  X  starts after T rounds. Let  X  i be the average regret suffered on the i th epoch. In other words, if x i,t ( a i,t ) is the t th state (action) in the i th epoch, then  X  i = E We therefore can express the average regret of A  X  as: From Theorem 1, we know there exists a T i and choices for i and  X  i so that  X  i &lt; 2  X  i so long as  X  i  X  T i . Let  X  1 = T 1 , and  X  i = max { 2  X  i  X  1 ,T i } . These choices for  X  , i and  X  i guarantee that  X  i  X  1  X   X  i / 2, and also  X  i &lt; 2  X  i . Applying these facts respectively to Equation 1 allows us to conclude that: Theorem 1 also implies that e ( T )  X   X  as T  X   X  , and so A  X  is indeed a no regret algorithm.
 A.2. Proof of Corollary 2 Restatement of Corollary 2: If the don X  X -know bound of KWIK is B ( , X  ) = O (  X  d log k  X   X  1 ) for some d &gt; 0 ,k  X  0 then there are choices of , X  so that the average regret of Algorithm 1 Depending on the family of trapdoor functions, the sec-ond condition usually holds under an assumption that some problem is intractable (e.g. prime factorization). We are now ready to describe ( F  X  , A , X ). Fix n , and let X = Z n and A = Z n  X  X  a  X  } . For any h  X   X  H  X  , let h  X  denote the inverse function to h  X  . Since h  X  may be many-to-one, for any y in the image of h  X  , arbitrarily define h  X  1  X  ( y ) to be any x such that h  X  ( x ) = y . We will define the behavior of each f  X   X  F  X  in what follows. First we will define a family of functions G  X  . The behavior of each g  X  will be essentially identical to that of f  X  , and for the purposes of understanding the construction, it is useful to think of them as being exactly identical.
 The behavior of g  X  on states x  X  Z n is defined as follows. Given x , to get the maximum payoff of 1, an algorithm must invert h  X  . In other words, g  X  ( x , a ) = 1 only if h  X  ( a ) = x (for a  X  Z n , and not equal to the  X  X pecial X  action a  X  ). For any other a  X  Z n , g  X  ( x , a ) = 0.
 Specifically g  X  ( x , a  X  ) = 0 . 5 and g  X  ( x , a  X  ) = 0 if x is not in the image of h  X  . It X  X  useful to pause here, and consider the purpose of the construction. Assume that  X  pub is known. Then if x and a ( a  X  Z n ) are presented simultaneously in the supervised learning setting, it X  X  easy to simply check if h ( x ) = a , making accurate predictions. In the fixed-state optimization setting, querying a  X  presents the algorithm with all the information it needs to find a maximizing action. However, in the bandit setting, if a new x is being drawn uniformly at random and presented to the algorithm, the algorithm is doomed to try to invert h  X  .
 Now we want the identity of  X  pub to be revealed on any input to the function f  X  , but want the behavior of f  X  to be essentially that of g  X  . In order to achieve this, let b X c  X  be the function which truncates a number to p = 2 d + 2 bits of precision. This is sufficient preci-sion to distinguish between the two smallest non-zero Also fix an encoding scheme that maps each  X  pub to a unique number [  X  pub ]. We do this in a manner such [  X  pub ]. Intuitively, f  X  mimics the behavior of g  X  in its first p bits, then encodes the identity of  X  pub in its subsequent p bits. [  X  pub ] is the smallest output of f  X  , and  X  X cts as X  zero.
 and then providing BANDIT with the state h  X  ( k t ). At which point, BANDIT will output an action and demand a reward. If the action selected by bandit is the special action a  X  , then its reward is simply b 0 . 5 / (1 + k ) c  X  + [  X  pub ]. If the action selected by bandit is a t satisfying h reward is [  X  pub ].
 By hypothesis, with probability 1 / 2, the actions a t generated by BANDIT must satisfy h ( a t ) = h  X  ( k t ) for at least one round t  X  T . Thus, if we choose a round  X  uniformly at random from { 1 ,...,q ( T ) } , and give state h ( k  X  ) to BANDIT on that round, the action a  X  returned inverts h  X  ( k  X  ), and contradicts the assumption that h  X  belongs to a family of cryptographic trapdoor func-tions.
 A.4. Proof of Theorem 5 We now show that relaxing KWIK to supervised no-regret insufficient to imply no-regret on MAB.
 Restatement of Theorem 5: (Relaxing KWIK to supervised no-regret insufficient to imply no-regret on MAB) There exists a class F that is supervised no-regret learnable such that if N ( t ) = for any learning algorithm A and any T , there is a sequence of trials in the arriving action model such that R A ( T ) /T &gt; c for some constant c &gt; 0 . Proof. First we describe the class F . For any n -bit string x , let f x be a function such that f x ( x ) is some large value, and for any x 0 6 = x , f x ( x 0 ) = 0. It X  X  easy to see that F is not KWIK learnable with a polyno-mial number of don X  X -knows  X  we can keep feeding an algorithm different inputs x 0 6 = x , and as soon as the algorithm makes a prediction, we can re-select the target function to force a mistake. F is no-regret learn-able, however: we just keep predicting 0. As soon as we make a mistake, we learn x , and we X  X l never err again, so our regret is at most O (1 /T ).
 Now in the arriving action model, suppose we initially start with r distinct functions/actions f i = f x i  X  F , i = 1 ,...,r . We will choose N ( T ) = sublinear, and r = as we want. So we have a no-regret-learnable F and a sublinear arrival rate; now we argue that the arriving action MAB problem is hard.
 Pick a random permutation of the f i , and let i be the indices in that order for convenience. We start the task sequence with all x 1  X  X . The MAB learner faces the problem of figuring out which of the unknown f i s has x 1 as its high-payoff input. Since the permutation
