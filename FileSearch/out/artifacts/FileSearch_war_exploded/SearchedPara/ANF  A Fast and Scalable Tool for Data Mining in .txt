
Graphs are an increasingly important data source, with such important graphs as the Internet and the Web. Other fa-miliar graphs include CAD circuits, phone records, gene se-quences, city streets, social networks and academic citations. 
Any kind of relationship, such as actors appearing in movies, can be represented as a graph. This work presents a data mining tool, called ANF, that can quickly answer a number of interesting questions on graph-represented data, such as the following. How robust is the Internet to failures? What are the most influential database papers? Are there gen-der differences in movie appearance patterns? At its core, 
ANF is based on a fast and memory-efficient approach for approximating the complete "neighbourhood function" for a graph. For the Internet graph (268K nodes), ANF's highly-accurate approximation is more than 700 times faster than the exact computation. This reduces the running time from nearly a day to a matter of a minute or two, allowing users to perform ad hoc drill-down tasks and to repeatedly answer questions about changing data sources. To enable this drill-down, ANF employs new techniques for approxi-mating neighbourhood-type functions for graphs with dis-tinguished nodes and/or edges. When compared to the best existing approximation, ANF's approach is both faster and more accurate, given the same resources. Additionally, un-like previous approaches, ANF scales gracefully to handle disk resident graphs. Finally, we present some of our results from mining large graphs using ANF. *This material is based upon work supported by the Na-tional Science Foundation under Grants No. IIS-9910606, 
IIS-9988876, IIS-0083148, IIS-0113089, IIS-0209107 and by the Defense Advanced Research Projects Agency under Con-tract No. N66001-00-1-8936. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, DARPA, or other funding parties. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. esting to the data mining community, for example in un-derstanding the Internet and the WWW. These entities are modeled as graphs where each node is a computer, admin-istrative domain of the Internet, or a web page, and each edge is a corlnection (network or hyperlink) between the resources. Go0gle is interested in finding the most "impor-tant" nodes irl such a graph [2]. Broder et al. studied the connectivity ill_formation of nodes in the Internet [13, 3]. 
The networking community has used different measures of node "importarice" to build a hierarchy of the Internet [20]. 
Another source of graph data that has been studied are cita-tion graphs [18], Here, each node is a publication and each edge is a citation from one publication to another and we wish to know the most important papers. There are many more examples of graphs which contain interesting informa-tion for dates raining purposes. For example, the telephone calling records from a long distance carrier can be viewed as a graph, and by mining the graph we may help iden-tify fraudu! X rlt behaviour or marketing opportunities. DNA sequencing con also be viewed as a graph, and identifying common sub~eqtlences is a form of mining that could help scientists. Cir X ~t design, for example from a CAD system, forms a graph ~nd data mining could be used to find com-monly used cor~ppnents, points of failure, etc. ple, in this paper .we use a graph derived from the Internet of these data sources, we find some prototypical questions which have motivated this work: 
It is possible to answer all of these questions by comput-ing three graph properties pertaining to the connectivity or neighbourhood structure of the graph: Graph Similarity: Given two graphs, Gi and G2, do the graphs have similar connectivity / neighbourhood structure (independent of their sizes). Such a similarity measure is useful for answering questions 1, 4, and 5. Subgraph Similarity: Given two subsets of the vertices of the graph, Vi and V2, compare how these two induced subgraphs are connected within the graph. Such a similarity measure is useful for answering questions 2, 4, 8, 9, and 10. Vertex Importance: Assign an importance to each node in the graph based on the connectivity. This importance measure is useful for answering questions 1, 3, 6, and 7. 
We answer questions 1, 7 and 10 in this paper, one from each of the three types. The remaining questions can be answered in a similar fashion, using various forms of the Neighbourhood Function. The basic neighbourhood func-tion, N(h), for a graph, also called the hop plot [8], is the number of pairs of nodes within a specified distance h, for all distances h. In section 2 we will define this more formally and present a more general form of the neighbourhood func-tion that can be used to compute all three graph properties. 
The main contribution of this paper is a tool that allows us to compute these three graph properties, thereby enabling us to answer interesting questions like those we suggested. Beyond simply answering the questions, we want our tool to be fast enough to allow drill-down tasks. That is, we want it to be possible to interactively answer users requests. For example, to determine the best roads to close for a parade, the city planner would want to interactively consider vari-ous sets of street closures and compare the effect on traffic. Almost in contrast to the need to be able to run interac-tively on graphs, we also want a tool that scales to very large graphs. In [3, 13], measuring properties about the web required hardware resources that are beyond the means of most researchers. Instead, we produce a data mining tool that is useful given any amount of RAM. These two goals give rise to the following list of properties that our tool must satisfy when analyzing a graph with n nodes and m edges: Error guarantees: estimates must be accurate at all dis-tances (not just in the limit). Fast: scale linearly with # of nodes and # edges (n, ra). Low storage requirements: use only O(n) additional storage. Adapts to the available memory: when the node set does not fit in memory, make effective use of the available memory. 
Parallelizable: for massive graphs, must be able to dis-tribute the work over multiple processors and/or multiple workstations, with low overheads. 
Sequential scans of the edge file: avoid random accesses to the graph. Random accesses exhibit horrible paging per-formance for the common case that the graph is larger than the available memory. 
Estimates per node: must be able to estimate the neigh-bourhood function from each node, not just for the graph as a whole. This paper presents such a tool, which we call ANF for 
Approximate Neighbourhood Function. In the literature, we set of concluding nodes. We are interested in the number of pairs starting from a node in S to a node in C within distance h: for u at h given C is the number of nodes in C within dis-tance h. number of,pairs of a node from S and a node from C that are within distance h or less. 
In section 5 we will use the neighbourhood function to characterize graphs. We will compare NG1 (h) to NG2 (h) to measure the similarity in connectivity/neighbourhood struc-ture of graphs G1 and G2. For example, if we want to know the structural similarly of the Web from 1999 to to-day's Web, we can compute their neighbourhood functions and compare them at all points. Comparing subgraphs in-duced by vertex sets Vi and V2 can be done by comparing 
N+(h, V1,V1) to N(h, V2,V2). E.g., let I,~ be the routers in the Canadian domain and V2 be the routers in the French domain. Finally, we will use the individual neighbourhood function for a node to characterize its importance, with re-spect to the connectivity. E.g., the most important router is the one that in some way reaches the most routers the fastest. bourhood function efficiently, then we can answer the graph questions that we posed in the introduction. [VI + [E I respectively. N(2) is reminiscent of the size of the (self-)join of the edge relation: each edge is viewed as a tuple with attributes "first" and "second" and N(2) -N(1) is the size of the result of the query select distinct El.first, E2.second from edge-rel El, edge-tel E2 where El.second = E2.first 
Writing N(2) -N(1) in this way illustrates the difficulty in efficiently computing N(h) for any h &gt; 2. The means that we must know which of the n Y possible pairs of nodes have already been counted and we must take care not to over count in the presence of multiple paths between the same pair of nodes. One approach to computing N(h) repeatedly multiply the graph's adjacency matrix. Asymp-totically, this could be done in O(n 23s) time. Unfortunately, we would also require O(n 2) memory, which is prohibitive. Instead, it is common to use breadth first searches in the graph. A breadth-first search beginning from u can easily compute IN(u, h) for all h. We can compute N(h) ning a breadth-first search from each node u and summing over all u. This takes only O(n + m) storage but the worse case running time is O(nrn). For large, disk resident graphs, a breadth-first search results in an expensive random-like ac-cess to the disk blocks. This appears to be the state of the art solution for exact computation of N(h). 
The transitive closure is N(c~) or, equivalently, N(d), where d is the diameter of the graph. Lipton and Naughton [14] presented an O(nv/~ ) algorithm for estimating the tran-sitive closure that uses an adaptive sampling approach for selecting starting nodes of breadth-first traversals. Moti-vated by this work, in section 4 we will experimentally eval-uate a similar sampling strategy to discover that it scales poorly to large graphs and, due to the random-like access to the edge file, it does not scale to graphs larger than the RAM. Most importantly, however, we will find that the quality of this approximation can be quite poor (we show an example where even a sample as large as 15% does not provide a useful approximation!). Lipton and Naughton's work was improved by Cohen, who gave an O(m) time al-gorithm using only O(n + m) memory [4]. Cohen also pre-sented an O(m log n) time algorithm for estimating the indi-vidual neighbourhood functions, IN(u, h). This appears to be the only previous work which attempts to approximate the neighbourhood function. More details of this algorithm, which we refer to as the RI approximation, appear in sec-tion 4.1.1 when we experimentally compare it to our new approximation. Our experiments demonstrate that the RI approximation is ill-suited for large graphs; this is due to its extensive use of random-like access (for breadth first search, heap data structures, etc.). 
The problems of random access to a disk resident edge file has been addressed in [15]. They find that it is possible to define good storage layouts for undirected graphs but that the storage blowup can be very large. Given that we are in-terested only in very large graphs and graphs with directed edges, this does not solve the problems related to large edge files. Instead, we will need to find a new computation strat-egy which avoids random access to disk. 
State-of-the-art approaches to understanding and char-acterizing the Internet and the Web very often make use of neighbourhood information [3, 13, 1, 20]. Other recent work in data mining for graphs has focused on mining frequent substructures. Essentially, the problem of finding frequent itemsets is generalized to frequent subgraphs. Systems for this include SUBDUE [5] and AGM [11]. Graphs have been used to improve marketing strategies [7]. A survey of work on citation analysis appears in [18]. We are given a graph G = (V,E). We assume that V = {0, 1,... , n-1} and that E contains m directed edges. 
Undirected graphs can be represented using pairs of directed edges. We wish to approximate the function N+(h, S, C) and IN+(x, h, C) for a node x, allowing us to compute //Set A.4(x,0) = {x} FOR each node x DO FOR each distance h starting with 1 DO FOR each node x DO M(x,h) = M(x,h -1) Vo Update .A4(x, h) by adding one step R each edge (x,y) DO //Compute the estimates for this h FOR each node x DO Individual estimate I1V(x, h) = (25)/.77351 where b is the average position of the least zero bits in the k bitmasks The estimate is: 2f/(h) = ~all x IiV(x, h) 
Figure 2: Introduction to the basic ANF algorithm and IN(x, h). The approximation must be done accurately and in such a way that we will be able to handle disk resident graphs. In this section, we construct such an approximation gradually. First, we approximate N(h) and/or IN(x, h) as-suming memory-resident data structures. We extend this al-gorithm to approximate N + (h, S, C) and I N+ (x, h, C) but still requiring sufficient RAM for processing. Next, we move the data structure to disk and to create an algorithm that meets all of our requirements. Finally, we will extend this algorithm with bit compression to further increase its speed. 
A graph traversal effectively accesses the edge file in ran-dom order. Thus, if our algorithm is going to be efficient on a graph that does not fit in memory, we cannot perform any graph traversals. Instead, we are going to build up the nodes reachable from x within h steps by first finding out which nodes its neighbours can reach in h -1 steps. More formally, let .A4(x, h) be the set of nodes within distance h of x. Clearly, .Ad(x, 0) = {z}, since the only node within distance 0 of z is x itself. To compute .A4(x, h) we note that x can still reach in h or fewer steps the nodes it could reach in h -1 or fewer steps. But, x can also reach the nodes in .A4(y, h -1) if there is an edge from x to y. That is: .M(x,0) = {x} for all x 6 V 
FOR each distance h DO fl4(x, h) = .A4(x, h -1) for all x 6 V 
FOR each edge (x, y) DO fl4(x,h) = .hA(x,h) U A4(y,h -1) 
This iterates over the edge set instead of performing a traversal. The trick will be to efficiently compute the num-ber of distinct elements in A4(x,h). One possibility is to use a dictionary data structure (e.g., a B-tree) to represent the sets fl4(x, h). However, this approach needs O(n 2 logn) time and space, which is prohibitive. An approach that people, particularly C hackers, often employ is to use bits to mark membership. That is, each node is given one of n bits and a set is a bit string of length n. To add a node to the set, we mark its bit. The union of two sets is the bitwise-OR of the bitmasks. Unfortunately, this approach still uses O(n 2) memory, which will be prohibitive for large graphs. 
Instead, we're going to use a clever probabilistic count-ing algorithm [9] to approximate the sizes of the sets using shorter bit strings (log n + r bits, for some small constant r). We refer to the bit string that approximates .A4(x,h) x M(x,O) 0 100 100 001 1 010100100 2 100 001 100 3 100 100 100 4 100 010 100 IF x 6 C THEN FOR each node x DO Mlast(x) = Mcur(x) FOR each edge (x, y) DO FOR each node x DO 1Q+(h, S, C) = Zzes IN(x, h, C) as M(x, h). Instead of giving each node its own bit, we are going to give about half the nodes bit 0, a quarter of them bit 1, and so on (give a node bit i with probability l/T+1). 
We still mark a node by setting its bit and use bitwise-OR for the set-union. Estimating the size of the set from the small bit string is done based on the following intuition. If we expect 25% of the nodes to be assigned to bit 1 and we haven't seen any of them (bit 1 is not set), then we prob-ably saw about 4 or less nodes. So, the approximation of the size of the set .A4(x, h) is proportional to 2 b, where b is the least bit number in M(x, h) that has not been set. 
We refer the reader to [9] for a derivation of the constant of proportionality and a proof that this estimate has good error bounds. 
A single approximation is obviously not very robust. We do k parallel approximations by treating M(x, h) as a bit-string of length k(log n+r) bits. Figure 2 shows the complete algorithm implementing the edge-scan based ANF. 
Example. Figure 3 shows the bitmasks and approxima-tions for a simple example of our most basic ANF algorithm. 
The purpose is to clarify the concatenation of the bitmasks and to illustrate the computation. The input is a 5 node undirected cycle and we used parameters k = 3 and r --0. 
The first FOR loop of the algorithms generates the table of random bitmasks M(x, 0). That is, using an exponential distribution, we randomly set one bit in each of the three concatenated bitmasks. (In the figure, bit 0 is the leftmost bit in each 3-bit mask.) Then, each iteration uses the 0R operation to combine the nodes that it could reach in h-1 steps plus the ones that its neighbours could reach in h -1 steps. For example, M(2, 1) is just M(1, 1) OR M(2, 1) OR 
M(3, 1), because nodes 1 and 3 are the neighbors of node 2. The estimates, for example I1V(2, 1), are computed from 24/3/.77359 = 3.25). 
The algorithm in Figure 2 uses an excessive amount of memory and does not estimate the more general forms of the neighbourhood function. Figure 4 depicts the same al-gorithm, with t:he following improvements:  X  M(x, h) nses M(y, h -1) but never M(y, h -2). Thus we use Mcur(x) to hold M(x, h) and Mlast(y) to hold 
M(y, h -1) during iteration h. 
The ANF-0 algorithm meets all but one of the require-ments set out in the introduction: Error guarantees: each IN+(x,h,C) is provably esti-mated with low error with high confidence. Fast: running time is O((n + re)d) which we expect to be fast since d is typically quite small (verified in section 4). Low storage requirements: only additional memory for Mcur and Mlast. Adapts to the available memory? No[ We will address this issue in the next section. Easily parallelizable: Partition the nodes among the pro-cessors and then each processor may independently compute Mcur for each x in its set. Synchronization is only needed after each'iteration. Sequential scans of the edge file: Yes. Estimates IN(x, h): Yes, with provable accuracy. 
The ANF-0 algorithm no longer accesses the edges in ran-dom order, but we now access Mcur and Mlast in an effec-tively random order. When we see the edge (x, y) we read and write Mcur(x) and read Mlast(y). If these tables are larger than the available memory, swapping will kill perfor-mance. We propose a small amount of preprocessing, to make these accesses predictable. Our idea is to break the large bitmasks Mcur and Mlast into bl and b2, resp., equal-sized pieces. We partition the edges into bl x b2 buckets. In most cases, a one pass bucket sort can be used to partition the edges. Given that we have partitioned the edges, we would like to run the following algorithm to update Mcur: FOR each bucket i of Mcur DO Load bucket i of Mcur FOR each bucket j of Mlast DO Write bucket i of Mcur 
The cost of this algorithm is exactly the same cost as running ANF-0 plus the cost of the I/O (we have simply reordered the original computation). If Mcur and Mlast are N bytes long, then the cost of the I/O required to update the bitmasks is: 2N to load and store each bucket of Mcur and biN to read Mlast once for each bucket of Mcur. That is, the cost of this I/O is (bl + 2)N. Thus, we select bl and b2 such that bl is minimal, given that we have enough file descriptors to efficiently perform the bucket sort in one pass. 
Note that by reordering the computation to bucketize the edges, we now have very predictable I/O. Thus, we will in-sert prefetching operations which allows the computation and the I/O to be performed in parallel. The complete al-gorithm with prefetching appears in Figure 5. This algorithm now meets all of our requirements. 
Like the mark bit, this counter can be prepended to the bitmask. In our experiments, we will find that leading ones compressions provide a significant speed-up, up to 23% in 
Figure 1. 
In this section we present an experimental validation of our ANF approximation. Two alternative approaches will be introduced and then we will describe our data sets. Next, we propose a metric for comparing two neighbourhood func-tions (functions over a potentially large domain). We con-duct a sensitivity analysis of the parameter r. Then, we pick a value of r and we compare ANF to the approximation pre-sented in [4] for various settings of the parameter k. We then show that sampling can provide very poor estimates and, fi-nally, we examine the scalability of all approaches. The key results from this section are to answer these questions: 1. Is ANF sensitive to r, the number of extra bits? 2. Is ANF faster than existing approaches? 3. Is ANF more accurate than existing approaches? 4. Does ANF really scales to very large graphs? Do the others? 
The RI approximation algorithm is based on the approx-imate counting scheme proposed in [4]. To estimate the number of distinct elements in a multi-set, assign each a random value in [0, 1] and record the least of these values added to the set. The estimated size is the reciprocal of the least value seen, minus 1. This approximate counting scheme was used to estimate the individual neighbourhood functions with the following algorithm. 
We need to know for each node, u the minimum value vn of a node reachable from u in h hops. Then, the es-timate for IN(u, h) is ~ -1. An equivalent, but more efficient algorithm was presented which uses breadth-first searches. It was shown that this improved procedure takes only O(m log n) time (with high probability). To reduce the variance in the estimates, the entire algorithm is repeated, averaging over the estimates. 
We can sample by selecting random edges, random nodes or random starting nodes for the breadth-first search. Ran-domly selecting a set of nodes (and all edges for which both end-points are in this set) and randomly selecting a set of edges is unlikely to produce a useful sample. For exam-ple, imagine sampling a cycle -anything but a very large sample will leave disconnected arcs which have very dif-ferent properties. For completeness we verified that these approaches produced useless estimates. The last approach is much more compelling. It is akin to the sampling done in [14]. Recall that the neighbourhood function is: ~ev IN(u,h). Rather than summing over all nodes, u, we could sum over only a sample of the nodes while using breadth-first searches to compute the exact IN(u, h). call this method exact-on-sample and it has the potential to provide great estimates -a single sample of a cycle will provide an exact solution. However, experimentally we find that this approach also has the potential to provide very Name es #Edges Max. | Avg. Diam. Orient. Cornell 844 1,647 131 1.95 9 Dir Cycle 1,000 1,000 2 2.00 500 Undir Grid 10,000 19,800 4 1.98 100 Undir Uniform 65,378 199,996 20 3.06 8 Undir 
Cora 127,083 330,198 457 2.60 35 Dir 80-20 166,946 449,832 723 2.69 10 Undir 
Router 284,805 430,342 1,978 1.51 13 Undir poor estimates. Additionally, we find that it does not scale to large graphs because of the random-like access to the edge file due to its use of breadth-first search. 
We have collected three real data sets and generated three synthetic data sets. These data sets have a variety of prop-erties and cover many of the potential applications of the neighbourhood function. Some summary information is pro-vided in Table 2. "Prac. Diam." is the Practical Diameter which we use informally to mean the distance which includes most of the pairs of points. We use three real data sets: 
Router: Undirected Intemet routers data from ISI [19], including scans done by Lucent Bell Laboratories [12]. Cornelh A crawl of the Cornell web site by Mark Craven. 
Cora: The CORA project at JustResearch found research papers on the web and provided a citation graph [6]. and four synthetic data sets: Cycle: A single simple undirected cycle (circle). Grid: A 2D planar grid (undirected). 
Uniform: Graph with random (undirected) edges. 80-20: Very skewed graph generated in an Internet like fashion with urldirected edges using the method in [17]. We are approximating functions defined over d points. Let 
N be the true neighbourhood function and N be the candi-date approximation. To measure the error of N(h), we use the standard relative error metric. To measure the overall error of N we use the Root Mean Square (RMS) of point-wise relative errors. Thus, the error function, e, is: Note that the RMS is computed beginning with h = 2. 
Since N(0) = IVI and N(1) = IEI we do not require approx-imations for these points. ANF has two parameters: the number of parallel approx-imations, k, and the number of additional bits, r. The num-ber of approximations, k, is a typical trade-off between time and accuracy. We consider this in section 4.2.2 and fix k = 64 for the time being. Additional experiments were run with other values of k which produced similar results. To measure the sensitivity we averaged the RMS error over 10 We now examine the accuracy of the ANF approximation.  X  ANF's error is independent of the data sets.  X  RI approximation's error varies significantly between data sets.  X  ANF achieves less than 10%, 7% and 5~0 errors for k = 32, k = 64 and k = 128, respectively.  X  RI has errors of 27%, 14% and 12% for k = 32, k = 64 and k = 128, respectively.  X  ANF is much faster than RI, particularly on the larger graphs, with up to 3 times savings.  X  Using much less time, ANF is much more accurate than RI. 
Thus, even for the case of graphs that may be stored in 
There are three problems with the described exact-on-
We now provide an example which demonstrates the of the sampled nodes will be from the N center nodes and very few will be from the s source nodes. This will result in an error that is a factor of around sip for exact-on-sample using a p% sample. We measure the error and the running time over 20 trials for a variety of sample sizes ranging from .1% to 15% on a graph generated with N = 25,000, s = 100, t = 100 and d = 6. Figure 8(b) shows the large errors, more than 20%, even for very large samples. constructed a graph with N = 250,000, s = t = 5 and d = 6. 
We then increase s and t proportionately to generate larger graphs. Figure 8(c) shows that as the graph grows larger exact-on-sample scales about as well as ANF but as soon as the edge file no longer fits in memory (approximately 27 million edges) we see approximately a two order of magni-tude increase in the running time of the exact-on-sample ap-proach. Thus, we conclude that exact-on-sample scales very poorly to graphs that are larger than the available memory. unloaded Pentium II-450 machine for both the exact compu-tation (Breadth-First search) and ANF with k = 64 parallel approximations. We chose k = 64 since it provides much less than a 10% error, which should be acceptable for most situations. The approximations are quite fast and, for the 
Router data set, we have reduced the running time from ap-proximately a day down to less than 2 minutes. This makes it possible to run drill down tasks on much larger data sets than before. Overall, we find that ANF is up to 700 times faster than the exact computation on our data sets. tives. We generated random graphs placing edges randomly between nodes. We increased the number of nodes and edges while preserving an edge:node ratio of 8:1 (based on the average degree found in two large crawls of the Web [3]). 
Figure 1 (in the introduction) shows the running times for the ANF variants, the KI approximation and example-on-sample. Parameters for each alternative were chosen such that they all had approximately the same running time for the first data point. These values are k = 32 for the ANF variants, k = 8 for RI and p = 0.0015 for exact-on-sample. 
We find that: (a) Graph exhibiting huge errors 4. Approximate counting methods [9, 4] are not enough for disk resident graphs. 5. ANF/ANF-C scale the best, growing piece-wise lin-early with the size of the graph. The break points are: all data fits in memory (about 8 million edges), Mcur fits in memory (about 16 million edges) and neither fits in memory (the rest). This is as expected. 6. ANF-C offers up to a 23% speed-up over ANF. 
Thus, ANF is the only algorithm that scales to large graphs and does so with a linear increase in running time. With our highly-accurate and efficient approximation tool, 
ANF, it is now possible to answer some of the prototypical graph mining questions that we posed in the introduction. 
Due to the limits of page constraints and data availability, we will report on answers to only a representative sample of those questions. However, all 10 questions can be answered by the same approaches that we will now demonstrate. The approach is to compute various neighbourhood functions and then to compare them. Our tool allows for a detailed comparison of these functions. However, comparing neigh-bourhood functions requires that we compare two functions over potentially large domains (the domain is {1, 2,... , d}). Instead, in this paper we will focus on a summarized statis-tic derived from the neighbourhood function, called the hop exponent. Many real graphs [8] have a neighbourhood func-tion that follows a power law N(h) 0 X  h n. The exponent, 7-/, has been defined as the hop exponent (similarly, ~= is the individual Jiop exponent for a node x). 
There are three interesting observations about the hop exponent that make it an appealing metric. First, if the power-law holds, the neighbourhood function will have a linear section with slope 7"l when viewed in log-log scale. Second, the hop exponent is, informally, the intrinsic di-mensionality of the graph. A cycle has a hop exponent of 1 while a grid has a hop exponent of 2, which corresponds with some idea of their dimensionality. Third, if two graphs have different hop exponents, there is no way that they could be 
Figure 9: ANF finds the best starting move for X similar. While not all neighbourhood functions will actu-ally follow a power-law, we have found that using the hop exponent still fairly reasonably captures the growth of the neighbourhood function. 
To compute the hop exponent, we first truncate the neigh-bourhood function at deft, the effective diameter, compute the least-squares line of best fit in log-log space to extract the slope of this line. The slope is the hop exponent of the graph and we use it as our measure of the growth of a neighbourhood function. We define def t to be the least h such that it include 90% of the pairs of nodes. We use the individual hop exponent, 7-(x, as a measure of a node's importance with respect to the connectivity of a graph. We can answer some of the proposed questions. 
Tic-tac-toe is a simple game in which two players, one using X and the other using 0, alternatively place their mark on an unoccupied square in a 3x3 grid. The winner is the first player to connect 3 of their symbols in a row, column or diagonal. The best opening move for X is the center square, the next best is any of the 4 corners and the worst moves are the 4 remaining squares. To verify that our notion of importance has some potential use, we will use our ANF tool to discover this same rule. Construct a graph where each node is a valid board and add an edge from board x to board y to indicate that this is a possible move. Let C, the concluding set, be the set of all boards in which X wins. 
Compute the individual neighbourhood functions for each of the 9 possible first moves by X, which is their importance (speed at which they attain winning positions from each of these moves). Figure 9 shows these importances along with the difference between each and the best move. ANF determined the correct importance of each opening move. 
Using Figure 9(b), we see that the center is only slightly better than a corner square which is, in turn, much better than the remaining 4 squares. This shows both the correct ordering of the starting moves and the relative importance of each. 
The Internet Movie Data Base (IMDB) [10] is a collec-tion of relations about movies. We will map a subset of the relations into graphs to illustrate questions that we can now answer thanks to our ANF tool. First, construct the actor-movie graph by creating a node for each actor and a node for each movie. An undirected edge is placed between an actor, a, and a movie, m, to indicate an appearance by a in m. Let S be the nodes we created for the actors. We now employ another relation of the IMDB. Each movie has been identified as being in one or more genres (such as documen-taries, dramas, comedies, etc). For each genre, we take the set of movies in that genre and the set of actors which appear in one or more of those movies. We then cluster these graphs by computing the hop exponents and forming clusters that have similar hop exponents (less than 0.1 difference). This 
Figure 10: Movie genre clusters sorted in increasing hop exponent value clustering appears in Figure 10. One interesting cluster is mystery, musical, western, war which actually corresponds to movies that are typically older. Finally, other fringe gen-res such as Adult turn out to be well separated from the others. 
In the networking community, a study used an early ver-sion of our ANF tool (ANF-0) to look at the inherent robust-. hess of the Internet. That is, the robustness that we observe from the topology itself. Each router was a node and edges were used to indicate communication links. Here we repro-duce some of the results of this study, including Figure 11, from [16]. The goal is to determine how robust the Inter-net is to router failures. As an experiment, we delete some number of routers and then measure the total connectivity (number of pairs of routers that are still able to communi-cate) and the hop exponent of the graph. The three lines differ in how the deleted routers are selected. First, ran-domly selected nodes are deleted. Second, nodes axe deleted in decreasing order of their importance. Third, routers are deleted in decreasing order of their degree. Here we see some very interesting results: 2. It may be possible to take a random sample of the In-3. Targeted failures of routers can very quickly and very This type of study was infeasible before our ANF tool as an exact computation would have required over a year of computation time. 
In this paper we presented 10 interesting data mining questions on graph data, proposed an efficient and accu-rate approximation algorithm that gives us the tool, ANF, we needed to answer these questions, and presented results for three of these questions on real-world data. We have found ANF to be quite useful for these and other questions that can be addressed by studying the neighbourhood struc-ture of the underlying graphs (e.g., we have used ANF to study the most important movie actors). We experimentally verified that ANF provides the following advantages: Highly-accurate estimates: Provable bounds which we also verified experimentally, finding less than a 7% error (a) Number of pairs of nodes that can communicate vs. number of deleted nodes when using k = 64 parallel approximations (for all our syn-thetic and real-world data sets). Is orders of magnitude faster: On the seven data sets used in this paper, our algorithm is up to 700 times faster than the exact computation. It is also up to 3 times faster than the RI approximation scheme. Has low storage requirements: Given the edge file, our algorithm uses only O(n) additional storage. Adapts to the available memory: We presented a disk-based version of our algorithm and experimentally verified that it scales with the graph size. Can be parallelized: Our ANF algorithm may be paral-lelized with very few synchronization points. Employs sequential scans: Unlike prior approximations of the neighbourhood function, our algorithm avoids random access of the edge file and performs one sequential scan of the edge file per hop. Individual neighbourhood functions for free: ANF computed approximations of the individual neighbourhood functions as a byproduct of the computation. These approx-imations proved to be very useful in identifying the "impor-tant" nodes in a graph. 
Even for the case that graphs (and data structures) fit into memory, ANF represents a significant improvement in speed and accuracy. When graphs get too large to be pro-cessed effectively in main memory, ANF makes it possible to answer questions that would have been at least infeasible, if not impossible, to answer before. In addition to its speed, we found the neighbourhood measures to be useful for dis-covering the following answers to our prototypical questions: 1. We found the best opening moves to tic-tac-toe. 2. We clustered movie genres. 3. We found that the Internet is resilient to random fail-ures while targeted failures can quickly create discon-nected components. 4. We found that sampling the Internet actually preserves some connectivity patterns while targeted failures truly distort it. [1] L. A. Adamic. The small world Web. In Proceedings of the 
European Conf. on Digital Libraries, 1999. [2] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and 
ISDN Systems, 30(1-7):107-117, 1996. [3] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, and 
R. Stata. Graph structure in the Web. In Proceedings of the 9th International World Wide Web Conference, pages 247-256, 2000. [4] E. Cohen. Size-estimation framework with applications to transitive closure and reachability. Journal of Computer and System Sciences, 55(3):441-453, December 1997. [5] Cook and Holder. Graph-based data mining. ISTA: 
Intelligent Systems 8J their applications, 15, 2000. [6] CORA search engine, http://~nm.cora.whizbang.com. [7] P. Domingos and M. Richardson. Mining the network value of custome:rs. In KDD-$O01, pages 57-66, 2001. [8] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law relationships of the internet topology. In SIGCOMM, 1999. [9] P. Flajolet and G. N. Martin. Probabilistic counting algorithms for data base applications. Journal of Computer and System Sciences, 31:182-209, 1985. [10] IMDB. http ://~n~w. imdb. com. [11] A. Inokuchi, T. Washio, and H. Motoda. An apriori-based algorithm tot mining frequent substructures from graph data. In PDKK, pages 13-23, 2000. [12] http ://ce. bell-labs, com/who/ches/map/. [13] S. R. Kumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, A. Tomkins, and E. Upfal. The Web as a graph. In ACM SIGMOD-SIGACT-SIGART Symposium on Principles of 
Database Systems, pages 1-10, 2000. [14] R. J. Lipton and J. F. Naughton. Estimating the size of generalized transitive closures. In Proceedings of 15th 
International Conference on Very Large Data Bases, pages 315-326, 1989. [15] M. H. Nodine, M. T. Goodrich, and J. S. Vitter. Blocking for external graph searching. In Prec. ACM PODS 
Conference (PODS-93), pages 222-232, 1993. [16] C. R. Palmer, G. Siganos, M. Faloutsos, C. Faloutsos, and P. Gibbons. The connectivity and fault-tolerance of the Internet topology. In Workshop on Network-Related Data 
Management (NRDM-~O01), 2001. [17] C. R. Palmer and J. G. Steffan. Generating network toplogies that obey power laws. In IEEE Globecom ~000, 2000. [18] G. Salton ~md M. MeGill. Introduction to Modern 
Information Retrieval. McGraw-Hill, 1983. [19] http ://w~m. isi. edu/scan/mereator/mape, html. [20] S. L. Tauro, C. Palmer, G. Siganos, and M. Faloutsos. A simple conceptual model for the Internet topology. In 
IEEE Globecom ~001, 2001. 
