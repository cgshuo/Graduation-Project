 Recommender systems are frequently used in domains in which users express their preferences in the form of graded judgments, such as ratings. Current ranking techniques are based on one of two sub-optimal approaches: either they op-timize for a binary metric such as Average Precision, which discards information on relevance levels, or they optimize for Normalized Discounted Cumulative Gain (NDCG), which ignores the dependence of an item X  X  contribution on the rel-evance of more highly ranked items. We address the short-comings of existing approaches by proposing GAPfm , the Graded Average Precision factor model, which is a latent factor model for top-N recommendation in domains with graded relevance data. The model optimizes the Graded Average Precision metric that has been proposed recently for assessing the quality of ranked results lists for graded relevance. GAPfm  X  X  advantages are twofold: it maintains full information about graded relevance and also addresses the limitations of models that optimize NDCG. Experimen-tal results show that GAPfm achieves substantial improve-ments on the top-N recommendation task, compared to sev-eral state-of-the-art approaches.

In many recommedation domains user feedback is explic-itly collected in a graded form, e.g., ratings of users on movies. Typically higher grades reflect a stronger prefer-ence for the item. One way to measure the fit of a learned model for graded relevance data (e.g., ratings) is to use a metric such as Root-Mean-Square Error. This metric was adopted as the evaluation metric in the Netflix Prize con-test 1 . However, it is now widely recognized that recom-mendation approaches optimized to minimize the error rate usually achieve poor performance on the top-N recommen-dation task [7, 8]. In practice, users focus their attention on only a small number of recommendations, effectively ignor-ing all but a short list of N recommended items. For this reason, it is more useful to focus the recommendation model on making this short list of top-N items as relevant as pos-sible. This task is essentially a ranking task, i.e., ranking items according to their relevance to the user.

Various models use learning to rank [16] techniques to op-timize binary relevance ranking metrics. For example, sev-eral CF models [19, 21, 22] compute near optimal ranked lists with respect to the Area Under the Curve (AUC), Av-erage Precision (AP) [18] and Reciprocal Rank [26] metrics. However, metrics that are defined to handle binary relevance data are not directly suitable for graded relevance data. In order to apply binary metrics, and CF methods that opti-mize for these metrics, to graded relevance data, it is nec-essary to convert the data to binary relevance data. This conversion is generally accomplished by imposing a thresh-old (e.g., defining rating levels 1-3 as non-relevant and 4-5 as relevant). This process has two major drawbacks: 1) we lose grading information among the rated items, e.g., items rated with a 5 are more relevant than items rated with a 4. This information is crucial in building precise models. 2) the choice of the threshold relevance is arbitrary and will have an impact on the performance of different recommendation approaches.

A well-known metric in the area of information retrieval (IR) is Normalized Discounted Cumulative Gain (NDCG) [11], which can be used to measure the performance of ranked re-sults with graded relevance and is often used for evaluating recommender systems [3, 14, 15, 25, 28]. NDCG is depen-dent on both the grades and the positions of the items in the ranked list. However, NDCG is a so-called  X  X on-cascade X  metric. Under  X  X ascade metrics X , such as Average Precision and Reciprocal Rank, the contribution of a given item has a dependence on the relevance of higher ranked items. In-stead, NDCG assumes independence between the items in the ranked list, i.e., each item contributes to the quality of the ranked list solely based on its own grade and position, http://www.netflixprize.com/ while ignoring the impact of items that are ranked above it. The  X  X on-cascade X  nature of NDCG, has recently drawn crit-icism from authors, who point out the advantages of cascade metrics [5, 20].

Graded Average Precision (GAP) [20] has been proposed as a generalized version of Average Precision in the use sce-nario with graded relevance data. GAP, being similar to Average Precision, reflects the overall quality of the top-N ranked items. Moreover, it inherits all the desirable prop-erties of AP: top-heavy bias, high informativeness, elegant probabilistic interpretation, and solid underlying theoreti-cal basis [20]. In this paper, we propose a new CF ap-proach, i.e., a latent factor model for Graded Average Preci-sion ( GAPfm ), that learns latent factors of users and items so as to directly optimize GAP of top-N recommendations. We propose a novel CF approach GAPfm that directly opti-mizes GAP, which outperforms state-of-the-art CF methods for various evaluation metrics including GAP, Precision and NDCG.
Latent factor models [1, 9] (or more specifically, matrix factorization techniques) have attracted significant research attention, due to their superior performance on the rating prediction problem, as witnessed during the Netflix Prize contest [12, 13]. The methods developed to attack the Net-flix Prize were highly effective for the rating prediction task, but have turned out to have relatively poor performance on the top-N recommendation task [7].

A few contributions have been proposed specifically to address the ranking problem in CF. Bayesian personalized ranking (BPR) [19] and Collaborative Less-is-More Filter-ing (CLiMF) [22] seek to improve top-N recommendation by directly optimizing binary relevance measures, i.e., Area Under the Curve (AUC) in BPR and Reciprocal Rank in CLiMF. In a similar spirit, TFMAP [21] directly optimizes Average Precision for context-aware recommendations. All of these methods use binary implicit-feedback data. How-ever, as discussed in Section 1, they are not well-suited for graded relevance datasets, since they are not able to fully exploit the information encoded in the relevance levels.
Research that deals with the ranking problem for cases in-volving graded relevance data includes EigenRank [14] and probabilistic latent preference analysis [15], which exploit pair-wise comparisons between the rated items. Collabora-tive competitive filtering [31] has further advanced the per-formance of top-N recommendation by imposing local com-petition, i.e., constraining items that users have seen but not rated to be less preferred that items both seen and rated. However, none of these methods are designed to optimize for any specific ranking/evaluation measure.

To our knowledge, the only existing CF approach, that di-rectly optimizes a graded evaluation measure is CofiRank [28], which minimizes a convex upper bound of the NDCG loss through matrix factorization. Some of the latest contribu-tions aim at enhancing the performance of CofiRank and boosting the NDCG score of the ranking results [3, 25]. These approaches are often referred to as collaborative rank-ing , and are evaluated by their performance on ranking graded items. Note that these approaches solve a different problem. Instead of addressing the top-N recommendation task, they rank a list of rated items that has already been given, i.e., pre-specified. As mentioned in Section 1, even results that are ranked optimally in terms of NDCG may still yield sub-optimal top-N recommendations. The new approach intro-duced by this paper, GAPfm, directly optimizes a recently proposed cascade metric GAP. In our experimental evalua-tion, we demonstrate that GAPfm outperforms CoFiRank with respect to a range of conventional top-N evaluation metrics.

The task of learning to rank is to learn a ranking function that is used to rank documents for given queries [16]. In-spired by the analogy between query-document relations in IR and user-item relations in recommender systems, many CF methods were proposed recently [3, 10, 19, 21, 22, 25, 28]. Our work in this paper also falls into this category, and in particular, it is closely related to one sub-area of learning to rank, i.e., direct optimization of evaluation metrics.
Learning to Rank. The key challenge of directly op-timizing evaluation metrics lies in the non-smoothness [4] of these measures. Research conducted to directly optimize evaluation metrics by exploiting structured estimation tech-niques includes [24, 30] that minimize convex upper bounds of loss functions based on evaluation measures, e.g., SVM-MAP [32] and AdaRank [29]. In addition, SoftRank [23] and its extensions [6] were proposed to use smoothed versions of evaluation measures, which can then be directly optimized. The current work can be considered part of this research direction, since we optimize a smoothed version of GAP. We designate the graded relevance data from M users to N items as a matrix Y M  X  N , in which the entry y mi denotes the grade given by user m to item i . Note that we have y mi  X  { 1 , 2 ,...,y max } , in which y max is the highest grade. Note also that y mi = 0 indicates that user m  X  X  preference for item m is unknown. | Y | denotes the number of nonzero entries in Y . In addition, I mi serves as an indicator function that is equal to 1, if y mi &gt; 0, and 0 otherwise. We use U to denote the latent factors of M users, and in particular U denotes a D -dimensional (column) vector that represents the latent factors for user m . Similarly, V D  X  N denotes the latent factors of N items and V i represents the latent factors of item i . Note that the latent factors in U and V are model parameters that need to be estimated from the data (i.e., a training set). The relevance between user m and item i is predicted by the latent factor model, i.e., using the inner product of U m and V i : f mi =  X  U m ,V i  X  = P D d =1 U
To produce a ranked list of items for a user m all items are scored using the inner product and ranked according to the scores. In the following, we use R mi to denote the rank position of item i for user m , according to the descending order of predicted relevances of all items to the user. For example, if the predicted relevance of item i is higher than that of all the other items for user m , i.e., if f mi &gt; f 1 , 2 ,...,N and j 6 = i , then R mi = 1.

Taking into account both the original definition of GAP in [20] and the notation introduced above, we can write the formulation of GAP for a ranked item list recommended for user m as follows: GAP m = 1 where I (  X  ) is an indicator function, which is equal to 1 if the condition is true, and otherwise 0.  X  l denotes the thresh-olding probability [20] that the user sets as a threshold of relevance at grade l , i.e., regarding items with grades equal or larger than l as relevant ones, and items with grades lower than l as irrelevant ones. Z m is a constant normalizing co-efficient for user m , defined as: Z m = P y max l =1 n ml where n ml denotes the number of items rated with grade l by user m .

For notational convenience in the rest of the paper, we substitute the last term of the parentheses in Eq. (1), as shown below: We assume that each grade l is an integer ranging from 1 to y max , since usually a non-integer grade scale can be transformed to an integer grade scale by multiplying by a constant factor, e.g., the scale of 1 to 5 stars with half star increment can be transformed to the scale of 1 to 10 stars by multiplying factor 2. As suggested in [20], the value of  X  each grade needs to be empirically tuned according to the specific use cases. In this paper, we adopt an exponential mapping function that maps the grade l to the thresholding probability  X  l , as shown in Eq. (3). Note that other expressions for the definition of  X  l can be also used, without influencing the main results on the opti-mization of GAP, as presented in the next section.
As shown in Eq. (1), GAP depends on the rankings of the items in the recommendation lists. However, the rankings of the items are not smooth with respect to the predicted user-item relevance, and thus, GAP results in a non-smooth func-tion with respect to the latent factors of users and items, i.e., U and V . Therefore, standard optimization methods cannot be used to maximize the objective function as in Eq. (1). In this work, we exploit core ideas from the literature on learning to rank [6] and recent work that successfully used smoothed approximations of evaluation metrics for CF with implicit feedback data [21, 22]. We approximate the rank-based terms in the GAP metric with smoothed functions with respect to the model parameters (i.e., the latent fac-tors of users and items). Specifically, the rank-based terms 1 /R mi and I ( R mj  X  R mi ) in Eq. (1) are approximated by smoothed functions with respect to the model parameters U and V , as shown below: where g ( x ) is a logistic function, i.e., g ( x ) = 1 / (1 + e The basic assumption of the approximation in Eq. (4) is validated in [6], i.e., the condition of item j being ranked higher than item i is more likely to be satisfied, if item j has relatively higher relevance score than item i .

We attain a smoothed version of GAP m by substitut-ing the approximations introduced in Eq. (4) and (5) into Eq. (1), as shown below: Note that for notational convenience, we make use of the substitution f m ( j  X  i ) :=  X  U m ,V j  X  X  X  X  U m ,V i  X  . In Eq. (6), we drop the coefficient 1 /Z m , which is independent of latent factors, and thus, has no influence on the optimization of GAP m . Taking the average GAP across all the users and the regularization for the latent factors, we obtain the objective function of GAPfm : F ( U,V ) = 1 k U k and k V k are Frobenius norms of U and V , and  X  is the parameter that controls the magnitude of regularization. Note that the constant coefficient 1 /M is also dropped in the following, since it has no influence on the optimization of F ( U,V ).
Since the objective function in Eq. (7) is smooth over the model parameters U and V , we can optimize it using stochastic gradient ascent. In each iteration, we optimize F ( U m ,V ) for user m independently of all the other users. The gradients of F ( U m ,V ) with respect to user m and item i can be computed as follows:
The derivation of the gradient in Eq. (8) is rather straight-forward. However, the derivation of Eq. (9) is more sophis-ticated, since the latent factors of different items are cou-pled. The complexity of computing the gradient in Eq. (8) is O ( DS 2 m + D ), where S m denotes the number of items rated/graded by user m . Taking into account all the M users, the complexity of computing gradients in Eq. (8) in one iteration can be denoted as O ( D S 2 M + DM ), in which S is the average number of rated items across all the users. Similarly, for a given user m , the complexity of computing the gradient in Eq. (9) is O ( DS 2 m + DS m ), and the complex-ity in one iteration over all the users is O ( D S 2 M + D SM ). Note that since we have the conditions | Y | = SM and | Y | &gt;&gt; M, S,D , the overall complexity of GAPfm in one iteration can be regarded as | Y | , which is linear in the num-ber of observed ratings in the given dataset. Summarizing, the entire learning algorithm of GAPfm is illustrated in Al-gorithm 1. The GAP measure can be regarded as a generalization of AP to multi-grade data [20], GAPfm can be also seen as a generalization of [21], a CF approach that directly optimizes AP in the implicit feedback domain. This can also be seen by looking at the smoothed version of GAP in Eq. (6), for the case of y max = 1. This characteristic indicates that al-though GAPfm is specifically designed for recommendation domains with graded relevance data, it can also be utilized for the optimization of AP in the domains with implicit feed-back data , as it becomes equivalent (for y max = 1) to the approach proposed in [21].

Second, since GAP is an approximation to the area un-der the graded precision-recall curve as illustrated in [20], GAPfm can also be extended to the optimization of graded precision (GP) and graded recall (GR) at the top-N part of the recommendation list. Note that similar to GAPfm , we can first approximate GP@n and GR@n by smoothed functions of latent factors, and then learn the latent factor models in the same fashion as approached in GAPfm (c.f. Section 4.1 and 4.2).
In this section we present a series of experiments to evalu-ate the proposed GAPfm algorithm, in order to address the following two research questions: 1) Is GAPfm effective for optimizing GAP? 2) Does GAPfm outperform state-of-the-art CF approaches for top-N recommendation? Table 1: Statistics of the training set in the experiments.
The Netflix Prize dataset is one of the most used graded relevance dataset for CF 2 . Two parts of the dataset are used, i.e., the training set and the probe set. The training set consists of ca. 99M ratings (integers scaled from 1 to 5) from ca. 480K users to 17.7K movies. The probe set contains ca. 1.4M ratings disjoint from the training set. The training set is used for building recommendation models, and the probe set is used for the evaluation. In the experiments, we exclude the users with less than 50 ratings from the training set. This choice is made to guarantee that users have sufficient number of ratings so as to facilitate our investigation on different cases in terms of the number of ratings per user. Note that this exclusion criterion removes one third of users, without dramatically reducing the size of the dataset, i.e., it only results in an reduction of 4% of the number of ratings. The statistics of the training set are summarized in Table 1.
We randomly select a certain number of rated movies and their ratings for each user in the training set to form a train-ing data fold. For example, under the condition of  X  X iven 10 X , we randomly select 10 rated movies for each user in or-der to generate a training data fold, from which the ratings are then used as the input data to train the recommenda-tion models. In the experiments we investigate a variety of  X  X iven X  conditions, i.e., 10, 20, and 30. Note, that the probe set was originally designed for measuring the accuracy of rat-ing prediction in the Netflix Prize competition thus there is no guaranty that the performance of a recommendation list is measurable for all users. For example, it is infeasible to measure the performance of a ranked list if a user has only one rating in the probe set. In our experiments we choose to measure the recommendation performance only based on the ground truth of the users who have at least 5 rated movies in the probe set (ca. 14% users in the probe set). The choice of 5 is set to allow all the evaluation metrics (as introduced later in this section) to achieve the highest possible value 1 for the task of top-5 recommendation.

In addition to GAP, two additional evaluation metrics are used in the experiments to measure the recommendation performance, i.e., NDCG and Precision. We set the rel-evance threshold to be 5 (the highest rating value in the dataset), when measuring the precision of the recommenda-tion list [7].

Note that in the evaluation we cannot treat all the un-rated items/movies as irrelevant to a given user. We adopt a widely-used practical strategy, [7, 12, 21], is to first ran-domly select 1000 unrated items (which are assumed to be irrelevant to the user) in addition to the ground truth (i.e., rated items) for each user in the probe set, and then evaluate the performance of the recommendation list that consists of only the selected unrated items and the ground truth items.
Parameter setting. We randomly select 1.5% (a simi-lar size to the probe set) of the data in the training set to generate a validation set, which is used to determine param-http://en.wikipedia.org/wiki/Netflix Prize eters that are involved in GAPfm and baseline approaches. We set the dimensionality of latent factors to be 10 for both GAPfm and other baseline approaches based on latent factor models. The remaining parameters are empirically tuned so as to yield the best performance in the validation set, i.e., for GAPfm we set the regularization parameter  X  =0.001 and the learning rate  X  =10  X  5 .
In the first experiment we investigate the effectiveness of GAPfm , i.e., whether learning latent factors based on GAPfm contributes to the improvement of GAP. We use the training set under the conditions  X  X iven 10 X ,  X  X iven 20 X  and  X  X iven 30 X , respectively, to train the latent factors, U and V , which are then used to generate recommendation lists for individual users. The performance of GAP is mea-sured according the hold-out data in the validation set along the iterations of the learning algorithm as described in Sec-tion 4.2. The results are shown in Fig. 1, which demonstrates that GAP gradually improves along the iterations and at-tains convergence after a certain number of iterations. For example, under the condition of  X  X iven 10 X , it converges af-ter 60 iterations, while it converges with less iterations as more data from the users is available for training. Accord-ing to the observation from this experiment, we can confirm a positive answer to our first research question.
We compare the performance of GAPfm with three base-line approaches. Each of the baseline approaches is listed and briefly introduced below:
In Table 2, we present the performance of GAPfm and the baseline approaches for  X  X iven X  10 to 30 items per user in the training set. Under all three conditions, GAPfm greatly outperforms all the baselines, i.e., over 30% for P@5, 15% for NDCG@5 and 10% for GAP@5. All the improvements are statistically significant, according to the Wilcoxon signed rank significance test with p &lt; 0.01. The results indicate that the proposed GAPfm is highly competitive for the top-N recommendation task. The results also demonstrate that the optimization of GAP leads to improvements in terms of precision and NDCG. We also notice that SVD++ is only slightly better than PopRec in P@5 (which has also been reported in the literature [7]), but worse than PopRec with respect to both NDCG@5 and GAP@5. This result again indicates that optimizing rating predictions does not neces-sarily lead to good performance for top-N recommendations. We have presented GAPfm , a new CF approach for top-N recommendation, by learning a latent factor model that directly optimizes GAP. We propose an adaptive selection strategy for GAPfm so that it could attain a constant com-putational complexity, which guarantees its usefulness for large scale use scenarios. Our experiments also empirically validate the scalability of GAPfm . GAPfm is demonstrated to substantially outperform the baseline approaches for the top-N recommendation task, while also being competitive for the performance of ranking graded items, compared to the state of the art. Inspired by statistical analysis of eval-uation metrics [27], we would like to analyze the relations and differences between learning methods that optimize dif-ferent evaluation metrics. Considering the multi-facet rele-vance judgments in recommender systems, such as accuracy, diversity, serendipity, we would also like to investigate the possibilities of optimizing top-N recommendation with mul-tiple cohesive or competing objectives [2].
This work is funded as part of a Marie Curie Intra Eu-ropean Fellowship for Career Development (IEF) awards held by Alexandros Karatzoglou (CARS, PIEF-GA-2010-273739). http://graphlab.org/toolkits/collaborative-filtering/ http://www.cofirank.org/ [1] D. Agarwal and B.-C. Chen. Regression-based latent [2] D. Agarwal, B.-C. Chen, P. Elango, and X. Wang. [3] S. Balakrishnan and S. Chopra. Collaborative ranking. [4] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to [5] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [6] O. Chapelle and M. Wu. Gradient descent [7] P. Cremonesi, Y. Koren, and R. Turrin. Performance [8] A. Gunawardana and G. Shani. A survey of accuracy [9] T. Hofmann. Latent semantic models for collaborative [10] L. Hong, R. Bekkerman, J. Adler, and B. D. Davison. [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] Y. Koren. Factorization meets the neighborhood: a [13] Y. Koren, R. Bell, and C. Volinsky. Matrix [14] N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented [15] N. N. Liu, M. Zhao, and Q. Yang. Probabilistic latent [16] T.-Y. Liu. Learning to rank for information retrieval. [17] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [18] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [19] S. Rendle, C. Freudenthaler, Z. Gantner, and S.-T. [20] S. E. Robertson, E. Kanoulas, and E. Yilmaz.
 [21] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [22] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [23] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [24] I. Tsochantaridis, T. Joachims, T. Hofmann, and [25] M. N. Volkovs and R. S. Zemel. Collaborative ranking [26] E. M. Voorhees. The trec-8 question answering track [27] J. Wang and J. Zhu. On statistical analysis and [28] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [29] J. Xu and H. Li. Adarank: a boosting algorithm for [30] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. [31] S.-H. Yang, B. Long, A. J. Smola, H. Zha, and [32] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A
