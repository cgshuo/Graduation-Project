 We investigate tag and query logs to see if the terms peo-ple use to annotate websites are similar to the ones they use to query for them. Over a set of URLs, we compare the distribution of tags used to annotate each URL with the distribution of query terms for clicks on the same URL. Understanding the relationship between the distributions is important to determine how useful tag data may be for im-proving search results and conversely, query data for im-proving tag prediction. In our study, we compare both term frequency distributions using vocabulary overlap and rela-tive entropy. We also test statistically whether the term counts come from the same underlying distribution. Our results indicate that the vocabulary used for tagging and searching for content are similar but not identical. We fur-ther investigate the content of the websites to see which of the two distributions (tag or query) is most similar to the content of the annotated/searched URL. Finally, we analyze the similarity for different categories of URLs in our sample to see if the similarity between distributions is dependent on the topic of the website or the popularity of the URL. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Design, Measurement, Experimentation. Keywords: Query Log, Tag, Folksonomy, Web Search.
Social bookmarking systems like Delicious 1 and Bibson-omy 2 offer a rich source of information about popular web-pages and research articles. In these systems, users annotate resources with a small set of unstructured terms, called tags , that they choose from an unlimited vocabulary. Despite this freedom users generally choose to annotate webpages with common terms from natural language that best describe the http://delicious.com http://www.bibsonomy.org content, purpose or functionality of the website. Popular tags for a resource over a population of users can then be as-sumed representative of the consensus opinion. Social book-marking systems have recently attracted academic interest, both because they provide an alternative model for finding information (browsing rather than searching) and also be-cause they can be used as an additional source of relevance information when ranking pages in Web search [6, 1, 13].
In this paper we investigate the similarity between query logs and tag data from the perspective of the terms used to search for and annotate individual resources. To the best of our knowledge, no thorough investigation has been per-formed to date regarding the problem of whether the tags that people use to annotate resources are the same or sim-ilar to those used to search for resources. This question is critical for determining if and how social bookmarking data can be used to improve Web search. In particular in this study we attempt to answer the following questions:
The paper is structured as follows. We first discuss related work before describing our tag and query dataset. We in-vestigate whether the tag and query distributions are similar according to different metrics and perform statistical tests to determine wether the two distributions can be considered the same. Finally, we investigate the content term distribu-tions for the websites in our dataset and compare similarity across the three different term distributions.
This paper builds on a large body of work on analyzing large query logs from Web search engines. Researchers have shown that the data in query logs can be mined for vari-ous purposes including: tuning retrieval function parame-ters and thereby improving search engine performance (for example in the  X  X earning to rank X  frameworks [7]); query spelling correction; query suggestion/auto-completion and query disambiguation [9]; personalization of search results [3]; and trend analysis [2].
Various researchers have investigated the applicability of social bookmarking data to the problem of improving Web search results. For example, Yanbe et al. [13] implemented a search engine using data extracted from Delicious such as the tag date and the popularity of a URL. These social bookmarking features were used to supplement a standard retrieval model. Bao et al [1] also implemented two ranking algorithms that integrated social bookmarking data into the retrieval model; one exploiting the overlap between query terms and tags, and the second using the popularity of web pages as indicated by the tag data. An evaluation on a small set of queries indicated improvements over a BM25 baseline.
An analysis of the social bookmarking site Delicious, was conducted by Heymann et al. [6], who found that the book-mark data had a good coverage of interesting pages on the Web, in the sense that bookmarked URLs were dispropor-tionately common in search results. Using popular queries, they found that 19% of the top 10 search engine results and 9% of the top 100 results were present in Delicious. Other important findings included the observation that ap-proximately 12.5% of URL X  X  posted by users are new pages (unindexed by a search engine). Therefore, social bookmark-ing data may be a useful supplementary resource for index-ing and crawling. It was also discovered that a large over-lap existed between popular query terms and tags (over the query/tag logs as a whole ), indicating that tagging may be a useful resource to promote popularly tagged urls within a ranking. Another interesting observation was that tags were present in the content of only 50% of pages and 16% in titles of pages annotated by users. Bookmarking data may therefore be a useful resource to address problems such as vocabulary mismatch [4], namely the differences in language used by those searching for content from those writing it.
None of these works have investigated the distribution of terms used by searchers and taggers in a principled way to see if there is overlap or correlation between the terms used to query for a particular webpage and those used to annotate the same page . We believe that such an analysis is a critical first step for better understanding the nature of tag data and thereby assessing its usefulness for various tasks such as improving Web search.
In this study we make use of the AOL query log [11], which is to our knowledge the only sizable and recent Web search engine query log (containing primarily English queries) that is generally available to academic researchers. Highly con-troversial when it was first released, the log contains the search requests of some 657,426 users over a period of 3 months from March to May, 2006. In this study we protect the anonymity of search engine users by only analyzing data, which is aggregated across all users. Moreover our study works toward protecting the anonymity of future search en-gine users: By comparing query logs with publicly available tag data we hope to answer the question as to whether such public data can be used as a substitute for private search data in future research.
We first scanned the query log selecting the 50,000 most frequently clicked URLs according to the number of distinct (non-identical) queries associated with each. We were forced to select only common webpages to ensure that there was Figure 1: Number of occurrences of top-level Dmoz categories in the URL dataset. sufficient data in the query term distribution to draw mean-ingful inferences between the query log and Delicious tag data. The clicked URLs in the AOL log had been trun-cated to display only the domain of the URL and not the corresponding directory structure. For example the URL  X  X ttp://www.domain.com/something/index.htm X  would be recorded in the logs simply as  X  X ttp://www.domain.com X . This posed a challenge for our analysis because the terms used to generate queries for different pages from the same domain may be completely different. Consider on the BBC news website (http://news.bbc.co.uk). An article regarding  X  X onstructions for the Beijing Olympics X  may contain many instances of the terms  X  X eijing X  and  X  X lympics X . While these terms are representative of the particular story they are not representative of the BBC website as a whole, and thus the tags used to annotate the BBC website may not contain those terms. So before we could compare the query log with tag data from Delicious, we first attempted to recon-struct the complete URL by submitting each query to the Google search engine (as that is arguably the same engine that was used to generate the results seen by the AOL search users) and selected the first occurrence of the given domain amongst the top 32 results returned. Obviously this method is not guaranteed to return the actual URL that was clicked on by the AOL users, but it is a reasonable approximation, given that the query logs are already quite noisy, and may indeed have the unintended benefit of removing some of the noise from the log. 3
We then selected all  X  X omplete URLs X  from the newly re-constructed log that were associated with at least 50 queries (i.e. searchers X  had clicked on the URL at least 50 times) and had been bookmarked a minimum of 20 times in De-licious. This process resulted in query and tag data for a set of 4145 URLs. This sample was considered sufficient for our purposes in order to provide statistically significant re-sults. Figure 1 shows the frequency of URLs in our sample with each of the top level categories from the Open Direc-tory Project (Dmoz). We see that while the distribution over categories for our sample does not follow the distribu-
We note that our query dataset may have a small bias toward consistently popular websites, i.e. sites that were popular in 2006 and have remained so until 2009. Mean 955.3 1105.8 17.6 139.6 Std deviation 6464.7 1533.4 12.8 137.7 Median 278.0 393.0 15.0 83.0 Table 1: Basic Statistics of URL Query/Tag Dataset tion of URLs found in Dmoz 4 , our sample does have good coverage over all the topics in Dmoz. We will use the Dmoz categorizations in later sections to determine how and if the similarity between tag and query distributions depends on the topic of the URL.
Web users often enter the URL they intend to navigate to as a search term in the query box rather than using the browser X  X  navigation bar. Taggers on the other hand, rarely enter the URL of the webpage they are annotating. To cor-rect for this difference, we removed any instance of the URL domain (e.g.  X  X ww.flickr.com X ) from both the query and tag log. Moreover, since the search engine performed auto-mated spelling correction on malformed URLs, many queries in the log involve simple misspellings of the intended url, such as:  X  X w.flickr.com X . We removed all such queries, by calculating the Levenshtein distance between the domain of the URL and the query (/bookmark) and removing those which lie within 3 edits of one another. We also removed all punctuation, which is very common in tag data, where users sometimes use it to separate concepts. Finally we removed the most frequent and uninformative query terms using a standard set of Web search engine stopwords and applied Porter X  X  stemmer to reduce the size of the vocabulary. The last step is particularly important given the limited size of our samples (of queries and tags) for each URL. Basic sum-mary statistics of our dataset are given in Table 1. It is interesting to note that while the average sample size for queries and tags is similar, the average vocabulary size is very different, with tag data having a much larger vocab-ulary on average than query data. This could be due to the fact that users can enter many times the same term in different queries, (where they are unlikely to enter a term more than once in a bookmark) or because searchers tend to use the same words as one another, while taggers tend to be more inventive.
In this section we review a number of different similarity measures in order to compare and quantify the similarity between tag and query term distributions.
The first and most basic comparison that can be made is to compare vocabularies to see whether the same terms are being used as queries for a URL and as tags to annotate it. To calculate the relative overlap between the vocabulary of query terms (denoted V q ) and tag terms ( V  X  ), we use the
The query log sample is biased towards US websites (hence the low value for the  X  X orld X  category) and to popular web-sites (hence the high value for  X  X ews X ). Figure 2: Vocabulary overlap between queries and tags. URLs are ordered from most similar to least. Comparison between the complete query/tag vocab-ularies and those modified to remove noise by re-moving low frequency terms. so-called Overlap coefficient [10]:
We do not use more common set overlap metrics such as the Jaccard or Dice coefficients because those metrics are sensitive to the relative size of the two sets (the vocabular-ies). The relative size of these vocabularies is not important to us and is highly dependent on the the sampling process over which we had little control (for some URLs we have lots of queries and very few bookmarks, while for others the reverse is true). The Overlap coefficient tells us how much of the smaller vocabulary is contained in the larger one.
Figure 2 shows the amount of overlap between the query and tag vocabularies across the set of 4145 URLs. The URLs have been ordered from most similar to least. The uninterrupted line compares the complete vocabularies con-taining all terms, while the other two lines compare smaller query/tag vocabularies after we attempt to reduce noise by removing low frequency terms. For the complete case, we see that all URLs share some vocabulary and well over half of the URLs in the sample show a vocabulary overlap of 0.5 or more. This constitutes a large amount of overlap and demonstrates that query and tag vocabularies are in-deed very similar. We postulated that a large number of the not overlapping vocabulary terms would be singletons (terms occurring once only in the sample) as they are more likely to be the result of noise in the logs than the more fre-quently occurring terms. Indeed, after removing singletons from the distributions we see a slight increase in the over-lap in Figure 2. This indicates that removing low frequency terms may be a useful policy for applications where query vocabularies are used to approximate tag vocabularies and vice-versa. If however, we take this approach to the extreme and only keep the 20 most frequent terms from each distri-bution, the relative overlap between distributions drops off significantly.

We decided to investigate whether the vocabulary overlap between tags and queries was in any way dependent on the Figure 3: Vocabulary overlap between all terms in tag and query distributions across different URLs divided into the 7 most frequent Dmoz Categories.
 Successive curves have been shifted to the right to aid comparison. topic area (domain) of the URL. To do this we compared the vocabulary overlap across URLs from different Dmoz cate-gories. Figure 3 shows the overlap values for the seven most frequent top-level categories from the Dmoz directory. The curve for successive categories has been shifted to the right in the figure to facilitate better comparison. (Thus the fact that one curve lies to the right of another doesn X  X  indicate higher overlap in this case.) All curves are essentially the same, with the data points spread out with roughly the same distribution (density) along them. In other words, none of the curves has a much higher density at one end than an-other. This indicates that the amount of vocabulary overlap between tags and queries is not domain dependent.
Having investigated the vocabulary overlap, we now in-vestigate measures that compare the term frequency distri-butions with each other directly. In particular, we calculate the Kullback-Leibler (KL) divergence between the query and tag term frequency distributions. The formula for the KL divergence is: where p q and p  X  are the relative frequencies of term w in the query and tag distributions respectively. In this ordering the query distribution is considered to be the  X  X rue X  distribution and the tag distribution is seen as an approximation to it. The relative frequency is calculated as follows: where tf q ( w ) is the number of occurrences of term w in the query log for a particular URL. Since the KL divergence be-comes infinite whenever the probability p  X  ( w ) is zero (which happens for any term in the query vocabulary but not in the tag vocabulary), we replace the relative frequency by a Laplace smoothed estimate  X  p  X  ( w ). By performing Laplace Figure 4: Kullback-Leibler divergence between the query and tag distributions with Laplace Smoothing applied to the tag distribution. smoothing over all terms in the union of the vocabularies we guarantee that the KL divergence will be finite:
We considered using other smoothing techniques such as linear interpolation with a collection model (term frequen-cies over the whole query/tag log), but decided to compute the similarity metric in the  X  X awest X  form possible so as not to introduce additional smoothing coefficient parameters.
Figure 4 shows the KL divergence between queries and tags. This plot tells us how well the smoothed tag term distribution does at describing the term frequencies observed in the query data. 5 We note that in contrast to previous graphs, higher values on this graph indicate lower similarity between queries and tags. Removing infrequent terms to reduce noise has a slightly positive effect on the similarity. In order to decide whether or not tag data does a good job at approximating the query distribution, we added a fourth line to the plot. To calculate values along the line, the approximating tag distribution was replaced by a uniform distribution over the same vocabulary. The fact that the new line lies well above the other plots indicates that the smoothed tag distribution is indeed a better than random approximation to the query distribution.

The reverse KL divergence, from tags to queries, is shown in Figure 5. This plot tells us how well the smoothed query distribution approximates the tag data. In this case the curve for the uniform distribution is much lower and even crosses over the KL ( p  X  || p q ) line, indicating that for a more than one quarter of the URLs, a random distribution over the query vocabulary does a better job at approximating the tag data, meaning the distributions are not similar at all. We see that removing terms from the distributions in order to remove noise actually has the opposite effect in this case, slightly increasing the difference between the distributions.
In Information Theory the KL divergence is interpreted as the number of extra bits needed to encode a message generated by the first distribution using an encoding that is optimal for the second. Figure 5: Kullback-Leibler divergence between the tag and query distributions with Laplace Smoothing applied to the query distribution. Figure 6: Jensen-Shannon divergence between the tag and query distributions.

While the KL divergence measures model similarity in an asymmetric fashion, it is also possible to measure it in a sym-metric fashion using the Jensen-Shannon (JS) divergence. The JS divergence simply calculates the average KL diver-gence between each distribution and the mean of the two: advantage that the vocabularies of the distributions don X  X  need to match and thus Laplace smoothing isn X  X  required.
The Jensen Shannon divergence for the URLs in our dataset is shown in Figure 6. We see that the uniform dis-tribution line lies well above the others indicating that the queries and terms are similar. We note here that removing low frequency terms and/or selecting only the highest fre-quency terms has a small positive effect on similarity. Since the JS divergence is symmetric (and thereby the most prac-tical) we use it in Figure 7 to compare different Dmoz cat-egories to see if website topic has an effect on query/tag Figure 7: Jensen-Shannon divergence between terms and queries comparing 7 Dmoz categories. Figure 8: Popularity of bookmark (URL) versus similarity of tag/query distributions. similarity. As was the case when investigating vocabulary overlap, there doesn X  X  appear to be any noticeable change in the distribution of similarity values with the topic.
We investigated whether the similarity between tags and queries was dependent on the popularity of the URL in the tag or query log. This is an important question, because if the similarity is correlated, it could mean that our results in this paper do not generalize to other less popular URLs in the logs. In order to allow for a fair comparison between URLs with different popularity values and hence different samples sizes, we first created equally sized samples each URL, by sampling queries and tags without replace-ment from our dataset for each URL. Figure 8 shows a plot of bookmark popularity versus Jensen-Shannon divergence. We see that there doesn X  X  appear to be any clear relation-ship between bookmark popularity and similarity (Pearson X  X  r = 0 . 10). The same was found for popularity in the query log (Pearson X  X  r = 0 . 13). The fact that we found no rela-
Sample sizes of 200, 500 and 1000 gave consistent results. tionship between popularity and similarity does not guaran-tee that our results in this paper generalize to less frequent URLs, but it suggests that they could.
In Section 4 we established that the tag and query distri-butions share a large amount of vocabulary and are corre-lated with one another. We now want to see whether the examples we have for each distribution could be considered two samples from the same underlying probability distribu-tion. We introduce two statistical tests to determine if and under what conditions the query and tag distributions could be considered the same in the sense that they are indistin-guishable from one another.
We can quantify the difference between the tag and query distributions in terms of a model selection problem. In this framework we ask the question, how likely is it that the observed sequence of query terms and tag terms were gener-ated by different underlying term distributions as opposed to the same underlying distribution. In other words, we are going to test the hypothesis H 1 : p q 6 = p  X  against the null hypothesis: H 0 : p q = p  X  . The log-likelihood ratio is given then by: where [ q 1 ,...,q n ] is the observed sequence of query terms (we ignore boundaries between queries in our analysis) and [  X  ,..., X  m ] is the observed sequence of tag terms. We will now use p q X  to denote the Maximum Likelihood estimate for the term distribution generating the combined sequence [ q ,...,q n , X  1 ,..., X  m ], i.e.: where V = V q  X  V  X  . The hypotheses can then be replaced in our formula for the log likelihood ratio:
LR ( url ) = 2 log
If we assume that the order of terms in the respective logs is not important, (only their relative frequency is impor-tant), then we can calculate the likelihood values as follows: which can be rewritten as: 7
LR ( url ) = 2 X
The log likelihood ratio as defined above follows a  X  distribution with | V | X  1 degrees of freedom [10], so we can use it to calculate a confidence value for rejecting the null hypothesis, (that the query and tag samples come from the same distribution). We ran the analysis using a confidence threshold of p = 0 . 01% and the results are shown in Table 2.
Note that if the samples of query and tag terms are of the same size (i.e. m = n ), then the log likelihood ratio above is simply 4 n times the Jensen-Shannon divergence.
 Table 2: Results of the Log Likelihood Ratio and Minimum Description Length tests. The first col-umn contains the counts of URLs for which the Likelihood Ratio test could not reject the hypothe-sis ( p  X  0 . 01% ) that the distributions are the same. The second counts cases where the MDL assigned the combined distributions a shorter code length.
 The test was able to reject the null hypothesis that the two samples came from the same underlying distribution for all but 4% of the URLs when considering all terms. The number of non rejections decreased when low frequency terms were removed from the distributions.
Another approach to quantify if two sequences of words come from the same distribution is to apply a test based on the Minimum Description Length (MDL) principle[5]. The principle states that the best model of the process that gen-erates an observed sequence is the one that gives the shortest description of the sequence, where the model itself is also a part of the description. In our case the principle can be written as: MDL ( url ) = min { DL ([ q 1 ,...,q n ]) + DL ([  X  1 ,..., X  where DL ( s ) is the minimum description length of sequence s and P is the penalty for separating the sequences, which is actually the cost of encoding the location of the separating point in the combined sequence length, i.e. P = log 2 ( n + m ). In order to calculate the description length of a sequence, we use the Krichevsky-Trofimov (KT) code length [8].
 where KT ( s ) is a marginal likelihood of the sequence s using a uniform Dirichlet prior. It can be calculated as follows [12]: where | s | is the length of the sequence. We performed this MDL-based test on the URLs in our collection with the re-sults shown in Table 2. The MDL test didn X  X  find any URLs for which it thought that the combined sequence was likely to have come from the same distribution. The fact that the MDL test is more restrictive than the Likelihood Ratio test makes sense since the MDL is not calculating whether it is possible that two samples come from the same distribution, but rather whether it is more likely that the sequences are the same or not.
Having analyzed the similarity between tag and query dis-tributions, we now turn our attention to the other term dis-tribution normally at the heart of Information Retrieval, namely the content of the documents themselves. Our aim Figure 9: Ternary plot of relative vocabulary over-lap between query, tag and content distributions. Points toward the top of the graph indicate large vo-cabulary overlap between queries and tags. Points toward the bottom left indicate large overlap be-tween query and content vocabularies and so on. is to answer two questions: Firstly, which of the term distri-butions, tag or query, is closer to the content of the book-marked/clicked document? Secondly, are the tag and query distributions more or less similar to each other than they are to the content? In order to assess similarity we use two measures introduced in Section 4, namely the vocabulary overlap and the Jensen-Shannon divergence, and introduce a three-way graph called a ternary (triangular) plot. Figure 9 shows such a plot. For each data point, the perpendicular distance to the side of the triangle is proportional to the similarity between the two distributions that meet at the opposite vertex. In other words, the closer the point is to a corner, the more similar the distributions. For example, the closer a point is to the top of the graph, the more similar the query and tag distributions, and the closer the point is to the bottom right corner of the graph, the more similar the content and tag distributions. Similarity in this plot is measured in terms of vocabulary overlap. We can see from the graph that the queries and content distributions tend to share more vocabulary than the tags and queries. Tags and content seem to exhibit the least amount of vocabulary over-lap in our dataset. The reason for this is probably due to the querying process itself: In order to click on a web page, the page must have appeared in the result set and hence the user is likely to have entered some keywords from the con-tent of the document itself, while taggers are less inclined to annotate a page with a term that is contained in it.
Figure 10 shows a ternary plot of the Jensen-Shannon di-vergence. In this case since the divergence measures dissimi-larity between distributions, the distribution names are now placed at the corners of the plot. The data points for this graph are concentrated together toward the bottom of the triangle, indicating that the tag and query distributions are more similar to each other than they are to the content.
We investigate whether the topic of the website has any noticeable affect on the relative similarity of the different dis-Figure 10: Ternary plot of relative Jensen-Shannon divergence between query, tag and content distribu-tions. tributions. Figure 11 shows two frequent Dmoz categories It doesn X  X  appear that the is any way to differentiate between categories based on the relative similarity values.
Finally, we briefly investigated whether tag data could be used to smooth a document Language Model (LM) for Infor-mation Retrieval (IR). In the LM approach to IR, the KL di-vergence between a query and a smoothed document content model is sometimes used as a retrieval function for ranking documents. The document model is usually smoothed (using Linear Interpolation or Dirichlet smoothing) with a collec-tion model generated from the entire corpus of documents. For our experiment we used unigram counts from the Google ngram dataset 9 to calculate a collection model. We used the collection model to smooth the content distribution for each URL, using mixture (Linear Interpolation) or Dirichlet smoothing, and optimizing parameter settings to minimize KL divergence between the query and the smoothed con-tent model over all the URLs in the dataset. Figure 12 shows these plots. We then introduced the tag distribution into the smoothed content model, calculating the mixture  X  p =  X  1 p  X  +  X  2 p c +  X  3 p col and optimizing for parameter val-ues  X  i . The low divergence values in the graph indicate that tag data may indeed be useful for smoothing content models for IR.
We have investigated the similarity between query term distributions and tag distributions for the same clicked/bookmarked URL. We have shown that the vocab-ularies contain a large amount of overlap and that the term frequency distributions are correlated. We have also found that the similarity between tags and queries does not seem to be dependent on the topic area -at least at the level of gran-ularity provided by the top-level Dmoz categories. We have performed two different statistical tests that indicate that,
Other pairs of Dmoz categories showed similar results. http://www.ldc.upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2006T13 Figure 11: Ternary plot of relative vocabulary over-lap for two Dmoz categories. while similar, the samples do not come from the same under-lying distribution. We have compared the tag/query term distributions with term distributions describing the content of the URLs in our sample. Our analysis indicates that in terms of vocabulary overlap, queries are more similar to con-tent than to tags, but in terms of frequency distributions, queries and tags are more similar to one another than to content. Finally, we have shown that tag data may be use-ful for smoothing document content models with the aim of improving document retrieval performance.

Our results are of great importance to research in im-proving Web Search by incorporating additional forms of evidence such as that inferred from tag data. Future work involves developing models for automatically removing noise from the tag and query logs, developing techniques for pre-dicting useful tags from query distributions and vice-versa, and developing techniques for the effective use of tag data to improve different forms of Web Search.
 Acknowledgments: This research was supported by the Hasler Foundation via the ADIR+ project and the EPSRC under grant number EP/F060475/1. [1] S. Bao, G. Xue, X. Wu, Y. Yu, B. Fei, and Z. Su. [2] S. M. Beitzel, E. C. Jensen, A. Chowdhury, [3] Z. Dou, R. Song, and J.-R. Wen. A large-scale [4] G. W. Furnas, T. K. Landauer, L. M. Gomez, and Figure 12: Kullback-Leibler divergence between the query distribution and various smoothed content distributions. Smoothing is performed using a col-lection model. Including tag data reduces diver-gence for the language model.
 [5] R. Gwadera, A. Gionis, and H. Mannila. Optimal [6] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [7] T. Joachims. Optimizing search engines using [8] R. Krichevsky and V. Trofimov. The performance of [9] M. Li, Y. Zhang, M. Zhu, and M. Zhou. Exploring [10] C. D. Manning and H. Sch  X  utze. Foundations of [11] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [12] Z. Talata. Model selection via information criteria. [13] Y. Yanbe, A. Jatowt, S. Nakamura, and K. Tanaka.
