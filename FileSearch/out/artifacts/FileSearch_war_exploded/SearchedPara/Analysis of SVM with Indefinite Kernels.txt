 Kernel methods [5, 24] such as Support Vector Machines (SVM) have recently attracted much atten-tion due to their good generalization performance and appealing optimization approaches. The basic space through a kernel function. The kernel function over samples forms a similarity kernel matrix which is usually required to be positive semi-definite (PSD). The PSD property of the similarity matrix ensures that the SVM can be efficiently solved by a convex quadratic programming. However, many potential kernel matrices could be non-positive semi-definite. Such cases are quite common in applications such as the sigmoid kernel [14] for various values of the hyper-parameters, hyperbolic tangent kernels [25], and the protein sequence similarity measures derived from Smith-Waterman and BLAST score [23]. The problem of learning with a non-PSD similarity matrix (in-used method is to convert the indefinite kernel matrix into a PSD one by using the spectrum trans-with indefinite kernels is regarded as the minimization of the distance between convex hulls in the pseudo-Euclidean space. In [20], general Reproducing Kernel Kre  X   X n spaces (RKKS) with indefinite kernels are introduced which allows a general representer theorem and regularization formulations. matrix problem [13] i.e. learning a proxy PSD kernel matrix to approximate the indefinite one. Without realizing that the objective function is differentiable, the authors quadratically smoothed gradient method and the analytic center cutting plane method.
 In this paper we follow the formulation of SVM with indefinite kernels proposed in [15]. We mainly of gradient-based algorithms. The main idea behind our analysis is from its saddle (min-max) rep-resentation which involves a penalty term in the form of Frobenius norm of matrices, measuring the distance between the indefinite kernel matrix and the proxy PSD one. This penalty term can be regarded as a Moreau-Yosida regularization term [12] to smooth out the objective function. be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous. Based on our analysis, in Section 4 we propose a simplified formulation of the projected gradient method We further develop Nesterov X  X  smooth optimization approach [17, 18] for indefinite SVM which has an optimal convergence rate of O (1 /k 2 ) for smooth problems. In Section 5, our analysis and proposed optimization approaches are validated by experiments on various benchmark data sets. In this section we review the regularized formulation of indefinite SVM presented in [15]. To this all n  X  n symmetric matrices. If A  X  S n is positive semi-definite, we write it as A  X  0 . The respectively denoted by k X k and  X  X  ,  X  X  .
 by Since we assume that K is positive semi-definite, the above problem is a standard convex quadratic d X  X spremont [15] proposed the following max-min approach to simultaneously learn a proxy PSD kernel matrix K for the indefinite matrix K 0 and the SVM classification: By the min-max theorem [2], problem (1) is equivalent to For simplicity, we refer to the following function defined by as the objective function . It is obviously concave since f is the minimum of a sequence of concave function f . the semi-definite cone S n + . Indeed, it was shown in [15] that the optimal solution is given by where, for any matrix A  X  S n , the notation A + denotes the positive part of A by simply setting  X  max ( A ) . The next lemma tells us that the optimal solution K  X  belongs to a bounded domain in Lemma 1. Problem (2) is equivalent to the formulation max  X   X  X  jective function can be defined by where Q 2 := Proof. By the saddle theorem [2], we have L (  X   X  , K  X  ) = min K  X  X   X   X  that 0  X   X   X   X  C , k  X   X  k 2  X  nC 2 . Combining this with the above inequality yields the desired lemma.
 It is worthy of mentioning that it was shown in [18, Theorem 1] that a function g has a Lipschitz continuous gradient if it enjoys a special structure: g (  X  ) = min { X  A X , K  X  +  X d ( K ) : K  X  Q} and hence the theorem there can not be applied to our case. mal value function [3, Theorem 4.1], essentially due to Danskin [7].
 Lemma 2. Let X be a metric space and U be a normed space. Suppose that for all x  X  X the on X  X  U and let Q be a compact subset of X . Define the optimal value function as f (  X  ) = given by  X  f (  X  ) =  X   X  L (  X , x (  X  )) .
 Applying the above lemma to the objective function f defined by equation (5), we have: Theorem 1. The objective function f defined by (3) (equivalently by (5)) is differentiable and its gradient is given by Proof. We apply Lemma 2 with X = S n and Q = Q 2  X  S n , U = Q 1 and x = K . To this  X   X  K Lemma 2 by noting that the derivative of L w.r.t. the first argument  X   X  L (  X , K ) = e  X  Y KY  X . Indeed, we can go further to establish the Lipschitz continuity of  X  f based on the strongly convex property of L (  X ,  X  ) . To this end, we first establish a useful lemma.
 Y  X  2  X  &gt; 2 Y/ (4  X  )) + k F  X  ( k  X  1 k + k  X  2 k ) k  X  1  X   X  2 k / (4  X  ) .
 lem arg min K  X  X   X   X  K (  X  1 ) k F . Consequently, where the last inequality follows from the fact that Y is an orthonormal matrix since y i  X  { X  1 } niques in matrix analysis. To see this, recall that a spectral function G : S n  X  S n is defined by applying a real-valued function g to the eigenvalues of its argument i.e. for any K  X  S n with perturbation inequality in matrix analysis [1, Lemma VII.5.5] shows that if g is Lipschitz continu-K 2 = K 0 + Y  X  2  X  &gt; 2 Y/ (4  X  ) implies equation (7), and hence Lemma 3. However, we prefer the original proof presented for Lemma 3 since it explains more clearly how the strong convexity of the regularization term k K  X  K 0 k 2 F plays a critical role in the analysis.
 From the above lemma, we can establish the Lipschitz continuity of the gradient of the objective function.
  X  f (  X  2 ) k X  can be bounded by Let X  X  begin with the first one by applying Lemma 3.  X  max h  X  proof of Theorem 2. This section is based on the theoretical analysis above, mainly Theorem 2. We first outline a sim-of O (1 /k ) where k is the iteration number. We can further develop a smooth optimization approach [17, 18] for indefinite SVM (5). This scheme has an optimal convergence rate O (1 /k 2 ) for smooth problems which has been applied to various problems, e.g. [6]. 4.1 Simplified Projected Gradient Method In [15], the objective function was smoothed by adding a quadratic term (see details in Section 3 there) and then they proposed a projected gradient algorithm to solve this approximation problem. 1 where the projection P Q Indeed, from Theorem 2 we can further obtain the following result by developing the techniques in Sections 2.1.5, 2.2.3 and 2.2.4 of [18].
 Lemma 4. Let  X   X  Proof. We know from Theorem 2 that  X  f is Lipschitz continuous with Lipschitz constant L =  X  max ( K 0 ) + nC 2  X  , then we have f (  X  )  X  f (  X  k )  X  X  X  X  f (  X  k ) ,  X   X   X  k  X  =  X  f (  X  k ) ,  X   X   X  k  X  d X   X  X  X  L  X  =  X  k +1 implies that  X  Combining this with the above inequality finishes the proof of the lemma.
 Theorem 3. Let  X   X  simplified projected gradient method in Table 1. Then, we have that Moreover, where  X   X  is an optimal solution of problem max  X   X  X 
Nesterov X  X  Smooth Optimization Method (SMM) 1. Let  X  &gt; 0 , k = 0 and initialize  X  0  X  X  1 and let L =  X  max ( K 0 )) + nC 2 / X  . 2. Compute  X  f (  X  k ) = e  X  Y 3. Compute  X  k = P Q 4. Compute  X  k = P Q 6. Set k  X  k + 1 . Go to step 2 until the stopping criterion less than  X  .
 Proof. Applying Lemma 4 with  X  =  X  k yields inequality (12). To prove inequality (13), we first apply Lemma 4 with  X  =  X   X  to get that, for any i , max  X   X  X   X  and also, noting from (12) that { max  X   X  X  pletes the proof of the theorem.
 iteration complexity of SPGM is O ( L/ X  ) for finding an  X  -optimal solution. 4.2 Nesterov X  X  Smooth Optimization Method In [18, 17], Nesterov proposed an efficient smooth optimization method for solving convex pro-gramming problems of the form U . It is assumed to be continuous and strongly convex on U with convexity parameter  X  &gt; 0 . optimization scheme detailed in [18] can be then applied to the function g with convergence rate in O (  X  f is established in Theorem 2 given by L =  X  max ( K 0 ) + nC 2 / X  . Translating the first-order Nesterov X  X  scheme [18, Section 3] to our problem (5), we can get the smooth optimization algorithm for indefinite SVM, see its pseudo-code in Table 2. One can see [17] for its variants with general step sizes.
 and  X  0 + however it can be made to monotonically increase by a simple modification of the algorithm [18]. further accelerate the algorithm by using a line search scheme [16]. 4.3 Related Work and Complexity Discussion the number of iterations to reach a target precision of  X  means that  X  f (  X  k )  X  min  X   X  X  overall complexity of SPGM is O ( n 3 L/ X  ) . As discussed in [15], ACCPM has an overall complexity methods at each iteration which would be slow for large scale datasets.
 Chen and Ye [4] reformulated indefinite SVM as an appealing semi-infinite quadratically constrained linear programming (SIQCLP) without applying extra smoothing techniques. There, the algorithm to find maximum violation constraints which involves eigen-decomposition of complexity O ( n 3 ) . to save the subset of increasing quadratically constrained conditions indexed by n  X  n matrices and iteratively solve a quadratically constrained linear programming (QCLP). The QCLP sub-problem can be solved by general software packages, e.g. Mosek (http://www.mosek.com/), which is gener-although pruning techniques were proposed to avoid too many quadratically constrained conditions. Based on our theoretical results (Theorem 2), Nesterov X  X  smooth optimization method can be ap-plied. The complexity of this smooth optimization method (SMM) mainly relies on the eigenvalue the convex region Q 1 which costs O ( n log n ) as pointed out in [15]. The first-order smooth op-timization approach [17, 18] has iteration complexity O ( Consequently, the overall complexity is O ( n 3 plexity of smoothing optimization is better than the simplified projected gradient method (SPGM) and SIQCLP. Compared with ACCPM, SMM has better dependence on the sample number n but with a worse precision i.e. worse dependence on  X  . We run our proposed smooth optimization approach and simplified projected gradient method on various datasets to validate our analysis. The experiments are done on several benchmark data sets from the UCI repository [19] including Sonar, Ionosphere, Heart, Pima Indians Diabetes, Breast Cancer, and USPS with digits 3 and 5 . For USPS dataset, we randomly select 600 samples for In each data split, as in [4] we first generate a Gaussian kernel matrix K with the hyper-parameter determined by cross-validation on the training data using LIBSVM and then construct indefinite ( E + E 0 ) / 2 where E is randomly generated by zero mean and identity covariance matrix. For all methods, the parameters C and  X  for Indefinite SVM are tuned by cross-validation and we terminate In Table 3, we report the average test set accuracy (%) and CPU time (seconds) across different algorithms: smooth optimization method (SMM), simplified projected gradient method (SPGM), analytic center cutting plane method (ACCPM), and semi-infinite quadratically constrained linear programming (SIQCLP). For the QCLP sub-problem in the SIQCLP method, we use Mosek soft-ware package (http://www.mosek.com/). We can see that test accuracies are statistically the same observe that SMM is consistently more efficient than other methods, especially for a large number of training samples. SIQCLP needs much more time since, in each iteration, it needs to solves a ation on Sonar and Diabetes for SMM, SPGM, and ACCPM. The SIQCLP approach is not included algorithms. From Figure 1, we can see that SMM converges faster than SPGM which is consistent with the complexity analysis. The convergence of ACCPM is quite similar to SMM, especially for Table 3: Average test set accuracy (%) and CPU time in seconds (s) of different algorithms where  X  max (  X  min ) denotes the average maximum (minimum) eigenvalues of the indefinite kernel matrix over training samples. Figure 1: Objective value versus iteration: Sonar (left) and Diabetes (right). Curves: SMM (blue), SPGM (red) and ACCPM (black) small-sized datasets which coincides with the complexity analysis in Section 4.3 since it generally has a high precision. However, ACCPM needs more time in each iteration than SMM and this ob-servation becomes more apparent for the relatively large datasets shown in the time comparison of Table 3. In this paper we analyzed the regularization formulation for training SVM with indefinite kernels proposed by Luss and d X  X spremont [15]. We show that the objective function of interest is continu-the application of gradient-based methods. We formulated a simplified version of the projected gra-dient method presented in [15] and showed that it has a convergence rate of O (1 /k ) . We further developed Nesterov X  X  smooth optimization method [17, 18] for Indefinite SVM which has an opti-mal convergence rate of O (1 /k 2 ) for smooth problems. Experiments on various datasets validate to real biological datasets such as protein sequence analysis using sequence alignment measures. This work is supported by EPSRC grant EP/E027296/1. [1] R. Bhatia. Matrix analysis . Graduate texts in Mathematics. Springer, 1997. [2] S. Boyd and L. Vandenberghe. Convex optimization . Cambridge University Press, 2004. [3] J. F. Bonnans and A. Shapiro. Optimization problems with perturbation: A guided tour. SIAM [4] J. Chen and J. Ye. Training SVM with Indefinite Kernels. ICML , 2008. [5] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines and other [6] A. d X  X spremont, O. Banerjee and L. El Ghaoui. First-order methods for sparse covariance [7] J.M. Danskin. The theory of max-min and its applications to weapons allocation problems , [8] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer. Classification on pairwise [9] B. Haasdonk. Feature space interpretation of SVMs with indefinite kernels. IEEE Transac-[10] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis . Cambridge University Press, 1991. [11] R. I. Kondor and J. Laffferty. Diffusion kernels on graphs and other discrete input spaces. [14] H.-T. Lin and C. J. Lin. A study on sigmoid kernels for SVM and the training of non-psd [15] R. Luss and A. d X  X spremont. Support vector machine classification with indefinite kernels. [16] A. Nemirovski. Efficient methods in convex programming . Lecture Notes, 1994. [17] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course . Springer, 2003. [18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming , [19] D. Newman, S. Hettich, C. Blake, and C. Merz. UCI repository of machine learning datasets. [20] C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with non-positive kernels. ICML , [21] E. Pekalska, P. Paclik, and R. P. W. Duin. A generalized kernel approach to dissimilarity-[22] V. Roth, J. Laub, M. Kawanabe, and J. M. Buhmann. Optimal cluster preserving embedding of [23] H. Saigo, J.P.Vert and N. Ueda, and T. Akutsu. Protein homology detection using string align-[24] B. Sch  X  olkopf, and A.J. Smola. Learning with kernels: Support vector machines, regulariza-[26] G. Wu, Z. Zhang, and E. Y. Chang. An analysis of transformation on non-positive semidefinite
