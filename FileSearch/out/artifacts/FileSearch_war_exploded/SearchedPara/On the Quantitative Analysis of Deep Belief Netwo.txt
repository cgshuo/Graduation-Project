 Ruslan Salakhutdinov RSALAKHU @ CS . TORONTO . EDU Deep Belief Networks (DBN X  X ), recently introduced by Hinton et al. (2006) are probabilistic generative models th at contain many layers of hidden variables, in which each layer captures strong high-order correlations between the activities of hidden features in the layer below. The main breakthrough introduced by Hinton et al. was a greedy, layer-by-layer unsupervised learning algorithm that allo ws efficient training of these deep, hierarchical models. The learning procedure also provides an efficient way of per-forming approximate inference, which makes the values of the latent variables in the deepest layer easy to infer. Thes e deep generative models have been successfully applied in many application domains (Hinton &amp; Salakhutdinov, 2006; Bengio &amp; LeCun, 2007).
 The main building block of a DBN is a bipartite undirected graphical model called the Restricted Boltzmann Machine (RBM). RBM X  X , and their generalizations to exponential family models, have been successfully applied in collab-orative filtering (Salakhutdinov et al., 2007), informatio n and image retrieval (Gehler et al., 2006), and time series modeling (Taylor et al., 2006). A key feature of RBM X  X  is that inference in these models is easy. An unfortunate limitation is that the probability of data under the model is known only up to a computationally intractable normaliz-ing constant, known as the partition function. A good es-timate of the partition function would be extremely helpful for model selection and for controlling model complexity, which are important for making RBM X  X  generalize well. There has been extensive research on obtaining determin-istic approximations (Yedidia et al., 2005) or determin-istic upper bounds (Wainwright et al., 2005) on the log-partition function of arbitrary discrete Markov random fields (MRF X  X ). These variational methods rely critically on an ability to approximate the entropy of the undirected graphical model. However, for densely connected MRF X  X , such as RBM X  X , these methods are unlikely to perform well. There have also been many developments in the use of Monte Carlo methods for estimating the partition function, including Annealed Importance Sampling (AIS) (Neal, 2001), Nested Sampling (Skilling, 2004), and many others (see e.g. Neal (1993)). In this paper we show how one such method, AIS, by taking advantage of the bipartite structure of an RBM, can be used to efficiently estimate its partition function. We further show that this estimator , along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to training or test data. Thi s result allows us to assess the performance of DBN X  X  as gen-erative models and to compare them to other probabilistic models, such as plain mixture models.
 A Restricted Boltzmann Machine is a particular type of MRF that has a two-layer architecture in which the visi-ble, binary stochastic units v  X  { 0 , 1 } D are connected to hidden binary stochastic units h  X  X  0 , 1 } M . The energy of the state { v , h } is: E ( v , h ;  X  ) =  X  where  X  = { W, b , a } are the model parameters: W sents the symmetric interaction term between visible unit and hidden unit j ; b that the model assigns to a visible vector v is: p ( v ;  X  ) = where p  X  denotes unnormalized probability, and Z (  X  ) is the partition function or normalizing constant. The condition al distributions over hidden units h and visible vector v are given by logistic functions: where  X  ( x ) = 1 / (1+exp(  X  x )) . The derivative of the log-likelihood with respect to the model parameter W can be obtained from Eq. 2: where E data distribution and E spect to the distribution defined by the model. The ex-pectation E practice learning is done by following an approximation to the gradient of a different objective function, called th e  X  X ontrastive Divergence X  (CD) (Hinton, 2002): The expectation E from running the Gibbs sampler (Eqs. 5, 6), initialized at the data, for T full steps. Setting T =  X  recovers maxi-mum likelihood learning, although T is typically set to one. Even though CD learning may work well in practice, the problem of model selection and complexity control still re-mains. Suppose we have two RBM X  X  with parameter values  X  ber of hidden units and was trained using different learning rates and different numbers of CD steps. On the validation set, we are interested in calculating the ratio: which requires knowing the ratio of partition functions. Suppose we have two distributions defined on some space V with probability density functions: p and p tio of normalizing constants is to use a simple importance sampling (IS) method. Suppose that p p ( v ) 6 = 0 : Z Z Assuming we can draw independent samples from p unbiased estimate of the ratio of partition functions can be obtained by using a simple Monte Carlo approximation: the estimator  X  r spaces, the variance of  X  r infinite), unless p 3.1. Annealed Importance Sampling (AIS) Suppose that we can define a sequence of intermediate probability distributions: p = p B , which satisfy the following conditions:
C1 p
C2 We must be able to easily evaluate the unnormalized
C3 For each k = 0 , ..., K  X  1 , we must be able to draw
C4 We must be able to draw (preferably independent) The transition operators T density of transitioning from state v to v 0 . Constructing a suitable sequence of intermediate probability distributi ons will depend on the problem. One general way to define this sequence is to set: with 0 =  X  Once the sequence of intermediate distributions has been defined we have: Note that there is no need to compute the normalizing con-stants of any intermediate distributions. After performin g M runs of AIS, the importance weights w ( i ) can be substi-tuted into Eq. 8 to obtain an estimate of the ratio of partitio n functions: Neal (2005) shows that for sufficiently large number of in-termediate distributions K , the variance of  X  r proportional to 1 /M K . Provided K is kept large, the total amount of computation can be split in any way between the number of intermediate distributions K and the number of annealing runs M without adversely affecting the accuracy of the estimator. If samples drawn from p dent, the number of AIS runs can be used to control the variance in the estimate of  X  r the importance weights. 3.2. Ratios of Partition Functions of two RBM X  X  Suppose we have two RBM X  X  with parameter values  X  ability distributions p RBM can have a different number of hidden units h A  X  { 0 , 1 } M A and h B  X  { 0 , 1 } M B . The generic AIS interme-diate distributions (Eq. 10) would be harder to sample from than an RBM. Instead we introduce the following sequence of distributions for k = 0 , ..., K : where the energy function is given by: E k ( v , h ) = (1  X   X  k ) E ( v , h A ;  X  A ) +  X  k E ( v , h with 0 =  X   X  0 = 0 p some interpolation between p Let us now define a Markov chain transition operator T ( v 0 ; v ) that leaves p k ( v ) invariant. Using Eqs. 13, 14, it is straightforward to derive a block Gibbs sampler. The conditional distributions are given by logistic functions : p ( h B j = 1 | v ) =  X   X  k ( X Given v , Eqs. 15, 16 are used to stochastically activate hid-den units h A and h B . Eq. 17 is then used to draw a new sample v 0 as shown in Fig. 1 (left panel). Due to the special structure of RBM X  X , the cost of summing out h is linear in the number of hidden units. We can therefore easily evalu-ate: p ( v ) = X We will assume that the parameter values of each RBM satisfy |  X  | &lt;  X  , in which case p ( v ) &gt; 0 for all This will ensure that condition C1 of the AIS procedure is always satisfied. We have already shown that conditions C2 and C3 are satisfied. For condition C4, we can run a blocked Gibbs sampler (Eqs. 5, 6) to generate samples from p the AIS estimator will still converge to the correct value, provided our Markov chain is ergodic (Neal, 2001). How-ever, assessing the accuracy of this estimator can be diffi-cult, as it depends on both the variance of the importance weights and on autocorrelations in the Gibbs sampler. 3.3. Estimating Partition Functions of RBM X  X  The partition function of an RBM can be found by finding the ratio to the normalizer for  X  v v X  with a zero weight matrix. From Eq. 3, we know: Moreover, so we can draw exact independent samples from this  X  X ase-rate X  RBM. AIS in this case allows us to obtain an unbi-ased estimate of the partition function Z closely resembles simulated annealing, since the interme-diate distributions of Eq. 13 take form: p ( v ) = We gradually change  X  to 1, annealing from a simple  X  X ase-rate X  model to the final complex model. The importance weights w ( i ) ensure that AIS produces correct estimates. In this section we briefly review a greedy learning algo-rithm for training Deep Belief Networks. We then show how to obtain an estimate of the lower bound on the log-probability that the DBN assigns to the data. 4.1. Greedy Learning of DBN X  X  Consider learning a DBN with two layers of hidden fea-tures as shown in Fig. 1 (right panel). The greedy strategy developed by Hinton et al. (2006) uses a stack of RBM X  X  (Fig. 1, middle panel). We first train the bottom RBM with A key observation is that the RBM X  X  joint distribution layer weights tied to W 2 = W 1 &gt; . We now consider untying For any approximating distribution Q ( h 1 | v ) , the DBN X  X  log-likelihood has the following variational lower bound: ln p ( v | W 1 , W 2 )  X  X where H (  X  ) is the entropy functional. We set Q ( h 1 | v ) = p ( h 1 | v , W 1 ) defined by the RBM (Eq. 5). Initially, when W 2 = W 1 &gt; , Q is the DBN X  X  true factorial posterior over h bound will lead to an increase in the true likelihood of the model. Maximizing the bound of Eq. 19 with frozen W 1 is equivalent to maximizing: This is equivalent to training the second layer RBM with vectors drawn from Q ( h 1 | v ) as data.
 This scheme can be extended by training a third RBM on h W on the log-likelihood, though the log-likelihood itself ca n fall (Hinton et al., 2006). Repeating this greedy, layer-by -layer training several times results in a deep, hierarchica l model.
 In practice, when adding a new layer l , we typically do not the new RBM does not need to be the same as the number of the visible units of the lower-level RBM. 4.2. Estimating Lower Bounds for DBN X  X  Consider the same DBN model with two layers of hidden features shown in Fig. 1. The model X  X  joint distribution is: where p ( v | h 1 ) is defined by Eq. 6), and p ( h 1 , h joint distribution defined by the second layer RBM. Note that p ( v | h 1 ) is normalized.
 By explicitly summing out h 2 , we can easily evaluate an approximating factorial distribution Q , which we get as a byproduct of the greedy learning procedure, and the varia-tional lower bound of Eq. 19, we obtain: ln X The entropy term H (  X  ) can be computed analytically, since Q is factorial. The partition function Z is estimated by run-ning AIS on the top-level RBM. And the expectation term can be estimated by a simple Monte Carlo approximation:
X where h 1( i )  X  Q ( h 1 | v ) . The variance of this Monte Carlo estimator will be proportional to 1 /M provided the vari-terested in calculating the lower bound averaged over the test set containing N 1 N In this case the variance of the estimator induced by the Monte Carlo approximation will asymptotically scale as 1 / ( N t M ) . We will show in the experimental results sec-tion that the value of M can be small provided N The error of the overall estimator  X  r mostly dominated by the error in the estimate of ln Z . In our experiments, we obtained unbiased estimates of  X  Z and its standard deviation  X   X  using Eqs. 11, 12. We report ln and ln (  X  Z  X   X   X  ) .
 Estimating this lower bound for Deep Belief Networks with more layers is now straightforward. Consider a DBN with L hidden layers. The model X  X  joint distribution and its ap-proximate posterior distribution Q are given by: p v , h 1 , ..., h L ) = p ( v | h 1 ) ...p ( h L  X  2 | h L  X  1 Q ( h 1 , ..., h L | v ) = Q ( h 1 | v ) Q ( h 2 | h 1 ) ...Q ( h The bound can now be obtained by using Eq. 22. Note that most of the computation resources will be spent on estimating the partition function Z of the top level RBM. In our experiments we used the MNIST digit dataset, which contains 60,000 training and 10,000 test images of ten handwritten digits (0 to 9), with 28  X  28 pixels. The dataset was binarized: each pixel value was stochastically set to 1 in proportion to its pixel intensity. Samples from the train -ing set are shown in Fig. 2 (top left panel). Annealed im-portance sampling requires the  X  of intermediate distributions. In all of our experiments th is sequence was chosen by quickly running a few preliminary experiments and picking the spacing of  X  mize the log variance of the final importance weights. The biases b A of a base-rate model (see Eq. 18) were set by maximum likelihood, then smoothed to ensure that p ( v ) &gt; 0 ,  X  v  X  X  . Code that can be used to reproduce experimen-tal results is available at www.cs.toronto.edu/  X  rsalakhu. 5.1. Estimating partition functions of RBM X  X  In our first experiment we trained three RBM X  X  on the MNIST digits. The first two RBM X  X  had 25 hidden units and were learned using CD (section 2) with T =1 and T =3 respectively. We call these models CD1(25) and CD3(25). The third RBM had 20 hidden units and was learned using CD with T =1. For all three models we can calculate the ex-act value of the partition function simply by summing out the 784 visible units for each configuration of the hiddens. For all three models we used 500  X  0 to 0.5, 4,000  X  10,000  X  14,500 intermediate distributions.
 Table 1 shows that for all three models, using only 10 AIS runs, we were able to obtain good estimates of partition functions in just 20 seconds on a Pentium Xeon 3.00GHz machine. For model CD1(25), however, the variance of the estimator was high, even with 100 AIS runs. However, figure 3 (top row) reveals that as the number of annealing runs is increased, AIS can almost exactly recover the true value of the partition function across all three models. We also estimated the ratio of normalizing constants of two RBM X  X  that have different numbers of hidden units: CD1(20) and CD1(25). This estimator could be used to do complexity control. In detail, using 100 AIS runs with uniform spacing of 10,000  X  ln ( Z CD1(20) /Z CD1(25) ) =  X  24 . 49 with an error estimate CD1(25) was generated by starting a Markov chain at the previous sample and running it for 10,000 steps. Com-pared to the true value of  X  24 . 18 , this result suggests that our estimates may have a small systematic error due to the Markov chain failing to visit some modes.
 Our second experiment consisted of training two more re-alistic models: CD1(500) and CD3(500). We used exactly the same spacing of  X  rate model. Results are shown in table 1 (bottom row). For each model we were able to get what appears to be a rather accurate estimate of Z . Of course, we are relying on an em-pirical estimate of AIS X  X  accuracy, which could potentiall y be misleading. Nonetheless, Fig. 3 (bottom row) shows that as we increase the number of annealing runs, the value of the estimator does not oscillate drastically.
 While performing these tests, we observed that contrastive divergence learning with T =3 results in considerably better generative model than CD learning with T =1: the differ-ence of 20 nats is striking! Clearly, the widely used prac-tice of CD learning with T =1 is a rather poor  X  X ubstitute X  for maximum likelihood learning. Inspired by this result, we trained a model by starting with T =1, and gradually increasing T to 25 during the course of CD training, as suggested by (Carreira-Perpinan &amp; Hinton, 2005). We call this model CD25(500). Training this model was computa-tionally much more demanding. However, the estimate of the average test log-probability for this model was about  X  86 , which is 39 and 19 nats better than the CD1(500) and CD3(500) models respectively. Fig. 2 (bottom row) shows samples generated from all three models by randomly ini-tializing binary states of the visible units and running alt er-nating Gibbs for 100,000 steps. Certainly, samples gener-ated by CD25(500) look much more like the real handwrit-ten digits, than either CD1(500) or CD3(500).
 We also obtained an estimate of the log ratio of two parti-tion functions  X  r using 10,000  X  the individual log-partition functions were ln  X  Z 451 . 28 and ln  X  Z CD3(500) = 280 . 09 , in which case the log ratio is 451 . 28  X  280 . 09=171 . 19 . This is in agreement (to within three standard deviations) with the direct estimate of the ratio,  X  r For a simple comparison we also trained several mixture of Bernoullis models (see Fig. 2, top left panel) with 10, 100, and 500 components. The corresponding average test log-probabilities were  X  168 . 95 ,  X  142 . 63 , and  X  137 . 64 data generated from the mixture model looks better than CD3(500), although our quantitive results reveal this is du e to over-fitting. The RBM X  X  make much better predictions. 5.2. Estimating lower bounds for DBN X  X  We greedily trained three DBN models with two hidden layers. The first model, called DBN-CD1, was greedily learned by freezing the parameter vector of the CD1(500) model and learning the 2 nd layer RBM with 2000 hidden units using CD with T =1. Similarly, the other two models, DBN-CD3 and DBN-CD25, added 2000 hidden units on top of CD3(500) and CD25(500), using CD with T =3 and T =25 respectively. Training the DBN X  X  took roughly three times longer than the RBM X  X .
 Table 2 shows the results. We used 15,000 intermediate distributions and 500 annealing runs to estimate the parti-Further sampling was required for the simple Monte Carlo approximation of Eq. 23. We used M =5 samples from the approximating distribution Q ( h | v ) for each data vec-tor v . Setting M =100 did not make much difference. Ta-ble 2 also reports the empirical error in the estimate of the lower bound  X  r Var ( X  r Note that models DBN-CD1 and DBN-CD3 significantly outperform their single layer counterparts: CD1(500) and CD3(500). Adding a second layer for those two models im-proves model performance by at least 25 and 7 nats. This corresponds to a dramatic improvement in the quality of samples generated from the models (Fig. 2, bottom row). Observe that greedy learning of DBN X  X  does not appear to suffer severely from overfitting. For single layer models, the difference between the estimates of training and test log-probabilities was about 3 nats. For DBN X  X , the corre-sponding difference in the estimates of the lower bounds was about 4 nats, even though adding a second layer intro-duced over twice as many (or one million) new parameters. The result of our experiments for DBN-CD25, however, was very different. For this model, on the test data we ob-tained  X  r mate of  X  86 . 34 for the average test log-probability of the CD25(500) model. Clearly, we cannot confidently assert that DBN-CD25 is a better generative model compared to the carefully trained single layer RBM. This peculiar resul t also supports previous claims that if the first level RBM al-ready models data well, adding extra layers will not help (LeRoux &amp; Bengio, 2008; Hinton et al., 2006). As an ad-ditional test, instead of randomly initializing parameter s of units switched (see Fig. 1). This initialization ensures th at the distribution over the visible units v defined by the two-layer DBN is exactly the same as the distribution over v training data log-likelihood can only improve. After care-fully training the second level RBM, our estimate of the lower bound on the test log-probability was only  X  85 . 97 Once again, we cannot confidently claim that adding an ex-tra layer in this case yields better generalization. The original paper of Hinton et al. (2006) showed that for DBN X  X , each additional layer increases a lower bound (see Eq. 19) on the log-probability of the training data, pro-vided the number of hidden units per layer does not de-crease. However, assessing generalization performance of these generative models is quite difficult, since it require s enumeration over an exponential number of terms. In this paper we developed an annealed importance sampling pro-cedure that takes advantage of the bipartite structure of th e RBM. This can provide a good estimate of the partition function in a reasonable amount of computer time. Further-more, we showed that this estimator, along with approx-imate inference, can be used to obtain an estimate of the lower bound on the log-probability of the test data, thus al-lowing us to obtain some quantitative evaluation of the gen-eralization performance of these deep hierarchical models . There are some disadvantages to using AIS. There is a need to specify the  X  diate distributions. The number and the spacing of  X  be problem dependent and will affect the variance of the estimator. We also have to rely on the empirical estimate of AIS accuracy, which could potentially be very misleading (Neal, 2001; Neal, 2005). Even though AIS provides an unbiased estimator of Z , it occasionally gives large overes-timates and usually gives small underestimates, so in prac-tice, it is more likely to underestimate of the true value of the partition function, which will result in an overestimat e of the log-probability. But these drawbacks should not re-sult in disfavoring the use of AIS for RBM X  X  and DBN X  X : it is much better to have a slightly unreliable estimate than no estimate at all, or an extremely indirect estimate, such as discriminative performance (Hinton et al., 2006). We find AIS and other stochastic methods attractive as they can just as easily be applied to undirected graphical models that generalize RBM X  X  and DBN X  X  to exponential family distributions. This will allow future application to mod-els of real-valued data, such as image patches (Osindero &amp; Hinton, 2008), or count data (Gehler et al., 2006). Another alternative would be to employ deterministic ap-proximations (Yedidia et al., 2005) or deterministic upper bounds (Wainwright et al., 2005) on the log-partition func-tion. However, for densely connected MRF X  X , we would not expect these methods to work well. Indeed, preliminary results suggest that these methods provide quite inaccurat e estimates of (or very loose upper bounds on) the partition function, even for small RBM X  X  when trained on real data . We thank Geoffrey Hinton and Radford Neal for many helpful suggestions. This research was supported by NSERC and CFI. Iain Murray is supported by the govern-ment of Canada.

