 We study the problem of predicting the labelling of a graph in the online learning framework. Con-sider the following game for predicting the labelling of a graph: Nature presents a graph; nature minimise the total number of mistakes M = |{ t :  X  y t 6 = y t }| . If nature is adversarial, the learner will always mispredict, but if nature is regular or simple, there is hope that a learner may make only a few mispredictions. Thus, a central goal of online learning is to design algorithms whose total mispredictions can be bounded relative to the complexity of nature X  X  labelling. In [9, 8, 7], the cut size (the number of edges between disagreeing labels) was used as a measure of the complexity of a graph X  X  labelling, and mistake bounds relative to this and the graph diameter were derived. The strength of the methods in [8, 7] is in the case when the graph exhibits  X  X luster structure X . The apparent deficiency of these methods is that they have poor bounds when the graph diameter is large relative to the number of vertices. We observe that this weakness is not due to insufficiently tight bounds, but is a problem in their performance. In particular, we discuss an example of a n -vertex labelled graph with a single edge between disagreeing label sets. On this graph, sequential prediction using the common method based upon minimising the Laplacian semi-norm of a labelling, subject to constraints, incurs  X  ( incurred by an optimal online algorithm is bounded by O (ln n ) .
 We solve this problem by observing that there exists an approximate structure-preserving embedding of any graph into a path graph. In particular the cut-size of any labelling is increased by no more than a factor of two. We call this embedding a spine of the graph. The spine is the foundation on which we build two algorithms. Firstly we predict directly on the spine with the 1-nearest-neighbor algorithm. We demonstrate that this equivalent to the Bayes-optimal classifier for a particular Markov random field. A logarithmic mistake bound for learning on a path graph follows by the Halving algorithm analysis. Secondly, we use the spine of the graph as a foundation to add a binary support tree to the original graph. This enables us to prove a bound which is the  X  X est of both worlds X   X  if the predicted set of vertices has cluster-structure we will obtain a bound appropriate for that case, but if instead, the predicted set exhibits a large diameter we will obtain a polylogarithmic bound.
 Previous work. The seminal approach to semi-supervised learning over graphs in [3] is to predict with a labelling which is consistent with a minimum label-separating cut. More recently, the graph Laplacian has emerged as a key object in semi-supervised learning, for example the semi-norm induced by the Laplacian is commonly either directly minimised subject to constraints, or used as a regulariser [14, 2]. In [8, 7] the online graph labelling problem was studied. An aim of those papers was to provide a natural interpretation of the bound on the cumulative mistakes of the kernel perceptron when the kernel is the pseudoinverse of the graph Laplacian  X  bounds in this case being relative to the cut and (resistance) diameter of the graph. In this paper we necessarily build directly on the very recent results in [7] as those results depend on the resistance diameter of the predicted vertex set as opposed to the whole graph [8]. The online graph labelling problem is also studied in [13], and here the graph structure is not given initially. A slightly weaker logarithmic bound for the online graph labelling problem has also been independently derived via a connection to an online routing problem in the very recent [5]. We study the process of predicting a labelling defined on the vertices of a graph. Following the makes a prediction  X  y t for the label value, after which the correct label is revealed. This feedback information is then used by the learning algorithm to improve its performance on further examples. We analyse the performance of a learning algorithm in the mistake bound framework [12]  X  the aim is to minimise the maximum possible cumulative number of mistakes made on the training sequence. A graph G = ( V,E ) is a collection of vertices V = { v 1 ,...,v n } joined by connecting (possibly weighted) edges. Denote i  X  j whenever v i and v j are connected so that E = { ( i,j ) : i  X  j } is the set of unordered pairs of connected vertex indices. Associated with each edge ( i,j )  X  E is a weight A ij , so that A is the n  X  n symmetric adjacency matrix . We say that G is unweighted if A ij = 1 for every ( i,j )  X  E and is 0 otherwise. In this paper, we consider only connected graphs  X  that is, graphs such that there exists a path between any two vertices. The Laplacian G of a graph G is the n  X  n matrix G = D  X  A , where D is the diagonal degree matrix such that D ii = P j A ij . The quadratic form associated with the Laplacian relates to the cut size of graph labellings. Definition 1. Given a labelling u  X  IR n of G = ( V,E ) we define the cut size of u by the number of cuts.
 We evaluate the performance of prediction algorithms in terms of the cut size and the resistance diameter of the graph. There is an established natural connection between graphs and resistive networks where each edge ( i,j )  X  E is viewed as a resistor with resistance 1 /A ij [4]. Thus the unit current flow between v i and v j . The effective resistance may be computed by the formula [11] where  X  +  X  denotes the pseudoinverse and e 1 ,..., e n are the canonical basis vectors of IR n . The between any pair of vertices on the graph. As we will show, it is possible to develop online algorithms for predicting the labelling of a graph which have a mistake bound that is a logarithmic function of the number of vertices. Conversely, we first highlight a deficiency in a standard Laplacian based method for predicting a graph labelling. Given a partially labelled graph G = ( V,E ) with | V | = n  X  that is, such that for some `  X  n , y semi-norm interpolant is defined by We then predict using  X  y i = sgn(  X  y i ) , for i = 1 ,...,n .
 The common justification behind the above learning paradigm [14, 2] is that minimizing the cut (1) encourages neighbouring vertices to be similarly labelled. However, we now demonstrate that in the online setting such a regime will perform poorly on certain graph constructions  X  there exists a trial sequence on which the method will make at least  X  ( Definition 2. An octopus graph of size d is defined to be d path graphs (the tentacles) of length d (that is, with d + 1 vertices) all adjoined at a common end vertex, to which a further single head vertex is attached, so that n = | V | = d 2 + 2 . This corresponds to the graph O 1 ,d,d discussed in [8]. Theorem 3. Let G = ( V,E ) be an octopus graph of size d and y = ( y 1 ,...,y | V | ) the labelling such that y i = 1 if v i is the head vertex and y i =  X  1 otherwise. There exists a trial sequence for which online minimum semi-norm interpolation makes  X  ( p | V | ) mistakes.
 Proof. Let the first query vertex be the head vertex, and let the end vertex of a tentacle be queried at each subsequent trial. We show that this strategy forces at least d mistakes. The solution to the min-imum semi-norm interpolation with boundary values problem is precisely the harmonic solution [4] unique and the graph labelling problem is identical to that of identifying the potential at each vertex of a resistive network defined on the graph where each edge corresponds to a resistor of 1 unit; the harmonic principle corresponds to Kirchoff X  X  current law in this case. Using this analogy, suppose is queried. Suppose a current of k X  flows from the head to the body of the graph. By Kirchoff X  X  law, a current of  X  flows along each labelled tentacle (in order to obey the harmonic principle at every vertex it is clear that no current flows along the unlabelled tentacles). By Ohm X  X  law  X  = 2 d + k . Minimum semi-norm interpolation therefore results in the solution Hence the minimum semi-norm solution predicts incorrectly whenever k &lt; d and the algorithm makes at least d mistakes.
 The above demonstrates a limitation in the method of online Laplacian minimum semi-norm inter-polation for predicting a graph labelling  X  the mistake bound can be proportional to the square root of the number of data points. We solve these problems in the following section. We demonstrate a method of embedding data represented as a connected graph G into a path graph, we call it a spine of G , which partially preserves the structure of G . Let P n be the set of path graphs with n vertices. We would like to find a path graph with the same vertex set as G , which solves If a Hamiltonian path H of G (a path on G which visits each vertex precisely once) exists, then the approximation ratio is  X  H ( u )  X  however, and such a path is not guaranteed to exist. As we shall see, a spine S of G may be found efficiently and satisfies  X  S ( u )  X  We now detail the construction of a spine of a graph G = ( V,E ) , with | V | = n . Starting from any node, G is traversed in the manner of a depth-first search (that is, each vertex is fully explored of the vertices ( m  X  | E | ) in the order that they are visited is formed, allowing repetitions when of pairs of consecutive vertices in V L . Let u be an arbitrary labelling of G and denote, as usual,  X  contains every element of E G no more than twice,  X  L ( u )  X  2 X  G ( u ) .
 We then take any subsequence V 0 L of V L containing every vertex in V exactly once. A spine S = ( V,E S ) is a graph formed by connecting each vertex in V to its immediate neighbours in the subsequence V 0 L with an edge. Since a cut occurs between connected vertices v i and v j in S only if a cut occurs on some edge in E L located between the corresponding vertices in the list V L we have Thus we have reduced the problem of learning the cut on a generic graph to that of learning the cut on a path graph. In the following we see that 1-nearest neighbour (1-NN) algorithm is a Bayes optimal algorithm for this problem. Note that the 1-NN algorithm does not perform well on general graphs; on the octopus graph discussed above, for example, it can make at least  X  ( and even  X  ( n ) mistakes on a related graph construction [8]. We consider implementing the 1-NN algorithm on a path graph and demonstrate that it achieves a mistake bound which is logarithmic in the length of the line. Let G = ( V,E ) be a path graph, where neighbour algorithm, in the standard online learning framework described above, attempts to predict a graph labelling by producing, for each query vertex v i t , the prediction  X  y t which is consistent with the label of the closest labelled vertex (and predicts randomly in the case of a tie).
 Theorem 4. Given the task of predicting the labelling of any unweighted, n -vertex path graph P in the online framework, the number of mistakes, M , incurred by the 1-NN algorithm satisfies where u  X  X  X  1 , 1 } n is any labelling consistent with the trial sequence.
 Proof. We shall prove the result by noting that the Halving algorithm [1] (under certain conditions on the probabilities assigned to each hypothesis) implements the nearest neighbour algorithm on a path graph. Given any input space X and finite binary concept class C  X  { X  1 , 1 } | X | , the Halving algorithm learns any target concept c  X   X  X  as follows. Each hypothesis c  X  X  is given an associated revealed in accordance with the usual online framework. Let F t be the set of feasible hypotheses at it predicts randomly if this is equal to 1 2 ). It is well known [1] that the Halving algorithm makes at most M H mistakes with We now define a probability distribution over the space of all labellings u  X  X  X  1 , 1 } n of P such that the Halving algorithm with these probabilities implements the nearest neighbour algorithm. Let a cut occur on any given edge with probability  X  , independently of all other cuts; Prob( u i +1 6 = u i ) =  X   X  i &lt; n . The position of all cuts fixes the labelling up to flipping every label, and each of these two resulting possible arrangements are equally likely. This recipe associates with each possible labelling u  X  X  X  1 , 1 } n a probability p ( u ) which is a function of the labelling X  X  cut size This induces a full joint probability distribution on the space of vertex labels. In fact (6) is a Gibbs measure and as such defines a Markov random field over the space of vertex labels [10]. The mass function p therefore satisfies the Markov property where here N i is the set of vertices neighbouring v i  X  those connected to v i by an edge. We will give an equivalent Markov property which allows a more general conditioning to reduce to that over boundary vertices .
 Definition 5. Given a path graph P = ( V,E ) , a set of vertices V 0  X  V and a vertex v i  X  V , we define the boundary vertices v ` ,v r (either of which may be vacuous) to be the two vertices in V 0 that are closest to v i in each direction along the path; its nearest neighbours in each direction. The distribution induced by (6) satisfies the following Markov property; given a partial labelling of P defined on a subset V 0  X  V , the label of any vertex v i is independent of all labels on V 0 except those on the vertices v ` ,v r (either of which could be vacuous) Given the construction of the probability distribution formed by independent cuts on graph edges, we can evaluate conditional probabilities. For example, p ( u j =  X  | u k =  X  ) is the probability of an even number of cuts between vertex v j and vertex v k . Since cuts occur with probability  X  and there are | k  X  j | s possible arrangements of s cuts we have Likewise we have that Note also that for any single vertex we have p ( u i =  X  ) = 1 2 for  X   X  X  X  1 , 1 } .
 Lemma 6. Given the task of predicting the labelling of an n -vertex path graph online, the Halving algorithm, with a probability distribution over the labellings defined as in (6) and such that 0 &lt;  X  &lt; 1 2 , implements the nearest neighbour algorithm.
 Proof. Suppose that t  X  1 trials have been performed so that we have a partial labelling of a subset V 0  X  V , { ( v i y  X  1  X  j &lt; t ) &gt; 1 if this probability is equal to 1 2 ). We first consider the case where the conditional labelling includes vertices on both sides of v i t . We have, by (8), that p ( u i t = y | u i j = y j  X  1  X  j &lt; t ) = p ( u i t = y | u ` = y  X  ( ` ) ,u r = y  X  ( r ) ) are queried, respectively. We can evaluate the right hand side of this expression using (9, 10). To show equivalence with the nearest neighbour method whenever  X  &lt; 1 2 , we have from (9, 10, 11) produces predictions exactly in accordance with the nearest neighbour scheme. We also have more simply that for all i t ,` and r and  X  &lt; 1 2 This proves the lemma for all cases.
 A direct application of the Halving algorithm mistake bound (5) now gives that the bound is vacuous when  X  P ( u ) n  X  1 &gt; 1 2 since M is necessarily upper bounded by n ) giving This proves the theorem.
 The nearest neighbour algorithm can predict the labelling of any graph G = ( V,E ) , by first trans-ferring the data representation to that of a spine S of G , as presented in Section 4. We now apply the above argument to this method and immediately deduce our first main result.
 Theorem 7. Given the task of predicting the labelling of any unweighted, connected, n -vertex graph G = ( V,E ) in the online framework, the number of mistakes, M , incurred by the nearest neighbour algorithm operating on a spine S of G satisfies where u  X  X  X  1 , 1 } n is any labelling consistent with the trial sequence.
 Proof. Theorem 4 gives bound (4) for predicting on any path, hence M  X   X  S ( u ) log 2 n  X  1  X  ln 2 + 1 . Since this is an increasing function of  X  S ( u ) for  X  S ( u )  X  n  X  1 and is vacuous at  X 
S ( u )  X  n  X  1 ( M is necessarily upper bounded by n ) we upper bound substituting  X  S ( u )  X  2 X  G ( u ) (equation (3)).
 We observe that predicting with the spine is a minimax improvement over Laplacian minimal semi-norm interpolation. Recall Theorem 3, there we showed that there exists a trial sequence such that Laplacian minimal semi-norm interpolation incurs  X  ( to  X  ( p  X  G ( u ) n ) mistakes by creating a colony of  X  G ( u ) octopi then identifying each previously separate head vertex as a single central vertex. The upper bound (12) is smaller than the prior lower bound.
 The computational complexity for this algorithm is O ( | E | + | V | ln | V | ) time. We compute the spine in O ( | E | ) time by simply listing vertices in the order in which they are first visited during a depth-first search traversal of G . Using online 1-NN requires O ( | V | ln | V | ) time to predict an arbitrary vertex sequence using a self-balancing binary search tree (e.g., a red-black tree) as the insertion of each vertex into the tree and determination of the nearest left and right neighbour is O (ln | V | ) . The Pounce online label prediction algorithm [7] is designed to exploit cluster structure of a graph G = ( V,E ) and achieves the following mistake bound for any  X  &gt; 0 . Here, u  X  IR n is any labelling consistent with the trial sequence, X = { v i 1 ,v i 2 ,... }  X  V is the set of inputs and N ( X, X ,r G ) is a covering number  X  the minimum number of balls of resistance diameter  X  (see Section 2) required to cover X . The mistake bound (13) can be preferable to (12) whenever the inputs are sufficiently clustered and so has a cover of small diameter sets. For example, consider two ( m + 1) -cliques, one labeled  X  +1  X , one  X   X  1  X  with cm arbitrary interconnecting edges ( c  X  1) here the bound (12) is vacuous while (13) is M  X  8 c + 3 (with  X  = 2 m , N ( X, X ,r G ) = 2 , and  X  G ( u ) = cm ). An input space V may have both local clus-ter structure yet have a large diameter. Imagine a  X  X niverse X  such that points are distributed into many dense clusters such that some sets of clusters are tightly packed but overall the distribution is quite diffuse. A given  X  X roblem X  X  X  V may then be centered on a few clusters or alternatively encompass the entire space. Thus, for practical purposes, we would like a prediction algorithm which achieves the  X  X est of both worlds X , that is a mistake bound which is no greater, in order of magnitude, than the maximum of (12) and (13). The rest of this paper is directed toward this goal. We now introduce the notion of binary support tree, formalise the Pounce method in the support tree setting and then prove the desired result.
 Definition 8. Given a graph G = ( V,E ) , with | V | = n , and spine S , we define a binary support tree of G to be any binary tree T = ( V T ,E T ) of least possible depth, D , whose leaves are the vertices of S , in order. Note that D &lt; log 2 ( n ) + 1 .
 We show that there is a weighting of the support tree which ensures that the resistance diameter of the support tree is small, but also such that any labelling of the leaf vertices can be extended to the support tree such that its cut size remains small. This enables effective learning via the support tree. A related construction has been used to build preconditioners for solving linear systems [6]. Lemma 9. Given any spine graph S = ( V,E ) with | V | = n , and labelling u  X  { X  1 , 1 } n , with 4)(log 2 (log 2 n + 2)) 2 .
 Proof. Let v r be the root vertex of T . Suppose each edge ( i,j )  X  E T has a weight A ij , which is the number of edges in the shortest path from v to v 0 . Consider the unique labelling  X  u such that, for 1  X  i  X  n we have  X  u i = u i and such that for every other vertex v p  X  V T , with child Suppose the edges ( p,c 1 ) , ( p,c 2 )  X  E T are at some depth d in T , and let V 0  X  V correspond to the leaf vertices of T descended from v p . Define  X  S ( u V 0 ) to be the cut of u restricted to vertices (  X  u p  X   X  u c 1 ) 2 + (  X  u p  X   X  u c 2 ) 2  X  2  X  2 X  S ( u V 0 ) . Hence of all vertices at depth d form a partition of V , summing (14) first over all parent nodes at a given depth and then over all integers d  X  [1 ,D ] gives We then choose Further, R T = 2 P D d =1 ( d + 1)(log 2 ( d + 1)) 2  X  D ( D + 3)(log 2 ( D + 1)) 2 and so D  X  log 2 n + 1 gives the resistance bound.
 Definition 10. Given the task of predicting the labelling of an unweighted graph G = ( V,E ) the augmented Pounce algorithm proceeds as follows: An augmented graph  X  G = (  X  V ,  X  E ) is formed by attaching a binary support tree of G , with weights defined as in (16), to G ; formally let T = ( V
T ,E T ) be such a binary support tree of G , then  X  G = ( V T ,E  X  E T ) . The Pounce algorithm is then used to predict the (partial) labelling defined on  X  G .
 Theorem 11. Given the task of predicting the labelling of any unweighted, connected, n -vertex graph G = ( V,E ) in the online framework, the number of mistakes, M , incurred by the augmented Pounce algorithm satisfies where N ( X, X ,r G ) is the covering number of the input set X = { v i 1 ,v i 2 ,... }  X  V relative to the resistance distance r G of G and u  X  IR n is any labelling consistent with the trial sequence. Furthermore, Proof. Let u be some labelling consistent with the trial sequence. By (3) we have that  X  S ( u )  X  2 X  G ( u ) for any spine S of G . Moreover, by the arguments in Lemma 9 there exists some labelling have By Rayleigh X  X  monotonicity law the addition of the support tree does not increase the resistance between any vertices on G , hence Combining inequalities (19) and (20) with the pounce bound (13) for predicting  X  u on  X  G , yields 12 X  G ( u ) R T + 2 and the result follows from the bound on R T in Lemma 9. We have explored a deficiency with existing online techniques for predicting the labelling of a graph. As a solution, we have presented an approximate cut-preserving embedding of any graph G = nearest-neighbours algorithm is an efficient realisation of a Bayes optimal classifier. This therefore achieves a mistake bound which is logarithmic in the size of the vertex set for any graph, and the complexity of our algorithm is of O ( | E | + | V | ln | V | ) . We further applied the insights gained to a second algorithm  X  an augmentation of the Pounce algorithm, which achieves a polylogarithmic performance guarantee, but can further take advantage of clustered data, in which case its bound is relative to any cover of the graph.

