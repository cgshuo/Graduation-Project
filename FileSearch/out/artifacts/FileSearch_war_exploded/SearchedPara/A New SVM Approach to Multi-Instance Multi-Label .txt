
In recent years, multi-instance multi-label learning (MIML) has attracted significant attention in the machine learning community. MIML is a recently proposed learning framework where each example is associated with a bag of instances as well as a set of labels [1], [2], [3], [4]. There are many real-world applications such as scene classification, text categorization, and gene sequence encoding, which can be formalized under the MIML framework. In scene classification, an image generally partitions into several segments, each can be represented as an instance, while such an image can be labeled into multiple semantic classes simultaneously, such as airplane, ground, building, sky, face, lizard, rock, . . . as shown in Figure 1. In text categorization, each document usually comprises of several sections or paragraphs, each can be regarded as an instance, while the document can be assigned to a set of predefined topics such as politics, celebrities, Nobel Prize. In bioinformatics, a gene sequence generally encodes a number of segments, each can be expressed as an instance, while this sequence may be associated with several functional classes, such as metabolism, transcription and protein synthesis.

In many real-world MIML applications, we observe that given an MIML example, each instance in a bag of instances is mostly associated with a single label and the set of labels of the example is the aggregation of all instance labels. In scene classification, as shown in Figure 1 the object airplane (top left image) is comprised of multiple segments, e.g. two airplane-wing-like segments, an airplane-tail-like segment, and an airplane-body-like segment. These airplane-like seg-ments should be classified into the airplane object class. Similarly, in text categorization each topic in a document is usually described by one or more sections or paragraphs. These sections or paragraphs should be categorized into the topic class that they discuss. In bioinformatics, a functional class in a gene sequence is usually described by several gene segments and these gene segments should be classified into the same functional class as well. Hence, in our approach we make an explicit model assumption that each instance in an MIML examples is associated with exactly a single label.
In previous approaches [1], MIML problem is reduced to its equivalence in the traditional supervised learning, i.e. single-instance single-label learning (SISL) where each example is restricted to have only one instance and only one label. This reduction is done by assigning each instance in a bag of instances to each label in the set of labels. Although this transformation from MIML to SISL is feasible, there may be many mislabeled instances. For example, in the airplane image (top left image in Figure 1) the airplane-like segments can be mislabeled as the ground object class and vice versa. Similarly, our proposed approach also reduces MIML problem to SISL problem. By contrast, each instance in the bag of instances is assigned to a single best suitable label in the set of labels in stead of all possible labels in the label sets. In our airplane example, the airplane-like segments should be assigned to a single label: the airplane class. Here, we make an assumption that each instance in an MIML example can be described by at most one semantic class label.

In this paper, we propose a new SVM approach to MIML named SISL-MIML, denoting a novel reduction of MIML problem to the traditional SISL problem. In brief, given an MIML example SISL-MIML seeks the best suitable single label belonging to the set of labels for all instances in the bag of instances simultaneously. Hence, the connections between the instances and labels of an MIML example are explicitly exploited by SISL-MIML. Subsequently, the set of labels of a test example is determined by aggregating all labels of instances in the bag. Each instance is involved in determining the set of labels and the connections between different classes are also addressed in the aggregation phase.
The rest of this paper is organized as follows. Section II reviews the formal definition of MIML and the related work. Section III describes the novel SVM approach to MIML. Section IV reports experimental results on an artificially generated data set as well as the two real-world MIML applications. Finally, Section V summaries and concludes our work.

In this section, we first reintroduce the formal definition of MIML. In the multi-instance multi-label learning frame-work, a learning algorithm typically takes a set of labeled training examples L = { ( X 1 ,Y 1 ) , ( X 2 ,Y 2 ) ,..., ( X as input, where X i  X  X is a bag of instances 1 ,x i 2 ,...,x i n i } and Y i  X  Y is a set of labels 1 ,y i 2 ,...,y i l i } associated with X i . Here n i is the number of instances in X i and l i is the number of labels in Y i goal of MIML is to form a hypothesis h MIML : 2 X 7 X  2 Y which maps a bag of instances X i to a set of labels Y The MIML framework can be considered as a generalization from the learning frameworks of multi-instance learning [5], multi-label learning [6], [7] and traditional supervised learning.

Multi-instance learning [5], or multi-instance single-label learning (MISL), was first proposed by Dietterich et al. in their study of predicting the drug molecule activity level. MISL is proposed as a variation of traditional supervised learning framework with incomplete knowledge about labels of training examples. The goal of MISL is to learn a hypothesis h MISL : 2 X 7 X  { +1 ,  X  1 } from a set of MISL training examples { ( X i ,y i ) | 1  X  i  X  n } , where X i a bag of instances { x i 1 ,x i 2 ,...,x i n the binary label of the instance X i . Since the initial work of Dietterich et al. [5], a large number of novel algorithms has been developed in contribution to the development of MISL [8], [9], [10], [11], [12], [13]. In addition, there are many successfully real-world applications of MISL especially in image categorization and retrieval [14], [15], [16], [17], [18]. A more thorough review of multi-instance learning can be found in [19].

Multi-label learning [6], [7], or single-instance multi-label learning (SIML), refers to the classification problem where each example can be assigned to multiple class labels simultaneously. SIML is emerged from the investigation of text categorization problems. The goal of SIML is to learn a mapping h SIML : X 7 X  2 Y from a set of SIML training examples { ( x i ,Y i ) | 1  X  i  X  n } , where x i  X  X  is a single instance and Y i  X  X  is a set of labels { y i 1 ,y i 2 ,...,y ciated with x i . SIML has found applications in many differ-ent domains, such as natural language processing, computer vision, human computer interaction, bioinformatics, health care, and physiology [20], [21]. There are many existing learning algorithms proposed to exploit the similarity of examples and the correlation among classes [22], [23], [24], [25], [26], [7], [6], [27]. A more thorough review of multi-label learning can be found in [28].

While a large body of work exists on MIML, here we highlight some of the more relevant work. In [1], Zhou and Zhang have formalized the MIML framework and proposed two MIML algorithms named MIMLBOOST and MIMLSVM. The two algorithms transformed MIML into traditional supervised learning (SISL) using MISL and SIML as the connection, respectively. In particular, MIMLBOOST transforms the MIML problem into a multi-instance learning problem (MISL). Each MIML example ( X i ,Y i ) is converted into |Y| number of MISL examples { ([ X i ,y ] , I ( y  X  Y i ) | y  X  Y} , where [ X i ,y ] contains n i instances { [ x i 1 ,y ] , [ x i 2 ,y ] ,..., [ x i n i catenating each of X i  X  X  instance with label y , and I ( y  X  Y ) =  X  1 is the corresponding label indicating whether the label y belongs to the set of label Y i . In order to solve the derived MISL problem, MIMLBOOST employs a spe-cific algorithm named MIBOOSTING [29]. MIBOOSTING reduces the MISL problem into an SISL one under the assumption that each instance in the bag contributes equally and independently to a bag X  X  label. By contrast, MIMLSVM transforms the MIML problem into a multi-label learning problem (SIML). Each MIML example ( X i ,Y i ) is converted into an SIML example (  X  ( X i ) ,Y i ) , where  X  (  X  ) combines all instances in a bag X i into a single instance  X  ( X constructive clustering. In order to solve the derived SIML problem, MIMLSVM employs a specific algorithm named MLSVM [20]. This algorithm constructs a binary classifier for each class label y  X  Y where an instance x i associated with a label set Y i is assigned to the positive class ( +1 ) if y  X  Y i otherwise it is assigned to the negative class (  X  1 ). Similar to our new proposed algorithm SISL-MIML, these two algorithms [1] transform the MIML problem into its equivalence SISL problem. However, our proposed algorithm is able to exploit the similarity between instances in a bag and the correlation between labels in the label set.
Zhang and Zhou [2] proposed a maximum margin method for the MIML problem named M 3 MIML. The algorithm assumes a linear model for each class, where the output on one class is set to be the maximum prediction of all the MIML example X  X  instances with respect to corresponding linear model. Subsequently, the outputs on all possible classes are combined to define the margin of the MIML example over the classification system. Similar to our new proposed algorithm SISL-MIML, M 3 MIML is able to ex-plicitly exploit the connections between the instances and the labels of an MIML examples. In addition, both algo-rithms utilize the maximum margin learning framework to formulate the optimization problem. However, our proposed algorithm seeks to assign a single best label for each of instances in the example bag simultaneously.

Finally, in [3] the authors developed an innovative neu-ral network style algorithm named MIMLRBF, i.e. Multi-Instance Multi-Label Radial Basis Function. The algorithm is derived from the popular radial basis function method where the first network layer consists of medoids (i.e. bags of instances) formed by performing k-MEDOIDS clustering on MIML examples for each possible class, in which a variant of Hausdorff metric [30] is utilized to measure the distance between bags [31]. Second layer weights of MIMLRBF network are optimized by minimizing a sum-of-squares error function and worked out through singular value decomposition (SVD) [32]. Similar to our new pro-posed method, connections between instances and labels are directly exploited in the MIMLRBF algorithm. By contrast, our proposed algorithm utilizes a different learning frame-work, i.e. a maximum margin approach.

In this section, we discuss how our proposed approach of the MIML problem can be formalized using the maximum margin learning framework. Given a set of MIML labeled training examples L = { ( X 1 ,Y 1 ) , ( X 2 ,Y 2 ) ,..., ( X our proposed algorithm SISL-MIML is also formulated to minimize the regularized empirical risk, and where  X (  X  ) is a convex and monotonically increasing function which serves as a regularizer with a regularization constant  X  &gt; 0 ; and l ( X i ,Y i ,w ) is a nonnegative loss function of an example ( X i ,Y i ) measuring the amount of inconsistency between the correct label Y i and the predicted label arising from using the weight parameter w .

Similar to the Multiclass-SVM proposed by [33], we consider a mapping  X  : X  X Y 7 X  F which projects each instance-label pair ( x,y )  X  X  X Y to  X ( x,y ) in a new space F , which is defined as where I (  X  ) is the indicator function.
 In brief, our proposed algorithm, named SISL-MIML, consists of two main components: (1) LABEL-PROPAGATION: given an MIML example we seek the most  X  X uitable X  single label for each instance; (2) MARGIN-MAXIMIZATION: given the current labels for all instances, we find the maximum-margin hyperplane that best separates current labeled instances.

In the first step, we make an explicit model assumption that each instance in an MIML examples is associated with exactly a single label. In other words, given an MIML example ( X i ,Y i ) our proposed algorithm SISL-MIML seeks the best single label y i j  X  Y i for each instance x simultaneously. Based on our model assumption, we also enforce that there should be at least one or more different instances in the bag of instances assigned to each label in the set of labels. Hence, in addition to assigning a single label for each instance, our approach also requires that there are at least K  X  1 number of instances in a bag belonging to a given class label in the set of labels. For example, in Figure 1 there are multiple image segments or patches belonging to each object such as face, airplane, building, lizard, rock, etc. Similarly in text categorization, there should also be multiple or at least one section(s) or paragraph(s) to describe any given topics in a document. Given the set of labels Y i for a bag of instances X i , we can view the process of assigning a single label to each instance in the bag as a label propagation procedure which propagates the bag label to each individual instance in the bag. In the margin-based learning framework, given a current weight parameter w and a requirement that there are at least K  X  1 instances belonging to each class label, we can seek the best suitable label assignment for all instances in X i by solving the following Integer Programming (IP), called LABEL-PROPAGATION:
O PTIMIZATION I: LABEL-PROPAGATION subject to: where I (  X  ) is the indicator function; P n i j =1 I ( X  y i the number of instances assigned to the class label y i  X  w value associated with any labels  X  y i j  X  X  . In Equation 1, the objective of the given LABEL-PROPAGATION formulation seeks the labels belong to the set of label Y i whose associ-ated scores are as large as the maximum score. Furthermore, the LABEL-PROPAGATION integer programing also ex-ploits the connection between instances and labels of a given MIML example by enforcing the constraint that there are at least K  X  1 instances belonging to each class label and ensuring all instance labels belong to the set of correct labels. Consequently, the solution of the LABEL-PROPAGATION optimization in Equation 1, Y i = { y i 1 ,..., y i n viewed as the approximated true labels of all instances in a bag of instances X i given the current weight parameter w , which can be used to compute the loss function l ( X i ,Y in the next step. In other words, our first step can be viewed as a relaxation of an explicit division of all instances into different groups which is assigned to different labels in the label set.

In the second step, the SISL-MIML algorithm can be obtained by considering the situation where we use the L2-norm regularization, and the loss function l ( X i ,Y i ,w ) is set to the average hinge loss of all instances in the bag of instances X i , 1 n where Y i = { y i 1 ,..., y i n of all instances in the bag of instances (i.e. the solution of integer programming 1). Specifically, the SISL-MIML learns a weight vector w and slack variables  X  via the following quadratic optimization:
O PTIMIZATION II: MARGIN-MAXIMIZATION subject to:  X  ( X i ,Y i )  X  L and  X  x i j  X  X i : where Y i = { y i 1 ,..., y i n PROPAGATION in equation 1.

In the testing phase, after we have learned the weight parameter w and slack variables  X  , the classification of a test MIML example X t is done by Both of our optimization formulations, LABEL-PROPAGATION and MARGIN-MAXIMIZATION, are depended on the solution of each other. In the LABEL-PROPAGATION, given the weight parameter w , the given integer programming is a convex optimization. Similarly, in the MARGIN-MAXIMIZATION formulation, given the assigned labels of instances for all examples, Y ,  X  1  X  i  X  n , the given quadratic programming is also a convex optimization. Hence, in order to solve the proposed quadratic programming, we employ both the alternating optimization procedure [34] and the stochastic gradient descent approach which has shown to be very efficient and does not require transforming to the dual formulation [35], [36]. Here, we embed the integer programming problem in the stochastic gradient decent procedure to solve the quadratic programming problem. Similar to [36], we restrict the search space to the sphere of radius 1 / The algorithm alternates between a gradient descent step which also include solving the LABEL-PROPAGATION integer programming, and a projection step until reduction of the regularized risk objective function is less than a pre-specified tolerance, . In each iteration, for each MIML example ( X i ,Y i ) the algorithm first solves the LABEL-PROPAGATION integer programing to obtain the approximated true labels Y i = { y i 1 ,..., y i n algorithm computes instances that violate the constraints in the MARGIN-MAXIMIZATION optimization problem. Then the weight parameter w is updated according to the violated instances found in the previous step. In the projection step, the weight parameter w is projected to the sphere of radius 1 / algorithm are given in Algorithm 1.

In order to use the kernel trick , as pointed out in [36], we set w 1 = 0 then w t can be written as where  X  xy is a scalar associated with a vector  X ( x,y ) . Hence, we can incorporate the usage of kernel when com-Algorithm 1 : MIML Algorithm: SISL-MIML
Input: L -the fully labeled data
Initialize w 1 such that k w 1 k X  1 / repeat be the solution of the LABEL-PROPAGATION integer programming until ( R reg ( w t  X  1 )  X  R reg ( w t )) &lt; 
Output: w t puting inner product operations, i.e.:  X  w,  X ( x 0 ,y 0 )  X  = X In our experiment, we use the radial basis function (RBF) as a kernel, where the radius  X  determines the smoothness of the deci-sion boundary.

In this section, we compare performance of our proposed method SISL-MIML with other recently developed MIML algorithms: MIMLRBF [3], M 3 MIML [2], and MIMLSVM, MIMLBOOST [1]. For fair comparison, the RBF kernel is used for all MIML algorithms with the radius  X  = 0 . 2 . Specifically, the MIMLRBF algorithm involves two different parameters: the fraction parameter  X  and the scaling factor  X  which are determined by two-fold cross validation on the training examples where  X   X  { 10% , 20% , 30% } and  X   X  X  0 . 5 , 0 . 6 , 0 . 7 } . In addition, the M 3 MIML algorithm re-quires two different parameters: the cost parameter C which is set to the best values in the range { 10 i |  X  by two-fold cross validation using the training examples and  X  which is set to the default value of 1 . Furthermore, the parameters for the two algorithms MIMLSVM and MIMLBOOST is set according the best values as reported in [1]. Finally, our proposed algorithm SISL-MIML also associates two different parameters: (1) the regularization constant,  X  ; and (2) the number of instances required to belong to a single label class, K . Both of these parameters are determined by two-fold cross validation on the training examples where  X   X  { 10 i |  X  4  X  i  X  4 } and K  X  X  1 , 2 , 3 , 4 } .

To evaluate the performance of different MIML algo-rithms we look at a set of five standard multi-label per-formance measures: Hamming Loss, One Error, Coverage, Ranking Loss, and Average Precision. For Average Preci-sion, the bigger the value the better the performance. While for the other four measures, the smaller the value the better the performance. A more detailed description of these five performance measures can be found in [7], [25].

We evaluate performance of different MIML algorithms on an artificially generated data set and two real-word MIML applications. The artificially generated data set is originated from the USPS data in the UCI repository [37] which contains 9 K SISL examples of 256 features belonging to 10 possible class labels. The generated data contains 4 K MIML examples for training and 10 K MIML examples for testing. We generated multiple versions of the MIML data set in which we vary the number of labels per example, i.e. the size of the set of labels for each MIML example, and instances per label, i.e. the number of instances belongs to each label in the set of labels for each MIML example. Hence, the number of instances per example is the product of the number of instances per label and labels per example. In our experiment, the number of labels per example and instances per label takes values from 2-to-5 and 2-to-4, respectively. Given the number of labels per example and instances per label, i.e. l 1 and l 2 , an MIML example is generated as follow: (1) first we randomly generate l 1 different labels from the set of all possible labels Y based on the uniform distribution and (2) for each generated label we make an uniformed random selection of l 2 instances belonging to the given class label to form the MIML example. In addition, the data sets for the two real-world applications are (1) the image data set collected from the COREL image collection and the Internet; and (2) the text data set collected from the widely studied Reuters-21578 collection [38]. In Table I, we present a brief characteristic description of the two real-world data sets. A more detail description of the data sets are found in [3], [2]. For these two data sets, performance of different MIML algorithms is reported based on ten-fold cross validation.

In Figure 2 (top row), we plot the overall average per-formance of MIML algorithms among different values of the number of labels per example and instances per label on the USPS artificially generated data set. Across different performance metrics, our proposed algorithm SISL-MIML consistently produces superior performance against other MIML methods. Especially there is a significant margin of improvement in Coverage, Ranking Loss and Average Precision performance measures. The improvement in per-formance demonstrates that our proposed algorithm SISL-MIML is able to take advantage of our explicit model assumption since the artificially generated data is constructed based on this assumption.

Furthermore, in Figure 2 (bottom two rows), we plot the average performance (mean  X  standard error) of MIML algorithms on both the image and text data sets. The new SISL-MIML algorithm consistently yields results equal to or better than MIMLRBF and M 3 MIML. Similar to [3], [2], we also observe the significant improvement in performance of both MIMLRBF and M 3 MIML over MIMLSVM and MIMLBOOST. In addition, we also observe that SISL-MIML is able to outperform both MIMLRBF and M 3 MIML on the image data set. This behavior is confirmed our model assumption that each segment in an image is associated with only a single class label.
 Moreover, we also investigate the behavior of different MIML algorithms as we vary the number of labels per example and instances per label of the artificially generated data set. As shown in Figure 3 and 4, we plot the average performance of MIML algorithms versus the number of labels per example and instances per label, respectively. As the number of labels per example increases, performance on Hamming Loss and Coverage gets worse, while perfor-mance on One Error and Average Precision improves. These conflicting trends demonstrate that the size of the set of labels affects different performance measures differently. For example, since the Hamming Loss measures the intersection between the true set of labels and the predicted set of labels, the measure would reflect the difficulty of the MIML problem when the set of labels increases in size. Since the One Error only pays attention to the instance with the highest confidence, the measure reflects the easiness of the MIML problem when the set of labels increases in size. By contrast, as the number of instances per label varies, the performance measures of all MIML algorithms do not seem to be affected. The effect can be explained by the fact that the number of instances belonging to a same class label in each MIML example may not contribute in the process of determining the set of predicted labels. Furthermore, to determine whether a class label belongs to the set of predicted labels for an MIML example, the deciding factor is that there exists an instance of that class label.

In this paper, we introduce SISL-MIML, a novel SVM method for the MIML problem. Our proposed algorithm reduces the MIML problem to the traditional SISL problem. Specifically, given an MIML example, SISL-MIML seeks the best suitable single label belonging to the set of labels for all instances in the bag of instances simultaneously. Hence, the connections between the instances and labels of an MIML example are explicitly exploited by SISL-MIML. Experiments using both the artificially generated data and two real-world applications shows that SISL-MIML are able to produce superior performance in comparison to other MIML algorithms. In addition, using the artificial generated data we also investigate the behavior of different MIML algorithms when we vary the number of labels per examples and instances per label.

