 Life is full of information. The links between the information form all sorts of information networks, such as social network formed by people X  X  interactions on social media, citation network generated by the reference relationship between the papers in academic science and the famous WWW (World Wide Web). The basic composition unit of network is a node, which can be a user, a paper, or a webpage. Apperently, the edge has different meaning in different networks. Network embedding is a very important component of network analysis and study. The large scale and high dimension network can be mapped to a low dimensional space for certain optimization goal. The embedding node vectors preserve the original network X  X  global features and local features, and have a lot more than the network node original representation [ 3 ]. After learning the embeddings of nodes, the embedding vectors are applied into various important data mining tasks, like node classification [ 19 ], network visualization [ 14 ]and link prediction [ 15 ]. task, which can be regarded as learning a mapping function from the nodes to a set of pre-defined and non-overlapping categories. The mapping relationship is the classifier. When the classification task is applied to the network, traditional methods embed the network first, and then use some algorithms like Support Vector Machine (SVM) [ 13 ] to do the classification. This is typically a kind of unsupervised learning method. This label attribute should be considered as well for it can distinguish different nodes, which is usually ignored in the previous algorithms.
 mainly consists of the following two points: on the one hand, for the real network contains a huge amount of data, the learning algorithm should handle the large-scale network. Unfortunately, many existing network embedding algorithms [ 1 , 4 , 17 ] perform well on small networks, but could not deal with large scale networks due to their high computational complexity. On the other hand, adding label information to the embedding learning process may improve the discriminability of node embeddings, but it X  X  a worth thinking problem about where and how to add the label information.
 Embedding proposed by [ 16 ]) method, we propose a new transductive algorithm named TLINE, which uses the SVM (support Vector Machine) as the training classifier. Unlike previous unsupervised network embedding methods, the node embeddings and the SVM classifier are optimized simultaneously in TLINE. By using the edge sampling and the negative sampling techniques in the sto-chastic gradient descent process, the algorithm complexity of TLINE is greatly reduced. So our model is able to learn embeddings of the large networks at a very small time and memory cost. We test TLINE algorithm on Citeseer and DBLP datasets. The performance of TLINE is compared with three competitive baselines, including two popular unsupervised baselines, Deepwalk and LINE, and a state-of-the-art transductive method, MMDW (Max-Margin DeepWalk) [ 18 ]. The experimental results show that the performance of TLINE in node classification task is significantly better than other baselines. The stability of TLINE is also shown in the parameter sensitivity experiments.
 related work about this problem. Section 3 introduces some notations that will be used in the following paper. And Sect. 4 introduces TLINE model which is inspired by LINE and SVM. Section 4 talks about our experiments of TLINE, and compares the results with other algorithms. Section 5 draws a conclusion about this paper and provides the direction for future work. Network embedding aims to create feature representations in low-dimensional space, which preserves the original network structure.
 decomposition) are the common methods to project data into low-dimensional space. And many other primitive network representation learning methods, such as MDS (multi-dimensional scaling) [ 4 ], LLE [ 12 ], Laplacian Eigenmap [ 1 ]and DGE [ 3 ], are also based on spectral factorization. And there is still another kind of method based on the probabilistic graphical models. The key point of this kind of algorithm is modeling generative process of the network and associated texts information by sampling. Some representative algorithms are Link-PLSA-LDA [ 9 ], RTM [ 2 ] and PLANE [ 7 ]. However, the high computational complexity prevents them from being applied to large scale networks.
 Recently, inspired by the widely used distributed representation learning techniques in NLP domain, like Skip-Gram [ 8 ], researchers propose some novel network embedding methods to learn distributed representations for networks. DeepWalk [ 10 ] is proposed by Perozzi and his colleagues, which uses the trun-cated random walks on the networks to generate node sequences and feeds the sequences to the Skip-Gram model as pseudo sentences to obtain node rep-resentations. In order to handle the large-scale networks, Tang et al. propose LINE [ 16 ], which optimizes the objective function to preserve both the local and the global network structures. However, both DeepWalk and LINE are unsuper-vised models, which means that they are not able to utilize the label information or the category information in the network. Actually, label or category informa-tion is common in network data, such as the conference or journals a paper publish on in a paper citation network, or the affiliation of an author in a coau-thor network. Therefore, the distinguishability of the learnt representations is limited in these unsupervised frameworks.
 To take advantage of the label information, semi-supervised learning approaches are employed to learn node representations. LSHM [ 5 ] and MMDW [ 18 ] are two representative methods. LSHM can be applied in heterogeneous networks, which learns node representations in a common latent space for all the different node types. MMDW utilizes the label information and max-margin principle to learn node representations. However, MMDW is hard to be applied for large networks because it is a unified learning framework based on matrix factorization.
 Our motivation is optimizing the loss function of LINE and applying SVM at the same time to make full use of the label information in networks. For a smoother and easier read, we first introduce some notations which will be used in this paper. Consider a partly labeled network G =( V, E ), V is the set of nodes and E is the set of edges. For each edge e i,j  X  E , w edge. { v the unlabeled nodes. And we also assume there are K label types in the network. If v is in class k ,weset y k i = 1, otherwise y k i =  X  1.
 Traditional way for predicting labels of the unlabeled nodes based on unsu-pervised learning methods have two steps, which are embedding and classifica-tion. Embedding means to learn a vector in low-dimension space d | V | . The traditional way is only embedding the node without consid-ering the label information, and in the next step, we use the embeddings of the labeled nodes to train a classifier and make predictions for the unlabeled nodes. But for these transductive learning methods, given the training set { ( v i ,y i ) } ,i =1 , 2 , ..., L and testing set { v j } transductive learning is to find the node representation u function f : u v  X  y which can have a good performance on training set. The difference between the traditional unsupervised way and the transductive way is that transductive manner merges the two steps embedding nodes and training the classifier into one step. 4.1 Large Scale Information Network Embedding In the real world network, the direct relations between different nodes observed by us are actually a small part of the network information. If we only take the edges between nodes into account, there will be a considerable proportion of the information loss. From a global view, in the social network, if two people have many mutual friends, even they are not friends, they are likely to get to know each other through one mutual friend and become friends because of the same hobbies or interests. This global network structure is also called as second-order proximity.
 an algorithm named LINE (Large Scale Information Network Embedding), which uses p u =( w u, 1 , ..., w u, | V | ) to denote that the connect situation of v |
V | X  1 nodes and uses the similarity of p u and p v to measure the proximity of the global network structure.
 contexts are assumed to have the close embeddings. So every node has two roles, one is the node itself, denoted as a vector u i , the other one represents the impact on the other nodes as a context, which is denoted as a vector every directed edge e i,j , we can define the probability of v by v i as: each other if they have the similar distribution of contexts. Then the empirical distribution of p 2 ( v j | v i ) is defined as: where w i,j presents the weight of the edge e i,j , d i denotes the out-degree of node v i ,and N ( i ) is the set of the nodes which have the edge where the starting point is v i . In order to preserve the global structure of the information network, the algorithm should resemble p 2 and  X  p 2 as closely as possible, which also means that we would minimize the KL distance: After omiting some constants, we have the final objective function: After using the negative sampling technique to reduce the computational complexity, the Eq. ( 4 ) is rewritten as:
O where P n ( v )  X  ( d v ) 0 . 75 is the noisy node distribution, M is the number of negative edges, and  X  presents the sigmoid function. Just like the second-order proximity, the first-order proximity is defined as below (See [ 16 ] for details): 4.2 Classification Based on Support Vector Machine For the binary classification problem of the label k , linear support vector machine is equivalent to the optimization problem as below: L ( y term and is expressed as the L2-norm with coefficient  X  of the parameter vector for label k .
 Binary classification is just a special case of multi-class classification. We can also expand the optimization problem to multi-class classification: 4.3 Transductive Network Embedding Given a network where only some of the nodes have labels, the task is to tag the label to the unlabeled nodes. The traditional way have two parts, which can be clearly seen in Fig. 1 , the first step is embedding the node in the information network to a low-dimensional vector, and the second step is using the training set to train the classifier and then do the classification.
 Just as Fig. 2 shown below, the embedding learning and classifier training are proceed simultaneously. The process of embedding learning and classification has influence on each other. As a result, the information of labels will contribute to the quality of node low-dimensional vectors, and the embedding of nodes also have influence on the parameters of the classifier. So the node embedding is more explicit, and the meaning of it is richer.
 improve the results of classification, we combine LINE and SVM together, which means that where  X  is the trade-off parameter used to balance LINE and SVM. For the second-order proximity, we substitute loss function ( 5 ) and loss function ( 8 )into the formula ( 9 ). Finally we have the objective function of TLINE(2nd) as: The same as the first-order proximity: We use ASGD [ 11 ] (asynchronous stochastic gradient descent algorithm) to optimize the objective function of TLINE(1st) and TLINE(2nd). And the learning rate is dynamically computed by using the method mentioned in [ 16 ]. Specifically, the learning rate  X  0 =0 . 025 in the beginning, then it changes as  X  =  X  0 (1  X  t T ), where T is the amount of sampling edges. 5.1 Data Sets We select the following two typical datasets to evaluate our approaches.  X  Citeseer. Citeseer is a paper citation network data used in [ 18 ]. It contains 3312 nodes, 4732 edges and 6 labels. It is an unweighted network, where the citation relationships between papers form a typical network.  X  DBLP. DBLP is a coauthor network data used in [ 16 ]. It contains 18058 nodes, 103011 edges, and 3 labels. The co-author relationships between authors form the network. Two nodes are connected by an edge if and only if they are coauthors. Compared with Citeseer data, the DBLP network is a weighted network, and the weight of the edge is encoded by the number of co-authored papers. 5.2 Compared Methods In the experiments, we compare the following 6 methods to exam the perfor-mance of our approaches.  X  DeepWalk [ 10 ] . DeepWalk is an unsupervised method proposed by Perozzi et al. in 2014, which learns latent representations of vertices in a network. We set parameters as follows, the sliding window size w = 10, the length of each node sequence t = 40, and the number of node sequences for each node  X  = 80. We use liblinear to do the classification, while LINE(1st), LINE(2nd) and
MMDW also use this lib.  X  LINE(1st) [ 16 ] . We employ the LINE with first-order proximity for compar-ison. We sample 5 million edges for Citeseer data, and 50 million edges for
DBLP data. The edge sampling for LINE(2nd), TLINE(1st) and TLINE(2nd) is the same. We set the dimension d = 200 for LINE(1st), LINE(2nd) and
TLINE(2nd).  X  LINE(2nd) [ 16 ] . LINE algorithm with second-order proximity, which assumes that nodes with similar neighbors distributions will have similar embedding vectors.  X  MMDW [ 18 ] . MMDW is a semi-supervised transductive network learning method based on matrix decomposition. MMDW employs the labeling infor-mation and max-margin principle to learn vertex representations. MMDW also use SVM as its classifier. We use the code provided by [ 18 ] and set the dimension d = 200.  X  TLINE(1st). TLINE with first-order proximity. We set  X  =0 . 5,  X  =0 . 02, the dimension d = 10.  X  TLINE(2nd). TLINE with second-order proximity. We set  X  =0 . 5,  X  =0 . 02. 5.3 Node Classification We evaluate the quality of the node embeddings learned by different models when the training ratios vary from 10 % to 90 %. Tables 1 and 2 show the classification accuracies with different training ratios on the two datasets. All results listed are averaged over 20 runs. From the results, we have following observations: (1) The proposed method TLINE consistently outperforms all the baseline (2) MMDW fails to generate results on DBLP data when our workstation has (3) Transductive network embedding methods perform better than unsupervised network embedding methods in most cases. For example, compared with
LINE(1st), TLINE(1st) obtains around 7 % improvement on the Citeseer data and nearly 10 % improvement on the DBLP data. It suggests that label information is crucial to network representation learning and can improve the classification accuracy. 5.4 Parameter Sensitivity TLINE model. The Figs. 3 and 4 show the sensitivity experiment results of the trade-off parameter  X  and the regularization coefficient  X  for TLINE(1st) and TLINE(2nd). In the experiments of TLINE(1st) and TLINE(2nd) on Citeseer dataset,  X  varies from 0.001 to 10 while  X  varies from 0.001 to 10. With the increasing of  X  , the Micro-F1 has an obvious increase from the very beginning but a slight decrease at end. And in DBLP dataset, when  X   X   X  [10 , 100], the Micro-F1 of TLINE(1st) and TLINE(2nd) both have a rela-tively good performance. Through this experiment, we find that the two para-meters have some correlation, and when  X  gets a better value,  X  is less sensitive. In the rest experiments of this paper, we set  X  =0 . 5and  X  =0 . 02 to get better performances on both datasets.
 From Fig. 5 we can see, TLINE(1st) is a little sensitive to the vector space dimension in Citeseer while TLINE(2nd) is insensitivite in both datasets. It means TLINE(2nd) is more universal and robust than TLINE(1st) for vector dimensions. In the other experiments of this paper, we set d = 10 for TLINE(1st) and d = 200 for TLINE(2nd). 5.5 Network Visualization Visualization is an intuitive way to verify whether the learnt representations is discriminative. In this section, we use t-SNE [ 6 ] to display the 2D representations of vertices. Figure 6 shows the results of DeepWalk, LINE and TLINE on DBLP data. In this figure, each dot represents a vertex while colors are encoded into categories. In this case, we choose red, blue and green to indicate authors labeled  X  X ata mining X ,  X  X achine learning X  and  X  X omputer vision X  respectively. From Fig. 6 , we observe that neither DeepWalk nor LINE create clear bound-aries among three different communities, and there are plenty of overlaps in Fig. 6 (a) and (b). However, Fig. 6 (c X  X ) indicates that the boundaries are becom-ing clear gradually with the increase of the training ratio. Particularly, we can obtain well-separated clusters when the training ratio equals 0.9, as shown in Fig. 6 (e) and (f). The node embeddings learnt by TLINE are much more dis-criminative, which indicates effectiveness and improvements of our method. This paper proposes a new transductive algorithm named TLINE which is inspired by LINE. TLINE uses the SVM classifier in node embeddings learn-ing process to improve the nodes X  distinguish degree. By adopting the stochastic gradient descent algorithm, the edge sampling and the negative sampling tech-niques to our method, the complexity of TLINE is greatly reduced, so TLINE is able to handle large scale network at a very small time and memory cost. line algorithms on Citeseer and DBLP datasets. Compared with the newest semi-supervised learning algorithm MMDW, TLINE achieves significantly higher accuracy in node classification task. And the parameter sensitivity experiments also show the stability of TLINE.
 real world, homogeneous networks are just a small part of various information networks, while heterogeneous networks are more common. And we may also have a try to optimize our classification algorithm. In recent years, the deep learning techniques, such as convolution neural network and recursive neural network, outperform the traditional classification algorithms on various catego-rization tasks. So the next step of our work is to replace the old classifier SVM with the more complex deep neural networks.

