 Online reviews have been popularly adopted in many appli-cations. Since they can either promote or harm the reputa-tion of a product or a service, buying and selling fake reviews becomes a profitable business and a big threat. In this paper, we introduce a very simple, but powerful review spamming technique that could fail the existing feature-based detection algorithms easily. It uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. Fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art com-putational approaches and human readers acquire an error rate of 35%-48%, just slightly better than a random guess. While it is challenging to detect such fake reviews, we have made solid progress in suppressing them. A novel defense method that leverages the difference of semantic flows be-tween synthetic and truthful reviews is developed, which is able to reduce the detection error rate to approximate-ly 22%, a significant improvement over the performance of existing approaches. Nevertheless, it is still a challenging research task to further decrease the error rate. Synthetic Review Spamming Demo: www.cs.ucsb.edu/~alex_morales/reviewspam/ I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  text analysis ; K.4.1 [ Computers and Society ]: Pub-lic Policy Issues X  abuse and crime involving computers Review Spam; Spam Detection; Classification such as Amazon, Yelp, and TripAdvisor, allowing users to exchange their personal experiences. Positive reviews can in-crease reputations and bring significant financial gains, while negative ones often cause dramatic sales loss. This fact, un-fortunately, results in strong incentives for review spam, i.e. , writing fake reviews to mislead readers. illustrate three example reviews in Figure 1, among which two are fake reviews. Without very careful scrutiny, it is even difficult for human readers to identify deceptive reviews from truthful ones. Review (a) is a truthful review; (b) is a deceptive review written by a customer who hasn X  X  visited that hotel; and (c) is a deceptive review synthesized from multiple truthful reviews (our algorithm).
 growth of review spam. For instance, Jindal et al. [11] focused on the detection of disruptive review spam (e.g., comments on brands only, non-opinion texts), which is less threatening due to easy identification by human readers. Feng et al. [6] proposed a detection strategy based on the assumption that fake reviews tend to distort the natural distribution of review scores. However, these techniques are not effective in handling the new kinds of fake reviews shown in Figure 1, since they are indeed authentic-looking reviews either deliberately written by humans or synthesized from other human writings.
 Figure 1) that are deliberately written to mislead readers. Based on n-gram and psychological deception features, the detector proposed by Ott et al. can achieve nearly 90% ac-curacy for human written deceptive reviews. It is observed that liars tend to have different writing patterns, such as absence of specific spatial information and excessive use of exclamatory marks, from reviewers that have true experi-ences [19].
 has to spend money to hire human writers, in order to gen-erate such deceptive reviews. Actually, Ott et al. [19] spen-t $1 per review to create 400 deceptive reviews using A-mazon Mechanical Turkers in 14 days. In this paper, we first pretend to be evil by asking the following question \If we were attackers, can we employ a much more economi-cal, high-throughput approach to generate deceptive reviews with minimum human involvement?" . Setting off from this malicious perspective, we bring into attention an automatic review synthesis process. We show that an attacker is able to fake an authentic-looking review (such as (c) in Figure 1) by merely organizing sentences extracted from existing on-line truthful reviews. This process works in a much more economical way in terms of both time and costs; and the most thrilling result is that the state-of-the-art detection al-gorithms including Ott X  X  are not able to handle this kind of fake reviews: The detection error rate is around 35%-44%, just slightly better than a random guess.
 synthesized reviews is quite simple: All the sentences in these reviews were actually written by people who had true experiences. To counter such spam attacks, we develop a new defense framework, based on the observation that subtle semantic incoherence exists in a synthesized review. Pair-wise and multiple sentence coherence features are developed to improve the detection performance. Classifiers built on these features can reduce the detection error rate to around 22%, a significant improvement over the existing approach-es.
 bring into attention an automated, low-cost process for gen-erating fake reviews, variations of which could be easily em-ployed by evil attackers in reality. To the best of our knowl-edge, we are the first to expose the potential risk of machine-generated deceptive reviews. By doing that, we aim at stim-ulating debate and defense against this deception scheme; (2) Incapable are the state-of-the-art detection algorithms which only deal with disruptive spam and human-written deceptive reviews. We fill the hole by proposing a general framework to detect machine-synthesized fake reviews, and further instantiate this framework with our proposed mea-sures that capture semantic coherence and flow smoothness. 2, we describe an automatic process to synthesize deceptive reviews and show how current deception detectors and hu-man readers perform on synthetic reviews. Based on coher-ence examination, we propose a general framework to detect synthetic reviews in Section 3, followed by instantiations of the framework in Section 4. Section 5 presents our detailed experimental results. Related work is reviewed in Section 6. We conclude this work in Section 7. that is able to generate deceptive reviews automatically from a pool of truthful reviews, followed by performance analysis of human readers and existing detection algorithms. itive reviews by mixing up those existing reviews? Not only it is possible (for the same category of products or services), it works perfectly to fool human readers and detection al-gorithms. We now describe this process shown in Figure 2. [ Review pool ] Take hotel review as an example. We first collect truthful reviews from online websites like TripAdvi-sor with high positive scores and containing more than 150 characters. We discard short reviews since many of them are not content rich. [ Base review ] A base review is randomly drawn from the pool, based on which a synthetic review will be generated. [ Synthesizer ] A synthesizer takes the base review as a template and synthesize a new one using the reviews in the pool. There could be many mechanisms to transform a base review to a synthetic review using different rewriting tech-niques. In our work, we adopt the simplest strategy: The synthesizer replaces each sentence in a base review by the most similar (not exactly the same) sentence in the review pool. A synthetic review is output after a full replacement of sentences in the base review. This strategy is simple, but very effective.
 note the base review set. The review synthesizer works as follows: { s 1 , s 2 , ..., s n } , get a similar sentence in R for each sentence s by tence sequence { s  X  1 , s  X  2 , ..., s  X  n } .
 The sim function could be instantiated by any sentence sim-ilarity measure. Two typical measures are cosine similarity, and set similarity (the number of overlapped words in two sentences). Let s i and s j denote the set of words in two sen-tences, cosine similarity is defined as cos( s i , s j ) = where v i ( v j ) is the word frequency vector for sentence s ( s ), and  X  X  X  X  is the  X  2 norm of a vector. Cosine similarity tends to replace one sentence with a shorter one whereas set similarity tends to replace one sentence with a longer one. To make a synthetic review have the similar length of the base review, we randomly choose either cosine similarity or set similarity as sim when doing sentence replacement. In th is way, one cannot use review length as a criterion to detect fake reviews.
 looking deceptive reviews due to the large variety of sen-tences. Additionally, in practice one might need to do loca-tion/name check in synthesized reviews whereas in this work we put little emphasis on this issue. Our to-be-proposed detection methods will not rely on any location/name in-formation, and besides, human readers are generally able to notice the location/name conflict (if any) alertly. due to two reasons. First, the base review, written by hu-man beings, provides a very good skeleton, which makes the synthetic review as authentic-looking as possible. Sec-ond, in terms of information flow and content richness, on-line reviews that consist of at most several paragraphs are much simpler than research papers, news articles, and novel-s. While automatically synthesizing those complicated texts are much more difficult (though there are successful stories. Some synthetic research papers even passed reviewers X  ex-amination [21]), online review synthesis is relatively easier and hard to detect. reviews, ten volunteers are solicited. Half of them are grad-uate students and half are their family members who read online reviews quite often. All of the volunteers do not have knowledge on how fake reviews are generated and detected; they should be good representatives of general customer-s. The testing dataset contains 10 truthful reviews (from TripAdvisor, we carefully select these reviews) and 10 syn-thesized reviews. We prefer this small set of reviews because the volunteers can easily get fatigued by many reviews and consequently their performance of detecting fake reviews is reduced. Readers are welcome to try the synthetic re-view detection task via www.cs.ucsb.edu/~alex_morales/ reviewspam/ . error rate is defined as the percentage of misclassified reviews over all reviews. The average error rate is around 48%, in-dicating that it is very hard for a reader to distinguish fake reviews from truthful ones. art fake review detectors [19, 15, 10] on detecting the syn-thesized reviews. In this set of tests, we used the same ex-perimental setting as in Section 5. Table 1 shows the result of three algorithms.
 s 59 . 5% accuracy when applied to synthesized reviews. It is evident that relying on abnormal writing styles of fake reviews is no longer effective in dealing with synthesized re-views.
 views, the writing of a synthetic review might be of low qual-ity. We therefore also tested a quality-examination method proposed in [15], which extracts statistical features to mea-sure the quality of product reviews. We regard our truthful (synthetic) reviews as high-quality (low-quality) in their set-ting. This algorithm achieves a 34 . 5% misclassification error rate.
 ing methods in [19] with statistical features extracted from the review text. It achieves a 43 . 3% error rate. man readers, their performance is not impressive. The au-tomated synthesizer, which takes advantage of existing on-line reviews and simple sentence-replacement strategies, is therefore easy to implement but turns out to generate quite authentic-looking and hardly detectable reviews. cating the base review. However, a duplication of an entire review could be detected more easily. In contrast, using sentence-wise replacement, one can generate a much larg-er set of fake reviews, significantly increasing the fake re-view space and detection difficulty. To pass those sentence-level duplication detectors, one could further use automatic rewriting/paraphrasing techniques [17, 8], e.g., synonym re-placement. Generally speaking, the local text content (such as n-grams of a text) is not a unique fingerprint of one specif-ic review. Reviewers might also cite what peers mentioned in previous reviews, or use the same words to praise or crit-icize. Therefore, using text reuse as an indicator of being fake, e.g., employing the search engine to check whether an n-gram of a review is covered in other texts, might cause high false positive rate. Details involving sentence para-phrase and paraphrase detection will deviate too much from the current focus of this work. We stay focused on the simple sentence replacement strategy and resort to feature-based defense techniques. Nonetheless, our to-be-proposed methodology is directly applicable to paraphrased synthetic reviews. ing sentence transplants bear subtle semantic incoherence between sentences. We now advocate a general and exten-sible methodology for coherence analysis, which consists of two components: pairwise sentence coherence and multiple sentence coherence. Figure 4 shows the framework. Each filled circle denotes one sentence in a review. f denotes a genera l measure (feature) that is imposed on either a sen-tence pair or multiple sentences.
 going to measure the coherence of a review. Each measure will be instantiated with greater details in Section 4. Pair-wise sentence coherence aims to measure the information flow smoothness between sentences: human writings, given a word in one sentence, one could expect to observe certain words in its following sentence with some probability.
 (conditional probability), words generally demonstrate co-occurrence patterns (joint probability) in two consecutive sentences.
 transition and co-occurrence properties, pairwise similarity takes into account the word/semantic overlap between two consecutive sentences.
 Multiple sentence coherence measures the stretch and changes of topics in multiple consecutive sentences: resentation of each sentence (e.g., topic distribution), we quantify how dispersed/focused the content of a review is. Let { v 1 , v 2 , ..., v n } be the semantic vector representation cor-responding to each sentence in one review. We define the semantic dispersion as: where centroid = 1 n a vector. Since sentences in a synthetic review are originally from different contexts, we expect SD of synthetic reviews to be generally larger than that of truthful ones. semantic flow, running length takes into consideration the occasional semantic jumps between two adjacent sentences in a review. Given any pairwise sentence similarity mea-sure sim , we compute the similarity between two adjacen-t sentences s i and s i +1 in a review with a sentence se-quence s 1  X  s 2  X  X  X  X  X  X  s n . For a given threshold  X  , once sim ( s i , s i +1 ) is less than  X  , we break the flow edge from s to s i +1 . The original review is hence segmented to pieces. We count the number of sentences in the k th piece, denoted as l k and measure the overall compactness of the review by  X  k =1 l k /K , where K is the total number of pieces we have. We denote this measure as running length (RL). To make the measure robust, one might consider either choosing sim-ilarity functions based on words X  semantics or using multiple sentences as a basic unit s i . the aforementioned concepts in Section 3. w i to word w j as the probability of observing w j by random-ly drawing a word in a sentence given word w i in the previous sentence. The pointwise transition probability matrix (PT-P), with each element ( i, j ), records this probability. Given a set of words W , the transition probabilities from word w to other words form the i th row of PTP as follows: lihood estimation from the training dataset. All the consec-utive sentence pairs ( s, s  X  ) are extracted with word w s . The words in s  X  are observed sample words following w We denote the sentence set formed by s  X  as S i . Let c ( w, S denote the frequency of w in S i .
 two sentences s 1 and s 2 , the probability to observe s 2 s , i.e. s 1  X  s 2 , can be estimated using the following dif-ferent models since s 1 and s 2 have multiple words. Figure 5 shows the pointwise transition probability between two sets of words.
 Best Edge : Choose the edge with the highest transition probability as the transition probability of two sentences. Pivot Node : Choose the node that generates the highest productive transition probability for all the words in s 2 where | X | is the number of words in a sentence. The distri-bution of sentence length P ( | X | ) can be estimated from the training set.
 Pivot Edge : Choose the best set of edges that generate the highest productive transition probability for all the words in s .
 Mixtu re : Use a mixture model to mix all the edges together. where  X  ( w i , s 1 ) = c ( w i , s 1 ) / | s 1 | weighs word w s .
 on perplexity [2]. Perplexity has been used for the evaluation of different language models whereas in our work we apply it as a feature to discriminate truthful reviews from synthetic reviews. Following the convention, the perplexity of review r with a sentence sequence { s 1 , s 2 , . . . , s n } is defined as:
P erplexity ( r ) = exp { X  log( P ( s 1 We use log( P erplexity ( r )) as our PTP coherence measure to make it at the same magnitude with other features proposed later. The measure can be instantiated with the four differ-ent sentence transition models proposed above, respectively denoted as ptp be , ptp n , ptp e and ptp m .
 sumption that human writings demonstrate word transition patterns between two consecutive sentences. Such transi-tion patterns can be impaired by sentence replacements in the review synthesis process, in that the new sentences are originally from different reviews. Consequently, the sentence transition models trained from human written reviews gen-erally cannot well explain the observation of two consecutive sentences in a synthetic review (lower likelihood is observed); therefore, larger PTP values on synthetic review are expect-ed, compared with those on truthful reviews. secutive sentences, we leverage three important probabili-ties: randomly drawing two consecutive sentences, the prob-ability of observing word w i (denoted as P i ), the probability of observing word w j (denoted as P j ), and the probability of observing word w i in one sentence and word w j in the other (denoted as P i,j ). All of them are directly estimated from the training dataset. We define the word co-occurrence score for word w i and w j as: This sco re characterizes the significance of observing two words X  co-occurrence in a consecutive sentence pair, in the sense that it not only takes into account the co-occurrence probability, but also counteracts the effect that one word might occur quite frequently by itself and naturally co-occur with many others. The measure differs from the classic Pointwise Mutual Information (PMI) [5] in that P i,j is not the joint probability of observing word w i and w j in a con-secutive sentence pair, where w i and w j might co-occur in the same sentence.
 co-occur more frequently in long sentences. Therefore, we need to discount the influence of sentence length, either us-ing the average or maximum word co-occurrence score: scores as the sentence co-occurrence score (SCO) for two consecutive sentences. core as the sentence co-occurrence score. For a review r , with a sentence sequence { s 1 , s 2 , ..., s define a coherence measure based on the average of sentence co-occurrence scores, namely, sco ( r ) = 1 n  X  1 where sco ( s i , s i +1 ) can be instantiated by Eqn. 8 or Eqn. 9. We denote the corresponding coherence measures as sco as and sco bs respectively.
 by combining sentences from multiple reviews. We expect abnormal word co-occurrence phenomena in synthetic re-views, i.e., two words observed in a consecutive sentence pair of a synthetic review hardly co-occur in that of truthful reviews. Therefore, the measure SCO tends to give a lower value on synthetic reviews than on truthful reviews. to measure the transition of sentences. It focuses on the similar words or topics that are shared by two consecutive sentences. There are multiple ways to measure similarity of two sentences. Here we briefly introduce three mechanisms: putes the proportion of overlapped words in two sentences [12]: wolap sim ( s 1 , s 2 ) = 2 | s 1  X  s 2 | | s s English words into sets of cognitive synonyms (named synset ). Each word can belong to multiple synsets. A WordNet-based similarity measure can derive a similarity score at the semantic level. For any two sentences s 1 and s , the WordNet-based similarity measure is defined as the synsets that word w i ( w j ) can belong to. sim ( c i the path similarity between two synsets [18]. We denote this measure as wonet sim ( s 1 , s 2 ) (LSI)[4] employs Singular Value Decomposition (SVD) to uncover the latent semantic patterns in the usage of words. Based on our training review dataset, we first formulate a matrix D of size N  X  M , where N is the number of words and M is the total number of sentences. D i,j is the term frequency-inverse document frequency (tf-idf) of word w i sentence s j . Through SVD, we obtain D = U SV T , where U N  X  K reflects the main K patterns of word co-occurrences. For a test sentence s , we compute its projection to the latent LSI space determined by the K principal components: Given a sentence pair ( s 1 , s 2 ), their latent semantic similar-ity is determined by: lsi sim ( s 1 , s 2 ) = cos( L s 1 , L L 1 and L s 2 are obtained according to Eqn. 10.
 r , the coherence score is defined as the average pairwise-sentence similarity over all the consecutive sentence pairs, can be directly instantiated using any pairwise sentence sim-ilarity measure. For the semantic dispersion measure, we can instantiate it based on Latent Semantic Indexing: For each sentence in a review, we first obtain a vector representation v according to Eqn. 10. Then Eqn. 1 is directly applicable to obtain SD. This LSI-instantiated measure is denoted as ning length using pairwise similarity functions discussed in the previous section, do not work very well. It is partially because the synthesis process uses similar sentences as re-placement so that deceptive reviews share similar topic vari-ations with truthful reviews. As a future study, it would be interesting to redefine the concept of running length to ac-commodate measures such as sentence transition and word co-occurrence. our proposed methodology on detecting synthesized reviews. Then we extend the application of our coherence measures to ranking reviews based on their authenticity. various deception detectors, we need to create experimental datasets with ground truth. We collected 12 , 500 reviews of hotels located in New York City. These reviews have five scores and contain more than 150 characters. Based on this collection, 10 datasets are created: For each dataset, we first randomly sample 500 reviews from the collection and treat them as base reviews. The remaining 12 , 000 are put into the review pool. For each of the 500 base reviews, we synthesize one review following the pipeline shown in Figure 2. A single dataset is composed of 500 synthetic reviews and 500 base reviews, and totally 10 such datasets are created. Multiple ways to construct the experimental datasets might exist; we select a most straightforward, but nontrivial one as we will show later.
 SCO, and LSI-related measures, require a dataset to learn parameters in their model. We form an additional dataset with 12 , 000 new reviews. In order to avoid any detection bias, we allow no overlap among this review set and the previously constructed datasets.
 rithms were implemented in Python. All the experiments were performed on a 2.67GHZ, 12GB, Intel PC running Fe-dora 13. tures discussed in Section 4, and transform a review to a feature vector representation. These review vectors are in-put to a classifier to tell if one review is truthful or syn-thetic. In our experiments, three classifiers, SVM with a linear kernel (linearSVM) and a polynomial kernel (polyno-mialSVM), and Naive Bayes (NB) classifier are employed. We use a public data mining software, Weka [9], to run all the classification tasks. All the parameters in the classifiers are set at default except that we choose a quadratic kernel in polynomialSVM.
 validation procedure. Following conventions in supervised learning literature, we use three measures to evaluate the detection performance on each dataset: (1) misclassification error rate (ER), i.e., the percentage of misclassified reviews over all reviews; (2) true positive rate (TPR); (3) false posi-tive rate (FPR). In computation of TPR and FPR, we treat truthful reviews as positive instances. We finally show the average performance over the 10 datasets. measures. Detection results based on our coherence mea-sures are summarized in Table 2. P air. , M ulti. , and Comb. respectively denote pairwise sentence, multiple sentence co-herence measures, and combinations of them. The three classifiers generally achieve consistent performance. Among the four variants of PTP, the pivot edge measure ptp e and mixture measure ptp m achieve much better performance than the other two. For SCO, the average score variant sco as performs comparably to ptp e and ptp m . Potential reason-s for PTP and SCO to work well have been discussed in Section 4.
 of their error rate and omit TPR and FPR due to space constraint. In contrast with PTP and SCO, SIM measures do not perform as well as one might expect. This could be due to the fact that our synthesis algorithm uses similar sentences as replacement, making similarity based measures ineffective. We next provide a formal analysis. Given a review r with a sentence sequence { s 1 , s 2 , . . . , s tence can be viewed as a point in a high dimensional space. Here we use a 2-d space plot (Figure 6) to illustrate these points. The solid line shows the transition of sentences (de-noted as black points) in review r ; the dash line shows the transition of sentences (denoted as white points) in the syn-thesized review r  X  . For each point s i , the synthesis algorithm will find a close point s  X  i in the review pool. In the following theorem, we show that if the distance between s i and s  X  small enough, the average distance between two consecutive sentences in r  X  would not be significantly different from the one in r .
 Theorem 1: Let d be a distance function between two sen-tences, satisfying the triangle inequality. Given two reviews r = ( s 1 , s 2 , . . . , s n ) and r  X  = ( s  X  1 , s  X  2 d ( s i , s  X  i )  X   X  , then | m  X  m  X  | X  2  X  and |  X  2  X  where m ( m  X  ) and  X  2 (  X   X  2 ) are the average and variance of the pairwise sentence distance in r ( r  X  ) respectively. Proof: Let l i = d ( s i , s i +1 ) and l  X  i = d ( s  X  i to the triangle inequality, we have: then the difference between the average pairwise distance in r and r  X  satisfies Sim ilarly, | distance metric, if s i and s  X  i are close to each other, those coherence features measured by the same distance function, might not work well in synthetic review detection, because they tend to be similar on synthetic reviews and truthful reviews. The above analysis can partially explain the poor performance of SIM. As a validation, it is clear that on synthetic reviews the mean and standard deviation of SIM measures in Table 3, are quite close to those on truthful reviews.
 s out to be effective. We verify the difference between the distribution of SD lsi on truthful and synthetic reviews in Tabl e 4: Semantic Dispersion Using SD  X  2 and SD cos . Table 3. Running length (RL) does not work well, partially due to the fact that their instantiated measures are related to similarity measures used to synthesize reviews. SD to that the Euclidean distance measure (i.e.,  X  2 norm) is significantly different from the cosine similarity measures in our review synthesis model. To empirically show this, we conduct one control test: Replace the Euclidean distance in SD with cosine similarity, i.e., replace  X  v i  X  centroid cos( v i , centroid ) in Eqn.1 (denoted as SD cos ), and compare SD cos with the original SD (denoted as SD  X  2 ). The results are shown in Table 4, from which we observe that replac-ing the Euclidean distance with cosine similarity severely undermines the detection accuracy.
 t the most effective variant from each measure category to combine with each other. As shown in Table 2, the combi-nation of PTP and SCO only gives around 3%-5% improve-ment compared with either of them. However, combining pairwise sentence measures with the multiple sentence mea-sure SD significantly decreases the error rate to around 22%. This result is reasonable, in that PTP and SCO measure the pairwise (local) coherence from a similar perspective while SD tries to capture the coherence among multiple sentences
Table 5: Adaptability Study of Different Methods. (globally) from a different perspective: These two kinds of measures complement each other to enhance the detection performance. Compared with the state-of-the-art spam de-tectors, PTP+SCO+SD significantly reduces the error rate by roughly 13%, improves the true positive rate from 0.68 to 0.82 while decreasing the false positive rate from 0.32 to 0.25. We further compare the receiver operating character-istic, or ROC curve, of PTP+SCO+SD (under linearSVM) with that of our competing methods in Figure 7. The area under the curve obtained by our method is much larger than that by other methods, indicating that our method possesses better predictive power of truthful and synthetic reviews. is possible that no training datasets from that forum are available. In such cases, one desideratum is that a decep-tion detector can still perform well although it is trained on a not-so-relevant dataset. Such deception detectors are regarded as adaptable. To test the adaptability of differen-t methods, we employ one of the previous 10 datasets for training and an additional dataset for testing. This addi-tional dataset was constructed in the same manner as before, except using reviews from Washington D.C. instead of New York City. We repeat training-testing for 10 times by chang-ing the training dataset each time. The average performance for each method is shown in Table 5. For PTP+SCO+SD, linearSVM is used for classification since it performs best in previous experiments. In this adaptability study, our method still achieves the best error rate, and significantly improves the true positive rate while depressing the false positive rate. applications, it could also be useful to present users a list of reviews in descending order of authenticity. We refer this problem as review ranking. Here we show how to apply our coherence measures to rank reviews, and evaluate their effectiveness based on the quality of the sorted review list. Let r be the set of features for a review with label y (truthful or synthetic), we quantify the authenticity of each review as the posterior probability P ( y = truthf ul | r ), which is directly obtained from the output of Naive Bayes classifier. P ( y = truthf ul | r ). With a different feature set, one review can be determined authentic to a different degree, therefore resulting in a different rank in the list. In our experiments, review ranking is evaluated by the ratio of truthful reviews in the top K review list, denoted as precision@K. We first calculate precision@K on each dataset and show the aver-aged result over the 10 datasets. To compare our measures with others, we also implement the features proposed in [19, 15, 10] as the feature set and obtain P ( y = truthf ul | based on the output of Naive Bayes classifier. The perfor-mance of these features is shown in Table 6. When increas-ing the length (K) of the review list, our proposed measure PTP+SCO+SD consistently achieves over 0.9 precision, in-dicating that our proposed features also work well for the review ranking task. categories: (1) Review spam detection; (2) Text reuse de-tection; (3) Text quality study.
 Review spam detection. Review spam (or opinion s-pam) tries to mislead readers by composing untruthful views, which has been studied recently in [11, 13, 19, 6]. In [11, 13], researchers focus on detecting disruptive review spam such as reviews with irrelevant texts by utilizing information such as statistical features from review texts, behaviors of review spammers, and relationships among reviewers. Feng et al. [6] detects those reviews as opinion spam which distort the underlying background distribution of opinions. Recent work by Ott et al. [19] studies deceptive reviews written by Amazon Mechanical Turkers based on n-gram and psy-chological deception features. To enhance the deception de-tection performance, Harris et al. [10] further combines the algorithm by Ott et al. [19] with easily-obtained statistical information from the review text.
 matically synthesized reviews. These reviews are extremely hard to detect because sentences in each review were written by people who had true experiences. Neither coarse-grained statistical features such as number of words in a review nor abnormal writing patterns of liars can be employed to tell the difference between truthful and synthetic reviews. Text reuse detection. Text reuse refers to repetitively using parts of the texts in previously created documents. Extensive research studies on text reuse detection have been conducted in the web search context. Examples include the detection of duplicate or near-duplicate documents [20, 3], and phrase-level duplication [7]. More recently, researchers in [1] identify reused and modified local texts on the web such as sentences or passages instead of whole documents. reuse, in that each review is generated by combining sen-tences from other existing reviews. However, as we discussed in Section 2.4, to pass sentence-level text reuse detectors, one could further use paraphrasing techniques [17, 8]. Besides, local text content is not a unique fingerprint of one specific review. It might cause high false positive rate if one utilizes the presence of reused texts as an indicator of a deceptive review.
 Text quality study. In our review synthesis model, fake reviews, formed by sentence replacements, might result in low-quality writing in terms of sentence connection or coher-ence. Research studies such as [15, 16] focus on classifying reviews as helpful (considered as high quality) or unhelp-ful (considered as low quality) based on human judgements. Features employed by those approaches do not particularly account for the coherence of a review X  X  text, which, however, is the key factor to detect synthetic reviews in our setting. Authors of [12, 14] discuss automatic coherence evaluation of a general text using various sentence similarity measures or discourse relations, some of which have been employed to instantiate our framework. Our new coherence measures turn out to outperform both the low quality detection fea-tures and those sentence similarity measures. erful review synthesis technique, which could be employed by evil attackers for large scale spamming. Furthermore, we propose a general framework to defend the review commu-nities against such automatically synthesized reviews. Com-pared with existing deception detectors, the instantiated framework with our new coherence measures can signifi-cantly improve the detection performance by roughly 13%. While our method achieves the initial success, it is still an open research problem to further improve the detection ac-curacy. One meaningful extension is to study the prevalence of synthesized reviews in real review environment. al Science Foundation under grant IIS-0917228. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Govern-ment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstand-ing any copyright notice herein. [1] M. Bendersky and W. Croft. Finding text reuse on the [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] A. Broder. Identifying and filtering near-duplicate [4] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, [5] R. Fano. Transmission of information: A statistical [6] S. Feng, L. Xing, A. Gogar, and Y. Choi.
 [7] D. Fetterly, M. Manasse, and M. Najork. Detecting [8] J. Ganitkevitch, C. Callison-Burch, C. Napoles, and [9] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [10] C. Harris. Detecting deceptive opinion spam using [11] N. Jindal and B. Liu. Opinion spam and analysis. In [12] M. Lapata and R. Barzilay. Automatic evaluation of [13] E.-P. Lim, V.-A. Nguyen, N. Jindal, B. Liu, and [14] Z. Lin, H. Ng, and M. Kan. Automatically evaluating [15] J. Liu, Y. Cao, C. Lin, Y. Huang, and M. Zhou. [16] Y. Liu, X. Huang, A. An, and X. Yu. Modeling and [17] N. Madnani and B. Dorr. Generating phrasal and [18] G. Miller and C. Fellbaum. Wordnet: An electronic [19] M. Ott, Y. Choi, C. Cardie, and J. T. Hancock. [20] B. Stein, M. Koppel, and E. Stamatatos. Plagiarism [21] J. Stribling, M. Krohn, and D. Aguayo. Scigen-an
