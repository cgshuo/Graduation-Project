 Within the emerging mobile IR environment, the focus is over context models including user X  X  interests and environmental data (time, location, near persons, activity, device and networks) [1]. Contextual IR evaluation in this environment aims at measuring the system performance by integrating the user context in the evaluation process [2]. We can classify evaluation methodologies within mo-bile contextual IR, to two main types: ev aluation by context simulations and evaluation by user studies.

The first kind of evaluation simulates users and interactions by means of well defined retrieval scenarios (hypothesis). Contextual simulation frameworks al-low systems to be evaluated, according to a formative view, with less regard for constraints that arise from using sensor technologies, and several social and personal differences of users in interaction with the system. The contextual sim-ulation framework proposed in [3] is based on hypothetic user search context and queries. User context is represented by a set of possible locations and users X  interests are integrated in the evaluation strategy according to a simulation al-gorithm that generates them using hypothetic user interactions for each query. In [4], authors propose a contextual simulation framework based on a set of sim-ulated context descriptors that include lo cation, time and user activities. User X  X  queries are automatically formulated from the context descriptors using different techniques. Context simulation based evaluation method is worthwhile since it is less time consuming and costly than ex periments with real users. However, the method has still areas of uncertainty, for example the choice of assumptions underlying the major scenarios is open to criticism for its lack of realism.
The evaluation by user studies is carried out with real users, called partici-pants, to test the system performance through real user X  X  interactions with the system. To evaluate the performance of contextualized search, each participant is required to issue a certain number of te st queries and determine whether each result is relevant in its context. There are two types of user studies adopted in the domain. The first one [5] is based on the evaluation framework proposed in [6] which makes use of  X  X imulated work task situations X  and where users are assigned a set of predefined tasks to perform in predefined situations. This kind of user studies is criticized because it st ill rely on artificial information needs and may be confounded by inter-subject and order effects. The second kind of contextual evaluation by user studies [7 ] is carried out in realistic use settings. In these latter, users are free to use the system as they would wish to use it and for only as long as they want, submitting their own queries arising from their natural information needs within real and natural situations, rather than asking them to perform some predefined series of tasks. The advantage of user studies based evaluation is that they are conducted with real users and thus the relevance can be explicitly specified by them. The main limitation is that experiments are not repeatable, the extra cost they induce and they may be of little use if the system is not fully developed.
 In the absence of a standard evaluation benchmark for a mobile contextual IR task, we propose in this poster an evaluation framework based on a diary study. Our evaluation framework keeps up the benefits of user study based eval-uation by allowing evaluation with real users and real contexts and alleviates its requirement that the system be fully developed by allowing the evaluation of an early stage development system; moreover we estimate our framework to be easily extensible to include any oth er contextual aspects from the mobile environment (eg. near persons, activity, ...). Our approach is based on a di-ary study where mobile users are asked to log their queries annotated by their search context, here location and time. User X  X  interests are explicitly acquired or implicitly learned based on their expressed relevance judgments for the re-trieved documents for their queries. Two evaluation protocols training/test in chronological order and cross validatio n are experienced within this framework.
This poster is organized as follows: we first present our evaluation framework and introduce our experimental design in Section 2. We then present our ap-proach for mobile search personalization, and its performance evaluation using the proposed evaluation framework in Section 3. Finally, we conclude and give perspectives for future work.
 In our previous work [3] we have proposed an evaluation framework based on context simulation. The contribution of this poster is twofold: first we proposed a new evaluation framework based on a diary study as a tool that enables eval-uation with real users in real contexts, second we compared evaluation results obtained using the two evaluation protocols.

Diary study is a method that has its roots in both psychological and anthro-pological research. In its simplest form, it consists of a representative sample of subjects recording information about their daily lives in situ for a given period. The data captured can then be analyzed in a variety of ways depending upon the nature of the data. Diary studies are presented in an early work by Rieman [8] as a workplace-oriented tool to guide laboratory efforts in the HCI field, they are exploited in [9] to analyze mobile information needs. In our work, we propose to undertake a diary study as a basis for collecting mobile information queries together with their external context namely time and location in situ. The di-ary entries are used as building blocks that compose the evaluation framework datasets. 2.1 Methodology The focus of our framework is the evalu ation of the effectiveness of a context-aware personalization technique for mobile search, in an early stage development. Such techniques involve the consideration of mobile search user X  X  contexts namely interests, location and time in the development and the evaluation processes. The general process we adopted to build our framework is shown in Figure 1. First, a diary study is conducted in situ, were real users are asked to log their queries together with their context when ever and wherever it occurs. The entire resulting diary entries are processed to e xtract user queries a nd contexts. Then, users queries are submitted to a standard search engine via an API. After, the resulting top N search engine documents are crawled, users are asked to judge these documents according to their querie s and contexts. Finally, user X  X  queries and contexts are integrated in the evaluation protocols. The general guidelines for conducting the diary study are: (1) Set the number of participants and the time of the diary study. (2) Assure that all the participants already have experience with using search engines on the web, using a PC or a mobile phone. (3) Set a description of the recording activities your are asking for, namely: recording the date, the time, the location, and the query the user have while he is mobile. (4) To avoid participants forgetting to record entries, send periodic reminders in order to keep participants on track. In what follows, we describe our datasets and evaluation protocols. 2.2 Datasets Contextual Query Set. The diary study entries constitute a set of contextual queries. While many contextual inform ation can be recorded, in this paper we only focused on the spatio-temporal context and users X  X  interests. A contextual t ,and g u i )representsthe i th query (resp. time, location and interests) of the diary entries of a user u . Each contextual query is annotated with a descrip-tion of its associated information needs and a narrative about what would be a relevant document belonging to it. Location ( l u i )andtime( t u i ) information can be expressed as low level data or using semantic concepts depending on the application needs. The user interests ( g u i ) can be manually specified by partic-ipants themselves or automatically learned from the user manual judgments of returned documents for their past queries.
 Ground Truth in Context. The document collection is to be built by col-lecting the top N results retrieved from a publicly available Web search API for each query blind of context. The relevance assessments for the documents are to be collected through an assessment tool (av ailable on line). To do, each user who submitted a query (in the diary study), is asked to judge whether a document from the set of top N retrieved results as response to his query was relevant or not according to his query and its conte xt. Relevance judgments are to be made using a three level relevance scale: relevant, partially relevant, or not relevant. 2.3 Evaluation Protocols In order to evaluate contextualized techniques for mobile search, the set of queries is to be divided into two sets: a training set for learning the parameters of the underlying contextualization technique, and a testing set to evaluate the effectiveness of this technique. Having a set of K contextual queries by user, that contains time information, two evaluation protocols are possible: training/test in chronological order and K-fold cross validation, to be applied on each users X  set of queries. The only recommendation to be observed is to respect a minimum of 25 testings queries [10] in order to make the evaluation process outcomes significant. These two evaluation strategies are described as follows: 1. Training/Test in Chronological Order: this strategy keeps queries in 2. K-fold Cross Validation: this strategy divides the query set into k equally We expect that the two protocols are applicable, and despite the difference be-tween the two protocol strategies and the number of queries they allow to test, the evaluation results are expect ed to be consistent between them. We have deployed our proposed evaluation framework and exploited it to vali-date the performance of our spatio-temporal personalization approach for mobile users [11]. The main objectives of the experimental evaluation are 1) showing the feasibility of our evaluation framework within a real testing scenario, 2) measur-ing the consistency of results using the tw o evaluation protocols. In what follows we first give an overview of our personalization approach, describe the frame-work evaluation in a real diary study and then present a comparative evaluation of the two protocols. 3.1 Our Approach for Personalizing Mobile Search Using a Here we give an overview of our approach for personalizing mobile search de-veloped in our previous work [11]. It will serve as a testing scenario for our evaluation framework. Our personalization technique aims to adapt search re-sults according to user X  X  interes ts in a certain situation. A user U is represented by a set of situations with their corresponding user profiles (interests), denoted: U = { ( S i ,G i ) } ,where S i is a situation and G i its corresponding conceptual graph user profile. A situation S i refers to the geographical and temporal con-text of the user when submitting a query to the search engine. Each situation can be represented by an aggregation of four dimensions:  X  Location type: refers to a class name (such as beach, school, ...) extracted  X  Season: refers to one of the year X  X  seasons,  X  Day of the week: refers either to workday, weekend or holiday,  X  Time of the day: refers to time zone of the day: morning, midday, afternoon, User profiles are built over each identified situation by combining graph-based query profiles. A query profile G s q is built by exploiting clicked documents D s r by the user and returned with respect to the query q s submitted at time s .Firsta keyword query context K s is calculated as the centroid of documents in D s r : K s is matched with each concept c j of the ODP 2 ontology represented by single term vector concepts are propagated over the semantic links as explained in [12]. The user profile G i , within each identified situation S i , is initialized by the profile of the first query submitted by the user at the situation S i . It is updated by combining it with the query profile G  X  of a new query for the same situation as follows: where sw c i ( c j ) is the weight of concept c j in the profile G i and sw c  X  ( c j )is the weight of concept c j in the profile G  X  . A case-based reasoning approach is adopted for selecting the most similar profile G opt to use for personalization ac-cording to a new situation by exploiting a similarity measure between situations as explained in [11]. Personalization is achieved by re-ranking the search results of queries related to the same search situation. The search results are re-ranked by combining for each retrieved document d k , the original score returned by the score f ( d k ) as follows: Where  X  ranges from 0 to 1. The personalized score score c ( d k ,G opt ) is computed using the cosine similarity measure between the result d k and the top ranked concepts of the user profile C opt as follows: Where sw ( c j ) is the similarity weight of the concept c j in the user profile G opt . 3.2 Evaluation Framework Application We conducted a diary study, where users were asked to record the date, the time, the location, and the query they have while they are mobile (out of desk and home). Seven volunteers participated to our study (3 female and 4 male), ages ranged from 21 to 36. The diary study lasted for 4 weeks and it generated 79 di-ary entries, with an average of 11.28 entries per person. Table 1 shows an example of such diary entries, each diary entry rep resents a userid, date, time, place and the user query. From the diary study entries, we obtained a total of 79 queries expressed principally in the French language. Query length varies between 1 and 5, with an average of 2,99. The user intent behind these queries is mostly infor-mational  X  velo hauteur selle  X  or transactional  X  paris hotel cardinal  X . From the diary study entries, we extract location and time information associated with each query. While the location information is already expressed in semantic con-cepts, the time entries are not. Thus, according to our personalization approach, we transformed each date time on a semantic period of the day or the week. We totally obtained 36 different situations, with an average of 5 different situations by user (min=2, max=12) and an average of 3 (min=1, max=8) queries within a same situation. We submit the total queries to Yahoo boss search API 3 ,and crawled the top 50 obtained results for each query. These documents are pre-sented for relevance judgment to our diary study participants via an assessment tool available on line and developed in our lab 4 . The user interests are integrated in the evaluation protocol according to an automatic algorithm that generates them based on the users manual judgments of the documents like described in section 3.1.

This first diary study allows us to verify the feasibility of our evaluation frame-work and its ability to provide as with the desired functionality. In what follows we present our experiment to test result s consistency over the two evaluation protocols. 3.3 Measuring Results Consistency over the Two Evaluation The goal here is to measure results consistency over the two proposed evaluation protocols. For this aim, we applied these latter for evaluating the effectiveness of our personalized approach. We mention here that the two protocols satisfy the minimum of 25 testing queries, and as it can be expected, the k-fold cross validation allows us to test more queries (68 against 29 for the training/test in chronological order protocol). We first study the effect of combining the original document X  X  rank of Yahoo boss (corresponding to the original document score in formula 3) and the personalized docu ment rank obtained according to our approach, on the retrieval effectivene ss. Figure 2 (resp. Figure 3) shows the improvement of our personalized search in terms of P@10,P@20, nDCG@10 and nDCG@20 obtained when using the training/test in chronological order protocol (resp. when using the cross validation protocol) with varying the combination parameter  X  in the interval [0 1]. Results show that the best performance is obtained when  X  is between 0 . 8 and 1 for the two protocols. This is likely due to the fact that all the results on the top 50 match the query well and thus the distinguishing feature is how well they match the user profile.

In a second time, we compare our persona lized retrieval effectiveness to the baseline search using the best  X  value for each protocol. Table 2 shows the improvement of our personalized search in terms of P@10, P@20, nDCG@10 and nDCG@20 over the two protocols. Significant improvement are noted by * in table 2 according to a statistical t-test assuming the significance level fixed at  X  = 5%. Results prove that personalized search achieves higher retrieval precision of almost the queries. Moreover, our approach enhances the initial nDCG@10 and nDCG@20 obtained by the standard search and improve thus the quality of the top search results lists.

When comparing the two protocols resu lts, we can observe that there is some difference in improvement of our approach over the two protocols. To deter-mine whether or not an evaluation protocol might be better than another, we conducted a t-test. More precisely, we stated the null hypothesis (denoted H0) specifying that both evaluation protocols achieved similar performance levels, here evaluated between the means obtained on P@10, P@20, nDCG@10 and nDCG@20 over the common queries. This hypothesis would be rejected at the significance level fixed at  X  =5%.Weobtaineda p -value of 0 . 434 for P@10, 0 . 478 for P@20, 0 . 387 for nDCG@10 and 0 . 365 for nDCG@20, wich are all greater than 0 . 05. We can then accept the null hypothesis and conclude that there is no sig-nificant difference between the two protocols. In this poster we have presented a new evaluation framework for evaluating context-aware personaliz ation techniques for mobile search. It is based on a di-ary study approach. More precisely, we exploit diary study entries to collect mobile queries, an API web search service and real user judgments to construct our ground truth, in context. We have deployed our proposed framework and exploit it for evaluating the search eff ectiveness of our personalized approach comparatively to a standard search. We compared the two evaluation protocols training/test in chronological order and K-fold cross validation and showed the consistency of the obtained results. Our example application illustrates the fea-sibility and usefulness of our proposed evaluation framework. In future, we plan scaling our diary study to include more users and for more long time in order to collect more contextual search situations.
 The authors acknowledge the support of the project QUAERO, directed by OSEO agency, France, and thank all persons who participated in the experiment.
