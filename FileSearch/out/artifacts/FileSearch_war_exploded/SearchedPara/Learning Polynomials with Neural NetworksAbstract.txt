 Microsoft Research Microsoft Research Stanford University Microsoft Research Neural networks have drawn signification attention from the machine learning community, in part due to the re-cent empirical successes (see the surveys Bengio (2009; 2013)). Neural networks have led to state-of-the-art sys-tems for crucial applications such as image recognition, speech recognition, natural language processing, and oth-ers; see, e.g., (Krizhevsky et al., 2012; Goodfellow et al., 2013; Wan et al., 2013). There are several key concepts that have been instrumental in the success of learning the neural networks, including gradient descent, parallel im-plementations, carefully-designed network structure (e.g., convolutional networks), unsupervised pre-training (e.g., auto-encoders, RBMs) (Hinton et al., 2006; Ranzato et al., 2006; Bengio et al., 2007), among others.
 With the flurry of empirical successes and anecdotal evi-dence suggesting (sometimes conflicting) principles in the design and training of neural networks, there seems to be a need for a more solid theoretical understanding of neu-ral networks. Perhaps the most natural starting point for such investigations is the question of when a neural net-work provably learns a given target function. A classical result is that, if the target function is linear, the perceptron algorithm will learn it. This is equivalent to saying that gradient descent on a neural network with a single node (or layer) can successfully learn a linear function.
 But what can we prove about gradient descent on networks with more than one layer? In full generality, this question is nearly hopeless: standard complexity-theoretic results strongly suggest there are no efficient algorithms for learn-ing arbitrary functions (let alone gradient descent), even for target functions representable by very low-depth networks (circuits) (Applebaum et al., 2006). Nevertheless, we may hope to prove concrete results in restricted settings whose structure is reminiscent of the structure present in practical settings.
 In this work, we consider learning bounded degree poly-nomials by neural networks. Polynomials are an expres-sive class of functions since they can be used to approxi-mate many  X  X easonable X  functions, for example, any Lip-shitz function can be well-approximated by a bounded de-gree polynomial (see, e.g., Andoni et al. (2014)). We estab-lish provable guarantees for gradient descent on two-layer neural networks (i.e., networks with one hidden layer of neurons) for learning bounded degree polynomials. Our main result shows this setting can learn any bounded degree polynomial, provided the neural network is sufficiently large. Specifically, suppose the target function f is a degree d polynomial over an n -dimensional variable x  X  C n , with x distributed according to a product distribution (for our main results, the distribution can be Gaussian or uniform distributions over the reals). The two-layer network with m hidden units outputs a function g ( x ) = P m i =1  X  i  X  ( w where  X  i  X  C ,w i  X  C n are network parameters, and  X  is a non-linear activation function (e.g., a sigmoid). It has been known (Barron, 1993) that, when m  X  n d , there ex-ists some choice of parameters w i , X  i so that the network g closely approximates the function f . We show that, in fact, with high probability, even if the bottom layer ( w i  X  X ) is set to be random , there is a choice for top layer (  X  i  X  X ) such that the neural network approximates the target function f . Hence, the perceptron algorithm, when run on only the top layer while keeping the bottom layer fixed, will learn the correct weights  X  i .
 Learning polynomials. Analyzing gradient descent on the entire network turns out to be more challenging as, un-like when the bottom layer is fixed, the error function is no longer convex. We show that even if gradient descent is run on the entire network, i.e. on both top and bottom layers, the neural network will still find the network pa-rameters  X  i and w i , for which the network approximates the target function f . This can be interpreted as saying that the effect of learning the bottom layer does not negatively affect the overall learning of the target function. Indeed, we show that the learning of the top layer (  X  i  X  X ) happens sufficiently fast, in particular before the bottom layer ( w significantly changes. This insight may be useful in analyz-ing gradient descent for the neural networks with multiple layers.
 We further explore the landscape of the gradient descent by showing a conceptually compelling, though incompa-rable result: we show that for sufficiently large networks, in a large region of the parameter space, there are no ro-bust local optima. That is, for any point in the parameter space that satisfies some mild conditions on the weights, with constant probability, a random perturbation will re-duce the error by a non-negligible factor. Hence even when the gradient descent is stuck at a local minimal, a random perturbation would take it out. Because of the conditions on the weights of the neural network, we can not use this theorem to conclude that gradient descent plus random per-turbation will always converge to the global optima from any initial neural network initialization; nevertheless, this provides a rigorous, and conceptually compelling explana-tion for why the existence of local optima do not deter the usage of neural networks in practice.
 We stress that the random perturbation on the weights is complex-valued but the input can still be real valued. In contrast, the same  X  X obustness X  result does not hold when the random perturbation is strictly real; in this case there are robust local minima such that a random perturbation will increase the error. Intuitively, the complex-valued weights give extra dimensions, and hence nicer geometry, to the lo-cal structure of the space of neural networks, which allows it to escape from a local minimum. We find this result in-triguing and hope it can further motivate the construction of new types of neural networks.
 Learning sparse polynomials. It is natural to ask whether, in the case of a target function that is  X  X impler X  (e.g., be-cause it can be represented by a small number of hidden units), the gradient descent can actually learn it more effi-ciently .  X  X ore efficiently X  here can mean both: 1) with a smaller network, 2) at a faster convergence rate. To study this general question, we suggest the following concrete open problem. Define the sparsity of a polynomial as the number of its nonzero monomial terms. We know, thanks to Barron (1993), a k -sparse degree-d polynomial can be represented by a neural network with nk  X  d O ( d ) hidden nodes, but can gradient descent learn such a polynomial by a neural network with only ( nk  X  d d ) O (1) hidden nodes, and in a similar amount of time? We note here we require the target function to be a k -sparse degree-d polynomial, more restrictive than requiring the target to be representable by a small network as in Barron (1994). This might be an easier problem, especially given that recent work has shown that one can indeed learn, for Gaussian or uniform distribution, any sparse polynomial in nk  X  d O ( d ) time (Andoni et al., 2014), albeit via an algo-rithm that does not resemble gradient descent. We should emphasize that it is important to consider well-behaved dis-tribution such as Gaussian or uniform distributions. Oth-erwise, if we allow arbitrary distribution, the sparsity as-sumption may not be helpful. For example, in the boolean case (where x is distributed over the boolean cube), this is at least as hard as  X  X earning parity with noise X  and  X  X earning juntas X   X  problems that we suspect do not admit any effi-cient algorithms and are even used as cryptographic primi-tives (Alekhnovich, 2003; Regev, 2005; Peikert, 2009; Ap-plebaum et al., 2010).
 While we do not have the answer to this natural but chal-lenging question, we give some indications why this may indeed be possible. In particular we show that, if the target function depends only on k n variables, then the neural network will learn a function that also depends on these k variables. Additionally, we provide some strong empirical evidence that such small networks are capable of learning sparse polynomials.
 With the recent success of neural networks in practice, there is an increasing demand of insights and tools for an-alyzing gradient descent for learning neural networks. Our work represents one such effort. We note that Saxe et al. (2013) is another recent work in this space.
 We will only sketch proof ideas in this abstract. The full details can be found in the supplementary material. We will mostly use complex-valued inputs and network pa-rameters, to simplify the description. Most results general-ize to other distributions.
 For a complex number c  X  C , write c as its conjugate, and define its norm as | c | = write P  X  as the adjoint of P , i.e., ( P  X  ) ij = P ji . We denote by C ( r ) the uniform distribution of complex number with norm exactly r , N (  X  2 ) the real Gaussian dis-tribution with mean 0 and variance  X  2 , and U ( r ) the uni-form distribution on [  X  r,r ] . For a distribution P , denote by P n the n -fold product distribution. For any given dis-tribution D and two functions f,g : C n 7 X  C , define the inner product  X  f,g  X  D = E x  X  D [ f ( x ) g ( x )] and the norm k f k D = p  X  f,f  X  D = p E x  X  D | f ( x ) | 2 .
 Polynomials. For any J = ( J 1 ,  X  X  X  ,J n )  X  N n , write a monomial x J = x J 1 1  X  X  X  x J n n . Define | J | = P k J a polynomial p ( x ) = P J b J x J where b J  X  C , its degree deg( p )  X  = max b J 6 =0 | J | . All the degree d polynomials form an n d -dimensional linear space. For any given distribution D , a polynomial basis P 1 ,P 2 ,... is orthonormal w.r.t. D if  X  P i ,P j  X  D = 1 if i = j and is zero otherwise.
 We will use the following basic but important fact that monomials form an orthonormal basis for D = C (1) n . Fact 2.1.  X  w J ,w J 0  X  wise.
 For Gaussian and uniform distributions, the corresponding orthonormal bases are the Hermite and Legendre polyno-mials respectively.
 Neural networks. Assume  X  : C 7 X  C is an analytic func-tion without poles. Write  X  ( z ) = P j  X  0 a j z j . For any w  X  C n , define  X  w : C n 7 X  C as The (two-layer) neural network is defined as a function of the form g ( x ) = P i  X  i  X  w i ( x ) . Here  X  is called the activa- X   X  X  as the  X  X pper layer X  and w i  X  X  as the  X  X ower layer X . In this paper, we will consider activation functions  X  ( z ) = e z and its truncated version  X  d ( z ) = P d with higher than degree d terms truncated (in this case, we call it a truncated neural network ). While a more common  X  is the sigmoid function, the particular form of  X  is not that important as long as it has  X  X ood X  coverage of all the degrees in its Taylor series (see, for example, the discus-sion in Barron (1993)), and for analysis convenience, it is defined everywhere on the complex plane. Our particular choice is just for the sake of a cleaner analysis. Learning. Here learning is defined to construct the rep-resentation of an unknown (or target) function, from some function family F , by given random samples. In this pa-per, we focus on learning low degree polynomials, where the input distribution is drawn from C (1) n . We will com-ment when the results can be extended to the other distri-butions. Learning the neural network via gradient descent proceeds by minimizing the error k f  X  g k 2 D as a function of weights w s and  X  s  X  X  of  X  . To focus on the main ideas, we will assume that there are enough samples such that the empirical estimate of gradient is sufficiently accurate. So we will be working with the model where we do have the access to the gradient. In this section we prove that any polynomial can be repre-sented using a linear combination of a sufficient number of hidden units with weights w i initialized randomly. This is a strengthening of (Barron, 1993), who showed that there exist some weights w i , for which a linear combination of the hidden units yields a given polynomial. In addition, we show that the weights at the top node has small norm. Compared to the representability result in Barron (1993), we require more units, O ( n 2 d ) , rather than O ( n d ) there. Theorem 3.1. [Representation Theorem] Let  X  ( z ) = e z and the distribution D = C (1) n . For m  X  O ( n 2 d / 2 we choose m random w 1 ,w 2 ,...,w m from the distribu-tion C (1 / nomial p of degree d and norm 1 , there exist  X  1 ,..., X  m where P i |  X  i | 2 = O ( n 2 d /m ) such that k P i  X  i p k D  X  .
 The result also holds whenever D is Gaussian distribution N (1) , or the uniform distribution U (1) .
 To prove the theorem, we consider general  X  and D first and then instantiate them by estimating the relevant param-eters. The following is immediate from (2.1) and Fact 2.1. Observation 3.2. For any x  X  C n and any J  X  N n , E In the following, we will show, constructively, that for the given set of units, how to pick the top layer weights to approximate any given p . Define for any polynomial p ( x ) = P J b J x J , r  X  0 , and weight vector w , T p,r P J c J ( r ) b J w tion 3.2, for any x , Hence T p,r ( w )  X  w is centered around p in the functional space. We can apply standard concentration bound to show that the average over m such terms, with w cho-sen independently and m large enough, one can approxi-mate p ( x ) arbitrarily close. More precisely, suppose that w ,...,w m are sampled from C ( r ) n . Let  X  ( x ) denote the error 1 m P m i =1 T p,r ( w )  X  w i ( x )  X  p ( x ) . We obtain,
E Let a ( d ) = min | J | X  d | a J | , and define k p k 1 When w is from C ( r ) n for r  X  1 , we have | T p,r ( w ) | X  X Lemma 3.3. Whenever m  X   X  D ( d ) 2  X  D ( d,r ) / ( a ( d )) with high probability E w 1 ,...,w m k  X  k D  X  for any p where deg( p )  X  d and k p k D = 1 .
 Note that the number of required hidden units is determined by various parameters dependent on  X  and D .
 Now consider  X  ( z ) = e z . Then a ( d )  X  1 /d ! . Suppose that D = C (1) n . Then  X  D ( d,r ) = O ( e 2 r = O (1 / k p k D = P J | b J | 2 , so  X  D ( d ) = O ( parameters into Lemma 3.3, we have m = O ( n 2 d / 2 ) . In addition  X  i = T p,r ( w i ) /m , so X The same bounds can be derived for the distributions such as standard Gaussian distribution in R n and the uniform distribution in [0 , 1] n . This proves Theorem 3.1. In this section we show that gradient descent on a two-layer neural network can learn a polynomial function, given enough hidden units and small enough learning rate. The statement relies on the fact that the weights have been ini-tialized randomly. The formal statement is in Theorem 4.2. First we prove a warm up (simpler) statement: if we run gradient descent only in the upper layer (and leave intact the weights in the lower layer), then the gradient descent will converge to a network approximating the polynomial up to a small error. We use the representation theorem from the previous section. In this simpler statement, we also as-sume that we have access to the exact value of the gradient. Then we show a more general statement, which reaches the same conclusion even if we run the generic gradient de-scent (i.e., on the entire network), assuming that the num-ber of hidden units is sufficiently large. The main intuition behind the result is that the top-layer weights converge to a good state (as in the simplified theorem) before the mod-ifications in the lower layer change enough to affect the overall representability. In particular, one step of a gradient descent will update the weights  X  i of all the hidden units at once. Hence the  X  X otal speed X  of convergence of weights  X  is essentially proportional to the number of hidden units, whereas the speed of change of each w i is not. Once we have enough hidden units, namely n O ( d ) , the speed of con-vergence of  X  i is sufficiently high to overtake the changes to the weight w i of any particular hidden unit.
 Theorem 4.1. Fix some degree-d polynomial f of norm 1, and desired error &gt; 0 . Consider a two-layer neural net-work with m =  X ( n 2 d / 2 ) hidden units. We initialize the  X   X  X  such that k  X  k X  1 (e.g.,  X  = 0 would do), and choose the weights w i randomly from C (1 / gorithm where we run the gradient descent on the weights in the upper layer (keeping weights in lower layer fixed), with access to exact gradient. Then, for a learning rate  X  &lt; 1 /m , the algorithm will converge to a network g such Proof. As in preliminaries, g denotes the function pro-duced by the neural network: g ( x ) = P m i =1  X  i  X  ( w Abusing notation, we will think of all functions in the base of monomials x J . In particular, g in the function space is a sum of m vectors p 1 ...p m , where p i =  X  w i depends on vector w i : g = P m i =1  X  i p i .
 The gradient descent minimizes the quantity E =  X  e,e  X  = e  X  e , for the error function e = f  X  g . We would like to take gradient with respect to  X  i , but E is not analytic with respect to e . Instead we rely on Wirtinger calculus , and consider the following gradient: 1 In one step of the gradient descent the new function g 0 is and the new error function: e = f  X  g 0 = e  X   X  X where I is the identity matrix. Let P be the matrix whose columns are p i  X  X . Then P i p i p  X  i = PP  X  .
 After l iterations of the gradient descent, the error is where e (0) is the starting error function.
 We apply the representation Theorem 3.1 to e (0) to obtain e (0) = x + r , where k r k X  , and x can be expressed as x = P i a i p i , with k a k Note that x = Pa .
 Then we can rewrite the above as: e ( l ) = ( I  X   X PP  X  ) l ( r + Pa ) = ( I  X   X PP  X  ) l r +( I  X   X PP Let X  X  see what happens after one iteration to the second term: ( I  X   X PP  X  ) Pa = Pa  X   X PP  X  Pa = P ( I  X   X P  X  P ) a. Hence, ( I  X   X PP  X  ) l Pa = P ( I  X   X P  X  P ) l a.
 k a As we have that k P k X  Finally, note that we can have at most k a (0) k 2 must have that k Pa ( l ) k  X  , and hence k e ( l ) k  X  k We now continue to our main result, which shows that the gradient descent on the entire network will converge as well, given enough hidden units and small enough learning rate. The proof of the theorem appears in the supplemen-tary material.
 Theorem 4.2 (General gradient descent) . Fix target error &gt; 0 and degree d  X  1 . Suppose the weights  X  are initial-ized to zero and w i  X  X  are random C (1 / number of hidden units is m =  X ( n 6 d / 3 ) and the learn-ing rate is  X   X  1 / 4 m . Then, given a degree-d polynomial f ( x ) of unit norm, the gradient descent will converge to a net, which approximates f up to error . The number of steps required is O n 2 d  X  2 m , and the number of samples required is M = m O (1) . The results of the previous section show that for a suffi-ciently large neural network, with high probability over the random initialization of the weights, the gradient descent will learn a degree d polynomial. In this section, we prove a conceptually compelling, though incomparable result: we show that for sufficiently large networks, in a large region of the parameter space, while there may exist local min-ima, there are no robust local minima . That is, for any point in the specified parameter space, as long as the error is not vanishingly small, with constant probability a ran-dom perturbation will reduce the error by at least 1 /n O ( d ) factor (see Theorem 5.1). Because of the conditions on the parameters, we can not use this theorem to conclude that gradient descent will always converge to the global optima from any initial neural network initialization. Nevertheless, this provides a rigorous explanation for why local optima may not be so damaging for neural networks in practice. Furthermore, this perspective may prove useful for going beyond the results of the previous section, for example for addressing the harder questions posed in the later Section 6. We stress that this result relies crucially on the fact that we allow complex valued weights. In fact we also show such a result is not true when the weights are real-valued. We now state the main result of this section. Define the fourth-norm k  X  k 4 = ( P i |  X  i | 4 ) 1 / 4 .
 Theorem 5.1. There exist constant c 1 ,c 2 ,c 3 ,c 4 &gt; 0 such that for a truncated neural network g = P i  X  i  X  w i where m =  X ( n c 1 d ) , k w i k = O (log n ) , and k  X  k of each w sj drawn from C (1 / by a factor of at least 1 + 1 /n c 4 d , with constant probability. Note that by Theorem 3.1, when m is large enough, m = n  X ( d ) , the conditions in the theorem can all be met. We first sketch the proof idea. Under a given distribution D , for a target function f and a neural network g , con-sider the error function k e k 2 D = k g  X  f k 2 D . For a local perturbation g +  X  g of g , k g +  X  g  X  f k 2 D = k e k 2 2 Re(  X   X  g,e  X  D ) + k  X  g k 2 D . Hence the change in error is  X  k e k 2 D = 2 Re(  X   X  g,e  X  D ) + k  X  g k 2 D . We shall show that, with constant probability, the linear term is negative and overwhelms the quadratic term if the perturbation is suf-ficiently small. The proof consists of two steps. First we consider a single hidden unit. We show that a local perturbation can create non-negligible correlation with any bounded degree polynomial. Secondly we show that, by the anti-concentration inequality 2 , when we perturb many hidden units independently, the aggregated correlation is still large and, when the number of hidden units is large enough, exceeds the quadratic term, which can be bounded by the standard concentration bound.
 Our claim applies when the error function e = g  X  f has bounded degree d , which is the reason the result applies to networks with a truncated activation function  X  d where  X  ( z ) = P 0  X  j  X  d a j z j . For the simplicity of notation, we will simply write it as  X  ( z ) .
 Here it is important that w is complex. The distribution D on x can be over the reals, for example, D can be stan-dard Gaussian distribution N (1) n in R n or the uniform distribution U (1) n over [  X  1 , 1] n . We first show our state-ment for D = C (1) n . Then we extend it to the case when D = N (1) n or U (1) n . We will only sketch the main steps of proofs in this section.
 We give further details of the proof; the missing proofs are in the supplementary material. 5.1. Random perturbation of one hidden unit We first show that for a single hidden unit, a random per-turbation will create large correlation with any bounded degree polynomial. Recall that  X  w ( x ) = P J a J w and a ( d ) = min | J | X  d | a J | . For x  X  C n , we define k x k  X  = max j | x j | . We denote by  X   X   X  w =  X  w +  X   X  the perturbation of a hidden unit  X  w by  X  . We have Theorem 5.2. For any x  X  C n , E  X   X  C ( r ) n [ X   X   X  w ( x )] = 0 . For any  X  such that deg(  X  )  X  d , and k  X  k D  X  1 , we have that for any 0 &lt; r  X  1 and k w k  X   X  rL , Proof sketch. Clearly for any x ,  X   X   X  w ( x ) can be writ-ten as a polynomial in  X  without constant term. By E Write B ( w ) =  X   X  w , X   X  D . As  X  is a polynomial with degree d , so is B ( w ) . By the above, we have We lower bound E  X   X  C ( r ) n [ | B ( w +  X  )  X  B ( w ) | We first show the case when w = 0 . Then we apply the  X  X hifting X  lemma (Lemma 5.4) to complete the proof. Lemma 5.3. For 0  X  r  X  1 , E  X   X  C ( r ) n [ | B (  X  )  X  B (0) | r Lemma 5.4. Suppose that f is a degree d polynomial on n variables. Let v = ( v 1 ,...,v n ) such that k v k  X   X  L . Let f ( x ) = f ( v + x ) . Then k f v k 2  X  n d ( L + 1) 2 d k f k 2 . By the above two lemmas, we can show that E and further transfer this bound to (Re  X   X   X   X  w , X   X  complete the proof.
 We note that the above theorem holds for large range of r and w . But to suppress the second order term, we will only need the theorem in the range where r = O (1 / k w k = O (log n ) . 5.2. Random perturbation of many hidden units Now consider a neural network g ( x ) = P m i =1  X  i  X  w where each k w i k = O (log n ) . Let g 0 ( x ) = P C (1 / k e First consider k g 0  X  g k 2 D = k P m i =1  X  i  X   X  i  X  w can view  X   X   X  w as a vector in the functional space, r = O (1 / bound, with high probability For the linear term Re(  X  g 0  X  g,e  X  D ) , by using Theorem 5.2 and anti-concentration inequality, we can show that when k  X  k 4  X k  X  k /n cd , with constant probability, say 1 / 4 , Combining (5.2,5.3,5.4), we have whenever k  X  k 4  X  n cd k  X  k and k  X  k  X  k e k D /n O ( d ) , we have that k e k e k 2 D  X  X   X  kk e k D /n O ( d ) with constant probability. Hence, we have proved the main theorem. 5.3. Extension to distributions on reals The above proof can also be extended to the case where the x is not complex but chosen from a Gaussian distribution N (1) n in R n or uniform distribution U (1) n on [  X  1 , 1] This follows from the following observation that relates the norm of a polynomial under different distributions. Observation 5.5. Let P ( x ) be a degree d poly-nomial. Then k P k D =  X (1 /d d/ 2 ) k P k C (1) n and O ( d d/ 2 k P k C (1) n ) , where D = N (1) n or U (1) n The above observation implies that if we replace C (1) n by N (1) n or U (1) n , the bound in Theorem 5.2 is only affected by a factor dependent on d only ( d d or 2 d ). Since we as-sume d to be constant, we have: Corollary 5.6. The same statement in Theorem 5.1 holds when D = N (1) n or D = U (1) n . 5.4. Robust local minima for real weights The perturbation theorem 5.1 uses random perturbation in the complex plane to escape a local minimum. It is natural to ask whether real-valued perturbation would be sufficient instead. We show that this is not the case: there are exam-ples, where a real-valued perturbation does not improve the error. This suggest that using complex perturbations may be useful.
 Lemma 5.7. Consider a network with activation function  X  ( z ) = P d l =0 a l z l , where a l  X  0 , on a neural network with one layer of hidden units, with real weights; the input dis-tribution is x  X  U (1) n . There exist a set of network param-eters where the gradient is zero and a random perturbation on the weights { w i } i goes in the direction away from the target function, i.e., k e 0 k D &gt; k e k D with high probability. Proof. We will give a construction of a set of parameters, which are a local minimum for the gradient descent, and a real-valued random perturbation of weights is expected to increase the error. This point is where all hidden units have identical weights w i = 0 and  X  i = 1 /m (for m hidden units), so that P  X  i = 1 .
 We show why local perturbation does not reduce error, ex-cept with a very small probability. Let  X  i be the perturba-tion in weight w i ; for concreteness, suppose each pertur-bation is uniform from [  X   X , X  ] for some  X  1 /m . Let  X  g = g 0  X  g denote the change in the output function of the neural net. We argue that E  X  i [ X  g ] is non-zero. Note that the change  X  g can be written as a polynomial in the change in the weights  X  ij . Fix one particular hidden unit i , and fixed j  X  [ n ] , and consider the term corresponding to second Legendre polynomial in x j , L 2 ( x j ) (remember that the Legendre polynomials are the basis for our input distribution, so we are considering one  X  X oordinate X  in the polynomial basis). We claim that its coefficient is positive: in fact it is a (positive) combination of even powers of  X  for all j 0  X  [ n ] . In particular, say in a term a l (  X  have contribution to L 2 ( x j ) only from terms of the form  X  x J , where vector J is even (any other term has correla-tion 0 with L 2 ( x j ) ).
 We can now choose e = g  X  f so that  X  e,  X  g  X  is positive by choosing f ( x ) = g (0)  X  P j L 2 ( x j ) . Then the change in The error strictly increases with probability 1. It is also clear why the gradient is zero:  X  X   X  X  composed only of a linear in x terms, whereas e = g  X  f is has no constant or linear terms.
 We remark that the above holds even if we perform a small random perturbation on the weights  X  i as well (it suffices that  X  i and perturbed versions remain positive). In this section we study whether smaller neural networks are sufficient for learning sparse polynomials (containing few monomials). As an intermediary step towards this goal, we will also consider the setting where the polynomial only depends on a small subset of the n variables (and hence is also sparse). These questions can be viewed as clean and potentially theoretically tractable special cases of the general question of understanding the relation between the representation complexity and the learning complexity of neural networks for some given class of functions. 6.1. Learning n -sparse polynomials Can a neural network with O ( n ) hidden units learn a quadratic or cubic polynomial that has  X  n monomials? We provide strong empirical evidence (see Fig. 1) suggest-ing that, for the case of n -sparse polynomials over n vari-ables, a neural network with O ( n ) hidden units can learn the function. We train the net using 5 n hidden units while varying n through the values 10 , 20 , 40 , and 80 . The poly-nomial is constructed using randomly chosen n monomials. The plots show that the training error drops significantly af-ter a reasonable number of iterations that depends on n . 6.2. Learning polynomials over few variables As an intermediary step towards the sparse polynomial case, we investigate whether a small neural network suf-fices to learn a sparse polynomial which also depends only on k variables. Here, a simpler goal may be to prove that gradient descent learns the polynomial using only k hidden units instead of n O ( d ) . We are unable to prove this but provide some evidence in this direction. First we show (Lemma 6.1) that assuming x  X  C (1) n , at the termina-tion of the gradient descent, the final function output by the net will depend on the k relevant variables only. We show a similar result for the (more realistic) case where x  X  N (1) n , for a specific transfer function  X  , built using Hermite polynomials. We will use H J ( x ) to denote the Hermite polynomial over x corresponding to a vector J of degrees in the n variables in x . (Some details are deferred to full version.) Lemma 6.1. If x  X  C (1) n and the target function f does not depend on a variable x i , then: whenever the gradient descent converges to a point with zero gradient, the out-put g does not depend on x i . This also holds for the input distribution x  X  N (1) n , provided one uses a special acti-vation function  X  w ( x ) = P J a J H J x J w J .
 Proof. The main idea is that if a variable, say x 1 , is not used in f , then the error e can be written as a sum of two polynomials, one that does not involve x 1 and another that has x 1 in every monomial and these two are orthonormal. Let e = f  X  g = f  X  P s  X  i  X  ( w i  X  x ) . By expanding the polynomial  X  and gathering terms that are dependent on x 1 and others we get e = q ( x 2 ,x 3 ,..,x n ) + h ( x ) where h ( x ) = P i  X  i P J c J w J i x J where each J has non zero degree in x 1 (that is J 1 is non zero) and q does not depend on either x 1 (or its weights w i, 1 ). All we need to show that P error with respect to w i, 1 is Therefore w and hence which must be non-zero as h = P i  X  i P J c J w J i x P This means that P i w i, 1  X   X   X  X  least one of the  X   X   X  X  the gradient descent has not converged yet.
 A similar proof works for the case when x is real and we instead use a modified polynomial  X  where each monomial is replaced by a Hermite polynomial. The proof is based on the orthonormality of these polynomials over N (1) n . Further we point out that neural networks are able to learn polynomials based on their best possible sparsity under any orthonormal transform; i.e., the notion of sparsity is inde-pendent of the chosen coordinate system. This is because the neural net works with dot products of the input point. Observation 6.2. If a neural network can learn a k -sparse polynomial in time T ( k,d,n ) , then it can learn also learn any polynomial that is k -sparse under any orthonormal transform in the same time complexity.
 Proof. This follows from the rotational invariance of the gradient descent process when g is a function of w t i .x over the different hidden units i . That is rotating the coordinate system does not change the gradient descent process. In order to show the gradient descent succeeds with k O ( d ) units, we need to assume a  X  X igh-Rank Condition X  (a vari-ant of the condition in Theorem 3.1) for similar analysis to Section 4 to hold. But we are currently unable to prove the  X  X igh-rank condition X  and leave it as an open question. Alekhnovich, Michael. More on average case vs approxi-mation complexity. In Proceedings of the Symposium on Foundations of Computer Science (FOCS) , 2003.
 Andoni, Alexandr, Panigrahy, Rina, Valiant, Gregory, and Zhang, Li. Learning sparse polynomial functions. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA) , 2014.
 Applebaum, Benny, Ishai, Yuval, and Kushilevitz, Eyal.
Cryptography in nc X 0. SIAM Journal on Computing , 36 (4):845 X 888, 2006.
 Applebaum, Benny, Barak, Boaz, and Wigderson, Avi. Public-key cryptosystem from different assumptions. In
Proceedings of the Symposium on Theory of Computing (STOC) , 2010.
 Barron, Andrew R. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transac-tions on Information Theory , 39(3):930 X 945, 1993. Barron, Andrew R. Approximation and estimation bounds for artificial neural networks. Machine Learning , 14: 115 X 133, 1994.
 Bengio, Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning , 2009.
 Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layer-wise training of deep networks. In NIPS , 2007.
 Bengio, Yoshua. Deep learning of representations: Look-ing forward. arXiv preprint arXiv:1305.0445 , 2013. Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. In ICML , 2013. Hinton, G. E., Osinderoand, S., and Teh, Y. A fast learning algorithm for deep belief nets. Neural Computation , 18: 1527 X 1554, 2006.
 Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In NIPS , 2012.
 Peikert, Chris. Public-key cryptosystems from the worst-case shortest vector problem. In Proceedings of the Sym-posium on Theory of Computing (STOC) , 2009.
 Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. Effi-cient learning of sparse representations with an energy-based model. NIPS , 2006.
 Regev, Oded. On lattices, learning with errors, random linear codes, and cryptography. In STOC  X 05: Pro-ceedings of the thirty-seventh annual ACM symposium on Theory of computing , pp. 84 X 93, New York, NY,
USA, 2005. ACM. ISBN 1-58113-960-8. doi: http: //doi.acm.org/10.1145/1060590.1060603.
 Saxe, Andrew M, McClelland, James L, and Ganguli,
Surya. Dynamics of learning in deep linear neural net-works. NIPS Workshop on Deep Learning , 2013.
 Wan, Li, Zeiler, Matthew, Zhang, Sixin, LeCun, Yann, and
Fergus, Rob. Regularization of neural networks using
