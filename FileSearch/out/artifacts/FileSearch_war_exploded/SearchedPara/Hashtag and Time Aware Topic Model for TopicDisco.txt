 Nowadays, the attention of text mining focuses on the user-generated corpus in social networks instead of traditional long corpus. Especially in Twitter, as one of the most famous global social network sites, it attracts people around world to record their details and share their opinions about products or social events, hence gradually making itself a valuable data source for semantic analysis. The twitter dataset with both text data and document metadata (tags, which can be viewed as features of the corresponding document) are called the semi-structure data [6]. How to characterize the semi-structured corpus data has became a challenge in tweets modeling. In recent years, topic models such as PLSA [3] and LDA are widely used in the learning topic of document corpus. They succssfully achieve mining in normal and long document mining. However, the sparsity problem of short text poses new chanllenge to trditional topic model. Moreover, the informality and personal typos of tweet language make it even harder for traditional topic models to capture semantic information.
 help to leverage the two challenges for traditional topic models mentioned above. In Twitter dataset, in order to highlight their valuable opinions, users always add a hashtag (start with the #) or select more than one existing relevant hashtags find there are mainly three types of usage of hashtags: (1) marking events or topics; (2) defining the types of content, like#ijustsayin, #quote; (3) realizing some specified functions, like #fb means importing the tweet to Facebook in the meanwhile. Meanwhile, each tweets has specific relaease time, which indicates a latent relation of hashtag and tweets.
 typos and extremely similar hashtags in the whole dataset since most of hahstags are added manually by users. Meanwhile, people X  X  attention and behavior also showed a regulation with the development of time and events. To support these points, we count several written styles of #jan25, which is co-related the social events  X  X gypt Revolution X . Then, we show the distribution of #goodnite and #goodmorning over 24 hours.
 Fig. 1: A: Statistics of various written styles on hashtag #Jan25. B: Regulation of # goodnight and # goodmorning distribution over a whole day.
 which indicates that various written styles of one hashtags can connect more dis-tinct tweets. Then Fig.1 B shows #good morning and #goodnight distribution over 24 hours in a whole day. This regulation also applies to the social events evolution. In other words, the same hashtag shares in different tweets, and it has highly overlapping underlying topics. Tweets are usually sent over time and consequently their content may evolve and change over time development. hashtag and temporal information of Twitter to build a reasonable topic model is important. In this paper, we propose a novel method of a Hashtag and Time Aware Topic Model (HTTM). In HTTM, we use the user wisdom hashtags and temporal information to handle tweets problem of sparsity and face challenge in tweet topic modeling. HTTM is a generative topic model that could mine se-mantic information of tweets, hashtags and times. Compared with baselines, we add the hashtag and time aware layer as a generation layer betweet topics and tweets. Moreover, our model makes the hashtag and time and topics with one tweet generated in a multi-way. Additionally in order to leverage the relations in various style hashtags, we build a labled hashtag collection for typos of each hashtags based on the co-occurance of hashtags and tweets.
  X  We propose a novel method to alleviate the informal problem in mining  X  Our model adds a hashtag and time aware layer between tweets and topics,  X  We conduct experiments by using real datasets to prove the effectiveness The rest of the paper is organized as follows: Section 2 introduces related works of this paper; Section 3 describes the proposed novel model in details; Section 4 evaluates the proposed model in real dataset; Finally we conclude the paper in Section 5. To tackle the sparness in tweets, some researches aggregated tweets as a large document. For example, Hong et al. [4] aggregated tweets by the same user, word or hashtags. Yan et al. [13] clustered tweets by a non-negative matrix factoriza-tion before modeling topics. The other was to link short text with additions from long text to enrich short text. Hu et al. [5] modeled tweets by transform-ing them to a semantica structure by adding the term relationship defined in Wikipedia. Beside traditional content mining, to discover the latent semantic structure of document corpus, many topic related models were proposed based on the foundation of LDA. They work well with traditional tasks in plain text mining. However these models fail to model semi-structured dataset and solve the sparness and moise problems in tweets. We survey the specific modification of LDA, which is divided into add layers to involve factors or add attributes of corpus. 2.1 Modifications of LDA by adding layers The Author-Topic model (ATM) [8] was a special method to model text via tags by regard tags as authors. In ATM, each topic is associated with a topic distribution and each topic is associatated with a word distribution. HGTM [10] was a modification of ATM, which treated hashtag as author and used a hashtag graph to leverage the sparisity problem. MGe-LDA [11] was similar to HGTM beside the mutually generation methods of hashtag layer and topic layer of one tweet. However, HGTM and MGe-LDA ignore the real-time characteristic of Twitter. In our paper, we compare with HGTM as a strong baseline for our model. 2.2 Modifications of LDA by Adding Attributes Labeled LDA [7] manually defined that each topic was restricted to be associated with unique one label. Topic over time (TOT) [9] was an LDA-based model to detect the evolution of topic by counting the number of documents related to topic at each timestamp. Both of L-LDA and TOT considered the impact of label and time of one document respectively in LDA. Li et al. [6] proposed TWDA represented the tags prior on the topic distribution over topic. The idea of hashtag weight in our paper is based on the hashtag aware method rather than a determination of priority. MA-LDA [11] made a semi-supervised model by considering time and hashtag attributes into LDA, but MA-LDA focued on the weight of each attribute and ignored the relationship among hashtag, time and tweets. In order to show the outperformance of adding layers, we use the MA-LDA as one of the baseline to prove the effectiveness of our model. 3.1 The Generative Process of HTTM Fig. 2: Graphical model of HTTM. The red box represents the traditional LDA. sequence of document d is W d = { w d 1 ,w d 2 ,...,w d N of document d is H d = { h d 1 ,h d 2 ,...,h d H of words and hashtags in document d , respectively. Each document also has a to represent the tweets corpus. Similar to LDA, each word has a multinomial distribution over K topics,  X  and  X  are hyperparameters. K is the number of latent topics. The generative process of LDA is listed in three steps as illustrated in the red line box of Figure 2. (1)one choose a multinomial topic distribution  X  for document d under the Dirchlet prior with paprameter  X  . (2)For the word in document d , one choose a topic z by a multinomia topic distribution of  X  . (3) The word is generated accroading to word distributions  X  over topic, which is a multinomial under a Dirchlet prior with papramete  X  .
 hashtags, tweets and time information in Twitter, the proposed model HTTM adds a hashtag and time amared layer between topic and document layer and the generative process that tweets and topics can generated each other. Here, differ-ent from LDA, the generation of one tweets includes generating word sequence, hashtag sequence and timestamp belonging to this tweet. Each hashtag in a tweet is associated with a multinomial distribution over topics and each topic is a multinomial distirbution over words. We use x di and z di present the hashtag assignments for the word sequence of tweets. Meanwhile h in the document plate is present the distinct hashtag sequence of tweet d . For each word w di in tweet d , HTTM first choose a hashtag x di based on the probality of p ( h | z h ,t ), where the z h is represent the topic assignment for the word squence of hashtag h, and the topic sassignment for the timestamp t for the word squence of tweet d . That is beacuse intuitively, the choice of hashtag when we pose tweets is related to their topic distributions and other tweets distributions over time. For example, when an event suddenly breaks out, the social media plateforms always bring in many public attention. Moreover, timestamp is associated with each tweets, generated by the rejection and importance sampling. The mixture of per-topic Beta distri-butions over time is the mixture weight as the per-tweets over topics [9]. Thus accroding to the topic assignment of hashtag and the temporal influence give a wise choice of hashtag for one tweets text. Then similar to LDA, one choose a topic z di with respect to the topic distribution of s di and  X  . w di is chosen based on the word distribution  X  . The generative process are listed as follows:  X  for each hashtag h, choose a topic distribution  X  h  X  Dirichlet (  X  );  X  for each topic k, choose a word distribution  X  k  X  Dirichlet (  X  );  X  for each time t,choose a time distribution  X  t  X  Beta ( z di );  X  for each tweet d in the Twitter corpus distribution  X  h , which is the appearance frequence of hashtag h in the whole corpus. [ p ( z h | h ) is the current topic assignment for the word sequence of hashtag h . p ( t | h ) is the the time distribution over the hashtag h , which is co-related to the beta distribution paremeters  X  . The equation (4) shows the sampling details of hashtag h . 3.2 Parameter Estimation In HTTM, the words, hashtags and timestamps of each document are observed while topics are hidden variables guided by the latent distribution parameters. In order to infer the hidden variables, we need to compute the posterior distribution of the hidden variables. We employ Gibbs Sampling to estimate paremeter. The whole corpus X  X  probability is listed as follows: To get the topic distributions of hashtags and mine latent imformation from corpus, we have to infer the latent variables  X  and  X  . We firstly estimate the posterior of z . By intergrating out  X  , we get p ( z | x, X  ): where C TH ih is the number of times that topic i has been assigned to words whose hashtag assignment is hashtag h . Then, we intergrate out  X  , we get p ( w | z, X  ): where C WT wt is the number of times that word w has been assigned to topic t . In our model each topic is associated over time, and for each tweet, the mix-ture distribution over topics is influence by the word and hashtag co-occurence and the tweet timestamps. Accroding to the generative process of HTTM, the posterior probability of x is: by intergrating over  X  . It X  X  logical to consider obtaining a dFistribution over topics, conditioned on a timestamp. It X  X  allow us to see the topic and hashtag occurrence patterns over time to direct the hashtag sampling process. current topic of the word. After interations, S ( t ) 2 k and  X  k,i can be evaluated as follows: After iterative sampling, the final result of  X  and  X  are 4.1 Experiment Setting In this section, we do text mining on the Twitter dataset to prove the effective-ness of HTTM. We use the real word dataset called TREC 2011 1 . It contains tweets in several languages from January 23rd to February 8th in 2011. To obtain a better analysis, we select twitter corpus in English, and we filter noisy data by following the steps of [12]. Meanwhile, we remove the retweets and comments to analysis better. After data preprocessing, there are 292,200 tweets, 90,214 distinct words and 84,331 hashtags prepared for HTTM. The average length of each document is 5.5 and each document has more than one hashtag. All of the dataset is used for training our model to get satisfactory results. eters  X  ,  X  are 0.5 and 0.1. We regard  X  as random number range 0 from 1 to improve the performance to sample hashtag of current tweet. Topic number T is 50 in the step of case study. We run our model 800 times in Gibbs sampling in Python.
 an antomatic metric called conherence score [1]. Then, the H-socore [12] is used to evaluate the quality of topic representation of tweets. Finally, we present the temporal influence of HTTM by comparing the topic discovery over time in details. Baseline models are listed as follows: 1) LDA: the standard LDA, which treats each tweets as a ducument. 2) MA-LDA: the multi-attributed LDA, which regrad hashtag and time as at-3) HGTM: the hahstag graph topic model, which used the hashtag relation and 4.2 Quality of Topics Here, we study the quality of topic discovered by all baselines. After running the HTTM and baselines, we randomly choose there latent topics discovered by the proposed models. For each topics, the top 15 words redered by are listed to represent the topic P ( w | z ). Indeall, a high quality topic should be conherent as much as possible. Table 2 shows the topc words of the topic select by the topic  X  X now X  and  X  X ob X . we find that job are the highest ranking of these four models. However, the LDA discoveries some irrelated words such as  X  X eb X ,  X  X ser X  and  X  X ebsite X , which are more related to the topic of  X  X nternet X . In MA-LDA and LDA, the  X  X eb X ,  X  X ebsite X  and  X  X oogle X  still include. Then the topic  X  X now X  , again we can see that such  X  X eather X  and  X  X ar X  , which belongs to different classifications, are discovered together in LDA, HGTM and MA-LDA. In HTTM, the weather-related words  X  X emperature X  and  X  X ind X  are minning precisely. Besides the two topics, we also find simalar phenomenon in remaining topics, which indicates that topic discovered by the HTTM are preciser and coherent than the three baselines.
 all the performance more prominet analysis, we use an empirically demonstrated that the coherence score is highly correlated with human-judged topic coherence. The method is proposed by Mimno et al [1] for topic quality evaluatio.The score is defined as follows: frequence of word v, D ( v,v 0 ) is the number of document word v and v 0 co-occurred. The score is approprite for measuring frequence words in a topic To evaluate the overall quality of a topic set, we calculated the average coherence score for each baselines. The results are listed in Table 3.
 Table 2: Topic mining performance of topic  X  X ob X ,topic  X  X now X  and topic  X  X gypt X  of four models. Each topic is shown with top-15 words in the range of word-topic probality. Table 3: :Average coherence score on the top T words in topic discovered by LDA, HGTM, MA-LDA, HTTM we set the top-T from 5 to 20 . We find the result is similare to the case study mention above. HTTM receives better coherence scores than other baselines. HGTM outperform LDA and MA-LDA slightly. 4.3 Quality of Tweets Clustering To see topic representation ability for tweets. We observe the performance of tweets clustering. However there is no category information for tweets. Man-ual labeling might be difficult due to the incomplete and informal content of tweets. Luckily, the hashtag has used as keyword by Twitter users. The ma-jor function of hashtag is marking events and topics. In evaluation we man-ually choose 50 frequent hashtags as cluster lables used in [12]. Each doc-ument can be represented by a vector of posterior distribution of topics as d tion methods. Then the following evaluation is based on the fact that these clusters should have low intra-cluster distance and high inter-cluster distance: Then we use cosine similarity to measure the similarity degree of two tweets. So the distance between two tweets is: The average intra-cluster distance and inter cluser distance are: cluster distance, the tweets cluster performance well based on the labeled clus-ters. So we calculate the following ratio to evaluate the quality of one topical representaion of document as[]: Table 4: H score for tweet clustering in four models, smaller value is better. The significant levels(P-value by t-test) are denoted as 0.1*, 0.01**, 0.001***. the HTTM get the best performance (P-value &lt; 0.001) than other baselines. MA-LDA outperforms LDA slightly, means that the time and hashtag as attributes added in LDA is a right way to improve the performance of social media dataset mining. But the sparisity problem is still exist in these two models. HGTM has performance much better than other LDA and MA-LDA, implying that hashtag relation as a layer added in the document and words bring benifits. Here the LDA is worst for the semi-structured data collection. It suggests that it is more useful to model the strengh of hashtag and time aware layer as the bridge to mining the relationship between tweets in topic model. 4.4 Topic Discovery in Tweets In order to shows the temporal information contributes to topic mining perfor-mance, table5 tells examples of 3 topics (out of 50) learned by HTTM. Four HTTM topics, their most likely words, their Beta distributions over time, their actual histograms over time, as well as comparisons against their most similar HGTM topic(by KL divergence), are shown in Fig.3. Immediately see that the HTTM topics are more neatly and narrowly focused in time.
 Fig. 3: Four topics discovered by HTTM and HGTM for the dataset. The titles are our own interpretation of the topics. Histograms show how the topics are distributed over time. The dotted lines indicate the tendency of events over time. the topical trend was occurring and how modeling time can help sharpen and improve the topical word distribution, we list the top-10 words of specific topic. We use one word with the highest probality to denote the topic. The first topic shows the rise and fall of the Egypt Revolution, with a peak in January 25th. The top-10 words are much co-related with  X  X gypt X  or  X  X an25 X  in both HGTM and HTTM, but only the HTTM correctly identifies  X  X ights X , which is a key word for social revolution of public attention . The Second topic shows Super Bowl games was popular with public, with a peak in final game hold in February 6th. The HTTM, mentioned  X  X hampion X , is about final game of global inflencial games. The third topic represent a daily topic  X  X ife X  which is the co-related with  X  X ife X ,  X  X riend X  and  X  X od X . After observation, the leisure time give people more space to relax themselves , focus on some life topic in their tweets on weekends (January 23th, January 30th and February 5th in experiment data collection). Compared with HGTM, the proposed model mining  X  X est X , which is closer to  X  X ife X . For the distribution in Fig.3 we can see that HTTM topics are better localize in time. Table 5: The top-10 words with their probability in three topic crresponding to Fig.3 are shown in table. The HTTM topics are better localized in time and discover more event-specific topical words. In this paper, we proposed a novle model called HTTM to resolve informal and noisy problems in user-generated corpus. Meanwhile, by utilizing the relation of various hashtags X  typoes and the human behavior regulation over time, we add the hashtag and time aware layer in LDA as a generation layer betweet topics and tweets, which makes the hashtag and time of one tweet and its latent topic generated in a multi-way. The experimental results of experiments show that adding hashtag and time aware layer has influenced the performance of topic model. In the future, we will do more in the daynamic hashtag and time sensitive relations of topic model in short text environment.

