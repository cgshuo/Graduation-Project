 that F  X   X  -quantile function, such that F  X   X  -pinball loss L L -risk of f by R function that minimizes the L ified) empirical L X f implies that f far only little justification for using f  X  P  X  -quantile t  X   X  R and a constant c quantile t  X  . Moreover, if Q has a Lebesgue density h bounded away from zero on [ t  X   X   X ,t  X  +  X  ] since we can use c x  X  X , have the same  X  -quantile type  X  .
  X  -quantile of p -average type  X  , if Q tional probability Q distribution having the positive and bounded Lebesgue dens ity h The  X  -quantile function is t  X  ( x ) := f  X  b ( x ) = inf { h Q all y  X  R , and h b ( x ) = min h Q where u because  X  ( x )  X  [  X , 1 / X  ] . Hence b  X  1  X  L ditional probability  X  Q of tion  X  P f  X , Q x = m ( x ) +  X  ( x ) f p -average type  X  .
 P satisfying R R L where is a distribution on X , and B space L we write D measure defined by D with with Then there exists a constant K  X  &gt; 0 we have with probability not less than 1  X  3 e  X   X  that R converges to R  X  into rates for k  X  f  X  for  X   X  (0 , 1) . Using the Lipschitz continuity of L E for all f satisfying R then see that k  X  f situation f  X  has a unique median f  X  have a positive mass concentrated around f  X  SVM using the  X  -insensitive loss approximates f  X  i.e. R 2.5 seem also possible, but are out of the scope of this paper. distribution P and an f : X  X  R the L -risk is then defined by R and, as usual, the Bayes L -risk, is denoted by R  X  by C have and [7, Theorem 3.2] further shows that x 7 X  C  X  with respect to the marginal distribution P M M Q Now assume we have two losses L that our goal is to estimate the excess L for all  X   X  [0 ,  X  ] . In the following we sometimes write  X  if both C  X  the excess risks of L Theorem 3.1 Let P be a distribution on X  X  Y with R  X  Proof: Let us first consider the case R  X   X  (  X  ) for all  X   X  [0 ,  X  ) we see by Jensen X  X  inequality that Moreover, using (8) and (9) we obtain for P form of kk Let us finally deal with the case R exist constants c  X  = R L where the last step is analogous to our considerations for R H  X  older X  X  inequality we then conclude R g : X  X  [0 ,  X  ) we have of C Let us now compute the inner risks of L and R Moreover, using (11) we find t Q([0 ,t ))  X  write i.e. Q Furthermore, note that this definition immediately yields C  X  ing [7] we now define the self-calibration loss of L by where X has a complete  X  -algebra and P( | x )  X  X   X  L to the self-calibration loss by setting C function was defined by  X  second it equals the calibration function of the P -instance  X  L Proof of Theorem 2.5: Let Q be a distribution on R with C  X  of
Q . Then the formulas of Proposition 3.2 show  X  where q To this end we define  X   X  (  X  ) := s p where a and L being the  X  -insensitive loss we have C we further have In particular, if Q [  X   X   X , X  +  X  ] = 0 for some  X  &gt; 0 then C we note that for 0  X  a  X  b  X  X  X  Equation (11) yields Moreover, the definition of L implies Using the symmetry of Q yields  X  R t  X   X  C and hence (18) implies Using we further obtain From this and R t  X   X  The symmetry of Q implies R t  X   X  This and yields By we obtain if Combining this with Hence C C of C C
