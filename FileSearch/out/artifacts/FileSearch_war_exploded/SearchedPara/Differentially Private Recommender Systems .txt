 We consider the problem of producing recommendations from collective user behavior while simultaneously providing guar-antees of privacy for these users. Specifically, we consider the Netflix Prize data set, and its leading algorithms, adapted to the framework of differential privacy .

Unlike prior privacy work concerned with cryptographi-cally securing the computation of recommendations, differ-ential privacy constrains a computation in a way that pre-cludes any inference about the underlying records from its output. Such algorithms necessarily introduce uncertainty X  i.e., noise X  X o computations, trading accuracy for privacy.
We find that several of the leading approaches in the Net-flix Prize competition can be adapted to provide differential privacy, without significantly degrading their accuracy. To adapt these algorithms, we explicitly factor them into two parts, an aggregation/learning phase that can be performed with differential privacy guarantees, and an individual rec-ommendation phase that uses the learned correlations and an individual X  X  data to provide personalized recommenda-tions. The adaptations are non-trivial, and involve both careful analysis of the per-record sensitivity of the algo-rithms to calibrate noise, as well as new post-processing steps to mitigate the impact of this noise.

We measure the empirical trade-off between accuracy and privacy in these adaptations, and find that we can provide non-trivial formal privacy guarantees while still outperform-ing the Cinematch baseline Netflix provides.
 H.2.8 [ Database Management ]: [Data Mining] Algorithms, Security, Theory differential privacy, netflix, recommender systems
A recommender system based on collaborative filtering is a double-edged sword. By aggregating and processing pref-erences of multiple users it ma y provide relevant recommen-dations, boosting a web site X  X  revenue and enhancing user experience. On the flip side, it is a potential source of leak-age of private information shared by the users. The focus of this paper is on design, analysis, and experimental validation of a recommender system with built-in privacy guarantees. We measure accuracy of the system on the Netflix Prize data set, which also drives our choice of algorithms.

The goals of improving accuracy of recommender systems and providing privacy for their users are nicely aligned. They are part of a virtuous cycle where better accuracy and stronger privacy guarantees relieve anxiety associated with sharing one X  X  private information, leading to broader and deeper par-ticipation which in turn improves accuracy and privacy in the same time.

Consider a recommender system that collects, stores, and processes information from its user base. Even if all se-curity measures such as proper access control mechanisms, protected storage, encrypted client-server communications are in place, the system X  X  output visible to any user (i.e., recommendations) is derived in part from other users X  in-put. A curious or malicious user, or a coalition thereof, may attempt to make inferences about someone else X  X  input based on their own and the view exposed through the stan-dard interfaces of the recommender system. The threat is especially ominous in the context of open-access web sites with weak identities and greater potential for on-line active attacks, where the adversary is able to create multiple ac-counts, inject its own input into the recommender system, observe the changes and adapt its behavior, constrained only by the network speed and the system X  X  turnaround time.
There are two common arguments used to deflect privacy concerns presented by recommender systems. We address these arguments in turn.

Non-sensitivity of data. In many instances the infor-mation shared by users is assumed to be non-sensitive and treated as such. We observe that sensitivity of data is con-textual, heavily dependent on the user X  X  circumstances and the attacker X  X  axillary knowledge, and any global policy deci-sions ought to err on the conservative side. Unsophisticated users may not be aware of the amount of personal data made available (and often collected) in the course of routine web surfing, such as the IP address, timing information, HTTP headers, etc., with far-reaching privacy implications. More-over, a correct determination of sensitivity of information is difficult and itself is a moving target. Narayanan and Shmatikov [22] demonstrated a powerful and practical de-anonymization attack linking records in the Netflix Prize data set with public IMDb profiles. Taking their attack one step further, consider a scenario where an individual main-tains two different profiles (say, a professional blog and a pseudonymous page on a social network) with occasional discussions of disjoint sets of movies, all rated on Netflix. It is then possible that they could be linked to the same Net-flix Prize row and thus to each other. Whether the movie ratings attributed to this individual are embarrassing, re-vealing, or at all remarkable is irrelevant, it is the fact that the two personas are linked that can be perceived as deeply invasive and disturbing.

Implicit properties of quality recommender sys-tems. A recommender system that is overly sensitive to one person X  X  input would normally be considered deficient. However, when assessing security of a system, one has to consider not its typical state but rather all feasible states into which it can forced by a determined and resourceful attacker. For instance, in a recommender system based on user-user similarities, the attacker can create a fictitious pro-file (or many profiles) resembling an individual based on the attacker X  X  partial knowledge and induce the recommender system to faithfully report all items highly rated by the in-dividual. Systems that allow submission of entries (such as digg.com or stumbleupon.com) are vulnerable to similar attacks even if they relied exclusively on item-item similari-ties. Although complexity of this attack increases with com-plexity of the recommender system, which is typically kept secret, security through obscurity is a discredited security practice.

In other words, a privacy-preserving recommender system must retain its security properties against any feasible at-tacker (or a well-defined class of attackers) who has access to the system X  X  design and unrestricted auxiliary informa-tion about its targets.
The main contribution of this work is to design and ana-lyze a realistic recommender system built to provide modern privacy guarantees. The task is non-trivial: prior recom-mender systems are not designed with an eye towards pri-vacy, and prior privacy research has focused on more modest algorithms without attempts at practical validation. Rec-ommender systems add the additional complexity that their end-to-end behavior should reflect each users private data, requiring us to crisply separate a privacy-preserving  X  X earn-ing X  phase from a highly non-private X  X rediction X  X hase (con-ducted by the user, in the privacy of their own home).
One natural approach would follow the large volume of work that has occurred on anonymized data publication such as k -anonymity [23], where, as Netflix has done, data is re-leased with an attempt to remove sensitive information and overly specific combinations of attributes. In addition to the uncertain privacy guarantees [18], Brickell and Shmatikov [8] find that these techniques applied to high dimensional data cause irreparable damage for data mining algorithms. In-stead, we integrate the privacy protection into the computa-tion itself, ensuring that the learned recommendations pre-serve privacy using the framework of differential privacy.
Our findings are that privacy does not need to come at substantial expense in accuracy. For the approaches we consider, privacy-preservin g algorithms can be parameter-ized to essentially match the recommendation performance of their non-private analogues. While there is some special-ized analysis required, the methodology itself is relatively straight forward and accessible. As an additional contribu-tion of this note, we hope to demonstrate the integration of modern privacy technology to practical and realistic learning systems.
We base our choice of algorithms on leading solutions to the Netflix Prize [4, 5, 6]. We adapt algorithms exemplifying two approaches that emerged as main components of Netflix Prize contenders: factor models and neighborhood models.
Several papers have recently introduced and studied the application of differential privacy to problems in learning and data mining, surveyed by Dwork [14]. Algorithms such as k -means clustering, perceptron classification, association rule mining, decision tree induction, and low rank approxi-mation are all shown to have differentially private analogues, with theoretical bounds on their accuracy [7]. While we borrow substantially from these works for our underlying privacy technology, our focus is more on building and eval-uating a full end-to-end recommender system, rather than isolated components.

The wholesale release of data with anonymized user iden-tities by Neflix has been shown to have far-reaching privacy implications [22], establishing, in particular, that most rows can be identified with near certainty based on as few as a dozen partially known data points. Although a commercial recommender system is unlikely to willingly disclose all or substantial fraction of its underlying data, a recent work by Calandrino et al. [9] demonstrates that passive observations of Amazon.com X  X  recommendations are sufficient to make valid inferences about individuals X  purchase histories.
The focus of prior work on cryptographic solutions to the problem of secure recommender systems is on removing the single trusted party having access to everyone X  X  data [10, 11, 2, 3]. It does not attempt to limit amount of information leaked through the system X  X  recommendations in the course of its normal execution. Our solution can be combined with the modular approach of the Alambic framework [2].
It is important to distinguish our approach, of privacy preserving computation, from much prior work on privacy studying the release of anonymized records. One could imag-ine building a recommender system, or any machine learn-ing technology, on top of anonymized data, drawing privacy properties from the anonymization rather than reproducing them itself. However, especially for rich, high-dimensional data, most anonymization techniques appear to cripple the utility of the data [8, 1]. By integrating the privacy guaran-tees into the application, we can provide it with unfettered access to the raw data, under the condition that its ulti-mate output X  X ubstantially less information that an entire data set X  X espect the privacy criteria.
We start with an introduction to some of the approaches applied to the Netflix prize. The approaches we consider were actual contenders at one point, but are understandably simpler than the current state of the art. While their level of accuracy has since been surpassed, we hope that by under-standing their private adaptations we can derive methodol-ogy that may continue to apply to the progressively more complex recommender systems.

The setting we consider has both users and items, with ratings for a subset of the (user, item) pairs. Given such a partial set of ratings, the goal is to predict certain held out values at specified (user, item) locations.

Global Effects. A common first step in these systems is to center the ratings by computing and subtracting average ratings for users and for items. To stabilize this computa-tion, the average is often computed including an additional number of fictitious ratings at the global average; users and movies with many ratings drift to their correct average, but averages with small support are not allowed to overfit.
Covariance. Having factored first order effects, derived from properties of the ratings themselves, it is very common to look at correlations between items. 1 A common approach is to look at the covariance matrix of the items, whose ( i, j ) entry is the average product of ratings for items i and j across all users. Of course, relatively few users have actually rated both i and j , and so the average is taken across only those users who have rated both items.

Geometric Recommendation Algorithms. Oversim-plifying tremendously, to a first approximation once we have computed the covariance matrix of the items we have enough information at hand to apply a large number of advanced learning and prediction algorithms. The covariance matrix encodes the complete geometric description of the items, and any geometric algorithm (eg: latent factor analysis, near-est neighbor approaches, geometric clustering, etc) can be deployed at this point. Importantly, from our perspective, these approaches can be applied for each user using only the covariance information and the user X  X  collection of ratings. If the covariance measurement can be conducted privately, any algorithm that does not need to return to the raw data of other users can be deployed at this point with privacy guar-antees. We will experiment with several, borrowing almost entirely from previous research published about the Netflix prize, but defer the discussion of the specific algorithms for now.
We will formalize the previous sketch into an algorithm that is non-private, but will form the skeleton of our privacy preserving approach. The steps in the algorithm may appear especially pedantic, but writing them in a simplistic form will allow us to adapt them easily to their private forms.
Following [4] we use r to refer to a collection of ratings, with the notation r ui for the rating of user u for movie i and r u for the vector of ratings for user u . Weusethenotation e ui and e u for the binary elements and vectors indicating the presence of ratings (allowing us to distinguish from reported zero values).

We start by subtracting the movie averages from each movie, where the average is dampened by a number  X  of ratings with the global average.
 1. For each item i , compute totals and counts:
While user-user correlations may also be useful, they have proven less successful in the Netflix competition (and would be much more challenging to accomodate privately). 2. Compute global average G = 3. For each item i , compute the stabilized average: 4. For each rating r ui , subtract the appropriate average:
We perform exactly the same operation for the users, com-puting stabilized averages and subtracting the appropriate averages from each rating.

Our covariance computation is also direct, but for reasons that will become clear we will want to take a weighted com-bination of the contributions from each user, using a weight 0  X  w u  X  1foruser u : 1. For each movie-movie pair ( i, j )
The matrix Avg now contains our estimate for the co-variance matrix. We could then pass this matrix to one of a number of geometric approaches proposed by other re-searchers as being especially effective on the Netflix data set. While the choice of subsequent algorithm is obviously very important for the performance of the recommender system, we will not attempt to derive any privacy properties from their specifics. Rather, we providing them only with inputs that have been produced using differential privacy.
Differential privacy [13], surveyed in [15], is a relatively recent privacy definition based on the principle that the out-put of a computation should not allow inference about any record X  X  presence in or absence from the computation X  X  in-put. Formally, it requires that for any outcome of a random-ized computation, that outcome should be nearly equally likely with and without any one record.
 We say two data sets A and B are adjacent , written A  X  B , if there is exactly one record in one but not in the other.
Definition 1. A randomized computation M satisfies -differential privacy if for any adjacent data sets A and B , and any subset S of possible outcomes Range ( M ) ,
One interpretation of the guarantee differential privacy provides is that it bounds the ab ility to infer from any output event S , whether the input to the computation was A or B . From an arbitrary prior p ( A )and p ( B ), we see that When A  X  B , differential privacy bounds the update to the prior by a factor of exp( ), limiting the degree of infer-ence possible about slight differences in the input data sets. Specifically, inference about the presence or absence (and consequently the value of) any single record is bounded by afactorofexp( ).

We stress that differential privacy is a property of the com-putation that produces the output, not of the output itself. At the same time, the probabilities are purely a function of the randomness of the computation, and not of possible randomness or uncertainty in the input.

There is a large volume of literature on privacy, and many other definitions and approaches have been suggested that provide other guarantees. Many of these, such as the pop-ular k -anonymity, only provide syntactic guarantees on the outputs, without the semantic implications used above. Un-like the majority of these other approaches, differential pri-vacy has proven resilient to attack, continuing to provide privacy guarantees for arbitrary prior knowledge, under re-peated use, for arbitrary data types.

Several approaches have looked at weakened versions of differential privacy, exchanging the generality of the guar-antee (protecting perhaps against only a subset of priors) for improved accuracy or usability [21]. Nonetheless, the techniques we outline here can provide the stronger guar-antee, and it is not clear that the weakened definitions are needed (although, we will consider one of the relaxations next).
We will also consider a relaxed form of differential privacy that permits an additive term in the bound, as well as the multiplicative term, introduced in [16].

Definition 2. A randomized computation M satisfies ( ,  X  ) -differential privacy if for any adjacent data sets A and B , and any subset S of possible outcomes Range ( M ) ,
One interpretation of this guarantee is that the outcomes of the computation M are unlikely to provide much more information than for -differential privacy, but it is possible. For any  X &gt; ,take S  X  to be the set of outcomes x for which Combining this constraint with the definition of ( ,  X  )-differential privacy, we can conclude that such outputs are unlikely: While  X  much larger than is possible, the probability is effectively bounded by  X  . Moreover, for the privacy mech-anisms we use (described in the next section), there will always be a trade-off between and  X  ;onecandecreaseei-ther arbitrarily, at the expense of increasing the other. In a sense, the amount of information released (measured as the ratio of the two probabilities) is a random variable which is most likely to be small.

Importantly, approximate differential privacy satisfies se-quential composition logic:
Theorem 1. If M f and M g satisfy ( f , X  f ) and ( g , X  g differential privacy, respectively, then their sequential com-position satisfies ( f + g , X  f +  X  g ) -differential privacy.
We will use this theorem to be able to derive bounds on the end-to-end privacy guarantees of our recommender sys-tem, comprised of multiple independent ( ,  X  )-differentially private computations.
The simplest approach to differential privacy when com-puting numerical measurements is to apply random noise to the measurement, and argue that this masks the possible influence of a single record on the outcome. If we aim to compute a function f : D n  X  R d , the following results de-scribe prior privacy results achieved through the addition of noise [17].

Theorem 2. Define M ( X ) to be f ( X )+ Laplace (0 , X  ) d M provides -differential privacy whenever
We can achieve an approximate differential privacy guar-antee using Gaussian noise, proportional to the smaller  X  distance between f ( A )and f ( B ). Writing N (  X ,  X  2 )forthe normal distribution with mean  X  and variance  X  2 .Itis proven in [16] that
Theorem 3. Define M ( X ) to be f ( X )+ N (0 , X  2 ) d . M provides ( ,  X  ) -differential privacy, whenever
Notice that for any one parameter  X  to the noise distribu-tion, there are many valid settings for and  X  . Specifically, for any positive there is a  X  =  X  ( ) associated with it. As such, we will often focus only on the  X  value, without deriving specific ( ,  X  )pairs.
There are relatively few statistics we will need to measure from the data to begin adapting recommendation algorithms from previous work. Global effects, such as per-movie aver-ages and per-user averages, play an important role in predic-tion. Additionally, the movie-movie covariance matrix forms the basis of many geometric algorithms, and specifically the SVD factorization approaches and the kNN geometric dis-tance approaches. Before continuing to the specifics of our approach, we see how these quantities can be measured in the previously described frameworks.

Counting and sums are relatively easy functions to ana-lyze. If f : D n  X  R d partitions the records (ratings) into d bins and counts the contents of each, for both  X  1 and  X  2 Consequently, we can report counts of arbitrary partitions of the records (our interest is in ratings per movie) with appropriate additive noise providing privacy.

Sums are more complicated only in that we must explicitly constrain the range of values each element contributes. In the case of ratings, the scores initially range from 1 to 5, but this will grow and shrink as we apply various operations to the data. If a single record has maximum range at most B , then for both  X  1 and  X  2
The most complex measurement we will take is the movie-movie covariance matrix. Simplifying a bit (specifically, ig-noring weights for now), which we can the covariance matrix write as (using r u for the rating vector for person u ) This formulation makes it very clear that a single change to a single record can have a limited influence on the sum. If a single rating changes, changing from r a u to r b u , the difference between the two covariance matrices is Taking r a u  X  r b u to be one (corresponding to a single change), This bound may be large for users with many ratings, which is what leads us to introduce weights to the terms contribut-ing to the covariance matrix. The weights will be selected to carefully normalize the contributions of each user, ensuring that the norms of the possible differences are at most a fixed constant. So normalized, we can simply compute and report the covariance matrix, with a fixed amount of additive noise applied to each of its entries. As we expect the magnitude of the values in the covariance matrix to grow linearly with the number of data points, this influence of this noise should intuitively diminish as the amount of training data grows.
Our algorithm consists of several steps, measuring (with noise) progressively more challenging aspects of the data be-fore feeding the measurements to appropriately parameter-ized variants of the currently top learning algorithms. We first describe the approach at a high level, before describing the sequence of precise calculations more concretely. Global effects. We start with the noisy measurement of and baseline correction for various global effects. We first measure and publish the sum and count across all ratings to derive a global average. We then measure and publish, for each movie, the number and sum of ratings for that movie. We use these two quantities to produce a per-movie aver-age, stabilized by including a number  X  m of ratings at the global average. Finally, we remeasure the global average, as above, for upcoming use in centering each user X  X  ratings. The algorithms and privacy implications for these steps are described in detail in Section 4.2.

We next invest some effort in preparing each user X  X  rat-ings for covariance measurement. We do not want to release per-user statistics, such as the average rating for each user, as to do so with sufficient accuracy to be useful for learn-ing would demolish our privacy guarantees. Instead, we will apply several transformations to a user X  X  ratings before mea-surement, and argue that the transformations are such that privacy guarantees made of their outputs propagate to their inputs. Our specific operations include the centering of each user X  X  ratings, again including a number of fictitious ratings at the global average, as well as a clamping of the resulting value to a more compact interval (increasing privacy, at the expense of error in outlying values).The algorithms and pri-vacy implications for these steps are described in detail in Section 4.3.
 Covariance matrix. We next measure the covariance ma-trix of the resulting user rating vectors. To achieve privacy, we incorporate noise into each coordinate, following [7].
As an example of the subtle nature of effectively inte-grating privacy, consider the computation of latent factors from geometric data. An important step in many geometric learning approaches, we might like to find a low dimensional subspace that best fits the data, when projected onto it, in terms of the mean squared error. There are several ways to compute such a space, but three otherwise equivalent ap-proaches are to compute the SVD of the user  X  movie data matrix, compute the SVD of the movie  X  movie covariance matrix, and compute the SVD of the user  X  user Gram ma-trix.

While these three approaches are equivalent in non-private computation, they are very different when faced with the task of incorporating privacy. Consider the simple tech-nique of adding noise to measurements to provide privacy: To mask the data matrix sufficiently, we must add noise to every entry in the matrix, in a process known as ran-domized response. While the independence of the noise leads to some amount of cancelation, the error in the sys-tem still grows with the number of participants. Adding noise to the covariance matrix scales with the number of movies involved, but does not need to grow as the num-ber of participants increases, which gives the potential for arbitrarily accurate measurements for arbitrarily large pop-ulations. Working with the Gram matrix, with an entry for each pair of users, is a disaster; one must add enough noise to each entry (quadratic in the participants) proportional to the largest covariance any two users might have, linear in the number of movies. This quickly becomes unmanageably disruptive.

While the three techniques are similar without privacy constraints, there is a clear ordering on them when we need to introduce noise for privacy (covariance, data matrix, Gram matrix).
As before, r ui stands for the rating of user u for movie i , r for the entire vector of ratings for user u , and similarly e and e u denote the binary elements and vectors indicating the presence of ratings (allowing us to distinguish from reported zero values). We use c u = e u 1 for the number of ratings by user u .

In our exposition, we will distinguish between private data and released data by using lower case and upper case, respec-tively. The reader should verify that whenever an upper case variable is assigned, it is a function only of upper case vari-ables or lower case variables with noise added. When we add noise to a variable x ,wesimplywrite where the distribution of the noise is yet unspecified. We then bound the amount by which the variable x could change under  X  1 and  X  2 with addition of a single rating to the data set, allowing for the addition of either Laplace or Gaussian noise, and providing privacy guarantees through Theorems 2 and 3 respectively.
We start with a few global effects that are easy to mea-sure and publish accurately without incurring substantial privacy cost. We first measure and publish the number of ratings present for each movie, and the sum or ratings for each movie, with random noise added for privacy: We use these to derive a global average, G = GSum / GCnt. Next, we sum and count the number of ratings for each movie, using d dimensional vector sums.
 We produce a stabilized per-movie average rating by intro-ducing  X  m fictitious ratings at value G for each movie:
With these averages now released, they can be incorpo-rated arbitrarily into subsequent computation with no ad-ditional privacy cost. In particular, we can subtract the corresponding averages from the every rating to remove the per-movie global effects.
Having published the average rating for each movie, we will subtract these averages from each rating before contin-uing. We then center the ratings for each user, taking an average again with a number  X  p of fictitious ratings at the recomputed global average: Unlike with movies, we do not report the averages, we will just subtract them from the appropriate ratings. We also clamp the resulting centered ratings to the interval [  X  B, B ], to lower the sensitivity of the measurements at the expense of the relatively few remaining large entries: We now argue that the presence or absence of a single rating has a limited effect on this centering and clamping process.
Theorem 4. Let r a and r b differ on one rating, present in r b .Let  X  be the maximum possible difference in ratings For centered and clamped ratings b r a and b r b ,wehave
Proof. If r a and r b are two sets of ratings with a single except for the ratings of user u . For the ratings in common between r a and r b , the difference is at most the difference in the subtracted averages: For the Netflix Prize data set  X  =4.
 For the new rating, r ui , its previous contribution of zero is replaced with the new centered and clamped rating, at most B in magnitude. Therefore For  X  2 2 ,since c b u = c a u + 1 the first term is maximized at c =  X  p + 1 and can be bounded from above by  X  2 / 4  X  p . By choosing  X  p sufficiently large we can drive the  X  2 dif-ference arbitrarily close to B . The same is not true of  X  and we just cancel its c a u term with the denominator.
The final measurement we make of the private data is the covariance of the centered and clamped user ratings vectors. However, we will want to take the average non-uniformly, using per-user weights w u equal to the reciprocal of e u The choice of norm will determine the norm in which we derive a stability bound.

Notice that if a single rating r ui is in difference between r a and r b , only the terms contributed by user u contribute to a difference in the matrices. We now bound the norms of this difference using Theorem 4.

Theorem 5. Let ratings r a and r b have one rating in difference. Taking w u =1 / e u 1 we have For  X  p at least  X  2 / 4 B 2 ,taking w u =1 / e u 2 we have
Proof. We rewrite the difference w a w For  X  1 and  X  2 ,as e b u  X  e a u  X  1, we have that The norm of the original matrix difference is bounded by As b r i  X  b e i  X  B for any norm, the normalizations cancel the norms of the ratings, giving the claim via Theorem 4.
A similar result holds for the weight matrix, but can be optimized substantially as the e i vectors do not undergo centering.

Theorem 6. Let ratings r a and r b have one rating in difference. Taking w u =1 / e u 1 we have Taking w u =1 / e u 2 we have
Proof. Between the two weight matrices, ( c a change from w a p to w b p ,and2 c b p  X  1 entries emerge with weight w .For  X  1 , this bound is For  X  2 and c a p &gt; 0, we observe that the difference in weights w a p  X  w b p is at most the derivative of x  X  1 / 1 / 2( c a p ) 3 / 2 , which implies The case of c a p = 0 is handled by direct computation.
The mathematics we have done so far describe the amount of noise required to mask the presence or absence of a single rating. A stronger privacy guarantee would mask the pres-ence or absence of an entire user, providing uniform privacy guarantees even for prolific movie raters.

To update the mathematics to provide per-user privacy we only need to apply a more aggressive down-weighting by the number of ratings, scaling each contribution down by e u . For the contribution to sums and counts, a new user contributes exactly their weighted rating and count vectors:
Likewise, the contribution to the covariance and weight matrices is exactly the new outer product of weighted vec-tors, whose norms are the square of the norms of the vectors:
This normalization is much more aggressive than with per-rating privacy, and results in less accurate prediction and recommendation. However, it is still the case that the amount of noise remains fixed even as the number of users increases.
The covariance matrix we have computed is somewhat noisy, and while we could hand it of to one of many recom-mendation algorithms, we will first clean it up a bit.
As a first step we apply the  X  X hrinking to the average X  method [4] with separate constants for the diagonal and off-diagonal entries, setting the matrix to
There is substantial theoretical and empirical evidence that low rank matrix approximations (of the same form as the matrix factorization approaches) are highly effective at removing noise from matrices while retaining the significant linear structure. By computing a rank-k approximation to our covariance matrix, we can remove a substantial amount of  X  X quared error X  that we have introduced, without remov-ing nearly as much from the underlying signal.

Before applying the rank-k approximation, we would like to unify, to the extent possible, the variances of the noise. Covariance entries with relatively fewer contributed terms have higher variance in their added noise (as it was divided by a smaller Wgt ij ). It has also been observed that the de-noising of low rank approximations is most effective when the variances of the entries are equivalent (the error bounds, to a first approximation, scale with the maximum per-entry variance, and it causes no harm to scale up lesser entries while increasing the amount of  X  X ignal X  each contributes).
To correct this, we borrow a te chnique from [12] and scale each entry Avg ij upwards by a factor of (MCnt i MCnt j ) before applying the rank-k approximation. We then scale each entry down by the same factor, and return the results to our recommendation algorithm of choice.

One additional benefit of this step is that it produces a highly compressed representat ion of the covariance matrix, which can now be sent in its entirety to the client computers.
We evaluate our approach on the Netflix Prize data set that consists of roughly 100M ratings of 17770 movies con-tributed by 480K people. By adjusting the parameters of the noise distributions we use, our computation will pro-vide varying differential privacy guarantees, and its output will have measurable accuracy properties. The accuracy is measured by the root mean squared error (RMSE) on the qualifying set (3M ratings) and can be self-tested on the probe set with similar characteristics (1.5M ratings).
While it is natural to parameterize differential privacy us-ing a variety of ( ,  X  ) pairs, we simplify to a single parameter. For each measurement f i , we will parameterize the magni-tude of the noise we use as where the  X  i are required to sum to a pre-specified value  X  . In fact, we will take each  X  i to be a fixed fraction of  X  ,whose value we will take and vary as our single parameter.
By Theorem 2, using Laplace noise, measurement i pro-vides i -differential privacy for By Theorem 3, using Gaussian noise, measurement i pro-vides ( i , X  i )-differential privacy for As Theorem 1 tells us that and  X  values add, our final guarantees have the form (for  X  1 and  X  2 respectively) By taking a common value of  X  i ,wecanseethat  X  = scales the value of linearly. By varying  X  , and thus the  X  , we can add more or less noise to our measurements and provide more or less privacy, respectively. From any  X  ,we can reconstruct a range of ( ,  X  )pairs.

Section 4 describes privacy-preserving computations of global effects and the covariance matrix. There are three impor-tant measurements our algorithm makes of the data: the global average, the per-movie averages, and the covariance matrix. For any  X  , we will set the respective  X  i according to We choose  X  1 so small because every rating contributes to its computation, and even with substantial additive noise the resulting average is very accurate.

Given the covariance matrix and with global effects fac-tored out, we apply the k -Nearest Neighbor (kNN) method of Bell and Koren [4] and the standard SVD-based prediction mechanism (SVD), both with ridge regression. We use the weight matrix Wgt as the similarity metric for kNN. Both methods can be preceded by the cleaning step (Section 4.6), which may improve or degrade performance depending on the value of the privacy parameter.

All global parameters are optimized for the value of  X  = 0 . 15 where both SVD and kNN preceded by the cleaning step match Netflix X  X  Cinematch benchmark. The dimensionality for all algorithms is fixed at k = 20, the shrinking parameters  X  m =15and  X  p = 20, the clamping parameter B =1 . 0.
The parameters for the cleaning step as well as for the kNN-and SVD-based recommendation mechanisms are trained for each data set and the privacy parameter separately, since it can be done with relatively few new measurements that depend on private data. We use the gradient descent method which repeatedly evaluates each mechanism with varying pa-rameters. This fitting does not require use to re-measure the covariance matrix, and so we do not incur additional privacy cost there. However, we must evaluate the RMSE, which can be measured with excellent precision even with very low setting of the privacy parameter.

Our main findings are presented in Figure 1. As the value of  X  , which is inversely proportional to  X  and the qualitative amount of privacy, increases, so does accuracy of the rec-ommendation algorithms. Both k-NN and SVD (both with cleansing) cross the Cinematch threshold at  X   X  0 . 15. With-out the cleansing, both k-NN and SVD do pass the baseline, but with less noise and consequently privacy. While the post-processing  X  X leansing X  of the covariance matrix helps substantially in the high noise (small  X  ) regime, it impairs the analysis when less noise is used. This is perhaps a conse-quence of optimizing the cleansing parameters for  X  =0 . 15, and it is possible that a more delicate post processing could accommodate both regimes.

Corresponding graphs for the Laplace noise and user-level privacy appear in the full version of the paper.
Our algorithms (like most differentially-private computa-tions) introduce a fixed amount of error to any measurement, that is increasingly dominated by actual data records as the size of the data sets increase. With more and more users and ratings, we expect the additive error we introduce for any fixed value of  X  to eventually vanish.

To explore how the loss due to the privacy-preserving property of the recommender mechanism decreases with the amount of available data (for the fixed value of  X  =0 . 15), we simulated the data gathering process at different times between 2000 and 2006 (including the peculiar property of including users with fewer than 20 ratings). Consistently with the Netflix Prize data set, the probe set was the 9 most recent ratings for each user chosen with probability 1/3 each. Fig. 2 plots the difference in RMSE (as percent-age points) between privacy-preserving k-NN (after scaling) and the same algorithm without privacy guards.  X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X  Figure 2: Left scale X  X ccuracy loss, right scale X  X he number of records. The x -axisisthenumberofdays elapsed since 7/1/2000.
We conclude that a recommendation system with differen-tial privacy guarantees is feasible without taking significant hit in the recommendations accuracy. The loss in accuracy (for a fixed value of the privacy parameter) decreases as more data becomes available.

In our experiments we fixed several parameters that had the potential to vary freely, and it is natural to expect that more in-depth experimentation could lead to noticeably im-proved prediction accuracy. The chosen dimensionalities, smoothing weights, and distribution of  X  X ccuracy X   X  i tween the measurements could be adjusted and possibly im-proved.

Directions for future work include efficient methods for direct privacy-preserving computations of latent factors and incorporation in the differential privacy framework of ad-vanced methods for collaborative filtering that do not im-mediately admit factorization into two phases such as the integrated model of [19]. [1] C. C. Aggarwal. On k-anonymity and the curse of [2] E. A  X   X meur, G. Brassard, J. M. Fernandez, and [3] E. A  X   X meur, G. Brassard, J. M. Fernandez, F. S. M. [4] R. M. Bell and Y. Koren. Scalable collaborative [5] R. M. Bell, Y. Koren, and C. Volinsky. The BellKor [6] R. M. Bell, Y. Koren, and C. Volinsky. The BellKor [7] A. Blum, C. Dwork, F. McSherry, and K. Nissim. [8] J. Brickell and V. Shmatikov. The cost of privacy: [9] J. Calandrino, A. Narayanan, E. Felten, and [10] J. F. Canny. Collaborative filtering with privacy. In [11] J. F. Canny. Collaborative filtering with privacy via [12] A. Dasgupta, J. E. Hopcroft, and F. McSherry. [13] C. Dwork. Differential privacy. Invited talk. In [14] C. Dwork. An ad omnia approach to defining and [15] C. Dwork. Differential privacy: A survey of results. In [16] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, [17] C. Dwork, F. McSherry, K. Nissim, and A. Smith. [18] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. [19] Y. Koren. Factorization meets the neighborhood: a [20] Y. Li, B. Liu, and S. Sarawagi, editors. Proceedings of [21] A. Machanavajjhala, D. Kifer, J. M. Abowd, [22] A. Narayanan and V. Shmatikov. Robust [23] L. Sweeney. k-anonymity: A model for protecting
