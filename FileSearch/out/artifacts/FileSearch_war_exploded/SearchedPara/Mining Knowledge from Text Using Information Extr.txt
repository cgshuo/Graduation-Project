 An important approach to text mining involves the use of natural-language information extraction. Information ex-traction (IE) distills structured data or knowledge from un-structured text by identifying references to named entities as well as stated relationships between such entities. IE systems can be used to directly extricate abstract knowl-edge from a text corpus, or to extract concrete data from a set of documents which can then be further analyzed with traditional data-mining techniques to discover more general patterns. We discuss methods and implemented systems for both of these approaches and summarize results on min-ing real text corpora of biomedical abstracts, job announce-ments, and product descriptions. We also discuss challenges that arise when employing current information extraction technology to discover knowledge in text. Most data-mining research assumes that the information to be  X  X ined X  is already in the form of a relational database. Unfortunately, for many applications, available electronic information is in the form of unstructured natural-language documents rather than structured databases. Consequently, the problem of text mining , i.e. discovering useful knowledge from unstructured text, is becoming an increasingly impor-tant aspect of KDD.
 Much of the work in text mining does not exploit any form of natural-language processing (NLP), treating documents as an unordered  X  X ag of words X  as is typical in information retrieval. The standard a vector space model of text repre-sents a document as a sparse vector that specifies a weighted frequency for each of the large number of distinct words or tokens that appear in a corpus [2]. Such a simplified repre-sentation of text has been shown to be quite effective for a number of standard tasks such as document retrieval, clas-sification, and clustering [2; 16; 66; 60].
 However, most of the knowledge that might be mined from text cannot be discovered using a simple bag-of-words rep-resentation. The entities referenced in a document and the properties and relationships asserted about and between these entities cannot be determined using a standard vector-space representation. Although full natural-language understand-ing is still far from the capabilities of current technology, existing methods in information extraction (IE) are, with
Production of nitric oxide ( NO ) in endothelial cells is regulated by direct interactions of endothelial nitric oxide synthase ( eNOS ) with effector proteins such as Ca2+ --calmodulin .

Here we have ... identified a novel 34 kDa protein , termed NOSIP ( eNOS interaction protein ) , which avidly binds to the carboxyl terminal region of the eNOS oxygenase domain .
 reasonable accuracy, able to recognize several types of enti-ties in text and identify some relationships that are asserted between them [14; 25; 53].
 Therefore, IE can serve an important technology for text mining. If the knowledge to be discovered is expressed di-rectly in the documents to be mined, then IE alone can serve as an effective approach to text mining. However, if the doc-uments contain concrete data in unstructured form rather than abstract knowledge, it may be useful to first use IE to transform the unstructured data in the document corpus into a structured database, and then use traditional data-mining tools to identify abstract patterns in this extracted data.
 In this article, we review these two approaches to text min-ing with information extraction, using one of our own re-search projects to illustrate each approach. First, we intro-duce the basics of information extraction. Next, we discuss using IE to directly extract knowledge from text. Finally, we discuss discovering knowledge by mining data that is first extracted from unstructured or semi-structured text. Information Extraction (IE) concerns locating specific pieces of data in natural-language documents, thereby extracting structured information from unstructured text. One type of IE, named entity recognition , involves identifying refer-ences to particular kinds of objects such as names of people, companies, and locations [4]. In this paper, we consider the task of identifying names of human proteins in abstracts of biomedical journal articles [10]. Figure 1 shows part of a sample abstract in which the protein names are underlined. In addition to recognizing entities, an important problem is extracting specific types of relations between entities. For example, in newspaper text, one can identify that an orga-nization is located in a particular city or that a person is Sample Job Posting: Job Title: Senior DBMS Consultant Location: Dallas,TX Responsibilities: DBMS Applications consultant works with project teams to define DBMS based solutions that support the enterprise deployment of Electronic Commerce, Sales Force Automa-tion, and Customer Service applications.
 Desired Requirements: 3-5 years exp. developing Oracle or SQL Server apps using Visual Basic, C/C++, Powerbuilder, Progress, or similar. Recent experience related to installing and configuring Oracle or SQL Server in both dev. and deployment environments.
 Desired Skills: Understanding of UNIX or NT, scripting language. Know principles of structured software engineering and project management Filled Job Template: title: Senior DBMS Consultant state: TX city: Dallas country: US language: Powerbuilder, Progress, C, C++, Visual Basic platform: UNIX, NT application: SQL Server, Oracle area: Electronic Commerce, Customer Service required years of experience: 3 desired years of experience: 5 affiliated with a specific organization [73; 24]. In biomedical text, one can identify that a protein interacts with another protein or that a protein is located in a particular part of the cell [10; 23]. For example, identifying protein interactions in the abstract excerpt in Figure 1 would require extracting the relation: interacts ( NOSIP , eNOS ).
 IE can also be used to extract fillers for a predetermined set of slots (roles) in a particular template (frame) relevant to the domain. In this paper, we consider the task of extract-ing a database from postings to the USENET newsgroup, austin.jobs [12]. Figure 2 shows a sample message from the newsgroup and the filled computer-science job template where several slots may have multiple fillers. For exam-ple, slots such as languages, platforms, applications, and areas usually have more than one filler, while slots related to the job X  X  title or location usually have only one filler. Similar applications include extracting relevant sets of pre-defined slots from university colloquium announcements [29] or apartment rental ads [67].
 Another application of IE is extracting structured data from unstructured or semi-structured web pages. When applied to semi-structured HTML, typically generated from an un-derlying database by a program on a web server, an IE sys-tem is typically called a wrapper [37], and the process is sometimes referred to as screen scraping . A typical applica-tion is extracting data on commercial items from web stores for a comparison shopping agent ( shopbot ) [27] such as MySi-mon ( www.mysimon.com ) or Froogle ( froogle.google.com ). For example, a wrapper may extract the title, author, ISBN number, publisher, and price of book from an Amazon web page.
 IE systems can also be used to extract data or knowledge from less-structured web sites by using both the HTML text in their pages as well as the structure of the hyperlinks be-tween their pages. For example, the WebKB project at Carnegie Mellon University has explored extracting struc-tured information from university computer-science depart-ments [22]. The overall WebKB system attempted to iden-tify all faculty, students, courses, and research projects in a department as well as relations between these entities such as: instructor(prof, course), advisor(student, prof), and member(person, project). There are a variety of approaches to constructing IE sys-tems. One approach is to manually develop information-extraction rules by encoding patterns (e.g. regular expres-sions) that reliably identify the desired entities or relations. For example, the Suiseki system [8] extracts information on interacting proteins from biomedical text using manually de-veloped patterns.
 However, due to the variety of forms and contexts in which the desired information can appear, manually developing patterns is very difficult and tedious and rarely results in robust systems. Consequently, supervised machine-learning methods trained on human annotated corpora has become the most successful approach to developing robust IE sys-tems [14]. A variety of learning methods have been applied to IE.
 One approach is to automatically learn pattern-based ex-traction rules for identifying each type of entity or relation. For example, our previously developed system, Rapier [12; 13], learns extraction rules consisting of three parts: 1) a pre-filler pattern that matches the text immediately pre-ceding the phrase to be extracted, 2) a filler pattern that matches the phrase to be extracted, and 3) a post-filler pat-tern that matches the text immediately following the filler. Patterns are expressed in an enhanced regular-expression language, similar to that used in Perl [72]; and a bottom-up relational rule learner is used to induce rules from a corpus of labeled training examples. In Wrapper Induction [37] and Boosted Wrapper Induction (BWI) [30], regular-expression-type patterns are learned for identifying the beginning and ending of extracted phrases. Inductive Logic Programming (ILP) [45] has also been used to learn logical rules for iden-tifying phrases to be extracted from a document [29]. An alternative general approach to IE is to treat it as a se-quence labeling task in which each word (token) in the docu-ment is assigned a label (tag) from a fixed set of alternatives. For example, for each slot, X , to be extracted, we include a token label BeginX to mark the beginning of a filler for X and InsideX to mark other tokens in a filler for X . Finally, we include the label Other for tokens that are not included in the filler of any slot. Given a sequence labeled with these tags, it is easy to extract the desired fillers. One approach to the resulting sequence labeling problem is to use a statistical sequence model such as a Hidden Markov Model (HMM) [57] or a Conditional Random Field (CFR) [38]. Several earlier IE systems used generative HMM mod-els [4; 31]; however, discriminately-trained CRF models have recently been shown to have an advantage over HMM X  X  [54; 65]. In both cases, the model parameters are learned from
Figure 3: Sample Extraction Rule Learned by Rapier a supervised training corpus and then an efficient dynamic programming method based on the Viterbi algorithm [71] is used to determine the most probable tagging of a complete test document.
 Another approach to the sequence labeling problem for IE is to use a standard feature-based inductive classifier to pre-dict the label of each token based on both the token itself and its surrounding context. Typically, the context is rep-resented by a set of features that include the one or two tokens on either side of the target token as well as the la-bels of the one or two preceding tokens (which will already have been classified when labeling a sequence from left to right). Using this general approach, IE systems have been developed that use many different trained classifiers such as decision trees [3], boosting [15], memory-based learning (MBL) [43], support-vector machines (SVMs) [40], maxi-mum entropy (MaxEnt) [17], transformation-based learning (TBL)[68] and many others [64].
 Many IE systems simply treat text as a sequence of un-interpreted tokens; however, many others use a variety of other NLP tools or knowledge bases. For example, a number of systems preprocess the text with a part-of-speech (POS) tagger (e.g. [18; 9]) and use words X  POS (e.g. noun, verb, adjective) as an extra feature that can be used in hand-written patterns [8], learned extraction rules [13], or induced classifiers [64]. Several IE systems use phrase chunkers (e.g. [59]) to identify potential phrases to extract [64; 73]. Others use complete syntactic parsers (e.g. [21]), particularly those which try to extract relations between entities by examin-ing the synactic relationship between the phrases describ-ing the relevant entities [24; 61]. Some use lexical semantic databases, such as WordNet [28], which provide word classes that can be used to define more general extraction patterns [13].
 As a sample extraction pattern, Figure 3 shows a rule learned by Rapier [13] for extracting the transaction amount from a newswire concerning a corporate acquisition. This rule extracts the value  X  X ndisclosed X  from phrases such as  X  X old to the bank for an undisclosed amount X  or  X  X aid Honeywell an undisclosed price X . The pre-filler pattern matches a noun or proper noun (indicated by the POS tags  X  X n X  and  X  X n X , respectively) followed by at most two other unconstrained words. The filler pattern matches the word  X  X ndisclosed X  only when its POS tag is  X  X djective. X  The post-filler pat-tern matches any word in WordNet X  X  semantic class named  X  X rice X . If the information extracted from a corpus of documents represents abstract knowledge rather than concrete data, IE itself can be considered a form of  X  X iscovering X  knowl-edge from text. For example, an incredible wealth of bio-logical knowledge is stored in published articles in scientific journals. Summaries of more than 11 million such articles and processing this knowledge is very difficult due to the 1 Available on the web at lack of formal structure in the natural-language narrative in these documents. Automatically extracting information from biomedical text holds the promise of easily consoli-dating large amounts of biological knowledge in computer-accessible form. IE systems could potentially gather infor-mation on global gene relationships, gene functions, protein interactions, gene-disease relationships, and other important information on biological processes. Consequently, a grow-ing number of recent projects have focused on developing IE systems for biomedical literature [33; 23; 7; 55; 32]. We recently compared many of the IE methods introduced in section 2.2 on the task of extracting human protein names [10; 58]. Specifically, we evaluated Rapier , BWI, MBL, TBL, SVM, MaxEnt, HMM, and CRF by running 10-fold cross-validation on set of approximately 1,000 manually-annotated Medline abstracts that discuss human proteins. Overall, we found that CRF X  X  gave us the best result on this particular problem. However, although CRF X  X  capture the dependence between the labels of adjacent words, it does not adequately capture long-distance dependencies between potential extractions in different parts of a document. For example, in our protein-tagging task, repeated references to the same protein are common. If the context surrounding one occurrence of a phrase is very indicative of it being a protein, then this should also influence the tagging of an-other occurrence of the same phrase in a different context which is not typical of protein references. Therefore, we recently developed a new IE method based on Relational Markov Networks (RMN X  X ) [69] that captures dependencies between distinct candidate extractions in a document [11]. Experimental evaluation confirmed that this approach in-creases accuracy of human-protein recognition compared to a traditional CRF.
 We have also evaluated several approaches to extracting pro-tein interactions from text in which protein names have al-ready been identified [10]. Blaschke et al. [7; 8] manually de-veloped patterns for extracting interacting proteins, where a pattern is a sequence of words (or POS tags) and two protein-name tokens. Between every two adjacent words is a number indicating the maximum number of arbitrary words that can be skipped at this position. Therefore, we developed a new IE learning method, ELCS [10], that au-tomatically induces such patterns using a bottom-up rule learning algorithm that computes generalizations based on longest common subsequences [35]. Below is a sample pat-tern that it learned for extracting protein interactions: where PROT matches a previously tagged protein name. The induced patterns were able to identify interactions more pre-cisely than the human-written ones.
 Another approach we have taken to identifying protein inter-actions is based on co-citation [39]. This approach does not try to find specific assertions of interaction in the text, but rather exploits the idea that if many different abstracts ref-erence both protein A and protein B, then A and B are likely to interact. Particularly, if two proteins are co-cited signifi-cantly more often than one would expect if they were cited independently at random, then it is likely that they interact. In order to account for the case where the co-citing abstracts http://www.ncbi.nlm.nih.gov/entrez/ do not actually concern protein interactions but cite multi-ple proteins for other reasons, we also used a Bayesian  X  X ag of words X  text classifier trained to discriminate between ab-stracts that discuss protein interactions from those that do not [39]. In order to find interactions, protein pairs that are highly co-cited were filtered for those which are specifically co-cited in abstracts that the Bayesian text-classifier assigns high-probability of discussing protein interactions. Using these techniques, we recently completed the initial phase of a large-scale project to mine a comprehensive set of human protein interactions from the biomedical litera-ture. By mining 753,459 human-related abstracts from Med-line with a combination of a CRF-based protein tagger, co-citation analysis, and automatic text classification, we ex-tracted a set of 6,580 interactions between 3,737 proteins. By utilizing information in existing protein databases, this automatically extracted data was found to have an accu-racy comparable to manually developed data sets. Based on comparisons to these existing protein databases, the co-citation plus text-classification approach was found to be more effective at identifying interactions than our IE ap-proach based on ELCS. We are currently developing an im-proved IE system for identifying direct assertions of inter-actions in biomedical text using an SVM that exploits a relational string kernel similar to that utilized by Zelenko et al. [73].
 By consolidating our text-mined knowledge with existing manually-constructed biological databases, we have assem-bled a large, fairly comprehensive, database of known human protein interactions containing 31,609 interactions amongst 7,748 proteins. More details on our database of protein in-teractions have been published in the biological literature ing automated text mining has helped build an important knowledge base of human proteins that has been recognized as a contribution worthy of publication in Genome Biology and will hopefully become a valuable resource to biologists. If extracted information is specific data rather than abstract knowledge, an alternative approach to text mining is to first use IE to obtain structured data from unstructured text and then use traditional KDD tools to discover knowledge from this extracted data. Using this approach, we developed a text-mining system called DiscoTEX (Discovery from Text EXtraction) [48; 49] which has been applied to mine job postings and resum  X es posted to USENET newsgroups as well as Amazon book-description pages spidered from the web.
 In DiscoTEX , IE plays the important role of preprocess-ing a corpus of text documents into a structured database suitable for mining. DiscoTEX uses two learning systems to build extractors, Rapier [12] and BWI [30]. By training on a corpus of documents annotated with their filled tem-plates, these systems acquire pattern-matching rules that can be used to extract data from novel documents. After constructing an IE system that extracts the desired set of slots for a given application, a database can be con-structed from a corpus of texts by applying the extractor to each document to create a collection of structured records. Standard KDD techniques can then be applied to the result-http://bioinformatics.icmb.utexas.edu/idserve ing database to discover interesting relationships. Specifi-cally, DiscoTEX induces rules for predicting each piece of information in each database field given all other informa-tion in a record. In order to discover prediction rules, we treat each slot-value pair in the extracted database as a dis-tinct binary feature, such as  X  X raphics  X  area  X , and learn rules for predicting each feature from all other features. We have applied C4.5rules [56] to discover interesting rules from the resulting binary data. Discovered knowledge de-scribing the relationships between slot values is written in the form of production rules. If there is a tendency for  X  X eb X  to appear in the area slot when  X  X irector X  appears in the applications slot, this is represented by the produc-tion rule,  X  X irector  X  application  X  Web  X  area  X . Sam-ple rules that C4.5rules mined from a database of 600 jobs that Rapier extracted from the USENET newsgroup austin.jobs are shown in Figure 4. The last rule illustrates the discovery of an interesting concept which could be called  X  X he IBM shop; X  i.e. companies that require knowledge of an IBM operating system and DBMS, but not a competing DBMS, also require knowledge of Lotus Notes, another IBM product.
 We also applied Ripper [20] and Apriori [1] to discover interesting rules from extracted data. Sample rules mined from a database of 600 resum  X es extracted from the USENET newsgroup misc.jobs.resumes by BWI are shown in Fig-ure 5. The first two rules were discovered by Ripper while the last two were found by Apriori . Since any IE or KDD module can be plugged into the DiscoTEX system, we also used an information extractor (wrapper) manually devel-oped for a book recommending system [44] to find interest-ing patterns in a corpus of book descriptions. Sample asso-ciation rules mined from a collection of 1,500 science fiction book descriptions from the online Amazon.com bookstore are shown in Figure 6.
 Ghani et al. [34], have developed a system similar to Disco-TEX and used it to mine data about companies extracted from the web. One of the patterns they discovered was  X  X d-vertising companies are usually located in New York. X  Unfortunately, the accuracy of current IE systems is limited,
Figure 6: Sample rules mined from book descriptions. and therefore an automatically extracted database will in-evitably contain a fair number of errors. An important ques-tion is whether the knowledge discovered from a  X  X oisy X  ex-tracted database is significantly less reliable than knowledge discovered from a  X  X lean X  manually-constructed database. We have conducted experiments on job postings showing that rules discovered from an automatically extracted data-base are very close in accuracy to those discovered from a corresponding manually-constructed database [49]. These results demonstrate that mining extracted data is a reliable approach to discovering accurate knowledge from unstruc-tured text.
 Another potential problem with mining extracted data is that the heterogeneity of extracted text frequently prevents traditional data-mining algorithms from discovering useful knowledge. The strings extracted to fill specific data fields can vary substantially across documents even though they refer to the same real-world entity. For example, the Mi-crosoft operating system may be referred to as  X  X indows, X   X  X icrosoft Windows, X   X  X S Windows, X  etc.. We developed two approaches to addressing this problem [47].
 One approach is to first  X  X lean X  the data by identifying all of the extracted strings that refer to the same entity and then replacing sets of equivalent strings with canonical entity names. Traditional KDD tools can then be used to mine the resulting  X  X lean X  data. Identifying textually distinct items that refer to the same entity is an instance of the general database  X  X eduping X  or record-linkage problem [42; 5]. We have developed methods for learning string-similarity measures and classifiers that identify equivalent items by training on a sample of manually  X  X eduped X  data [6]. Another approach to handling heterogeneity is to mine  X  X oft matching X  rules directly from the  X  X irty X  data extracted from text. In this approach, the rule induction process is generalized to allow partial matching of data strings in order to discover important regularities in variable textual data. We developed two novel methods for mining soft-matching rules. First, is an algorithm called TextRISE 3 that learns rules whose conditions are partially matched to data using a similarity metric [50]. We also developed SoftApriori , a generalization of the standard Apriori algorithm for dis-covering association rules [1] that allows soft matching us-ing a specified similarity metric for each field [51]. In both methods, similarity of textual items is measured using ei-ther standard  X  X ag of words X  metrics [63] or edit-distance measures [35]; other standard similarity metrics can be used for numerical and other data types. Experimental results in several domains have demonstrated that both TextRISE and SoftApriori allow the discovery of interesting  X  X oft-matching X  rules from automatically-extracted data. These rules accurately capture regularities not discovered by tra-ditional methods [46]. proach to unifying rule-based and instance-based methods. Information extraction remains a challenging problem with many potential avenues for progress. In section 4, we dis-cussed mining knowledge from extracted data; this discov-ered knowledge can itself be used to help improve extraction. The predictive relationships between different slot fillers dis -covered by KDD can provide additional clues about what information should be extracted from a document. For ex-ample, suppose we discover the rule  X  X ySQL  X  language  X   X   X  X atabase  X  area  X . If the IE system extracted  X  X ySQL  X  language  X  but failed to extract  X  X atabase  X  area  X , we may want to assume there was an extraction error and add  X  X atabase X  to the area slot. We have developed methods for using mined knowledge to improve the recall of extraction but not the precision [48; 52]. McCallum and Jensen [41] propose using probabilistic graphical models to unify IE and KDD; however, actual results on this approach are a goal of on-going research.
 Most IE systems are developed by training on human anno-tated corpora; however, constructing corpora sufficient for training accurate IE systems is a burdensome chore. One approach is to use active learning methods to decrease the amount of training data that must be annotated by select-ing only the most informative sentences or passages to give to human annotators. We presented an initial approach to active learning for IE [70]; however, more research is needed to explore methods for reducing the demand for supervised training data in IE.
 Another approach to reducing demanding corpus-building requirements is to develop unsupervised learning methods for building IE systems. Although some work has been done in this area [19; 36], this is another promising area for future research. Developing semi-supervised learning methods for IE is a related research direction in which there has been only a limited amount of work [62].
 With respect to handling textual variation when mining ex-tracted data, it would be nice to see experimental compar-isons of the two approaches mentioned in section 4; i.e. au-tomated data cleaning versus mining  X  X oft matching X  rules from  X  X irty X  data. Do both approaches discover equally ac-curate knowledge with similar computational efficiency? When mining  X  X oft-matching X  rules, our current methods use a fixed, predetermined similarity metric for matching rule antecedents to variable text data. However, we have developed adaptive learned similarity metrics for data clean-ing and  X  X eduping X  [6]. It would be interesting to use such learned similarity metrics when discovering  X  X oft-matching X  rules since judging the similarity of textual strings is often domain dependent. In this paper we have discussed two approaches to using natural-language information extraction for text mining. First, one can extract general knowledge directly from text. As an example of this approach, we reviewed our project which extracted a knowledge base of 6,580 human protein inter-actions by mining over 750,000 Medline abstracts. Second, one can first extract structured data from text documents or web pages and then apply traditional KDD methods to discover patterns in the extracted data. As an example of this approach, we reviewed our work on the DiscoTEX sys-tem and its application to Amazon book descriptions and computer-science job postings and resum  X es.
 Research in information extraction continues to develop more effective algorithms for identifying entities and relations in text. By exploiting the lastest techniques in human-language technology and computational linguistics and combining them with the latest methods in machine learning and traditional data mining, one can effectively mine useful and important knowledge from the continually growing body of electronic documents and web pages. Our research on bioinformatics extraction was done in col-laboration with Arun Ramani, Edward Marcotte, Ruifang Ge, Rohit Kate, and Yuk Wah Wong. The DiscoTEX system was developed primarily by Un Yong Nahm. This research was partially supported by the National Science Foundation under grants IIS-0325116 and IIS-0117308. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Informa-[3] S. W. Bennett, C. Aone, and C. Lovell. Learning to tag [4] D. M. Bikel, R. Schwartz, and R. M. Weischedel. An al-[5] M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, [6] M. Bilenko and R. J. Mooney. Adaptive duplicate de-[7] C. Blaschke and A. Valencia. Can bibliographic point-[8] C. Blaschke and A. Valencia. The frame-based mod-[9] E. Brill. Transformation-based error-driven learning [10] R. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J. [11] R. C. Bunescu and R. J. Mooney. Collective information [12] M. E. Califf and R. J. Mooney. Relational learning of [13] M. E. Califf and R. J. Mooney. Bottom-up relational [14] C. Cardie. Empirical methods in information extrac-[15] X. Carreras, L. M`arquez, and L. Padr  X o. A simple [16] S. Chakrabarti. Mining the Web: Discovering Knowl-[17] H. L. Chieu and H. T. Ng. Named entity recognition [18] K. W. Church. A stochastic parts program and noun [19] F. Ciravegna, A. Dingli, D. Guthrie, and Y. Wilks. Min-[20] W. W. Cohen. Fast effective rule induction. In Proceed-[21] M. J. Collins. Three generative, lexicalised models for [22] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, [23] M. Craven and J. Kumlien. Constructing biological [24] A. Culotta and J. Sorensen. Dependency tree kernels for [25] DARPA, editor. Proceedings of the Seventh Message [26] P. Domingos. Unifying instance-based and rule-based [27] R. B. Doorenbos, O. Etzioni, and D. S. Weld. A scalable [28] C. D. Fellbaum. WordNet: An Electronic Lexical Data-[29] D. Freitag. Toward general-purpose learning for infor-[30] D. Freitag and N. Kushmerick. Boosted wrapper induc-[31] D. Freitag and A. McCallum. Information extraction [32] C. Friedman, P. Kra, H. Yu, M. Krauthammer, and [33] K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. [34] R. Ghani, R. Jones, D. Mladeni  X c, K. Nigam, and [35] D. Gusfield. Algorithms on Strings, Trees and Se-[36] T. Hasegawa, S. Sekine, and R. Grishman. Discovering [37] N. Kushmerick, D. S. Weld, and R. B. Doorenbos. [38] J. Lafferty, A. McCallum, and F. Pereira. Conditional [39] E. Marcotte, I. Xenarios, and D. Eisenberg. Mining lit-[40] J. Mayfield, P. McNamee, and C. Piatko. Named entity [41] A. McCallum and D. Jensen. A note on the unifica-[42] A. McCallum, S. Tejada, and D. Quass, editors. Pro-[43] F. D. Meulder and W. Daelemans. Memory-based [44] R. J. Mooney and L. Roy. Content-based book recom-[45] S. H. Muggleton, editor. Inductive Logic Programming . [46] U. Y. Nahm. Text Mining with Information Extraction . [47] U. Y. Nahm, M. Bilenko, and R. J. Mooney. Two ap-[48] U. Y. Nahm and R. J. Mooney. A mutually beneficial [49] U. Y. Nahm and R. J. Mooney. Using information ex-[50] U. Y. Nahm and R. J. Mooney. Mining soft-matching [51] U. Y. Nahm and R. J. Mooney. Mining soft-matching [52] U. Y. Nahm and R. J. Mooney. Using soft-matching [53] National Institute of Standards and Technol-[54] F. Peng and A. McCallum. Accurate information ex-[55] C. Perez-Iratxeta, P. Bork, and M. A. Andrade. As-[56] J. R. Quinlan. C4.5: Programs for Machine Learning . [57] L. R. Rabiner. A tutorial on hidden Markov models and [58] A. K. Ramani, R. C. Bunescu, R. J. Mooney, and [59] L. A. Ramshaw and M. P. Marcus. Text chunking us-[60] E. M. Rasmussen. Clustering algorithms. In W. B. [61] S. Ray and M. Craven. Representing sentence structure [62] E. Riloff. Automatically generating extraction patterns [63] G. Salton. Automatic Text Processing: The Transfor-[64] E. F. T. K. Sang and F. D. Meulder. Introduction to [65] S. Sarawagi and W. W. Cohen. Semi-markov condi-[66] F. Sebastiani. Machine learning in automated text cate-[67] S. Soderland. Learning information extraction rules [68] L. Tanabe and W. J. Wilbur. Tagging gene and protein [69] B. Taskar, P. Abbeel, and D. Koller. Discriminative [70] C. A. Thompson, M. E. Califf, and R. J. Mooney. Ac-[71] A. J. Viterbi. Error bounds for convolutional codes [72] L. Wall, T. Christiansen, and R. L. Schwartz. Program-[73] D. Zelenko, C. Aone, and A. Richardella. Kernel meth-
