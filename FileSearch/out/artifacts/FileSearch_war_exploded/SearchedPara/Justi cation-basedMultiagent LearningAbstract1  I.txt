 San tiago Onta~ non santi@i iia.csic.es Enric Plaza enric@i iia.csic.es IIIA, Arti cial Intelligence Researc h Institute CSIC, Spanish Council for Scien ti c Researc h Campus UAB, 08193 Bellaterra, Catalonia (Spain) Lazy learning can pro vide multiagen t systems with the capabilit y to autonomously learn from exp erience. We presen t a framew ork where agen ts that use lazy learning can collab orate in order to solv e classi ca-tion tasks. All the agen ts in our systems are able to solv e the problems individually , but the incen tive for collab oration is to impro ve the classi cation accuracy . The main topic here is how to aggregate the predic-tions coming from di eren t agen ts into a global predic-tion. We prop ose a multiagen t collab oration scheme called the Justi c ation Endorse d Col laboration policy in whic h the agen ts pro vide a sym bolic justi cation of their individual results. These justi cations can be then examined by other agen ts in the system to as-sess a measure of con dence on eac h individual result. The individual results are nally aggregated by means of a weigh ted voting scheme that uses these computed con dence measures as weigh ts.
 A strongly related area is the eld of multiple mo del learning (also kno wn as multiclassi er systems, en-sem ble learning, or committees of classi ers). A gen-eral result in multiple mo del learning (Hansen &amp; Sala-mon, 1990) pro ved that com bining the results of mul-tiple classi ers gives better results than having a sin-gle classi er. Speci cally , when the individual classi-ers mak e uncorrelated errors, and have an error rate lower than 0.5, the com bined error rate must be lower than the one made by the best of the individual classi-ers. The BEM ( Basic Ensemble Metho d ) is presen ted in (Perrone &amp; Cooper, 1993) as a basic way to com-bine con tinuous estimators by taking the average of all the predictions. Since then, man y metho ds have been prop osed: Casc ade Gener alization (Gama, 1998), where the classi ers are com bined in a sequen tial way; or Bagging (Breiman, 1996) and Boosting (Freund &amp; Schapire, 1996), where the classi ers are com bined in parallel, are good examples.
 The main di erence of our approac h is that ensem-ble metho ds suc h as Bagging or Boosting are cen tral-ized metho ds, that have con trol over the entire train-ing set, and that they arti cially create di eren t clas-si ers with the only goal of impro ving classi cation accuracy . In our approac h, we assume that we have sev eral individual agen ts, and that eac h one has col-lected exp erience on its own. Therefore, we don't have any con trol over the con ten ts of the individual agen ts' training sets. Moreo ver, the agen ts will keep priv ate their individual data, so no agen t can have access to the internal exp erience of the other agen ts. In Bag-ging, for instance, eac h individual classi er is created starting from a sample of cases of the original train-ing set, but with rep etitions, i.e. there is overlapping among the training sets of the individual classi ers, and it is this that enables Bagging to impro ve the ac-curacy over a single classi er. However, in our system, the agen ts do not kno w if there is some overlapping among the training sets of the individual agen ts or not. Therefore, the collab oration metho d to be used has to be strong enough to work with or without over-lapping, and whatev er the individual training sets are. In the exp erimen ts section, we presen t exp erimen tal results sho wing that our metho d works well in di er-ent scenarios, while standard voting fails in some of them.
 The structure of the article is as follo ws: Section 2 succinctly de nes the multiagen t systems with whic h we work. Then, Section 3 de nes the learning metho d that the individual agen ts use in our system to solv e the classi cation problems and to build the sym-bolic justi cations. Section 4 presen ts the justi cation mec hanism used by the agen ts in order to aggregate the individual predictions, and nally Section 5 sho ws an exp erimen tal evaluation of the justi cation metho d. The pap er closes with the conclusions section. Eac h agen t in our systems use case based reasoning (CBR) in order to solv e classi cation tasks, and eac h individual agen t owns a priv ate case base. There-fore, a multiagen t system is a collection of pairs: f A i ;C i g i =1 :::n , where C i is the local case base of agen t A i and n is the num ber of agen ts. As we have said, eac h agen t is able to completely solv e a problem by its own, but it will collab orate with other agen ts if this can impro ve the classi cation accuracy .
 In previous work, we presen ted the committe e collab o-ration policy (On ta~ non &amp; Plaza, 2001) for multiagen t systems. When solving a problem using this strategy , eac h agen t can vote for one or more solution classes and the nal solution is the most voted solution. Using this metho d, the agen ts were able to obtain higher clas-si cation accuracies than working individually . The committe e collab oration policy uses a variation of Ap-pro val Voting called Bounde d Weighte d Appr oval vot-ing . In this section we describ e LID (Armengol &amp; Plaza, 2001), the CBR metho d used by the agen ts in our exp erimen ts. LID is a CBR metho d that can pro-vide a sym bolic justi cation of why it has classi ed a given problem in a speci c solution class. We will rst presen t the represen tation of the cases in LID , then the heuristic measure used for guiding searc h, and nally the main steps of LID .
 LID uses the feature term formalism for represen ting cases. Featur e Terms ( -terms ) are a generalization of the rst order terms. The main di erence is that in rst order terms (e.g. person ( barbara;john;dianne )) the parameters of the terms are iden ti ed by posi-tion, and in a feature term the parameters (called fea-tures ) are iden ti ed by name (e.g. person [ name : = barbara;father : = john;mother : = dianne ] ). An-other di erence is that feature terms have a sort , for instance, the previous example belongs to the sort per-son . These sorts can have subsorts (e.g. man and woman are subsorts of person ). Feature terms have an informational order relation ( v ) among them called subsumption, where v 0 means all the information con tained in is also con tained in 0 (we say that subsumes 0 ). The minimal elemen t is ? and is called any , that represen ts the minim um information. All the feature terms of other sorts are subsumed by any . When a feature term has no features (or all of its features are equal to ? ) it is called a leaf . A path ( ;f i ) is de ned as a sequence of features going from the term to the feature f i .
 Figure 1 sho ws a graphical represen tation of a feature term. Eac h box in the gure represen ts a node. On the top of the boxes the sort of the nodes is sho wn, and on the lower part, all the features that have a value di eren t than any are sho wn. The arro ws mean that the feature on the left part of the arro w tak es the node on the righ t as value. In Figure 1 the two nodes lab elled with No and the Tylostyle node are leaf nodes. The path from the root to the Tylostyle node in the example is Spiculate-Skeleton.Me gascler es.Smo oth-form .
 LID only considers the leaf features of the terms and uses a heuristic measure to decide whic h are the most discriminatory leaf features con tained in a problem de-scription. The heuristic used is the minimization of the RLM distance (Lopez de Mantaras, 1991). The RLM distance assesses how similar are two partitions over a set of cases (less distance, more similarit y). Giv en a feature f , its possible values induce a partition f over the set of cases. Therefore, we can measure the dis-tance between the correct partition c (giv en by the solution classes) and the partition induced by a fea-ture. We say that a feature f is mor e discriminatory than the feature f 0 if RLM ( f ; c ) &lt; RLM ( f 0 ; c ). In other words, a more discriminatory feature classi es the cases in a more similar way to the correct classi -cation of cases.
 LID uses a top-do wn heuristic strategy to build a sym-bolic description D for a problem P . The partial de-scription D satis es the follo wing three conditions: D subsumes P , D con tains the most discriminatory fea-tures of P and D subsumes a subset of the case base: S
D (called the discriminatory set ). We can consider D as a similitude term , i.e. a sym bolic description of sim-ilarities between a problem P and the retriev ed cases S The main steps of LID are sho wn in Figure 2. LID ini-tially receiv es these parameters: S D C i of the agen t, the problem P to solv e, an initially empt y similitude term D 0 = ? , and the set of solu-tion classes K . The goal of LID is to nd the righ t solution class for the problem P . LID works as fol-lows: the rst step is to chec k the stopping-c ondition , that tests whether all the cases in S D same solution class or not. If this stopping condition is not satis ed, LID selects one of the features (the most discriminan t feature according to the RLM distance) of the problem P and adds it to the curren t simili-tude term D i (sp ecializing it with the value found in P ) to construct the new similitude term D i +1 . Next, LID is recursiv ely called using the new similitude term D i +1 and a new discriminatory set S D i +1 con taining only those cases in S D cess con tinues until the similitude term D i is speci c enough to be able to satisfy the stopping condition or until there are no more possible features to add to D i . The output of LID for a problem P is the tuple hS
D ; D ;K D i , where K D is the predicted solution class (or solution classes if LID has not been able to se-lect a single solution class), i.e. K D = f S k j 9 x 2 S
D ^ solution ( x ) = S k g . As the similitude term D con-tains the common information between the retriev ed cases (discriminatory set) and the problem P (the rel-evant information that LID has used to classify the problem into the predicted solution class), D will be used as the sym bolic justi cation for having classi ed the problem P in the class (or classes) K D . Example: Let us illustrate the LID metho d with an example. Imagine that we have a case base of 252 cases of the marine sponge iden ti cation problem. Eac h sponge can belong to one of three classes: As-trophorida , Hadr omerida or Axinel lida and we have a problem P to solv e. Let's call the initial simili-tude term D 0 = ? . The rst discriminatory set S D will con tain the entire case base of 252 cases (95 As-trophorida cases, 117 Hadr omerida cases and 68 Ax-inel lida cases).
 The rst leaf feature selected by LID as the most discriminatory feature according to the RLM dis-tance is the feature iden ti ed by the path \ External-featur es.Gemmules ". This path has the value \ No " in the problem P , so the path \ External-featur es.Gemmules " with the value \ No " is added to the similitude term D 0 to build the new simili-tude term D 1 as we see in Figure 1 (that sho ws the similitude term returned by LID at the end of ex-ecution). The new discriminatory set S D now only those cases subsumed by D 1 (i.e. all those cases from S D ture \ External-fe atur es.Gemmules "): 16 Astr ophorida cases, 35 Hadr omerida cases and 8 Axinel lida cases. The next path selected by LID is \ Spiculate-skeleton.Me gascler es.Smo oth-form " whic h has the value Tylostyle in the problem P , so this path with this value is added to the similitude term D 1 to build the new similitude term D 2 . The new discriminatory set S D Hadr omerida cases.
 LID now selects the path \ Spiculate-skeleton.Uniform-length " with value \ No ", whic h is also added to D 2 to build the new similitude term D 3 . The new discrimina-tory set S D Finally , the stopping condition is met because the dis-criminatory set S D lution class. Therefore, LID will return the similitude term D 3 , the discriminatory set S D class K D = f Hadromerida g . The similitude term can be interpreted as a justi c ation for having classi ed the problem P into the class Hadr omerida , i.e. \The problem P belongs to the class Hadr omerida because it has no Gemmules , the spiculate skeleton does not have a uniform length and the megascler es (in the spiculate skeleton ) have a tylostyle smo oth form ". The discrim-inatory set returned by LID is the set of cases that endorse the justi cation. In the committe e collab oration policy men tioned in Section 2, the agen ts aggregated all the solutions ob-tained individually through a voting pro cess. However, this is not alw ays the best way to aggregate the infor-mation. In this section we presen t a new collab oration policy (the Justi c ation Endorse d Col laboration ( JEC ) policy) in whic h eac h agen t can give a sym bolic jus-ti cation of its individual solution. In this way, the solutions that are not endorsed by good justi cation will not have the same strength in the voting pro cess as the solutions that are well endorsed.
 To create a justi cation of the individual solution for a problem P , an agen t solv es P individually using LID . The answ er of LID is a tuple hS D ; D ;K D i . The sym-bolic similitude term D is used as the justi cation for the answ er. Speci cally , the agen t builds a justi e d endorsement record : De nition: A justi e d endorsement record (JER) J is a tuple h S; D ;P;A i , where S is the solution class for problem P , D is the similitude term given by LID i.e. the sym bolic justi cation of the classi cation of P in S , and A is the agen t creator of the record. We will use the dot notation to denote an elemen t in the tuple (i.e. J : D is the similitude term D in J ). Eac h JER con tains the justi cation that endorses one solution class as the possible correct solution for a problem P . When the output of LID con tains more than one possible solution class sev eral JERs are built. We will denote by J A for a problem P . An agen t will generate as man y JERs as solution classes con tained in the set K D given by LID . As we will see later, these JERs are sen t to the other agen ts in the system for examination. When an agen t receiv es a JER to examine, the justi cation con-tained in the JER is con trasted against the local case base. To examine a justi cation J , an agen t obtains the set of cases con tained in its local case base that are subsumed by J : D . The more of these cases that belong to the same solution class J :S predicted by J , the more positiv e the examination will be. All the in-formation obtained during the examination pro cess of a JER by an agen t is stored in an examination record : De nition: An examination record (XER) X is a tuple h J ;Y A j J ;N A j J ;A j i , where J is a JER, Y jf x 2 C j j J : D v x ^ J :S = solution ( x ) gj is the num-ber of cases in the agen t's case base subsume d by the justi cation J : D that belong to the solution class J :S prop osed by J , N A j J = jf x 2 C j j J : D v x ^ J :S 6 = solution ( x ) gj is the num ber of cases in the agen t's case base subsume d by justi cation J : D that do not belong to that solution class, and A is the agen t that has cre-ated the examination record.
 As we will see, those examination records are used to create a con denc e measure about eac h individual so-lution. Then, using these con dence measures, the individual solutions are aggregated to obtain the -nal prediction. In the next section, the JEC policy is explained in detail. 4.1. Justi cation Endorsed Collab oration When an agen t A i wants to solv e a problem P us-ing the justi cation endorsed collab oration policy , the pro cedure follo wed consists of: 1. A i (called the convener agent ) broadcasts problem 2. eac h agen t A j in the system solv es the problem P 3. eac h agen t A j broadcasts its justi ed endorsemen t 4. every agen t A j of the system now has a col-5. the con vener agen t A i now has the set X ( P ) con-Of course, we could give a normalized class con dence by computing the ratio between the sum of positiv e cases and the sum of positiv e and negativ e cases of all relev ant examination records, but we are only inter-ested in the partial order that tells us whic h solution is the one with greatest con dence.
 Notice that the examination pro cess of the JERs is not sensitiv e to the distribution of cases among the agen ts. In other words, given a JER J , the con -dence measure C ( J ) that the con vener agen t computes will not change if we completely redistribute the cases among the agen ts. C ( J ) also remains unc hanged if we change the num ber of agen ts. A demonstration can be found in (On ta~ non &amp; Plaza, 2003). This ensures that the con dence measures computed are robust and that they are alw ays computed taking adv antage of all the available information in the system. However, the con-dence estimated dep end on the degree of redundancy |i.e. if we duplicate a case in the system, the con -dence estimates will vary .
 Example: Let us illustrate how the pro cess works with an example. Imagine a system comp osed of 3 agen ts: A 1 , A 2 and A 3 . The agen t A 1 wants to solv e the problem P of marine sponge classi cation (thus, A 1 will play the role of convener agen t). First, A 1 sends the problem P to A 2 and A 3 . All three agen ts try to solv e the problem individually using LID . After solving the problem P using LID , the agen t A 1 has found that the solution class for the problem P is Hadr omerida and the justi cation is the one sho wn in Figure 1. Therefore, A 1 builds a justi ed endors-ing record J 1 = h Hadromerida; D 1 ;P;A 1 i , where D 1 is the justi cation sho wn in Figure 1. Then, J 1 is sen t to A 2 and A 3 . Analogously , A 2 builds the JER J 2 = h Axinel lida; D 2 ;P;A 2 i and A 3 builds the JER J 3 = h Hadromerida; D 3 ;P;A 3 i , and sends them to the other agen ts (notice that eac h agen t could gener-ate more than one JER if D i covers cases in more than one solution class).
 At this point, eac h agen t ( A 1 , A 2 and A 3 ) has the set of JERs J ( P ) = f J 1 ; J 2 ; J 3 g , con taining all the JERs built by all the agen ts. It's time to build the examination records.
 For instance, when A 2 starts examining the JER J 1 coming from the agen t A 1 , all the cases in the case base of A 2 that are subsumed by the justi cation J 1 : D are retriev ed: 13 cases, 8 cases belonging the Hadr omerida solution class, and 5 cases belonging to Astr ophorida . That means that A 2 kno ws 5 cases that completely satisfy the justi cation given by A 1 , but that do not belong to the Hadr omerida class. Therefore, the XER built for the justi cation J 1 is X 1 = h J 1 ; 8 ; 5 ;A 2 Then, A 2 con tinues by examinating the next JER, J 2 (its own JER), and nds only 3 cases belonging the Axinel lida class. Therefore, A 2 builds the follo wing XER: X 2 = h J 2 ; 3 ; 0 ;A 2 i . Finally , A 2 also builds the XER for the JER J 3 : X 3 = h J 3 ; 5 ; 1 ;A 2 i . Those three XERs are sen t to the con vener agen t A 1 . In the same way, A 3 also builds its own XERs and sends them to A 1 . The con vener agen t A 1 also builds its own XERs and stores them.
 After having receiv ed the rest of examination records: X 4 = h J 1 ; 7 ; 0 ;A 3 i ; X 5 = h J 2 ; 2 ; 5 ;A X 6 = h J 3 ; 10 ; 0 ;A 3 i ; X 7 = h J 1 ; 15 ; 0 ;A 1 i ; X h J 2 ; 1 ; 4 ;A 1 i ; X 9 = h J 3 ; 6 ; 1 ;A 1 i , A 1 builds the set X ( P ) = f X 1 ;:::; X 9 g . Then, A 1 computes the con-dence measures for eac h JER. For the JER J 1 , the con dence measure will be obtained from the XERs X 1 , X 4 and X 7 (the XERs that refer to J 1 ): C ( J 1 ) = (8 + 7 + 15) = (8 + 5 + 7 + 0 + 15 + 0) = 0 : 85. In the same way, the con dence C ( J 2 ) = 0 : 40 will be computed from X 2 , X 5 and X 8 and the con dence C ( J 3 ) = 0 : 91 from X 3 , X 6 and X 9 . Notice how the weak est JER ( J 2 ) has obtained the lowest con dence, while stronger justi cations obtain higher con dence values. Once all the con dence measures of the JERs have been computed they can be aggregated to obtain the con dences of the solution classes. For instance, there are two JERs ( J 1 and J 2 ) endorsing Hadr omerida as the solution, and one JER ( J 2 ) endorsing Axinel lida as the solution. Therefore, the con dence measures for the solution classes are: C ( Hadromerida ) = 0 : 85+ 0 : 91 = 1 : 76 and C ( Axinel lida ) = 0 : 40. The selected solution class is Hadr omerida because it is the one with highest con dence. In this section we presen t exp erimen tal results sho w-ing the bene ts of using the JEC policy versus the standard committee collab oration policy . We use the marine sponge classi cation problem as our testb ed. Sponge classi cation is interesting because the di-culties arise from the morphological plasticit y of the species, and from the incomplete kno wledge of man y of their biological and cytological features. Sponges have a complex structure, making them amenable to build complex justi cations using LID .
 In order to compare the performance of the justi ca-tion based approac h and the voting approac h, we have designed an exp erimen tal suite with a case base of 280 marine sponges pertaining to three di eren t orders of the Demosp ongiae class ( Astr ophorida , Hadr omerida and Axinel lida ). In an exp erimen tal run, training cases are randomly distributed to the agen ts. In the testing phase, problems arriv e randomly to one of the agen ts. The goal of the agen ts is to iden tify the correct bi-ological order given the description of a new sponge. We have exp erimen ted with 5, 7, 9, 10 and 16 agen t systems using LID as the classi cation metho d. The results presen ted here are the result of the average of 5 10-fold cross validation runs. For eac h scenario, the accuracy of both the JEC policy (justi cation) and the committee are presen ted. The accuracy of the agen ts working in isolation is also presen ted for comparison purp oses. In the individual results, the individual ac-curacy of the con vener agen t is considered. In order to test the generalit y of the technique, three di eren t kind of scenarios will be presen ted: the un-biase d scenarios , the biase d scenarios and the redun-dancy scenarios . In the unbiased scenarios, the train-ing set will be divided into sev eral disjoin t subsets in a random way, and eac h subset will be given to eac h one of the individual agen ts. In the biased scenarios, those subsets will not be randomly generated, and the agen ts will have a skewed view of the problem, i.e. some agen ts will receiv e very few (or no) problems of some classes, and will receiv e lots of problems of some other classes. And nally , in the redundancy scenar-ios, the subsets of the training set will not be disjoin t, i.e. some of the cases will be presen t in more than one subset. 5.1. Unbiased Scenarios In the unbiased scenarios, the training set is divided into as man y subsets as there are agen ts in the system. Then, eac h one of these subsets is given to eac h one of the individual agen ts as the training set. For instance, in the 5 agen t scenario, the training set of eac h agen t consists of about 50 sponges and in the 16 agen t sce-nario it consists of about 16 sponges. Therefore, in the scenarios with man y agen ts, as the individual case bases are smaller, the individual accuracies will also be lower, leading to a greater incen tive to collab orate. Table 1 sho ws the classi cation accuracies obtained by sev eral groups of agen ts using the JEC policy and the committee collab oration policy . The rst thing we notice is that the agen ts using justi cations ob-tain higher accuracies. This di erence is noticeable in all the scenarios, for instance in the 5 agen t sce-nario, the agen ts using justi cations obtained an ac-curacy of 88.50% and the agen ts using the committee collab oration policy obtained an accuracy of 88.36%. The di erence is not great in systems with few agen ts, but as the num ber of agen ts increase, the accuracy of the agen ts using the committee policy drops. For instance, in the 16 agen ts scenario, the accuracy for the agen ts using justi cations is of 87.28% and the accuracy for the committee policy is 85.71%. For test-ing whether this e ect prev ails when increasing even more the num ber of agen ts, we have performed an ex-perimen t with 25 agen ts (where eac h agen t have only about 10 cases and the individual accuracy is 58.21%), and the result is that the agen ts using justi cations still obtained an accuracy of about 87.28% while the agen ts using the committee collab oration strategy ob-tained an accuracy of 84.14%. These results sho w that even when the data is very fragmen ted, the con dence measures (that are not a ected by the fragmen tation of the data) computed by the agen ts are able to select whic h of the individual solutions are better based on the justi cation given.
 Table 1 also sho ws the accuracy obtained by the agen ts when they solv e the problems individually (without collab orating with the other agen ts). The table sho ws that as the num ber of agen ts in the exp erimen ts in-creases (and the size of the individual training sets diminishes) the individual accuracy drops fast. For instance, in the 16 agen ts scenario, the individual ac-curacy is only 67.07% while the accuracy obtained in the same system using the JEC policy is 87.28%. For comparison purp oses, we have tested the accuracy ob-tained if we have a system con taining only one agen t (ha ving the whole training set), and we have obtained an accuracy equal to 88.2%. 5.2. Biased Scenarios In the biased scenarios, the training set is also divided in as man y subsets as agen ts in the system, but this time, eac h subset in not chosen randomly . We have forced some of the agen ts in the system to have more cases of some classes and to have less cases of some other classes. There can be also agen ts that don't have cases of some of the classes, or even agen ts that have only cases of a single class. This bias diminishes indi-vidual accuracy (On ta~ non &amp; Plaza, 2002). Table 2 sho ws the classi cation accuracies obtained by sev eral groups of agen ts using the JEC policy and the committee collab oration policy in the biased scenar-ios. As we have said, the individual accuracies when the agen ts have biased case bases are weak er than in the unbiased scenarios. This weak ening in the indi-vidual results is also re ected in the Committee accu-racy . This can be seen in Table 2, where the accuracy achiev ed by the Committee is clearly lower than the accuracy obtained in the unbiased scenarios sho wn in Table 1. However, comparing the results of Tables 1 and 2, we can see that the results obtained by the agen ts using the JEC policy are not a ected by the bias of the case bases. Therefore, the increase in ac-curacy obtained with the JEC policy is greater in the biased scenarios. For instance, in the 5 agen ts sce-nario, the accuracy obtained with the JEC policy is 88.78% and the accuracy with the committee policy is only 84.36%, while the accuracy for the 10 agen ts sce-nario is 87.93% versus 85.62%. Again, we can see here how the con dence measures are indep enden t of the case distribution among the agen ts, ensuring a robust aggregation of the individual predictions. 5.3. Redundancy Scenarios In the redundancy scenarios, the training set is divided into subsets that are not disjoin t, i.e. there are cases that are presen t in more than one subset. To create these disjoin t subsets, we have duplicated 50% of the cases in the training set before distributing it among the agen ts. This means that if the original training set had 252 cases, the new training set has 378 cases (126 of them occur twice). We have said that the compu-tation of the con dence values of the JERs is not sen-sitiv e to the distribution of cases among the agen ts, however the amoun t of redundancy in the system does a ect it. These scenarios have been created to test the sensitivit y of the computation of the con dence values of the JERs to redundancy .
 Table 3 sho ws the accuracy obtained by both the JEC policy and the committee policy (the individual accu-racy is also sho wn for comparison purp oses). The rst thing we see is that the JEC policy again outp erforms the committee policy . This di erence is very clear in the scenarios with man y agen ts (9, 10 and 16). For instance, in the 16 agen ts scenario, the JEC policy ob-tained an accuracy of 88.28% and the committee policy just 87.71%. However, in these scenarios, the two poli-cies are closer than in the previous two. For instance, in the 5 and 7 agen t scenario, it is not so clear whic h policy works better: 89.64% for the JEC policy versus 89.78% in the 5 agen ts scenario, and 89.57% versus 89.50% in the 7 agen ts scenario.
 If we look at the individual results in Table 3, we notice that they are much better than the individual results without redundancy: this is due to an increase of size in the individual case bases. This greater individual accuracy also enables the committee and JEC policies to obtain higher accuracies.
 Notice that in the redundancy scenario there is over-lapping among the agen t's case bases. The examina-tion pro cess is sensitiv e to the degree of overlapping because there are cases that (as presen t in sev eral case bases) may be coun ted more than once for computing con dence estimates. However, as the results sho w, the JEC policy is robust enough to keep performance equal or better than the committee policy . We have presen ted a metho d to aggregate predictions coming from sev eral agen ts into a single prediction using sym bolic justi cations. Allo wing the agen ts to give a justi cation of their individual results is crucial in multiagen t systems since in an environmen t where one's conclusions may dep end on kno wledge pro vided by third parties, justi cations of these conclusions be-come of prime imp ortance (van Harmelen, 2002). In the exp erimen ts section, we have sho wn that both Committee and JEC policies obtain higher accuracies than the individual accuracy . However, the JEC policy is more robust than the Committee policy . Commit-tee works well when the individual agen ts can pro vide good individual predictions. However, as our exp eri-men ts sho w, when the individual accuracy drops, the accuracy of the Committee also drops. The JEC policy is more robust because the accuracy decreases much less (as can be seen in all the 16 agen ts scenarios). Moreo ver, when the distribution of the data among agen ts is not uniform and the individual agen ts have a skewed view of the data, the JEC policy is still able to keep the accuracy at the same level. More favorable scenarios for the Committee policy are the redundancy scenarios, where the individual agen ts have higher in-dividual accuracies |although the JEC policy is still equal or better than the Committee.
 We have only tested our metho d using LID as the clas-si cation algorithm, but in fact, any metho d that can pro vide a sym bolic justi cation of the prediction can be used. As future work, we plan to evaluate the ap-proac h with decision trees, from whic h a sym bolic jus-ti cation can be constructed.
 We have seen that the con dence estimation is inde-penden t of both the num ber of agen ts in the system and the distribution of cases among them. However, it is dep enden t on the degree of redundancy among the agen t's case bases. In the future, we plan to use a new con dence estimation system that tak es into accoun t the redundancy level among the agen t's case bases. Armengol, E., &amp; Plaza, E. (2001). Lazy induction of descriptions for relational case-based learning. ECML 2001 (pp. 13{24).
 Breiman, L. (1996). Bagging predictors. Machine Learning , 24 , 123{140.
 Freund, Y., &amp; Schapire, R. E. (1996). Exp erimen ts with a new boosting algorithm. Proc. 13th ICML (pp. 148{146). Morgan Kaufmann.
 Gama, J. (1998). Local cascade generalization. Proc. 15th ICML (pp. 206{214). Morgan Kaufmann, San Francisco, CA.
 Hansen, L. K., &amp; Salamon, P. (1990). Neural net works ensem bles. IEEE Transactions on Pattern Analysis and Machine Intel ligenc e , 993{1001.
 Lopez de Mantaras, R. (1991). A distance-based at-tribute selection measure for decision tree induction. Machine Learning , 6 , 81{92.
 Onta~ non, S., &amp; Plaza, E. (2001). Learning when to collab orate among learning agen ts. 12th Eur opean Confer enc e on Machine Learning (pp. 394{405). Onta~ non, S., &amp; Plaza, E. (2002). A bartering aproac h to impro ve multiagen t learning. AAMAS 2002 (pp. 386{393).
 Onta~ non, S., &amp; Plaza, E. (2003). Justi c ation-b ased multiagent learning (Technical Rep ort). Arti cial Intelligence Researc h Institute (IIIA). TR-2003-07. Perrone, M. P., &amp; Cooper, L. N. (1993). When net-works disagree: Ensem ble metho ds for hydrid neu-ral net works. In Arti cial neur al networks for speech and vision . Chapman-Hall. van Harmelen, F. (2002). How the seman tic web will change KR. The Know ledge Engine ering Review , 17 ,
