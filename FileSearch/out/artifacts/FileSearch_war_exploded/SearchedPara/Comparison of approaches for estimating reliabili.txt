 1. Introduction
When using supervised learning for modeling data, we aim to achieve the best possible prediction accuracy for the un-seen examples which were not included in the learning process [1]. The most common measures for evaluating prediction accuracy are averaged accuracy measures such as the mean squared error (MSE) and the relative mean squared error (RMSE).
Although these estimates evaluate model performance by summarizing the error contributions of all test examples, they ing upon predictions may have financial or medical consequences (e.g. medical diagnosis, stock market, navigation, control do not fulfill this requirement, reliability measures for individual predictions are needed in this area.
Various methods have been developed to enable the users of classification and regression models to gain more insight into the reliability of individual predictions. Some of these methods were focused on extending formalizations of existing classification and regression models, enabling them to make predictions with their adjoined reliability estimates. Another group of methods focused on the development of model-independent approaches, which are more general but also harder to analytically evaluate with individual models.
Since some of the approaches from the first group were able to make use of probabilistic and model-specific properties,
Confidence measures have values spanning the interval [0,1], where 0 represents the confidence of the most unreliable pre-diction and 1 the confidence of the most reliable one. Since estimates developed using other approaches do not necessarily
In this paper we compare five approaches to model-independent reliability estimation for individual examples in regres-sion. We mostly focus on comparing the sensitivity-based estimates developed in our previous work to four other ap-proaches implemented using ideas from related work. We evaluate the performance of selected reliability estimates on sion trees, support vector machines, locally weighted regression, random forests and generalized additive model. With the aim of improving the performance of the estimates, we test the performance of combined (averaged) pairs of estimates.
This paper is organized as follows. Section 2 summarizes previous work from related areas of individual prediction reli-ability estimation and Section 3 presents and proposes the reliability estimates. We describe the experiments and testing protocol, and interpret the results in Section 4. Section 5 provides conclusions and ideas for further work. 2. Related work differentiating between various approaches is whether they target a specific predictive model or whether they are model-independent. While the model-specific approaches are less general, they are usually founded on exact mathematical or prob-numbers. As such, the metrics X  values have no probabilistic interpretation.
 and credibility and showed that the basic model can be successfully expanded with reliability estimates. Also focusing on perceptron, and Heskes, Carney and Cunningham [7,8] the ensembles of neural networks. It is obvious that these approaches cannot be used with an arbitrary predictive model due to their definition, which is bound to the specific model formalism.
In contrast, the methods that are independent of the predictive model are also more generally applicable. These methods and presented a meta-algorithm for predicting the leave-one-out error. The simulations showed that the proposed meta-learning approach compares favorably to the conventional theoretical leave-one-out error bounds.
 a set of reliability measures which successfully separate correct and incorrect classifications and are independent of the learning algorithm.

We later adapted this transductive approach to regression by proposing a simple linear model for predicting prediction vector machines) and motivating its development by the minimum description length principle [17]. In our previous work, we based our motivation for developing model-independent reliability estimates on the following ideas from related re-search fields:
Approaches which perturb data. The design of these approaches implies that different subsets of learning examples cause learning [23 X 25] and reinforcement learning [26,27] .
 idea suggests that we should observe how the presence of a particular new example in the learning set influences the model X  X  predictive accuracy. The idea also offers the possibility of making inferences about prediction reliability for an example by observing the resulting change in the model.
 pendent of the predictive model [33 X 36] . Namely, this technique requires no knowledge of the model X  X  mathematical properties. Instead, the model is considered as a black box, with inputs and outputs as the only parameters being influ-enced and observed. We focus on this research field in more detail in Section 3.1.

The work presented here extends our initial work [14 X 16] and compares the performance of the previously proposed esti-mates to four adapted approaches from related work. They are summarized in the following section. 3. Reliability estimates 3.1. Local sensitivity analysis reliability estimates 3.1.1. Sensitivity analysis thechangesinmodeloutputs(predictions) whenwemodifyitsinput (thelearningdataset).Apartfrommodifyingthelearning set,thesensitivityanalysisapproachcouldalsoobservetheoutputvariationwithrespecttochangesinothermodelparameters.
For influencing the input of the system (regression model), we expanded the learning set with an additional learning exam-ing to information about prediction reliability. 3.1.2. Reliability estimates
In our previous work [16] we used the sensitivity analysis approach to develop two reliability estimates (RE which estimate the local variance and the local bias for a given unlabeled example. For greater clarity, we refer to these the reliability for a given example, we expanded the learning set with that particular example, labeling it with label value of the additional learning example and therefore indirectly defines the magnitude of the induced change in
In the next step, a sensitivity model was induced on the modified learning set and a sensitivity prediction K the same particular example.

After computing different sensitivity predictions using different values of parameter e (in our previous work, we selected allowed us to widen the observation window in the local problem space and to define more robust reliability estimates by averaging the individual components. The estimates were defined by observing the differences between the initial and tions made using e and e . The estimates were defined as follows: and
In the above estimates, K represents the prediction of the initial regression model (the initial prediction ), K the sensitivity predictions , and E denotes a set of used sensitivity parameters e . 3.2. Variance of a bagged model bagging technique, we generalize the proposed reliability estimate for use with other regression models.
Given a bagged aggregate of m predictive models, where each of the models yields a prediction K an example is predicted by averaging the individual predictions: and reliability estimate BAGV is defined as the prediction variance: m = 50 model instances. 3.3. Local cross-validation reliability estimate
Existing work has demonstrated various uses of only local information to perform learning. For example, Schaal and Atke-local cross-validation error.

In our work, we propose the LCV (Local Cross-Validation) reliability estimate, which is computed using the local leave-one-out (LOO) procedure. The estimate is similar to the mean squared local cross-validation error as in [42], except that all eight regression models that we studied. Suppose that we are given an unlabeled example for which we wish to compute the prediction and the LCV estimate. Focusing on the subspace defined by k nearest neighbors (parameter k is selected in advance), we then generate k local models, each of them excluding one of the k nearest neighbors. Using the generated mod-els, we compute the leave-one-out predictions K i , i =1, ... , k for each of the nearest neighbors. Since the labels C the nearest neighbors are given, we are therefore able to calculate the absolute local leave-one-out prediction errors
E = j C i K i j . The prediction for the given local example is then computed using the k nearest neighbors (k-NN) algorithm and x an example for which we are computing the LCV estimate.

In our experimental work, we implemented the preceding algorithm to be adaptive to the size of the neighborhood with where j L j denotes the number of learning examples.
 3.4. Density-based reliability estimate predictions which are made in denser problem subspaces (a portion of the input space with a more learning examples), and means that we trust the prediction with respect to the quantity of information that is available for its computation.
A typical use of this approach is with decision and regression trees, where we trust each prediction according to the pro-examples X  labels. This causes the method to perform poorly with noisy data and in cases when distinct examples are not clearly separable.

We define the reliability estimate DENS as the value of the estimated probability density function for a given unlabeled example. To estimate the density, we use Parzen windows [44,45] with the Gaussian kernel. We reduced the problem of computing the multidimensional Gaussian kernel to computing the two-dimensional kernel by using a distance function ap-plied to pairs of example vectors. Given the learning set L =(( x ( x ,_) is therefore defined as defining the reliability estimate as 3.5. Local modeling of prediction error ity using the nearest neighbors X  labels. Given a set of nearest neighbors N ={( x ( C example X  X  prediction K (using the model that was generated on all learning examples): iments we computed estimate CNK using five nearest neighbors. The design of the estimate CNK is illustrated in Fig. 5 . 4. Experimental results
We tested and compared the estimates SAvar and SAbias, developed in our previous work, to the other estimates de-scribed in Sections 3.2 X 3.5 . Testing was performed using the leave-one-out cross-validation procedure. For each learning mance of the reliability estimates was measured by computing the Pearson correlation coefficient between each reliability two-sided t -test for correlation coefficients.
 mates are founded so that higher absolute values represent less reliable predictions and lower absolute values represent mates except SAbias and CNK can take only positive values. Besides the magnitude (absolute value), which we interpret as two estimates. We therefore performed the experiments by correlating the magnitudes of estimates to the absolute predic-(absolute), BAGV, LCV, DENS, CNK-s and CNK-a.

The performance of reliability estimates was tested using eight regression model implemented in the statistical package R [46]. Here are some key properties of the models used: average label of examples, and trees were unpruned, Linear regression (LR) : linear regression with no explicit parameters,
Neural networks (NN): three-layered perceptron [48] with five hidden neurons and tanh activation function, learning was performed using backpropagation with adaptive gradient descent, Bagging (BAG) : bagging [18] with 50 regression trees,
Support vector machines (SVM) : an implementation of the support vector e -regression [12,49] , implemented in the library for support vector machines (LIBSVM) [50,51] . We use a radial basis function (RBF) kernel with parameter c = 1/(number of attributes), parameter C = 1 and the precision parameter = 0.1,
Locally weighted regression (LWR) : local regression with Gaussian kernel for weighting examples according to their distance, Random forests (RF) : random forests [52] with 100 trees, Generalized additive model (GAM) : linear model [53,54] with no special parameters.

The aim of our research was to evaluate the reliability estimates with models being treated as black boxes. Therefore, the focus of our research was not to optimize the above model parameters to improve prediction accuracy, but to evaluate the learning community. Each data set is a regression problem. The application domains vary, including medical, ecological, technical, mathematical and physical. Most of the data sets are available from the UCI machine learning repository [55] the data sets is given in Table 1 . 4.1. Testing of individual estimates
The summarized results of the experiments are shown in Table 2 and in Fig. 6 . The data in Table 2 indicate the percent of same results are presented graphically in Fig. 6 for each of the tested regression models. The graphs show the performance decreasing order with respect to the percentage of positive correlations.

The last line in Table 2 presents the results of the reliability estimates, averaged across all eight testing regression models. We can see that the best results were achieved using the estimates BAGV, CNK-a, LCV and SAvar (in decreasing results.
 relations in all regression model/reliability estimate pairs.

By examining the detailed results (Table 2) we can see that the performance of the estimate CNK-s with the regression atively correlated in any experiment. Estimate SAbias-s achieved similar performance, with positive correlation in 82% of ear regression, generalized additive model) and regression trees than the estimate BAGV, which was also the estimate with the highest average number of significant positive correlations (53%).

The results achieved by estimates SAbias-a and SAbias-s, also confirm our expectations and analysis from previous work these models.

The results indicate that the estimates SAbias, CNK, BAGV and LCV have good potential for estimating prediction reliabil-estimates in order to compose a new estimate which would perform well with all models. 4.2. Combination of estimates
To create a new estimate that would perform well on all tested regression models, we combined pairs of estimates using a linear model:
Different choices of the value c give different weights to each of the combined estimates in Eq. (8), making the combined estimate behave more similarly to the estimate with the greater weight. Since we want to create a combined estimate that to the limitation that they can be reasonably combined with respect to correlation to signed or absolute prediction error.
Thus, individual estimates which were correlated to the signed prediction error were combined only with other such esti-mates; the estimates which were correlated to the absolute prediction error were combined only with other such estimates.
The testing of combined estimates was performed using the same experimental protocol as the testing of individual esti-mates. The results are shown in Table 4 .
 From the results on combined pairs of reliability estimates, we can see that the combination of estimates BAGV and
CNK-a achieved the best performance. Comparing the results in Tables 2 and 4 reveals that the combined estimate with neural networks and with bagging. However, the combination of BAGV and CNK-a on average had significant positive icant advantage over the estimate BAGV, which had significant positive correlation with the prediction error in 53% of tests.

Since the combination of the estimates BAGV and CNK-a performed well with some regression models and achieved good average results, we selected it for further evaluation. Let us therefore define the estimate BVCK (Bagging Variance  X  ( C neighbors K )) which is defined as mates X  values for that particular example.
 Fig. 7 presents a comparison of the results, averaged across all regression models for all reliability estimates.
Example 1. Analyzing prediction reliability Suppose we are given a learning set L and two unlabeled examples ( x ( x ,_) for which we wish to estimate and compare reliability. After computing the predictions and nine reliability estimates using a selected regression model for each unlabeled example, we obtain the values shown in Table 5 . Five out of seven
However, since values of the each estimate belong to the estimate-specific interval, they are difficult to compare with crepancy between the values of DENS and the other estimates reveals that some estimates may be more suitable for a given problem than others. Both these issues provide motivation for further work in improving estimates X  interpretability and selecting the most appropriate reliability estimate for a given problem (see Section 5). 5. Conclusion and further work
In this paper, we focused on the importance of reliability and prediction accuracy in supervised learning, as already dis-our previous work with four other approaches to estimating the reliability of individual predictions, namely variance of and eight regression models showed promising results for using estimates SAbias-s and CNK-s with regression trees. These average performance was achieved by the estimate BAGV, which turned out to be the best choice for use with neural networks, bagging and locally weighted regression. With linear models (linear regression and generalized additive model), the estimate CNK-a performed better than BAGV, thus improving on the performance of all other tested approaches.
With the aim of improving the estimates X  performance, we combined pairs of reliability estimates, which perform differ-ently with different regression models. The combination of the estimates BAGV and CNK-a performed better than any other estimate with neural networks and with bagging. On average, this combination performed comparably to the most success-Based on these results, we selected this estimate for further evaluation and denoted it BVCK.
 selected regression predictors. They also show the favorable performance of the newly proposed local error modeling esti-mate CNK, as compared with the other estimates. These results and new ideas offer challenges for further work:
Good performance of the signed reliability estimates (SAbias-s with regression trees and CNK-s) with the signed predic-for estimation of prediction reliability is more feasible in some domains than in others. The domain and model character-istics which lead to good performance of the reliability estimates shall also be analyzed.

Since different estimates performed differently with different domain/model pairs, an approach to automatic selection of the optimal reliability estimate for a given domain, model and example shall be developed. For example-based selection of the most appropriate estimate, a mapping of estimates X  values onto the same interval is necessary to ensure comparability of estimates X  across different examples.

References
