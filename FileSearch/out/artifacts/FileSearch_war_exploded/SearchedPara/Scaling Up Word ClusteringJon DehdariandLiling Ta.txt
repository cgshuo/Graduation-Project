 Words can be grouped into equivalence classes to reduce data sparsity and generalize data. Word clus-ters are useful in many NLP applications. Within machine translation, word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wue-bker et al., 2013), reordering (Cherry, 2013), pre-ordering (Stymne, 2012), SAMT (Zollmann and Vo-gel, 2011), and OSM (Durrani et al., 2014).

Word clusterings have also found utility in pars-ing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), and NER (Miller et al., 2004; Liang, 2005; Turian et al., 2010; Ritter et al., 2011), among many others.
Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001b). This reduces the normalization time from O ( | V | ) (the vocabulary size) to  X  X  ( The exchange algorithm (Kneser and Ney, 1993) uses an unlexicalized (two-sided) model: class c i of the predicted word w i is condi-tioned on the class c i  X  1 of the previous word w i  X  1 . Goodman (2001a) altered this model so that c i is conditioned directly upon w i  X  1 : tionates the history more, but it greatly speeds up hypothesizing an exchange since the history doesn X  X  change. The resulting partially lexicalized (one-sided) model gives the accompanying predictive exchange algorithm (Uszkoreit and Brants, 2008) a time complexity of O (( B + | V | )  X  | C |  X  I ) where B is the number of unique bigrams, | C | is the number of classes, and I is the number of training iterations, usually &lt; 20 . ClusterCat is word clustering software designed to be fast and scalable, while also improving upon the predictive exchange algorithm. We describe in this section improvements in the model, the algorithm, as well as in the implementation. 3.1 Model and Algorithm We developed a bidirectional, interpolated, refin-ing, and alternating (BIRA) predictive exchange algorithm. The goal of B IRA is to produce bet-ter clusters by using multiple, changing models to escape local optima. This uses both forward and reversed bigram class models in order to improve cluster quality by evaluating log-likelihood on two different models. Unlike using trigrams, bidirec-tional bigram models only linearly increase time and memory requirements, and in fact some data struc-tures can be shared. The two directions are interpo-lated to allow softer integration of these two models: P ( w i | w i  X  1 ,w i +1 ) , P ( w i | c i )  X  (  X P ( c i (1  X   X  ) P ( c i | w i +1 )) . Furthermore, the interpolation weight  X  for the forward direction alternates to 1  X   X  every a iterations i to help escape local optima. The time complexity is O (2  X  ( B + | V | )  X | C | X  I ) . The original predictive exchange algorithm can be obtained by setting  X  = 1 and a = 0 .

Cluster refinement improves both cluster quality and speed. The vocabulary is initially clustered into | G | sets, where | G | | C | , typically 2 X 10 . This groups words into broad classes, like nouns, verbs, etc. After a few iterations ( i ) of this, the full par-titioning C f is explored. Clustering G converges very quickly, typically requiring no more than 3 it-erations. In contrast to divisive hierarchical cluster-ing and coarse-to-fine methods (Petrov, 2009), af-ter the initial iterations, any word can still move to any cluster X  X here is no hard constraint that the more refined partitions be subsets of the initial coarser partitions. This gives more flexibility in optimiz-ing on log-likelihood, especially given the noise that naturally arises from coarser clusterings. We ex-plored cluster refinement over more stages than just two, successively increasing the number of clusters. We observed no improvement over the two-stage method described above.

The contributions of each of these, relative to the original predictive exchange algorithm, are shown in Figure 1 . The data and configurations are dis-cussed in more detail in Section 4. The greatest im-provement is due to using lambda inversion ( +Rev ), followed by cluster refinement ( +Refine ), then in-terpolating the bidirectional models ( +BiDi ), with robust improvements by using all three of these X  X n 18% reduction in perplexity over the predictive ex-change algorithm. We have found that both lambda inversion and cluster refinement prevent early con-vergence at local optima, while bidirectional models give immediate and consistent training set PP im-provements, but this is attenuated in a unidirectional evaluation. 3.2 Implementation We represent the set of bigrams B as an array of records that track the number of predecessors, as well as having a pointer to an array of the predeces-sors X  IDs. This allows for easy prefetching to reduce memory latency, while also keeping memory over-head low. We dispense with the predictive exchange RemoveWord procedure for tentative steps, since this does not change the final clustering.

Most of the computation for the predictive ex-change algorithm is spent on the logarithm func-tion in  X   X   X   X  N ( w,c )  X  log N ( w,c ) . 1 Since the codomain of N ( w,c ) is N 0 , and due to the power law distribution of the algorithm X  X  access to these entropy terms, we precompute p N  X  log N q up to, say results in a considerable speedup of around 40% . We evaluate ClusterCat on training time, two-sided class-based language model (LM) perplexity (cf. Brown et al., 1992; Uszkoreit and Brants, 2008), and B LEU scores in phrase-based MT. 4.1 Intrinsic Evaluation For the two-sided class-based LM task we used 800 and 1200 classes for English, and 800 classes for Russian. The clusterers (cf. Sec. 2) are Brown-cluster (Liang, 2005), ClusterCat (introduced in Sec-tion 3), mkcls (Och, 1995), Phrasal X  X  clusterer (Green et al., 2014), and word2vec X  X  clustering fea-ture (Mikolov et al., 2013).

The data comes from the 2011 X 2013 News Crawl periments the data was deduplicated, shuffled, tok-enized, digit-conflated, and lowercased. In order to have a large test set, one line per 100 of the resulting this gave 1B training tokens, 2M training types, and 12M test tokens. For Russian, 550M training tokens, 2.7M training types, and 6M test tokens.

All clusterers had a minimum count threshold of 3 occurrences in the training set. All used 12 threads and 15 iterations, except single-threaded mkcls which used the default one iteration. Clusterings were performed on a 2.4 GHz Opteron 8378 ma-chine featuring 16 threads and 64 GB of RAM.
Table 1 presents wall clock times. The predictive exchange-based clusterers (ClusterCat and Phrasal) exhibit slow time growth as | C | increases, while the other three (Brown, mkcls , and word2vec) are much more sensitive to | C | . ClusterCat is three times faster than Phrasal for all sets. For both En-glish and Russian we observe prohibitive growth for mkcls , with the full Russian training set taking over 3 days, compared to 1.5 hours for ClusterCat.
We performed an additional experiment on Clus-3.0 hours to cluster 2.5 billion training tokens , using 40 GB of memory for | C | = 800 . When the number of clusters was tripled to | C | = 2400 , the same 2.5B corpus was clustered in under 8 hours.

The clusterings are also evaluated on the perplex-ity (PP) of an external 5-gram two-sided class-based LM. Botros et al. (2015) found that the two-sided model (which mkcls uses) tends to give better PP in two-sided class-based LM experiments, but the one-sided model of the predictive exchange that we em-ployed produces better PP for training LSTM LMs.
Table 2 shows perplexity results using a varying number of classes. As word2vec is the only clus-terer not optimized on log-likelihood, its perplexity is quite high, and remains high as more training data is added. 6 On the other hand, mkcls gives the low-est perplexity, although this is an artefact of the two-sided evaluation. ClusterCat gives lower perplex-ity than the original predictive exchange algorithm (in Phrasal) and Brown clustering. The Russian experiments yielded higher PP for all clusterings, but otherwise the same comparative results. The metaheuristic techniques used in mkcls can be ap-plied to other exchange-based clusterers X  X ncluding ours X  X or further improvements.

It is also interesting to look at time-sensitive clus-tering. Figure 2 shows what perplexity can be ob-tained within a given training time frame. For each clusterer, each successive rightward point in the fig-ure represents an order of magnitude more training on 10 times more data than either mkcls or Brown-cluster and produces better perplexity than either, within a given amount of time .
 4.2 Extrinsic Evaluation We also evaluated mkcls and ClusterCat extrinsi-cally in machine translation, for word alignment. As training sets get larger every year, mkcls strug-gles to keep pace, and is a substantial time bot-tleneck in MT pipelines. We compare time and B
LEU scores of using either mkcls or ClusterCat for Russian  X  English translation.
 The parallel data comes from the WMT-2015 Common Crawl Corpus, News Commentary, Yan-dex 1M Corpus, and the Wiki Headlines Cor-2014 News Commentary and News Crawl arti-cles. The dev and test sets contain 3000 sen-tences from EN  X  RU manually translated news ar-ticles. We used standard configurations, like true-casing, MGIZA alignment, GDFA phrase extrac-tion, phrase-based Moses, quantized KenLM 5-gram MKN LMs, and M ERT tuning.

Table 3 presents the B LEU score changes across varying cluster sizes. 8 The B LEU score differences between using mkcls and ClusterCat are minimal but there are a few statistically significant changes, using bootstrap resampling (Koehn, 2004).
Figure 3 shows translation model training times, before M ERT . Using ClusterCat reduces the transla-tion model training time with 500 clusters from 20 hours using mkcls (of which 60% of the time is spent on clustering) to just 8 hours (of which 5% is spent on clustering). In this article we have presented improvements to the predictive exchange algorithm that address long-standing drawbacks of the original algorithm com-pared to other clustering algorithms. Bidirectional models, lambda inversion, and cluster refinement produce better word clusters, as we showed in sev-eral two-sided class-based LM experiments. On these large datasets the quality of the resulting clus-ters is better than predictive exchange clusters and Brown clusters, and approaches the stochastic ex-change clusters produced by mkcls , which takes 35 X 85 times longer.

We also improved upon the speed of the algorithm by cluster refinement and entropy term precalcula-tion. MT experiments showed that word alignment models using ClusterCat fully match those using mkcls in B LEU scores, with time savings found by using ClusterCat. The software, as well as additional compatibility and visualization scripts, are available under a Free license at https://github.com/ jonsafari/clustercat .
 We would like to thank Hermann Ney, Kazuki Irie, and the reviewers. This work was supported by the QT21 project (Horizon 2020 No. 645452).
