 Learning from structured data has recently attracted a great deal of attention within the machine learning community ([1]). The reason for this is that it is in general hard to represent most of real world data as a flat table. Recently it has been also realized that one strength of the kernel-based learning paradigm is its ability to support input spaces whose representation is more general than attribute-value ([2, 3, 4]). The latter is mainly due to the fact that the proper definition of a kernel function enables the structured data to be embedded in some linear feature space without the explicit computation of the feature map. The main advantage of this approach is that any propositional algorithm which is based on inner products can be applied on the structured data.
 First we propose a novel database oriented approach and define our algorithms and op-erations over relational schema where learning examples come in the form of intercon-nected relational tables. There exists a single main relation each tuple of which gives rise to a relational instance that spans through the relations of the relational schema. Second we define a family of kernel functions over relational schemata which are gen-erated in a  X  X yntax-driven X  manner in the sense that the input description specifies the kernel X  X  operation. We show that the resulting kernels can be considered as kernels defined on typed, unordered trees and analyze their formal properties. In this paper we concentrate on the instance-based learning paradigm and we exploit these kernels to de-fine a relational distance, however since the kernels are valid in terms of their mathemat-ical properties any kernel-based algorithm could be used. Finally we report the results of an instance-based learner on a number of standard relational benchmark datasets. Consider a general relational schema that consists of a set of relations R = { R } . Each tuple, R i , of a relation R represents a relationship between a set of values { R ij } of the set of attributes { R j } related via R .The domain , D ( R j ) , of attribute R j is the set of values that the attribute assumes in relation R . An attribute R j is called a potential attribute X i of relation X is a foreign key if it references a potential key R j of relation R and takes values in the domain D ( R j ) in which case we will also call the R j a referenced key . The association between R j and X i models one-to-many relations, i.e. one element of R can be associated with a set of elements of X .A link is a quadruple of the form l ( R, R k ,X,X l ) where either X l is a foreign key of X referencing a potential key R k of R or vice versa. We will call the set of attributes of a relation R that are not keys (i.e. referenced keys, foreign keys or attributes defined as keys but not referenced) standard attributes and denote it with { S j } . The notion of links is critical for our relational learner since it will provide the basis for the new type of attributes, i.e. the instance-set type that lies in the core of our relational representation. links l ( R, R k ,X,X f k ) in which R k is referenced as a foreign key by X f k of X .We will call the multiset of X relations, denoted as L ( R, R k ) { 1 } ,the directly dependent relation of R for R k .By L ( R, )=  X  k L ( R, R k ) we note the list of all links in which one of the potential keys of R is referenced as a foreign key by an attribute of another relation. Similarly for a given foreign key R f k of R , L  X  1 ( R, R f k ) will return the link l ( R, R f k ,X,X k ) where X k is the potential key of X referenced by the foreign key R f k . We will call relation X the directly referenced relation of R for R f k and denoted  X  R , and by L  X  1 ( R, ) { 1 } the corresponding multiset of relations to which these foreign keys refer.
 the main relation , M . Then one of the attributes of this relation should be defined as the class attribute, M c , i.e. the attribute that defines the classification problem. Each instance, M i ,ofthe M relation will give rise to one relational instance , M + i , i.e. an instance that spans the different relations in R . To get the complete description of M + i one will have to traverse possibly the whole relational schema according to the associ-ations defined in the schema. More precisely given instance M i we create a relational instance M + i that will have the same set of standard attributes { S j } and the same values for these attributes as M i has. Furthermore each link l ( M, M k ,R,R f k )  X  L (
M, )  X  L  X  1 ( M, ) adds in M + i one attribute of type instance-set .Thevalueofan attribute of type instance-set is defined based on the link l and it will be the set of in-stances (actually also relational instances) with which M i is associated in relation R when we follow the link l (these are retrieved directly by a simple SQL query). By re-cursive application of this procedure at each level we obtain the complete description of the relational instance M + i .
 ture whose root contains M i . Each node at the second level of the tree is an instance from some relations R  X  X  related via a link l ( M, M k ,R,R f k ) with instance M i .In the same way nodes at the d level of the tree are also instances from a given relation. Each of these instances is related with one of the instances found in nodes of the d  X  1 level. In other words M + i is a tree where each node is one tuple from one of the relations that are part of the description of the relational instance and the connections between the nodes are determined by the foreign key associations defined within the relational schema. This means that the resulting tree is typed (i.e. each node is of a given type determined by one of the relations within the relational schema) and unordered (i.e. the order of instances appearing as children of a given node is not important). To limit the size of the resulting tree and to make the computation of our algorithms less expensive we sometimes prune the tree to a specific depth d .
 given relational instance can easily produce self replicating loops. In order to avoid that kind of situation we will have to keep track of all the instances of the different relations that appear in a given path of the recursion; the moment an instance appears a second time in the given recursion path the recursion terminates.
 problem that should be tackled by any relational learning algorithm that could exploit the relational structure that we have sketched thus far. A kernel is a symmetric function k : X  X  X  X  R , where X is any set, such that for all embedded with an inner product, actually a pre-Hilbert space.
 space X be a vector space  X  X t can be any kind of set which we can embed in the feature space  X  via the kernel. This property allows us to define kernels on any kind of struc-tures that will embed these structures in a linear space. The attractiveness of kernels lies in the fact that one does not need to explicitly compute the mappings  X  ( x ) in order to compute the inner products in the feature space.
 k use in our experiments. 3.1 Kernels on Relational Instances In order to define a kernel on the relational instances we will distinguish two parts, R s ,R i set , in each relational instance R i found in a relation R . R i s denote the vector of standard attributes { S j } of R ,let D s = |{ S j }| ; R i set denotes the vector of attributes that are of type instance-set and for a relation R are given by L ( R, )  X  L  X  1 ( R, ) ,let D X lational kernels: direct sum kernel ( k  X  ( ., . ) ) and the kernel which is derived by direct application of the -Convolution kernel ( [2]) on the set X ( k ( ., . ) ). Since these ker-nels are defined over multi-relational instances they are computed following the same recursion path as the retrieval of a multi-relational instance.
 is itself a kernel, [5], which would give the following kernel on the set X (if |{ S j }| =0 ) k type of elementary kernel defined on the set { S j } of the standard attributes of R and K then the kernel defined over standard attributes vanishes and we obtain k  X  ( R i ,R j )= defined on these attributes. In order to factor out that effect among different relations we use a normalized version of k  X  (if |{ S j }| =0 ) defined as: If |{ S as described in [2]. The main idea in the -Convolution kernel is that composite objects consist of simpler parts that are connected via a relation . Kernels on the composite objects can be computed by combining kernels defined on their constituent parts. Let x  X  X be a composite object and x = x 1 , ..., x parts. Then we can represent the relation x are the parts of x by the relation on the set X 1  X  X 2  X  ...  X  X D  X  X where ( x ,x ) is true iff x are the parts of x .Let  X  1 ( x )= { x : ( x ,x ) } , a composite object can have more than one decomposing possibilities. Then the -Convolution kernel is defined as: Since we defined only one way to decompose a relational instance R i the sum in the equation 2 vanishes and we obtain the product of kernels defined over attributes of tributes are present). In case |{ S j }| =0 the resulting -Convolution kernel is de-of k ( ., . ) is affected by the number of attributes that are of type instance-set so we opted for the following normalization version of the kernel: periment and on which we are going to base our distance computations. Having a kernel it is straightforward to compute the distance in the feature space  X  in which the kernel computes the inner product as d (  X  ( x ) , X  ( y )) = K ( x, x )  X  2 K ( x, y )+ K ( y, y ) . as a tree-like structure where each node is one tuple from one of the relations and con-nections between nodes are determined by the foreign key associations. This makes the relational kernel a kernel over trees. The input trees are typed which results in the defi-nition of a graded similarity. The similarity of nodes of different type, i.e. nodes coming from different relations, is zero. The similarity of nodes of the same type is determined on the basis of the attributes found on the relation associated with the given type. At the same time input trees are unordered which means that the order of comparison of the descendants is not important, the only constraint being that only descendants of the same type can be compared. In other words the subtree comparison is meaningful only between subtrees that are rooted on nodes that come from the same relation. 3.2 Kernels on Sets To complete the definition of the kernel on the relational structure we define here a kernel over sets of instances by exploiting the -Convolution kernel from equation 2 (we put be x  X   X  1 ( x )  X  x  X  x ). Consequently we obtain the cross product kernel K The computation of the final kernel is based on recursive alternating applications of K ble to cardinality variations; sets with larger cardinality will dominate the solution. This leads us to the issue of normalization of the cross product kernel, so that we obtain: where f norm ( x ) is a normalization function which is nonnegative and takes non-zero values. Different choices of f norm ( x ) give rise to different normalization methods, [6]. By putting f norm ( X )= | X | we obtain the Averaging normalization method ( k  X  A ( ., . ) ). Defining f norm ( X )= k set ( X, X ) we get the Normalization in the feature space ( k of the feature space can be constructed, Section 3.3. 3.3 Feature Space Induced by Relational Kernels far we will specify the feature space associated with them. We start with the defini-tion of the feature space induced by the cross product kernel defined in Section 3.2. Lets assume  X   X  | (i.e.  X   X  or  X  ) is an embedding function into a feature space F
 X  | ( F  X  or F ) for the kernel K  X  | ( K  X  or K ) on the right hand of the defi-is easy to show that the feature space induced by this kernel is given by  X  set ( X )= that this normalization method amounts to computing the inner product, in the fea-ture space induced by the elementary kernels, between the two centroids of the cor-responding sets. In case f norm ( X )= K set ( X, X ) the feature space is given by  X  angle between the two normalized resultants of the vectors of the two sets.
 Convolution ( K ) kernels. Lets assume that  X   X  (  X  ) is an embedding function into a feature space F  X  ( F ) for kernel K  X  ( K ). Let also  X  s , X  set 1 ,..., X  set | D ding functions into feature spaces F s ,F set 1 ,...,F set | D k set | D set | which constitute the K  X  and K kernels, respectively. It is easy to show that F  X  denotes the direct sum and  X  denotes the tensor product of vector spaces. In other words the F is constructed by computing all the possible products of all the dimen-sions of its constituent spaces, where each product becomes a new dimension of F . In contrast the F  X  is constructed by a simple concatenation of the dimensions of its resentation induced by the relational kernel one has to recursively combine the feature spaces induced by the kernel on sets and the direct sum or the -Convolution kernels. the -Convolution kernel, F , should be more difficult than in this induced by the direct sum kernel, F  X  . This is because the dimensionality of F is much higher than F
 X  (this holds if the elementary kernels induce a feature space of finite dimensionality, otherwise they are both of infinite dimension). On the other hand the -Convolution kernel is more expressive since it accounts for feature interactions. 3.4 Time Complexity Here we analyze the time complexity of the relational kernel defined above. Let TrI = {
TrI 1 ,TrI 2 ,...,TrI n } be a set of tree representations of the relational instances in a given relational schema. Let also TrR be a tree representation (with the depth d ) of the relational schema at the  X  X elation X  level where each node is a relation and the connections between the nodes are again determined by the foreign key associations. In case there are loops in the relational schema the depth is limited to an arbitrary value so that a valid tree is constructed. It is worth noting that depths of each tree in TrI are at most d . Having defined TrI and TrR let BF I be the maximal out-degree of all nodes in all trees in the set TrI while BF R be the maximal out-degree of all nodes in the TrR .It is easy to show the computation of the relational kernel between two tree representation of relational instances is proportional to O (( BF I 2 ) d  X  1 )= O ( BF I 2( d  X  1) ) (here we assume that the root of a tree is at level 1). The overall time complexity is proportional to O ( BF R d  X  1 BF I 2( d  X  1) ) . This complexity is dominated by BF I since BF R &lt;&lt; BF I . This is the pessimistic estimate of the time complexity and more accurate would be acquired if the average branching factors were used.
 O A and B are two finite sets, the elementary kernel is polynomial elementary kernel with the exponent p (without the bias towards lower order monomial) and input space is R N . This means that for sets with high cardinality and for low values of N and p it is bet-ter to explicitly map the instances to the feature space and compute the inner product there. We will compare the selected kernel-based distance measures on a number of relational problems: musk -version 1, diterpenes and mutagenesis. In the diterpene dataset [8] the goal is to identify the type of diterpenoid compound skeletons given their 13 C-NMR -Spectrum. The musk dataset was described in [9]; here the goal is to predict the strength of synthetic musk molecules. We worked with version 1 of the dataset.The Mutagenesis dataset was introduced in [10]. The application task is the prediction of mutagenicity of a set of 230 aromatic and heteroaromatic nitro-compounds. We worked with the  X  X egression friendly X  version of the dataset. We defined two different versions of the learning problem. In version 1 the examined compounds (in the main relation) consist of atoms (in the atom relation) which constitute bonds (in the bound relation). The recursion depth was limited to four. In version 2 the compounds consist of bonds while bonds consists of atoms and the recursion level was limited to three. In both versions the recursion depth was limited because of the time complexity of the algorithm. All the results are given in table 1.
 reduced to computing kernels on sets of vectors requiring thus no recursion. In these cases the K  X  ( ., . ) and K ( ., . ) relational kernels are equivalent (up to a normalization term) so we report results only for the former. In the mutagenicity problem it will be possible to move beyond a single level comparison of the instances and have many levels of recursion. We report results for different set normalization schemes; the sub-script A will denote averaging and the subscript FS feature space normalization. Here we give results for p = { 2 , 3 } ,a =1 (normalized polynomial kernel k P p,a ( ., . ) ) and the effect of different elementary kernels, the effect of different kernel set normaliza-tions as well as the relative performance of the K  X  ( ., . ) and K ( ., . ) kernels. We will experiment with a single nearest neighbor.
 statistical significance of observed differences using McNemar X  X  test (sig. level=0.05). We also establish a ranking schema of the different kernel-based distance measures, based on their relative performance as determined by the results of the significance tests, as follows: in a given dataset if kernel-based distance measure a is significantly better than b then a is credited with one point and b with zero points; if there is no significant difference then both are credited with half point.
 To compare the different elementary kernels we fix a dataset and average the ranks of k
P and k G , ignoring their parameter settings. There is an advantage of the polynomial over the Gaussian RBF elementary kernel for diterpenes dataset -the average rank of polynomial kernels is 5.5 (for Gaussian RBF 1.5). For both formulations of mutagenesis and musk 1 the average rank of polynomial kernels is 3.5 (3.5).
 an influence on the final results. For diterpenes Averaging had an average rank of 3.625 over the different elementary kernels and Feature space normalization an average rank of 3.375. For musk 1 the corresponding figures were 3.5 and 3.5. One explanation for this might be that the two denominators in the explicit feature space representations of the normalized relational kernels from Section 3.3 are correlated, which makes sense since sets of higher cardinality will have probably a higher x  X  X  X   X  | ( x ) ,atleast for the datasets we examined.
 K ( ., . ) . Here again it did not have a big influence on the final results: for both for-mulations of the mutagenesis problem K  X  ( ., . ) and K ( ., . ) had an average rank of 3.5. This is rather surprising since as we have seen before instance-based learning in the space induced by the R-Convolution kernel should be harder than in the space in-duced by the direct sum kernel. However the R-Convolution kernel is more expressive than the direct sum kernel because it accounts for feature interactions. The trade-off between hardness of learning in space of higher dimensionality and the higher expres-siveness might explain similar performance of the -Convolution and the direct sum kernel.
 tems we give the best results reported in the literature on the same benchmark datasets. All the results denote the accuracy and all have been estimated with ten fold cross-validation. The best result for the musk 1 dataset is 92.40 % (IAPR algorihtm) and it was reported in [9]. In comparison our best kernel gave 88.04 % of accuracy. For the diterpenes dataset the best accuracy was achieved using the DeS algorithm which comes from [3]. The authors got 97.10 % whereas our best kernel gave 91.75 % of accuracy. For the mutagenesis dataset we obtained 85.11 % of accuracy while the best result from the literature was 90.4 % and was taken from [11].From the results reported above we can see that our kernel-based learner compares favorably with the results achieved by special-purpose algorithms applied to structured data. One of the first systems exploiting the concepts of relational algebra and foreign keys was MIDOS , [12]. However [12] is focused on the KDD subgroup discovery task. mentioned in Section 3. To our best knowledge our kernel is the first time the original -Convolution kernel, [2], was applied to the type of relational structures we considered here.
 ods to different types of structured data e.g. sets, trees, graphs, lists. The representation formalism used was that of typed  X  -calculus. The representation framework allows for the modeling of arbitrary complex objects which however is not at all a trivial task. Under this framework the authors explicitly defined kernels on sets and multisets. In [3] elementary (atomic) kernels are defined for each attribute separately, while our kernel assumes elementary kernels defined on the level of relations thus treating relations as indivisible objects. Besides of this in [3] a kernel over tuples of objects is always de-fined as a direct sum of its constituent parts whereas in our framework one is able to use either the direct sum kernel or the -Convolution, which have different representational powers.
 lution kernels where instances are considered to be labeled ordered directed trees. The idea of these kernels is based on the notion of a number of common subtrees in a tree i.e. the kernel function is the inner product in the space which describes the number of occurrences of all possible subtrees. The main difference between [4] and [13] is that the former is applicable only to trees where no node shares its label with any of its siblings. [13] overcomes this limitation by defining the substructures of a tree as a tree such that there is a descendants order preserving mapping from vertices in the sub-structure to vertices in the tree. There are two main differences between our kernel and kernels defined in [13]. First the trees considered in [13] are labeled trees, i.e. each node is characterized by a discrete label so two nodes are either the same or different, there is no graded similarity. In our case however nodes are not labeled but typed which results in the definition of a graded similarity. Second the trees in [13] are ordered whereas in our case there is no order restriction, the only restriction imposed is that comparison is performed only between subtrees rooted at nodes of the same type, i.e. same relation, and only descendants of the same type can be compared.
 In this paper we proposed a kernel based relational instance based learner which, con-trary to most of the previous relational approaches that rely on different forms of typed logic, builds on notions from relational algebra. Thus we cover what we see as an im-database community. We concentrated here on the instance-based learning paradigm however our kernel can be plugged to any kernel-based classification algorithm. in the sense that we define a kernel on a composite object by means of kernel on the parts of objects. On the other hand our kernels could be also seen as being defined over typed and unordered trees. Since in other areas of computational biology many prob-lems can be described using similar structures we believe that our kernel could also useful there.
 type of attributes i.e. the instance-set type. We believe that there is still a lot to be gained in classification performance if more refined kernels are used for this type of attributes. We have followed a rather simple approach where the kernel between two sets was simply the sum of all the pairwise kernels defined over all the pairs of elements of the two sets. A more elaborate approach would take into account only the kernels computed over specific pairs of elements based on some mapping relation of one set to the other defined on the feature space.

