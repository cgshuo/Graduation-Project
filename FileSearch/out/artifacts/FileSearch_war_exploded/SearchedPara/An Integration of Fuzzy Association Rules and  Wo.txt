 With the rapid growth of text documents, document clustering has become one of the main techniques for managing large document collections [2]. Several effective document clustering algorithms have been proposed including the k -means, Bisecting k -means Algorithm, Hierarchical Agglomera tive Clustering (HAC), Unweighted Pair Group Method with Arithmatic Mean (UPGMA), etc. However, there still exist some challenges for the clustering quality [1][4][6], such as (1) have high-dimensional term user to specify the number of clusters as an input parameter, which is usually unknown in advance, (4) do not provide a meaningful label (or description) for cluster, and (5) do not embody any external knowledge to extract semantics from texts. 
To resolve the problems of high dimensionality, large size, and understandable algorithm, namely Hierarchical Frequent Term-based Clustering (HFTC), where the frequent itemsets are generated based on the association rule mining, e.g., Apriori [3]. experiments of Fung et al. [4] showed that HFTC is not scalable. For a scalable algorithm, Fung et al. proposed a novel approach, namely Frequent Itemset-based document clustering can reduce the dimension of a vector space effectively. Yu et al. [14] presented another frequent itemset-based algorithm, called TDC, to improve the clustering quality and scalability. This algorithm dynamically generates a topic directory from a document set using only closed frequent itemsets and further reduces dimensionality. Recently, WordNet [10], which is one of the most widely used thesauruses for [8]. Many existing document clustering algorithms mainly transform text documents features for document representation and subsequent clustering. However, synsets would decrease the clustering performance in all experiments without considering word sense disambiguation. Accordingly, Hotho et al. [6] used WordNet in document clustering for word sense disambiguation to improve the clustering performance. 
In order to flexibly conduct the association rule mining for more applications, some research works [5][9] have been proposed to integrate fuzzy set theory and understandable to humans because it integrates linguistic terms with fuzzy sets. Thus, we propose an effective Fuzzy Frequent Itemset-based Document Clustering (F 2 IDC) approach based on fuzzy association rule mining in conjunction with WordNet for appropriate topic labels for the derived clusters. Moreover, we show how to take into account hypernyms of WordNet for capturing co nceptual similarity of terms, and add these hypernyms as term features for the document representation. 
In summary, our approach has the following advantages: 1. It presents a means of dynamically deriving a hierarchical organization of 3. By conducting experimental evaluation on Reuters-21578 dataset, it has been our approach with an example is presented. The experimental evaluation is described and the results are shown in section 4. Finally, we conclude in section 5. f )}, is represented by a set of pair (term, frequency) where the frequency f ij represents the occurrence of the key term t j in d i .
 { t of T D , including only meaningful key terms, wh ich are not appeared in a well-defined terms, respectively. clusters. A candidate cluster (,) document set D , such that it includes those documents which contain all the key terms in  X  = { t To illustrate, c can also be denoted as {trade}), as the term  X  X rade X  appeared in these documents. 
For assigning each document to a fitting cluster, each candidate cluster fuzzy frequent itemset  X  are considered in the clustering process.  X  will be regarded as Matrix (DCM) will be constructed to calculate the similarity of terms in d i and q formal illustration of DCM can be found in Fig. 1. target clusters is defined as follows. where v ix and v iy stand for two entries, such that d i  X  q decide whether two target clusters should be merged. follows: (1) Document Analyzing. After the steps of document preprocessing and enrichment, (2) Fuzzy Frequent Itemsets mining. Starting from the document representation of all (3) Document Clustering. To represent the degree of importance of a document d i in 3.1 Stage 1: Document Analyzing form a term vector. The immediate drawback of a term vector for document clustering clustering inefficient and difficult in principle. Hence, in this paper, we employ tf-idf threshold method to produce a low dimensional term vector. Document Pre-processing. We describe the details of the pre-processing in the following: (1) Extract terms. Divide the sentences into terms and extract nouns as term features. (2) Remove the stop words. We use a stop word list that contains words to be (3) Conduct word stemming . Use the developed stemming algorithms, such as Porter follows:  X  d i , d i  X  D }| is the number of documents containing t j . Document Enrichment. After the step of document preprocessing, we additionally enrich the document representation by using WordNet, a source repository of synsets, together with a hypernym/hyponym hierarchy. 
The basic idea of document enrichment is to add the generality of terms by corresponding hypernyms of WordNet based on the key terms appeared in each document. Each key term are linked up to five levels of hypernyms. For a simple and effective combination, these added hypernyms form a new key term set, denoted K D = { t of 0 will be assigned to several terms appearing in some of the documents but not in d to accumulate as the frequency hf ij of h j . 
The reason of using hypernyms of WordNet is that hypernyms can reveal hidden [12]. For example, a document about  X  X ale X  may not be associated to a document about  X  X rade X  by the clustering algorithm if th ere are only  X  X ale X  and  X  X rade X  in the key term set. But, if the more general term  X  X ommerce X  is added to both documents, their later mining can be derived by Algorithm 1. 3.2 Stage 2: Fuzzy Frequent Itemsets Mining In the mining process, it is considered that documents and key terms are transactions and purchased items, respectively. In the following, we define the membership functions and present our fuzzy association rule mining algorithm for texts. represented by three fuzzy regions, namely Low , Mid , and High , to depict its grade of range [0, 2], where r can be Low , Mid , and High , and the corresponding membership membership functions are shown in Fig. 3. max ( f ij ) is the maximum frequency of terms in D , and avg ( f ij ) =  X  min ( f ij ) or max ( f ij ), and | K | is the number of summed key terms. The fuzzy association rule mi ning algorithm for texts. Algorithm 2 generates fuzzy frequent itemsets based on pre-defined membership functions and the minimum support value  X  , from a large textual document set, and obtains a candidate cluster set according to the minimum confidence value  X  . Since each discovered fuzzy frequent importance that the itemset contributes to the document set. 3.3 Stage 3: Document Clustering The objective of Algorithm 3 is to assign each document to the best fitting cluster q i c , and finally obtain the target cluster set for output. The assignment process is based on the Document-Cluster matrix (DCM) derived from the Document-Term Matrix (DTM) and the Term-Cluster (TCM). For avoiding low the clustering accuracy, the clusters is calculated to merge the small target clusters with the similar topic. 3.4 An Example each entry denotes the frequency of a key term (the column heading) in a document d i (the row heading), to make our presentation more concise. This representation scheme will be employed in the following to illustrate our approach. the membership functions defined in Fig. 3, the minimum support value 70%, and the minimum confidence value 70% as inputs. The fuzzy frequent itemsets discovery procedure is depicted in Fig. 5. Moreover, consider the candidate cluster set Algorithm 3 and shows the final results. We have experimentally evaluated the performance of the proposed algorithm by comparing with that of the FIHC method. We make use of the FIHC 1.0 tool 1 to evaluation program to ensure a fair comparison. Moreover, Steinbach et al. [13] have indicated that UPGMA and Bisecting k -means are the most accurate clustering algorithms. Therefore, we compare the performance of our algorithm with that of FIHC, HFTC, and UPGMA algorithms in term of the clustering accuracy. 4.1 Data Sets these datasets is described as follow: 2. Reuters : The documents in Reuters-21578 are divided into 135 topics mostly our experiments. 1. Its domain is not specific. 2. It is freely available for download. 3. It is not specially designed to combine with WordNet for facilitating the clustering 4. It had been compared in the experiments of the FIHC method. 4.2 Clustering Evaluation documents is pre-defined into a single cluster. doc ( c j ) is the number of documents in 
Fung et al. [4] measured the quality of a clustering result C using the weighted sum of such maximum F-measures for all natural clusters according to the cluster size. Let | D | denote the number of all documents in the documents set D . This measure is called the overall F-measure of C, denote F ( C ) by Formula (7): 
In general, the higher the F ( C ) values, the better the clustering solution is. 4.3 Results Table 1 presents the obtained F-Measure values for F 2 IDC, FIHC, HFTC, and UPGMA by comparing four different numbers of clusters, namely 3, 15, 30, and 60. The results show that our algorithm, F 2 IHC, outperforms other algorithms in terms of accuracy, specifically on the Re0 dataset. Although numerous document clustering methods have been extensively studied for many years, there still exist several challenges for increasing the clustering quality. In considering hypernyms of WordNet in the process of document clustering is to extract individual clusters. Moreover, our experiments show that the proposed algorithm has better accuracy quality than FIHC, HFTC, and UPGMA methods compared on Reuters-21578 dataset. 
