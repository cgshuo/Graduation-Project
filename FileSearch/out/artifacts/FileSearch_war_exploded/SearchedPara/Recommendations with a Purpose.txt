 The purpose of recommenders is often summarized as  X  X elp the users find relevant items X , and the predominant opera-tionalization of this goal has been to focus on the ability to numerically estimate the users X  preferences for unseen items or to provide users with item lists ranked in accordance to the estimated preferences. This dominant, albeit narrow, view of the recommendation problem has been tremendously helpful in advancing research in different ways, e.g., through the establishment of standardized evaluation procedures and metrics. In reality, recommender systems can serve a vari-ety of purposes from the point of view of both consumers and providers. Most of the purposes, however, are signif-icantly underexplored, even though many of them are ar-guably more aligned with the real-world expectations for rec-ommenders than our current predominant paradigm. There-fore, it is important to revisit our conceptualizations of the potential goals of recommenders and their operationalization as research problems. In this paper, we discuss a framework of recommendation goals and purposes and highlight pos-sible future directions and challenges related to the opera-tionalization of such alternative problem formulations. Foundations of Recommender Systems; Recommendation Goals and Purposes Automated recommendations have become a pervasive part of the daily user experience on the web. Today, many major e-commerce websites, media streaming platforms, and social networks use a part of the user interface to display recom-mendations to their users [3]. The overarching goal for such recommendation services is to create some sort of utility (or benefit), e.g., provide users with relevant information, im-prove customer retention, increase revenue. A number of studies show that recommenders can measurably influence the behavior of online consumers [2, 5].
 the context of which an evaluation is made) are indirectly described with the help of abstract tasks of a recommender. The tasks include  X  X nnotation in Context X ,  X  X ind Good Items X ,  X  X  X ind All Good Items X ,  X  X ecommend Sequence X , and  X  X ust Browsing X . Annotation in Context corresponds to the rating prediction problem, and Find Good Items to item ranking. Find All Good Items is a special case where the goal is not to miss any relevant item, and Recommend Sequence is a task that is, for example, common for next-track music recommendation. Just Browsing finally refers to situations where there is no imminent  X  X uying decision X  on the user X  X  side and other factors than accuracy, e.g., user interface aspects or content-richness, are important.
To evaluate the performance for a number of aforemen-tioned tasks, mainly a variety of standard accuracy measures are used. In recent years a number of additional ways to quantify recommendation quality aspects beyond accuracy have also been more widely used, including different forms of assessing the diversity, novelty, or serendipity of the recom-mended items. The Recommend Sequence task typically re-quires specific algorithms and datasets, e.g., for next-basket recommendation, but can also be evaluated with accuracy measures like recall, while measuring the success of a system in support of the Just Browsing task is underexplored [4]. A question that is rarely asked explicitly in recommender systems research, is: What is a good recommender system? (Or: What is a good recommendation? ) According to to-day X  X  established task-oriented framework for evaluating al-gorithms, one could argue that the answer to this question is straightforward: a good system is one that provides a low RMSE or whatever other computational metric (like MAE, F-measure, NDCG, MAP, hit rate) researchers decide to ap-ply for their problem. The choice of the metric(s) ideally should be aligned with an operational system task that the proposed algorithm is being designed for.

For example, when addressing the rating prediction prob-lem, metrics like RMSE or MAE are used, and when ad-dressing the item ranking problem, NDCG or MAP are em-ployed. A general observation is that the vast majority of recommender systems research takes an operational perspec-tive on recommendation by focusing on one of the standard, well-defined operational system tasks.

As mentioned in Section 1, having standard operational tasks (such as rating prediction using user-item matrix com-pletion approaches) provides a lot of benefits to the research community. At the same time, it is important to note that operational system tasks often represent a significant over-simplification of real-world situations. For example, the op-erational task  X  X ind good items X  on a movie streaming web-site may correspond to very different recommendation pur-poses . 1 Furthermore, it is likely that these purposes may differ dramatically depending on whether the recommender system X  X  overarching goal is considered from the consumer X  X  or the provider X  X  point of view. In particular, the  X  X ind good items X  task on a movie streaming website can be representa-tive of having the  X  X ntertainment X  (or  X  X atisfying emotional experience X ) purpose from the consumer X  X  viewpoint, while the same  X  X ind good items X  task might represent the  X  X ncrease user engagement X  purpose from the provider X  X  viewpoint.
Th is has a couple of important implications. First, the consumer X  X  and provider X  X  purposes are not always aligned. In this example, more movie watching (good for provider) may not necessarily mean more satisfaction (good for con-sumer). Therefore, the two viewpoints may often need to be modeled and evaluated separately. And second, the tradi-tional computational metrics that are associated with stan-dard operational tasks  X  e.g., the NDCG ranking metric for the  X  X ind good items X  task  X  may be reflective of neither the consumer X  X  nor the provider X  X  viewpoint. For example, bet-ter preference-based ranking, as measured by NDCG, does not automatically guarantee more consumption/engagement nor more satisfaction. Thus, it is important for the recom-mender systems community to (a) move beyond the stan-dard operational perspectives and explicitly take real-world strategic perspectives (purposes and goals) into account, and also to (b) design a more comprehensive set of operational system tasks and corresponding computational metrics for different recommendation purposes and viewpoints. To summarize, we propose the following framework for un-derstanding and evaluating recommender systems utility as shown in Table 2:  X  Overarching goals : Goals describe the general underlying personal or organizational motivation of using or provid-ing a recommender system.  X  Recommendation purposes : Purposes capture the specific utility of the service for a consumer or provider, i.e., the expected measurable effects or value of deploying the sys-tem. Examples are given in Table 1.  X  System tasks : These represent operational, algorithmic tasks to be accomplished by the recommendation system.  X  Computational metrics : Metrics provide numeric quan-tifications of the extent to which the recommender is able to accomplish a given system task.

As an illustrative example, consider an online music ser-vice provider, whose overarching goal is to increase revenue via improving customer long-term loyalty. One of the possi-ble purposes of the recommendation system could, therefore,
We hope that the proposed framework will help to facil-itate the discussion about revisiting the more foundational aspects related to the purposes and goals of recommender systems and to reconsider and extend the way we opera-tionalize the recommendation problem in academic settings. The framework also points to different possible directions for future work, e.g., in the following areas. From the operational perspective of system tasks and com-putational metrics, a number of general challenges arise when we consider that recommendations can serve multiple pur-poses. More research is needed for understanding potential trade-off situations and how to systematically evaluate them with standardized approaches. E.g., consider the recent stream of research on diversified and serendipitous recom-mendations  X  in most domains higher diversity is achieved only with compromises on ranking accuracy. No clear un-derstanding, however, exists on how much diversity is actu-ally desirable (for a domain) or how large of a compromise on accuracy should be tolerable. As a result, this calls for more standardized multi-metric optimization and evaluation schemes in the context of a specific task.

Whenever new recommendation purposes are addressed and corresponding system tasks are designed, the question arises whether the existing metrics are appropriate to assess the system X  X  ability to accomplish the task. Looking again at the diversity problem, using the average pairwise diver-sity of items using metadata features is one common way to assess the diversity level of a recommendation set. However, even for such a common metric, it is often not clear if it re-flects something that is truly valuable to the consumers for a given recommendation purpose. Whenever new metrics are developed for a new task, it is important to empirically validate, e.g., through user studies or live tests, to which extent the quantitative metric is capable of truly capturing what it should it measure (i.e., the metric-task-purpose fit).
A general dilemma in this context is that certain recom-mendation purposes and corresponding system tasks might be very specific to a given application domain. The chal-lenge is to identify or develop the next wave of metrics that can generalize across different recommendation prob-lems/domains  X  such metrics would provide a standard set of problems for a research community to focus on. Notable examples of highly desirable metrics include metrics that could help predict online recommendation success from of-fline experimentation, since the way online success is mea-sured can often be very idiosyncratic for different domains, purposes, and tasks [2, 3]. Many of the sketched directions for future research require the existence of new evaluation protocols and benchmark data sets, which carry much more information than the snapshots of explicit rating databases that serve as a ba-sis for much of today X  X  research. As an example, consider the RecSys 2015 challenge where the tasks were to predict from click-stream data whether a visitor of an e-commerce site will make a purchase in a given session and which item will be eventually purchased. As this problem setting can be relevant for a number of e-commerce recommendation purposes, it has the potential to be further developed to a standardized system task in our framework. But, general
