 The Movielens dataset and the Herlocker et al. study of 1999 have been very influential in collaborative filtering. Yet, the age of both invites re-examining their applicability. We use Netflix challenge data to re-visit the prior results. In par-ticular, we re-evaluate the parameters of Herlocker et al. X  X  method on two critical factors : measuring similarity between users and normalizing the ratings of the users. We find that normalization plays a significant role and that Pearson Cor-relation is not necessarily the best similarity metric. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval, Information Filtering Experimentation
The algorithmic basis for some of the most commonly used and influential collaborative filtering (CF) methods was de-veloped and first evaluated around 1999 using the Movielens data set [3]. From this study, the preferred method 1 was neighborhood-based using Pearson correlation coefficient for the similarity measure, choosing the k most similar users for neighborhood generation, and the  X  X eviation from mean X  method for normalization. A later study [2] recommended deviation from mean or z-score for normalization.
In October 2006, a new collaborative filtering data set was introduced to the CF community: the Netflix Challenge set. This new data set provides an opportunity to re-visit the conclusions about neighborhood-based collaborative filter-ing drawn in earlier studies using the Movielens data. We conducted a factorial study varying the parameters of the HKBR method to assess their impact on performance for Movielens as well as subsets of the Netflix challenge set. In particular, we study parameter choices for similarity metrics and normalization schema and examine differences between the Movielens and Netflix data sets to establish the gen-erality of results, gauge the appropriateness of the HKBR We will refer to this as  X  X KBR method X  after the authors. method as a baseline in comparative studies, and examine the effects of the data sets.
Given similarity measure s , neighborhood selection method h , normalization scheme n , and ratings R , the rating user gives item i is predicted with the following algorithm. Predict(a, i, s, n, h, R) 1. R = n(R) # Normalize R according to n 2. Generate users, U s.t. R u,i =0 and u = a,  X  u  X  U 3. For each u  X  U 4. H = h(U, Sim) # Construct the neighborhood 5. return n  X  1 (
Steps 2 -4 are the same as [3]. Steps 1 and 5 differ in how the normalization is applied. These steps allow for nor-malization before computing similarity ( pre-normalization ); HKBR normalized during prediction ( post-normalization ). The variable settings tested were { 25, 50 } for h , { pre and post normalization } for normalization timing, { none, mean-centering ( mc ), unit standard deviation ( usd )andzeromean / unit standard deviation ( zmus ) } for n and { cosine vector similarity, Pearson correlation, Spearman correlation, Man-hattan, Euclidean, L  X  and Hamming } for s .

For our analyses, we set normalization timing and h pa-rameters from the empirical results. In our results, normal-ization timing exerts no effect, except that pre-normalization improves CVS. So, we use pre-normalization. For h , we find that 25 works best for Movielens and 50 for Netflix.
We used two datasets: Netflix ( http://www.netflixprize. com/ ) and Movielens ( http://www.cs.umn.edu/research/ grouplens ). We produced 10 subsets of Netflix following the procedure of the earlier study while trying to match the Movielens set for two of the statistics: number of items (fixed at 1776  X  1) and number of ratings (912552  X  130774). We constructed an additional Netflix subset (Dense) to ex-amine what happens when a key statistic (mean ratings per user) is very high (1279 versus 106 for Movielens and 30 for the subsets). The datasets were divided into training and testing using a method similar to that of [3]. Ten percent of users were randomly selected for the test set; for each such user, five ratings were withheld.
We address three primary questions about the conclusions of the prior studies in our experiments.
The results clearly show interaction effects, especially with the Netflix data subsets. For Movielens, an Analysis of Vari-ance (ANOVA) test to check for interaction between similar-ity and normalization shows significant main and interaction effects when the input includes none and usd normalization, but only a main effect of similarity when none and usd are removed. Pearson and Spearman correlation only perform well on either data set when the data are mean-centered (with mc or zmus ). The results also show that when the users are normalized with mc , CVS performs significantly better than Pearson correlation (t-test on Movielens yields p  X  1 . 23  X  10  X  07 ,onNetflix p  X  0 . 02).
For normalization, Herlocker et al. found no significant difference between mc and zmus [3], but t-tests on our re-sults show that using the pre-normalize method and zmus provides statistically significantly ( p&lt; 10  X  10 ) better perfor-mance than mc or the other normalization schemes.
For similarity, Breese et al. found that Pearson corre-lation performed better than cosine vector similarity [1]. Our results show that on the Movielens data set, measuring similarity with CVS performs significantly better than us-ing Pearson correlation, no matter which normalizer is used. The improvement is very small, but is highly significant.
Surprisingly, we found that Hamming similarity with no normalization outperforms all of the other methods on the Netflix subsets. Yet, this is the combination that performed the worst on the Movielens data. Also, Hamming similar-ity is the only similarity measure that performed better on either data set with no normalization than with the mean-centering scheme.
We find some differences in performance of parameters when tested with Movielens and Netflix data subsets. In particular, for Netflix, For both, generally, normalization improves the performance with zmus as the best scheme.

We used the Dense subset to follow-up on the difference in preferred parameterizations. As one would expect, the MAEs are, with a few exceptions, significantly lower for the Dense subset. The best performing parameters are Co-sine similarity with zmus or mc normalization; Pearson and Spearman similarity with some normalization perform just slightly worse. Hamming similarity performs better on the dense subset than on the random subsets.
The Movielens dataset and the HKBR method have been very influential in comparative evaluation of CF methods as benchmark dataset and baseline method, respectively. We assessed some of the recommendations from earlier studies in light of new alternative parameterizations and a new sub-stantial dataset for the same application. We found that many of the prior recommendations do not generalize to the new dataset and that the parameters exhibit significant, but previously inadequately explored interaction effects. In par-ticular,
We note though that many of the observed differences, al-though statistically significant, are none the less small. Im-proving the mean absolute error by 0.01 on the data sets we used means that the average user will see their movie rating predictions improve by 0.01 stars. Such a small improve-ment may not be noticed by the average user.

Although our observations complicate comparative eval-uation, they also afford opportunities to better understand the evolution of Collaborative Filtering and of what makes the methods successful. [1] J. S. Breese, D. Heckerman, and C. M. Kadie. Empirical [2] J. Herlocker, J. A. Konstan, and J. Riedl. An empirical [3] J. L. Herlocker, J. A. Konstan, A. Borchers, and [4] J.L.Herlocker,J.A.Konstan,L.G.Terveen,andJ.T. [5] J. Wang, A. P. de Vries, and M. J. T. Reinders.
