 With the rapid development of the internet, big data era is coming. Text data is increasing with an explosive rate so it has become difficult to develop an infor-mation retrieval application with high efficiency. The sorting of text data also becomes an essential part, and one of the effective resolution is classification. As a key technology for the natural language processing (NLP), text classifi-cation has been continuously developed and widely concerned. The common methods used in text classification include Naive Bayesian[23], Support Vector Machine[17][25], k-Nearest Neighbor[11], Neural Network[26] and so on. enhance performance of the classifier, it is necessary to improve the quantity and quality of samples. In fact, it is easy to get many unlabeled data. However, marking samples is time-consuming and expensive, which is most likely to lead to bottlenecks.
 hensive learning method with the combination of labeled and unlabeled samples, was developed. With the guidance of the labeled samples and the effective infor-mation of the unlabeled samples, that method could improve the performance of the classifier.
 take full advantage of the extensive knowledge in unlabeled data, so that the text classification is limited to the contents of the original labeled data. This problem causes that all samples are limited in a particular domain. In addition, semi-supervised learning requires a large amount of unlabeled data, but the improper selection of unlabeled data may affect the performance of classification. expand the training samples is more likely to solve the problem of text classifica-tion. Wikipedia that is artificial calibration contains a wealth of articles, and its links between the articles constitute a huge knowledge network. So Wikipedia can assist the text classification to overcome the difficulty of classifying unlabeled samples, reduce the burden and cost of manually annotated samples, and achieve the purpose of improving the generalization and practicality of text classification. Using Wikipedia to make inductive transfer for text classification more effective in, and using Wikipedia to construct a thesaurus of concepts automatically to enhance previous approaches for text classification[1][22].
 extension. The correlation between text and concepts has become an important part of data mining for Wikipedia. By calculating the correlation between con-cepts and the labeled sample, as well as the rich links between concepts, our approach selects appropriate articles from Wikipedia to build an extended sam-ple set. On basis of that, the generalization of the classifier is improved. The experimental results show that the performance of text classification improved after sample expansion.
 and work of establishing the model. Section 3 introduces the structure of the model. Section 4 describes the experimental process and the analysis of results. Section 5 summarizes this article. 2.1 Text Classi cation With the development of information technology, in order to effectively manage and utilize the massive data from Internet, the information retrieval and mining based on content has become an area of concern. Text classification is an im-portant data analysis method in information retrieval, information mining and other areas of research hotspots. Text classification is determining the categories of text automatically. The main goal of its study is to improve classification performance.
 tion technology. Luhn proposed word frequency statistical that is mainly used for automatic classification in this field for a pioneering study. Since then, a large number of researchers began to study the related fields about text clas-sification[8]. Kspark, Salton and Jones and other scholars in this field carried out fruitful research work[20]. Since the 1990s, the automatic text classification technology based on machine learning and statistics has gradually become the mainstream of text classification. They are commonly used in the supervised text classification[2].
 corpus, which causes large workload, time consuming and certain technical de-mands, etc. In order to solve the problems caused by the shortage of training samples, the text classification based on semi-supervised learning came into be-ing. Semi-supervised learning is using a large number of unlabeled data com-bined with a small amount of labeled data to train the model. Shahshahani and Landgrebe begun the research of the semi-supervised learning in 1994. Since then, semi-supervised learning has become a hot topic in the field of research[5]. Chapelle et al. proposed nuclear-based semi-supervised learning[3]. Wajeed pro-posed a semi-supervised text categorization method based on KNN, that method uses different similarity measurements and different vector generation techniques to improve the accuracy of classification[21]. In[12], Li et al. proposed a self-trained SVM algorithm. Pavlinek proposed a semi-supervised LDA text classi-fication method, which used the semi-supervised LDA theme model to find a topic[16].
 massive training sample expansion. In fact, the qualities of sample sets are the key to improve the accuracy of classifiers. Therefore, the appropriate selection of non-marked data, the reasonable labeling and screening mechanism for building training sample candidate sets, is the core of extending training sample and improving the performance of the classifier. 2.2 Correlation Calculation Measuring concepts semantic correlations is the basic work in the field of NLP[9]. In the field of text classification, the semantic correlation of different terms can improve its effect greatly[10]. There are many researches in the calculation of similarity of text, which has been applied in many fields[14].
 formation by the calculation of correlation between words[15]. In[19], Turney proposed a non-guided method of calculating the interpoint information, which can also be applied to the calculation of words correlation.
 of which is the application of Term Frequency-Inverse Document Frequency (TF-IDF) weight calculation and cosine similarity. TF-IDF is a simple, intuitive, fast processing method of text feature weighting, has been widely used in the text processing[20]. Cilibrasi and Vitanyi use Google X  X  search results to calculate correlation[4]. In[18], Michael and Simone first proposed a Wiki Relate algorithm based on the semantic correlation calculation of Wikipedia. In[7], Gabrilovich and Markovitch used the method of explicit semantic analysis to compute word-to-word correlation based on the textual content of Wikipedia entries. Eric and Eneko et al. calculated the word correlation using a typical PageRank method on a Wikipedia chart. David proposed to use pages links to build space vector, and calculate the correlation between words[24]. 3.1 Data Preprocessing The original text needs to be expressed in the form of simple preprocessing. The initial data set is very different in format and text type. In order to achieve better classification results, a series of pretreatments are required, including simplifica-tion of traditional words, word segmentation, remove punctuation, numbers, and stop words. Especially for the complex format of the original text, it is necessary to use regular expressions to filter out them, that is, filter out the reference, comments, external links and other useless information. The tool for simplifica-tion of traditional words is the open source project: Open Chinese Convert. This paper uses the word segmentation tool is an open source tool: jieba. The filter uses a stop word list containing 2112 Chinese words. The stop word list includes functional words (such as virtual words, modifiers, conjunctions, adverbs) and some other words that contain the lowest lexical meaning. At this point, the preprocessing of complex text has been completed. 3.2 Wikipedia Sample Extension(WSE) In order to expand the samples properly, this paper use the initial labeled dataset to mark a large, semantically rich corpus, and select data from that corpus to extend the training set by the appropriate screening mechanism. Articles in Wikipedia are rich and they describe separate concepts clearly. Therefore, it is suitable as a sample extension candidate set. In particular, articles have a large number of links, which contain semantic relations of concepts. Since the links are marked by artificial means, their semantics are less ambiguous. There are thou-sand levels of link relationships, and each article has an average of 34 links. Thus, according to the size and structure of the background knowledge, Wikipedia is a very good dataset for the sample extension based on the correlation. This paper proposes a sample extension method that is based on Wikipedia and correlation calculation. On this basis, combined with the concept links, extract and use the rich links information and articles of Wikipedia to expand samples. The WSE contain four steps: step1, calculate the correlation of every concept in Wikipedia for each class; step2, screen data based on correlation and construct a sample extension candidate set; step3, select the appropriate article corresponding to a concept to expand the training sample; step4, train a better classifier. The roadmap for the sample extension is shown as follows: Correlation Calculation. This paper uses TF-IDF as a basis for calculating the correlation of Wikipedia concepts and sample categories. Specific practices and formulas are illustrated as follows: ( tf idf i ) = T F IDF = ( count ( W ord w ) = ( size ( Document D )) log ( T otalCount ) The formula (1) calculates the TF-IDF for each Wikipedia concept and each document. In the formula, TF indicates the frequency of the word w appears in the document D, which reflecting the importance of word relative to a document. IDF is actually the reciprocal of the DF (Document Frequency), and DF is the total number of documents where the word appears in the entire corpus. The formula (2) calculates the average TF-IDF for each category. Where S is the total number of documents for class A, and T Avg A is the average of TF-IDF for those documents.
 Build a Sample Extension Candidate Set. According to the correlation of Wikipedia concepts and each category, it is necessary to screen out concepts that are representing a category significantly to expand samples. Especially, those concepts, which are not only highly relevant to this category, but also highly relevant to other categories, ought to be excluded. The specific method is to use the formula (3) to calculate the significant value of each Wikipedia concept for each category: T V alue I (I=A, B) In the formula (3), " is set to 0.00001 for avoiding the denominator is 0. The T V alue A represents importance of the concept in category A compared to other categories. The value of T Avg A is larger and the other categories T Avg is smaller, the value of T V alue A is greater, that is, the significance of the concept for category A is also greater. Therefore, it is the better representation of category A. For a concept, ranking its T V alue of all categories, select the most significant category to mark it, and put it into the candidate set of that category. In the formula (4), If I = A, the word is classified as a category A, and its T V alue max is recorded. Finally, according to the appropriate threshold of T V alue , select concepts for each category, and put the articles corresponding to those concepts into the training dataset to complete the expansion of training dataset. The choice of thresholds is based on a large number of comparative experiments in this paper. Wikipedia Sample Extensions with Links (WSE-L). Wikipedia concept links make up a tight complex network structure, and they represent close rela-tionships between concepts. Therefore, based on the WSE, this paper proposes an enhanced sample extension method-Wikipedia sample extensions with links (WSE-L), that is, using rich Wikipedia links to extract the close related articles to expand the sample further. Extract concepts and their links from Wikipedia. Each concept is as a link node, and each link is as a relationship between nodes. So a linked graph of concepts is constructed. For example, the article of  X  X co-nomics X  has a link to the concept  X  X tock X , and then the node  X  X conomics X  has a directed side to the node  X  X tock X . The analysis of rich link relationships shows that there is a high correlation between nodes if they point to each other. We mine new concepts that have relationship in graph with the concept that has been in the original candidate sample set, and add their articles to the extended sample.
 Wikipedia Sample Extensions with Limited Links (WSE-LL). Al-though the links between Wikipedia concepts symbolize the correlation between each other, enhancement of the sample extension based on the links results in a significant increase in the number of training samples set. To this end, we present Wikipedia sample extensions with limited links (WSE-LL). WSE-LL adds a lim-ited condition to the method WSE-L: the new concept must have appeared in the sample extension candidate set got by WSE. For example, for the concept  X  X conomics X , stock X  and it links to each other, and  X  X tock X  appears in candidate set. Then we extract the article of  X  X tock X  into the extended sample set. Finally, under limited conditions, we can achieve a more accurate sample expansion by rich Wikipedia links. 4.1 Experimental Data The training samples in this paper consist of labeled and unlabeled samples. Among them, the labeled samples are from Sina news datasets, the unlabeled samples are from Wikipedia. Web crawler is used to obtain the Sina online news for the category of technology, sports, economic, military, and entertainment. In order to distinguish categories between them and improve the training effect of the model, 300 news are selected for each category as the unlabeled sample set. Wikipedia contains millions of concept pages, and each page contains rich links. Extract all 1188008 concepts, constitute the unlabeled sample set by the corresponding articles, and construct the graph (node, relation) between those concepts. The test dataset is Sohu news and the text classification corpus of Fudan University, including articles of the category for technology, sports, eco-nomic, military, entertainment, and the number of each category is 300. 4.2 Experimental Process Sample Expansion. First, the text dataset is obtained by preprocessing the news datasets. Based on the significant value of the Wikipedia concept calculat-ed by the sample extension algorithm of Section 2, the corresponding concept is marked and added to the candidate set. According to the threshold, the corre-sponding articles of each category are pre-processed and added to the training set. In addition, WSE-L, which is based on the WSE, extracts the articles of in-terrelated concept from the graph, and adds these marked articles to the training sets. Under the condition of the sample extension candidate sets, WSE-LL filters articles with conceptual links, marks them and puts them to the training sets. Training Classi er. Construct the text classification model by the classifica-tion algorithm. In this paper, the classifier is a latent Dirichlet Allocation (LDA) model[13]. The LDA model can classify datasets by implicit subjects in the text, which solves the problem that the semantic similarity measure is not considered in the traditional text classification. By training LDA model, a large number of documents are expressed as a number of subject information. This method uses the full training samples to analyze the related parameters of LDA.
 characteristic frequency to select the characteristic words. The first 1500 words are selected for each category, a unique id is defined for each word and a feature dictionary is built. According to the feature dictionary, the LDA training matrix is calculated. Each row represents a training text in the matrix, and the head of each row is the total number of different words in the LDA dictionary. The matrix is generated in the form  X  X d: count X , and the count is frequency of the word in the text. Each data is separated by spaces. The number of columns in the matrix is equal to the sum of words in the dictionary. The number of rows of the matrix is equal to the sum of texts in the whole training set. Sampling with the configuration of the number of topics as 3(5), the number of probability words in each topic as 50, the number of iterations as n to 1000, the parameters as 0.1, and as 0.01. After the training of LDA model, the relevant parameters, topic and specific keywords for all topics are calculated. 4.3 Evaluation Criteria In this paper, in order to evaluate the performance of the text classification, two general evaluation criteria are used: Accuracy, F-measure. The standard formula are shown in formula (5) and formula (6): Accuracy is the probability that all samples are classified correctly. Recall means the probability that the samples of this class are classified correctly, that is, the ratio of the sum of documents that are classified correctly and the total number of test document sets for this category. Precision is the probability that the classifier classified correctly, that is the ratio of the sum of documents that are classified correctly and the total number of documents that are divided into this category. F-measure is a combination of Recall and Precision reflects the comprehensive indicators. It is the harmonic average of the two. The values of evaluation criteria are between 0 and 1, if their value is closer to 1, its Accuracy, F-measure, Recall and Precision would be higher. 4.4 Results and Analysis Compare our method in this paper with semi-supervised text classification which is based on topic information proposed by Dorado et al. In 2016[6]. Calculate the Accuracy and F-measure for each category, and analyze the performance of the classifier.
 tively, we increased the number of unlabeled training samples and observed the Accuracy trend of the WSE, WSE-L, WSE-LL, and the semi-supervised text classification method. Using Sohu news as a test set, their experimental results are shown in Fig. 2, Fig. 3. In addition, corpus of the Fudan University is also used as a test set, the comparison of Accuracy are shown in Fig. 4.
 the semi-supervised text classification method. When the number of samples reaches a certain value, the Accuracy increased nearly 10%. In addition, the Accuracy of the WSE-LL is still higher in Fig. 3. Observing the percentage of unlabeled training samples, we could determine the threshold as 100%. The excessive expansion of Wikipedia data increases the noise of the classifier. curacy of the WSE is significantly improved. The Accuracy of the WSE-L has been improved obviously, when the number of extended samples is small. The classification per-formance of the WSE-L is better in more comprehensive and smaller corpus.
 kind of news. Fig. 5 shown that F-measure of the method proposed in this paper is relatively stable.
 WSE-LL has great improvements in performance and generalization of the text classification. The analysis of the characteristics of the two different test sets indicated that WSE-LL is more generalized. The approach not only has a good performance of classification in the context of news, but also plays a robust classification when covering journal articles, presentations, and so on. According to the analysis of the data characteristics and F-measure, the  X  X conomic X  class of text is concise and timeliness strong. The WSE-LL makes the classifier more suitable to other scenes. However, for those categories of sample that is highly professional or samples with relatively rich semantics, the improvement of the classifier based our method is not very obvious. Thus, the method in this paper is more suitable for the training sample whose content is limited and the application of the scene is uncertain. This paper calculates the correlation to mark the Wikipedia concepts, and filter the appropriate text to expand the training samples of text classification. We proposed several approaches (WSE, WSE-L, WSE-LL) to expand the samples. The result shows that WSE-LL is superior that supervised learning approaches. The addition of Wikipedia expands the quantity and quality of training sam-ples properly. In particular, use its rich links to enhance the sample expansion effectively. It also shows that the our method improves the performance of text classification, which is a very viable method.
 parallelization techniques to accelerate classification speed. Due to the large amount of corpus, the improvement of computational efficiency is an important aspect in our future research. (2) Considering applying the Wikipedia sample extension to other fields of NLP, such as emotional analysis, etc.

