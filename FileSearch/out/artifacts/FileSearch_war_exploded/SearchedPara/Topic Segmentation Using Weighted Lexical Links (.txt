 This paper presents two new approaches of lexical chains for topic segmentation using weighted lexical chains (WLC) or weighted lexical links (WLL) between repeated occurrences of lemmas along the text. The main advantage of using these new approaches is the suppression of the empirical parameter called hiatus in lexi-cal chain processing. An evaluation according to the WindowD-iff measure on a large automatically built corpus shows slight im-provements in WLL compared to state-of-the-art methods based on lexical chains.
 Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing General Terms: Performance Keywords: Linear topic segmentation, Unsupervised segmenta-tion, Lexical links, Lexical chains.
Linear topic segmentation consists in placing boundaries inside a text at positions where the topic changes. An approach based on lexical chains, introduced in [4], was recently integrated in LC [3]. Results indicate superior performance over state-of-the-art un-supervised methods based on lexical cohesion such as C99 [1] which uses a local ranking of the sentence similarity matrix. This unsu-pervised approach uses word distribution and collocations in the text. This is topic and style independent and, to a certain extent, language independent. Our first proposal considers that the weight can introduce a differentiation between syntactical categories of re-peated words and between lemmas occurring at different density levels, leading to a concept of weighted lexical chains (WLC). Our following proposal is to extend the concept of lexical chains to weighted lexical links (WLL). Lexical links give an adapted weight to each repetition in the text, and can be either bounded or un-bounded in contrast to lexical chains.
A lexical chain links repetitions of a lemma if the gap between two repetitions is lower than a certain value, called the hiatus. A lemma can generate several chains or none, depending on the hiatus and on the repartition of its occurrences. The classical use of this structure is to determine whether the chain of a lemma is active at a given position in a text, i.e. if there is an occurrence of the lemma at a distance (in words or sentences) smaller than the hiatus h .We Figure 1: Lexical chain construction for four lemmas A, B, C and D along a text (vertical lines indicate sentences boundaries) consider that the influence of each lemma covers a least h sentences before and after each occurrence.

Figure 1 illustrates the process of lexical chain construction of a lemma and how lexical chains can occur in a text. The vertical lines indicate sentence boundaries. The hiatus used for this sample is 3 sentences. The occurrences of the lemma  X  X  X  illustrates the impor-tance of the hiatus in determining the boundaries of a chain. The representation of chains of lemmas  X  X " and  X  X " shows that these two lemmas are repeated in two distinct parts of the text that can belong to two different topics. The representation of the two chains of lemma  X  X " illustrates that a single lemma can occur frequently in several distinct parts of the text. This example of 4 chained lemmas also suggests a topic boundary between the last sentence where the lexical chain of lemma  X  X " is active and the first sentence activat-ing the chain corresponding to lemma  X  X ", since there is no active lexical chain at this position.

Intuitively, the nearer the repetitions of a given lemma, the more significant the lemma for the topic. However the classical use of lexical chains is binary: a lexical chain is either active or not. This does not capture the differences between lemma significance. That is why we propose to weight the chains according to their lemma density. Thus, at a given point of the text, a chain will be active to a varying degree rather than the previous binary case. We also hypothesise that this weight can depend on the syntactic or the se-mantic category of the lemma (proper nouns may be more repre-sentative of a topic than other categories). The weight of a chain is computed as : where cat m is the syntactic weight of the lemma, nb m is the num-ber of occurrences of lemma m in the chain, L text the length of the text and L chain is the length of the chain.

The example given for lemma  X  X " in Figure 1 suggests that an adaptive hiatus could break the final link in the chain and increase its weight. The adaptive hiatus is computed with the average dis-tance between each occurrence in the whole text. This states the notion now referred to as weighted lexical chains (WLC). Another option allowed by the weighting scheme is to use no hiatus at all. This means that any chain starts at the first occurrence of its lemma and ends at the last lemma. It also means that there is necessar-ily and only one chain associated to each lemma. As the notion of chains supposes that one knows the maximal length of the links composing it, this new principle is referred to as "weighted lexical link" (WLL).

After processing WLC or WLL, we compute a similarity score between each successive sentence [4]. The score compares the ac-tive lexical chains or links of preceding and following sentences : where w ( X, m ) is the weight of the lexical chain or link C ac-tivated in sentence X, and where C corresponds to lemma m in sentence X given by W ( C, m ) in Equation 1.

Lastly, boundaries are selected between the least similar pairs of sentences where sim(A, B) is lower than the empirically derived threshold : with  X  and  X  are the mean and the variance of all computed similar-ities [3]. K is a parameter that can be empirically fixed or that can be computed dynamically to achieve a user need such as an average segment size.
Our test corpus is composed of 50 composite texts extracted and built from Le Monde, a French newspaper. Each document is com-posed of ten extracts from several articles randomly chosen in dif-ferent thematic categories. The extract size varies between 9 and 11 sentences. The corpus can be considered as a valid reference since the segments composing the documents are chosen in dif-ferent texts to ensure thematic variability. The advantages of this corpus compared to a manually annotated corpus are the amount of data we can test, a cheaper human cost work, and objectivity.
The W indow Dif f measure [5] evaluates the effectiveness of a segmentation tool by comparing, for each document of the cor-pus, the initial boundaries and the boundaries computed by our seg-menter. By moving an analysis window on equivalent sentences in a pair (reference document, hypothesised document), W indow Dif f measures the difference between the number of boundaries between the positions i and i + k in the hypothesised document h and the number of boundaries in the reference document r . Thus, if the difference is null, we can consider that the words at positions i and i + k are locally in the same segments (if the text was reduced to the current analysis window). The short added segments are penalised as well as the others. The measure is the normalised sum of the differences between the numbers of boundaries.
 where b ( x i ,x j ) represents the number of boundaries between po-sitions i and j in text x , and N represents the number of sentences of the text. The weaker the W indow Dif f measure, the more effective the segmenter.

WLC, WLL and LC (classical lexical chains) [3] (as a reference) approaches have been jointly implemented in the LIA _ topic _ seg segmentation tool. The part-of-speech tagger LIA _ tagger the lemmatisation (where the lemmas are of syntactical categories: http://www.lia.univ-avignon.fr/chercheurs/ bechet/download_fred.html verbs, adjectives, nouns and proper nouns) and only keeps lem-mas which occur at least twice. The difference between approaches stands in hiatus determination (pre-defined at 11 sentences for LC, or at higher number of sentences than the largest text (here 120 sen-tences) to simulate no hiatus (WLL), or automatically computed with the average distance between two occurrences (WLC)) and weighting. The weight of each chain (or link) is 1 for LC imple-mentation, and is computed with Equation 1 (with Cat m =1 all lemmas) for WLL and WLC approaches. The results of segmen-tation with LC, WLL and WLC approaches are in the first line of Table 1. These results show that the WLL method performs slightly better than LC.
 Table 1: Window Diff evaluation for segmentation with LC ap-proach (hiatus 11), WLL approach (no hiatus), and WLC ap-proach (the hiatus is computed for each repeated lemma)
We proposed an effective segmentation method which uses the new concept of weighted lexical links (WLL) which is an improve-ment on the lexical chains approach. This method has been im-plemented in the freely available modular tool LIA _ topic _ seg as part of the technolangue OURAL project supported by the French Ministry of Research ( http://projetoural.org/ ).

Future work plans to improve results by determining best weight-ing parameters. Previous experiments which over-weighted named entities has already shown that this adversely affects performance. The weighting equation could include the idf (inverse document frequency) of the lemma. The mutual information with other lem-mas occurring in the same sentence could also improve results [2]. [1] F. Y. Y. Choi. Advances in domain independent linear text [2] O. Ferret. Using collocations for topic segmentation and link [3] M. Galley, K. McKeown, E. Folser-Lussier, and H. Jing. [4] M. A. Hearst. Multi-paragraph segmentation of expository [5] L. Pevzner and M. A. Hearst. A critique and improvement of
