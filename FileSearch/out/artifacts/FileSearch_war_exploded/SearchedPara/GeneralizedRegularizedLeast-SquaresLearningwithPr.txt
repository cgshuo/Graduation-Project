 continuous values, and classication which predicts a class label for an input object. by where V is a con vex loss function, k f k (RKHS) H K induced by a positi ve denite function (a kernel) K k f k 2 K is also called a regularizer .
 linear combination of translates of the kernel K by the training data max (1 yf ( x ) ; 0) corresponds to the classical support vector machines(SVM). from deteriorating for inappropriate kernels.
 results have conrmed the benets brought by the algorithm.
 size of a matrix is sometimes added as a subscript, such as O m ` . Suppose the space H K decomposes into the direct sum: H K = H 0 H 1 ; where H 0 is spanned regularized least-squares (G-RLS) learning model as where P f is the orthogonal projection of f onto H 0 .
 Suppose f is the minimizer of (2). For any f 2 H K , let f = f + g where 2 R and g 2 H K . No w tak e deri vative w.r.t. and notice that @L where h ; i setting g = K P f is the orthogonal projection of f onto H 0 and hence, So (4) is simplied to where orthogonality constraint between P f and f P f , which can be written as or equi valently due to the property of reproducing kernels, (2) equal to zero. an existing trick. 3.1 A kernel construction trick Let' s consider the follo wing reproducing kernel where is any strictly positi ve denite function, and ' 0 ' ; ; ' ` w.r.t. x 1 ; ; x ` , which satises in this paper are given belo w.
 Another property is that the matrix H = ( H ( x i ; x j )) m be used in the computations belo w.
 onto H K which has a subspace H 0 = span ( ' 0 we can see that ' 0 3.2 Computation ten as: onal property between ' 0 To determine the values of ~ = ~ 1 ; ; ~ ` T and ~c = (~ c ` +1 ; ; ~ c m ) T , we need and set the deri vative to zero, and we get we have i.e.
 and ~c by belo w. ( ' 1 = 1 ; ' 2 = x; ' 3 = x 2 ) were used for G-RLS algorithm along with a Gaussian RBF kernel set tak es the values f 1 ; 1 g . The predicted label of any x depends on the sign of (28) criterion has been reported by the empirical results[13 ][14 ][15 ]. the stability of the algorithm. ( accompanied in C4.5 package 2 .
 Figure 2 compares the performance of G-RLSC, RLSC and SVM. It is sho wn that G-RLSC reports while the embedding of pLSA features still impro ves the accurac y. RKHS regularization point of vie w, or investigated for empirical evaluations. The second part of our work presented a practical computation method based on the model. An Ten pLSA features along with a linear kernel were used for G-RLSC. Both bag-of-wor ds (BoW) one-v ersus-all strate gy. SVM used one-v ersus-one strate gy. potentially facilitated.
 applications have conrmed the effecti veness of the algorithm. Ackno wledgments by RGC Earmark ed Grant #4173/04E and #4132/05E of Hong Kong SAR and RGC Research Grant Direct Allocation of the Chinese Uni versity of Hong Kong.

