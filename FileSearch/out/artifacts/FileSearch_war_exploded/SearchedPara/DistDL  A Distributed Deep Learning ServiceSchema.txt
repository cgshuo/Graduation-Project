 Along with the development of data explosion and the coming of big data age, it is possible to distill more informative knowledge from the huge size of data. Deep Learning (DL) is a powerful artificial intelligence tool that enables us to make sense of the story behind data. And it X  X  providing the solution on previ-ously unaddressed problems. In essence, neural networks are revitalised by DL in the big data era, where made possible by massive amounts of information to train them, so that they approach behavioural pattern of human cognition when dealing with various problems.

In order to handle challenging applications with accurate result, the training data sizes are increasing exponentially along with the complexity increase of topology in neural networks. However, the computation overhead of using a large scale deep learning network could be really time-consuming under current single node implementation. A single node is often inadequate since the model and data may be too large to fit in memory. Therefore, it is impossible to satisfy the requirements of real-time computing by single machine in that DL systems are scaling to cluster environments for purpose of harnessing additional memory, CPU/GPGPU processors and storage resources.

These scalability needs arise from at least two aspects: massive data volume, such as the size of Facebook social photos posted per day with up 10TB [1]; and abundant model size, such as the Google Brain deep neural network [2] containing billions of parameters. Many important DL algorithms iteratively update a large set of parameters with 9 to 12 orders of magnitude [3]. Hence, addressing the DL questions is transferring from machine learning field to system problems result in the need to design a scalable application framework. 1.1 Motivation Example In order to accelerate the convergence o f deep neural networking training, the iterative DL algorithm must run in parallel. The traditional way in the single node is to carry out in one machine by leveraging customized heterogeneous hard-ware, for instance GPGPU, FPGA etc whereas the pure hardware solution makes the expenditure increasingly. Adopting the parallel version of multi-threads is another option, but which is of no code reusing and less readable. It urges the scholars to look up efficient distributed frameworks with the large-scale adoption of clusters for DL applications. A naturally desirable goal for distributed deep learning systems is to pursue a system with maximally unleashing the combined computational power in a c luster of any given size .
 There are a good deal of open questions remain for the design of distributed DL training frameworks. Integrated with the MapReduce-compatibled frame-works is an easy thing to think about for our purpose. How to achieve the model synchronization between nodes for the next iteration training need be taken into consideration primarily. Otherwise, MapReduce has no communi-cation API and heavy model transformation cost. Parallel implementations of iterative convergent algorithms are naturally expressed using the Bulk Synchro-nization Parallel [4] model of computation. For example, a typical distributed computing architecture of BSP, HAMA [5] based on Hadoop [6] needs vast syn-chronous overhead. As the Figure 1 shown based on the expansibility test, we could always reduce computation time in a near-linear fashion according by the increasing of the number of machines. But, the communication time increased as number of machines increased. Furthermore, the stragglers X  problem where every transient slowdown of any node can delay all other nodes should be avoided.
Therefore, building a scalable deep le arning training system is the key to unlocking the full potential of the age of big data. It is urgent to explore a novel deep learning service schema to satisfy the requirements of scaling and communication spending. The customi zed acceleration techniques for DL such as cuDN N [7] also enable the opportunity to further improve the performance of deep neural network. 1.2 Main Contributions Motivated by above-mentioned need, we propose in this paper our work DistDL , a scalable and robust framework for deep learning services. DistDL aims to provide Deep Learning as a Schema(DLaaS) for both research and application users. The schemes of our presented distributed deep learning Service DistDL are listed below:  X  DistDL proposed the hierarchical architecture of data partition with the  X  DistDL leverages Bloom Filter to build a Bitmap through encoding the data  X  Since GPUs have been tremendously successful in adding compute horse-Our prototype is rapidly developing based on Spark [8], cuDNN [7], and Cas-sandra [9]. The performance evaluation proved that DistDL is of scalability and efficiency. 2.1 Deep Learning Model A full-connected neural network with 2 hidden layers are illustrated in Fig. 2. There are three layer types to build the deep network, as an input layer whose values are fixed by the input data, hidden layers whose values are derived from previous layers and an output layer whose values are derived from the last hid-den layer. The number of hidden layers and every layers X  neurons can be config-ured with the  X  X inks X  between layers fully or partially depends on the training algorithms details. These adjustable parameters, often called weights, are real numbers that can be seen as  X  X nobs X  that define the input-output function. In practice, most practitioners use a pro cedure called stochastic gradient de-scent (SGD) synchronously [10] or asynchronously [11]. Each layer can only be fully computed after its preceding layer has been completely computed by the feed forward algorithm. The back propagation uses for getting the errors, com-puting the average gradient, and adjusting the weight preparing for the next iterations [12].
 2.2 Proposed System Architecture The proposed frameworks and data flows of DistDL are shown in Fig. 3 (a) and (b) separately with the features of data partition, model parallelism, model updating, consistence control, and fau lt recovery etc. We will give more details later. In the DistDL architecture, these nodes of clus ters are split into three sets of nodes: worker, name and parameter. The DL instances with executable codes are deployed into the worker nodes to train the partitioned data by leveraging GPU resources. The name nodes are responsible for the data metadata manage-ments, job scheduling, parameters exchanging and system load balancing. The model partition is handled by the parameter nodes with push and pull operations to carry out the model updating. The choice between synchronization and asyn-chronism ways to update the weights is a b ig concerning for per formance tradeoff consideration. The entire design of DistDL aims to provide flexible consistency, elastic scalability, and continuous fault tolerance. 2.3 DistDL Data Flowing The training data is divided into several parts by name nodes, then move to the worker nodes as described in Fig. 3 (b). Each work node only handles its own part, sends the results to other workers in every iteration, and obtains updates from other worker nodes through parameter nodes. In the model updating pro-cessing, there are four elements: Weights( w ), Bias( b ), Input ( x ) and Output ( y ). Weights were possessed by each units th eir own and we can increase or decrease the weight by whether they are active or i nactive. The learning algorithm will pick a random training case ( x , y ), then run the neural network on input y , finally modify connection weights ( w ) to make prediction closer to y . This calculation can be mathematically represented as equ ation 1. The novel design for comput-ing in parallel for data ( x ,( y )) and model( w , b ) can accelerate the training rate significantly. Our design has brought out some tricks for the components in our frameworks. The mechanisms of Data Partition, Model Updating, Parameter Servers, and Fault Recovery will be addressed in the following subsections.
 3.1 Data Partition and Locality Conventionally, there exists huge of duplications, including data set and com-puting results between every time X  X  iteration and the re-computing units whose data set only did increment updating as well. Reloading the training data set from Cloud file system, such as HDFS into the local disk is a time-consuming job. Additionally, re-computing the same data set by the same activation functions leads lots of resources X  wasting. In ord er to improve the system efficiently, we make use of the Bloom filter technologies, a simple data structure conceived by Burton Howard Bloom in 1970 [13], to enlarge the data locality and reduce the communication by avoiding the same data transferring. As shown in Fig. 4, when the whole training data is feeding into DistDL , every work node is responsible for 1 /N mini-batch of this iterations training data set. Inside the every node, the hierarchical (3-layer) Bloom filter strategy will help us to achieve better par-allelism. The first layer Bloom filter is taking place between the whole data set in cloud file system and the work node. The second layer comes up between the CPU memory/SSD and hard disk inside the work node. The third one exists between CPU and GPU memory locally.
 3.2 Model Updating and Caching As demonstrated in Fig. 5, we will build a Bitmap generated by Bloom filtering algorithm through encoding the data/computing information in every work node. Each work node could share the Bloom filters with the central coordinator -Akka , an Actor model [14] implementation in Scala which the DistDL is using. Every training data loading/re-computing does essentially the same checking: look at data in  X  X hunks X  (all data is operating in block level) and store only a single copy of each unique chunk locally. Finally, the sole data sets will be saving in the work node persistently in order to reduce the frequent communication between the data nodes and work nodes.

In other words, every work node will build the bitmap based on the initial training data set. This bitmap includes m bits, all set to 0. There must also be k different hash functions defined, each of which maps or hashes the set element to one of the m array positions with a uniform random distribution. This bitmap is to discern the possible replicated data in order to reduce redundant computing/data storing.

When the data-training request is arrived, the data analyzer will calculate the data set by k hash functions, then lookup in the built bitmap to check it is in the local disk. If the data is already stored in this work node, the data loading could be avoided. If not, the data is placed into the local disk and the bitmap updated. The similar checking and bitmap updating operation will happen in the other layers of bloom filters.
 3.3 Parameter Servers and Fault Recovery The parameter server is the key to achieve the model/data parallelism and re-duce the communication cost dramatically. A model consists of two parts, the architecture parts (network topology and related functions) and the parameters. How to positively alternate the model to wards an effective representation is the main mission of deep learning. In the iter ative learning procedure, the parame-ters are accessed and updated at differen t time and locations. Large models are also partitioned and parameters are maintained by the parameter nodes. This component is responsible for user-facing model description, and low latency ac-cess and policy orientated update for parameters.
 Currently all the parameters are stored on the group of parameter nodes. Parameters should be partitioned acros s multiple servers when the parameter size get large. Parameter server client sh ould route requests to different servers accordingly. To get better performance, client can cache the parameter and store updates through Bloom filters Bitmaps. For the purpose of system reliability, it should be restarted on another node when a parameter node crashes. In order to achieve fast fault recovery, Data of a parameter server should be periodically check-pointed so it can be transferred when a server is restarted. When a task is restarted, it should not rerun finished iterations. Big data processing frameworks (BDPF) are of unexpectedly excellent perfor-mance for massive data processing, which are the key for knowledge mining in the big data era. Most BDPFs are based on MapReduce [15] paradigms, which is a programming model for processing and generating large data sets with a parallel, distributed algorithm on a cluster. Our designed DistDL will be com-patible with MapReduce based BDPFs, the implementation will be based on Spark [8] with many advantages regard to Hadoop. Firstly, Spark has embeds the MLlib [18] as the general machine learning library, where the deep learning functions can be inherited conveniently. We provided the two level abstractions: Layer and NeuralNetwork . Layers includes Weights, Bias, Activation, Gradient &amp; Inter-layer communication. The Neural Network are designed by the functions of Topology (Input/Output/Hidden), Iterate, Error and Predict. The parameter nodes cluster is developed with the supporting data Pull/Push operations based on Cassandra [9], an efficient NoSQL database with the well support by Spark community.

In the procedure of data pipeline, Data will be fetched from Cloud file sys-tem, like HDFS in the practical flows, then store into RDD and partition across workers. The role of name nodes is replacing by Spark master node. During each iteration, each worker gets parameters from parameter nodes then computes new parameters based on old parameters and training data in this partition. Finally each worker updates parameters to param eter nodes. Worker communicates with parameter nodes through a parameter daemon, which is initialized in  X  X askCon-text X  of this worker. It enables in memor y data sharing to decrease the overhead of disk I/O, thus less data movement. Additionally, It provides interactive shell for easy interface and has built in fault tolerance to automatically rebuild on failures.

DistDL integrated GPU accelerators into c urrent Spark platform to achieve further calculation acceleration. The u sers can obtain Plugin style design C current Spark applications can choose t o enable/disable G PU acceleration by changing in configuration file. Existing system codes can be easily imported to the heterogeneous platform. We adopted the NVIDIA cuDNN [7] as our GPU accelerating backend. In the point of vi ew users, their code will be reused af-ter the GPU backend scheduler called the corresponding API to carry out the matrix calculation according to the configuration. 5.1 Testbed Configuration We employed the MNIST [16], a benchmark of handwritten digits, with 60,000 samples from 250 writers in 28x28 gray scale pixels. The total dataset has a training set of 60,000 examples, and a test set of 10,000 examples. And all of them are labeled. Our cluster is group by the EC2 instances in AWS cloud platform. We leased 12 t 2 .medium for the scaling test. In the GPU parts, the G 2 instances with High-performan ce NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory are emplo yed for the GPU accelerating testing. 5.2 Scalability Evaluation We conduct the experiments on speed-up gained from Spark. The efficiency and scalability of DistDL is tested on cloud clusters with 2,4,6,8,10, and 12 nodes with 60000 samples. The test input is the whole MNIST data set. From our results, we can clearly conclude that the running time of tasks is linearly dependent on the aspects of nodes numbers in the cluster. The slightly mismatch between result and our anticipation is because o f the overhead of system architecture. Our result proves DistDL have an excellent speed-up performance. Fig. 6 (a) shows the running time has an inverse ratio relationship with the nodes number approximately. Our result proves our schema has an expected scalability. 5.3 GPU Evaluation Due to the impressive price/performance and performance/watt curves, GPU has become the wisest option for deep neural network training. The preliminary result to compare the performance of CPU and GPU with various iteration and samples is introduced in table 1, get 40% decrease in average compared with CPU training. What X  X  more, we have trained in the DistDL with 1500 iterations with 6000 samples based on Two CPU Nodes, One CPU &amp; One GPU Node, and Two GPU Nodes. The result is shown in Fig. 6 (b). We noticed that the time consumption can reduce up to 80% compared the no-GPU manner with GPU accelerating. A lot of efforts has been made to build distributed machine learning systems. Mahout [17], based on Hadoop [6] and MLI [18], based on Spark [8], adopt the iterative MapReduce [15] frameworks. [2,3,19] have successfully trained deep neural networks. The Google Brain deep neural network [2] containing billions of parameters, where described two complementary (synchronous) variants: they decompose the set of variables over seve ral different and com pute different parts of the objective function respectively. Many important DL algorithms iteratively update a large set of parameters with 9 to 12 orders of magnitude. Mu Li et al [3] present their design and implementation of a distributed parameter server for general machine learning cases. Their work presented the Parameter Servers to partition the workloads and data into cluster computing environments in or-der to address the distributed deep learning problems and get a quick result on a very large dataset. Microsoft [19] brought out the Project Adam, simi-lar to Parameter Servers [3], an efficient and scalable deep learning, which can enhance 2 times accuracy co mpared with the previous benchmarking recorders on ImageNet [20] object classification benchmark. Petuum [21]is a framework for iterative-convergent distributed ML, by treating data, parameter and vari-able blocks as computing units to be dynamically scheduled and updated in an error-bounded manner.
 Since GPU X  X  computing model makes it 1  X  10X times faster than CPU, using GPU to speed-up training is a popular approach in the machine learning commu-nity. Baidu Research built a large supercomputer, named Minwa [22] dedicated to training deep neural networks with 36 server nodes &amp; 4 Nvidia Tesla K40m GPUs in every node. By training a large model using high-resolution images and seeing a large number of examples, Min wa has achieved excellent results on multiple benchmarks. The resulting model achieved a top-5 error rate of 4.58% on ImageNet [20]. NVIDIA released the cuDNN [7] library, a deep learning and GPU-accelerated convolut ional net library that runs efficiently on their GPUs, also as our GPU backend. It emphasizes pe rformance, ease-of-use, and low mem-ory overhead. A cluster of GPU servers with InfiniBand interconnects and MPI is proposed to training the network in DistBelief [2] by leveraging inexpensive computing power in the form of GPUs [23]. By assimilating the advantages of the above systems X  design and trade off the various elements should be consid-ered, DistDL , aims at providing a deep learning service schema where cluster X  X  nodes can share data and model by harnessing GPU capability of heterogenous. Briefly, DistDL not only serves large scale models, but also speed up the training time by leverage the GPU heterogeneous resource. Currently, we have developed the prototype based on Spark, Cassandra and NVIDIA cuDNN as the GPU backend. In general, we successfully designed and implemented a deep learning system to valid in a typical benchmarking on large scale of data which running on top of Amazon Web Service platform. We are convinced that with the application of GPUs, DistDL is capable of successfully enhancing the recognizing handwrit-ing digits with great efficiency and accu racy. Improvements on efficiency and accuracy have been achieved by our system. We also observe that the running time of processing data is decreasing as the number of nodes increasing, almost in an inverse linear mode. Training time by adopting the GPUs can be reduce up to 80% off. The slightly mismatch between experiments and theory is because of the system overhead when larger number of nodes is introduced into the system. In the future we are planning to apply our algorithm on more complex deep learning applications including face recognition, speech recognition and human language processing. Currently we have just implemented BSP protocol, in that Stale Synchronous Parallel (SSP) consistency model will be supported. Acknowledgement. We thank the anonymous reviewers of Asia Pacific Web Conference (APWeb) for their feedback on previous versions of this paper. This Project supported by the National Natural Science Foundation of China (No. U2012A002D01).

