 Jason Weston jweston@google.com Chong Wang chongw@cs.princeton.edu Ron Weiss ronw@google.com Adam Berenzeig madadam@google.com Google, 76 9th Avenue, New York, NY, 10011 USA There exist today a growing number of applications that seamlessly blend the traditional tasks of retrieval and recommendation. For example, when users shop for a product online they are often recommended items that are similar to the item they are currently brows-ing. This is a retrieval problem using the currently browsed item as the query, however the user X  X  profile (including other items they may have browsed, bought or reviewed) should be taken into account making it a personal recommendation problem as well. Another related task is that of automatic playlist creation in music players. The user can request the creation of a playlist of songs given a query (based for instance on a seed track, artist or genre) but the songs retrieved for the query should also be songs that the user likes given their known profile.
 We call this class of problems collaborative retrieval tasks. To our knowledge these tasks have not been studied in depth, although there are several related areas which we will discuss later in the paper. Meth-ods designed for this task need to combine both the retrieval and recommendation aspects of the problem into a single predictor. In a standard collaborative filtering (recommendation) setup, one is given a user  X  item matrix indicating the known relevance of the given item to a given user, but many elements of the matrix are unknown. On the other hand, In a typi-cal retrieval task one is given, for each query, a list of relevant items that should be retrieved. Our task is the blend of the two, which is achieved by first build-ing a tensor comprising of the query  X  user  X  item training data. Typically in a retrieval task, and some-times in a recommendation task as well, one also has access to content-based features for the items, e.g. in document retrieval one has access to the words in the documents. Hence any algorithms designed for the col-laborative retrieval task should potentially be able to take advantage of those features, too.
 In this paper, we develop a novel learning algorithm for the collaborative retrieval task. We introduce a factorized model that optimizes the top-ranked items returned for the given query and user. We also general-ize it to work on either the collaborative retrieval ten-sor only, or using content-based features as well. The rest of the paper is as follows. Section 2 describes the collaborative retrieval task and our method for solving it. Section 3 discusses prior work and connections to other areas. Finally, Section 4 reports empirical re-sults where we show our method outperforms several reasonable baselines, and Section 5 concludes. Latent Collaborative Retrieval We define a scor-ing function for a given query, user and item: where R is a |Q| X |U| X |D| tensor, where Q is the (finite) set of possible queries, U is the set of users and D is the set of items. Any given element of the tensor is the  X  X elevance score X  of a given item with respect to a given query and a given user, where a high score corresponds to high relevance.
 We are typically given m training examples { 1 , . . . , |D|} and outputs y i  X  R , i = 1 , . . . , m . Here, ( q i , u i , d i ) can be used to index a particular element of R (i.e, a particular query, user and item) and y i is the relevance score, for example based on implicit user clicks, activity or explicit user annotations. One could simply collate the training data to build a suitable tensor R and use that, but the problem is that the tensor would be sparse and hence for many queries, users and items no prediction would be made. For that reason, collaborative filtering has connections with matrix completion , and almost all approaches can be seen as estimating the unknown matrix from data. For instance, many approaches such as SVD or NMF ( Lee &amp; Seung , 2001 ), solve such tasks by optimizing the deviation (e.g. squared error) from the known elements of the matrix. However, for retrieval tasks, and even for many recommendation tasks, humans evaluate the performance of a method from the top k results returned. Hence, precision or recall @ k measures are often appropriate. The method we propose in this paper thus has the following properties: (i) We learn a ranking of items given a user and a (ii) We learn model parameters for this task that at-To fulfill property (i) we must model the combination of the users, queries and items during inference. We thus propose a model of the following form: f ( q, u, d ) =  X  Q ( q ) &gt; S &gt; U u T  X  D ( d )+ X  U Here, S is a n  X |Q| matrix, T is a n  X |D| matrix, V is a n  X |U| matrix and n is the low dimensional embedding where queries, users and items will be represented (this is a hyperparameter of the system, typically n |D| and n |U| ). U i is a n  X  n matrix per user ( i = 1 , . . . , |U| ).  X  D ( d ) is the feature map of the item, the simplest choice of which is to map to a binary vector of all zeros and a one in the d th position.  X  Q ( q ) and  X 
U ( u ) act similarly for queries and users. In that case, the entire model can hence be more succinctly written as: However the  X (  X  ) notation will be useful for subse-quent modifications of the algorithm (later, we will consider general feature transformations rather than just switching on a single dimension).
 An intutive explanation of our model is as follows. The first term maps both the query (via  X  Q ( q ) &gt; S &gt; the item (via T  X  D ( d )) into a low dimensional space and then measures their similarity in that space, after linearly transforming the space dependent on the user (via U u ). Hence, the first term alone can model the relevance score (match) between a query q and item i with respect to a user u . The second term can be seen as a kind of  X  X ias X  that models the relevance score (match) between user u and item i but is constant w.r.t the query.
 It is also possible to consider some interesting special cases of the above model by further constraining the user-transformation matrices U i :  X  U i = I : by forcing all user-transformations to be  X  U i = D i : by constraining each user k to have a  X  U i = ( U LR i ) &gt; U LR i + D i : instead of considering a Content-Based Method In the typical collabora-tive filtering setting one has access to a user  X  item matrix only, and methods are agnostic to the content of the items, be they text documents, audio or images. For some tasks one has access to the actual content of the items as well, for example for each item i one is given a feature representation  X   X  D ( i )  X  R n D . In doc-ument retrieval this is the more common setting, e.g.  X   X 
D ( i ) represents the words in the document i . For rec-ommendation tasks this is called content-based recom-mendation and is particularly useful for the cold-start problem where an item has very few or no users asso-ciated to it (the relevant collaborative filtering column of R is very sparse). In that case, collaborative filter-ing methods have almost no data to generalize from, but content-based methods can perform well. In our setting, latent collaborative retrieval, we can also take advantage of such content features by slightly modi-fying our method from above. Our proposed content-based model consists of the following form: Here, the model is similar to before except an addi-tional set of parameters W D (a n  X  n D matrix) maps from item features to the n -dimensional latent embed-ding space. Other aspects of the model remain the same.
 Further, if we are given a feature representation for queries as well, where for each query i we have  X   X  Q ( i )  X  R f ( q, u, d ) =  X   X  Q ( q ) &gt; W &gt; Q U u W D  X   X  D ( d ) + V where W Q is a n  X  n Q matrix. This allows us to con-sider any possible query rather than being restricted to a finite set Q as in our original definition. Collaborative and Content-Based Retrieval Finally, we can consider a joint model that takes into account both collaborative filtering (CF) data and content-based (CB) training data. In this case, our model consists of the following form: f ( q, u, d ) = S &gt; q U u W D  X   X  D ( d ) + S &gt; q U The first two terms match the query and user with the CF and CB versions of the item respectively. Terms three and four are similar except they use the content features of the query instead. The final two terms are the  X  X ias X  terms comparing the user to the CF and content versions of the item. Note this model can be considered a special case of eq. ( 1 ).
 Training To Optimize Retrieval For The Top k We are interested in learning a ranking function where the top k retrieved items are of particular interest as they will be presented to the user. We wish to optimize all the parameters of our model jointly for that goal. A standard loss function that is often used for retrieval is the margin ranking criterion ( Herbrich et al. , 2000 ; Joachims , 2002 ), in particular it was used for learn-ing factorized document retrieval models in Bai et al. ( 2009 ). Let us first write the predictions of our model for all items in the database as a vector  X  f ( q, u ) where tive retrieval setting the loss can then be written as: For each training example i = 1 , . . . , m , the positive item d i in the example triplet is compared to all pos-sible negative items j 6 = d i , and one assigns to each pair a cost if the negative item is larger or within a  X  X argin X  of 1 from the positive item. These costs are called pairwise violations . Note that all pairwise vi-olations are considered equally if they have the same margin violation, independent of their position in the list. For this reason the margin ranking loss might not optimize the top k very accurately as it cares about the average rank.
 To instead focus on the top of the ranked list of re-turned items we employ a recently introduced loss function that has been developed for document re-trieval ( Usunier et al. , 2009 ; Weston et al. , 2010 ; 2012 ). To the best of our knowledge this method has not been applied to collaborative filtering type tasks be-fore. The main idea is to weigh the pairwise violations depending on their position in the ranked list. One considers a class of ranking error functions: where rank d i (  X  f ( q i , u i )) is the margin-based rank of the labeled item given in the i th training example: where  X  is the indicator function, and L (  X  ) transforms this rank into a loss: Different choices of  X  define different weights (impor-tance) of the relative position of the positive examples in the ranked list. In particular it was shown that by choosing  X  i = 1 /i a smooth weighting over positions is given, where most weight is given to the top position, with rapidly decaying weight for lower positions. This is useful when one wants to optimize precision at k for a variety of different values of k at once ( Usunier et al. , 2009 ). (Note that choosing  X  i = 1 for all i we have the same AUC optimization as equation ( 7 )). We optimize this function by stochastic gradient de-scent (SGD) following the authors of ( Weston et al. , 2010 ), that is samples are drawn at random, and a gradient step is made for each draw. Due to the cost of computing the exact rank in ( 8 ) it is approximated by sampling. That is, for a given positive label, one draws negative labels until a violating pair is found, and then approximates the rank with where b . c is the floor function, |D| is the number of items in the database and N is the number of trials in the sampling step. Intuitively, if we need to sample more negative items before we find a violator then the rank of the true item is likely to be small (i.e., at the top of the list, as few negatives are above it). Finally, our models have many parameters to be learnt. One can regularize them by preferring smaller weights. We constrain the parameters using || S i || X  C , i = 1 , . . . , |Q| , || V i || X  C , i = 1 , . . . , |U| , || T 1 , . . . , |D| (leaving U unconstrained). During SGD one projects the parameters back on to the constraints at each step, following the same procedure used in several other works, e.g. ( Weston et al. , 2010 ; Bai et al. , 2009 ). 3.1. Connections to matrix factorization for Many works for collaborative filtering tasks have pro-posed using factorized models. In particular, Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF) ( Billsus &amp; Pazzani , 1998 ; Lee &amp; Seung , 2001 ) are two popular choices. The main two differences between our approach and these general matrix factorization techniques is that (i) each rec-ommendation we make is seeded with a query (i.e. the collaborative retrieval task), and (ii) in collaborative retrieval tasks we are interested in the top k returned items, so our method optimizes for that goal. Most collaborative filtering work does not consider a ranking type loss that optimizes the top k , but one notable exception is ( Weimer et al. , 2007 ). They do not consider tensor factorizations.
 There are several ways to factorize a tensor, some clas-sical ways are Tucker decomposition ( Tucker , 1966 ) and PARAFAC ( Harshman , 1970 ). Several collabo-rative filtering techniques have considered tensor fac-torisations before, in particular for taking into account user context features like tags ( Rendle &amp; Schmidt-Thieme , 2010 ), web pages ( Menon et al. , 2011 ), age and gender ( Karatzoglou et al. , 2010 ), time ( Xiong et al. , 2010 ) or user location for mobile phone recom-mendation ( Zheng et al. , 2010 ) but not, to our knowl-edge, for the collaborative retrieval task.
 Finally, we should also note that some works have com-bined collaborative filtering data with content-based features before, e.g. ( Wang &amp; Blei , 2011 ). 3.2. Connections to matrix factorization and In information retrieval one is required to rank items (documents) given a query using the content-features of the items, e.g. for document retrieval one uses the words in the document. In that case, Latent Semantic Indexing ( Deerwester et al. , 1990 ), and related meth-ods such as LDA ( Blei et al. , 2003 ), are unsupervised methods that choose a low dimensional feature repre-sentation of the words. The parameterization of those models is a special case of our models. If we consider our model from equation ( 5 ) but remove the influence of the user model, i.e. set U u = I and V u = 0 we are left with a standard document retrieval model: More recently, factorized models that are supervised to the task of document retrieval have been proposed, for example Polynomial Semantic Indexing (PSI) ( Bai et al. , 2009 ). PSI considers polynomial terms between document words and query words for higher order sim-ilarities. For degree 2 it has the form of ( 10 ) but for degree 3 it uses tensor factorizations based on: This is closely related to our model ( 5 ) when constrain-ing U i = D i and replacing the user input by the docu-ment input, i.e. we compute f ( q, d, d ) in order to ob-tain interactions between document words rather than between document and user.
 Methods like LSI or LDA optimize the reconstruction error (mean squared error or likelihood). PSI opti-mizes the AUC ranking loss, which is more related to our ranking approach but does not optimize the top k results like ours. Methods for annotating images ( We-ston et al. , 2010 ) and labeling songs with tags ( Weston et al. , 2012 ) have been proposed that do use the WARP loss we employ in this paper. Many methods for doc-ument retrieval also optimize the top k but typically not using factorized models like ours, see e.g. ( Yue et al. , 2007 ).
 Finally, our models are applicable to the task of  X  X er-sonalized search X  where some topic model approaches have recently been studied ( Harvey et al. , 2011 ; Lin et al. , 2005 ; Sun et al. , 2005 ; Saha et al. , 2009 ). We will compare to generalized SVD and NMF models in our experiments which are related to these works. Traditional collaborative filtering datasets like the Netflix challenge dataset, and information retrieval datasets, like LETOR for instance, cannot be used in the collaborative retrieval framework, as they either lack the query or the user information necessary. We therefore use the three datasets described below. 4.1. Lastfm Dataset We used the  X  X ast.fm Dataset -1K users X  dataset available from http://www.dtic.upf.edu/  X  ocelma/ MusicRecommendationDataset/lastfm-1K.html .
 This dataset contains (user, timestamp, artist, song) tuples collected from the Last.fm ( www.lastfm.com ) API. This dataset represents the listening history (until May 5th, 2009) for 992 users and 176,948 artists. Two consecutively played artists by the same user are considered as a query  X  user  X  item triple. This mirrors the task of playlisting, where a user selects a seed track, and the machine has to automatically build a list of tracks. We consider two artists as  X  X onsecutive X  if they are played within an hour of each other (via the timestamp), otherwise we ignore the pair. One in every five days (so that the data is disjoint) were left aside for testing, and the remaining data was used for training and validation. Overall this gave 5,408,975 training triples, 500,000 validation triples and 1,434,568 test triples. 4.2. Playlist head and tail datasets We had access to a larger scale proprietary and anonymized dataset of user playlists where we could both construct a query  X  user  X  item matrix from consecutive tracks, and had access to content-based features as well so we can test our content-based fea-ture methods. The first extracted dataset ( X  X ead X  dataset) consists of 46,000 users and 943,284 tracks from 146,369 artists (each artist appears at least 10 times). The data is split into 17M training triples for training, 172,000 for validation and 1.7M for test. The above  X  X ead X  dataset can be built for artists where we have enough training data. However, a user may want to do retrieval with a query or an item for which we have no tensor training data at all (i.e., the cold-start problem). In that case, content-based fea-ture approaches are the only option. To evaluate this setup we hence built a  X  X ail X  testing dataset consisting of 10,000 triples from 5442 artists where we only have a single test example. The idea in that case is to train on the head dataset, and test on the tail (as it is not possible to train on the tail).
 For each track (including head tracks) we have ac-cess to the audio features of the track, which we pro-cessed using the well-known Mel Frequency Cepstral Coefficent (MFCC) representation. MFCCs take ad-vantage of source/filter deconvolution from the cep-stral transform and perceptually-realistic compression of spectra from the Mel pitch scale and have been used widely in music and speech ( Foote , 1997 ; Rabiner &amp; Juang , 1993 ). We extracted 13 MFCCs every 10ms over a Hamming window of 25ms, and first and sec-ond derivatives were concatenated, for a total of 39 fea-tures. We then computed a dictionary of 2000 typical MFCC vectors over the training set (using K-means) and represented each song as a vector of counts, over the set of frames in the given song, of the number of times each dictionary vector was nearest to the frame in the MFCC space. The resulting feature vectors thus have dimension n D = 2000. 4.3. Baselines We compare to Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF) which are both popular methods for collaborative filtering tasks. For SVD we use the Matlab implementation and for NMF we use the implementation at http://www. csie.ntu.edu.tw/  X  cjlin/nmf/ . Standard SVD and NMF operate on matrices, not tensors, so we compare our method on those tasks (where we consider only user  X  item matrices or only query  X  item matrices) as well. For the query  X  user  X  item tensor we considered the following generalization of SVD or NMF: f ( q, u, i ) =  X ( q ) &gt; U &gt; QI V QI  X ( d ) +  X   X ( u ) That is, we perform two SVDs (or NMFs), one for the user  X  item matrix and one for the query  X  item ma-trix, and then combine them with a mixing parameter  X  which is chosen on the validation set.
 For LCR, we compare both the versions from equation ( 3 ) and equation ( 2 ) on the query  X  user  X  item task. The former is directly comparable to the SVD and NMF tensor generalizations we use (they are the same paramaterization) while the latter takes into account three-way interactions between query, user and item in a single joint formulation. For the user  X  item and query  X  item tasks we employ either only the first or the second term respectively of equation ( 3 ). The validation set is used to choose the hyperparameters, e.g. the best choice of learning rate, regularization parameter and as a stopping criterion for the gradient descent.
 For content-based features we also compare to LSI ( Deerwester et al. , 1990 ) and using cosine similar-ity. Both of these methods perform retrieval given the query, and ignore the user term. 4.4. Evaluation For any given query q , user u , item i triple we compute f ( q, u,  X  i ) using the given algorithm for each possible item  X  i and sort them, largest first. For user  X  item or query  X  item tasks the setup is the same except either q or u is not used in all the competing models. The evaluation score for a given triple is then computed according to where item i appears in the ranked list. We measure recall@k, which is 1 if item i appears in the top k , and 0 otherwise. We report mean recall@k over the entire test set. Note that as we only consider one positive example per query (the element i of the triple) precision@k = recall@k / k . 4.5. LastFM dataset Results We first report results on the lastfm dataset. De-tailed results where we fixed the embedding dimension of all methods to n = 50 are given in Table 1 . Results for other choices of n are given in Table 4 . On all three tasks (query  X  item, user  X  item and query  X  user  X  item) LCR is superior to SVD and NMF for each top-ranked set k considered. Furthermore, our full LCR query  X  user  X  item model (c.f. equation 2 , U i un-constrained) gives improved results compared to both (i) any competing methods, including LCR itself, that do not take into account both query and user ; and (ii) LCR itself (and other methods) that do not model the query and user in a joint similarity function (i.e. LCR query x user x item (cf. eq. ( 2 )) outperforms LCR query x item + user x item (cf. eq. ( 3 ))).
 Loss function evaluation Some of the improve-ment of LCR over SVD and NMF can be explained by the fact that neither SVD nor NMF optimize a rank-ing function that optimizes the top-ranked items. To show the importance of the loss function, we report the results of LCR using an alternative loss function opti-mizing average rank (AUC) as in equation ( 7 ) instead of the WARP loss from equation ( 8 ). The comparison, given in Table 2 shows a clear gain on all tasks by opti-mizing for the top k (using WARP). Optimizing AUC instead yields results in fact similar to SVD. SVD opti-mizes mean squared error, not AUC, but the similarity is that neither loss function pays special attention to the top k results.
 Changing the embedding dimension We report results varying the embedding dimension n in Table 4 . It should be noted that n affects both test per-formance, evaluation time and storage requirements, so low dimensional embeddings are preferable if they perform well enough. LCR outperforms the baselines for all values of n that we tried, however all methods degrade significantly when n = 10. SVD on the query  X  user  X  item shows the same performance for n = 50 and n = 100 while LCR improves slightly. 4.6. Playlist dataset results Collaborative filtering type data The Playlist dataset is larger scale and has both collaborative-filtering type data and content-based features. We first tested using collaborative filtering type data only on the same three tasks as before (query  X  item, user  X  item and query  X  user  X  item). The results are given in Table 3 . They again show a performance improve-ment for LCR over the SVD and NMF baselines on the query  X  item task, although on the user  X  item task it performs similarly to the baselines. However, on the most interesting task, query  X  user  X  item, we again see a large performance gain.
 Using content-based features We compared dif-ferent algorithms using content-based features on the tail dataset where collaborative filtering cannot be used. (We also attempted to combine both collabo-rative filtering and content-based information on the head dataset, but we observed no gain in performance over collaborative filtering alone, probably because the content-based features are not strong enough, which is not really a surprising result ( Slaney , 2011 )). The re-sults on the tail dataset are given in Table 5 . LCR q  X  i (which does not use user information, as in eq. ( 10 )) already outperforms cosine similarity and LSI. Adding user information further improves performance: LCR q  X  u+q  X  i uses the model form of eq. ( 4 ) with U i = I and LCR query  X  user  X  item uses eq. ( 4 ) with U i = D i In this paper we introduced a new learning framework called collaborative retrieval which links the standard document retrieval and collaborative filtering tasks. Like collaborative filtering, the task is to rank items given a user, but crucially we can also take into ac-count a query term. Like document retrieval we are given a query and the task is to rank items, but cru-cially we also take into account the user in the form of a user  X  query  X  item tensor of training data. We proposed a novel learning algorithm for this task that learns a factorized model to rank the items given the query and user, and showed it empirically outper-forms some standard methods. Collaborative retrieval is rapidly becoming an important task and we expect this to become a well studied research area.

