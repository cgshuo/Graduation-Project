 Random tree ensembles introduce different random elements to construct di-versified decision trees. For classification problems, results from these trees are combined by an ensemble method to produce the final prediction. Random De-cision Trees [8] is one that is constructed without conventional test selection criteria, which questions the utility of these heuristics that are widely employed in many decision tree learning algorithms. The underlying argument is that they are effective to compute accurate single trees but there is no guarantee on the final accuracy of a tree ensemble.
 yses and compares complete-random trees with other decision tree ensembles. This paper aims to explore complete-random trees and compare them with Bag-ging [3] and Random Forests [5] which are widely accepted and use techniques such as randomized feature selection, bootstrap sampling and voting. The fun-damental objective of randomization in tree construction is to create diversity. After all, there is no point in combining a forest of identical trees. Section 2 of this paper discusses how increasing tree diversity can lower the generalization error of tree ensembles. Since there are many randomization methods, a system-atic framework to characterize each method is necessary to guide the research in this area. A taxonomy is provided in section 3 and the focus of our study is sample randomization and complete-random test selection . Section 4 describes the random tree learning process, section 5 provides the experimental settings and results, and follows by conclusions in the last section. In Breiman X  X  analysis [5] on strength and correlation, he gives an upper bound trees and  X  is the mean correlation. The implication of this upper bound is that no ensemble can do better than the boundary given its strength and correlation. Generally, this upper bound is applicable to classifier based ensemble, including complete-random trees the subject of this paper. Lowering PE  X  can be achieved by either minimizing  X  or increasing s . Building complete-random trees forgoes strength obtained from a test selection criterion. However, it helps to achieve higher tree diversity. The taxonomy of tree randomizations is summarized as follows: 1. Randomization before model induction 2. Randomization during model induction This paper focuses on sub-categories (1a) sample randomization and (2b) complete-random test selection to investigate whether complete-random test se-lection produces good results. For the experiment, our implementation is based on C4.5 release 8 [10] with modifications to cater for bootstrap sampling, multiple trees, complete-random test selection, tree height restriction, random split point selection for continuous features and ensemble methods. The tree height restriction is originated from [8]. Let k be the total number of features, setting tree height to k 2 is called half height tree. Alternatively, unrestricted tree growth is called full height tree. Consider a rule or a branch in a tree, when selecting i features from k features, there are C produces the largest number of combinations. Fan et. al. [8] uses this argument as the basis to choose the tree height limit of k 2 , but allowing any value of i is more desirable as it gives the maximum choice or diversity. Thus, the total number of possible unique combinations to include any value of i is T ( i )= i C k i . Since T ( k ) &gt;T ( k 2 ) &gt;C k k For continuous feature split point selection , random split point is determined by randomly selecting two different sample values and assigning it as the mid point between the two. This increases the possible split points from l  X  1to i =0 i , l is the number of distinct feature values. Hence, it increases diversity. Missing values for probability averaging are handled by: 1. growing missing value branches; 2. classifying them with reduced weight w = w p n missing n the classification weight from the parent node, n missing is the number of missing value samples and n total is node size. This avoids disruption of the usual weight disseminating routine in handling missing values.
 erated using these counts. To predict a class given a test case z , the predicted class c p is obtained by: 1. Probability averaging, c p = arg max c ( 1 N N i =1 ( w n h i ,c n are reported to cause overfitting [7]. However, it is worth noting that none of the empirical evaluations are conducted in the context of complete-random trees. There are three parameters in the experiments. The followings are the abbrevi-ations used in the experiments: variant is represented by three letters, for example  X  X FO X  refers to a random trees ensemble with parameters V oting, F ull height tree and O riginal training samples. factors among the eight possible variants; 2. if complete-random trees overfits. All variants will be compared with the benchmarking Bagging and Random Forests . The results are assessed by a sign test using 95% confidence level to determine whether the wins are statistically significant. Forty data sets are selected from the UCI repository [2]. Their data sizes range from one hundred to twenty thousand. This experiment uses ten thousand trees for each ensemble to see if any variants overfit. Tenfold cross-validation is conducted for each data set and the average error rate is reported. 5.1 Results The average error is shown in table 1 and a pairwise comparison summary is presented in table 2. Comparing variants with the benchmark classifiers, we summarize the results as follows:  X  PFO, PFB and PHO perform comparable to Bagging .  X  PFO and PFB perform comparable to Random Forests .  X  PFO has the most wins against the two benchmark classifiers having 23 and  X  PFO has thirteen data sets with the best error rates as marked with asterisks For each of the three parameters, we summarize the results as follows:  X  all probability averaging variants are significantly better than their voting  X  full height tree performs better than half height tree.  X  bootstrap sampling impairs accuracy as suggested earlier on in section 3. The results above suggest that the most accurate variant PFO is comparable to the benchmark classifiers and the probability averaging is the main contributing factor to complete-random decision trees. We call PFO  X  Max-diverse En-semble  X . Max-diverse Ensemble performs better against all variants and has the lowest mean-error rate 17.3% as shown in table 1. Regarding overfitting, none of the data sets suffers from overfitting in general. It dispels the concern of using probability averaging with complete-random trees causes overfitting. In this paper, we first discuss that maximizing tree diversity is a way to lower generalization error. Then, we provide a taxonomy on tree randomization as a systematic framework to characterize existing tree randomization methods. We find that complete-random test selection produces diverse trees. Finally, we thor-oughly investigate the complete-random decision trees by exploring eight possi-ble variants. The most accurate variant Max-diverse Ensemble has the maximum diversity according to our analysis and uses only simple probability averaging without any feature selection criterion or other random elements. For future work, it would be valuable to determine situations where Max-diverse Ensemble would perform better than other methods and vice versa.

