 Local based approach is a major category of methods for spatial outlier detection ( SOD ). Currently, there is a lack of systematic analysis on the statistical properties of this framework. For example, most methods assume identical and independent normal distributions (i.i.d. normal) for the calculated local differences, but no justifications for this critical assumption have been presented. The methods' detec tion performance on geostatistic data with linear or nonlinear trend is also not well studied. In addition, there is a lack of theoretical connections and empirical comparisons between local and global based SOD approaches. This paper discusses all thes e fundamental issues under the proposed G eneralized L ocal S tatistical ( GLS ) framework. Furthermore, robust estimation and outlier detection methods are designed for the new GLS model. Extensive simulations demonstrated that the SOD method based on the GLS model significantly outperformed all existing approaches when the spatial data exhibits a linear or nonlinear trend. D.2.8 [ Database Management ]: Database Applications  X  data mining. I.5.3 [Pattern Recognition]: Outlier Detection.
 Algorithms, Theory, and Experimentation Spatial Outlier Detection, Spa tial Gaussian Random Field. The ever-increasing volume of spatial data has greatly challenged our ability to extract useful but implicit knowledge from them. As an important branch of spatial data mining, spatial outlier detection aims to discover the objects whose non-spatial attribute values are significantly different from the values of their spatial neighbors [1]. In contrast to tr aditional outlier detection, spatial outlier detection must differentiate spatial and non-spatial attributes, and consider the spatial continuity and autocorrelation between nearby samples. By the first law of geography, "Everything is related to everything else, but nearby things are more related than distant things [3]." There are two main classes of spatial outlier detection ( SOD ) methods: local and global based approaches. Local based approaches [4] first calculate the local difference (statistic) for each object which is the difference between the non-spatial attribute of the object and the aggr egated value (e.g., average) of its spatial neighbors. By assuming i.i.d. normal distributions for these local differences, the local based approaches discover outlier objects by robust estimation of model parameters such as the aggregated values, mean, a nd standard deviation. Various methods have been presented by using different spatial neighborhood definitions and robust estimation techniques [5-9]. The second class, global based methods, is to identify outliers using the robust estimator of a gl obal kriging model which is the best linear unbiased estimator for geostatistical data. Particularly, Christensen et al. [10] proposed diagnostics to detect spatial outliers on the estimation of covariance function. Cerioli and Riani [11] developed a forward search procedure to identify spatial outliers for an ordinary kriging model. Militino et al. [12] further generalized the forward search method in [11] to a universal kriging model. This paper focuses on local based methods, because local based methods can achieve higher computational efficiency with minimal loss of accuracy. This feature of the local based appr oaches is demonstrated through extensive simulations described in Section 5. This work is primarily motiv ated by the current situation where there is no systematic study on the statistical properties of local based SOD methods. For example, exis ting works assume i.i.d. on local differences, but justificati ons for the assumption have never been proposed. Also their perform ance on spatial data with linear or nonlinear trends has not been well studied. There is also a lack of research on the theoretical connections and empirical comparisons between local and global based SOD methods. To that end, this paper provides a generalized framework for local based SOD methods and theoretically and empirically compares it against global based SOD methods. The proposed framework is cast within the statistical abstrac tion of a spatial Gaussian random field which is the most popular mode l for geostatistical data [1,2]. A major reason for its popularity is that the optimal solution based on the Gaussian random field is equivalent to a best linear unbiased estimator (BLUE) for non-Gaussian data. It has been shown to provide accurate results in a variety of practical situations [1,2]. Sections 5.8, 6.3. 3, and 7.4 in [2] give an in-depth discussion on the applicability of Gaussian random field. A spatial Gaussian random field refers to a collection of dependent random variables that are associated with a set of spatial indexes,  X   X   X   X   X   X  X   X  X  X  |  X   X  , where  X  refers to a continuous fixed region. This fam ily of random variables can be characterized by a joint Gaussian probability density or distribution. In real applications , only partial observations of one realization (or a partial sample of size one) are available. In order to make this model operational, the requirements for stationarity and isotropy, such as second-order or intrinsic stationarity, are further imposed. Imposing such assumpti ons helps reduce the number of model parameters required to be estimated. When the data is second-order stationary and isot ropic, the spatial correlation structure is described by a semiva riogram or covariance function, in which the correlations between variables are dependent on their spatial distance. Statistical inferences are then performed by assuming explicit forms of the covariance and mean functions. Our major contributions are as follows:  X  Design of a generalized local statistical ( GLS ) framework:  X  Robust estimation and outlier detection methods based on  X  In-depth study on the connection between different SOD  X  Comprehensive simulations to validate the effectiveness This paper is organized as fo llows. Section 2 reviews spatial local statistics and related works. Sec tion 3 introduces the generalized local statistical model and presents a rigorous theoretical treatment of its fundamental statistical prope rties. In Section 4, we introduce several robust estimation and outlier detection methods for the GLS model, and analyze the connection between different SOD methods. Section 5 provides the simulations and discussions. Section 6 gives the conclusion. Given a set of observations  X   X   X   X   X   X   X ,  X   X   X   X   X ,...,  X   X  spatial statistic [4] is defined as where  X  X  X   X   X   X   X ,...,  X   X   X  X   X  is a set of spatial locations,  X  X  X  ,  X   X   X   X   X  X  represents the value of  X  attribute at location  X  ,  X   X   X   X  is the set of spatial neighbors of  X  , and  X   X  the average  X  attribute value of the neighbors of  X  . It is assumed that the set of local spatial statistics  X   X   X   X  independently and identically normally distributed (i.i.d. normal). Then the popular  X  -test [4] for detecting spatial outliers can be described as follows: Spatial statistic  X   X   X   X   X   X  X  X   X   X   X   X   X  X  X  where  X  is the cumulative distribution function ( CDF ) of a standard normal distribution,  X  refers to a significance level and is usually set to 0.05, and  X   X  and  X   X  refer to the sample mean and sample standard deviation, respectively. well-known masking and swamping effects. When multiple outliers exist in the data, the quantities  X   X  are biased estimators of the population means and standard deviation. As a result, some true outliers are "masked" as normal objects and some normal objects are "swamped" and misclassified as outliers. The authors proposed an iterative approach that detects outliers by multi-iterations. Each iteration identifies only one outlier and then modifies its attribute value so that it will not impact the results of subsequent iterations. Later, Chen et al. [6] proposed a median based approach that uses median estimator for the quantities  X   X  deviation (MAD) estimator for  X   X  . Hu and Sung [7] proposed an approach similar to [6], but using trimmed mean to estimate  X   X  [8] presented a spatial local outlie r measure to capture the local behavior of data in their neighborhood. Shekhar et al. [9] employed a graph-based method to define spatial neighborhoods transportation network. for this assumption have ever been proposed. As we will discuss in next sections, this i.i.d. a ssumption is only approximately true in certain scenarios, and the de pendencies between different local differences (statistics) must be c onsidered when the spatial data exhibit linear or nonlinear trend or the selected neighborhood size for each object is small. As shown in our simulations in Section 5, the violation of i.i.d. assumption can significantly impact the accuracies of the outlier detection methods. This section first introdu ces some preliminary background on spatial Gaussian random field, then presents the generalized local statistical ( GLS ) model, and finally discusses the statistical properties of the GLS model. Table 1 summarizes the key notations used in this paper. Symbol Descriptions  X   X   X   X   X   X   X   X  X  X  X   X  A given set of observations, where  X   X   X   X   X   X   X   X   X  X  X  X   X   X   X   X   X   X  is a vector of covariates of  X   X  X  X  X  X  A general definition of spatial neighbors of  X  .  X   X   X   X   X  K-nearest neighbors of  X  . This paper considers SOD S patial O utlier D etection 
GLS G eneralized L ocal S tatistics Model  X  , X  ,  X   X  The unknown parameters in the GLS model Given a spatial Gaussian random field  X   X   X   X   X   X  X  X  X  X , consider the following decomposition of the process [1] (mean) of the process,  X   X   X   X  is the smooth-scale variation that is a variance  X   X   X  . The large scale trend  X   X   X   X   X   X  , X   X   X  X  X   X   X   X   X   X  , where  X  is a vector of trend parameters, and  X   X   X   X  is a vector of covariates that are the basis functions of sp atial coordinates of s (See Section 5.1 for illustrations). The nonlinear degree of the trend is dependent on the polynomials of covariates in x  X  s  X  . For the smooth-scale variation  X  (s), we assume that it is an isotropic second-order stationary process, in which the covariance  X Cov X   X   X  a function of the spatial distance between  X   X  and  X   X   X  ; X   X  , where  X  are function parameters. A variety of distance metrics may be selected, such as  X   X  (Euclidean distance),  X  (Manhattan distance), and graph distance [10]. There are two popular models for the covariance function  X  , including spherical model and exponential model (see Equations 8 and 11). observations of a particular realization of the spatial Gaussian random field, let  X  X   X   X   X   X   X   X   X ,...,  X   X   X   X   X   X  ,  X  X   X   X   X   X   X  X  X   X   X   X   X   X   X   X ,...,  X   X   X   X   X   X  , and  X  X   X   X   X   X ,..., the random vector  X  has the decomposition form as where  X  ~  X   X   X   X  X  X  X  , X   X  X  X  X   X  and  X  ~  X   X   X   X  X  X  X   X ,  X   X   X  can be reformulated as the matrix form where  X  X  X   X  X  X  X  is a neighborhood weight matrix with  X  if  X  X   X  ;  X   X  X  X   X / X  X  X 1 if  X   X   X  X   X   X   X   X   X  ; and  X   X  X  X   X 0 , otherwise. By Equations 3 and 4, we can readily derive the generalized local statistical ( GLS ) model as Recall that  X   X  X  X   X  X  X  X  X  X   X   X  X  X   X   X ; X   X  . The GLS model has the unknown components  X , X   X  ,and  X  , including  X  |  X  |  X 1 X  |  X  |  X  parameters. Because the covariance function  X   X   X   X  (e.g., spherical or exponential model) is non linear and nonconvex, it requires iterative reweighted generalized least squares algorithm to estimate all these parameters whic h is computationally expensive and can only guarantee a local optimum [14]. As shown in Section 3.2, an important property of the GLS model is that the component  X  X  X  X   X  can be approximated by  X  Hence the GLS form (5) becomes asymptotically equivalent to As discussed in Section 4.1, the model fitting for the GLS form (6) is a convex problem and can be efficiently solved. By Section 3.2 Theorem 1, when the neighborhood size is relatively large with  X 10  X  , the component  X  further approximated by  X   X   X   X  . This leads to a simpler form of GLS Discussion : Local statistics is a popular technique used to reduce the dependence between sample poi nts. However, by employing the decomposition form as indicated in Equations 2-4, we observe that local statistics help reduce the correlations between sample points caused by smooth-scale random variations, but at the same time it also induces "new" correlations due to the averaging of white noise variations. As discusse d in [2], correlated data can be expressed as linear combination of uncorrelated data. The approximate GLS form (6) explicitly models the "new" correlations caused by the averaging of white noises variations. The approximate GLS form (7) essentially ignores these "new" correlations. The form (7) may be considered when users expect high efficiency and allow some loss of accuracy. This tradeoff is studied in Section 5 by simulations. This sub-section studies th e properties of two major covariance components  X   X   X   X  X  X   X  and  X  X  X  X   X  , and discusses the situations where they can be approximated by  X   X   X   X  and  X   X   X  , respectively. As shown the random vectors  X   X   X  X  X  X  and  X   X   X  X  X  X  , respectively. We focus on the study of their correlation structures. Because  X   X   X  are both multivariate normally distributed, the correlation structure gives important in formation about the related dependence structure (e.g., in-co rrelation implies independence). Three related theorems are stated as follows: Theorem 1: The random vector  X   X  has two major properties where  X   X   X  refers to the i-th element in the vector  X  (Readers are referred to [15] for the proof.) Theorem 1 indicates that when the neighborhood size is relative large, the correlations be tween the components in  X   X  are very low (e.g., smaller than 0.2 when  X 10 ) and the variance of each component is very close to  X   X   X  . In this case,  X  However, for a small neighborhood si ze, as shown in simulations (Section 5), the dependence between the components in  X  be considered. The next two theorems are related to the random vector  X  an isotropic second order stationary process, and even when the explicit form of the covariance f unction is known, the statistical properties of  X   X  are still not straightforward. For this reason, several additional assumptions need to be considered. The following are three assumptions required for Theorem 2: A1. If  X   X   X   X   X   X   X  X   X   X   X   X   X   X  X  , then,  X  X   X   X ,  X   X , A2. If  X   X   X  X   X   X   X   X   X   X ,  X   X  X   X   X   X   X   X   X   X  X  X ,  X   X   X   X  A3. The distance between any points that are k-nearest neighbors The intuition on assumptions A1 and A2 is that, because neighbors are close to each other, they share similar between-distances and similar distances to points that are not their neighbors. The assumption A3 is valid when the spatial locations follow a uniform distribution or a grid structure. The assumption A3 holds in many practical situa tions [13]. The situations where assumptions A1 and A2 are potentia lly violated will be discussed in Theorem 3. Theorem 2: If the above assumptions A1 and A2 hold, then the random vector  X   X  has two major properties 1) The variance  X  X  X   X   X   X   X   X   X   X  X  X  X  2) The correlation  X  X  X   X   X   X ,  X   X   X  X  X  X   X  where  X   X  its K-nearest neighbors, and  X   X   X  X   X  0  X  refers to the constant variance for each component of  X  . Further, if the assumption A3 also holds, then the variance  X  X  X   X   X   X   X   X  becomes constant everywhere. (Readers are referred to [15] for the proof.) 
Theorem 2 indicates that the correlations between the components in  X   X  are mostly zero, except for neighboring points. Particularly, the correlations between neighboring points are all negative, and their major impact factor is the neighborhood size  X  . The greater the value of K , the less the neighbor points are correlated. However,  X  cannot be arbitrary large; otherwise, the assumptions made above will be violated. For example, suppose  X 200  X  and  X 10 X  , then only about 5% of pairs are correlated. For these correlated components, the correlations are only close to  X 0.1 . As shown in Figure 1, 0.1 indicates a negligible correlation. 
Figure 1: An example of correlation: it reflects the noise and Theorem 2 states two approximate properties of  X  it is not directly known how thes e properties are impacted if assumptions A1 and A2 are violated. The next Theorem 3 will delve deeper into this issue and provide more specific analysis on  X   X   X  . For Theorem 3, the following less restrictive assumptions are employed: B2. The spatial distance is defined by  X   X  (Euclidean) distance; B3. The covariance function  X  X  X  X  X   X   X   X   X   X  X  X ,  X   X  X  X  X  X   X   X   X  , where B4. Consider 4 or 12-nearest neighbors as spatial neighbors for Assumptions B1 and B2 are generic properties that can be readily applied to spatial data in general [1, 2]. In many applications, the total number of spatial locations is smaller than 300 [1]. Here, we consider a much enlarged range with  X 2500  X  , for the purpose of generality. For assumption B3, a spherical model is defined as  X   X   X ; X   X   X  X  X  where  X  X  X   X   X , X   X   X   X 0 X  X 0, X , . Note that  X  X  X   X  0; X   X  refers to decreasing function on the distance h . The reason for using a spherical model as opposed to exponential or Gaussian models is that the spherical model leads to closed-form analytical resu lts. The closed-form results will provide important insights into its statistical properties. As for assumption B4,  X  is set to 4 or 12 due to the use of the grid structure (assumption B1). In a grid data, each object has four nearest objects with the same distance  X  , eight next-nearest cell size. Hence, we can select ...., X 4,12,24  X  We select the first two values with  X 4 and 12  X  , which are equivalent to defining neighborhoods with radiuses of and 2r  X  , respectively. Theorem 3: Under the above four assumptions, the random vector  X   X  has following properties on the correlation structure 1) If  X 4 X  , then 2) If  X  X  X  X  X 12,  X   X   X ,  X   X  X 4 X  X  X  , then  X  X  X   X   X   X ,  X   X   X  X 0 3) If  X  X 4  X , X 12  X  , then 4) If  X  X  X  X  X  X   X  X  X   X  X 4  X , X 12  X   X   X  X  X  X  X   X   X   X   X   X  X  X  X  X  X   X  X  X   X   X  X  5) If  X  X  X  X  X  X , X  X 4  X , X 12  X   X   X  X  X  X  X   X   X   X   X   X  X  X  X  X  X ,  X   X  X  X  X  X   X   X  where  X  refers to the grid cell size;  X  X  X   X   X   X   X  and  X  X  X   X   X  the row and column locations of the object  X   X  in the grid structure;  X   X  X  X   X  X  X  X  X   X   X ,  X   X  is the Euclidean distance between  X  and  X   X  . (Readers are referred to [15] for the proof.) Theorem 3 implies similar patterns as drawn by Theorem 2 although Theorem 2 provides onl y approximate properties. Theorem 3 is a further justifica tion of these patterns. In the following discussions, we consider the situation with  X 5  X  . The situation with  X 5 X  will be discussed separately. By Theorem 3,  X  X  X  X   X   X   X ,  X   X   X  X  X 0.18 when  X 12  X  . It indicates small absolute correlation values for different  X  values. The correlation values slightly decreases when K increases. It can also be shown that most correlations are negative and are close or equal to zero. Readers are referred to [15] for detailed information Theorem 2. We have a comparison between  X   X   X   X  X  X   X  and  X  X  X  X  two typical situations:  X 4 X  to represent a small neighborhood; and  X 12 X  to represent a relatively large neighborhood. If  X   X  4 , then  X  X  X  X   X   X   X ,  X   X   X  X  X 0.4 and  X  X  X  X   X   X   X ,  X   X   X  X  X 0.22 . If  X 12 X  , then correlation values (degrees) are shown in Figure 1. Although both K decreases, the absolute correlation  X  X  X  X   X   X  drastically. Based on these results, we will approximate  X  X  X  X   X   X   X  for different settings of  X  , but will only approximate  X  by  X   X   X   X  , when  X  is relatively large, e.g.,  X 10 X  . Theorem 3 also indicates that when  X  is small  X   X  X 5 X g.,.e  X  , some correlations are relatively high (e.g.,  X  X  X  X   X  if , X  X 1 X  X 4, X  and  X  X  X  X   X   X ,  X   X  X  X  ). In this case, an important observation is that the correlation matrix of  X   X  structure as that of  X   X  . Particularly, if  X  X  X  , these two correlation matrices become identical. In this situation, it is still reasonable to approximate the correlation matrix of  X   X  as identity or unit matrix. The lost structure information by this approximation will be recovered while estimating the parameter  X   X  for the vector  X  because of the similar structure between the covariance matrices constant variance for each component of  X  is  X  Var  X   X   X   X  X  X  X  X   X   X   X  , and Var  X   X   X   X   X  X ar  X   X  X  X   X   X  X  X  X  X  X   X   X  X   X  X  X  X   X   X   X   X  ~  X   X  X  X  X  X  X , X  X  X   X   X  X   X   X   X  X  X   X   X   X  X   X  , X  X  X  X   X   X   X  approximate model becomes  X  X  X  X   X   X   X   X  ~  X   X , X  X  X  X   X   X  X  X  X  Using robust parameter estimati on, the approximate model can completely recover the true distribution, e.g., by setting the estimated parameters  X  X  X 0 X  and  X  X   X   X   X   X   X   X   X  X   X   X  . Spatial outlier detection ( SOD ) is usually coupled with a robust estimation process for the related statistical model. This section presents robust estimation and ou tlier detection methods to reduce the masking and swamping effect s, and then discusses the connection between the proposed GLS -SOD methods and existing representative methods, such as kriging-based and Z-test SOD methods. We are focused on the estimation techniques for the GLS form (6) that is regarded as the default model. The GLS form (7) will be explicitly stated when discussing its techniques. Given a set of observations  X   X   X   X   X   X   X ,...,  X   X   X  to estimate the parameters  X  and, X , X   X  for the proposed GLS model. We consider mean squa red error (MSE) as the score function which is the most popular error function in spatial statistics [11]. This leads to a ge neralized least square problem for the GLS form (6) and can be formulated as: arg min Note that we scale  X   X  and  X  by a factor  X  with  X   X   X   X / X  X  , such that  X   X   X   X   X   X   X   X   X 1 . Without this constraint, the objective function in (9) will always be minimized by setting  X   X   X  X  X  X  X  , and  X  to any value. For simplicity, we directly use the original symbols  X   X  and  X  , rather than  X  As shown in Theorem 4, the problem  X 9 X  is a convex optimization problem which can be efficiently solved by numerical optimization methods such as in terior point method [14]. Note that when the neighborhood size  X  is large, the following approximation holds:  X   X   X   X  X  X   X   X  X   X   X   X  (see Section 3.2 Equation 7). Then the problem (9) reduces to a regular least squares regression problem and an explicit solution is available with  X  X  X   X   X   X   X   X   X  X  X   X   X  X   X   X   X   X   X  X  X  , and  X   X   X   X  X   X   X   X   X   X   X  X  X  X  X  X  X  X  X   X  , X 1 X  X  where  X  is the size of the vector  X  . For the purpose of outlier detection, it is unnecessary to further derive the explicit forms of  X  and  X   X  . Theorem 4 : The problem (9) is a convex optimization problem . corresponding (orthonormal) eigenvectors of the matrix  X  X  X  can be readily shown that the pr oblem (9) is equivalent to argmin  X , X  function, or equivalently  X   X   X   X  When the parameters  X  and, X , X   X  are estimated by generalized least squares, we can calculate st andard residuals and use standard statistic test procedure to identify outliers. This method works well for sample data with sma ll data contamination, but is susceptible to the well-known masking and swamping effects when multiple outliers exist. For the GLS model, the masking and swamping effects originate from two phases of the estimation process: 1) Phase I contamination occurs in the process of calculating local differences  X  X  X  . For example, suppose we define neighbors by the K-nearest-neighbor rule. Consider an outlier object  X   X   X   X   X   X   X  X   X   X   X   X   X  X   X  , where  X   X   X   X   X  is the normal value but it is contaminated by a large error  X   X  , and suppose only one of its contamination error. The local difference  X diff X   X  X   X   X   X   X   X   X  error is marginalized and we obtain a normal local difference for a outlier object  X   X   X   X   X   X  which will be identified as a normal object. difference is contaminated by the error  X   X  swamping effect where the normal object  X   X   X   X  misclassified as an outlier. For a relatively large  X  (e.g., 8), it can be readily shown that Phase I c ontamination is more significant for a spatial sample with clusters of outliers than a spatial sample with isolated outliers. Another im portant observation is that the masking and swamping effects will not completely distort the ordering of true outliers. The t op ranking outliers are still usually a subset of the true outliers. This observation motivates the backward algorithm presented in Section 4.3. 2) Phase II contamination occurs in the generalized regression process, where we regard  X   X   X  X  X  X  as the pseudo "obs erved" values. The masking and swamping effects in this phase are the same effects occurred in a general least square s regression process. This is consequence of the biased estimat es of the regression parameters (e.g.,  X  ,  X  , and  X   X  ) due to abnormal observations in  X  Drawbacks of existing robust estimation techniques : Most existing robust regression techniques are designed to reduce the effect of Phase II contamination. There are two major categories of estimators [13]. The first cate gory (also called M-estimators) is to replace the MSE function by more robust score function such as L1 norm and Huber penalty function. The second category is to estimate parameters based on a robus tly selected subset of data, such as least median of square ( LMS ), least trimmed square ( LTS ), and forward search ( FS ) method. Unfortunately, all these robust techniques cannot be directly app lied to address both Phase I and Phase II contaminations concurren tly. As with the M-estimators, the application of robust penalty function (e.g., L1) will lead to a non-convex optimization problem where local optimal solution may be found. With the second type of estimators based on subset selection, the estimation results ar e highly sensitive to the selected objects which can detrimentally impact neighborhood quality. The next sub-section will adapt ex isting robust methods to resolve the problem of concurrently handling Phase I and Phase II contaminations. As discussed above, exis ting methods only address Phase II contamination. The motivation fo r our proposed backward search algorithm is to address both Phase I and Phase II contaminations concurrently. The algorithm is described as follows: Algorithm 1 ( Backward search algorithm ) Given a spatial data set  X   X   X   X   X   X   X ,...,  X   X   X   X   X  , the covariate vectors  X   X   X   X  the value of K for defining K -nearest neighbors, and the confidence interval  X   X   X  0,1  X  , 1. Set  X   X   X   X   X   X   X   X   X   X ,...,  X   X   X   X   X   X ,  X   X   X   X   X   X  2. Estimate the parameters  X , X , X   X  of the GLS model by solving 3. Calculate the absolute values of standard estimated residuals 4. Set  X   X   X  X ax  X   X   X   X   X  X  X  X  |  X   X  | . In the above algorithm, the confidence interval  X  can be set to 0.001, 0.01, and 0.05. In step 2, we apply interior point [14] method to solve the optimization problem (9). When the neighborhood size is large, we may approximate  X   X  The parameters  X , X , X   X  can be efficiently estimated by least squares regression:  X  X  X   X   X   X   X   X   X  X  X   X   X  X   X   X   X   X   X  X  X  , and  X   X   X   X  X  X  X  X  X  X  X  X   X   X   X   X 1 X  X  X  X  X  X / , where  X  is the size of the vector  X  . observation that top ranked outliers identified by the least squares techniques are still true outliers (in most cases) under both Phase I and II contaminations. Suppose a true outlier  X  is removed after the first iteration, then both Phas e I and Phase II contaminations in the next iteration will be reduced. To illustrate this process, we use the same example in Section 4. Recall that an outlier object  X   X   X   X   X  is decomposed into two additive components  X   X   X   X   X   X   X   X   X   X   X  X  , where  X   X   X   X  represents the normal value and  X  represents the contam ination error. Suppose  X  is the only outlier neighbor of an object  X   X  that happens to be an outlier as well. Then the local difference  X diff X   X  X   X   X   X   X   X   X  if  X  X  X  X  X   X  . Suppose now that the true outlier  X   X   X   X  is removed and the newly replaced neighbor for  X   X  is normal, then  X diff X   X   X   X   X   X   X  X  X  X  X   X   X   X   X   X   X  difference becomes an abnormal va lue and the masking effect is removed. Similarly, suppose  X   X   X   X   X   X  is a normal object, then its local difference is contaminated (swamped) by the error  X  because of its outlier neighbor  X   X   X   X  . The removal of  X  will make  X  contamination, the removal of  X   X   X   X  leads to the removal of an differences will therefore have less contamination. The center of the distribution is less attracted by outliers, and the distributional shape becomes less distorted. As a result, outliers tend to be more separated and normal objects tend to be closer together. The masking and swamping effects are therefore reduced. This section adapts the popular Forward Search ( FR ) algorithm [13] to the GLS parameters estimation problem. There are several restrictions to apply FR here. As discussed in Section 4.1, FR starts from a robustly selected subset of sample, but GLS is a statistical model based on neighborhood aggregations. Considering only a subset of the observations  X   X   X   X  will significantly impact the quality of the calculated local differences. To apply FR algorithm, we make the assumption that Phase I contamination is negligible compared to Phase II contamination. As discussed in S ection 4.1, this is reasonable for the case of isolated outliers. Based on this assumption, we consider the local differences  X  X  X iff X   X   X   X   X   X  X ,...,diff X   X   X  pseudo "observations", and then apply FR algorithm to estimate the model parameters. By simulations, we also noticed that in this case there is no significant difference on accuracy between the GLS forms (6) and (7). For the sake of efficiency, we only consider the GLS form (7) and apply regular least squares regression to estimate the parameters , X , X  and  X   X  search algorithm is described as follows: Algorithm 2 ( Forward Search algorithm ) Given a spatial data set  X   X   X   X   X   X   X ,...,  X   X   X   X   X  , the covariate vectors  X   X   X   X  and the value of K for defining K -nearest neighbors, 1. Calculate the local differences:  X  X  X  X   X   X   X   X  X  X  X  , and set 2. Set  X  X  X   X   X   X   X ,..,  X   X   X ,  X   X   X   X   X   X   X   X   X   X   X   X   X ,...,  X   X   X  3. Apply least trimmed squares (LTS) [13] to identify a robust 4. Estimate the parameter  X  based on  X   X   X   X   X   X  and  X  5. Find the minimal residual of the test set  X   X  X  X  X  X   X  : The proposed FR algorithm provides an ordering of objects based on their agreements with the GLS model. To identify outliers, it plots and monitors the change of the minimal residual with the increasing size of the normal set  X   X  . A drastic drop implies that an outlier was added to  X   X  . This plot could also help identify masked or swamped objects. Readers are referred to [13] for details. A direct method for calculating the local differences can be achieved via robust mean functions such as median and trimmed mean. However, as indi cated by our simulation study, this direct approach will de teriorate the performance of GLS .  X  X  X   X   X   X   X  ~  X   X  X  X  X  X  X , X  X  X   X   X  X   X   X   X  X  X   X   X  . If we replace the left hand side  X  X  X  X   X   X   X   X  X  X  X  by medians or trimmed means, the right side will remain unchanged and thus still employs the average matrix  X  . The increased bias caused by this inconsistency is much larger than the reduction of contamination effects achieved through robust means. This section studies the connection between global (kriging) based [11, 12, 13], local statistics ( LS ) based [4-10], and the proposed GLS -SOD methods. First, we review the first two approaches: Kriging-SOD and LS-SOD . Kriging-SOD basically applies robust methods to estimat e the parameters of a global kriging model. The method then uses the estimat ed statistical model to predict the  X  attribute value of each sample location  X  , normal distribution, where  X   X  is the estimated standard deviation. If a residual is outside the range  X   X  X   X  X   X  2/ X   X  , X   X  X   X  2/ X   X   X  corresponding object is reported as an outlier, where  X  is the CDF and  X  is usually set as 0.05. The LS-SOD method assumes regarded as i.i.d. sample points of a normal distribution  X   X   X , X  Robust techniques are then designed to estimate  X  and  X  . The remaining steps are similar to Kriging-SOD . Theorem 5 : Suppose that  X  X  X  X   X   X  X   X   X  and the parameters of Kriging-SOD and GLS-SOD are correctly calculated by robust estimations, then Kriging-SOD and GLS-SOD are equivalent . Proof : For Kriging-SOD , we consider a universal kriging model [1], since other kriging models (e.g., ordinary kriging) are simply special cases. It suffices to prove that the standardized residuals calculated by Kriging-SOD and GLS-SOD are identical. Without particular sample point  X   X   X   X   X  . Let  X   X   X   X   X   X   X  and  X  X  X  X  X   X   X   X ,  X   X   X   X   X  T . By Section 3.1 Equation 3, we have that  X  X  X   X   X , X  X   X  , where  X  X  X  X  X  X   X   X   X  X  X  X   X   X   X   X   X   X  Cov  X   X   X   X   X   X   X ,  X   X   X  X  X  , and  X Var X   X   X   X   X   X  X  X   X   X  .
 Then, the standard residual by Kriging-SOD is The standard residual by LS-SOD is The following will prove that The condition  X  X  X  X   X   X  X   X   X  implies  X   X   X  X  X  X   X   X   X   X   X   X  X  . It follows that  X   X  X  X  X  X   X   X  X  X   X   X   X   X   X   X  X   X   X  X  X  X  X  X  X  X   X   X  X  X   X  Further, given that  X  X  X  X  where  X   X   X  X   X   X  X   X  X   X   X  X   X  and  X   X   X  X   X   X  X   X   X   X   X  X  X   X  . Then,  X   X   X  X  X  X  X   X   X  X  X   X   X   X   X   X   X   X  X  X  X  X  X  X  X   X   X   X  X  X  The above indicates that We conclude that Kriging-SOD and G LS-SOD are equivalent. Theorem 6 . If  X  X  X  X   X   X  X   X   X  ,  X   X   X   X  X  X   X   X  X   X   X   X  , the parameters of GLS-SOD and LS-SOD are correctly calculated by robust estimations, and one of the followi ng conditions is true, then GLS-SOD becomes equivalent to LS-SOD. (1)  X   X   X   X  has a constant trend (mean):  X  X  X  X c X  , where c is a (2)  X   X   X   X  is a linear trend of spatia l coordinates, and each point Proof : For either condition (1) or (2), it can be readily derived that  X  X  X  X  X  X  X  . By conditions  X  X  X  X   X   X  X   X   X  and  X   X   X   X  X  X   X   X  X  have  X ~ X  X  X   X  0,  X   X   X   X  X   X   X   X   X   X  which is consistent with the i.i.d. assumption in LS-SOD . If we use the same robust methods to estimate the parameters, such as using median and median absolute deviation ( MAD ) to estimate the mean and standard deviation, then GLS-SOD becomes equivalent to LS-SOD . Discussion : By Theorem 6, LS-SOD is a special form of GLS-SOD . LS-SOD assumes  X  X  X  X Var X   X   X   X   X  X  X   X   X  for some constant  X  , but no justifications are presen ted. From this perspective, GLS-SOD actually provides a theoretical foundation for LS-SOD . Section 3.1 discusses the situations where  X  X  X  X Var X   X   X   X   X  can be approximated by  X   X   X   X  X   X   X   X   X  . Furthermore, under the conditions of Theorem 6, LS-SOD is equivalent to GLS-SOD and since the conditions also include "  X  X  X  X   X   X  X   X   X  ", then by Theorem 4 we have that GLS-SOD is equivalent to Kriging-SOD . Therefore, LS-SOD becomes equivalent to Kriging-SOD in this situation . Hence, it can be seen that the proposed GLS framework can be parameterized to become instances of LS-SOD or Kriging-SOD . Further study on various outlier de tection methods can be greatly enhanced under the lens of this unifying GLS framework. As discussed in Section 3.1,  X  X  X  X   X  can be reasonably approximated by  X   X   X  . From Theorem 5, the major difference between Kriging-SOD and GLS-SOD is for which approach the related model parameters can be estimated more accurately and efficiently. From this perspective, G LS-SOD is superior to Kriging-SOD based on three major reasons: First, G LS-SOD has less uncertainty than Kriging-SOD , since Kriging-SOD needs to further assume a semivariogram model. If the semivariogram model is not selected properly, the performance may be significantly impacted. Second, G LS-SOD is a convex optimization problem and theref ore a global optimal solution exists. However, Kriging-SOD is a non-convex optimization problem and relies on an iterativel y reweighted generalized least square ( IRWGLS ) approach [12] to determine a local solution. Finally, as shown in Section 5 simulations, GLS-SOD runtime performance is superior to Kriging-SOD . This section conducts extensiv e simulations to compare the performance between the proposed GLS based SOD methods and other related SOD methods. The experimental study follows the standard statistical approach for evaluating the performance of spatial outlier detection methods presented in [11, 12, 1, 2]. Data set: The simulation data are generated based on the following statistical model: where  X   X   X   X  is a Gaussian random fiel d with covariogram model  X   X   X ; X   X  . We consider two popular c ovariogram models: spherical model and exponential model. See Equa tion 8 in Section 3.2 for the definition of spherical model. The exponential model is defined as  X   X   X ; X   X   X  X  X  These two models have the same parameters  X  and  X  . Recall that  X  is also the constant variance for each  X   X   X   X  . trend  X   X   X   X   X  is a polynomial of order tw o. The nonlinearity of the trend is decided by the regression parameters  X  . For example, if  X  X  X   X  1,1,1,0,0,0  X   X  , then the trend is linear. For the white noise component , we employ the standard model: There are three related parameters  X   X   X ,  X  and  X  .  X  contaminated error that generates outliers, and  X  is used to control the number of outliers. Note that it is possible the distribution  X   X   X ,0  X   X   X  will also generate some normal white noises. All true outliers must be only identified ba sed on standard statistical test by calculating the conditional mean and standard deviation for each observation [2]. We also consider the case of clustered outliers. This can be simulated by constraining that the noises of a random cluster of  X  X  X  points follow  X   X   X ,0  X   X   X  . In the simulations, we tested several representative settings for each parameter, which are summarized in Table 2. 
Variable Settings 
Covariance Outlier type Isolated, Clustered Outlier detection methods: We compared our methods with the state of the art local and global based SOD methods, including Z-test [4], Median Z-test [6], Iterative Z-test [5], trimmed Z-test [7], SLOM-test [8], and universal kriging ( UK ) based forward search [11,12] (noted as UK-forward ). Our proposed methods are identified as GLS-backward-G , GLS-backward-R , and GLS-forward-R . GLS-backward-G refers to the GLS backward algorithm using generalized least squares regression. GLS-backward-R refers to the GLS backward algorithm using regular least square regression (See Sections 4.2 and 4.3). The implementations of all existi ng methods are based on their published algorithm descriptions. Performance metric: We tested the performance of all methods for every combination of parameter setting in Table 2. For each specific combination, we ran the experiments six times and then calculated the mean and standard deviation of accuracy for each method. To compare the accuracies of each method, we used the standard ROC curves. We further collected accuracies of top 10, 15, and 20 ranked outlier candidate s for each method, and then the counts of winners are shown in Table 3. To calculate these winning counts, we used as an example the GLS-backward-R result in the top left cell of table 4: "47, 47, 45". This column refers to the constant trend cases. If within this particular case, we only consider the true accuracy of the top 10 candidate outliers, then the GLS-backward-R has  X  X on X  47 times over all combination of parameters agai nst all other methods. A win is given to the method that exhibits the highest accuracy. Consequently, if we consider the true accuracy of the top 20 candidate outliers, then the GLS-backward-R has won 45 times. All the simulations were conducted on a PC with Intel (R) Core (TM) Duo CPU, CPU 2.80 GHz, and 2.00 GB memory. The development tool is MATLAB 2008. We compared the outlier detection accuracies of different methods based on different combina tions of parameter settings as shown in Table 2. Six representative results are displayed in Figure 3. First we considered the detection performance between local based methods. For a consta nt trend, our methods were competitive with existing techniques. For data sets exhibiting linear trends, our GLS algorithms achieved an average 10% improvement over existing local based methods. However, for data sets with nonlinear trends, our GLS algorithms exhibited more significant improvement (a pproximately 50% increase) over existing local methods. For the ot her combination of parameter settings in Table 2, the winning statistics for each method are displayed in Table 3. These results further justify the preceding performance results. We also compared our GLS algorithms against the global based method UK-forward . Overall, our methods were comparable to UK-forward . Particularly, GLS-backward-G attained better accuracy than UK-forward on about half of the data sets. For the remaining data sets, the GLS-backward-G was still competitive to the UK-forward . Additionally, as shown in Section 5.3, the UK-forward incurred a significantly much higher computational cost than the GLS algorithms. As discussed in section 4.1, when K is small, the effects of  X   X   X   X  X  X   X  must be considered and a generalized least regression is necessary. The theorems indicate that GLS-backward-G should perform better then GLS-backward-R , this was justified in Figure 3 c). Table 3: Competition statistics for different combinations of parameter settings. Each cell contains 3 values, representing the win times for the related method on the accuracies of top 10, 15, and 20 ranked outlier candidates for all methods. Algorithm Constant GLS-backward-R 47, 47, 45 79, 72, 82 76, 81, 77 GLS-backward-G 88, 86, 89 114,102,120 141,144, 138 GLS-forward-R 13, 11, 14 22, 25, 27 40, 36, 47 Z-test 47, 35, 40 29, 30, 13 0, 0, 0 Iterative Z-test 35, 46, 63 16, 20, 21 0, 0, 0 Median Z-test 20, 23, 29 1, 7, 8 0, 0, 0 Trimmed Z-test 15, 23, 32 5, 13, 13 0, 0, 0 
SLOM-test 0,0, 0 0, 0, 0 0, 0, 0 The comparison on computational co st is shown in Figure 2. The results indicate that the time cost of UK-forward is much higher than other methods. Even the second slowest method GLS-backward-G , is still three times faster than UK-forward . The other local methods are approximately equal and hence much faster than UK-forward . From the comparisons of both the accuracy and computational cost, it can be seen that our proposed GLS SOD algorithms (especially GLS-backward-G ) is significantly more accurate than existing local based algorithms when the spatial data exhibits either a linear or nonlinear spatial trend. Our GLS algorithms are comparable to the global based method UK-forward on accuracy, but significantly faster than UK-forward . This paper presents a generalized local statistical ( GLS ) framework for existing local ba sed methods. This generalized statistical framework not only pr ovides theoretical foundations for local based methods, but can also significantly enhance spatial outlier detection methods. This is the first paper to present the theoretical connection between local and global based SOD methods under the GLS framework. As future work we will design other algorithms to further improve the efficiency of the GLS backward and forward methods. [1] Cressie, N.A. 1993. Statistic s for Spatial Data, Wiley. [2] Schabenberger O. and Gotw ay C. A. 2005 Statistical [3] Tobler, W. R. 1979. Cellular geography, in Philosophy in [4] Shekhar , S., Lu, C.-T. and Zhang, P. 2003. A Unified [5] Lu, C.-T., Chen, D. and Kou, Y. 2003. Algorithms for [6] Lu, C.-T., Chen, D. and Chen, F. 2008. On Detecting Spatial [7] Hu, T. and Sung, S.Y. 2004. A trimmed mean approach to [8] Sun, P. and Chawla, S. 2004. On Local Spatial Outliers, [9] S. Shekhar, Lu, C.-T. and Zh ang, P. 2001. Detecting graph-[10] Christensen, R., Johnson, W. and Pearson, L.M., 1993. [11] Cerioli, A. and Riani, M. 1999. The ordering of spatial data [12] Militino, A.F., Palacios, M. B. and Ugarte, M.D. 2006. 
Figure 2: Comparison on comput ational cost (setting: Linear trend, isolated outliers,  X  X  X  X . X , X   X   X   X  X  X  X  X  X  X  X  X  X , X  X  X  X  X  X , X  X  X  X , ) [13] Atkinson, A.C. and Riani, M. 2000. Robust Diagnostics [14] S. Boyd and L. Vanderberghe . 2004. Convex Optimization. [15] Chen. F, Lu, C-T, and Boedihardjo, Arnold P., 2010. GLS-a) Constant trend, isolated outliers,  X  X  X  X . X , X   X   X   X  X  X  X  X  X  X  X  X , X  X  X  X , b) Linear trend, isolated outliers,  X  X  X  X . X , X   X   X   X  X  X  X  X  X  X  X , X  X  X  X , c) Nonlinear trend, isolated outliers,  X  X  X  X . X  X  X , X   X   X   X  X  X  X  X  X  X  X  X , X  X  X  X  X  X , d) Constant trend, cluster outliers,  X  X  X  X . X , X   X   X   X  X  X  X  X  X  X  X  X , X  X  X  X ,
