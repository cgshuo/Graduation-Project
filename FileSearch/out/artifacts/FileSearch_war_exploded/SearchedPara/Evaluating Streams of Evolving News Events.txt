 Peopletracknewseventsaccordingtotheirinterestsand available time. For a major event of great personal interest, they might check for updates several times an hour, tak-ing time to keep abreast of all aspects of the evolving event. For minor events of more marginal interest, they might check back once or twice a day for a few minutes to learn about the most significant developments. Systems generating streams of updates about evolving events can improve user perfor-mance by appropriately filtering these updates, making it easy for users to track events in a timely manner without undue information overload. Unfortunately, predicting user performance on these systems poses a significant challenge. Standard evaluation methodology, designed for Web search and other adhoc retrieval tasks, adapts poorly to this con-text. In this paper, we develop a simple model that simu-lates users checking the system from time to time to read updates. For each simulated user, we generate a trace of their activities alternating between away times and reading times. These traces are then applied to measure system ef-fectiveness. We test our model using data from the TREC 2013 Temporal Summarization Track (TST) comparing it to the effectiveness measures used in that track. The pri-mary TST measure corresponds most closely with a modeled user that checks back once a day on average for an average of one minute. Users checking more frequently for longer times may view the relative performance of participating systems quite differently. In light of this sensitivity to user behavior, we recommend that future experiments be built around clearly stated assumptions regarding user interfaces and access patterns, with effectiveness measures reflecting these assumptions.
 Categories and Subject Descriptors: H.3.4 [Informa-tion Storage and Retrieval]: Systems and Software  X  Per-formance evaluation (efficiency and effectiveness) Keywords: search; streams; evaluation
On December 4, 2012, Typhoon Bopha made landfall on the eastern coast of the Philippine island of Mindanao. The strongest typhoon in the island X  X  recorded history, the storm system had formed in the Pacific in late November, growing into a tropical storm by November 28th, and into a severe tropical storm by November 30th. On December 1st, as the storm moved west towards the island, it rapidly intensified into a category 4 typhoon. Two days later, only miles from shore, it reached category 5, a super typhoon, with winds up to 150 mph. At 4:45AM it hit. Heavy rains and waves ex-ceeding 25 feet washed across villages and towns, destroying homes, flooding roads, and cutting communications. Initial reports limited the death toll to one, but by the next day it had increased to over 200, mostly killed by flooding and debris. Over the next week, the number of fatalities grew to a final total of over 600, with more than 200,000 people displaced or otherwise impacted.

In this paper we consider how to evaluate a search en-gine that aggregates a stream of information and provides a means for people to obtain the latest, most relevant infor-mation regarding an event of interest. As time permits, a user of such a search engine will check back multiple times in order to find out new information about the event.
We envision that there will be a wide variety of users of such a search engine. Government and health care workers, as well as people in the immediate vicinity, would likely need to learn of new information soon after it becomes available. People interested in the event, but not directly affected, would likely have a less urgent need for new information. Some people would be willing to consume a large amount of content in the hope of not missing any relevant information, while others may desire a minimal amount of content.
One way of designing an information retrieval system for streams of evolving events would be to allow users to visit and query the system whenever they want the latest infor-mation. In this type of system, the user pulls information. An alternative approach would be to create a system that pushes select pieces of information to users that have set up a search query or profile. In either case, users would want to control the frequency and duration of their interaction with the system. In the pull scenario, the user controls the fre-quency by how often they visit and controls the duration by how long they consume content during a visit X  X  session. In the push scenario, the user would need to specify up front how often content should be sent to them as well as the amount of content to be sent.
In this paper, we propose a new effectiveness measure for the evaluation of retrieval systems that produce streams of content for evolving events. We call this measure modeled stream utility (MSU). This new effectiveness measure models users in terms of the time spent away from the system and the time spent with the system. In effect, we model users in terms of their frequency and duration of interaction with the system, which is a model applicable to either push or pull variants of this type of retrieval system. We measure utility in the number of relevant information nuggets the user is likely to find. Search users naturally vary, and we model this variation as well by modeling a population of users and simulating many different users X  interaction with the retrieval system. A significant advantage of MSU as an effectiveness measure is that by making it user-centered, the measure has the potential to be easily calibrated using recorded user behavior.
 We demonstrate MSU using the TREC 2013 Temporal Summarization Track [4]. Participating groups in the tem-poral summarization track (TST) take a large stream of con-tent and attempt to produce a filtered stream of content rel-evant to a given search topic. For each topic, the track has determined a set of relevant nuggets of information and has judged a pool of content and determined which nuggets are present in the content.

To evaluate the runs submitted to TST, we use MSU in a scenario where users pull information from the retrieval system. MSU simulates user interaction with the retrieval system. When a simulated user visits the retrieval system and queries it for a given search topic, the user is shown the most recently filtered content in reverse chronological order. In TST, filtered content consists of sentence length material extracted from a stream of larger documents. After a simulated user has read an element of content, we measure theuser X  X gainintermsofthenumberofnuggetstheuser is likely to have found relevant in the content. If a user has read a nugget earlier, MSU assumes that a repeated nugget will not be found relevant and has a gain value of zero. The temporal summarization track has estimated when nuggets are first known in the aggregate stream, and in the MSU simulation, nuggets delivered late to a user are less likely to be found relevant.

In addition to demonstrating MSU applied to the TREC 2013 temporal summarization track, we compare MSU to the track X  X  existing primary measure, ELG, as well as con-duct a parameter sweep over a range of the MSU user model X  X  parameters to better understand how the measure behaves. We find that:
Given that the amount of content consumed appears to affect system rankings the most, this result has implications for the design of retrieval systems that generate streams of information updates. The temporal summarization track gave participating groups the task of filtering a stream, but different users are going to be interested in different amounts of material. Rather than filter a stream, retrieval systems for evolving events may be better designed as best-match rankers that must balance recency and relevancy. No mat-ter how such systems are designed, MSU provides a user-centered measure that allows for calibration with actual mea-sured user behavior.
The news about an event, such as Typhoon Bopha, has an impact on large sections of society. The sudden onset of such events can generate related information needs among governments, aid agencies, observer groups, as well as those affected and their families. Usually such events are unex-pected and can be dynamic and evolve over time.

A large number of documents will be authored in connec-tion with significant events and made available for consump-tion via the Internet in near real time. Publishers of this content include social networking services such as Twitter and Facebook as well as newspapers, news feeds, and blogs. Authors can range from journalists to government agencies to persons experiencing the event first hand. We consider all newly created content in aggregate as a stream of material from which a person might want to find relevant informa-tion.

The closest analogues to this type of retrieval are mi-croblog search [12] and temporal summarization [2, 4]. The TREC 2011 Microblog track [12] included a single adhoc re-trieval task where the retrieval system receives a query at time t and must retrieve the most relevant and recent Twit-ter postings prior to time t . The track organizers used preci-sion at rank 30 to evaluate submissions, but found that the task was under-specified with respect to how results should be ranked in terms of relevance and recency, making com-parisons between participating groups difficult.

In the TREC 2012 Microblog track [14], the adhoc re-trieval task remained and was adjusted so that participants were simply to score the microblog posts for ranking pur-poses. The only notion of recency was that the postings had to have occurred before the time of the query. Relevance of a posting was formulated with respect to the informa-tiveness of the posting without considerations of recency or novelty. To evaluate a ranking, the organizers computed its precision at 30 and its receiver operating characteristic (ROC). For TREC 2013, the microblog track continued to use precision at 30 as its primary retrieval measure [11].
Temporal summarization systems attempt to extract rel-evant information from an aggregate stream and relay this information to the user as soon as possible. In contrast to a microblog search task, a temporal summarization task extends over a period that roughly corresponds to the pe-riod that an event is in the news. Being a summarization task, the evaluation of temporal summarization systems has focused on determining the overall quality of the resulting summary even though the nature of temporal summariza-tion means that the summary grows over time and is likely being consumed by an end user at regular intervals, e.g. for a multi-day event, we would expect a user to seek new in-formation daily.
 The temporal summarization work of Allan, Gupta, and Khandelwal [2] defined new precision and recall measures that incorporate notions of usefulness and novelty. A useful sentence is one that would be helpful for use in a summary and a novel sentence is one that is new and contains infor-mation not previously seen. In addition, they examine the importance of summary size and recency of information.
Guo et al. [10] primarily focus on finding updates for rapidly evolving news events for which the information need is urgent (or time-critical). They consider an event to be composed of a number of subtopics. Each update may con-tain one or more subtopics as well. Accordingly, the preci-sion for an update is the fraction of the update X  X  subtopics that are also subtopics for the event, and, the recall for an update is the the fraction of the event X  X  subtopics contained in the update. They define measures expected precision and expected recall to be the average precision (and recall respec-tively) over the complete set of updates.

The TREC 2013 Temporal Summarization Track (TST) [4] follows the temporal summarization framework defined by Allan et al. [2] with two significant changes. The first change is that while Allan et al. required that summariza-tion happen on-the-fly as each news story was received, in TST, systems are only restricted to be certain that sum-maries produced at time t only use content available at t or earlier. For example, if a system wanted to, it could wait to process several days of content before producing a single result. The TST directly evaluates the recency of informa-tion and thus does not have to restrict systems in when or how they produce summaries. The second change is that the TST uses a web-scale aggregate stream of world wide web content as opposed to the comparatively small collec-tion available to Allan et al. in 2001.

The TST uses sentences ( updates in TST terminology) as the primary retrieval unit. Groups participating in TST had the task of filtering the aggregate stream and emitting sentences containing timely and novel information regarding a news event. The TST judges the relevance of sentences by noting the presence of individual nuggets of information. Sentences are annotated such that it is known what pieces (nuggets) of relevant information the sentences contain. The TST assessors identified nuggets about an event from Wikipedia. For example, Typhoon Bopha has its own de-tailed Wikipedia page 1 . TST attached a timestamp to every nugget as determined by the first occurrence of the nugget in the Wikipedia edit history for the topic X  X  article.
The TST organizers crafted effectiveness measures that are nugget-based variants of precision and recall that incor-porate several key criteria for the updates produced by a system. In addition to an update needing to contain novel nuggets, the nuggets should be emitted as early as possible (latency) and shorter updates are better than longer updates (verbosity). Each update is assigned a score based on these criteria and the track X  X  set-based effectiveness measures are computed as averages over all updates.

Participating groups produced runs consisting of emitted updates. When an update is emitted, it is given a timestamp equal to the most recent material consumed by the system from the aggregate stream of material. Thus, a system that http://en.wikipedia.org/wiki/Typhoon_Bopha delayed emitting any updates until it had consumed all of the material in the period of interest, would have all of its up-dates given a timestamp equal to the very end of the period. If an update contains a nugget, the nugget X  X  latency is the difference between the update X  X  timestamp and the nugget X  X  Wikipedia assigned time. The gain for a reported nugget is discounted as a function of its latency, such that, with increasing latency, the gain for a nugget drops substantially within the first 24 hours, after which its discounted gain stays under 20%. It is possible for an update to be  X  X arlier X  than the nugget X  X  timestamp, in which case the latency dis-count function awards the update a bonus for reporting the nugget early. Reflecting novelty, repeated nuggets have no value, providing zero gain.
 Finally, sentences are penalized for being overly verbose. To incorporate verbosity into the track X  X  measures, an up-date may count as more than a single update in the compu-tation of the average. The verbosity of an update is based on the extra number of nuggets the update could have con-tained. For example, if an update contains zero nuggets and is 60 words long, and if the average number of words per nugget is 15, then this update has a verbosity penalty of 60 / 15 = 4 and when averaged counts as 5 updates, i.e. itself (1) plus its verbosity penalty (4). If the number of words in an update equals the sum of the words in the nuggets in the update, then the update counts as a single update.
TST X  X  evaluation has two primary metrics, expected la-tency gain (ELG), which measures the gain per update while discounting latency and penalizing verbosity, and latency comprehensiveness (LC), which measures coverage of nuggets relating to the topic. ELG and LC are analogous to preci-sion and recall respectively. While both ELG and LC are important to be considered together, we consider the ELG measure to be the primary measure of the track given that runs were presented from best to worst ELG in the track overview [4]. ELG for a set D of system generated updates is formulated as, where, V ( d ) is the verbosity normalization of update d , G ( d, D ) is the latency discounted gain for update d ,and G ( d, D )isnonzerowhen d is the earliest update from the set D to report one or more nuggets for the topic. Once a nugget is reported, it does not contribute to gain if it ap-pears again in later updates. The LC metric replaces the denominator in equation (1), with n  X  X  R ( n ), where N is the set of nuggets identified by the assessors and R ( n ) is the relevance for n based on its importance ( R ( n )=1 for binary relevance). Essentially LC computes the recall of relevant material by the system. Unjudged sentences are elided from run submissions.

It is clear that users are kept in mind in the formulation of effectiveness measures for temporal summarization. For example, an update should be timely and contain relevant, novel information without extraneous material. In addition to these qualities, we believe that it important to also con-sider the user X  X  desired frequency and duration of interaction with the system. A government agency may well assemble a team of people to monitor a stream of updates 24 hours a day for the duration of a crisis. An individual in the midst of a crisis may be limited by extrinsic circumstances to only having a few moments each day to check for updates.
ID Time Nugget n 9 12/05/12 n 10 12/06/12 n 11 12/04/12 n 12 12/04/12 n 13 12/05/12 n 14 12/05/12 Table 1: Nuggets reported to user (read by user) in the example session (Table 2). Times for each nugget are their time of first occurrence in Wikipedia X  X  edit history.
We have designed our new effectiveness measure, mod-eled stream utility (MSU), to be an easily calibrated user-centered effectiveness measure. A user-centered effectiveness measure must take into consideration both the user interface of the retrieval system and user behavior with the user inter-face. For an effectiveness measures to be easily calibrated to user behavior, the measure X  X  user model must be designed such that the model X  X  parameters can be set based on actual measurements of user behavior.
Our hypothetical user interface provides a means for the user to query the system and receive a ranked list of results (i.e. updates about an event). Each result is a short piece of text. The retrieval system produces a stream of information that is consumed by the user in the order produced. In other words, after issuing a query and receiving the ranked list of results, the user reads the results in rank order. No other interaction with the retrieval system is possible.
Our simulated user is not limited to one visit with the retrieval system. Our simulated user will repeatedly visit the search engine with a frequency reflective of their interest in the evolving event or reflective of their availability to visit. On each visit, the simulated user enters the same query, and expects to receive the most recent and relevant updates concerning their event of interest. As with their frequency of visits, the simulated user will read during a visit for an amount of time reflective of their interest or time available. We will call visits sessions , with each having a duration.
No two users are the same. Each user will visit the search engine with different frequencies and durations. Each user will have their own reading speed. We model frequency and duration by modeling the time a user spends away from the system and by separately modeling the time a user spends with the system. We model each user with three parameters:
To estimate user performance with the retrieval system, we draw multiple simulated users from population distribu-tions and average user performance over all simulated users. We model the user population X  X  time away and their session duration with two separate log-normal distributions. Log-normal distributions are such that when one takes the nat-ural log of the data, a normal distribution fits the resulting distribution. Usually one describes a log-normal distribu-tion in terms of the mean,  X  , and standard deviation,  X  ,of the normal distribution fit to the log of the data.
For both the time-away and the session-duration distri-butions, we will describe them in this paper in terms of the underlying distribution X  X  mean and standard deviation. If the underlying data has mean M and standard devia-tion S , then, for the log-normal distribution, the variance is  X  2 = log (1 + S 2 /M 2 ), and the mean is  X  = log ( M )  X  0 . 5  X  As we describe later in the paper, we consider a wide range of user behavior by varying the time away and session duration population distributions. To model the population X  X  read-ing speed, we use the log-normal parameters from Clarke and Smucker [8] wherein, the distribution of reading speed across users is described by a log-normal distribution with  X  =1 . 29 and  X  =0 . 558.

Given these three population distributions, we can gener-ate as many simulated users as needed for numerical preci-sion in our estimate of average user performance. For exam-ple, a possible reasonable setting of the population parame-ters is: A random user drawn from this distribution will spend more or less time away, have longer or shorter session durations, andreadfasterorslowerthantheabovepopulationaver-ages. Overall, the population of simulated users will have average behavior equal to the population means. To simu-late a single user X  X  interaction with the search engine over a given period of time, we construct a user-trace (Figure 1d) consisting of alternating sessions with the search engine and time intervals spent away from the search engine.
It would be unusual for a user to visit on a fixed frequency and visit for the exact same amount of time on each visit. To model variation in a user X  X  time away and session duration, we use their mean time away and mean session duration as parameters to two separate exponential distributions. An exponential distribution is commonly used to model phe-nomena such as the time between radioactive decay of nu-clei or the time between events in a Poisson process. For example, to simulate random session duration times with a mean of D , we draw random deviates from an exponential distribution [9] with a rate parameter of  X  =1 /D . A simulated user will repeatedly visit the search engine. During a visit, the user will read the search results in rank order. Each search result has an amount of gain (possibly zero) associated with it. The simulated user will accumulate  X  X tR X  means  X  X ime to Read X  and  X  X TtR X  means  X  X umulative Time to Read X . gain at the end of each search result. If the simulated user does not finish reading a search result during a session, the result is considered unread and no gain is recorded. We consider unjudged updates as non-relevant, and we do not elide them.

We measure the gain of a search result as the number of nuggets of relevant information contained in the result. A nugget can be considered an atomic piece of information. Example nuggets from the TST 2013 qrels are shown in Ta-ble 1. Our simulated users consider only novel nuggets to be relevant. Previously seen nuggets provide no gain.
Users interested in evolving events want the update pro-duced by the search engine to be recent as well as relevant. Nuggets delivered late to a user are less likely to be consid-ered relevant by the user. Given that different users will visit the search engine at different times, our notion of nugget timeliness must be relative to the simulated user. To make nugget timeliness relative to the user, we define a nugget as being late if it existed in the aggregate stream at a time equal to or before the start of the previous visit. In other words, if a user reads a result that contains a nugget that the search engine could have delivered during a previous visit, the nugget is late. To measure how late a nugget is, we de-fine a function  X  ( n ) that returns how many sessions ago the nugget could have been reported (Figure 1f). A nugget that is reported on time has an  X  ( n )ofzero.

While we do not know how the probability that a user will consider a nugget relevant changes with its lateness, an exponential decay seems reasonable. Therefore we compute the gain for every read nugget n as: where, L is the decay parameter for late reporting of nuggets and can vary between 0 and 1. L is a pre-determined value representing how much less a user is likely to consider the nugget to be relevant if it is reported late. For our exper-iments, we vary L from 0 to 1, where 0 indicates that a nugget loses all value if it is reported late, and 1 indicates that the nugget does not lose any value ever regardless of how late the reporting.

The gain from each read nugget is summed to get the cumulative gain for a user over the user trace (Figure 1d). Thus a simulated user X  X  MSU for a search topic is given by: where n is the set of nuggets read by the user. We compute the MSU for a system by computing for each simulated user their mean MSU over all topics and then averaging all users X  mean MSU to produce a system mean MSU. To increase the numerical precision of our system X  X  MSU estimate, we only need to increase the number of simulated users. To demonstrate modeled stream utility (MSU), we use the TST 2013 test collection. The TST is a close but imperfect fit for MSU. The primary issue with using TST to demon-strate MSU is that the runs submitted by the participating groups were guided by the set-based effectiveness measures of TST.

As described in section 2, each participating group sub-mitted runs to the TST that consisted of a set of updates. Each update has both a timestamp and a confidence asso-ciated with it. The timestamp is when the system emitted the update and the confidence is a system generated score indicating how relevant the update is.

For MSU, the simulated users check the retrieval system as per their user-trace to read the system X  X  updates. At the start of every session in the user-trace, the updates emit-ted between the end of the last session and the start of the current session are presented to the user in a reverse chrono-logical order, so that most recent information is read first. Updates with the same timestamp are shown in descending order given their confidence. The simulated user starts read-ing the latest update and then reads the next older update, and so on, until the user runs out of time or encounters an already read update and stops reading further. In case the session ends, the last update that is partially read in the ses-sion, is considered as unread. The first user session always begins at the start of the query duration. To determine if a nugget is late, we use the TST assigned Wikipedia times as described in section 2. As with the TST, the gain of a novel, on-time nugget is 1.
Figure 1 illustrates the MSU model of evaluation. Figures 1a and 1b show the input to and output of a system that gen-erates a stream of updates. The evaluation process begins at Figure 1c, where the nuggets, their time of first occurrence and the updates containing the nuggets are identified. The user-trace in Figure 1d is generated for a modeled user by alternatively sampling the exponential distributions for the session duration and the away time respectively. With the user-trace overlaid over the update-trace we get a reading-trace (Figure 1e). The reading-trace identifies updates avail-able to read at each user session, and by incorporating the reading speed of the user it determines which updates are actually read. Thus the reading-trace determines if any rel-evant updates were read by the user or not, for every user session.

For a more concrete example, consider a modeled user with A of 1 day, D of 60 seconds and a reading speed V ,of 225 words per minutes. Let us also assume that this user considers late information to be half as likely to be relevant, for every session where it is unreported ( L =0 . 5). Suppose that this user checks for updates at about 10:00 am every day and had previously checked for updates at 10:02 am on Dec 4, at 10:11 am on Dec 5, and at 9:50 am on Dec 6, 2012.
On Dec 7, 2012, the user starts a session at 9:55 am. The user finds the updates listed in Table 2 at the start of the session. The times at which the updates were emitted by the system are listed in the first column. In this case, all updates were emitted on the morning of Dec 7 before the user started the session. The updates are presented to the user in reverse chronological order. In case multiple updates are emitted at the same time, the updates are ordered by their system assigned confidence (as for the 4 updates emitted at 9:52).
The user starts by reading the most recent update (deliv-ered at 9:52 am). The user continues on to read the second update and finds 4 nuggets n 11 , n 12 , n 13 , n 14 .Table1lists the nuggets found in the session. These are nuggets that the user had not seen before and therefore experiences an increase in gain.

However, as per each nugget X  X  Wikipedia time, n 11 first occurred at 6:31 p.m. on Dec 4, 2012. It should ideally have been reported to the user at the session starting at 10:11 am on Dec 5. The system further missed reporting thenuggetatthesessionat9:50onDec6. Therefore  X  ( n 11 ) is 2. By similar calculation  X  ( n 12 ) = 3, as it should have been reported for the user session at 10:02 on Dec 4. For nuggets n 13 and n 14 ,  X  ( n 13 )and  X  ( n 14 )are1.
As the user reads further down the list, more nuggets ( n n 10 ) are read and gain increases accordingly. n 10 is reported in time at the current session and  X  ( n 10 )=0. Theuser gets no gain from reading the update emitted at 9:15 as nugget n 14 was read earlier. Since the session duration was 60 seconds, the last update was partially read and the user gets no gain on reading it. Thus the total gain (MSU) for this user, from this session on Dec 7, is 2.875.
We explore the parameter space of MSU by applying it to evaluate the runs submitted to the TST 2013. We compare MSU X  X  ranking of the TST runs to the TST X  X  track X  X  primary measure, ELG. The ELG measure is described in section 2.
The 2013 TST evaluated 26 runs from 6 groups over 9 topics. Topics in the track are instances of event types from the set { accident, bombing, earthquake, shooting, storm } Each topic had a time period (query duration) of 10 days. In other words, a run must summarize 10 days of material from the TREC KBA stream corpus [1] for each topic.
The KBA corpus is essentially a time ordered document stream spanning from October 2011 to January 2013, con-taining over 1 billion documents crawled from the web. The corpus contains documents with 3 categories, i.e. the docu-ments were crawled from URLs of public newswires (news), blogs and forums (social), and the URLs submitted for short-ening at www.bitly.com . Documents are segmented into sentences and some documents are tagged with named en-tities using NLP tools. There are on average 93,037 docu-ments per hour [5] in the corpus.
Keeping the user population X  X  reading speed distribution the same across all parameter sets, we sweep the parameter space by setting MSU X  X  parameters to the following values: For the standard deviations of the session duration, S D ,and thetimeaway, S A , we multiplied the mean by the values 0.5, 1, 2. For example, a mean session duration of 30 seconds, was associated with standard deviations of 15, 30 and 60 seconds. In total we generated 7 ( M A )x3( S A )x6( M D ) x3( S D )x7( L ) = 2646 parameter-tuples (points in the parameter space). For instance, the parameter tuple ( M A 6 hours, S A = 3 hours, M D =5minutes, S D =5minutes, L = 1), is a point in the parameter space that represents users who spend an average of 5 minutes every 6 hours reading updates, unaffected by late reporting of nuggets.
For each tuple (selected point from the parameter space) in the parameter sweep, we simulated 1000 users. For each simulated user, we generated their user-trace, determined which updates they read, and computed their average MSU across the topics for every run submitted to TST 2013.
We believe that a reasonable setting of MSU X  X  parameters is a user population where the users visit on average for 2 minutes every 3 hours and for whom late material quickly Figure 2: MSU with reasonable parameter settings ( M A = 3 hours, S compared to TST 2013 measures, ELG and LC. becomes less likely to be considered relevant. This corre-sponds to the parameter tuple ( M D = 2 minutes, S D =1 minute, M A = 3 hours, S A = 1.5 hours, L =0 . 5).
Both Figure 2a and Table 3 show the results of MSU vs. the TST measure ELG, given these reasonable parameter settings. For both MSU and ELG, larger values indicate better runs. At these parameter settings, MSU and ELG do not rank systems the same (Kendall X  X   X  =0 . 471). As Figure 2a shows, the runs in the middle-lower positions of the ELG ranking jump to the top positions. A similar ob-servation can be made in Figure 2b for MSU vs. the LC measure. Here as well, MSU and LC rank systems differ-ently (Kendall X  X   X  =  X  0 . 108). ELG and LC are themselves negatively correlated (Kendall X  X   X  =  X  0 . 28). The AP corre-lation coefficient (  X  AP ) [16, 13], which is more sensitive to changes in higher ranks is also reported in Figure 2.
Across the entire set of swept parameter settings, the max-imum Kendall X  X   X  correlation was found to be 0.625 for the parameters: M A = 24 hours, S A = 12 hours, M D =1minute, S
D =2minutes, L = 0.1. These results are shown in Fig-ure 3. While not a high correlation, this result shows us that ELG best correlates with a simulated user population where users on average visit once a day for 1 minute. In other words, over the 10 days of the query period, an av-erage user reads about 10 minutes of material. If we were to understand ELG in terms of the users that would pre-fer its top ranked runs, then ELG is tuned for highly time constrained and selective users with low tolerance for late reporting.

A correlation of 0.625 correlation is not high, and MSU and ELG are behaving quite differently. We believe that there are likely two main reasons for the different rank or-der of the runs. The first reason is that run performance with MSU is affected by the amount of material that the simulated user reads. If a run does not supply enough ma-terialforasessionvisit,thenthesimulateduserwillstop reading and also stop accumulating gain.

For example, Table 3 shows that the top ELG run clus-ter5 averages only 21.9 updates per topic over a 10 day query duration. With the reasonable parameter settings, MSU simulated users spend about 2 minutes every 3 hours, i.e. about 160 minutes reading, on average, over a 10 day query duration. With an average length of each TST up-date being 63 words, and the average reading speed being 4.3 words per second, it takes on average just 319 seconds to read all the 21.9 updates of cluster5 .Asimulateduser, who is willing to read for 160 minutes in total over 10 days, thus derives very low gain from cluster5 because such a user X  X  visit is cut short from a lack of material to read.
Figure 4 shows the user performance at the point ( M A =5 minutes, S A = 10 minutes, M D = 30 minutes, S D =15min-utes, L = 1) that has the minimum correlation of MSU with ELG (Kendall X  X   X  = -0.04), in our parameter set. This set of users seem inclined to spend almost all their time with the systemreadingupdatestakinga5minutebreakevery30 minutes on average, without any dissatisfaction for late re-porting of information. Unsurprisingly, these users achieve the highest amount of MSU (22.11) with run rg4 .While seemingly an unreasonable parameter setting for MSU, such user behavior could be achieved by a team interested in con-stant monitoring of a stream of updates.

This result also shows that MSU scores are related to the amount of material read by the simulated users. MSU mea-sures gain as the number of relevant nuggets read in the total content consumed by a user, while discounting for lateness if required. The amount of material read depends on the characteristic behavior of the user (or user population) as well as the number of updates emitted by the system. MSU for a system is simply the average over all simulated users.
A second reason for differences between ELG and MSU is likely that ELG is a set-based measure that measures av-Figure 3: Maximum correlation of MSU with ELG is ob-tained with parameters ( M A = 24 hours, S A = 12 hours, M
D =1minute, S D =2minutes, L =0.1). Figure 4: Minimum correlation of MSU with ELG is ob-tained with parameters ( M A =5minutes, S A =10minutes, M
D = 30 minutes, S D = 15 minutes, L =1). erage quality of an update. In contrast, MSU is estimating the total number of nuggets read and considered by the sim-ulated user to be relevant. To see how much the set-based nature of ELG is causing it to be different than MSU, we transformed MSU into a similar measure by taking MSU and dividing it by the time the simulated user spent reading (MSU/second).

In our experiments, MSU/second has the highest correla-tion (Kendall X  X   X  = 0.754) with ELG (Figure 5), with param-eters ( M A = 24 hours, S A = 12 hours, M D =1minute, S D 1 minutes, L = 0.1). The correlation between MSU/second and ELG is greater than the maximum correlation between MSU and ELG. Of note, the top ranked run for MSU/second is now the same as for ELG: cluster5 . The MSU/second top 4 runs in Figure 5 are 4 of the 5 runs with the lowest number of updates submitted (Table 3).
Noting how the ranking of systems can change greatly based on the wide range of parameter settings in our sweep, we decided to find the instances in our parameter sweep for which a particular system was ranked the highest across all parameter sets. Table 3 shows the results of this analysis on its right side. Some systems achieved their best rank for multiple parameter sets. In such cases, we chose the parameter set for which the system had the highest MSU.
The relationship between time spent and user performance can be seen as we look at the parameter settings from top to bottom of Table 3. Systems that had very few updates submitted, performed well for users who might visit a system about once a day for 30 seconds to 30 minutes on average. The run cluster2 seems to be the best performing system for users who return to the system about every 3 hours for 30 seconds on average. As we go lower in the table, we see that spending more time reading (larger M D ) and taking shorter breaks (smaller M A ) improves performance of systems that ranked lower on ELG. These systems are typically those that submitted a large number of updates.

As Table 3 shows, almost all groups have at least one run that is able to achieve a rank 1 performance under some setting of MSU X  X  parameters. If we want to know which system is the best, we must calibrate MSU given actual user behavior. This result may also show that submitted TST systems had very different notions of the appropriate amount of material to make available to users. TST X  X  ELG measure is set-based, but the decision of how large of a set to return to users is left to system designers. In contrast, MSU guides system designers to return a suitable amount of material based on the model of user behavior.
Our work builds directly on recent research in modeling user behavior for improved evaluation of information re-trieval systems [7]. Of these new effectiveness measures, Clarke and Smucker X  X  [8] Time Well Spent (TWS) measure is the most similar. Like modeled stream utility (MSU), TWS provides a means to evaluate systems that produce a stream of content. Unlike TWS, MSU models multiple vis-its to the same system, and MSU models variation in each simulated user X  X  behavior. MSU should be amenable to the same statistical analyses that TWS enables.

Other researchers have also evaluated systems by defin-ing a hypothetical user interface and then simulating user behavior with the interface. Of the more recent work that goes beyond a single query and results list is that of Baskaya, Keskustalo, and J  X  arvelin [6] who conducted an experiment that considered many different parameter settings for their model and investigated various query reformulation strate-gies that could be utilized by a user. Yang and Lad [15] also employ a user model based evaluation for information distillation systems and it would be interesting to compare MSU with their method in future work. Figure 5: Maximum correlation of MSU/second with ELG is obtained with parameters ( M A = 24 hours, S A = 12 hours, M
D =1minute, S D =1minutes, L =0.1).
ELG has aspects that try to capture characteristics of good updates, i.e. updates should be on time, not long, and have relevant material. ELG seems to be a measure oriented towards measuring the performance of systems that push highly relevant updates with low frequency to the user. ELG does not provide an easy means to be calibrated to known user behavior.

Our experiments and analysis show that it matters how much material is read. By specifying the amount of ma-terial in user terms, we have a way of then calibrating a measure once we know actual user behavior. Observing ac-tual user behavior while evolving events are actually tak-ing place would involve monitoring users X  browsing histories. The sudden nature of news events makes a live user study difficult to organize. Search log-analysis may provide some indirect insight into user behavior when such events are run-ning. Our analysis also shows that there may be a case for personalization of stream filtering systems for different user behaviors. As other future work, we hope to extend our un-derstanding of MSU, both in terms of its formal properties and in terms of empirical meta-evaluation criteria, includ-ing robustness to noise, discriminativeness between systems, and strictness [3].
We introduced an effectiveness measure that utilizes a model of user behavior for evaluating systems producing streams of information about evolving events. Our measure is designed to be calibrated based on actual usage data. Our user model simulates a user checking back with the system to read the most recent information from time to time. Users can check back with different frequencies and for different amounts of time depending on various factors. By model-ing user behavior, our effectiveness measure produces a score that is easily interpretable as the number of relevant nuggets of information read by the user. As would be expected, we found that for streams of updates, the gain is sensitive to the amount of time a user spends for reading updates. While temporal summarization systems have traditionally been evaluated in term of their precision and recall, we be-lieve that it is important to consider both the user interface and the user behavior with this interface when evaluating such systems. Given the degree to which the rankings of systems changed as we modified the behavior of the user population, we believe that an effectiveness measure such as ours when calibrated with user data will allow system de-velopers and researchers to build better performing systems tuned to user behavior. This work was made possible by the facilities of SHARC-NET (www.sharcnet.ca) and Compute/Calcul Canada, and was supported in part by GRAND NCE, in part by an Ama-zon AWS in Education Research Grant, in part by NSERC, in part by a Google Founders Grant, and in part by the University of Waterloo.
