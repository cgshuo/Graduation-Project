 We are now faced with finding  X  X rustworthy X  answers not only  X  X elevant X  answers. Although IR pioneered some of the approaches used on the web for locating relevant research. 
The main thrust of our talk will be based our experience in developing and apply-ing an answer trustworthiness-based QA model. Contrary to the past researches which focused simple trust factors of a document, we identify three different answer trust-worthiness factors. The following, we (1) review some past studies relative to identify trustworthiness of documents or answers; (2) illustrate an overview of the proposed QA model; (3) describe our three different answer trustworthiness factors in detail; (4) analyze the effect of the proposed QA model with several experiments; (5) finally conclude with a suggestion for possible future works. relevant documents, but high-quality documents as well. 
QA systems have proven to be helpful to users because they can provide succinct answers that do not require users to wade through a large number of documents. As same with our model, Ko [1] applied a probabilistic graphical model for answer rank-ing, which estimates the joint probability of correctness of all answer candidates. However, they focused on just candidate answers X  correlations, not their trustworthi-ness or quality. 
Trust is also an important area in question answering, since contradictory answers can be obtained from diverse sources in answer to a question. JiWoon [2] extracted a set of features from a sample of answers in Naver, a Korean QA portal similar to Yahoo! Answers. They built a model for answer quality based on features derived from the particular answer being analyzed, such as answer length, number of points service that is attached to a community-based question answering web site and achieved significant improvement in retrieval performance. 
According to Su [3], the quality of answers in commercial QA portals was good on average, but the quality of specific answers varies significantly. Agichtein [4] investi-gated methods for exploiting the community feedback to automatically identify high quality content. They introduced a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. In particular, for the community QA domain, they showed that their system is able to separate high-quality items from the rest with accuracy close to that of humans. 
As the latest research, Cruchet [5] presented a solution using an existing QA sys-tem and investigated whether the quality of the answers extracted depends on the the HON question answering system. 
Most of works were based on only document quality analysis and a few researchers tried to use the quality of answers in a collection of question and answer pairs in the retrieval process. They used simple trust factor for improving only the quality of document in IR or answers in QA field. On the other hand, we propose a QA model using three different answer trustworthiness factors: 1) in the document layer incorpo-rating document quality; 2) in the answer source layer representing the authority and reputation of answer sources; 3) in the sub-QAs layer verifying the answers by con-sulting various QA systems. Fig. 1 illustrates our system overview, which consists of Question Analysis , Answer Manager, and Multiple sub-QA modules based on several Knowledge bases generated by Answer Indexing from heterogeneous Sources . In this section, we describe how to analyze user questions and how to select the best appropriate answer candidate for the factors, to be explained in the Section 4, are combined. 
A user question in the form of natural language is entered into the system and ana-lyzed by the Question Analysis component that employs various linguistic analysis techniques such as POS tagging, chunking, answer type (AT) tagging [6], and some semantic analysis such as word sense disambiguation [7]. An internal question gener-ated from the Question Analysis has the following form: theme of the question, and SS is the information related to the expected answer source or QA module from which the answer is to be found [8]. 
Our QA framework using a learnable strategy makes use of a number of independ-ent QA modules employing different answer finding methods. A strategy that deter-mines the QA modules to be invoked when finding an answer is selected based on expected answer sources [8]. The Answer Manager selects a strategy based on the internal query, translated by Question Analysis for the given question, invokes one or more QA modules depend-ing on whether the answer confidence for each answer candidate is strong enough, and finalizes the answer by incorporating answers and their evidence values returned from multiple QA modules if necessary. For example of  X  How to play Starcraft be-tween 2 computers connected by a router? X  , the question should be analyzed as: then, the strategy for invoking community-QA is selected. When the candidate answer from the community-QA is higher than a threshold, the candidate can be served as the final answer to the user. If not, other QA modules are activated in sequence as in the strategy until the stopping condition is satisfied [8]. 
The goal of the Answer Manager is to determine which answer candidate has the maximum confidence value to support for the given question. Let Q and a be the user question and a candidate, respectively. Then the final answer a * is selected by: the chosen strategy by the Question Analysis module, C(a, Q) is defined as follows: where S(a, q i ) is the semantic distance between the answer candidate and the original date a , and tw (1to3)i are three answer trustworthiness factors to be explained in the next Section. Semantic distance values can be computed with a lexical database [7] like Korean WordNet. When the original question looks for a  X  X ocation, X  for example, it can be matched with  X  X ity, X   X  X rovince, X  or  X  X ountry X  with a decreasing values of semantic distance. Additional details can be found in [8]. We now focus on the task of determining trustworthiness of candidate answers and describe our overall approach to solving this problem. As shown in the Fig 1, we identify a set of trustworthiness factors for three different layers: document layer (Section 4.1), answer source layer (Section 4.2), and sub-QAs layer (Section 4.3). All these factors are used in Answer Manager as an input to the ranking equation (3) that can be tuned for confidence value for a particular answer candidate. 4.1 tw 1 : Trustworthiness at the Document Layer The first trustworthiness factor, tw 1 , represents the intrinsic quality of answers and it is determined depending mainly on how the document X  X  contents are informative and the big potential of containing answers. The informative documents can be detected by following processes: 1) spam filtering, 2) duplicate detection, and 3) document quality evaluation [9]. spam, and ham (valid document) using SVM (Supported Vector Machine [10]) classifi-cation algorithm. Duplicated documents can be detected by breaking a document into sentences and comparing them with sentences of original documents. For efficiency of indexing process, duplicated documents should be filtered whereas  X  X uplication X  is an only keep the original document, while remaining the number of duplication times. 314 H.-J. Oh et al. 
Document quality is evaluated according to seven quality metrics reflecting text-related features [9], such as: After the document quality scores are normalized from 0 to 1, we cut off documents which have lower scores than a threshold at the Answer Indexing stage. 4.2 tw 2 : Trustworthiness at the Answer Source Layer reliable . As shown in Fig 1, we collect documents from heterogeneous sources such as public media including news article, personalized media including blogs, internet community board articles, and commercial co ntent providers (CPs). In usual, public media contents are more liable than blogs or community boards. Contrary to general expectations, in some cases, a particular community board might have a high reputa-when it is extracted from the official user BBS developed by  X  X lizzard Entertain-ment X  company, by which the game was created. It indicates that answer sources have different authority scores according to the question theme (QT), especially the ques-tion target. We built 1,000 &lt;question, answer&gt; pairs which are annotated the question answer sources according to 137 high-frequently asked question targets. 
To calculate the reputation score of answer sources, we also use four reputation metrics reflecting non-textual features such as: The final tw 2 can be calculated by authority scores and reputation scores in a ratio of five to three. 4.3 tw 3 : Trustworthiness at the Sub-QAs Layer The third trustworthiness factor, tw 3 , represents the relevance to questions. It is deter-mined depending on how the answering strategy is appropriate for the given question. As explain in Section 3, the strategy invokes the most suitable sub-QA module and attempts to verify the answers by consulting other modules. The performance of our selected based on the sub-QA source (SS) of the analysis result, which determines the sub-QA module to be invoked first. Invoking more than one module can compensate any possible errors in the Question Analysis and in the answers from a sub-QA module. In other words, answers from the first sub-QA module are verified, and their confidence values are boosted if appropriate [8]. 
For strategy learning, we use 250 question/answer pairs of training data of various sorts in terms of sub-QA sources and difficulty levels, which are part of the entire set and four QA modules without loss of generality. To evaluate our proposed QA model, various information of answer sources should be collected including non-textual data such as content provider X  X  name, publication date, and RSS feed tags. Therefore, we decide to develop a vertical QA focused on the  X  X ame X  domain and analyzed the entire answer sources in advance. Nonetheless, it does not mean that our proposed methodologies are biased on a specific-domain. For effectiveness comparisons, we employed a mean reciprocal rank (MRR, [11]). We also used F-score , which combines precision and recall , with the well-known  X  X op-5 X  measure that considers whether a corr ect nugget is found in the top 5 answers. Like the TREC QA track [11], we have constructed various levels of question/answer types. We collected 5,028 questions from the Korean commercial web portal logs (Naver tm Manual QA Service (http://kin.naver.com), Paran tm Game IR Service (http://search.paran.com/pgame/). Among them, a total of 250 question/answer pairs were used for building and tuning the system, and an additional 140 pairs were used for evaluation. answer indexing stage in document layer. If a document X  X  tw 1 value exceeds a prede-termined threshold, the document can be indexed as an answer candidate source. Otherwise, the document is considered as a noise then we ignored it. When the cut off threshold was 0.3, the total documents reduced by 96%; accordingly the indexing time also reduced by 92 % (28,993 to 2,293 min.) 
Contrary to the first trustworthiness factor, the second and third factors, tw 2 and tw 3 , can be determined at answering stage in real-time since they reflects relatedness with the given user question and candidate answers. To see the value of the impact of trustworthiness in answer selection, four cases were examined: (1) a traditional QA, For a traditional QA (the baseline), C(a, Q) in equation (3) is simplified as follows: To archive the best performance for each case, we optimized not only the four differ-ent QA models but also individual sub-QA modules in Fig 1. The overall comparison shows in Table 1. 
As shown in Table 2, the final MRR values for the traditional QA (baseline), using 0.506, respectively. Our proposed model using all factors also shows the highest overall performance in both F-score on Top1 (0.464) and Top5 (0.571). 316 H.-J. Oh et al. 
The relative improvement shows the impact of each trustworthiness factor. In 100% (0.150 to 0.310), whereas the tw 2 addition obtains only 24.2% (0.310 to 0.385) using tw 1 and tw 2 . By reflecting this observation, the final QA model showed the best accuracy (0.506 MRR) when we set weight values,  X  ,  X  , and  X  , in Equation (3) respec-tively are 0.5, 0.2, and 0.3. 
To sum up experimental results, the proposed method using all answer trustworthi-ness factors showed dramatic improvement: 237% (0.150 to 0.506 MRR) for answer-ing effectiveness and 92% (28,993 to 2,293 min.) for indexing efficiency. These but also provide better answers, while suppressing unreliable ones. This paper proposed a QA model based on answer trustworthiness. Contrary to the past researches which focused simple trust factors of a document, we identified three different answer trustworthiness factors: 1) incorporating document quality at the document layer; 2) representing the authority and reputation of answer sources at the answer source layer; 3) verifying the answers by consulting various QA sys-tems at the sub-QAs layer. 
To prove the efficacy of the proposed model, we analyzed the impact of answer trustworthiness in indexing stage as well as the answering stage. In indexing, distill-ing unreliable documents which have lower trustworthiness value than 0.3 brings not only 96% reduced document size but also 92% speedy indexing time. To reveal trust effects on answering, we have conducted experiments for four different cases: (1) a proposed method using all answer trustworthiness factors obtained an improvement over the traditional QA by 237% in effectiveness. 
We plan to improve each answer trustworthiness calculation method extending various features. In addition, to overcome false-alarm of reliable document in the indexing process, a dynamic cut-off scheme will be developed. This work was supported in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No. 2008-S-020-02. 1. Ko, J.W., Si, L., Eric, N.: A Probabilistic Graphical Model for Joing Answer Ranking in 3. Su, Q., Pavlov, D., Chow, J.-H., Baker, W.C.: Internet-scale collection of human-reviewed 4. Agichtein, E., Castillo, C., Donato, D.: Finding High-Quality Content in Social Media. In: 5. Cruchet, S., Gaudinat, A., Rindflesch, T., Boyer, C.: What about trust in the Question An-
