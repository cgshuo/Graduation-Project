 Lisa Friedland lfriedl@cs.umass.edu David Jensen jensen@cs.umass.edu Michael Lavine lavine@math.umass.edu The following tasks come from different domains, but they share a common core:  X  Can we infer social ties among people whose  X  Can we detect (and block) coalitions of attackers  X  Can we identify duplicate records to be merged in  X  Can we determine with confidence whether Each task concerns data in which most entities (people or records) are distinct and independent, but certain pairs or small groups are unusually similar. The simi-larity reflects an unobserved link we would like to de-tect, such as  X  X hese people are acting in coordination X  or  X  X hese are two traces of the same object. X  This class of problems arises in fields such as so-cial network analysis (Adamic &amp; Adar, 2003; Bejder et al., 1998), entity resolution (see Section 3), fraud and plagiarism detection (Friedland &amp; Jensen, 2007; Sorokina et al., 2006), security (Yang et al., 2011) and forensics (Committee on DNA Forensic Science, 1996). From a privacy perspective, we ask the same question with an opposing goal: when is an individual X  X  behav-ior or attributes distinctive enough to be identifiable across multiple sightings (Whang &amp; Garcia-Molina, 2011; Narayanan &amp; Shmatikov, 2008)? Many of these applications are longstanding, well-studied problems, but each is addressed separately. This motivates us to connect them as instances of a single formal task. In these problems, the goals are to identify the links and to assess their significance. Intuitively, a pair is more likely to be linked the more the entities are similar and the more the entities (or merely their shared aspects) are rare . (Pairs can also occur in dense regions, but those pairs will be less distinguish-able.) Across the literature, numerous measures of pair strength have been developed. These usually de-scribe the similarity of the entities, and sometimes also their rarity. Some measures are probabilistically based, and almost all are domain-specific.
 We, instead, explicitly model how both paired and non-paired entities are generated. With a likelihood ratio that compares the paired and non-paired models, our method takes into account both similarity and rar-ity. We work with the simplest of systems X  X ontinuous data and Gaussian distributions X  X n order to minimize domain-specific aspects and focus on these questions:  X  Supposing we knew everything about a domain,  X  Do we even need a model, or will a simple  X  As we approach realistic scenarios, in which the In Section 2 of this paper, we present a generative model for continuous data in k dimensions, and for inference, a likelihood ratio score ( X  LR  X ) to compute for every pair. In the synthetic data of Section 5, we find that one key parameter most affects performance: t , which describes how far apart the linked pairs may be. We compare LR to baseline methods that mea-sure only similarity of pairs ( X  d  X , for distance), only rarity, or sub-optimal combinations of the two. Sur-prisingly, we find that d can perform almost as well as LR  X  X hat is, rarity doesn X  X  matter X  X ut only for the easiest problems, those with the smallest values of t . By examining the theoretical distributions of positive (i.e., linked) and negative (non-linked) pairs, we are able to explain why this happens.
 Moving towards situations where parameters are un-known (and true labels might be unavailable), we ex-amine performance when our estimate  X  t mismatches the model and discover it governs the score X  X  balance of similarity vs. rarity. When the optimal t is un-tive. In Section 6 we apply the model to two real data sets constructed to be labeled instances of this task. As we vary  X  t , the performance trends are compara-ble to those in synthetic data. We find that both real data sets are in a middle range of difficulty, a range where performance is only moderate, but where LR distinctly outperforms d . The model below makes the following assumptions, which are reasonable for many applications. First, the number of linked entities is low. Second, the linked en-tities appear only in disjoint pairs, not larger groups. Third, the non-linked entities X  X he vast majority X  X an be modeled as being independently generated from some distribution  X  . Finally, the pairs can be mod-eled as being generated jointly in a process  X  that in-volves  X  but also involves a distribution keeping pairs close together. We deliberately keep the model simple so that we can study the effects of parameter choices. Yet it is flexible, in that arbitrary domains and distri-butions could be swapped in with different choices of  X  and  X  ; in particular, one could specify an that makes pairs be far apart or in another specific configuration. 2.1. Generative Process and Task The output will be n points, x 1 ,..., x n in R k , where some pairs are generated together. Let  X  be the dis-tribution of singleton points. Let  X  be the process for generating pairs; within  X  , we must specify , a distri-bution by which pairs of points are displaced from their common midpoint. Two variables are unobserved: r , the actual number of pairs, and C = { c ij } , a (binary) adjacency matrix describing which points are in pairs. We control the number of pairs with the variable q , such that the expected number of pairs E ( r ) = qn . When c ij = 1 we say that the points x i and x j form a pair (or a link ), or equivalently, that the pair is posi-tive ; when c ij = 0 we say that the points are singletons or that the pair is negative .
 The generative process is as follows. First, choose how many and which points are in pairs. 1. Generate r , the number of pairs: 2. Generate C = { c ij } uniformly from among all ma-At this stage, for each x i , we know whether it will be a singleton or part of a pair with x j . 3. Generate x 1 ,..., x n : This is essentially a mixture model for the data: one mixture component is a distribution of points (  X  ), the other is a distribution of pairs (  X  ). The distributions are connected in that  X  uses  X  : the pairs X  midpoints are generated the same way as the singleton points. 2.2. Inference In this paper, we never explicitly infer r or C . Instead, to make inference efficient, we reason about each pos-sible link as if it were independent of the others. We produce a likelihood ratio for each c ij and evaluate this ranking against the true set { c ij } . The likelihood ratio (below) is rank-equivalent to the probability of the pair being positive: P ( c ij = 1 | x ) = LR 1+ LR . We approximate, for every pair of points: Line (2) is an application of Bayes X  Rule. In Line (3), we use Step 3 of the generative model to write out the likelihoods for positive and negative pairs, respectively. The generative process for positive pairs was described in terms of m ij and d ij , so the most natural way to write its likelihood function would be P ( m ij , d ij | c 1) = P ( m ij |  X  ) P ( d ij | ). Since Lines (2) and (3) are written as functions of ( x i , x j ), we have to perform a change of variables; the mapping is one-to-one but introduces the constant 1 2 k (see Lemma 8.1 1 ). The term for the prior P ( c ij = 1) is r divided by the to-tal number of pairs, so 2 r n ( n  X  1) when r is known. When r is unknown, we compute the term by summing over possible values 2 of r (Eq. (4)). In Eq. (5), P ( r = k | q ) is expanded using r  X  Binomial( n/ 2 , 2 q ). In either case, P ( c ij = 0) = 1  X  P ( c ij = 1).
 2.3. Limitations of this Inference Method The output of inference is a list of likelihood ratios, one for each potential pair. We can turn this into a discrete set of positive pairs, if desired, by threshold-ing the scores. One drawback to treating each pair as independent is that, in violation of the generative model, the resulting (thresholded) adjacency matrix  X  C may assign points to more than one pair. We could remedy this situation with additional post-processing (instead of or in addition to the thresholding), keeping only the highest-probability links. Alternatively, we could reconsider the model X  X  assumptions: if a point is matched to more than one pair, we may have under-estimated  X  in that region or the points may actually belong to a group of more than two. It could be a strength if the method is able to detect such groups when the generative process only describes pairs. Another way to avoid assigning any point to more than one pair would be to infer the full C : compute P ( C l | x 1 ,..., x n ) for every valid matrix C l and choose the one with maximum likelihood. This would be com-putationally challenging: for a typical data set in this paper, there are more than 1 . 6  X  10 16 such matrices. Another simplification is that we model all negative pairs as if they were formed by singleton points. In truth, of the n ( n  X  1) 2  X  r negative pairs, 2 r ( n  X  r  X  1) of them involve at least one point from a positive pair. As r rises from 1 to n 2 , the fraction of non-modeled pairs increases from near-0 to near-all of them. In Section 5.4, we discuss how these non-modeled negatives can under certain circumstances affect performance. This task differs from clustering in that our expected clusters (links) are tiny and rare; if the data does con-tain large-scale clusters, they should be modeled in  X  so that we can recognize deviations from them. The task has more in common with significance testing: we want to distinguish true pairs from singletons that are close together by chance. It can also be seen as an anomaly detection problem (Chandola et al., 2009), not in the generic sense of  X  X utlier detection X  but in the sense of  X  X etecting a specific unusual pattern. X  In that vein it is similar to Eskin X  X  (2000) mixture model of normal and anomalous elements.
 One central related task is link prediction in social net-works based on shared interests or behavior. Adamic &amp; Adar (2003) develop a score to combine rarity with similarity of shared interests; Liben-Nowell &amp; Klein-berg (2007) compare a variety of distance measures between nodes in an observed network; and Friedland &amp; Jensen (2007) compute the rarity of the shared com-ponent of people X  X  job histories. Most similar to our work is a generative model by Crandall et al. (2010) in which pairs of friends travel to locations together. The other closely related area is entity resolution, or record matching (Elmagarmid et al., 2007; Winkler, 2006). That literature, while extensive, makes some key assumptions that prevent its methods from being directly transferable here. Generally the duplicates to identify are database records that correspond to the same real-world entity, and the records consist of text fields such as names and addresses. Although numer-ous text comparison metrics have been developed, lit-tle has been done with continuous data. Finally, that work does not restrict links to be rare or disjoint. One popular text matching function explicitly incor-porates rarity: it weights each word (or substring) by its tf  X  idf measure, then takes the cosine similarity of the resulting vectors (Cohen et al., 2003). Chaudhuri et al. offer a complementary approach in which, re-gardless of the distance measure, clusters are required to be both close together and in sparse regions (2005). Much of probabilistic record matching is based on the Fellegi-Sunter model (1969). It ranks pairs by the like-pair X  X   X  X omparison vector. X  If  X  is merely a distance measure, then that model would be like our baseline LR [ d ] (see Section 5.2). Since typically  X  also encodes which particular words match, the resulting score is higher when matching strings are rare. Our likelihood ratio of Eq. (3) could be seen as a general form of the Fellegi-Sunter model, in which  X  is the points them-selves ( x i , x j ), and in which P (  X  | c ij ) is provided by the generative model rather than estimated from data. Compared to related tasks, our work X  X  strength is in abstracting away the domain-specific elements, allow-ing a focus on the problem X  X  more general principles. We evaluate performance by comparing a ranked list of predicted pairs to the set of true pairs, calculating the AUC (area under the ROC curve) of the ranking. We considered other common measures of ranking such as average precision or Hand X  X  H measure (2009), but they were unsuitable because, unlike AUC, they fluc-tuate when the number of true positives or negatives does. In realistic scenarios it may also be important to focus attention on the very top of the ranked list or on the individual probability estimates. These paths are left to future work.
 For present purposes, the ranked list contains all pairs. In larger data sets, efficiency would become a concern, as it is in entity resolution. Existing techniques from that literature address efficiency either by making the score calculation faster or by scoring only those sub-sets of pairs that are judged similar according to some preliminary measure (Elmagarmid et al., 2007). Mc-Callum et al. (2000) describe a method for continuous data that could be used here: in each dimension, cre-ate overlapping bins for the data, and only consider pairs that lie within the same bin in some dimension. For the data sets in this paper and practical values of parameters, applying this method, i.e., filtering out pairs with a high d ij , would probably bring gains in efficiency at little loss to performance. In this section, we study the behavior of the algorithm when the data has been generated by the model. For the following analyses and experiments we set  X  and to be radially symmetric normal distributions:  X  = Normal(  X  , X  2 I ), and = Normal( 0 , X  2 I ). 5.1. Simplifying the Score Starting from Eq. (3), we plug in normal probability density functions for the terms involving  X  and :
P ( m ij |  X  ) P ( d ij | )
P ( x i |  X  ) P ( x j |  X  ) For Eq. (8), we have defined m = k m ij  X   X  k = k the subscript ij when it is clear from context) and ap-plied Lemma 8.2.
 Substituting the densities back into Eq. (3) X  X  likeli-hood ratio gives: P ( c ij = 1 | x i , x j )
P ( c ij = 0 | x i , x j ) The likelihood ratio in Eq. (9) is fairly simple: instead of depending on the full data vectors x i and x j  X 2 k coordinates in all X  X t uses just two measures of the pair, m and d .
 We assume (for now) that the model parameters are available at inference time. Among them, n and r individual scores, but not the ranking. We also need  X  and  X  . However, it turns out we can rewrite the score as a function of their ratio t =  X   X  . Eq. (10) shows the final, reparametrized LR as a function of m 0 = m  X  , d = d  X  , and t =  X   X  without  X  : In the rest of Section 5, we will address (a) how the task X  X  difficulty is affected by model parameters (pri-marily t , but also the dimensionality k , the number of points n , and the number of pairs r or q ); (b) how the score for an individual pair varies as a function of t and its ( m 0 ,d 0 ) values (Section 5.3); and (c) how performance is affected by changing the value  X  t used during inference (Section 5.4). 5.2. Performance on Synthetic Data For synthetic data experiments, given any parameter setting of n , q and t , we generate 100 data sets from the model. Within each data set, we score every pair and evaluate the AUC of the ranked list compared to the true pairs. These experiments use k = 2 dimensions and (without loss of generality)  X  = 1.
 The likelihood ratio ( X  LR  X ) of Eqs. (3) and (10) is the Bayes estimate for distinguishing positive from nega-tive pairs, so it should perform close to optimally, de-pending on how closely the data matches the two mod-eled classes. We compare it to four baseline methods. One, d , measures only the similarity of points in a pair: it ranks by d ij , the distance between the points, with smaller distance meaning more likely positive. The second, m , measures only the rarity (i.e., local sparseness) of the pair: it ranks by m ij , the distance from the origin to their midpoint, with higher distance meaning more likely positive. It can be seen from Eq. (10) that using m (or m 0 ) is rank-equivalent to using LR if d 0 is held constant. Likewise, using d (or d 0 ) is rank-equivalent to using LR if m 0 is held constant X  provided that 1 t 2 &gt; 2, or t &lt; 1 /  X  2  X  0 . 71. Generally we will use t 1, so this will be the case.
 The third baseline, called LR [ d ], is a likelihood ratio designed to take into account only d , not m . It is com-is similar to Eq. (10), but the discriminant function in the exponential reduces to d 0 2 2  X  1 t 2 . The fourth bine the the terms for similarity and for rarity. But it is actually a reasonable approximation to the full LR of Eq. (3) when d is small enough, because in that case P ( m |  X  )  X  P ( x i |  X  )  X  P ( x j |  X  ) and the terms cancel out. In the synthetic data, this method is rank-Figure 1 shows performance as we vary t for one set-ting of ( n,q ). (Other settings were similar.) The re-sults can be divided into three realms. First, when t is very low (see inset), the AUCs of both LR and d are almost perfect. LR is always above d , but they are nearly indistinguishable. Next, as t approaches 1 /  X  2 , both LR and d drop, and they diverge; at its mini-mum value, LR matches m , while d is nearly 0.5, or random. When t &gt; 1 /  X  2 , LR increases again, while d continues to decrease, now ranking pairs in the wrong order. Meanwhile, m is much lower and steady. The third and fourth baselines each partially augment d : LR [ d ] is identical except that it changes the direction performs near optimal for low t , but it does not change 5.3. Understanding Performance Conceptually, we can explain why t = 1 /  X  2 is al-ways a turning point, regardless of the form of  X  . In each dimension l , d l = x il  X  x jl 2 , so for negative pairs, E( d l | X  ) = 0 and Var( d l | X  ) = 1 2 Var( x l ) =  X  2 l positive pairs, by definition Var( d l | +) =  X  2 l = ( t X  so when we set t = 1 /  X  2 , the positives X  Var( d l | +) = matches that of the negatives. In these experiments, not only do the variances of d match at t = 1 /  X  2 , but since  X  and are normals and is centered at 0, the distributions of d l are normals, identical for the posi-tive and negative pairs. Therefore d contains no dis-tinguishing information, and LR is only using m . At higher t , the positives become farther apart, on aver-age, than the negatives.
 We next examine how the LR score of an individual pair combines the two measures of it, m 0 and d 0 . Figure 2 shows that the score increases when m 0 increases; for the boxes in which t &lt; 1 /  X  2 , the score increases when d 0 decreases, and when t &gt; 1 /  X  2 , the score increases when d 0 increases, as discussed above. At t  X  1 /  X  2 the contour lines are vertical, which shows visually that the only information is contained in m . Now, con-sider the smallest setting of t , in which empirically d performs almost as well as LR. The contour lines in the first box are almost horizontal, indicating that d 0 contains almost all the information (in the LR score, two methods are almost indistinguishably strong. Figure 2 becomes more informative once we know not only what score is assigned to a given position, but also the distributions of positive and negative pairs along these axes. It turns out that with normal distributions for  X  and in R k , the distributions of positive and negative pairs have closed forms (full derivations are in Section 8.2). Each distribution is a product of two independent  X  k distributions, one describing m 0 , one describing d 0 : The peak of  X  k is at peak is at (1 ,t ) for the positive pairs and ( 1 /  X  2 , 1 / for the negatives. As t changes, the only effect is on the d 0 dimension of the positives. Visually, it is clear that the distributions are well separated at small t and begin to overlap as t grows. In higher dimensions, the distributions become better separated (see Section 8.3), so the task should become easier as k increases. 5.4. Sensitivity to Parameters and to When n increases or q decreases, intuition suggests that since true pairs are less frequent, the problem get harder. However, since AUC is unaffected by changes to class proportions, a glance at the class distributions of Figure 2 should help solidify the (more relevant) in-tuition that changing the number of positives or nega-tives will not affect the separation between the classes. At inference time, if we mis-guess q , the probability es-timates for pairs change, but the LR ranking does not. At data generation time, the situation is more subtle. For a given n , as the number of pairs increases towards n/ 2, the performance of LR can actually decrease X  but only for large t &gt; 1 /  X  2 . This is due to interference of the non-modeled pairs described in Section 2.3: at large t , the positive points no longer resemble the sin-gletons, so the majority of negatives no longer resemble the modeled negatives. However, we observe no such performance effects with smaller t .
 In many realistic problem scenarios, we will not know q nor, more importantly, t . Figure 3 shows how per-formance degrades when using an incorrect value  X  t for inference. For LR ,  X  t determines the balance between d and m 0 , and the direction of d 0  X  X  effect. When  X  t ap-proaches 0, LR approaches d ; when  X  t reaches 1 /  X  2 , LR matches m , then continues to drop; and the optimal is surprisingly robust: when  X  t is underestimated, per-formance drops just like LR  X  X , but when  X  t is overesti-has no turning point in its use of d : as  X  t  X  X  X  , P ( d | ) merely puts less weight on d and eventually converges to m . Meanwhile, LR [ d ] simply matches d , and its AUC flips to (1  X  d ) when  X  t &gt; 1 /  X  2 .
 The implications for data sets with unknown param-eters can be summarized as follows. Mis-guessing q does not affect the ranking, and our inference meth-ods seem to work well even when the data contains a large number of pairs, as long as t &lt; 1 /  X  2 . As long as we know positive pairs are closer together than neg-ative pairs, then when using LR ,  X  t should always be less than 1 /  X  2 . Finally, mis-guessing t can be harmful, but there are several options for avoiding the perfor-mance drop-off: (a) use d , which is parameter-free and often performs well, (b) underestimate t , rather than overestimate it, to ensure performance will not drop overestimates of t .
 To apply this model to an arbitrary data set in R we need to specify several parameters. The distribu-tion of singletons is straightforward: estimate  X  (of any desired form) from the entire data set. For pos-itive pairs, we preserve the generative process  X  in which m  X   X  and d  X  . We let remain a nor-mal, but it should no longer be radially symmetric, since the variables might be at different scales. We define the vector version of t such that t l =  X  l  X   X  each dimension l , where  X   X  l is the (empirical) estimate of the variance of the negatives. Then we can write d  X  =Normal(0 , t 0  X   X   X  1 t ) where  X   X  is a diagonal co-variance matrix estimated from the data. As before, the key parameter to specify is t , which describes the distance between the positive pairs. That distance will match the negative pairs when t = 1 /  X  2 (1 , 1 ,..., 1). The baseline methods d and m can be generalized as P ( d | ) and 1 P ( m |  X  ) , respectively. When all the components of t are equal, P ( d | ) becomes rank-equivalent to a natural k -dimensional measure, scaled Euclidean distance. The method LR [ d ] requires an es-timate of P ( d | c ij = 0); for this, we fit a normal to the set of all pairwise displacement vectors d . 6.1. Data sets The Matched Multiple Birth Data from the National Center for Health Statistics (2000) contains infant birth and mortality data for all twins and larger multi-ples born in the U.S. from 1995 X 2000. In this data, two variables could potentially serve to re-identify paired infants: birthweight (grams) and Apgar score (a 0 X 10 assessment of newborn baby health). True pairs of twins might be expected to have one baby larger and healthier than the other. Yet tests of a sample of twins show the pairs X  values are correlated (with a Pearson correlation of 0.79 for weight, 0.44 for Apgar), so there is at least some signal for the algorithm to work with. The second data set is derived from the Reality Mining data, cell phone data collected from 94 students and faculty over a nine-month period (Eagle &amp; Pentland, 2006). Our task instances address the question  X  X s an individual X  X  phone usage pattern distinctive enough to identify them? X  We summarize each user X  X  weekly be-havior with seven aggregate features: total commu-nication events; number of distinct contacts; number of calls made, received, and missed; number of SMS X  X  received and sent. Each such person-week becomes a point in a data set, and the pairs are defined as in-stances of the same individual in two different weeks. From each data source, we construct 100 labeled in-stances of the pair detection task. An instance of twins data consists of five pairs of twins and 90 singleton babies. An instance of cell phone data consists of five pairs of person-weeks and 75 singletons. In the exper-iments below,  X  is always a normal distribution with diagonal covariance. 6.2. Experiments and Results Since we know ground truth, we can experiment here with different values of  X  t . It has one component for each variable, and for these domains all we know in advance is that pairs should be  X  X lose together X  X  X .e., each component is in the range (0 , 1 /  X  2 ). For the two-variable twins data, we explore a grid of possible val-ues. For the seven-variable cell phone data, the expo-nential state space becomes a problem, so we restrict  X  t to the form a  X  (1 , 1 ,..., 1) for some constant a . Figure 4 shows that the methods behave very much the same way on real data as they do on synthetic. As alternative when  X  t is unknown.
 The grid search on twins data reveals that when we vary the individual components of  X  t , this affects the relative strengths of the variables. For instance, set-ting  X  t weight = 0 . 001 (stringently small) but leaving  X  t apgar = 0 . 7 (flexible) is almost equivalent to ranking only by d weight . For a fixed ratio among the compo-nents of  X  t , the relative strengths of the variables are held constant, and only the balance with m will vary. As a comparison, we also estimate a best fit t from a large sample of twins: that ( t weight ,t apgar ) = (0 . 33 , 0 . 57) is not far from the  X  t = (0 . 3 , 0 . 5) found by searching. Separate experiments with single variables show that for twins, weight is a strong feature, but Ap-gar is not. With Reality Mining, the strongest features are number of SMS X  X  sent and number of contacts. It is not surprising that both these tasks turn out to be difficult given their respective feature sets; in par-ticular, it has been noted that for the Reality Mining data, phone communication is not nearly as consistent as proximity patterns (Eagle et al., 2009). If the trends of Figure 1 generalize to here, then the relatively low AUCs may go hand in hand with the high values of  X  t and the performance boost of LR over P ( d | ). This paper introduces a simple model for the task of distinguishing tightly linked pairs from singleton points, given a mixture of both. This task has not been previously described in a general form, although spe-cific instances have been studied in numerous contexts. From the generative model, we derive a likelihood ra-tio incorporating both the similarity and rarity of the pairs. A single parameter describing the distances be-tween pairs turns out to govern the task X  X  difficulty; at inference time, this same parameter describes how to trade off a pair X  X  similarity with its rarity. This method always outperforms using only similarity, but in a certain parameter range, similarity turns out to be surprisingly competitive. We discuss how to apply the model to real-world data sets having unknown pa-rameters. In the future, we intend to explore versions of this model for more complex domains.
 Adamic, L. A. and Adar, E. Friends and neighbors on the web. Social Networks , 25(3):211 X 230, July 2003. Bejder, L., Fletcher, D., and Br  X ager, S. A method for testing association patterns of social animals. Ani-mal Behaviour , 56(3):719 X 725, 1998.
 Chandola, V., Banerjee, A., and Kumar, V. Anomaly detection: A survey. ACM Computing Surveys , 41 (3):1 X 58, July 2009.
 Chaudhuri, S., Ganti, V., and Motwani, R. Robust identification of fuzzy duplicates. In Proc. 21st Int X  X 
Conf. on Data Engineering (ICDE 2005) , pp. 865 X  876. IEEE, April 2005.
 Cohen, W. W., Ravikumar, P. D., and Fienberg, S. E.
A comparison of string distance metrics for name-matching tasks. In Proc. IJCAI-03 Workshop on
Information Integration on the Web (IIWeb-03) , pp. 73 X 78, 2003.
 Committee on DNA Forensic Science: An Update, Na-tional Research Council. The Evaluation of Foren-sic DNA Evidence . The National Academies Press, 1996.
 Crandall, D. J., Backstrom, L., Cosley, D., Suri, S.,
Huttenlocher, D., and Kleinberg, J. Inferring social ties from geographic coincidences. Proceedings of the National Academy of Sciences , 107(52):22436 X  22441, December 2010.
 Eagle, N. and Pentland, A. Reality mining: Sensing complex social systems. Personal and Ubiquitous Computing , 10(4):255 X 268, 2006.
 Eagle, N., Pentland, A. S., and Lazer, D. Inferring friendship network structure by using mobile phone data. Proceedings of the National Academy of Sci-ences , 106(36):15274 X 15278, September 2009.
 Elmagarmid, A. K., Ipeirotis, P. G., and Verykios, V. S. Duplicate record detection: A survey. IEEE
Transactions on Knowledge and Data Engineering , 19(1):1 X 16, January 2007.
 Eskin, E. Anomaly detection over noisy data using learned probability distributions. In Proc. 17th Int X  X 
Conf. on Machine Learning (ICML 2000) , pp. 255 X  262, 2000. Morgan Kaufmann.
 Fellegi, I. P. and Sunter, A. B. A theory for record linkage. Journal of the American Statistical Associ-ation , 64(328):1183 X 1210, December 1969.
 Friedland, L. and Jensen, D. Finding tribes: Identi-fying close-knit individuals from employment pat-terns. In Proc. 13th Int X  X  Conf. on Knowledge Dis-covery and Data Mining (KDD 2007) , pp. 290 X 299, 2007. ACM.
 Hand, D. J. Measuring classifier performance: a co-herent alternative to the area under the ROC curve. Machine Learning , 77(1):103 X 123, October 2009. Liben-Nowell, D. and Kleinberg, J. The link-prediction problem for social networks. Journal of the Ameri-can Society for Information Science and Technology , 58(7):1019 X 1031, 2007.
 McCallum, A., Nigam, K., and Ungar, L. H. Efficient clustering of high-dimensional data sets with appli-cation to reference matching. In Proc. 6th Int X  X  Conf. on Knowledge Discovery and Data Mining (KDD 2000) , pp. 169 X 178, 2000. ACM.
 Metwally, A., Agrawal, D., and Abbadi, A. E. De-tectives: Detecting coalition hit inflation attacks in advertising networks streams. In Proc. 16th Int X  X 
Conf. on World Wide Web (WWW 2007) , pp. 241 X  250, 2007. ACM.
 Narayanan, A. and Shmatikov, V. Robust de-anonymization of large sparse datasets. In IEEE
Symposium on Security and Privacy , pp. 111 X 125, 2008. IEEE Computer Society.
 National Center for Health Statistics. Matched multi-ple birth data, 1995 X 2000. Public-use data file and documentation, 2000. URL http://ftp.cdc.gov/ pub/Health_Statistics/NCHS/Datasets/mmb2/ .
 Sorokina, D., Gehrke, J., Warner, S., and Ginsparg, P. Plagiarism detection in arXiv. In Proc. 6th Int X  X 
Conf. on Data Mining (ICDM 2006) , pp. 1070 X 1075, 2006. IEEE Computer Society.
 Su, C. and Srihari, S. N. Evaluation of rarity of finger-prints in forensics. In Advances in Neural Informa-tion Processing Systems 23 , pp. 1207 X 1215, 2010. Whang, S. and Garcia-Molina, H. Managing infor-mation leakage. In Proc. 5th Biennial Conf. on In-novative Data Systems Research (CIDR 2011) , pp. 79 X 84, 2011.
 Winkler, W. E. Overview of record linkage and current research directions. Technical report, U.S. Census Bureau, February 2006.
 Yang, Z., Wilson, C., Wang, X., Gao, T., Zhao, B. Y., and Dai, Y. Uncovering social network sybils in the wild. In Proc. Internet Measurement Conf. (IMC
