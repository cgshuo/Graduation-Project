 This paper examines important factors for link prediction in networks and provides a general, high-performance frame-work for the prediction task. Link prediction in sparse net-works presents a significant challenge due to the inherent disproportion of links that can form to links that do form. Previous research has typically approached this as an unsu-pervised problem. While this is not the first work to explore supervised learning, many factors significant in influencing and guiding classification remain unexplored. In this pa-per, we consider these factors by first motivating the use of a supervised framework through a careful investigation of issues such as network observational period, generality of ex-isting methods, variance reduction, topological causes and degrees of imbalance, and sampling approaches. We also present an effective flow-based predicting algorithm, offer formal bounds on imbalance in sparse network link predic-tion, and employ an evaluation method appropriate for the observed imbalance. Our careful consideration of the above issues ultimately leads to a completely general framework that outperforms unsupervised link prediction methods by more than 30% AUC.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Performance, Theory Link Prediction, Networks, Machine Learning, Class Imbal-ance
Link prediction is an important task in network science that offers unique ways whereby the study of networks can benefit researchers and organizations in a variety of fields. Security agencies can more precisely focus their efforts based on probable relationships in malicious networks that have heretofore gone unobserved [10]. In social networks, indi-viduals can efficiently and effectively find companions, as-sistants, or colleagues [9]. In medicine and biology, link pre-diction can be used to find relationships and associations that exist, but which might otherwise surface only after ar-duous and expensive research and study on a huge selection of agents. Finally, researchers can easily adapt link predic-tion methods to identify links that are surprising given their surrounding network, or which may not belong at all [15]. Put simply, any environment that naturally maps to a net-work probably has an equally coherent mapping from link prediction in that network back to an important question in the environment.

This broad applicability demands a powerful yet general framework, and we promote supervised learning. Unsuper-vised methods, which receive the most attention in link pre-diction literature, are fundamentally unable to cope with dynamics, interdependencies, and other properties in net-works. We recognize that this is not the first paper to apply supervised learning to the link prediction problem, but there are important differences versus past work. First, in spite of the excellent intentions of past researchers, they have fallen prey to unique pitfalls endemic to problems with highly im-balanced class distributions. In [2], the holdout test set is undersampled to balance, and the authors of [17] also con-tribute only a sample of the negative instances to their test set. As researchers familiar with high skew are aware, mod-ifying the data distribution on which testing is performed generates uninterpretable results. The distribution of the resulting testing data no longer presents the same challenges as the real-world distribution, and performance measures in testing no longer reflect the real capabilities and limitations of the model. Additionally, both of these works employ se-mantic and contextual information that pertains almost ex-clusively to the bibliographic domain. Finally, these works do not consider the important impact of geodesic distance and the intricacies of class imbalance specific to the task of link prediction.

We demonstrate that decomposition by geodesic distance has important impacts on predictor performance irrespective of the choice of predictor. We also expand the library of un-supervised measures with an intuitive flow-based metric that is over 15% AUC more predictive than baseline methods in certain networks. After illustrating the benefits of super-vised learning, we cast link prediction as a problem in class imbalance. The result of these considerations is a framework that improves upon the best baseline unsupervised methods by over 30% AUC in our test networks. Furthermore, the framework is entirely general, operating over any class of network whether it be weighted, unweighted, directed, or undirected. It does not require any node attributes but is capable of accepting them.

In Section 2 we describe the data sources and evalua-tion measures. Section 3 explains standard unsupervised approaches and defines a new metric, PropFlow .Section4 lays out the rationale for supervised learning. This leads to a discussion of class imbalance in Section 5. In Section 6 we describe our realization of the framework and Section 7 presents results. Finally, Section 8 provides recommenda-tions and concludes.
In order to make a compelling, novel case for a supervised framework, we offer a comprehensive explanation of the na-ture of link prediction, primarily through an examination of two real-world data sets. We also report the prediction results relative to an appropriate metric for predicting in imbalanced environments. It is therefore necessary to first present the two data sources and the principal measure of performance that we employ.
The first data source is a stream of 712 million cellular phone calls from a major non-American cellular phone ser-vice provider. We construct weighted, directed networks from the calls by creating a node v i for each caller and a weighted, directed link e ij from v i to v j if and only if v calls v j . Weights correspond to the number of calls over the link. We shall henceforth refer to this network as phone . For all experiments except those in section 4.1 we use the first 5 weeks of data (5.5M nodes, 19.7M links) for extract-ing features and the sixth week (4.4M nodes, 8.5M links) for obtaining ground truth.

The second data source is a stream of 19,464 multi-agent events representing condensed matter physics collaborations from 1995 to 2000. We construct weighted, undirected net-works from the collaborations by creating a node for each author in the event and a weighted, undirected link connect-ing each pair of authors. Weights correspond to the number of collaborations two authors share. We shall henceforth re-fer to this network as condmat . For all experiments involving condmat , we use the years 1995 to 1999 (13.9K nodes, 80.6K links) for extracting features and the year 2000 (8.5K nodes, 41.0K links) for obtaining ground truth.

The networks exhibit different quantitative characteris-tics. Table 1 contains some summary network statistics in order to provide context for the two networks. These statistics result from the complete 6 weeks of data for phone and the complete 1995-2000 network for condmat .Theas-sortativity coefficient measures the tendency to find highly connected nodes that are connected to each other. The av-erage clustering coefficient measures the tendency of nodes in the network to be connected in dense groups. Strongly-connected components (SCCs), or connected components in the undirected network, are clusters of vertices in the net-work in which every vertex in the cluster has a path to all other vertices in the cluster. The size and diameter of such components provides some insight into the broad topological structure of the network.
Scalar measures often used in link prediction, such as pre-cision on the top-N predictions and factors of improvement in precision over random models, rely upon the application of an arbitrary and often unjustified threshold. Most of our evaluation relies instead upon receiver operating character-istic (ROC) curves. These curves present achievable true positive rates ( TP ) with respect to all false positive rates ( FP ) by varying the decision threshold on probability esti-mations or scores. ROC curves provide information about the operating range of classifiers. For example, classifier A may outperform classifier B when we dictate FP &lt; 20% but B may outperform A when we allow FP  X  20%. The ex-pected performance of a random classifier is the line y = x , and curves below this line indicate an inverted predictor. Finally, we can say that classifier A dominates classifier B in ROC space if all points on the convex hull of A dominate all points on the convex hull of B in the xy -plane, and this is a condition known to correlate highly with superiority in many other measures [13]. The area under the ROC curve (AUC) is a related scalar measure of the performance over all thresholds. AUC has classically been used as a measure of performance in imbalanced learning.
Most existing studies in link prediction consider baseline unsupervised methods to assign scores to potential links. The state-of-the-art in these methods is aggregated and com-pared in [11], and in Section 3.1 we offer a brief explanation of the particular methods that we study. Moreover, since our goal is to derive a robust feature set, we introduce a novel, effective method in Section 3.2.
Most unsupervised methods either generate scores based on node neighborhoods or path information. The common neighbors predictor is the number of neighbors, or out-degree neighbors in our directed network, that are shared by nodes v i and v j . Jaccard X  X  coefficient simply divides the num-ber of common neighbors by the number of total neighbors. The Adamic/Adar measure [1] weights the importance of a common neighbor v k by the rarity of relationships between other nodes and v k . Finally, the preferential attachment link prediction score [3, 12] is the product of the degrees of v and v j . When we observed especially poor performance for this predictor in phone , we tried using in-degree, out-degree, and their sum but observed only minor differences. We re-port our results based on out-degree performance. From the path-based methods we employ the unweighted Katz Algorithm 1 PropFlow Predictor Require: network G =( V,E ), node v s ,maxlength l Ensure: score S sd for all n  X  l -degree neighbors v d of v 1: insert v s into Found 2: push v s onto NewSearch 3: insert ( v s , 1) into S 4: for CurrentDegree  X  0to l do 5: OldSearch  X  NewSearch 6: empty NewSearch 7: while OldSearch is not empty do 8: pop v i from OldSearch 9: find NodeInput using v i in S 10: SumOutput  X  0 11: for each v j in neighbors of v i do 12: add weight of e ij to SumOutput 13: end for 14: Flow  X  0 15: for each v j in neighbors of v i do 16: w ij  X  weight of e ij 18: insert or sum ( v i ,Flow )into S 19: end for 20: if v i is not in Found then 21: insert v i into Found 22: push v i onto NewSearch 23: end if 24: end while 25: end for measure [8], which had better, more stable performance in the networks than the weighted variant. This method con-tributes each path to a sum with an influence damped in exponential proportion to its length, l , using the parameter  X  . We select  X  =0 . 005 and, for performance reasons, we restrict our examination of the measure such that l  X  5.
We introduce a new unsupervised prediction method on networks, PropFlow , which corresponds to the probability that a restricted random walk starting at v i ends at v j steps or fewer using link weights as transition probabilities. The restrictions are that the walk terminates upon reaching v or upon revisiting any node including v i . The walk selects links based on their weights. This produces a score s ij that can serve as an estimation of the likelihood of new links. PropFlow is somewhat similar to Rooted PageRank , but it is a more localized measure of propagation, and is insensitive to topological noise far from the source node. Unlike Rooted PageRank, the computation of PropFlow does not require walk restarts or convergence but simply employs a modified breadth-first search restricted to height l .Itisthusmuch faster to compute. It may be used on weighted, unweighted, directed, or undirected networks. We supply the detailed procedure for weighted, directed networks in Algorithm 1.
In the phone network, PropFlow outperforms baseline un-supervised methods by &gt; 15% AUC on average. It out-performs Rooted PageRank by more than 8.75% AUC. We attribute this success to the nature of the mechanisms in phone underlying the appearance of new links. Although it may be used in any network, PropFlow has special intuitive significance as a link predictor in networks where some re-source such as information flows, propagates, or cascades. In transportation networks, when a resource frequently travels from one node through neighbors to another, there is of-ten some cost for the intermediaries. When the expected cost inherent in traveling through intermediaries overcomes the cost of establishing a new link, one can expect forma-tion of that particular link. In transmission networks, the measure represents the link-weighted probability that a ran-domly outward-propagated transmission sent by one node will reach another. In condmat , there is no strong analogy and PropFlow is not as effective. Later in the paper, we will explore the utility of PropFlow both as an individual predictor and as a feature in our supervised classification framework.
While past studies on link prediction have focused on un-supervised single metrics, some recent works have used a supervised classification scheme, and rightly so. If one ac-cepts the basic premise that ground truth, whether a link forms or not, is always available from prior incarnations of the network, there is no practical disadvantage to using a supervised framework. Even training classifiers based on a single unsupervised method has the potential to outperform rankings generated by sorting the scores of the method if there are multiple differentiating boundaries in the domain of scores. Supervised algorithms are also able to capture important interdependency relationships between topologi-cal properties. While past studies simply acknowledged this fact and trained classifiers, we probe more deeply into the relevant issues so that we can fully understand how to frame the prediction problem and why a supervised framework is best for the task. We first address the how question in Sec-tion 4.1 by examining how to best transform network data into standard data sets. We then address the why question in sections 4.2, 4.3, and throughout section 5. More specif-ically, Section 4.2 explains that supervised approaches are adaptive and may be more general whereas unsupervised methods are invariant. Section 4.3 demonstrates that un-supervised methods cannot be, or at least have not been, combined into ensembles to reduce variance. Section 5 ex-plains that unsupervised methods are inherently incapable of combating extreme class imbalance, a natural character-istic of link prediction in nearly any network.
Some networks may always be observable, such as WWW, the Internet, and electricity grids. Others are observable only through events that indicate the presence of links. In the former, one need only select a moment at which to ob-serve the structure directly. In the latter, one must col-lect events to construct an approximation of the underlying structure. Regardless, the network evolves through time to present a longitudinal source of data. We see then that link prediction, a domain in which unsupervised topologi-cal measures receive much attention, is very often suitable for supervised learning. The acquisition of ground truth for constructing models does not mitigate the necessity of the task; future forms of the static network will raise the same questions that exist in the present.

In a typical supervised learning task, we are given a unified set of data with each instance of the form ( x, y ). To convert networks such as phone or condmat into this format, we have Figure 1: Performance in the second-degree neigh-borhood as a function of  X  x . to select two values  X  x and  X  y . These values correspond to the lengths of two adjacent periods over which we want to record events to construct networks. From the first network, G x =( V x ,E x ), constructed from t 0 to t 0+  X  x , we extract topological measures, and potentially node attributes, that serve as features for each pair of nodes ( v i ,v j ). From the second network, G y =( V y ,E y ), constructed from t  X  x t +  X  y , we examine ( v i ,v j )todiscoverwhether e ij exists and determine the class label. This yields a data set in the standard ( x, y )formatwith | V x | 2  X  X  E x | instances.
The two parameters  X  x and  X  y have important but pre-dictable influences on the success of models. We can expect that increasing  X  x will increase the quality of topological measures as the network reaches saturation. This is the point at which  X  x is large enough that the observed events form a topology that closely reflects the underlying static network. As  X  x approaches this point, the topological mea-sures converge to their actual unobservable static network values, thus allowing improved individual predictive capac-ity. We can expect that increasing  X  y will increase the num-ber of positives. We investigate  X  x on the phone network in Figure 1.

Increasing  X  x has the expected result. The strength of the predictors increases greatly from  X  x = 2 weeks to  X  x =5 weeks and again from  X  x = 5 weeks to  X  x = 8 weeks. Al-though measures of network saturation and convergence are outside the scope of this paper, we can remark that they are highly correlated with performance. Since we observe an in-crease in the predictive power of unsupervised methods, we can expect increases in supervised classification performance too. In effect, the features more closely reflect actual rela-tionships underlying observable events, so models are more closely related to reality.

Although this suggests that the results in Section 7 could be even higher with  X  x = 8 weeks, we choose to present the rest of the paper based on  X  x = 5 weeks and  X  y =1 week. This observational period corresponds to a network that is only partially approaching saturation and might more realistically represent the data available for training in real-world environments.
While classifiers can generalize well to many environments in the sense that they can adjust models depending on poste-Figure 2: Preferential attachment performance by scoring region. rior information, unsupervised methods are domain-specific. The figures in [11] show that predictors that serve well in one network do not necessarily serve well in all networks; our observations concur. It is clear throughout our results that the performance of the unsupervised methods is un-stable not only from one network to the other, but from one graph-distance to another. The preferential attachment predictor is a particularly clear example in Figure 2. The figure shows the percentage of a given score that is positive. Intuitively, the model serves as a good predictor when low scores produce low percentages and high scores produce high percentages and an inverted predictor when the opposite is true.

In the phone network, we see that the predictions are in-verted, with a higher percentage of positives falling into low scores than high scores. In the condmat network, the predic-tions are much better, with the highest scores corresponding to much higher incidences of links. Finally, for both net-works, we observe a similar trend with increasing geodesic distance n . The greater the distance, the better preferential attachment models the appearance of links. That is, we see lower percentages for lower scores and higher percentages Figure 3: AUC performance variation in common neighbors  X  X nsemble X  created by randomly selecting p percent of edges for removal 10 times, obtain-ing the measure, and combining it using the series statistic. forhigherscoresaswemovefrom n =2to n = 4. This sup-ports the intuition that preferential attachment is better as a global indicator where underlying local mechanisms such as neighbor recommendations are weaker.
Yet another benefit of supervised learning is that classi-fication algorithms, especially unstable algorithms like de-cision trees, can benefit from reduced variance by placing them in an ensemble framework. Ensembles consist of many models that have been trained on slightly perturbed varia-tions of the data. It is difficult or impossible to accomplish the same goal with unsupervised methods common in link prediction because the score is invariant for a given poten-tial link. Furthermore, it is likely that network analogs to common ensemble sampling techniques are fundamentally flawed as a rough corollary of work in [16], where samples of networks with ill-behaving distributions produce new net-works with different properties. Nevertheless, we wanted to explore the potential for one method of ensemble construc-tion using unsupervised methods. To achieve the values in Figure 3, we construct 10 new networks, randomly selecting p percent of the edges in the original network for each. Then, we compute a common neighbors score for the pair ( v i ,v in each network and combine the scores using a summary statistic.

The figure shows that the attempt at constructing an en-semble out of an unsupervised method fails. The best AUC appears at 100%, where the network is unsampled and there is no ensemble, which suggests that sampling the network to construct the ensemble does nothing but remove important information, a result we find unsurprising. What the figure does not show is that the p = 100 ROC curve dominates all ROC curves for p&lt; 100, including mean and max, and that transformations of the ROC curves into precision-recall space show p = 100 greatly outperforms even p = 95. We performed these experiments only for the common neighbors classifier, but expect the same results for other unsupervised methods.

Supervised classification, on the other hand, offers many strong options for reducing variance such as bagging [4] and random forests for decision trees [5], the latter of which also increases classification efficiency. While a single classifier that incorporates several of the unsupervised methods can greatly improve classification versus those methods, variance reduction techniques can further improve it.
A significant novelty of link prediction as a supervised learning problem is its extreme imbalance, which reaches past the most skewed distributions studied by the imbal-ance community. While unsupervised methods cannot com-bat this imbalance because they are agnostic to class distri-butions by definition, supervised learning schemes are able to balance data and focus on class boundaries. In this sec-tion, we will study some of the properties of that imbalance, especially as it relates to graph distance.
We proceed by constructing a formal proof of the lower bound on the class imbalance ratio for link prediction in sparse networks. The proof operates on two reasonable, al-most ubiquitously satisfied assumptions. First, the network maintains the property of sparseness throughout the period of interest. Second, the network growth is limited such that the number of nodes may only double during the period of interest, although the theorem holds for any factor of growth g such that g | V | .

Definition 1. Let a network G =( V,E ) be described as sparse if it maintains the property | E | = k | V | for some con-stant k | V | .

Theorem 1. The class imbalance ratio for link prediction in a sparse network G is  X  | V | 1 when at most | V | nodes may join the network.

Proof. The number of possible links in G is | V | 2 .Then the number of missing links, | E C | ,is | V | 2  X  k | V | X  Let | V | nodes and | E | links join the network. Since | |
V | X  2 | V | X   X ( | V | ), | E | + | E | X   X ( | V | ), which requires that | E | X  O ( | V | ). The number of positives is | E | there are ( E  X  E ) C  X   X ( | V | 2 ) negatives. This gives us
Thus the imbalance issue in the general link imbalance problem becomes clear. No matter how many links we hope to anticipate, TP , we must accept a baseline random model that produces FP such that FP  X  TP  X | V | .Evenamodel thousands of times better than random performs poorly. The severity of the problem is exacerbated by the fact that positives often represent occurrences of greater interest.
In link prediction, graph distance plays a primary role in determining the imbalance ratio. We define the n -degree neighborhood of a node v i as the set of nodes exactly n hops away from v i .As n increases, the number of potential links will increase in proportion to the superlinear increase in the number of neighbors. Simultaneously, it is reasonable to expect that the new links will tend to form between nodes that are close, such as in phone where local influences such as recommendations and common neighbors pertain. Figure 4 illustrates the imbalance behavior of the phone and condmat networks. It also demonstrates the distribution of distances between pairs of nodes for all distances where the under-lying computation is feasible. It is important to note here that phone has a diameter in its largest strongly connected component of 25 while condmat has a diameter of only 19 in its largest connected component. Further, the n  X  6-degree neighborhood of any given node in phone still includes only a moderate fraction of nodes in the network. In the much smaller condmat network, the n  X  6-degree neighborhood of any given node often approaches the periphery and includes almost every node in the network.

The simultaneous severe increase in unformed potential links and severe decrease in links that actually form causes even more dramatic increases in imbalance ratios. phone imbalance goes from 131:1 at n = 2 to 32,880:1 at n = 4 and 606,926:1 at n =6. condmat imbalance goes from 179:1 at n = 2 to 6,247:1 at n = 4. Fortunately, there is often little reason to believe that the benefit of successfully predicting links to nodes at high n is greater than the benefit of predicting them at low n .

Given that imbalance increases so sharply between neigh-borhoods, and local mechanisms quickly give way to global mechanisms at higher values of n , we suggest that each neighborhood should be treated as a separate problem in supervised learning. This also allows us to avoid the V :1 imbalance of the general problem. Additionally, in the case of the entire class of neighbor-based models, there is null output for n  X  3 in undirected networks because there is no sense in which such nodes can have common neighbors. Many unsupervised methods have implicit or explicit adjust-ments for graph distance, but the fundamental distinction of neighborhood comes for free. Supervised models may bene-fit from a decrease in noise and, for networks in which the distance spanned by the predicted link is inconsequential, the consideration of low n saves computational time.
With the nature of the problem and the advantages of supervised learning carefully considered, we now present the details of the high-performance link prediction (HPLP) framework. In most cases, we reserved two-thirds of the la-beled data for training the model and the remaining third for testing, but for the presentation of significance results, we employed 10-fold cross-validation with care to use un-modified folds for testing. At no time do we change the class distribution in any testing data. Due to computational com-plexity, we restricted our consideration of classifiers to the WEKA [18] C4.5 [14] equivalent, J48 (parameters -A -U), na  X   X ve Bayes (default parameters), and WEKA bagging (10 bags, de-fault parameters) with random forests (10 trees, default pa-rameters). The last is easy to parallelize.
Any network, no matter its type or source, necessarily supports basic topological measures such as v i and v j in-degree and out-degree, v i and v j in-volume and out-volume, or their undirected equivalents in the case of condmat .We also employ the baseline unsupervised models from Section 3, including PropFlow , and path-oriented measures such as the number of shortest paths from v i to v j and the maximum flow that can travel from v i to v j within 5 steps. Though we use only these features for generality, we could use any other available features, including measures of reciprocity, or node attributes such as age and gender.

To illustrate that we are able not only to achieve perfor-mance that vastly exceeds baseline methods, but that we do so without using them as features, we include both a re-stricted feature set that does not use the existing unsuper-vised methods (HPLP) and the full feature set (HPLP+). Table 2 contains details of the features.
We capitalize on the ability of supervised frameworks to reduce variance, as described in Section 4.3, by using ensem-bles of classifiers. We use two different ensemble methods: bagging and random forests. Random forests is an excel-lent method for these data sets for two reasons. First, the data sets are composed of a combination of strong features and weak features. While the weak features are occasion-ally helpful, random forests is an excellent method to pre-vent overfitting them. Second, the decreased training time for each single tree counters the increased training time to build the forest, making it an especially efficient method of variance reduction for these large data sets.

In both data sets, after undersampling to balance, we found on average a 4.04% AUC improvement moving from a single tree to 10 bagged trees and an additional 3.91% im-provement moving from 10 bagged trees to 10 bagged ran-dom forests. The total average improvement of 8.11% jus-tifies the use of these variance reduction methods, and the sheer size of the testing sets lends significance to even frac-tional percentage improvements. We found that neither 100 bagged trees without random forests nor 100 random for-est trees without bagging offered the same improvements. Finally, we note that the improvements we observed after the application of these techniques are impressive, but still suboptimal. We constructed our ensembles from the same selection of undersampled negative class instances. With a minimal penalty in computational time, each member of the ensemble could make use of a random selection of the entire set of negative class instances.
Aside from reducing variance, we considered different ap-proaches to overcome the imbalance described in Section 5. In doing so, we had to carefully consider the enormity of the data sets, especially for large values of n .AsFigure4 shows, in phone n = 2 produces 81.4 million instances and n = 4 produces 2.2 billion instances. Even in the smaller condmat network, n = 2 produces 431.6 thousand instances and n = 4 produces 8.1 million instances.
 One of the best oversampling strategies, SMOTE [6], is O ( p 2  X | x | ), the product of the number of positive class in-stances p and the length of the feature vector. While this may work for condmat where p is on the order of thousands, it certainly will not work for phone . We also theorize that phone ,with p in the order of tens or hundreds of thousands, does not suffer as much from lack of definition in the positive class as from a strong classifier bias toward f ( x )=0from prior information. Furthermore, oversampling approaches only increase data set size and training time. We considered Figure 5: Performance reaction of a single C4.5 de-cision tree to different undersampling levels. The x-axis is in terms of the percentage of all training examples that are positive. training skew-insensitive decision trees based on Hellinger distance [7]. Such trees are best when trained on the origi-nal training set distribution, however, and performed poorly with undersampled data. Without undersampling, the train-ing set sizes for the data often render training with these trees infeasible. Undersampling, on the other hand, can help to mitigate the problem of class imbalance while also reduc-ing the size of the training set.

In section 5.2 we argue for treating each neighborhood as a separate problem. This also allows for skew-combating methods that are appropriate to the particular neighbor-hood. If n  X  2 is combined into a single data set and sub-sequently uniformly undersampled, negative representatives of n = 2 will be underrepresented causing a distortion of the real n = 2 class boundary.

The class ratio to which the data set is undersampled serves as a significant parameter to our framework. In Fig-ure 5 we explore a wide range of possible sampling param-eters using a single C4.5 decision tree evaluated according to AUC. The performance of the phone data set is rela-tively stable, but it exhibits an interesting trend wherein AUC drops slightly when the class distribution is balanced. condmat achieves AUC values that are higher with increasing negative class representation through the ratios we tested. Despite these results, in Section 7 we undersample the train-ing sets to balance to present a consistent view of perfor-mance.

We would like to mention that the beneficial relationship between link prediction researchers and class imbalance re-searchers is mutual. Class imbalance research contributes many options to the link prediction community. Simultane-ously, link prediction offers the potential for a wide variety of data sets that match or surpass the imbalance ratios of the most demanding publicly available data. Any large network can become an authentic source for data with selectable fea-tures, selectable imbalance ratios depending on the chosen value of n , and a large pool of positive instances from which to draw the desired positive class cardinality.
To achieve the following results, we trained bagged ran-dom forests, which exhibited universally superior perfor-mance. For uniformity of reporting, all training sets are undersampled to balance rather than to a level optimized for each network and n . Because AUC alone can sometimes be misleading, we also include ROC curves. Figure 6 contains curves describing the performance of unsupervised methods and the supervised framework.

The phone and condmat curves illustrate that the mech-anism by which links arise is indeed different both across networks and geodesic distances. In fact, this leads to an interesting broad observation about mechanisms of link for-mation. In the condmat network, individuals have a global view of the topology through a variety of means. In essence, researchers know of other eminent researchers in the field however remote they may be in terms of geodesic distance. In the phone network, there is little reason to suspect that individuals have much knowledge of other individuals at re-mote locations in the network. The performance of the pref-erential attachment method supports this theory in the two networks; it is much stronger for condmat than for phone . Additionally, it shows performance that increases with n in both networks. The more distance potential links span in a network, the weaker local influences such as neighbor rec-ommendations or path-based considerations become. The discriminative power of methods based on these principles generally drops accordingly. On the other hand, global influ-ences such as degree have the same interpretation at any dis-tance in the network. As the local influences lose their mean-ing with increasing n , the preferential attachment method becomes an increasingly pure estimation of link formation biases.

We can clearly see the deterioration in predictive capac-ity of the local methods. Neighbor-based methods perform worse for n =3thanfor n =2 phone , and they perform much worse for n =4thanfor n = 3. Neighbor-based methods have no meaning for n  X  3 in directed networks such as condmat ; there is no sense in which two unconnected individuals greater than two hops away from each other can share a neighbor. The PropFlow predictor degrades more gracefully than neighbor-based methods in phone but suffers mediocre performance on condmat . Despite much greater curve areas in phone , PropFlow does not dominate other measures. Instead, methods based on common neighbors achieve slightly higher TP rates at very low FP rates, but PropFlow rapidly surpasses them. Importantly, the HPLP dominates all other methods in ROC space in every case except phone n =2,where PropFlow actually crosses at FP =0 . 99. On a more general note, especially in phone where imbalance ratios grow higher, the increasing difficulty of more distant neighborhoods is exhibited in the form of ROC curves that converge toward TP = FP .

We now move to the discussion of AUC values and Fig-ure 7. HPLP achieves performance levels as much as 30% higher than the best unsupervised methods. The difference in performance between HPLP and HPLP+ averages &lt; 1% AUC. Though it does not appear in the figures, in phone we also created a data set with all unsupervised methods except PropFlow . We found that PropFlow alone achieves higher performance than using all other unsupervised meth-ods put together when using the same basic supporting fea-tures, such as node degree. To substantiate the hypothe-sis that there are useful dependencies between features, we compared na  X   X ve Bayes to a single C4.5 tree and confirmed that the latter wins by &gt; 2 . 3%. Nonetheless, even using asingle,fastna  X   X ve Bayes classifier, HPLP always greatly outperforms the strongest of the unsupervised methods. To provide statistical significance to this statement, we used two-tailed paired t-tests. The paired samples come from 10-fold cross validation AUC scores. For all values of n ,HPLP outperforms unsupervised methods at over 99% confidence and HPLP+ outperforms them at over 99.99% confidence.
The general framework we propose in this paper achieves major improvements over existing methods. Although it outperforms such methods by &gt; 30% in terms of AUC, it does not require any domain-specific node attributes to do so. It can be applied in any domain exactly as described or it can accept any number of domain-specific features. It is also highly scalable; feature computation for a single path-based method requires more time than the entire classifica-tion framework. The feature computation itself is embar-rassingly parallel.

In addition to the results, the supporting study allows for some recommendations. Unsurprisingly, for networks where topological convergence and saturation may be a concern, the training observation period,  X  x , should be as long as pos-sible. The parameter  X  y for the static network from which labels are gathered should match the size of the real-world prediction window so that testing and real-world prior dis-tributions are as similar as possible. In link prediction on networks such as the Internet or electricity grids, these con-cerns are moot since snapshots contain the entire network structure.

Optimal class ratios for undersampling are specific to the problem at hand, but the results we obtained for both net-works indicate that undersampling to balance may not be ideal in the link prediction domain. Those employing this classification framework should be aware of this fact and should investigate other ratios as resources permit. For small networks where computational resources are not problem-atic, we advise the use of skew-insensitive classifiers such as Hellinger trees in an ensemble framework. We encour-age the community to consider the link prediction task as a separate problem for each desired neighborhood in do-mains where local mechanisms are likely to pertain. This not only decreases computational time by considering those links most likely to rank highly regardless but has the po-tential to sensitize supervised classification to the specific mechanisms and boundaries present for predictions within the target graph distances.

In general, the application of unsupervised methods, at least without due study and consideration, is highly subop-timal. No such method, no matter how high its performance in some subset of our data, provides acceptable performance for the entire range of problems. Where there is some limita-tion on supervised learning due to the unavailability of labels for training, we hope that the included study of unsuper-vised performance measures proves helpful in the selection of an appropriate option. For networks where one expects local mechanisms to dominate, especially local mechanisms related to flow or propagation, we highly encourage the use of the unsupervised method, PropFlow , proposed in this pa-per.

We have published all scripts and source code for pre-diction and evaluation at http://www.nd.edu/~rlichten/ linkpred along with the condmat data set. We regret that we are unable to make the phone data set available due to non-disclosure agreements. Research was sponsored in part by the Army Research Laboratory and was accomplished under Cooperative Agree-ment Number W911NF-09-2-0053 and in part by the Na-tional Science Foundation (NSF) Grant BCS-0826958. The views and conclusions contained in this document are those of the authors and should not be interpreted as represent-ing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation hereon.

We sincerely thank Mark E.J. Newman for the condmat data set and Albert-L  X  aszl  X o Barab  X  asi for the phone data set. Finally, we thank our colleagues in the University of Notre Dame Interdiscplinary Center for Network Science and Ap-plications (iCeNSA). [1] L. Adamic and E. Adar. Friends and neighbors on the [2] M. Al Hasan, V. Chaoji, S. Salem, and M. Zaki. Link [3] A.-L. Barab  X  asi, H. Jeong, Z. N  X  eda, E. Ravasz, [4] L. Breiman. Bagging predictors. Machine Learning , [5] L. Breiman. Random forests. Machine Learning , [6] N. V. Chawla, K. W. Bowyer, L. O. Hall, and P. W. [7] D. A. Cieslak and N. V. Chawla. Learning decision [8] L. Katz. A new status index derived from sociometric [9] H. Kautz, B. Selman, and M. Shah. Referral web: [10] V. E. Krebs. Mapping networks of terrorist cells. [11] D. Liben-Nowell and J. Kleinberg. The link-prediction [12] M. E. J. Newman. Clustering and preferential [13] F. Provost and T. Fawcett. Robust classification for [14] J. R. Quinlan. C4.5: Programs for Machine Learning . [15] M. J. Rattigan and D. Jensen. The case for anomalous [16] M. P. Stumpf, C. Wiuf, and R. M. May. Subnets of [17] C. Wang, V. Satuluri, and S. Parthasarathy. Local [18] I. H. Witten and E. Frank. Data Mining: Practical
