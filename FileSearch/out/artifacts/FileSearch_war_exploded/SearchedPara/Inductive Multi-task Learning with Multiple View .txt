 In many real-world applications, it is becoming common to have data extracted from multiple diverse sources, known as  X  X ulti-view X  data. Multi-view learning (MVL) has been widely studied in many applications, but existing MVL methods learn a single task individually. In this paper, we study a new direction of multi-view learning where there are multiple related tasks with multi-view data (i.e. multi-view multi-task learning, or MVMT Learning). In our MVMT learning methods, we learn a linear mapping for each view in each task. In a single task, we use co-regularization to obtain functions that are in-agreemen t with each other on the unlabeled samples and achieve low classification errors on the labeled sam-ples simultaneously. Cross different tasks, additional regularization functions are utilized to ensure the functions that we learn in each view are similar. We also developed two extensions of the MVMT learning algorithm. One extension handles missing views and the other handles non-uniformly related tasks. Experimental studies on three real-world data sets demonstrate that our MVMT methods significantly outperform the existing state-of-the-art methods. H.4 [ Information Systems Applications ]: Miscellaneous; I.2.6 [ Artificial Intelligence ]: Learning X  performance measures Algorithms,Experimentation Multi-view learning, co -regularization, missi ng view, inductive multi-task learning, task relationship learning
In many real-world applications, it is becoming common to have data extracted from multiple diverse sources, known as  X  X ulti-view X  data. Each data source is referred as a view [13]. Mining and learning with multi-view data (i.e. multi-view learning, or MVL) has been studied extensively [2, 6, 13, 18, 21, 22]. For instance, in multi-lingual text categoriza tion [2], each language represents a view. For scientific document categorization, each paper has two views: the bag-of-words features and their citations [18]. In classi-fying webpages [6], we may have three views for a given webpage: the content of the webpage, the text of any webpages linking to this webpage, and the link structure of all linked pages. The limitation of these MVL methods is that they essentially learn a single task individually at a time.

In this paper, we study a new direction of multi-view learning where there are multiple related tasks with multi-view data (i.e. multi-view multi-task learning, or MVMT Learning). MVMT Learn-ing has many real-world applications. For instance, sentiment clas-sification for music reviews and news comments is two related tasks, and they both share word features from the comments or re-views. In protein functional classification, each protein has features from multiple views (e.g. protein sequences, 3D structures) and may be associated with multiple f unctional classes. In image anno-tation [7], each image has features extracted from multiple sources and can be annotated with multip le objects such as a boat, a bird and etc. The interplay of multiple views and multiple tasks in the same learning problem motivates us to investigate multi-task learn-ing in the multi-view framework.

Despite the wide application areas, multi-task learning (MTL) with multi-view data only caught th e attention of the research com-munity recently. Cavallanti et al . [8] developed linear algorithms for online multi-task learning. Though there may be multiple views, each task has only one view in their settings. He et al. [16] proposed a graph-based iterative algorithm (IteM 2 ) for multi-view multi-task learning with applications to text classification. The IteM rithm projects any two tasks to a new Reproducing Kernel Hilbert Space (RKHS) based on the common views shared by the given two tasks. Though impressive preliminary results have been ob-tained in text categorization applications, the treatment of MVMT learning in IteM 2 is limited. First IteM 2 is a transductive learning method, hence it is unable to generate predictive models on inde-pendent, unknown testing samples. A consequence of the trans-ductive learning is that the training data set must have the same fraction of positive samples as in the testing data set. In addition, the method is designed primarily for text categorization where the feature values are all non-negative 1 , since otherwise Proposition 4.1 and Theorem 4.2 in [16] will not hold. There are many real-world data sets with a significant portion of features with negative values that the IteM 2 method is unable to handle. In this paper IteM 2 is the most important competing method that our proposed MVMT method will compare with. Refer to page 3 in He et al. [16]
We propose an inductive learning framework to address the gen-eral MVMT learning problem. Our starting point is a MVL frame-work based on co-regularization where we train multiple classi-fiers, one for each view, with the goal to obtain those classifiers that are in-agreement with one another on the unlabeled samples and achieve minimal classification errors on the labeled samples si-multaneously [21, 22]. We add a couple of regularization factors (e.g. 2 regularization [15]) to ensure the functions that we learn from each task are similar to each other, resulting in a convex ob-jective function, which makes the subsequent optimization easy. In addition, our algorithm is flexible when one view is completely missing for a task. In many application we may have structured view missing, and our algorithm extends to these cases naturally with the regularization function that we use. Moreover, it is quite often that not all tasks in multi-task learning are uniformly related to each other [10, 17]. To handle this, we introduce a positive semi-definite matrix to capture the relationship of tasks that may not be uniformly related to each other.
 In summary, the main contributions of this paper are two-fold. First we propose a general inductive learning framework for the challenging multi-view multi-task problems using co-regularization and task relationship learning. Secondly we design efficient algo-rithms to optimize the objective function with either close-form so-lutions or iterative optimization solutions. We conducted compre-hensive experimental evaluations of our MVMT methods on three real-world MVMT data sets, and compared our MVMT methods with the state-of-the-art methods, including the competing IteM method.

The rest of this paper is organized as follows. In Section 2, we briefly review related work on multi-view learning and multi-task learning. We explain our co-regularized MVMT algorithms and their implementations in details in Section 3. We have experimen-tally evaluated our methods on three real-world data sets and com-pared our results with those from the state-of-the-art methods. We present the results in Section 4 and conclude our work in Section 5.
Multi-view semi-supervised learning has attracted significant re-derlying assumption of multi-view algorithm is that each view is conditionally independent from other views [6] and is sufficient for constructing a predictive model [2, 14]. We briefly overview MVL methods that are widely used. Based on how information from mul-tiple views is integrated, existing MVL algorithms can be roughly classified into a variety of categories. Co-training [6] iteratively labels unlabeled examples using the models built from existing la-beled examples, and expands the pool of labeled examples until convergence of performance [14, 20]. Manifold co-regularization [22] was proposed based on a reproducing kernel Hilbert space with a data-dependent co-regularization norm to explore the struc-ture of unlabeled multi-view data. Recently co-regularization [21] attracted the attention of the community due to its simplicity of op-timizing a single regularized objective function.

When each task has only a limited number of samples, multi-task learning has been empirically as well as theoretically shown to pro-vide better predictive models with closely related tasks [3, 4]. Some early MTL approaches assumed that either the function parameters of different tasks are similar [15] or multiple tasks share a subset of features [4]. These MTL methods imposed a regularization term to enforce the difference between multiple task functions to be small. Recent studies on MTL proposed th at the relatedness of multiple tasks has a structure such as a graph or a tree [10, 17], or different tasks share a common subspace representation [9]. Another group of MTL methods [25] pose no assumption on the structure of task relatedness, and learn task relationship automatically from the in-put data, and provide more modeling flexibility.
In this section, we propose a general inductive learning frame-work for multi-task learning with multiple view data. We apply co-regularized MVL within each task, and the multiple related tasks learning. We handle the special case that some entire views are missing from some tasks, analyze the complexity of our MVMT methods, and develop efficient optimization algorithms to achieve optimal solutions.
In this paper, we use bold capital letters (e.g. X )torepresent Lowercase letters (e.g. x ) are used for scalars, and Greek letters (e.g  X  ) for regularization parameters. We use [ m : n ] ( denote a set of integers in the range of m to n inclusively. Unless stated otherwise, all vectors are column vectors.

Suppose we are given a set of N labeled and M unlabeled data samples for T tasks. In general we have limited supply of labeled examples but abundant supply of unlabeled examples, i.e. M N .Weuse N t and M t denote the number of labeled and unlabeled examples in task t  X  [1 : T ] , thus we have N = t N M = total number of features from all the V views is denoted as D v be the number of features in the view v  X  [1 : V ] , and we have D
For each view v present in task t , the feature matrix of the la-beled examples is X v t  X  R N t  X  D v . The feature matrix of the un-labeled examples is U v t  X  R M t  X  D v .Let y t  X  X  1 ,  X  the label vector of the labeled examples in the task X responding to the concatenated feature matrix of the labeled and unlabeled examples for task t , respectively. It is common that in some applications not all tasks have features available from all the V views, so we introduce an indicator matrix I d  X  X  1 , 0 } T  X  V mark which view is missing from which task, i.e. I d ( t, v the view v is missing from task t , and = 1 otherwise. Using this no-tation, we only handle  X  X tructured X  missing views in the sense that if a view is present in a task, it is present in all the samples in the task; if a view is missing from a task , it is missing in all the samples in the task. Throughout the paper we use subscripts to denote tasks and superscripts to denote views.
We illustrate the problem of learning multiple related tasks with multi-view data in Figure 1. We hypot hesize that we can construct better classification models by considering information from mul-tiple views and learning multiple related tasks simultaneously. In our method we learn one linear mapping function f v t : { 1 ,  X  1 } for each view v present in the task t . We search those mapping functions based on two intuition. First, for a given task we expect that the mapping functions f v t  X  X  from all its views agree with one another as much as possible on the unlabeled samples. Second, for a given view, we expect that all the mapping functions in different tasks behave similarly. We formalize the two intuitions using regularization functions in a supervised learning framework. The details of the regularization, as well as our supervised learning framework, are introduced below.
The basic assumption underlying multi-view lear ning for a sin-gle task is that the multiple views are conditionally independent and each view generates a predictive model that can be used to make predictions on data examples, while the final models are obtained from these view models. Without prior knowledge on which view contributes more to the final models than other views, we assume that all views contribute equally, following [21, 22]. The final mod-els are obtained by averaging the prediction results from all view functions as follows: where x has totally V views, x v is the set of features for view and f v is the view function generated on view v .

In multi-view learning, we want the models built on each single view to agree with one another as much as possible on unlabeled examples. Co-regularization is a technique to enforce such model agreement on unlabeled examples. The view functions f v for all views v  X  X  are obtained from the following objective function, min where . denotes the 2 norm. L ( ., . ) is the loss function that penalizes the misclassification on labeled examples. f v ( prediction results by applying the function f v to each sample in the unlabeled data for the view v .  X  is the parameter that regulates the strength of the 2 -norm regularization on view functions, and  X  is the coupling parameter that regularizes the disagreement of different views. By minimizing the three terms jointly, an optimal set of view functions that minimize the misclassification of labeled examples and maximize the agreement on the prediction results on unlabeled examples can be identified.
Given multiple related tasks with multi-view data, it is advan-tageous to learn these tasks in the same framework to achieve the benefits of both multi-task learning and multi-view learning. One approach to extend the co-regularized MVL framework is to learn each of the multiple task individually, as presented below. where f v t is a view specific mapping function for the view task t .

Apparently the formula does not take advantage of the presence tional information, we apply an additional regularization function that penalizes the difference of the view specific functions on the same view across different tasks. For each view v  X  [1 : task t  X  [1 : T ] , we learn a linear mapping function indexed by a parameter w v t  X  R D v  X  1 . We denote w t  X  R D  X  1 as the column vector by concatenating all w v t for the task t , and we have:
Using the least square loss function, we have the objective func-tion for the T tasks with V views for each task as follows: min where w v t , w t ,  X  ,and  X  were explained before.  X  is a new reg-ularization parameter to penalize the difference of view mapping function across different tasks for the same view. We call this for-malization  X  X he regularized multi-view multi-task learning X  ( reg-MVMT ).

To solve the related optimization problem of regMVMT ,wede-note the objective function as F , and compute its derivative regard-ing to each w v t as follows: Set Eq.(6) to zero, rearrange the terms and we have: where I D v is a D v  X  D v identity matrix. Note that we can have such an equation for each view v in the task t . To learn must also learn w v t from other views v ( v = v )inthetask t from other tasks to learn all w v t  X  X  jointly from a large set of equations as in Eq.(7) by taking derivatives regarding to each view in each task, as in the following linear equation system: where L X  R TD  X  TD is a sparse block matrix with TV  X  TV blocks. Each block corresponds to a view in a task and its size is the feature dimensionality of the view.

Matrix L has the following form:  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X 
R =Vec([ E 11 , ..., E 1 V ,E 21 , ..., E 2 V , ..., E T 1 where Vec() denotes the function stacking the column vectors in a matrix to a single column vector. Here column vector W and generated by stacking all column vectors w v t and E v t , respectively. Note that L is a sparse block matrix with TV  X  TV blocks, and block matrix A tv , B t vv ,and C t v are defined in Eq.(7). The an-alytic solution of W can be easily obtained from Eq.(8) by taking the inverse of matrix L . When there is a new data example from task t with x  X  t as the concatenated row feature vector of all the view, the predictive outputs y  X  t is given by: Note that by setting  X  =0 the regMVMT algorithm degenerates to the co-regularized multi-view learning algorithm. Similarly set  X  =0 the regMVMT algorithm degenerates to the regularized multi-task learning algorithm. These two methods are special cases of our MVMT method when there is only one task or one view for each task, and we will implement them as comparison baseline methods. In Figure 2 we show a comparison between the regMVMT method and the competing transductive IteM 2 method on a data set for webpage classification. Here we could see that regMVMT im-proves significantly upon the IteM 2 method. Detailed experimental studies are in Section 4.
In many MTL applications, tasks may not be uniformly related to each other. A remedy to those situation, as we investigate here, is to introduce a task relationship inference component [25]. The basic idea is to use a positive semi-definite  X   X  R T  X  T the similarity among T related tasks, and hence  X  must be with finite complexity. For single-view MTL problem, we may utilize  X  in the following objective function: min s . t .  X  0 , tr(  X  )=1 , (11) where W is a matrix whose t th column is w t for task t . the matrix Frobenius norm. tr () is the trace of a matrix.
When we have V views for the T task, we learn a task similarity matrix  X  v for each view v of the T tasks, and the MVMT objective function is as follows: min where w t is a column vector concatenated from w v t  X  X  for all [1 : V ] ,and W v is a matrix whose t th column is w v t for all [1 : T ] .Theinverseof  X  v is a T  X  T matrix whose element at ( i, j ) is the similarity between task i and j .

Though the objective function is convex regarding to both { and {  X  v }, simultaneous optimization of them is technically and computationally challenging. Alternate optimization of w  X  v  X  X  individually can achieve the optimal solutions efficiently. If we fix the { w t } X  X  and optimize the {  X  v }, take the partial derivative of the objective function in Eq.(12) with regard to {  X  v to zero. The analytic solution to {  X  v }is: Once  X  v is learned, we can use the same algorithm that we have derived in Eq. 7. We find that only the block matrix A tv are different from the formulas in Eq.(7), while the formulas of matrix B t vv and E tv remain unchanged. The new A tv and as follows: where c v ij denote the element ( i, j ) of the inverse of matrix and it is a scalar constant in each iteration. We denote this approach regMVMT+ since it is a variant extension of the regMVMT method.
In the previous subsections, we consider the ideal case that all tasks in a data set have complete data. When we have incom-plete data, the MVMT learning problem becomes more challeng-ing. Missing value imputation has been widely discussed, and here we are not concerned about randomly missed feature values. We aim to handel the case of  X  X tructured X  missing views. In the con-text of this discussion, we focus on completely missing views for a task in the sense that if a view is present in a task, it is present in all the samples in the task; if a view is missing from a task, it is missing in all the samples in the task. We recognize that there is a more challenging case where we have partially observed views (i.e. some views are missing from some samples in a task). Partially ob-served views is beyond the scope of this paper. A straightforward strategy to handel structured missing views is to remove any tasks with missing views, which, however, will significantly reduce the number of related tasks available and also discard useful informa-tion present in the remaining views of those tasks.

To handel structured missing views, we introduce an indicator matrix I d  X  1 , 0 T  X  V to mark the missing views for the with a total of V views for each task, i.e. I d ( t, v )= view v  X  [1 : V ] is missing in the task t  X  [1 : T ] ,and otherwise. Let V t  X  V and T v  X  T denote the real number of views in task t and the number of tasks for view v , and we need to use V t and T v to replace V and T in all the above equations, respectively. In addition, the indicator scalars I tv must be associated with the corresponding matrices from X In matrix  X  v , a task is considered uncorrelated to any other tasks in terms of view v if it does not has the view v , and the dimension of
 X  v will be reduced by one row and one column. Though  X  v may have different dimensionality for different v , we learn each  X  v individually and we only need the trace of the resulting product matrix.

If view v is missing from task t , w v t in W , E v t in R 1)
V + v ) -th block row and block column in matrix L are all-zero matrices. After removing these all-zero blocks from L R , we have their compact versions L , W ,and R , respectively. Eq.(8) is converted to: Solving this equation is similar to Eq.(8).
The objective function in Eq.(5) is convex regarding to hence it has a global minimum. From the derivation of the ma-trix L , we can tell that it is symmetric. Since matrix L from the derivative of a convex function, it must be at least positive semi-definite. If there is no all-zero row or column in matrix i.e. each task has at least one view and each view present in at least one task, matrix L is also positive semi-definite.

The implementation of the regMVMT algorithm is straightfor-ward with the analytic solution. We first calculate the block ma-trices A v t ,B t vv ,C v t , and the column vector E v present in each task t , construct the large sparse square matrix and the long column vector R according to the equations in Eq.(9). We then easily obtain the solutions w t  X  X  as in Eq.(8) by computing the inverse of matrix L for a given set of regularization parameters. The parameters  X  ,  X  ,and  X  will be optimized using standard cross validation. Note that the first column of each input feature matrix is an extra added unit vector to offset the intercepts ( b functions. Since the intercepts are not included in any regulariza-tion terms, an identity matrix with the first diagonal element as 0 is multiplied with the constant terms (  X  +  X  ( T  X  1) and  X   X  when constructing the matrix A tv and C tv .
 Let D T denote the number of rows or columns in L , and we have D v . When there is no missing view in any task, we have D T = D . Though the process of constructing square matrix L is complex, the speed-limiting step of the regMVMT algorithm is the inversion of the large sparse matrix L in Eq.(9). The construction of matrix L has a time complexity of O ( T ( T  X  1) D 2 +2( N + M ) D 2 computing its inverse matrix is of complexity O ( D 3 T ) , so the total number of features instead of the number of data examples domi-nates the time complexity. Matrix L is highly sparse, symmetric, and positive semi-definite, and the overall time complexity of the regMVMT algorithm is dependent on its sparsity structure.
L EMMA 3.1. The time complexity of the regMVMT algorithm is O ((1 + 2  X  n/T ) D 2 samples in each task, D T = T,V t,v =1 I tv D v .

P ROOF . It is straightforward that constructing matrix L tor R has time complexity of O ((1 + 2  X  n/T ) D 2 T ) ,andtheinver-sion of the positive semi-definite matrix L has time complexity of O ( 1 3 D 3 T ) using Chomsky decomposition. Hence we have the de-scribed overall time complexity.

The constant factor hidden behind the asymptotic complexity de-pends on the sparsity structure of matrix L .Since D T is generally much greater than T,V,N ,and M , the running time is determined by the total number of features D T from all views present in all tasks. In addition, when we store matrices in sparse matrix format, the space complexity of our algorithm is dependent on the number of non-zero elements in matrix L and the space used by other much small matrices is eligible. We can easily obtain the space complex-ity from the structure of matrix L .

P ROPOSITION 3.1. The space complexity of the regMVMT al-gorithm is approximately O ( t D 2 t + D t T ( T  X  1)) .
In real-world data sets, the number of views V is usually small ( 2  X  5 ), and the total number of features D fromallviewsisgen-erally much greater than the number of tasks T . Generally be in the range of a few hundred to thousands, and feature selection approaches are needed when it is even larger. Due to the limit of the matrix size in most computer systems, our algorithm can only handle up to tens of tasks, and learning problems with hundreds of tasks or more are hence beyond the scope of this paper.
The implementation of the regMVMT+ algorithm is similar to regMVMT algorithm. One difference is the construction of block matrix A tv  X  X  and C tv  X  X . An additional difference is that regMVMT+ is an iterative algorithm, and within each iteration the optimization of w t  X  X  uses a procedure that is very similar to the regMVMT al-gorithm. We repeat the procedure until the predefined convergence thresholds of w t  X  X  and  X  v  X  X  have been met or we have reached a Algorithm 1 The regMVMT+ Algorithm 2: Output: { W t } T t =1 , {  X  v } V v =1 3: Initialize W 0 := 0 and  X  v 0 := 1 T I T for v  X  [1 : 4: for it =1 to N it do 5: for ( t, v )  X  [1 : T ]  X  [1 : V ] do 6: Construct matrix A tv in Eq.(14) and vector E tv in Eq.(7) 7: Construct matrix B t vv in Eq.(7) and C t v in Eq.(14) for 8: end for 9: Construct square matrix L and column vector R in Eq.(9) 10: Compute W := L  X  1 R 11: Update  X  v using Eq.(13) for each v  X  [1 : V ] 12: if W  X  W 0 1 &lt; &amp;  X  v  X   X  v 0 1 &lt; then 13: break 14: else 15: W 0 := W 16:  X  v 0 :=  X  v for each v  X  [1 : V ] 17: end if 18: end for 19: Split W into T vectors, and return { W t } T t =1 and maximal number of iterations. It is expected that this method needs significantly more computational time for the numerical solutions.
We design an efficient iterative algorithm to optimize both and  X  v  X  X  alternately and implement it as in  X  X he regMVMT+ Al-gorithm X  pseudo code block 3.8. First  X  v is initialized to is the T  X  T identity matrix) for each view v , which corresponds to the assumption that all tasks are initially uncorrelated. We op-timize the convex objective function in Eq.(12) over w t  X  X , update  X  v  X  X  using Eq.(13), and then plug in the new  X  v  X  X  to Eq.(12) to op-timize w t  X  X  again. The procedure is repeated until the convergence of both w t  X  X  and  X  v  X  X  under a predefined threshold.
In this section, we present the experimental results of the two proposed MVMT methods and four baseline methods on three real-world multi-view data sets with multiple tasks. The baseline meth-ods that we used are detailed below.

Regularized MTL (regMT): If we consider no co-regularization on different views in a given task, we convert the MVMT learning problem into the single-view MTL problem [15] by concatenating merging multiple views into one view. Comparison with this base-line method can help demonstrate the benefits from using multiple views instead of a single view.

Co-regularized MVL (coMV): By ignoring the multi-task related-ness, we apply the co-regularized MVL method [21] on each task. The implementation is obtained by setting the parameter  X  the regMVMT formulation.

Multi-task Relationship Learning (regMT+): We may learn the task relationships among the multiple related tasks from the input data when we consider no regularization on different views in a given task. The implementation is obtained by setting the parame-ter  X  =0 in the regMVMT+ formulation.

Iterative MVMT (IteM 2 ): We also compare our methods with the state-of-the-art MVMT method proposed by He et al. [16]. The au-thors didn X  X  release the software, and we implemented it according to the pseudo-code provided in the IteM 2 algorithm in the origi-nal paper. This is the most important competing method that our methods will compare with.
We collected three multi-view data sets with multiple tasks. The first one is the WebKB data set [6] with 1,051 webpages collected from four universities, and the goal is to classify whether each web-page is course related or not. Here each university is a task, and we have four tasks. There are three views for each webpage: the bag-of-word features from the webpage title, from the webpage main text, and from the main text of the webpage with hyperlinks to the given webpage. There is no missing views in this data set. The second one is the email spam data set in the ECML 2006 Discovery Challenge 2 , and the goal is to classify if each email is spam or not. There are three email users with 2,500 emails for each user, and each user is considered a task. Four bag-of-words views are created: one common view shared by all three tasks, and three task specific views with each for a task. The common view consists of the common vocabulary that exists in all the three tasks, while each task specific view consists of the vocabulary unique to each task, as discussed in He et al. [16]. Each task has two missing views, which are the two views specific to the other two tasks.
The third data set is extracted from the NUS-WIDE Object web image database [12] where each image is annotated by objects such as  X  X ook X ,  X  X ird X , and etc. We removed all images that are associ-ated with zero or only one object, resulting in an object data set con-sisting of 3,545 samples in 31 tasks. We used a total of 634 features extracted from images [12], which can be considered as five views: 64-dim color histogram, 144-dim color correlogram, 73-dim edge direction histogram, 128-dim wavelet texture, and 225-dim block-wise color moments. We merged the five types of features for each sample into two views with the 225-dim block-wise color moments as one view and the rest as the other view. By removing those tasks with too few positive or negative examples, we obtained a multi-view data set with 11 tasks. Since this data set consists of a significant portion of negative features, when we applied the IteM method to it, we added a positive constant to the values of neg-ative features of all samples so that all negative features become non-negative. There is no missing views in this data set. The three data sets are summarized in Table 1, where N p and denote the total number of positive and negative examples available in each data set. As stated in Section 3, T is the number of tasks, is the number of views, and D is the total number of features from all views.
 Data Set WebKB ECML2006 NUS-WIDE Object V 34 2  X  5 T 43 11 View Missing? No Yes No N p 230 2,543 389  X  1325 N n 821 2,929 2220  X  3156
D 2,096 5,597 634
In each MVMT data set we randomly select the same number n of labeled samples and the same number m of unlabeled sam-ples for each task, where n is varying in the range [20,80] with the increment of 10, and m is generally 2  X  4 times of n . For each subset consisting of n labeled samples and m unlabeled samples for each task, we first randomly selected 20% labeled samples as the independent testing set, and the remaining 80% labeled and all unlabeled samples are the training set. http://www.ecmlpkdd2006.org/challenge.html
The regularization parameters in our methods allow flexible trade-offs between different regularization terms. We apply five-fold cross validation on the training set to optimize the parameters for each method discussed in this paper:  X  ,  X  and  X  for regMVMT and regMVMT+ ,  X  and  X  for regMT and regMT+ ,and  X  and  X  for coMV . For the IteM 2 method, we performed five-fold cross valida-tion to optimize the parameter  X  and b , but found not much differ-ence. We simply used the optimal values of parameter  X  =0 and b =1 that were set in the original paper [16]. We applied grid searching to identify optimal values for each regularization param-eter.

After obtaining the optimal parameters for each method, we con-struct a predictive model using all the training samples and calcu-late the classification error for each task on the independent testing set using the final task functions in Eq.(9). Each experiment was re-peated for 10 times, the mean classification error for each task was calculated separately, and the mean and standard deviation of the classification errors across all tasks in each data set were reported. Here classification error =1  X  ( TP + TN ) /N ,where TP,TN and N stand for the number of true positives, true negatives, and the total number of samples in the testing set, respectively.
We first consider the ideal case that all tasks have complete-view data, i.e. each of the T tasks has features from all the V the indicator matrix I d is a unit matrix. We perform experiments on the WebKB data set which has four tasks ( T =4 ) and each task shares all the three views ( V =3 ). The left panel in Figure 3 shows for each method how the mean classification error changes with regard to the number of labeled samples. Note that the standard deviations are marked as error bars in each curve.

We observe a common trend that the classification errors and the variances for all methods decrease as the number of labeled samples for training increases. The two MTL baseline methods ( regMT and regMT+ perform marginally better than the MVL base-line method ( coMV ), but the performance difference among the three baseline methods is not significant in a two-sample t-test. We test the significance of the results between regMVMT and the two baselines regMT and coMV with two-sample t-test, and find that regMVMT significantly outperform regMT and coMV at the 5% significance level. Similarly t-test shows that regMVMT+ is sig-nificantly better than the baseline regMT+ . Our proposed inductive MVMT methods ( regMVMT and regMVMT+ ) significantly outper-form the transductive MVMT method (IteM 2 )byHe et al. with the improvement margin of up to 9  X  12 %.

Next we investigate the performance of our MVMT methods on the NUS-WIDE Object data set. We conduct similar experiments and present the results in the right panel in Figure 3. We observe similar trends to the results of the WebKB data set. Our proposed MVMT methods ( regMVMT and regMVMT+ ) perform better than all the the baselines, especially the IteM 2 method, and the differ-ence is statistically significant with a two-sample t-test.
Different from the previous data set, the regMVMT method with regularization performs slightly better than the variant MVMT method with task relationship learning ( regMVMT+ ), but the dif-three MTL/MVL baseline methods are close and their curves are entangled. In this data set the improvement margin of our MVMT methods is up to 8% over the transductive counterpart.
As we discussed earlier, it is common that the multiple tasks in real-world data sets do not share all views. We would like to examine the performance of our MVMT methods compared with the IteM 2 method and other baseline methods in this setting. In the WebKB data set, we randomly select one view in each task and masked it as a missing view. Hence a multi-view data set with missing views is artificially creat ed. Here we enforce a constraint that no more than two tasks miss the same view. We perform the same experiments on this new data set using all the methods, and present the results in the left panel in Figure 4. First we observe that the trend of the performance curves of all methods are very similar to the left panel in Figure 3. Due to the information loss in the missing views, the performance of all methods decreases for about 5%. Our proposed MVMT methods build accurate models using the existing views in each task, and significantly outperform the IteM 2 method with an improvement margin of up to 12%. In addition, the regMVMT+ method slightly performs better than the regMVMT method, maybe due to the more flexible modeling of task relationships.

We then conduct experiments on the ECML2006 Email data set which consists of three task and four views. There is one common vocabulary view that is shared by all the three tasks, while each of the other three views is task specific and owned by only a particular task. Since the four views are not shared by all the three tasks, it is equivalent to the case there are two entire views missing from each of the three tasks. The experimental results of all methods are plot-ted in the right panel in Figure 4. Our MVMT methods performs better than the three baseline methods and the transductive MVMT method IteM 2 with the margin of up to 7%, and repeated measures t-test (paired two-sample t-test) demonstrates that the difference is statistically significant. All other similar trends are also observed. Since there are more missing views in each task, the overall perfor-mance of all methods shrinks more significantly.
We perform a case study to understand how our methods learn task relationships using the NUS-WIDE Object data set with 11 tasks. For any two tasks, we calculate the pairwise correlation using the fraction of samples that are simultaneously active or inactive to the two tasks. For the regMVMT method, we calculate the learned relationship between any two tasks using the Gaussian kernel of the 2 norm of the difference between their decision functions as in Eq.(5). We make a 2D plot with each dot representing a pair of tasks, as presented in Figure 5. The correlation coefficient 0 . 8978 reveals the learned task relationships ( y -axis) are highly correlated to the pairwise task correlation from the data ( Figure 5: Correlation analysis ( regMVMT ) on the Object data set.
Looking into the 11 tasks, we find that they form three clusters: animals (5 tasks), vehicles (3 tasks), and plants (3 tasks). We then apply the regMVMT+ algorithm on this data set and obtain the ma-trices  X  v  X  X  that model the task relationships of the 11 tasks. After taking the mean of all task relationship matrices  X  v ,v =1 obtain the learned task relationship matrix whose element ( dicates the relationship index of task i and j . For each task in the animal cluster, we identify the four most significantly correlated tasks, which are found mostly also the tasks in the animal class with only two violations. For the vehicle class and the plant class, we also find two violations that are not in the same class as the seed task, one in each class. The experiments demonstrate that the reg-MVMT+ method can well model the task relationships through the learning process.
In this paper, we proposed an induc tive multi-view learning al-gorithm for multiple related tasks. In our algorithm we developed a co-regularized framework. We utilized several regularization func-tions to control the complexity of the learning algorithms. We also developed two extensions. One handles structured missing views and the other handles non-uniformly related tasks. Experimental results demonstrated that our MVMT methods significantly outper-form the state-of-the-art MVMT method IteM 2 and other baseline methods. In the future, we will further extend our work to different types of missing values.
 This work has been supported by the KU Specialized Chemistry Center (NIH U54 HG005031) and the NSF grant IIS 0845951. [1] S. Abney. Bootstrapping. In Proceedings of the 40th Annual [2] M.-R. Amini, N. Usunier, and C. Goutte. Learning from [3] R. Ando and T. Zhang. A framework for learning predictive [4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature [5] M.-f. Balcan and A. Blum. A pac-style model for learning [6] A. Blum and T. Mitchell. Combining labeled and unlabeled [7] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. Learning [8] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear [9] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse and [10] X. Chen, S. Kim, Q. Lin, J. Carbonell, and E. Xing. [11] C. M. Christoudias, R. Urt asun, and T. Darrell. Multi-view [12] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. [13] M. Culp, G. Michailidis1, and K. Johnson. On multi-view [14] S. Dasgupta, M. Littman, and D. McAllester. Pac [15] T. Evgeniou and M. Pontil. Regularized multi-task learning. [16] J. He and R. Lawrence. A graph-based framework for [17] S. Kim and E. P. Xing. Tree-guided group lasso for [18] A. McCallum, K. Nigam, J. Rennie, and S. Kim. Automating [19] I. Muslea, S. Minton, and C. A. Knoblock. Adaptive view [20] K. Nigam and R. Ghani. Analyzing the effectiveness and [21] V. Sindhwani and P. Niyogi. A co-regularized approach to [22] V. Sindhwani and D. Rose nberg. An rkhs for multi-view [23] W. Wang and Z. H. Zhou. Analyzing co-training style [24] W. Wang and Z. H. Zhou. A new analysis of co-training. In [25] Y. Zhang and D.-Y. Yeung. A convex formulation for
