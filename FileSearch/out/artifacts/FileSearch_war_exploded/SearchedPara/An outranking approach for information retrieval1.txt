 Mohamed Farah  X  Daniel Vanderpooten Abstract Over the last three decades, research in Information Retrieval (IR) shows performance improvement when many sources of evidence are combined to produce a ranking of documents. Most current approaches assess document relevance by computing a single score which aggregates values of some attributes or criteria. They use analytic aggregation operators which either lead to a loss of valuable information, e.g., the min or lexicographic operators, or allow very bad scores on some criteria to be compensated with good ones, e.g., the weighted sum operator. Moreover, all these approaches do not handle using a new aggregation mechanism based on decision rules identifying positive and negative reasons for judging whether a document should get a better ranking than another. The resulting procedure also handles imprecision in criteria design. Experimental results are reported showing that the suggested method performs better than standard aggregation operators.
 Keywords Information retrieval Relevance Outranking approach Multiple criteria Aggregation 1 Introduction Information Retrieval (IR) is concerned with situations where a user, having information needs, performs queries on a collection of documents to find a limited subset of the most relevant ones. Performances of IR systems is measured by its ability to search and retrieve relevant documents as efficiently and effectively as possible. In this paper, efficiency, which refers to the ability of a system to provide results within reasonable response times, is not our main concern. We primarily focus on retrieval effectiveness, which refers to the ability of a system to deliver the most relevant results first. Relevance is indeed the main challenge for most search engines as shown by several comparative studies reporting limitations in their performances (Hawking et al. 2001 ).

In the literature, a wide range of models have been proposed to rank documents according to their relevance to queries. They result in different rankings depending on the way they define relevance. In fact, relevance is reflected by the sources of evidence that are considered, as well as the way they are combined.

Most of the current approaches assess document relevance by computing a single score which aggregates values of elementary attributes related to the query terms, the document (Salton et al. 1975 ), the Okapi BM25 probabilistic model (Robertson et al. 1994 ) as well as language models (Cao et al. 2005 ), term frequency ( tf ), document frequency ( df ) and combined in the term weighting formulation which corresponds to a first aggregation phase. The resulting scores are in turn considered to compute document relevance status value (rsv) to queries, as a second aggregation phase.

With the advent of hypertext collections, such as the Web, attributes characterizing the hyperlink structure are considered and led to link-based measures such as Kleinberg X  X  HITS scores (Kleinberg 1999 ), PageRank scores (Brin and Page 1998 ) and HostRank scores (Amento et al. 2000 ).

All these text-and link-based attributes can be combined to get better performance. A variety of aggregation operators have been used such as the min and max operators in (Fox and Shaw 1994 ) or the weighted linear operator in (Craswell et al. 2005 ). Other aggre-gation operators include similarity-based measures (Van Rijsbergen 1979 ; Salton and McGill 1983 ; Frakes and Baeza-Yates 1992 ), P-norms (Salton et al. 1983 ), or fuzzy-logic conjunctive and disjunctive operators (Dubois and Prade 1984 ).

In some cases, aggregation is performed in an ad-hoc manner. For instance, in (Kraaij et al. 2002 ) link-based attributes such as in-degree and URL, are used as priors in language models. Another way consists in aggregating evidence in two stages. In the first stage, text-based attributes are combined to get scores of documents. In the second stage, the resulting top ranked documents are re-ordered according to link information by using techniques such as spreading activation or probabilistic argumentation (Savoy and Rasolofo 2000 ). Thus, these approaches do not explicitly use link-attributes.

Each aggregation operator conveys a specific aggregation logic which reflects the degree of compensation we are ready to accept. In the IR literature, two main classes of operators are in use. The first class corresponds to a totally compensatory logic . It consists of building a single score using a more or less complex operator such as the weighted sum. For such operators, a very bad score on one criterion can be compensated by one or several good scores on other criteria. These operators often require inter-criteria information such as weights, which are sometimes difficult to define and interpret. Indeed, these weights aim at capturing at the same time the relative importance of criteria but also a normalization factor when criteria are expressed on different scales.

The second class corresponds to a non-compensatory logic . In this case, aggregation is important criterion. The remaining criteria are only used to discriminate documents with similar scores. This gives rise to min-based or lexicographic-based operators, variations of which are the discrimin and leximin operators (Boughanem et al. 2005 ). A clear weakness of this class of operators is that a large part of the scores is ignored or plays a minor role.
Moreover, in both classes, we do not consider imprecision underlying criteria design resulting from the fact that there are many acceptable formulations of the same criterion: for instance, Anh and Moffat ( 2002 ) proposed four alternative formulations of the tf criterion. Therefore, it is important to give a limited interpretation to values, i.e., we should consider that slight differences in values are often not meaningful. This way, the resulting rankings are more robust .

In this paper, we propose a multiple criteria framework which combines any set of criteria while taking into consideration the imprecision underlying the criteria design process. We first put emphasis on the importance of the design of good criterion families capturing complementary aspects of relevance and give clues to the design of such fam-ilies. Then, we describe ranking procedures based on natural decision rules.

Multiple criteria techniques were previously used in IR, especially in information fil-tering (Pasi et al. 2007 ) as well as in data fusion (Bordogna et al. 2003 ; Bordogna and Pasi 2004 ). Nevertheless, the proposed methods basically use fuzzy sets theory. In this paper, we use a different kind of aggregation mechanisms.

The paper is organized as follows. We first introduce the multiple criteria framework where we describe the overall approach and its component phases (Sect. 2 ). Then, we highlight some specificities of the IR problem which are addressed in the proposed approach (Sect. 3 ). Section 4 deals with the modeling phase which consists in designing a set of relevance criteria. We present in Sect. 5 , a filtering procedure whose purpose is to obtain a reduced set of potentially relevant documents. Section 6 shows how to aggregate such criteria and build the final ranking. The complexity of the whole approach is investigated at the end of this section. We report experimental results in Sect. 7 and provide conclusions in a final section. 2 A multiple criteria framework for IR Many studies argued that the reason why no consensus has been reached on the relevance concept is that there are many kinds of relevance, not just one, as stated by Borlund ( 2003 ). Moreover, different sources of evidence are contributing to capture the relevance concept. Therefore, being able to make effective use of these sources of evidence can significantly improve retrieval effectiveness.

We propose a formal approach for IR where relevance is explicitly defined as multi-dimensional (by a set of criteria) and ranking is derived from pairwise comparisons of document performance vectors ( document profiles ) using decision rules identifying posi-tive and negative reasons for judging whether or not a document should get a better ranking than another. The overall approach can be split into four phases (see Fig. 1 ) which will be detailed in the following sections:  X  The modeling phase consists in identifying various attributes affecting relevance.  X  The filtering phase aims at identifying the set of potentially relevant documents with  X  The aggregation phase aggregates partial preference relations derived from pairwise  X  The exploitation phase processes global preference relations resulting from the The last two phases correspond to the ranking phase .

It is worth noting that the proposed method is collection-and representation-indepen-dent to some extent. It can thus be used for any type of collection and combined with the phase in order to devise relevant criterion families. 3 Specificities of the IR problem The IR problem can be considered as a multiple criteria decision problem when we explicitly consider the multidimensional nature of relevance. Nevertheless, it has some particularities that have an impact on the modeling phase as well as on the aggregation and exploitation phases.
 3.1 Specificities for the modeling phase Specificity 1: Two kinds of criteria need to be considered to assess documents relevance: query-dependent and query-independent criteria.

Query-dependent criteria measure semantic proximity between documents and queries and are derived from attributes about the form of occurrences of query terms in the document and the collection. Examples of such attributes are term frequency ( tf ) and document frequency ( df ).

The evaluation of query-dependent criteria depends on the structure of the query. In fact, we should distinguish one-term queries from multi-terms queries . Some criteria are only relevant in the second case. Moreover, for multi-terms queries, two evaluation levels evaluations. Therefore, the design of such criteria deserves thorough analysis. This is addressed in Sect. 4.1 .

Query-independent criteria mainly refer to characteristics of the document and the collection. They can be evaluated independently of the query. Examples of such criteria are document length ( dl ) and PageRank. We need such criteria to better help discriminating between documents. In fact, the query frequently consists of two or three terms in average, and this cannot be sufficient to rank thousands or millions of documents.

Specificity 2: Criteria can play different roles depending on which phase they are used in. In the filtering phase, they are primarily used to build acceptance profiles which help separating potentially relevant documents. In the ranking phase, they are used for pairwise comparisons. 3.2 Specificities for the ranking phase Specificity 3: Criteria to be used to establish relevance are not specified by the user. They are rather based on attributes evidenced to best capture relevance by the IR community. Consequently, it is difficult to get precise preference information regarding their relative importance. In this case, we assume that each criterion is neither prevailing nor negligible. Therefore, we should use appropriate ranking procedures.

Specificity 4: The query is too poor to justify a precise ranking of documents. One can expect that many of the  X  X ost relevant X  documents should be present in the head of the ranking, but their exact ranking is meaningless. This can also be justified in terms of users behavior when interacting with the results pages of search engines. In fact, research in eye-tracking analysis of users behavior has shown that once users have started scrolling, rank becomes less of an influence for attention (Granka et al. 2004 ). Therefore, even if a overemphasized. 4 Modeling phase In our context, a criterion models relevance between documents, regarding a specific ments and aims at comparing any pair of documents d and d 0 , on a specific point of view, as follows:
For instance, considering the term frequency criterion ( tf ), it is always common to consider that when one query term occurs more frequently in the body of document d than in document d 0 , then d is judged more relevant than d 0 , ceteris paribus: tf  X  d  X  tf  X  d 0  X ) d  X  X s at least as relevant as X  d 0 according to criterion tf .

Choosing the right criterion family depends on the task at hand as well as the type of information that documents encompass. In fact, retrieving images or video sequences differs greatly from retrieving textual documents since each kind of information encom-important impact on the final ranking.

Although many candidate criterion families could be derived from the same considered requirements:  X  each criterion should be concerned with a specific point of view,  X  all attributes deemed to be important in comparing two documents should be captured  X  we should avoid redundancy, i.e., we should not consider the same attribute more than  X  while building the criterion family, we should have in mind the way it will be used in
It is worth noting that many formulations of the same criterion are possible. Therefore, we should not overemphasize the criterion scores of documents. We briefly discuss two important issues of the modeling phase. 4.1 Evaluation of query-dependent criteria To build some query-dependent criteria, such as the tf -like criterion, we need to make a clear distinction between one-term and multi-terms queries. For one-term queries, building criteria has no specific difficulties, but to deal with multi-terms queries, i.e., conjunctive and/or disjunctive queries, we can proceed in two steps:  X  build a sub-criterion corresponding to each term of the query. Each literal of the query  X  select an aggregation operator corresponding to each query-type (conjunctive query,
Since elements being aggregated in the sub-aggregation step are homogeneous, we can use analytic aggregation operators like conjunctive, disjunctive or compensatory operators (Dubois and Prade 1984 ), depending on the aggregation logic we wish to use and on the interpretation given to the juxtaposition of terms.

For instance, let us suppose that we want to assess the relevance of documents to some step, we combine these different scores into one single score using some aggregation operator such as the average operator, i.e., tf  X  d  X  X  tf  X  d ; t k  X  n 4.2 Modeling imprecision It is often inadequate to consider that slight differences in evaluation should give rise to clear-cut distinctions. This is particularly true when different formulations of criteria are acceptable. Imprecision underlying criteria design can be modeled using the following discrimination thresholds (Roy 1989 ):  X  X n indifference threshold allows for two documents with close criterion values to be  X  X  preference threshold is introduced when we want or need to be more precise when
A criterion g j , having indifference and preference thresholds, q j and p j , respectively ( p j C q j C 0), is called a pseudo-criterion . Comparing two documents d and d 0 according to a pseudo-criterion g j leads to the following partial preference relations: where I j , Q j and P j represent respectively indifference, weak preference and strict pref-erence relations restricted to criterion g j . These three relations could be grouped into an covered by criterion g j .

To model situations where a very low score of a document d 0 with respect to d , according to some criterion g j , cannot be compensated by a good score on one or several other criteria, we use a veto thresholdv j ( v j C p j ) and define the following veto relation V evant as X  X  , whatever the scores on other criteria.

Figure 2 summarizes the different preference situations that can be derived from the comparison of two documents d and d 0 .

We illustrate these different preference relations using the following example. Let us consider Table 1 which gives the scores of five documents evaluated according to a documents d i and d j according to criterion g . Table 3 reports the differences of document scores and Table 4 gives the relational interpretation of such differences. For instance, since q g 13  X  0 : 3 p the weak preference relation holds between d 1 and d 3 . Moreover, since g 15 [ v [ p both the strict preference relation as well as the veto relation hold assertion  X  d 5 is at least as good as d 1  X , whatever the scores on other criteria. 5 Filtering procedure In this section, we show how it is possible to get the top k best relevant documents using acceptance profiles. In fact, acceptance profiles allows us to discriminate documents that can be considered better than the acceptance profile. Different procedures can be used to obtain the top k best relevant documents. We give one such procedure.

Suppose that we have a set D of n documents, possibly resulting from the application of a first boolean filter, and we need to retain only the top k best documents, using an acceptance profile, i.e., acceptance thresholds a j on each criterion g j . The problem is setting and adjusting values a j ( j = 1, ... , p ) is to use a single parameter a corresponding criterion, a proportion a of the documents so as to retain globally a proportion k n of documents from D , a can be set to an initial value of can be adjusted so as to obtain the required size for the filtered set A . 6 Ranking procedure In order to get a global relevance model on the set of documents, we use outranking approaches (Roy 1991 ), which are quite appropriate regarding the specificities of Sect. 3.2 and are based on a partial compensatory logic . They consist of two phases: an aggregation phase and an exploitation phase. We hereafter give details about both phases. In Sect. 6.3 , we illustrate how they work precisely using a simple example. 6.1 Aggregation phase terion family and aggregate them into one or more global preference relation(s) S . They are particularly relevant in our context since they (i) permit considering imprecision in doc-ument evaluations, (ii) can handle criteria expressed on heterogeneous scales, (iii) use all the available information on document performances, and (iv) do not necessarily require inter-criteria information, such as weights.

In order to accept the assertion dSd 0 , stating that  X  X ocument d is at least as relevant as document d 0  X , the following conditions should be met:  X  X  concordance condition which ensures that a majority of criteria are concordant with  X  X  discordance condition which ensures that none of the discordant criteria strongly
In this paper, we suppose that there is no available information on the relative based on the criteria supporting (positive reasons) or refuting (negative reasons) this assertion. Obviously, the rules for defining this support may be more or less demanding, resulting in different outranking relations. For example, let  X  F ={ g 1 , ... , g p } be a family of p criteria,  X  H be a global preference relation, where H is P , Q , I , V or S ,  X  H -be a relation such that dH d 0 ( ) d 0 Hd ;  X  H j be a partial preference relation, i.e., restricted to criterion g j ,  X  c ( dHd 0 ) the number of items in C ( dHd 0 )
A candidate outranking relation is: which is a well established, but usually poor, relation since it only holds if all the criteria are concordant with dSd 0 .

We can also use less demanding outranking relations such as:
To accept dS 2 d 0 , there should be more criteria concordant with dPd 0 than criteria sup-condition. At the same time, no discordant criterion should strongly disagree with this assertion. This corresponds to the discordance condition.
To accept dS 3 d 0 , only criteria that are concordant with dPd 0 can conceal criteria sup-porting a strict preference in favor of d 0 but criteria supporting a weak preference in favor of d 0 can be concealed by criteria concordant with either a strict or a weak preference in assertion.

Observe that these three relations get richer and richer, i.e., we have S 1 S 2 S 3 ; but less and less well-established.

It is worth noting that the proposed aggregation mechanism, which compares the size of various coalitions of criteria, does not require that criteria are defined on a common scale. Therefore, normalizing criterion scales, which is always somewhat arbitrary, is unneces-sary in our approach. 6.2 Exploitation phase Outranking relations are not necessarily transitive and do not lend themselves to immediate exploitation to get the final ranking. Therefore, we need exploitation procedures in order to derive the final document ranking. We propose the following procedure which finds its roots in (Roy and Hugonnard 1982 ). It consists in partitioning the set of documents into r ranked classes where each class C h contains documents with the same score. This is coherent with specificity 4 of Sect. 3.2 . Considering that s outranking relations S 1 S have been defined, let:  X  R be the set of potential relevant documents for a query,  X  F i  X  d ; E  X  X  card  X f d 0 2 E : dS i d 0 g X  be the number of documents in E  X  E R  X  that could  X  s i  X  d ; E  X  X  F i  X  d ; E  X  f i  X  d ; E  X  be the qualification of d in E according to S i .
Each class C h results from a distillation process . It corresponds to the last distillate of a series of sets E 0 E 1 E r  X  r 1  X  ; where E 0  X  R n X  C 1 [[ C h 1  X  and E i is a reduced subset of E i 1 resulting from the application of the following procedure: 1. compute for each d 2 E i 1 its qualification according to S i , i.e., s i  X  d ; E i 1  X  ; 3. E i  X f d 2 E i 1 : s i  X  d ; E i 1  X  X  s max g
The distillation stops either when card ( E r ) = 1 or when r = s . 6.3 Illustrative example This section tries to illustrate the concepts and procedures introduced previously. Let us mance vectors of the documents of R w.r.t. a family of four pseudo-criteria F = { g 1 , g 2 , g 3 , g }. Indifference, preference and veto thresholds of these criteria are summarized in Table 6 .

We retain the outranking relations of Eqs. 1 and 2 to carry pairwise contests of the aggregation phase. We hereafter give details about the computation of the outranking considered criterion g j . All these matrices derive directly from Tables 5 and 6 . For instance, since | g 1 ( d 2 ) -g 1 ( d 1 )| = |0.7 -0.8| = 0.1 B q 1 , then d 2 S 1 d 1 holds.
For the aggregation phase, let us consider the outcome of pairwise comparison of d other hand, criterion g 3 does not support this assertion as shown in the matrix corre-sponding to relation S 3 . Therefore, d 2 S 1 d 5 does not hold according to Eq. 1 .
Criteria g 1 and g 2 are concordant with the assertion d 2 Pd 5 , therefore c ( d 2 Pd 5 ) = 2. At refutes this assertion  X  c  X  d 2 V d 5  X  6  X  0  X  ; therefore, d 2 S 2 d 5 does not hold. The computation of the global outranking relations S 1 and S 2 is reported in Table 11 .
We now move to the illustration of the exploitation phase which is responsible for building the final ranking of the documents.
 (resp. column) of the matrix of outranking relation S k , the consensus ranking is obtained as follows:
Iteration 1 : To get the first class C 1 , we compute the qualifications of all the documents of E 0 = R with respect to S 1 . They are respectively 0, 1, 2, -2 and -1. For instance, since F and C 1 = E 1 = { d 3 } since d 3 is the only document of the first distillate.
Iteration 2 : To run a new iteration and compute the next class C 2 , we first remove document d 3 from the outranking matrices by removing its corresponding rows and col-umns in both matrices. We compute the new qualifications of the documents of the new document d 2 having the maximum qualification 1 constitutes the only document of the second class C 2 .

Iteration 3 : To get the third class, we remove d 2 from both matrices of S 1 and S 2 . The remaining documents d 1 , d 4 and d 5 have the same qualification value 0. Thus, the first reduce this set. The qualifications of the documents of E 1 are respectively 2, -1 and -1. Therefore, the second distillate is E 2 = { d 1 } and corresponds to the third class C 3 .
Iteration 4 : Computing the qualifications of the remaining documents d 4 and d 5 give the same value 0 in both S 1 and S 2 . Therefore the last class C 4 corresponds to these documents: both relations do not permit building more refined ranked classes.
 The consensus ranking is finally { d 3 } ? { d 2 } ? { d 1 } ? { d 4 , d 5 }.

It is worth noting that using more standard aggregation operators lead to different rankings. For instance, supposing that document performances are normalized and that all criteria have similar weights, ranking documents according to the sum aggregation oper-particularly, document d 3 which is initially ranked first using the outranking approach is now ranked third according to the sum operator. This shows an important feature of outranking approaches: documents with acceptable and more balanced profiles (e.g., d 3 ) are preferred to documents with rather more contrasted profiles (e.g., d 1 and d 2 ) as shown in Fig. 3 . 6.4 Complexity of our approach Before presenting experimental results, we briefly investigate and comment the complexity of our approach.

Considering n documents to be processed, given a family of p fixed criteria, the filtering phase also requires O ( n ) time, whereas the ranking phase requires O ( n 2 ) time due to the computation of the various preference matrices during the aggregation phase. Remark, however, that the filtering phase drastically reduces the number of documents to be processed by the ranking phase, which makes the whole approach quite efficient. 7 Experiments and results 7.1 Test setting To facilitate empirical investigation of the proposed methodology, we developed a pro-totype search engine, named WIRES, that implements a preliminary version of our multiple criteria approach. In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-13 Web track (Craswell and Hawking 2004 ). In this task, there are 75 topics where only a short description of each is given. For the experiments, we translated each topic to a conjunctive query following most search engine strategies. We have built an inverted index of the  X .GOV X  TREC test collection where we consider word stems as index terms using the Porter stemming algorithm and discard common English stopwords. We also used the hyperlink structure of this collection to build link-based criteria.
At a first level, we had to define the set F of criteria, for which we used the following elementary features which are the main attributes used in the literature:  X  tf k : frequency of term t k in document d ,  X  df k : number of documents the term t k occurs in,  X  max tf : maximum frequency tf k of all terms t k 2 d ;  X  l k , a : a binary value which equals 1 if term t k occurs in location L a and 0 otherwise. The  X  C -( d ): set of incoming hyperlinks to d ,  X  Child ( d ): set of children documents of d . Document d 0 is in Child ( d ) if it appears in a  X  prox : proximity of query terms in document d . It corresponds to the size (number of  X  ql : query length, i.e., the number of terms of the query,  X  dl : document length, and  X  depth ( d ): depth of the URL of d , which is the number of intermediary sub-directories
Based on these features, we defined the following candidate criteria:  X  Frequency: For one-term queries (i.e., q = t k ), g 1  X  d ; t k  X  X  tf k max tf  X  Position: For one-term queries, g 2  X  d ; t k  X  X   X  Authority: g 3 ( d ) = card ( C -( d ))  X  Prominence: g 4 ( d ) = card ( Child ( d ))  X  Proximity: g 5  X  d  X  X  ql prox if prox 6  X  0 ; and 0 otherwise  X  Document length: g 6 ( d ) = dl ( d ))  X  Rareness: For one-term queries, g 7 ( d , t k ) = df k For multi-terms queries, we used the average operator.

It is worth noting that the concrete choice of the features as well as the criterion family should be monitored to best correspond to the specific application context. For our experiments, we focused on features which capture the major well-known IR evidences. somewhat arbitrary, thus we considered simple formulations, but more refined formula-tions can be used too.

In the TD task, a successful relevance ranking should favour  X  X ood entry points X  although they could contain little detailed information. This is captured by the prominence criterion ( g 4 ).

For evaluation, we used the  X  X rec_eval X  standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Average e.g., Craswell and Hawking ( 2004 )).

Our approach effectiveness is compared against some high performing official results from TREC-13 using the paired t-test which is shown to be highly reliable (more than the sign or Wilcoxon tests) according to Sanderson and Zobel ( 2005 ). In the experiments, significance testing is mainly based on the t -student statistic which is computed on the basis of the AvP values of the compared runs. In the tables of the following section, we have marked with an asterisk statistically significant differences. 7.2 Results With the criteria described before, we performed several retrieval runs. In the first set of runs, we rank documents according to each criterion and report performances in Table 12 . We aim at showing which criteria are really relevant for the TD task.

Table 12 shows that the run with the prominence criterion ( g 4 ) performs significantly better than the others. Runs carried out with the first 4 criteria perform significantly better than runs carried out with the last 3. Moreover, the random run random performs better than the same 3. Therefore document length ( g 5 ), proximity ( g 6 ), and rareness ( g 7 ) do not play an important role for the TD task.

In the second set of runs, we only considered the best four criteria, i.e., criteria g 1  X  g 4 .In our baseline run  X  mcm  X  ; the set R of potentially relevant documents is obtained in two stages: we first use the boolean filter to identify a first set A which is then extended to a set A + that includes each document pointing to at least two documents in A . Many of the added documents should, in fact, correspond to good entry points to relevant sites. In the aggregating procedure of Sect. 6.1 , each criterion is supposed to be a pseudo-criterion where indifference, preference and veto thresholds are set to 20%, 60% and 90%, respectively. These thresholds are set after some tunings carried with respect to TREC-12 Web track TD topics. We suppose that there is no information on the relative importance of criteria and use the outranking relation S 2 defined by (2). We implement the exploitation procedure of Sect. 6.2 .

We now try to catch the impact of profile filtering on performance using the procedure presented in Sect. 5 which allows us to get a reasonably small set R of documents. We carried out some runs where we tried to get different numbers of filtered documents: for each run mcm -filter -x ; x corresponds to the number of the filtered documents.
Table 13 shows that mcm -filter -x runs differs only with respect to AvP and R -p : All the other measures remain the same. This is because all these runs have the same ranking at the top. When we filter 50 documents, performance decreases rather significantly, whereas considering the R -p : measures, performance slightly increases. Performances do not sig-nificantly decrease with respect to those of mcm when we filter 1,000, 800 or 500 documents. We can conclude that filtering is beneficial for IR since it considerably reduces the size of the set of documents to be compared in the ranking procedure, and at the same time, it does not lead to significant performance drop.
 We compare now our basic run mcm with other aggregation strategies.

In Table 14 , we report performances of four aggregation operators which are max, min, sum and product operators. For these runs, documents performances are normalized so that mances are significantly worse than those of mcm . This shows that a total compensatory logic (e.g., sum and prod runs) as well as a non-compensatory logic (e.g., max and min runs) perform worse than a partial compensatory logic (e.g., mcm run) using outranking approaches for example.

We end this section by reporting performances of the official runs from TREC-13 (Craswell and Hawking 2004 ) and compare our approach accordingly.

In Table 15 , we first report performances of the best runs of the first five teams which participated to the track. Then, we computed average and median performances of all the submitted runs. From this table, we can see that mcm has similar performances compared to those of the best ones. Moreover, mcm performs significantly better than the average or the median runs. 8 Conclusions In this paper, we propose a multiple criteria framework for evidence combination where a set of candidate relevance criteria are proposed and used to determine how documents should be ranked using a set of decision rules.

The proposed approach overcomes limits of classical analytical retrieval formulas which do not allow considering complex logics when aggregating various criteria. It is also straightforward to show that the proposed approach, based on decision rules, fulfill intu-itive and desirable formal requirements that any reasonable retrieval method should satisfy according to Fang et al. ( 2004 ) work. Interestingly, these authors show that none of the formulas used in the vector space model, the probabilistic model, or the language model, satisfies these requirements unconditionally.

From the first TREC experiments, this work seems to have the potential for high impact in the field of IR, given the possible application of evidence combination. It presents the advantage that it is applicable whatever is the collection under consideration provided that a pertinent criterion family is used. It also overcomes criteria heterogeneity problems by using a set of decision rules which are easy to grasp. Moreover, the proposed approach easily helps considering domain and context specific criteria in a natural way, rather than using complex formula which are difficult to interpret.

Approaches from multiple criteria decision theory, and especially outranking approaches, are generally used as an aid for decision makers. In the TREC context, cantly better performances. At the same time, we can outline an advantage of the proposed approach since we can easily carry out the study from the user perspective by and valuable aid.

Future work will consist of additional experiments to strengthen the results. More specifically, applying our method in a human centered context would be an interesting vailing not negligible. But when there is some evidence that some criteria are more important than others without being able to assign precise values, specific outranking approaches such as Melchior (Leclercq 1984 ) are more appropriate. References
