 A new shot level video brow sing method based on semantic visual features (e.g., car, mount ain, and fire) is proposed to facilitate content-based retrieval. The video X  X  binary semantic feature vector is utilized to cal culate the score of similarity between two shot keyframes. The score is then used to browse the  X  X imilar X  keyframes in terms of semantic visual features. A pilot user study was conducted to bette r understand users X  behaviors in video retrieval context. Thr ee video retrieval and browsing systems are compared: temporal neighbor, semantic visual feature, and fused browsing system. The initial results indicated that the semantic visual feature browsing was effective and efficient for Visual Centric tasks, but not for Non-visual Centric tasks. H.3.3 [ Information Search and Retrieval ]: Relevance feedback Algorithms, Design, Experimentation, Content-based, video browsing, video retrieval, user interface Browsing technologies are supported in video retrieval to augment text based query search, particularly when exact queries are hard to form. Browsing usually follows a search operation to pinpoint the correct matches. For shot level content-based retrieval (where a shot represents a series of consecutive frames with no sudden transition), temporal neighbor browsing is the most common navigation method [2]. Temporal neighbor browsing allows users to navigate around the representative of the content of a shot) from a text query returns. Potential relevant shots may appear just before or after the sample one due to the asynchronous of the visual content and its related transcript. Mezaris et al.[3] noted that a visual similarity re-search using a sample picked keyframe is a good design for retrieval. Various visual features includi ng color histograms, text, camera movement, face detection, and moving objects can be utilized to technology such as visual networks can be used to enhance visual similarity browsing [2], but the effectiveness needs to be further verified. One limitation of these technologi es is that no functions are provided to support users to look for specific visual objects, such as people, car, map, etc., even though automatic semantic visual feature extraction technologies [1] have been developed to make these metadata easily obtainable. Targeting this problem, a new semantic visual feature browsing technology is developed in this paper. semantic description of video content, such as indoor/outdoor, people, car, and explosion. Naphade et al. [4] proposed a 39-feature lightweight ontology for TRECVID project (which was also used in our study). This ontology has a two-layer structure: the top layer includes seven categor ies: Program category, People, Objects, Setting/Scene/Site, Activ ities, Events, and Graphics; the second layer contains sub-categorie s for further classification. For instance, under the top layer category vehicle, the sub-categories include airplane, car, bus, truck, and boat/ship. Semantic visual feature browsing allows users to navigate around shots that have similar visual features of a selected sample shot. For instance, to search for shots with the face of  X  X ondoleezza Rice, X  a list of  X  X imilar X  shots that share or partly share the features of  X  X olitics, face, person, government leader, police/private security personnel X  are retrieved for more matches. The selection of the similar shot keyframes are based on the score of  X  X eature similarity X . In the study, each keyframe F dimension binary feature vector F i = (f i1, f i2, f i3,..., ontology proposed by Naphade et al. [4], and ij 1 For a selected keyframe F s, the feature similarity d and another keyframe F j is for a selected keyframe F s will be where m is the total number of keyframes in the collection. In practice, usually only top n el ements (or none zero elements) in the index will be utilized to support semantic visual feature browsing. In our study, n was defined as six. A new content-based video retr ieval and browsing system was developed as a research platform to examine the effectiveness video browsing technologies. The system supports two types of browsing: temporal neighbor br owsing and semantic visual feature browsing. Figure 1 is the main interface of the system. On the top part (part A) a traditional text input field is provided for text-based query. In our study the videos X  transcripts were utilized for text-based retrieval. of a selected shot is displayed in Part B, while Part C shows the matched shot keyframes in storyboard style. At the bottom of the interface (Part D) is a browsing panel where two browsing methods are supporte d. After performing a text query, users can subsequently proceed with further navigation in this area to find more matches. A tabbed layout is adopted to  X  X EMPORAL X  tag will lead to temporal neighbor browsing and  X  X EATURE X  tag will go to the semantic visual feature browsing. All the neighboring or similar frames will be displayed in the same size as the sample, which is highlighted in the middle of the filmstrip. A pilot user study was conducted to evaluate the effectiveness of the semantic visual browsing technology. Two types of video searching tasks were selected: Visual centric tasks (VCT) focus on visual features of a keyframe and Non-visual centric tasks (NCT) focus on non-visual features of a keyframe. The data was obtained from the TRECVID 2005 data collection, including about 86 hours of news videos (137 segments with average duration of about half an hour). Th e semantic visual features were collaboratively created by TRECVID 2005 project participants. Three types of retrieval systems were compared: Temporal neighbor (TN), Semantic visual feature (SF), and Fused (FU) browsing system. The Fused browsi ng system allows users to use both the temporal neighbor and se mantic visual feature browsing system interface (Temporal and Feature tabs are located on the left top of the browsing filmstrip in area D). Six volunteer participants from multiple majors and programs of campus participated in the study. Initial data for the effectiveness (indicated by the users themselves) and efficiency (average time spent on tasks) of the three systems are listed below. Table 1: Average effectiveness (1-5 scale with 5 very effective) 
Table 2: Average efficiency (seconds used to complete the We found that Semantic Visual F eature (SF) was very effective and efficient for Visual Centri c tasks (VCT), but not for Non-Visual Centric Tasks (NCT). The participants also described the system as easy to learn and manipulate. One user, however, complained about the slow re sponse of the system. In the future a large-scale usability study based on an improved browsing system and evaluation plan will be conducted to validate and further explore the relationships between the browsing technologies and video sear ch tasks. In addition, we will consider evaluating the system with different video genres. I am grateful to Dr. Dietmar Wolfram and Dr. Wooseob Jeong for their valuable comments about the pilot user study design. [1] Chang,S.-F., Hsu, W., Kennedy, L., Xie, L. , Yanagawa, A. , [2] Heesch, D., Howarth, P., Magalhaes, J., May, A., Pickering, [3] Mezaris, Y., Doulaverakis, H ., Herrmann, S., Lehane, B., [4] Naphade, R. M., Kennedy, L., Kender, R. J. , chang, S., 
