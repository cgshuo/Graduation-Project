 Graphical models have been applied to various information retrieval and natural language processing tasks in the recent literature. In this paper, we apply a probabilistic graphical model for answer ranking in question answering. This model estimates the joint probability of correctness of all answer candidates, from which the probability of correctness of an individual candidate can be inferred. The joint prediction model can estimate both the correctness of individual an-swers as well as their correlations, which enables a list of accurate and comprehensive answers. This model was com-pared with a logistic regression model which directly esti-mates the probability of correctness of each individual an-swer candidate. An extensive set of empirical results based on TREC questions demonstrates the effectiveness of the joint model for answer ranking. Furthermore, we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Answer ranking, probabilistic graphical model, question an-swering
Question Answering (QA) aims at finding exact answers to natural language questions in a large collection of docu-ments. Most QA systems combine document retrieval with question analysis and extraction techniques to identify a set of likely candidates, from which the final answer(s) are se-lected [21, 4, 9]. Since question analysis, document retrieval and/or answer extraction may produce erroneous results, the selection process can be very challenging, as it often en-tails identifying relevant answer(s) amongst many irrelevant ones.

For example, given the question  X  X hich city in China has the largest number of foreign financial companies? X  , the answer extraction component produces a ranked list of five answer candidates: Beijing (AP880603-0268) 1 , Hong Kong (WSJ920110-0013), Shanghai (FBIS3-58), Taiwan (FT942-2016) and Shanghai (FBIS3-45320). Due to the imprecision in answer extraction, an incorrect answer ( X  X eijing X ) can be ranked at the first position. On the other hand, the correct answer ( X  X hanghai X ) was extracted from two different doc-uments and ranked at the third and the fifth positions. In order to rank an answer like  X  X hanghai X  in the top position, we have to address two interesting challenges:
Although many QA systems address these issues sepa-rately, there has been little research on generating a prob-abilistic framework that allows any relevance and similarity features to be easily incorporated.

In our previous work [14], we proposed a probabilistic an-swer ranking model to address these two challenges. The model used logistic regression to estimate the probability Answer candidates are shown with the identifier of the TREC document where they were found.
 that an individual answer candidate is correct given rele-vance of the answer and the amount of supporting evidence provided by a set of similar answer candidates. The exper-imental results on TREC factoid questions show that the model significantly improves answer ranking performance for different extraction techniques.

However, this model considered each answer candidate separately, and did not consider correlation of the correct-ness of answer candidates, which is problematic for gener-ating accurate and comprehensive answers. For example, several similar answers may be ranked high in the final an-swer list, but a less redundant answer may not be ranked high enough to reach the user X  X  attention. In this paper, we propose a new probabilistic answer ranking framework that uses an undirected graphical model to estimate the joint probability of the correctness of all answer candidates, from which the probability of the correctness of an individual can-didate can be inferred. The proposed model considers both the correctness of individual answers as well as their cor-relation to generate an accurate and comprehensive answer list.

In comparing the previous logistic regression model (which considers answers independently) to the new graphical model (which jointly considers answers), we will refer to the for-mer as an independent prediction model and the latter as a joint prediction model . Experimental results on the TREC factoid questions [25] show that the joint prediction model significantly improves answer ranking performance over a baseline model, and produces a set of unique an-swers whose precision is higher than those produced by the independent prediction model. Furthermore, we incorporate the independent prediction model into the joint prediction model for improving the efficiency and accuracy of answer ranking.

This paper is organized as follows: Section 2 describes related work. Section 3 presents the independent prediction model and the joint prediction model. Section 4 lists the features used for the models. In Section 5, we describe our experimental methodology and the results. Finally, Section 6 concludes with suggestions for future research.
To identify relevant answers from a list of extracted can-didates, several answer selection approaches have used ex-ternal semantic resources. One of the most common ap-proaches relies on WordNet, CYC and gazetteers for answer validation or answer reranking. In this approach, answer candidates are either removed or discounted if they are not found within the resource X  X  hierarchy corresponding to the expected answer type of the question [26, 17, 3, 6]. The Web also has been used for answer reranking by exploit-ing search engine results produced by queries containing the answer candidate and question keywords [15]. Wikipedia X  X  structured information has been used for answer type check-ing [2].

Although each of these approaches uses one or more se-mantic resources to independently support an answer, few have considered the potential benefits of combining resources together as evidence. There was an attempt to combine geographical databases with WordNet for type checking of location questions [23]. However, the experimental results show that the combination did not improve performance be-cause of the increased semantic ambiguity which accompa-nies broader coverage of location names. This is evidence that the method of combining potential answers may mat-ter as much as the choice of resources.

Collecting evidence from similar answer candidates to boost the confidence of a specific answer candidate is also impor-tant for answer selection. As answer candidates are ex-tracted from different documents, they may contain iden-tical, similar or complementary text snippets. Some previ-ous work [19, 11] has used heuristic methods like manually compiled rules to cluster evidence from similar answer can-didates. Graph-based clustering was also used to consider non-transitiveness in similarity [11].

Similarity detection is more important in list questions which require a set of unique answers. In many systems, cutoff threshold has been used to select the most probably top N answers [8, 13] or exhaustive search to find all possible candidates has been applied [27].

Although previous work has utilized evidence from sim-ilar answer candidates for a specific answer candidate, the algorithms only modeled each answer candidate separately and did not consider both answer relevance and answer cor-relation to prevent the biased influence of incorrect similar answers. As far as we know, no previous work has jointly modeled the correctness of available answer candidates in a formal probabilistic framework, which is very important for generating an accurate and comprehensive answer list.
The independent prediction model estimates the probabil-ity of correctness of each answer candidate. It considers two factors. The first factor tries to identify relevant answers by estimating the probability P ( correct ( A i ) | A i ,Q ), where Q is a question and A i is an answer candidate. The second factor tries to exploit answer similarity by estimating the probability P ( correct ( A i ) | A i ,A j ), where A j is similar to A . By combining these two factors together, the indepen-dent prediction model estimates the probability of an answer as: P ( correct ( A i ) | Q,A 1 ,...,A n ).

Instead of addressing each answer candidate separately, the joint prediction model estimates the joint probability of correctness of available answer candidates. In particular, the joint model estimates the probability of P ( correct ( A 1 correct ( A n ) | Q,A 1 ,...,A n ), where n is the number of answer candidates in consideration. The marginal probability of P ( correct ( A i ) | Q,A 1 ,...,A n ) for each individual answer as well as the conditional probability P ( correct ( A i ) | correct ( A ,Q,A 1 ,...,A n ) can be naturally derived from the joint prob-ability.
The independent prediction model [14] directly estimates the probability of correctness of each individual answer can-didate. It was implemented with logistic regression.
Figure 1 shows how logistic regression predicts the proba-bility that an answer candidate is correct given multiple an-swer relevance features and answer similarity features. K1 and K2 are the number of features for answer relevance and answer similarity scores, respectively. n is the number of answer candidates for a question. Each rel k (A i ) is a feature function used to produce an answer relevance score for an individual answer candidate A i . Each sim k ( A i ,A j ) is a scor-ing function used to calculate an answer similarity between A i and A j . Each sim 0 k ( A i ) represents one similarity feature ( A i ) ,sim 0 1 ( A i ) ,...,sim 0 K 2 ( A i ))
P
P ( A ,A j ) .
 + X for an answer candidate A i and is obtained by summing N-1 answer similarity scores to represent the similarity of one answer candidate to all other candidates.

The parameters  X  , {  X  k } , and {  X  k } are estimated from training data by maximizing the log likelihood. In particu-lar, the Quasi-Newton algorithm [16] is used.

After applying the independent prediction model, answer candidates are reranked according to their estimated prob-ability. For factoid questions, the top answer is selected as a final answer to the question. As logistic regression can be used for a binary classification task with a default threshold of 0.5, we may use the model to identify incorrect answers: if the probability of an answer candidate is lower than 0.5, it may be considered to be a wrong answer and is filtered out of the answer list. This is useful in deciding whether or not a valid answer exists in the corpus [24].
The joint prediction model estimates the joint probability of all answer candidates, from which the probability of an in-dividual candidate is inferred. This estimation is performed using a probabilistic graphical model.

Graphical models have been applied to solve problems in many different domains such as artificial intelligence, com-putational biology, image processing, computer vision, infor-mation retrieval and natural language processing. A graph-ical model is either directed or undirected. Directed graphs can be used to represent causal relationships between vari-ables [20]. Undirected graphs can be used to represent correlations between variables [5]. In this paper, we used an undirected graph for the joint prediction model.
Undirected graphical models are defined as a product of cliques (Equation 1). A clique is a complete subgraph whose nodes are fully connected.
 where  X  c is a positive potential function for a clique in the graph, C is a set of cliques in the graph, and Z is a normal-ization constant.

A Boltzmann machine [10, 12] is a special type of undi-rected graphical model whose node S i has a binary value: either { 0,1 } or { -1,1 } . The joint probability of this graph is represented in Equation 2: where  X  ij =0 if nodes S i and S j are not neighbors in the graph.
 We adapted a Boltzmann machine for answer ranking. Each node S i in the graph represents an answer candidate A i and its binary value represents answer relevance (Equa-tion 3). The weights on the edges represent answer similar-ity between two nodes. If two answers are not similar, the weight between them is 0.
 The joint probability of the model can be represented in Figure 2. Each rel k ( A i ) is a feature function used to produce an answer relevance score for an individual answer candidate and each sim k ( A i ,A N ( i ) ) is a feature function used to calcu-late the similarity between an answer candidate A i and its neighbor answer A N ( i ) . The parameters  X  , {  X  k } , and {  X  are estimated from training data using the Quasi-Netwon algorithm.

As each node has a binary value (either 0 or 1), this model uses the answer relevance scores only when an an-swer candidate is correct (S i =1) and uses the answer sim-ilarity scores only when two answer candidates are correct (S =1 and S N ( i ) =1). If S i =0, the relevance and similarity scores are ignored. If S N ( i ) =0, the answer similarity scores are ignored. This prevents the biased influence of incorrect similar answers.

As the joint prediction model is based on a probabilistic graphical model, it can support probabilistic inference to identify a set of accurate and comprehensive answers. Fig 3 shows the algorithm for selecting answers using this model. Examples will be found in the next section.
 1. Create an empty answer pool. 2. Estimate the joint probability of all answer candidates: P ( S ,...,S n ) X
P ( S i = 1 ,S 1 ,...,S i  X  1 ,S i +1 ,...,S n ) 5. For the remaining answer candidates,
Keeping consistent with the independent prediction model, answer candidates whose marginal probability is lower than 0.5 are removed from the answer list. If only one answer should be provided for any factoid question, the answer whose marginal probability is highest is selected as the final answer to the question.
Both the independent prediction model and the joint pre-diction model provide a general probabilistic framework to estimate the probability of correctness of an individual an-swer candidate from answer relevance and similarity fea-tures. But the independent prediction model directly esti-mates the probability of an individual answer and the joint prediction model estimates the joint probability of all an-swers, from which the probability of correctness of an indi-vidual candidate is inferred.

One advantage of the joint prediction model is that it supports probabilistic inference. For example, the question  X  X ho have been the U.S. presidents since 1993? X  requires a list of person names as the answer. As person names can be represented in several different ways (e.g.,  X  X ill Clinton X ,  X  X illiam J. Clinton X ,  X  X linton, Bill X ), it is important to find unique names as the final answers. This task can be done by using the conditional probability inferred from the joint prediction model. For example, assume that we have three answer candidates for this question:  X  X illiam J. Clinton X ,  X  X ill Clinton X  and  X  X eorge W. Bush X . The probability of correctness of each answer has been calculated by marginal-izing the joint probability of all answer candidates. P(correct(William J. Clinton))= 0.758 P(correct(Bill Clinton)) = 0.755 P(correct(George W. Bush) = 0.617 In this example, the marginal probability P(correct(Bill Clinton)) and P(correct(William J. Clinton)) are high be-cause  X  X ill Clinton X  and  X  X illiam J. Clinton X  are support-ing each other. Based on the marginal probabilities, we first choose the answer candidate A i whose marginal probability is the highest. In this example,  X  X illiam J. Clinton X  is cho-sen and added to the answer pool. Then we calculate the conditional probability of the remaining answer candidates given the first answer.

P(correct(Bill Clinton) | correct(William J. Clinton)) = 0.803
P(correct(George W. Bush) | correct(William J. Clinton)) = 0.617
Even though the marginal probability P(correct(Bill Clin-ton) is higher than P(correct(George W. Bush)),  X  X ill Clin-ton X  is not chosen as the second answer because the condi-tional probability of  X  X ill Clinton X  given  X  X illiam J. Clin-ton X  is high, which indicates that the answer of  X  X ill Clin-ton X  tends to be redundant to the answer of  X  X illiam J. Clinton X .
 On the other hand, P(correct(George W. Bush) | correct( William J. Clinton)) is much smaller than P(correct(Bill Clinton) | correct(William J. Clinton)). Therefore, accord-ing to the algorithm in Figure 3,  X  X eorge W. Bush X  is chosen as the second answer even though its marginal probability is low. In this way we can select the best unique answers from a list of answer candidates.

In terms of efficiency, the independent prediction model has better time performance than the joint prediction model because the joint prediction requires O (2 N ) time complexity for calculating the joint probability, where N is the size of the graph (i.e. number of answer candidates). To address this issue, approximate inference (e.g. Markov chain Monte Carlo sampling or variational inference) can be used.
This section summarizes the features used to generate an-swer relevance scores and answer similarity scores (more de-tails on the features found in [14]).
Multiple external resources were used as answer relevance features. Each answer relevance feature produces a relevance score which predicts whether or not an answer candidate is a relevant answer to the question.
The knowledge-based features involve searching for facts in a knowledge base such as gazetteers and WordNet.
Gazetteers : Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc. To identify relevant answers, three gazetteer resources were used: the Tipster Gazetteer, the CIA World Factbook (https://www.cia.gov/ cia/publications/factbook/index.html) and information about the US states provided by 50states.com. These resources were used to assign an answer relevance score between -1 and 1 to each candidate. For example, given the question  X  X hat continent is Togo on? X , the candidate  X  X frica X  re-ceives a score of 1.0 because gazetteers can answer this ques-tion. The candidates  X  X sia X  receive a score of 0.5 because it is a continent name in gazetteers and matches to the ex-pected answer type of the question. But  X  X hana X  receives a score of -1.0 because it is not a continent in gazetteers. A score of 0 means the gazetteers did not contribute to the answer selection process for that candidate.

For some numeric questions, range checking was used to validate numeric questions [22]. For example, given the question  X  X ow many people live in Italy? X  , if an answer can-didate is within  X  10% of the population stated in the CIA World Factbook, it receives a score of 1.0. If it is in the range of 20%, its score is 0.5. If it significantly differs by more than 20%, it receives a score of -1.0. The threshold may vary based on when the document was written and when the census was taken 2 .

WordNet : WordNet[7] was used to identify answer rel-evance in a manner analogous to gazetteers. For example, given the question  X  X ho wrote the book  X  X ong of Solomon X ? X , the candidate  X  X ark Twain X  receives a score of 0.5 because its hypernyms include writer . For the question  X  X hat is the capital of Uruguay? X , the candidate  X  X ontevideo X  receives a score of 1.0 because WordNet contains this information. As with the gazetteer score, a score of 0 means that Word-Net does not contribute to the answer ranking process for a candidate.
Wikipedia and Google were used in a data-driven ap-proach by calculating tf.idf and word distance.

Wikipedia : To generate an answer relevance score, Wiki-pedia documents as well as Wikipedia X  X  structured informa-tion were used. A query consisting of an answer candidate is sent to Wikipedia. If there is a Wikipedia document whose title matches the answer candidate, the document is ana-lyzed to obtain the term frequency (tf) and the inverse term
The ranges used here were found to work effectively, but were not explicitly validated or tuned. frequency (idf) of the candidate, from which a tf.idf score is calculated. When there is no matched document, each ques-tion keyword is also sent to Wikipedia as a back-off strategy, and the answer relevance score is calculated by summing the tf.idf scores of each keyword. To calculate word frequency, the TREC Web Corpus 3 was used as a large background corpus.

Google : Following Magnini et al. [15], a query consisting of an answer candidate and question keywords was sent to the Google search engine. The top 10 text snippets returned from Google were then analyzed to generate an answer rele-vance score by computing the word distance between a key-word and the answer candidate.
The similarity between two answer candidates was mea-sured with a string distance metric and a list of synonyms.
String Distance Metric : There are several different string distance metrics to calculate the similarity of short strings. We used Levenshtein distance for the experiments reported here.

Synonyms : Synonymity can be used as another metric in calculating answer similarity. If one answer is a synonym of another answer, their similarity score is 1. Otherwise the score is 0. To build a list of synonyms, three knowledge bases were used: WordNet, the CIA World Factbook and Wikipedia.

In addition, manually generated rules are used to obtain synonyms for different types of answer candidates.
This section describes our experiments to compare the performance of the joint prediction model, the independent prediction model and the baseline algorithm. The JAVELIN QA system [19] was used for the evaluation.
A total of 1818 questions from the TREC8-12 QA evalu-ations were used as the testbed, and 5-fold cross validation was used to evaluate the models.

To better understand how the performance of the models varies for different extraction techniques, we tested the an-swer ranking models with three JAVELIN answer extraction components: http://ir.dcs.gla.ac.uk/testcollections/wt10g.html
Answer ranking performance is measured by the average answer accuracy: the number of correct top answers divided by the number of questions where at least one correct an-swer exists in the candidate list provided by an extractor. Three measures are used: TOP1, TOP3, MRR. TOP1 is the average accuracy of the top ranked answers. TOP3 is the average of correct answers ranked in the top 3 positions 4 MRR is the average of mean reciprocal rank of the top 5 answers.

Even though the data set contains only factoid questions, approximately 36% of the questions have more than one correct answer, and the average number of correct answers for those questions is 5. Especially, location, person name, numeric and temporal questions tend to have more than one correct answer. For example, given the question  X  X here is the tallest roller coaster located? X , there are three answers in the TREC corpus: Cedar Point, Sandusky, Ohio. All of them are correct, although they represent geographical areas of increasing generality. Some questions require more than one correct answer. For example, for the question  X  X ho is the tallest man in the world? X , the correct answers are  X  X onjane, Gabriel Estavao, Robert Wadlow, AliNashnush, Barman X . In addition, there are some list questions (e.g.  X  X ame one of the major gods of Hinduism. X ). Therefore, we evaluate the average precision of the top 5 answers in order to see how effectively the joint prediction model can identify unique answers. The average precision is calculated by counting the number of unique correct answers among the top N answers. For example, when the first two answers
If at least one correct answer exists among the top 3 an-swers, the score is 1. otherwise, the score is 0. are  X  X illiam J. Clinton X  and  X  X eorge Bush X , and the third answer is  X  X linton, Bill X , the precision at rank 3 is 2/3.
The baseline was calculated with the answer candidate scores provided by each individual extractor; the answer with the best extractor score was chosen, and no valida-tion or similarity processing was performed. For Wikipedia, we used data downloaded in November 2005 (1,811,554 ar-ticles).
This section compares the performance of the joint pre-diction model with the independent prediction model. Fur-thermore, we incorporate the independent prediction model into the joint prediction model to improve the efficiency of the joint prediction.
As the joint prediction model is based on a graphical model, it requires O(2 N ) time complexity where N is the size of the graph (i.e. number of answer candidates). Therefore, we tested it only with the top 10 answer candidates provided by each individual answer extractor 5 .

Table 1 shows the performance of the joint prediction model, compared with the independent prediction model and the baseline when ranking the top 10 answer candi-dates. As for TOP1, the joint prediction model significantly improved performance over the baseline for all three extrac-tors. This shows the effectiveness of the probabilistic graph-ical model for selecting the most relevant answer. When compared with the independent prediction model, the joint prediction model performed as well as the independent pre-diction model in ranking the relevant answer at the top po-sition.

TOP3 and MRR show the performance of the three algo-rithms when they return multiple answers for each question. It can be seen that the joint prediction model performed bet-ter than the independent prediction model because it could identify unique correct answers by estimating conditional probability.

To further investigate how much the joint prediction model could identify comprehensive results, we analyzed the aver-
We calculated the marginal and conditional probability us-ing exact inference (brute force enumeration).
 individual extractor.
 age precision at top 5 answers. Table 2 shows the average precision of the three models. It can be seen that the joint prediction model produced the answer list whose average precision is higher than the independent prediction model. This is additional evidence that the joint prediction model can produce a better comprehensive answer list.
Efficiency is one issue for the joint prediction model be-cause it requires O(2 N ) time complexity for calculating the joint probability. Table 5 shows the average number of an-swer candidates provided by individual extractors. LIGHT and SVM often return more than 30 answer candidates per question. Even though FST returns a small number of an-swer candidates, it sometimes returns more than 30 answer candidates for a question. This requires more than O(2 30 which is intractable with exact inference.
 Table 5: The average number of answer candidates per question.

To address this issue, approximate inference may be used (e.g., Markov chain Monte Carlo sampling or variational in-ference). Instead of using inexact variational inference or slow sampling methods, we propose an efficient version of the joint prediction model by preselecting the answer can-didates with the independent prediction model.

In this approach, we first apply the independent predic-tion model with all the candidates provided by an answer extractor. Then we choose the top 10 answer candidates returned from the independent prediction model as the in-put to the joint prediction model. Finally, we run the joint prediction model with the top 10 answers.

Table 3 compares the performance of the efficient joint prediction model with the independent prediction model. It shows that the efficient joint prediction model performed as well as the independent prediction model when selecting the top relevant answer. When comparing TOP3 and MRR, the efficient joint prediction model performed better than the in-dependent prediction model because it could identify unique correct answers by estimating the conditional probability.
We also analyzed the average precision of the independent prediction model and the efficient joint prediction model. Table 4 shows that the efficient joint prediction model per-formed much better than the independent prediction model. For example, the efficient joint prediction model improved the average precision at rank 2 by 33% (FST), 43% (LIGHT) and 42% (SVM) over independent prediction. This is quite a significant improvement over the original joint prediction model because the original joint prediction model improved the average precision at rank 2 by just 10% (FST), 6% (LIGHT) and 9% (SVM).

As for the average precision at rank 5, the extended joint prediction model improved performance by 53% (FST), 83% (LIGHT) and 82% (SVM) over the independent prediction model. When comparing performance gain in the three dif-ferent extraction techniques, we had less improvement for FST because it produces an average of 4.19 answer candi-dates (as shown in Table 5).

This additional analysis on average precision shows clearly that the revised version of the joint prediction model can generate more comprehensive results in an efficient way.
In this paper, we proposed a new probabilistic answer ranking model based on a probabilistic graphical model and evaluated its performance by comparing it with an existing independent prediction model.

Even though the independent prediction and joint predic-tion models both provide a general probabilistic framework for estimating the probability of an individual answer can-didate from answer relevance and similarity features, they differ in how they estimate the probability. The indepen-dent prediction model directly estimates the probability of correctness of an individual answer candidate. On the other hand, the joint prediction model uses an undirected graph to estimate the joint probability of correctness of available answer candidates. From the joint probability, we can infer the marginal probability of the correctness of an individ-ual candidate and the conditional probability of correctness for different answers. This enables better answer ranking results for a more accurate and comprehensive answer list.
An extensive set of empirical results on TREC questions shows that the joint prediction model significantly improved answer ranking performance and is better at finding a unique set of correct answers (e.g. for a list-type question). Fur-thermore, we also extended the joint prediction model to improve its algorithmic efficiency by utilizing the outputs produced by the independent prediction model.

As far as we know, this is the first research work that pro-poses a formal probabilistic framework to jointly model the correctness and correlation of answer candidates in question answering. We plan to evaluate this new answer ranking model with other types of questions such as list and defini-tion questions in the future. As definition questions require long text answers, different features should be used for an-swer ranking. Possible relevance features include question keyword inclusion and predicate structure match [18]. For answer similarity, we intend to explore other novelty mea-surements (e.g., in Allan et al. [1]). We also plan to apply approximate inference algorithms like variational inference to implement the joint prediction model.
This work was supported in part by ARDA/DTO Ad-vanced Question Answering for Intelligence (AQUAINT) pro-gram award number NBCHC040164. [1] J. Allan, C. Wade, and A. Bolivar. Retrieval and [2] D. Buscaldi and P. Rosso. Mining Knowledge from [3] J. Chu-Carroll, K. Czuba, J. Prager, and [4] C. Clarke, G. Cormack, and T. Lynam. Exploiting [5] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. [6] A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, [7] C. Fellbaum. WordNet: An Electronic Lexical [8] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, [9] S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, [10] G. Hinton and T. Sejnowski. Learning and relearning [11] V. Jijkoun, J. van Rantwijk, D. Ahn, E. T. K. Sang, [12] M. Jordan, Z. Ghahramani, T. S. Jaakkola, and [13] B. Katz, J. J. Lin, D. Loreto, W. Hildebrandt, [14] J. Ko, L. Si, and E. Nyberg. A Probabilistic [15] B. Magnini, M. Negri, R. Pervete, and H. Tanev. [16] T. Minka. A Comparison of Numerical Optimizers for [17] D. Moldovan, D. Clark, S. Harabagiu, and [18] E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti, [19] E. Nyberg, T. Mitamura, R. Frederking, V. Pedro, [20] J. Pearl. Causality: Models, Reasoning, and Inference . [21] J. Prager, E. Brown, A. Coden, and D. Radev. [22] J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, [23] S. Schlobach, M. Olsthoorn, and M. de Rijke. Type [24] E. Voorhees. Overview of the TREC 2002 question [25] E. Voorhees. Overview of the TREC 2003 question [26] J. Xu, A. Licuanan, J. May, S. Miller, and [27] H. Yang, H. Cui, M. Maslennikov, L. Qiu, M.-Y. Kan,
