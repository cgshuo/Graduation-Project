 As a large amount of time series data have been collected in many domains such as finance and bioinformatics, time series data mining has drawn a lot of attention in the literature. Particularly, multivariate time series classification is becoming very important in a broad range of real-world applications, such as health care and activity recognition [1 X 3].

In recent years, a plenty of classificatio n algorithms for time series data have been developed. Among these classification methods, the distance-based method k -Nearest Neighbor ( k -NN) classification has been empirically proven to be very difficult to beat [4, 5]. Also, more and more evidences have shown that the Dynamic Time Warping (DTW) is the best sequence distance measurement in most domains [4 X 7]. Thus, the simple combination of k -NNandDTWcould reach the best performance of classification in most domains [6]. Other than sequence distance based methods, feature -based classification methods [8] follow the traditional classification framework. As is known to all, the performance of traditional feature-based methods depends on the quality of hand-crafted features. However, unlike other applications, it is difficult to design good features to capture intrinsic properties embedded in various time series data. Therefore, the accuracy of feature-based methods is usually worse than that of sequence distance based ones, particularly 1 -NN with DTW method. On the other hand, although many research works use 1 -NN and DTW, both of them cause too much computation for many real-world applications [7].
 Motivation . Is it possible to improve the accuracy of feature-based methods? So that the feature-based methods are not only superior to 1-NN with DTW in efficiency but also competitive to it in accuracy .

Inspired by the deep feature learning for image classification [9 X 11], in this paper, we explore a deep learning framework for multivariate time series classifi-cation. Deep learning does not need any ha nd-crafted features by people, instead it can learn a hierarchical feature repres entation from raw data automatically. Specifically, we propose an effective Multi-Channels Deep Convolution Neural Networks (MC-DCNN) model, each channel of which takes a single dimension of multivariate time series as input and learns features individually. Then the MC-DCNN model combines the learnt features of each channel and feeds them into a Multilayer Perceptron (MLP) to perform classification finally. To estimate the parameters, we utilize the gradient-based method to train our MC-DCNN model. We evaluate the performance of our MC-DCNN model on two real-world data sets. The experimental results on both data sets show that our MC-DCNN model outperforms the baseline methods with significant margins and has a good generalization, especially for weakly labeled data.

The rest of the paper is organized as follows. Section 2 depicts the definitions and notations used in the paper. In sect ion 3, we present the architecture of MC-DCNN, and describe how to train the neural networks. In section 4, we conduct experiments on two real-world data sets and evaluate the performance of each model. We make a short review of related work in section 5. Finally, we conclude the paper and discuss future work in section 6. In this section, we introduce the definitions and notations used in the paper. Definition 1 Univariate time series is a sequence of data points, measured typ-ically at successive points in time spaced at uniform time intervals. A univariate time series can be denoted as T = { t 1 ,t 2 , ..., t n } ,and n is the length of T . Definition 2 Multivariate time series is a set of time series with the same timestamps. For a multivariate time series M ,eachelement m i is a univariate of univariate time series in M .
As previous works shown [12], it X  X  common to extract subsequences from long time series to do classification instead of classifying with the whole sequence. Definition 3 Subsequence is a sequence of consecutive points which are ex-k is the length of subsequence. Similarly, multivariate subsequence can be denoted as Y = { m  X  i , m  X  i +1 , ..., m  X  i + k  X  1 } ,where m  X  i is defined in Definition 2.
Since we perform classification on multivariate subsequences in our work, in remainder of the paper, we use subsequence standing for both univariate and multivariate subsequence for short according to the context. For a long-term time series, domain experts may manually label and align subsequences based on experience. We define this type of data as well aligned and labeled data . Definition 4 Well aligned and labeled data: Subsequences are labeled by domain experts, and different subsequences belonging to same pattern are well aligned. Fig.1 shows a snippet of time series extracted from BIDMC Congestive Heart Failure data set [13]. Each subsequence i s extracted and lab eled according to the red dotted line by medical staffs. However, to acquire the well aligned and labeled data, it always needs great manual cost.

In contrast to well aligned and labeled data, in practice, weakly labeled data can be obtained more easily [12, 1]. We define it as follows.
 Definition 5 Weakly labeled data: A long-term time series is associated with a single global label as shown in Fig.2.

Due to the alignment-free property of weakly labeled data, it requires to extract subsequences by specific algorithm. The most widely used algorithm is sliding window [14]. By specifying sliding step , we can extract large amount of redundant subsequences from long-term time series.

In summary, in this paper, we will prim arily concentrate on the time series of the same length and conduct experiments on both labeled data that is well aligned and weakly labeled data. In this section, we will introduce a deep learning framework for multivariate time series classification: Multi-Channels Deep Convolutional Neural Networks (MC-DCNN). Traditional Convolutional Neural Networks (CNN) usually include two parts. One is a feature extractor, which learns features from raw data auto-matically. And the other is a trainable fully-connected MLP, which performs classification based on the learned features from the previous part. Generally, the feature extractor is composed of multiple similar stages, and each stage is made up of three cascading layers: filter layer, activation layer and pooling layer. The input and output of each layer are called feature maps [11]. In the previous work of CNN [11], the feature extractor usually contains one, two or three such 3-layers stages. Due to space constraint, we only introduce the components of CNN briefly. More details of CNN can be referred to [11, 15]. 3.1 Architecture In contrast to image classification, the input of multivariate time series classi-fication are multiple 1D subsequences but not 2D image pixels. We modify the traditional CNN and apply it to multivariate time series classification task in this way: we separate multivariate time series into univariate ones and perform feature learning on each univariate series individually. Then we concatenate a normal MLP at the end of feature learning to do classification. To be under-stood easily, we illustrate the architecture of MC-DCNN in Fig. 3. Specifically, this is an example of 2-stages MC-DCNN for activity classification. It includes 3-channels inputs and the length of each input is 256. For each channel, the input (i.e., the univariate time series) is fed into a 2-stages feature extractor, which learns hierarchical features through filter, activation and pooling layers. At the end of feature extractor, we flatten the feature maps of each channel and combine them as the input of subsequent MLP for classification. Note that in Fig. 3, the activation layer is embedded into filter layer in the form of non-linear operation on each feature map. Next , we describe how each layer works. Filter Layer. The input of each filter is a univariate time series, which is denoted x l i  X  n l 2 ,1  X  i  X  n l 1 ,where l denotes the layer which the time series comes from, n l 1 and n l 2 are number and length of input time series. To capture local temporal information, it requires to restrict each trainable filter k ij with a small size, which is denoted m l 2 , and the number of filter at layer l is denoted m 1 . Recalling the example described in Fig. 3, in first stage of channel 1, we have n l 1 =1, n l 2 = 256, m l 2 =5and m l 1 = 8. We compute the output of each filter according to this: i x l  X  1 i  X  k l ij + b l j ,wherethe  X  is convolution operator and b l j is the bias term.
 Activation Layer. The activation function introduces the non-linearity into neural networks and allows it to learn more complex model. The most widely used activation functions are sigmoid ( t )= 1 1+ e  X  t and tanh (  X  ). In this paper, we adopt sigmoid (  X  ) function in all activation layers due to its simplicity. Pooling Layer. Pooling is also called subsampling because it usually subsam-ples the input feature maps by a specific factor. The purpose of pooling layer is to reduce the resolution of input time series, and make it robust to small variations for previous learned features. The simplest yet most popular method is to compute average value in each neighborhood at different positions with or without overlapping. The neighborhood is usually constructed by splitting input feature maps into equal length (larger than 1) subsequences. We utilize average pooling without overlapping for all stages in our work. 3.2 Gradient-Based Learning of MC-DCNN The same as traditional MLP, for multi-class classification task, the loss function of our MC-DCNN model is defined as: E =  X  t k y  X  k ( t )log( y k ( t )), where y ( t )and y k ( t ) are the target and predicted values of t -th training example at k -th class, respectively. To estimate parameters of models, we utilize gradient-based optimization method to minimize the loss function. Specifically, we use simple backpropagation algorithm to train our MC-DCNN model, since it is efficient and most widely used in neural networks [16]. We adopt stochastic gradient descent (SGD) instead of full-batch version to update the parameters. Because SGD could converge faster than full-batch for large scale data sets [16].
A full cycle of parameter updating procedure includes three cascaded phases [17]: feedforward pass, backpropagation pass and the gradient applied. Feedforward Pass. The objective of feedforward pass is to determine the pre-dicted output of MC-DCNN on input vectors. Specifically, it computes feature maps from layer to layer and stage to stage until obtaining the output. As shown in the previous content, each stage contains three cascaded layers, and activation layer is embedded into filter layer in form of non-linear operation on each feature map. We compute output feature map of each layer as follows: where down (  X  ) represents the subsampling function for average pooling, x l  X  1 i and z j denote the input and output of filter layer, z output of activation layer, x l j and x l +1 j denote the input and output of pooling layer.

Eventually, a 2-layer fully-connected MLP is concatenated to feature extrac-tor. Since feedforward pass of MLP is standard and also the space is limited, more details of MLP can be referred to [16, 17].
 Backpropagation Pass. Once acquiring predicted output y , the predicted error E can be calculated according to the loss function. By taking advantage of chain-rule of derivative, the predicted error propagates back on each parameter of each layer one by one, which can be used to work out the derivatives of them. We still don X  X  present backpropagation pass of final MLP for the same reason of feedforward pass.

For pooling layer in the second stage of feature extractor, the derivative of x j is computed by the upsampling function up ( opposite to the subsampling function down (  X  ) for the backward propagation of errors in this layer. For filter layer in second stage of feature extractor, derivative of z l j is computed similar to that of MLP X  X  hidden layer: where  X  denotes element-wise product. Since the bias is a scalar, to compute its derivative, we should summate over all entries in  X  l j as follows:
The difference between kernel weight k l ij and MLP X  X  weight w l ij is the weight sharing constraint, which means the weights between ( k l ij ) u and each entry of x l j mustbethesame.Duetothisconstraint, the number of parameters is reduced by comparing with the fully-connected MLP, Therefore, to compute the derivative of kernel weight k l ij , it needs to summate over all quantities related to this kernel. We perform this with convolution operation: where reverse (  X  ) is the function of reversing corresponding feature map. Finally, we compute the derivative of x l  X  1 i as follows: where pad (  X  ) is a function which pads zeros into  X  l j from two ends, e.g., if the Gradients Applied. Once we obtain the derivatives of parameters, it X  X  time to apply them to update parameters. To converge fast, we utilize decay and momentum strategies [16]. The weight w l ij in MLP is updated in this way: filter layer and b l in MLP are updated similar to the way of w l ij . The same as [18], we set momentum =0 . 9, decay =0 . 0005 and =0 . 01 for our experiments. It is noted that [19] claimed that both the initialization and the momentum are crucial for deep neural networks, hence, we consider how to select these values as a part of our future work. In this section, we will conduct two groups of experiments on real-world data sets from two different application domains. Particularly, we will show the per-formance of our methods via comparing with other baseline models in terms of both efficiency and accuracy.

To the best of our knowledge, indeed, there are many public time series data sets available, e.g., the UCR Suite [20]. However, we decide not using the UCR Suite for the following reasons. First, we focus on the classification of multivariate time series, whereas most data sets in UCR Suite only contain univariate time series. Second, data sets in UCR Suite are usually small and CNN may not work well on such small data sets [21]. Thus, we choose two data sets which are collected from real-world applications, and we will introduce the data sets in the next subsections.

We consider three approaches as baseline methods for evaluation: 1 -NN (ED), 1 -NN (DTW-5%) and MLP. Here, 1 -NN (ED) and 1 -NN (DTW-5%) are the methods that combine Euclidean Distance and Window Constraint DTW [7]) 1 with 1 -NN, respectively. Besides these tw o state-of-the-art methods, MLP is chosen to demonstrate that the feature learning process can improve the clas-sification accuracy effectively. For the purpose of comparison, we record the performance of each method by tuning their parameters. Notice that some other classifiers are not considered here, since it is difficult to construct hand-crafted features for time series and many previous works have claimed that feature-based methods cannot achieve the accuracy as high as 1 -NN methods. Also, we do not choose the full DTW due to its expensive time consumption. Actually, at least more than a month will be cost if we use full DTW in our experiments. 4.1 Activity Classification (Weakly Labeled Data) Data Set. We use the weakly labeled PAMAP2 data set for activity classifica-tion . It records 19 physical activitie s performed by 9 subjects. On a machine with Intel I5-2410 (2.3GHz) CPU and 8G Memory (our experimental platform), according to the estimation, it will cost nearly a month for 1 -NN (DTW-5%) on this data set if we use all the 19 physical activities. Hence, currently, we only consider 4 out of these 19 physical a ctivities in our work, which are  X  standing  X ,  X  walking  X ,  X  ascending stairs  X  X nd X  descending stairs  X . And each physical activity corresponds to a 3D time series. Moreover, 7 out of these 9 subjects are chosen. Because the other two either have different physical activities or have different dominant hand/foot. Experiment Setup. We normalize each dimension of 3D time series as x  X   X   X  , where  X  and  X  are mean and standard deviation of time series. Then we apply the sliding window algorithm to extract subsequences from 3D time series with different sliding steps . To evaluate the performance of different models, we adopt the leave-one-out cross validation (LOOCV) technique. Specifically, each time we use one subject X  X  physical activities as test data, and the physical activities of remaining subjects as training data. Then we repeat this for every subject. To glance the impact of depths, we evaluate two models: MC-DCNN(1), MC-DCNN(2). They are 1-stage and 2-stages feature learning models, respectively. Experimental Results To evaluate efficiency and scalability of each model, we get five data splits with different volumes by setting sliding step as 128, 64, 32, 16, 8, respectively. In addition, to ensure each subsequence to cover at least one pattern of time series, we set the sliding window length as 256.
As is well known, feature-based models have an advantage over lazy classifica-tion models (e.g., k -NN) in efficiency. As shown in Fig. 4, the prediction time of 1 -NN model increases linearly as the size of training data set grows. In contrast, the prediction time of our MC-DCNN model is almost constant no matter how large the training data is.

We also evaluate accuracy of each model on these five data splits. Fig. 6 shows the detailed accuracy comparisons of each subject at different step settings. From this figure we can see that for each subject our MC-DCNN model is either the most accuracy one or very close to the mos t accuracy one. Especially, for subject 3, the 2-stages MC-DCNN leads to much better accuracy than other approaches. We suppose that 2-stages MC-DCNN may learn high-level and robust feature representations so that it has a good generalization. We also show the average and standard deviation of accuracy in Table 1. From the table we can see that our model leads to the highest average accuracy and the lowest standard deviation. 4.2 Congestive Heart Failure Detection (Well Aligned Data) Data Set Well aligned BIDMC data set was downloaded from Congestive Heart Failure database 2 [13]. Long-term electrocardi ograph (ECG) data was recorded from 15 subjects, each of them suffers seve re Congestive Heart Failure. Different from PAMAP2 data, in BIDMC data set, each type of heart failure corresponds to a 2D time series. In this experiment, we consider four types of heartbeats to evaluate all the models:  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X .
 Experiment Setup. We still normalize each univariate of 2D time series as mentioned before. Different from weakly data, we extract subsequences centered at aligned marks (red dotted line in Fig. 1). And each subsequence still has a length of 256. Similar to the classification of individuals X  heartbeats [12], we mix all data of 15 subjects and randomly split it into 10 folds to perform 10-folds cross validation. Because as [12] noted, it can be able to obtain huge amounts of labeled data in this way and a unhealthy individual may have many different types of heartbeats. To glance the impact of depths, we also evaluate two models: MC-DCNN(1), MC-DCNN(2). The former performs 1-stage feature learning, and the latter performs 2-stages. To determine the epochs, we separate one third of training data as validation set. As shown in Fig. 7, we set epoch to 40 and 80 for 1-stage and 2-stages MC-DCNN models respectively. Since the test error is stable when epochs are greater than them.
 Experimental Results. We illustrate the accuracy of each model on BIDMC data set in Fig. 5. From this figure, we can see that accuracies of 1-stage MC-DCNN and 2-stages MC-DCNN models are 94.67% and 94.65%, which are also higher than the accuracies of 1 -NN(ED) (93.64%), 1 -NN(DTW-5%) (92.90%) and MLP (94.22%). Due to the space limit we do not report the prediction time of each model on BIDMC data set. However, the result is similar to Fig. 4 and it also supports that feature-based models have an advantage over lazy classification models (e.g., k -NN) in efficiency. Many time series classification methods have been proposed based on different sequence distance measurements. Among these previous works, some researchers claimed that 1 -NN combined DTW is the current state of the art [6, 7]. How-ever, the biggest weakness of 1 -NN with DTW model is its expensive computa-tion [7]. To overcome this drawback, a pa rt of researchers explored to speed up the computation of distance measure (e.g., DTW) in certain methods (e.g., with boundary conditions) [7]. While another part of researchers tried to reduce the computation of 1 -NN by constructing data dictionary [12, 7, 14, 22]. When the data set grows large, all these approaches improve the performance significantly in contrast to simple 1 -NN with DTW. Some feature-based models have been explored for time series classification [2, 23], however, most of previous works extracted the hand-crafted statistical features based on domain knowledge, and achieved the performance not as well a s sequence distance based models.
Feature learning (or representation learning) is becoming an important field in machine learning community in recent y ears [9]. The most successful feature learning framework is deep neural networks, which build hierarchical represen-tations from raw data [10, 11, 24]. Particularly, as a supervised feature learning model, deep convolutional neural netwo rks achieve remarkable successes in many tasks such as digit and object recognition [18], which motivates us to investigate the deep learning in time series field. I n the literature, there are few works on time series classification using deep learning. Ref.[25] explored an unsupervised feature learning method with convolutional deep belief networks for audio clas-sification, but in frequency domain rather than in time domain. Ref.[3] adopted a special time delay neural network (TDNN) model for electroencephalography (EEG) classification. However, their TDNN model only included a single hid-den layer, which is not deep enough to learn good hierarchical features. To the best of our knowledge, none of existing works on time series classification has considered the supervised feature lear ning from raw data. In this paper, we ex-plore a MC-DCNN model for multivariate time series classification and intend to investigate this problem in another way. Time series classification is becoming very important in a broad range of real-world applications, such as health care and activity recognition. However, most existing methods have high computational complexity or low prediction accu-racy. To this end, we developed a novel deep learning framework (MC-DCNN) to classify multivariate time series in the paper. This model learns features from individual univariate time series in each channel automatically, and combines in-formation from all channels as feature representation at final layer. A traditional MLP is concatenated to perform classification. We evaluated our MC-DCNN model on two real-world data sets. Experimental results show that our MC-DCNN model outperforms the competing baseline methods on both data sets, especially, the improvement of accuracy on weakly labeled data set is significant. Also, we showed that 2-stages MC-DCNN is superior to 1-stage MC-DCNN. It provides the evidence that the deeper architecture can learn more robust high-level features, which is helpful for improving performance of classification.
There are several research directions for future work. First, in this paper we simply use the 1-stage and 2-stages feature learning for better illustration, and in the future we plan to study and extend other deep learning models for multivariate time series classification on more data sets. Second, we also intend to perform unsupervised algorithms on unlabeled data to pre-train the networks. Acknowledgement. This research was partially supported by grants from the National Science Foundation for Distinguished Young Scholars of China (Grant No. 61325010), the National High Technology Research and Development Pro-gram of China (Grant No.2014AA015203), the Anhui Provincial Natural Science Foundation (Grant No. 1408085QF110), the Sc ience and Technology Develop-ment of Anhui Province, China (Grants No. 13Z02008-5 and 1301022064), and the International Science &amp; Technology Cooperation Plan of Anhui Province (Grant No. 1303063008).

