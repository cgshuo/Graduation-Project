 Most evaluations of novel algorithmic contributions assess their accuracy in predicting what was withheld in an of-fline evaluation scenario. However, several doubts have been raised that standard offline evaluation practices are not ap-propriate to select the best algorithm for field deployment. The goal of this work is therefore to compare the offline and the online evaluation methodology with the same study participants, i.e. a within users experimental design. This paper presents empirical evidence that the ranking of algo-rithms based on offline accuracy measurements clearly con-tradicts the results from the online study with the same set of users. Thus the external validity of the most commonly applied evaluation methodology is not guaranteed.
 User study, Evaluation methodology, Experimental within users design
From a methodological viewpoint research contributions in the field of recommender systems focus mainly on novel methods, that mainly come with the promise of more ac-curately identifying relevant content in a variety of applica-tion domains. For instance, the survey of Jannach et al. [7] gives evidence that validation of recommender systems re-search focuses mainly on offline evaluation scenarios. Thus, according to the paradigm of Machine Learning a predic-tive model is trained on a subset of the available data and it is optimized to correctly predict the withheld portions of the dataset. Due to the maturing of the field several works have been focusing on the methodological aspects of this evaluation approach in recommender systems research. For instance, Said &amp; Bellog  X  X n. [9] identified several incon-sistencies when recommendation methods and offline eval-uation practices are compared across different framework platforms. While this recent work focuses on the internal validity of offline evaluation practices other authors focus on the external validity. Research questions targeting the external validity are, for instance, if and how confirmed re-sults from an offline evaluation can be generalized to a rec-ommender system X  X  encounter with real users. Therefore, in addition to offline evaluation on dead data many authors strongly advocate user-centric evaluation approaches [5, 11, 8]. Up to now several authors have reported results from comparing recommendation methods in an offline and an online evaluation setting [1, 4, 3] and did find evidence that algorithm performance in offline and online evaluation set-tings differs. However, none of the research works so far has been able to demonstrate in a within users experimental design statistically significant differences in algorithm per-formance between both evaluation settings. This paper X  X  contribution therefore lies in presenting an innovative evalu-ation design that researches algorithms X  precision and their ability to present novel and relevant items in offline and on-line evaluation settings.
The goal of this research is to assess the external valid-ity of a comparative evaluation of different recommendation methods on offline data. We would therefore like to answer the following research questions: 1. Does the relative ranking of algorithms based on of-2. Does the relative ranking of algorithms based on of-3. Do offline accuracy measurements allow to predict the For the purpose of answering these research questions we are employing a novel study design that compares offline and online measurements in a within users experimental design that is depicted in Figure 1. We are exploiting the Movie-Lens dataset in order to train different algorithms that will then be assessed according to the standard offline and online evaluation methodology by our study participants. In order to check for differences between offline and online method-ology we selected four rather distinctive algorithms for this experimental setup: 1. Most-popular as a baseline algorithm, that is agnostic 2. A matrix factorization model [12] with a 80 factor 3. The same matrix factorization model, but with 400 4. Item-based nearest neighbour approach [10] as a simple
Although there have been many different algorithms pro-posed recently the algorithm does not constitute a limita-tion of the findings in this study. This is because we focus on the different results of the same algorithms in an online and an offline setting and not on finding the most accurate algorithm according to a specific setting. The dataset used to train the algorithms is the well-known MovieLens 1M (ML1M) dataset. This dataset contains 1,000,209 ratings from 6,040 users and 3,706 different movies. Ratings are integer values from an ordinal scale ranging from 1 to 5, where 5 is the best feedback. However, we wanted to ensure that efforts for our participants remain at acceptable levels and therefore decided to ask users only for unary preference statements. Therefore the ML data was transformed into unary user feedback. For each individual ML user record Figure 2: Screenshot of recommendations evaluation page. only 4 and 5 ratings that are greater than the user X  X  average rating score were transformed into an unary like statement. Furthermore we considered only movies with an entry in the Freebase data repository in order to ensure the same data quality of movie descriptions when asking users for ratings or presenting recommendations. After this preprocessing the dataset consisted of 6,038 users and 3,086 movies. This data was then used to train the models for the four algorithms considered.

In the first phase we invited participants via our universi-ties X  mailing lists to browse our specifically developed movie portal and mark those items that they like. We designed and implemented a Django web application that offers users the possibility to participate at this user study from any lo-cation. In the first round users could access a portal that contained detailed information about all movies contained in the MovieLens dataset. The collected dataset contains pos-itive feedback on items the user probably knows and likes and no feedback on items the user either dislikes or does not know. The algorithmic models trained on MovieLens could therefore be evaluated how accurate they were in predicting the liked items of our study participants in an offline eval-uation scenario. The generation of the recommendation list for a user was performed using an all-but-one validation on the user data. In total 241 users have participated in the first phase that provided on average 137 unary ratings each.
In the second round we invited first-round users to eval-uate around twenty recommendations produced from these four different algorithms. Each algorithm computed 5 rec-ommendations and we sorted them in a stratified way, such that the recommendations derived from each algorithm were equally distributed along the ranks. Since two or more algo-rithms can recommend the same item, the number of recom-mendations for each user went from a theoretical minimum of 5 to a maximum of 20 items, while the actual average was 17.37. Therefore users were asked to assess sequentially every recommended item and to answer a set of questions. For each recommended item a separate screen with informa-tion about the movie such as the title, the year of launch, the plot, the poster image extracted from Freebase and a snapshot of the related Wikipedia page was displayed (see Figure 2). The questionnaire items were the following:
With the first question we measure second order novelty [1], i.e. an item is considered novel if the user has never heard of it. We determine relevance of recommended items by asking about potential conversion, i.e. if the user consid-ers to watch this movie. By combining answers from both questions we are able to identify useful recommendations, i.e. relevant items that the user did not yet know about.
In total 122 users returned to the second phase, out of which 100 provided their individual assessment to all rec-ommended items. We selected these 100 users for our study and discarded data from all other users.
One basic evaluation approach in recommender systems is borrowed from the field of Information Retrieval (IR). Ground truth is represented by a user  X  items matrix with binary rating values, where 1s indicate that the user liked the particular item and 0s indicate dislike or no rating. As users tend to give only positive ratings and ignore disliked items, the default assumption in an offline evaluation scenario is to treat unrated items as they were disliked. The standard IR measures are Precision and Recall where precision measures the share of True Positives in the list of recommended items while recall constitutes the share of True Positives among all items relevant for a specific user in ground truth. However, the semantics of these two basic IR measures are somewhat different in offline and online evaluation scenarios [6]: Based on these assumptions only Precision can be compared in our within users experiments in order to contrast online and offline evaluation results. Furthermore, we hypothesize that Precision in an offline evaluation scenario should be lower than Precision measured online due to mentioned un-derestimation of True Positives and the overestimation of False Positives in the offline setting. Furthermore, we also compute Precision for long tail items following the proposi-tion of Cremonesi et al. [2], where the most popular items in the dataset are treated as negatives. The most popular items that could aggregate one third of all positive ratings in the dataset are considered as belonging to the short head. Table 1: Accuracy statistics for all items, long tail items and useful recommendations All Items Offline 0.438 0.504 0.454 0.34 All Items Online 0.546 0.598 0.604 0.516 Long Tail Offline 0.28 0.018 0.36 0 Long Tail Online 0.356 0.054 0.528 0 Figure 3: Precision offline Figure 5: Domination graph for Precision on long tail items (offline and online)
Research question 1 is about the relative ranking of algo-rithms in offline and online accuracy assessments. In Table 1 the Precision measurements for all four algorithms and both evaluation settings are given. Furthermore, Figure 3 gives the domination graph based on pairwise comparison of a Wilcoxon signed rank test. An outgoing arc means that the method outperforms the method it is pointing to with a p-value below the significance level denoted on the arc. Both matrix factorization techniques perform better than the popularity baseline and the 80 factor model also clearly outperforms the I2I nearest neighbor method. However the online measurements clearly contradict this picture. As de-picted in Figure 4 the 400 factor model outperforms the I2I at a 10% error rate, but the 80 factor model does not. Therefore this study gives empirical evidence that statisti-cally significant differences measured in an offline evaluation scenario could not be confirmed in an online study.
Due to the strong bias of traditional accuracy measure-ments on popular items, Cremonesi et al. [2] proposed to ex-clude the most popular items from accuracy measurements (i.e. count correct predictions of very popular items as False Positives). Research Question 2 therefore asks if Precision on long tail items in offline measurements more reliably pre-dicts the results that can be achieved in the online setting. Table 1 gives the actual figures and Figure 5 presents the domination graph. The Precision measurement of long tail items seems to better discriminate between methods and leads to exactly the same statistically significant ranking of algorithms in both evaluation settings. Obviously it clearly contradicts the offline Precision measurement, the 80 factor Figure 6: Domination graph based on Precision for items users liked and did not know (online) model is by far worse in proposing non popular items than the 400 factor model or the I2I neighborhood model.
Finally the third research question explicitly explores the utility of recommendations. We simply define useful rec-ommendations as propositions the user likes but did not yet know. We question if accuracy measurements on offline data provide an indication if an algorithm will be able to make more useful recommendations to users. In Table 1 Preci-sion figures for useful recommendations are given, that are obviously much smaller than Precision on all and long tail items. When again testing for statistically significant differ-ences the ranking of algorithms looks completely different from where we started in Figure 3. I2I collaborative filter-ing and the 400 factor model seem to be on par and both clearly outperform the two other methods. Neither tradi-tional Precision measurements nor the proposed Precision of long tail items measure by [2] were able to predict this algorithm ranking by offline experiments. Although I2I CF seemed to recommend more popular items than the 400 fac-tor model, the neighborhood method did still very good in identifying items that were novel to our study participants. Consequently, the presented empirical results clearly chal-lenge the assumption of external validity of common evalua-tion practices of recommender systems and should therefore stimulate further methodological work into this direction.
The vast majority of the literature on recommender sys-tems apply offline evaluation methodologies to assess the quality of the proposed approaches [7]. However, the ex-ternal validity of such methodologies has been frequently questioned and recent works tried to compare offline and online evaluations. In this work we described a novel com-parison approach between an offline evaluation protocol and an online user study. A dataset with like statements by 100 users on Movielens movies has been collected and accu-racy statistics for models trained on the Movielens dataset have been evaluated on the collected dataset. These statis-tics have been compared with online statistics collected with a user study, where the same 100 users answered a set of questions on around 20 recommendations proposed to them. This work showed that offline precision underestimates on-line precision when both are considering all items and only long tail items. Furthermore, offline precision measurement does not provide the same ranking of algorithms as online precision does. Finally, the real utility for users of previously unknown but relevant recommended items determined on an online user study ranks algorithm in such a way that is not reproducible with offline evaluation metrics, neither on all items nor only on long tail items. Such results are a fur-ther clue that the external validity of commonly used offline evaluation protocols is not always guaranteed. [1] Paolo Cremonesi, Franca Garzotto, and Roberto [2] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. [3] Michael D. Ekstrand, F. Maxwell Harper, Martijn C. [4] Florent Garcin, Boi Faltings, Olivier Donatsch, Ayar [5] Jonathan L. Herlocker, Joseph A. Konstan, Loren G. [6] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich. [7] D. Jannach, M. Zanker, M. Ge, and M. Groening. [8] Pearl Pu, Li Chen, and Rong Hu. A user-centric [9] Alan Said and Alejandro Bellog  X  X n. Comparative [10] Badrul Sarwar, George Karypis, Joseph Konstan, and [11] Guy Shani and Asela Gunawardana. Evaluating [12] Vikas Sindhwani, Serhat S. Bucak, Jianying Hu, and
