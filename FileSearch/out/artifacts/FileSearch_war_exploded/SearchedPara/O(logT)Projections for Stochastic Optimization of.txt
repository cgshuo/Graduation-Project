 Lijun Zhang  X  zhanglij@msu.edu Tianbao Yang  X  tyang@ge.com Rong Jin  X  rongjin@cse.msu.edu Xiaofei He  X  xiaofeihe@cad.zju.edu.cn  X  GE Global Research, San Ramon, CA 94583, USA The goal of stochastic optimization is to solve the op-timization problem using only the stochastic gradients of F ( w ). In partic-ular, we assume there exists a gradient oracle, which for any point w  X  D , returns a random vector  X  g ( w ) that gives an unbiased estimate of the subgradient of F ( ) at w . A special case of stochastic optimization is the risk minimization problem, whose objective func-tion is given by where ( x , y ) is an instance-label pair,  X  is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint dis-2009 ; Hu et al. , 2009 ). The performance of stochastic optimization algorithms is typically characterized by the excess risk where T is the number of iterations and w T is the solution obtained after making T calls to the gradient oracle.
 For general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O (1 / 1983 ; Nemirovski et al. , 2009 ). For strongly convex functions, the algorithms proposed in very recent works ( Juditsky &amp; Nesterov , 2010 ; Hazan &amp; Kale , 2011 ; Rakhlin et al. , 2012 ; Chen et al. , 2012 ) achieve the optimal O (1 /T ) rate ( Agarwal et al. , 2012 ). Al-though these convergence rates are significantly worse than the results in deterministic optimization, stochas-tic optimization is appealing due to its low per-iteration complexity. However, this is not the case when the domain D is complex. This is because most stochastic optimization algorithms require projecting the solution at each iteration into domain D to en-sure its feasibility, an expensive operation when the domain is complex. In this paper, we show that if the objective function is smooth and strongly convex, it is possible to reduce the number of projections dramati-cally without affecting the convergence rate. Our work is motivated by the difference in conver-gence rates between stochastic and deterministic op-timization. When the objective function is smooth and convex, under the first-order oracle assumption, Nesterov X  X  accelerated gradient method enjoys the op-timal O (1 /T 2 ) rate ( Nesterov , 2004 ; 2005 ). Thus, for deterministic optimization of smooth and convex func-tions, we can achieve an O (1 / forming O ( T 1 / 4 ) updating . When the objective func-tion is smooth and strongly convex, the optimal rate for first-order algorithms is O (1 / X  k ), for some con-stant  X  &gt; 1 ( Nesterov , 2004 ; 2007 ). In other words, for deterministic optimization of smooth and strongly convex functions, we can achieve an O (1 /T ) rate by only performing O (log T ) updating . The above obser-vations inspire us to consider the following questions. 1. For Stochastic Optimization of Smooth and Con-2. For Stochastic Optimization of Smooth and For the 1st question, we have found a posi-tive answer from literature. By combining mini-batches ( Roux et al. , 2008 ) with the accelerated stochastic approximation ( Lan , 2012 ), we can achieve the optimal O (1 / jections ( Cotter et al. , 2011 ). However, a naive appli-cation of mini-batches does not lead to the desired O (log T ) complexity for SOS 2 C. The main contribu-tion of this paper is a novel stochastic optimization al-gorithm that answers the 2nd question positively. Our theoretical analysis reveals, both in expectation and with a high probability, that the proposed algorithm achieves the optimal O (1 /T ) rate by only performing O (log T ) projections. In this section, we provide a brief review of the existing approaches for avoiding projections. 2.1. Mini-batches based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batches based algorithms use the average gradient over multiple calls to update the so-lution ( Roux et al. , 2008 ; Shalev-Shwartz et al. , 2011 ; Dekel et al. , 2011 ). For a fixed batch size B , the num-ber of updates (and projections) is reduced from O ( T ) to O ( T /B ), and the variance of the stochastic gradi-ent is reduced from  X  to  X / ancing between the loss cased by a smaller number of updates and the reduction in the variance of stochas-tic gradients, it is able to maintain the optimal rate of convergence.
 The idea of mini-batches can be incorporated into any stochastic optimization algorithm that uses gradient-based updating rules. When the objective function is smooth and convex, combining mini-batches with the accelerated stochastic approximation ( Lan , 2012 ) leads to rate of convergence ( Cotter et al. , 2011 ). By setting B = T 3 / 4 , we achieve the optimal O (1 / only O ( T 1 / 4 ) projections. When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly con-vex functions ( Hu et al. , 2009 ; Ghadimi &amp; Lan , 2012 ), leading to rate of convergence ( Dekel et al. , 2012 ). In order to maintain the optimal O (1 /T ) rate, the value of B can-not be larger than jections are required. In contrast, the algorithm pro-posed in this paper achieves an O (1 /T ) rate with only O (log T ) projections. 2.2. Projection free algorithms Due to the low iteration cost, Frank-Wolfe algo-rithm ( Frank &amp; Wolfe , 1956 ) or conditional gradient method ( Levitin &amp; Polyak , 1966 ) has seen a recent surge of interest in machine learning ( Hazan , 2008 ; Clarkson , 2010 ; Lacoste-Julien et al. , 2013 ). At each iteration of the Frank-Wolfe algorithm, instead of performing a projection that requires solving a con-strained quadratic programming problem, it solves a constrained linear programming problem. For many domains of interest, including the positive semidefi-nite cone and the trace norm ball, the constrained linear problem can be solved more efficiently than a projection problem ( Jaggi , 2013 ), making this kind of methods attractive for large-scale optimization. In a recent work ( Hazan &amp; Kale , 2012 ), an online vari-ant of the Frank-Wolfe algorithm is proposed. Al-though the online Frank-Wolfe algorithm exhibits an O (1 / unable to achieve the optimal O (1 /T ) rate for strongly convex functions. Besides, the memory complexity of this algorithm is O ( T ), making it unsuitable for large-scale optimization problems. Another related work is the stochastic gradient descent with only one projec-tion ( Mahdavi et al. , 2012 ). This algorithm is built upon the assumption that the solution domain can be characterized by an inequality constraint g ( w )  X  0 and the gradient of g ( ) can be evaluated efficiently. Unfor-tunately, this assumption does not hold for some com-monly used domain (e.g., the trace norm ball). Com-pared to the projection free algorithms, our proposed method is more general because it make no assumption about the solution domain. 3.1. Preliminaries We first define smoothness and strongly convexity. Definition 1. A function f : D  X  R is L -smooth w.r.t. a norm kk if f is everywhere differentiable and k X  f ( w )  X  X  X  f ( w  X  ) k  X   X  L k w  X  w  X  k ,  X  w , w  X   X  X  . where kk  X  is the dual norm.
 Definition 2. A smooth function f : D  X  R is  X  -strongly convex w.r.t. a norm kk , if f is everywhere differentiable and k X  f ( w )  X  X  X  f ( w  X  ) k  X   X   X  k w  X  w  X  k ,  X  w , w  X   X  X  . To simplify our analysis, we assume that both kk and kk  X  are the vector  X  2 norm in the following discussion. Following ( Hazan &amp; Kale , 2011 ), we make the follow-ing assumptions about the gradient oracle.  X  There is a gradient oracle, which, for a given input  X  The gradient oracle is G -bounded, i.e., Define w  X  as the optimal solution that minimizes F ( w ), i.e., w  X  = argmin convexity of F ( w ), we have ( Hazan &amp; Kale , 2011 )  X  2 3.2. The Algorithm Algorithm 1 shows the proposed method for Stochas-tic Optimization of Smooth and Strongly Convex func-tions (SOS 2 C), that achieves the optimal O (1 /T ) rate of convergence by performing O (log T ) projections. The inputs of the algorithm are: (1)  X  , the step size, (2) M , the fixed number of updates per epoch/stage, (3) B 1 , the initial batch size, and (4) T , the total num-ber of calls to the gradient oracle. With a slight abuse of notation, we use  X  g ( w , i ) to denote the stochastic gradient at w obtained after making the i -th call to the oracle. We denote the projection of w onto the domain D by  X  D ( w ).
 Similar to the epoch gradient descent algo-rithm ( Hazan &amp; Kale , 2011 ), the proposed algorithm consists of two layers of loops. It uses the outer ( while ) loop to divide the learning process into a sequence of epochs (Step 5 to Step 12). Similar to ( Hazan &amp; Kale , 2011 ), the number of calls to the gradient oracle made by Algorithm 1 increases exponentially over the epoches, a key that allows us to achieve the optimal O (1 /T ) convergence rate for strongly convex functions. We note that other tech-niques, such as the  X  -suffix averaging ( Rakhlin et al. , 2012 ), can also be used as an alternative.
 In the inner ( for ) loop of each epoch, we combine the idea of mini-batches ( Dekel et al. , 2011 ) with extra-gradient descent ( Nemirovski , 2005 ; Juditsky et al. , 2011 ). We choose extra-gradient descent because it al-lows us to replace in the excess risk bound E[ k  X  g ( w ) k tic gradient  X  g ( w ), thus opening the door to fully ex-ploring the capacity of mini-batches in variance reduc-tion.
 To be more specific, in the k -th epoch, we maintain two sequences of solutions { w k t } M t =1 and { z k t } M t =1 is an auxiliary solution that allows us to effectively explore the smoothness of the loss function. At each iteration t of the k -th epoch, we calculate the aver-B k times (Steps 6 and 8), and update the solutions w t and z 9). The batch size B k is fixed inside each epoch but doubles from epoch to epoch (Step 11). This is in contrast to most mini-batches based algorithms that have a fixed batch size. This difference is critical for Algorithm 1 log T Projections for SOS 2 C 1: Input: parameters  X  , M , B 1 and T 2: Initialize w 1 1  X  X  arbitrarily 3: Set k = 1 4: while 2 M P k i =1 B i  X  T do 5: for t = 1 to M do 6: Compute the average gradient at w k t over B k 7: Update 8: Compute the average gradient at z k t over B k 9: Update 10: end for 12: k = k + 1 13: end while 14: Return: w k 1 achieving O (1 /T ) convergence rate with only O (log T ) updates.
 Finally, it is worth mentioning that the Euclidean projection in Steps 7 and 9 can be replaced by the more general  X  X rox-mapping X  defined by a Bregman distance function to better capture the geometry of D ( Nemirovski , 2005 ). 3.3. The main results The following theorem bounds the expected excess risk of the solution return by Algorithm 1 and the number of projections.
 Theorem 1. Set the parameters in Algorithm 1 as The final point w k 1 returned by Algorithm 1 makes at most T calls to the gradient oracle, and has its excess risk bounded by and the total number of projections bounded by Theorem 1 shows that in expectation, Algorithm 1 achieve an O (1 /T ) convergence with O (log T ) updates. The following theorem gives a high probability bound of the excess risk for Algorithm 1 .
 Theorem 2. Set the parameters in Algorithm 1 as where  X  is defined below. For any 0 &lt;  X  &lt; 1 , let k  X  = log 2
N = log 2 The final point w k 1 returned by Algorithm 1 makes at most T calls to the gradient oracles, performs projections, and with a probability at least 1  X   X  , has its excess risk bounded by F ( w k 1 )  X  F ( w  X  )  X  Remark: It is worth noting that we achieve the high probability bound without making any mod-ifications to Algorithm 1 . This is in contrast to the epoch gradient descent algorithm ( Hazan &amp; Kale , 2011 ) that needs to shrink the domain size in or-der to obtain the desirable high probability bound, which could potentially lead to an additional compu-tational cost in performing projection. We remove the shrinking step by effectively exploring the peeling tech-nique ( Bartlett et al. , 2005 ).
 The number of projections required by Algorithm 1 , according to Theorem 2 , exhibits a linear dependence on the conditional number L/ X  , which can be very large when dealing with ill-conditioned optimization problems. In the deterministic setting, the conver-gence rate only depends on the square root of the conditional number ( Nesterov , 2004 ; 2007 ). Thus, we conjecture that it may be possible to improve the de-pendence on the conditional number to its square root in the stochastic setting, a problem that will be exam-ined in the future. We here present the proofs of main theorems. The omitted proofs are provided in the supplementary ma-terial. 4.1. Proof of Theorem 1 Since we make use of the the multi-stage learning strat-egy, the proof provided below is similar to the proof in ( Hazan &amp; Kale , 2011 ). We begin by analyzing the property of the inner loop in Algorithm 1 , which is a combination of mini-batches and the extra-gradient descent. To this end, we have the following lemma. Lemma 1. Let  X  = 1 / [ we have where Taking the conditional expectation of the inequality, we have E where E k  X  1 [ ] denotes the expectation conditioned on all the randomness up to epoch k  X  1 .
 The quantity in ( 5 ) illustrates the advantage of the extra-gradient descent, i.e., it is able to produce variance-dependent upper bound when applied to stochastic optimization. Because of mini-batches, the expectations of k  X  g k t  X  g k t k 2 and k  X  f k t  X  f than G 2 /B k , which leads to the tight upper bound in the second inequality.
 Based on Lemma 1 , we get the following lemma that bounds the expected excess risk in each epoch. Lemma 2. Define Set the parameters  X  = 1 / [ B 1 = 12  X  X  in Algorithm 1 . For any k , we have Proof. It is straightforward to check that We prove this lemma by induction on k . When k = 1, we know that Assume that E[ X  k ]  X  V k for some k  X  1, and we prove the inequality for k + 1. From Lemma 1 , we have Thus We are now at the position to prove Theorem 1 . Proof of Theorem 1 . From the stopping criterion of the outer loop in Algorithm 1 , we know that the num-ber of the epochs is given by the largest value of k such that Since the final epoch is given by and the final output is w k  X  +1 1 . From Lemma 2 , we have where we use the fact The total number of projections is 4.2. Proof of Theorem 2 Compared to the proof of Theorem 1 , the main differ-ence here is that we need a high probability version of Lemma 1 . Specifically, we need to provide high prob-ability bounds for the quantities in ( 5 ) and ( 6 ). To bound the variances given in ( 5 ), we need the fol-lowing norm concentration inequality in Hilbert Space ( Smale &amp; Zhou , 2009 ).
 Lemma 3. Let H be a Hilbert Space and let  X  be a random variable on ( Z ,  X  ) with values in H . Assume k  X  k  X  B &lt;  X  almost surely. Let {  X  i } m i =1 be indepen-dent random draws of  X  . For any 0 &lt;  X  &lt; 1 , with a probability at least 1  X   X  , Based on Lemma 3 , it is straightforward to prove the following lemma.
 Lemma 4. With a probability at least 1  X   X   X / 2 , we have Similarly, with a probability at least 1  X   X   X / 4 , we have We define the Martingale difference sequence: In order to bound the summation of Z k t in ( 6 ), we make use of the Bernstein X  X  inequality for martin-gales ( Cesa-Bianchi &amp; Lugosi , 2006 ) and the peeling technique described in ( Bartlett et al. , 2005 ), leading to the following Lemma.
 Lemma 5. We use E 1 to denote the event that all the inequalities in ( 9 ) hold. On event E 1 , with a probabil-ity at least 1  X   X   X / 4 , we have X where Substituting the results in Lemmas 4 and 5 into Lemma 1 , we obtain the lemma below.
 Lemma 6. For any 0 &lt;  X   X  &lt; 1 , with a probability at least 1  X   X   X  , we have F where n is given in ( 10 ).
 Based on Lemma 6 , we provide a high probability ver-sion of Lemma 2 , that bounds the excess risk in each epoch with a high probability.
 Lemma 7. Set the parameters  X  = 1 / [ 4 / [  X  X  ] and B 1 =  X  X  X  in Algorithm 1 , where  X  is de-fined in ( 3 ). For any k , with a probability at least (1  X   X   X  ) k  X  1 , we have Now, we provide the proof of Theorem 2 . Proof of Theorem 2 . The number of epochs made is given by the largest value of k satisfying 2 M P k i =1 B i  X  T . Since k  X  defined in ( 2 ) is the value of the final epoch, and the final output is w k  X  +1 1 . From Lemma 7 , we have with a probability at least (1  X   X   X  ) k  X  where we use the fact We complete the proof by using the property that (1  X  1 x ) x is an increasing function when x &gt; 1, which implies  X  1  X  In this section, we present numerical experiments to support our theoretical analysis. We studied the fol-lowing algorithms: 1. log T : the proposed algorithm that is optimal for 2. EP GD: the epoch gradient descent developed 3. SGD: the stochastic gradient descent with step We first consider the a simple stochastic optimization problem adapted from ( Rakhlin et al. , 2012 ), which is both smooth and strongly convex. The objective func-tion is F ( W ) = 1 2 k W k 2 F and the domain is the 5  X  5 dimensional positive semidefinite (PSD) cone. The stochastic gradient oracle, given a point W , returns the stochastic gradient W + Z where Z is uniformly dis-tributed in [  X  1 , 1] 5  X  5 . Because of the noise matrix Z , all the immediate solutions are not PDS and we need to project them back to the PSD cone. To ensure the eigendecomposition only involving real numbers, we further require Z to be symmetric. Notice that for this problem we know W  X  = argmin W o F ( W ) = 0 5  X  5 . Since the gradient of W  X  is 0 5  X  5 , it can be shown that SGD also achieves the optimal O (1 /T ) rate of conver-gence on this problem ( Rakhlin et al. , 2012 ). Let W T be the solution returned after making T calls to the gradient oracle. To verify if the proposed al-gorithm achieves an O (1 /T ) convergence, we measure ( F ( W T )  X  F ( W  X  ))  X  T versus T , which is given in Fig. 1(a) . We observe that when T is sufficiently large, quantity ( F ( W T )  X  F ( W  X  ))  X  T essentially becomes a constant for all three algorithms, implying O (1 /T ) convergence rates for all the algorithms. We also ob-serve that the constant achieved by the proposed al-gorithm is slightly larger than the two competitors, which can be attributed to the term (log log T ) 4 in our bound in Theorem 2 . To demonstrate the advantage of our algorithm, we plot the value of the objective func-tion versus the number of projections P in Fig. 1(b) . We observe that using our algorithm, the objective function is reduced significantly faster than other al-gorithms w.r.t. the number of projections.
 In the second experiment, we apply our algorithm to the regularized distance metric learning ( Jin et al. , 2009 ). The goal is to solve the following problem min where x i is the instance, and y i is x i  X  X  label, y ij is derived from labels y i and y j (i.e., y ij = 1 if y i = y j and  X  1 otherwise), k x k 2 M = x  X  M x , and  X  ( z ) = log(1 + exp(  X  z )) is the logit loss. We set  X  = 0 . 1 and test our algorithm on the Mushrooms and Adult data sets ( Chang &amp; Lin , 2011 ).
 During the optimization process, the call to the gra-dient oracle corresponds to generate a training pair us two training examples belonging to the same class (i.e., a must-link constraint), the stochastic gradient is a PSD matrix, which could result in non-PSD inter-mediate solutions and makes the projection step nec-essary. To estimate the value of objective function, we evaluate the average empirical loss on 10 4 testing pairs, which are also generated randomly. Fig. 2 shows the value of the objective function versus the number of projections P . Again, this result validates that the proposed algorithm log T is able to reduce the num-ber of projections dramatically without hurting the performance. More results can be found in the supple-mentary material. In this paper, we study the problem of reducing the number of projections in stochastic optimization by ex-ploring the property of smoothness. When the target function is smooth and strongly convex, we propose a novel algorithm that achieves the optimal O (1 /T ) rate of convergence by only performing O (log T ) pro-jections.
 An open question is how to extend our results to stochastic composite optimization ( Lan , 2012 ), where the objective function is a combination of non-smooth and smooth stochastic components. We plan to explore the composite gradient mapping tech-nique ( Nesterov , 2007 ), to see if we can achieve an O (1 /T ) rate of convergence with only O (log T ) up-dates.
 This work is partially supported by Office of Navy Research (ONR Award N00014-09-1-0663 and N000141210431), National Basic Research Program of China (973 Program) under Grant 2012CB316400, and National Natural Science Foundation of China (Grant No: 61125203).
