 Support vector machines (SVMs) have been promising methods for classification and regression analysis because of their sol id math-ematical foundations which convey several salient propert ies that other methods hardly provide. However, despite the promine nt properties of SVMs, they are not as favored for large-scale d ata mining as for pattern recognition or machine learning becau se the training complexity of SVMs is highly dependent on the size o f a data set. Many real-world data mining applications involve mil-lions or billions of data records where even multiple scans o f the entire data are too expensive to perform. This paper present s a new method, Clustering-Based SVM (CB-SVM) , which is specifi-cally designed for handling very large data sets. CB-SVM app lies a hierarchical micro-clustering algorithm that scans the e ntire data set only once to provide an SVM with high quality samples that carry the statistical summaries of the data such that the sum maries maximize the benefit of learning the SVM. CB-SVM tries to gen-erate the best SVM boundary for very large data sets given lim ited amount of resources. Our experiments on synthetic and real d ata sets show that CB-SVM is highly scalable for very large data s ets while also generating high classification accuracy.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier de-sign and evaluation support vector machines, hierarchical cluster
The work was supported in part by U.S. National Science Foun-dation NSF IIS-02-09199, Univ. of Illinois, and an IBM Facul ty Award.
 Copyright 2003 ACM 1-58113-737-0/03/0008 ... $ 5.00.

Support vector machines (SVMs) have been promising methods for data classification and regression [23, 4, 14, 6, 20, 24]. Their success in practice is drawn by its solid mathematical found ations which convey the following two salient properties:
The success of SVMs in machine learning naturally leads to it s possible extension to the classification or regression prob lems for mining a huge amount of data. However, despite the prominent properties of SVMs, they are not as favored for large-scale d ata mining as for pattern recognition or machine learning becau se the training complexity of SVMs is highly dependent on the size o f data set. (It is known to be at least quadratic to the number of data points. Refer [6] for more discussions on the complexity of S VMs.) Many real-world data mining applications involve millions or bil-lions of data records. The following example shows how unsca lable a standard SVM is on a large data set.

E XAMPL E 1. The forest cover type data set from UCI KDD 10 quantitative and 44 binary attributes. Figure 1 shows the train-ing time of an SVM on different numbers of training data rando mly sampled fron the original data set. From the graphs, we can in fer that it would take years for an SVM to train a million data. (We which gave fairly good results among others. We ran them usin g a Pentium III 800Mhz with 906Mb memory.)
Researchers have proposed various revisions of SVMs to incr ease the training efficiency by mutating or approximating it. How ever, they are still not feasible with very large data sets where ev en multi-ple scans of the entire data set are too expensive to perform, or they end up losing the benefits of using an SVM by the over-simplific ations. (See Section 6 for the discussions on related work.) Figure 1: Non-scalability of SVMs. x-axis: # of data points; y-axis: training time in hours This paper presents a new approach for scalable and reliable SVM classification. The method, called Clustering-Based SVM (CB-SVM) , is specifically designed for handling very large data form worse with training from the entire data than training f rom a fine quality of samples of the data set [19]. Selective sampli ng (or active learning) techniques with SVMs try to sample the trai ning data intelligently to maximize the performance of SVMs, but they normally require many scans of the entire data set [19, 22] (S ec-tion 6). Our CB-SVM using the similar idea applies a hierarch ical micro-clustering algorithm that scans the entire data set o nly once to provide an SVM with high quality samples that carry the sta tis-tical summaries of the data such that the summaries maximize the benefit of learning the SVM. CB-SVM is scalable in terms of the training efficiency while maximizing the performance of SVM s.
The key idea of CB-SVM is to use a hierarchical micro-cluster ing technique to get finer description closer to the boundary and coarser description farther from the boundary, which can be efficien tly pro-cessed as follows: CB-SVM first constructs two micro-cluste r trees from positive and negative training data respectively. In e ach tree, a node in a higher level is a summarized representation of its c hildren nodes. After constructing the two trees, CB-SVM start train ing an SVM only from the root nodes. Once it generates the  X  X ough X  boundary from the root nodes, it selectively decluster only the data summary near to the boundary into lower (or finer) levels usin g the tree structure. The hierarchical representation of the data sum-maries is a perfect base structure for CB-SVM to perform the s elec-tive declustering effectively. CB-SVM repeats this select ive declus-tering to the leaf level.

CB-SVM can be used for linear classification or regression an al-ysis for very large data sets, including streaming data or da ta in large data warehouses, especially where random sampling hu rts the performance because of infrequently occurring important d ata or irregular patterns of incoming data, which causes differen t proba-bility distributions between training and testing data. We discuss this more in Section 5.1.3.

Our experiments on the network intrusion data set (Section 5 .2), a good example which shows that random sampling could hurt, show that CB-SVM is scalable for very large data sets while al so generating high classification accuracy. Based on the best o f our knowledge, the proposed method is currently the only SVM for very large data sets which tries to generate the best results given limited amount of resources.

The remainder of the paper is organized as follows. We first provide an overview of SVMs in Section 2. In Section 3, we intr o-duce a hierarchical micro-clustering algorithm for very la rge data sets, originally exploited by T. Zhang et al. [25]. In Sectio n 4, we present the CB-SVM algorithm that applies the hierarchical micro-clustering algorithm to a standard SVM to make the SVM scalab le for very large data sets. Section 5 demonstrates experiment al re-sults on artificial and real data sets. We discuss related wor k in Section 6 and conclude our study in Section 7.
In machine learning theory, the  X  X ptimal X  class boundary fu nc-tion (or hypothesis) given a limited number of training data set ( is the label of ) is considered the one that gives the best generalization performance which denotes the performance on  X  X nseen X  examples rather than on the training data. The perf or-mance on the training data is not regarded as a good evaluatio n measure for a hypothesis because the hypothesis ends up overfit-ting when it tries to fit the training data too hard. When a problem is easy to classify and the boundary function is complicated more than it needs to be, the boundary is likely overfit. When a prob -lem is hard and the classifier is not powerful enough, the boun d-ary becomes underfit. SVMs are excellent examples of supervi sed learning that tries to maximize the generalization by maxim izing the margin and also supports nonlinear separation using advanced kernels, by which SVMs try to avoid overfitting and underfitti ng [4, 23]. The margin in SVMs denotes the distance from the boundary to the closest data in the feature space.

In SVMs, the problem of computing a margin maximized bound-ary function is specified by the following quadratic program ming (QP) problem: The number of training data is denoted by variables, where each component corresponds to a training data ( , ). the outliers (or noise) in training data.

The kernel for linear boundary function is scalar product of two data points. The nonlinear transforma tion of the feature space is performed by replacing with an advanced kernel, such as polynomial kernel kernel an attractive computational short-cut, which forgoes an ex pensive creation of a complicated feature space. An advanced kernel is computing the scalar product of their images in a usually muc h higher-dimensional feature space (or even an infinite-dime nsional space), which allows one to work implicitly with hyperplane s in such highly complex spaces.

Another characteristic of SVMs is that its boundary functio n is described by the support vectors (SVs) which are the data the clos-est to the boundary. The above QP problem computes a vector , each element of which specifies the weight of each data, and th e SVs is the data whose corresponding is greater than zero. In other words, the other data rather than the SVs do not contrib ute to the boundary function, and thus computing an SVM boundary function can be viewed as finding the SVs with the correspondi ng weights to describe the class boundary.

There have been many attempts to revise the original QP formu -lation such that it can be solved by a QP solver more efficientl y [8, 1]. (See Section 6 for more details.) We do not revise the orig i-nal QP formulation of SVMs. Instead, we try to provide a small er but high quality data set that is beneficial to computing the S VM boundary function effectively by applying a hierarchical c lustering algorithm. Our CB-SVM algorithm substantially reduces the total number of data points for training an SVM while trying to keep the high quality of SVs that describes the boundary the best.
The hierarchical micro-clustering algorithm we present he re and will apply to our CB-SVM in Section 4 was originally exploite d by T. Zhang et al. at 1996 [25], which is named BIRCH . The concept of a  X  X icro-cluster X  is similar to those in [25, 12], which de notes a statistically summarized representation of a group of dat a which are so close together that they are likely to belong to the sam e clus-ter. Our hierarchical micro-clustering algorithm has the f ollowing characteristics.

Further improved hierarchical clustering algorithms have been developed including CURE [10] or Chameleon [15]. Chameleon has shown to be very powerful at discovering arbitrarily sha ped quality clusters with complex shapes, and its complexity is also lin-ear to the number of objects, but its parameter setting in gen eral has a significant influence on the results. The CF tree of BIRCH car ries the spherical shapes of hierarchical clusters and captures the statis-tical summaries of the entire data set. Thus it provides an ef ficient and effective structure for CB-SVM to run.
We start from defining some basic concepts. Given -dimensional data points in a cluster: where and radius of the cluster are defined as: % is the average distance from member points to the centroid.
The concepts of clustering feature (CF) tree is at the core of the hierarchical micro-clustering algorithm which makes the c luster-ing incremental without expensive computations. A CF is a tr iple which summarizes the information that a CF tree maintains fo r a cluster.
 Given d-dimensional data points in a cluster: where = 1, 2, , , the CF vector of the cluster is defined as a triple: cluster, is the linear sum of the data points, i.e., , and SS is the square sum of the data points, i.e., .
 [25]] Assume that are the CF vectors of two disjoint clusters. Then the CF vecto r of the cluster that is formed by merging the two disjoint cluste rs is: Refer to [25] for the proof.
 From the CF definition and additivity theorem, we know that th e CF vectors of clusters can be stored and calculated incremen tally and accurately as clusters are merged. The centroid dius of each cluster can be also computed from the CF of the cluster.

The CF is a summary of a cluster X  X  set of data points. Managing only this CF summary is efficient, saves spaces significantly , and is sufficient for calculating all the information for buildi ng the hi-erarchical micro-clusters which will facilitate computin g an SVM boundary for a very large data set.
A CF tree is a height-balanced tree with two parameters: bran ch-ing factor and threshold . A CF tree of height is showing in the right side of Figure 2. Each nonleaf node consists of at most entries of the form the subcluster represented by this child. A leaf entry , the entry in a leaf node, only has a a nonleaf node represents a cluster made up of all the subclus ters represented by its entries. The threshold is a constraint for the leaf entries to satisfy such that the radius of an entry in a leaf no de has to be less than .

The tree size is a function of . The larger is, the smaller the tree is. The branching factor can be determined by a page size such that a leaf or nonleaf node fit in a page.

This CF tree is a compact representation of the data set becau se each entry in a leaf node is not a single data point but a subclu s-ter (which absorbs many data points with radius under a speci fic threshold ).
A CF tree is built up dynamically as new data objects are in-serted. The ways that it inserts a data into the correct subcl uster, merges leaf nodes, and manages nonleaf nodes are similar to t hose in a B+-tree, which can be sketched as follows: 1. Identifying the appropriate leaf : Starting from the root, 2. Modifying the leaf : If the leaf entry can absorb the new 3. Modifying the path to the leaf : It updates the CF vectors
Due to the limited number of entries in a node, a highly skewed input could cause two subclusters that should have been in on e clus-ter split across different nodes, and vice versa. These infr equent but undesirable anomalies can be handled in the original BIRCH a lgo-rithm by further refinement with additional data scans. Howe ver, we do not perform this further refinement because the infrequ ent and localized inaccuracy do not impact the performance of CB -SVM much.
The choice of the threshold is crucial for building the tree in the right size which fits in the available memory because if is too small, we run out of memory before all the data are scanned. Th e original BIRCH algorithm initially sets very low, and iteratively increases until the tree fits in the memory. T. Zhang proved that rebuilding the tree with a larger requires a re-scan of the data inserted in the tree so far and at most extra pages of memory, where is the height of the tree [25]. The heuristics for updating is also provided in [25]. Due to space limitations and to keep the focus of the paper, we skip the details of those. In our experi ments, we set the initial threshold intuitively based on the number of data points and dimensions and the value range of each dimension such that is proportional to , and the tree of mostly fits in the memory.
After the construction of a CF tree, the leaf entries that con tains far fewer data points than average are considered to be outli ers. A low setting of outlier threshold can increase the classifica tion per-formance of CB-SVM especially when the number of data is rel-atively large compared to the number of dimensions and the ty pe of boundary functions are simple (which is related to having a low VC dimension in machine learning theory) because the non-tr ivial amount of noise in the training data which may not be separabl e by the simple boundary function prevents the SVM boundary fr om converging in the quadratic programming. For this reason, w e en-abled the outlier handling with a low threshold in our experi ments in Section 5 because the type of data we are targetting is of la rge number of data points with relatively low dimensions, and th e type of the boundary functions is linear with VC dimension is the number of dimensions. See Section 5 for more details.
A CF tree that fits in a memory can have the nodes at maxi-mum where is the size of memory and is the size of a node. The height of a tree is of data set. However, if we assume that memory is unbounded an d the number of the leaf entries is equal to the number of data po ints due to a very small threshold , then
Insertion of a node into a tree requires the examination of en-tries, and the cost per entry is proportional to the dimensio n . Thus, the cost for inserting data points is .
 In case of rebuilding the tree due to the poor estimation of , ad-ditional re-insertions of the data already inserted has to b e added in the cost. Then the cost becomes where is the number of the rebuildings. If we only consider the depend ence of the size of data set, the computation complexity of the alg orithm is . Experiments from the original BIRCH algorithm have also shown the linear scalability of the algorithm with resp ect to the number of objects, and good quality of clustering of the d ata.
In this section, we present the CB-SVM algorithm which train s a very large data set using the hierarchical micro-clusters (i.e., CF tree) to construct an accurate SVM boundary function.

The key idea of CB-SVM can be viewed being similar to that of selective sampling (or active learning), i.e., selectin g the data which maximizes the benefit of learning. The selective sampl ing for SVMs selects and accumulates the low margin data at each round that are close to the boundary in the feature space beca use the low margin data have higher chances to become the SVs of th e boundary for the next round [22, 19]. Appreciating this idea , we decluster the entries near the boundary to get finer samples n earer to the boundary and coarser samples farther from the boundar y. In this way, we induce the SVs, the description of the boundary, as fine as possible while keeping the total number of training data p oints as small as possible.

While selective sampling needs to scan the entire data set at each round to select the closest data point, CB-SVM runs based on t he CF tree which can be constructed in a single scan of the entire data structing an SVM boundary efficiently and effectively. The s ketch of the CB-SVM algorithm follows: 1. construct two CF trees from positive and negative data set 2. train an SVM boundary function from the centroids of the 3. decluster the entries near the boundary into the next leve l, 4. construct another SVM from the centroids of the entries in
The CF tree is a suitable base structure for CB-SVM to perform the selective declustering efficiently. The clustered data also pro-vides better summaries for SVMs than random samples of the en -tire data set because the random sampling is susceptible to a biased (or skewed) input, and thus it may generate undesirable outp uts especially when the probability distributions of training and testing data are not similar, which is common in practice. (The netwo rk in-trusion data set from the UCI KDD repository that we experime nt on in Section 5 is a good example of having substantially diff erent distributions in training and testing data set due to the fac t that in the real world, the patterns of network intrusions are very i rregular.)
Without loss of generality, let us consider linearly separa ble cases for the convenience of explanation.

Let positive tree We first train an SVM boundary function from the centroids of the root entries of contains the CF information from which we can efficiently com pute the center point an example of the SVM boundary with the root clusters and the corresponding positive tree.

With the boundary function and the root entries, we determine the low margin clusters that are close to the boundary and thus needs to be declustered into the finer level. Let support clusters be the clusters whose center points are the SVs of the boundary , e.g., the circles of bold lines in Figure 2. Let be the distance from the boundary to the centroid of a support cluster, and let be the distance from the boundary to the centroid of a cluster . Then, we consider a cluster which satisfies the following constraint as a low margin cluster . where is the radius of the cluster .

The clusters that satisfy the constraint (4) have chances fo r their subclusters to be the support clusters of the boundary as ill ustrated in Figure 3 where five clusters initially satisfied the constr aint (4) (three of them were the support clusters) and thus were declu s-tered into finer levels, which results in the right picture of Figure 3. The subclusters whose parent clusters do not satisfy cons traint (4) would not be the support clusters of the boundary because the surfaces of the parent clusters are farther than the SVs o f the boundary. Thus we have the following remark.
 and be the radius of a cluster and the distance from the boundary to the centroid of the cluster respectively. Given a sep-arable set of positive and negative clusters and the SVM boundary of the set, the subclusters of have the possibilities to be the support clusters of the boundary only if centroid of a support cluster.

The example we illustrated was a separable case with the hard constraints of SVMs. In practice, the soft constraints are n ecessary to cope with noise in the training set. Using the soft constra ints generates the SVs having different distances to the boundar y. For the declustering condition with the soft constraints of SVM s, we re-place with , the maximum distance of all , which would include all the clusters whose subclusters have the possibi lities to be the support clusters of the soft boundary . The subclusters whose parent clusters do not satisfy constr aint (5) would not be the support clusters of the soft boundary because the surfaces of the parent clusters are farther than the most distant SV of the boundary.
 soft constraints of SVMs, the subclusters of have the possibilities to be the support clusters of the boundary only if , where is the maximum distance from the boundary to the cen-troids of all the support clusters.

Figures 4 and 5 describe the CB-SVM algorithm with the soft constraints of the declustering.
Building a CF tree from number of data points costs of entries in a node, the height of the tree, and the number of rebuildings. Once the CF tree is built, the training time of C B-SVM becomes dependent on the number of entries instead of the num ber of data points.

Let us assume where is the training time of algorithm . (Note that is known to be at least quadratic to and linear to the number of dimensions. Refer to [6] for more discussion on the complexity of SVMs.) The number of the leaf entries is at most . Thus, from the leaf entries becomes .

Let support entries be the SVs when the training data are entries in some nodes. Assume that is the average rate of the number of the support entries among the training entries. Namely, number of support entries among the training entries, e.g., for standard SVMs with large data sets.
 number of the leaf entries of a CF tree is equal to the number of Input: -positive data set , negative data set Output: -a boundary function Notation: -cluster tree from a data set -getRootEntries( ): return the root entries of a tree -getChildren( ): return the children entries of an entry set -getLowMargin( , ): return the low margin entries from a set which are close to the boundary (See Figure 5) Algorithm: 1. 2. := getRootEntries( 3. Do loop 3.1. := SVM.train( ); 3.2. := getLowMargin( , ); 3.3. := ; 3.3. := getChildren( ); 3.4. Exit if = ; 3.5. := ; 4. Return ; training data points , then CB-SVM trains asymptotically times faster than standard SVMs given the CF tree, where is the average rate of SVs and the height of the tree
P ROOF . If we approximate the number of iterations in CB-SVM SVM given the CF tree is: where tion of CB-SVM. The number of training data points at the -th iteration is: where is the number of data points in a node, and is the number of the SVs among the data. If we assume , by approximation of If we accumulate the training time of all iterations, Input: -a boundary function , a entry set Output: -a set of the low margin entries Algorithm: 1. // return the maximum distance of the support vectors from the boundary 2. := getLowerMarginData( // return the data whose margin is smaller than 3. Return ; Therefore, faster than which is for .

Theorem 2 states that CB-SVM trains asymtotically times faster than a standard SVM given a CF tree. The training time of CB-SVM is asymptotically equal to that of a standard SVM on ly if all the training data points become the SVs. The rate of the SVs is variant, depending on the type of problems, the type of ker nels, the number of dimensions, the number of data points, and the S VM parameters. However, mostly data sets. So, the performance difference between CB-SVM an d a standard SVM goes higher as the data set becomes larger.
In this section, we provide empirical evidence of our analys is on CB-SVM using synthetic and real data sets, and we discuss t he results. All our experiments are done in a Pentium III 800Mhz machine with 906MB memory.
To verify the performance of CB-SVM in realistic environmen ts while providing visualized results, we perform binary clas sifica-tions on two-dimensional data sets we generated as follows. 1. We randomly created clusters such that for each clus-2. We labeled the clusters based on the -axis value of each 3. Once the characteristics of each cluster are determined, the use -SVM with linear kernel. We enabled the shrinking heuris-tics for fast training [13]. -SVM has an advantage over standard SVMs: The parameter has a semantic meaning which denotes the upper bound of the noise rate and the lower bound of the SV rate in training data [6]. In our experiments, we set very low ( or set is large and the noise is relatively small.
Figure 6(a) shows an example of the data set generated accord ing to the parameters of Table 1. The data generated from the clus ters in the left side and in the right side are positive ( X  ( X   X ) respectively.

Figure 6(b) shows 0.5% randomly sampled data from the origin al data set of Figure 6(a). Random sampling could hurt the SVM performance in the following ways:
Figure 6(c) shows the training data points at the last iterat ion in CB-SVM. We set threshold with the standard deviation. It generated a CF tre e of
Note that the training data points of CB-SVM are not the ac-tual data but the summary of the clusters of them, so they tend not to have narrowly focused data points as it does in the rand om sampling. Also, the areas far from the boundary thus not like ly to contribute to the SV will have very sparse data points becaus e the clusters representing those areas would not be declustered in the process of CB-SVM.
 Figure 7(a) and (b) show the intermediate data points that CB -SVM generated at the first and second iterations respectivel y. The data points in Figure 7(a) are the centroids of the root entri es, which are very sparse. Figure 7(b) shows dense points around the bo und-ary which are declustered into the second level of the CF tree . Fi-(a) Data distribution at the first iteration ( ) Figure 7: Intermediate results of CB-SVM.  X   X   X : negative data Number of data points 113601 597 603 SVM Training time (sec.) 160.792 0.003 0.003 Table 2: Performance results on synthetic data set (# of trai n-ing data = 113601, # of testing data = 107072). FP:false positive; FN:false negative; Sampling time for CB-SVM: time for contr uct-ing the CF tree nally, Figure 6(c) shows a better data distribution for SVMs by declustering the support entries to the leaf level.

For fair evaluation, we generated a testing set using the sam e clusters and radiuses but different probability distribut ions by ran-domly re-assigning the number of points for each cluster. We report the number of false predictions (# of false negative + # of false positive) on the testing data set because the data size is so big compared to the number of false prediction that the accuracy itself does not show much difference between them.
 Table 2 shows the performance results on the testing data set . CB-SVM based on the clustering-based samples outperforms t he standard SVM with the same number of random samples. The  X  X umber of data points X  for CB-SVM in Table 2 denotes the num-ber of training data points at the last iteration as shown in F igure 6(c). The  X  X raining time X  for CB-SVM in the table indicates t he SVM training time on that data, which is almost equal to that o f 0.5% random samples since both generated similar number of d ata points. The  X  X ampling time X  for CB-SVM denoting the time to t he construction of the 597 data points of Figure 6(c) definitly t akes longer than the random sampling because it involves the cons truc-tion of a CF tree. (The long construction time of the CF tree is partly caused by our non-optimized implementation of the hi erar-chical micro-clustering algorithm.)
However, as the data size grows, the random sample size that generates similar accuracies as that of CB-SVM also increas es, for which the SVM training time ( ) becomes dominating over the  X  X ampling time X  for CB-SVM ( with a fixed ), and thus the total training time of the SVM with random sampling ends u p longer than that of CB-SVM. (See the next section.)
We generated a much larger data set according to the paramete rs of Table 3 to verify the performance of CB-SVM compared to ran -Table 3: Data generation parameters for the very large data s et Table 4: Performance results on the very large data set (# of training data = 23066169, # of testing data = 233890). S-Rate: sampling rate; T-Time: training time; S-Time: sampling tim e; ASVM: selective sampling dom sampling and ASVM (selective sampling or active learnin g with SVMs) for very large data sets. Table 4 shows the perfor-mance results of random sampling, ASVM, and CB-SVM on the  X  X ery large X  data set. We did not run an SVM on the entire data s et since it will take years to finish training. Note that due to th e simple linear boundary on the very large amount of training data, ra ndom sampling does not increase the performance of SVMs at some po int as the sample size increases. ASVM and CB-SVM showed the error rates around 15% lower than the random sampling of the highes t performance. The training time of CB-SVM in total (T-Time + S -Time) was shorter than that of ASVM or the random sampling of the highest performance. ASVM [19] showed the similar results as ours since the basic idea is similar, which implies that fo r large data sets, SVMs perform better with a fine quality of samples than a large amount of random samples. However, ASVM takes much longer than CB-SVM for very large data sets that do not fit in th e memory because it needs to scan the entire data set at each rou nd to select the closest data point, thereby generating too muc h I/O cost to undergo as many rounds as it needs to get enough traini ng data. In this experiment, we ran the ASVM with (starting from one positive and one negative sample and adding five samp les at each round), which gave fairly good results among others. ( is commonly set below ten. If is too high, its performance con-verges slower which ends up with larger amount of training da ta to achieve the same accuracy, and if is too low, ASVM may need to undergo too many rounds [19, 22].) It underwent 61 rounds r e-sulting in 61 times of data scans to sample 307 training data, which took 56872.213 seconds in total for training. We discuss ASV M further in Section 6.
In this section, we experiment on the network intrusion dete ction data set from the UCI KDD archive which was used for the KDD ing data and three hundred thousands of testing data. As prev iously noted, CB-SVM works for very large data sets including strea ming data or data warehouse analysis especially where random sam pling hurts the performance due to infrequent occuring important data or irregular patterns of data incoming which causes different proba-bility distributions of training and testing data. The netw ork intru-sion data set is a good application for CB-SVM because the tes ting data is not from the same probability distribution as the tra ining data, and it also includes specific attack types not in the tra ining data. (The datasets contain a total of 24 training attack typ es, with an additional 14 types in the test data only.) This is because they were collected in different times of periods, which makes th e task more realistic. (Some intrusion experts believe that most n ovel at-tacks are variants of known attacks and the  X  X ignature X  of kn own attacks can be sufficient to catch novel variants.) Our exper iments on this data set show that our method based on the clustering-based samples significantly outperforms the random sampling havi ng the same number of samples.
Each data object consists of 41 features (34 continuous feat ures and 7 symbolic features). We normalized the continuous feat ure values into between zero and one by dividing them by their max -imum values. We created one independent  X  X ero-one X  (predic ate) feature for each value of the symbolic features such that  X  X n e X  in-dicates the existence of the value. Our way of combining mult i-variable features may not be the best way for SVMs. Using more sophisticated techniques for pre-processing the features could im-prove the performance further.

We set tures in this data set becomes about 50 times larger than that in our synthetic data set and the range of each value is the same. The out-lier threshold in this data set was tuned with a lower value be cause the outliers in the network intrusion data set could have val uable information. However, tuning the outlier threshold involv es some heuristics depending on the type of data set and the type of bo und-ary function. Further definition and justification on the heu ristics for specific types of problems is a subsequent future work.
We used the same SVM implementation with the same way of optimizing parameters as in the experiments on the syntheti c data sets. Linear kernel also showed good performance (over 90% a c-curacy) in this experiment, which implies the classificatio n on this network intrusion data set is likely separable by a linear fu nction. We briefly discuss the usage of nonlinear kernel in CB-SVM in Section 7.
Our task is to distinguish normal connections from attacks. Table 5 shows the performance results of random sampling, ASVM, an d CB-SVM on the network intrusion data set. Running the SVM with the larger amount of samples did not improve the perform ance much for the same reason as discussed in Section 5.1.4. ASVM a nd CB-SVM also generated better results than the random sampli ng, and the total training time of CB-SVM is much faster than that of ASVM. (We run ASVM with the same parameters as in Section 5.1.4.)
Our work is in some aspects related to: (1) SVM fast implemen-tations, (2) SVM approximations, (3) on-line SVMs or increm en-tal and decremental SVMs for dynamic environments, (4) sele ctive sampling (or active learning) for SVMs, and (5) random sampl ing techniques for SVMs.

Many algorithms and implementation techniques have been de -veloped for training SVMs efficiently since the running time of the Table 5: Performance results on the network intrusion data set (# of training data = 4898431, # of testing data = 311029). S-Rate: sampling rate; T-Time: training time; S-Time: samp ling time; ASVM: selective sampling standard QP algorithms grows too fast. Most effective heuri stics to speed up SVM training are to divide the original QP problem into small pieces, thereby reducing the size of each QP probl em. Chunking, decomposition [13, 7], and sequential minimal op ti-mization [18] are most well-known techniques. Our CB-SVM al -gorithm runs on top of these techniques to handle very large d ata sets by condensing further the training data into the statis tical sum-maries of large data groups such that coarse summary is made f or  X  X nimportant X  data and fine summary is made for  X  X mportant X  d ata.
SVM approximations have been attempted to improve the com-putational efficiency of SVMs by altering the QP formulation to the extent that it keeps a similar semantic of the original SVM wh ile it is faster to be solved by a QP solver [8, 1]. However, their new formulations are still not proven to be efficient and reliabl e enough to work with very large data sets.

On-line SVMs or incremental and decremental SVMs have been developed to handle dynamically incoming data efficiently [ 21, 5, 16]. In this senario that an SVM model is incrementally const ructed and maintained, the newer data have a higher impact on the SVM model than older data. In other words, recent data have a high er chance to be the SVs of the SVM model than older data. Thus, for the analysis of an archive data which should treat all the data equally, they would generate undesirable outputs.

Selective sampling or active learning is to intelligently s ample a small number of training data from the entire data set that ma xi-mizes the degree of learning, i.e., learning maximally with a mini-mum number of data points [9, 22, 19]. The core of the active le arn-ing technique is to select the data intelligently such that t he degree of learning is maximized by the data. A common active learnin g paradigm iterates a training and testing process as follows : (1) con-struct a model by training an initially given data, (2) test t he entire data set using the model, (3) by analyzing the testing output , select the data (from the entire data set) that will maximize the deg ree of learning for the next round, (4) accumulate the data to the tr aining data set, and train them to construct another model, and (5) r epeat from (2) to (5) until the model becomes accurate enough.
The idea of selective sampling for SVMs is to select the data close to the boundary in the feature space at each round becau se the data near the boundary have higher chances to be SVs in the next round, i.e., a higher chance to move the boundary furthe r [22, 19]. They iterate until there exists no data nearer to the bou ndary than the SVs. However, an active learning system needs to sca n the entire data set at every round to select the data, which gener ates too much I/O cost for very large data sets.

Some random sampling techniques [2, 11] developed to reduce the training time of SVMs for large data sets are also based th e same idea as the selective sampling which samples the data ne ar the boundary with higher probabilities. They also need to sc an the entire data set at each round when the samples are add in. Anot her method using random sampling [17] was developed for nonline ar SVMs using the random sampling technique in the kernel trick .
Based on the best of our knowledge, our proposed method is cur -rently the only SVM for very large data sets which tries to gen erate the best results given limited amount of resource. This paper proposes a new method called CB-SVM (Clustering-Based SVM) that integrates a scalable clustering method wit h an SVM method and effectively runs SVMs for very large data sets . The existing SVMs are not feasible to run such data sets due to their high complexity on the data size or frequent accesses o n the large data sets causing expensive I/O operations. CB-SVM ap plies a hierarchical micro-clustering algorithm that scans the e ntire data set only once to provide an SVM with high quality micro-clust ers that carry the statistical summaries of the data such that th e sum-maries maximize the benefit of learning the SVM. CB-SVM tries to generate the best SVM boundary for very large data sets giv en limited amount of resource based on the philosophy of hierar chi-cal clustering where progressive deepening can be conducte d when needed to find high quality boundaries for SVM. Our experimen ts on synthetic and real data sets show that CB-SVM is very scala ble for very large data sets while generating high classificatio n accu-racy.

CB-SVM is currently limited to the usage of linear kernels si nce the hierarchical micro-clusters would not be isomorphic to a new high-dimensional feature space once the space is transform ed by a nonlinear kernel. In other words, the statistical summarie s of data such as radius and distances computed in the input space will not be preserved in the transformed feature space. Constructing e ffective indexing structures for nonlinear kernels is an interestin g direction of future work since it has high practical value especially f or pattern recognition of large data sets, such as classifying forest c over types or the pictures from a huge amount of satellite data. [1] D. K. Agarwal. Shrinkage estimator generalizations of [2] J. L. Balczar, Y. Dai, and O. Watanabe. A random sampling [3] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. [4] C. J. C. Burges. A tutorial on support vector machines for [5] G. Cauwenberghs and T. Poggio. Incremental and [6] C.-C. Chang and C.-J. Lin. Training nu-support vector [7] R. Collobert and S. Bengio. SVMTorch: Support vector [8] G. Fung and O. L. Mangasarian. Proximal support vector [9] R. Greiner, A. J. Grove, and D. Roth. Learning active [10] S. Guha, R. Rastogi, and K. Shim. CURE: an efficient [11] O. W. J. L. Balczar, Y. Dai. A random sampling technique [12] W. Jin, A. K. H. Tung, and J. Han. Mining top-n local [13] T. Joachims. Making large-scale support vector machin e [14] T. Joachims. Text categorization with support vector [15] G. Karypis, E.-H. Han, and V. Kumar. Chameleon: [16] J. Kivinen, A. J. Smola, and R. C. Williamson. Online [17] Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced support [18] J. Platt. Fast training of support vector machines usin g [19] G. Schohn and D. Cohn. Less is more: Active learning with [20] A. Smola and B. Sch. A tutorial on support vector regress ion. [21] N. Syed, H. Liu, and K. Sung. Incremental learning with [22] S. Tong and D. Koller. Support vector machine active [23] V. N. Vapnik. Statistical Learning Theory . John Wiley and [24] H. Yu, J. Han, and K. C. Chang. PEBL: Positive-example [25] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: an
