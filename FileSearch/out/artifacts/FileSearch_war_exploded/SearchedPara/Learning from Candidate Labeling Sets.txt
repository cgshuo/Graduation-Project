 supervised learning to semi-supervised learning [7, 26].
 reference, we call this type of learning problem a Candidate Labeling Set (CLS) problem. problems can be naturally casted into the CLS framework.
 reaching performances comparable to fully-supervised lea rning algorithms. set, the better the performance and the faster the algorithm will be. where the correct labels could be considered as latent varia bles. ambiguous labeling problem described in [17] from single in stances to bags of instances. X . In the CLS setting, the N training data are provided in the form {X M set of L other words there are L We assume that the correct labeling vector for X is not equivalent to just associating L a two instances bag { x they can not share the same label, then z we will assume that the labeling set Z practical example on how to construct this set using the prio r knowledge on the task. Given the training data {X Z , i.e. L of M i instances could have maximum C M i labeling vectors, which becomes a clustering problem. However, we are more interested in situations when L 2.1 Large-margin formulation by X the generic bag of M instances { x labeling vectors, and y = { y We start by introducing the loss function that assumes the tr ue label y known where  X ( z z function. Hence, if the vector z is the predicted label for the bag,  X  of misclassified instances in the bag.
 of this loss: We also define, with a small abuse of notation,  X  A bag case, and prove that  X  A following we introduce another loss that upper bounds  X  A We assume that the prediction function f ( x ) we are searching for is equal to arg max x P labels in y . With the definitions above, we can rewrite the function F as where we defined  X ( X , y ) = P M scalar product between w and a joint feature map between the bag X and the labeling vector y . Remark. If the prior probabilities of every candidate labeling vect ors z they could be incorporated by slightly modifying the featur e mapping scheme in (2). We can now introduce the following loss function where | x | Proposition.  X  Proof. Define  X  z = arg max We now consider the case in which  X  z /  X  X  . We have that  X   X  ( X , Z ; w )  X   X  The loss  X  introduce an algorithm to minimize it efficiently. 2.2 A probabilistic interpretation It is possible to gain additional intuition on the proposed l oss function  X  the most likely correct member of Z . It can be easily verified that max bound of P ( Z|X ;  X  ) . The learning problem becomes to minimize the ratio for the b ag: If we assume independence between the instances in the bag, (4) can be factorized as:  X  log obtain the loss function in (3). optimization problem for the CLS learning problem: vectors in Z problem can be solved using the constrained concave-convex procedure (CCCP) [19, 23]. 3.1 Optimization using the CCCP algorithm r problem. When this function is non-smooth, such as max CCCP replaces max The subgradient of a point-wise maximum function g ( x ) = max union of subdifferentials of the subset of the functions g C i = { z  X  X  i : F ( X i , z ; w
X We are free to choose the values of the  X  ( r ) for  X  z  X  Replacing the non-convex loss  X  round of the CCCP is With our choice of  X  ( r ) 3.2 Solve the convex optimization problem using the Pegasos framework optimal hyperplane lives can be calculated by considering t hat Greedily searching for the most violating labeling vector  X  z the structure of Z worst case complexity of searching the maximum of  X  z /  X  X  number of unique possible labels for x for example [20, Section 4] for a discussion on some specific p roblems and special cases. Algorithm 1 The CCCP algorithm for solving MMS 1: initialize: w (1) = 0 2: repeat 5: until convergence to a local minimum Algorithm 2 Pegasos Algorithm for Solving Relaxed-MMS (8) 2: for t = 1 , 2 ,...,T do 3: Draw at random A t  X  X  1 ,...,N } , with | A t | = K 4: Compute  X  z k = arg max 5: Set A + 6: Set w 8: end for annotated by their associated captions. We benchmark MMS ag ainst the following baselines: We implemented our MMS algorithm in MATLAB 4 , and used a value of the 1 /N for the regular-ization parameter  X  in all our experiments. In (1) we used  X ( z 4.1 Experiments on artificial data randomly into bags of fixed size B with probability at least P randomly chosen from { 1 ,...,B } , and the new labels are chosen among a predefined ambiguous whether the ambiguous pair of a label is present equals P and each instance is considered separately.
 Varying P the algorithms. For example, when P set. Meanwhile, P distinguish between these two classes. The parameters P For each difficulty level, we run three different training/t est splits. where the two largest classes (among seven) dominate the who le dataset (more than 85% of the with the average over all the possible labeling vectors can l ead to poor performance. 4.2 Applications to learning from images &amp; captions A huge amount of images with accompanying text captions are a vailable on the web. This cheap the candidate labeling sets (see Figure 2 for a practical exa mple). additional class, except for MIML algorithms where unknown faces can be automatically consid-tions can also be made here: MMS achieves performance compar able to the fully-supervised SVM algorithms (4.1% higher than 1vA-SVM on Yahoo! data), while outperforming the other baselines for ambiguously labeled data. our framework provides a principled way to encode prior know ledge about relationships between (e.g., a clustering problem with linkage constraints).
 [4] D. P. Bertsekas. Convex Analysis and Optimization . Athena Scientific, 2003. [12] Y. Grandvalet. Logistic regression for partial labels . In Proc. IPMU , 2002. [17] R. Jin and Z. Ghahramani. Learning with multiple labels . In Proc. NIPS , 2002.
