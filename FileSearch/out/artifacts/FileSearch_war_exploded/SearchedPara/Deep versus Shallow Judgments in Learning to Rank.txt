 Much research in learning to rank has been placed on devel-oping sophisticated learning methods, treating the training set as a given. However, the number of judgments in the training set directly affects the quality of the learned sys-tem. Given the expense of obtaining relevance judgments for constructing training data, one often has a limited bud-get in terms of how many judgments he can get. The major problem then is how to distribute this judgment effort across different queries. In this paper, we investigate the tradeoff between the number of queries and the number of judgments per query when training sets are constructed. In particular, we show that up to a limit, training sets with more queries but shallow (less) judgments per query are more cost ef-fective than training sets with less queries but deep (more) judgments per query.
 H3.3 [ Information Search and Retrieval ]: Retrieval Models Theory, Measurement, Experimentation Learning to Rank, Training, Relevance Judgments
Most algorithms for building search engines are based on learning to rank. Given a training set composed of different queries and documents judged for relevance for the given queries, machine learning methods are used to learn to rank the documents in correct order for the given query.
Much thought and research in learning to rank has been placed on feature extraction and the development of sophis-ticated learning algorithms. However, relatively little re-search has been conducted on the choice of queries and doc-uments for constructing training data.

Obtaining relevance judgments for learning is an expen-sive procedure. When constructing training data for learn-ing to rank, one is often faced with the problem of how to distribute the available judgment budget across differ-ent queries so that better performance is obtained. One major design decision when training sets are constructed is  X  X iven a fixed judgment budget, is it better to judge as many queries as possible, with fewer judgments per query (shallow judgments) or to judge fewer queries but more documents per query (deeper judgments)? X 
A similar such question was recently analyzed in context of evaluation in Million Query Track [3]. The results of the track suggest that when the goal is to evaluate the relative quality of search engines, up to a point, it is better to judge more queries with less judgments per query.

In this paper, we ask we ask a similar question for training purposes. Note that training is quite a different task than evaluation as the quality of a training set highly depends on its informativeness. Focusing on LambdaRank as the learning algorithm [2], we show that similar to evaluation, given a fixed judgment budget for building training sets, better test set performance is obtained when training sets with more queries but shallow judgments per query are used as opposed to training sets with less queries but deep (more) judgments per query.
In order to test the effect of varying the number of queries and the number of judgments per query in the quality of learning algorithms, we mainly focus on LambdaRank [2].
LambdaRank is an algorithm that is designed to opti-mize the nonsmooth evaluation metric NDCG [4]. To over-come the problem of nonsmoothness, it uses the approach of defining the gradient of the target evaluation metric only at the points needed. Given a pair of documents, the virtual gradients used in LambdaRank are obtained by scaling the RankNet [1] cost with the amount of change in the value of the metric obtained by swapping the two documents.
In order to test the effect of varying the number of queries vs. the number of judged documents per query, we use data obtained from a commercial search engine. The dataset con-tains 382 features and is split into train, validation and test sets with 2K, 1K, and 2K queries, respectively. The average number of judged documents in the training set is 350 per query, the number highly varying across different queries.
To test the effect of judging more queries vs. more docu-ments per query, we form different training sets by (1) sam-pling p % of queries, keeping the number of documents per query fixed to the maximum available and (2) sampling p % of documents per query, keeping the number of queries fixed. We then train the LambdaRank algorithm using these differ-ent training sets and compute the test set NDCG(10) value. In order to avoid the variance in results due to sampling, we repeat the procedure 10 times and report the average NDCG(10) value over 10 different samples.

Figure 1 shows the result of this experiment for different sampling percentages. The x axis in the figure shows the sampling percentage used. The line with the plus marks Figure 1: Test set NDCG(10), training on p % sam-pled training sets formed by sampling queries vs. sampling documents. corresponds to sampling queries and the dotted line corre-sponds to sampling documents. It can be seen that given a fixed sampling percentage (thus a fixed judgment budget), sampling queries for the training set results in worse perfor-mance than sampling documents per query. The difference in all cases is statistically significant with p = 0 . 05.
This result suggests that given limited number of judg-ments, it is better to judge more queries but less documents per query than less queries with more documents. Further-more, when sampling documents per query, test performance reaches its optimum at 50% of the judgement effort, suggest-ing that additional documents do not result in any additional improvement in the quality of the training set.

The experiments above did not have complete control over the total number of judgments in the training set as the number of judgments in the original training set varied per query (when queries are sampled, including queries with more judged documents in the training set (as opposed to queries with less documents) may result in larger sets). We would also like to compare the effect of the number of judg-ments per query and the number of queries on the quality of the training set. Also, even though the above results suggest that judging more queries with fewer documents is better, there should be a lower bound on the number of judgments per query.

In order to fix the total number of judgments in the train-ing set, we extracted 1K queries with 128 judgments each from the original training set. We form different sets by halving the number of queries in the training set, resulting in different training sets with 1024, 512, 256, 64, 32 and 16 queries, each containing 128 documents. Similarly, we also form different sets by halving the number of judgments per query (128, 64, 32, 16, 8, 4 and 2 documents), keeping the number of queries fixed (1K).

In Figure 2, we then report the test set NDCG(10) value using these different training sets. The x axis in the figure shows the total number of judged documents in the train-ing set (num-queries*num-docs per query). The line with the plus marks corresponds to halving the queries (having 128 judgments per query) and the line with the dotted line corresponds to halving the number of judged documents per query (keeping all the 1K queries). Next to each plus (or dot), we report the number of documents in the training set (or the number of queries in the training set).
 The results in this figure are consistent with the results in Figure 1: Given the same amount of training data, it is bet-ter to judge more queries with less documents per query. It Figure 2: Test set NDCG(10), training on sets of different sizes formed by halving number of queries vs. halving the number of judgments per query. can be seen that with as few as 16 documents per query, test set NDCG(10) values are comparable to using the entire 128 documents. However, decreasing the number of judged doc-uments further results in a sharp decrease in performance. The results suggest that the lower bound on the number of documents per query is 8, if one has to decrease the total number of judgments further, it is better to decrease the number of queries in the training set.

The results above are actually related to the informative-ness of the training set. Given some number of judged doc-uments per query, judging more documents for this query does not really add much information to the training set. However, including a new query is much more informative since the new query may have quite different properties than the queries that are already in the training set.
Given the expense of obtaining relevance judgments, we analyze the tradeoff between the number of queries and the number of judgments per query when training sets are con-structed. Focusing on the LambdaRank algorithm, we show that training sets with more queries but shallow judgments per query are more cost effective than training sets with less queries but deep judgments per query. [1] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [2] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to [3] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and [4] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods
