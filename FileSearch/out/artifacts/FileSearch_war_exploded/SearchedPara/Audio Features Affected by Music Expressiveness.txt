 Within a Music Information Retrieval perspective, the goal of the study presented here is to investigate the impact on sound features of the musician X  X  affective intention , namely when trying to intentionally convey emotional contents via expressiveness. A preliminary experiment has been per-formed involving 10 tuba players. The recordings have been analysed by extracting a variety of features, which have been subsequently evaluated by combining both classic and ma-chine learning statistical techniques. Results are reported and discussed.
The sound and music computing literature has investi-gated several approaches relevant for Music Information Re-trieval (MIR) tasks. A prominent one is the identification of emotions expressed by music so as to facilitate emotion-based music organization, indexing, and retrieval (see [1] for a discussion). Yet, when such perspective is embraced a de-ceptively simple question should be answered: which facets of music (such as: harmony, rhythm and timbre) and which combinations of these features can be effectively exploited for emotion identification?
In order to make a principled step in such direction, here we present a preliminary study on how audio features are likely to be modulated while musicians try to intentionally convey emotional contents through expressiveness.

Reviews made by [7] and [20] revealed that there is not a dominant feature among the others able to distinguish dif-ferent emotions, instead a large set of features is necessary to get some result. As to expressivenes, [10] found a set of relevant score-independent features (such as: Roughness , Attack time , Peak level and Notes per second ). In [4] a great variety of features is used to achieve emotion classifi-cation and are also related to which emotion they outline, e.g. Dynamics , Timbre and Articulation . Research in [19] was aimed at finding which aspects of the signal other than brightness and attack time were relevant in timbre-evoked emotions, and determined the relevance of odd-even ratio , a feature strictly related to the harmonic properties of the sig-nal. Eventually, [17] suggest the choice of tempo , dynamics , and articulation as key feature-classes for studying music ex-pressiveness, while demonstrating that musicians play in a different way whether they are trying to express an emotion rather than when feeling an emotion.

In the study reported here, as a novel contribution, we set up an experiment leading the musician to mostly ex-ploit on emotional contagion and auditory sensations rather than musical expectancy , episodic memories or others sub-jective phenomena. Then, we have exploited a mix of sta-tistical analysis ( ANalysis Of VAriance , ANOVA) and ma-chine learning techniques [2], such as dimensionality reduc-tion ( Principal Component Analysis , PCA) and automatic classification ( Support Vector Machines , SVM), to analyse the emotional content conveyed by the audio features.
The music ability to elicit emotions intrigues researchers since a long time [3]. Juslin and V  X  astfj  X  all reviewed several works related to this topic, and, most important, identified six mechanisms through which music can evoke emotions [6]: 1. Brain stem reflexes : Emotion is induced by sound itself because one or more acoustical characteristics of the music are taken by the brain stem to signal a potentially important event; reflexes account for the impact of auditory sensations 2. Evaluative conditioning : Listened music is repeatedly paired with other positive or negative stimuli. 3. Emotional contagion : The listener  X  X erceives X  the emo-tional expression of the music, and then  X  X imics X  this ex-pression internally, which leads to an induction of the same emotion by means of either peripheral feedback from mus-cles, or a more direct activation of the relevant emotional representations in the brain. 4. Visual imagery : The listener conjures up visual im-ages while listening to the music; experienced emotions are the result of a close interaction between the music and the images. 5. Episodic memory : The music evokes a memory of a particular event in the listener X  X  life. 6. Music expectancies : A specific feature violates, delays, or confirms the listener X  X  expectations about the continua-tion of the music.

It is clear that mechanisms 2, 4, 5 and 6 are strictly sub-jective, while 1 and 3 rely on some degree of objective (or at least shared) aspects.In particular, emotional contagion
Figure 1: The ad hoc composed music fragment is likely to be deeply rooted in simulation and motor-based mechanisms of emotion and thus relying on a shared man-ifold between subjects [18]. Under this rationale, 1 and 3 have been considered the mechanisms to be principally elicited in setting up the experiment.
Ten tuba players have been asked to play 7 times a mu-sic fragment composed ad hoc , each time trying to express one different emotion among the six basic emotions namely, anger, disgust, fear, happiness, sadness and surprise [5], plus a neutral performance. This approach is different from [15], since the mechanisms we are investigating are closer to low-level affect processing rather than that involved in music expectancies [6].

The players came from different geographical areas and cultural and musical backgrounds: a tuba teacher, a gradu-ated professional, a graduating student, a semi-pro, 3 young mid-level students and 3 amateurs. Each performer had suf-ficient skills to perform the melodic fragment: being able to express emotions regardless of the technical aspects was a basic condition in order to add expressiveness to the perfor-mance.

Materials: Instrument and Music Fragment . Ma-terials choice aimed at maximizing the influence of shared mechanisms ( brain stem reflexes and emotional contagion ) over acoustic outcomes with respect to other mechanisms (cfr. Section 2).

In such precise perspective, we chose the tuba since its sound is generated by the musician and then amplified, tuned and filtered by the instrument: this allows the artist to take control over many aspects of the timbre, enabling great ex-pressiveness. Further, the  X  embodied X  characteristics of how tuba sound is produced are intimately connected to low-level aspects of affect production Of course, many other brasses share the same advantages, but our knowledge of this in-strument allowed to conduct a sound analysis taking into account its potential and capabilities. What is important here is that brasses are instruments the most close to the human voice from both a technical and a timbral point of view.

A music fragment (shown in Figure 1) has been composed ad hoc . It presents no hints about known tunes, this should avoid emotions evoked by episodic memories and likely eval-uative conditionings . Moreover, it sounds more like a suc-cession of pitches rather than a melody , almost resembling a serial sequence: this should help avoiding any music ex-pectancy . Just a small variance in note durations has been introduced to let some room for the performer to add ex-pressiveness without stimulating much rhythmic expectan-cies. Note that the score has been written with neither key signatures nor time indications, dynamics and agogics. Per-formers are used to read scores containing tempo, phrasing and articulation signs; a score like this should guarantee more degrees of freedom to the interpreters.

Data acquisition. Once the performer was comfortable with the setup, 7 performances have been recorded: 6 while the musician tried to express basic emotions and a neutral one, which served as a baseline reference.

To prevent systematic bias in collecting data due to a common sequence pattern, the 6 basic emotions have been considered in random order. For each emotion the performer has been given some time to enter the right mood, and then the performance was recorded and listened back until the musician was satisfied of the outcome.

The neutral recording was the last task, made after a small debriefing phase and with a metronome to a fixed tempo of 85 BPM: the attention given to the metronome and to a rig-orous execution was meant to leave no space for emotions in the performance, thus flattening the expressiveness. To pre-vent influences between musicians, they were not allowed to listen to others X  performance before the experiment ended.
All recordings have been made in the same environment (a small concert hall), with the same equipment and with the same settings. The aim was limiting the influence of ambience on musicians while keeping the level differences consistent across the performances.

All signals have been recorded as mono PCM 44.100kHz 24bit files, the hardware and software tools used are: an AKG C 419 Clip Microphone for wind instruments; an M-AUDIO MobilePre USB sound card; a laptop PC running Cockos Reaper v4.77 x64 digital audio workstation. To pre-vent biases in the computation of some features, each recorded track has been manually processed in order to set leading and trailing silence to 0 . 5 seconds. Since the microphone was attached to the instrument, almost no reverberation was captured by the recordings (e.g. the ratio of dry vs. wet signal is very high).
Feature extraction has been performed within the Matlab environment, using both toolboxes (such as the MIRToolbox [9] and the Timbre Toolbox [11]) and custom algorithms, such as [13]. Table 1 shows the complete feature set together with the results obtained. Below we briefly discuss how each feature maps over the tuba performance. -Beats Per Minute (BPM) corresponds to the speed of the execution. -Root Mean Square (RMS) is a measure of the signal energy; here it reflects how loud the instrument has been played. -Low Energy (LOW) is the percentage of signal with a level below the average [16]. This is a good measure of the amount of amplitude modulation and legatos . -Attack leap (ATK) is a measure of the dynamic range of the attack phase of the sound [9]. This is influenced by tran-sients, staccatos and abrupt dynamic changes, in opposition to legatos and soft attacks. -Harmonic energy (HAE), Noise Energy (NOE), Noisiness (NSN) and Harmonic Spectral Deviation (HRD) measure the periodic vs. non-periodic component of the sound [11, 8]; in our context is intended to measure the balance between pitched notes and buzz. -Brightness (EBF) by and large measures the amount of high-frequencies in the signal. Many algorithms have been proposed to measure brightness, for our task the one de-scribed in [13] and implemented in [14] gives the best results. Many aspects of sound production may influence EBF. -Tristimulus (T1-3) , a concept borrowed from colour per-ception studies, is triple which in MIR is used for measuring the contribution of the fundamental, the second and third harmonic, and all the remaining components on the overall sound [12]. In our context, T1 and T3 can be thought as how many harmonics are excited. -Inharmonicity (INH) and Roughness (ROH) are re-spectively a measure of the partials that fall outside the harmonic series and a measure of the intrinsic dissonance of the sound [11]. They both measure harshness of sound and may be good indicators of sforzato , singing through the instrument or other peculiar techniques. -Odd-Even Ratio (OER) is the ratio between the energy of odd and even harmonics. Usually it depends only on the instrument, but it may also reflect the intention of the performer to obtain a softer or a bitter sound.

Folllowing [11], time-varying features are collapsed into scalar values through their medians (M) and interquartile ranges (IQR). Since a certain amount of subjectivity and cultural differences are present also in the contagion mech-anism, and since the technical skills of the performers were not as uniform as expected, all features were normalized by performer. This allowed to evaluate the relation among different emotions rather than the artist X  X  skills or instru-ment manufacture. The only feature that was considered by its non-normalized value is BPM (denoted BPM nn in Ta-ble 1), since it showed some predictive accuracy also in the non-normalized version.

ANOVA was performed on each feature to test statistical significance when distinguishing between emotions. All fea-tures can be considered statistically significant ( p &lt; 0 . 05), except for ATK IQR and HRD IQR , which were thus discarded from further analyses. We also analysed the feature space to test when and where features means were significantly dif-ferent among the emotions; in other words we investigated which classes were distinguished by each feature. The out-comes are graphically shown in Table 1, each dot meaning that the feature can help distinguishing the corresponding pair of classes. The table shows that each pair of emotions is distinguishable by at least one feature.

A redundancy analysis over the remaining 24 features was then performed via PCA [2], showing that 4 principal com-ponents (PCs) have an eigenvalue greater than one, explain-ing 82 . 1% of the variance. To explain more than 90%, 7 PCs are necessary. An ANOVA test run over all PCs revealed that PC 1-3 are useful to differentiate basic emotions, but none of the remaining are statistically significant. Moreover, always according to ANOVA, no PC can help distinguishing disgust vs. surprise . Feature coefficients related to PC shown in Table 1.

To assess the goodness of ANOVA and PCA results in terms of predictive accuracy, we automatically classified record-ings through an SVM [2], then we considered the F -Scores obtained from the confusion matrices generated according to a leave-one-out strategy [2] over the 70 available recordings. The test considered: the whole feature set (called 24F in Table 2), sets of relevant PCs (respectively 7PC, 4PC, and 3PC) and different subsets of features.

Clearly, according to ANOVA, we expect to see lower F -scores for emotions with less features binding them. As to PCA, a failure in isolating the right components will result in better F -Scores when using feature subset rather than PCs sets.

SVM behaved as expected for what concerns the 24F set, presenting smaller F -Scores for fear , disgust and surprise , while holding a greater accuracy in other classes, thus con-firming ANOVA results. SVM scores seem to be higher with reduced feature sets, suggesting the need of dimensionality reduction. Unfortunately PCA seems to fail to isolate the right components: 7F, 4F, and 3F present better results than 7PC, 4PC, and 3PC.
We outlined an experiment to investigate the effect of ex-pressiveness on audio features, where a set of musicians was asked to play a music fragment composed ad hoc while try-ing to express different emotions. Overall preliminary results look promising.

Recorded data have been processed by extracting 26 fea-tures subsequently analysed via ANOVA, PCA and an SVM classifier. The ANOVA test discarded 2 features and gave a broad view of which features can distinguish each pair of emotions. PCA on the one hand revealed the presence of redundancy but, on the other hand for what concerns our goals, failed in isolating the correct descriptors, probably due to the small size of our dataset (70 recordings and 24 features). Different dimensionality reduction techniques will be explored in future works. SVM classification confirmed the reliability of ANOVA results, in particular the confu-sion between some classes of emotions, which needs to be addressed to understand whether the cause of the clutter-ing is a matter of chosen features or a matter of eliciting mechanisms.
 In the experiment we considered a brass instrument (tuba). The rationale for this choice was motivated in Section 3. An issue to be further investigated is how the results translate to other classes of instruments such as stringed, keyboards and percussions. [1] M. Barthet, G. Fazekas, and M. Sandler.
 [2] C. M. Bishop. Pattern Recognition and Machine [3] M. Budd et al. Music and the emotions: The [4] T. Eerola, O. Lartillot, and P. Toiviainen. Prediction [5] P. Ekman and K. Scherer. Expression and the nature [6] P. N. Juslin and D. V  X  astfj  X  all. Emotional responses to [7] Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, [8] J. Krimphoff et al. Characterization of the timbre of [9] O. Lartillot, P. Toiviainen, and T. Eerola. A Matlab [10] L. Mion and G. D. Poli. Score-independent audio [11] G. Peeters, B. L. Giordano, P. Susini, N. Misdariis, [12] H. F. Pollard and E. V. Jansson. A tristimulus [13] G. Presti and D. Mauro. Continuous Brightness [14] G. Presti, D. Mauro, and G. Haus. TRAP: TRAnsient [15] E. Schubert. Update of the hevner adjective checklist. [16] G. Tzanetakis and P. Cook. Musical genre [17] A. G. Van Zijl, P. Toiviainen, O. Lartillot, and [18] J. Vitale, M.-A. Williams, B. Johnston, and [19] B. Wu, A. Horner, and C. Lee. Musical timbre and [20] Y.-H. Yang and H. H. Chen. Machine recognition of
