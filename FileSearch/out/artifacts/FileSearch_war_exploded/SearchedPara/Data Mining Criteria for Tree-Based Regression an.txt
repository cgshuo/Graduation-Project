
This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees. To this end, we pro-pose new splitting criteria for growing trees. Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit be-tween the left and the right side. By contrast, we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data, as opposed to model-hag all of the data equally well. The new criteria do not split based on a compromise between the left and the right bucket; they effectively pick the more interesting bucket and ignore the other. 
As expected, the result is often a simpler characterization of interesting subsets of the data. Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions. Surprisingly, it is a "flaw" that works to their advantage: The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges. This so-called "end-cut problem" leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees. 
CART, splitting criteria, Boston Housing data, Pima Indi-ans Diabetes data 
We assume familiarity with the basics of classification and regression trees. A standard reference is Breiman et aL [3], hereafter referred to as CART; a concise introduction can be *Technology Consultant tInstructor requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 
Figure 1: An artificial example of a simple and in-terpretable yet unbalanced tree 
In regression, the impurity measure for buckets is the vari-ance of the response values, and the splitting criterion is a comproml.qe between left and right side bucket in terms of a weighted average of the variances: many situations: in real data the dependence of the mean response on the predictor variables is often monotone; hence extreme response values are often found on the periphery of variable ranges, just the kind of situations to which the cri-teria for one-sided extremes would respond. Finally, recall that the present goal is not to achieve superior fit but en-hanced interpretability. 
We consider only the two-class situation and leave more than two classes as an open problem. The class labels are denoted 0 and 1. Given a split into left and right buckets, on the left and on the right, respectively. Here are some of the conventional measures of loss or impurity, expressed for the left bucket: * Misclassification rate: mln(p O, pl).. Implicitly one as-e Entropy: -p~ logp  X  -p~ logpL This can also be 
These impurity criteria for buckets are conventionally blended into compromise criteria for splits by forming weighted sum~ marginal probabilities of the left and the fight bucket given the mother bucket, the compromise takes this form: 
These impurity functions are the smaller the stronger the majority of either label is. Misclassification rate is problem-atic because it may lead to many indistinguishable splits, some of which may be intuitively more desirable than oth-ers. The problem is illustrated, for example, in (CART, p.96). One therefore uses entropy or the Gini index instead, both of which avoid the problem. CART uses the Gini index, while C4.5 Quinlan [7] and S-Plus (Statsci [9], or Venables and Ripley [10]) use entropy. For two classes there does not seem to exist a clear difference in performance between en-tropy and the Gini index. In the multi-class case, however, Breiman [2] has brought to light significant differences. 
We constructed several trees based on both CART and 1. CART, Figure 3 
Somewhat balanced tree of depth 6. The major vari-ables are RM (3x, = 3 times) and above all LSTAT (6x). Minor variables appearing once each are NOX, CRIM, 
B, PTRATIO, DIS, INDUS, with splits mostly in the expected directions. Two of the top splits act on the size of the homes (RM), and below it, especially for areas with smaller homes, an monotone decreasing de-pendence on the fraction of lower status population (LSTAT) is apparent. Except for the peeling on RM at the top and the subsequent peeling on LSTAT, the tree is not simple. 2. One-sided puri~y, Figure 4 Unbalanced tree of depth 9. The minor variable PTRA~ 
TIO (lx) is allowed the first split with a small bucket of size 9%; apparently a cluster of school districts has significantly worse pupil-to-teacher ratios than the ma-jority. Crime-infested neighborhoods are peeled off next in a small bucket of size 5% (CRIM, lx) with ex-tremely low mean. NOX makes surprisingly 3 appear-ances, which would have made Harrison and Rubinfeld happy. In the third split from the top, NOX breaks off 12% of highly polluted areas with a low bucket mean of 16, as compared to 25 for the rest. LSTAT (3x) cre-ates next a powerful split into buckets of size 41% and 34%, with means of 30 and 19, respectively. RM (2x) plays a role only in "high-status" neighborhoods. 3. One-sided extremes: high mean, Figure 5 
An extremely unbalanced tree. There are no single powerful splits, only peeling splits with small .buckets on one side. The repeated appearance of just two vari-ables, RM (2x, levels 1 and 3) and LSTAT (8x), how-ever, tells a powerful story: For highest housing prices (bucket mean 45), the size of homes (RM &gt; 7.59) is the only variable that matters. For RM &lt; 7.08, a persistent monotone decreasing dependence on LSTAT takes over, down to a median housing value of about 17. This simple interplay between RM and LSTAT lends striking interpretability to the tree and tells a simple but convincing story. At the bottom, crime (CRIM, 2x) and pollution (NOX, lx) show some re-maining smaller effects in the expected directions. 4. One-sided extremes: low mean, Figure 6 
Again an extremely unbalanced tree. It tells a simi-lar story as the previous one, but greater precision is 
The class labels of the Pima data are 1 for diabetes and 0 
We constructed four trees based on entropy and the new 1. Entropy, Figure ? 
Typical balanced tree of depth 6. The strongest vari-able is PLASMA (5x), which creates a very successful split at the top. BODY (3x) is the next important variable, but much less so, followed by PEDIGREE (3x) and AGE (2x). The class ratios in the terminal buckets range from 1.00:0.00 on the left to 0.16:0.84 on the right. All splits are in the expected direction. 
Overall, the tree is plausible but does not have a simple interpretation. 2. One-sided purity, Figure 8 
Extremely unbalanced tree of depth 12. In spite of the depth of the tree, its overall structure is simple: 
As the tree moves to the right, layers high in class 0 (no diabetes) are being shaved off, and, conversely, as the tree steps left, layers high in class 1 (diabetes) are shaved off (with the exception of the BP split near the bottom). The top of the tree is dominated by BODY and PLASMA, while AGE and PEDIGREE play a role in the lower parts of the tree, where the large rest bucket gets harder and harder to classify. 3. One-sided extremes: high class 0, Figure 9 
Extremely unbM~nced tree with simple structure: Be-cause the criterion searches for layers high in class 0 interpretable trees. dence. for quality of fit are not very informative. plicity of trees. 
