 Pavlos S. Efraimidis 1 Abstract We consider the problem of privacy leaks suffered by Internet users when they perform web searches, and propose a framework to mitigate them. In brief, given a  X  X ensitive X  search query, the objective of our work is to retrieve the target documents from a search engine without disclosing the actual query. Our approach, which builds upon and improves recent work on search privacy, approximates the target search results by replacing the private user query with a set of blurred or scrambled queries. The results of the scrambled queries are then used to cover the private user interest. We model the problem theoretically, define a set of privacy objectives with respect to web search and investigate the effectiveness of the proposed solution with a set of queries with privacy issues on a large web collection. Experiments show great improvements in retrieval effectiveness over a previously reported baseline in the literature. Furthermore, the methods are more versatile, predictably-behaved, applicable to a wider range of information needs, and the privacy they provide is more comprehensible to the end-user. Additionally, we investigate the perceived privacy via a user study, as well as, measure the system X  X  usefulness taking into account the trade off between retrieval effectiveness and pri-vacy. The practical feasibility of the methods is demonstrated in a field experiment, scrambling queries against a popular web search engine. The findings may have implications for other IR research areas, such as query expansion, query decomposition, and distributed retrieval. Keywords Query scrambler Search privacy Query-based document sampling Mutual information Set covering Inter-user agreement In 2006, AOL released query-log data containing about 21 million web queries collected from about 650,000 users over 3 months (Pass et al. 2006 ). To protect user privacy, each IP address had been replaced with a random ID. Soon after the release, the first  X  X nony-mous X  user had been identified from the data: the user assigned the ID 4417749 was identified as the 62-old Thelma (Barbaro and Zeller 2006 (accessed June 5, 2014)). In-terestingly, this identification was made solely on the queries attributed to her anonymous ID. Even though AOL withdrew the data a few days after the privacy breach, copies of the collection still circulate freely online. The incident only substantiated what was already known: web search can pose serious threats on the privacy of Internet users.
 The AOL personal data breach has motivated lots of research in web-log anonymization Carpineto and Romano ( 2013 ) and solutions using anonymized or encrypted connections, agents, obfuscating by random additional queries (Murugesan and Clifton 2009 ) or added keywords (Domingo-Ferrer et al. 2009 ), and other techniques (Shen et al. 2007 ). Another way to perform privacy-preserving searching is to employ cryptographic tools, as in Cao et al. ( 2014 ) or Boneh and Waters ( 2007 ). However, the cryptography-based techniques presented in these papers are not applicable in the context of this work, since they rely on collaborative search engines.

Popular search engines use query-logs to build user profiles, which can then be used to offer personalized search services to the users Hannak et al. ( 2013 ). However, the user profiles built by search engines can pose serious threats against the privacy of the users. Sometimes, it is even possible to infer personal information, for example certain demographic data, from seemingly irrelevant personal data of the users Bhagat et al. ( 2014 ). Therefore, many works in the literature focus on controlling the personal information that flows into the profiles built by search engines or on obfuscating these profiles by submitting fake queries.

A popular tool that permits users to obfuscate their web-search profile is the TrackMeNot add-on (Howe and Nissenbaum 2009 ) for the Firefox browser. TrackMeNot tries to obfus-cate the profile of a user by submitting some additional random queries. In this way, the real queries are hidden in a larger set, and the task of identifying the actual interests of the user is hindered to some extent. Another interesting add-on is OptimizeGoogle which, among other features, trims information leaking data from the interaction of a user with Google. An interesting combination of anonymization tools is employed in the Private Web Search tool queries are used to distort in a disciplined way the user profiles built by search engines.
A community-based approach to support user privacy in information retrieval is pre-sented in Domingo-Ferrer et al. ( 2009 ), where a user gets her query submitted by other users of a peer-to-peer community. The main idea is distort user profiles by having search queries submitted by other users. A similar approach is presented in Castella ` -Roca et al. ( 2009 ), and is further extended in Lindell and Waisbard ( 2010 ) to handle malicious adversaries.

Finally, there are search engines such as DuckDuckGo 1 and Ixquick 2 that state that they do not collect any personal user information, and some relay services like Startpage 3  X  X nd until recently Scroogle X  X hat submit search queries to Google on behalf of anonymous users. For a recent extensive review on the literature, we refer the reader to Arampatzis et al. ( 2013 ) and Peddinti and Saxena ( 2014 ).

There is an important reason why the above methods alone might be inadequate: in all cases, the query is revealed in its clear form. Thus, such approaches would not hide the existence of the interest at the search engine X  X  end or from any sites in the network path. In addition, using anonymization tools or encryption, the plausible deniability 4 towards the existence of a private search task at the user X  X  end is weakened. In other words, when a user employs the above technologies, the engine still knows that someone is looking for  X  X  X awyers for victims of child rape X  X , and the user cannot deny that she has a private search task which may be the aforementioned one. Additionally, there are cases of web searches where the search query should not be disclosed. For example, the search may concern some new research or business idea. Finally, for very sensitive web searches, avoiding the disclosure of the actual search query may be the only safe approach. Noteworthily, it has been shown very recently, that the use of anonymization tools like Tor 5 cannot always assure that a web search will not be associated with the user who submitted the query. More precisely, Peddinti and Saxena ( 2014 ) demonstrated that an adversarial search engine, equipped with only a short-term history of a users search queries, can break the privacy guarantees of Tor and TrackMeNot by only utilizing off-the-shelf machine learning techniques.

In this work, we consider how to perform web searches without disclosing the actual search query and the corresponding search interest of the user to any party, including the web search engine. We call this the Query Scrambling Problem (QSP). A potential solution to the QSP must work with existing web search engines, assure the privacy of the web search query (and not necessarily the profile of the user at the search engine), and at the same time manage to retrieve useful results (retrieval effectiveness). The retrieval effec-tiveness of any method proposed for the QSP is a fundamental requirement. By definition the ground truth are the results that a web search engine would return had the private search query been submitted to it. Thus, the scrambled or blurred or whatever queries replace the private search query must retrieve as many as possible of the target results of the private query.

A way to achieve the above X  X hich has been proven a rather ambitious goal X  X as introduced in Arampatzis et al. ( 2011 ), called query scrambler , and works as follows. Given a private query, generate a set of scrambled queries corresponding loosely to the interest, thus blurring the true intentions of the searcher. The set of scrambled queries is then submitted to an engine in order to obtain a set of result-lists called scrambled rankings . Given the scrambled rankings, it is attempted to reconstruct, at the searcher X  X  end, a ranking similar to the one that the private query would have produced, called target ranking . The process of reconstruction is called descrambling . The scrambler employed semantically more general queries for the private query, by using WordNet X  X  ontology. The key assumption was: the more general a concept is, the less private information it conveys.
A semantic approach to generate  X  X  X lurred X  X  queries is also presented in Sa  X  nchez et al. ( 2013 ). However, there the focus is on obfuscating in a controlled way the user profiles built by search engines. There is no requirement for retrieval effectiveness of the  X  X  X lurred X  X  queries, and consequently no experiments about the retrieval effectiveness are presented.
Dealing with the QSP problem, i.e., with how to find the target results for a particular search query without disclosing the query, is a plausible and interesting problem in the field of privacy-enhanced web searching. Nevertheless, we are not aware of related works beyond the above that address this problem or some close variation of it. Most related work either focuses on query-logs or on user profiles built by search engines. Approaches that focus on query-logs do not fit the QSP problem, since in the QSP the query should not be disclosed in the first place, and moreover, in these approaches there is usually no requirement for retrieval ef-fectiveness. In the case of user profiles, the common practice is to submit the search query either in a way that is not linkable to the user or hidden within a set of fake queries. Such approaches can be used to protect the user profile but generally do not fit the QSP problem, since the QSP requires that the search query must not be disclosed at all.

The main contributions of this work are the following. In contrast to the semantic framework used in previous work, we employ a purely statistical framework. Within this statistical framework, we define three comprehensive privacy objectives X  X ncluding the equivalent of the privacy objective introduced in Arampatzis et al. ( 2011 ). These objectives are used to define and quantify the privacy guarantees for a given web search task. All statistics needed for generating scrambled queries are estimated on a query-based document sample of the remote engine (Callan and Connell 2001 ); we also provide some empirical heuristics for taking better samples faster. Additionally, we investigate an alternative approach based on set covering, aiming to diversify the selection of scrambled queries in the sense that each one covers a different part of the private information need. Compared to the semantic approach, our methods are found to be significantly better in retrieval effectiveness, better defined, more versatile, predictably behaved, applicable to a wider range of information needs, and the privacy they provide is more comprehensible to the end-user. Furthermore, we conduct a user study which confirms that our word-frequency based criteria can provide strong indicators to what users perceive as private or  X  X afe X . Last, via a field experiment with a real web search engine, we identify some technical challenges in putting the proposed methods into operation.
The rest of this paper is structured as follows. In Sect. 2 we present our approach for privacy-enhanced web search. The creation of document samples of search engines is described in Sect. 3 , and the production of scrambled search queries in Sect. 4 . We evaluate our methods with an extensive set of experiments in Sect. 5 . Moreover, we present a set covering approach for query scrambling (Sect. 6 ), a user study on how actual users perceive the privacy concepts and criteria that we propose (Sect. 7 ), and conduct a field experiment of query scrambling on a real web search engine (Sect. 8 ). Conclusions are drawn in Sect. 9 . We assume an Internet user with an information need expressed as a query for a public web search engine like Google, Bing or Baidu. The retrieval task we focus on is document discovery, i.e. finding documents that fulfill the information need of the user.

The Query Scrambling Problem (QSP) (Arampatzis et al. 2011 ) for privacy-preserving web search is defined as: given a private query q for a web search, it is requested to obtain the related web documents as if q had been submitted to a search engine. To achieve this, it is allowed to interact with search engines, but without revealing q ; the query and the actual interest of the user must be protected. The engines cannot be assumed to be collaborative with respect to user privacy. Moreover, the amount of information disclosed in the process about q should be kept as low as possible. 2.1 Privacy in web search Given a private query q , we identify two types of privacy-sensitive resources:  X  The q itself representing the information need of the user. In this work, we use q and  X  The document set matching q , given by a public search engine. An adversary Some definitions are in order. Let N be the size of the document collection, H q the set of documents matching q , and df q  X j H q j the document frequency of q , i.e. the number of documents in the collection that q hits. Finally, let df w ; q  X j H w \ H q j , for any queries w and q . Let us, for now, assume that w and q are single-term queries, so H w and H q are determined simply by the document sets their terms occur in; in Sects. 4.1 and 4.2 we will see how we deal with multi-term queries.

We can now define two privacy primitives for web-search. The first one is based on the popular k-anonymity (Sweeney 2002 ), or k-indistinguishability , concept, which in the context of our work means that an adversary should not be able to come closer than a set of k possible alternatives to the private resource. Given q , for a candidate scrambled query w the first primitive k w is a privacy measure between the two queries based on the concept of k-indistinguishability of the results. In other words, k w gives the number of documents that each target document is hidden within, in the set of results hit by scrambled query w ; the larger its k w , the more private w is. Note, that k w is the inverse precision of the retrieval results of w with respect to the results of q . From a privacy perspective, when submitting w instead of q , each of q  X  X  target documents is  X  X idden X  within k w 1 other documents.

The second primitive g w is a measure of the generality of w . In other words, g w is the fraction of the collection that a scrambled query w hits. The rationale behind g w is that a general query can be assumed to be less exposing. As an indication of how general a query is, we use a pure statistical measure: The more documents of the collection a query hits, the more general the query is.
Based on the above primitives we define the following privacy objectives and present a use-case for each of them:  X  Anything-But-This privacy or ABT k : assume a researcher in academia or industry who  X  Relative-Generalization privacy or RG r : a person might be looking for information  X  Absolute-Generalization privacy or AG g : consider a citizen in some totalitarian regime. These three privacy types may be combined, if such a privacy request arises. Indeed, the query scrambling approach that we present, can support arbitrary combinations of the above criteria. Nevertheless, in the experimental evaluation we will focus on each privacy criterion independently, in order to be able to draw conclusions from the outcomes. Finally, note that, by their definition, the minimum RG privacy  X  RG 1  X  also assures the minimum ABT privacy  X  ABT 1  X  but not the other way around. 6
Clearly, in realistic settings, it is not feasible to calculate the exact values of the privacy measures defined above, since no one but the engine itself has access to its full collection. However, we can resort to estimating the needed quantities from a query-based document sample of the engine. 2.2 Overview of query scrambling Let us give an overview of our approach for query scrambling. First, we obtain a collection sample of size N with a query-based document sampling tool; this is done offline, however, the sample should be updated often enough to correspond to significant collection updates at the remote engine. In the online phase: 1. A private query q is decomposed into a set of scrambled queries. The scrambled 2. The scrambled queries are submitted as independent searches and all results are 3. The query q may be locally executed on the scrambled results (local re-indexing), or The tool we propose is intended to be used in the following way: a user can install it locally and then use it to scramble privacy-sensitive queries. It does not rely on some trusted third party for the scrambling process. However, the vendor of the tool may distribute document samples of popular search engines in frequent time intervals. Our approach requires a document sample of the remote search engine. Here, we will describe how we take such a sample, based on methods previously reported in the literature.
 We take a sample of the collection using random queries similarly to Callan and Connell ( 2001 ); Tigelaar and Hiemstra ( 2010 ). We have slightly modified the procedure in order to achieve higher efficiency (i.e. to increase speed and reduce network load by issuing fewer queries) and reach a  X  X icher X  sample (i.e. a sample consisting of a larger number of unique and non-unique terms). The modified procedure works as follows.

We bootstrap the procedure with the arbitrary initial query  X  X ww X . At each step, the procedure retrieves the first K results of the random query and adds them to the sample; we set K  X  1. The last two aforementioned works have shown that the choice of the initial query is not important and that K  X  1 is best suited for heterogeneous collections such as the web. Then, a term is uniformly selected from the unique terms of the current sample and used as the next random query until the desired sample size is reached. Here, we introduce the following additional restrictions over the standard procedure described in past literature: (1) candidate terms are at least three characters long and cannot be numbers, and (2) should have a document frequency of more than 1 in the current sample, as long as the sample contains at least two documents. Our experiments with sampling a few thou-sand documents from the ClueWeb09_B dataset 7 have shown that not using such a document frequency cutoff results to around 5 % of the random queries hitting previously retrieved documents. The cutoff removes this inefficiency by likely discarding misspellings and unique identifiers, and it moreover produces a  X  X icher X  sample faster by slightly re-ducing fixation on previously seen topics: the number of terms (both total and unique) per sample size is usually higher.
 Concerning the sample size, previous research, such as e.g. Callan and Connell ( 2001 ); Tigelaar and Hiemstra ( 2010 ), has shown that 200 X 500 documents, depending on collec-tion characteristics, are sufficient. Most previous research, however, evaluated samples for their vocabulary coverage and document frequency distribution with respect to the whole collection. Beyond these two features, we are also interested in the quality of term co-occurrence statistics, as it will become obvious in the following section. We are also interested in knowing whether the set covering methods we will apply using a sample (Sect. 6 ) transfer well to the whole collection. Consequently, we will mainly experiment with a sample of 5000 documents from our test collection, which X  X lthough it is by an order of magnitude larger than the size suggested by previous research X  X t may or may not be large enough for our purposes; we will empirically examine this by also experimenting with larger samples, e.g. 20,000. Even larger samples X 23,000 and 50,000 documents X  will be used when sampling a real web search engine in Sect. 8 . For generating scrambled queries, we follow a statistical approach using the local docu-ment sample of the remote search engine. So far, for simplifying the definition of the privacy objectives in Sect. 2.1 , we have assumed single-term private and scrambled queries. In the next two subsections, we will see how we can generalize the methods to work with multi-term private queries (Sect. 4.1 ) and multi-term scrambled queries (Sect. 4.2 ).

As soon as we generate a set of candidate scrambled queries, these are filtered for privacy according to the objectives defined in Sect. 2.1 . The remaining candidates are ranked according to their expected retrieval effectiveness, described in the third sub-section (Sect. 4.3 ), and the top-v scrambled queries are submitted to the remote engine. Algorithm 1 at the end of this section gives an overview pseudo-code for the generation method. 4.1 Dealing with multi-term private queries If q is a single-term query, then its document frequency df q can be determined directly from the document sample. The question is how to treat a multi-term q , or else, what the df q of such a query is and which subset of df q documents will be assumed as matching q so we can harvest from it related terms to be used as scrambled queries.

Given df q , the question of which subset of documents is matching q can be settled as: we rank the sample documents with respect to q using some best-match retrieval model and ORed q , and take the top-df q documents. Estimating df q can be seen as a typical rank-thresholding problem. Recent approaches on rank thresholding, such as Arampatzis et al. ( 2009 ), assume a binary document relevance to the query, fit a binary mixture of probability distributions on the total score distribution, and seek to draw a score/rank threshold that optimizes a given retrieval effectiveness measure. We instead use a simpler approach, which we describe next, that does not involve relevance or selecting a measure to optimize. This approach argues about reasonable upper and lower bounds for df q and uses those.
 The minimum number of documents hit by a query q can be found by submitting q in an ANDed fashion to the collection sample and count the number of results, enforcing a minimum of 1 for practical reasons; we will refer to this lower bound as aDF. Now, the maximum number of results an ANDed query can retrieve is min i df i , where i is a query term (i.e. the number of documents the query X  X  least frequent term hits); we will refer to this number as mDF. This maximum number of results in an ANDed query is achieved only when all other query terms co-occur in the documents hit by the query term with the least df; in any other case, less than mDF documents are hit by the ANDed query. The term with the least df is also the most informative: if we were to reduce a multi-term q to a single term, this is the term we would keep. In these respects, mDF gives some upper bound. Thus, we have aDF df q mDF.

While aDF may be too restrictive especially for a long q , mDF may be too  X  X oose X  especially if q does not contain a low frequency term. So, we employ and evaluate both aDF and mDF as estimators of df q . From a retrieval perspective, it is easier to create scrambled queries to retrieve smaller sets of documents, thus, using aDF makes the task easier than using mDF. From a privacy perspective, mDF is the largest df possible so it is safer. For example, let us consider the information need represented by the query  X  X  X ig bad wolf X  X . Using aDF will point to documents about the  X  X  X ittle Red Riding Hood X  X  fairy tale (correctly), while using mDF will point to all documents referring to wolves including the fairy tale. Since aDF X  X  target set is smaller, it can be easier retrieved by scrambled queries. But using mDF instead, corresponds to trying to hide all wolves (would provide stronger privacy). 4.2 Generating multi-term scrambled queries For single-term scrambled queries, df w can be determined directly from the document sample. However, we can also generate multi-word scrambled queries. The question is how to treat these, or else, what the df w of such a scrambled query is and which subset of df w sample documents will be assumed as occurring in.

From the documents matching q , we enrich the set of candidate scrambled single-term queries by using a sliding window of length W and generating all unique unordered combinations of 2 and 3 terms. We use a window instead of whole documents so as to limit the number of combinations; currently, we set W  X  16 which was shown in past literature to perform best in ensuring some relatedness between terms (Terra and Clarke 2003 ) (see also Sect. 4.3 ). We limit the scrambled query length to 3, since a typical web query is usually between 2 and 3 words, which also helps to keep the number of combinations practically manageable. In this procedure, we exclude all stopwords, using the list of stopwords from the Text Categorization Project 8 , but keep a stopword if it exists in q .
The document set hit by such a scrambled query is estimated similarly to the method of aDF described in Sect. 4.1 : the ORed scrambled query is submitted to the sample and the top-df w documents are considered matching, where df w is the number of documents matching the ANDed scrambled query. The choice of aDF over mDF is made purely on targeting the best privacy. aDF produces lower df w estimates than mDF, so these queries are less general and will be removed earlier as g increases. Also, using aDF implies that queries are more targeted, achieving higher precision, so they will be removed earlier as k increases. 4.3 Ranking scrambled queries After dropping candidate scrambled queries w that violate privacy criteria either on k w or g , the remaining queries are ranked according to their expected retrieval quality with respect to the document set matching the private query, i.e. the target set. For example, we can measure this quality in terms of precision and recall, and combine those in one number such as the F b -measure (Manning et al. 2008 ). Although F b seems like exactly what we need for our purpose, initial tests showed that it may not be the best choice for our task due to the difficult-to-quantify effects that a retrieval model X  X  weighting scheme has on the terms of the chosen scrambled queries. Furthermore, since it has not been commonly used before for detecting the best related terms, we also looked for alternatives.

Topically-related terms can be ranked via several methods; a common one is by computing pointwise mutual information (PMI) using large co-occurrence windows (Brown et al. 1992 ). For the task at hand, it is appropriate to consider whole documents as windows; PMI scores each w co-occurring with q as where P  X  q ; w  X  is the probability of q and w co-occurring in a document, and P  X  q  X  , P  X  w  X  , the probabilities of occurrence of q , w , in a document, respectively. Using a large corpus and human-oriented tests, Terra and Clarke ( 2003 ) did a comprehensive study of a dozen word similarity measures and co-occurrence estimates. From all combinations of estimates and measures, document retrieval with a maximum window of 16 words and PMI (run tagged as DR-PMI16 in the latter cited paper) performed best on average. 9 However, both document or windows-oriented approach for frequency estimates produced similar results on average.

Although PMI has been widely used in computational linguistics literature, classifica-tion, and elsewhere, it has a major drawback in our task. Removing constant factors from Eq. 3 , which do not affect the relative ranking of terms for a given q and collection, PMI terms with the same ratio but different frequencies of co-occurrence with the query (hits), although we would prefer terms with more hits to achieve recall so a lower volume of scrambled queries is used. This low-frequency bias may not be undesirable for some tasks (e.g. collocation extraction), but it is a drawback in our case due to our high precision and recall preference. A workaround is to use instead a normalized version of PMI such as NPMI (Bouma 2009 ), which divides PMI by log P  X  q ; w  X  , reducing some of the low frequency bias but not all. In any case, our task X  X hile related X  X s not exactly a linguistic similarity one, where PMI works well in finding synonyms for TOEFL synonym tests (Terra and Clarke 2003 ), or collocation identification, where NPMI works well (Bouma 2009 ).
 Our task seems more related to scoring features for feature selection in classification. Yang and Pedersen ( 1997 ) review feature selection methods and their impact on classifi-cation effectiveness. They find that PMI (which, confusingly, they refer to as just MI) is not competitive with other methods, and that the best methods are the v 2 -statistic and the expected mutual information (MI) (Manning et al. 2008 , Chapter 13.5.1, Equation 13.17) lected terms are intended to be used simultaneously in order to classify a new object. Here, we use selected terms as queries one by one in order to cover the target set of documents. Beyond query volume, other parameters such as the number of documents retrieved per related query and the cardinality of the target document set may impact the effectiveness of the procedure.

There are still other IR methods that may work in the task at hand, e.g. taking the centroid of the documents used to harvest scrambled queries and rank the scrambled queries according to their centroid weight. All in all, since our task is different than determining linguistic similarity or feature selection, it makes sense to evaluate some common term similarity measures and feature selection methods, as well as some un-common ones, in this context. In initial experiments, we compared PMI, NPMI, MI, F 1 , F 2 and the weight of the centroid (of idf-only weighted documents), and found that MI and centroid weight work best for the task of ranking scrambled queries. F b with b  X  2, i.e. weighing recall twice to precision, is slightly behind but competitive; the F-measure however requires an extra parameter  X  b  X  . NPMI works better than PMI, but both are left quite behind. We will not present these results for space reasons, and will stick with MI for the rest of the paper. In order to evaluate the effectiveness of the scrambler and how its retrieval quality trades off with scrambled query volume  X  v  X  and scrambling intensity ( k or g ) over the different privacy types (ABT/RG/AG) and scrambled query generation methods (aDF/mDF), we set up an offline experiment. For comparison purposes, we used the set-up described in Arampatzis et al. ( 2013 ). 5.1 Datasets, tools and methods The private query dataset is available online 10 and consists of 95 queries selected inde-pendently by four human subjects from various query-logs. The selection was based on the rather subjective criterion of: queries which may have required some degree of privacy. Table 8 presents a sample of the test queries. As a document collection, we used the ClueWeb09_B dataset consisting of the first 50 million English pages of the ClueWeb09 dataset. 11 The dataset was indexed with the Lemur Toolkit, Indri V5.2, using the default settings, except that we enabled the Krovetz stemmer. 12 We used the baseline language model for retrieval, also with the default smoothing rules and parameters. This index and retrieval model simulate the remote web search engine.
 We took a document sample of the  X  X emote X  collection with the method described in Sect. 3 . After initial experiments and according to the heuristic estimates of the necessary sample size made in Sect. 3 , we decided to use a sample of 5000 documents which provides a good compromise between effectiveness and practical feasibility. We used the same types of indexing and retrieval model for the sample as for the remote engine.
We targeted the top-50 documents of the remote engine. Since our document sample was much smaller than 1/50th of the remote collection, all target top-50 documents cor-responded to less than 1 document in the sample. In this respect, in order to improve the focus of the scrambled queries, it makes sense to harvest those from a set of sample documents of a smaller cardinality than df q . In initial experiments we found that a good compromise between focus and reasonably good statistics of document frequencies is to take the top-t q sample documents returned by q , where t q  X  min f 10 ; df q g , i.e. we harvested scrambled queries from the at most top-10 sample documents. Similarly, we defined t w and t q ; w for the new set and calculated MI using these numbers instead; this was found to improve retrieval effectiveness. Of course, the privacy constraints were applied to the unmodified frequencies as described in Sect. 2.1 .

Concerning the evaluation measures, we simplified the matters in relation to Arampatzis et al. ( 2013 ) where scrambled rankings were fused via several combination methods and the fused ranking was evaluated against the target one via Kendall X  X  s and a set intersection metric. The fusion methods tried in the previous study were deemed weak in comparison to a local re-indexing approach, i.e. index locally the union of top-1000 documents retrieved by all scrambled queries and run the private query against the local index in order to re-construct the target ranking. Nevertheless, even with local re-indexing the ceiling of achievable performance was not reached: there were quite a few target documents retrieved by scrambled queries that could not be locally ranked in the top-50. This was attributed to having biased DF statistics in the local index. The experimental effort in the aforemen-tioned study concluded with a bare experiment evaluating only the number of target top-50 documents found by the union of the top-1000 documents retrieved by all scrambled queries. This allowed to remove the effect of de-scrambling and evaluate only the quality of scrambling; this is what we will also do.

Furthermore, in this paper, we report results as fractions of the top-K target documents found rather than absolute numbers of documents. Using fractions makes it easier to directly compare results across experiments targeting different numbers of top documents, i.e. using different values for K . 5.2 Results The two left-most columns of Table 1 , marked as  X  X nfiltered X , show results with no pri-vacy; these can be considered as the ceiling of achievable performance when de-com-posing a user query q with the current methods. Even with no privacy, we do not get 50 out of 50 target documents because there are cases where we cannot exactly reproduce q from the sample for the following reasons. First, a term of q may not occur in the sample, e.g.  X  X hamblee X  from  X  X  X efinition of chamblee cancer X  X . However, such a term may occur in the remote collection. Second, the terms of a multi-term q , e.g.  X  X efinition X ,  X  X hamblee X , and  X  X ancer X , may not occur within a window of 16 terms in sample documents. Third, we generate scrambled queries only up to three terms; obviously, longer private queries cannot be re-produced. Nevertheless, we do not consider all these as problems of the method, since they are introduced by technical choices we made in order to speed up the task. Only the first reason produces some uncertainty, although the problem may be eased by simply using larger samples. The other two can be completely removed by using no windows at all but whole documents, and generate longer scrambled queries.

Table 1 also shows results for ABT privacy. The minimum privacy  X  k  X  1  X  removes only scrambled queries which occur in all documents of the sample target set. This has a larger impact to a single-term q which may loose its 50 out 50 effectiveness. The table also shows that for light or no privacy requirements mDF works better than aDF; this happens because the sample target set of mDF is larger than this of aDF, so more scrambled queries are harvested/generated leading to better results. However, the effectiveness of mDF de-grades faster than aDF as k increases, so aDF works better, as expected and explained in Sect. 4.1 . For large k (e.g. for k 2), the effectiveness of mDF roughly halves for every doubling of k , suggesting a linear relation in log-log space or a power-law. The effec-tiveness of aDF degrades slower.

Tables 2 and 3 show results for RG and AG privacy respectively. Using mDF, RG ef-fectiveness roughly halves for every doubling of generalization, suggesting again a power-law. Concerning AG privacy, the g values shown correspond to document frequency cut-offs of 32, 64, 128 and 256 in the current sample size of 5000 documents. If a private query is already general enough for a g value, it is not scrambled since it already complies with the privacy requirements. Such queries are excluded from the average results of Table 3 . The numbers of private queries scrambled X  X hus contributing in the results X  X er g value and choice of aDF/mDF are shown in the last row (# q ). The effectiveness of mDF is similar for the first three small g cut-offs but then falls off. In other words, we can generalize private queries relatively well by using scrambled queries hitting up to roughly 2.5 %  X  g  X  : 0256  X  of the sample documents. At such an AG level, 66 % (63 out of 95) of the private query dataset is deemed as not general enough so it is scrambled. Again, the aDF method is much better than mDF in all cases, providing a less steep decrease in effectiveness as generalization increases.
The fact that aDF is more effective than mDF in all privacy types when more than light privacy is required, does not mean that it should be the preferred method. As we noted in Sect. 4.1 , mDF represents stricter privacy than aDF which is experimentally proved to trade off with retrieval effectiveness. The final choice between aDF/mDF should be left to the end-user or determined via a user-study.

Concerning scrambled query volume, in all privacy types and methods effectiveness increases with higher volumes. However, due to the nature of the experimental setup, we see diminishing returns as effectiveness gets closer to 100 %. At high privacy levels where effectiveness suffers, we can see roughly a doubling of effectiveness for every five-fold increase in volume, i.e. another power-law albeit a very steep one, suggesting that hun-dreds or even thousands of scrambled queries may be needed for getting close to 100 % effectiveness. 5.3 A comparison to semantic query scrambling The previous literature dealt only with RG privacy, so we will compare our RG method and results to it. The best effectiveness reported by Arampatzis et al. ( 2013 ) is 12.7 out of 50, i.e. 0.254, obtained at low volume (i.e. as many scrambled queries as can be produced up to ten) and low scrambling (in the next paragraph there is an explanation of the classification into low/medium/high scrambling of the semantic approach) by averaging the results for 94 of the 95 user queries. One query did not produce any scrambled queries at low scrambling. At higher volume, ironically, effectiveness slightly decreased, an effect we attribute to averaging only the 55 user queries having numbers of low-scrambled queries in the 26 X 50 range. Effectiveness decreased fast X  X elow ten and even five docu-ments X  X t medium or high scrambling.

The most obvious problems of the semantic approach are the following. First, not all user queries can be scrambled at a requested scrambli ng intensity, due to WordNet X  X  ontology being generic thus not  X  X ense X  enough. The problem seems severe: at high scrambling, only 58 out of the 95 user queries had at least 1 scrambled query. Second, the levels of low/medium/high scrambling were defined by taking arbitrary ranges of values of some semantic similarity measure between each scrambled query and q . Thus, scrambling intensity is difficult to be explained to the end-user: how much exposing is a scrambled query with, say, 0.8 similarity to q ?
Our statistical approach does not have the problems of the semantic one. First, we always seem to produce enough scrambled queries. This may not be the case for very small document samples, but it does hold for our X  X easonably small X 5000 sample. Second, our approach to RG can easier be explained to the end-user: the information need expressed by a scrambled query is satisfied by at least r times more documents than her private query. This can give her a better idea on how much she is exposed, in contrast to giving her a raw similarity threshold as in the semantic approach.

Moreover, we seem to get much better effectiveness. Although the levels of privacy or generalization are not absolutely identical due to the arbitrary definitions of low/medium/ high scrambling of the semantic approach, comparing the methods at minimum scrambling (i.e. low scrambling vs. g  X  g q ) at volume 10 we see improvements of ? 145 % or ? 67 % (out of 50 target documents, 12.7 found in Arampatzis et al. ( 2013 ) versus 31.1 with aDF or 21.2 with mDF found according to the results in Table 2 multiplied by 50). Never-theless, we should investigate which levels of privacy are roughly comparable across the two approaches.

Let us attempt a comparison of RG at the minimum level, as well as, at levels of the statistical approach which result to around 12.7 target documents on average for volume 10, according to Table 2 . For the private user query  X  X  X un racks X  X , Table 4 compares the scrambled queries resulting from the semantic approach (the two left-most columns of Table 4 are taken from a similar table in Arampatzis et al. ( 2013 )) against the scrambled queries of the statistical approach. The semantic approach is capable of generating only seven scrambled queries at low scrambling but ten at medium scrambling. None of the scrambled queries hit any of the target documents at any scrambling intensity. A bold number next to a query is the number of target results hit (if any), while the last row shows the number of distinct target results hit by all scrambled queries per column. The statistical approach achieves good results (above the 12 : 7 average) in two out of three cases. Nev-ertheless, it seems difficult to decide where the methods stand privacy-wise: is  X  X  X eapon have the last word on this by reviewing the set of scrambled queries before submission.
All in all, using the strictest privacy provided by mDF, we roughly matched or improved the best retrieval result of the semantic approach, for k up to 4 and g up to 2 g q or .0256 at volume 10, and for k up to 8 and g up to 4 g q or .0512 at volume 50. At lighter privacy requirements, we outperformed the semantic approach by far. In all cases, our methods managed to scramble all private queries where this was needed, in contrast to the semantic approach. Moreover, we detected power-law relations between the privacy levels and retrieval effectiveness of ABT and AG, as well as, between volume and retrieval effec-tiveness. Thus, our methods are more well-defined and easier explained to the end-user, can be applied to a wider-range of private information needs, are more effective and behave predictably, retrieval-wise.
 Last, there are two other advantages of our current approach over the semantic one. First, in the semantic approach the user had to manually select the part-of-speech and sense of every term in her query in order to select the right node in WordNet X  X  ontology. The statistical approach does not require these time-consuming steps. Second, in Arampatzis et al. ( 2013 ) we arrived at the conclusion that the best method to de-scramble ranked-lists is to locally re-index the union of documents hit by all scrambled queries and run q against this local index. Nevertheless, even with local re-indexing the ceiling of achievable per-formance was not reached: there were quite a few target documents retrieved by scrambled queries that could not be locally ranked in the top-50. This was attributed to having biased DF statistics in the local index due to the fact that the local documents represented a far from uniform collection sample: they were all retrieved by a set of semantically-related scrambled queries. The document sample used by our approach is more representative of the remote collection, so its DF statistics can be used in the local re-indexing approach removing most of the bias. After having established the feasibility of our query scrambling approach and the improved results with respect to previous work, we proceed with an independent experiment that extends our work and shows strong evidence about the possibilities of further optimization. In particular, first we see the problem of ranking candidate scrambled queries as a set covering problem (Sect. 6.1 ). Then, in Sect. 6.2 , we present an approach based on expo-nential weights to guide the covering algorithm of the target results and show further improvements in retrieval effectiveness. 6.1 Algorithmic foundations of query scrambling From an algorithmic point of view, the query scrambling approach is a set covering problem (Caprara et al. 1998 ); there is a set of documents, the target results of the private query are a specific subset of this set, and one tries to  X  X over X  all the target results by retrieving sets of scrambled results. The definition of the original set covering problem follows.
 Definition 1 (Set Covering) Given a collection S of subsets of a finite set B, a cover of B is a subset C of S, such that the union of the sets in C is equal to B that is, each item of B is covered.

In the context of this work, B is the set of the results to the user query, S is the collection of the results to scrambled queries (one set of results for each scrambled query) and C S is a set of scrambled results that cover the set B.

Set covering is a fundamental combinatorial problem and one of the first to be shown to be NP-complete (Karp 1972 ). There are various extensions of the original set covering problem. One of them is particularly interesting for this work and is called red-blue set covering (Carr et al. 2000 ). In red-blue set covering the items are distinguished into red and blue and the objective is to find a cover of the blue items which minimizes the number of red items covered. The formal definition of red-blue covering follows.
 Definition 2 (Red-blue set covering) Given a finite set of  X  X ed X  elements R, a finite set of  X  X lue X  elements B and a family S 2 R [ B , the red-blue set cover problem is to find a subfamily C S which covers all blue elements, but which covers the minimum possible number of red elements.

In the context of query scrambling, the blue elements would be the target results, while the red elements would be the rest of the documents in the document collection. However, the objectives of query scrambling are more subtle and cannot be completely captured with red-blue set covering. For example, a perfect solution for red-blue set covering without any red elements (if possible), would not be very attractive for query scrambling, because the target results of the seed query would be identifiable and this would reveal important information about the private query.

For the needs of query scrambling we define scrambled set covering, a multi-objective extension of set covering (Fig. 1 ).
 Definition 3 Scrambled Set Covering SSC  X  v ; k ; g  X  : Given a finite universe U of all documents of a collection, a partition of U into sets H q and U H q , and a collection S of subsets of U , the requirement is to find a subset C of S to satisfy the following objectives and/or constraints: i. maximize  X  ii. j C j v , where v is the maximum number of scrambled queries, iii. for each H w 2 C , the corresponding scrambled query w must satisfy k w [ k , iv. for each H w 2 C , the corresponding scrambled query w must satisfy g w [ g .
Note that the last definition supports both primitive privacy criteria, k -anonymity and generality g , and any combination of them. Consequently, any query scrambling task with one or more of the criteria ABT, RG and AG can be formulated as an instance of SSC  X  v ; k ; g  X  .

In this work, we supply bounds on the size of C, the index k and the index g, and optimize the coverage of the items in H q . One may define other variations of SSC by switching the objective function with one of the constraints or defining an objective function comprising more than one constraints, for example minimizing a weighted sum of k and g. Moreover, scrambled set covering can be extended with further objectives, like the maximum number of times any item of H q or any item of U H q can appear in the cover, etc.

The computational complexity of SSC  X  v ; k ; g  X  can easily be shown to be NP-Complete by reducing it to the original set covering problem. Moreover, the approximability of scrambled set covering with respect to objective (ii) cannot better than log n where n is the number of the target results, since this bound holds for the original set covering problem (Lund and Yannakakis 1994 ). The results on the approximability of red-blue set covering presented in Carr et al. ( 2000 ) do not fit the SSC problem. 6.2 A guided covering approach We use an approach based on set covering to guide the selection of the scrambled queries. Since the scrambled set covering problem (Definition 3 ) is NP-Complete, we will not try to solve it optimally. Instead, we will devise a polynomial time greedy algorithm as a heuristic for the query scrambling task. Note that greedy algorithms are a popular way to deal with set covering problems (Chvatal 1979 ; Young 2008 ). Given a private query q , let t  X  min f 10 ; df q g . Moreover, let T q be a set of the top t q documents that q hits in the sample collection. Instead of choosing scrambled queries based only on the MI criterion, we will choose them incrementally while keeping track if and how many times each document in T q has been hit so far.

We present the following greedy algorithm with exponential weights for covering the documents in T q . Each item i in T q has an initial weight b i of one, and this weight is multiplied by f  X  0 : 5 every time the document is covered by a scrambled query that is selected. In each round, a score for each candidate scrambled query is calculated. The scrambled query with the best score is selected and the weights of all items in T q are updated. The process is repeated until t scrambled queries are selected.

More precisely, let t q and T q be as defined above. Let C q be the set with the incremental outcome of the algorithm. Initially, C q  X ; . Let S q be the set of candidate scrambled queries for q . We will run experiments for volume sizes t  X  2, 10, and 50, as in the previous sections. We bound the size of S q by n  X  2 v 2 max  X  1, where v max is the maximum volume size. This size assures a sufficiently large pool of documents for the covering algorithm, while keeping the computational complexity of the algorithm under control. 13 Since in this set of experiments v max is 50, the maximum size of S q is 2 50 2  X  1  X  5001 scrambled queries achieving the highest MI scores (see Sect. 4.3 ). The precision and the recall of each scrambled query are estimated with the documents frequencies (df q ,df w and df q ; w ). The initial weight of each document i in T q is b i  X  1, and the discount factor is f  X  0 : 5. 1. The following steps 2 and 3 are repeated t q times. 2. For each scrambled query j in S q n C q , (that is, the set difference j 2 S q and j 6 2 C q ), 3. The scrambled query j 2 S q n C q that achieves the highest score r j is added to C q . 4. For each document i hit by the scrambled query j , update b i  X  f b i .
 The score r j as defined in Eq. 4 weighs two criteria for the incremental selection process; the retrieval effectiveness of each potential scrambled query and the current coverage level of each of the documents that have to be covered. A scrambled query with a high precision or high recall is amplified with respect to other candidate scrambled queries. This heuristic achieved the best results in our tests. The overall selection procedure is a greedy set covering algorithm adapted to the needs of our query scrambling approach.

We performed a large set of experiments, which showed that the retrieval results are improved with this guided covering algorithm. The improvement is more evident for larger sample sizes. This is a rather expected outcome, since larger samples can offer more accurate statistics about the underlying collection. We do not present the results of all the above-mentioned experiments here for space reasons; we just present some results with a larger sample size and smaller numbers of retrieved and targeted documents, as an indi-cation of how the methods work for different parameters.

In Tables 5 , 6 , and 7 we present the comparative results of the MI-based approach (basic) and the covering-based approach when a 20,000 document sample is used. In these experiments, we evaluated the number of target top-10 documents found in the union of the top-100 documents retrieved by all scrambled queries. The results show consistent im-provements in almost all cases (italicized) of about 3 : 1 % on average (3 : 8 % for ABT, 2 : 9 % for RG, and 2 : 3 % for AG), where each improvement is calculated as the average difference between the effectiveness of the covering-based algorithm and the basic algorithm.

The results of the basic method presented in Tables 5 , 6 , and 7 are generally slightly worse than the results presented in Tables 1 , 2 , and 3 . Since background experiments have shown that increasing the sample size improves effectiveness, we are inclined to blame the worse performance in the latter three tables on the more challenging task of targeting the top-10 instead of the top-50, in percentage/fraction terms, in combination with retrieving only the top-100 documents per scrambled query instead of the top-1000. However, we find the setup used in this Section more realistic for web retrieval (where the first page of top-10 results matters most) and it is more efficient (faster).

We would like to note that in the set covering-based approach that we presented, the selection priority of the scrambled queries is a combination of the covering criterion and the statistical MI criterion. Consequently, the concept of set covering is used as an add-on concept X  X  that the statistical query scrambling approach can be further enhanced with algorithmic techniques. However, the idea to exploit the combinatorial nature of the underlying algorithmic problem in order to guide the scrambled query selection process needs further investigation, which is outside of the scope of this work, and may be part of our future work. Our privacy criteria are based on statistical considerations of term co-occurrence. In order to investigate the privacy level that the end-users actually perceive, we conducted a user study.
 We selected a subset of ten private queries from our experimental setup, reported in Table 8 , trying to cover diverse categories from the full 95 query set. For those, we generated scrambled queries using a 20,000 documents sample of ClueWeb09_B. We assumed a web-search setup and targeted the top-10 documents in ClueWeb09_B, re-trieving the top-100 per submitted scrambled query. The fraction of the target documents found by the union of results of the best 2 or 10 scrambled queries (according to MI) is shown in the top part of Tables 9 and 10 (row labeled  X  X etrieval X ), for the two privacy criteria and several privacy levels and methods. The results seem generally better that the corresponding results of Sect. 5 , but they are not directly comparable since here we use a larger sample, only 10 private queries, retrieve the top-100 per scrambled query (not the top-1000) and target only the top-10 (not the top-50) results of the remote engine. Thirty 3rd-year students at the Electrical &amp; Computer Engineering department of Democritus University of Thrace, Greece, participated in the experiment. They were asked to imagine having an information need expressed by each of those queries at a time (together with their categories) and that they want to keep this need private. Then, they were shown a list of scrambled queries and asked to mark those queries that expose their specific need (ABT privacy), and those queries that expose their specific need or expose them in another similar or worse way (RG privacy). The keywords in multi-term scrambled queries were sorted in a decreasing document frequency (i.e. most to least general), trying to achieve a more  X  X atural X  query look (think, e.g., of  X  X  X ad wolf X  X  vs.  X  X  X olf bad X  X ).
We did not evaluate AG privacy; in preliminary tests, we found it especially difficult to formulate the right question to the users. AG privacy is not directly connected to the information need but rather to environmental (e.g. societal) and/or cultural factors affecting the user. It seemed that AG privacy could better be used in an interactive environment where users turn the  X  X nob X  until they deem the candidate scrambled queries as  X  X afe X  with respect to their own, possibly completely different, reasons. A user study of AG privacy would have required a completely different setting.

The middle part of Tables 9 and 10 (row labeled  X  X rivacy X ) shows the fraction of the submitted scrambled queries that were deemed  X  X afe X  by users. The trade-off between retrieval effectiveness and perceived/achieved privacy is now obvious. At the largest values of the privacy levels shown, we get perceived privacy of 88.5 X 99.2 %. in this respect, we investigated a good range of parameter values. The numbers also confirm that mDF indeed provides stricter privacy than aDF.

Operationally, the usefulness of such a query scrambling system should be measured by both the privacy and retrieval quality it achieves. Best retrieval with no privacy or full privacy with no target results both defeat the purpose. In this respect, we can provide a measure of usefulness by averaging retrieval and privacy effectiveness; the low part of the tables shows their harmonic average. The harmonic average is biased to the smaller of retrieval and privacy; it is more appropriate than their arithmetic average since it captures better the fact that having one without the other defeats the purpose. Roughly, for ABT, usefulness peaks at around k  X  2 for both aDF and mDF; but with aDF the system seems usable in a wider k -range from k  X  1upto k  X  16. Similarly for RG, with aDF the system is usable in a wider g -range than with mDF, and usefulness seem to peak between 2 g q and 4 g q .

In any case, the perceived privacy is less than 100 % for many of the private infor-mation needs we experimented with, suggesting that such a tool should, at least in some cases, not be run in a fully automatic way. To assure privacy or at least to keep privacy leakage under control at the user side, sets of candidate scrambled queries could be first presented to the user and some of the suggested queries may have to be discarded manually. Such a tool, nevertheless, shows a great potential in pre-selecting good scrambled query sets, minimizing the user effort. Although our privacy criteria are simple and word-frequency based, this study shows that they are strong indicators of what users perceive as private.
 7.1 Inter-user agreement Table 11 shows the total number of scrambled queries evaluated by each user for all ten private information needs and the inter-user agreement on the  X  X afety X  of scrambled queries, per privacy type and method. While the observed agreements are strong (75.3 X  93.3 %), the expected by-chance agreements are also strong (58.6 X 91.0 %) revealing that the rating distributions are skewed: the largest percentage of scrambled queries are deemed as private, from the viewpoint of each user. On the one hand, this means that our methods produce mostly  X  X afe X  scrambled queries, especially in the case of RG/mDF (very few scrambled queries are marked as  X  X nsafe X  per user since the expected agreement is 91.0 %) as it was expected by its high privacy seen in Table 10 . On the other hand, Fleiss X  j (Fleiss 1971 ) and average pairwise Cohen X  X  j (Cohen 1960 ) show weaker agreements; both these measures correct agreement rates for the rate of chance agreement.

Common guidelines used in the literature for j values characterize 0.21 X 0.40 as  X  X air X  and 0.41-0.60 as  X  X oderate X  agreement. Such guidelines are however by no means uni-versally accepted and they are rather based on personal opinion. Under these common guidelines, we get a moderate agreement for ABT privacy and a fair agreement for RG privacy. In this respect, RG privacy seems like a more  X  X ersonal X  matter than ABT privacy. Concerning the methods, there is no consistent pattern: mDF produces higher agreement in ABT privacy, while aDF produces higher agreement in RG privacy.

Manning et al. ( 2008 ) suggest that  X  X  X greement below 0.67 is seen as data providing a dubious basis for an evaluation [of relevance], though the precise cutoffs depend on the purposes for which the data will be used. X  X  Inter-judge agreement of relevance has been measured within the TREC evaluations and for medical IR collections: the level of agreement normally falls in the range of 0.67 X 0.8 and characterized as  X  X air X . In any case, all our privacy agreements X  X s measured with two versions of j here X  X all in the range of 0.228 X 0.559, thus they are weaker than a typical relevance agreement. Our weaker agreements may be attributed to the notion of privacy being more subjective than relevance. In this section, we investigate the applicability of our query scrambling approach on real web search engines. We apply and evaluate query scrambling against the popular web search engine of Google.

In our approach for query scrambling, a prospective user has to build a sample col-lection of the documents indexed by a search engine, generate scrambled queries for each private query, submit them in a privacy-preserving way to the search engine, and, finally, collect the scrambled results and identify the target results in them. We are well aware that the above steps are not trivial.

One may dismiss this as an unrealistic approach and contend that we cannot expect users to perform all the above steps. However, the protection of user privacy for certain web searches can be of crucial importance for privacy, economic, political, or democratic reasons, and users may take the cost to use query scrambling, if they have the option to do it. Query scrambling is not intended to displace regular web searches; it should only be applied selectively for really sensitive web searches. In this case, the overhead for the user should be well-justified.

In any case, all the steps required can be packaged in an application and performed automatically. Furthermore, document samples of search engines may be centrally dis-tributed, often enough (similarly to signature databases for anti-virus applications), by a provider/vendor who does not have to be necessarily trusted. Updated samples also solve the problem of non-static document collections indexed by search engines. In this sense, our solution is feasible and practical, since a single user with commodity hardware re-sources can use it; there are no assumptions about special search engines or trusted parties.
One may also doubt the feasibility of the approach when using a real web search engine like Google or Bing and that it will retrieve any useful results. To investigate this issue, we built some prototype components and conducted a field experiment of query scrambling on a real web search engine. We chose Google for the experiment, but other web search engines like Bing or Baidu could also be used.
 First, we created two query-based samples of the search engine X  X  document collection. We used the sampling algorithm described in Sect. 3 . We built two samples, one of 50K documents and one of about half the size, 23K documents. The small sample is a snapshot taken during the construction of the large one.

To significantly reduce the number of queries submitted to the search engine, a set of 12 private queries was chosen, the number of target results per private query and the number of retrieved results per scrambled query were reduced to 10 and 100, respectively. Finally, we run experiments for scrambled query volume t  X  50 and privacy criterion ABT with k  X  1 using mDF. For this experiment, we chose the minimum privacy in order to in-vestigate the retrieval effectiveness of our approach; additionally, we were more interested in the feasibility of the infrastructure needed and indicative time consumption. The out-come, however, was better than expected, which clearly calls for further experiments with higher privacy guarantees.

The field experiment was executed mainly with the same algorithms and tools that we used in our main experiments in Sect. 5 . The first challenge was to submit the scrambled queries in such a way that the queries are unlinkable to each other, i.e., it should not be possible for the search engine to find out which scrambled queries correspond to the same private information need. The second challenge was that we had to retrench our query submissions in order to avoid excessive load on commercial search engines and the risk of our clients being locked out.

There are several ways to deal with these challenges, like for example using the Tor anonymity network, using anonymity proxies, or some crowd-based approach, and none of them was trivial. We opted for a crowd-sourcing approach where a set of volunteers agreed to submit queries on behalf of our query scrambling coordinator. Each volunteer client simply installed a dedicated Chrome extension which took over the interaction with the coordinating server of the experiment.
We managed to build a community of about 30 clients/volunteers, with each client submitting a query every few minutes (while the client was up). The samples were built during November and December 2012, and the private queries were issued in December 2012. The same crowd-sourcing platform was used for both tasks, creating the samples and running the scrambled queries. More precisely, it took about 6 weeks to create both samples, and about a week to run the scrambled queries. The total number of  X  X ransactions X  with the search engine for the complete set of query scrambling experiments (not including the generation of the samples) was: (12 private queries) (volume 50) (10 pages with results for each scrambled query) (2 samples) = 12,000, plus the 12 private queries for the ground truth. With an average number of ten clients active at any time and each client submitting one query (requesting ten results) approximately every 8 min, it took less than 7 days to retrieve the results for all scrambled queries from Google. None of the par-ticipants reported any significant (or even noticeable) computational load during the ex-periment. The computational load for a normal user of our query scrambling solution would probably not be different.

We were positively surprised by the high quality of the results, especially when the larger sample was used (Table 12 ). When using the 50K sample. we retrieved nine or ten target documents for most private queries, and the others do not fall far behind. Admit-tedly, this is a small-scale experiment with the lightest privacy requirements, but the outcome indicates the feasibility and the potential of our query scrambling approach.
We performed a failure analysis on the two worst-performing queries, i.e.  X  X  X srael po-litical system X  X  and  X  X  X ow to make bombs X  X . Their inferior performance was traced to the following factors: (a) the documents in our samples, unfortunately, do not seem to cover the topic of  X  X  X srael political system X  X , pointing to the need of larger samples, and (b) Google is not exactly a bag-of-words engine; phrases like  X  X  X ow to X  X ,  X  X  X hat is X  X , and others, seem to have a special meaning and thus weighted more heavily, while in our case these are just bags of high frequency unimportant words which do not influence much the scoring/ranking. Furthermore, while query word order seem to matter in Google, this did not seem to have an adverse effect in our experiment.

Operationally, the long times consumed indicate that document samples should be constructed and distributed to interested first-time or occasional users by a central authority (vendor) or in some community-based manner in regular time intervals, and that many more clients than just 30 should participate in such a crowd-sourcing approach in order to reduce task completion times. Regular users of query scrambling could have their own low priority sampling task constantly running in the background at their side. Then, given the appropriate sample, the scrambling of a single private query could take from a few seconds to several hours, depending on the privacy measures taken for the submission of the scrambled queries. We introduced a method for search privacy on the Internet, which is orthogonal to X  X nd should be combined with X  X tandard methods such as using anonymized connections, agents, obfuscating by random additional queries or added keywords, and other techniques reducing private information leakage. The method enhances plausible deniability towards query-logs by employing alternative less-exposing queries for a private query. More im-portantly, the proposed approach does not disclose the original search query X  X nd thus the exact search interest of the user X  X o any party, including the search engine. We defined and modeled theoretically three types of privacy, providing a framework on which similar approaches may be built in the future.

In contrast to previous literature, we followed a statistical approach which does not use word/concept ontologies, semantic analysis or natural language processing. We investi-gated the practical feasibility of the proposed method and the trade-off between quality of retrieved results and privacy enhancement. In Arampatzis et al. ( 2011 ), the best result was 25 % of the top-50 target documents found, and was achieved at the lightest possible privacy requirements; our method can match this at higher-than-minimum privacy levels and for more and better-defined privacy types which can easier be explained to the end-user. At our lightest privacy level, our method outperforms the semantic one by far; we retrieve up to 56 X 76 % of the target results. Moreover, our method can be applied to a wider range of information needs and performs more predictably retrieval-wise.

The guided covering algorithm showed that the retrieval effectiveness can be further improved by carefully guiding the scrambled query selection process. Even though the improvements were small, we consider them very important because (a) they occurred in almost all cases we examined, and (b) the size of the sample that was used to guide the covering procedure was minuscule with respect to the size of the collection, nevertheless, covering the sample seemed to still transfer to the whole collection. Furthermore, the field experiment on Google showed that our approach, even though time-consuming, is prac-ticable on real search engines.

While it is easy to evaluate the retrieval effectiveness of our methods, we evaluated the actual privacy perceived by end users via a user study. The study showed that our simple word-and phrase-frequency based criteria can be strong indicators of what users find acceptable and safe from a privacy perspective. A further outcome is that even in cases where the average level of privacy of the scrambling tools is high, it may not be possible to assure 100 % privacy in a fully automatic way. This suggests that users should specify their privacy requirements in a very conservative way, or for even higher assurance, they may opt to review the set of scrambled queries supplied by the system, discarding queries they find  X  X nsafe X .
