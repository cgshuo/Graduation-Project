 Sentiment extraction aims to extract and group aspect and opinion words from online reviews. Previous works usually extract aspect and opinion words by leveraging association between a single pair of aspect and opinion word[5] [14] [9] [4][11], but the structure of aspect and opinion word clusters has not been fully exploited.

In this paper, we investigate the aspect-opinion associa-tion structure , and propose a  X  X irst clustering, then extract-ing X  unsupervised model to leverage properties of the struc-ture for sentiment extraction. For the clustering purpose, we formalise a novel concept syntactic distribution consis-tency as soft constraint in the framework of posterior regu-larization; for the extraction purpose, we extract aspect and opinion words based on cluster-cluster association. In com-parison to traditional word-word association, we show that cluster-cluster association is a much stronger signal to distin-guish aspect (opinion) words from non-aspect (non-opinion) words. Extensive experiments demonstrate the effectiveness of the proposed approach and the advantages against state-of-the-art baselines.
 I.2.7 [ Natural language processing X  X ext Analysis ] Sentiment Analysis; Opinion Mining; Sentiment Extraction; Information Extraction
Sentiment extraction is the task of extracting and group-ing aspect words and opinion words from online reviews. This task is important because, on top of extracted aspects and opinions, we can aggregate various opinions according to a product X  X  aspects (or attributes), and provide much  X  detailed, complete, and in-depth summaries of a large num-ber of reviews. More specifically, aspect words refer to a product X  X  or service X  X  properties (or attributes) which peo-ple have expressed opinion upon, such as battery, screen , and so on. While opinion words are words that people use to express sentiment, such as amazing , and interest-ing . Since aspect/opinion is usually expressed by differ-ent synonymous aspect/opinion words, we need to group aspect/opinion words into clusters.

Previous works for sentiment extraction focus on model-ing the association between an aspect word and an opin-ion word [5] [14] [9] [4][11]. The central assumption lies in that, if a word is likely to be an opinion word, the words which has strong association with the word will be more possible to be aspect words, and vice versa. The word-word association has been modeled by nearest neighbor rule [5], manually designed dependency pattern[14], word alignment model [9], and statistical correlation such as likelihood ra-tio test[4]. Despite the success of previous works, they may discover some  X  X alse Opinion Relations X (false word-word as-sociations) as discussed in [16]. For example, the phrase  X  X utofocus camera X  can be matched by a dependency pat-tern  X  X dj-amod-Noun X , which is widely used in previous works[16][14]. But this phrase doesn X  X  bear any sentiment orientation. The mined association between  X  X utofocus X  and  X  X amera X  is a false word-word association. Besides, we also find that previous works cannot fully employ the structure of the constructed graph, which is actually very useful for both extracting and clustering as we will show later.
Aspect-opinion association structure (Fig. 1) refers to the structure that the bipartite graph is particularly dense, nearly complete with regard to an associated pair of aspect and opinion. In other words, considering the subgraph which contains an associated pair of aspect and opinion word clus-ter, the bipartite subgraph is very dense and nearly com-plete. As shown in Fig.1, consider all the aspect words in aspect cluster  X  X creen X , and all the opinion words in opin-ion cluster  X  X lear(or not) X . Since most synonymous words can be used exchangeably, we observe that there is a very dense connection structure between words in aspect cluster  X  X creen X  and words in opinion cluster  X  X lear(or not) X .
From the perspective of sentiment extraction, aspect-opinion association structure preserves two properties: 1. Synony-mous aspect words are likely to be modified by the same opinion word, and vice versa. 2. The associations between aspect and opinion clusters are stronger and less prone to noises than word-word association.

The first property is useful for clustering aspect/opinion word candidates. This property leads to our syntactic distri-bution consistency assumption, assuming that synonymous aspect words are likely to be connected to the same opinion word through same dependency relation . This assumption is further formulated as constraint to guide the clustering process. Our clustering model is especially useful for dis-tinguishing words with similar contexts but different syn-tactic distributions. For example,  X  X arket X  and  X  X rice X  are somehow semantic-related and may have similar context. But they are modified by different opinion word candidates. With the help of syntactic distribution, we can assign them to different clusters. As a result, our clustering model can produce more pure clusters.

Inspired by the second property, we extract aspect and opinion by leveraging the association between an aspect can-didate cluster and an opinion candidate cluster. Strongly associated cluster pairs are extracted as pairs of aspect and opinion. The extraction rule is illustrated as follows. As mentioned above, the resultant clusters can boost the as-sociation between an aspect word and an opinion word by exploiting cluster-cluster association. Now consider the non-aspect/non-opinion words, such as  X  X uggestion X  and  X  X trong X . Non-aspect/non-opinion words usually don X  X  have that much synonymous words as aspect/opinion words do. Ideally, many non-aspect/non-opinion clusters would contain few words (as shown in Fig. 1). Thus the clustering process will not enhance the association of those cluster pairs. There-fore, the association between a pair of aspect and opinion is much stronger than that of non-aspect/non-opinion pairs.
In practice, the non-aspect/non-opinion clusters usually contains several words. But our assumption still holds well, and this is verified by our clustering results. The associ-ation between a pair of aspect cluster(with more than 10 aspect words) and opinion cluster(with more than 10 opin-ion words) is 6.8 times as much as the association between other cluster pairs on average.

To leverage the two properties of aspect-opinion associa-tion structure , we propose a  X  X irst clustering, then extract-ing X  unsupervised model for sentiment extraction. Firstly, we cluster all aspect and opinion word candidates based on both context information and syntactic information. Sec-ondly, we extract aspect and opinion by constructing a graph to model association between aspect and opinion candidate clusters.
Our contributions are listed as follows:
The rest of this paper is organized as follows. We intro-duce the framework of our methodology in Section 2, along with the clustering model in Section 2.1, and the extraction model in Section 2.2. We present experiment results in Sec-tion 3. In Section 4, we survey related work. We summarize our work in Section 5.
To extract and cluster aspect and opinion words from re-views, we propose a  X  X irst clustering, then extracting X  frame-work, as shown in Fig. 2. Our model consists of two com-ponents as follows: 1) The Clustering component(Section 2.1) aims to cluster aspect/opinion candidates separately into word clus-ters. In this paper, we select nouns/adjectives to be as-pect/opinion candidates. We cluster those candidates by leveraging both context information and syntactic informa-tion. 2) The Extracting component(Section 2.2) aims to extract aspect/opinion clusters from all candidate clusters. For each extracted cluster, we further rank each word of the cluster to find aspect or opinion words. We extract as-pect/opinion clusters based on cluster-cluster association, and rank words in a cluster based on word-cluster associa-tion.
In this section, we firstly present the syntactic distribution consistency assumption and show how to model it as con-straint. Then, this constraint is integrated into a context-based probabilistic model(Multinomial Naive Bayes) under the framework of Posterior Regularization (PR).

We present our method only from the perspective of as-pect word candidates. The process for opinion word can-didates is similar. We use ac to represent an aspect word candidate, and acc to represent a golden aspect candidate cluster.
In this subsection, we describe syntactic distribution con-sistency formally, and show how to use it robustly on un-balanced data.

We propose a novel corpus-level syntactic representation for each aspect word candidate, by simply counting how many times the word is connected to each opinion word candidate through each direct dependency relation in the corpus. Taking aspect word candidate  X  X creen X  for example, its syntactic representation is For an aspect candidate cluster, we simply add all syntactic vectors of the cluster X  X  members together.

Given an ideal cluster acc i , let us consider its syntactic representation. In a large review dataset, aspect words in cluster acc i could be modified by different opinion words through different dependency relations many times. Each time can be viewed as a trial, and the syntactic vector of candidate cluster acc i follows a certain multinominal distri-bution ~p i , which is the syntactic distribution of acc i where n i is calculated by adding all elements in vector Syn ( acc together.

Based on the syntactic representation, we further assume that syntactic vector of candidate word ac j follows the same distribution that its cluster follows. This leads to our syn-tactic distribution consistency assumption: n is calculated by adding all elements in vector Syn ( ac j assumption is verified on our dataset.

However, the frequencies of aspect word candidates could vary from very small to very large, making the estimate of these distributions unstable. As shown in Fig 3 estimated syntactic distribution of frequent word is reliable, and our assumption holds well. But for less frequent words, their estimated distributions are different from that of their clusters due to insufficient observations.

We calculate the syntactic distribution on the laptop do-main. For simplicity, we only show probability on 5 dependency-opinion features. A frequent aspect candidate could be connected to tens of opinion candidates in total.
We employ Pearson X  X  chi-squared test to judge whether this difference between the expected distribution and ob-served candidate syntactic distribution is due to sampling variation, or differs significantly. The chi-square statistic is a summary measure of how well the observed frequencies of categorical data match the frequencies that would be ex-pected under certain multinomial distribution. In our case, we want to measure how well Syn ( ac j ) match ~p i , which is the syntactic distribution of acc i . According to Pearson X  X  chi-squared test , we have the following equation. dim ( ~p i ) represents the dimension of vector ~p i , and  X  corre-sponds to a certain confidence level. We look up the chi-square distribution table to find the right threshold corre-sponding to a certain confidence level(such as 95%,  X  = 0 . 05) with the freedom of dim ( p i )  X  1. If aspect candidate ac j belongs to aspect candidate cluster acc i , then we are confident that the above equation holds. In other words, if the above equation doesn X  X  hold, we are confident that ac acc i according to our assumption.

We introduce an indicator variable z ij to represent whether aspect candidate ac j belongs to aspect candidate cluster acc i , as follows: This leads to our syntactic distribution consistency(SDC) constraint function. Thus, we can leverage syntactic information for both fre-quent words and less frequent words in a very robust way. Only when we have enough observations for ac j and the reli-able estimated syntactic distribution of ac j is really different from that of acc i , we apply ac j /  X  acc i .
In this section, we present our probabilistic model which employs both context information and syntactic distribu-tion.

First of all, we extract a context document d to represent each candidate, by collecting the preceding and following t words of a candidate in each review. We use D to repre-sent the document collection for all aspect word candidates. Assuming that the documents in D are independent and identically distributed, the probability of generating D is then given by: where y j is a latent variable indicating the cluster label for aspect word candidate ac j , and  X  is the model parameter.
In our problem, we are actually more interested in the pos-terior distribution over cluster label, i.e., p  X  ( y j | d learned parameter  X  is obtained, we can get our clustering result from p  X  ( y j | d j ), by assigning cluster label acc largest posterior to aspect word candidate ac j . We can also enforce SDC-constraint in expectation(on posterior p  X  ). We use q ( Y ) to denote the valid posterior distribution that sat-isfy our SDC-constraint, and Q to denote the valid posterior distribution space, as follows:
Since posterior plays such an important role in joining the context model and SDC-constraint, we formulate our prob-lem in the framework of Posterior Regularization (PR). PR is an efficient framework to inject constraints on the poste-riors of latent variables. Instead of restricting p which might not be feasible, PR penalizes the distance of p to the constraint set Q . The posterior-regularized objective is termed as follows:
By trading off the data likelihood of the observed con-text documents (as defined in the first term), and the KL divergence of the posteriors( p  X  ( Y | D )) to the valid posterior subspace( Q ) defined by SDC-constraint (as defined in the second term), the objective encourages model with both de-sired posterior distribution and data likelihood. In essence, the model attempts to maximize data likelihood of context subject (softly) to SDC-constraint.

In spirit to [19], we use Multinomial Naive Bayes (MNB) to model the context document. Let w d j ,k denotes the k word in document d j , where each word is from the vocabu-lary V = { w 1 ,w 2 ,...,w | V | } . For each aspect phrase f probability of its latent aspect cluster label being acc i generating context document d j is where p ( acc i ) and p ( w d j ,k | acc i ) are parameters of this model. Each word w d j ,k is conditionally independent of all other words given the cluster label acc i .

The optimization algorithm for our model is an EM-like algorithm, which can be easily implemented as described in [3]. So we omit the algorithm here. After each E-step, the syntactic distribution of each cluster is updated by adding and normalizing all syntactic vectors of the cluster X  X  candi-dates together according to the current posterior distribu-tion q .
Our extraction model consists of two steps: 1. We extract aspect cluster and opinion cluster based on cluster-cluster association. 2. Since clustering results may not be per-fect and each cluster may still contain some noisy words, we propose a word ranking algorithm based on word-cluster association. At the end of this section, we also present an in-depth comparison between our model and models based on word-word association.
As mentioned earlier, the clustering process leads to dra-matic boosting on the association between associated pairs of aspect and opinion, and little or no boosting on the as-sociation of other cluster pairs. This leads to our following assumption,
In spirit to [4], we adopt the Likelihood Ratio Test (LRT) statistics to measure cluster-cluster association. LRT com-putes a contingency table of aspect candidate cluster AC i and opinion candidate cluster OC j , derived from corpus statistics, as given in Table 1. k 1 ( AC i ,OC j ) is the number of times that words in AC i and words in OC j is connected by an direct dependency relation. k 2 ( AC i , OC j ) is the number of dependency relation connected pair that contains words in AC i but not words in OC j . k 3 ( AC i ,OC j ) is the number of dependency relation connected pair that contains words in OC j but not words in AC i . k 4 ( AC i , OC j ) is the number of dependency relation connected pair that contains neither words in AC i nor words in OC j . Note that our purpose here is to measure how strongly pair-wise clusters are as-sociated with each other given the corpus statistics, rather than performing an actual statistics test.

Based on the corpus statistics shown in Table 1, the LRT model captures the statistical association between aspect candidate cluster AC i and opinion candidate cluster OC by employing the following function: where,
The larger the quantity LRT ( AC i ,OC j ) is, the stronger the statistical association between aspect candidate cluster AC i and opinion candidate cluster OC j . We rank all cluster pairs by the LRT score, and top ranked pairs are extracted as aspect clusters and opinion clusters.
After cluster level extraction, we get pairs such as  X  X s-pect:Screen X  - X  X pinion:Clear(or not) X . However, there might be some noisy words in the  X  X spect:Screen X  cluster or  X  X pin-ion:Clear(or not) X  cluster. For each word in the  X  X spect:Screen X  cluster, the stronger the association of the word to the  X  X pin-ion:Clear(or not) X  cluster, the more likely the word is an aspect word in  X  X spect:Screen X .

This motivates our word ranking method. For an ex-tracted aspect/opinion cluster, we select its corresponding opinion/aspect cluster with the strongest association, and calculate the association between each word with the cor-responding cluster. The higher the association score is, the word is more likely to be the aspect/opinion word of this cluster. The word-cluster association is also calculated by LRT, based on the corpus statistics shown in Table 2 2
First of all, for each selected aspect/opinion cluster, we se-lected the opinion/aspect cluster with the largest LRT score as the corresponding paired cluster. Then, for each word T in the aspect/opinion cluster, we calculate the LRT score LRT ( T j ,C i ) between the word and its cluster X  X  correspond-ing paired cluster C i . Finally, each word is ranked by the LRT score. The higher the LRT score is, the word is more likely to be an aspect/opinion word of the cluster.
As mentioned above, existing models based on word level association may discover some  X  X alse Opinion Relations X (false word-word associations) as discussed in [16]. We will show that our cluster level associations are more robust and less prone to noises than word level associations.

As shown in Fig. 4, there is a false association that con-nects a non-aspect word na and an opinion word o . This non-aspect word na may be extracted as  X  X spect word X  by word-word association based models. In our model, we ex-amine the association between the cluster of the non-aspect word na and the cluster of the opinion word o . The associa-tion between this two clusters will be much weaker than that between a real pair of aspect and opinion cluster. Thus this pair of clusters will not be extracted by our cluster-cluster association based method. In practice, our method is also better at filtering non-aspect/non-opinion words.

Another important difference is that we not only extract but also cluster aspect/opinion words, while existing models that leverage word-word association only extract aspect/opinion words. For instance, for the aspect  X  X icture X , the model ex-tracts opinion clusters  X  X lear(or not) X ,  X  X right(or not) X  and  X  X ood(or not) X  as shown in Table 5 in experiment.
In this section, we evaluate our approach on reviews from various domains. To assess the effectiveness of our approach, we conduct three experiments on the task of aspect extrac-tion, clustering, aspect and opinion word extraction. The corpus statistics is calculated in a similar way as in Table 1. Since there is no prior work on clustering opinion words, the result of opinion word clustering is qualitatively evaluated in this paper.
We evaluate our method on a large Chinese review corpus, as used in [21]. This dataset contains reviews from four domains: Camera , Cellphone , Laptop , and MP3/MP4 . The statistics of the corpus are present in Table 3.

We create a gold standard for aspect/opinion word ex-traction and clustering. 3 Since the dataset is very large, it is impossible to go through every review and label each word manually. Therefore, we select all nouns with frequency larger than 3 (5 for the cellphone domain) as aspect can-didates, and all adjectives as opinion candidates. We go through every candidate word, and read sampled reviews which contain this candidate to judge whether it X  X  an as-pect/opinion word or not. Since it is impossible to discover all the aspect/opinion words, it is infeasible to evaluate the recall measure.
 Evaluation Metrics : Similar to previous work [2] [13] we adopt precision@k (or p@k), where k is the rank position to compare different approaches. As just mentioned, recall is not applicable in our settings.
Aspect extraction refers to the task of extracting and clus-tering aspect words simultaneously. Existing models for as-pect extraction are all topic-model based methods[1][2][13].
In this section, we compare our model with three topic-model based aspect extraction methods: Latent Dirichlet al-location( LDA ), Seeded Aspect and Sentiment model( SAS )[13] and topic modeling with Automatically generated Must-links and Cannot-links( AMC )[1]. Those baselines represent three types of topic models: unsupervised, semi-supervised, self-learning.
We will release our gold standard later. Settings:
The parameter of all topic models are set to  X  = 1,  X  = 0 . 1. For LDA and AMC, each sentence is treated as a document.

Setting the number of topics/aspects is often tricky. We follow the approach in [13] and assume that we already know the gold aspect number.

Note that the topic models used in our experiments are also with only candidate words but not with all words with-out filtering. The filtering process always promotes the per-formance of these topic models. We believe this configura-tion makes it more fair to compare our method with topic model baselines.

For our method, we set the cluster number to be 100, and the extracted aspect number to be the gold aspect number. For clustering, we set the window size to be 4 to extract con-texts, and set  X  to be 0.05 to get the chi-squared threshold. Since our method depends on the random initiation, we use the average result of 5 runs as the final result.
We follow the evaluation methods in [2]. For each topic (or cluster in our setting), we judge it as a  X  X ood topic X  if the top 15 words contain at least 5 synonymous aspect words. If so, we also assign the corresponding aspect to this topic.
For each good topic, we evaluate whether the top-ranked words belong to the corresponding aspect of the topic in terms of precision@k (or p@k).

Experiment results are shown in Table 4 and Fig.5. We can see that our approach can extract more good topics with better precision than all baselines on all domains. The mer-its of our approach may come from the following facts: https://github.com/czyuan/AMC.git
Table 5 shows two example aspects ( X  X icture X  and  X  X at-tery X ) and its top 10 words produced by each methods in MP3/MP4 domain. From Table 5, we can see that our method discovers more correct and meaningful aspect words at the top positions. While AMC and LDA fail to discover fine-grained aspect  X  X icture X .

Compared with topic-model based methods, our method can extract aspect cluster and opinion cluster simultane-ously. As a result, our methods can capture the many-to-many association between aspect clusters and opinion clus-ters. For example, opinion  X  X ood(or not) X  is related to both aspect  X  X icture X  and aspect  X  X attery X  in our extraction re-sult. Our method can also extract and distinguish fine-grained opinion cluster. For aspect  X  X icture X , we extract three related fine-grained opinion clusters:  X  X lear(or not) X ,  X  X right(or not) X  and  X  X ood(or not) X . While topic model based methods can only extract one mixed opinion topic for each aspect topic.

We believe that extracting fine-grained opinion clusters has potential value for generating fine-grained opinion sum-mary. For example, we may want to know more detailed opinions about  X  X icture X , i.e., whether it X  X   X  X lear(or not) X ,  X  X right(or not) X , or generic opinions such as  X  X ood(or not) X .
We can also see that some opinion cluster contains both synonymous and antonymous opinion words. Separating positive opinion words and negative opinion words is beyond the scope of this paper.
In this section, we want to evaluate the effectiveness of leveraging syntactic distribution consistency in the cluster-ing component. We conduct an extrinsic experiment that evaluate the final aspect extraction performance, by chang-ing the clustering component. We compare our clustering model with two simple clustering methods: Kmeans and MNB-EM . The result is shown in Fig.6. We can see that our SDC-MNB clustering model can extract more good topics with better or comparable precision than baselines, which demon-strates the merits of syntactic distribution consistency in the clustering component. Besides, our extraction model with kmeans is comparable to topic model baselines, which demonstrates the effectiveness of our  X  X irst clustering, then extracting X  framework.
 Kmeans 8.4/27 10.4/26 9.4/25 10.6/27 MNB-EM 14.0/27 14.2/26 11.2/25 17.8/27 SDC-MNB 16.8 /27 15.6 /26 15.4 /25 18.0 /27
In this section, we evaluate our method on the aspect and opinion word extraction task. In this task, we don X  X  care about which cluster each word belongs to. An extracted word is correct, as long as it X  X  an aspect/opinion word. We choose precision@k as evaluation measure. We compare our method with two strong baselines: DP and LRT . Extracted aspect/opinion words are ranked by frequency in DP and LRT.

The result is shown in Table 6. We can see that our method outperforms two baselines for most cases. This re-sult demonstrates that, based on candidate clustering result, our simple ranking method works well for aspect and opinion word extraction task.

We also find that our method works especially well for smaller k in term of p@k. As k increases, the performance of our method drops only slightly. The reason is as follows: In the first step of extraction, we extract aspect/opinion cluster. If the extracted cluster number is small, then the extracted clusters may not cover all aspect/opinion words. So our method doesn X  X  perform well for aspect/opinion word extraction in terms of p@k for larger k. However, since we don X  X  care about the aspect number in this task, a larger cluster number could solve this problem.
Sentiment extraction is an important task for aspect-level sentiment analysis. Previous works can be divided into two categories: sentence level extraction and corpus level extrac-tion.

For sentence-level extraction, previous methods mainly aimed to identify all opinion target/word mentions in sen-tences by supervised learning[15] [12][7] [17]. They regarded it as a sequence labeling task, where several classical models were used. Jin and Ho applied a lexicalized HMM model to learn patterns to extract aspects and opinion expressions[6]. Li et al. integrated two CRF variations, i.e., Skip-CRF and Tree-CRF, to extract aspects and also opinions[7].

This paper falls into corpus level extraction, and aims to generate an aspect/opinion cluster list rather than to iden-tify mentions in sentence. Previous corpus level extraction methods can be divided into two categories: knowledge-based topic models that leverage word-word co-occurrence information implicitly, and graph-based models that model the association between aspect and opinion candidates ex-plicitly.

Knowledge-based topic models performed aspect term ex-traction and clustering simultaneously by leveraging word-word co-occurrence information implicitly. To generate co-herent topics, several knowledge-based topic models, have been proposed. Mukherjee and Liu proposed Seeded Aspect and Sentiment model(SAS) by using human-generated seeds to learn coherent topics[13]. Chen et al. proposed MC-LDA to leverage both must-sets and cannot-sets[2]. Chen et al. also proposed AMC to learning knowledge from other do-main[1].

Our method is similar to topic models in terms of discov-ering aspect and opinion based on word co-occurrence. The key difference is that topic models can X  X  distinguish aspect words and background(or non-aspect) words based on word association in an unsupervised manner, but our model can. Topic models such as ME-LDA[22] includes a word classifi-cation module, which classifies words into three categories: aspect, opinion, and background. But they need heavy an-notation. Topic models such as SAS work in a weakly super-vised manner. But they treat all words as either aspect or opinion words. Since not all words are aspect words or opin-ion words, the learned topic must contains some background words. If a background word is frequent, it X  X  usually hard to filter it. While our method can filter frequent non-aspect words based on cluster-cluster association.

Graph-based models extracted aspect and opinion terms by modeling association between aspect and opinion candi-dates explicitly. Those models usually started from seeds and propagate label on a graph. The word-word associa-tion has been studied by many researchers. Hu and Liu exploited nearest neighbor rules to mine association among words[5]. Qiu et al. designed syntactic patterns to perform this task[14]. Liu et al. employed word alignment model to capture word-word association rather than syntactic pars-ing[9][8][10]. Hai et al. explore robust statistical correlation in a bootstrapping framework[4].

Our method is similar to graph-based models in terms of employing association strength for extraction. However, our method extract aspect and opinion by leveraging cluster-cluster association, while graph-based models employ word-word association. We found that the bond between aspect and opinion clusters are stronger and less prone to noises than word level associations.

Another line of work focused on clustering aspect terms based on context information. Several weakly supervised models has been proposed. Zhai et al. proposed an EM-based semi-supervised learning method to group aspect ex-pressions into user-specified aspects[18]. They employed lex-ical knowledge to provide a better initialization for EM. In Zhai et al.[19], an EM-based unsupervised version was pro-posed. The so-called L-EM model first generated softly la-beled data by grouping feature expressions that share words in common, and then merged the groups by lexical simi-larity. Zhai et al.[20] proposed a LDA-based method that incorporates must-link and cannot-link constraints. Zhao et al. proposed an EM-based unsupervised method by leverag-ing data-driven sentiment consistency constraint[21].
Compared with previous work, our model has two advan-tages: 1. Our model can extract aspect and opinion based on cluster-cluster association. 2. Our model is totally unsu-pervised, doesn X  X  need any labeled data, human-generated knowledge, or seeds.
In this paper, we propose a novel sentiment extraction method by leveraging aspect-opinion association structure . First, we cluster all aspect candidates and opinion candi-dates by our syntactic distribution consistency inspired model. Second, we extract aspect and opinion word clusters based on cluster-cluster association. Compared with existing topic-model based aspect extraction methods, our method per-forms an explicit extraction process and filter many non-aspect/non-opinion candidates. Compared with existing graph-based methods that extract aspect words and opinion words, our method can extract aspect and opinion at cluster level. Experiments show that our approach outperforms state-of-the-art baselines remarkably.
This work was partly supported by the National Basic Re-search Program (973 Program) under grant No.2012CB316301 /2013CB329403, the National Science Foundation of China under grant No.61272227/61332007, and the Beijing Higher Education Young Elite Teacher Project. The work was also supported by Tsinghua University Beijing Samsung Telecom R&amp;D Center Joint Laboratory for Intelligent Media Com-puting. [1] Z. Chen and B. Liu. Mining topics in documents: [2] Z. Chen, A. Mukherjee, B. Liu, M. Hsu, [3] J. V. Graca, L. Inesc-id, K. Ganchev, B. Taskar, J. V. [4] Z. Hai, K. Chang, and G. Cong. One seed to find [5] M. Hu and B. Liu. Mining opinion features in [6] W. Jin and H. H. Ho. A novel lexicalized hmm-based [7] F. Li, C. Han, M. Huang, X. Zhu, Y.-J. Xia, S. Zhang, [8] K. Liu, L. Xu, Y. Liu, and J. Zhao. Opinion target [9] K. Liu, L. Xu, and J. Zhao. Opinion target extraction [10] K. Liu, L. Xu, and J. Zhao. Syntactic patterns versus [11] K. Liu, L. Xu, and J. Zhao. Extracting opinion targets [12] T. Ma and X. Wan. Opinion target extraction in [13] A. Mukherjee and B. Liu. Aspect extraction through [14] G. Qiu, B. Liu, J. Bu, and C. Chen. Opinion word [15] Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase [16] L. Xu, K. Liu, S. Lai, Y. Chen, and J. Zhao. Mining [17] B. Yang and C. Cardie. Joint inference for fine-grained [18] Z. Zhai, B. Liu, H. Xu, and P. Jia. Grouping product [19] Z. Zhai, B. Liu, H. Xu, and P. Jia. Clustering product [20] Z. Zhai, B. Liu, H. Xu, and P. Jia. Constrained lda for [21] L. Zhao, M. Huang, H. Chen, J. Cheng, and X. Zhu. [22] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly
