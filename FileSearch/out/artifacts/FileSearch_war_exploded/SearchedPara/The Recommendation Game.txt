 This paper describes a casual Facebook game to capture recommendation data as a side-effect of gameplay. We show how this data can be used to make successful recommenda-tions as part of a live-user trial.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering; H.3.5 [ Online Information Services ]: Web-Based Services Recommender Systems; Games-with-a-Purpose; Human Com-putation; Crowdsourcing
Recommender systems suggest items to users. Most rely on user preferences, such as past ratings [1]. Many use ma-trix factorization methods to find hidden patterns within these preferences [9]. Yet others harness inter-user similarity to identify groups of like-minded users [2]. And some lever-age social network data to infer trust relationships between users [6]. Collecting this information at scale is the subject of much research. Many approaches have been considered, from using explicit feedback such as transaction histories to inferring interest from implicit signals such as read-times or sharing [8]. More recently, crowdsourcing ideas have been considered (e.g. [11]); see also CrowdRec workshops 1 . In this paper we consider a crowdsourcing approach by developing  X 
This work is supported by Science Foundation Ireland through the Insight Centre for Data Analytics under grant number SFI/12/RC/2289.  X  Sam Banks completed this work as a final-year Computer Science student in University College Dublin. http://crowdrecworkshop.org/ c  X  a so-called game-with-a-purpose (GWAP) for soliciting rec-ommendation data as a by-product of gameplay.

GWAPs are games that are simple and fun to play with gameplay contributing to some secondary problem solving goal; e.g. the ESP Game, arguably the original of the species, invites pairs of players to guess words for images [15] to im-prove indexing in image search. Other games have been developed to index audio and video [5, 10, 14]. GWAPs have also been developed for object segmentation [12] and even protein folding [3]. The power of a GWAP stems from its ability to attract many players who collectively contribute to a greater goal through their gameplay. Can GWAPs also be used for recommender systems? This question has been asked by [16] in the context of the Curator system, a game-with-a-purpose for recommending collections of items that go together; see also [4, 7]. In this paper we focus on a complementary set of recommendation matters related to user-user tie strength and user-item relevance.
Our game is a Facebook app wherein a player p is tasked with matching a set of movies with their friends Friends ( p ). We know movies each friend likes ( Likes ( f )) from their FaceBook  X  X ikes X . Figure 1 shows the game in action. Movie posters float across the screen, becoming more erratic as the game progresses. By dragging a movie m to a friend f  X  Friends ( p ), player p matches m with f ( match ( p,m,f )) if they believe that f will like m . The more matches a player generates the more we learn their beliefs about the prefer-ences of their friends; the set of matches that p generates is Matches ( p ) (Equation 1). But how can we decide if these matches are correct? And, if we can decide, then doesn X  X  this confirm recommendation data that we already know?
Matches ( p ) = [
For a match we either know that f likes m (a known match ) or we have no such knowledge (an unknown match ). Either way, we can infer recommendation data as follows.
In the case that f is known to like m ( m  X  Likes ( f )) then we learn something about p  X  X  understanding of f  X  X  (movie) interests. The more such known matches, the better p seems to know f ; useful recommendation data in itself. For exam-ple, we can estimate this as the proportion of known matches that p generates for f relative to all matches p generates for f in a given set of games; see Equations 2  X  4.

FriendMatches ( p,f ) =
KnownMatches ( p,f ) =
Even when we have no information about f  X  X  interest in m ( m /  X  Likes ( f )) this does not mean it is a poor match; Likes ( f ) is not exhaustive. And the fact that p assigns the match suggests that p believes that f will be interested in m , establishing m as a plausible, novel recommendation candidate for f . If other players also match m with f then this strengthens the possible interest of f in m . Equation 5 captures this as an interest score based on the number of players who have matched some unknown m with f , and their tie strength with f .
The game is implemented as a Facebook app to gain ac-cess to the social graph and likes of players; this way friends can be chosen and (known) matches can be verified. The game has been developed in PHP and is responsible for stor-ing all player data, friendship links, movie data, and game sessions. The game uses the Rotten Tomatoes API to col-lect additional movie data (poster graphics, trailers etc.) for the purpose of enhancing gameplay. On the client-side the front-end is written in Javascript using jQuery and some task specific libraries for animation and image manipulation.
Game mechanics are simple and enjoyable. At the start of the game the player p is presented with a subset of friends; currently 5 friends are picked who have at least 10 movie likes. Each game comprises 18 movies which are chosen from a mixture of the friends X  profiles/likes and the Rot-ten Tomatoes most popular movie list. This ensures a mix of known movies, with which to validate matches and infer tie strength, plus unknown movies to generate novel recom-mendation candidates.

During the game a graphic of a movie X  X  poster floats across the screen following a particular trajectory and speed. As the game continues the screen becomes more cluttered and the trajectory of movies more erratic thereby making the matching process more frenetic and challenging. And when a player matches a movie by dragging its icon over a friend X  X  avatar, the player is rewarded by an audio and graphical flourish and a score.

Scoring is more challenging than might first appear. It seems intuitive to award a score if the player matches a known (liked) movie with a friend. But if they match an un-known movie should they miss out, given how these matches are potentially valuable, novel recommendation candidates? If we only score known matches then players may avoid mak-ing more novel matches. And what if a novel match has been made often during other games? How should it effect the scoring?
In the end we chose the scoring metric shown in Equation 6 which combines scores for known and unknown matches; we set  X  = 0 . 5 in this work but this could be adjusted to give more or less weight to each match type. This returns a score between up to 10 for each match. For example, if p matches m with f , such that m  X  Likes ( f ), and 3 other players have made the same match before, then p will receive a score 6.25. If on the other hand m /  X  Likes ( f ) and p is the first to make such a match then p receives a score of 5. There are no doubt many possible variations on this scoring metric that could (and should) be tested in the future. score ( p,m,f ) =
To test the data generated by gameplay we ran a small two-part, live-user trial based on 27 mutual friends and friends-of-friends; a mixture of male and female undergrad-uates and postgraduates. Participants acted as both players and friends during gameplay. During each game a subset of 5 friends was chosen at random for a given p ; the number of friends per participant varied from 5 to 12. For each partic-ipant we also had an initial set of movie likes from Facebook (on average, 27 movies per participant).
The evaluation took place in two phases. Phase 1 focused on collecting data by asking participants to play the game a few times; in fact players played an average of 9 games each, suggesting many found it at least somewhat enjoyable. On average 11.69 matches were made per game. These data were then used in phase 2 to generate recommendations for each of the 27 users. We generated 6 movie recommenda-tions per user from 3 different recommendation strategies and asked each participant to rate all 18 recommendations as either satisfactory or unsatisfactory; a simple binary rat-ing per movie. We were careful to interleave the order of recommendations from each strategy to avoid any positional bias.
As to the recommendation strategies used: we distinguish each in terms of the candidate items they consider for rec-ommendation and how these are ranked to produce a top-6 for the target user u t , as follows.
Here we rely purely on gameplay data to generate and rank our recommendations. The candidates are those movies matched with u t (but unknown to u t ) by player-friends of u t during gameplay; see Equation 7. These candidates are then ranked in terms of interest ( m,u t ) as per Equation 5.
CS Candidates ( u t ) =
Thus, movies that are often matched with u t , by many players who know u t  X  X  preferences well, will rank higher than movies less often matched with u t by less familiar players.
Next we implemented a version of collaborative filtering by choosing movies from the profiles (likes) of u t  X  X  friends; see Equation 8. Collaborative filtering usually ranks rec-ommendations based on the similarity between u t and the friends/neighbours from where they originate; similarity is usually based on some form of ratings correlation. In our small trial we instead use the knows ( f,u t ) metric as a proxy for user similarity and rank the candidates according to interest ( m,u t ). In this way the CS and the CF techniques differ primarily in the source of the candidates (gameplay vs. profiles, respectively).
 Finally, as a content-based strategy we used the Rotten Tomatoes API call, movie similar ( m ), to obtain a set of 5 movies that are similar (based on meta-data) to each m  X  Likes ( u t ). The candidate movies for u t are the collection ( L )) of all movies returned by Rotten Tomatoes for each of the movies in Likes ( u t ).

At recommendation time 6 content-based candidates are selected from these candidates at random. Since these col-lections may contain duplicates, if the same movies are re-turned by Rotten Tomatoes for different seeds, then this approach will tend to recommend movies that are more fre-quently represented in the candidate list, giving priority to movies that are considered similar to many of u t  X  X  likes.
Satisfaction results are shown in Figure 1 and indicate a benefit to the CS approach compared to CF or CB. 90% of users found the CS recommendations to be satisfactory compared to only 79% and 77% for CF and CB, respectively. This speaks to the quality of the recommendation data being created as part of the gameplay; remember the difference between CS and CF is only a matter of how the movies were sourced (gameplay vs. profiles).

It is also interesting to examine the diversity of recommen-dations made by the different approaches; diversity speaks to the ability of the recommender to offer the user more or less variety through its suggestions [13]. As a simple measure of diversity we compared the percentage of unique recommen-dations made by each of the 3 approaches in Figure 1. CS presents with lower levels of diversity than either CF or CB. Only 21% of the movies recommended by CS were unique compared to 37% and 50% for CF and CB.

One explanation for this is that the CS approach tends to skew towards popular movies because players recognise their posters more quickly and naturally gravitate towards these during gameplay. Moreover, because these movies are popular they may also be easier to match with friends lead-ing to a greater likelihood that they will be liked, hence the improved satisfaction scores. The higher level of diversity using the CB approach on the other hand, can be partially attributed to the fact that it can draw recommendations from a larger pool of candidates.

It is a matter for future work to consider this relation-ship between satisfaction and diversity further. It may be possible to guide gameplay towards more novel items by ma-nipulating the size and speed of items according to the item popularity. For example, perhaps unusual movies could be presented with a larger icon and/or a slower trajectory, mak-ing them more attractive gameplay targets; scoring could also be adapted with respect to inverse item popularity.
The purpose of this paper has been to explore the use of a GWAP to collect recommendation data as a side-effect of casual gameplay. We have implemented and tested a simple movie matching game and described the different types of recommendation data that we can collect from its gameplay. We have shown how this data can be useful in a recommen-dation context as part of a live-user trial. And, it should be noted that, as a practical matter, the approach is not limited to movie preferences because similar ideas could be used for many other types of items.

This work is of course limited in many ways. The system is a working prototype and the small scale of our evaluation offers little more than a proof-of-concept for what we are trying to achieve. Nevertheless we believe it serves as a first-step to highlight the potential for new ways to think about recommendation data and recommender systems while con-tributing to a growing interest in the role of crowdsourcing in recommender systems research. [1] G. Adomavicius and A. Tuzhilin. Toward the Next [2] G. K. Christian Desrosiers. A Comprehensive Survey [3] S. Cooper, F. Khatib, A. Treuille, J. Barbero, J. Lee, [4] A. Felfernig, M. Jeran, M. Stettinger, T. Absenger, [5] R. Gligorov, M. Hildebrand, J. van Ossenbruggen, [6] J. Golbeck. In K. St X len, W. H. Winsborough, [7] S. Hacker and L. von Ahn. Matchin: eliciting user [8] D. Kelly and J. Teevan. Implicit Feedback for [9] Y. Koren, R. Bell, and C. Volinsky. Matrix [10] E. Law and L. von Ahn. Input-Agreement: A New [11] P. Organisciak, J. Teevan, S. Dumais, R. C. Miller, [12] A. Salvador, A. Carlier, X. Giro-i Nieto, O. Marques, [13] B. Smyth and P. McClave. Similarity vs. diversity. In [14] R. van Zwol, L. Garcia, G. Ramirez, [15] L. von Ahn and L. Dabbish. Labeling Images with a [16] G. Walsh and J. Golbeck. Curator: a Game with a
