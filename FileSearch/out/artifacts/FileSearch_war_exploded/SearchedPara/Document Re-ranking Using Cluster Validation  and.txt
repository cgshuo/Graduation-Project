 This paper proposes a novel doc ument re-ranking approach in information retrieval, which is done by a label propagation-based semi-supervised learning algorithm to utilize the intrinsic structure underlying in the large document data. Since no labeled relevant or irrelevant documents are generally available in IR, our approach tries to extract some pseudo labeled documents from the ranking list of the initial retrieval. For documents from the top ones vi a cluster validation-based k-means clustering; for pseudo irrele vant ones, we pick a set of documents from the bottom ones. Then the ranking of the documents can be conducted via label propagation. Evaluation on benchmark corpora shows that the approach can achieve significant improvement over sta ndard baselines and performs better than other related approaches. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Clustering Languages, Algorithms. Document Re-ranking, Informati on Retrieval, Data Manifold Structure, Label Propagation Document re-ranking (DR) in info rmation retrieval has been a hot research topic during the last decade. Normally, it can be achieved in two ways: direct re-ranking on initial retrieved documents and indirect re-ranking via automatic query expansion (QE). While automatic query expansion assumes that top ranked documents are more lik ely to be relevant, the terms in these documents can be used to augment the original query and a better ranking can be expected via a second retrieval, direct re-ranking improves the rankings of the initial retrieved documents by directly adjusti ng their positions without a second retrieval. Normally, these two approaches can be integrated. For example, direct re-ranking can be used to improve automatic query expansion since better ra nking in top retrieved document can be expected to improve the quality of the augmented query. This paper will focus on direct document re-ranking. According to the different info rmation sources used, traditional document re-ranking can be classified into three categories. The first category uses in ter-document relationship. For example, [1] re-ranked documents by using document distances to modify their relevance weights while [8] proposed their approach based on document clustering. The second category uses various external resources, such as manually built thesaurus [15], manually crafted grammars [2] and controlled vocabularies [6]. The third category uses specific information extracted from documents or queries. For example, [11] used the information in retrieval and augmented un-st emmed words in queries in document re-ranking; [17, 18] ma de use of global and local information to re-rank the documents via local context analysis; [12] used maximal marginal rele vance to adjust the contribution of relevant terms; [19, 20] us ed query terms, which occur in both queries and top retrieved doc uments, to re-rank documents. Recently, there is a trend to e xplore the intrinsic structure of documents to re-rank documents. [21] proposed an affinity graph to re-rank documents by optimizing their diversity and information richness. [7] pr oposed a structural re-ranking approach using asymmetric relationships among documents induced by language models. [5] used score-regularization to adjust ad-hoc retrieval scores from an initial retrieval so that topically related documents received similar scores. This paper follows the tr end and proposes a novel document re-ranking approach by exploring the intrinsic information among top retrieved documents. Th is is done by using a label propagation-based learning algorithm to integrate pseudo labeled data with unlabeled data . This algorithm first represents labeled and unlabeled examples as vertices in a connected graph, then propagates the label information from any vertex to nearby vertex through weighted edges and finally infers the labels of unlabeled examples until the propagation process converges. Label propagation [22] is a kind of semi-supervised learning algorithms, which is characterized of using unlabeled data in the learning process. The rationale behind this algorithm is that the instances in high-density areas tend to carry the same labels. Label propagation has been successfu lly applied in NLP, such as word sense disambiguation [13] and text classification [3, 16, 22, 23]. The rest of this paper is orga nized as follows. In Section 2, we describe the re-ranking approach in details while the experimental results are given in Section 3. Finally we conclude this paper with some remarks in Section 4. In this paper, document re-ranking is recast as a two-class label propagation problem. For this purpos e, we need three sets of data: labeled relevant data as positive instances, labeled irrelevant data as negative instances, and unlabeled data. Since we do not have the labeled data except the query, which can be seen as a simple labeled relevant data, we try to generate some pseudo labeled data from the initial retrieval. Given a query q , suppose that we get M ranked documents in the initial retrieval. For irrelevant data, we simply pick N bottom ones as the pseudo irrelevant da ta. Regarding relevant data, a similar method would be to select top K documents as the pseudo relevant data. However, if noisy documents dominate the top ones, this method would fail. So, we turn to determine some clusters of documents among the top ones, and then select one closest to the query. After that, we take the documents in the cluster and the query itself as pseudo relevant documents. retrieved documents, and use a cl uster validation-based [14] K-means clustering algorithm to de termine the document clusters. First, a stability-based cluster va lidation approach is used to automatically determine the number of clusters. Then, the k-means clustering algorithm is used to cluster these documents. assumption that all the R documents in the cluster most similar with the query tend to be relevant documents with higher probabilities. capable of identifying both important feature words and true model order (cluster number). Important feature subset is selected by optimizing a cluster validity criterion subject to some constraint. For achieving model order identification capability, this feature selection procedure is conducted for each possible value of cluster number. The feature subset and cluster number which maximize the cluster validity criterion are chosen as answer. Given a query q and M ranked retrieved documents, we now have three datasets for label propagation: M unlabeled examples, R pseudo relevant exam ples derived from top K documents and N pseudo irrelevant exampl es as labeled data. As a result, document re-ranking can be achieved by ranking the M unlabelled documents according to their similarities with the R pseudo relevant documents via a label propagation algorithm as shown in Figure 1. algorithm in document reranking: o q : the query o { r j } (1  X  j  X  R ): the R pseudo relevant labeled documents o { n j } (  X  j  X  N ): the N pseudo irrelevant labeled documents o { m j } (1  X  j  X  M ): the M pseudo unlabeled documents, i.e. o C = { c j } (1  X  j  X  2) denotes the class set of documents o Y 0  X  H s X 2 (s= R + N + M ) represents initial soft labels in X is represented as a connected graph and the label information of any vertex in the graph is propagated to nearby vertices through weighted edges until the propagation process converges. Here, each vertex corresponds to a document, and the edge between any two documents x i and x j is weighted by w to measure their similarity. Here w ij is defined as follows: w distance between x i and x j (for example: cosine distance, Jenson-Shannon divergence distance), and  X  is a scale to control the transformation. In this paper, we set  X  as the average distance between labeled documents in di fferent classes. Moreover, the probability t ij = P(j  X  i) = w ij /(  X  s k=1 w kj probability to propagate a label from document x j x . In principle, larger weight s between two documents mean easy travel and similar labels between them according to the global consistency assumption applied in this algorithm. Finally, t is normalized row by row:  X  maintain the class probability interpretation of Y. The s  X  s matrix [ ij t ] is denoted as T . to push out labels through unlab eled data. With this push originates from labeled data, th e label boundaries will be pushed much faster along edges with larg er weights and settle in gaps along those with lower weights. Ideally, we can expect that w across different classes should be as small as possible and w within a same class as big as possible. In this way, label propagation happens within a same class most likely. solution [22] with u = M and l = R + N : where I is u  X  u identity matrix. uu T and sub-matrices In theory, this solution can be obtained without iteration and the initialization of Y U 0 is not important, since Y affect the estimation of U Y  X  . However, the initialization of Y helps the algorithm converge in practice. In this paper, each row in Y U 0 is initialized according the similarity of a document with the query. Input: re-ranked; from top K documents using cluster validation; the bottom of the ranked retrieved documents Y (probability of being a relevant document) We evaluated our approach on both the NTCIR-3 (NII-NACSIS Test Collection for IR Systems) CLIR Chinese SLIR document TREC-8 Ad-Hoc data using the vector space model (VSM) and OKAPI BM25 model, respectively. in a vector space and each dimension of the vector is a word for the English language or bi-gram for the Chinese language. Here, using the following TF/IDF weighting scheme: where w( b,d ) is the weight for b in d ; TF( b, d ) is the frequency of b in d ; P is the number of documents in the document set; and DF( b ) is the number of doc uments that contain b . given by the following weighting scheme: where TF( b, q ) is the frequency of b in q . For OKAP BM25 model, we use the default parameter settings. comparison, the same docum ent re-ranking algorithm was applied once to each query and the overall performance was averaged over all the queries. Moreover, we used standard Mean Average Precision (MAP) to measure the overall retrieval performance. Finally, we set K as 10, which means that we only seek the pseudo relevant doc uments among top 10 documents. During the cluster-validation, we suppose that the cluster number ranges from 2 to 6, which means that the most appropriate number of clusters existing in top 10 documents for all the queries falls within 2 to 6, although it may vary with different queries. Stability-based cluster validation was used to infer the most appropriate cluster number for each query. In addition, Jenson-Shannon (JS) divergence [9] is applied to measure document distance d ij . Relaxed relevance and rigid rele vance are adopted by NTCIR to measure the performance. Relaxed relevance measurement (relaxed) considers highly rele vant, relevant, and partially relevant documents as rele vant, while rigid relevance measurement (rigid) only consider s highly relevant and relevant documents as relevant. In NTCIR-3, each query is a description of a topic in the Chinese language, e.g. relevant documents, our first experiment only used 1 pseudo relevant document (query q, R =1) and 5 pseudo irrelevant documents ( N =5). The experimental results show that the re-ranking doesn X  X  improve the pe rformance. This means that, although the last five documents provide some information about irrelevant documents, th e query itself may provide too little information about relevant documents. via cluster validation-based k-m eans clustering when different numbers of retrieved documents are to be re-ranked. Here, the number of pseudo irrelevant documents N is set to 5. In Table 2, the first column indicates numbe r of top documents to be re-ranked, INI refers to initial re trieval result, MAP(rigid) and MAP(relaxed) represent MAP values on rigid relevance measurement and relaxed relevance measurement respectively, and +x% (-x%) denotes improvement (decrease) against baseline [INI]. MAP(rigid) and MAP(relaxed) from 10.5% to 30.5% and from 10.2% to 31.4% respectively when the number of retrieved documents increases from 40 to 1000. To see whether the improvement is significant, we conducted the paired t-test. Table 3 also shows the signifi cance marks. Here, **, * and ~ denote p-values smaller than 0.01, in-between (0.01, 0.05] and bigger than 0.05, and mean si gnificantly better, better and almost the same, respectively. MAP(rigid),  X  Document re-ranking achieves better performance when  X  Document re-ranking achieves significantly better does contribute to the label propagation-based document re-ranking approach, which leads to improvement of the performance. where PreAt10(rigid) and PreAt10(relaxed) represents the precision at top 10 documents on rigid relevance and relaxed relevance measure, respectively. Table 3 shows that document re-ranking improves PreAt10(rigid) and PreAt10(relaxed) from 0.3321 to 0.3333 and from 0.4566 to 0.4595 respectively when the number of retrieved documen ts increases from 40 to 90. Table 3 Precisions after document re-ranking ( K =10, N =5) extracted from top K retrieved documents via cluster validation-based k-means clustering. Notice that when computing recall, we only consider the actually re levant documents occurring in top K documents, since the clustering is only conducted on the percentage of actually relevant documents in top K ones. Comparing precision of C and accuracy of top K , we can see that the quality of the automatically acquired pseudo relevant documents is much better (20% higher in precision) than simply treating top K documents as relevant ones. This implies that cluster validation-based k-mean s clustering is effective in choosing pseudo relevant docum ents. Figure 2 shows the MAP comparison between the cluste ring-based pseudo relevant documents ( K =10, N =5; denoted as Cluster in Figure) and top 10 documents directly as ps eudo relevant documents ( R =10, N =5; denoted as non-Cluster in Figure). This suggests that the selection of pseudo relevant docum ents by clustering is useful for improvement of MAPs. 0.17 0.22 0.27 MAP
Table 5 Comparison: actua lly relevant documents vs. pseudo relevant documents Table 5 compares the pe rformance of using actually relevant documents in top 10 documents as relevant documents (if no actually relevant documents occur in top 10, we simple use top 3 documents to fill the gap) and 5 pseudo irrelevant documents. It shows that document re-ranki ng using actually relevant documents in top 10 performs mu ch better than document re-ranking using pseudo relevant docum ents. Paired t-tests between them show that the performance difference is significant. This implies that, although document re-ranking using pseudo relevant documents achieves pr omising improvement, there are still much potential for further improvement. To further explore the impact of K (the number of top retrieved documents) in determin ing pseudo relevant documents, we fixed the number of ps eudo irrelevant documents N to 5 and changed K from 10 to 20. Figure 3 shows the performance tendency when 1000 documents are re-ranked, where INI and DR represent MAP values before and after document re-ranking, respectively. It demonstrates that, when K changes from 10 to 16, both DR(rigid) and DR(relaxed) increase steadily, while when K changes from 16 to 20, both DR(rigid) and DR(relaxed) decrease slightly. To see why, we checked the quality of the selected cluster of pseudo rele vant documents, whose F-scores went up from 0.4886 to 0.4973, a nd then down to 0.4476. This again suggests that the quality of pseudo relevant documents is an important factor. MAP To explore the impact of N (pseudo irrelevant documents) in document re-ranking, we fixed K to 10 and changed N from 1 to 10. Figure 4 shows the MAP tendency, which demonstrates that the performance reaches the maximum when N falls in 5 to 7. This implies it is a bette r choice to choose 5-7 pseudo irrelevant documents. In comparison with ot her approaches, Figure 5 shows the performance of our approach ( K =10, N =5) and Zhang et al. X  X  affinity graph-based approach [21] with the ranking-combination scheme as (  X  =0.5;  X  =0.5). It demonstrates that our approach achieves better performance than theirs. This may be due to the fact that their approach focuses more on diversify and information richness and cares le ss on precision of the retrieval results. 0.15 0.25 MAP We also compared our approach ( K =10, N =5) with Kurland et al. X  X  structural re-ranking approach [7]. Figure 6 shows the MAP comparison where parameters of their approach are set as (R-W-In+LM: Recursive Weighted Influx + Language Model). It demonstrates that our approach gets comparable performance with theirs. However, their approach is based on language models which require large scale training data to be effective, our method is based on the intrinsic manifold structure of top retrieved documents with little training data needed, and pseudo labeled data is automatically created from the ranking list of the initial retrieval. 0.15 0.25 MAP We also compare our method with Mitra et al. X  X  maximal marginal relevance (MMR) method [12], which uses term correlation to re-order retrieved documents. If { w the set of query words presented in document d (ordered by calculated by following formula: where idf ( w i ) is the inverse document frequency of word w retrieved documents to be re-ranked, P( w i | w j ) is the word correlation between w i and w j in top K retrieved documents calculated by the formula: where S refers to document set. Figure 7 shows the co mparison of performance between Mitra X  X  and our document re-ranking method ( K =10, N =5). In the experiment, we re-rank top 50, 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 documents respectively. 0.15 0.25 MAP From Figure 7, we can see that our method (DR) achieves better performance than that of Mitra X  X  for both MAP(rigid) and MAP(relaxed) consistently at every document number setting. On the other hand, for our method, the improvement increases in a stable way as the number of documents to be re-ranked increases, while for Mitra X  X  met hod, the improvement generally decreases as the document number increases. For example, Mitra(rigid) decreases from 0.1751 to 0.1749 and 0.1676 as document number increase from 50 to 100 and 1000, while our DR(rigid) increases from 0.1895 to 0.2036 and 0.2203. Another finding is that Mitra X  X  method is only applicable to top (50 to 100) ranking documents, as was clai med in Mitra X  X  paper, while our method is more robust and a pplicable to both smaller and larger scope of documents. Now that document re-ranking can improve the performance of initial search, it s hould help query expansion. To confirm it, we combined docum ent re-ranking with standard Rocchio X  X  relevance feedback. Th at is, we first re-rank top 100 retrieved documents by using ( K =10, N =5), and then applied standard Rocchio X  X  relevance feedback on re-ranked top documents. In the experiment, we selected 200 bi-grams from the top F ( F =15, 20, 25 or 30) retrieved documents, and the selected units were added to the original queries to form new ones. Table 6 gives the MAP values for standard QE (Rocchio) and extended QE (re-ranking + Rocchio), where the first column indicates the number of the t op documents used for query expansion. From Table 6, we can see that extended QE achieved better results against standard QE with 13.9%-17.0% improvement. This indicates that the re-ranking helps query expansion to improve precision. Paired t-te sts between extended QE and standard QE shows that the di fference is significant, which indicates that extended QE not only improves the performance of initial retrieval, but also significantly outperforms standard QE. To see whether the same finding on Chinese dataset comes out on English dataset, we perform ed experiments on the TREC-8 ad-hoc retrieval dataset, which consists of 50 ad hoc topics 410-450. We use all collections on Tipster disks 4&amp;5 (no CR). Each query consists of two parts: TITLE and DESCRIPTION. We formed query vectors based on  X  X ITLE X  field with some preprocessing, such as stemmi ng and tossing out stop words. Table 7 shows the pe rformance of document re-ranking on top M retrieved documents ( K =10, N =5), which demonstrates that document re-ranking improves MAP from 9.8% to 17.2%. To see whether the improvement is significant, we also conducted paired t-tests, in which MAPs of the 150 topics are regarded as sampled observati ons. Table 7 also lists the significance marks. This suggests th at the method also applies to English data. In comparison with ot her approaches, Table 8 shows the performance of our approach ( K =10, N =5) and the score regularization approach [5] with the Okapi BM25 scores. From Table 8, our approach achieves comparable performance when more than 500 documents are re-ranked, and achieves better performance than theirs when less than 500 documents are re-ranked. Table 8 Comparison: Score regularization based &amp; ours This paper proposes a novel doc ument re-ranking approach in information retrieval. It is done by using a label propagation-based semi-supervised learning algorithm to integrate labeled data with unlabeled data. available in IR, we try to automatically create some pseudo labeled data from the initial retrieval. Given an initial ranked list of retrieved documents, our approach extracts a set of documents from the top ones vi a cluster validation-based k-means clustering as pseudo releva nt data and picks a set of documents from the bottom ones as pseudo irrelevant data while recasting the whole initial ranked lis t of retrieved documents as manifold structure underlying in the retrieved documents can contribute to the ranking via label propagation. consistency assumption that sim ilar examples in a high-density area should have similar labels. Our experiment demonstrates effectiveness in document re-ranking approach. example, the quality of the pseudo relevant documents is a key factor in document re-ranking, and we will explore more effective approaches to find pseudo relevant documents. As another example, there are also other semi-supervised methods being proposed to determine the manifold structure of data, we may compare these methods in document re-ordering. In addition, we may apply this method to other data sets as well. [1] Balinski, J., Danilowicz, C. 2005. Re-ranking Method [2] Bear J., Israel, D., Petit J., Martin D. Using Information [3] Belkin, M., &amp; Niyogi, P.. 2002. Using Manifold Structure [4] Crouch, C., Crouch, D., Chen, Q. and Holtz, S. 2002. [5] Diaz, F., Regularizing Ad Hoc Retrieval Scores. In the [6] Kamps, J. 2004. Improving Re trieval Effectiveness by [7] Kurland O., Lee L. 2005. PageRank without Hyper-links: [8] Lee K., Park Y., Choi, K.S. 2001. Document Re-ranking [9] Lin, J. 1991. Divergence Measures Based on the Shannon [10] Liu, X.Y and Croft W.B, 2004. Cluster Based Retrieval [11] Luk, R. W. P., Wong, K. F. 2004. Pseudo-Relevance [12] M. Mitra., A. Singhal. and C. Buckley. 1998. Improving [13] Niu Z.Y., Ji D.H., and Tan C.L. 2005. Word Sense [14] Niu Z.Y., Ji D.H., and Tan C.L. 2004. Document [15] Qu, Y.L., Xu, G.W.,Wang J. 2000. Rerank Method Based [16] Szummer, M., &amp; Jaakkola, T.. 2001. Partially Labeled [17] Xu J., Croft, W.B. 1996. Query Expansion Using Local and [18] Xu J., Croft, W.B. 2000. Improving the Effectiveness of [19] Yang L.P., Ji D.H. 2005(a). Chinese Information Retrieval [20] Yang L.P. Ji D.H. and Leong M.K. 2005(b). Chinese [21] Zhang B.Y, .Li H., Liu Y., Ji L., Xi W., Fan W., Chen Z., [22] Zhu, X. &amp; Ghahramani, Z. 2002. Learning from Labeled [23] Zhu, X., Ghahramani, Z., &amp; Lafferty, J. 2003. Semi-
