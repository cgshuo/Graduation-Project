 Corruption of data by class-label noise is an important prac-tical concern impacting many classi fi cation problems. Stud-ies of data cleaning techniques often assume a uniform label noise model, however, which is seldom realized in practice. Relatively little is understood, as to how the natural la-bel noise distribution can be measured or simulated. Using email spam-fi ltering data, we demonstrate that class noise can have substantial content speci fi c bias. We also demon-strate that noise detection techniques based on classi fi er con-fi dence tend to identify instances that human assessors are likely to label in error. We show that genre modeling can be very informative in identifying potential areas of mislabel-ing. Moreover, we are able to show that genre decomposition can also be used to substantially improve spam fi ltering ac-curacy, with our results outperforming the best published fi gures for the trec05-p1 and ceas-2008 benchmark collec-tions.
 H.3.3 [ Information Search and Retrieval ]: Information fi ltering Measurement, Performance, Reliability class noise, email spam fi ltering, genre classi fi cation
Contamination of training/test data by class-label noise is an important practical concern that impacts the applicabil-ity of machine learning techniques to classi fi cation problems. With signi fi cant levels of such noise during model creation, learners that try to fi t the data too closely are in danger of over fi tting, which would result in poor test-time perfor-mance. When class noise is present at test time, there is a potential for wrongly interpreting the results and misesti-mating the classi fi cation accuracy.

The presence of class noise is particularly problematic when high accuracy is required. In the email spam fi ltering domain it was con fi rmed that the presence of class label noise signi fi cantly decreases the expected performance of spam fi l-ters, particularly those that perform the best on noise free data [18]. Moreover, it was found that naturally occurring class noise is non-uniform, and the bias in the noise distrib-ution poses a more di ffi cult problem for the classi fi ers than noise distributed uniformly. When labeled data come from a large number of users, another type of a noise-related issue may crop up. While the personal spam/ham assignment by each user may be correct, the same kind of a message may be labeled as ham by some users while spam by others, which is also known as the  X  X raymail e ff ect X  [21]. More generally, various labeling inconsistencies may plague the evaluation data, which makes it di ffi cult to arrive at a gold standard.
While it is generally accepted that class-label noise makes the creation of accurate models more di ffi cult, it is not fully understood why natural distribution of such noise tends to pose a more di ffi cult challenge than a distribution that is random. In particular, in the spam fi ltering context, a recent spam fi ltering competition 1 revealed that a realistic distri-bution of class noise may reduce some solutions to random guessing. In this work, we seek to quantify why natural distribution of class noise is more problematic and gain in-sight into how classi fi er design can utilize such information in order to maintain acceptable performance. The paper is organized as follows: In Section 2 the problem of class noise in the spam fi ltering domain is presented in more de-tail. Section 3 discusses related work. In Section 4 we out-line the class-noise measurement framework and contrast the methodologies relying on human assessors vs. those relying on automatic techniques. Section 5 proposes genre based decomposition as a vehicle for better understanding of class noise distributions, also with application to improving the quality of fi ltering by using genre-based features. Section 6 presents the experimental framework, with the results dis-cussed in Section 7. The paper is concluded in Section 8.
Email spam has become a major nuisance for Internet users and a great amount of e ff ort has been spent on trying to eradicate it. Many di ff erent techniques have been applied http://www.ceas.cc/2008/challenge/ to the fi ltering problem and although some of them are quite e ff ective, the majority of SMTP email tra ffi cremainsspam and individual users still continue to receive spam in their inboxes. The spam fi ltering problem can be addressed from the machine learning and data mining perspectives. Email messages represent semi-structured text documents, which in addition to content, also contain information about the sender and the recipient as well as some routing data about the path that the message took on its way to the end user. Spam email is de fi nedasanUnsolicitedCommercialMes-sage (UCE) and so for each message there is underlying truth signifying whether or not the message was solicited and com-mercial. Spam fi ltering can therefore be addressed as a su-pervised learning problem [17], where labeled data is used to extract patterns and learn classi fi cation models that are then applied to unlabeled messages in order to separate spam from legitimate emails, which are known as  X  X am X . What makes the spam fi ltering problem particularly challenging is the dynamic nature of email, where content tends to natu-rally change over time, often quite unpredictably, and where learning is inherently adversarial [10], with spammers con-tinuously working on bypassing existing fi ltering defences. Recent years have seen a growing interest in the spam fi l-tering problem from the research community and the body of literature devoted to this problem has grown to be quite large(see[7]foracomprehensivereview).

In the spam fi ltering domain, class noise poses di ff erent challenges depending on whether one treats the problem from a personal fi ltering or community fi ltering perspec-tive. In the personal spam fi ltering context, the labels of the training data are provided by the target user, who ulti-mately should always be able to determine if a piece of mail is spam or not. Nevertheless, people make mistakes, which may be due to a variety of reasons, e.g., lack of attention, confusing user interface, ambiguity of content (e.g., in the case of emails from a commercial entity, a user may treat some of them as spam and some not). Also, the de fi nition of spam depends on the  X  X olicitation X  by the target recip-ient. However, due to the nature of business relationships a user may in fact solicit mail from various entities without being aware of it. Consequently, many user-provided labels re fl ect the wanted/unwanted nature of each email, rather than the desired solicited/unsolicited.

When fi ltering for a community, the problem is compounded by the fact that spam de fi nitions are personal. Many com-mercial emails are sent out in campaigns, where highly sim-ilar content is sent to multiple recipients. These messages may have been solicited by only a fraction of the recipi-ents or else may be wanted by only a fraction of them. In either case, one cannot perform a campaign-wide decision that would satisfy all recipients, which makes the fi ltering problem ambiguous.

A variant of the community e ff ect occurs when a group of assessors is asked to label messages that were in fact sent to di ff erent recipients. In such situations (common in dataset preparation), it is sometimes very di ffi cult to tell if a message may have been solicited or wanted by the recipient, and in e ff ect the assessors X  labels may be di ff erent from one that the target user would have assigned to the same message.
Research in the machine learning community has been fo-cusing on detecting noisy training cases and either removing them from the training set or trying to assign them their cor-rect label [5][14]. The relationship between the amount of label noise present and the deterioration of classi fi er perfor-mance has also been investigated.

The problem of the impact of data noise on the e ff ective-ness of machine learning and data mining procedures has been studied from the class noise [5] and attribute noise perspectives. Attribute noise often arises from pure mea-surement errors and data corruption, which may for exam-ple result in missing attributes, misspelling of text, etc. The problem of class noise tends to be more challenging, since mislabelings are hard to distinguish from naturally occurring outliers, which represent legitimate rare manifestations of the target class. Detection of mislabeled instances depends on the assumption that such instances tend to be classi fi ed with lower con fi dence. Thus when many di ff erent classi fi ers are built [5][20], some of them are likely to disagree about the label of a mislabeled instance. The diversity of opinion in the ensemble can be increased when each classi fi er sees only a portion of the data, which is quite natural in distrib-uted data environments [22]. In the case of a single model on the other hand, a mislabeled instance is likely to be clas-si fi edasbelongingtoaclassdi ff erent from the one stated, or even if it is classi fi ed to the class stated, the classi fi cation con fi dence (e.g., measured by the instance X  X  distance from the decision boundary) is likely to be low.

Given that noisy instance detection is by itself imperfect, a question arises as to the optimal use of the information returned by such a process. Approaches studied in the lit-erature include instance removal [5], correction [14][18] and weighting [16]. The results reported indicate that removing the potentially mislabeled instances tends to be more ben-e fi cial than trying to correct them. Also, more recently it has been demonstrated that weighting instances according to their mislabeling con fi dence tends to outperform instance removal [16].

One of the problems associated with studying the class noise problem is the di ffi culty of obtaining the ground truth, especially for large datasets. Typically, researchers assume that the original data is noise free (which is not necessar-ily the case) and apply an arti fi cial noise model, e.g., by changing class labels for a fraction of instances uniformly at random. Such a process, however, is unlikely to re fl ect the reality where human assessors make labeling mistakes for instances that are particularly confusing for humans. This has been recognized, for example, in [5] where for multilabel datasets the authors identify pairs of classes that are prone to be confused during labeling. This represents a process closer to capturing naturally occurring class noise and re-sultsindicatethatrulebasednoiseposesindeedatougher problem when compared to the uniform model. The pro-cedure used in [5] requires domain knowledge and is not applicable to two-class problems. It also assumes that for pairs of classes where confusion is possible, all instances are equally likely to be confused.
Some of the most e ff ective ways of detecting class noise in a data set are based on classi fi er ensembles [5][20]. The original training data are used to induce a number of di ff er-ent classi fi ers, which are then run on the data to be cleansed and, for each instance, the amount of classi fi er disagreement is measured. Instances where this disagreement is high are considered as potential labeling errors. There are some chal-lenges in measuring classi fi er disagreement this way, since the base accuracy of individual classi fi ers may be di ff er-ent and thus equally weighting their contributions is not always appropriate. Alternatively, if only a single, but well-calibrated, classi fi er is present, instances which the classi fi er determines to have a di ff erent class label with high con fi -dence can be considered as potentially mislabeled. In both approaches there is a risk that instances that are not misla-beled but hard to classify might be identi fi ed as noise.
For a dataset D = { ( x i ,y i ) } m i =1  X  X X Y , existing meth-odsrelyonassigningtoeachinputpair ( x i ,y i ) a reliability measure r i that quanti fi es (not necessarily in a calibrated way) the probability estimate that the true label of x i is indeed y i i.e., where l i  X  Y isthetruelabelof x i .Whentheobjectiveis to clean the training data, instances for which r i &lt;T are removed from the dataset, are  X  X orrected X  to their estimated true label or are weighted according to some function of r There are reasons to believe that at least some instances identi fi ed in this way as label noise are in fact outliers or instances that are correctly labeled but ambiguous. This is because,  X  X orrecting X  the label to the one predicted by a classi fi er ensemble (or a single classi fi er) tends to lead to lower overall accuracy than simply removing the problematic instances from the training pool [18]. We are more interested in estimating the noise level in D , which can be expressed as where T is a user-speci fi ed threshold that a ff ects the accu-racy of the estimate.

It should be noted that committee or con fi dence-based class noise detection re fl ects the protocols employed in data preparation with human labelers. For example, for datasets used by the Text Retrieval Conference (TREC 2 )underthe patronage of NIST, oftentimes each data instance is eval-uated by several assessors and instances for which there is disagreement may be adjudicated by another expert. Also, in many labeling interfaces found in practice, one has the option of associating some form of con fi dence with the la-beling decision so as to allow the target system to identify potential problem cases.

People can make labeling mistakes for a variety of reasons, not necessarily related to the di ffi culty of the task at hand (e.g., lack of concentration, carelessness, etc.). It is therefore unclear if instances identi fi ed as  X  X oisy X  by humans would overlap with instances deemed as noisy according to an au-tomatic data cleaning technique. In this work we attempt to quantity the agreement between these two methodologies using email data. http://trec.nist.gov Table 1: Email genres: note that many can contain both ham and spam. category description advertising commercial o ff ers listserv discussion forums newsletter periodic news/bulletin updates personal person to person communications scam phishing, 409/Nigerian scams transactional order con fi rmations, noti fi cations, alerts
While assessing the overall (or per class) noise level is useful, it inherently assumes that mislabeling is more or less uniform. I.e.,ifweweretosimulatethesameamountof noise for a noise free dataset, in the absence of further infor-mation we would resort to altering the labels for a fraction of randomly chosen instances. This type of uniformity does not have to be the case, however, and is likely to be far from a realistic scenario. It is reasonable to expect that label-ing errors would be concentrated near the optimum bound-ary separating the classes in the input space. Thus, if this boundary were known (and of course it is not in practice), one might generate arti fi cial class noise by varying the prob-ability of label errors inversely proportionally to the distance between an instance and the optimum decision boundary. One can argue, however, that such a methodology, while general, does not shed much light on why the class noise oc-curs in a particular domain and its application is thwarted by the uncertainty of where the optimum decision boundary is located, which after all is the problem we are trying to solve in the fi rst place.

In the email domain one can expect that certain types of messages (e.g., commercial advertising and phishing) will be harder to label than others (e.g., personal correspon-dence), especially if label assignment is performed by some-body other than the original recipient. More generally, we can expect that for some domains the input data can be subdivided into a number of subregions sharing common characteristics (e.g., similarity of content). We hypothe-size that di ff erent regions may exhibit di ff erent amounts of class-noise, which provides a domain speci fi cquanti fi cation of the labeling noise bias. A key question is how to choose the regions in which the amount of class noise is measured. For text-classi fi cation problems one possible way of accom-plishing this is to project the original instances onto a fi xed content-based taxonomy (e.g., DMOZ 3 ). One has to be care-ful when choosing a taxonomy, however, since with a large number of potential categories some may receive too few in-stances for e ff ective estimation to be possible. Alternatively, the regions could be identi fi ed via clusters naturally occur-ring in the data, although the results of clustering are not always easy to interpret. In this work, we propose to de fi ne the regions via a number of email speci fi c content categories, shown in Table 1, which are believed to capture the most common usage patters. Similar categories could be de fi ned for other problem domains. It is important to note that gen-res can span the class boundary. For example, both spam http://www.dmoz.org and ham can contain messages advertising products. This makes the genre decomposition di ff erent from past e ff orts, where each class was subdivided into categories speci fi cto that class, so as to facilitate cost-sensitive learning [13].
Once the distribution of class noise is known it is possible to simulate the impact of natural mislabeling errors over a noise free dataset. An obvious question, however, is whether the distribution of class noise estimated using one particular dataset re fl ects global mislabeling tendencies or if it is in fact speci fi c to users associated with these data. In this work we consider this question by using data from two di ff erent data sources.
There is certain similarity between characterizing class-noise by region and using content based reliability indica-tors when fusing the outputs of several classi fi ers [2] . In the fi rst case, the knowledge of a region membership for an instance provides a level of information to the learner about the extent to which the label assignment can be trusted. In the latter case, the knowledge of a region membership al-lows one to rank the classi fi ers according to their expected reliability (e.g., to pick the most promising one) or even to use the reliability information directly in merging the clas-si fi er outputs. Given that classi fi ers tend to fi nd noisy data quite challenging, it is interesting to consider to what an ex-tent region-membership information might provide a useful feature to a learning algorithm, which would allow it to dis-tinguish between labels that are potentially noisy and the ones that are likely to be solid. We therefore consider a transformation of the original training set into where R captures the information associated with the region membership. This could consist simply of the region label (e.g., one of the symbolic names in the fi rst column of Table 1). However, R could also contain the class-noise estimate for each label and/or the con fi dence associated with assign-ing x i to R i . Note that in the extreme case, each instance can be considered to represent a separate region and if then R i = r i (i.e., R i is the instance-level label reliability indi-cator) one arrives at the instance-based weighting approach to class noise mitigation proposed in [16].
Here we analyze two datasets exhibiting di ff erent aspects of community based noise. The trec05p-1 dataset was used in the 2005 TREC Spam Filtering Competition. It contains messages from the Enron public corpus, as well as additional ham and spam emails collected from private users. Where possible the messages were assigned labels by the original recipients and were otherwise carefully adjudicated by two expert assessors to provide reference labels. While the re-sulting gold-standard label set is not class-noise free, it is believed that the noise level is low (i.e., around 0.5%). In the trec05p-1 dataset, in addition to labels provided by target users each message was labeled by a variable num-ber of volunteer human assessors who were not the target recipients of that message. This large scale labeling e ff ort (known as the SpamOrHam project) provides a unique view into human judgement disagreements pertaining to email spam. In particular, it is possible to measure inter-assessor disagreement, as well as the disagreement between the asses-sors and the gold standard. The number of assessor labels per message is quite varied, with some messages receiving no judgements and some receiving multiple ones, with the median being around 4.

The ceas-08 dataset corresponds to a sample of messages collected from a large community of users of a major ISP (the pool of users was larger and more diverse than in the case of trec05p-1 ), where each labeling decision was per-formed by the target recipient. The data was used as the private corpus in the Spam Filtering Competition 4 held in conjunction with the 2008 Conference on Email and Anti-Spam (CEAS-08 5 ). Collectively, this dataset captures the noise related to emails of similar content being considered to be spam or ham by di ff erent users. The class noise level for this dataset is believed to be high and, in fact, we expect that poor performance of some the classi fi er entries in the CEAS 2008 competition, which used this dataset, was due the signi fi cant presence of class noise.

For the trec05p-1 dataset, it is possible to study the ef-fects of class noise by randomly fl ipping the labels provided by the gold standard (this has been done in [18]). It is also possible to compare the e ff ect of uniform fl ipping to that of using natural noise introduced by human assessors. This type of study was performed in [18], determining that nat-ural noise tends to be more di ffi cult than a random one, but it was not quanti fi ed what is the nature of this natural noise. One could conjecture that it should correspond to messages that are rather ambiguous (i.e., closer to the decision bound-ary), but one could also expect that certain types of content are more ambiguous than others. One possible way of as-sessing such a relationship is to project each email message onto a fi xed taxonomy. Given the knowledge of such a pro-jection for all messages in a corpus, together with the gold standard and assessors-provided labels, one can then mea-sure the prevalence of class-noise on a per-category basis. A question arises if the natural noise represents the most di ffi cult type of noise possible (from the standpoint of a classi fi er), or if in fact a more di ffi cult setup could be ac-complished by altering the distribution of class noise among di ff erent categories.

Another question is whether the natural distribution of class noise is consistent with the one that can be derived from the classi fi er disagreement achieved when training a number of classi fi ers over the gold standard. I.e., to the ex-tent that classi fi er disagreement detects outliers as well as class noise, to what a degree outliers detected by an auto-mated method re fl ect human disagreements.
In all experiments we followed the on-line procedure, whereby the messages in each dataset were ordered according to their arrival time, and when evaluating any particular message only earlier messages could be used to provide model infor-mation. This type of an experimental setup is most realistic for email fi ltering, since it acknowledges the strong depen-dence of both ham and spam on the time axis [9].
Our overall objective is to model the distribution of label-ing errors, for the purpose of achieving better understanding http://www.ceas.cc/2008/challenge/ http://www.ceas.cc/2008/ of their nature, better estimates of fi lter e ff ectiveness and, ultimately, better spam fi lters. We consider the e ff ect of three factors of labeling error: self-reported classi fi er con fi -dence, agreement within a committee of classi fi ers, and the genre of the labeled message. We determine the in fl uence of these factors through their ability to predict disagreement among human adjudicators and through their overall con-tribution to fi lter e ff ectiveness. To this end, we tested four speci fi chypotheses.
 E1: Does classi fi er agreement predict label noise ? To E2: Does classi fi er con fi dence predict label noise ? We E3: Does message genre predict label noise ? To test E4: Does partitioning by genre improve performance ? E5: Is population-speci fi c genre adjudication necessary to
Messages from the TREC 2005 Public Spam Corpus trec05p-1 6 were used for our experiments E1 through E4 .Thegold standard labels associated with the corpus were used only to evaluate the results of E4 ;theyplayednoroleintrain-ing the classi fi ers or in estimating noise. For these purposes we used labels rendered by participants in the SpamOrHam Internet labeling e ff ort [12]. The TREC corpus contains 92,189 messages, 39,399 labeled ham and 52,790 labeled spam. SpamOrHam comprises 342,771 for messages selected at random from the corpus, with replacement, an average of 3 . 7 per message in trec05p-1 . The messages form a chronolog-ical sequence that is presented to the spam fi lter for on-line classi fi cation, following the TREC methodology [8].
From trec05p-1 , we selected only those messages for which there was at least one SpamOrHam label, and from those la-bels, selected one at random for use as a training label. For estimating noise in E1 , E2 and E3 , we selected those mes-sages for which there were three or more SpamOrHam labels, and from those labels, we selected three at random (not necessarily including the training label) to form the human tribunal. Because the SpamOrHam acquired labels for mes-sages selected at random, the training messages form an independent identically distributed (i.i.d.) sample of the TREC messages, and the estimation labels are furthermore an i.i.d. sample of the training messages. For evaluating overall spam fi ltering e ff ectiveness in E4 we use the TREC gold standard labels. To construct the genre classi fi er in E3 , the authors adjudicated a total 3 , 239 messages for member-ship in each of the six genres.
 The CEAS 2008 [1] corpus was used exclusively for E5 . No statistics, labels, or messages from the corpus were used to tune the genre classi fi er or the ensemble members. The CEAS corpus was collected from messages delivered to clients of a large service provider. The labels were rendered by the clients themselves, in response to adjudication requests pre-sented by the user interface for randomly selected messages.
The corpus CEAS contains 198,574 messages, of which 89,451 are labeled ham, and 109,123 spam. There is exactly one label per message, so it is not possible to measure la-bel noise directly. Filter evaluation results suggest that the overall noise level is comparable to that of the SpamOrHam labels  X  about 6% .

The classi fi er tribunal consisted of three spam fi lters known to exhibit state-of-the art performance: DMC [4], ROSVM [19] and Bogofilter [15]. Our implementation [6] of on-line gra-dient descent logistic regression [11], itself a state-of-the-art spam fi ltering method, was used for the genre categoriza-tion, for the genre-speci fi c ensemble members, as well as for the overall meta-classi fi er. All classi fi ers performed either a very simple extraction of word token features from each message,orreliedoncharactern-gramsasanevensimpler document representation. trec.nist.gov/data/spam.html Table 2: Individual spam fi lter e ff ectiveness at pre-dicting unanimity among a tribunal of human asses-sors.
 E1: Agreement between classi fi er and human tribunals . DMC , E2: Predicting agreement from classi fi er con fi dence. Figure 1: Distribution density of trec05-p1 mes-sages as a function of the absolute value of the score assigned to them by logistic regression. A corre-sponding distribution of class-noise (measured by human assessor disagreement) is shown for compar-ison.
 Table 3: Email genres: note that many can contain both ham and spam. Table 4: Number of messages ( n ) , human tribunal disagreement (1  X  u ) , and noise ( e ) accordingtogenre. Ham messages only. Table 5: Number of messages ( n ) , human tribunal disagreement (1  X  u ) ,andnoise ( e ) according to genre. Spam messages only. E3: Predicting noise from genre . Under the assump-E4: Improving classi fi er performance . First the mes-Table 6: TREC corpus AUC and LAM scores for genre-speci fi c committee of experts, single overall classi fi er, and metaclassi fi er combining the two. Table 7: CEAS corpus AUC and LAM scores for genre-speci fi c committee of experts, single overall classi fi er, and metaclassi fi er combining the two. E5: Transfer learning to another corpus . We repeated
Table 8: E ff ect of CEAS-speci fi c genre training.
After conducting E5 we conducted a sequel ( E5a )tode-termine whether or not training the genre classi fi er on population-speci fi c examples would improve performance. About 100 CEAS messages per genre were adjudicated using the same active learning technique in E3 . These messages were added to the training examples used to induce the genre catego-rizer.Theresult,detailedintable8showednomeasurable improvement. This would suggest that email genre de fi ni-tions are rather stable and it is encouraging from the per-spective of applying this method in practice, since measur-able improvements in performance can be gained without re-launching an expensive corpus-speci fi c labeling e ff ort.
We investigated the nature of class noise in the spam fi l-tering domain using two of the largest available datasets. Our results indicate that label noise has a clear content-basedbias,withcertaingenresofemailbeingmuchmore likely to confuse than others. We hypothesize that similar patterns could be found in other classi fi cation problems in-volving text. Experiments demonstrate that email messages that automatic classi fi ers, such as logistic regression, fi nd confusing correspond quite closely to messages that human assessors are likely to fi nd confusing. Thus classi fi er con fi -dence based data-cleaninig methods can be thought of as a good substitute for the more expensive approach of having each message reviewed by a number of human judges.
We proposed a method of quantifying the content bias in the distribution of class noise based on genres. While the particular genres used in this study are email speci fi c, they could easily be rede fi ned for any particular domain. Our results show than for genres spanning the class boundary, label noise is more of a problem for the class in which the genre is less likely to be present. Interestingly, incorporat-ing genre membership indicators into the classi fi er learning process also leads to signi fi cant performance improvements, with the results obtained in this work outperforming pre-viously reported results for the two datasets in question. Moreover, the improvements appear to be stable even if the genre de fi nition is transferred across collections. Further re-search into the e ff ective use of genre information in classi fi er design will be the subject of future work. [1] The CEAS 2008 live spam challenge. [2] P.N.Bennett,S.T.Dumais,andE.Horvitz.
 [3] A. Bradley. The use of the area under the ROC curve [4] A. Bratko, G. V. Cormack, B. Filipi  X  c, T. R. Lynam, [5] C.E.BrodleyandM.A.Friedl.Identifyingmislabeled [6] G.V.Cormack.Universityofwaterlooparticipation [7] G.V.Cormack.Emailspam fi ltering: A systematic [8] G.V.CormackandT.R.Lynam.TREC2005Spam [9] G.V.CormackandT.R.Lynam.On-linesupervised [10] N. N. Dalvi, P. Domingos, Mausam, S. K. Sanghai, [11] J. Goodman and W. tau Yih. Online discriminative [12] J. Graham-Cumming. SpamOrHam. Virus Bulletin , [13] A. Ko  X  czandJ.Alspector.SVM-based fi ltering of [14] S. Lallich, F. Muhlenbach, and D. A. Zighed.
 [15] E. S. Raymond, D. Relson, M. Andree, and G. Louis. [16] U.RebbapragadaandC.E.Brodley.Classnoise [17] M. Sahami, S. Dumais, D. Heckerman, and [18] D. Sculley and G. V. Cormack. Filtering spam in the [19] D. Sculley and G. M. Wachman. Relaxed online [20] S. Verbaeten and A. V. Assche. Ensemble methods for [21] W. Yih, R. McCann, and A. Ko  X  cz. Improving spam [22] X. Zhu, X. Wu, and Q. Chen. Eliminating class noise
