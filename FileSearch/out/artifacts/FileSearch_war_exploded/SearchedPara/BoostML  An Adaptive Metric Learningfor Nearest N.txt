 Nearest Neighbor (NN) methods for pattern recognition are widely applicable and have proven to be very useful in machine learning. Despite their simplicity, their performance is comparable to other, more sophisticated, classification and regression techniques. A nearest neighbor classifier works by assigning to a query point the label of the majority class in its neighborhood. Their only assumption is smoothness of the target function to be estimated. Therefore each point in the neighborhood votes for the prediction based on its distance from the query point.
The performance of a nearest neighbor classifier depends critically on two major factors: (a) the distance metric used and (b) K , size of the neighbor-hood. K denotes the number of nearest neighbors. The size of the neighborhood controls the smoothness of the predicted function and is usually tuned through cross-validation. A small K implies small bias but high variance, and vice-versa. Typical  X  X etric Learning X  algorithms aim at finding a metric that results in small intra-class and large inter-class distances [1,2].  X  X etric Learning X  has also been introduced as a bias reduction strategy in high dimensions [3]. In this paper we focus on the latter version, that is optimizing a distance metric to reduce bias. Our goal is an optimal metric that depends on the problem at hand, as charac-terized by the respective class distribution and, within a given problem, on the location of the query point in that space.

Bias can be introduced due to a variety of reasons. The primary reason for the introduction of bias is the  X  X urse-of-dim ensionality X  (COD) effect. Let us consider training data of size N drawn from a uniform distribution in a p -dimensional unit hypercube. The expected diameter of a K = 1 neighborhood using Euclidean number of dimensions, a very large amount of training data is required to make even a K = 1 nearest neighborhood relatively small. This has the consequence that the bias can be large even for the smallest possible value of K .Biascan be reduced by learning a metric that gives no influence to the irrelevant fea-tures (feature selection). This removes irrelevant features, thereby reducing the dimensionality. This in turn reduces the diameter of the K-NN neighborhood hence lowering the bias.

In order to understand why metric learning can improve classification perfor-mance, we need to analyze the classificat ion performance itself. In a classifica-tion scenario, all that is required to obtain an optimal decision is that largest estimated class probability correspond to the class with the largest true prob-ability, irrespective of its actual value or the values (or the order) of the esti-mated probabilities for the other classes, as long as the following equation holds:  X  j :max  X  f j ( x )=max f j ( x )where j is class index,  X  f is the estimated probability, and f is the unknown true probability. It can often be the case that bias, though very large, affects each of  X  f j ( x ) in same way so that their order relation is similar enough to that of true underlying probabilities { f j ( x ) } J 1 to realize an optimal class assignment [3]. But the requirement of a large neighborhood for high di-mensions and the presence of irrelevant features can affect bias differentially for the respective class probability estimates enough to cause non-optimal assign-ment, therefore decreasing classificatio n performance. This differential bias can be reduced by taking advantage of the fact that the class probability functions may not vary with equal strength or in the same manner in all directions in the measurement space. Elongating a neighborhood in the directions in which class probabilities do not change and constricting along those dimensions where class probabilities do change X  X y choosing an appropriate metric X  X ot only re-duces bias, but will also result in smoother class conditional probabilities in the modified neighborhood, resulting in bette r classification performance (refer to figure 1).

In this paper we propose a technique for local adaptive metric learning to reduce bias in high dimensions. As will be discussed in section 2, current work in adaptive metric learning determines feature relevance at a query point using some numerical index. This index gauges the relevance of a feature and controls the form of the metric around the query point. Our proposed index is inspired by work in the field of boosting [4], where at each iteration data is partitioned across the most discriminative dimension. The index is based on the logit-transform of the class probability estimate. In our work using this index, we pick the dimension that is most discriminative. This is similar to  X  X oosting classifiers X  where at each iteration a feature is sel ected on which the data can be classified most accurately based on the weight distribution. Friedman in [3] proposed a technique for reducing bias in high dimensional ma-chine learning problems. Our work is inspired by this paper. The main difference of our work from [3] is feature relevance determination at each step. We have used a measure inspired by the boosting literature, wherea s in [3] a GINI-like (entropy-based) index is used for feature relevance. In our work a feature is deemed more relevant if it is more discriminatory, but in [3] a feature is consid-ered relevant if the class label varies the most.

In [5], Hastie and et al. proposed an adaptive metric learning algorithm (DANN) based on linear discriminant analysis. A distance metric is computed as a product of properly weighted within-an d between-class sum-of-squares matri-ces. The major limitation of their method is that, in high dimensions there may not be sufficient data to fill in p  X  p within-class sum-of-square matrices (due to sparsity). In our work we estimate only the diagonal terms of the matrix.
Some other notable techniques for adaptive metric learning are proposed in [6,7,8,9]. In [6] an algorithm is proposed for adaptive metric learning based on analysis of the chi-squared distance. Similarly, an algorithm for metric learning has been proposed in [7] that uses SVM-b ased analysis for feature relevance. A similar, but slightly modified, method for metric learning based on SVMs is proposed in [9]. As will be discussed in section 3, our method differs from these methods in the sense that it is r ecursive. We recursively home in around the query point and the estimated metric is modified iteratively. In the above mentioned methods, however, a metr ic is estimated in a single cycle.
Other research on query-sensitive metric methods includes Zhou et al. [10], who investigated a query-sensitive metric for content-based image retrieval. In this section we describe our two algorithms, BoostML1 and BoostML2, for adaptive metric learning. BoostML2 is a variant of BoostML1. In the following discussion we will denote the query point by x 0 and training points by x n ,where n =[1 ,...,N ], and N is the number of training data. P denotes the number of features, and x 0 p and x np denote the value at the p th feature of the x 0 and x n data points respectively. 3.1 Feature Relevance We start by describing our local feature relevance measure technique. The fea-ture used for splitting is the one that maximizes the estimated relevance score p ( x 0 ) as evaluated at query point x 0 . The estimate of relevance is: p  X  ( x 0 )= argmax 1  X  p  X  P c p ( x 0 )where c p ( x 0 ) is defined as: and I p ( x 0 ) is defined in the following equation: The in equation 2 is introduced for numerical tractability. Small I p (close to zero) implies that there is an equal split of positive and negative training data points in the neighborhood of x 0 , whereas large I p implies that one class dominates the other class. The computation of Pr( c | x np = x 0 p ) in equation 2 is not trivial, as we may not have sufficient data in the neighborhood of the query point to accurately define the probability. The probabilities in equation 2 are computed as in equation 3.
 A small neighborhood around query point x 0 denoted by N ( x 0 ) is defined and a value of  X  p is chosen to make sure that the neighborhood contains L points: In other words, we look for L points that are close to the query point on feature p and compute the probabilities in equation 2 using these points. The output of feature relevance analysis is a p  X  p diagonal matrix, the diagonal terms of which are the estimated relevances of th e features. Based on equation 1 we can write the distance metric as a matrix A for local relevance (equation 5). This local metric is used t o measure distances.
 3.2 Details of the Algorithm Given a query point x 0 and training data { ( x n ,y n ) } N n =1 , the goal is to estimate the label of the query point. Our method starts by initializing the neighborhood of the query point to be the entire measurement space ( R 0 ). The neighborhood is split in two based on the feature that maximizes the relevance score. Thus for the same training data, different fea tures can be selected for this split for different query points x 0 based on the relevance of that feature at that location in the input measurement space. The split divides the input measurement space into two regions: R 1 ( x 0 ), that contains the query point and the M 1 training points that are closest to it on the chosen feature, and other (complement) region R 2 ( x 0 ), that contains the N  X  M 1 points that are farthest from x 0 on that feature. R 2 ( x 0 ) is removed from further consideration. Thus the result of the split is just one region, R 1 ( x 0 ). The above procedure is then applied again on region R 1 ( x 0 ). We have named this method BoostML1 and its outline is given in algorithm 1. Refer to figure 1 for an illustration of BoostML1 algorithm. Algorithm 1. BoostML1: Local Adaptive Metric Learning Algorithm
As can be seen in algorithm 1, the splitti ng procedure is recursively applied until there are only k training observations left in the final neighborhood. The metric A (equation 5) obtained at the final step is used to measure the distance to the k nearest neighbors that predict the label of query point. At each step, a region is split on the feature that is estimated to be most relevant in terms of capturing the variation of the target functions within that region. All diagonal terms of the A matrix in equation 5 are ignored except the one with the maximum value, which is retained to split the region at each step. This is a greedy approach which is not necessarily effective all the time. BoostML2 is a variant of the above method, but at every iteration it splits the region based on the metric defined by matrix A in equation 5, as computed in the current iteration. As will be shown in section 4, BoostML2 is an improvement on algorithm 1 and results in an improvement in classification performance.
 In this section we show the results of our adaptive metric learning algorithm on some well known databases from UCI Machine Learning Repository [11]. Databases were selected such that the com peting techniques perform best on at least one of the databases. The other competing local adaptive metric learning techniques against which we tested our algorithms are as follows:  X  k-NN: k nearest neighbor classifier with Euclidean distance.  X  DANN: Discriminative Adaptive Nearest Neighbor classifier based on [5]  X  ADAMENN: Adaptive metric nearest neighbor classification technique  X  Machette: Recursive partitioning algorithm as described in [3].  X  X cythe: This is a generalization of the Machette algorithm in which fea- X  X oostML1: Algorithm 1. The implementation details regarding tuning of  X  X oostML2: Variant of BoostML1 as described in section 3.2.
 To obtain error rates, we used leave-one-out cross-validation for the Iris, Ionosh-phere, Dermatology, Echocardiogram and Heart data sets. 10 rounds of two-fold cross-validation were used for the Credit and Diabetes data sets.

Our metric learning algorithm results in an improvement of k-NN classifica-tion. This improvement, however, doe s come at an extra cost. BoostML1 has introduced two new tuning parameters. The value of L in equation 4 determines bias-variance trade-off but does not effect the performance provided it is neither too small nor too large. A value of 20 for L was used in our experiments. The  X  parameter which controls the size of the neighborhood at each step is critical to the performance. A large value of  X  results in a better performance at an increased computational cost. A small value of  X  results in poorer performance, but will be faster. A tradeoff has to be ac hieved between computational cost and performance. A value of 0 . 8 is used in all our experiments.

Table 1 shows the average classification e rror rates for different techniques. Each database X  X  name is followed by number of data, features and classes. It can be seen that BoostML1 and BoostML2 perform well on the majority of data sets. It re-sults in significant improvement over the performance of the basic k-NN classifier, and also performs better than the competing algorithms in some cases.
To compare the robustness of our algorithm with other algorithms we used the technique described in [3]. This test measures how well a particular method m performs on average in situations that are most favorable to other procedures. Robustness can be measured by computing the ratio b m of its error rate e m and the smallest error rate over all other methods that are compared in that example. That is: The best method m  X  will have b  X  m = 1 and all other methods will have values larger than 1. The larger the value of b m the worse the performance is of the m th method in relation to the best one for that data set. Figure 2 shows the distribution of b m for each method over all data sets considered. BoostML2 turned out to be most robust among all the methods, with DANN coming second. In this work we introduced an adaptive metric learning algorithm based on an index inspired by work on boosting for reducing bias in high dimensional spaces. We tested our algorithm on a variety of well-known machine learning databases and found that our system performs be tter than several well known techniques for adaptive metric learning. This im provement, however, comes at an extra cost. Our algorithm is computationally expensive compared to simple k-NN. We had to introduce two new parameters, the values of which should be optimized. Though this complicates matters, other competing algorithms also have one or more tuning parameters, so it should not be taken as a major drawback of our algorithm.

