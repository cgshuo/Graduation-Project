 Letter-to-phoneme (L2P) conversion (also called grapheme-to-phoneme conversion) is the task of predicting the pronunciation of a word given its orthographic form by converting a sequence of letters into a sequence of phonemes. The L2P task plays a crucial role in speech synthesis sys-tems (Schroeter et al., 2002), and is an important part of other applications, including spelling cor-rection (Toutanova and Moore, 2001) and speech-to-speech machine translation (Engelbrecht and Schultz, 2005). Many data-driven techniques have been proposed for letter-to-phoneme conversion systems, including neural networks (Sejnowski and Rosenberg, 1987), decision trees (Black et al., 1998), pronunciation by analogy (Marchand and Damper, 2000), Hidden Markov Models (Taylor, 2005), and constraint satisfaction (Bosch and Can-isius, 2006).

Letter-phoneme alignment is an important step in the L2P task. The training data usually consists of pairs of letter and phoneme sequences, which are not aligned. Since there is no explicit infor-mation indicating the relationships between indi-vidual letter and phonemes, these must be inferred by a letter-phoneme alignment algorithm before a prediction model can be trained. The quality of the alignment affects the accuracy of L2P con-version. Letter-phoneme alignment is closely re-lated to transliteration alignment (Pervouchine et al., 2009), which involves graphemes representing different writing scripts. Letter-phoneme align-ment may also be considered as a task in itself; for example, in the alignment of speech transcription with text in spoken corpora.

Most previous L2P approaches induce the align-ment between letters and phonemes with the ex-pectation maximization (EM) algorithm. In this paper, we propose a number of alternative align-ment methods, and compare them to the EM-based algorithms using both intrinsic and extrin-sic evaluations. The intrinsic evaluation is con-ducted by comparing the generated alignments to a manually-constructed gold standard. The extrin-sic evaluation uses two different generation tech-niques to perform letter-to-phoneme conversion on several different data sets. We discuss the ad-vantages and disadvantages of various methods, and show that better alignments tend to improve the accuracy of the L2P systems regardless of the actual technique. In particular, one of our pro-posed methods advances the state of the art in L2P conversion. We also examine the relationship be-tween alignment entropy and alignment quality.
This paper is organized as follows. In Sec-tion 2, we enumerate the assumptions that the alignment methods commonly adopt. In Section 3, we review previous work that employs the EM ap-proach. In Sections 4, 5 and 6, we describe alter-native approaches based on phonetics, manually-constructed constraints, and Integer Programming, respectively. In Section 7, we propose an algo-rithm to refine the alignments produced by EM. Sections 8 and 9 are devoted to the intrinsic and extrinsic evaluation of various approaches. Sec-tion 10 concludes the paper. We define the letter-phoneme alignment task as the problem of inducing links between units that are related by pronunciation. Each link is an in-stance of a specific mapping between letters and phonemes. The leftmost example alignment of the word accuse [@kjuz] below includes 1-1, 1-0, 1-2, and 2-1 links. The letter e is considered to be linked to special null phoneme .

The following constraints on links are assumed by some or all alignment models:  X  the monotonicity constraint prevents links  X  the representation constraint requires each  X  the one-to-one constraint stipulates that each These constraints increasingly reduce the search space and facilitate the training process for the L2P generation models.

We refer to an alignment model that assumes all three constraints as a pure one-to-one (1-1) model. By allowing only 1-1 and 1-0 links, the align-ment task is thus greatly simplified. In the sim-plest case, when the number of letters is equal to the number of phonemes, there is only one pos-sible alignment that satisfies all three constraints. When there are more letters than phonemes, the search is reduced to identifying letters that must be linked to null phonemes (the process referred to as  X  X psilon scattering X  by Black et al. (1998)). In some words, however, one letter clearly repre-sents more than one phoneme; for example, u in Figure 1. Moreover, a pure 1-1 approach cannot handle cases where the number of phonemes ex-ceeds the number of letters. A typical solution to overcome this problems is to introduce so-called double phonemes by merging adjacent phonemes that could be represented as a single letter. For example, a double phoneme U would replace a se-quence of the phonemes j and u in Figure 1. This solution requires a manual extension of the set of phonemes present in the data. By convention, we regard the models that include a restricted set of 1-2 mappings as 1-1 models.

Advanced L2P approaches, including the joint n-gram models (Bisani and Ney, 2008) and the joint discriminative approach (Jiampojamarn et al., 2007) eliminate the one-to-one constraint en-tirely, allowing for linking of multiple letters to multiple phonemes. We refer to such models as many-to-many (M-M) models. Early EM-based alignment methods (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005) were generally pure 1-1 models. The 1-1 alignment problem can be formulated as a dy-namic programming problem to find the maximum score of alignment, given a probability table of aligning letter and phoneme as a mapping func-tion. The dynamic programming recursion to find the most likely alignment is the following: where  X  ( x ter x notes a probability that a null letter aligns with a phoneme y often set to zero in order to enforce the represen-tation constraint, which facilitates the subsequent phoneme generation process. The probability ta-ble  X  ( x tribution and is iteratively re-computed (M-step) from the most likely alignments found at each it-eration over the data set (E-step). The final align-ments are constructed after the probability table converges.

M2M-aligner (Jiampojamarn et al., 2007) is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple let-ters to multiple phonemes. Algorithm 1 describes the E-step of the many-to-many alignment algo-rithm.  X  represents partial counts collected over all possible mappings between substrings of let-ters and phonemes. The maximum lengths of let-ter and phoneme substrings are controlled by the Algorithm 1 : Many-to-many alignment maxX and maxY parameters. The forward prob-ability  X  is estimated by summing the probabilities from left to right, while the backward probabil-ity  X  is estimated in the opposite direction. The F
ORWARD -M 2 M procedure is similar to line 3 to 10 of Algorithm 1, except that it uses Equation 2 in line 8 and 3 in line 11. The B ACK WARD -M 2 M procedure is analogous to F ORWARD -M 2 M .
In M-step, the partial counts are normalized by using a conditional distribution to create the mapping probability table  X  . The final many-to-many alignments are created by finding the most likely paths using the Viterbi algorithm based on the learned mapping probability table. The source
Although the many-to-many approach tends to create relatively large models, it generates more intuitive alignments and leads to improvement in the L2P accuracy (Jiampojamarn et al., 2007). However, since many links involve multiple let-ters, it also introduces additional complexity in the phoneme prediction phase. One possible solution is to apply a letter segmentation algorithm at test time to cluster letters according to the alignments in the training data. This is problematic because of error propagation inherent in such a process. A better solution is to combine segmentation and decoding using a phrasal decoder (e.g. (Zens and Ney, 2004)). The EM-based approaches to L2P alignment treat both letters and phonemes as abstract symbols. A completely different approach to L2P align-ment is based on the phonetic similarity between phonemes. The key idea of the approach is to rep-resent each letter by a phoneme that is likely to be represented by the letter. The actual phonemes on the phoneme side and the phonemes representing letters on the letter side can then be aligned on the basis of phonetic similarity between phonemes. The main advantage of the phonetic alignment is that it requires no training data, and so can be read-ily be applied to languages for which no pronunci-ation lexicons are available.

The task of identifying the phoneme that is most likely to be represented by a given letter may seem complex and highly language-dependent. For ex-ample, the letter a can represent no less than 12 different English vowels. In practice, however, ab-solute precision is not necessary. Intuitively, the letters that had been chosen (often centuries ago) to represent phonemes in any orthographic system tend to be close to the prototype phoneme in the original script. For example, the letter  X  X  X  rep-resented a mid-high rounded vowel in Classical Latin and is still generally used to represent simi-lar vowels.

The following simple heuristic works well for a number of languages: treat every letter as if it were a symbol in the International Phonetic Alphabet (IPA). The set of symbols employed by the IPA in-cludes the 26 letters of the Latin alphabet, which tend to correspond to the phonemes that they rep-resent in the Latin script. For example, the IPA symbol [ sonant, which is the phoneme represented by the letter m in most languages that utilize Latin script.
ALINE (Kondrak, 2000) performs phonetic alignment of two strings of phonemes. It combines a dynamic programming alignment algorithm with an appropriate scoring scheme for computing pho-netic similarity on the basis of multivalued fea-tures. The example below shows the alignment of the word sheath to its phonetic transcription [ ALINE correctly links the most similar pairs of phonemes (s:
Since ALINE is designed to align phonemes with phonemes, it does not incorporate the repre-sentation constraint. In order to avoid the prob-lem of unaligned phonemes, we apply a post-processing algorithm, which also handles 1-2 links. The algorithm first attempts to remove 0-1 links by merging them with the adjacent 1-0 links. If this is not possible, the algorithm scans a list of valid 1-2 mappings, attempting to replace a pair of 0-1 and 1-1 links with a single 1-2 link. If this also fails, the entire entry is removed from the training set. Such entries often represent unusual foreign-origin words or outright annotation errors. The number of unaligned entries rarely exceeds 1% of the data.

The post-processing algorithm produces an alignment that contains 1-0, 1-1, and 1-2 links. The list of valid 1-2 mappings must be prepared manually. The length of such lists ranges from 1 for Spanish and German ( x :[ks]) to 17 for English. This approach is more robust than the double-phoneme technique because the two phonemes are clustered only if they can be linked to the corre-sponding letter. One of the advantages of the phonetic alignment is its ability to rule out phonetically implausible letter-phoneme links, such as o: terested in establishing whether a set of allow-able letter-phoneme mappings could be derived di-rectly from the data without relying on phonetic features.

Black et al. (1998) report that constructing lists of possible phonemes for each letter leads to L2P improvement. They produce the lists in a  X  X emi-automatic X , interactive manner. The lists constrain the alignments performed by the EM algorithm and lead to better-quality alignments.

We implement a similar interactive program that incrementally expands the lists of possible phonemes for each letter by refining alignments constrained by those lists. However, instead of employing the EM algorithm, we induce align-ments using the standard edit distance algorithm with substitution and deletion assigned the same cost. In cases when there are multiple alternative alignments that have the same edit distance, we randomly choose one of them. Furthermore, we extend this idea also to many-to-many alignments. In addition to lists of phonemes for each letter (1-1 mappings), we also construct lists of many-to-many mappings, such as ee: total, the English set contains 377 mappings, of which more than half are of the 2-1 type. The process of manually inducing allowable letter-phoneme mappings is time-consuming and in-volves a great deal of language-specific knowl-edge. The Integer Programming (IP) framework offers a way to induce similar mappings without a human expert in the loop. The IP formulation aims at identifying the smallest set of letter-phoneme mappings that is sufficient to align all instances in the data set.

Our IP formulation employs the three con-straints enumerated in Section 2, except that the one-to-one constraint is relaxed in order to identify a small set of 1-2 mappings. We specify two types of binary variables that correspond to local align-ment links and global letter-phoneme mappings, respectively. We distinguish three types of local variables, X , Y , and Z , which correspond to 1-0, 1-1, and 1-2 links, respectively. In order to min-imize the number of global mappings, we set the following objective that includes variables corre-sponding to 1-1 and 1-2 mappings: minimize : X We adopt a simplifying assumption that any let-ter can be linked to a null phoneme, so no global variables corresponding to 1-0 mappings are nec-essary.

In the lexicon entry k , let l sition i , and p der to prevent the alignments from utilizing letter-phoneme mappings which are not on the global list, we impose the following constraints: For example, the local variable Y ( i, j, k ) is set if l able G ( l phoneme mappings includes the link ( l Activating the local variable implies activating the corresponding global variable, but not vice versa. Figure 2: A network of possible alignment links.
We create a network of possible alignment links for each lexicon entry k , and assign a binary vari-able to each link in the network. Figure 2 shows an alignment network for the lexicon entry k : wriggle [r I g @ L]. There are three 1-0 links (level), three 1-1 links (diagonal), and one 1-2 link (steep). The local variables that receive the value of 1 are the following: X(1,0, k ), Y(2,1, k ), Y(3,2, k ), Y(4,3, k ), X(5,3, k ), Z(6,5, k ), and X(7,5, k ). The correspond-ing global variables are: G( r ,r), G( i ,I), G( g ,g), and G( l ,@L).

We create constraints to ensure that the link variables receiving a value of 1 form a left-to-right path through the alignment network, and that all other link variables receive a value of 0. We ac-complish this by requiring the sum of the links entering each node to equal the sum of the links leaving each node.  X  i,j,k X ( i, j, k ) + Y ( i, j, k ) + Z ( i, j, k ) =
We found that inducing the IP model with the full set of variables gives too much freedom to the IP program and leads to inferior results. Instead, we first run the full set of variables on a subset of the training data which includes only the lexicon entries in which the number of phonemes exceeds the number of letters. This generates a small set of plausible 1-2 mappings. In the second pass, we run the model on the full data set, but we allow only the 1-2 links that belong to the initial set of 1-2 mappings induced in the first pass. 6.1 Combining IP with EM The set of allowable letter-phoneme mappings can also be used as an input to the EM alignment algo-rithm. We call this approach IP-EM . After induc-ing the minimal set of letter-phoneme mappings, we constrain EM to use only those mappings with the exclusion of all others. We initialize the prob-ability of the minimal set with a uniform distribu-tion, and set it to zero for other mappings. We train the EM model in a similar fashion to the many-to-many alignment algorithm presented in Section 3, except that we limit the letter size to be one letter, and that any letter-phoneme mapping that is not in the minimal set is assigned zero count during the E-step. The final alignments are generated after the parameters converge. During our development experiments, we ob-served that the technique that combines IP with EM described in the previous section generally leads to alignment quality improvement in com-parison with the IP alignment. Nevertheless, be-cause EM is constrained not to introduce any new letter-phoneme mappings, many incorrect align-ments are still proposed. We hypothesized that in-stead of pre-constraining EM, a post-processing of EM X  X  output may lead to better results.

M2M-aligner has the ability to create precise links involving more than one letter, such as ph :f. However, it also tends to create non-intuitive links such as se :z for the word phrase [f r e z], where e is clearly a case of a  X  X ilent X  letter. We propose an alternative EM-based alignment method that instead utilizes a list of alternative one-to-many alignments created with M2M-aligner and aggre-gates 1-M links into M-M links in cases when there is a disagreement between alignments within the list. For example, if the list contains the two alignments shown in Figure 3, the algorithm cre-ates a single many-to-many alignment by merg-ing the first pair of 1-1 and 1-0 links into a single ph:f link. However, the two rightmost links are not merged because there is no disagreement between the two initial alignments. Therefore, the resulting alignment reinforces the ph :f mapping, but avoids the questionable se :z link.
In order to generate the list of best alignments, we use Algorithm 2, which is an adaptation of the standard Viterbi algorithm. Each cell Q tains a list of n -best scores that correspond to al-Algorithm 2 : Extracting n -best alignments ternative alignments during the forward pass. In line 9, we consider all possible 1-M links between letter x end of the main loop, we keep at most n best align-ments in each Q Algorithm 2 yields n -best alignments in the Q the set of high-quality alignments, we also dis-card the alignments with scores below threshold R with respect to the best alignment score. Based on the experiments with the development set, we set R = 0 . 8 and n = 10 . For the intrinsic evaluation, we compared the gen-erated alignments to gold standard alignments ex-tracted from the the core vocabulary of the Com-bilex data set (Richmond et al., 2009). Combilex is a high quality pronunciation lexicon with ex-plicit expert manual alignments. We used a sub-set of the lexicon composed of the core vocabu-lary containing 18,145 word-phoneme pairs. The alignments contain 550 mappings, which include complex 4-1 and 2-3 types.

Each alignment approach creates alignments from unaligned word-phoneme pairs in an un-supervised fashion. We distinguish between the 1-1 and M-M approaches. We report the align-ment quality in terms of precision, recall and F-score. Since the gold standard includes many links that involve multiple letters, the theoretical up-per bound for recall achieved by a one-to-one ap-proach is 90.02%. However, it is possible to obtain the perfect precision because we count as correct all 1-1 links that are consistent with the M-M links in the gold standard. The F-score corresponding to perfect precision and the upper-bound recall is 94.75%.

Alignment entropy is a measure of alignment quality proposed by Pervouchine et al. (2009) in the context of transliteration. The entropy indi-cates the uncertainty of mapping between letter l and phoneme p resulting from the alignment: We compute the alignment entropy for each of the methods using the following formula:
Table 1 includes the results of the intrinsic eval-uation. (the two rightmost columns are discussed in Section 9). The baseline BaseEM is an im-plementation of the one-to-one alignment method of (Black et al., 1998) without the allowable list. ALINE is the phonetic method described in Sec-tion 4. SeedMap is the hand-seeded method de-scribed in Section 5. M-M-EM is the M2M-aligner approach of Jiampojamarn et al. (2007). 1-M-EM is equivalent to M-M-EM but with the restriction that each link contains exactly one let-ter. IP-align is the alignment generated by the IP formulation from Section 6. IP-EM is the method that combines IP with EM described in Section 6.1. EM-Aggr is our final many-to-many alignment method described in Section 7. Oracle corresponds to the gold-standard alignments from Combilex.

Overall, the M-M models obtain lower preci-sion but higher recall and F-score than 1-1 models, which is to be expected as the gold standard is de-fined in terms of M-M links. ALINE produces the most accurate alignments among the 1-1 methods, with the precision and recall values that are very close to the theoretical upper bounds. Its preci-sion is particularly impressive: on average, only one link in a thousand is not consistent with the gold standard. In terms of word accuracy, 98.97% words have no incorrect links. Out of 18,145 words, only 112 words contain incorrect links, and further 75 words could not be aligned. The rank-ing of the 1-1 methods is quite clear: ALINE fol-lowed by IP-EM , 1-M-EM , IP-align , and BaseEM. Among the M-M methods, EM-Aggr has slightly better precision than M-M-EM, but its recall is much worse. This is probably caused by the ag-gregation strategy causing EM-Aggr to  X  X ose X  a significant number of correct links. In general, the entropy measure does not mirror the quality of the alignment. In order to investigate the relationship between the alignment quality and L2P performance, we feed the alignments to two different L2P systems. The first one is a classification-based learning sys-tem employing TiMBL (Daelemans et al., 2009), which can utilize either 1-1 or 1-M alignments. The second system is the state-of-the-art online discriminative training for letter-to-phoneme con-version (Jiampojamarn et al., 2008), which ac-cepts both 1-1 and M-M types of alignment. Ji-ampojamarn et al. (2008) show that the online dis-criminative training system outperforms a num-ber of competitive approaches, including joint n -grams (Demberg et al., 2007), constraint satisfac-tion inference (Bosch and Canisius, 2006), pro-nunciation by analogy (Marchand and Damper, 2006), and decision trees (Black et al., 1998). The decoder module uses standard Viterbi for the 1-1 case, and a phrasal decoder (Zens and Ney, 2004) for the M-M case. We report the L2P performance in terms of word accuracy, which rewards only the completely correct output phoneme sequences. The data set is randomly split into 90% for training and 10% for testing. For all experiments, we hold out 5% of our training data to determine when to stop the online training process.

Table 1 includes the results on the Combilex data set. The two rightmost columns correspond to our two test L2P systems. We observe that al-though better alignment quality does not always translate into better L2P accuracy, there is never-theless a strong correlation between the two, espe-cially for the weaker phoneme generation system. Interestingly, EM-Aggr matches the L2P accuracy obtained with the gold standard alignments. How-ever, there is no reason to claim that the gold stan-dard alignments are optimal for the L2P genera-tion task, so that result should not be considered as an upper bound. Finally, we note that alignment entropy seems to match the L2P accuracy better than it matches alignment quality.

Tables 2 and 3 show the L2P results on sev-eral evaluation sets: English Celex, CMUDict, NETTalk, OALD, and French Brulex. The train-ing sizes range from 19K to 106K words. We fol-low exactly the same data splits as in Bisani and Ney (2008).

The TiMBL L2P generation method (Table 2) is applicable only to the 1-1 alignment models. ALINE produces the highest accuracy on four out of six datasets (including Combilex). The perfor-mance of IP-EM is comparable to 1-M-EM , but not consistently better. IP-align does not seem to measure up to the other algorithms.

The discriminative approach (Table 3) is flexi-ble enough to utilize all kinds of alignments. How-ever, the M-M models perform clearly better than 1-1 models. The only exception is NetTalk, which Figure 4: L2P word accuracy vs. alignment en-tropy. can be attributed to the fact that NetTalk already includes double-phonemes in its original formu-lation. In general, the 1-M-EM method achieves the best results among the 1-1 alignment methods, Overall, EM-Aggr achieves the best word accuracy in comparison to other alignment methods includ-ing the joint n -gram results, which are taken di-rectly from the original paper of Bisani and Ney (2008). Except the Brulex and CMUDict data sets, the differences between EM-Aggr and M-M-EM are statistically significant according to Mc-Nemar X  X  test at 90% confidence level.

Figure 4 contains a plot of alignment entropy values vs. L2P word accuracy. Each point rep-resent an application of a particular alignment method to a different data sets. It appears that there is only weak correlation between alignment entropy and L2P accuracy. So far, we have been unable to find either direct or indirect evidence that alignment entropy is a reliable measure of letter-phoneme alignment quality. We investigated several new methods for gener-ating letter-phoneme alignments. The phonetic alignment is recommended for languages with lit-tle or no training data. The constraint-based ap-proach achieves excellent accuracy at the cost of manual construction of seed mappings. The IP alignment requires no linguistic expertise and guarantees a minimal set of letter-phoneme map-pings. The alignment by aggregation advances the state-of-the-art results in L2P conversion. We thoroughly evaluated the resulting alignments on several data sets by using them as input to two dif-ferent L2P generation systems. Finally, we em-ployed an independently constructed lexicon to demonstrate the close relationship between align-ment quality and L2P conversion accuracy.

One open question that we would like to investi-gate in the future is whether L2P conversion accu-racy could be improved by treating letter-phoneme alignment links as latent variables, instead of com-mitting to a single best alignment.
 This research was supported by the Alberta In-genuity, Informatics Circle of Research Excel-lence (iCORE), and Natural Science of Engineer-ing Research Council of Canada (NSERC).

