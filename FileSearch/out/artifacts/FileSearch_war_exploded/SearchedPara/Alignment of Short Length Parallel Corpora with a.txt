 With evolving Web, short length parallel corpora is becom-ing very common and some of these include user queries, web snippets etc. This paper concerns situations where short length parallel corpora has to be analyzed in order to find meaningful unit-alignment. This is similar to dealing with parallel corpora where a sentence level alignment of trans-lations is required, but differs in that the alignment is to be inferred at unit (word or phrase) level. A Conditional Ran-dom Field (CRF) based approach is proposed to discover this unit alignment. Given pairs of semantically or syntac-tically similar entities, the problem is formulated as that of mutual segmentation and sequence alignment problem . The mutual segmentation refers to the process of segmenting the first entity based on units (or labels ) in the second entity and vice-versa. The process of optimizing this mutual seg-mentation also results in optimal unit alignment. Since our training data is not segmented and unit-aligned, we modify the CRF objective function to accommodate unsupervised data and iterative learning. We have applied this framework to Web Search domain and specifically for query reformula-tion task. Finally, our experiments suggest that the pro-posed approach indeed results in meaningful alternatives of the original query.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation, Search process; I.7.0 [ Document and Text Pro-cessing ]: General thors were at Yahoo! Labs.
 Algorithms, Performance Parallel corpora, Mutual segmentation, Sequence alignment, Web search, Query reformulation
Analysis of parallel corpora often requires text alignment which identifies equivalent or similar text segments [3]. Tra-ditionally parallel corpora is assumed to be text placed along-side its translation. In this case, text alignment identifies sentence-level equivalent or similar text segments.
This paper addresses a more general problem where the two halves of the parallel corpora are short text segments given to be semantically similar. These text segments may or may not be direct translations. The text alignment in this case would refer to identifying and mapping equivalent or similar  X  X nits X . We refer to this as  X  X utual segmentation and sequence alignment X  process.

Many application of such nature exist such as alignment of product titles across many pages in Information Extrac-tion and alignment of units in user queries in Information Retrieval of Web search. In this paper, we consider Web search as our application domain with query reformulation as specific task. User queries tend to be short and may not express complete intent of the user. Even if it is complete, there may be other variants of the query which can result in even more meaningful search results. User queries, there-fore, can be reformulated by replacing part of the query with another unit. Manually coming up with the list of these suit-able replacements is a difficult task. It may be even more difficult if the data comes from another language. The so-lution presented in this paper can come up with this list of suitable replacements automatically.

The primary contribution of this paper is to define one single framework where segmentation and sequence align-ment can be optimized iteratively. In contrast, if we con-sider the above-mentioned example of unit-mining for query reformulation, previous works have done away with the seg-mentation part of the problem by making some simplifying assumptions [5, 8].

In [5] only those query-pairs (parallel text) were consid-ered which differed in one-unit. The differing units then became replacement candidates. Independence hypothe-sis likelihood ratio (LLR) was used to rank these unit-mappings. In [8], units were constrained to be words, pri-marily owing to the complexity of the algorithm. KL di-vergence of contextual models was used to find these unit-mappings in [8]. The proposed method is superior in that it does not only incorporate the segmentation, but also re-sults in better alignment that optimizes a global objective function. Previous approaches as mentioned above essen-tially exploit normalized counts of two units occurring to-gether in the parallel text optimizing only the local pair-wise mapping. Previous approaches also filter out a lot of useful information. For example, the information that the queries britney spears mp3s and madonna songs cannot be used for the mapping mp3s&lt;-&gt;songs because there is no common unit.

In this paper, we formulate the problem as that of mu-tual query segmentation and unit alignment . Our approach starts with a dataset consisting of pairs of similar entities (parallel text) { ( q, q  X  ) } . The mutual segmentation refers to the process of segmenting the entity q based on the con-tent (units) of the entity q  X  and vice-versa. In the process of finding optimal mutual segmentation, we try to derive optimal unit-mappings and this step is referred to as unit alignment . In the example above, our mutual segmenta-tion and alignment approach will try to find the mappings {{britney spears&lt;-&gt;madonna} {mp3s&lt;-&gt;songs}} .
Conditional random fields (CRFs) [6] have long been suc-cessfully used for both segmentation and alignment tasks for labeled data. Given the nature of the problem, CRF is con-sidered with necessary modifications described below. We consider the units in q  X  as labels for the observation sequence q . The CRF training iteratively improves the segmentation of the two halves of the parallel text and discovers most relevant unit-mappings while optimizing a global objective function. Since our training data is not segmented and unit-aligned (labeled), we modify the CRF training process to accommodate unsupervised data and iterative learning.
After the CRF is trained, for every unit we have a sorted list of suggestions or labels . These unit-mappings are used for query reformulations. Given a query, we generate a list of label sequences by replacing every unit in the query by its suggestions. These sequences are sorted based on their conditional probabilities. To avoid any concept-drift , we re-strict our approach to keeping only one unit-substitution in the label sequence and keep rest of the units in the original query unchanged.

In previous labeling and segmentation tasks using CRF, the target label set L is well defined and limited. Given an input observation sequence X , a number of different la-bel sequences { Y | Y  X  X } can be generated by assigning different labels to different elements of X . CRF defines a conditional probability distribution p ( Y | X ) and the la-bel sequence maximizing this probability is assigned to the observation sequence X . CRF training generally takes a la-beled or supervised training set { ( X i , Y i ) } and estimates the parameters such that the total log likelihood of the data i.e. P i log p ( Y i | X i ) is maximized.

Our formulation differs from the above approach in two ways. First, our label set L lies in the same space as the el-ements of the observation sequence and is much larger than what is considered in previous published works. This is be-cause given parallel text ( q, q  X  ), we consider units in q  X  as labels for segmenting the observation sequence q and vice-versa. The second difference comes from the fact that we do not have labeled training data. Very few previous works on CRF have dealt with unsupervised training. An unsu-pervised text segmentation problem was solved using CRFs in [7]. However, structured reference tables are assumed provided. These tables are exploited to generate the CRF model for each attribute in the reference table.

The rest of the paper is organized as follows. Section 2 presents proposed segmentation and alignment using CRF. We present experimental evaluation in Section 3 and con-clude in Section 4.
Consider a parallel corpora where each element, { ( q l , q is a pair of similar entities. Each entity comprises of some basic units. The segmentation of the entities in terms of these units is not available.

If we can somehow segment these entities in terms of their units, at least one of the units (potentially more) in q l should have mapping with unit(s) in q r in order for the en-tities to be similar. Identifying these mappings is referred here as sequence alignment problem. It is easy to see that better unit-mappings will result in better segmentation and vice-versa.

Therefore, the segmentation and sequence-alignment steps can be repeated iteratively to result in optimal segmentation as well as optimal unit-mappings. In this paper, we propose a CRF based solution for this problem.

For a given pair ( q l , q r ), we start by considering units in q , ( u rj ), as possible labels for the units in q l , ( u we can hypothesize a set of label sequences ( { Y  X  q r } ) for q , such that each label sequence Y comprises of units ( u rj in q r . Let f k ( q l , Y, i ) denote an indicator binary function which is equal to 1 if the i th unit in q l is equal to u its label in Y is equal to u  X  k , and equal to 0 otherwise. Let  X  k denote degree or importance of this indicator function. Thus, the number of parameters (  X  k ) is equal to the number probability of a label sequence Y  X  X  ( Y denoting the set of all possible label sequences of q l ) is given by: The summation over i is basically summing over various tion function) to make sure that:
Consider the following example query pair that is given to be semantically similar: {five star hotels placeX,luxury accommodation cityY}
In this case, the observation sequence q l = {five star hotels placeX} can be segmented in var-ious different ways such as: seg1: {five}{star hotels}{placeX} seg2: {five star}{hotels}{placeX} seg3: {five star}{hotels placeX} seg4: {five placeX}{star}{hotels} ....

Similarly, q r = {luxury accommodation cityY} can be segmented to produce a number of different label sequences. Our goal is to train the CRF in such a way that when we compute: It should correspond to the mutual segmentation and align-ment: {five star,luxury}{hotels,accommodation}{placeX,cityY}
Note that label sequences are denoted by Y  X  q r and are composed of units in q r .

The process starts with considering all possible mutual segmentations of all the pairs { ( q l , q r ) } in the corpora. This provides an exhaustive set of labels for each possible unit in the database. An indicator function f k ( . ) is hypothesized for each of these mappings and corresponding weights (  X  k become parameters of the system that need to be estimated such that the following total expected log likelihood of the data is maximized.
 where  X  P ( . ) denotes the probability computation using cur-rent set of parameters such that:
Since we are dealing with unsupervised data (without seg-mentation and labels), our goal is to improve our estimates of P ( Y | q l ) (over Y  X  q r ) and L (Eq. 4) iteratively.
We must also consider another important point about the variable length of the resulting segmentation and therefore the label sequence here. In our solution, different label se-quences Y may have different lengths. A sequence having higher number of labels would therefore have a tendency to result in higher probability (Eq. 1) compared to a shorter label sequence. To account for this, we introduce length-normalization in the probability computation as follows: where,
The total expected log likelihood (Eq. 4) as a function of the parameters ( X ) is now computed as: =
X
Solving for all the parameters {  X  k |  X  k  X   X  } simultaneously in Eq. 7 becomes intractable. We follow the iterative scaling algorithm proposed in [6]. The length-normalization incor-porated in the likelihood computation (Eq. 6) results in a simpler solution as follows: where  X  k is the update for parameter  X  k required to result in the increase in the lower-bound of the total expected log likelihood of the data.

In Eq. 8 P ( Y | q l ) is the probability estimate using cur-rent set of CRF parameters  X . Note that this is different from  X  P ( Y | q l ) for two reasons: 1) P Y  X  q P
Y  X  X  P ( Y | q l ) = 1, and 2) The estimate of P ( Y | q l with every iterative scaling step whereas the estimate of  X  P ( Y | q l ) would only change in the expectation step of the expectation-maximization (EM) iteration. Thus, every ex-pectation step is followed by maximization step consisting of several sub-iterations of iterative scaling. The updated parameter(s) are then given by:
As we considered Web Search as an application, in this section we provide experimental evaluation of the proposed approach utilizing the logs of Yahoo! Search [1]. In specific, we evaluate our approach for query reformulation task of web search and compare with unit substitutions approach of Jones et al. [5], referred as  X  X LR X  Method in the rest of the section.

For generating unit substitutions, we used one week of Ya-hoo! Search logs. We use successive queries issued by a user as parallel text for query reformulation task. Parallel cor-pora contained about 2.3 Billion unique query pairs. From these we filtered those query pairs which have occurred to-gether only once in order to remove noise. This resulted in 78.6 million query pairs. On the filtered data we used strati-fied sampling to randomly select 100K query pairs from each decile in terms of query pair frequency, giving us 1 million query pairs for training the CRF model. Using these unit suggestions, rewrites are generated for the queries in the evaluation set where the queries selected for evaluation is from a different day.

For the LLR Method, we used the filtered query pair set for generating the unit substitutions as described by Jones et al. [5]. As expected, the LLR method required more number of query pairs to generate the same number of unit substitutions as that of our method. This is because the LLR method imposes further constraints on the overlap be-tween the two queries in a pair. The queries are segmented into units and the unit substitutions are generated from the query pairs where only one segment has changed. For gener-ating query reformulations, the queries from the evaluation set are first segmented and then the units are replaced by its substitutions. As in the CRF method, we allow only one unit to be replaced from the original query for generating the reformulations. These reformulations are ranked using the linear regression model specified in [5].

We compute Discounted Cumulative Gain (DCG) [4] to evaluate the quality of proposed technique for query refor-mulation. The query and reformulation pairs were judged by human annotators on a 4-point scale -Precise Match, Ap-proximate Match, Possible Match and Clear Mismatch , as per the guidelines described in [5]. Precise Match and Ap-proximate Match rewrites together is considered as Specific Rewriting which can be used to retrieve highly relevant re-sults as the rewrites have very close meaning to the original query. As the rewrites with Possible Match label have some categorical relationship with the original query and hence preserve the user interests, the rewrites with label Possible Match along with Precise and Approximate Match is con-sidered for Broad Rewriting. We randomly sampled a set of 1000 queries from our evaluation query set for which our and comparison methods have at least five suggestions. These queries along with top five suggestions were editorially la-beled. Table 1 shows the distribution of labels for CRF and LLR. It can be seen that our method has higher percent of rewrites for both Specific Rewriting and Broad Rewriting tasks. CRF method generates 12.14% higher number of Spe-cific Rewrites and 1.06% higher Broad Rewrites compared to the LLR method. DCG @1 7.13 6.26 13.89 DCG @2 11.92 10.48 13.74 DCG @3 15.76 13.85 13.79 DCG @4 19.05 17.19 10.82 DCG @5 22.06 20.22 9.09 Table 2: Comparison of DCG values of CRF and LLR Method The DCG for a query is defined as: where g ( i ) is the gain associated with labeling of the re-sult at rank i and K is the maximum depth of results to be considered. This takes into account the importance of order-ing by discounting the gain at higher ranks. The DCG @ K for a query set, also called as the mean DCG @ K value, is obtained by taking the arithmetic mean of the per-query DCG @ K values. Since we have 5 rewrites per query we cal-culate DCG for 5 ranks to evaluate all the methods. We use the gain values of 10,7,3 and 0 for the labels Precise Match , Approximate Match , Possible Match and Clear Mismatch respectively, following [2]. The DCG results from Table 2 shows that our method significantly improves the DCG @ K for all the five rewrites with a consistent improvement over 13.79% for the first 3 ranks compared to the LLR method.
Results from human evaluation concludes that the pro-posed approach is significantly better than LLR method.
In this paper we presented an approach for segmentation and alignment of short parallel text where one unit can be replaced by its appropriate unit. The unit-suggestions are generated from pairs of parallel text. We considered web search as an application to illustrate our approach where two queries in a user session are considered as parallel text.
We iteratively segment the queries based on their mutual content and derive meaningful unit-mappings from these im-proved segmentations. We have adapted the CRF frame-work to achieve this where both the segmentation and the unit-mapping ( labeling ) steps can be iterated to optimize a global objective function. We have modified the CRF training to accommodate unsupervised training and itera-tive learning. [1] Yahoo! search. http://search.yahoo.com/ . [2] A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski, [3] J. Chen and J. yun Nie. Parallel web text mining for [4] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [5] R. Jones, B. Rey, O. Madani, and W. Greiner.
 [6] J. Lafferty, A. McCallum, and F. Pereira. Conditional [7] X. Li, Y.-Y. Wang, and A. Acero. Extracting structured [8] X. Wang and C. Zhai. Mining term association patterns
