 In this paper, we propose a Bayesian learning approach to promoting diversity for information retrieval in biomedicine and a re-ranking model to improve retrieval performance in the biomedical domain. First, the re-ranking model com-putes the maximum posterior probability of the hidden prop-erty corresponding to each retrieved passage. Then it iter-atively groups the passages into subsets according to their properties. Finally, these passages are re-ranked from the subsets as our output. There is no need for our proposed method to use any external biomedical resource. We evalu-ate our Bayesian learning approach by conducting extensive experiments on the TREC 2004-2007 Genomics data sets. The experimental results show the effectiveness of the pro-posed Bayesian learning approach for promoting diversity in ranking for biomedical information retrieval on four years TREC data sets.
 H.3.3 [ Information Search and Retrieval ]: Text Mining Algorithm Beyesian Learning, Promoting Diversity, Biomedical IR Advances in computational and biological methods during the last decade have remarkably changed the scale of ge-nomic research. Current genomic research is characterized by immense volume of data, accompanied by a tremendous increase in the number of genomics and biomedical related publications. This wealth of information has led to an in-creasing amount of interest and need for applying informa-tion retrieval techniques to access the scientific literature in genomics and related biomedical disciplines.
 Copyright 2009 ACM 978-1-60558-483-6/09/07 ... $ 5.00.
Information Retrieval (IR) in the context of biomedical databases has the following three major problems [2]: the frequent use of (possibly non-standardized) acronyms, the presence of homonyms (the same word referring to two or more different enti ties) and synonyms (two or more words referring to the same entity). How to deal with an abundant number of lexical variants of the same term is a challenging task in biomedical IR. These problems have raised many new data analysis and search challenges in the field of biomedical information retrieval, especially given that the genomic and biomedical literature is expanding in an unprecedented rate.
The Genomics Track of Text REtrieval Conference (TREC) provide a common platform to evaluate the methods and techniques proposed by various research groups for biomed-ical IR. The motivation of the TREC Genomics Track is to propose something in a system that attempts to provide short, specific answers to questions and put them in context by providing linking to original sources [7]. Systems are tasked with extracting out passages and grouped them into aspects identified by one or more MeSH terms. The perfor-mance is scored using mean average precision (MAP) at the document-level, passage-level, aspect-level and passage2-level [7]. The aspect-level is to address a question from different aspects, which indicates how comprehensive the question is answered. For example, the question  X  X hat is the role of gene PRNP in the Mad cow disease? X  can be answered from aspects like  X  X iagnosis X ,  X  X eurologic manifestations X , or  X  X rions/Genetics X .

In this paper, we focus on promoting diversity in ranking using a re-ranking model. The central idea of the model is to compute the maximum probability of its hidden proper-ties corresponding to each retrieved passage iteratively until all subsets achieve stability, and then re-rank these passages from different subsets. To the best of our knowledge, there is little work devoted to genomics aspect search for promot-ing diversity in ranking and a systematic comparison for biomedical information retrieval. In the rest of this paper, we use  X  X spect search X  and  X  X romoting diversity in ranking X  exchangeable. In our work, we conducted an extensive and careful evaluation with different tuning constant values of our IR system. All the results show stable improvements in terms of the document-level, passage-level, aspect-level and passage2-level MAP over the baseline results on the TREC 2004-2007 Genomics data sets.

The remainder of this paper is organized as follows. We first give a brief survey in previous work in Section 2. In Section 3, a Bayesian learning approach is proposed. Then we describe the IR environment in Section 4. Following that, we present the experiments that we have conducted on the TREC Genomics data sets in Section 5 and 6. Finally, conclusions are given in Section 7. A lot of work has been done on biomedical information re-trieval in the past five years. In 2003 as the first year of the TREC Genomics Track, two tasks are featured: ad hoc re-trieval and information extraction. Only the document-level performance is evaluated and a total of 25 groups have sub-mitted 49 official runs for scoring with the highest document-level performance of 0.4165 [6]. The two tasks in the sec-ond year of the TREC Genomics Track are the standard ad hoc retrieval task and categorization of full-text docu-ments. The document-level performance is also used. The best document-level performance is 0.4075 [6]. The tasks in 2005 are more detailed than in 2004. Retrieval performance is measured with Bpref (Binary PREFerence relations) [6]. The Bpref score for the topic is the average of the scores of its relevant documents. The score for each relevant docu-ment is the percentage of non-relevant documents (among the top R non-relevant) that it ranks better than. Bpref tracks closely to MAP (Mean Average Precision) with com-plete judgments, but degrades much more gracefully than MAP as judgments become more incomplete.

In the 2006 TREC Genomics Track, an emphasis is placed on returning relevant passages that discuss different aspects of the topic [7]. The participants X  submissions are scored in three different ways. First, the passage-level retrieval performance is found by measuring the amount of overlap between returned passages and passages the judges deem relevant. Second, the aspect-level retrieval performance is scored by computing how diverse the set of passages re-turned is. Third, the document-level retrieval performance is calculated by essentially counting the number of relevant documents for which a passage is returned. In the 2007 TREC Genomics Track, an alternative passage MAP  X  X as-sage2 X  that calculated MAP as if each character is each pas-sage were a ranked document is defined to compared the accuracy of the extracted answers [7]. Passage2 was used as the primary passage retrieval evaluation measure in 2007. Aspect retrieval was measured using the average precision for the aspects of a topic averaged across all topics in 2007.
However, there is not too much previous work conducted on Genomics aspect search with promoting diversity in rank-ing. In the 2006 TREC Genomics Track, University of Wis-consin re-ranked the passages using a naive clustering-based approach called GRASSHOPPER to promote diversity [5]. Existing methods to improve diversity in ranking include maximum marginal relevance (MMR) [3], mixture models [17], subtopic diversity [16] and diversity penalty [18]. The basic idea is to penalize redundancy by lowering an item X  X  rank if it is similar to items already ranked. GRASSHOP-PER is an alternative to MMR and variants with a princi-pled mathematical model and strong empirical performance on artificial data set [21]. Unfortunately, this re-ranking method actually hurt promoting diversity and its aspect-level performance was not as good as the original results [5].

Later in the 2007 TREC Genomics Track, most teams tried to obtain the aspect-level performance through their passage2-level results, instead of working on the aspect-level search directly. For example, National Library of Medicine (NLM) combined resources to find passages containing an-swers to biomedical questions [4]. University of Illinois at Chicago interpreted queries into two types for passage ex-traction and they kept the same passage ranking list for aspect search [20]. Bayesian learning is a learning process based on Bayes rule which is used to update the prior distribution of the pa-rameters of the learning mod el and compute the posterior distribution for prediction purpose. In this section, we will first introduce our model in Section 3.1 which iteratively calculates the hidden properties for the retrieved passages and re-ranks the passages into a new list. Then the pseudo codes for an algorithm are presented in Section 3.2.
A re-ranking model is proposed to calculate the hidden properties of the retrieved passages. In the model, we build up a probability space ( X  , F , P) for the probability cal-culations. Then retrieved passages are iteratively grouped into different aspect subsets according to their hidden aspect estimations. Finally a re-ranking list is generated from dif-ferent aspect subsets. Here each aspect subset corresponds to a group of retrieved passages that contain the common hidden property.

A probability space ( X  , F , P) is proposed as our whole measure space [13]. The sample space  X  is the genomics data set presented as the index, which is introduced in Section 4.2. The events set F is a  X   X  algebra of subsets of  X , whose elements are the keyword sequences of the topics. The probability measure P is a function from F .Thetargetsof the re-ranking model are N retrieved passages outputted by the IR system, but not the whole data set. Therefore as a subset of  X , the subspace  X  is generated by the N retrieved passages. F is a  X   X  algebra of subsets of  X  .Thenin this model, all calculations are under the probability space ( X  , F , P).
 For each topic, the system outputs N retrieved passages. Each passage can be expressed by x i as a vector and the retrieved results for each topic can be presented as a matrix X . Topics and the retrieved passages are represented as the keyword sequences. As follows t shows a topic where w stands for a keyword.
 X  is a set of properties with  X  = {  X  1 , X  2 , ...,  X  k } each  X  j ( j =1 , 2 , ..., k ) is a hidden property of some re-trieved passages and each  X  j is independent. It is reasonable to assume that  X  has a Poisson distribution such that the expected parameter of passages X  occurrences is  X  j . Then the prior density is shown in Equation 4, where n is the num-ber of passages X  occurrences. As the input, the probability of x i is known based on the original results. In Equation 5, the likelihood function is presented by P ( x i ). The pos-terior probability is presented by Equation 6 which can be interpreted by l (  X  j | x i )and P (  X  j )because P ( x i
The maximum probability of property  X  1 j for the 1st iter-ation is estimated by Equation 7 according to the posterior probabilities of  X  j given x i .
Initially we set the aspect subset number k = N so that each retrieved passage corresponds to one of these N subsets. After the computation of Equation 7, a new k value is gen-erated and each passage is assigned to the subset which has the same estimation parameter  X  1 j . This computation pro-cess is repeated until there are no passages moving among the subsets. As follows are the final property set and pas-
For re-ranking results,  X  p +1 is generated by sorting  X  p cording to its value v j . We choose the top passage in  X  thefirstoneinanewlistthenremoveitfromthe  X  p +1 1 sub-set. Similarly we select the top one from  X  p +1 2 as the second one in the new list and remove it from this subset. Every time we select and remove the top passage from each subset and rank them in the new list. This re-ranking process is repeated until all the passages are chosen from the above k subsets.

Our algorithm is presented in Figure 1. There are three major phases in this iterative algorithm. For the first phase, we initialize the settings for the algorithm. The initial k value is set to be N , different n values are computed for the prior probabilities, and  X  is set for convergence. For the second phase, an iterative process is described to compute and update  X  p j after p iterations. Here the prior probabilities, posterior probabilities, the subsets are updated in each itera-tion until the exit condition is satisfied. For the third phase, we present how to do re-ranking. In this phase, the property subsets are sorted by their weights v j in Equation 9. w ( x of passage x ji is refined by a result combination model which is introduced in our paper [8]. Then passages are selected and removed from the subsets in turns and formed to be a new list as the output.
We used Okapi BSS (Basic Search System) [15] as our main search system and conducted our information retrieval experiments using the improved Okapi system [9, 10, 11, 12, 19]. Okapi is an information retrieval system based on the probability model of Robertson and Sparck Jones [1, 15]. The retrieval documents are ranked in the order of their probabilities of relevance to the query. Search term is as-signed weight based on its within-document term frequency and query term frequency. The weighting function used is BM25. where N is the number of indexed documents in the collec-tion, n is the number of documents containing a specific term, R is the number of documents known to be rele-vant to a specific topic, r is the number of relevant docu-ments containing the term, tf is within-document term fre-quency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, the k i s are tuning constants (which depend on the database and possibly on the nature of the queries and are em pirically determined), K equals to k  X  ((1  X  b )+ b  X  dl/avdl ), and  X  indicates that its following component is added only once per document, rather than for each term.

In our experiments, the tuning constants k 1 and b are set to be different values. k 2 and k 3 are set to be 0 and 8 respectively.
One important issue that IR systems have to deal with is the size of the retrieved passages and the granularity of the indexed information. In the context of text retrieval, the granularity of the indexed text can be defined as the length of the indexed text unit and the size can be defined as the length of the retrieved passage. In this paper, we call an indexed text unit as a passage .

Three indices are built on the genomics 2007 and 2006 data sets according to three passage extraction methods [8] and a paragraph-based index is built on the genomics 2005 and 2004 data sets. Sentence-based indexing is based on passages each of which has up to 3 sentences. Paragraph-based indexing is generated on passages each of which is a paragraph. Here a paragraph is defined as the sequence of sentences between the &lt; p &gt; and &lt; /p &gt; tags from the HTML data set. Word-based indexing forms passages that contain one, two or three sentences each, where the number of words in the passages may only slightly exceed 47. We evaluated our models on four TREC data sets: the Genomics 2007 and 2006 data sets with 36 topics in 2007 and 28 topics in 2006, the Genomics 2004 and 2005 data sets with 50 topics respectively.

TREC 2007 and 2006 Genomics data sets provide a test collection of 162,259 full-text documents. The TREC 2007 queries are in the form of questions asking for lists of specific entities. The definitions for these entity types are based on controlled terminologies from different sources, with the source of the terms depending on the entity type [7]. The TREC 2006 queries are derived from the set of biologically relevant questions based on the Generic Topic Types (GTTs) [6, 7]. All these queries are listed on the official genomics website at: http://ir.ohsu.edu/genomics.
TREC 2005 and 2004 Genomics data sets consists of a document collection for the ad hoc retrieval task which is a 10-year subset of MEDLINE with completed citations from the database inclusive from 1994 to 2003. This pro-vides a total of 4,591,008 records [6]. Each record is an abstract of a document. In this paper, we take the abstract as a paragraph-based passage. There are 50 queries for each year respectively. More information can be found at: http://www.ncbi.nlm.nih.gov/entrez/query/static/help/ pmhelp.html#MEDLINEDisplayFormat
Evaluation Measures in terms of the document-level, passage-level, aspect-level and passage2-level are presented in this paper. Each of these provides insight into the overall performance for a user trying to answer the given queries and measured by some variant of mean average precision (MAP). Their definitions can be found in [6, 7]. In this section, we describe a series of experiments that have been conducted to evaluate the effectiveness of the proposed model on the TREC 2004-2007 genomics data sets. First, we present the original results on word-based, sentence-based and paragraph-based indices under five parameter settings ( k 1 ,b). Second, the re-ranking results and the relative im-provements are shown corresponding to the original results.
Table 1 shows the performance for five parameter set-tings with three different indices in terms of the document-level, passage-level, aspect-level and passage2-level on the genomics 2004-2007 data sets respectively. The first and second columns are the parameters settings. The third one is for the indices and the rest columns are the evaluation measure for the 2004-2007 data sets.
Corresponding to the original results, we generate the im-proved re-ranking results using our proposed algorithm in Figure 1. The performance and improvements are presented in Table 2. The values in the parentheses are the relative rates of improvement over the original results.
In this section, we first investigate the influence of us-ing different indices. Then, we investigate the influence of different tuning constant values. Furthermore, the number k r of aspect subsets generated by the proposed re-ranking model is discussed. Finally, we make a comparison and anal-ysis with the K -mean algorithm in terms of the aspect-level performance.
In order to investigate the influence of different indices, we will continue to analyze the experimental results pre-sented in Section 5.1. To illustrate the results in Table 1 graphically, we re-plot these data in Figure 2, 3 and 4. The performance of the original results is shown in terms of the document-level, passage-level, aspect-level and passage2-level. The x-axis represents the evaluation measures, where word, sen and par stand for word-based, sentence-based and paragraph-based indices. The y-axis shows the MAP performance.
The three figures show that sentence-based index pro-duces the best results in terms of the document-level, word-based index for the best results in terms of the passage-level and paragraph-based index for the best results in terms of the aspect-level and passage2-level. Table 2 shows that the best re-ranking aspect results always come from the best Okapi baseline results in Table 1. The best baseline results and their corresponding best re-ranking results have been highlighted as boldface.

For all the baseline results, their re-ranking results and their relative rates of improvement are presented in Table 2, where the relative rates of improvement in the parentheses are always positive. In other words, no matter which base-line is chosen, the proposed Bayesian learning approach can always generate the better re-ranking results. The baseline results used in this paper are generated on four data sets. We believe that a good aspect-level MAP performance can be achieved if a good baseline result is available.
In order to investigate the influence of the tuning constant values, the experimental results in Section 5.1 are further discussed and analyzed. We re-plot the results in Table 1 and 2 graphically as Figure 6. Nine sub-figures stand for the performance in terms of four evaluation measures on genomics 2007 data set, three on genomics 2006 data set, one on genomics 2005 data set and one on genomics 2004 data set respectively. The performance of the original and re-ranking results is shown and compared. The x-axis represents the indices under five parameter settings. The y-axis shows the MAP performance. Figure 4: Performance of Baselines 2005 and 2004
We can observe the following three phenomena. First, when the parameter setting values ( k 1 , b) is equal to (2.0,0.4), the best aspect-level and passage2-level performance can be obtained for the 2007 and 2006 topics respectively. When the parameter setting values ( k 1 , b) is equal to (0.5,1.3), the best passage-level performance can be obtained. Second, the average performance of both Okapi 2007 and 2006 baseline results on three indices is increasing when k 1 increases from 0.4 to 2.0 and b decreases from 2.0 to 0.4. Figure 5 shows this trend. Third, in order to discriminate the good and bad performance obtained in the experiments, we define a border line. The border line is to differ the results on four data sets. For all the measures, the results above the border line are treated as good results and the results below the border line are considered as bad results. If we consider the performance interval [50%, 100%] of the best baseline performance as the good performance interval, the 50% border lines in Figure 6 can be used to discriminate the good and bad performance.  X 50%, Original X  and  X 50%, Re-ranking X  indicate the border lines for the original results and re-ranking results respec-tively. We can see that the interval (0 . 5  X  2 . 0 , 1 . 3 for the parameter settings ( k 1 ,b) should be chosen in order to obtain better MAP performance for all the measures on four data sets.
In this paper, we focus on promoting diversity in rank-ing using a re-ranking model. In the re-ranking model, we generate hidden aspect subsets by iteratively computing the maximum posterior probabilities. The exact aspect num-ber k r , which is the number of hidden aspect subsets in the re-ranking model, is determined dynamically for each topic. Different topic usually has different aspect number k r .We calculate the average number k r for all topics by simple av-eraging the aspect numbers on each data set.

As shown in Table 3, the average aspect numbers ob-tained from the best runs for each setting are independent of the tuning constant values ( k 1 ,b) and indexing granular-ities. For different tuning constant values ( k 1 ,b) and index-ing granularities, the average numbers k r from the best runs are around 40, 48, 31 and 36 respectively for the genomics 2007-2004 data sets. This shows that the number of topic as-pects is determined by the document collection given, which matches our intuition. We have done an extensive experi-ments with many different settings. Due to space limitation, we did not present all experimental results for the parameter settings within intervals of given values. However, all other results are consistent with those presented here.
As defined in Section 3.1,  X  = {  X  1 , X  2 , ...,  X  k } is a set of aspects, where  X  j is a hidden aspect vector.  X  is determined by Bayesian learning approach statistically, which is not nec-essarily the same as the set of real aspects defined by the human genomics experts. This explains why k r may not be equal to the aspect number provided by the gold standard of the TREC Genomics Track.
In order to further evaluate our proposed approach to pro-moting diversity in ranking, we study how K -mean algo-rithm [14] performs on four data sets. We also choose a genomics topic as an example for more discussion in Section 6.5.

Here we only present the average improvements of K -mean algorithm on the data sets. However the experiments of K -mean algorithm are conducted under five parameter settings and three indices. The average improvements are calculated by the relative rates of K -mean algorithm over their corresponding original results. A series of K values are tested as 5, 10, 20, 50, 80, 100, 200 and 500. For the 2007 and 2006 data sets, we focus on the aspect-level. For the 2005 and 2004 data sets, we focus on the document-level. Although the proposed re-ranking model mainly con-tributes to promote diversity in ranking, it also works for the document-level retrieval. Table 4 presents us the improve-ments for both re-ranking model and K -mean algorithm in terms of average aspect-level and document-level MAP. The improvements of the re-ranking model are the relative rates over the corresponding original results. We can conclude that the re-ranking model is much stabler than K -mean al-gorithm.
For the proposed method, the aspect number k is deter-mined dynamically for each topic. However, the number k in K -mean algorithm is set to a fixed number at the be-ginning. K -mean may not perform well if a non-optimal k is given. Let us take the Topic 220 of the Genomic 2007 Track as an example and consider it on paragraph-based index. As shown in Table 5, the aspect number k =32 is obtained by the proposed method and the correspond-ing aspect-level MAP performance is 0.9167. This result is significantly better than the original result and all results acquired by K -mean.

K -mean algorithm achieve the best results when k is ini-tialized to 20 or 50, but are still worse than the original result. That is, for Topic 220, K -mean algorithm makes neg-ative contributions for re-ranking the original results using different k s . We believe this is because K -mean is not run-ning on an optimal k , which is difficult to find without good prior knowledge. By going through the re-ranking results from K -mean algorithm, we find that this typically happens on many other genomics topics. So we conclude this is one of the reasons why K -mean always performs worse than our approach. The contribution of this paper is three-fold. First, we pro-pose a re-ranking model for promoting diversity in ranking in the biomedical domain. We build up this model by gener-ating the aspect subsets through iteratively computing the maximum probability of the hidden property for each pas-sages and re-ranking the results based on these subsets. We find that performance can be improved by re-ranking when hidden properties can be properly captured. Second, the exact number of k r subsets for each topic is determined dy-namically when there are no passages moving among the subsets. We find that the subset number k r is indepen-dent of the indices and the parameter settings. Third, a series of comprehensive experiments have been conducted to evaluate and analyze the results under different param-eter settings ( k 1 ,b) on different indices. The experimental results on the 2004-2007 Genomics data sets show that the Bayesian learning approach is promising. We also compare the performance of our method with K -mean algorithm in terms of the aspect-level. The experimental results show the stability of our proposed model. K -mean algorithm can generally improve performance over base line results. How-ever, its problem is in that we usually do not have good prior knowledge about k . In addition, in order to obtain the better performance, how to choose the parameter set-tings ( k 1 ,b) are carefully analyzed and the interval of ( k for good performance has been found. Simple rules have been found regarding how to adjust them to reach better performance.

Our future work includes investigating the effectiveness of the Bayesian learning approach on other data sets such as TREC blog data sets and better baseline results from other groups. We will work on the opinion retrieval for blogs and focus on searching diversity of blogs. In addition, we plan to apply the EM method and PLSA model to promoting diversity on Genomics research. This is also our ongoing work.
This research is supported in part by NSERC of Canada and the Early Researcher Award/Premier X  X  Research Excel-lence Award. We thank four anonymous reviewers for their excellent comments on this paper.
