 Crowdsourcing became an essential tool for a broad range of Web applications. Yet, the wide-ranging levels of expertise of crowd workers as well as the presence of faulty workers call for quality control of the crowdsourcing result. To this end, many crowd-sourcing platforms feature a post-processing phase, in which crowd answers are validated by experts. This approach incurs high costs though, since expert input is a scarce resource. To support the expert in the validation process, we present a tool for ExpeRt guidance In validating Crowd Answers (ERICA) . It allows us to guide the expert X  X  work by collecting input on the most problematic cases, thereby achieving a set of high quality answers even if the expert does not validate the complete answer set. The tool also supports the task requester in selecting the most cost-efficient allocation of the budget between the expert and the crowd.
 Categories and Subject Descriptors: H.1.2 [User/Machine Sys-tems]: Human information processing Keywords: crowdsourcing; validation; guiding user feedback
Crowdsourcing has been established as an efficient and scalable approach to overcome problems that are computationally expensive or unsolvable for machines, but rather trivial for humans [6]. In essence, crowdsourcing enables users to post tasks in the form of questions, which are then answered by crowd workers for finan-cial rewards. Crowd workers, however, have different backgrounds and wide-ranging levels of expertise and motivation [3], so that the obtained answers are not necessarily correct. Even aggregation of answers from multiple workers to a single question cannot guarantee result correctness. Platforms for crowdsourcing, such as Amazon Mechanical Turk (AMT), aim at including the answer trustworthi-ness by including a validation phase. That is, the obtained crowd answer are checked against expert answers that are supposedly cor-rect. Involving an expert incurs high costs, though. Hence, only a fraction of the obtained crowd answers can be considered in the validation phase, which raises the question of how to involve experts effectively and efficiently.

This demo presents ERICA: a tool for ExpeRt guidance In validat-ing Crowd Answers . Its functionalities are summarized as follows: (1) Minimizing expert efforts : ERICA guides experts during the (2) Handling faulty workers : Faulty workers, such as spammers (3) Estimating quality : During the validation process, ERICA main-(4) Optimizing overall cost : ERICA enables crowdsourcing users (5) Detecting expert mistakes : In practice, expert input may contain
To the best of our knowledge, ERICA is the first system to provide such comprehensive support for expert-based validation of crowd-sourcing results, thereby providing a valuable add-on to existing crowdsourcing services. In contrast to related approaches to the detection of faulty workers, quality estimation or cost optimization, all our techniques are grounded in expert feedback, which results in higher quality and better performance [5]. In the remainder, we first discuss the implementation of these functionalities in Section 2. Next, Section 3 demonstrates the ERICA by means of an example scenario, before Section 4 closes the paper with a summary.
This section discusses how ERICA handles large amounts of crowd answers and implements the above functionalities. The un-derlying theory of the respective techniques is discussed in detail in our earlier work [5].
 Minimizing expert efforts. To guide the expert validation, ERICA selects and ranks questions for which expert feedback should be sought based on their expected benefit. The expected benefit of a validation question is measured using an information theoretic model. Then, the question with the highest information gain is shown first for expert feedback elicitation [5].
 Handling faulty workers. To detect faulty workers, ERICA com-putes the likelihood of a worker to be spammer ( spammer score ) [5]. To this end, confusion matrices are constructed based on the ex-pert feedback obtained so far. Answers from workers with a high likelihood to be a spammer are removed by ERICA.
 Estimating quality. The quality of the aggregated answers is mea-sured using k-fold cross validation [4]. That is, ERICA arbitrarily partitions the expert feedback into test and training sets. Then, the training set is used to compute the aggregated answers for the test set and the similarity between two aggregated answers sets (one created using only the training set, one using the whole feedback set) is measured. Repeating this procedure k times, ERICA estimates the quality of the aggregated answers.
 Optimizing cost. ERICA helps to find the optimal distribution of a pre-defined budget between an expert and the crowd based on characteristics of the crowd. Since these characteristics are not known beforehand, it samples the crowd with a few test questions and simulates the crowd to decide on the best budget distribution [5]. Detecting expert mistakes. ERICA also uses k-fold cross valida-tion to detect expert mistakes. Here, k is fixed to 1, though, in order to detect incorrect feedback. Then, the aggregated answer for the question in the test set is compared with the feedback by the expert. If they are different, the expert is encouraged to revisit the respective question and reconsider the answer.
 To achieve an efficient implementation for large-scale data, ERICA employs two techniques: parallelization and partitioning . First, since the computations of the information gain and the spammer score are independent for different questions, they are executed in parallel concurrently for all questions. Second, common answer matrices are sparse [1]. Therefore, ERICA uses sparse matrix parti-tioning [2] to divide a large answer matrix into smaller dense ones that fit for human interactions and can be handled more efficiently. ERICA is implemented as a web service. The back-end is written in Python and Java. The front-end is an HTML5 website communicat-ing with the server using Javascript and jQuery.
To demonstrate the application of ERICA, we first present its user interface before we turn to an example scenario.
 User interface. ERICA offers a rich user interface (see Figure 1,2), which consists of 2 main components: Demonstration scenario. We use the example from Figure 1 to demonstrate how to use ERICA to analyze and validate the answer matrix (a screencast of the demonstration is publicly available
Before posting a task to a crowdsourcing platform, a user may compute the budget allocation between an expert and the crowd. First, a user posts a set of test questions to study the characteristics of the crowd. Based on the answers, ERICA will simulate the crowd and compute the best budget allocation. Then, the user decides how many questions are posted to the crowd.

After all answers have been collected for the crowdsourcing task, the expert starts the validation. First, ERICA will select the question, for which the feedback is most beneficial. The question and its pos-sible answers are shown in Panel A. Once the expert has submitted their answer, ERICA will recompute the aggregated answers and the reliability of the workers. After several feedback iterations, the expert can remove the answers of a worker, if the estimated reliabil-ity of the worker is too low. ERICA will measure the effect of this removal and recompute the result accordingly. The expert can stop the validation process when the estimated precision shown in Panel B is high or the aggregated answers and the feedback are similar for a consecutive number of times, as shown in Panel C.
This paper presented ERICA: a tool for expert guidance in val-idating crowd answer. Unlike traditional tools for crowdsourcing, ERICA is the first to streamline the validation process by an expert for crowd answers. The expert feedback provides a reliable means to detect and remove faulty workers, which can improve the quality of the result significantly. ERICA helps crowdsourcing users by computing the best budget allocation between the expert and the crowd. As such, ERICA serves not only as a tool to reduce the vali-dation effort, but also acts as a decision-support system to achieve a reduction of validation cost and an improvement of result quality. Acknowledgment. The research has received funding from the EU-FP7 EINS project (grant 288021) and the ScienceWise project. [1] H. J. Jung et al. Improving quality of crowdsourced labels via [2] G. Karypis et al. Metis-unstructured graph partitioning and [3] G. Kazai et al. Worker types and personality traits in [4] B. Mozafari et al. Scaling up crowd-sourcing to very large [5] Q. V. H. Nguyen et al. Minimizing efforts in validating crowd [6] A. J. Quinn et al. Human computation: a survey and taxonomy
