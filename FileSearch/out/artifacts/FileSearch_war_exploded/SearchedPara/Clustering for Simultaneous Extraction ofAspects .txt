 If you are thinking of buying a TV for watching football, you might go to websites such as Amazon to read customer reviews on TV products. How-ever, there are many products and each of them may have hundreds of reviews. It would be helpful to have an aspect-based sentiment summarization for each product. Based on other customers X  opinions on multiple aspects such as size, picture quality, motion-smoothing, and sound quality, you might be able to make the decision without going through all the reviews. To support such summarization, it is essential to have an algorithm that extracts product features and aspects from reviews.
Features are components and attributes of a prod-uct. A feature can be directly mentioned as an opin-ion target (i.e., explicit feature) or implied by opin-ion words (i.e., implicit feature). Different feature expressions may be used to describe the same aspect of a product. Aspect can be represented as a group of features. Consider the following review sentences, in which we denote explicit and implicit features in boldface and italics, respectively. 1. This phone has great display and perfect size . 2. Good features for an inexpensive android. The 3. The phone runs fast and smooth , and has great
In review 1, display is an explicit feature, and opinion word  X  X ast X  implies implicit feature speed . The task is to identify both explicit and implicit fea-tures, and group them into aspects, e.g., { speed, fast, smooth } , { size, big } , { price, inexpensive } .
Many existing studies (Hu and Liu, 2004; Su et al., 2006; Qiu et al., 2009; Hai et al., 2012; Xu et al., 2013) have focused on extracting features with-out grouping them into aspects. Some methods have been proposed to group features given that feature expressions have been identified beforehand (Zhai et al., 2010; Moghaddam and Ester, 2011; Zhao et al., 2014), or can be learned from semi-structured Pros and Cons reviews (Guo et al., 2009; Yu et al., 2011). In recent years, topic models have been widely stud-ied for their use in aspect discovery with the advan-tage that they extract features and group them simul-taneously. However, researchers have found some limitations of such methods, e.g., the produced top-ics may not be coherent or directly interpretable as aspects (Chen et al., 2013; Bancken et al., 2014), the extracted aspects are not fine-grained (Zhang and Liu, 2014), and it is ineffective when dealing with aspect sparsity (Xu et al., 2014).

In this paper, we present a clustering algorithm that extracts features and groups them into aspects from product reviews. Our work differs from prior studies in three ways. First, it identifies both features and aspects simultaneously. Existing clustering-based solutions (Su et al., 2008; Lu et al., 2009; Bancken et al., 2014) take a two-step approach that first identifies features and then employs standard clustering algorithms (e.g., k-means) to group fea-tures into aspects. We propose that these two steps can be combined into a single clustering process, in which different words describing the same as-pect can be automatically grouped into one clus-ter, and features and aspects can be identified at the same time. Second, both explicit and implicit fea-tures are extracted and grouped into aspects. While most existing methods deal with explicit features (e.g.,  X  X peed X ,  X  X ize X ), much less effort has been made to identify implicit features implied by opin-ion words (e.g.,  X  X ast X ,  X  X ig X ), which is challeng-ing because many general opinion words such as  X  X ood X  or  X  X reat X  do not indicate product features, therefore they should not be identified as features or grouped into aspects. Third, it is unsupervised and does not require seed terms, hand-crafted patterns, or any other labeling efforts.

Specifically, the algorithm takes a set of reviews on a product (e.g., TV, cell phone) as input and pro-duces aspect clusters as output. It first uses a part-of-speech tagger to identify nouns/noun phrases, verbs and adjectives as candidates. Instead of applying the clustering algorithm to all the candidates, only the frequent ones are clustered to generate seed clusters , and then the remaining candidates are placed into the closest seed clusters. This does not only speed up the algorithm, but it also reduces the noise that might be introduced by infrequent terms in the clus-tering process. We propose a novel domain-specific similarity measure incorporating both statistical as-sociation and semantic similarity between a pair of candidates, which recognizes features referring to the same aspects in a particular domain. To further improve the quality of clusters, several problem-specific merging constraints are used to prevent the clusters referring to different aspects from being merged during the clustering process. The algorithm stops when it cannot find another pair of clusters sat-isfying these constraints.

This algorithm is evaluated on reviews from three domains: cell phone, TV and GPS. Its effective-ness is demonstrated through comparison with sev-eral state-of-the-art methods on both tasks of feature extraction and aspect discovery. Experimental re-sults show that our method consistently yields bet-ter results than these existing methods on both tasks across all the domains. Feature and aspect extraction is a core component of aspect-based opinion mining systems. Zhang and Liu (2014) provide a broad overview of the tasks and the current state-of-the-art techniques.

Feature identification has been explored in many studies. Most methods focus on explicit features, including unsupervised methods that utilize associ-ation rules (Hu and Liu, 2004; Liu et al., 2005), dependency relations (Qiu et al., 2009; Xu et al., 2013), or statistical associations (Hai et al., 2012) between features and opinion words, and supervised ones that treat it as a sequence labeling problem and apply Hidden Markov Model (HMM) or Condi-tional Random Fields (CRF) (Jin et al., 2009; Yang and Cardie, 2013) to it. A few methods have been proposed to identify implicit features, e.g., using co-occurrence associations between implicit and ex-plicit features (Su et al., 2006; Hai et al., 2011; Zhang and Zhu, 2013), or leveraging lexical rela-tions of words in dictionaries (Fei et al., 2012). Many of these techniques require seed terms, hand-crafted rules/patterns, or other annotation efforts.
Some studies have focused on grouping features and assumed that features have been extracted be-forehand or can be extracted from semi-structured Pros and Cons reviews. Methods including simi-larity matching (Carenini et al., 2005), topic mod-eling (Guo et al., 2009; Moghaddam and Ester, 2011), Expectation-Maximization (EM) based semi-supervised learning (Zhai et al., 2010; Zhai et al., 2011), and synonym clustering (Yu et al., 2011) have been explored in this context.

To extract features and aspects at the same time, topic model-based approaches have been explored by a large number of studies in recent years. Stan-dard topic modeling methods such as pLSA (Hof-mann, 2001) and LDA (Blei et al., 2003) are ex-tended to suit the peculiarities of the problem, e.g., capturing local topics corresponding to ratable as-pects (Titov and McDonald, 2008a; Titov and Mc-Donald, 2008b; Brody and Elhadad, 2010), jointly extracting both topic/aspect and sentiment (Lin and He, 2009; Jo and Oh, 2011; Kim et al., 2013; Wang and Ester, 2014), incorporating prior knowl-edge to generate coherent aspects (Mukherjee and Liu, 2012; Chen et al., 2013; Chen et al., 2014), etc.
Very limited research has focused on exploring clustering-based solutions. Su et al. (2008) pre-sented a clustering method that utilizes the mutual reinforcement associations between features and opinion words. It employs standard clustering algo-rithms such as k-means to iteratively group feature words and opinion words separately. The similarity between two feature words (or two opinion words) is determined by a linear combination of their intra-similarity and inter-similarity. Intra-similarity is the traditional similarity, and inter-similarity is calcu-lated based on the degree of association between feature words and opinion words. To calculate the inter-similarity, a feature word (or an opinion word) is represented as a vector where each element is the co-occurrence frequency between that word and opinion words (or feature words) in sentences. Then the similarity between two items is calculated by cosine similarity of two vectors. In each iteration, the clustering results of one type of data items (fea-ture words or opinion words) are used to update the pairwise similarity of the other type of items. After clustering, the strongest links between features and opinion words form the aspect groups. Mauge et al. (2012) first trained a maximum-entropy classifier to predict the probability that two feature expressions are synonyms, then construct a graph based on the prediction results and employ greedy agglomerative clustering to partition the graph to clusters. Bancken et al. (2014) used k-medoids clustering algorithm with a WordNet-based similarity metric to cluster semantically similar aspect mentions.
 These existing clustering methods take two steps. In the first step, features are extracted based on as-sociation rules or dependency patterns, and in the second step features are grouped into aspects using clustering algorithms. In contrast, our method ex-tracts features and groups them at the same time. Moreover, most of these methods extract and group only explicit features, while our method deals with both explicit and implicit features. The method pro-posed in (Su et al., 2008) also handles implicit fea-tures (opinion words), but their similarity measure largely depends on co-occurrence between features and opinion words, which may not be efficient in identifying features that are semantically similar but rarely co-occur in reviews. Let X = { x 1 ,x 2 ,...,x n } be a set of candidate fea-tures extracted from reviews of a given product (e.g., TV, cell phone). Specifically, by using a part-of-secutive nouns (e.g.,  X  X attery life X ) are identified as candidates of explicit features, and adjectives and verbs are identified as candidates of implicit fea-tures. Stop words are removed from X . The algo-rithm aims to group similar candidate terms so that the terms referring to the same aspect are put into one cluster. At last, the important aspects are se-lected from the resulting clusters, and the candidates contained in these aspects are identified as features. 3.1 A Clustering Framework Algorithm 1 illustrates the clustering process. The algorithm takes as input a set X that contains n can-didate terms, a natural number k indicating the num-ber of aspects, a natural number s ( 0 &lt; s  X  n ) indi-cating the number of candidates that will be grouped first to generate the seed clusters , and a real number  X  indicating the upper bound of the distance between two mergeable clusters. Instead of applying the ag-glomerative clustering to all the candidates, it first selects a set X 0  X  X of s candidates that appear most frequently in the corpus for clustering. The reasons for this are two-fold. First, the frequently mentioned terms are more likely the actual features of customers X  interests. By clustering these terms first, we can generate high quality seed clusters. Second, as the clustering algorithm requires pair-wise distances between candidates/clusters, it could be very time-consuming if there are a large number of candidates. We can speed up the process by clus-tering only the most frequent ones.
 Algorithm 1: Clustering for Aspect Discovery Input : X = { x 1 ,...,x n } , k , s ,  X 
Output : { A j } k j =1 Select the top s most frequent candidates from
Set C 1  X  X  x 0 1 } ,...,C s  X  X  x 0 s } ;
Set  X   X  X  C 1 ,...,C s } ; while there exist a pair of mergeable clusters from  X  do 5 Select a pair of closest clusters C l and C m 7  X   X   X   X  X  C l } ; for x i  X  X  X  X 0 do 9 Select the closest clusters C d from  X  such 10 if there exist such cluster C d then in its own cluster C i , and  X  is the set of all clusters. In each iteration, a pair of clusters C l and C m that are most likely composed of features referring to the same aspect are merged into one. Both a similarity measure and a set of constraints are used to select such pair of clusters. We propose a domain-specific similarity measure that determines how similar the members in two clusters are regarding the particular domain/product. Moreover, we add a set of merg-ing constraints to further ensure that the terms from different aspects would not be merged. The cluster-ing process stops when it cannot find another pair of clusters that satisfy the constraints. We call the obtained clusters in  X  the seed clusters . Next, the algorithm assigns each of the remaining candidate x i  X  X  X  X 0 to its closest seed cluster that satisfies the merging constraints. At last, k clusters are se-lected from  X  as aspects 2 . Based on the idea that the frequent clusters are usually the important aspects of customers X  interests, we select the top k clusters having the highest sum of members X  frequencies of occurrence. From the k aspects, the nouns and noun phrases (e.g.,  X  X peed X ,  X  X ize X ) are recognized as ex-plicit features, and the adjectives and verbs (e.g.,  X  X ast X ,  X  X ig X ), are recognized as implicit features. 3.2 Domain-specific Similarity The similarity measure aims to identify terms refer-ring to the same aspect of a product. Prior studies (Zhai et al., 2010; Zhai et al., 2011) have shown that general semantic similarities learned from the-saurus dictionaries (e.g., WordNet) do not perform well in grouping features, mainly because the sim-ilarities between words/phrases are domain depen-dent. For example,  X  X ce cream sandwich X  and  X  X p-erating system X  are not relevant in general, but they refer to the same aspect in cell phone re-cell phone domain than they are in hair dryer do-main. Methods based on distributional informa-tion in a domain-specific corpus are usually used to determine the domain-dependent similarities be-tween words/phrases. However, relying completely on the corpus may not be sufficient either. For ex-ample, people usually use  X  X nexpensive X  or  X  X reat price X  instead of  X  X nexpensive price X ; similarly, they use  X  X unning fast X  or  X  X reat speed X  instead of  X  X ast speed X . Though  X  X nexpensive X  and  X  X rice X  or  X  X ast X  and  X  X peed X  refer to the same aspect, we may not find they are similar based on their context or co-occurrences in the corpus.

We propose to estimate the domain-specific sim-ilarities between candidates by incorporating both general semantic similarity and corpus-based statis-tical association. Formally, let G be a n  X  n similar-ity matrix, where G ij is the general semantic sim-ilarity between candidates x i and x j , G ij  X  [0 , 1] , G ij = 1 when i = j , and G ij = G ji . We use UMBC Semantic Similarity Service 4 to get G . It combines both WordNet knowledge and statistics from a large web corpus to compute the semantic similarity between words/phrases (Han et al., 2013).
Let T be a n  X  n association matrix, where T ij is the pairwise statistical association between x i and x j in the domain-specific corpus, T ij  X  [0 , 1] , T ij = 1 when i = j , and T ij = T ji . We use normal-ized pointwise mutual information (NPMI) (Bouma, 2009) as the measure of association to get T , that is, where f ( x i ) (or f ( x j ) ) is the number of documents where x i (or x j ) appears, f ( x i ,x j ) is the number of documents where x i and x j co-occur in a sen-tence, and N is the total number of documents in the domain-specific corpus. NPMI is the normaliza-tion of pointwise mutual information (PMI), which has the pleasant property NPMI ( x i ,x j )  X  [  X  1 , 1] (Bouma, 2009). The values of NPMI are rescaled to the range of [0 , 1] , because we want T ij  X  [0 , 1] .
A candidate x i can be represented by the i -th row in G or T , i.e., the row vector g i : or t i : , which tells us what x i is about in terms of its general semantic similarities or statistical associations to other terms. The cosine similarity between two vectors ~u and ~v can be calculated as: By calculating the cosine similarity between two vectors of x i and x j ( i 6 = j ), we get the following similarity metrics: sim g ( x i ,x j ) provides the comparison between g i : and g j : . Similar row vectors in G indicate simi-lar semantic meanings of two terms (e.g.,  X  X rice X  and  X  X nexpensive X ). sim t ( x i ,x j ) provides the com-parison between t i : and t j : . Similar row vectors in T indicate similar context of two terms in the domain, and terms that occur in the same con-texts tend to have similar meanings (Harris, 1954) (e.g.,  X  X ce cream sandwich X  and  X  X perating sys-tem X ). sim gt ( x i ,x j ) provides the comparison be-tween the row vector in G and the row vector in T of two terms. sim gt ( x i ,x j ) is designed to get high value when the terms strongly associated with x i (or x ) are semantically similar to x j (or x i ). By this measure, the domain-dependent synonyms such as  X  X mooth X  and  X  X peed X  (in cell phone domain) can be identified because the word  X  X mooth X  frequently co-occurs with some other words (e.g.,  X  X ast X ,  X  X un X ) that are synonymous with the word  X  X peed X .

Because G ij  X  [0 , 1] and T ij  X  [0 , 1] , the values of sim g ( x i ,x j ) , sim t ( x i ,x j ) , and sim gt ( x range from 0 to 1 . In addition, sim g ( x i ,x j ) = sim g ( x j ,x i ) , sim t ( x i ,x j ) = sim t ( x j ,x sim gt ( x i ,x j ) = sim gt ( x j ,x i ) . When i = j , we set all the similarity metrics between x i and x j to 1 . Finally, the domain-specific similarity between x i and x j ( i 6 = j ) is defined as the weighted sum of the above three similarity metrics: sim ( x i ,x j ) = w g sim g ( x i ,x j ) + w t sim t ( x w gt sim gt ( x i ,x j ) , where w g , w t and w gt denote the relative weight of importance of the three similar-ity metrics, respectively. The values of the weight ranges from 0 to 1 , and w g + w t + w gt = 1 .
Based on the domain-specific similarities between candidates, we now define the distance measures of clustering as: where dist avg ( C l ,C m ) is the average of candidate distances between clusters C l and C m , r ( C l ) is the most frequent member (i.e., representative term ) in cluster C l , and dist rep ( C l ,C m ) is the distance be-tween the representative terms of two clusters. The two clusters describing the same aspect should be close to each other in terms of both average distance and representative distance, thus the final distance is defined as the maximum of these two: 3.3 Merging Constraints Prior studies (Wagstaff et al., 2001) have explored the idea of incorporating background knowledge as constraints on the clustering process to further im-prove the performance. Two types of constraints are usually considered: must-link constraints specifying that two objects (e.g., words) must be placed in the same cluster, and cannot-link constraints specifying that two objects cannot be placed in the same cluster. We also add problem-specific constraints that spec-ify which clusters cannot be merged together, but in-stead of manually creating the cannot-links between specific words, our cannot-link constraints are auto-matically calculated during the clustering process.
Specifically, two clusters cannot be merged if they violate any of the three merging constraints: (1) The distance between two clusters must be less than a given value  X  (see Algorithm 1). (2) There must be at least one noun or noun phrase (candidate of explicit feature) existing in one of the two clusters. Because we assume an aspect should contain at least one explicit feature, and we would not get an aspect by merging two non-aspect clusters. (3) The sum of frequencies of the candidates from two clusters co-occurring in the same sentences must be higher than the sum of frequencies of them co-occurring in the same documents but different sentences. The idea is that people tend to talk about different aspects of a product in different sentences in a review, and talk about the same aspect in a small window (e.g., the same sentence). In this section, we evaluate the effectiveness of the proposed approach on feature extraction and aspect discovery. Table 1 describes the datasets from three different domains that were used in the experiments. The cell phone reviews were collected from the on-line shop of a cell phone company, and the GPS and TV reviews were collected from Amazon.

Three human annotators manually annotate the datasets to create gold standards of features and as-pects. These annotators first identify feature expres-sions from reviews independently. The expressions that were agreed by at least two annotators were se-lected as features. Then the authors manually spec-ified a set of aspects based on these features, and asked three annotators to label each feature with an aspect category. The average inter-annotator agreement on aspect annotation was  X  = 0 . 687 ( stddev = 0 . 154 ) according to Cohen X  X  Kappa statistic. To obtain the gold standard annotation of aspects, the annotators discussed to reach an agree-ment when there was a disagreement on the aspect category of a feature. We are making the datasets
Table 1 shows the number of reviews, aspects, unique explicit/implicit features manually identified by annotators, and candidates of explicit (i.e., noun and noun phrase) and implicit (i.e., adjective and verb) features extracted from the datasets in three domains.
 We use  X  X AFE X  ( C lustering for A spect and F eature E xtraction) to denote the proposed method. We assume the number of aspects k is specified by the users, and set k = 50 throughout all the exper-iments. We use s = 500 ,  X  = 0 . 8 , w g = w t = 0 . 2 ,w gt = 0 . 6 as the default setting of CAFE, and study the effect of parameters in Section  X  X nfluence of Parameters X . In addition, we evaluate each indi-vidual similarity metric  X   X  X AFE-g X ,  X  X AFE-t X  and  X  X AFE-gt X  denote the variations of  X  X AFE X  that use sim g ( x i ,x j ) , sim t ( x i ,x j ) , and sim gt as the similarity measure, respectively. We empir-ically set  X  = 0 . 4 for  X  X AFE-g X ,  X  = 0 . 84 for  X  X AFE-t X  and  X  = 0 . 88 for  X  X AFE-gt X . 4.1 Evaluations on Feature Extraction We compared CAFE against the following two state-of-the-art methods on feature extraction:  X  PROP : A double propagation approach (Qiu  X  LRTBOOT : A bootstrapping approach (Hai et Both methods require seeds terms. We ranked the feature candidates by descending document fre-quency and manually selected the top 10 genuine features as seeds for them. According to the study (Hai et al., 2012), the performance for LRTBOOT remained almost constant when increasing the seeds from 1 to 50. Three association thresholds need to be specified for LRTBOOT. Following the original study in which the experiments were conducted on cell-phone reviews, we set ffth = 21 . 0 , ooth = 12 . 0 , and performed grid search for the value of foth . The best results were achieved at foth = 9 . 0 for cell-phone reviews, and at foth = 12 . 0 for GPS and TV reviews.
 The results were evaluated by precision = N agree where N result and N gold are the number of features in the result and the gold standard, respectively, and N agree is the number of features that are agreed by both sides. Because PROP and LRTBOOT extract only explicit features, the evaluation was conducted on the quality of explicit features. The performance of identifying implicit features will be examined by evaluation on aspect discovery, because implicit fea-tures have to be merged into aspects to be detected.
Table 2 shows the best results (in terms of F-score) of feature extraction by different methods. Both PROP and LRTBOOT obtain high recall and relatively low precision. CAFE greatly improves precision, with a relatively small loss of recall, resulting in 21.68% and 9.36% improvement in macro-averaged F-score over PROP and LRTBOOT, respectively. We also plot precision-recall curves at various parameter settings for CAFE and LRT-BOOT in Figure 1. For CAFE, we kept s = 500 , w g = w t = 0 . 2 ,w gt = 0 . 6 , and increased  X  from 0.64 to 0.96. For LRTBOOT, we kept ffth = 21 . 0 , ooth = 12 . 0 , and increased foth from 6.0 to 30.0. For PROP, only one precision-recall point was obtained. From Figure 1, we see that the curve of CAFE lies well above those of LRTBOOT and PROP across three datasets. Though LRTBOOT achieved similar precision as CAFE did at the re-call rate of approximately 0.37 for GPS reviews and at the recall rate of approximately 0.49 for TV re-views, it performed worse than CAFE at increasing recall levels for both datasets.

The key difference between CAFE and the base-lines is that CAFE groups terms into clusters and identifies the terms in the selected aspect clusters as features, while both baselines enlarge a feature seed set by mining syntactical or statistical associations between features and opinion words. The results suggest that features can be more precisely identified via aspect clustering. Generally, CAFE is superior to its variations, and CAFE-g outperforms CAFE-gt and CAFE-t. 4.2 Evaluations on Aspect Discovery For comparison with CAFE on aspect discovery, we implemented the following three methods:  X  MuReinf : A clustering method (Su et al.,  X  L-EM : A semi-supervised learning method  X  L-LDA : A baseline method (Zhai et al., 2011)
These three methods require features to be ex-tracted beforehand, and focus on grouping features into aspects. Both LRTBOOT and CAFE are used to provide the input features to them. We set  X  = 0 . 6 for MuReinf, because their study (Su et al., 2008) showed that the method achieved best results at  X  &gt; 0 . 5 . All three methods utilize dictionary-based se-mantic similarity to some extent. Since CAFE uses the UMBC Semantic Similarity Service, we use the same service to provide the semantic similarity for all the methods.

The results were evaluated using Rand Index (Rand, 1971), a standard measure of the similarity between the clustering results and a gold standard. Given a set of n objects and two partitions of them, the Rand Index is defined as 2( a + b ) that the agreements/disagreements between two par-titions are checked on n  X  ( n  X  1) pairs of objects. Among all the pairs, there are a pairs belonging to the same cluster in both partitions, and b pairs be-longing to different clusters in both partitions. In this study, the gold standard and the aspect clusters may not share the exact same set of features due to the noise in feature extraction, therefore we consider n the number of expressions in the union of two sets.
Table 3 shows the Rand Index achieved by dif-ferent methods. Among the methods that generate partitions of the same features provided by CAFE, CAFE achieves the best macro-averaged Rand In-dex, followed by CAFE + MuReinf, CAFE + L-LDA, and CAFE + L-EM. CAFE outperforms the variations using the single similarity metric, i.e., CAFE-g, CAFE-t and CAFE-gt. The results imply the effectiveness of our domain-specific similarity measure in identifying synonym features in a par-ticular domain. Using the input features from LRT-BOOT, the performance of MuReinf, L-EM and L-LDA decrease on all three domains, compared with using the input features from CAFE. The decrease is more significant for L-EM and L-LDA than for MuReinf, which suggest that the semi-supervised methods L-EM and L-LDA are more dependent on the quality of input features.

Table 4 illustrates a sample of the discovered as-pects and features by CAFE. The algorithm identi-fies the important aspects in general sense as well as the important aspects that are not so obvious thus could be easily missed by human judges, e.g., suc-tion cup for GPS and glare for TV. In addition, both explicit and implicit features are identified and grouped into the aspects, e.g., expensive and price , big and size , sensitive and signal , etc. 4.3 Influence of Parameters We varied the value of  X  (distance upper bound ), s (the number of frequent candidates selected to gen-erate seed clusters) and w gt (the weight of sim gt ) to see how they impact the results of CAFE, for both feature extraction (in terms of F-Score) and aspect discovery (in terms of Rand Index). Both F-score and Rand Index increases rapidly at first and then slowly decreases as we increase  X  from 0.64 to 0.96 (see the left subplot in Figure 2). Because more clusters are allowed to be merged as we increase  X  , which is good at first but then it introduces more noise than benefit. Based on the experiments on three domains, the best results can be achieved when  X  is set to a value between 0.76 and 0.84. The mid-dle subplot illustrates the impact of s , which shows that CAFE generates better results by first clustering the top 10%-30% most frequent candidates. Infre-quent words/phrases are usually more noisy, and the results could be affected more seriously if the noises are included in the clusters in the early stage of clus-tering. Experiments were also conducted to study the impact of the three similarity metrics. Due to the space limit, we only display the impact of w gt and w g given w t = 0 . 2 . As we can see from the right subplot in Figure 2, setting w gt or w g to zero ev-idently decreases the performance, indicating both similarity metrics are useful. The best F-score and Rand Index can be achieved when we set w gt to 0.5 or 0.6 across all three domains. In this paper, we proposed a clustering approach that simultaneously extracts features and aspects of a given product from reviews. Our approach groups the feature candidates into clusters based on their domain-specific similarities and merging con-straints, then selects the important aspects and iden-tifies features from these aspects. This approach has the following advantages: (1) It identifies both as-pects and features simultaneously. The evaluation shows its accuracy on both tasks outperforms the competitors. (2) Both explicit and implicit features can be identified and grouped into aspects. The map-pings of implicit features into explicit features are accomplished naturally during the clustering pro-cess. (3) It does not require labeled data or seed words, which makes it easier to apply and broader in application. In our future work, instead of selecting aspects based on frequency, we will leverage domain knowledge to improve the selection.
 We would like to thank Lushan Han and UMBC Ebiquity Lab for kindly allowing us to access their Semantic Similarity Service. This research was partially supported by NSF awards CNS-1513721  X  X ontext-Aware Harassment Detection on Social Media X  and EAR-1520870  X  X azards SEES: Social and Physical Sensing Enabled Decision Support for Disaster Management and Response X .
