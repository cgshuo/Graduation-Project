 The rapid development of communication technol-ogy nowadays makes it easier than ever before to communicate with other people independent of dis-tance. With distances becoming irrelevant, one of the last barriers that hinders communications are dif-ferent languages. Although English has become a lingua franca in large parts of the world, in many sit-uations and for many people it is not an option. The different languages in the world also carry cultural heritage that needs to be protected. Forcing people to speak the same language will lead to a sever loss of cultural diversity.

There exist multiple possibilities to overcome this language divide. One possibility is to use inter-preters for simultaneous interpretation. But since this is a very costly method, it is only possible in certain areas. One example is the European Parlia-ment, where the demand for translation services is met by human interpreters.

Another area that can benefit from translation ser-vices are universities in non English speaking coun-tries. Looking at the statistics, universities in En-glish speaking countries have on average a higher percentage of students from abroad. One reason for this difference is the language barrier. While offer-ing lectures in English might increase a university X  X  attractiveness towards foreign students it is not desir-able due to the loss in cultural identity and intellec-tual diversity that occurs when universities around the world stop teaching in their native language. Un-like the European Parliament, universities do not have the funds to employ sufficient amounts of hu-man interpreters to simultaneously translate their lectures. Therefore, we developed a fully automatic translation solution that fits a university X  X  budget and deployed it within the Karlsruhe Institute of Tech-nology (KIT). By combining state-of-the-art auto-matic speech recognition (ASR) and machine trans-lation (MT) with auxiliary technologies, such as re-segmentation, punctuation prediction, and unsuper-vised speaker and domain adaptation we created a system that performs this task.

Developing systems for simultaneous translation poses several challenges. While the output should be of reasonable quality in order to being useful, the system is required to produce it in a timely fashion. Interactive scenarios like university lectures demand low latency. The delay of the output should be as low as possible in order to match the slides and the lec-turers gestures. Due to reasons, such as multimodal channels for the consumer and the lack of a need of additional technology in the lecture hall, we display the translation result as captions in a web browser that students can view on their own devices, such as laptops, tablets and smart phones. Preliminary stud-ies have shown that textual output is easier to digest than synthesized speech, especially if it does contain errors. Lately, we introduced various improvements in our setup to decrease the latency, e.g., by out-putting preliminary captions fast and, if necessary, updating parts as both the transcription and transla-tion hypotheses stabilize over time as more context is becoming available. The development of systems for speech translation started in the 90s. First systems were able to trans-late very domain specific and formalized dialogues. Later, systems supported greater variety in language, but were still built for specific domains (St  X  uker et al., 2007).

Despite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F  X  ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 2015). The goal is to find the optimal threshold be-tween quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014).
With ongoing research and development, the sys-tems have matured over the years. In order to as-sess whether our system helps students to better un-derstand lectures, we have conducted a user study (M  X  uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. The Speech Translation Framework used for the lec-ture translation system is a component based archi-tecture. It is designed to be flexible and distributed. There are 3 types of components: A central server, called the  X  X ediator X ,  X  X orkers X  for performing dif-ferent tasks and clients that request certain services. Our setup has 3 different kinds of workers: ASR sys-tems, punctuation predictors and MT systems. But the communication protocol itself does not distin-guish between these different types and does not limit the types of work be to performed.

Each worker registers on the central mediator, providing a  X  X ingerprint X  and a name the mediator. The fingerprint tells the mediator which type of ser-vice the worker provides. Based on these finger-prints, the mediator selects the appropriate chain of workers to perform the requested task. E.g., if a client asks for a Spanish transcription of English au-dio, the mediator would first select an English ASR worker and would then route the output through a segmenter for English Text and finally run the out-put through the MT to translate the English text into Spanish. 4.1 System Description The Lecture Translator (LT) at KIT was imple-mented based on the speech translation framework described above (Cho et al., 2013). We devel-oped all workers in-house. The audio is being tran-scribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acous-tic model was trained using several hundred hours of recordings from lectures and talks.
For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for produc-ing the translation. We use POS-based word reorder-ing (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 mil-lion sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lec-ture halls, among them KIT X  X  largest hall, called  X  X udimax X .
 In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is cap-tured via the PA from the microphone that the lec-turer uses to address the audience. The operation of the system itself is time controlled: It starts at the time when the lecture begins and runs until the lec-ture is finished. The workers of the system run dis-tributed over multiple servers. This ensures overall system stability as it allows for fail-overs in case of server failure. There are multiple instances of each worker running in order to translate multiple lectures in parallel.

During the every day operation the LT does not re-quire any special preparations from the lecturer prior to each lecture because of the integration into the PA and the time controlled operation. But the quality of the output can be improved if slides or lecture notes are being made available beforehand. This way, the system is able to adapt to the specific domain of a lecture by covering any terms or named entities spe-cial to this lecture. The second advantage that we use is that the same lectures are usually given re-peatedly in different terms. This way, we can use several iterations of the same lecture to improve the performance. Using the collected data, we adapt the ASR to certain speakers and ASR and MT to certain topics.

As the goal is to provide the service as cost effi-cient as possible, we decided to use the devices that the students already own to display the output. The Lecture Translator is therefore a web based service. Listeners wanting to see the transcription can go to running sessions. Depending on the permissions from the lecturer, the output can be displayed either only to people who know the password or viewers from within KIT or globally. A screen-shot from the user interface running an active session is shown in Figure 1. The transcription is displayed on the left part of the window while the translation is shown on the right. The user has the choice of various target languages, depending on the source language.
Our system currently supports the translation from German audio into English and French text. Using English as input language, the system is able to produce French, German and Spanish output. One of the main problems of earlier versions of our speech translation framework was the latency of the system. Since machine translation systems are usu-ally trained on sentence level, the translation can only be displayed if the whole sentence is recog-nized. In order to overcome this drawback, we ex-tended our framework to handle intermediate out-puts. This allows us to display a translation of a partly recognized sentence and later update it with the translation of the whole sentence. The same technique is also be applied to the to display inter-mediate hypotheses from the speech recognition that are later updated.

In the framework, each message has properties defining the time span to which its content relates. For example, if the MT component generates a new translation, it will generate a message with the start and end time of the translation and the translation itself. In the baseline system, the start time has to be equal or greater than the end time of all previous messages.

In order to limit the complexity, we only allow to update the most recent messages. Every time a mes-sage with a new starting time is received, this im-plicitly will mark all messages prior to this starting time as final and no updates to the content of these messages is allowed. Allowing updates for every message would be too complex, as we also allow to change the time span of the updated messages. This would lead to difficulties for all messages except the most recent one. Furthermore, in this case the dif-ferent components would need to store information about the whole session instead of only information about the non-final sections.

In order to facilitate the new possibilities of the framework, each component was extended in order to handle intermediate output and input. On the in-put side, the content of the new message can no longer be simply attached to the previous output, but it might also overwrite part of the stored content. Therefore, additional bookkeeping is necessary. On the output side, we can now already output prelim-inary results and later update them with better hy-potheses.

When generating new messages we have to make sure that we do not mark content as final by using a new start time for the next message although the input for this text has not been marked final by the previous component. In this paper we presented our automatic simultane-ous translation system for university lectures. The lecture translator is installed in four lecture halls at KIT and has been running for several years now. The system features several techniques that are specif-ically tailored at the needs of a simultaneous sys-tem processing an unsegmented stream of continu-ous speech. Feedback from the students and a sys-tematic user study have shown that the system helps students to better follow the lectures if they are not (yet) completely fluent in German. Currently we are increasing the number of lecture halls at KIT that the system is installed in and are working with other universities that are also interested in deploying the system.

