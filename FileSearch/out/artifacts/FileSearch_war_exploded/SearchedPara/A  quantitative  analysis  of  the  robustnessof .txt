
When asked about the essential differences between knowledge-based systems (KBSs) and conventional software, one often hears the claim that KBSs can deal with incomplete, incorrect and uncertain knowledge and data, whereas conventional software is typically very brittle in these respects (see e.g., Hayes-Roth (1984) for a very early formulation of this claim). Although, nowadays researchers no longer view this distinction as either necessary or sufficient to define a KBS, it is believed that the ability of KBSs to deal with missing or invalid data is an essential dimension of KBS validation.

There have been both practical experien ce and theoretical analysis over many years to back the mentioned claim. As an e xample of practical experience, Preece et al. (1997) reports that, in a number of verification exercises, errors were found in the knowledge-base of KBSs, which were nevertheless still functioning at ac-ceptable levels. As an example of theore tical analysis, ten Teije and van Harmelen (1996, 1997) prove that, for a large class of diagnostic systems, the computed set of diagnoses degrades gracefully and predi ctably when either the system input (ob-servations) or the knowledge-base degrades in quality.
 plete, incorrect or uncertain knowledge an d data has been limited to such practical experience and qualitative analysis. Little or no attempt has been made at a quan-titative analysis of the proclaimed robus tness of KBSs. A recent special issue of a journal was dedicated to methods for evaluating KBSs (Menzies and van Harme-len 1999). None of the papers in that special issue performed any quantitative an-alysis on the quality of KBSs. The editorial of this special issue lists only a handful of quantitative evaluation studies that have been performed over a decade or more of KBS research. In fact, one paper in that special issue (Shadbolt et al. 1999) even seems to suggest that global qualitative evaluations are about as much as we can ex-pect from KBS evaluation projects. Finally, one of the reviewers of this paper even remarked:  X  X or a long time, the KA community has decried the lack of good evalua-tion metrics to measure the quality of the KA process and of the resulting knowledge bases. X  We consider this a serious defect in the study of KBSs, particularly because such robustness is often proclaime d as a unique characteristic of KBSs. of KBSs is both possible and useful. To argue this claim, we present a case study in which we perform such a quantitative analysis for a particular KBS. In Sect. 2, we describe our approach to measuring robustness by degradation studies, and we give definitions for the basic notions involved in such degradation studies. In subsequent sections, we apply this approach in a cas e study. Section 3 describes the KBS that we subjected to a degradation study. Section 4 gives an overview of the degradation studies we performed. Thereafter, Sect. 5 reports our robustness results with respect to the data input and Sect. 6 the robustness results with respect to the knowledge base used. The results obtained will be analysed in these two sections. Finally, Sect. 7 summarizes the main points of the paper and, in Sect. 8, we look at future steps to be taken.
In this section, we describe our approach to measuring robustness by degradation studies and we give definitions for the basic notions involved in such degradation studies. Our aim is to define a very gener al set of notions that can be widely used in future degradation studies. We regard this section as the central contribution of this research: the definitions in this section should form the basis of similar analyses by other researchers and practitioners.
The IEEE Standard Glossary of Software Engineering Terminology (IEEE 1990) gives the following definition for robustness:
Informal Definition 2.1 (Robustness). The degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions.
In other words, robustness of a KBS is concerned with the way in which the quality of the KBS output degrades as a function of a decrease in the quality of the KBS input. This definition immediately lead s to the idea of degradation studies:
Informal Definition 2.2 (Degradation study). In a degradation study , we gradually decrease the quality of the KBS input and measure how the KBS output quality changes as a result.

This informal definition contains the con cepts of  X  X BS input (quality) X  and  X  X ut-put quality, X  which we will discuss in more detail in the Sects. 2.2 and 2.3. Of course, we must be more precise about the rather vague notion of quality of the
KBS input and output. Concerning the KBS output, we assume that this is always a set of answers. In fact, for many typical KB S tasks, this is a realistic assumption: a set of consistent classes in a classification task, a set of likely hypotheses in a di-agnostic task, a set of potential designs in a configuration task, etc. stated:
Assumption 2.1. For the KBSs that we consider, we assume that their output can be interpreted as a discrete set of answers.

Under this assumption, we define two measures for KBS output quality. Let correct ( I ) be the set of all correct answers for a given input I ,and output the set of actually computed answers for a given input I .

Definition 2.1 (Recall). The recall ( I ) of a KBS for a given input I is defined as:
In other words, the recall is the fraction o f correct answers that the system actu-ally computes. It can, of course, happen that correct ( I answer. For example, this happens when the system is presented with a case I for which no correct output exists, such as an inconsistent set of observations for a clas-sification system or an inconsistent set of requirements for a design system. In this case we define: as all correct answers are recalled by the system.

Definition 2.2 (Precision). The precision ( I ) of a KBS for a given input I is defined as: correct. In the case output ( I ) =  X  (i.e., the system returns no output), we define: This reflects the intuition that the only correct output in this case is no output at all. correct answers) but, in practice, they are a ntagonistic: a higher precision (recall) is usually paid for with a lower recall (precision).
 quality. First, these definitions are well known from the literature on information retrieval (e.g., Salton and McGill 1983) and have proven to be useful, informative and intuitive measures in many studies in th at field and elsewhere (see, for instance,
Schumann and Fischer (1997) for an application of these measures to deduction-based software component retrieval). Second, these measures are completely general and make no commitment to either the task or the domain of the KBS that we wish to study. Consequently, the approach proposed can be directly applied to other KBSs, even when they are very different from the one that we happen to have chosen in our own case study.
 soundness and completeness: recall corre sponds to the degree of completeness of the system and precision corresponds to the degree of soundness of the system.
These measures provide a quantitative angle on earlier work by van Harmelen and ten Teije (1998), which was strictly qualitative.
We can represent a KBS graphically as in Fig. 1: the data, the knowledge base, the algorithm and the output. By KBS input, we mean the data given for example by a user as well as the knowledge base used by the system. The degradation experi-ments we propose for measuring the robustness of the system will either be with respect to the quality of the data input or with respect to the quality of the knowledge base.
 urement. These aspects are in completeness or incorrectness of the data or knowledge.
Although these aspects are quite general, they may not be applicable to every know-ledge base. The aspects proposed also do not cover all possible aspects one might want to measure. More explicitly formulated, with incompleteness and incorrectness, we mean the following:
Incomplete: A part of the data input or part of the knowledge used is missing from the KBS input. For example, the data input could be a number of observations.
Some of the observations might be to hard or too expensive to obtain. A part of the knowledge might be missing becau se the knowledge was obtained from human experts who forgot to provide it.

Incorrect: In contrast with incomplete data or knowledge, a certain part of the data or knowledge is provided by a user or expert. However, it is incorrectly rep-resented. For example, incorrect data could be caused by a user who has made a wrong observation. Incorrect knowledge could, for example, be caused by faulty knowledge of an expert, misunderstandings when coding the knowledge into the system or limitations of the representation used by the system.

These two aspects are important in KBS validation. For example, a knowledge base will most likely be incomplete and often partially incorrect. Hence, to analyse the robustness of a knowledge base with respect to incompleteness or incorrectness is a realistic and important issue.
 Notice that we have already taken a step further than van Harmelen and ten
Teije (1998). In that paper, the authors did not commit to any definition of quality on input or output, and only d emanded that whatever the definition was, it should respect a partial ordering. In our approach with degradation testing, we commit to a specific definition of output quality while leaving input quality open to be defined for each specific application.
The only notion that is still left undefined is some ordering on robustness: when do we call a system more robust or less robust than another? Unlike output quality (where we have given a single widely a pplicable definition) and input quality (the definition of which is deliberately left open to depend on the task type), we have not been able to determine a good answer to this question.

When input quality decreases, a system that produces an output with a fluctu-ating quality is much less predictabl e than a system that produces an output with a monotonically decreasing quality. We therefore demand that any system that is called robust at least produces an output with monotonically decreasing quality as a function of decreasing input quality.

Definition 2.3 (Monotonicity). A robust system will show a monotonically decreas-ing output quality as a function of deteriorating input quality.
Note that this demand corresponds precisely to the usual demand on anytime al-gorithms that their output quality monotoni cally increases with increasing run time (Dean and Boddy 1988). However, this demand is insufficient for ordering various systems according to their ro bustness. We therefore giv e additional competing defi-nitions without choosing one over the other.
Definition 2.4 (Quality value). Asystem S 1 is more robust than a system S a set of inputs if, everywhere on that input set, the output quality of S than the output quality of S 2 .
 Definition 2.5 (Rate of quality change). Asystem S 1 is more robust than a system
S 2 for a set of inputs if, everywhere on that input set, the output quality of S decreases more slowly than the output quality of S
Definition 2.6 (Integral of quality value). Asystem S 1
S 2 for a set of inputs if, on that input set, the integral of the output quality of S is larger than the same integral for S 2 .
 (and is concerned with which system produces the best output), while Definition 2.5 compares the first derivative of the output quality of the systems (and is therefore concerned with which system produces the most stable output). Definition 2.6 com-pares the overall quality of the output quality over an entire interval even when neither system always dominates the other (as required in Definition 2.4). These definitions are illustrated in Fig. 2. 3 range. Instead, they should be applied to an interval of interest. Usually, this will be some interval including 1 (i.e., [ x , 1 ] ).
 according to Definition 2.4 because, on that interval, its output quality is always higher than that of S 2 . However, according to Definition 2.5, S of the two because its output quality decreas es more gradually (reading the graph from right to left). Definition 2.6 allows us to take a more overall perspective: it takes the size of the area under the output quality graph as a measure of the overall quality. Under this definition, S 1 is more robust on the entire interval the value of 1 0 output quality d ( input quality ) is larger for S that the situation is rather different in Fig. 2b. Although the output quality again increases monotonically from 0 to 1 over the same interval, the comparisons between
S and S 2 on various subintervals are very different. Now, for example, on the interval [0.1, 0.3], S 2 is more robust than S 1 when using Definition 2.4, but S than S 2 when using Definition 2.5.

At the current point in our research, we simply propose each of these defini-tions as reasonable, without cl aiming superiority of any definition in all cases. In fact, we believe that, under different pragmatic circumstances, different definitions will be preferable: if steep drops in system performance are to be avoided, the sec-ond definition is preferable. If one is in terested in upholding output quality as long as possible in the face of declining input , the other two definitions may be pre-ferred.
For our case study, we have used a classification system for commonly occurring vegetation in Southern Germany. The plant-classification system was created with the D3 Shell-Kit, which is a tool for the development of KBSs. We will not discuss this tool here but refer to a number of publications about D3 (Puppe et al. 1994, 1996). It is also possible to download a demo version of the software from the URL http://d3.informatik.uni-wuerzburg.de .

The plant-classification system that we studied can have 40 different observables as input and has 93 different plant names as output. The knowledge base consists of 7,586 rules. Furthermore, with the sys tem, we received 150 test cases. Each of these cases consisted of the set of observations for that case (color and shape of flowers, leafs, stem, etc.), together with th e (supposedly correct) answer for these observations as given by a human expert. Around 97% could be answered correctly by the system.

The input observations can be entered in a graphical user interface, but the user is not restricted to the ordering in this int erface. The observations can be entered in any order, thus the input can be seen as a set . This is not entirely true because some observations are dependent on other observations and will only appear when certain input conditions are met. Because of these d ependencies, the maximum number of observations that can be given for one case is 30.

For our degradation tests, we translated the plant classification system into Prolog code. This resulted in a knowledge base with 11,724 rules, all with the following representation:
Each time a new observation is given to the s ystem, all rules are collected con-taining the same Observation and Value . For each of these rules, the score of the Plant mentioned in the rule is adjusted by adding the Score to its current score. When the score crosses a threshold, it is output by the system.
In fact, the rules do not actually contain numerical scores but descriptions, which were used to make it easier for the experts to express their knowledge. The de-scriptions are, however, translated to num erical scores whenever they are used by the system. We will therefore use the descriptions as if they are numbers. The de-scriptions range from P 1 through P 6 for positive scores and N negative scores. Furthermore, | Pi |=| Ni | for i = 1 ,... , i = 1 ,... , 5.

In the following sections, we will report our robustness analysis of the plant classi-fication system. In Sect. 5, we will analyse the robustness with respect to the data input, while in Sect. 6, we will analyse the robustness with respect to the knowledge base used.
 only serves to illustrate our proposal to analyse the robustness of KBSs through degradation studies. The important aspects of this case study are the quantities meas-ured and how they were analysed, not the obtained robustness results of the plant classification system.
 values of recall and precision in this case s tudy. Because, for every case, there is at most one correct answer (namely the name of the actual plant on which the ob-servations were made), we have for any case I , | correct the knowledge base or | correct ( I ) |= 0 iff the case is not in the knowledge base.
As a result, the only values that recall ( I ) can assume are either 0 or 1. For the same reason, precision ( I ) is either 0 or 1 / | output ested in the specific behaviour of the system for a particular case but in the average behaviour of the system. Hence, we are inter ested in the average recall (i.e., the sum of all recall values divided by the number o f cases used) and the average precision.
We will represent these averages in our figures, but will use for example the term  X  X ecall X  when in fact we mean  X  X verage recall. X 
In this section, we present the robustness results with respect to the data input that we have obtained in empirical experiments with the plant-classification system. First, we will define the input quality measure we will use in Sect. 5.1. Thereafter, we will give results using the ordering on the observables found in the test cases (Sect. 5.2). We analyse the effect of other orderings in Sect. 5.3. We give some conclusions in
Sect. 5.4.
According to the definitions from Sect. 2, we must still decide on what to use as a measure on the input quality. In this case study, we choose the completeness of the input as the measure of input quality. In our classification system, completeness of the input can be directly translated as the number of available observations .
Robustness: in many practical classification settings, the input observations are not
Anytime behaviour: Even when all observations are present, there are practical set-
As a result of this second reason, the degradation results that we present in this section can also be seen as anytime performance profiles for the plant-classification system. Performance profiles are a basic tool in the study of anytime algorithms (Dean and Boddy 1988). They plot the output quality as a function of available run time. Because available run time can be interpreted as one aspect of input quality, such performance profiles are simply a special case of our more general proposal: performance profiles only study output degradation as a function of decreased run time, whereas our approach is applicable to any aspect of input quality that one chooses to model.

In the following, we will present a number of graphs analysing the robustness of the plant-classification system. Each of these graphs plot output quality (measured by either recall or precision) against input quality (measured by the number of obser-vations that were available to the system). If one is interested in anytime behaviour, these graphs can be read from left to right:  X  X hat happens when the system has time to process more and more of the inputs? X  If one is interested in robustness, these graphs should be read from right to left:  X  X hat happens when the system is provided with fewer and fewer of the inputs? X 
Now that we have established that the number of available observations will be the input aspect that we will degrade in our studies, we have to decide in which order observations will be made available to the system. Our first choice is simply based on the order in which the observations a ppeared in the test cases for the plant-classification system. Each such test case c onsisted of a list of observables and their values for that case. Figure 3a shows how the precision of the answers from the system (as defined in Definition 2.2) incr eases when longer initial sequences of the test cases were given to the system. The first surprise that this graph has in store for us is its monotonic growth:
Surprise 5.1. Both average precision and average recall (see Fig. 3) grow monoton-ically (or almost monotonically in the case of precision) when adding more obser-vations. This is somewhat surprising becau se this is not true for individual cases. and negative scores. This means that the answer set can both grow and shrink when adding more observations. In fact, only 58% of the test cases have a monotonically growing answer set. As mentioned above, such monotonic behaviour is desirable from both a robustness and from an anytime p erspective, so on a case-by-case basis, the plant-classification system does not score very well on this. Surprisingly, the average-case behaviour of the system is apparently much better.
 not able to make any sensible guess at likely solutions (this holds up to about six observations). For higher number of available observations, the graph is surprising for two reasons:
Surprise 5.2. After about 12 observations, adding more observations does not in-crease the precision. This is surprising because most cases contain as much as 19 X 30 observations. Figure 3a suggests that 12 observations is sufficient to obtain the max-imally achievable precision on average.

Surprise 5.3. The region in which additional observations actually contribute to an increase in precision is surprisingly small, namely between the 6 and 12 observations.
Of the 19 X 30 observations per case, all observable changes to the output seem to be in this small segment of observations! the recall from Definition 2.1.
 is rather significant. It shows that the distribution of the actual precision values that were obtained for the different cases are actually spread rather widely around the average. 5 average from Fig. 3a. Each line in this fi gure shows the percentage of cases that achieved a precision of at least a certain valu e after the given number of observations.
The lowest line shows that, after 12 obser vations, 40% of the cases have already reached the maximum precision (namely 1). Fu rthermore, and more surprisingly, this percentage then stops growing! This means that:
Surprise 5.4. When aiming for the maximum precision of 1, there is no need to use any more than 12 observations (out of a maximum of 30!). If the maximum precision has not been reached after the first 12 obser vations, adding furt her observations will not help.
 that extending beyond 12 observations was not useful on average . Here we see that, for harder cases, a few more observations do actually help, although not more than 20 observations in total.
 the percentage of cases with a precision of at least 0.2 continues to increase during a longer interval. Apparently, harder cases (those that ultimately achieve a lower pre-cision) benefit more from additional observations than easy cases (those that achieve precision 1). Nevertheless, even there we see that no increase is gained after about 20 observations:
Surprise 5.5. Whatever the final precision that is ultimately obtained by the system, this level of precision is already obtaine d after at most 20 observations. It seems that asking for any more then 20 observations will not improve the output quality any further. This is surprisi ng because many cases (in fact 98% of the test set) contain more than 20 observations.

Looking at the initial segment of observa tions, we see another surprise: although we may expect that a low number of observations leads to a low average precision, sixth observation: Surprise 5.6. No increase in precision can be gain ed from the first six observations.
This means that, in an anytime setting, interrupting the system before the sixth observation is completely useless becau se no increase in precision will have been obtained yet.

Figure 4 is particularly interesting from an anytime perspective: it tells us, for each partially processed input, what the ch ance is that the system has already ob-tained a certain precision in its output: for instance, after having fed the system ten observations, there is a 30% chance that it has already obtained the maximum pre-cision of 1, a 45% chance that it has alread y obtained a precision of at least 0.5, and a 60% chance that it has already obt ained a precision of at least 0.3. formation can be used by the user to determine if it is useful to continue feeding the system more input or if a sufficiently high precision has already been obtained for the purposes of the user so that processing (and acquiring potentially expensive observations) can be stopped. Our graph (when interpreted as a performance profile) contains much more information than the usual performance profiles presented in the literature (e.g., Zilberstein 1996) . These graphs typically give only a single expected value for the output quality at any point in time (compare our Fig. 3a), whereas we give a probability distribution of the expected output value, which is much more informative.
 omitted this because, in our case study, the recall is either 0 or 1, thus the resulting figure would be the same as Fig. 3b.
In the profiles so far, we have degraded the input by removing observations in the order in which they were listed in each test case. Similar degradation experiments were performed using different orderings, which are reported in Fig. 5. line shows the theoretically optimal aver age-recall profile: at each step in each case, we computed which next observation would contribute maximally to an increase in recall. This computation can only be done theoretically for test cases where all ob-servations are already present. The rightmo st line shows the slowe st average-recall profile. Note that every other recall profil e must lie between these two lines. Fig-ure 5 also shows a bundle of lines. Each line corresponds with a random ordering.
Finally, we also performed experiments with the ordering shown by the graphical user interface (GUI), which is represented by the line close to the dotted line. These experiments lead us to the following observation:
Surprise 5.7. The degradation sequence taken from the test cases and GUI is sur-prisingly effective in obtai ning a high recall after only a few observations. In fact, it is much closer to the theoretically optimal sequence than the randomly generated (information-free) sequences.
From the first experiment in which we used the test-case ordering, it is clear that the robustness of the plant classification system in not very uniform across the dis-tribution of input quality. When degrading the input quality, the system first appears extremely stable against missing observations, as no quality loss occurs at all. This holds until about 12 X 15 observation. At that point, the robustness of the system is very low and the input quality drops dramatically.

The experiments with different orderings showed that the plant classification sys-tem is very sensitive to the specific order in which the observations are presented to the system, which had effects on the robustness profile obtained. This type of ro-bustness is not covered by our definitions in Sect. 2.4. The definitions there are all concerned with comparing the behaviour of different systems on the same degrading input. The phenomenon observed in Fig. 5 concerns the behaviour of a single system on different ways of degrading the input. We leave it for further research how to include this type of robustness in our approach.
The degradation experiments on data input already yielded interesting insights into the behaviour of a realistic KBS. In this section, we will perform the same kind of analysis with respect to the knowledge base used. For the input quality, we will measure the incompleteness and incorrectness of the knowledge base. In Sect. 6.1, we will analyse robustness with respect to an incomplete knowledge base, whereas in Sect. 6.2, we will analyse robustness with respect to an incorrect knowledge base.
Our experiment is as follows: we select some percentage of rules at random from the knowledge base, remove the selected ru les and compute the recall and precision value for each case with the modified knowledge base. The average recall (i.e., the sum of all recall values divided by the num ber of cases) and the average precision will indicate the robustness of the system with respect to the percentage of rules removed. Of course, the experiment has to be repeated multiple times to obtain the robustness of the system on average. Note that, as we are no experts ourselves in plant classification, and therefore are unable to check if some rules are missing from the knowledge base, we will call the provided plant classification system complete, although in reality it may not be.

The results of the experiments are repor ted in Fig. 6b for the recall and in Fig. 6a for the precision. For each percentage, the average recall and average precision were obtained over ten different r uns. Besides average value s, the minimal and maximal obtained values are also plotted in both graphs.

Both graphs show an almost smooth decrease of our measured values. The graphs are surprising in that the measures start to decrease around 40% in both graphs.
Surprise 6.1. In the plant classification system, almost 40% of the knowledge base can be removed at random without severe quality loss.
 average recall and precision are computed over 150 cases. When a rule is removed from the knowledge base, it will only have an effect on a small number of cases and therefore only a small effect on the a verage recall and precision. When more rules are removed, more cases will be eff ected, which results in a decreasing recall and precision.
 robustness behaviour of the system analyse d. Nevertheless, the experiment performed has a drawback. It is not realistic to a ssume that each element in the knowledge base has the same chance of being forgotten. It would be more realistic to remove parts of the knowledge base based on some probability measure. Several measures should be compared when it is not obvious which measure should be used.
 random from the knowledge base. Our results with this probability measure are also in Fig. 6. Obviously, the results have changed dramatically. Whereas our previous results (also in Fig. 6) showed that 40% of the knowledge base could be removed without severe quality loss, the new results show that, when 40% of the knowledge base is removed in a biased way, the system becomes useless. Furthermore, the drop in output quality already starts at 15% and is much steeper than before. analysing the robustness of KBSs. The experimental setup should be as realistic as possible and care has to be taken when interpreting the results, as these can be sensitive to the chosen setup, as we demonstrated with our case study.
In our case study, the knowledge base consists of rules. Each rule consists of some observations, a name of a plant and a score. To test the robustness with respect to incorrect knowledge, the rules need to be modified in some way. However, we like to do this in a realistic setting. Modifying the observations at random is therefore no option. First, some observations are usually answered right while others are prone to errors. Second, if an observation is wrong, it is usually not done at random. Some answers look alike while others are clearly distinct. Modifying the observations of the rules realistically therefore requi res domain knowledge, which we lack. How-ever, modifying the score of a rule in our case study does not require any domain knowledge.

In our experiments with incorrect knowledge, we address the question:  X  X ow robust is the system for incorrectly entered scores? X  Our experiment is therefore as follows: we select some percentage of rules at random from the knowledge base, modify the score of the selected rules, an d compute the recall and precision value for each case with the modified knowledge base. The average recall (i.e., the sum of all recall values divided by the numbe r of cases) and the average precision will indicate the robustness of the system with respect to the percentage of rules modified.
Of course, the experiment has to be repeated multiple times to obtain the robustness of the system on average.
 Only one question remains:  X  X n what way do we modify the score of a rule? X  We identified the following parameters:
Direction: Scores can be changed to a higher or lower value. Most likely, scores are damaged in both directions. Neverthel ess, we also investigate the effect of a biased expert, i.e., an expert who always assesses something too high (or too low). We will use the word positive when scores are only changed to a higher value and the word negative when scores are only changed to a lower value. When both words do not occur, it means scores are changed in both directions.
Size: The scores of rules are changed from one class to another. Most likely, the new class is only one higher or one lower than the old class. We will call this kind of damage a near miss. To be complete , we also investigate the effect when the difference of the new class and old class is two classes. We will call this kind of damage an error.

The parameters lead to six different experi ments, but before discussing the actual results, let us first consider wh at results are to be expected.
The plant classification system we are analysing uses a threshold and only gives plants with a score higher than this threshold as output. When we change the scores of rules to a higher value, it follows that the final score of the plants can only increase. As the recall only measures if th e correct answer is given as output, it follows that the recall can never decrease. The recall will probably increase as more plants (including the correct answer) are now more likely to cross the threshold and be given as output.

Hypothesis 6.1. When the scores of rules are increased, the recall of the system will increase.

The converse also holds. When we change the scores of the rules to a lower value, it follows that the recall can never i ncrease. When the scores are decreased sufficiently, the recall will decrease as outputted plants that are correct will drop below the threshold.

Hypothesis 6.2. When the scores of rules are decr eased, the recall of the system will decrease.
 complex. Considering the formula for the precision (Definition 2.2), we can identify two causes for an increase in precision: I1. The score of the correct plant is increased and thereby crosses the threshold.
I2. The scores of one or more incorrect pl ants decrease and thereby drop below
D1. The score of the correct plant drops below the threshold. The precision changes D2. The score of one or more incorrect plants increase and cross the threshold.
However, I1 is unlikely, as previous experiments showed that the recall of the system is already close to the optimal value of 1. It follows that we expect a decrease in precision.

Hypothesis 6.3. When the scores of rules are incr eased, the precision of the system will decrease.

As these causes change, the precision into two different directions it is not obvious to predict the outcome of the precision.
 to lower values and to higher values, as all four causes for the change in precision can occur. Also the recall can now both incr ease and decrease and is therefore harder to predict. Nevertheless, for the recall, we know that the performance profile has to lie somewhere between the results for the strictly positive and negative changes.
Hypothesis 6.4. When some rules are changed positively while other rules are changed negatively, the recall of the system will be between the recall values of the strictly positive and negative changes.
 changes. We expect that a larger change will have a larger effect on the average values. For example, when the scores are ch anged positively and the recall increases with near misses, we expect the recall to increase even more when we experiment with positive errors.

Hypothesis 6.5. When comparing experiments in which we make errors instead of near misses without changing the direction of the changes, the effect observed in the near-miss experiment will be made stronger. Now we will discuss the results, which are shown in the Figs. 7 through 9. First note that our hypotheses hold in the shown figures. In the case of positive
Changes, the recall increases as is shown by Fig. 7b (Hypothesis 6.1). In the case of negative changes, the recall Decreases, as is shown by Fig. 8b (Hypothesis 6.2).
In case of positive changes, the precision Decreases, as is shown by Fig. 7a (Hy-pothesis 6.3). In case of near misses, the recall (Fig. 9b) will lie between the values found in the strictly positive experiment (Fig. 7b) and the strictly negative experi-ment (Fig. 8b). The same holds for the expe riments with errors (Hypothesis 6.4). In case of positive changes, we find that the recall in the experiment with errors lies everywhere above the recall in the experi ment with near misses (Fig. 7b). In the case of negative changes, we find that the recall in the experiment with errors lies every-where below the recall in the experiment with near misses (Fig. 8b) (Hypothesis 6.5).
For example, in the case of positive near misses (Fig. 7a), we find a linear decrease in precision whereas the decrease is nonlinear when we increase the size of the changes.
Precision, as there are two causes that could change the precision in opposite direc-tions (causes I2 and D2 discussed in Sect. 6.2.1). Figure 8a tells us that I2 happens more often than D1 in the first segment of the figure as the precision increases.
Hence, in most cases, first some incorrect plants drop below the threshold before the correct plant drops below the threshold. It follows that, in most cases, the cor-rect plant has outscored most of the incorrect plants, which is the desired behaviour of the system.
 errors (Fig. 8); however, the curve of the negative near misses is compressed into the first half of the figure because the size o f the changes has increased. Note that this holds for both the precision and the recall.
 the system. Probably the most realistic setting is the experiment with near misses (Fig. 9). Both precision and recall show a very stable curve, indicating a robust knowledge base for near misses. Although the knowledge base appears to be robust on the entire range of the x-axis, from a practical point of view, usually only the first part of the figure is important.
In this paper, we have argued for the need for quantitative analysis of the qual-ity of KBSs. In particular, we have shown how robust behaviour in the light of incomplete system input as well as an incomplete and incorrect knowledge base is amenable to such quantitative analysis. Our quantitative analysis is based on the idea of degradation studies : analyse how the quality of the output changes as a function of degrading input. We have proposed a set of general definitions that are general enough that they can be used in similar degradation experiments by others, even if the systems concerned are of a very different nature than the one in our case study.
We have shown the practicality of our approach by applying it to a particular case study . This yielded a number of surprising insights into the behaviour of the system under study.

We believe that the following issues are the most important in our proposed approach of degradation studies for measuring the robustness of KBSs: Generality: Degradation studies are general enough to be applicable to many KBSs.
The output quality is measured using the recall and precision, which are well known in information retrieval. Furthermore, the general concepts of incomplete-ness and incorrectness can usually be used as measures for the quality of the input.

Insights: Degradation studies provide insights into the behaviour of the system ana-lysed. As we demonstrated in our case study, this includes its anytime perform-ance with respect to incomplete data or its robustness with respect to incomplete or incorrect data or knowledge.

Performability: The degradation study we performed could quite easily be per-formed. We were able to do this becau se we had easy and quick access to the
KBS (i.e., giving input to the system and reading its output by an external pro-gram). Furthermore, programs could be used to modify the input and knowledge base in a controlled way. These conditions should not be too hard to realize for many other KBSs.

Setup: Before performing a degradation study on a KBS, one should carefully con-sider the experimental setup. The way data or knowledge is removed or modified may greatly influence the outcome of the degradation studies.

The ultimate suggestion that follows from this work is that any KBS should, upon delivery, come accompanied with a set of de gradation statistics such as discussed in this paper as a quantitative way of measuring interesting and important aspects of the systems quality. This would contribute to a more empirical and quantitative analysis of AI systems in general and of KBSs in par ticular, very much in the spirit of Cohen (1995).
Although the results above already yield interesting insights in the behaviour of a re-alistic KBS, many other aspects could still be uncovered using further degradation studies. We discuss some of these extensions in this section:
The informal definition for robustness that we used as a starting point in Sect. 2 has not been carried through entirely in the paper. Functioning correctly in the pres-ence of invalid inputs has not been evaluated in the case study and should be included in future research.

Zilberstein (1996) suggests a category of measures on output quality that is not yet covered by the recall and precision that we have used in the above, namely speci-ficity of a solution. This is intended to represent the degree of detail in the system X  X  answer. An example would be a system that can compute names of ever finer grained plant families instead of only individual species (as above). This property was ir-relevant in our case study because the pla nt-classification system only deals with a flat list of candidates, not with a hierarchically organized space of candidates. We have therefore ignored this potential third dimension of KBS output quality, but we expect that good measures can be devised for this just as well as for the other two dimensions that we did handle.
 systems with a discrete output (a set of answers). Some KBS applications return real-valued answers (e.g., ratings). We must study how these systems can also be subjected to degradation st udies using acceptable measures . In fact, the plant-classifi-cation system not only returns a set of candidates, but indicates a numeric score for each candidate. Our current output measures co mpletely ignore this score. A further step would be to also include this score in the quality measures.
 pected quality of the output after a given number of observations. In effect, Fig. 4 is the result of learning the anytime performance profile through the test cases. As with any learning task, we can apply cross-validation to the set of test cases (Cohen 1995): cases to check the accuracy of the p redicted performance levels.
 cases where the correct answer is actually known. This is not as obvious as it may sound. In many applications (e.g., computing the best solution to a design problem) the correct (i.e., best) answer is not known to any human expert. In such cases, one must either resort to known approximations of the correct answer or fundamentally different quality measures must be defined.
 correctness of the answers computed by a system. Of course, there are many more aspects to the quality of a KBS, such as the quality of its explanation, its computa-tional efficiency, its interaction with its en vironment (be it users or other systems), etc. It is an open issue to us whether the same  X  X egradation-study approach can be taken to quantifying any of these other aspects of the systems quality.
