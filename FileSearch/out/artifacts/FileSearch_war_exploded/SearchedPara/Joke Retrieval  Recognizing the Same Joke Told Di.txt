 In a corpus of jokes, a human might judge two documents to be the  X  X ame joke X  even if characters, locations, and other details are varied. A given joke could be re told with an entirely different vocabulary while still maintaining its identity. Since most retrieval systems consider docum ents to be related only when their word content is similar, we propose joke retrieval as a domain where standard language models may fail. Other meaning-centric domains include logic puzzles, proverbs and recipes; in such domains, new techniques may be required to enable us to search effectively. For jokes, a necessary component of any retrieval system will be the ability to identify the  X  X ame joke, X  so we examine this task in both ranking and classification settings. We exploit the structure of jokes to develop two domain-specific alternatives to the  X  X ag of words X  document model. In one, only the punch lines, or final sentences, are compared; in the second, certain categories of words (e.g., professions and countries) are tagged and treated as interchangeable. Each technique works well for certain jokes. By combining the methods using machine learning, we create a hybrid that achieves higher performance than any individual approach. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms Humor, document similarity, domain-specific retrieval Humor is famously difficult for machines to comprehend. It brings into play ambiguities, implications, and exaggerations, all in the service of violating exp ectations X  X hich requires one to have expectations in the first place. To believe Hollywood writers, humor will be the last sk ill for artificial intelligences to acquire. To believe linguistic a nd computational researchers, jokes are a domain where  X  X he question of semantics can no longer be avoided X  [25]; and wors e, they contain  X  X anguage that requires deep conceptual knowledge about the details of human experience X  [5]. One concept that humor brings into focus is an alternative notion of document similarity. Even more than is true for the news stories and other informative documents typically used in information retrieval, jokes can be related without having many words in common. What we would consider  X  X ne joke X  can be retold in vastly different ways. 1 For instance, Figure 1 shows a joke, and Figure 2 outlines how its elements change in other documents in our collection. Characters can change, the setting can change; it is difficult to describe, at the word level, what it is that stays constant in a joke X  X  structure or meaning. One wa y to evoke this challenge is to try to formulate a search query for a joke X  X or instance, to determine if any version of it is present in a given corpus. (Perhaps we hazily recall the joke but want to see a full version before retelling it.) With the ex ample shown in Figure 1, we might begin with  X  X riest rabbi accident wine, X  but then pause, realizing the joke could really be about any two people, so it would be better to remove them from the query. Next,  X  X ccident X  could be  X  X rash X  or  X  X ollision, X  and  X  X ine X  could be  X  X hiskey X  or  X  X hampagne. X  What is left? Pe rhaps a few variations on  X  X rink police accident, X  a query which is less precise and would still fail to retrieve the version in Figure 3. The problems here are not just synonymy or paraphrasing, but also the difficulties of knowing which aspects of a joke are likely to vary and capturing the wide range of possible alternatives. Jokes are just one of many domains in which this structural, or logical, document similarity can frustrate searches. Difficulty formulating queries to describe a particular meaning also arises when searching for famous quotati ons. When one is asked,  X  X an you find that quote where Einstein said ..., X  sometimes all one can do is verify that Einstein didn X  X  say it, and that in fact no one said exactly that phrase, even though the quote we wanted is likely out there. Song lyrics also have this property: one must remember them verbatim to find them on the web. A similar domain is that of proverbs: a gi ven saying may be expressed in numerous ways, particularly across cultures, and it would be interesting to find versions of the same message. See, for example, the recent documentary The Aristocrats , which explores the variations of a single joke [27]. A genre closely related to jokes is logic and math puzzles: having solved how to use a five-gallon bucket and a three-gallon bucket to measure out exactly four gallons of water [26] shows one immediately, for example, how to use a 100 mL test tube and a 60 mL test tube to measure exactly 80 mL of hydrochloric acid. In fact, probably every grade school  X  X  tory problem X  fits into a small number of templates; it is easy to generate new problems of a given type (see [28]), but fortunately for teachers, no reverse tool (automatic recognition and solution of homework exercises) is yet known to us. Unfortunately for res earchers, neither is there yet a system to refer one to  X  X he same research problem X  that may have been studied using different terminology in another field X  although a few projects have been aimed at this idea [18]. One final example where structure can matter more than word content is cooking. Websites with recipes can suggest other dishes that contain  X  X hicken X  or  X  X reen beans. X  But in the process of learning to cook, it often takes s eeing a few examples before we begin to recognize a general technique, e.g., that one can roast pork with peppers using the same steps and the same group of seasonings as for the chicken with green beans. Retrieving other recipes with the same structure would make it easier for novices to learn which aspects one can vary. In all these domains, we expect search to be difficult because the user cares less about the word content of a document X  X he information captured by standa rd retrieval models X  X nd more about the logical relationships among its entities. In this paper, we limit ourselves to studying jokes. Specifically, we focus on recognizing whether two documents ar e  X  X he same joke, X  or as we will term it, whether they  X  X atch X  or belong to the same  X  X oke cluster. X  This functionality would be a critical component of a joke search engine that incorpor ates meaning. In response to a query, a results page could list several clusters; for each cluster, it would display one joke and a link to  X  X ther versions of this. X  Creating that list of other versions is the task we address here. To outline what follows in the pa per: Sections 3 and 4 introduce our data and models, including se veral document models specific to jokes. In Section 5, we c onsider the task of pairwise classification: given two jokes, d ecide if they match. Next, in Section 6 we move to a ranking se tting, which is closer to our eventual goal: given one joke, retr ieve a ranked list of matches. Finally, in Section 6.3 we improve the ranking function by incorporating a classifier. We begin now by situating this project within the context of other research on humor and on domain-specific retrieval; other related wo rk is deferred to Section 8. There is a small but growing body of research in computational humor. This area typically encompasses two tasks: distinguishing humorous from non-humorous docum ents, and generating humor. Binsted et al. collects the work of several groups in this field [5]. Some recognition tasks include using text classification to distinguish humorous one-liners from other sentences like headlines or proverbs [14], or recognizing pun-like jokes [19]. techniques as relying too heavily on the specific words in a document; it proposes an architectur e for neural networks that would infer the implied context of a sentence and then recognize jokes by the incongruities they contain [25]. Though only a toy system was implemented, the aut hors do discuss joke retrieval as a domain where the query words may not be found in a document, and where, they argue, one must then include semantics in search. As for humor generation, there are systems that generate pun-based riddles or humorous acronyms [5]; such systems begin with a specific humorous template and automatically instantiate it. Other applications include inserti ng humor into emails or chatbots as a means to improve human-com puter interactions [5], [14]. Finally, there is a search engine for jokes called Jester, but it has been created and studied exclusively as a recommender system [7]. The models of humor that help computers recognize or construct it could be valuable to anyone studying jokes, but none of the above articles considers variations of a single joke, the central idea of this paper. Outside of computer science, humor research has a long history in fields like linguistics, psychology and philosophy. Mihalcea offers a quick survey of this diverse work [13]; for more information, see the International Society for Humor Studies (http://www.hnu.edu/ishs) and its journal (e.g., [2]). The most influential current theories are the Semantic Script-Based Theory of Humor [16] and its extension, the General Theory of Verbal Humor (GTVH) [2]. Both descri be humor as resulting from two contrasting interpretations of th e same text. GTVH specifies six properties of a humorous text, from lower-level aspects, like the style and the target (i.e., butt) of a joke, to higher-level aspects, like the contrasting themes and logical mechanism by which the humor unfolds. A recent book by Ritchie offers a strong critique of these and related theories as being poor in testable hypotheses, if rich in intuition [17]. Drawing on methodologies from artificial intelligence and generative linguistic s, Ritchie lays out properties a formal model of humor would need to satisfy, and as a first step towards that goal, he begins a bo ttom-up, descriptive analysis of certain types of texts, among them puns. In a chapter on joke identity and similarity, Ritchie discusses the  X  X ame joke X  idea, that certain variations preserve a joke's core identity [17]. This concept was earlier suggested by Hofstadter and Gabor, who described how a given  X  X r-joke X  or  X  X keleton X  can underlie many different joke s [8]. Their example  X  X r-jokes X  permit quite a wide variety of in stantiations, as long as the key joke entities and their logical relationships are present. Ritchie points out that one can view jokes either as consisting of a central core with variations, or as having degrees of similarity to a great range of jokes, across several axes of variation. The latter view fits in with GTVH, in which the six properties are said to form a strict order. Changing the joke's target, for example, would purportedly create a variation farther from the original than if one changed the joke's narrative style. In the present work, we use an ad hoc rule to describe the jokes we consider to be the same, but the situation is not clear-cut (see Section 3). This project is unusual for co mbining humor studies with information retrieval. In the c ourse of building good statistical models for jokes X  X n particular, models to identify sets of jokes having the same meaning X  X e may expect to find new insights about jokes themselves. In addition, for a computational area whose practical utility has been doubted [17], a jokes search engine may be a good motivating a pplication. A search engine that could retrieve joke clusters in response to queries containing possibly different words would enable users to effectively search through jokes, something not curre ntly possible. Such a system would demand joke recognition for its spidering step, as well as the identification of matching jokes, our current goal, for organizing the results of a query. A search engine for jokes is an example of a domain-specific retrieval system. Previous author s have discussed the advantages of specialized systems for performing complex, domain-specific queries on structured data gathered from the web X  X or instance, on collections of research papers , movie show times , or airplane flights [9], [12]. Certainly the domains mentioned in the introduction (quotations, puzzles, etc.) could all be easier to navigate if they had specialized search tools. In the articles above, the main challenges of building such systems consisted of efficiently spidering the web and recognizing informative documents (a task we sidestep by using existing collections), and of correctly extracting the important fields from the text. Our situation is different: we do not know what information to extract. As noted above, there is no working model of what matters for a joke's identity; there is not even a good intuitive model. So, we shall begin by using language mode ling, which after all, performs well for that very complicated text domain of natural language. We also build models that capture structures we expect to be important to a joke's identity: namely, the punch line, and the (abstractions of) entities that appear in the joke. The approaches and task definitions we use here may inform work in other domains where the word content can vary widely without affecting the meaning. In particular, the notion of abstracting the entities, along with any future techniques for incorporating semantics (to the extent this turns out to be necessary), will be applicable to such domains. In Section 8 we discuss other technical approaches that could potentially be brought to bear on joke retrieval and these related tasks. The corpus consists of approxi mately 11,000 jokes. These were downloaded from 13 joke archiv e sites on the web. It was important for the corpus to contain multiple versions of a number of jokes; to increase the odds of such repetitions, several specialized collections were included, such as music jokes and profession jokes, that seemed likely to include repeats. A large number of the documents contained humor outside the scope of the jokes we wanted to study. We manually removed items like one-liners (which included  X  X o mama X  jokes), quotes, funny but true stories, sarcas tic commentaries,  X  X op ten ways to ..., X  and lists. The remainder c onsists of things like narrative stories (like in Figure 1), light bulb jokes, and question/answer jokes (e.g.,  X  X : What do you call 5000 dead lawyers at the bottom of the ocean? A: A good start! X  or  X  X : What do you call a snail on a ship? A: A snailor! X ). Duplicate and near-duplicate documents (e.g., those that became identical after stemming and stopping) were also removed. Sixty clusters of jokes were la beled manually. This was done by creatively constructing queries to find matches for particular jokes. (For humans, this wa s not difficult, but recall was imperfect: in several cases, the retrieval systems found matches that the authors had missed.) Most jokes do not appear to have matches, but the corpus certainly contains more clusters than those we labeled. The clusters range in size from 2 to 13 jokes, and they include a total of 217 documents. Judging whether two jokes match can be subjective. As a rule of thumb, we labeled them as matc hing if one might easily say,  X  X  know that joke , except in my version [something varies]. X  However, there are many ambiguities. For instance, consider light bulb jokes. They might be characterized as a single cluster, if only there were not thousands of them:  X  X ow many [people of some type] does it take to change light bulb? [More than one], because [they have some particular propert y]. X  At the same time, a rewrite into a non-light bulb joke poking fun at the same property X  X  transformation that would otherw ise seem minor X  X ight be seen recognizable, fixed form. For thes e reasons, we avoided labeling light bulb jokes and other  X  X  ifficult X  jokes altogether. In the corpus as a whole, almost half the jokes are just two sentences long. Those jokes we labeled tended to be longer stories, averaging about 12 senten ces. This was probably a bias in labeling, and it could imply that the results on the short jokes will be those most representative of fu ture performance. However, it is also possible that the same bias  X  X erhaps, that longer jokes were more interesting to look for, and that shorter jokes, often word puns like the  X  X nailor, X  were harder to vary X  X ould affect the queries of future users. We use a language modeling appr oach. The document models and similarity measures described next are employed in both the classification and ranking tasks. As noted earlier, we use a standard statistical language model as a baseline [11]. Then, we implement variations that specially treat those structures we expect to be important to a joke X  X  identity. The baseline is a standard unigram (bag of words) model. With this, each document is initially represented as a multinomial probability distribution over its words. The probabilities are estimated using maximum likelihood. That is, if word w occurs tf w,d times in a document d having length L d , then in the document model M d , To avoid assigning any words probability zero, we use linear interpolation smoothing to combine the above value with the probability of the word in the general corpus: We determine  X  through a parameter sweep, performed separately for each model and task. In the ranking setting, we find the value  X  = 0.99 to be optimal for all m odels; for classification, we find = 0.4 to be near-op timal for all models. Throughout this paper, the query is also a document from our collection. However, we do not n eed to smooth the query model, so we just use the maximum likelihood value for a query q : The first alternative to the base line captures the intuition that the constant despite the rest changing. For this punch line model, we simply identify the last sentence and throw away everything before it. In shortening the docum ent, we are losing information; however, we speculate that the final sentence contains the "key concepts" for the joke, which will he lp target the search [3]. The same equations above are used, but every document in the corpus is truncated. This approach is motivated by the idea that if the characters, setting and other details can change in a joke, then perhaps we could recognize those changeable elements and replace them with abstractions. For instance, at an abstract level, the joke from the introduction might read like this:  X  X  person and a person were traveling in vehicles that collided. X  We create such a representation by recognizing cer tain words and  X  X nnotating X  them with their category. Using th is representation, our judgments of joke similarity might impr ove for two reasons. First, the annotated words will now match: among jokes in the same cluster, these words may correctly match where the original text did not. Second, the un-annotated words will be informative when examined separately from the annotated words. This set will include both generic words and unus ual words; we hope it will be distinctive within each joke cluster. The following text shows a joke from our corpus and its annotated version (after stopping and stemming):  X  X : What's the difference between a dead snake in the road and a  X  X iffer dead #animal[snake] #location[road] dead #person[lawyer] As one might imagine, when using these annotations (and ignoring the words inside the brackets), the above joke matches identically to another that begins:  X  X : What's the difference between a dead dog in the road and a dead politician ... X  To implement the annotations, ther e are two aspects to decide: (a) how to annotate the text, and (b) how to treat the annotated text. For the first question, we create word lists for ten categories (see Table 1) using the web as well as gazetteers included with the information extraction tool GATE (http://gate.ac.uk). During preprocessing, any document word that matches a list word is tagged (respecting some order of precedence for the lists). This is a coarse method and yields obvious markup errors, for example with homographs and irregular plurals, but such problems are present already in the bag of words model. It would also have been possible to create the word lists using WordNet. Such an approach would be easier to gene ralize to other domains and other categories. But the manually constructed lists are sufficient for a first pass; in addition, they are easy to modify, which lets us correct the more salient markup errors. Once the documents are annotated, there are a number of options for how to treat the new tokens. A model could be used that treats  X #animal[dog] X  as similar but not identical to  X #animal[snake]. X  This would be similar to a transla tion model, as we will discuss in Section 8. Instead, we choose to treat all  X #animal[] X  tokens as identical. A translation model giving different probabilities for each substitution would behave midway between treating the identical, so we place the annotations model at that second extreme. Formally, for a plain, un-annotated word under the annotations model, P(w|M d ) MLE is as before. For a word w annotated from word list A , the probability becomes Once the documents have been annotated and subdivided into punch line and non-punch line portions, it is easy to invent additional document models that use this same information differently. For instance, one can use only the punch line, but use the annotations model within it. Or rather than using the annotated tokens within the bag of words, one could simply delete them, in the spirit of treating them like stop words; after all, almost every joke probably contains a  X #person.  X  In the realm of possible but probably unhelpful models, one can treat a document as a bag of just two types of tokens: punch line and non-punch line words; or, annotated and non-annotated words. Or, to test the conjecture that only some annotation categories are useful, one can choose to use some types of labels but not others, for instance treating all  X #animal X  tokens as identical, but ignoring  X #location X  tags and reverting these to the original words. In our code base, we provide a flexible syntax for specifying document models along the above lines, and we create 108 such variations. The scores from these m odels are given as inputs to the machine learning classifier introduced in Section 5.3. To measure the similarity of a query to a document, we use the Kullback-Leibler (KL) divergence of the query and document models. This measur e is used to rank the documents during retrieval (Section 6) as well as to evaluate the similarity of two documents (Section 5). KL di vergence is a natural (though asymmetric) measure of the di stance between two probability distributions; it is zero when the distributions are equal and positive otherwise. When the query is held constant, as in the retrieval setting, KL divergence is rank-equivalent to cross entropy, H(p,q) , as shown here [10]: Often, the summation in the formula is taken over all words in the vocabulary. Since our query model is not smoothed, P(w|M thus the whole term) is zero for words outside the query. The function above allows different weights (probabilities) for the query terms, as well as for the document terms. It is necessary to use a function with this property since in our framework the query is always a full document, not just a few distinct words. When the query weights are all equal, cro ss entropy reduces to standard query likelihood. In the classification task, the sy stem is given two documents, and it must determine whether they are variations of the same joke. We set this up as for a machine learning task X  X reating separate training and test sets and us ing cross validation X  X ven though most models only  X  X earn X  a cutoff threshold. The training and test sets contain positive and negativ e examples, the positives being joke pairs that match, and the ne gatives being joke pairs that do not match. The samples are divided into ten groups to allow ten-fold cross validation. In order that the trai ning and testing barrier be kept intact, no joke cluster contributes examples to more than one group. We also avoid letting any one large cluster dominate the examples, sampling no more than 15 positives and 15 negatives from any cluster. For any cluster, the positive exampl es are drawn from all pairs of jokes in the cluster. The negative examples have one joke in the cluster and one outside it. If the j oke from outside the cluster were picked uniformly at random, the ta sk would be unfairly easy; the pair of jokes would not be at a ll similar. So instead, we sample negatives so that they will be comparable in their ranks to the positives. That is, for each positive pair, we use one joke as a query, retrieve a ranked list of jokes, and record the rank (in that list) of the second joke. By repea ting this with every joke as the query, we estimate a distribution of ranks of positives. Then, to generate negatives, we take one j oke from the cluster, retrieve a ranked list of jokes, sample a de sired rank from our distribution, and pick a non-matching joke from at or near that rank. In this way we create negative examples that are, in theory, difficult to distinguish from the positives. We described KL divergence above . However, when the example at hand is a pair of documents a and b , with neither taking the role of query, it is better to measure their similarity using a symmetric score. We make the score symmetric by taking the average of both directions, that is, using: similarity =  X (KL(M a || M b ) + KL(M b || M a )) . It would have been possible to use the symmetric cross entropy rankings, we choose KL divergence because it has a minimum of zero. For cross entropy, the minimum score (occurring for perfectly matching documents) is the entropy of the query, which varies by query. In total, we have approximately 600 data points, of which 58% are negatives. During the training phase, the classifier computes the similarity score for each pair, then it chooses a decision threshold to maximize its accuracy X  X he number of correct predictions X  X n the training data. We evaluate the accuracy for each fold of the test data and th en compute an average across the folds. Table 2 shows the accuracies achieved by the three main document models described above. The first things to observe are that the accuracies are fairly high, and that the models that use joke structures have some advantage over the baseline. Also, there is diversity am ong the models; Table 3 shows how each model has some examples that only it predicts correctly. We further see that the models are erring on the side of caution by not recognizing positives when they appear. 
Document model Punch line 56 0.90 0.66 To take advantage of the dive rsity among the models, we try combining them using machine learning. We use the similarity scores from the models as inputs to a classifier and allow the classifier to make the predic tion. We use Weka X  X  logistic regression tool [20]; its other cl assifiers performed similarly or worse. We test several combina tions of features, beginning with the scores from the three models we have seen above. Next, hypothesizing that relative documen t lengths may be predictive, we add two more features: the ratio and average of the document lengths. Finally, we use as our features the scores from all 108 model variations describe d in Section 4.1.4. The results of the classifiers are shown in Table 4. We see that the classifier that uses the set of three features (top line) achieves better performance than any indi vidual model. Adding additional features does not help; if anything, it was useful to manually select the set of three features. Features Number of features Accurac y Baseline, annotations, punch line 3 0.818 
Above, plus ratio and average of document lengths 5 0.802 We assess significance using pair ed t-tests on the sets of individual predictions. At the p = 0.02 level, annotations beats baseline, and the best classifier beats annotations; however, for the punch line versus annotations and for the classifier versus punch line, they just miss significance, yielding p-values around 0.06. It is surprising that the punch line model performs so well here; in light of the poor scores we will see for it in Section 6.2, it is also somewhat misleading. Further anal ysis suggests that this model's high accuracy in classification is an artifact of the sampling procedure: by intent, we chose negative examples whose scores under the baseline model closely matched the scores of the positive examples. As a result, the baseline model has difficulty distinguishing the classes. The annotations model has a similar property. However, the punch line model tends to give different scores than the other two; thus its positive and negative examples were not pushed together by the choice of samples, and it could outperform the other mode ls in this setting. We next consider this  X  X ame j okes X  task in a ranking setting. Ranking is a more appropriate setti ng for evaluating the task if we anticipate using the system to retrieve  X  X ore jokes like this. X  In this setting, we use one joke as a query, and we use one of the document models described earlier to rank all the documents in the collection. The relevant docum ents for this query are defined as those jokes in the same cluster. We measure average precision, recall at various cutoffs, and R-pr ecision. We repeat this process for every joke in the cluster, and calculate the average of the measures for the cluster. After doi ng this for every cluster, finally we report the averages across all 60 clusters. The results of the ranking experime nts are displayed in Table 5. We see that the order of performance is reversed from the classification setting; here, the baseline model performs best and the punch line model worst. This holds across all four measures. The differences between the baselin e and annotations models are, however, not significant. One way to compare the performance of the models is with a scatterplot of their scores, as in Figure 4. The plots show how closely the annotations and baselin e models track each other, as their scores lie near the diagonal (Pearson correlation = 0.84). They also show how the baseline model almost always gives better results than the punch line model. However, we can also see that for each alternative model, there are some clusters in which it soundly beats the baseline. This diversity suggests that again there is potential for improvement by combining the scores of the three models. 
Figure 4. Mean average precision of each joke cluster (one data point per cluster). Diagon al shown for reference. Top, baseline model versus annotations. Bottom, baseline versus To combine the models, we return to the approach from above: training a pairwise classifier us ing scores from the three models. The Weka classifier outputs a probability score, not just a binary decision, so we can use this score for ranking. In order to use a pairwise classifier in the ranking setting, where the query is fixed, we have two immediate possibilitie s. First, we could pair the query with every other document in the collection, one by one, and use the classifier X  X  scores to rank all the documents. Or, we could take some set of top doc uments from the baseline model and use the classifier to re-rank them. We take the latter approach, for efficiency reasons, and also to exploit the fact that the baseline classifier already has high recall. To choose the number of documen ts to re-rank, we plot in Figure 5 the recall curve as a function of the number of documents. The curve levels o ff by 500 documents, at recall = 0.998. Figure 5. Recall of the baseline model, averaged over all jokes. In order to train a classifier to re-rank the top 500 documents, we must create a new training set reflecting the distribution where the model will now be applied. For the positives, we use all 442 pairs of jokes in all clusters, since we need all the positive examples we can get. To generate the negatives, we run the baseline ranking, identify the top 500 documents, a nd sample randomly from them. We use a ratio of about 1:2 for positives to negatives, which keeps the size of the training set reasona bly small. (We do not expect it to be important to keep constant the ratio of positives to negatives from training to test sets since we are using the model X  X  output for ranking, as opposed to for classification.) To create training and test splits, we divide the data into 10 groups of clusters for cross validation. For each cluster, the training data are the positives a nd negatives from the queries in the other 9 groups. Table 6 shows the results of using the classifier to re-rank the top 500 documents. (The baseline model, when restricted to its top 500 hits, gives the same scores as in Table 5.) This classifier, when used by itself, performs worse than the baseline. Once more, we examine the scatterplot of scores (Figure 6, top). This time we see that while the classifier does not perform as well as the baseline overall, it is a toss-up as to which works better for any particular cluster. This means that yet once again, we stand to benefit by combining these methods. Since the machine learning classifier has already been given the baseline score as a feature, we create this final combination by simply linearly interpolating between the output score of the classifier and the baseline score, giving them equal weight. This resulting ranking turns out to be significantly better than any of the others. The bottom of Figure 6 shows how, with the interpolated classifier, the mean average precision of almost every joke cluster improves compared to the baseline. Table 6. Ranking performance using classifier to re-rank. Document model MAP R-precision Recal l at 10 
Baseline top 500 re-ranked with classifier 
Baseline top 500 re-ranked with (0.5 classifier + 0.5 baseline) 
Figure 6. Mean average precision of each joke cluster (one data point per cluster). Top, baseline model versus classifier. We performed a few experiments analyzing the contribution of the classifier, and in particular , testing whether the improvement in score could be achieved in so me simpler way. The results of these experiments are shown in Table 7. One method for improving retrieval in many situations is to expand the query using pseudo-relevance feedback. We created such an expanded query using linear interpolation between the original query and the top t documents [23]. We used t = 2, and weighted the original query and the new terms 0.4 and 0.6, respectively. Its performance is virtually identical to the baseline. Next, we investigated whether th e boost from the classifier could be due to it using the symmetric version of KL divergence. For this run, we use the baseline m odel but use the symmetric version of the score. This by itself is clearly not helpful either. Document model MAP R-precision Recal l at 10 
Baseline with pseudo-relevance feedback 
Baseline using From these experiments we have learned that the annotations model performs fairly closely to the baseline bag of words model, while the punch line carries differing information. In the classification setting, the task is difficult for the baseline by design, so the punch line model scores well through its contrast. In the ranking task, where the comparison is more fair, the baseline prevails over the other two models. In both settings, we achieve the best results by comb ining the three document models. We gain some insight into the utility of the three models by looking at specific queries where they performed differently. Our intuition was that since words from the query would not necessarily appear in the relevant documents, the baseline model would have low recall. For the most part, it seems that if a joke is sufficiently long, certain words actually do appear in all its versions. In the challenging-looki ng joke cluster from Figures 1 X  3, for example, the baseline m odel gives a reasonable MAP for ranking of 0.62; the annotations model scores mildly higher. When a joke is short, the baseline model may still perform well provided there are distinctive words that appear in every version. For instance, the unusual words  X  X  rampoline X  and  X  X ire gauge X  in the joke versions in Figure 7 allow the baseline model to retrieve these clusters perfectly. There is a mild indication that joke length correlates with the success of the annotations model. In particular, for the cases where the annotations model works better than the baseline, the joke is either short (under 50 words) or long (over 120). For jokes of medium length, either the two models give comparable scores, or the baseline model wins. We can explain the success of the annotations model at short jokes by referring back to the example from Section 4.1.3 invol ving  X  X kid marks; X  in cases such as this, there are not always enough words preserved for the baseline model to use. For instance, in the example in Figure 8, the annotations model scored perf ectly. The baseline model had a MAP of 0.5; it found the correct documents by rank 2, but retrieved other tiger and polar bear jokes (respectively) as its top matches. As for punch lines, when the punc h lines match closely, this seems to be a sufficient condition for the jokes to match. However, this only happe ns for some jokes. Overall, it seems as though every j oke has some invariant phrases. However, it is difficult to describe, without actually looking at the joke, which phrases those might be. This is why using a combination of methods makes sense: each deals well with certain types of jokes. In terms of other possible methods for recognizing joke variants, we considered viewing variants as if they were translations into other languages and then learning a translation model of common word substitutions [6]. This is si milar to Berger and Lafferty X  X  use of translation models between (English-language) queries and documents, designed to help c onnect words having the same meaning or topic [4]. However, those models require a large amount of training data (matched documents), whereas our set of labeled documents, on the c ontrary, is quite small. The idea that most joke clusters ha ve particular invariant words or phrases relates to the idea of  X  X  ey X  or  X  X ore X  concepts, introduced by Allan et al. [1] and recently further developed for use with verbose queries [3]. Even absent any intuitions about jokes, this  X  X ey concept X  idea would seem relevant because our queries are long X  X ntire jokes; Bendersky and Croft argue that extraneous concepts tend to hinder retrieval performance [3]. It is not clear that concepts which are key in standard text X  X .g., proper nouns X  X ould play the same role in jokes, nor that we would have enough data to learn to identify the important terms. However, it would be interesting to try modifying these techniques for jokes. One possible approach for handling queries whose terms may not appear in the relevant documen ts comes from work on  X  X ague queries. X  This was introduced by Motro for the database community [15], but it could be seen as a type of query expansion. The idea is that if a query returns no matches, it can be broadened by examining the clos est matches in the corpus. For structured databases, refini ng the query requires having an appropriate similarity measure for each type of field X  X or instance, geographic proximity for cities but temporal distance for times. Zhu et al. pose an anal ogous problem in information retrieval [24]. They describe the challenge of searching for  X  X hat book about the guitar-playing sergeant, X  when the desired title is actually  X (Captain) Corelli's Mandolin. X  Since the data type in this case is words, the work uses a similarity measure defined over WordNet to suggest candidate modifications of the query terms. Among the many possible expansions or substitutions for the query, modified queries are judged good (as opposed to vague) if their terms frequently appear close together in the corpus at large. In an earlier in itiative, Woods et al. address this same situation, dubbing it the  X  X araphrase problem X  [21], [22]. Their approach involves building a  X  X onceptual index, X  a large semantic taxonomy describing re lationships among words and phrases. Given a query, the syst em searches among candidate modifications and generalizations of the query terms. The quality of a new query is judged both by the proximity of the terms within the retrieved documents and also by the similarity of the new query to the original. These approaches could be promis ing for querying for jokes, but we see a few drawbacks. First, th ey would be useful for retrieving some matches to a joke, but since they choose combinations of new query terms that are popular in the corpus, their recall could be low. Second, even for short queries, there might be an intractably large search space of plausible substitutions in the jokes domain. To state this more plainly, jokes are not just paraphrases. Paraphrasing might in fact describe our difficulties in searching for song lyrics and quotations. But for jokes and puzzles, it will not be enough to consider synonyms and related terms; entities can shift broadly in different versions, and large swaths of details can be modified or dropped. It is an open question whether, and to what extent, semantic processing needs to be added to statistical models of language [22]. For identifying  X  X he same X  joke, intuition suggests that we would need, at a minimum, information extraction for all the entities, events, and logical relations (each possibly implicit) in a joke X  X apabilities far beyond today's reach. Yet in many cases in our corpus, it seems to be raw words that matter, essential phrases like  X  X kid marks. X  Perhaps such words are informative because the corpus is of limited size, because distinctive phrases tend to be preserved in transmission, or because these phrases in themselves define the identity of the joke. Regardless, jokes are yet another domain where the bag of words m odel performs surprisingly well. Even when they are combined with our other models, however, there is much room left for improvement. We have used knowledge of a particular domain to build a retrieval system that performs be tter at ranking and classification than the standard model does in this domain. Along the way, we have used the domain, humor, to argue for alternative definitions of similarity between documents: that they exist and that they matter. In particular, documents in some domains are difficult to search for at present because one cannot be certain of any words the item will contain; only their relationships count. For a person learning a foreign la nguage, the standard advice goes that they will have mastered it only when they can tell jokes in the language. For computers proce ssing human language, perhaps humor will serve as that same challenge and yardstick. Our thanks to Mario Di Marzo for collaborating on an early version of this project. David Jensen provided support for this work, and also suggested the  X  X  ame research problem X  analogy. Thanks also to Mark Smucker fo r help indexing the corpus and Cindy Loiselle for her careful ed iting suggestions. This work was supported in part by the Center for Intelligent Information Retrieval. [1] Allan, J., Callan, J., Croft, W. B., Ballesteros, L., Broglio, J., [2] Attardo, S. and Raskin, V. 1991. Script theory revis(it)ed: Joke [3] Bendersky, M. and Croft, W. B. 2008. Discovering key [4] Berger, A. and Lafferty, J. 1999. Information retrieval as [5] Binsted, K., Bergen, B., Coulson, S., Nijholt, A., Stock, O., [6] Brown, P. F., Cocke, J., Della Pietra, S., Della Pietra, V. J., [7] Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. 2001. [8] Hofstadter, D. and Gabor, L. 1989. Synopsis of the workshop [9] Kruger, A., Giles, C. L., Coetzee, F. M., Glover, E., Flake, G. [10] Lafferty, J. and Zhai, C. 2001. Document language models, [11] Manning, C. D., Raghavan, P., and Sch X tze, H. 2008. [12] McCallum, A. K., Nigam, K., Rennie, J., and Seymore, K. [13] Mihalcea, R. 2007. Multidisciplinary facets of research on [14] Mihalcea, R. and Strapparava , C. 2006. Technologies that [15] Motro, A. 1988. VAGUE: A user interface to relational [16] Raskin, V. 1985. Semantic Mechanisms of Humor . Studies in [17] Ritchie, G. 2003. The Linguistic Analysis of Jokes . Routledge [18] Schatz, B. R. 2002. The Interspace: Concept navigation across [19] Taylor, J. M. and Mazlack, L. J. 2007. Multiple component [20] Witten, I. H. and Frank, E. 2005. Data Mining: Practical [21] Woods, W. A. 1997. Conceptual indexing: A better way to [22] Woods, W. A., Bookman , L. A., Houston, A., Kuhns, R. J., [23] Zhai, C. and Lafferty, J. 2001. Model-based feedback in the [24] Zhu, J., Eisenstadt, M., S ong, D., and Denham, C. 2006. [25] Zrehen, S. and Arbib, M. A. 1998. Understanding jokes: A [26]  X  X ogic Problems  X  easy, X  [27]  X  X he Aristocrats (2005), X  The Internet Movie Database, [28]  X  X rain Teasers and Math Puzzles, X  Syvum Technologies, 
