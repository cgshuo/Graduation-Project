 Statistical Machine Translation(SMT) i s curren t-ly the state of the art solution to the machine transl a tion. Phrase based SMT is also among the top pe r forming approaches available as of today. This approach is a purely lexical approach, using su r face forms of the words in the parallel corpus to generate the translations and estimate prob a-bilities. It is possible to incorporate syntactical information into this framework through diffe r-ent ways. Source side syntax based re -ordering as preprocessing step, dependency based reorde r-ing models, coh e siv e decoding features are among many available successful attempts for the integration of syntax into the translation model. Factored translation modeling is another way to achieve this goal. These models allow each word to be represented as a vector of fact ors rather than a single surface form. Factors can represent richer expression power on each word. Any factors such as word stems, gender, part of speech, tense, etc. can be easily used in this framework.

Previous work in factored translation modeling have reported consistent improvements from Part of Speech(POS) tags, morphology, gender, and case factors (Koehn et. a. 2007). In another work, Birch et. al. 2007 have achieved improvement u s ing Combinational Categorial Grammar (CCG) super -tag factors. Cr eating the factors is done as a preprocessing step, and so far, most of the e x-per i ments have assumed existence of external tools for the creation of these factors (i. e. Part of speech taggers, CCG parsers, etc.). Unfortunately high quality language proces sing tools, especia l-ly for the open domain, are not available for most la n guages.

While linguistically identifiable representations (i.e. POS tags, CCG supertags, etc) have been very frequently used as factors in many applic a-tions including MT, simpler representations have also been effective in achieving the same result in other application areas. Grzymala -Busse and Old 1997, DINCER et.al. 2008, were able to use fixed length suffixes as features for training a POS tagging. In another work Saberi and Per rot 1999 showed that reversing middle chunks of the words while kee p ing the first and last part intact, does not decrease listeners X  recognition ability. This result is very relevant to Machine Transl a-tion, suggesting that inaccurate context which is usual ly modeled with n -gram language models, can still be as effective as accurate surface forms. Another research (Ra w linson 1997) confirms this finding; this time in te x tual domain, observing that randomization of letters in the middle of words has little or no effect on the ability of skilled readers to understand the text. These r e-sults suggest that the inexpensive representatio n-al factors which do not need unavai l able tools might also be worth investigating.

These results encouraged us to introduce la n-gu age independent simple factors for machine translation. In this paper, following the work of Grzymala -Busse et. al. we used fixed length su f-fix as word factor, to lower the perplexity of the la n guage model, and have the factors roughly function as part of speech tags, thus increasing the gra m maticality of the translation results. We were able to obtain consistent, significant i m-provements over our baseline in 3 different exp e-riments, large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 
The rest of this paper is as follows. Section 2 briefly reviews the Factored Translation Models. In section 3 we will introduce our model, and se c tion 4 will contain the experiments and the analysis of the r esults, and finally, we will co n-clude this paper in section 5. Statistical Machine Translation uses the log l i-near combination of a number of features, to compute the highest probable hypothesis as the translation. e = argmax e p( e | f ) = argmax e p exp  X  i=1 n  X  i h i ( e , f )
In phrase based SMT, assuming the source and target phrase segmentation as {(f i ,e i )}, the most important features include: the Language Model feature h lm ( e , f ) = p lm ( e ) ; the phrase translation fe a ture h t ( e , f ) defined as product of translation prob a bilities, lexical probabilities and phrase p e-nalty; and the reordering probability, h d ( e , f ), source phrase reordering events.

Factored Translation Model, recently intr o-duced by (Koehn et. al. 2007), allow words to have a ve c tor representation. The model can then extend the definition of each of the features from a uni -dimensional value to an arbitrary joint and cond i tional combination of features. Phrase based SMT i s in fact a special case of Factored SMT. 
The factored features are defined as an exte n-sion of phrase translation features. The function  X (f j ,e j ), which was defined for a phrase pair b e-fore, can now be extended as a log linear comb i-generation feature, defining the relationship b e-tween final su r face form and target factors. Other features include additional language model fe a-tures over individual factors, and factored reo r-dering features.

Figure 1 shows an example of a possible fa c-tored model.

In this particular model, words on both source and target side are represented as a vector of four factors: surface form, lemma, part of speech (POS) and the morphology. The target phrase is generated as follows: Source word lemma gen e-rates target word lemma. Source word's Part of speech and morphology together generate the target word's part of speech and morphology, and from its le m ma, part of speech and morphology the surface form of the target word is finally ge n-erated. This model has been able to result in higher translation BLEU score as well as gra m-matical coherency for English to German, En g-lish to Spanish, English to Czech, Engli sh to Chinese, Chinese to English and German to En g-lish .
Part of speech tagging, constituent and depe n-dency parsing, combinatory categorical grammar super tagging are used extensively in most ap pl i-c a tions when syntactic representations are needed. However training these tools require medium size treebanks and tagged data, which for most la n guages will not be available for a while. On the other hand, many simple words features, such as their chara cter n -grams, have in fact proven to be comparably as effective in many applications. (Keikha et. al. 2008) did an experiment on text classification on noisy data, and compared seve r-al word representations. They compared surface form, stemmed words, chara cter n -grams, and s e mantic relationships, and found that for noisy and open domain text, character -ngrams outpe r-form other representations when used for text classific a tion. In another work (Dincer et al 2009) showed that using fixed length word en d-ing out performs whole word representation for training a part of speech tagger for Turkish la n-guage. 
Based on this result, we proposed a suffix fa c-tored model for translation, which is shown in Fi g ure 2. 
Based on this model, the final probability of the translation hypothesis will be the log linear comb i nation of phrase probabilities, reordering model probabilities, and each of the language models X  probabilities. 
Where p lm -word is the n -gram language model probability over the word surface sequence, with the language model built from the surface forms. probability over suffix sequences. p(e word -j &amp; e prob a bilities for each phrase pair i , used in by the d e coder. This probability is estimated after the phrase extraction step which is based on grow -diag he u ri s tic at this stage. We used Moses implementation of the factored model for training the feature weights, and SRI toolkit for building n -gram language models. The baseline for all systems included the moses sy s-tem with lexicalized re -ordering, SRI 5 -gram language models. 4 . 1 Small System from Dialog Domain: This system was TRANSTAC system, which was built on about 650K sentence pairs with the ave r age sentence length of 5.9 words. After choosing length 3 for suffixes, we built a new parallel co r pus, and SRI 5 -gram language models for each factor. Vocabulary size for the surface form was 110K whereas the word suffixes had about 8K di s tinct words. Table 1 shows the result (BLEU Score) of the system compared to the baseline.

As you can see, this improvement is consistent over multiple unseen datasets. Arabic cases and numbers show up as the word suffix. Also, verb numbers usually appear partly as word suffix and in some cases as word prefix. Defining a la n-guage model over the word endings increases the prob a bility of sequences which have this case and nu m ber agreement, favoring correct agre e-ments over the incorrect ones. 4 . 2 Medium System on Travel Domain: This system is the WMT08 system, on a corpus of 1.2 million sentence pairs with average se n-tence length 27.9 words. Like the previous exp e-riment, we defined the 3 character suffix of the words as the second factor, and built the la n-guage model and reordering model on the joint event of (su r face, suf fix) pairs. We built 5 -gram language mo d els for each factor. The system had about 97K distinct vocabulary in the surface la n-guage model, which was reduced to 8K using the suffix corpus. Having defined the baseline, the system results are as follows. 
Here, we see improvement with the suffix fa c-tors compared to the baseline system. Word en d-ings in English language are major indicators of word X  X  part of speech in the sentence. In fact most common stemming algorithm, Porter X  X  Stemmer, works by removing word X  X  suffix. Having a la n guage model on these suffixes pus h-es the common patterns of these suffixes to the top, making the more grammatically coherent sentences to achieve a better probability. 4 . 3 Large NIST 2009 System: Arabic to We used NIST2009 system as our baseline in this experiment. The corpus had abo ut 3.8 Mi l-lion se n tence pairs, with average sentence length of 33.4 words. The baseline defined the lexic a-lized reo r dering model. As before we defined 3 character long word endings, and built 5 -gram SRI language models for each factor. The result of this e xper i ment is shown in table 3. 
This result confirms the positive effect of the suffix factors even on large systems. As me n-tioned before we believe that this result is due to the abil i ty of the suffix to reduce the word into a very si m ple but rough grammatical represent a-tion. Defining language models for this factor forces the decoder to prefer sentences with more probable suffix s e quences, which is believed to increase the gra m maticality of the result. Future error analysis wil l show us more insight of the exact effect of this factor on the outcome. In this paper we introduced a simple yet very e f fe c tive factor: fixed length word suffix, to use in Factored Translation Models. This simple fa c-tor has been shown to be effective as a rough replac e ment for part of speech. We tested our factors in three experiments in a small, English to Iraqi sy s tem, a medium sized system of Spanish to English, and a large system, NIST09 Arabic to English. We observed consistent and signi ficant improvements over the baseline. This result, o b-tained from the language independent and ine x-pensive factor, shows promising new opportun i ties for all la n guage pairs. 
