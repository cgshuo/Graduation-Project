 The empirical study of machine learning and data mining methods often falls prey to the effects of publication bias that favors positive results over negative ones. Most, if not all, articles in conferences and journals report only positive results. This does not reflect the practice of a field where failures happen regularly. As in real life, we often learn more from negative results than we do from positive ones. It is time that we, as a community, start to regard failures as being as informative as successes. After all, we do know the difficulty of learning from positive only experiences; so how can we expect to learn about our field if all we ever see are successes? This special issue provides a forum for papers that describe clear, and somehow surprising, failures that stand in need of an explanation. We define as clear, or interesting, fail-ures that happen in situations where the learning or mining method is not only sub-optimal, but performs far worse than expected. To make the special issue of value to the largest possible audience, we sought papers that report failures of learning and mining strategies that are already popular and well-known in the community, or of novel ideas that are easy to comprehend and do not require extensive prior knowledge in a special niche area of machine learning or data mining. The main purpose of this special issue therefore is to bring together a sample of exemplary failures, with the goals of: 1. making these experiences accessible to fellow researchers 2. documenting the first few negative data points neces-We are aware of only one other previous collection of  X  X n-expected results X  articles devoted to data mining. A special issue of Machine Learning (Vol. 57, 2004) on Data Mining Lessons Learned  X  X nitiated following an ICML-2002 work-shop of the same name X  started with a premise similar to ours and included 6 contributed papers. These were com-plete papers whose focus was generally:  X  X ere is the chal-lenge we faced and here is how we overcame it. X  In this special issue, we wish to consider more of the situations of:  X  X ere is what we thought would happen and here is what ac-tually happened. X  The contributed papers are intentionally short, speaking directly to the negative and/or unexpected nature of the reported results. The importance of negative results has been recognized in several scientific disciplines, as evidenced by well-exploited peer-reviewed publications dedicated to such results. We list a few here as illustration. Interestingly, machine learning and data mining too have a journal. This may come as a surprise to some readers. It is called the Journal of Interesting Negative Results , and its explicit aim is to be  X  X  resource that gives a voice to nega-tive results which stem from intuitive and justifiable ideas, proven wrong through thorough and well-conducted experi-ments...[as well as] short papers/communications presenting counter-examples to usually accepted conjectures or to pub-lished papers. X  4 The journal started in April 2008, but since that time has published only one article! If other disciplines recognize the importance of negative re-sults to advancing knowledge about their disciplines why don X  X  we (Computer Scientists)? While we can X  X  answer this conclusively, we think some of it may have to do with http://www.jnrbm.com/ http://www.arjournals.com/ojs/ http://www.jnr-eeb.org/ http://jinr.site.uottawa.ca/ the fact that Computer Science grew out of mathematics as opposed to a more clinical or laboratory based science. However, over the past twenty years Computer Science has moved into a very much more applied discipline and perhaps the Data Mining sub-discipline is leading this evolution. In fact, since most of the problems we deal with are interdis-ciplinary, and require the design and evaluation of experi-ments perhaps we, as data miners, have much more to learn from negative results. After all, we are not mathematicians any more! The scientific method itself allows for failure  X  X e need to accept that negative results are part of our everyday profes-sional life. The metalearning that occurs through evaluation and reflection should be a valuable and important part of our work. The papers published in this special issue highlight unex-pected results found in data mining experiments. These re-sults can be summarized into lessons learned about the data mining technique, handling of data, and the importance of careful design of experiments. Atreya and Elkan report that Latent Semantic Indexing (LSI), a popular method for text analysis, performs very poorly on several of the benchmark TREC document col-lections. Despite trying several versions, no version of LSI achieves a worthwhile improvement in retrieval accuracy over BM25, the best currently known vector-based scor-ing method. It is hypothesized that the reason may be the large number of dimensions in the problem. However, ex-periments conducted do not validate this hypothesis. The authors have not yet been able to conclusively determine the reason for this poor performance of LSI.
 Perlich and  X  Swirszcz report on some surprising results when applying cross validation to derive conclusions about data especially when the (positive) signal is weak. The authors suggest that the technique generally produces an inverse sig-nal, which in turn, yields extremely low AUC (Area Under the Curve) values. The authors show that the problem par-ticularly affects popular ensemble methods, such as bagging, which are commonly regarded as very robust.
 Shi and Yu explore the limitations of trace norm minimiza-tion, particularly as a way of replacing missing values in a matrix (e.g., as is necessary for collaborative filtering). They point out that the main assumption of this approach, namely that the original matrix is of low rank, cannot be verified in practice. In addition, it may produce multiple solutions each of the same low rank. The authors conclude that trace norm minimization thus only works under certain very constrained situations. F  X urnkranz and Sima report on unexpected results of ex-periments with multilabel data mining where input data is augmented with information about the hierarchical re-lationships among the objects. The authors show that for binary class hierarchies this does the same as the well-known Pachinko machine. However it trains many redundant and therefore useless classifiers. It also performs worse than the normal not augmented pairwise classification. The problem seems to lie in the fact that the evaluation domain does not satisfy the authors X  so-called class fidelity assumption, i.e. assuming that instances are closer if their classes are closer in the label-hierarchy.
 Weninger et al investigate problems associated with auto-matically extracting lists from the Web. Contrary to their expectations, it seems that an extremely na  X  X ve approach out-performs existing more sophisticated techniques. Using the structure of the Web page and HTML clues is not sufficient to perform this task. Kohavi and Longbotham share several unexpected and erro-neous results found from experience in performing many dif-ferent online randomized experiments. They point out many issues to be careful about when conducting these types of controlled experiments. Although many of the results, such as the impact of caching and redirects, are obvious once ex-plained, they are not always the sorts of things that would be thought of when designing the experiments.
 In their reflective article, Attenberg and Provost discuss a number of challenging issues or questions that must be ad-dressed in using active learning in practice. While active learning promises to reduce the cost of acquiring labeled data, most research in the area overlooks some important is-sues that make it impractical, such as, how to choose a tech-nique, how to choose a base learner, how to deal with skewed distributions and disjunctive classes, and how to  X  X tart X  the process. The three major themes of the included papers: techniques, data, and experiments indeed highlight the major compo-nents of our field. It is interesting to note that one of the ma-jor issues evolving in Computer Science education is that of effective data analysis. We also find it interesting that there are very few  X  X esign and analysis of experiments X  classes in undergraduate Computer Science curricula. We feel strongly that our sub-discipline needs the reflective input provided by evaluating the failures of our own work. We strongly rec-ommend that, in addition to the more formal Journal of Interesting Negative Results , SIGKDD Explorations and/or our flagship conferences (e.g., KDD , ICML , etc.) have a reg-ular feature on carefully documented negative experimental results.
 The publication of this special issue would not have been possible without the enthusiastic responses of our invited colleagues, Charles Elkan, Jiawei Han, Ronny Kohavi, Fos-ter Provost, and Philip Yu, as well as the voluntary submis-sions of others, from which two papers were selected. We are also grateful to Chris Drummond, Johannes F  X urnkranz, Robbie Haertel, Michael Hahsler, J  X org-Uwe Kietz, Mallik Kotamarti, Gregory Piatetsky-Shapiro, Carlos Soares and Ricardo Vilalta, for helping us with the review of the pa-pers.

