 Numerous studies have examined the ability of query perfor-mance prediction methods to estimate a query X  X  quality for system effectiveness measures (such as average precision). However, little work has explored the relationship between these methods and user ratings of query quality. In this poster, we report the findings from an empirical study con-ducted on the TREC ClueWeb09 corpus, where we com-pared and contrasted user ratings of query quality against a range of query performance prediction methods. Given a set of queries, it is shown that user ratings of query quality cor-relate to both system effectiveness measures and a number of pre-retrieval predictors.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms : Human Factors, Performance Keywords: Query Performance Prediction
Estimating the quality (or difficulty) of a query is an important task [3, 4, 8, 9], which can aid in the develop-ment of adaptive Information Retrieval (IR) systems. For instance, if an IR system can determine which queries will perform poorly, then actions can be taken to ameliorate per-formance. The evaluation of Query Performance Prediction (QPP) methods has usually been performed in the context of system effectiveness (e.g., average precision); when a query results in low effectiveness, the query is considered poor or difficult, and conversely when the effectiveness is high, the query is considered good or easy. However, if we wish to develop better adaptive IR systems, it is also important to consider query quality from the user X  X  perspective and to de-termine whether the user thinks a query is hard or easy. This could be very useful when suggesting or ranking queries, by enabling the system to appropriately respond to the user X  X  perception. And since QPP methods are generally based on  X  X ules of thumb X  about how a user might rate a query X  X  performance, it is also interesting to examine whether QPP methods actually reflect the intuitions of human assessors. In this poster, we investigate whether users judge the quality of queries in accord with QPP methods, and if such methods can be used as a proxy for user query ratings.

Related to the study conducted in this poster are two lines of research that have been investigated: (1) user ratings vs. performance and (2) inferred ratings vs. performance. Of the first line, in an experiment in the late 1990 X  X  [6], a number of IR researchers were asked to classify TREC top-ics as either easy , medium or hard for a newswire corpus they were familiar with. The researchers were given the TREC topic statements, though not the search results. It was found that they were largely unable to predict the top-ics X  quality correctly and, surprisingly, they could also not agree among themselves on how to classify the topics. Of the second line, in [5, 7] initial experiments were performed that compared a user based measure (the median time to find a relevant document) with Clarity Score [3] and a range of lation was found for Clarity Score , while in [7], the best pre-retrieval predictor achieved a Kendall X  X  Tau rank correlation of  X  = 0 . 2. However, these experiments were conducted in limited contexts, i.e. IR researchers on a small data set [6] or using time as an implicit user rating of query quality [5, 7]. Here, we conduct an investigation on a large test web collection, with users who regularly use search engines, and compare their explicit ratings of query quality against sys-tem predictions from a range of pre and post-retrieval QPP methods.
Following on from the previous experiments [5, 6, 7], we performed a similar study but with eighteen post graduate computer science students as assessors using the most recent TREC test corpus: ClueWeb09 (cat. B) [2], a 50 million document crawl of the Web from 2009. We utilized the fifty consist of a query part (to be submitted to the IR system) and a description (the information need). In this study, we provided the assessors with the queries and descriptions and instructed them to judge on a scale from 1 (poor quality / hard) to 5 (high quality / easy), what they expect the search result quality to be, if the queries would be submitted to a Web search engine. Note, that the queries were not actually submitted to a search engine.
 On the system side, we indexed the corpus with the Lemur plied. For retrieval, we used a Language Model with Dirich-let Smoothing (  X  = 1000). The retrieval effectiveness was measured by estimated average precision (AP) and estimated precision at 30 documents (P@30), two new TREC mea-sures [1]. For system effectiveness predictions we used three pre-retrieval QPP methods: Max. Inverse Document Fre-quency ( MaxIDF ), Summed Term Weight Variability ( Sum-VAR ) [8] and Summed Collection Query Similarity ( Sum-SCQ ) [8], as well as three post-retrieval methods: Clarity Score [3], Query Feedback [9] and Query Commitment [4]. Note that the pre-retrieval predictors are parameter-free, while the post-retrieval predictors were evaluated over a Assessor Ratings : To investigate how well ratings of query quality matched system performance, we examined the cor-that on average across all assessors, the rank correlation was  X  = 0 . 33  X  , while the worst/best correlation between ratings and AP was  X  = 0 . 20 and  X  = 0 . 48  X  , respectively 6 . To ex-amine this more deeply, we split the set of queries into five equal partitions given the system measures (ordered from high to low). We then averaged all assessor ratings for the queries within each partition. Table 1 shows that the as-sessors tended to rate the better performing queries higher than the poorly performing queries for both AP and P@30. This indicates that on average assessor ratings were in line with system measures. However, the ratings of query qual-ity among assessors varied considerably, leading to a rather low inter-rater agreement. When comparing all pairs of as-sessors, we observed a maximum  X  = 0 . 54 (linearly weighted Cohen X  X  kappa); the average agreement between all pairs of assessors reached  X  = 0 . 36.
 Partitions AP P@30 AP P@30 Bottom Ten 0 . 005 0.038 2 . 51 (1 . 48) 2 . 40 (1 . 34) Table 1: Avg. performance given partitions based on AP and P@30 respectively (columns 2&amp;3); av-erage (std. dev.) assessor ratings given partitions based on AP and P@30 respectively (columns 4&amp;5). QPP -System Predictions : Table 2 reports the corre-lation between the system predictions made by each QPP method and system performance (columns 2&amp;3). The most striking result is that the pre-retrieval predictors ( SumSCQ and SumVAR ) obtained the highest correlations with sys-tem performance. This contrasts previous findings obtained on older test collections [3], where it is post-retrieval QPP methods that exhibit higher correlations. We suspect that the post-retrieval methods are adversely affected by the con-tent of the web pages in ClueWeb09 (i.e. they contain a lot of non-informative content, like ads, links, menus, etc.). Ratings vs. Predictions : Finally, we compared the as-sessor ratings against the QPP system predictions (Table 2, columns 4-6 show these correlations.). Due to the low level Pre/Post Ret. Performance Assessor Ratings Predictors AP P@30 Min Avg Max MaxIDF 0 . 35  X  0 . 19  X  0 . 09 0 . 09 0 . 29  X  SumSCQ 0 . 39  X  0 . 35  X  0 . 20 0 . 31  X  0 . 49  X  SumVAR 0 . 42  X  0 . 38  X  0 . 17 0 . 28  X  0 . 43  X  Clarity Score 0 . 27  X  0 . 18  X  0 . 10 0 . 02 0 . 19 Query Feedback 0 . 37  X  0 . 29  X  0 . 12 0 . 28  X  0 . 44  X  Query Commit. 0 . 26  X  0 . 11  X  0 . 15 0 . 01 0 . 18 Table 2: Kendall X  X  Tau correlations: QPP methods vs. performance, and vs. assessor ratings (shown are minimum, average and maximum correlation). of inter-rater agreement between the assessors, we report the minimum, average and maximum correlation between ratings and predictions. The highest correlations were ob-served between assessor ratings and the pre-retrieval pre-dictions by SumSCQ . This predictor assigns higher quality scores to more specific queries and was the best indicator of assessor ratings of query quality among all the predictors we evaluated. Although SumSCQ yields significant correlations with most assessors, the correlations are only moderate, at best. Of the evaluated post-retrieval predictors, only Query Feedback resulted in significant correlations when the best parameter was selected.
In this poster, we explored the relationship between ex-plicit user ratings by assessors and the system predictions of a number of QPP methods. We found that assessor rat-ings of query quality are significantly correlated to the pre-dictions of pre-retrieval predictors, but not consistently to post-retrieval predictors. However, while some QPP meth-ods provide a better explanation of user ratings than others, the relationship is still quite weak (with moderate correla-tions at best). This suggests that current QPP methods are unlikely to be adequate proxies of user ratings. Since most QPP methods only utilize system side information, perhaps there are gains to be had by developing more sophisticated methods/models of query performance prediction that in-clude the user and their state of knowledge in the process. In future work, we will investigate these findings in more detail and also consider how the amount of information pro-vided to the user and the QPP method affects their ability to accurately predict the quality of a query.
