 Machine Learning algorithms are often as good as the data they can learn from. Enormous amount of unlabeled data is readily available and the ability to efficiently use such amount of unlabeled data holds a significant promise in terms of increasing the performance of various learning tasks. We consider the task of supervised Domain Adaptation and present a Self-Taught learning based framework which makes use of the K-SVD algorithm for learning sparse representation of data in an unsupervised manner. To the best of our knowledge this is the first work that integrates K-SVD algorithm into the self-taught learning framework. The K-SVD algorithm iteratively alternates between sparse coding of the instances based on the current dictionary and a process of updating/adapting the dictionary to better fit the data so as to achieve a sparse representation under strict sparsity constraints. Using the learnt dictionary, a rich feature representation of the few labeled instances is obtained which is fed to a classifier along with class labels to build the model. We evaluate our framework on the task of domain adaptation for sentiment classification. Both self-domain (requiring very few domain-specific training instances) and cross-domain classification (requiring 0 labele d instances of target domain and very few labeled instances of source domain) are performed. Empirical comparisons of self-domain and cross-domain results establish the efficacy of the proposed framework. H.3.3 [ Knowledge Management ]: Classification and Clustering Algorithms Transfer Learning, Sparse Representation, Domain Adaptation Machine Learning algorithms are often as good as the data they can learn from. The expense involved and the difficulty of obtaining labeled data poses a severe bottleneck in the applicability of several machine learning algorithms for classification tasks. Enormous amount of unlabeled data is readily available and thus the ability to use such amount of unlabeled data holds significant promise in terms of increasing the performance of various learning tasks. Raina et al[3] proposed a new machine learning framework called Self-Taught Learning for using unlabeled data in supervised classification task. Self-Taught Learning requires that the structure learned from unlabeled data be  X  X sefu l X  for representing data from the classification task. Specifically considering the case of sentiment classification of reviews and domain adaptation, a large amount of labeled instances in each of the domains are required to achieve a reasonable classification accuracy. If automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become cumbersome. Learning a different system for each of the domain would prevent us from exploiting the information shared across domains. A better strategy could be to learn a single system from the set of domains for which labeled and unlabeled data are available and then apply it to any target domain. For this strategy to succeed the system should be able to discover intermediate abstractions that are shared and meaningful across domains. Other challenges faced while performing classification tasks include the problem of high dimensionality of observed data: an excessively large number of data features which hinder the model-building process and the inability to represent data (signals, images, etc.) in the most parsimonious terms. Models that propose that the signal of interest is sparse in some transform domain are preferred. We feel that the intermediate abstractions learnt by the dictionary could yield a better transfer across domains. Some of the customer sentiments on product quality, product servicing, etc. make sense across a wide range of domains. Since the same words or tuples of words may be used across domains to indicate the presence of these high er-level concepts , it should be possible to discover them. In this paper we present a framework which proposes solution to the above mentioned problems. We use the K-SVD algorithm (Aharon et al[2]) to learn a dictionary based sparse representation using unlabeled reviews spanning multiple domains. Given a set of unlabeled instances, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We then learn the representation of few labeled instances based on the obtained dictionary and feed it to a classifier. The K-SVD algorithm makes use of unlabeled instances to learn a representation that models intermediate abstractions across the domains thereby fulfilling the objective of Self-Taught Learning while at the same time solving the problem of high dimensionality by keeping a check on the number of dictionary elements to train. We evaluate our framework on a standard Amazon review dataset created by Blitzer et al [1]. Sparse and redundant representation modeling of data assumes an ability to describe signals (y  X  ) as linear combinations of a few atoms { from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. Representing a signal involves the choice of a dictionary, which is the set of elementary signals or atoms used to decompose the signal. When the dictionary forms a basis, every signal is uniquely represented as the linear combination of the dictionary atoms. Sparse coding is the process of computing the representation coefficients x based on the given signal y and the dictionary D. This process, commonly referred to as  X  X tom decomposition,  X  requires solving: where  X . X  X  is the 1o norm, counting the nonzero entries of a vector; and this is typically done by a  X  X ursuit algorithm  X  that finds an approximate solution. Sparse Coding is a necessary stage in the K-SVD algorithm we describe later. Exact determination of the sparsest representation is an NP hard problem and thus various approximate solutions have been proposed. The simplest ones are the matching pursuit (MP)[5] and the orthogonal matching pursuit (OMP) algorithms[6][7]. Dictionary training is a much more recent approach to dictionary design, and as such, has been strongly influenced by the latest advances in sparse representation theory and algorithms. The most recent training methods focus on L0 and L1 sparsity measures, which lead to simple formulations and enable the use of recently developed efficient sparse-coding techniques[8][9]. We next describe the KSVD algorithm (as proposed by [3]) used for learning adaptive dictionaries. Given a set of examples Y, the goal of the K-SVD is to find a dictionary D and a sparse matrix X which minimize the representation error, where  X  represent the columns of X, and the L0 sparsity measure;  X  .  X  counts the number of non-zeros in the representation. The K-SVD algorithm alternates between sparse-coding and dictionary update steps. Sparse coding is performed for each signal individually using any standard technique. The objective function is The penalty term can be written as: Thus the objective function can further be decomposed to N distinct problems of the form: Th is problem is adequately addressed by the pursuit algorithms discussed above. For the part of dictionary updating, only 1 column of the dictionary and the corresponding row in X is questioned, thereby re-writing the penalty term: Defining as the group of indices pointing to the examples that use the atom i.e. those where is zero. is defined as the matrix of size  X  with ones on the , position and zeroes elsewhere. When multiplying  X  = X   X  , this shrinks the row vector by discarding of the zero entries, resulting with the row vector of length . Thus the equation (4) becomes: and SVD can be used to find the final solution. For details the reader is directed to [3]. We use the K-SVD algorithm to train a dictionary based on all the unlabeled instances of all the domains and get their corresponding co-efficient matrix using OMP. Henceforth we use the rows of the representation matrix X as our feature vectors. Domain adaptation considers the setting in which the training and testing data are sampled from different distributions. Assume we have two sets of data: a source domain S providing labeled training instances and a target domain T providing instances on which the classifier is meant to be deployed. We do not make the assumption that these are drawn from the same distribution, but rather that S is drawn from a distribution and T from a distribution . The learning problem consists in finding a function realizing a good transfer from S to T. We consider the case of supervised Domain Adaptation, a setting where we have a large amount of labeled data from some source domain, a large amount of unlabeled data from a target domain, and additionally a small budget for acquiring labels in the target domain. In our setting we experimented on 3 different values of the fixed budget (5%, 10% &amp; 20%) of the total unlabeled instances we had initially in the self-domain. The instances which constitute those percentages were randomly selected and the labels for those instances were acquired from the oracle . Our data was based on the one used by [2]. They used the dataset consisting of product reviews for 4 items  X  books, DVDs, electronics and kitchenware. The raw dataset was taken from Amazon and consisted of product name, reviewer name, ratings (on a scale of 0-5), rating date and title followed by review text. The ratings were made binary (ratings&gt;3 were written as 1 and rest were made 0) for practical purposes. The review text was in the form of frequencies of various words appearing in the text. The remaining attributes were discarded, as they were found to be too ambiguous. We took the same dataset for our practical experiments (the dataset had 1000 positive and 1000 negative reviews). The nature of our experiments required unsupervised training examples and hence, for each of the 4 items, 1000 labeled (500 positive and 500 negative) reviews were separated out for testing purposes and the rest (totaling 4000) with labels discarded, were used for setting up the dictionary. The same dictionary was used for modeling a sentiment predictor for each of the 4 domains (the domains here being the items) as well as for domain transfer between them (the model building aspect is explained at a later stage). The basic preprocessing step was the one used by [2] in which the bag-of -words representation with frequency of each word was converted to a simple word presence/absence format. For computational reasons, only the most 5000 frequent unigrams and bigrams were shortlisted for further computations The experiments were conducted in the following steps : The first step was to come up with the dictionary representation using the 4000 unlabeled instances. By setting the appropriate parameters, the size of dictionary representation (the number of attributes in terms of which data instance can be represented) was kept at 50 (thus achieving dimensionality reduction from 5000 to 50). We came up with the dictionary by iteratively running the code till the error threshold was reached. To achieve sparse representation, the no, of coefficients to use in OMP coefficient calculation was empirically kept as 5. The second step was to train the data for a particular domain. The remaining 4000 instances (1000 labeled ones from each of the domains, after removing their labels) were represented in terms of the dictionary achieved in the step 1 using OMP coefficient calculations. The coefficient matrices obtained by the sparse coding stage were divided into training and testing sets of various sizes. In this paper, we show the results for training on 50,100 and 200 instances respectively (corresponding to 5%, 10% and 20% respectively in tables and figures) and testing on the remaining ones (from the 1000 labeled ones). The dictionary learned from unsupervised learning allowed training on so few samples to achieve reasonably satisfactory results. The algorithms used were Voted Perceptron, Logistic Regression and Bayesian Logistic Regression algorithms. The results are enumerated in Table 1. The second objective of our method was to enable cross-domain training to allow a single model to be applied in other domains as well. This is accomplished by training on one of the domains and testing on the remaining 3 domains using the same 3 algorithms mentioned earlier. The results are enumerated in Table 2. The next section provides a brief analysis of the results. Below are a few points summarizing the results achieved and how they stack up against our initial aims. 1.As visible, the results of self-domain training in case of minimal labeled examples is comparable to that of cross-domain training, thus fulfilling the initial aims of the algorithm. 2. The results also show that as theorized, building representations from multiple sources tends to trap some higher level and more abstract patterns and structures in data as well. This accounts for the results achieved in both Table 1 and Table 2. 3. Like in the case of [18] , the quality of our features obtained allow such a cross-domain accuracy but unlike in their case, we achieved these results without using any kind of hierarchical or multi-layer k-SVD algorithm (they had used stacked Denoising auto-encoder). In addition we also achieved a dimension reduction from 5000 attributes to mere 50 attributes along with a sparse representation (the no, of coefficients to use in OMP coefficient calculation was kept as 5 ) . Also, they had used 80% ie 1600 samples for training purpose, much higher than our own. 4. The results for cross-domain were achieved without any prior knowledge about the source and target domains. Important observation in case of cross-domain results is that the domains are not necessarily significantly related to each other (like kitchenware and books, for example). But our dictionary manages to achieve commendable results in-spite of this. 5. Further improvements in results for practical purposes is further possible by implementing an hierarchical version of the K-SVD algorithm. Another aspect is that for building up the dictionary, unlabeled reviews from more varied domains could be taken for a better representation. One important factor to note is that apart from the algorithmic difference from the previous approaches, the key difference in our framework is that in coherence to the Self-Taught Learning principle, we learn sparse representation using unlabeled instances from all four domains. We believe that such accuracies for domain transfer are achieved due to the rich representation learnt to represent reviews in our algorithm. [1] Blitzer, J., McDonald, R., and Pereira, F. (2006). Domain [2] Blitzer, Dredze, Pereira F.(2007) Biographies, Bollywood, [3] Daume III, Marcu D. (2006) Domain Adaptation for [4] M. Aharon, M. Elad, and A. M. Bruckstein, K-SVD: An [5] S. Chen, S. A. Billings, and W. Luo,  X  X rthogonal least [6] J. A. Tropp,  X  X reed is good: Algorithmic results for sparse [7] Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, Orthogonal [8] G. Davis, S. Mallat, and Z. Zhang,  X  X daptive time-frequency [9] S. S. Chen, D. L. Donoho, and M. A. Saunders,  X  X tomic [10] S. S. Chen, D. L. Donoho, and M. A. Saunders,  X  X tomic [11] A. M. Bruckstein, D. L. Donoho, and M. Elad,  X  X rom sparse [12] Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, [13] I. F. Gorodnitsky and B. D. Rao,  X  X parse signal [14] D. Needell and J. A. Tropp,  X  X oSaMP: Iterative signal [15] S. A. Haider, R. Mehrotra,  X  X orporate News Classification [16] S. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya, [17] R. Vidal, Y. Ma, and S. Sastry,  X  X eneralized principal [18] Glorot,Bordes, Bengio(2011) Domain Adaptation for Large-
