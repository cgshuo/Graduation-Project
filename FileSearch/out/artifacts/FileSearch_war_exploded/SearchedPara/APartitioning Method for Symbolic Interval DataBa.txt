 To solve the problem of situations with nonlinearly separa-ble clusters, kernel clustering methods have been proposed. Symbolic Data Analysis (SDA) has emerged to deal with variables that can have intervals, histograms, and even func-tions as values, in order to consider the variability and/or uncertainty innate to the data. In this paper, we present a K-means clustering method based in kernelized squared L 2 distance for symbolic interval-type data. Experiments with real and syntectic symbolic interval-type data sets are considered.
 I.5.3 [ Pattern Recognition ]: Clustering Theory Kernel, partitioning method, kernelized metric, interval data.
Currently, with the explosive growth in the use of databases and the huge volume of data stored, the clustering has be-come a subject of great interest and the cluster analysis has been used in many application domains like pattern recog-nition, machine learning, data mining and computer vision [7] [3]. The objective of the cluster analysis is to group a set of elements into clusters such that elements within a cluster have a high degree of similarity, while elements belonging to different clusters have a high degree of dissimilarity. One of the most popular partitioning method is K-Means. However, this algorithm present a serious limitation: it can only find linearly separable clusters. To solve the problem of situations with nonlinearly separable clusters, kernel cluster-ing methods have been proposed. These methods are able to produce nonlinear separating hyper surfaces among clusters [4]. The Kernel Fuzzy c-Means was proposed [9] aiming to replace a kernel-induced distance metric for the original Eu-clidean distance of the fuzzy partitioning method FCM. The methodology of kernelized metrics is described in [4] and [9]. Methods based on kernelization of the metric compute the cluster center and the distance between centers and patterns using kernel functions.

Research has shown that data sets involving numeric data only are not as rich in information for the complexity of the data found in real problems, such as databases used by the company. Therefore, Symbolic Data Analysis (SDA) has emerged to deal with variables in order to consider the vari-ability and/or uncertainty innate to the data [2]. The main goal of SDA is to use statistical techniques for representa-tion of information that is made using symbolic data asso-ciated with rules, probability distributions or intervals and extend classical data analysis techniques, such as, clustering, factorial techniques, decision trees and another. Thus, the techniques SDA become a powerful tool when used in clus-tering methods, which causes a constant growth in research to improve these techniques.

In this paper, we present a K-means clustering method based in kernelized squared L 2 distance for symbolic interval-type data. This method is an hard version of the fuzzy K-Means cluster algorithm based on methodology of ker-nelized metrics presented in [4] to treat symbolic interval data. The paper is organized as follows. In order to com-pare the proposed method in this paper with a hard cluster-ing method for interval data, Section 2 presents a K-Means clustering method introduced in [1]. In Section 3, we intro-duce K-Means clustering method based on kernelized metric for symbolic interval data. To validate this new method, Sec-tion 4 presents experiments with real and syntectic symbolic interval-type data sets. The evaluation of the clustering re-sults furnished by the methods is based on the computation of an external cluster validity index and the global error rate of classification. In Section 5, the concluding remarks are given.
Let  X  be a set of n objects indexed by i and described by p symbolic interval-valued variables indexed by j . Each object i is represented by a vector of intervals x i = ([ a 1 i , b [ a , b p i ]) T . Similar to the object, the prototype of a clus-tervals y k = ([  X  1 i ,  X  1 i ] , . . . , [  X  p i ,  X  p i P = ( C 1 , . . . , C K ) from  X  in K clusters where each cluster has a prototype described by the vector y k .
 The problem of clustering concerns to find a partition P of { y 1 , . . . , y K fined as: where
The prototype minimizing the criterion is an interval vec-tor given by:
Schema of the KM-IV algorithm 1. Initialization 2. Representation step: de nition of the best pro-3. Allocation step: de nition of the best partition 4. Stopping criterion
This section introduces k-means method for interval data with kernelized metric (here called KKM-IV). Let a non-linear function  X  that performs the mapping from the in-put space to an high dimensional interval feature space as  X  : x i  X   X  ( x i ). Consider a set of K centers in high interval feature space {  X  ( y 1 ) , . . . ,  X  ( y k ) } . The problem of clustering concerns to find a partition of  X  into K disjoint clusters P = { C 1 , . . . , C K } such that min-imizes a criterion function defined as
The function  X  is not explicit and it is not possible to compute the Equation (4). Nevertheless, it is possible to calculate the distances between patterns and clusters cen-ters by using a kernel trick for interval data called K . In this work, the kernel trick is defined by boundaries of the intervals separately and it given by K ( x i , x l ) =  X  ( x i )  X   X  ( x l ) =  X  ( x iL )  X   X  ( x where x iL = ( a 1 i , . . . , a p i ) T and x iU = ( b 1 the lower and upper bounds of the intervals of the i  X  th object of  X , respectively and  X  a nonlinear function that performs the mapping from a point input space to an high dimensional feature space.

The k  X  th cluster center is an interval vector given by:
Expanding the Equation (4) and using the kernel trick, the criterion function J  X  can be rewrite as:
Some kernel function examples for interval data are given as: wh ere
The algorithm sets a n  X  n kernel matrix K = { K ( x i , x and initial partition and performs iteratively the reassigning of the individual to closest cluster until the criterion J reaches a stationary value representing a local minimum.
Schem a of the KKM-IV algorithm 1. Pre-processing step: compute new feature space 2. Initialization step: obtaining an initial parti-3. Allocation step: updating the partition 4. Stopping criterion
In order to validate the efficiency of the proposed method, this section presents the clustering results considering real and synthetic interval data sets . Here, our aim is to achieve a comparison of the kernelized K-means method with the K-means method for interval data. Regarding the kernelized K-means method, it was used the RBF kernel function with  X  = 1. To measure the clustering quality provided by KM-IV and KKM-IV methods, we used the corrected Rand index (CR) [6] and the overall error rate of classification (OERC).
In order to build interval data sets from quantitative data sets, each point ( s 1 , s 2 ) of these quantitative data sets is considered as the  X  X eed X  of a rectangle. Each rectangle is therefore a vector of two intervals defined by ([ s 1  X   X   X  / 2] , [ s 2  X   X  2 / 2 , s 2 +  X  2 / 2]). The parameters  X  1 and  X  the width and the height of the rectangle. They are drawn randomly within a given range of values.

The interval data set 1 is formed by two rings, each rep-resenting a class. The first ring has radius equal to 12 and 100 individuals. The second ring has radius equal to 5 and 50 individuals. Figure 1 represents this data set with two rings using the predefined interval [1 , 2].

The interval data set 2 describes a spiral whose each arm is defined in polar coordinates ( r,  X  ) by the equation r = a + b X  , where a and b are real numbers and  X  in angular measurements. The parameter a controls the initial radius of the each arm of the spiral, while b is related with distance between successive turns. This data set has 90 individuals scattered among four classes contained 15, 20, 25 and 30, respectively. The total angle  X  = 120 degrees with a = 10 and b = 5. Figure 2 displays the interval data set 2 using the predefined interval [1 , 5].

In the framework of a Monte Carlo experiment, 100 repli-cations are considered for each interval data set, as well as Fig ure 1: Symbolic interval data set 1: two rings Fi gure 2: Symbolic interval data set 2: spiral with four arms and [1 , 20]. The average of the corrected Rand index as well as average of the overall error rate of classification are calcu-lated. In each replication a clustering method is run (until the convergence to a stationary value of the adequacy cri-terion J or J  X  ) 200 times and the best result, according to the criterion J or J  X  , is selected.
 Table 2 shows the CR index and the OERC metric for the KM-IV and KKM-IV methods for the interval data set 1. As expected for this data configuration with two nonlinear clusters, the performance of the KKM-IV method is superior to that of the KM-IV one.
 Table 2: Comparison of the clustering methods for interval data set 1
T able 3 shows the results of the CR index and OERC metric for interval data set 2. Again, the KKM-IV method outperforms the KM-IV one. T able 3: Comparison of the clustering methods for interval data set 2
This interval data set [5] concerns 37 cities, each city is de-scribed by 12 interval variables which are minimum and the maximum temperatures of 12 months in degree centigrade. Table 4 shows part of this data set.
 Table 4: Minimum and the maximum temperatures of cities in degree centigrade
The CR indices obtained from the comparison between the a priori partition and the partitions given by the KKM-IV and KM-IV methods are, respectively, 0 . 629 and 0 . 610 and the OERC for methods are, respectively, 0 . 189 and 0 . 270. Therefore, the performance of the KKM-IV method is better than to that of the KM-IV one for this application data set.
This data set was extracted from [8] and contains infor-mation on species of fungi. Agaricus is a genus of mostly medium to large gilled mushrooms. Well known members of the genus Agaricus are the bisporus, the  X  X utton mushroom X  common in supermarkets and the campestris, the  X  X eadow mushroom X , a popular edible mushroom which is common in pastures and grasslands in many temperate areas of the world. This data set consists of 24 mushrooms, each mush-room described by 5 interval variables: width of the pileus, width of the stipe, thickness of the stipe, height and width of spores. Table 5 shows part of this data set. It is composed of two classes: edible (E) and inedible (I).
 For this application, the CR indices calculated for the KM-IV and KKM-IV methods are, respectively,  X  0 . 599 and 0 . 314, respectively. The OERC metrics for the KM-IV and KKM-IV methods are, respectively, 0 . 500 and 0 . 167. Again, the KKM-IV is best option for this real interval data set. Table 5: Information about Agaricus species known to occur in California
In this paper, we have presented a new clustering method for symbolic interval data based kernelized metric. This method is an K-Means for symbolic interval data using ker-nel functions to compute distances between clusters and pro-totypes. The evaluation of this method in comparison with a K-Means clustering method for interval data having ade-quacy criterion based on single Euclidean distance was car-ried out. The accuracy of the results provided by these clus-tering methods were assessed by the corrected Rand index and overall error rate considering synthetic and real data sets. Concerning the CR index and the overall error rate for these synthetic and real interval data sets, the proposed method in this paper outperformed the K-Means clustering method. The authors would like to thank CNPq, CAPES and FACEPE (Brazilian Agencies) for their financial support. [1] F. D. Carvalho, P. Brito, and H. Bock. Dynamic [2] E. Diday and M. Noirhomme-Fraiture. Symbolic Data [3] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [4] M. Filippone, F. Camastra, F.Masulli, and S. Rovetta. [5] D. Guru, B. Kiranagi, and P. Nagabhushan.
 [6] L. Hubert and A. P. Comparing partitions. Journal of [7] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [8] M. Wood and F. Stevens. Agaricus. [9] D. Q. Zhang and S. C. Chen. Kernel-based fuzzy and
