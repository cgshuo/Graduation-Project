 Relation acquisition is an important aspect of ontology learning. Traditionally, semantic relations are either extracted as verbs based on grammatical structures [1], induced through term co-occurrence using large text corpora [2], or discov-ered in the form of unnamed associations through cluster analysis [3]. Challenges faced by conventional techniques include 1) the reliance on static patterns and text corpora together with rare knowledge resources, 2) the need for named en-tities to guide relation acquisition, 3) the difficulty in classifying composite or ambiguous names into the required categories, and 4) the dependence on gram-matical structures and the presence of verbs can result in the overlooking of indirect, implicit relations. In recent years, there is a growing trend in relation acquisition using Web resources such as Wikipedia [4] and online ontologies (e.g. Swoogle) [5] to partially address the shortcomings of conventional techniques.
In this paper, we propose a hybrid approach which integrates lexical sim-plification, word disambiguation and association inference for acquiring seman-tic relations using only the Web (i.e. Wikipedia and web search engines) for constructing lightweight domain ontologies. Our approach performs an iterative process of term mapping and term resolution to identify coarse-grained relations between domain terms. The main contribution of this paper is the resolution phase which allows our relation acquisition approach to handle complex and ambiguous terms, and terms not covered by our background knowledge on the Web. Our approach can be used to complement conventional techniques for ac-quiring fine-grained relations and to automatically extend knowledge resources such as Wikipedia. The rest of the paper is structured as follows. Section 2 and 3 present existing work related to relation acquisition, and the detail of our ap-proach, respectively. The outcome of the initial experiment is summarised in Section 4. We conclude this paper in Section 5. Techniques for relation acquisition can be classified as symbolic-based, statistics-based or a hybrid of both. The use of linguistic patterns enables the discovery of fine-grained semantic relations. For instance, Poesio &amp; Almuhareb [6] de-veloped specific lexico-syntactic patterns to discover named relations such as part-of and causation . However, linguistic-based techniques using static rules tend to face difficulties in coping with the structural diversity of a language. The technique by Sanchez &amp; Moreno [1] for extracting verbs as potential named relations is restricted to handling verbs in simple tense and verb phrases which do not contain modifiers such as adverbs. In order to identify indirect relations, statistics-based techniques such as co-occurrence analysis and cluster analysis are necessary. Co-occurrence analysis makes use of redundancy in large domain corpora to detect the presence of statistically significant associations between terms. However, the textual resources required by such techniques are difficult to obtain, and remain static over a period of time. For example, Schutz &amp; Buitelaar [2] manually constructed a domain corpus in the  X  X ootball X  domain containing only 1 , 219 documents from an online football site for relation acquisition. Clus-ter analysis [3], on the other hand, requires tremendous computational effort in preparing features from texts for similarity measurements. The lack of em-phasis on indirect relations is also evident in existing techniques. Many relation acquisition techniques in information extraction acquire semantic relations with the guidance of named entities [7]. Relation acquisition techniques which require named entities have restricted applicability since many domain terms with im-portant relations cannot be easily categorised. In addition, the common practice of extracting triples using only patterns and grammatical structures tends to disregard relations between syntactically unrelated terms.

In view of the shortcomings of conventional techniques, there is a growing trend in relation acquisition which favours the exploration of rich, heteroge-neous resources on the Web over the use of static, rare background knowledge. SCARLET [5], which stemmed from a work in ontology matching, follows this paradigm by harvesting online ontologies on the Semantic Web to discover re-lations between concepts. Sumida et al. [4] developed a technique for extracting a large set of hyponymy relations in Japanese using the hierarchical structures of Wikipedia. There is also a group of researchers who employ Web documents as input for relation acquisition [8]. Similar to the conventional techniques, this group of work still relies on the ubiquitous WordNet and other domain lexicons for determining the proper level of abstraction and labeling of relations between the terms extracted from Web documents. Pei et al. [9] employed predefined local (i.e. WordNet) and online ontologies to name the unlabeled associations between concepts in Wikipedia. The labels are acquired through a mapping process which attempts to find lexical matches for Wikipedia concepts in the predefined on-tologies. The obvious shortcomings include the inability to handle complex and new terms which do not have lexical matches in the predefined ontologies. Our relation acquisition approach is composed of two phases, namely, term map-ping and term resolution . The input is a set of domain terms T produced using a separate term recognition technique. The inclusion of a resolution phase sets our approach apart from existing techniques which employ resources on the Web for relation acquisition. This resolution phase allows our approach to handle com-plex and ambiguous terms, and terms which are not covered by the background knowledge on the Web. Figure 1 provides an overview of our approach.

In this approach, Wikipedia is seen as a directed acyclic graph W where vertices V are topics covered by Wikipedia, and edges E are three types of coarse-grained relations between the topics, namely, hierarchical H , associative A ,and polysemous P ,or E = H  X  A  X  P . It is worth noting that H , A and P are disjoint sets. These coarse-grained links are obtained from Wikipedia X  X  classification scheme,  X  X ee Also X  section, and disambiguation pages, respectively. The term mapping phase creates a subgraph of W for each set T , denoted as W
T by recursively querying W for relations that belong to the terms t The querying aspect is defined as the function map ( t ), which finds an equivalent topic u  X  V in W for term t , and returns the closed neighbourhood N t : Algorithm 1. termMap( t, W T ,M,root,iteration ) The neighbourhood for term t is denoted as ( V t ,E t ) where E t = { ( t, y ):( t, y )  X  H sets H t , A t and P t contain hierarchical, associative and polysemous links which connect term t to its adjacent terms y  X  V t . The process of term mapping is sum-marised in Algorithm 1. The term mapper in Algorithm 1 is invoked once for every t  X  T . The term mapper ceases the recursion upon encountering the base case, which consists of the root vertices of Wikipedia (e.g.  X  X ain topic classifi-cations X  ). An input term t  X  T which is traced to the root vertex is considered as successfully mapped, and is moved from set T to set M . Figure 2(a) shows the subgraph W T created for the input set T= {  X  X aking powder X , X  X hole wheat flour X  } . In reality, many terms cannot be straightforwardly mapped because they do not have lexically equivalent topics in W due to the non-exhaustive coverage of Wikipedia, the tendency to modify terms for domain-specific uses, and the polysemous nature of certain terms. The term mapper in Algorithm 1 returns different values, namely, composite , non-existent and ambiguous to indicate the causes of mapping failures. The term resolution phase resolves mapping fail-ures through the iterative process of lexical simplification, word disambiguation and association inference. Upon the completion of mapping and resolution of all input terms, any direct or indirect relations between the mapped terms t  X  M can be identified by finding paths which connect them in the subgraph W T .
Finally, we devise a 2-step technique to transform the subgraph W T into a lightweight domain ontology. Firstly, we identify the nearest common ancestor (NCA) for the mapped terms. The discussion on our simple algorithm for finding NCA is omitted due to space constraints. Secondly, we identify all directed paths in W T which connect the mapped terms to the new root NCA and use those paths to form the final lightweight domain ontology. The lightweight domain ontology for the subgraph W T in Figure 2(a) is shown in Figure 2(b). We discuss the details of the three parts of term resolution in the following three subsections. 3.1 Lexical Simplification The term mapper in Algorithm 1 returns the composite value to indicate the inability to map a composite term (i.e. multi-word term). Composite terms which have many modifiers tend to face difficulty during term mapping due to the absence of lexically equivalent topics in W . To address this, we designed a lexical simplification step to reduce the lexical complexity of composite terms in a bid to increase their chances of re-mapping. A composite term is comprised of a head noun altered by some pre-(e.g. adjectives and nouns) or post-modifiers (e.g. prepositional phrases). These modifiers are important in clarifying or limiting the extent of the semantics of the terms in a particular context. For instance, the modifier  X  X ne cup X  as in  X  X ne cup whole wheat flour X  is crucial for specifying the amount of  X  X hole wheat flour X  required for a particular pastry. However, the semantic diversity of terms created by certain modifiers is often unnecessary in a larger context. Our lexical simplifier make use of this fact to reduce the complexity of a composite term for re-mapping.

The lexical simplification step breaks down a composite term into two struc-turally coherent parts, namely, an optional constituent and a mandatory con-stituent . A mandatory constituent is comprised of but not limited to the head noun of a composite term, and has to be in common use in the language independent of the optional constituent. The lexical simplifier then finds the least dependent pair as the ideally decomposed constituents. The dependen-cies are measured by estimating the mutual information of all contiguous con-stituents of a term. A term with n -words has n  X  1 possible pairs denoted as term t is computed as MI ( x, y )= f ( t ) /f ( x ) f ( y ) where f is a frequency measure. In a previous work [10], we utilise the page count returned by Google to compute the relative frequency required for mutual information. Given that Z = { t, x, y } , the relative frequency for each z  X  Z is computed as f ( z )= n z n n z is the page count returned by web search engines, and n Z = u  X  Z n u .Upon identifying the two least dependent constituents, we re-map the mandatory por-tion. To retain the possibly significant semantics delivered by the modifiers, we also attempt to re-map the optional constituents. If the decomposed constituents are in turn not mapped, another iteration of term resolution is performed. Un-related constituents will be discarded. For this purpose, we define the distance of a constituent with respect to the set of mapped terms M as: where noW ( a, b ) is a measure of geodesic distance between topic a and b based on Wikipedia developed by the authors [11] known as n  X  of Wikipedia (noW) .A constituent is discarded if  X  ( { x, y } ,M ) &gt; X  and the current set of mapped terms is not empty, | M | = 0. The threshold  X  =  X  ( M )+  X  ( M ), where  X  ( M )and  X  ( M ) are the average and the standard deviation of the intra-group distance of M . 3.2 Word Disambiguation The term mapping phase in Algorithm 1 returns the ambiguous value if a term t has a non-empty set of polysemous links P t in its neighbourhood. In such cases, the terms are considered as ambiguous and cannot be directly mapped. To address this, we include a word disambiguation step which automatically resolves ambiguous terms using noW [11]. Since all input terms in T belong to the same domain of interest, the word disambiguator finds the proper senses to replace the ambiguous terms by the virtue of the senses X  relatedness to the already mapped terms. Senses which are highly related to the mapped terms have lower noW value. For example, the term  X  X epper X  is considered as ambiguous since its neighbourhood contains a non-empty set P pepper with numerous polysemous links pointing to various senses in the  X  X ood ingredients X  ,  X  X usic X  and  X  X ports X  domains. If the term  X  X epper X  is provided as input together with terms such as  X  X inegar X  and  X  X arlic X  , we can eliminate all semantic categories except  X  X ood ingredients X  . Each ambiguous term t has a set of senses S t = { s : s  X  V t  X  ( t, s )  X  P } . Equation 2, denoted as  X  ( s, M ), is used to measure the distance between a sense s  X  S t with the set of mapped terms M .

The senses are then sorted into a list ( s 1 , ..., s n ) in ascending order accord-ing to their distance with the mapped terms. The smaller the subscript, the smaller the distance, and therefore, the closer to the domain in consideration. An interesting observation is that many senses for an ambiguous term are in fact minor variations belonging to the same semantic category (i.e. paradigm). Referring back to our example term  X  X epper X  , within the  X  X ood ingredients X  do-main alone, multiple possible senses exist (e.g.  X  X ichuan pepper X  ,  X  X ell pepper X  ,  X  X lack pepper X  ). While these senses have their intrinsic differences, they are par-adigmatically substitutable for one another. Using this property, we devise a senses selection mechanism to identify suitable paradigms covering highly re-lated senses as substitutes for the ambiguous terms. The mechanism computes the difference in noW value as  X  i  X  1 ,i =  X  ( s i ,M )  X   X  ( s i  X  1 ,M )for2  X  i  X  n between every two consecutive senses. We currently employ the average stepwise difference of the sequence as the cutoff point. The average stepwise difference for with  X  i  X  1 ,i &lt; X   X  are accepted as belonging to a single paradigm for replac-ing the ambiguous term. Using this mechanism, we have reduced the scope of the term  X  X epper X  to only the  X  X ood ingredient X  domain out of the many senses across genres such as  X  X usician X  (e.g.  X  X epper (band) X  )and  X  X everage X  (e.g.  X  X r pepper X  ). In our example in Figure 4, the ambiguous term  X  X epper X  is replaced by {  X  X lack pepper X , X  X llspice X , X  X elegueta pepper X , X  X ubeb X  } . These k =4word senses are selected as replacements since the stepwise difference at point i =5,  X  4 , 5 =0 . 5 exceeds  X   X  =0 . 2417. 3.3 Association Inference Terms that are labelled as non-existent by Algorithm 1 simply do not have any lexical matches on Wikipedia. We propose to use cluster analysis to infer potential associations for such non-existent terms . We employ our term clustering algorithm with featureless similarity measures known as Tree-Traversing Ant (TTA) [11]. TTA is a hybrid algorithm inspired by ant-based methods and hierarchical clustering which utilises two featureless similarity measures, namely, Normalised Google Distance (NGD) [12] and noW . A detailed discussion on TTA is beyond the scope of this paper. Unlike conventional clustering algorithms which involve feature extraction and selection, terms are automatically clustered using TTA based on their usage prevalence and co-occurrence on the Web.
In this step, we perform term clustering on the non-existent terms together with the already mapped terms in M to infer hidden associations. The asso-ciation inference step is based on the premise that terms grouped into similar clusters are bound by some common dominant properties. By inference, any non-existent terms which appear in the same clusters as the mapped terms should have similar properties. The TTA returns a set of term clusters C = { C 1 , ..., C n } upon the completion of term clustering for each set of input terms. Each C i  X  C is a set of related terms as determined by TTA . Figure 5 shows the results of clustering the non-existent term  X  X onchiglioni X  with 14 mapped terms. The output is a set of three clusters { C 1 ,C 2 ,C 3 } . Next, we acquire the parent topics of all mapped terms located in the same cluster as the non-existent term by calling the mapping function in Equation 1. We refer to such clus-ter as target cluster . These parent topics, represented as the set R , constitute the potential topics which may be associated with the non-existent term. In our example in Figure 5, the target cluster is C 1 , and the elements of set R are {  X  X asta X , X  X asta X , X  X asta X , X  X talian cuisine X , X  X auces X , X  X uts of pork X , X  X ried meat X , X  X talian cuisine X , X  X ork X , X  X alumi X  } . We devise a prevailing parent selection mechanism to identify the most suitable parent in R to which we attach the non-existent term. The prevailing parent is determined by assign-ing a weight to each parent r  X  R , and ranking the parents according to their weights. Given the non-existent term t and a set of parents R ,the pre-vailing parent weight (  X  r ) where 0  X   X  r &lt; 1 for each unique r  X  R is de-fined as  X  r = common ( r ) sim ( r, t ) subsume ( r, t )  X  r where sim ( a, b ) is given by 1  X  NGD ( a, b )  X  ,and NGD ( a, b )isthe Normalised Google Distance [12] between a and b .  X  is a constant within the range (0 , 1] for adjusting the NGD distance. r in set R .  X  r =1if subsume ( r, t ) &gt; subsume ( t, r )and  X  r = 0 otherwise. The subsumption measure subsume ( x, y ) is the probability of x given y computed as n ( x, y ) /n ( y ), where n ( x, y )and n ( y ) are page counts obtained from web search engines. This measure is used to quantify the extent of term x being more gen-eral than term y . The higher the subsumption value, the more general term x is with respect to y . Upon ranking the unique parents in R based on their weights, we select the prevailing parent r as the one with the largest  X  . A link is then created for the non-existent term t to hierarchically relate it to r . We experimented with a prototype of our approach shown in Figure 1 using two manually-constructed data sets, namely, a set of 11 terms in the  X  X enetics X  domain, and a set of 31 terms in the  X  X oods X  domain. The system performed the initial mappings of the input terms at level 0. This results in 6 successfully mapped terms and 5 unmapped composite terms in the  X  X enetics X  domain. As for the terms in the  X  X oods X  domain, 14 were mapped, 16 were composite and 1 was non-existent. At level 1, the 5 composite terms in the  X  X enetics X  domain were decomposed into 10 constituents where 8 were remapped and 2 required further level 2 resolution. The non-existent term in the  X  X oods X  domain was successfully clustered and remapped at level 1. The 16 composite terms were decomposed into 32 constituents where 14, 10, 5 and 3 were remapped, composite, non-existent and discarded, respectively. Figure 6 summarises the experiment results.
Overall, the system has a 100% precision in the aspect of term mapping, lex-ical simplication and word disambiguation in all two levels using the small set of 11 terms in the  X  X enetics X  domain as shown in Figure 6. As for the set of foods-related terms, there was one false positive (i.e. incorrectly mapped) involv-ing the composite term  X 100g baby spinach X  which results in an 80% precision in level 2. In level 1, this composite term was decomposed into the appropriate constituents  X 100g X  and  X  X aby spinach X  . In level 2, the term  X  X aby spinach X  was further decomposed and its constituent  X  X pinach X  was successfully remapped. The constituent  X  X aby X  in this case refers to the adjectival sense of  X  X ompara-tively little X  . However, the modifier  X  X aby X  was inappropriately remapped and attached to the concept of  X  X nfant X  . The lack of information on polysemes and synonyms for basic English words is the main cause to this problem. In this regard, we are planning to incorporate dynamic linguistic resources such as Wiktionary to complement the encyclopaedic nature of Wikipedia. Other es-tablished, static resources such as WordNet can also be used as a source of basic English vocabulary. Moreover, the incorporation of such complementary resources can assist in retaining and capturing additional semantics of complex terms by improving the mapping of constituents such as  X  X ried X  and  X  X liced X  . General words which act as modifiers in composite terms often do not have cor-responding topics in Wikipedia, and are usually unable to satisfy the relatedness requirement outlined in Section 3.1. Such constituents are currently ignored as shown through the high number of discarded constituents in level 2 in Figure 6. Moreover, the clustering of terms to discover new associations is only performed at level 1, and non-existent terms at level 2 and beyond are currently discarded.
Upon obtaining the subgraphs W T for the two input sets, the system finds the corresponding nearest common ancestors. The NCA for the genetics-related and the foods-related terms is  X  X enetics X  and  X  X oods X  , respectively. Using these NCA s, our system constructed the corresponding lightweight domain ontologies as shown in Figure 7. The lightweight domain ontology for the  X  X oods X  domain is not included due to space constraint. However, a detailed account of this experiment is available to the public 1 . Acquiring semantic relations is an important part of ontology learning. Many existing techniques face difficulty in extending to different domains, disregard implicit and indirect relations, and unable to handle relations between compos-ite, ambiguous and non-existent terms. We presented a hybrid approach which combines lexical simplification, word disambiguation and association inference for acquiring semantic relations between potentially composite and ambiguous terms using only dynamic resources on the Web (i.e. Wikipedia and web search engines). During our initial experiment, the approach demonstrated the ability to handle terms from different domains, to accurately acquire relations between composite and ambiguous terms, and to infer relations for terms which do not exist in Wikipedia. The lightweight ontologies discovered using this approach is a valuable resource to complement other techniques for constructing full-fledged ontologies. Our future work includes the diversification of domain and linguis-tic knowledge by incorporating online dictionaries to support general words not available on Wikipedia. Evaluation using larger data sets, and the study on the effect of clustering words beyond level 1 is also required.
 This research is supported by the Australian Endeavour International Post-graduate Research Scholarship, the DEST (Australia-China) Grant, and the Inter-university Grant from the Department of Chemical Engineering, Curtin University of Technology.

