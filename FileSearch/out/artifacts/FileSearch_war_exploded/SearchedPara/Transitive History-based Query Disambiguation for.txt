 We present a probabilistic model of a user X  X  search history and a target query reformulation. We derive a simple transitive similarity algorithm for disambiguating queries and improving history-based query reformulation accuracy. We compare the merits of this ap-proach to other methods and present results on both examples as-sessed by human editors and on automatically-labeled click data. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Keywords: Personalization, Reformulation, Graphical Models
The importance of using web search history to personalize and improve search results has long been recognized and several ap-proaches have been proposed in the literature to leverage short or long-term user events to better predict relevant results ([1, 2, 3, 5, 6]). In this paper, we focus on improving query reformulation (a query rewriting technique for better search result selection) by us-ing search history for disambiguating a given user query.
Specifically, we address the problem of adjusting query similar-ity scores in the presence of additional history context. Given can-didate query reformulations, we build a query-history-reformulation model and update reformulation scores that can be used to rerank the candidate set or, as we do, to remove reformulations which be-come less relevant in the context of the search history.
Our model is general and can be used to derive various scor-ing algorithms depending on modeling choices. In this work, we describe a simple algorithm whose main feature is the use of the transitive nature of query similarity: given two candidate reformu-lations (possibly with very different senses) of a query q , the better rewrite is likely the one most related to past search queries relevant to q . By using rewrite-to-history similarity and history-to-current-query similarity, our model strives to account for the search intent that can be implicit in relevant search history. The algorithm builds on (but is not tied to) an independently-trained general discrimi-native pairwise query similarity model which extends the machine learned model in [4]. The pairwise similarity model can be treated as a black box and is not the focus of this paper. We just note that we use several types of features (e.g., lexical, semantic, searc h log-derived) and train the model on both web click log data and examples judged by human editors.

Let q be a search query, A the set of alternative reformulations of q , and H the vector of queries preceding q in the search his-tory. As an example (from Yahoo X  X  search logs), a user searches for q ="bec" and we map q to alternative rewrites, A = { a "bose einstein condensates" , a 2 = "bahamas electric company" , a "basic ecclesial community" } . All three reformulations are rea-sonable in the absence of other information. Knowing the previ-ous queries H = { h 1 = "bahamas" , h 2 = "facebook" , h 3 "bahamas electric" } , however, strongly suggests a 2 might the more appropriate alternative of the three. In this section, we wish to build a model that encodes the intuition that the subset of the history rel-evant to the current query should affect query-to-rewrite similarity through history-to-reformulation similarity.

We introduce a latent relevance indicator set, G (of size m ), whose setting determines the subset of the history related to the current query, q (search history can consist of many unrelated in-tents).For a given a  X  A , we write the joint probability, P ( q , a , H ) as
P ( q, a, H ) = P G.m P ( q, a, H, G, m ) where P ( m ) represents a prior on the size of the relevant history and P ( G | m ) , a prior on the distribution of relevant history (e.g., more recent history tends to be more relevant than distant history).
Let R G , m ( R when unambiguous) be the subset of H relevant to q given a setting of ( G , m ) , and  X  R = H \ R .

P ( q, a, H | G, m ) = P ( q, a | R,  X  R, G, m ) P ( H ) where we assume ( q , a ) is conditionally independent from ( G , m , given R . P (  X  R | R ) represents the degree to which it is likely to ob-serve the remaining history  X  R given the history relevant to q . Finally, we decompose P ( q, a, R ) as where we assume independence of queries in the history condi-tioned on q and a . Putting together eqn. 1, 2, and 3, we obtain where  X  G,m is the prior P ( G | m ) P ( m ) P (  X  R G,m
A graphical model (GM) corresponding to the history-reformulation model is shown in Fig. 1. The idea of reformulation transitivity is embodied in the dependence of q and a through R , even if we removed the direct edge from a to q .
Figure 1: GM representation of the history-reformulation model.
To implement eq. 4 into an algorithm that takes advantage of an optimized pairwise similarity model, we approximate P ( r | q, a ) by P ( r | a ) P ( r | q ) . This also addresses the data sparsity problem in estimating P ( r | q, a ) . Finally, we use a uniform prior P ( a ) and approximate the summation in eq.4 by a maximization (we pick a minimum query-history similarity threshold, t , so that R = { h  X  ( q, h ) ) and leave the implementation of the full Bayesian formu-lation for future work. With a log-space transformation and intro-duction of a history weight, w h , and history cutoff,  X  (if no history query passes the  X  bar of relevance to q , we drop the history con-tribution altogether), we obtain alg. 1.
 Algorithm 1 Transitive disambiguation algorithm 1: 2:
Another simple approach we propose to leverage history for query reformulation is based on cosine similarity in the vector space whose axes are the degrees of similarity to each query in the search his-tory: a pair ( q, a ) has high cosine similarity if q and a tend to match the same history H similarly. 1
For comparison, we have also implemented the Markov chain random walk method proposed in [1]. A random walk is performed on a query similarity graph, where the nodes are the current query, the user X  X  search history, and candidate rewrites. ( q, a ) pairs strongly connected to the same history queries have their similarity scores boosted and vice-versa for pairs with weak connections.
We evaluate our algorithms on a sample of 600 history-reformulation examples assessed by editors as good or bad, and on 2M automatically-labeled examples. The examples are randomly sampled across all Yahoo search users over a period of one month. Automatic label-ing is done by calculating reformulation click-through rates (CTR) on a separate large commercial search engine log, and labeling ex-amples with higher than average (normalized by position on the re-sults page) CTR as good reformulations. We summarize the history into a most relevant bag of words to obtain reliable CTR estimates. Search history can go back as far as one month.
We also extend this approach to the history-rewrite vector space to encourage rewrite set cohesiveness by favoring rewrites with high similarity to each other. Figure 2: Results on hand-labeled example set with varying history
Fig. 2 shows improvements on editorial data when history is used to varying degrees (the no-history baseline, w h = 0 , has accuracy 56%). The cosine similarity ( X 2.3) method has 58% accuracy on the same data, lower than the all-history extreme ( w h = 1 ) but higher than the no-history baseline. This reinforces the importance of indirect query-reformulation similarity. Similar results were ob-served using the Markov chain method (results did not improve with increasing random walk iterations).

Testing on the much larger automatically labeled data, the dis-ambiguation algorithm shows similar statistically significant gains (60% to 64%). An early version of the disambiguation algorithm was also run on live traffic over a week resulting in a 1% CTR lift X  X  significant result for a heavily optimized system.
We introduce a general history-reformulation probabilistic model and build a simple disambiguation approach on top of an optimized query-pair similarity function. Compared to the recent vlHMM model in [3], our approach presents the advantages that (i) refor-mulations are not restricted to history-based rewrites but can be derived from arbitrary sources (in our case, from search session s, bipartite query-url graphs, and query segment substitutions); (ii) the model can handle long mixed-goal sessions and put a prior on its subsets importance; no history segmentation is imposed (iii) the algorithm is less prone to sparsity problems an applies to rare queries/sessions.

Our algorithm does not require retraining. This is useful to easily apply it to a different setting, such as query-document matching, but can be a limitation in terms of obtaining the best accuracy.
We thank the reviewers for their extensive feedback.
