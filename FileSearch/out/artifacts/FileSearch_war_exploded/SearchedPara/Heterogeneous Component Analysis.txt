 Microarray and other high-throughput measurement devices have been applied to examine speci-mens such as cancer tissues of biological and/or clinical in terest. The next step is to go towards combinatorial studies in which tissues measured by two or mo re of such devices are simultaneously analyzed. However, such combinatorial studies inevitably suffer from differences in experimental conditions, or, even more complex, from different measurem ent technologies. Also, when concate-nating a data set from different measurement sources, we oft en observe systematic missing parts in a dataset (e.g., Fig 3A). Moreover, the noise levels may va ry among different experiments. All these induce a heterogeneous structure in data, that needs to be treated appropriately. O ur work will contribute exactly to this topic, by proposing a Bayesian me thod for feature subspace extraction, called heterogeneous component analysis (HCA, sections 2 a nd 3). HCA performs a linear feature extraction based on matrix factorization in order to obtain a sparse and structured representation. After relating to previous methods (section 4), HCA is appli ed to toy data and more interestingly to neuroblastoma data from different measurement techniqu es (section 5). We obtain interesting factors that may be a first step towards better biological mod el building. Let a matrix Y = { y vectors, where y assume the M -dimensional feature vector is decomposed into L disjoint blocks. Let I ( l ) denote a l 6 = l  X  .
 Figure 1: An illustration of a typical dataset and the result by the HCA. The observation matrix Y consists of multiple samples j = 1 , . . . , N with high-dimensional features i  X  I . The features whose distribution is highly structural depending on each b lock. HCA optimally factorizes the ma-trix Y so that the factor-loading matrix U has structural sparseness; it includes some regions of zero elements according to the block structure of the observed da ta. Each factor may or may not affect all rank of each factor-loading sub-matrix for each block (or an y set of blocks) can be different from the others. The resulting block-wise sparse matrix reflects a characteristic heterogeneity of features over blocks.
 We assume that the matrix Y  X  R M  X  N is a noisy observation of a matrix of true values X  X  R M  X  N whose rank is K ( &lt; min( M, N )) and has a factorized form: where E  X  R M  X  N , U  X  R M  X  K , and V  X  R N  X  K are matrices of residuals, factor-loadings, and factors, respectively. The superscript T denotes the matri x transpose. There may be missing or unmeasured observations denoted by a matrix W  X  X  0 , 1 } M  X  N , which indicates observation y missing if w Figure 1 illustrates the concept of HCA. In this example, the observed data matrix (left panel) is ties, missing rates, observation noise levels, and so on, wh ich we overall call heterogeneity . Such heterogeneity affects the effective rank of the observatio n sub-matrix corresponding to each block, and hence leads naturally to different ranks of factor-load ing sub-matrix between blocks. In ad-dition, there can exist block-wise patterns of missing valu es (shadowed rectangular regions in the left panel); such a situation would occur, for example in bio informatics, when some particular genes have been measured in one assay (constituting a block) but no t in another assay (constituting another block).
 To better understand the objective data based on the feature extraction by matrix factorization, we assume a block-wise sparse factor-loading matrix U (right panel in Fig.1). Namely, the effective rank of an observation sub-matrix corresponding to a block i s reflected by the number of non-zero components in the corresponding rows of U . Assuming such a block-wise sparse structure can decrease the model X  X  effective complexity, and will descri be the data better and therefore lead to better generalization ability, e.g., for missing value pre diction. Model For each element of the residual matrix, e distribution with a common variance  X  2 where l ( i ) denotes the pre-determined block index to which the i -th feature belongs. For a factor matrix V , we assume a Gaussian prior: The above two assumptions are exactly the same as those for pr obabilistic PCA that is a special case of HCA with a single active block. Another special case where each block contains only one active feature is probabilistic factor analysis (FA). Namely, max imum likelihood (ML) estimation based on the following log-likelihood includes both the PCA and th e FA as special settings of the blocks.  X  2 = (  X  2 summation P Another character of the HCA model is the block-wise sparse f actor-loading matrix, which is im-plemented by a prior for U , given by where T = { t t the possible block-wise mask patterns; a binary pattern vec tor whose dimensionality is the same as the factor-loading vector, and whose values are consistent , either 0 or 1 , within each block. When there are L blocks, each column vector of T can take one of 2 L possible patterns including the zero vector, and hence, the matrix T with K columns can take one of 2 LK possible patterns. Parameter estimation We estimated the model parameters U and V by maximum a posteriori maximized w.r.t. U , V and  X  .
 Maximization of the log-joint L w.r.t U , V , and  X  was performed by the conjugate gradient algo-rithm that was available in the NETLAB toolbox [1]. The stati onary condition w.r.t. the variance, where mean the objective function with the closed form solution plugge d in: maximization of L w.r.t. U , V , and  X  2 .
 Model selection The mask matrix T was determined by maximization of the log-marginal likeli-hood R L d U d V which was calculated by Laplace approximation around the MA P estimator: The log Hessian term, lndet H , which works as a penalty term for maintaining non-zero elem ents in the factor-loading matrix, was simplified in order for tract able calculation. Namely, independence in the log-joint was assumed: which enabled a similar tractable computation to variation al Bayes (VB) and was expected to pro-duce satisfactory results.
 To avoid searching through an exponentially large number of possibilities, we implemented a greedy search that optimizes each of the column vectors in a step-wi se manner, called HCA-greedy algo-rithm. In each step of the HCA-greedy algorithm, factor-loa ding and factor vectors are estimated maximum log-marginal. It terminated if zero vector is accep ted as the best mask vector. HCA with ARD The greedy search still searches 2 L possibilities per a factor, whose computation increases exponentially as the number of blocks L increases. The automatic relevance determination (ARD) is a hierarchical Bayesian approach for selecting rel evant bases, which has been applied to component analyzers since its first introduction to Bayesia n PCA (BPCA) [2].
 The prior for U is given by where  X  all elements of  X  function becomes According to this ARD approach,  X  is updated by the conjugate gradient-based optimization si -multaneously with U and V . In each step of the optimization,  X  was updated until the stationary condition of log-marginal w.r.t.  X  approximately held.
 In HCA with ARD, called HCA-ARD, the initial values of U and V were obtained by SVD. We also examined an ARD-based procedure with another initial value setting, i.e., starting from the result obtained by HCA-greedy, which is signified by HCA-g+ARD. In this work, the ideas from both probabilistic modeling of l inear component analyzers and sparse matrix factorization frameworks are combined into an analy tical tool for data with underlying het-erogeneous structures.
 The weighted low-rank matrix factorization (WLRMF) [3] has b een proposed as a minimization problem of the weighted error: where w as w ance 1 /w indeterminacy between U and V , and hence the resultant low-rank matrix U V T is identical to that by WLRMF.
 Bayesian PCA [2] is also a matrix factorization procedure, w hich includes a characteristic prior density of factor-loading vectors, ln p ( U |  X  ) =  X  1 Figure 2: Experimental results when applied to an artificial data matrix. (A) Missing pattern of the observation matrix. Vertical and horizontal axes correspo nd to row (typically, genes) and column (typically, samples) of the matrix (typically, gene expres sion matrix). Red cells signify missing elements. (B) True factor-loading matrix. Horizontal axis denotes factors. Color and its intensity factor-loading matrices estimated by SVD, WLRMF, BPCA, HCA-greedy, HCA-ARD, and HCA-g+ARD, respectively. The vertical line in panel (F) denotes the automatically determined number of components. Panel (I) shows missing value prediction per formance obtained by the three HCA algorithms and other methods. The vertical and horizontal a xes denote normalized root mean square of test errors and dimensionalities of factors, respective ly.
 HCA-ARD (eq. (10)) if we assume only a single block. Although this prior term obviously a simple L 2 norm in the WLRMF, it also includes hyper parameter  X  which constitute different regularization term and it leads to automatic model (intrinsic dimensional ity) selection when  X  is determined by evidence criterion.
 Component analyzers with sparse factor-loadings have rece ntly been investigated as sparse PCA (SPCA). In a well established context of SPCA studies (e.g. [ 4]), the tradeoff problem is solved between the understandability (sparsity of factor-loadin gs) and the reproducibility of the covariance matrix from the sparsified factor-loadings. In our HCA, the b lock-wise sparse factor-loading matrix is useful not only for understandability but also for genera lization ability. The latter merit comes noises, and missing observations, which have not been consi dered sufficiently in SPCA. Experiment 1: an artificial dataset We prepared an artificial data set with an underlying block structure. For this we generated a 170  X  9 factor-loading matrix U that included a pre-determined block structure (white vs. colored in Fig. 2(B)), and a 100  X  9 factor matrix V by applying orthog-onalization to the factors sampled from a standard Gaussian distribution. The observation matrix Y was produced by U V T + E , where each element of E was generated from a standard Gaussian. Then, missing values were artificially introduced accordin g to the pre-determined block structure (Fig. 2(A)). We applied three HCA algorithms: HCA-greedy, HCA-ARD, and H CA-g+ARD, and three existing matrix factorization algorithms: SVD, WLRMF and BPCA.
 SVD SVD calculated for a matrix whose missing values are imputed to zeros.
 WLRMF[3] The weights were set 1 for the value-existing entries or 0 for the missing entries. BPCA WLRMF with an ARD prior, called here BPCA, which is equivalent to HCA-ARD except The generalization ability was evaluated on the basis of the estimation performance for artificially accuracies are shown in Figure 2. Factor-loading matrices b ased on WLRMF and BPCA were sparsity in the factor-loading matrix.
 The HCA-greedy algorithm terminated at K = 10 . The factor-loading matrix estimated by HCA-greedy showed an identical sparse structure to the one consi sting of the top five factors in the true factor-loadings. The sixth factor in the second block was no t extracted, possibly because the second block lacked information due to the large rate of missing val ues. This algorithm also happened to extract one factor not included in the original factor-load ings, as the tenth one in the first block. Although the HCA-ARD and HCA-g+ARD algorithms extracted go od ones as the top three and four shown in panel (I), however, such a poorly extracted structu re did not increase the generalization error, implying that the essential structure underlying th e data was extracted well by the three HCA-based algorithms.
 Reconstruction of missing values was evaluated by normaliz ed root mean square errors: NRMSE def = p is the average over all the missing entries and the variance i s for all entries of the matrix. Figure 2(I) shows the generalization ability of missing val ue predictions. SVD and WLRMF, which incurred no penalty on extracting a large number of factors, exhibited the best results around K = 9 , but got worse with the increase in the number of K due to over-fitting. HCA-g+ARD showed the best performance at K = 9 , which was better than that obtained by all the other methods . HCA-greedy, HCA-ARD, and BPCA exhibited comparative performan ce at K = 9 . At K = 2 , . . . , 8 , the HCA algorithms performed better than BPCA. Namely, the spar se structure in the factor-loadings tended to achieve better performance. HCA-ARD performed le ss effectively than the other two HCA that HCA-g+ARD employing good initialization by HCA-greed y exhibited the best performance among all the HCA algorithms. Accordingly, HCA showed a bett er generalization ability with a smaller number of effective parameters than the existing me thods.
 Figure 3: Analysis of an NBL dataset. Vertical axes denote hi gh-dimensional features. Features measured by array CGH technology are sorted in the chromosom al order. Microarray features are Missing pattern in the NBL dataset. White and red colors denot e observed and missing entries in the data matrix, respectively. (B) and (C) Factor-loading m atrices estimated by the HCA-greedy and WLRMF algorithms, respectively.
 Experiment 2: a cross-analysis of neuroblastoma data We next applied our HCA to a neu-roblastoma (NBL) dataset consisting of three data blocks ta ken by three kinds of high-throughput genomic measurement technologies.
 Array CGH Chromosomal changes of 2340 DNA segments (using 2340 probes ) were measured Microarray 1 Expression levels of 5340 genes were measured for 136 tumors from NBL patients. Microarray 2 Gene expression levels in 25 out of 136 tumors were also measu red by a small-sized The dataset Microarray 1 was the same one as used in the previo us study [6], and the other two study. As seen in Figure 3(A), the set of measured samples was quite different in the three experi-ments, leading to apparent block-wise missing observation s. We normalized the data matrix so that the block-wise variances become unity. We further added 10% missing entries randomly into the observed entries in order to evaluate missing value predict ion performance.
 When HCA-greedy was applied to this dataset, it terminated at K = 23 , but we continued to obtain further factors until K = 80 . Figure 3(B) shows the factor-loading matrix from K = 0 to 23 . HCA-greedy extracted one factor showing the relationship b etween the three measurement devices and three factors between aCGH and Microarray 1. The other fa ctors accounted for either of aCGH or Microarray 1. The first factor was strongly correlated wit h patient X  X  prognosis as clearly shown by the color code in the parts of Microarrays 1 and 2. Note that the features in these two datasets are aligned by correlations to the prognosis. This suggests that the dataset Microarray 2 did not WLRMF extracted the identical first factor to HCA-greedy, but extracted much more factors con-cerning Microarray 2, all of which may not be trustworthy bec ause the number of samples observed in Microarray 2 was as small as 25. Figure 4: Missing value prediction performance by the six al gorithms. Vertical axis denotes nor-malized root mean square of training errors (A) or test error s (B and C). Horizontal axis denotes the number of factors (A and B) or the number of non-zero elements in the factor-loading matrices (C). Each curve corresponds to one of the six algorithms.
 We also applied SVD, WLRMF, BPCA and other two HCA algorithms t o the NBL dataset. For WLRMF, BPCA, HCA-ARD, and HCA-g+ARD, the initial numbers of f actors were set at K = as a measurement value of generalization performance. Note that the original data matrix included many missing values, but we evaluated the performance by usi ng artificially introduced missing values. Figure 4 shows the results.
 Training errors almost monotonically decreased as the numb er of factors increased (Fig. 4A), in-dicating the stability of the algorithms. The only exceptio n was HCA-ARD whose error increased from K = 30 to K = 40 ; this was due to local solution, because HCA-g+ARD employin g the same algorithm but starting from different initialization showed consistent improvements in its per-formance.
 Test errors did not show monotonic profiles except that HCA-g reedy exhibited monotonically better results for larger K values (Fig. 4B and C). SVD and WLRMF exhibited the best perfor mance at
K = 22 and K = 60 , respectively, and got worse as the number of factors increa sed due to over-fitting.
 emphasize, however, that HCA yields a clearer factor structure that is easier interpretable from the biological point of view. Complex structured data are ubiquitous in practice. For ins tance, when we should integrate data derived from different measurement devices, it becomes cri tically important to combine the infor-mation in each single source optimally  X  otherwise no gain ca n be achieved beyond the individual analyses. Our Bayesian HCA model allows to take into account such structured feature vectors factorization framework was applied to toy data and to neuro blastoma data collected by multiple high-throughput measurement devices which had block-wise missing structures due to different experimental designs. HCA achieved a block-wise sparse fac tor-loading matrix, representing the information amount contained in each block of the dataset si multaneously. While HCA provided a better or similar missing value prediction performance th an existing methods such as BPCA or WLRMF, the heterogeneous structure underlying the problem w as clearly captured much better. Furthermore the HCA factors derived are an interesting repr esentation that may ultimately lead to a better modeling of the neuroblastoma data (see section 5).
 In the current HCA implementation, block structures were as sumed to be known, as for the neu-roblastoma data. Future work will go into a fully automatic e stimate of structure from measured multi-modal data and the respective model selection techni ques to achieve this goal. Clearly there is an increasing need for methods that are able to reliably extract factors from multi-modal structured data with heterogeneous features. Our fut ure effort will therefore strive towards applications beyond bioinformatics and to design novel str uctured spatio-temporal decomposition methods in applications like electroencephalography (EEG ), image and audio analyses. Acknowledgement This work was supported by a Grant-in-Aid for Young Scientis ts (B) No. 19710172 from MEXT Japan.
 [1] I. Nabney and Christopher Bishop. Netlab: Netlab neural network software. [2] C.M. Bishop. Bayesian PCA. In Proceedings of 11th conference on Advances in neural infor-[3] N. Srebro and T. Jaakkola. Weighted low rank matrix appro ximations. In Proceedings of 20th [4] A. d X  X spremont, F. R. Bach, and L. El Ghaoui. Full regular ization path for sparse principal [5] S. Oba, M. Sato, I. Takemasa, M. Monden, K. Matsubara, and S. Ishii. A Bayesian missing
