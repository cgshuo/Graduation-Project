 analyses, i.e. , it would enable us to fit models involving more attributes and/or estimate models more accurately (with lower standard errors of estimates). At the same time, concerns about data confidentiality pose strong legal, regu-latory or even physical barriers to literally integrating the databases. These concerns are present even if the database  X  X wners X  are cooperating: they wish to perform the analy-sis, and none of them is specifically interested in breaking the confidentiality of any of the others X  data. This need to balance the utility of better combined analyses with the risk of privacy violation has received considerable interest lately [1, 5]. Specifically, consider two cases
The results of such analyses may be either used by the database owners themselves or disseminated more widely.
In this paper, we show how to perform secure linear regres-sion for  X  X ertically partitioned data X . (The problem of hori-zontally partitioned data is addressed in a companion paper [9]). Our work is similar in spirit to [12, 13] who describe methods for doing cluster analysis and association rule dis-covery in vertically partitioned data. The recent work by Du, et al [6] for the 2-agency setting also presents a comple-mentary approach for conducting secure linear regressions. The practical implentation of the algorithm we present is simpler since it can be implemented solely as secure data transfer among the agencies (it does not rely on circuit rep-resentations which implicitly involve a trusted  X  possibly software  X  third party), and it also permits some basic model agencies, A 1 ,A 2 ,...,A K , are involved. Agency A j possesses d columns of the predictor attributes ( x  X  X ), and I j denotes the index set of A j  X  X  predictors. In addition, we assume that all agencies know the  X  X esponse X  attribute, y .(We believe this is not strictly necessary; but if this is not so, we need to add another layer of security. The details of this are quite complex and they also raise some additional subtle issues, such as information asymmetry. We will deal with these issues in a separate paper.) Also, if u has components ( u 1 ,...,u m ), we will use u I j as shorthand for { u i } i  X  I j .The following example clarifies this notation.
 Example 1. If K =3 agencies are involved, and if agency A 1 knows x 1 , x 2 , x 3 , A 2 knows x 4 , x 5 , x 6 ,and A 3 knows x , x 8 , x 9 .Then d 1 = d 2 = d 3 =3 , I 1 = { 1 , 2 , 3 } I 2 = { 4 , 5 , 6 } and I 3 = { 7 , 8 , 9 } .Also, and
We consider the case where agencies A 1 ,A 2 ,...,A K col-lectively wish to compute  X  without sharing their possi-bly confidential data (they are also assumed to be unwill-ing to share summary statistics that relate their data to the other agencies X  data, such as correlations between at-tributes). Calculation of  X  using equation (6) requires the sharing of at least some summary statistics. We develop a strategy for a distributed computation of  X  using direct nu-merical minimization of E (  X  ). In our scheme, each agency A j will be able to obtain its component  X   X  I j of the global estimate  X   X  without revealing its data to any of the other agencies. It is assumed that all the agencies will share their  X   X 
I j with the other agencies so that everyone benefits from the analysis (and also so that everyone has some incentive to participate in this exercise). In addition to  X   X  I j , all agen-cies also learn the vector of residuals,  X  e = y  X  X  X   X  ,asa by-product of our procedure. They could use  X  e to conduct basic diagnostic tests about the regression model.
We now note some finer points related to our setup before proceeding with the details.
 Remark 1: As in other data sharing protocols, we require Remark 2: The databases need to have a common primary Remark 3: We assume that the attribute sets do not over-In our regession case (equation (6)), the  X  that minimizes readily obtained as
Consider K&gt; 2 cooperating, such that Agency j has a value v j , and suppose that the agencies wish to calculate v = K j =1 v j in such a manner that each Agency j can learn only the minimum possible about the other agencies X  values, namely, the value of v (  X  j ) = = j v . The secure summa-tion protocol [2, 4] can be used to effect this computation.
Choose m to be a very large number which is known to all the agencies such that v is known to lie in the range [0 ,m ). Agency 1 is assumed to be the leader. The remaining agen-cies are numbered 2 ,...,K . Agency 1 generates a random number R , chosen uniformly from [0 ,m ). Agency 1 adds R to its local value v 1 , and sends the sum s 1 =( R + v 1 )mod m to Agency 2. Since the value R is chosen uniformly from [0 ,m ), Agency 2 learns nothing about the actual value of v .

For the remaining agencies j =2 ,...,k  X  1, the algorithm is as follows. Agency j receives from which it can learn nothing about the actual values of v 1 ,...,v j  X  1 .Agency j then computes and passes on to Agency j +1
Finally, agency K adds v K to s K  X  1 (mod m ), and sends the result s K to agency 1. Agency 1, which knows R then calculates v by subtraction: and shares this value with the other agencies. This method for secure summation faces an obvious problem if, contrary to our assumption, some agencies collude. However, there exist collusion-resistant versions of secure summation that involve each agency partitioning their constribution to the sum and all agencies conducting multiple rounds of the sum-mation process to obtain the required sum [4, 9] (the order of the agencies could also be permuted to add an extra layer of security) 2 .
Our algorithm is essentially Powell X  X  algorithm implemented in such a manner that each agency A j updates its own com-ponents of the  X   X  X  and its own components of the search directions based on the data attributes it owns and one n -dimensional vector common to all agencies that is computed using secure summation. The details are as follows.
We thank two referees for pointing this out. 3. For r =1 , 2 ,..., ( p  X  1): Each A j updates s ( r ) I j  X  5. z , w and  X  are computed as before, and each A j  X  X  up-
Consider any one step of the iteration, the only common information exchanged by the agencies are the z and w vec-tors. Since each component of the vectors is computed using secure summation, the sources of disclosure threat for each component is the same as in using the secure summation protocol. However, the actual risk to the data x is less since there is some masking with components of the s vectors. Specifically, the vulnerability is highest in the first step of the iteration since (due to the way we have chosen the initial s ) only one agency contributes to the sum w at each round of the basic iteration block. We avoid risk of disclosure by having on the contributing agency compute  X  privately and announcing it to the others. If we assume that the agencies select their initial bases completely at random so that it is impossible for the others to guess it, and if the summation has been properly conducted in a secure manner according to the secure-summation protocol, then no private information is revealed if only z and w are common knowledge. If each iteration were independent, then the procedure is clearly se-cure. However, the values that each agency contributes to the sum are functionally related from one iteration to the next. This relation is complex and difficult to easily express. We expect that this complexity combined with the form of the secure sum protocol will make it impossible for malicious agencies to exploit the iteration-to-iteration dependency of the values to compromise data privacy.

In general we also observe that the data for agencies who have a larger number of variables is more secure since the components of X I j s ( r ) I j involve summing over a larger num-
After the regression coefficients are shared the agencies learn at least three useful quantities: 1. The global coefficients ,  X   X  . This enables the individ-2. The vector of residuals , e = y  X  X  X   X  is also known
We have presented a privacy preserving linear regession analysis algorithm that permits agencies to obtain the global regression equation as well as perform rudimentary goodness-of-fit diagnostics without revealing their data.

The local numerical computations to be conducted by each party are simple and efficient. The obvious dominant restricting factor in the computational effort required is con-ducting the O ( np 2 ) secure computations. We need to con-duct experiments to evaluate the limits of this method both inters of the scale of the data as well as the number of agen-cies
There are some situations that the agencies need to be aware of.
Outside the privacy scenario, our procedure also appears to be useful as a method for computing a common regression equation when the data reside in distributed databases. This application for distributed computation or as a strategy for a scalable method for linear regression is worth exploring further.

Our procedure also illustrates a more generally useful strat-egy for devising privacy-preserving data mining methods: Fitting of most data mining models involves the estima-tion of the parameters of the model by solving an optimiza-tion problem. Well-established models have well-established standard procedures for solving the optimization problem. Our approach indicates that exploring other non-standard methods for carrying out the optimization might lead to a method that is more suitable in the privacy preserving set-ting.
This research was supported by NSF grant EIA X 0131884 to the National Institute of Statistical Sciences. We wish to thank Max Buot, Christine Kohonen and four referees for useful discussion and comments. [1] ACM. SIGKDD Explorations , volume 4, December [2] J. Benaloh. Secret sharing homomorphisms: Keeping [3] R. Brent. Algorithms for minimization without [10] M. Powell. An efficient method for finding the [11] W. H. Press, S. A. Teulosky, W. T. Vetterling, and [12] J. Vaidya and C. Clifton. Privacy preserving [13] J. Vaidya and C. Clifton. Privacy preserving k-means [14] S. Weisberg. Applied Linear Regression . Wiley, 1985. [15] Y. Xing, M. G. Madden, J. Duggan, and G. J. Lyons.
