 Phonological features have received attention as a linguistically-based representation for sub-word in-formation in automatic speech recognition. These sub-phonetic features allow for a more refined repre-sentation of speech by allowing for temporal desyn-chronization between articulators, and help account for some phonological changes common in sponta-neous speech, such as devoicing (Kirchhoff, 1999; Livescu, 2005). A number of methods have been de-veloped for detecting acoustic phonological features and related acoustic landmarks directly from data using Multi-Layer Perceptrons (Kirchhoff, 1999), Support Vector Machines (Hasegawa-Johnson et al., 2005; Sharenborg et al., 2006), or Hidden Markov Models (Li and Lee, 2005). These techniques typically assume that acoustic phonological feature events are independent for ease of modeling.
In one study that broke the independence assump-tion (Chang et al., 2001), the investigators devel-oped conditional detectors : MLP detectors of acous-tic phonological features that are hierarchically de-pendent on a different phonological class. In (Ra-jamanohar and Fosler-Lussier, 2005) it was shown that such a conditional training of detectors tended to have correlated frame errors, and that improve-ments in detection could be obtained by training joint detectors. For many features, the best detector can be obtained by collapsing MLP phone posteriors into feature classes by marginalizing across phones within a class. This was shown only for frame-level classification rather than phone recognition.
Posterior estimates of phonological feature classes, as in Table 1, particularly those derived from MLPs, have been used as input to HMMs (Launay et al., 2002), Dynamic Bayesian Networks (DBNs) (Frankel et al., 2004; Livescu, 2005), and Conditional Random Fields (CRFs) (Morris and Fosler-Lussier, 2006). Here we evaluate phonological feature detectors created from MLP phone posterior estimators (joint feature models) rather than the independently trained MLP feature detectors used in previous work. CRFs (Lafferty et al., 2001) are a joint model of a label sequence conditioned on a set of inputs. No independence is assumed among the input; the CRF model discriminates between hypothesized la-bel sequences according to an exponential function of weighted feature functions: where P ( y | x ) is the probability of label sequence y given an input frame sequence x , i is the frame index, and S and T are a set of state feature functions and a set of transition feature functions, defined as: where  X  and  X  are weights determined by the learn-ing algorithm. In NLP applications, the component feature functions s binary indicator functions indicating the presence or absence of a feature, but in ASR applications it is more typical to utilize real-valued functions, such as those derived from the sufficient statistics of Gaus-sians (e.g., (Gunawardana et al., 2005)).

We can use posterior estimates of phone classes or phonological feature classes from MLPs as feature functions (inputs) within the CRF model. A more detailed description of this CRF paradigm can be found in (Morris and Fosler-Lussier, 2006), which shows that the results of phone recognition using CRFs is comparable to that of HMMs or Tandem systems, with fewer constraints being imposed on the model. State feature functions in our system are defined such that s  X ,f ( y i , x , i ) = where the MLP output for feature f at time i is N N f ( x i ) . This allows for an association between a phone  X  and a feature f (even if f is traditionally not associated with  X  ).

In this study, we experiment with different meth-ods of generating these feature functions. In various experiments, they are generated by training MLP phone detectors, by evaluating the feature informa-tion inherent in the MLP phone posteriors, and by training independent MLPs to detect the various fea-tures within the classes described. The use of CRFs allows us to explore the dependencies among feature classes, as well as the usefulness of phone posteriors versus feature classes as inputs. We use the TIMIT speech corpus for all training and testing (Garofolo et al., 1993). The acoustic data is manually labeled at the phonetic level, and we propagate this phonetic label information to every frame of data. For the feature analyses, we employ a lookup table that defines each phone in terms of 8 feature classes, as shown in Table 1. We extract acoustic features in the form of 12th order PLP fea-tures plus delta coefficients. We then use these as inputs to several sets of neural networks using the ICSI QuickNet MLP neural network software (John-son, 2004), with the 39 acoustic features as input, a varying number of phone or feature class posteriors as output, and 1000 hidden nodes. The first experiment contrasts joint versus indepen-dent feature modeling within the CRF system. We compare a set of phonological feature probabilities derived from the phone posteriors (a joint model) with MLP phone posteriors and with independently trained MLP phonological feature posteriors.
The inputs to the first CRF are sets of 61 state fea-ture functions from the phonemic MLP posteriors, each function is an estimate of the posterior proba-Phn.  X  Feat. 66.45 67.94 Table 2: Results for Exp. 1: Phone and feature pos-teriors as input to the CRF phone recognition bility of one phone. The inputs to the second CRF model are sets of 44 functions corresponding to the phonological features listed in Table 1. The CRF models are trained to associate these feature func-tions with phoneme labels, incorporating the pat-terns of variation seen in the MLPs.

The results show that phone-based posteri-ors produce better phone recognition results than independently-trained phonological features. This could be due in part to the larger number of param-eters in the system, but it could also be due to the joint modeling that occurs in the phone classifier.
In order to equalize the feature spaces, we use the output of the phoneme classifier to derive phonolog-ical feature posteriors. In each frame we sum the MLP phone posteriors of all phones that contain a given feature. For instance, in the first frame, for the feature LOW, we sum the posterior estimates at-tributed to the phones aa, ae and ao . This is repeated for each feature in each frame. The CRF model is trained on these data and tested accordingly. The re-sults are significantly better (p  X  .001) than the previ-ous features model, but are significantly worse than the phone posteriors (p  X  .005).

The results of Experiment 1 confirm the hypoth-esis of (Rajamanohar and Fosler-Lussier, 2005) that joint modeling using several types of feature infor-mation is superior to individual modeling in phone recognition, where only phoneme information is used. The difference between the phone posteriors and individual feature posteriors seems to be related both to the larger CRF parameter space with larger input, and the joint modeling provided by phone posteriors. In the second experiment, we examine the influence of each feature class on the accuracy of the recog-nizer. We iteratively remove the set of state fea-ture functions corresponding to each feature class Class Removed Feats. Phn. Acc. Phn. Corr.
 * p  X  .05, different from no features removed Table 3: Results of Exp. 2: Removing feature classes from the input from the input to the CRF. The original functions are the output of the independently-trained feature class MLPs. The phone recognition accuracy for the CRF having removed each class is shown in Table 3. In Table 4 we show how removing each feature class affects the labeling of vowels and consonants.
Manner provides an example of the influence of a single feature class. Both the Accuracy and Correct-ness scores decrease significantly when features as-sociated with Manner are removed. Manner features distinguish consonants but not vowels, so the effect is concentrated on the recognition of consonants.
The results of Experiment 2 show that certain fea-ture classes are redundant from the point of view of phone recognition. In English, Round is correlated with Front. When we remove Round, the phonemes remain uniquely identified by the other classes. The same is true for the Sonority class. The results show that the inclusion of these redundant features is not detrimental to the recognition accuracy. Accuracy and Correctness improve non-significantly when the redundant features are included.

Clearly, the  X  X ndependent X  phonological feature streams are not truly independent. Otherwise, per-formance would decrease overall as we removed each feature class, assuming predictiveness.
Removal of Place causes a slight worsening of recognition of vowels. This is surprising, because Place does not characterize vowels. An analysis of the MLP activations showed that the detector for Place=N/A is a stronger indicator for vowels than is the Sonority=Vowel detector. This is especially true for the vowel ax , which is frequent in the data, Table 4: Effect of removing each feature class on recognition accuracy of vowels and consonants thus greatly influences the vowel recognition statis-tic. Removing the Place detectors leads to a loss in vowel vs. consonant information. This results in an increased number of consonant for vowel substitu-tions (from 560 to 976), thus a decrease in vowel recognition accuracy.

Besides extending the findings in (Rajamanohar and Fosler-Lussier, 2005), this provides a cautionary tale for incorporating redundant phonological fea-ture estimators into ASR: these systems need to be able to handle correlated input, either by design (as in a CRF), through full or semi-tied covariance ma-trices in HMMs, or by including the appropriate sta-tistical dependencies in DBNs. We have shown the effect of using joint model-ing of phonetic feature information in conjunction with the use of CRFs as a discriminative classifier. Phonetic posteriors, as joint models of phonological features, provide superior phone recognition perfor-mance over independently-trained phonological fea-ture models. We also find that redundant features are often modeled well within the CRF framework. The authors thank the International Computer Sci-ence Institute for providing the neural network soft-ware. The authors also thank four anonymous re-viewers. This work was supported by NSF ITR grant IIS-0427413; the opinions and conclusions ex-pressed in this work are those of the authors and not of any funding agency.

