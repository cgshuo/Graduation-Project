 Authorship attribution (AA) is a traditional problem and has been studied by many re-searchers [11,35,8,12,6]. The early work on AA focused on analyzing Shakespeare X  X  plays and Bronte Sisters X  novels. Later on, it was used to identify other literary works such as American and English literature and news articles. More recently, AA was ap-plied to online texts such as emails [37], blogs [21], forum posts [31] and reviews [17]. The problem of AA is useful in many applications such as forensic investigations, de-tection of copyright infringement and internet plagiarism.

Existing approaches on authorship attribution are mainly based on supervised clas-sification [39,34,6,17,29]. The major wea kness of this method is that, for each author a large number of his/her articles are necessary to be used as the training data. For ex-ample, book length texts were used to classify the authorship of Bronte Sisters X  novels [19]. However, in the real life, it is actually hard to collect sufficient labeled data. For instance, in most cases of forensics, only one small text is available for a specific au-thor, which are not enough to serve as the training data. The small number of labeled documents makes it extremely challenging fo r supervised learning to train an accurate classifier.

In this paper, we consider the problem of authorship attribution with very few labeled data. Little work has been done in this area. A similar problem was once attempted in [22]. However, the number of training samples in each class is greater than 115 and 129 for the small and the large data set, respectively, which is still very large. In contrast, we consider a much more difficult problem, where the size of training samples in our setting is extremely small, i.e., 10 samples per aut hor for training. Luyckx and Daelemans also evaluated the effect of training set size [24], but their algorithm was not particularly designed for coping with few training data.

We propose a new framework to address the authorship classification problem with limited training data using a co-training framework in this paper. Following the basic idea of co-training, i.e., utilizing two su fficient and redundant views on the data, we partition the documents into two natural parts of lexical and syntactic structures and build two classifiers separately on two views. The predictions of each classifier on un-labeled examples are used to augment the training set of the other. This process repeats until a termination condition is satisfied, and the enlarged labeled set is finally used to train a classifier and make predictions on the test data. By exploiting the redundancy in the human languages, we tackle the problem of very few training data. Experiments on a real world data set show that the proposed co-training framework can effectively incorporate the unlabeled data to help impr ove classification performance by a large margin.

The rest of this paper is organized as follows. Section 2 reviews related work. Sec-tion 3 presents our co-training method for authorship attribution. Section 4 provides experimental results. Finally , Section 5 concludes the paper. Authorship attribution has received a great deal of atten tion in recent years. A variety of approaches have been developed for this problem. Existing methods can be categorized into two main themes. One focuses on finding appropriate features for quantifying the authors X  writing style, and the other focuses on developing efficient and effective tech-niques to perform the classification task.

There is a body of literature examining the effects of different features. The use of function words could date back about half a century ago [26]. Since then, various fea-tures have been proposed for modeling writing s tyles. The features that have been in-vestigated include function words [1,2], length features [7,8], richness features [11,19], punctuation frequencies [8], character n-grams [9,12], word n-grams [28], POS n-grams [7,13], and rewrite rules [11].

There are also a number of works that study the use of machine learning methods in attribution. An early study used Bayesian statistical analysis [26], but later work focused exclusively on classification, including discriminant analysis [35], PCA [14], neural networks [8,39], multi-layer perceptrons [8], clustering [28], decision trees [36,38], and SVM [5,7,19,12]. Among them, SVM is regarded as one of the best approaches for solving this problem [23,17].
 The main problem in traditional research is the unrealistic size of the training set. Basically, a size of about 10,000 words per author is regarded to be a reasonable size [2,4,7]. Even when no long documents available, hundreds of short texts can be selected [10,13] such that the total amount of words is large enough for training. A recent work [24] introduced the problem of AA with limite d data. Instead of presenting an effective approach to deal with the problem, the aut hors only investigated the effect of limited data in authorship attribution. Several ensemble-based methods were also introduced [33,21] with the basic idea of feature set subspacing. While the subspacing technique is appropriate for high dimensional feature space and sparse data, it is not developed for handling the problem with very few labeled data. Co-training, on the other hand, is a representative learning mechanism which combines both labeled and unlabeled data un-der a two view setting [3,27]. Although the c o-training paradigm has been successfully employed in many areas, the problem of authorship attribution was rarely addressed using a co-training framework. Kourtis and Stamatatos once introduced a variant of the self-training method for AA [22]. However, the number of labeled documents is still very large in that work, i.e., about 115 and 129 documents per author on average. Moreover, the self-training method in [22] uses two classifiers on one view. In con-trast, we adopt a two-view co-training framework and an extremely small number of documents (10 documents per author) are used as the initial training data. The problem of authorship attribution can be defined as follows: Let A = { a 1 , ..., a k } be a set of k authors (classes) and D = { D 1 , ..., D k } be k sets of documents with D i being the document set of author a i represented as a feature vector. Each feature represents a piece of information about the document, e.g., a word or a syntactic tag. A model or classifier is then built from the training data and applied to the test data to determine the author a of each test document d ,where a is from A ( a  X  A ).

One of the main challenges for modern authorship attribution is the small number of training samples. To address this problem, we propose a co-training approach which integrates two views into one framework. Co-training is semi-supervised method. It begins with a small set of labeled data, and enlarges the labeled data set by adding unlabeled data. The key aspect of co-training algorithm is the property of two views. It has been shown that the redundancy in two views contributes more information than the single view in practice and theory [3,25]. Since human languages naturally consists of two parts: a lexicon and a grammar, we apply the co-training framework to authorship attribution with limited annotated documents. 3.1 The Overall Framework In the context of authorship attribution, each document has two views of features: fea-tures about lexical structure and features about syntactic structures. Hence two clas-sifiers can be co-trained using these two views. The overall framework is shown in Algorithm 1. ments U = { u 1 , ..., u s } , a set of test documents T = { t 1 , ..., t t } Parameters: the number of iterations k , the size of selected unlabeled documents u ,the number of top predicted unlabeled documents p Output: t i  X  X  class assignment ( t i  X  T ) Steps: 1. Extract lexical and syntactic views L l , L s , U l , U s , T l , T s for L , U ,and T . 2. Loop for k iterations: 3. Learn the first view classifier C l from L based on lexical features L l ; 4. Use C l to label t i in T based on T l ; 5. Learn the second view classifier C s from L based on syntactic features L s ; 6. Use C s to label t i in T based on T s ;
In Algo. 1, step 1 extracts two types of views from the labeled, unlabeled, and test data, respectively. Step 2 iteratively co-t rains two classifiers by adding the most accu-rately predicted data from the other view into labeled set. The algorithm first randomly selects a small set of u documents. Although we can directly select from the large un-labeled set U , it is shown [3] that a smaller pool can force the classifier C l and C s to select instances that are more representative of the underlying distribution that gener-ates U . Hence we set he parameter u to 100, which is about 1/80 percent of the whole unlabeled set. It then iterates for the following steps. First, use the lexical and syntactic view on current labeled set to train a classifier C l and C s , respectively. Second, allow each of these two classifiers to examine the unlabeled set U X  and select p samples it most confidently labels as positive. The exam ples selected by the lexical classifier C l are added to the document set of syntactic view L s with the label assigned, and those selected by the syntactic classifier C s are added to the document set L l of lexical view. Finally, the u documents are removed from the unlabeled pool U X  . Steps 3-6 are used to assign the test document to a category (author) using the classifier learned from the first and second view in the augmente d labeled date, respectively.

Once we get the prediction values from the different classifiers, some additional algorithms can be added to decide the final author attribution for each test document t . One simple general method is voting. Unfortunately, this method is not appropriate for this task as we only have two classifiers. There are also other methods, which can depend on what output value the classifier produces. Here we present two strategies. These methods require the classifier to produce a predicted score, which can reflect the positive and negative certainty. Many classification algorithms produce such a score, e.g., SVM, logistic regression, and na  X   X ve Bayesian. Here we use SVM as an example. For each test case, SVM outputs a positive or negative score which can be interpreted as the certainty that a test case is positive or negative.

The two methods are given below: 1. ScoreSum: The learned model/classifier is first applied to classify all test cases 2. ScoreMax: This method also works similarly except that it finds the maximum 3.2 Lexical Features The lexical features are used to compose a lexical view for a document. It is straight-forward to view a text article as a bag-of-words, like that has been widely used in topic-based text classification. We represent each article by a vector of word frequencies. The vocabulary size for word unigram in our experiment is 195274 . We do neither word stemming nor stop word removal as in text cat egorization. This is because some of the stop words are actually function words which have been demonstrated discriminative for authorship identification. In addition, stemming can be also harmful to information extraction of an author. 3.3 Syntactic Features The syntactic features are used to compose a syntactic view for a document. Existing studies have shown the usefulness of syntactic information in supervised authorship classification [7,13]. In this paper, we use four typical content-independent structures including n-grams of POS tags ( n =1 .. 3 ) and rewrite rules.

The syntactic features are extracted from the parsed syntactic trees. For example, the tree for sentence  X  X his is the best book in the set X  is as follows: (ROOT (S This tree contains 15 POS 1-grams: S NP DT VP VBZ NP NP DT JJS NN PP IN NP DT NN, 14 POS 2-grams: S|NP NP|DT DT|VP VP|VBZ ... DT|NN, 13 POS 3-grams: S|NP|DT NP|DT|VP DT|VP|VBZ ... NP|DT|NN, and 7 rewrite rules: S-&gt;NP+VP NP-&gt;DT NP-&gt;DT+JJS+NN NP-&gt;DT+NN PP-&gt;IN+NP NP-&gt;NP+PP VP-&gt;VBZ+NP
Each POS n-gram or rewrite rule is encoded like a single pseudo-word and assigned a unique number id. The vocabulary sizes for POS 1-grams, POS 2-grams, POS 3-grams, and rewrites in our experiment are 63 , 1917 , 21950 ,and 19240 , respectively. These four types of syntactic structures are merged into a single vector. Hence the syntactic view of a document is represented as a vector with 43140 components. In this section, we evaluate the proposed approach. We first introduce the experiment setup, and then present the results using different parameter settings. Finally we com-pare our results with four types of baselines. All our experiments use the SV M multiclass classifier [15] with default parameter settings. 4.1 Experiment Setup We conduct experiments on the IMDB data set [ 30]. This data set has 62,000 reviews by 62 users (1,000 reviews per user). It is publicly available upon the request to authors. We randomly select 10 authors for experiments. We do not use a large number of authors in our experiments because when the number of a uthors increases, the performance of su-pervised classification deteriorates quick ly even with many training instances [34,21]. Thus it is very difficult, if not impossible, to evaluate the effects of co-training algo-rithm. For each author, we further split his/he r documents into the l abeled, unlabeled, and test set, 1% of one author X  X  documents, i.e., 10 documents per author, are used for training, 79% are used as unlabeled data, and the rest 20% for testing. We extract and compute the lexical features directly from t he raw data, and we use the Stanford PCFG parser [18] to generate the grammar structure of sentences in each document for extract-ing syntactic features. We normalize each feat ure X  X  value to [0, 1] interval by dividing by the maximum value of this feature in the training set.
 We report classification accura cy as the evaluation metric.
 4.2 Baseline Methods We first implement the semi-supervised learning approach presented in [22]. This ap-proach self-trains two classifiers from t he character 3-gram view using CNG and SVM classifiers. Our results show that the performance of CNG+SVM is very poor. CNG is a profile-based method which builds the classification model based on the dissimilarity of the profile of the text from each of the profil es of the candidate authors. Its accuracy is only 5.80% with the original 10 training documents. And this directly leads to the failure of the whole self-training framework. Thus we do not list the results for this method due to the space limitation.

Now we give four other baseline methods.  X  SLR (Single Lexical Representation of documents): This is a widely used method  X  SSR (Single Syntactic Representation of documents): This is another popular ap- X  CLSR (Combined Lexical and Syntactic Representation of documents): This  X  SSLF (SubSpacing on Lexical Features of documents): This is an ensemble method 4.3 Experimental Results In this section, we exploit the parameter sensitivity and then compare co-training to other baseline methods.
 Effects of the Number of Iterations k We first evaluate the effects of the number of iterations k . Figure 1 depicts the plots of accuracy versus parameter k using the classifier by two single views as well as the two combining strategies.

We have the following observations from Figure 1.  X  The classification performan ce increases with a larger number of iterations. This accuracy By Lexicon By Syntax  X  The lexicon-based classifier performs the best among four approaches. It even out-Effects of the Number of the Top Predicted Unlabeled Documents p We evaluate the effects of number of the top predicted unlabeled documents. Figure 2 shows the results. In Fig. 2, we see an upward trend in general, which is similar to that in Figure 1, showing that more labeled examples result in better performance. We also notice that the lexicon-based classifier gets the best result when selecting 30 top-predicted samples. This can be due to the fact that the labeled set contains more noises when more documents are added. Note that the classifier can not always predict an unlabeled data correctly. And thus it brings down the performance. All our experiments below use the model trained with p = 30.
 Comparing with the Baselines Among the four baselines, SLR and SSR are one-view based classifier, and the CLSR and SSLF are combined method. We first compare the results of our co-training method under lexical and syntactic view with those of SLR and SSR. The results are summa-rized in Table 1, where the number n in CTLSVn denotes the number of iterations.
From Table 1, it is clear that the proposed co-training framework outperform the traditional supervised training by a large margin on both the lexicon-based classifier and the syntax based classifier. For example, after co-training 10 iterations, our CTLSV method using a lexicon based classifier r eaches an accuracy of 85.70% , significant bet-ter than that of SLR, which is only 63.85%. Similarly, the performance by syntax-based classifier increases from 50.05 to 75.75 after 30 iterations, showing a 51.35% lead.
Next, we compare our results on the combined classifier with CLSR and SSLF. The results are shown in Table 2.

In Table 2, the number n in CTLSVn denotes the number of iterations. And the num-ber m in SSLFm denotes the number of feature subset, which means that the unigrams are randomly partitioned into m (3 or 5) splits. Classifiers are trained and tested on each split of the labeled and test data using SVM, and then the final decisions are made using the ScoreSum and ScoreMax strategies as those used in our framework. For CLSR, we combine the lexicon and syntax feature vector into one long vector, and directly apply SVM to the labeled and test data. Note that we are unable to use this combination ap-proach in our CTLSV because we co-train classifiers on two different views, that is to say, the instances selected and added to one labeled set may be not same as those added to the other labeled set. Hence the final labeled set of L s is different from L l .
We highlight three important points from Table 2.  X  The ScoreMax strategy performs worse th an ScoreSum. This indicates that the de- X  The SSLF algorithm does not benefit more from a large number of partitions. In  X  The proposed CTLSV framework performs the best among all methods. The per-In this paper, we investigate the problem of authorship attribution with very few labeled data. We present a novel co-training framework which utilizes the two natural views in human languages, i.e., the lexical and syntactic views. By iteratively learning a small number of samples from unlabeled data, the proposed approach effectively augments the labeled set and increases the performan ce. Experimental results on a real data set show that our method can significantly improv e the classification accuracy. It outper-forms the state-of-the-art methods by a large margin.

Our current study focuses on co-training using two views and evaluates on a small number of authors. In the future, we plan to extend our work by integrating more views and experimenting with more authors. In addition, our current experiment only involves one data set. Further experiments are required to determine the general behavior of co-training framework for authorship attribution.
 Acknowledgements. The work described in this paper has been supported in part by the NSFC projects (61272275, 61232002, 61202036, 61272110, and U1135005), and the 111 project(B07037).

