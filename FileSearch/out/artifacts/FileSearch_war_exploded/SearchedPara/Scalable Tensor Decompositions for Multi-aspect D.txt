
Modern applications such as Internet traffic, telecommunication records, and large-scale social net-works generate massive amounts of data with multi-ple aspects and high dimensionalities. Tensors (i.e., multi-way arrays) provide a natural representation for such data. Consequently, tensor decompositions such as Tucker become important tools for summarization and analysis.

One major challenge is how to deal with high-dimensional, sparse data. In other words, how do we compute decompositions of tensors where most of the entries of the tensor are zero. Specialized techniques are needed for computing the Tucker decompositions for sparse tensors because standard algorithms do not account for the sparsity of the data. As a result, a sur-prising phenomenon is observed by practitioners: De-spite the fact that there is enough memory to store both the input tensors and the factorized output tensors, memory overflows occur during the tensor factorization process. To address this intermediate blowup problem, we propose Memory-Efficient Tucker (MET). Based on the available memory, MET adaptively selects the right execution strategy during the decomposition. We pro-vide quantitative and qualitative evaluation of MET on real tensors. It achieves over 1000X space reduction without sacrificing speed; it also allows us to work with much larger tensors that were too big to handle before. Finally, we demonstrate a data mining case-study us-ing MET.
Many applications generate large amounts high di-mensional data with multiple aspects, which are natu-rally represented as tensors, or multi-arrays. Some ex-amples include (a) email exchanges, (b) bibliographic data, (c) hyperlinks on the web, and (d) Internet net-work traffic flows. These examples can be naturally represented as tensors as follows: (a) can be modeled as a 4D array, or a fourth-order tensor, with sender, recipeient, keyword, and datestamp as the four modes; (b) can viewed as a author-conference-keyword third-order tensor [19]; (c) hyperlinks on the web yield a third-order tensor of sources by desinations by anchor text [15]; and (d) network traffic is a fourth-order ten-sor with source IP, destination IP, port number, and time [18, 19].

Beyond being multi-aspect and high-dimensional, another key characteristic associated with these exam-ples is sparsity , meaning that most of entries in the ten-sor are zeros. Sparsity is a common property in tensor data, e.g., Table 1 illustrates the density of datasets used in this paper. Therefore, it is much more efficient to store the nonzeros only; in fact, very large sparse tensors can be stored using only moderate hardware. For example, a 10K-by-10K-by-10K tensor with 1 mil-lion nonzeros out of 1 trillion elements can be formed in a personal laptop with only 32MB memory 1 . How-ever, the challenge is, given a large sparse tensor, how can it be efficiently analyzed with fixed memory? density .0025% .0187% .0008% .0286%
To analyze such tensor data, various tensor decom-positions are proposed in the literature (see [14]). The Tucker decomposition [21] is has been applied in many different domains such as web search mining [20], net-work forensics [19] and social network [8] (see Section 6 for an overview).

Despite its popularity, how to apply Tucker on a large sparse tensor is still an open problem. One sur-prising phenomenon is observed by practitioners: De-spite that both the input (huge, sparse) tensor and the output (small, dense, factorized) tensor can be stored in memory, memory overflows may occur during the Tucker decomposition process. We call this the inter-mediate blowup problem. Due to this problem, up to now most of work on Tucker has been focused on rel-atively small tensors (e.g., of the order of 100-by-100-by-100 or smaller).

To address the intermediate blowup problem, we propose a Memory-Efficient Tucker decomposition (MET), which maximizes the computation speed while optimally utilizing the available memory. MET avoids constructing large intermediate results by handling the computation in a piecemeal fashion, adaptively select-ing the order of operations. Not only does MET elimi-nate the intermediate blowup problem, it also achieves significant memory savings without any loss of accu-racy. In many cases, this can be done without compro-mising on speed and is sometimes even faster. For ex-ample, Figure 1 is an example comparison on 100K-by-100K-by-100K random tensor with 1M nonzeros. Our proposed method MET yields a 1000X saving on mem-ory and is twice as fast as the standard Tucker.
Figure 1. Comparison of Tucker versus MET on a
In summary, the contributions of this paper are as follows.  X  We demonstrate the severity of the intermedi- X  we propose an adaptive algorithm called MET for  X  we introduce a novel heuristics to prioritize the
In this section, we introduce the notation, define the key tensor operations required in this paper. All the definitions presented here come from [14]. Definition 2.1 (Tensor) A tensor is a multi-way ar-ray. The order of a tensor is the number of dimensions, also known as ways or modes.

Higher-order ( N -way with N  X  3) tensors are de-noted by boldface Euler script letters, e.g., X . Ma-trices (tensors of order two) are denoted by boldface capital letters, e.g., A ; vectors (tensors of order one) are denoted by boldface lowercase letters, e.g., a ; and scalars are denoted by lowercase letters, e.g., a . The i th entry of a vector a is denoted by a i , element ( i, j ) of a matrix A by a ij , and element ( i, j, k ) element of a third-order tensor X by x ijk . Indices typically range from 1 to their capital version, e.g., i = 1 , . . . , I . The n th element in a sequence is denoted by a superscript in parentheses. For example, v ( n ) denotes the n th vec-tor in a sequence and v ( n ) i denotes the i th element on the n th vector v ( n ) . Likewise, A ( n ) denotes the n th matrix in a sequence and a ( n ) r denotes the r th column of the matrix A ( n ) .
 Definition 2.2 (Norm of a Tensor) The norm of Definition 2.3 (Tensor Fiber) A tensor fiber is a one-dimensional fragment of a tensor, obtained by fix-ing all indices but one.

Tensor fibers are the higher-order analogue of matrix rows and columns. Third-order tensors have column, row, and tube fibers, denoted by x : jk , x i : k , and x respectively.
 Definition 2.4 (Tensor Slice) A tensor slice is a two-dimensional fragment of a tensor, obtained by fix-ing all indices but two.

For example, the horizontal, lateral, and frontal slides of a third-order tensor X are denoted by X i :: , X Definition 2.5 (Matricization) Matricization , also known as unfolding or flattening , is the process of re-ordering the elements of an N -way array into a matrix. Definition 2.6 (Mode-n matrix product) The n -mode matrix product of a tensor X  X  R I 1  X  I 2  X  X  X  X  X  I N of size I 1  X  X  X  X  X  I n  X  1  X  J  X  I n +1  X  X  X  X  X  I N . Element-wise, we have Multiple mode-n matrix products can be performed in any order (see, e.g., [13]), i.e., for m 6 = n . From [4], we adopt the following shorthand notation for multiplication in every mode: and for multiplication in every mode but one: X
X  X  Definition 2.7 (Mode-n vector product) The n -mode vector product of a tensor X  X  R I 1  X  I 2  X  X  X  X  X  I N with a vector v  X  R I n is denoted by X  X   X  n v and is of size I 1  X  X  X  X  X  I n  X  1  X  I n +1  X  X  X  X  X  I N . Element-wise, we have Note that the order of the result is reduced by one. Multiplying a three-way tensor by a vector in one mode results in a two-way tensor (a matrix). It is possible to multiple a tensor by a vector in more than one mode as well. Multiplying a three-way tensor by vectors in two modes results in a one-way tensor (a vector); mul-tiplying in all modes results in a scalar.
Many tensor decompositions have been proposed (see [14] for a detailed survey), among which, CAN-DECOMP/PARAFAC (CP) [7, 11] and Tucker decom-position [21] are the two most popular. Here we focus on the Tucker decomposition.
 Definition 2.8 (Tucker Decomposition) Let X be a tensor of size I 1  X  I 2  X  X  X  X  X  I N . A Tucker decom-position of X yields a core tensor G of specified size J 1  X  J 2  X   X  X  X   X  J N and factor matrices A ( n ) of size I n  X  J n for n = 1 , . . . , N such that The Tucker decomposition approximates a tensor as a smaller core tensor (i.e., a compressed version of the original tensor) times the product of matrices that span appropriate subspaces in each mode. Typically, the factor matrices A ( n ) are assumed to be orthogonal. There are many options for storing sparse tensors. Here, we use coordinate format as proposed in [6]. As-sume X is a sparse tensor of size I 1  X  I 2  X  X  X  X  X  I N with P = nnz( X ) non-zeros. We store the tensor as Here, the p th nonzero value is given by v p and its sub-script is given by the p th row of S , i.e., s p : . In other words, The total storage for a sparse tensor with P non-zeros is the ( N +1) P elements. This is the format implemented in the MATLAB Tensor Toolbox, version 2.0 [5], which is the framework in which we do all our testing. Here, we summarize the main point. Let X be an N -way tensor of size I 1  X  I 2  X   X  X  X   X  I N and assume that we are doing mode-n vector products in the modes specified by It is possible to calculate using only P additional memory elements and the stor-age required for Z , which is This calculation can be done with O (nnz( X )) memory and is described in detail in [6].
We are interested in the case where X is a large, sparse tensor and we wish to find a Tucker approxi-mation as in Equation (1) such that the (dense) core tensor G has much smaller dimensions than the original sparse tensor X ; in other words, we assume Moreover, we assume that the dimensions J n are small enough that the dense tensor G (with Q J n elements) can easily fit in memory.

In fitting Tucker, the goal is to find a core tensor G of specified size J 1  X  J 2  X  X  X  X  X  J N and factor matrices A ( n ) of size I n  X  J n for n = 1 , . . . , N that minimize:
The core tensor G is determined uniquely by the factor matrices. That is, given a fixed set of factor matrices, the optimal core is Here we assume { A } is orthogonal.

The naive way of computing the error e = k
X  X  G  X { A }k 2 is very expensive, since X  X  G  X { A } is a large dense tensor. Fortunately, it can be simplified as the follows: Through the above transformation, the objective func-tion in Equation (2) can be rewritten as k X k 2  X  X  G k 2 .
Since k X k is fixed, Equation (2) is equivalent to
A common approach for solving Equation (4) is to use an alternating least squares (ALS) method, solving for one factor matrix at a time while holding the others fixed. In other words, we fix all the factor matrices except A ( n ) and solve Setting Equation (5) can be rewritten as which is solved via the SVD of Y ( n ) . See [9, 13] for details.

This full Tucker-ALS algorithm is shown in Algo-rithm 1. The initialization procedure is standard (see [14] and reference therein). Note that the first factor matrix does not need to be initialized since it is the one solved for in the first inner iteration. The problem boils down to calculating the leading singular vectors of a large, sparse matrix, which is straightforward. We discuss the inner loops in more detail below. Also ob-serve that G , calculated once per outer loop, is calcu-lated using the Y from the last inner iteration. This is straightforward to compute, as is its norm.
 Algorithm 1 Tucker-ALS for N-mode tensors 5: end for 10: end for
The bottleneck in this method is in the inner loop in the calculation of Y ; see Equation (6). Here we need to compute the product of a sparse tensor times a series of dense matrices. However, the intermediate products are dense and may be too large to fit in memory even if the final tensor Y does fit. We assume that there is enough memory to fit Y , i.e., that there is enough memory to store elements and focus entirely on the problem of how to compute Y . Once Y is obtained, finding its leading left singular vectors is straightforward.
To simplify the discussion, we first focus on the prob-lem of calculating Y in Equation (6) for N = 3. Con-sider the following calculation (the size of each object is listed above it), which we encounter in the first inner iteration: We assume that I 1 , I 2 , I 3 are relatively large and J , J 2 , J 3 are relatively small; e.g., I n = 1000 and J n = 10 for n = 1 , 2 , 3.

MET(0): Standard calculation (no modes element-wise) The standard way to calculate Equa-tion (7) in Tucker-ALS is as follows, where the brackets indicate the order of operations: The first intermediate result, is a dense tensor of size I 1  X  I 2  X  J 3 and may easily be too big to fit in memory. Depending on the amount of memory available, there are two alternatives for calcu-lating Y with less memory.

MET(1): Slice updates (one mode element-wise). We can calculate Y from Equation (7) one slice at a time using less memory. Specifically, we handle the third mode element-wise and have intermediate result is a matrix of size I 1  X  I 2 and we need to do J 3 calculations. Of course, it is possible to instead handle the second mode element-wise:
MET(2): Fiber updates (two modes element-wise). We can calculate Y from Equation (7) one fiber at a time with even less memory. Specifically, two modes are handled element-wise: Here the largest intermediate result is a vector of size I 1 and we need to do J 2 J 3 calculations. 4.2 N -way tensors
To generalize to the N -way case, two questions need to be answered:  X  Given fixed memory, how can we quickly compute  X  Which modes should be treated element-wise? Memory-Efficient Tensor Times Matrices (METTM): Assume that we know which subset modes should be treated element-wise, denoted by Then the order of the intermediate sub-tensors that are formed is N  X  | E | . For example, if N = 4 and E = { 3 , 4 } , then the intermediate results are two-way tensors (matrices). Moreover, if E is non-empty, the size of the largest intermediate result is and the number of element-wise calculations that needs to be performed is Selecting the modes to handle element-wise: There is some choice in which modes to handle element-wise. For example, trivially we can minimize the mem-ory by having all modes handled element-wise, i.e., E = { 1 , 2 , . . . , N } . The other extreme, which standard Tucker-ALS uses, is to have empty E , leading to the intermediate blow-up problem for large-scale tensors.
In order to balance the number of computations and the size of the intermediate result, we consider the re-duction ratios defined as Those modes with the largest values for K m are han-dled element-wise. For example, if N = 3 and | E | = 1, we would choose Equation (8) if J 3 /I 3 &gt; J 2 /I 2 and Equation (9) otherwise. The intuition is that the ma-trix A ( m ) with large K m typically has two nice proper-ties: 1) fewer columns ( J m is small) need to be handled element-wise, which implies fast computation; and 2) more elements in a single column, which implies big space reduction on tensor size when multiplying this matrix.

How many modes are handled element-wise depends on the available memory. Typically, we will choose the minimal number of modes in E that guarantees that the memory does not overflow. Given the order based on reduction ratio, this selection can be done easily.
In summary, we have implemented a general-purpose function for this calculation in which the only parameter is the available memory and all the others are handled automatically. Algorithm 2 illustrates the pseudo-code.
 Algorithm 2 MET for N-mode tensor 5: repeat { outer loop }
Now we evaluate our method on a number of large sparse tensors. First, we study their performance on synthetic tensors that we generate by varying several data dependent parameters. Second, we report the re-sults on various of real tensors. We consider three per-formance metrics:  X  Intermediate space consumption is the mem- X  CPU time is the execution elapsed time of the  X  Relative error quantifies the quality of the All experiments are performed using MATLAB 2007b with the Tensor Toolbox [4, 6, 5] on a workstation with Intel Xeon 3GHz processor and 16GB RAM. For both Tucker-ALS and MET, we set the maximum number of iterations to 50 and the threshold for the change in k
G k to 1e-4.
To understand how different tensor properties affect the memory and speed, we test MET on a set of low-rank sparse tensors by varying different parameters. There are four sets of data dependent parameters:  X  Density D is the percentage of nonzero elements  X  Mode N is the number of modes or ways in the  X  Dimensionality I is the size on a mode, e.g.,  X  core size J is the dimensionality in the core ten-
Given parameters D , N , I , and J , we first construct a random core tensor of size J  X  X  X  X  X  J matrices A (1) . . . A ( N ) of size I  X  J ; we then construct the full tensor X = G  X { A } ; finally we keep only the top D percent of large elements in X to construct a sparse tensor. Note that this data generation operation is not memory-efficient, since it requires formatting the full tensor and sorting all the elements. Because of this, the synthetic tensors are smaller than the real tensor datasets we experimented with.

To acquire deeper insights into Tucker, we first present how the space consumption and computational time vary when varying different data-dependent pa-rameters. Note that in all cases, the relative error is extremely small and consistent across all methods (the error varies from 1 e  X  9 to 1 e  X  19), so we omitted that part in the paper. Here we compare four variants of Tucker as described in Section 4. Figure 2-5 shows the space and time (y-axis) when varying each parameter while setting the other ones to default values. All fig-ures plot the y-axis in the logarithm scale (base 10).
Density: Figure 2 shows space and time (y-axis) versus density (x-axis). Compared to the standard Tucker-ALS, MET achieves a 100X to 1,000X space re-duction. As the density drops, the gap between Tucker-ALS and MET becomes bigger. CPU times are com-parable between MET and Tucker-ALS, except for the high density one, where MET takes much longer than Tucker-ALS. The rule of thumb is if the tensor is sparse,
Figure 2. Varying tensor density: mode N = 3, di-
Figure 3. Varying tensor modes: density . 001%, di-MET can perform almost as fast as Tucker-ALS but needs much less memory.

Mode: In Figure 3, space and time increase with the number of modes of the tensor. In all cases, MET require much less space while taking compara-ble time to Tucker-ALS. It is interesting to see that for the mode-5 tensor, Tucker-ALS is actually slower than MET. Note that the per-element CPU time is actually sub-linear in the number of modes.

Dimensionality: In Figure 4, space and time in-crease with the dimensionality of the tensor. Again MET outperforms Tucker-ALS significantly in space, and achieves that with comparable CPU-time. core size: Similarly, in Figure 5, as we increase the result size (core tensor size), both space consumption and CPU time increase. It may be hard to notice the upward trend in Figure 5(a), since the y-axis is in log-arithm scale. But actually, the increase for both space and time are proportional in this case. Again, MET is the best in all cases. 5.2.1 Description We select a diverse set of data from different domains including Email communication (Enron), bibliography
Figure 4. Varying tensor dimensionality: density Figure 5. Varying core size:density . 001%, mode data (DBLP), web links (Web), and network flow traffic (Network).

The Enron dataset comprises 4 modes: sender, recipient, date, and keyword, with size 1K  X  1K  X  1.1K  X  200 and over 5.39 million nonzero entries. We select the top 1000 users and the 200 most frequent keywords (after simple stop-word removal and stemming) from the email messages of 1,126 days in the Enron MYSQL database [22]. The element ( i,j,k,l ) is 1 if there exists an email that is sent from sender i to recipient j with keyword k on day l ; otherwise, it is 0. Note that the tensor is very sparse with only .0025% nonzeros out of all 220 billion potential elements.
The DBLP dataset comprises 3 modes: author, date, and keyword, with size 5K  X  1K  X  1K and 0.5 mil-lion nonzero entries, which is constructed using a sim-ilar procedure to the Enron dataset. By parsing the DBLP XML file [23], we select the 5000 most prolific authors, the 1000 most common keywords, and the 1000 must popular conferences up to 2005. Each el-ement in the tensor is the corresponding paper count. Again the tensor is very sparse with .0536% nonzeros out of 1 billion potential elements.
 The web dataset comprises 3 modes: source URL, outgoing URL and anchor text, with size 5K  X  5K  X  200 and 0.1 million nonzero entries. We con-struct this tensor from the well-known TREC Web Cor-pus WT10G [24], which has about 1.7 million web-pages (10GB raw text) Based on the hyperlinks in the WT10G data, we select the top 3000 most pop-ular words in the anchor texts to construct a source-destination-word tensor. Elements in this tensor are the word counts.
 The network dataset comprises 3 modes: source IP, destination IP and destination port number with the size 3K  X  5K  X  200 and .5 million nonzero entries. The data are aggregated from anonymized intranet traffic flows from a university for over a month. We selected the top 3000 IP address and top 200 ports based on the number of distinct IP connections. The element ( i,j,k ) in the tensor is 1 if there exists a con-nection from source i to destination j on port k , and otherwise 0.

Choosing the  X  X ight X  size of core tensors often de-pends on the applications, which is beyond the scope of this paper. In this paper, we will present the effect of core size on space and time for synthetic datasets. For these real datasets, we fix the core size across all experiments to be 10. For example, the Enron core tensor is 10  X  10  X  10  X  10. 5.2.2 Performance Figure 6 shows the space and time required for all four datasets. Note that for Enron data, only the MET(2) and MET(3) can run on that server, and Tucker-ALS and MET(1) failed due to memory overflow. For the other datasets, all methods completed, with a signifi-cant gap (1000X difference) between Tucker-ALS and our MET(1)-(2). Besides that, the similar trends are observed as in synthetic data: 1) space-wise, MET achieves order of magnitudes saving to the standard Tucker-ALS; and 2) time-wise, MET is comparable to Tucker-ALS. 5.2.3 Data mining case-study As a higher-order generalization of SVD and PCA, Tucker decomposition has widely been used as a core technique for analyzing sparse tensors [19, 20, 8]. The typical pattern is to use the factorized tensor for other mining tasks such as clustering, trend identification and anomaly detection. Here we first give a quick overview of how Tucker can help with different min-ing tasks. Then we demonstrate some results on the clustering task.

Recall, given an input tensor X , Tucker approx-imates X as G  X  { A } such that the error e = k
X  X  G  X { A }k is small. With the result of Tucker, several mining tasks can be approached as the follows: Figure 6. Performance of Tucker on real datasets: for mode n , any clustering algorithm can be applied on A ( n ) by treating each row of A ( n ) as a J n -dimensional point. The benefit of doing clustering on these sub-spaces is that they are jointly chosen across all modes (1  X  n  X  N ) to minimize the global reconstruction error, which cannot be achieved by looking at an indi-vidual mode.

Trend detection: The core tensor G captures the main trend in the tensor. The trend across mode i is encoded in G  X  n A ( n ) , e.g., PCA is the special case in the second order case.

Anomaly detection: The reconstruction error gives a global measure of the quality of the approxima-tion. However, the error can be calculated on an arbi-trary scale, e.g., on the element level or the sub-tensor level such as slices or fibers. Higher error on a specific sub-tensor indicates a deviation from the main trend in that specific region, and the sub-tensor is therefore an anomaly.

Now let us see one example of clustering on DBLP dataset using Tucker decomposition with k-means. Similar clustering has been proposed in [19]; however, due to the huge memory overhead in original Tucker-ALS, only a small set of data are involved in that work. In particular, only publications from two con-ferences are considered. However, with MET, now we can easily analyze much larger tensors with 1,000 con-ferences. More specifically, we apply Tucker powered by MET on 5 K  X  1 K  X  1 K DBLP to obtain three ma-trices: the author matrix A (1)  X  R 5 K  X  10 , the confer-ence matrix A (2)  X  R 1 K  X  10 , and the keyword matrix A (3)  X  R 1 K  X  10 . Then we apply k-means on the row vectors of each factor matrix. We set k to be 200, 100, and 100 on A (1) , A (2) , and A (3) , respectively.
The sample clusters on each mode are shown in Ta-ble 2. In the conference mode, it clearly clusters differ-ent conferences into their corresponding domains, such as KDD, ICDM, PAKDD for data mining, and ICDE, VLDB, SIGMOD for databases. In the keyword mode, it again identifies interesting group of keywords, such as database, data, query for databases; 3D, video, motion, images for graphics. Similarly, we are able to group au-thors into clusters based on their research interests. A more detailed analysis can be done to identify the cor-relation across different modes, but due to the space limitations, we omit that part.

In short, we believe with the help of MET, Tucker can be more effectively applied to many large scale data mining problems.

Table 2. Sample clustering results on DBLP dataset Computing tensor decompositions The ALS al-gorithm for computing Tucker was proposed by Kroo-nenberg an De Leeuw [16] for the 3-way case and later extended to the N -way case by Kapteyn, Neudecker, and Wansbeel [12]. Andersson and Bro [3] suggest sev-eral ideas for improving the computation in MATLAB. Later, De Lathauwer, De Moor, and Vandewalle [9] suggested futher improvements to improve the numeri-cal accuracy of the method, which they renamed to be the  X  X igher order orthogonal iteration. X  There has been no previous work on computing Tucker-ALS for large-scale sparse tensors. In their dis-cussion of n -mode multiplication of a tensor by a ma-trix A , Bader and Kolda [6] observe that  X  X nless A has special structure (e.g., diagonal) the result is dense. X 
Recently, other methods for computing Tucker fac-torizations have been developed. Of note, Eld  X en and Savas [10] have proposed a Newton-Grassmann opti-mization approach. The procedures developed here would also be important for applying their approach to large-scale sparse tensors.
 Tensor applications Tucker decompositions have been used in a variety of applications in data min-ing, and we highlight several distinctive examples here. Savas and Eld  X en [17] applied the HO-SVD (a version of the Tucker decomposition) to identifying handwritten digits. Acar et al. [1, 2] applied Tucker and other ten-sor decompositions to the problem of separating con-versations in online chatrooms, and Chi et al. [8] have applied it to the blogosphere. J.-T. Sun et al. [20] used Tucker for analyzing clickthrough data. J. Sun et al. [19, 18] have written a pair of papers on dynam-ically updating a Tucker approximation, with applica-tions ranging from text analysis to environmental and network modeling.
In conclusion, tensors, especially sparse tensors, are important data models for many different applications. Tensor decompositions such as Tucker are becoming standard tools for analyzing multiway data. How-ever, the  X  X ntermediate blowup X  issue in Tucker is a severe problem for even moderately sized real prob-lems. To address this problem, we propose memory-efficient Tucker decomposition (MET) for decompos-ing large sparse tensors. MET is adaptive to the avail-able system resources with automatic parameter tun-ing. Compared to the previous state-of-the-art Tucker-ALS algorithm, MET can achieve significant space re-duction (over 1000X) without sacrificing much speed. We demonstrate the performance on both synthetic and real datasets and illustrate the potential data min-ing tasks with the help of MET.

In the future, we plan to evaluate other types of ten-sor decomposition in the context of sparse data. Since tensor decompositions such as Tucker may in turn de-pend on matrix decompositions such as the SVD, an in-teresting extension is to leverage memory-efficient ma-trix operations in the tensor decomposition to further decrease memory consumption. Another different di-rection is to study the parallel version of the tensor algorithms.

