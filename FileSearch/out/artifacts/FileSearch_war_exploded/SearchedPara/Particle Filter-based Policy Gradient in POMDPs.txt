 coquelin@cmapx.polytechnique.fr Kaelbling et al., 1998)) defined by a state process ( X Y , a decision (or action) process ( A  X  horizon setting: with initial probability measure  X   X  M ( X ) (i.e. X transition function F and independent random numbers, i.e. for all t  X  1 , The observation process ( Y by the conditional probability measure P ( Y using a transition function G and independent random numbers, i.e.  X  t  X  1 , Y where V Now, the action process ( A history Y 1  X  s  X  t ), an action A t  X  A .
 bution ) conditionally to past observations. The belief state, wri tten b of all probability measures on X ) and is defined by b written b property of the state dynamics, the belief state b the current state X designing an optimal policy in the class of observations-ba sed policies. policies is summarized in Figure 1 (left): at time t , the state X which enables (at least in theory) to update b as input the belief state b filtering distribution. We write b usual notation b ( f ) def = R and denote b measure) could be used as well. Such a policy  X  : R K  X  A selects an action A in turn, yields a new state X localization (Fox et al., 2001). An PF approximates the beli ef state b selection procedure. In particular, the belief feature b the particles: A Particle Filter-based policies is described in Figure 1 (ri ght). In this paper, we consider a class of policies  X   X  and we search for the value of  X  that maximizes the resulting criterion J (  X  for constructing the belief state in continuous domains.
 optimization and localization.
 scheme for POMDPs where the belief feature b 2001) for details, approximates the belief state b (where  X  denotes a Dirac distribution) made of N particles x 1: N following steps: at time t , given observation y property (i.e. P N et al., 1993) chooses the selection indices k 1: N according to a multinomial distribution with parameters w 1: N optimal in terms of variance, see e.g. (Capp  X  e et al., 2005). Convergence issues of b N 2.1 Control of a real system by an PF-based policy We describe in Algorithm 1 how one may use an PF-based policy  X  system. Note that from our definition of F 2.2 Estimation of J (  X  ) in simulation Algorithm 1 Control of a real-world POMDP for t = 1 to n do end for J estimate, written J N for PF, one has lim would perform several calls to the algorithm, receiving J N Algorithm 2 Estimation of J for t = 1 to n do end for
Return J N  X  policy  X  common indices FD , which overcomes this problem.
 such functions may depend on time. 3.1 Naive Finite-Difference (FD) method in each direction. For h &gt; 0 we define the centered finite-difference quotient I Since J (  X  ) is differentiable then lim sistency of the PF, we deduce that lim I may prevent the stochastic gradient algorithm from converg ing to a local optimum. random numbers to estimate both J (  X  + h ) and J (  X   X  h ) (i.e.  X  a finite set 1: N ) are usually a non-smooth function of the weights w 1: N of the estimate when h is small, even when using common random number. 3.2 Common-indices Finite-Difference method Let us consider J random sample path  X  . Under our assumptions, the gradient  X  X   X  smooth, and the belief state b w.r.t.  X  , we write: k estimation . More precisely, let us write k 1: N consider a parameter  X   X  in a small neighborhood of  X  . Then, an PF estimate for b been generated using the same selection indices k 1: N (see (Coquelin et al., 2008) for the proof).
 neighborhood of  X  , such that for any  X   X  in this neighborhood, b N estimator of b use the same selection indices k 1: N smooth. We write: From the previous proposition we deduce that J N  X  The resulting estimator  X  M sample paths  X  feature estimator (3) thus to  X  dient approaches.
 robot. The robot is defined by its coordinates x Algorithm 3 Common-indices Finite Difference estimate of  X  X  Initialize likelihood ratios: for t = 1 to n do end for
Return:  X  of the squared distance to the origin (the goal): y the variance of the noise). At each time step, the agent may ch oose a direction a which results in moving the state, of a step d , in the corresponding direction: x where u the particle population m consider policies  X  a the robot towards the direction of the goal, whereas  X  The performance measure (to be minimized) is defined as J (  X  ) = E [ || x  X  x = 0 . 05 method inapplicable, whereas that of the CIFD is reasonable .
