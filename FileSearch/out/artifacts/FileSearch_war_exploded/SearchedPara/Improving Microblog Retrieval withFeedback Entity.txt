 When searching over the microblogging, users prefer using queries including terms that represent some specific entities. Meanwhile, tweets, though limited within 140 characters, are often generated with one or more entities. Entities, as an important part of tweets, usually convey rich information for modeling relevance from new perspectives. In this paper, we propose a feedback entity model and integrate it into an adaptive language modeling framework in order to improve the retrieval performance. The feedback entity model is estimated with the latest entity-associated tweets based upon a regularized maximum likelihood criterion. More specifically, we assume that the entity-associated tweets are generated by a mix-ture model, which consists of the entity model, the domain-specific language model and the collection language model. Experimen-tal results on two public Text Retrieval Conference (TREC) Twit-ter corpora demonstrate the significant superiority of our approach over the state-of-the-art baselines.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Search and Retrieval ]: Information Search and Re-trieval X  Retrieval models Algorithms, Experimentation, Performance Microblog Retrieval; Language Modeling; Feedback Entity Model; Freebase
With the rapid growth of microblogging, an increasing number of studies have paid much attention to the research on Information  X 
First two authors contribute to this work equally.  X  Corresponding author.
 c  X  Retrieval (IR) in the context of microblogosphere, especially Twit-ter 1 . To explore the information seeking behavior in microblogo-sphere, TREC first introduced a temporally-anchored ad hoc search task in 2011 [18] to attract more studies on how to retrieve relevant tweets according to users X  queries. In particular, the goal of the temporally-anchored ad hoc search task can be summarized as  X  At time T, give me the most relevant tweets about topic Q  X .
When searching over the microblogging, users tend to use enti-ties in their queries to express the certain information need. Among the topics (i.e. queries) released by TREC 2 Microblog track [18, 23, 13], more than half of the topics include at least one named entities. Nevertheless, different people are likely to use alternative aliases to represent the same entity. Hence, the vocabulary gap be-tween tweet-posters and the users who search tweets lies as a big challenge for effective IR over tweets. Moreover, as tweets are un-der the length limitation of 140 characters, the risk of mismatch between query terms and any word observed in relevant tweets is larger than that for traditional Web document retrieval.
Fortunately, we have observed that, while being not allowed to write long text in one tweet, users tend to leverage entities, usually regarding to specific real-world persons, organizations, and loca-tions, to illustrate the topic of their tweets (See Figure 1 for example tweets about  X  X ila Kunis X ). An analysis of a public Twitter corpus published by TREC has illustrated that more than 20% tweets con-tain at least one entity [18, 13].

Beyond the pure text, entities, as an important part of tweets, usually convey rich information for modeling relevance from new perspectives. Henceforth, to improve users X  experience of surfing in microblogosphere, it becomes more necessary to explore how http://www.twitter.com http://trec.nist.gov to take advantage of entity information from tweets to facilitate ad hoc search task in the microblogging context.

In this paper, we propose a novel feedback entity model and in-tegrate it into an adaptive language modeling framework. Our new framework takes advantage of the rich entity information in Twitter to address the challenges in microblog retrieval. We first estimate the entity model using the entity-associated feedback tweets based upon a regularized maximum likelihood criterion. In particular, we take advantage of the domain information in an open-domain on-tology (i.e. Freebase 3 ) to filter domain-specific background noise in the estimation of the entity model. Then, by leveraging a lan-guage modeling based IR framework, it is quite natural to integrate the entity model into the whole IR framework via query expansion. Entity model can help update the estimation of the query language model based on the extra evidence carried by the entity-associated feedback tweets. Furthermore, with using our new method, dif-ferent entity aliases referring to the same entity are likely to lead to similar entity models as they share a similar word context. This fea-ture can also mitigate the vocabulary gap between different users to some extent.

The main contributions of this paper include: (1) we propose a feedback entity model and integrate it into the adaptive language modeling framework in order for a more effective IR in microblog-ging; (2) we use a generative model to estimate the entity model with entity-associated feedback tweets; (3) we perform a set of ex-periments on two public twitter test collections published by TREC to compare our proposed method with the state-of-the-art baseline systems. And, the experimental results demonstrate that our pro-posed approach can give rise to significantly better retrieval perfor-mance.

The rest of the paper is organized as follows. Section 2 gives an overview of the related work. Then, we describe our adaptive lan-guage modeling framework in Section 3. In Section 4, we present our proposed feedback entity model in details. The experimental results as well as the comparisons with the-state-of-arts are shown in Section 5. Finally, we conclude the paper and outline our future work in Section 6.
Query expansion (QE) based on pseudo-relevance feedback (PRF) [12, 14, 15, 27, 8] is widely used in microblog search to im-prove the retrieval performance. However, traditional PRF-based query expansion may not work well as it relies on the assumption that most of the frequent terms in the pseudo-relevance documents are useful [16]. Cao et al. [2] re-examined this assumption and showed that it does not always hold in reality  X  many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful for the retrieval performance. They then in-tegrated a term classification process to predict the effectiveness of expanded terms. Liang et al. [12] proposed a Real Time Ranking Model (RTRM), which utilized a two-stage pseudo-relevance feed-back query expansion to estimate the query language model and expand documents with shortened URLs in microblog. In addition, RTRM can evaluate the temporal aspects of documents with the temporal re-ranking components. Miyanishi et al. [17] proposed a first-stage manual tweet selection feedback to improve the retrieval performance. They further used a two-stage PRF based on similar-ity of temporal profiles of the query and top retrieved tweets. How-ever, this method sometimes fails due to the redundancy of selected http://www.freebase.com tweets, which usually contain a significant number of meaningless words that may degrade search results. Just like Liang and Miyan-ishi, we attempt to improve the effectiveness of PRF based QE by utilizing a first stage query expansion.

Other works attempted to acquire knowledge from external sources to obtain additional query terms [5]. Li et al. [11] ex-plored the possibilities of using Wikipedia X  X  articles as an external corpus to expand ad-hoc queries and demonstrated that Wikipedia especially useful for the case of weak queries that PRF fails to improve. In their methods, expansion terms were extracted from the top ranked Wikipedia articles. Pan et al. [19] proposed us-ing Dempster-Shafer X  X  Evidence Theory to measure the certainty of expansion terms from the Freebase structure. Dalton et al. [3] proposed a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. In our study, we seek help from Freebase to guide estimating the domain background language model in feedback entity model estimation.
Entities have been well exploited in the search scenario, espe-cially in short text retrieval, where entities represent important se-mantics. Sokes et al. [24] employed ontology-based (MeSH and Entrez Gene) query expansion and entity-based relevance feedback for genomics search. Their approach works by identifying poten-tially relevant entity instances in an initial set of retrieved candi-date paragraphs. These entities are then directly added to the initial query with the aim of boasting the rank of passages containing lists of these entities.

Entities, not only real-world ones, but also user-generated ones (e.g. hashtags, user mentions), play a more important role in Twit-ter than in traditional documents due to the length limitation of tweets. Efron et al. [6] described the problem of  X  X ashtag re-trieval X , a type of entity search. They leveraged the retrieved query-related hashtags in query expansion and achieved better retrieval performance. They assumed that hashtags often reflect some im-ence. However, some hashtags are very informal, so using hash-tags as direct feedback words may lead to risky situations such as topic drift. Further analysis of the entity might gain some retrieval performance. To the best of our knowledge, entity models which model the entities with entity-associated feedback tweets, have not been adapted or leveraged in temporally-anchored search task yet, which might help a lot in retrieving relevant tweets.
To estimate and utilize the entity topic model, one important is-sue is to recognize entities in tweets and query. Many works have been attempted to extract entities in the microblogosphere. Among them, Ritter et al. [21] proposed a distantly supervised entity recog-nition approach. They first constructed large amounts of unlabeled data collection including large dictionaries of entities gathered from Freebase and information about an entity X  X  context across its men-tions, and then applied LabeledLDA [20] to leverage the unlabeled data. Their method has been reported with a good performance in named entity segmentation for tweets. In this study, we apply Rit-ter X  X  method to recognize entities in tweets
The detection of the named entity in queries is even harder as queries tend to be very short. Guo et al. [7] first addressed the prob-lem of Named Entity Recognition in Query (NERQ) and proposed taking a probabilistic approach to the task using query log data and Latent Dirichlet Allocation. It was found that keyword queries that people issue to retrieve information from Twitter are, on aver-age, significantly shorter than queries submitted to traditional Web search engines (1.64 words vs. 3.08 words) [25]. Henceforth, in this paper, we adopt a two-stage entity match method to recognize entities in queries. This method works quite well for the short Twit-ter queries, most of which are entities themselves.
To address the ad hoc search task in the microblogosphere, we employ language modeling approach. In particular, we assume that a query Q is generated by the query language model a document D is generated by the document language model  X  After estimating  X   X  Q and  X   X  D according to [9], the relevance score of D with respect to Q can be computed by the following negative KL-divergence function:
S ( Q,D ) =  X  KL (  X   X  Q ||  X   X  D )  X  X where V is the set of words in our vocabulary. To obtain an ef-fective ranking function for IR in microblogosphere, it is critical to accurately estimate  X   X  Q and  X   X  D , respectively.
 Figure 2: Overall architecture of the adaptive language model-ing framework.

Figure 2 shows the overall architecture of our adaptive language modeling framework, which is based on the KL-divergence model and adopts a two-stage query expansion method.

When a new query comes, we first extract all the entities in the query. In our proposed system, we adopt a novel two-stage entity match method to recognize the named entities in the query:
With the extracted entities, we estimate the feedback entity model with a generative model and update the original query model (i.e. first stage QE). Then, a traditional feedback query model is ap-plied to further update the query model (i.e. second stage QE). https://github.com/aritter/twitter_nlp
To model the dynamics of microblogosphere, we propose to gen-erate feedback entity model  X   X  E for extracted entity in the query. Feedback entity model is a probabilistic distribution of words { p ( w |  X   X  E ) } w  X  V and represents the common topics of a given en-tity. Section 4 will elaborated on the estimation of entity model. With the entity model  X   X  E , we are able to expand the original query language model, i.e.  X   X  Q  X   X   X  Q 0 as query expansion. We take ad-vantage of the linear interpolation for combining the original lan-guage model and the feedback entity model: where  X   X  [0 , 1] controls the influence of the entity model.
For the traditional feedback methods, the balance parameter  X  is usually set to a fixed value across all the queries. However, con-sidering that entities in different queries may not have the same importance, an adaptive weighting parameter should be utilized to dynamically balance the original query and feedback entity infor-mation. In our method, our balance parameter takes the form of: where E means the entities in the query Q ,  X   X  [0 , 1] is a fixed parameter and the IDF of terms in the entities can affect the final balance coefficient.

Note that, when there are more than one entities recognized in the query, we employ the weighted average of all entity models as the unified entity model to expand the corresponding query lan-guage model: where n is the number of entities in the given query.  X   X  entity model for entity E i , and IDF ( E i ) is the inverse document frequency for entity E i . We suppose that an entity with a higher inverse document frequency is more important than that with a lower inverse document frequency. Taking the query  X  X u Jintao visit to the United States X  as an example, the entity  X  X u Jintao X  ( IDF = 5 . 7 ) will have a higher weight compared with the entity  X  X nited States X  ( IDF = 3 . 8 ).
When the query is expanded with the feedback entity model, our approach can retrieve more relevant documents at the top, which contain more accurate word distribution than by initial search re-sult. Based on this hypothesis, we further utilize a model-based feedback to update the query representation. Note that the feed-back tweets for model estimation are top ranked results using the language modeling framework with first stage QE. More specifi-cally, we update the  X   X  Q 0 using the feedback query model is widely used in the microblogging retrieval [27, 12]. where  X   X  [0 , 1] is a parameter to control the weight of the model-based feedback.

The model-based feedback model generates a feedback docu-ment by mixing the feedback query model  X   X  F with the collection language model  X   X  C . Under this simple mixture model (SMM), the log-likelihood of feedback documents F is: log p ( F |  X   X  F ) = X where c ( w,F ) denotes the count of word w occurred in the set of feedback documents F . Then, we follow the work of [12, 4] and implement the EM algorithm with the best tuned parameter  X  .
In this section, we describe the details of estimating the feedback entity model. We first propose a general method to estimate the feedback entity model with entity-associated feedback tweets (i.e. EF ). Then, we discuss how to incorporate expert knowledge from Freebase into our model. Finally, we elaborate on two methods of fetching EF .
A natural way to estimate a feedback entity model  X   X  E is to as-sume that the EF is generated by a probabilistic model p ( EF |  X  ) . It is straightforward to leverage the unigram language model, which generates each word w in EF independently according to  X  . where c ( w,D i ) is the count of word w occurred in document D This simple model would be reasonable if the feedback tweets only contain information relevant to the corresponding entity. However, the content in those feedback tweets is of high diversity, as it usu-ally contains rich background information or even irrelevant topics. This problem is especially severe because of the redundancy of the tweet content. Therefore, it is extremely necessary to take advanced selection or filtration for these noisy feedbacks.

To address this problem, we propose a generative model to es-timate the entity model using the observed EF based upon a reg-ularized maximum likelihood criterion. The particular generative model we employ is a mixture of models, which incorporates not only the entity model, but also the collection language model and the domain-specific background language models. A graphical rep-resentation of the generative model is illustrated in Figure 3.
By using the mixture model, the log-likelihood for the EF is: log p ( EF |  X   X  ) = X where k is count of the corresponding domains for the entity E . Without prior expert knowledge of domains, we set k as the num-ber of Freebase domain categories. c ( w,D i ) is the count of word w occurred in document D i . Note that  X  C and  X  E are set empiri-cally and represent the amounts of background noise and domain-specific background noise, respectively. Intuitively, when estimat-ing the entity model, we try to  X  X urify X  the documents by eliminat-ing some background noise along with the domain-specific back-ground noise. The set of parameters includes: Figure 3: A graphical representation of the generative model.
We can apply EM algorithm [4] to compute a maximum likeli-hood estimate. The updating formulas are Eq.9-14.
Our framework aims at solving the problem with minimum su-pervision; however, if there exists prior expert knowledge on the structured semantics of entities, we also want to incorporate it into the model. Such prior knowledge, for example, can be obtained based on pseudo feedbacks [1] or click-through data [26] in the context of Web search. In our study, as an illustration, we seek help from Freebase, which provides a gold mine of knowledge for entities.

In Freebase, human knowledge is described by structured cate-gories, which are also known as types and each type has a number of defined properties . Just as properties are grouped into types, types themselves are grouped into domains . Each entity in Free-base has been assigned into several specific domains according to expert X  X  judgement. For instance,  X  X ila Kunis X  is involved in seven specific Freebase domains (i.e. Film, TV, People etc.). Hence, we can set the domain number k in the mixture model as 7 . In addition, we can use the type and property information of given domains to guide the discovering of domains by adding conjugate priors.
Specifically, we build a unigram language model { p ( w | d ) } for each pre-defined common domain d based on the information )  X   X   X  ( w |  X   X  E ) +  X  C p ( w |  X   X  C )]  X   X  ) p  X   X 
E ) +  X  C p ( w |  X   X  C ) c ( w,D i )  X  t ( n ) d ( w ) =1 c ( w 0 ,D i )  X  t c ( w,D i )  X  t ( n ) d ( w ) of Freebase entites. We first recognize all the entities in the twit-ter collection with the entity recognition tool described in Section 3, and then link them to Freebase with the search API 5 . After that, we collect all the text from type names and the corresponding prop-erties of these entities, and group the text according to their related domains. In this way, we could create a virtual document for each domain. A unigram domain language model can be built in each virtual document as: where c ( w,d ) is the count of word w occurred in the domain vir-tual document d . Table 1 shows the top occurred words for several typical domains in Freebase.
 Domain Top Words TV tv, program, episode, appear, series, contribute Organization organization, found, origin, leader, headquarter Chemistry chemical, compound, measure, element, identify Astronomy orbit, measure, relationship, star, object Book author, work, written, create, book Location locate, part, hud, area, place
Then, we could define a conjugate prior (i.e. Dirichlet prior) on each unigram language model, parameterized as: where  X  d is a confidence parameter for the prior. Since we use a conjugate prior,  X  d can be interpreted as the equivalent sample size because the effect of adding the prior would be equivalent to adding  X  d  X  p ( w | d ) pseudo counts for word w when we estimate the specific domain language model. Basically, the prior serves as some training data to intentionally bias the domain language model estimation. https://developers.google.com/freebase/v1/search-overview
To this end, the prior for all the parameters is given by: where  X  i = 0 if we do not have prior knowledge for some domain d . Then, we can use the Maximum A Posteriori (MAP) estimator to estimate all the parameters as follows: The MAP estimate can be computed using the essentially same EM algorithm as presented above with slightly different updating for-mula for the component language models (Eq. 19).
According to our proposed approach, EF yields a considerable influence on the feedback entity model. Two methods (i.e. entity match method and entity query method) are therefore proposed to collect the top M feedback tweets for each entity in a given query.
The entity match method first identifies all the entities from each tweet with the entity recognition tool described in Section 3. Then, we can obtain the feedback tweets by collecting all the tweets con-taining the very entity. To maintain the temporal characters of the feedback entity model, we can use the most recent M tweets con-taining that entity before query issue time T Q .
In this method, we regard each entity as a new query and search related tweets with a temporal based language model [10]. To this end, we enhance the selection of feedback tweets by taking into account the features related to both temporal information and rele-vance. In contrast to the previous method, the feedback tweets may not have an exact match for a given entity. However, this method can give rise to flexibility of balancing the tradeoff between tweet recency and relevance. To build the time based language model, we leverage Eq.1 to incorporate a time prior for each document D :
P where r is the exponential parameter that controls the temporal in-fluence. T Q is the query issue time and T D is time when the tweet was posted. Both T Q and T D are measured in the granularity of days. Note that T D is constantly less than T Q as we cannot use the future evidence. Finally, we use the top ranked M tweets as the feedback set.
In this section, we conduct several experiments to evaluate the effectiveness of our proposed adaptive language modeling method enhanced by the feedback entity model. In these experiments, we also conduct corresponding analysis to investigate: (1) the influ-ence of the various feedback entity model parameter settings on retrieval performance; (2) the effects of feedback tweets number; (3) the influence of the interpolation coefficient in query expansion and (4) a comparison of two different entity feedback acquisition methods.
In this section, we describe the experimental setup, including the dataset and evaluation methods which are adopted in TREC Microblog track [18, 23, 13].
Two corpora (i.e. Tweets11 and Tweets13 collection) are used in our experiments. Instead of distributing the microblog corpus via physical or direct downloading, TREC organizers release a stream-ing API to participants [13]. Using the official API 6 , we crawled a set of local copies of the canonical corpora. Our local Tweets11 collection has a sample of about 16 million tweets, ranging from January 24, 2011 to February 8, 2011 while Tweets13 collection contains about 259 million tweets, ranging from February 1, 2013 to March 31, 2013 (inclusive). In addition, we also crawled all the shortened URLs contained in Tweets11 and Tweets13 Corpora, and inferred their topic information to enrich the original tweets. In particular, we consider the topic information as the local context of the original tweets and combine it with the original tweets to form the tweet language model [12]. Tweets11 is used for evaluating the effectiveness of the proposed Twitter search systems over 50 official topics in the TREC 2011 Microblog track as well as 60 of-ficial topics in the TREC 2012 Microblog track, respectively. And, Tweets13 is used in evaluating the proposed Twitter search systems over 60 official topics in the TREC 2013 Microblog track. In our experiments, we only make use of those topics containing entities. The topics in TREC 2011 are used for tuning the parameters and then we use the best parameter setting to evaluate our methods with topics for TREC 2012 and TREC 2013. Table 2 summarizes basic statistics of the three years X  topics. From the table, we can observe that half of the topics include entities, which is quite consistent with reality.

In our experiments, the tweets and their corresponding topic in-formation would be preprocessed in several steps. First, we dis-carded those non-English tweets using a language detector with infinity-gram, named ldig 7 . After that, according to the Microblog https://github.com/lintool/twitter-tools http://github.com/shuyo/ldig Table 2: Summary statistics of topics in TREC Microblog track.
 track X  X  guidelines, all simple retweets, i.e. tweets beginning with the string  X  X T X , were removed. Moreover, each tweet was stemmed using the Porter algorithm and stopwords were removed using the InQuery stopwords list. Table 3 summarizes basic statistics of the two corpora after all the steps of preprocessing. In TREC X  X  temporally-anchored ad hoc search task, track organizers created several test topics, each of which contains query text Q and a times-tamp T Q . Only tweets posted prior to T Q were assessed for rele-vance [18]. Thus for each query Q , we built a dynamic dataset consisting of tweets whose timestamps are prior to T Q . Table 3: Summary statistics of Tweets11 and Tweets13 corpora.
In TREC Microblog track, tweets are judged on the basis of the defined information using a three-point scale [18]: irrelevant (la-beled as 0), minimally relevant (labeled as 1), and highly relevant (labeled as 2). In our experiments, we mainly leverage two widely-used evaluation metrics in IR, including Mean Average Precision (MAP) and Precision at N (P@N). Specifically, MAP for top 1000 ranked documents and P@30 with respect to allrel (i.e. tweet set labeled as 1 or 2) are the official main metrics for the temporally-anchored ad hoc search task in TREC, which are also used in this paper. Furthermore, we also do a query-by-query analysis and con-duct t-test to determine whether the improvements on MAP and P@N are statistically significant.
To demonstrate the superiority of our feedback entity model in query expansion for microblog retrieval, we compare our adaptive language model with several baseline methods. 1) The simple KL-divergence (denoted as SimpleKL ) [28] is used as our first baseline. SimpleKL estimates both  X   X  Q with empirical word distribution, in which we choose Dirichlet smoothing method for document model estimation. Throughout this paper, we set the Dirichlet smoothing parameter  X  = 100 . 2) In addition, we implement Efron X  X  hashtag-based relevance feedback method HFB1 [6] as an entity-based query expansion baseline (labeled as QEHashtag ). The model parameter k is set as 25 and  X  is set as 0 . 2 . 3) To compare with the external query expansion method, we employ a Wikipedia-based query expansion method QEWiki , which is similar with the work of [11]. We downloaded a local copy of Wikipedia data for faster access and indexed the articles using Lemur toolkit 8 (version 4.12). The expansion terms are de-rived from top ranked Wikipedia articles. In our experiments, we rank Wikipedia articles using language model (i.e. SimpleKL), and 10 terms with highest TFIDF scores are picked from the top 5 arti-cles. Then we treat the terms as a new query and interpolate it with the original query with a weight of 0 . 5 . 4) Simple Mixture Model [27] (denoted as SimpleKL + SMM ) is used as another baseline. SimpleKL + SMM is reported with a relatively better retrieval performance among the state-of-the-art PRF-based query expansion methods [14]. The number of feed-back documents is set as 5 and the number of terms in the feedback model is set as 7 . The interpolation parameter  X  is set as 0 . 6 . Note that the feedback tweets used for SimpleKL + SMM are derived from the initial search results, which often contain many irrelevant documents. Hence, we replace the feedback tweets with the top retrieval results using QEHashtag and QEWiki . In this way, we generate two additional baselines, i.e. QEHashtag + SMM and QEWiki + SMM . 5) Finally, we compare our approach with the state-of-the-art real-time ranking model (denoted as RTRM ), proposed in [12]. RTRM also adopts a two-stage query expansion method like our proposed adaptive language model does. The first stage query ex-pansion weight is set as 0 . 4 . For the second stage query expansion, we extract the top 7 feedback terms from the top 5 tweets and set the interpolation weight as 0 . 6 . In addition, it utilizes ranking position as temporal profile, and adopts the Gaussian temporal re-ranking function with  X  = 120 .
 All the parameters of these baseline methods are tuned using TREC 2011 topics.
In this section, we report the experimental results to demonstrate the effectiveness of our proposed feedback entity model based query expansion methods. In the following, we denote the KL-divergence retrieval model with feedback entity model based query expansion for query model  X   X  Q 0 as QEFEM , and that for QEFEM + SMM . When estimating the entity model, we set feed-back document count M as 50 with entity match method. The model parameter  X  E is set as 0.7 and  X  C is set as 0 . 5 . The first interpolation coefficient  X  is set as 0 . 6 in Eq.3 for both QEFEM and QEFEM + SMM . For the simple mixture model, we use top 5 feedback documents, while setting  X  to 0 . 6 in Eq.5. For the adap-tive language model with query model  X   X  Q 0 and fixed interpolation coefficient  X  , we label it as QEStaticFEM and set  X  = 0 . 4 for all queries in Eq.3. All these parameters are tuned using topics in TREC 2011.
Table 4 shows the MAP and P@30 performances of eight meth-ods with statistical significance test results for allrel tweets. The best performances are marked in bold typeface. Note that all the methods listed in the table estimate the document model as Sim-pleKL .

We first examine the effectiveness of our adaptive language model using first stage query expansion (i.e. QEFEM ).  X  ,  X  and  X  indicate that the corresponding improvements over SimpleKL , QEHashtag and QEWiki are statistically significant ( p &lt; 0 . 05 ), respectively. It can be clearly observed from Table 4 that all these query expansion methods can result in improvements in terms of MAP compared with the SimpleKL method for both TREC 2012 and 2013 topics, which indicates the importance of query expan-http://www.lemurproject.org/lemur.php sion in microblog search. Besides, QEFEM performs better than hashtag-based query expansion method QEHashtag , which indi-cates the importance of analyzing entities in twitter rather than simply using the entity terms. Moreover, QEFEM is better than the external expansion method QEWiki in terms of both MAP and P@30. This indicates that QEFEM can get purer entity-associated topic terms and thus leads to higher precision in top retrieved re-sults. For TREC 2012 topics, QEFEM improves the P@30 and MAP scores significantly compared with the three baselines.
When the query is expanded with the feedback entity model, our approach can retrieve more relevant documents at the top, which contain more accurate word distribution than by initial search (i.e. search results by SimpleKL ). Thus, we can further improve the retrieval performance by combining the model-based feedback method as second stage query expansion. Table 4 also shows all the performances using two-stage query expansion methods.  X  , 4 and N indicate that the corresponding improvements over SimpleKL + SMM , QEHashtag + SMM and QEWiki + SMM are statisti-cally significant ( p &lt; 0 . 05 ), respectively. Note that our QEFEM + SMM method outperforms the baseline SimpleKL + SMM sig-nificantly. More specifically, for TREC 2012 topics, the QEFEM + SMM improves the MAP and P@30 over those of SimpleKL + SMM by 21.8% and 13.2%, respectively; while the correspond-ing increments for TREC 2013 topics are 9.5% and 6.5%, respec-tively. Besides, QEFEM + SMM also performs betther than the other two-stage query expansion method QEHashtag + SMM and QEWiki + SMM . This proves the superiority of our feedback en-tity model based query expansion method.
 Table 5: Performance comparison of the adaptive language model with RTRM.

Finally, we compare our adaptive language model with the state-of-the-art baseline RTRM in Table 5. RTRM is reported to acheive the best MAP score in TREC 2011 Microblog Track [12]. It can be clearly observed that QEFEM + SMM even performs better than RTRM , which demonstrates the effectiveness of our approach.
In Table 6, we list the performances of our adaptive language model with both static weighting parameter (i.e. QEStaticFEM ) and adaptive weighting parameter (i.e. QEFEM ). From Table 6, we can observe that both of the methods perform well with respect to MAP and P@30. Besides, we can obtain additional performance improvement when using the adaptive weighting parameter  X  . This proves our assumption that entities in different queries have differ-ent influence, and thus we should adopt an adaptive parameter to balance the entity feedback and the original query.
 Table 6: Performance comparison of the adaptive language model with both static weighting parameter and adaptive weighting parameter.

To further study what percentage of entity queries are enhanced by the adaptive weighting parameter, we conduct a query by query performance analysis using TREC 2012-2013 topics involving enti-ties. Figure 4 shows the performance difference between QEFEM and QEStaticFEM in terms of MAP. It is apparent that using adap-tive weighting parameter is effective for improving most queries containing entities.
 Figure 4: Difference in MAP between QEFEM and QEStat-icFEM using the TREC 2012-2013 Microblog track topics.
Many parameters in our proposed approach can affect the system performance. In this section, we analyze the robustness of the pa-rameter setting in our adaptive language model. For the QEFEM + SMM , we expand the original query with feedback entity model as QEFEM does, and it also applies the traditional model-based feedback to further update the query. We set the second-stage query expansion parameter  X  as 0 . 6 and feedback document number as 5 , which are reported with good retrieval performance in microblog search [12]. All these experiments in this section are run on TREC 2011 topics, which are used for parameter selection. Note that we first conduct a grid search on  X  C ,  X  E , M and  X  in order to find the optimal parameters, then we check the sensitivity of each parameter when setting others as optimal values.
There are many parameters that may influence the effectiveness of feedback entity model in query expansion. To estimate the en-tity model, the parameter  X  E controls the amount of  X  X omain back-ground noise X  while the parameter  X  C controls the amount of  X  X ol-lection background noise X . The number of feedback documents for each entity is also very critical since it directly affects the avail-able terms for the entity. In this section, for each query, we collect the latest 50 entity-associated feedback tweets, using entity match method for model estimation. When incorporating the domain prior information in Freebase, we simply set all the Dirichlet parameters  X  ( i = 1 ,...,k ) as 1000 .

To evaluate the sensitivity of retrieval performance to the two feedback model parameters  X  C and  X  E , we fix the interpolation coefficient  X  as 0 . 6 . In the actual experiments, we truncated the estimated entity model by ignoring all terms whose probability is lower than 0.001, and renormalized it before interpolating. Then we conducted a grid search on  X  C and  X  E . Figure 5 shows the per-formance changes of the QEFEM and QEFEM + SMM against different  X  E while setting  X  C to a fixed value 0 . 5 . A smaller  X  can filter more domain background noise when estimating the en-tity model. However, from the figure, we can find that it might also decrease the retrieval performance when ignoring too many domain-specific words. In general, the performance change of the QEFEM with different  X  E is slight. However, if we set  X  and totally ignore the domain-specific language models, the perfor-mance of QEFEM declines a lot in terms of MAP, which proves the importance of domain background noise filtration.

We also conduct an experiment to analyze the performance change of that QEFEM against different values of  X  C . Experimen-tal results show the retrieval performance is not very sensitive to  X  , while totally ignoring the collection background model would lead to a performance drop.

To further demonstrate the effectiveness of our mixture model, we compare the best tuned QEFEM (i.e.  X  E = 0 . 7 , X  C = 0 . 5 , denoted as QEFEM-Filtering ) with QEFEM without any noise filtration (i.e.  X  E = 1 , X  C = 0 , denoted as QEFEM-ML ). From Figure 6, we can observe that, after filtering the domain and back-ground noise, we can increase both P@10 and P@30 scores, com-pared with the baseline method and the entity model without noise filtration.
Another important parameter is the number of entity-associated feedback tweets for model estimation. Too few feedback tweets may not be adequate to summarize the entity, while too many entity related tweets may give rise to much noise for the entity model and Figure 6: Precision at K for the methods of QEFEM-ML and QEFEM-Filtering. it will decrease the efficiency if applying the EM algorithm. Figure 7 shows the performance change of QEFEM and QEFEM + SMM against different settings of the number of feedback tweets, i.e. M . From the figure, we can observe that either too small or too large M will lead to a performance drop of QEFEM in terms of MAP. This indicates the importance of choosing an appropriate M . Figure 7: Sensitivity of retrieval performance to # of feedback tweets M .
Recall that we interpolate the estimated feedback entity model with the original maximum likelihood model estimated on the plain query text. The interpolation is controlled by an adaptive coeffi-cient. When  X  = 0 , we only use the original query model (i.e.  X   X  ); when  X  = 1 , our approach emphasizes the feedback entity model most. Actually, for queries which are entities themselves, our method of  X  = 1 simply uses the entity model while ignoring the original query model (i.e.  X  0 = 1 ). For the feedback entity model estimation, we set  X  E as 0 . 7 ,  X  C as 0 . 5 and use the top 50 entity-associated tweets to estimate the model. Then, we evaluate the performance with different  X  varying from 0 to 1 .
 Figure 8 shows the performance changes of QEFEM and QE-FEM + SMM against different values of  X  . Note that, when  X  = 0 , QEFEM degenerates to the baseline method SimpleKL , while QEFEM + SMM degenerates to SimpleKL + SMM . From the figure, we can observe that the value of  X  yields a significant effect on the retrieval performance. It is important to choose an appropriate  X  . Since the entity model can only represent the entity part of the original query, setting  X  too large will result in a perfor-mance drop as it leads to much information loss about the original query. When setting  X  around 0 . 6 , QEFEM and QEFEM + SMM can reach their optimal MAP scores. Besides, with the feedback en-tity model, the search engines can get more high-quality feedback tweets for the query. This could benefit the traditional PRF-based query expansion methods to gain additional performance improve-ments.
Note that we propose two methods to fetch the feedback tweets, i.e. entity match method and entity query method. In the previous sections, we mainly use the entity match method. In the following, we take further discussion on the main differences between these two methods. First, when using the entity match method, those tweets which have an exact entity match can be fetched. When us-ing the entity query method, we can get much more tweets since the partial match can also fetch back the related tweets. However, this could lead to a high probability of introducing irrelevant en-tities. Second, to reflect the temporal aspect of the entity model, the entity match method simply sorts the tweets by chronological order while the query match method incorporates the recency infor-mation into a temporal prior in the language modeling framework.
We evaluate the performance of QEFEM using entity match and entity query methods. Both of them use top 50 feedback tweets. The temporal prior r is set as 0 . 01 for the entity query method. The average and minimum count of feedback tweets fetched with different methods ( M = 50 ) are summarized in Table 7. For the model estimation, we set parameter  X  E = 0 . 7 and  X  C = 0 . 5 . The interpolation coefficient  X  is set as 0 . 6 .
 Table 7: Feedback tweets count analysis using different entity feedback acquisition methods.

Method Average Count Minimum Count entity match 34 2 entity query 46 17
Table 8 shows the performance comparison of QEFEM using the two feedback acquisition methods. From the table, we can observe that both of the methods can improve the retrieval per-formance significantly compared with the baseline method. This indicates the effectiveness of both methods. Besides, the retrieval performances of the two methods are comparative across all the evaluation metrics.
 Table 8: Performance comparison of different entity feedback acquisition methods.

In this study, we propose to use feedback entity model to uti-lize the rich entity information in Twitter and solve the challenges such as the vocabulary mismatch problem in microblog search. By incorporating the feedback entity model into language modeling framework, the queries containing entities can be more compre-hensible and thus more relevant documents can be retrieved. As a result, combining simple mixture model can gain further perfor-mance improvement as the feedback tweets contain more accurate word distribution than by initial search. Moreover, as we track the latest tweets containing the entity to build a feedback entity model, our method could also satisfy the real-time information need in microblog retrieval. Our thorough evaluation, using two standard TREC collections, demonstrates the effectiveness of the proposed method.

Many studies remain for the future work. (1) One of the most in-teresting directions is to generalize our method for topics not con-taining any entity by searching for top related entities (i.e. pseudo named entities of topics). (2) In this paper, we simply use the gen-eral entity model to update the query. In fact, entities can be am-biguous in different domains (e.g. apple in food and technology domains) and can have quite different word distribution in each do-main. Thus, if we can recognize the concerning domain of the user query, we can use the domain-specific entity model to update the query. (3) Besides, when using entity model to update the query, we use an adaptive parameter  X  which only takes account of the IDF information of entity and query terms. However, due to the difference in the entities and entity-associated tweets, this balance parameter can be optimized for each entity and feedback tweets with a supervised method. The work reported in this paper is supported by the National Natural Science Foundation of China Grant 61370116. We thank anonymous reviewers for their beneficial comments. We also thank Jiang Bian, Lili Yao and Yue Fei for valuable suggestions related to this paper. [1] A. Z. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, [2] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good [3] J. Dalton, L. Dietz, and J. Allan. Entity query feature [4] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood [5] F. Diaz and D. Metzler. Improving the estimation of [6] M. Efron. Hashtag retrieval in a microblogging environment. [7] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity [8] M. Keikha, S. Gerani, and F. Crestani. Time-based relevance [9] J. D. Lafferty and C. Zhai. Document language models, [10] X. Li and W. B. Croft. Time-based language models. In [11] Y. Li, W. P. R. Luk, K. S. E. Ho, and F. L. K. Chung. [12] F. Liang, R. Qiang, and J. Yang. Exploiting real-time [13] J. Lin and M. Efron. Overview of the TREC-2013 Microblog [14] Y. Lv and C. Zhai. A comparative study of methods for [15] K. Massoudi, M. Tsagkias, M. de Rijke, and W. Weerkamp. [16] M. Mitra, A. Singhal, and C. Buckley. Improving automatic [17] T. Miyanishi, K. Seki, and K. Uehara. Improving [18] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of [19] D. Pan, P. Zhang, J. Li, D. Song, J.-R. Wen, Y. Hou, B. Hu, [20] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [21] A. Ritter, S. Clark, Mausam, and O. Etzioni. Named entity [22] A. Ritter, Mausam, O. Etzioni, and S. Clark. Open domain [23] I. Soboroff, I. Ounis, and J. Lin. Overview of the [24] N. Stokes, Y. Li, L. Cavedon, E. Huang, J. Rong, and [25] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a [26] J.-R. Wen, H. Zhang, and J.-Y. Nie. Query clustering using [27] C. Zhai and J. D. Lafferty. Model-based feedback in the [28] C. Zhai and J. D. Lafferty. A study of smoothing methods for
