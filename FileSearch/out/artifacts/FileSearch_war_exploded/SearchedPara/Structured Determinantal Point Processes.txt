 The need for distributions over sets of structures arises frequently in computer vision, computational biology, and natural language processing. For example, in multiple target tracking, sets of structures of interest are multiple object trajectories [6]. In gene finding, sets of structures of interest are multiple proteins coded by a single gene via alternative splicing [13]. In machine translation, sets of structures of interest are multiple interpretations or parses of a sentence in a different language [12]. Consider as a running example the problem of detecting and tracking several objects of the same type (e.g., cars, people, faces) in a video, assuming the number of objects is not known a priori . We would like a distribution over sets of trajectories that (1) includes sets of different cardinality and (2) prefers sets of trajectories that are spread out in space-time, as objects are likely to be [11, 15]. Determinantal point processes [10] are attractive models for distributions over sets, because they concisely capture probabilistic mutual exclusion between items via a kernel matrix that determines which items are similar and therefore less likely to appear together. Intuitively, the model balances the diversity of a set against the quality of the items it contains (for example, observation likelihood of an object along the trajectory, or motion smoothness). Remarkably, algorithms for computing certain marginal and conditional probabilities as well as sampling from this model are O ( N 3 ) , where N is total number of possible items, even though there are 2 N possible subsets of a set of size N [7, 1] .
 The problem, however, is that in our setting the total number of possible trajectories N is exponential in the number of time steps. More generally, we consider modeling distributions over sets of struc-tures (e.g., sequences, trees, graphs) where the total number of possible structures is exponential. Our structured determinatal point process model (SDPP) captures such distributions by combining structured probabilistic models (e.g., a Markov random field to model individual trajectory quality) Figure 1: (a) A set of points in the plane drawn from a DPP (left), and the same number of points sampled independently (right). (b) The first three steps of sampling a DPP on a set of one-dimensional particle positions, from left to right. Red circles indicate already selected positions. The DPP naturally reduces the probabilities for positions that are similar to those already selected. with determinantal point processes. We introduce a natural factorization of the determinantal model into parts (as in graphical models and grammars), and show that this factorization together with a novel dual representation of the process enables tractable inference and sampling using message passing algorithms over a special semiring. The contributions of this paper are: (1) introducing SDPPs, (2) a concise dual representation of determinantal processes, (3) tractable message passing algorithms for exact inference and sampling in SDPPs, (4) experimental validation on synthetic mo-tion tracking and real-world pose detection problems. The paper is organized as follows: we present background on determinantal processes in Section 2 and introduce our model in Section 3; we de-velop inference and sampling algorithms in Section 4, and we describe experiments in Section 5. subsets of Y . P is called a determinantal point process (DPP) if there exists a positive semidefinite matrix K indexed by the elements of Y such that if Y  X  X  then for every A  X  X  , we have Here K A = [ K ij ] y adopt det( K  X  ) = 1 . We will refer to K as the marginal kernel, as it contains all the information needed to compute the probability of including any subset A in Y  X  X  . A few simple observations follow from Equation (1): That is, the diagonal of K gives the marginal probabilities of inclusion for individual elements of Y , and the off-diagonal elements determine the (anti-) correlations between pairs of elements: large values of K ij imply that i and j tend not to co-occur. Note that DPPs cannot represent distributions where elements are more likely to co-occur than if they were independent: correlations are negative. Figure 1a shows the difference between sampling a set of points in the plane using a DPP (with K ij inversely related to the distance between points i and j ), which leads to a set that is spread out with good coverage, and sampling points independently, where the points exhibit random clumping. Determinantal point processes, introduced to model fermions [10], also arise in studies of non-intersecting random paths, random spanning trees, and eigenvalues of random matrices [3, 2, 7]. The most relevant construction of DPPs for our purpose is via L-ensembles [1]. An L-ensemble defines a DPP via a positive semidefinite matrix L indexed by the elements of Y .
 where I is the N  X  N identity matrix. Note that P L is normalized due to the identity P
Y  X  X  det( L Y ) = det( L + I ) . L-ensembles directly define the probability of observing each subset of Y , and subsets that have higher diversity (as measured by the corresponding determinant) have higher likelihood. To get probabilities of item co-occurrence as in Equation (1), we can compute the marginal kernel K for the L-ensemble P L : Note that K can be computed from the eigen-decomposition of L = P N k =1  X  k v k v &gt; k by a simple re-scaling of eigenvalues: K = P N k =1  X  k  X  To get a better understanding of how L affects marginals K , note that L can be written as a Gram Y7 X  R D , where D  X  N and ||  X  ( y i ) || 2 = 1 . We can think of q ( y i ) as the  X  X uality score X  for item y and  X  ( y i ) &gt;  X  ( y j ) as normalized  X  X imilarity X  between items y i and y j .
 where  X  ( Y ) is a D  X | Y | matrix with columns  X  ( y i ) , y i  X  Y . We will use this quality*similarity based representation extensively below. Roughly speaking, P L ( y i  X  Y ) increases monotonically We briefly mention a few other efficiently computable quantities of DPPs [1]: where I Y\ A is the matrix with ones in the diagonal entries indexed by elements of Y\ A and zeros everywhere else. Conditional marginal probabilities P L ( B  X  Y | A  X  Y ) as well as in-clusion/exclusion probabilities P L ( A  X  Y  X  B  X  Y =  X  ) can also be computed efficiently using eigen-decompositions of L and related matrices.
 Sampling Sampling from P L is also efficient [7]. Let L = P N k =1  X  k v k v &gt; k be an orthonormal eigen-decomposition, and let e i be the i th standard basis N -vector (all zeros except for a 1 in the i th position). Then the following algorithm samples Y  X  X  L : Initialize: Y =  X  , V =  X  ;
Add each eigenvector v k to V independently with prob.  X  k  X  while | V | &gt; 0 do end Return Y ; This yields a natural and efficient procedure for sampling from P given an eigen-decomposition of L . It also offers some additional insights. Because the dimension of V is reduced by one on each iteration of the loop, and because the initial dimension of V is simply the number of selected eigenvectors in step one, the size of Y is distributed as the number of successes in N Bernoulli trials where trial k succeeds with probability  X  k  X  E To get a feel for the sampling algorithm, it is useful to visualize the distributions used to select y i at each time step, and to see how they are influenced by previously chosen items. Figure 1b shows this progression for a simple DPP where Y is the set of points in [0 , 1] , quality scores are uniformly 1, and the closer together they are. Initially, the eigenvectors V give rise to a fairly uniform distribution over points in Y , but as each successive point is selected and V is updated, the distribution shifts to avoid points near those already chosen. Y , Y, y i , N Y is the base set, Y is a subset of Y , y i is an element of Y , N is the size of |Y| DPPs are amazingly tractable distributions when N , the size of the base set Y , is small. However, we are interested in defining DPPs over exponentially sized Y . For example, consider the case where location of an object in the t -th frame of a video). Assuming there are n states at each time t and all state transitions are possible, there are n T possible sequences, so N = n T .
 In order to define a DPP over structures such as sequences or trees, we assume a factorization of decomposition. For a sequence, the scores can be naturally decomposed into factors that depend on assume a set of factors and use the notation y i X  to refer to the  X  part of the structure y i (similarly, we use y  X  to refer to the  X  part of the structure y ). We assume that quality decomposes multiplicatively We argue that these are quite natural factorizations. Quality scores, for example, can be given by a typical log-linear Markov random field, which defines a multiplicative distribution over structures. Similarity scores can be thought of as dot products between features of the two labelings. In our tracking example, the feature mapping  X  ( y it ) should reflect similarity between trajectories; e.g., features could track coarse-level position at time t , so that the model considers sets with tra-jectories that pass near or through the same states less likely. A common problem in multiple target tracking is that the quality of one object X  X  trajectory and its neighborhood  X  X ube X  is often much more likely than other objects X  trajectories as measured by an HMM or CRF model, so standard sampling from a graphical model will produce very similar, overlapping trajectories, ignoring less  X  X etectable X  targets. A sample from the structured DPP model would be much more likely to contain diverse trajectories. (See Figure 2.) Dual representation While the factorization in Equation (8) concisely defines a DPP over a structured Y , the more re-markable fact is that it gives rise to tractable algorithms for computing key marginals and condition-als when the set of factors is low-treewidth, just as in graphical model inference [8], even though L is too large to even write down. We propose the following dual representation of L in order to exploit the factorization. Let us define a D  X  N matrix B whose columns are given by B i = q ( y i )  X  ( y i ) , so that L = B &gt; B . Consider the D  X  D matrix C = BB &gt; ; note that typically D N (actually, the rank of B is at most O ( nT ) in the sequence case). The eigenvalues of C and L are identical, and eigenvalue  X  k . This connection allows us to compute important quantities from C .
 For example, to compute the L-ensemble normalization det( L + I ) = Q k (  X  k + 1) in Equa-tion (4), we just need the eigenvalues of C . To compute C itself, we need to compute BB &gt; = P dynamic programming solution. We discuss in more detail how to compute C for sequences (and for fixed-treewidth factors in general) in the next section. Assuming we can compute C efficiently, Figure 2: Sets of (structured) particle trajectories sampled from the SDPP (top row) and indepen-dently using only quality scores (bottom row). The curves to the left indicate the quality scores for the possible initial positions. we can eigen-decompose it as C = P k  X  k v k v &gt; k in O ( D 3 ) . Then, to compute P L ( y i  X  Y ) , the probability of any single trajectory being included in Y  X  X  L , we have all we need: Similarly, given two trajectories y i and y j , P L ( y i , y j  X  Y ) = K ii K jj  X  K 2 ij , where: We now turn to computing C using the factorization in Equation (8). We have C is equivalent to computing second moments of additive features (modulo normalization Z ). A naive algorithm can simply compute all O ( T 2 ) pairwise marginals p ( y  X  , y  X  0 ) and, by linearity of expectation, add up the contributions: C = Z P  X , X  0 P y However, we can use a much more efficient O ( D 2 T ) algorithm based on second-order semiring message passing [9]. The details are given in Appendix A of the supplementary material, but in short we apply the standard two-pass belief propagation algorithm for trees with a particular semiring in place of the usual sum-product or max-sum. By performing message passing under this second-order semiring, one can efficiently compute any quantity of the form: for functions p  X  0 , a , and b in time O ( T ) . Since the outer product in Equation (11) comprises D 2 quantities of the type in Equation (12), we can compute C in time O ( D 2 T ) .
 Sampling As described in Section 3, the eigen-decomposition of C yields an implicit representation of L : for this implicit representation is enough to efficiently perform the sampling procedure in Algorithm 1. The key is to represent V , the orthonormal set of vectors in R N , as a set  X  V of vectors in R D , with the mapping V = { B &gt; v | v  X   X  V } . Let v i , v j be two arbitrary vectors in  X  V . Then we have ( B V using their preimage in  X  V . This is sufficient to compute the normalization for each eigenvector B &gt; v , as required to obtain an initial orthonormal basis. Trivially, we can also compute (implicit) sums between vectors in V ; this combined with dot products is enough to perform the Gram-Schmidt orthonormalization needed to obtain  X  V  X  from  X  V and the most recently selected y i at each iteration. All that remains, then, is to choose a structure y i according to the distribution Pr( y i ) = the distribution can be rewritten as By assumption q 2 ( y i ) decomposes multiplicatively over parts of y i , and v &gt;  X  ( y i ) decomposes ad-ditively. Thus the distribution is a sum of |  X  V | terms, each having the form of Equation (12). We can therefore apply message passing in the second-order semiring to compute marginals of this distribution X  X hat is, for each part y  X  we can compute where the sum is over all structures consistent with the value of y  X  . This only takes O ( T |  X  V | ) time. In fact, the message-passing computation of these marginals yields an efficient algorithm for sam-pling individual full structures y i as required by Algorithm 1; the key is to pass normal messages forward, but conditional messages backward. Suppose we have a sequence model; since the forward pass completes with correct marginals at the final node, we can correctly sample its value before any backwards messages are sent. Once the value of the final node is fixed, we pass a conditional mes-sage backwards; that is, we send zeros for all values other than the one just selected. This results in condtional marginals at the penultimate node. We can then conditionally sample its value, and repeat this process until all nodes have been assigned. Furthermore, by applying the second-order semiring we are able to sample from a distribution quite different from that of a traditional graphical model. The algorithm is described in more detail in Appendix B of the supplementary material. We begin with a synthetic motion tracking task, where the goal is to follow a collection of particles as they travel in a one-dimensional space over time. This is the structured analog of the setting shown in Figure 1b, where elements of Y are no longer single positions in [0 , 1] , but are now sequences of such positions over many time periods. For our experiments, we modeled paths y i over T = 50 time steps, where at each time t a particle can be in one of 50 discretized positions, y it  X  X  1 , . . . , 50 } . While a real tracking problem would involve quality scores q ( y ) that depend on some observations, e.g., measurements over time from a set of physical sensors, for simplicity we determine the quality of a trajectory using only its starting position and a measure of smoothness over time: q ( y ) = in the middle with secondary modes on each side. The transition quality is given by q ( y t  X  1 , y t ) = f ( y t  X  1  X  y t ) , where f is the density function of the zero-mean Gaussian with unit variance. We scale the quality scores so that the expected number of selected trajectories is 5.
 We want trajectories to be considered similar if they travel through similar positions, so we define Intuitively, feature r is activated when the trajectory passes near position r , so trajectories passing through nearby positions will activate the same features and thus appear similar.
 Figure 2 shows the results of applying our SDPP sampling algorithm to this setting. Sets of trajec-tories drawn independently according to quality score tend to cluster in the middle region (second row). The SDPP samples, however, are more diverse, tending to cover more of the space while still respecting the quality scores X  X hey are still smooth, and still tend to start near the middle position. Pose estimation To demonstrate that SDPPs effectively model characteristics of real-world data, we apply them to a multiple-person pose estimation task. Our dataset consists of 73 still frames taken from various TV shows, each approximately 720 by 540 pixels in size 1 . As much as possible, the selected frames contain three or more people at similar scale, all facing the camera and without serious occlusions. Sample images from the dataset are shown in Figure 4. The task is to identify the location and pose of each person in the image. For our purposes, each pose is a structure containing four parts (head, torso, right arm, and left arm), each of which takes a value consisting of a pixel location and an orientation (one of 24 discretized angles). There are approximately 75,000 possible such values for each part, so there are about 4 75 , 000 possible poses. Each image was labeled by hand for evaluation. We use a standard pictorial strucure model [4, 5], treating each pose as a two-level tree with the torso as the root and the head and arms as leaves. Our quality scores are derived from [14]; they factorize  X  is a scale parameter that controls the expected number of poses in each sample, and  X  is a sharpness parameter that we found helpful in controlling the impact of the quality scores. (We set parameter values using a held-out training set; see below.) Each part receives a quality score q ( y p ) given by a customized part detector previously trained on similar images. The joint quality score q ( y p , y p 0 ) is given by a Gaussian  X  X pring X  that encourages, for example, the left arm to begin near the left shoulder. Full details of the quality terms are provided in [14].
 Given our data, we want to discourage the model from selecting overlapping poses, so we design our normal density function, and k y p  X  x r k 2 is the distance between the position of part p (ignoring angle) and the reference point x r . The parameter  X  controls the width of the kernel. Poses that occupy the same part of the image will be near the same reference points, and thus appear similar. We compare our model against two baselines. The first is an independent model which draws poses independently according to the distribution obtained by normalizing the quality scores. The second is a simple non-maxima suppression model that iteratively selects successive poses using the nor-malized quality scores, but under the hard constraint that they do not overlap with any previously selected pose. (Poses overlap if they cover any of the same pixels when rendered.) In both cases, the number of poses is given by a draw from the SDPP model, ensuring no systematic bias. We split our data randomly into a training set of 13 images and a test set of 60 images. Using the training set, we select values for  X  ,  X  , and  X  that optimize overall F 1 score at radius 100 (see below), as well as distinct optimal values of  X  for the baselines. (  X  and  X  are irrelevant for the baselines.) We then use each model to sample 10 sets of poses for each test image, or 600 samples per model. For each sample, we compute precision, recall, and F 1 score. For our purposes, precision is the fraction of predicted parts where both endpoints are within a particular radius of the endpoints of an expert-labeled part of the same type (head, left arm, etc.). Correspondingly, recall is the fraction of expert-labeled parts within a given radius of a predicted part of the same type. Since our SDPP model encourages diversity, we expect to see improvements in recall at the expense of precision. F 1 score is the harmonic mean of precision and recall. We compute all metrics separately for each sample, and then average the results across samples and images in the test set.
 The results over several different radii are shown in Figure 3a. At tight tolerances the SDPP performs comparably to the independent samples (perhaps because the quality scores are only accurate at the mode, so diverse samples are not close enough to be valuable). As the radius increases, however, the SDPP obtains significantly better results, outperforming both baselines. Figure 3b shows the curves for the arms alone; the arms tend to be more difficult to locate accurately. Figure 3c shows the precision/recall obtained by each model. As expected, the SDPP model achieves its improved F 1 score by increasing recall at the cost of precision. Figure 3: Results for pose estimation. The horizontal axis gives the distance threshold used to determine whether two parts are successfully matched. 95% confidence intervals are shown. Figure 4: Structured marginals for the pose estimation task on successive steps of the sampling algorithm, with already selected poses superimposed. Input images are shown on the left. For illustration, we show the sampling process for a few images in Figure 4. As in Figure 1b, the SDPP efficiently discounts poses that are similar to those already selected. We introduced the structured determinantal point process (SDPP), a probabilistic model over sets of structures such as sequences, trees, or graphs. We showed the intuitive  X  X iversification X  properties of the SDPP, and developed efficient message-passing algorithms to perform inference through a dual characterization of the standard DPP and a natural factorization.
 Acknowledgments The authors were partially supported by NSF Grant 0803256. [1] A. Borodin. Determinantal point processes, 2009. [2] A. Borodin and A. Soshnikov. Janossy densities. I. Determinantal ensembles. Journal of [3] D. Daley and D. Vere-Jones. An introduction to the theory of point processes: volume I: [4] P. Felzenszwalb and D. Huttenlocher. Pictorial structures for object recognition. International [5] M. Fischler and R. Elschlager. The representation and matching of pictorial structures. IEEE [6] D. Forsyth and J. Ponce. Computer Vision: A Modern Approach . Prentice Hall, 2003. [7] J. Hough, M. Krishnapur, Y. Peres, and B. Vir  X  ag. Determinantal processes and independence. [8] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques . The [9] Z. Li and J. Eisner. First-and second-order expectation semirings with applications to [10] O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied [11] J. MacCormick and A. Blake. A probabilistic exclusion principle for tracking multiple objects. [12] C. D. Manning and H. Sch  X  utze. Foundations of Statistical Natural Language Processing . MIT [13] T. Nilsen and B. Graveley. Expansion of the eukaryotic proteome by alternative splicing. [14] B. Sapp, C. Jordan, and B. Taskar. Adaptive pose priors for pictorial structures. In IEEE [15] T. Zhao and R. Nevatia. Tracking multiple humans in complex situations. IEEE Transactions
