 We present CONDOR, a tool for managing constraints towards im-proved data quality. As increasing amounts of heterogeneous data are being generated, integrity constraints are the primary tool for enforcing data integrity. It is essential that an accurate and up-to-date set of constraints exist to validate that the correct application semantics are being enforced. We consider the widely used con-straint, functional dependencies (FDs). CONDOR is an integrated system that identifies inconsistent data values (along with sugges-tions for clean values), and generates repairs to both the data and/or FDs to resolve inconsistencies. We extend the set of FD repair op-erations proposed in past work, by (1) adding a set of attributes to an FD; (2) transforming an FD to a conditional functional de-pendency (CFD); and (3) identifying redundant attributes in an FD. Our demonstration will showcase the visualization and interactive features of CONDOR to help users determine the best repairs that resolve the underlying inconsistencies to improve data quality.
As increasing amounts of heterogeneous data is generated, it has become increasingly difficult for organizations to derive value from raw data. Poor data quality has become a pervasive problem across all industries as real data often contains missing, erroneous, du-plicated, and incomplete information. Integrity constraints are the primary tool for preserving data integrity as they represent specific constraints are enforced in the database, then as the data evolves, it will continue to conform to the constraints. However, in prac-tice, not all the constraints will be strictly enforced, often for per-formance reasons. This occurs in data environments where there may be hundreds or thousands of constraints defined across depen-dent applications, and verifying each constraint can negatively af-fect performance. This weak enforcement policy allows the data to deviate from the constraints. Current declarative data cleaning ap-proaches have proposed making minimal cost changes to the data to make it consistent with the constraints [2, 8, 9].

However, modern applications are increasingly operating in dy-namic data environments where both the data and the constraints id Department Course Room Faculty t 1 Computer Science CS 240 JHE 110 Engineering t 2 Classical Studies CS 240 KTH 222 Humanities t 3 Communication Studies CS 240 KTH 225 Arts t 4 Computer Science CS 100 ITB 137 Engineering t 5 Materials Science MAT 165 JHE 242 Engineering t 6 Math MAT 165 MCH 210 Science t 7 Math MAT 144 MCH 135 Science t 8 Math MAT 364 MCH 120 Science t 9 Math MAT 475 MCH 330 Arts are evolving. Data does not remain static, but evolves as it is up-dated, and the usage and requirements of the data change. Simi-larly, the constraints may also change as the underlying business policies (that the constraints model) change, or in data integration tasks where the integrated data instance does not satisfy the con-straints at each data source. In these cases, the constraints and the data become misaligned. In such settings, it may be preferable to modify the constraints, rather than the data, because the semantics of the data or application has evolved [1, 6, 10], and the data may be more consistent under a new, modified set of constraints that rep-resent the application domain. For example, consider Table 1 list-ing information about University courses, and two functional de-pendencies (FDs), F 1 : Department  X  Faculty and F 2 : Course  X  Room . These FDs may have originally been defined such that they hold locally within each University department X  X  database (within each Faculty). However, if the data from departments across the University are integrated into a consolidated table such as Table 1, then the original FDs may no longer hold. We observe that tu-ples t 6  X  t 9 violate F 1 . Based on support in the data, it appears that the data value in t 9 [ Faculty ] should be updated from  X  X rts X  to  X  X cience X  . Tuples t 1  X  t 3 and t 5 ,t 6 do not satisfy F consider updating the room numbers to be the same, but there is insufficient evidence to determine which room number is the cor-rect one. Furthermore, we would be updating values all with the same error. Recent work has shown that in such cases, it may be preferable to modify the FDs [1, 6]. In this case, if we add attribute Department to the left side attributes of F 2 , then the modified F [Department, Course]  X  Room is now satisfied. The tuples that were in violation are now distinguished by adding the Department attribute. In addition to adding attributes to an FD F , we can con-sider finding a subset of tuples from the relation where F is satis-fied. This would require identifying a common characteristic (i.e., a condition) among the selected tuples. The FD F would then be modified to include this condition. As FDs are modified, the set of all FDs may, over time, contain redundant attributes. Having a set of non-minimal FDs with redundant attributes can negatively affect performance. In this case, we may want to identify such attributes and recommend they be removed. Hence, different types of incon-sistencies will require different types of repair operations. While past work has considered subsets of these repair operations [6, 5], this work aims to provide a full suite of data and constraint repair operations, along with interactive visualizations of repairs.
We have shown an example that highlights the need for tech-niques that consider both data and constraint modifications. This is crucial in environments where new data is integrated with tar-get databases, where business policies and regulations are regularly updated, and where the semantics and usage of the data evolves. In this demonstration, we present CONDOR , a system for CON-straint DiscOvery and Repair. Given a data instance and a set of constraints, CONDOR identifies the data inconsistencies and rec-ommends to a user how best to resolve these inconsistencies by proposing modifications to the data and/or to the constraints. We focus on functional dependencies (FDs), as such constraints are a primary tool used to enforce data consistency in practice. CON-DOR automates the repair process, and provides high accuracy re-pairs to re-align the data and the constraints.
CONDOR has been implemented as a working prototype us-ing C++ and Python, and offers an interactive web interface im-plemented using the web2py framework and JavaScript libraries. These include jQuery for simplified client-side HTML scripting, jQuery UI and Twitter Bootstrap for modular user interface com-ponents, and D3/NVD3 for data visualizations. CONDOR was de-ployed on an Intel Xeon CPU E5-2670 processor (8 cores, 20 MB cache) and 16 GB of memory, running nginx as the web server, and interfacing with the web2py application via the uWSGI protocol.
Figure 1 illustrates the architecture of CONDOR, consisting of the following components: violation detector, data repair genera-tor, constraint repair generator and unified repair engine . CON-DOR works as follows. Given a data instance I and a set of FDs  X  , they are passed to the violation detector. The violation detec-tor identifies all tuples that do not satisfy some FD F  X   X  . For F : X  X  Y , we define a tuple pattern p as a single tuple over XY that exists in  X  XY ( I ) . These tuple patterns are passed to the data repair generator and constraint repair generator for analysis. In the data repair generator, tuple patterns are compared against each other, and a set of candidate data repairs is considered. This candidate set of data repairs is passed to the unified repair engine which calculates the cost of the repairs. Alternatively, inconsisten-cies may have been caused by stale constraints, and CONDOR pro-poses modifications to the FDs. CONDOR considers three types of FD repair operations for F : adding a set of attributes to X , trans-forming F to a conditional functional dependency (CFD) [3], and identifying redundant attributes. Each of these FD modifications is considered, and passed to the repair engine, where the cost of the constraint repair is calculated. The unified repair engine compares the cost values across the different repair operations, and selects the repair with lowest cost. Candidate repairs that are not selected are returned to the repair generators. The unified repair engine consid-ers all types of repairs equally, and uses a cost model that allows data repairs and constraint repairs to be compared on an equal foot-ing. The lowest cost repair is selected and recommended to the user. In the following, we discuss each of these components in detail.
Violation Detector. This component takes the data and the set of defined FDs  X  , and computes the set of tuples that violate a F  X   X  . For example, in Table 1, tuples t 6  X  t 9 violate F 1 , and tuples t t ,t 6 violate F 2 .

Data Repair Generator. Given the set of violating tuples, we want to generate a set of candidate data repairs, i.e., modifications to the data that will resolve the inconsistencies. Let R be the set of relational attributes, I be an instance of a relation, and let N = | I | . A tuple pattern p is a single tuple over XY that exists in  X  We consider data repairs that can modify either values in the X or Y attributes. To compare potential data repairs, we assume for each A  X  XY a distance function, 0  X  dist A ( v 1 ,v 2 )  X  1 over the do-main values of A , indicating how similar the values v 1 and v We assume dist A ( v,v ) = 0 . Various distance functions tailored for the domain may be used. We use the Jaro-Winkler measure for strings. For tuple patterns, we assume the existence of a similar-ity measure between tuple patterns 0  X  sim ( p 1 ,p 2 )  X  1 where sim ( p,p ) = 1 . For our work, we define sim as the fraction of equal attributes in the patterns. We use a support threshold  X  and a pattern similarity threshold  X  . Intuitively, tuple patterns with a frequency over  X   X  N will not be considered as candidates for a data repair (we call these core patterns ). Furthermore, a candidate for a data repair (we call these deviant patterns ) must be at least as similar to some other (core) pattern as our threshold  X  . The data repair generator proposes a candidate set of data repairs that update the data values in deviant tuples to its corresponding core tuple X  X  data values.

Example: In Table 1, assume  X  = 0 . 3 , X  = 0 . 4 . We have the following core and deviant patterns, labelled p and d , respectively. For F 1 , p: (Math, Science) (with frequency = 3 / 9 &gt;  X  ), d: (Math, Arts) . We will consider a data repair to update the value Arts to the value Science . For F 2 , all the violating tuples have frequency equal to 1, and do not qualify to be core patterns.

Constraint Repair Generator. This component proposes a set of constraint repairs and passes them to the repair engine for cost-ing. In CONDOR, we consider three types of FD modifications described below: (i) Adding Attributes. For F : X  X  Y , we consider appending a set of attributes W  X  ( R \ XY ) to X . We use an algorithm based on reconstructability analysis that selects the set W based on the attribute value dependencies between W and XY . We evalu-ate attributes in W that have some correlation to attributes XY by computing a measure called the variance of information [6]. This computation measures how well the attributes in W separate the conflicting tuples in F . The objective is to make F more selective by adding additional attributes W that cleanly separate the violat-ing tuples from the clean tuples. Of course, we can consider mak-ing F a candidate key but this is the least desirable type of repair. In our continuing example, we consider adding attributes Course, Room to the antecedent of F 1 , and Dept, Faculty to F 2 . (ii) Transformation to a CFD. To transform F towards a CFD, our objective is to repair F by finding a subset of the data where F is satisfied. We use a similar approach to CFD discovery algorithms [4], where we identify the conditions (expressed by the attribute values in X ) where F is satisfied. (iii) Identify Redundant Attributes. Given the current set of FDs  X  , modifications to the FDs over time may result in a non-minimal set. This can be costly to enforce, particularly if redundant at-tributes exist in X . We apply the minimal cover algorithm to iden-tify any redundant attributes in  X  . For F : X  X  Y , we compute the closure of ( X \ B ) + for an attribute B  X  X . If the closure contains B , this indicates that we can infer B from the remaining FDs in  X  . We do this for each F  X   X  . We refer the reader to the full paper for further details [7].

Unified Repair Engine. We take the proposed repairs and put them together in a unified repair engine that computes the cost to implement each of these repairs. Given a set of constraints  X  and a data instance I that is inconsistent with  X  , the repair engine se-lects a set of low cost data and constraint repairs that produce a  X  and I 0 such that I 0 | =  X  0 . The repair engine uses a cost model based on the Minimum Description Length (MDL) Principle. We define M to be a model containing a set of signatures where each signature s  X   X  XY ( I ) is a tuple pattern. We want to find sig-natures that allow us to represent I via M as succinctly as pos-sible. The MDL principle allows us to quantify this intuition via the description length function, DL F = L ( M ) + L ( I | M ) where L ( M ) is the length of the model, and L ( I | M ) is the length of the data instance I given M . We want to minimize DL F . We define L ( M ) = ( S  X  X  XY | ) + r T , S is the number of signatures in the model M , and r T is the total data repair cost for updated tuples in I . Then L ( I | M ) = E  X  X  XY | where E is the number of tuples not modeled by the S signatures in M . For constraint repair, we consider a new FD F 0 modified by any one of the repair operations discussed above. | I | = F 0 | &gt; | I | = F | (there are more tuples satisfying F 0 than F ). We choose the constraint repair F DL F , where S 0 is the number of signatures in the model wrt F W is the number of attributes added to F 0 , and E 0 is the number of tuples not modeled by the S 0 signatures. At each step, the unified repair engine selects the repair that reduces DL the most.
Example: We show the cost evaluation for F 1 ( F 2 can be com-puted in a similar manner). The model M is initially empty, and we have L ( M ) = 0 and L ( I | M ) = 18 . For data repairs, we modify tuple t 9 [ Faculty ] = t 8 [ Faculty ] =  X  X cience X  . If we assume a unit cost to update each value, plus a unit cost to represent the (maximum) distance between the dirty and clean values, the cost of this update is 2  X  1 tuple = 2 . We get a signature (Math, Sci-ence) . We increase L ( M ) by 2 to represent the new signature (a unit cost to represent a value), and decrease L ( I | M ) by how fre-quent (Math, Science) occurs in I . The final description length is DL 0 data = L 0 ( M ) + r T + L 0 ( I | M ) = 2 + 2 + (18  X  8) = 14 . In constraint repair, we consider adding attribute Course to F antecedent. There are no signatures in M , and DL 0 fd = 27 (3 at-tributes in the new FD times 9 tuples). Since DL 0 data &lt; DL select data repairs as the preferred fix.
We highlight CONDOR X  X  features and visualizations by engag-ing the user towards the best set of data and/or constraint modifica-tions to improve overall data quality. We use two real datasets: (1) Yelp data, from the Yelp dataset challenge, which provides informa-tion on local businesses; and (2) movies data scraped from the film review site rottentomatoes.com . We present two demo scenarios: 1) Continuous Repair : Using a sample from the Yelp dataset and the movies dataset, we will guide the user through an iterative re-pair process, where on each iteration, the data and the constraints will change. In reality, data and constraints do not remain static, and evolve over time. We simulate this behaviour, and on each iter-ation, the data will differ from the previous iteration (i.e, data val-ues are updated, new tuples are inserted), and the set of constraints are modified to include new constraints, and antecedent attributes are added or removed. The user will be able to compare the dif-ferent repair alternatives as CONDOR provides the visualizations showing how much of the data and the constraints are corrected, and at what cost. This scenario reflects the conditions that modern applications operate in dynamic data environments, and highlights the evolution of the data and of the constraints, and how the repairs are changing due to this evolution. 2) Scalability Analysis : We will show the performance and scal-ability of CONDOR as the schema complexity increases. Specifi-cally, the user will be able to select from a set of data samples that have increasing attribute domains, and an increasing number of at-tributes. Similarly, the user will be able to evaluate the qualitative impact by selecting from a set of constraints that have an increasing number of antecedent attributes, constraints that have overlapping attributes, and an increasing number of constraints.
This demonstration highlights the features of CONDOR, an in-tegrated and interactive data quality tool that focuses on data and a suite of constraint repair operations to resolve underlying data in-consistencies. Our working prototype allows users to interactively manage their data quality constraints, and provides data visualiza-tions to facilitate a deeper understanding of the underlying attribute value and constraint relationships.
