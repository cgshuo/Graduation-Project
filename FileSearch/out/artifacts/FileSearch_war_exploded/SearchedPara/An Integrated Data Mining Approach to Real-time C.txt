 Clinical study found that early detection and intervention are essential for preventing clinical deterioration in patients, for patients both in intensive care units (ICU) as well as in general wards but under real-time data sensing (RDS). In this paper, we develop an integrated data mining approach to give early deterioration warnings for patients under real-time monitoring in ICU and RDS.

Existing work on mining real-time clinical data often focus on certain single vital sign and specific disease. In this paper, we consider an integrated data mining approach for general sudden deterioration warning. We synthesize a large feature set that includes first and second order time-series features, detrended fluctuation analysis (DFA), spectral analysis, ap-proximative entropy, and cross-signal features. We then sys-tematically apply and evaluate a series of established data mining methods, including forward feature selection, linear and nonlinear classification algorithms, and exploratory un-dersampling for class imbalance.

An extensive empirical study is conducted on real patient data collected between 2001 and 2008 from a variety of ICUs. Results show the benefit of each of the proposed techniques, and the final integrated approach significantly improves the prediction quality. The proposed clinical warning system is currently under integration with the electronic medical record system at Barnes-Jewish Hospital in preparation for a clinical trial. This work represents a promising step to-ward general early clinical warning which has the potential  X  Visiting doctoral candidate from Xidian University, China to significantly improve the quality of patient care in hospi-tals.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; J.3 [ Computer Applications ]: Life and medical sciences Experimentation, Algorithms, Performance Real-time clinical monitoring, deterioration warning, time-series classification, feature selection
Every year, 4-17% of patients undergo cardiopulmonary or respiratory arrest while in hospitals. Lots of these patients could have been saved if warning of serious clinical events could be provided early, before its occurrence rather than when it is happening. Early prediction based on real-time electronic monitoring data has become an apparent need in many clinical areas.

At Washington University, we have carried out a NIH-funded clinical trial of a real-time patient monitoring system in a step-down unit at Barnes-Jewish Hospital, one of the largest hospitals in the United States [9]. This clinical trial uses wireless sensing devices to collect real-time vital sign data for patients not only in ICU but also in general units. The goal of this pioneering study is to show the feasibility of using data mining algorithms to give early warning of sudden deterioration, ultimately leading to the prevention of death.

However, most prior studies focus on some specific disease prediction. For example, McQuatt et al. [2] introduced a de-cision tree method to analyze head injury. Loforte et al. [14] found statistical indexes for detecting sepsis by investigating the relationship between heart rate and respiration. Khosla et al. [6] applied multiple machine learning techniques for stroke prediction. There have been little study on general prediction and warning for serious clinical deterioration and death.

Data mining on clinical data has great potential to im-prove the treatment quality of hospitals and increase the survival rate of patients. Data-driven prediction technol-ogy strongly hinges on the data collection of patients X  vital signs. In most hospitals, only intensive care units (ICUs) are equipped with real-time electronic medical sensors. In general hospital units, patients X  vital signs are typically col-lected manually by a nurse, at a granularity of only a handful of readings per day, which confronts us with the challenged of sparseness and irregularity of data. However, most find-ings suggest that real-time monitoring data would be benefi-cial for a better treatment [10]. To handle this problem, our team has proposed a real-time data sensing (RDS) system, which enables patients X  vital signs data in general hospital units to be collected via wireless sensors. Currently, RDS can provide constant monitoring of patients X  heart rate and oxygen saturation rate. Through a pilot study at the Barnes-Jewish Hospital, the RDS system has been installed in its step-down unit. The success of RDS will dramatically en-large the population that have real-time monitoring data. Hence, there is a critical need for a data mining system that can effectively utilize the multi-dimensional, real-time time-series data from ICU and RDS in order to prevent de-terioration and mortality in hospitalized patients.
In this paper, we develop an integrated data mining ap-proach to identify the signs of clinical deterioration and pro-vide early warning for possible mortality. In particular, we build classification methods to monitor real-time signal of heart rate and oxygen saturation rate of patient and is-sue early warning alerts before clinical deterioration/death. This system enables at-risk patients to be timely checked and treated by healthcare professionals in order to prevent potential deterioration and death.

Studies have found that real-time clinical data has certain unique features, due to the underlying dynamics of biologi-cal systems [5]. Therefore, some dedicated algorithms based on nonlinear dynamics, such as detrended fluctuation anal-ysis (DFA) and spectral analysis, have been proposed for clinical data. However, such prior work has two downsides. First, most of them consider only a single vital sign. For example, Penzel et al. [12] used DFA and spectral analy-sis for sleep apnea detection leveraging on the heart rates. Mining multiple time series is a more challenging problem. Second, time series data has rich information that previous work did not make full use of. For single time series, there are first-order and second-order features, as well as some so-phisticated patterns, such as DFA, spectra and entropy. For multiple time series, features such as correlation and coher-ence can be used. Our approach not only leverages all these information but also performs feature selection to select the most relevant and discriminative features. For classification, researchers in the clinical community routinely use simple linear classifiers [5]. Our system incorporates more robust classifiers such as SVM with RBF kernels and logistic re-gression. Moreover, we use an exploratory undersampling method to address the challenge of class imbalance widely observed in clinical data.

In summary, this paper contains the following contribu-tions: 1. We develop an integrated data mining approach to 2. We bridge the gap between biomedical community and 3. We strengthen the early warning system by apply-4. We apply our integrated approach to a large collec-
The rest of this paper is organized as follows. Section 2 surveys the related work in detecting clinical deterioration. An overview on our early warning system is presented in Section 3. Section 4 describes our general approach and the evaluation criterion. Section 5 lists and describes all the features extracted from multiple vital sign time series. Section 6 and Section 7 describe the feature selection and prediction approach, respectively. Section 8 shows the ex-perimental result of our real-time early warning system. Fi-nally, we draw conclusions in Section 9.
Medical data mining is a key technique to extract useful clinical knowledge from medical records. A number of scor-ing systems exist that use medical knowledge for various medical conditions.

For example, Several Community-Acquired Pneumonia (SCAP) and Pneumonia Severity Index (PSI) were used to predict outcomes in patients with pneumonia [19]. Similarly, out-comes in patients with renal failures may be predicted us-ing the Acute Physiology Score, Chronic Health Score, and APACHE score [18]. In [21], Zhou proposed a Multi-Task Learning Formulation for Predicting Alzheimer X  X  Disease. The integration of heterogeneous data (neuroimages, demo-graphic, and genetic measures) for AD prediction based on a kernel method was proposed in [20].

Detrended fluctuation analysis (DFA) and spectral analy-sis for heart rate variability were evaluated to classify sleep apnea and normal sleep [12]. RR (inter-beat) interval and spontaneous respiration were analyzed using approximated entropy(ApEn) and regularity index to distinguish sepsis [14]. Decision tree is introduced in predicting the outcome of head injury patients using both background (demographic) data and temporal (physiological) data [2]. Also, SVM and fea-ture selection are employed to predict stroke [6].

However, most of these algorithms are designed for some specific diseases and to be used in some specialized hos-pital units. In contrast, the detection of clinical deterio-ration requires more general algorithms. For example, a team at the John Hopkins University developed the Modi-fied Early Warning Score (MEWS) [7], which uses manually-collected systolic blood pressure, pulse rate, temperature, respiratory rate, age and BMI to predict clinical deteriora-tion. Our team has also developed a learning algorithm to identify high-risk patients based on clinical data collected by nurses [9]. However, both of these works are applied to manually collected data which has only a handful of readings per day. Here, our goal is different. We are aiming at min-ing real-time vital signs read by electronic devices at ICU and wireless sensors in our project. Such data is very dif-ferent from manually collected data, as they are regular and have high frequency (reading gaps being minutes instead of hours).
Our work aims to prevent clinical deterioration in pa-tients, for both patents in ICU as well as in general hos-pital units equipped with sensing devices, such as our RDS system.

In our RDS pilot study at the Barnes-Jewish Hospital, patients are provided with wireless sensor network (WSN) devices which collect and stream real-time vital sign data to the learning system. If deterioration is predicted, a warning is sent to nurses on the patient X  X  floor over the hospital X  X  paging system. The nurses may then intervene to prevent deterioration. Figure 1 shows a WSN-based wireless pulse oximeter device developed by our team, which is capable of collecting equipped patent X  X  real-time heart rate and oxy-gen saturation rate and then route the data to a wired access point using an onboard radio based on the IEEE 802.15.4 radio standard. WSN network coverage is achieved by plug-ging additional TelosB nodes into electrical outlets in pa-tients X  rooms and in the hallway. These new nodes will autonomously locate other nearby nodes and participate in routing sensor data from patient nodes to a wired access point, where they are entered into our data mining system.
RDS enjoys robust system reliability and lifetime. It achieved high network reliability, with a median of 99.68% of packets successfully delivered to the base station. Network outages were infrequent and had a 95%-percentile time-to-recovery of 2.4 minutes. Patient nodes achieved a lifetime of up to 69 hours from a 9V battery power source.

The establishment of the RDS represents a important step toward early clinical warning that has the potential to sig-nificantly improve the quality and outcome of patient care in hospitals. RDS shows the feasibility that, real-time phys-iologic data will be available for not only patients at ICU, but also patients in the general hospital units with real-time monitoring. Due to the inevitable trend of popular uses of such sensing systems in hospitals, our data mining approach is meeting a critical challenge in the healthcare industry and has the potential of very broad usage.
In this section, we overview the proposed approach for early prediction of clinical deterioration/death based on real-time monitoring data from ICU and RDS. Specifically, in this work, we try to predict mortality (death) of hospitalized patients based on two vital signs that are most popular: heart rate and oxygen saturation rate. Our pulseox sensor can measure these two signs. The procedure consists of the following steps: Figure 1: A wireless pulse-oximeter node developed at Washington University.
The evaluation of the prediction performance is based on the following criteria: AUC (Area Under receive operat-ing characteristic Curve), PPV (Positive Predictive Value), NPV (Negative Predictive Value), sensitivity, and specificity. In clinical community, the PPV stands for the proportion of patients who actually suffer deterioration/death, among the candidate patients who are warned by our system. A high NPV means that patients who survived are rarely misclas-sified. PPV/NPV and sensitivity/specificity are trade-off pairs where typically lowering one makes the other higher. PPV and NPV are also sensitive to how imbalanced the dataset is. Hence, we adopt the AUC as our main metric since it is a comprehensive measurement combining sensitiv-ity and specificity to deal with the imbalance dataset.
In this section, we describe all the features extracted from patients X  vital sign time series -heart rate and oxygen sat-uration rate. There are in total 34 features from these two time series data, including features within single time series and features linking the two time series.
In stochastic processes, chaos theory and time series anal-ysis, detrended fluctuation analysis (DFA) is a method for determining the statistical self-similarity of a signal [5]. Self-similarity is a key feature that is widely observed in natural systems, including human physiological signs. Mathemati-cally, DFA is a scaling analysis method to reveal long-range power-law correlation exponents in noisy time series [5]. It Figure 2: DFA analysis on the heart rates of two ICU and two non-ICU patients. is most suitable for non-stationary time series with slowly varying trends, such as heart rate and oxygen saturation rate. The DFA of a time series is calculated as the average fitting error over all segments in different scales. Given a time series { x ( i ) } ,1  X  i  X  N , integration is performed to convert the original time series as follows: where Next, the integrated time series y ( i ) ( i = 1 , 2 ,...N ) is divided into boxes of equal length n . A polynomial function y is fitted to each box of length n by minimizing the least square error. Then, we detrend the integrated time series y by subtracting y n from each box. The root-mean-square fluctuation of the detrended time series is calculated by Typically, F ( n ) increases with n and follows the power law: F ( n )  X  n  X  . Using a log-log plot, we can get a nearly linear curve as shown in Figure 2. The scaling exponent  X  char-acterizes the self-similarity level of original time series x ( t ), which can be calculated using the slope of this curve.
As pointed out in [11], it is important to differentiate the short-range and long-range levels of self-similarity. Hence, we divide the curve into two pieces and fit a linear function to each piece of curve, which generates two slops - X  1 and  X  .  X  1 represents the slope of curve log( F ( n )) vs. log( n ) in the range 1  X  n  X   X  ( N ) while  X  2 represents the slope of curve in the range  X  ( N ) &lt; n  X  N . In this way, we can see that  X  1 reflects the short-range self-similarity level and  X  2 reflects the long-range self-similarity level. As different patients have time series of different length, we cannot apply the same curve-segmentation method as in [12]. Instead, we select the  X  ( N ) by a ratio  X  , where  X  ( N ) =  X   X  N . Regarding the selection of  X  , we first sample two groups of died patients and survived patients, respectively. Next, for a fixed  X  , we calculate the  X  1 for each patient as well as the sum of all  X  1 of each group.  X  is selected to maximize the difference between sums of  X  1 of the two groups.
Approximate entropy was introduced by Pincus to quan-tify the time series complexity closely related to entropy [13]. It is a measurement designed to quantify the degree of regu-larity versus unpredictability. It quantifies the unpredictabil-ity of fluctuations in a time series. A low value of the entropy indicates that the time series is deterministic while a high value means that the time series is unpredictable(randomness).
To compute the approximate entropy (ApEn) of a time series, we divide the time series into N  X  m + 1 sub-series, calculate the similarity between each other and then figure out the entropy. First, sub-series of vectors of length m , are derived from the original signal. The distance D ( i,j ) between two vectors v ( i ) and v ( j ) is defined as the maximum difference in the scalar components of v ( i ) and v ( j ). Then N m,r ( i ), the number of vectors j such that the distance between the vectors v ( j ) and the generic vector v ( i ) is lower than r , is computed. The index r is a fixed parameter which sets the  X  X olerance X  of the comparison. Then we consider C m,r ( i ), the probability to find a vector that differs from v ( i ) by a distance less than r , as: and the logarithmic average over all the vectors of the C probability is calculated as follows: ApEn is given by: As recommended in [13], we use m = 2, r = 20% of the standard deviation of the time series in our analysis.
Spectral analysis is another major method for analyzing clinical time-series data [12]. For the calculation of the power spectra, the time series was resampled at 3 . 4 Hz us-ing interpolation. The mean value, the standard deviation value was subtracted from the time series before applying the Fast Fourier Transformation (FFT). In case of having less than 2 N of records, zero-padding was used. The spec-tral analysis and the interpolation routine was implemented using Matlab 7.11.0. We calculated the component values for VLF ( &lt; =0.04HZ), LF (0.04-0.15HZ), HF (0.15-0.4HZ), and the ratio LF/HF for each time-series. Figure 3 shows an example spectral distribution of the heart rate of an ICU patient.
For first order features, we use some traditional statisti-cal ones, such as mean (  X  ), standard deviation (  X  ), skewnes s(  X  1 ), and kurtosis (  X  2 ). In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable. The skewness value can be positive or negative, or even undefined. Qual-itatively, a negative skew indicates that the tail on the left side of the probability density function is longer than the right side and the bulk of the values (possibly including the median) lie to the right of the mean. A positive skew indi-cates the opposite. A zero value indicates that the values are relatively evenly distributed on both sides of the mean, typi-cally but not necessarily implying a symmetric distribution. Kurtosis is a measure of the  X  X eakedness X  of the probability distribution of a real-valued random variable. In a similar way to the concept of skewness, kurtosis is a descriptor of the shape of a probability distribution and, just as for skewness, there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. The computation we used is as follows:
Here, we employ the description which related to co-occurrence features in one dimensional time series [1]. First, the data is quantized into Q levels. Second, a two dimensional matrix c ( i,j ) is constructed (1  X  i,j  X  Q ). Point (i,j) in the matrix represents the number of times that a point in the sequence with level i is followed, at a distance d 1 , by a point with level j . Figure 4 shows how this process works. Finally, five co-occurrence features, including energy ( E ), entropy Figure 4: The process that transfers an one dimen-sional time series to two dimensional matrix. ( S ), correlation (COR)(  X  x ,y ), inertia ( F ), and local homo-geneity (LH), are calculated using the following equations: where: In our experiments, we set Q = 10.
We also consider features that link multiple vital signs together, including linear correlation and coherence.
Correlation indicates the strength and direction of a linear relationship between two random variables. In general, it refers to the departure of two variables from independence and equals:
Coherence provides both amplitude and phase informa-tion about the frequencies held in common between the two time series and is defined by: where  X  x 1 x 2 is the cross-spectral density, and  X  x  X  2 x 2 are autospectral densities. Coherence is a value be-tween 0 and 1. If X 1 and X 2 correspondes to the single input and output of a constant parameter linear system, their coherence C 1 , 2 will always be equal to one. Otherwise, if X 1 and X 2 are completely unrelated, the coherence will be zero.
Feature selection in supervised learning has been well stud-ied, which aims to find the most relevant features and pro-duce better classification performance than using all the fea-tures. In contrast to other dimensionality reduction tech-niques like those based on projection (e.g. principal compo-nent analysis) or compression (e.g. using information the-ory), feature selection techniques do not alter the original representation of the variables, but merely select a subset of them. Thus, besides reducing the utilization time and stor-age requirements, they preserve the original semantics of the variables, hence offering the advantage of interpretability to users [16].

While there are many classes of feature selection meth-ods, here we adopt a forward feature selection algorithm which does not depend on a particular classifier. Forward feature selection is a simple search strategy to find useful fea-tures [4]. The basic idea is we starts with an empty feature subset and adds one variable each step until a predefined number of features is reached, or the approximation result does not improve any further.
 We use two metrics for our forward selection, AUC and F-score. The AUC is the area under the ROC curve. F-score [3] is defined as: F ( i )  X  ( x i where x i , x i (+) , x i (  X  ) are the average of the i th feature of the whole, positive,and negative data sets, respectively; x is the i th feature of the k th positive instance, and x the i th feature of the k th negative instance. The numerator indicates the discrimination between the positive and nega-tive sets, and the denominator indicates the one within each of the two sets. The larger the F-score is, the more likely this feature is more discriminative.
In our approach, we apply Support Vector Machine (SVM) and logistic regression for prediction. In addition, to deal with the class imbalence of our dataset, we adopt the ex-ploratory undersampling method.
SVM is one of the most popular classification methods based on statistical learning theory. The key idea is to learn an optimal hyperplane that can separate the training data set with maximum margin [17]. Some previous work in DFA and spectral analysis use a linear separator [5, 11, 12] which corresponds to SVM with a liner kernel. Generally, by im-porting non-linear kernel, SVM has higher accuracy com-pared to other linear classifiers.
Logistic regression is a model for predicting the probabil-ity of an event, which can also be used for binary classifi-cation. Logistic regression has the benefit of being able to output a numerical score to reflect the severity of the pa-tient. Also, logistic regression allows us to control the sen-sitivity/specificity tradeoffs by adjusting the classification threshold. Looking through the records, we have a skewed dataset. Among 772 records, there are 175 records that belongs to the positive (death) set. Undersampling [8] is a very popular method in dealing with the class-imbalance problem. The idea is to combine the minority class with only a subset of the majority class each time to generate a sampling set, and take the ensemble of multiple sampled models. We have tried undersampling on our data but obtained very modest improvements.

To improve the performance further, we used a recent method called exploratory undersampling [8], which makes better use of the majority class than simple undersampling. The idea is to iteratively remove those samples that can be correctly classified by a large margin to the class boundary by the existing model.

Specifically, we fix the number of the died patients, and then randomly choose the same amount of survived patients to build the training dataset at each iteration. The main difference to simple undersampling is that, each iteration, it removes 5% in both the majority class and the minority combination. class with the maximum classification margin. For logistic regression, we remove those died patients that are closest to 1 (the class label of death) and those survived patients that are closest to 0. For SVM, we remove those correctly classi-fied patients with the maximum distance to the boundary.
In this section, we present the experimental result of the performance of our integrated learning algorithms. We first introduce the database of real-time vital signs used in our study. Then, we show and discuss the experimental results on the performance and advantages of various proposed tech-niques.
Since our RDS system is still in a smaller scale clinical trial and does not yield enough data, we test our approach based on the MIMIC II (Multiparameter Intelligent Monitoring in Intensive Care) [15] database which contains comprehen-sive clinical data from tens of thousands of Intensive Care Unit (ICU) patients. Data were collected between 2001 and 2008 from a variety of ICUs (medical, surgical, coronary care, and neonatal). The database denotes the outcome of each patient while they are in ICU (died or survived). The database also includes thousands of records of contin-uous high-resolution physiologic waveforms and minute-by-minute numerical time series (trends) of physiologic mea-surements. Since our learning model is to be embedded into an early warning system that is based on heart rate and oxygen saturation rate collected by wireless sensors, we only extract the time series of heart rate and oxygen satu-ration rate of patients for training the model from MIMIC II. In addition, we only consider patients who died in hos-pital and who left hospital and was not reported dead at home, while excluding patients who left hospital but died at home. By doing this, we rule out the possibility of judging well-conditioned patients at hospital as soon-to-die cases be-cause of their death at home. We are finally left with 772 patient records and realize that this extracted dataset suf-fers the class-imbalance problem, i.e. most patients in the dataset are from one class (no deterioration). To deal with this skewed dataset(597:175), later on we will adopt explo-rary undersampling and demonstrate its improvement on performance.

Our experiment is based on 10-fold cross validation. That is, we divide the data set into 10 smaller datasets with 9 training sets and 1 test set. Each subset is validated once as testing set. The results are the average of all the 10 valida-tions. In addition to listing the AUC, we also give the sen-sitivity, specificity, PPV and NPV. For logistic regression, since we can vary its results by changing the threshold, we always set the threshold so that the specificity close to 0.95 by choosing a proper threshold when presenting sensitivity, PPV and NPV. For the implementation of nonlinear SVM, we use a RBF kernel.
Prior study [12] in clinical community only considers one single time series with linear separation. We evaluated a number combinations of classifiers and features listed in Ta-ble 1 to demonstrate that classifying multiple time series with nonlinear separation will improve the prediction per-formance for clinical data. For comparison, we employed different classifers with DFA features on heart rate only, oxygen saturation rate only, and both.

From Table 1, we could observe the following facts. First, on single time series, we discover that kernel SVM outper-forms linear SVM, showing that clinical data with sparse features is better separated by a nonlinear classfier. Second, the combination of both time series greatly improve the per-formance of each classifier, which justifies our motivation of combining multiple time series.
Previous methods only make use of limited features such as DFA [12]. In this section, we use a series of increasingly large feature set on two classifiers to see the performance of multiple features, as shown in Table 2. From the table, we can discover that as we add more features, the perfor-mance is improved. This is in contrast with Table 1, in which kernel SVM outperforms logistic regression when there are only a small number of features. It shows that all of the newly introduced features contribute to the improvement of performance. We can also observe that the logistic re-gression classifer outperforms SVM with RBF kernel in our dataset, which means that nonlinear SVM may suffer over-fitting when the number of features becomes larger.
To avoid overfitting and improve the generalization ability, we perform the forward feature selection (FFS) technique based on SVM and logistic regression to select the most rel-evant features. We adopt two kinds of selection criteria: F-Score and AUC. Table 3 shows the performance compar-RBF Kernel SVM Logistic Regression Table 2: The performance comparison of different features. ison of feature selection using SVM and logistic regression. We can observe that by using feature selection, both SVM and logistic regression get significant improvement on their performance. The selected features for SVM using F-score are standard deviation of Heart Rate, Inertia of Heart Rate, High Frequency of Oxygen Saturation, and Energy Oxygen Saturation. The selected features for SVM using AUC are standard deviation of Heart Rate, Approximate entropy of Heart Rate, Entropy of Oxygen Saturation, Local Homo-geneity of Oxygen Saturation, and Inertia of Heart Rate. And, logistic regression outperforms SVM, which makes lo-gistic regression our first choice for further experiments of our learning system. In addition, feature selection based on AUC outperforms the one based on F-Score, for both logistic regression and SVM. Finally, we can see that the number of features selected by logistic regression is larger than the one by SVM, which further demonstrates our conclusion that too many features tend to cause overfitting in SVM with RBF kernel.

Regarding the class-imbalance problem of our dataset, we apply an exploratory undersampling method whose result is shown in Table 4. We can see that, with all features used, logistic regression with exploratory undersampling trained on all features (AUC = 0.7767) outperforms the one with-out exploratory undersampling (AUC=0.7402). In addition, the combination of logistic regression, exploratory under-sampling and forward feature selection with AUC further improves the performance (AUC = 0.8082), which forms the current learning model for our early warning system. Com-paring the first row of Table 3 against the last row of Table 4, we can also see that exploratory undersampling also im-proves performance when feature selection is used on logistic regression.
Other than just achieving better result on prediction per-formance, with our learning system, we also provide some insights on the leading risk factors for patient deterioration and mortality. Table 5 lists the first dozen of features by our feature selection method with AUC score. We can see that all kinds of proposed features are selected, including DFA, spectral analysis, first order, second order, ApEn and cross-sign features.

Table 6 provides the 10 most significant features of our final logistic regression model, ordered by the magnitude of their coefficients in the model. Note that all feature values are normalize to between 0 and 1, so the coefficient denotes the sensitivity of the model to a feature. With this table, we can identify some factors that are highly related to dete-rioration. For example, if the local homogenity of patent X  X  Table 5: The first 12 selected features in logistic re-gression using forward feature selection with AUC.
 Table 6: The 10 highest-weighted variables of our fi-nal logistical regression model (with exploratory un-dersampling and feature selection based on AUC). heart rate (top ranked in Table 6) is low, then this patient is likely to suffer sudden clinical deterioration and death. The standard deviation of oxygen saturation rate is also a sig-nificant factor for clinical deterioration. It is confirmed by the clinical observation that high variation in a physiologic index strongly indicates the low stability of patients X  health status.
Preventing clinical deterioration and death in hospital pa-tients by mining electronic medical records is a promising and important trend in US hospitals. We have developed a predictive system that can provide early alert of deteriora-tion for patients under real-time monitoring in ICUs and in general hospital units, as enabled by wireless sensing systems such as the RDS system we developed in Barnes-Jewish Hos-pital. Our approach integrated features from a diversity of fields including chaos theory (DFA), signal processing (spec-tral analysis and entropy), and machine learning (time-series features). We showed that the combined feature set gives significant performance improvement. We also showed that robust classifiers such as kernel SVM and logistic regression outperform previously used linear classifiers. Moreover, we showed that using established data mining methods, includ-ing feature selection and exploratory undersampling, can also improve the performance. With a AUC over 0.8 and sensitivity near 0.5 at 0.95 specificity, our final logistic re-gression model is approved by medical experts for a clinical trial at a major hospital. on AUC score and F-score.
 Method AUC Specificity Sensitivity PPV NPV Logistic Regression + all features 0.7402 0.9483 0.3646 0.7000 0.8185 on the AUC. [1] R. J. Alcock and Y.Manolopoulos. Time-series [2] A.McQuatt, P.J.D.Andrews, D.Sleeman, V.Corruble, [3] Y. Chen and C. Lin. Combining svms with various [4] I. Guyon, S. Gunn, M. Nikravesh, and L. A.Zadeh, [5] K. Hu, P. Ch.Ivanov, Z. Chen, P. Carpena, and H. E. [6] A. Khosla, Y. Cao, C. Lin, H. Chiu, J. Hu, and [7] J. Ko, J. H. Lim, Y. Chen, R. Musvaloiu-E, A. Terzis, [8] X. Liu, J. Wu, and Z. Zhou. Exploratory [9] Y. Mao, Y. Chen, G. Hackmann, M. Chen, C. Lu, [10] C. Otto. Telemedicine in the canadian high artic and [11] C. Peng, S. Havlin, S. H. Eugene, and G. A. L. [12] T. Penzel, J. W.Kantelhardt, L. Grote, J.-H. Peter, [13] S. Pincus. Approximate entropy as a measure of [14] R.Loforte, G.Carrault, L.Mainardi, and A.Beuche. [15] M. Saeed, M. Villarroel, A. T. Reisner, G. Clifford, [16] Y. Saeys, I. Inza, and P. Larranaga. A review of [17] W. Vladimir N. Vapnik. Statistical Learning Theory . [18] W.A.Knaus, E.A.Draper, D.P.Wagner, and [19] Yandiola, P. P. Espana, A. Capelastegui, J. Quintana, [20] J. Ye, K. Chen, T. Wu, J. Li, Z. Zhao, R. Patel, [21] J. Zhou, Y. Lei, J. Liu, and J. Ye. A multi-task
