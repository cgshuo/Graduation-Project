 Clustering is a common technique used to extract knowl-edge from a dataset in unsupervised learning. In contrast to classical propositional approaches that only focus on sim-ple and flat datasets, relational clustering can handle multi-type interrelated data objects directly and adopt semantic information hidden in the linkage structure to improve the clustering result. However, exploring linkage information will greatly reduce the scalability of relational clustering. Moreover, some characteristics of vector data space utilized to accelerate the propositional clustering procedure are no longer valid in relational data space. These two disadvan-tages restrain the relational clustering techniques from being applied to very large datasets or in time-critical tasks, such as online recommender systems. In this paper we propose a new variance-based clustering algorithm to address the above difficulties. Our algorithm combines the advantages of divisive and agglomerative clustering paradigms to improve the quality of cluster results. By adopting the idea of Rep-resentative Object, it can be executed with linear time com-plexity. Experimental results show our algorithm achieves high accuracy, efficiency and robustness in comparison with some well-known relational clustering approaches. I.5.3 [ Patten Recognition ]: Clustering X  Algorithms Algorithms, Performance Clustering, Multi-type, Relational
Data mining aims to learn knowledge from a dataset. In unsupervised learning, clustering is a common technique to partition the dataset into a certain number of groups (clus-ters) with maximum intra-cluster similarity and minimum inter-cluster similarity. Classical clustering approaches only focus on simple and flat data, i.e. all data objects are of the same type and conventionally described by a list of nu-meric attribute values. The former condition assumes all data are stored in a single table, and the latter one make it possible to represent data as points in a multi-dimensional vector space. Hence, many mathematical methods, e.g. ac-cumulation or transformation, can be utilized to simplify the clustering process greatly. Unfortunately, the above as-sumptions are not held in many practical cases: Firstly, data might have various types of attributes: binary, categorical, string or taxonomy-based; secondly, data objects are usu-ally stored in several tables and their pairwise relationships are specified by the semantic links between tables. Those semantic links together with multi-type attributes compose a far more complex feature space for relational dataset than the Euclidean space, so both of them should be considered during the clustering process. Although the classical clus-tering algorithms are still applicable in the relational cases by means of combining the multiple tables into a single one with join or aggregation operations, it is not a good choice for the following reasons [11]:
For example, the ontology of a relational movie dataset is shown in Figure 1. Different concepts in the ontology have different data types, e.g.  X  X itle X ,  X  X earOfRelease X ,  X  X er-tificate X  and  X  X enre X  are string, numeric, categorical and taxonomy-based, respectively. An arrow in the figure indi-cate that an object of the source concept includes one or more objects of the target concept(s) as its member prop-erty. A bi-directional arrow means there exist recursive ref-erences between two concepts. Hence, a data object rep-resenting an actor will contain references of the movies he acted in, which in turn refer to other actors and directors he cooperated with in those movies. Figure 2 shows part of the object for Tom Hanks. Classical clustering approaches based only on data attributes (e.g. actors X  name) will gener-
Figure 2: Example object: Tom Hanks ( Depth =2 ) ate completely meaningless clusters. Instead, relational clus-tering approaches will consider movies, directors and other actors related to the current actor along the semantic links in the ontology to generate reasonable clusters. Even if ob-jects have no attributes, the linkage structure of the whole dataset itself can still provide useful information for cluster-ing.

Multi-type relational clustering has raised substantial re-search interest recently [3, 11, 15]. In contrast to classical propositional approaches, relational clustering can handle multi-type interrelated data directly and adopt the seman-tic information hidden in the linkage structure to improve the clustering result. However, the trade-off for the above advantages is: relational clustering needs to explore far more information, so its scalability and efficiency are reduced. For example, calculating the similarity between two data objects of type Actor as in Figure 2 becomes much more expensive than that in the propositional case. The second problem is that many conveniences in Euclidean space are not avail-able in relational space. For example, k -Means and BIRCH [16] require all data to be represented as vectors to support the operations of addition and division, because the gravity center of each cluster is used in the clustering procedure. However, such operations are not valid for data objects in the relational dataset, so relational clustering approaches (e.g. RDBC [8] and FORC [9]) have quadratic computa-tional complexity. The above two disadvantages restrain re-lational clustering techniques from being applied upon very large datasets, for example in an online movie recommender system or a bibliographic database system. Another non-trivial issue in the classical clustering approaches is: how to select the optimal number of clusters k as the input param-eter? Inappropriate value of k will lead to skew or  X  X nnatu-ral X  clusters [2]. This problem becomes even more severe in the reinforcement clustering algorithms, such as ReCoM [13] and LinkClus [15], because the skewed cluster result of one data type will be propagated al ong the relations to influence the partitioning of other data types.

In order to address the above difficulties, in this paper we propose a new variance-based clustering approach, named DIVA (DIV ision and A gglomeration). Our approach com-bines the advantages of divisive and agglomerative cluster-ing paradigms to improve the quality of cluster results. Un-like ReCoM or LinkClus that only consider the direct rela-tionships when clustering data objects of the current con-cept, we exploit multi-layered relational information in the phase of constructing data objects. By adopting the idea of Representative Object, we can perform the DIVA algorithm with linear computational complexity O ( N ), where N is the number of objects to be clustered. Since the multi-type re-lational information is considered when data instances are constructed and DIVA generates the clusters of different data types separately, the problem of skewness propagation withinreinforcementclusteringappr oachescanbeavoided.
The rest of our paper is organized as follows: Section 2 introduces our method for constructing multi-type relational objects and the corresponding similarity metric. On that basis, we explain in detail the DIVA algorithm and analyze its computational complexity in Section 3. Comprehensive experimental results are provided in Section 4. Section 5 presents some prominent algorithms for propositional and relational clustering. Finally, the conclusions are drawn in Section 6.
In this section, we will formally define our method of con-structing the multi-type relational objects as well as a recur-sive relational similarity metric according to the ontology.
Given an ontology represented as a directed graph G = ( C, E ), in which vertices C = { c i } stand for the set of concepts in the ontology and edges E = { e ij | edge e c  X  c j } for the relationships between pairwise concepts, i.e. concept c i includes concept c j as its member prop-erty. In Figure 1, concept  X  X ctor X  has member concept list MC (Actor)= { Name, Movie } and concept  X  X ovie X  has MC (Movie)= { Title, Genre, YearOfRelease, Certificate, Plot, Duration, Actor, Director } .Whenconstructinganobject x of concept c i , we will first build its member concept list MC ( c i ) and then link all objects related to x into the mem-ber property attributes of x . We say an object y of concept c is related to x when c j  X  MC ( c i ). In such case, y will be added into the member property attribute x.c j .Then for each y  X  x.c j , we launch the above procedure iteratively until MC ( c j )=  X  or a depth bound Depth (  X  0) is reached. As an example, Figure 2 shows the object for actor  X  X om Hanks X  with Depth =2. Figure 3: Example objects for Two Action Movie Stars ( Depth =1 )
For two relational objects x 1 and x 2 of concept c i ,we define the similarity metric as follows: where weight w ij ( w ij  X  1and j w ij =1)representthe importance of member concept c j in describing the concept c . InEquation1, fs set (  X  ,  X  ) is defined as [1]: fs When MC ( c j ) =  X  ,thevalueof fs ( y k ,y l )inEquation2is recursively calculated by Equation 1. Hence, this similarity metric can explore the linkage structure of the relational ob-jects. The above procedure continues until MC ( c j )=  X  the depth bound is reached, where the traditional proposi-tional similarity metrics can be used.

To demonstrate our recursive similarity metric, we give a simplified example in the movie dataset, assuming concept  X  X ovie X  only has member concepts { Title, Genre, YearOfRe-lease } . Figure 3 shows the relational objects for action movie stars  X  X rnold Schwarzenegger X  and  X  X ylvester Stal-lone X . We will compare them with the actor  X  X om Hanks X  represented in Figure 2. In the scenario of propositional clustering, some naive similarity metrics might be adopted: if the actor X  X  name is regarded as an enumerated value or a string, the true-or-false function or the Levenshtein distance can be used. The former metri c results in zero similarity value between each pair of actors because they always have different names. By using the latter, we have fs ( o A 2 = fs str ( X  X rnold Schwarzenegger X ,  X  X ylvester Stallone X ) = 0.095, fs ( o A 1 ,o A 3 )= fs str ( X  X om Hanks X , X  X ylvester Stal-lone X ) = 0.111, which is still unreasonable. Another common technique is to transform the relational information into a high dimensional vector space, e.g. constructing a binary vector to represent the movies that an actor has acted in and then calculating the pairwise similarity between actors as the cosine value of their movie vectors. As discussed in Section 1, such transformation often produces sparse data when the number of movies is large. Another disadvantage is that some deeper semantic information, e.g. the movie genre, is lost when calculating pairwise similarity between actors.

In our framework, by setting Depth = 1 and utilizing the relational similarity metric, we calculate the similarity value between movies  X  X erminator 2: Judgment Day X  and  X  X ocky X  as follows (the weights for all member concepts are ignored here for simplicity): where the hierarchical taxonomy for concept  X  X enre X  X s shown at the bottom of Figure 3 and the corresponding similarity metric fs taxonomy (  X  ,  X  ) is defined in [5].

Similarly, fs fs fs and hence
In the same way, we can get fs obj ( o A 1 ,o A 3 )=0 . 171, which is less than fs obj ( o A 2 ,o A 3 ). Therefore, by incorporat-ing the ontology of the dataset and applying the relational similarity metric, the new results reflect more credible sim-ilarity values among these actors.

Theoretically, calculating the similarity value between two objects should exhaustively explore their relational struc-tures and consider their member objects at all levels, i.e. setting Depth =  X  . This is infeasible and unnecessary in practice. From Equations 1 and 2, we see that the similar-ity value between two member objects of concept c j will be propagated into the similarity calculation of the upper level concept c i with a decay factor  X  d ( c j )= w ij | MC ( c i means concept c i is located at the d -th level of the root ob-ject X  X  relational structure. The total decay factor for concept c to impact the similarity calculation of two root objects is  X ( c j )= d  X  d . In many applications, this factor will be reduced very quickly as d increases, which means the impact from member objects at the deeper levels of the relational structure keeps decreasing. For instance, in the above sim-plified example, the total decay factors for concepts  X  X ame X  and  X  X earOfRelease X  to impact root concept  X  X ctor X  are as follows (the weights for member concepts are again ignored for simplicity):  X (Name) = 1 |
When applying the real ontology shown in Figure 1, the total decay factor  X  (Year) is no greater than 1 14 because MC (Movie)=7. Just like RDBC and FORC, we can set the depth bound Depth to a moderate value instead of  X  , so that enough information is retrieved from the relational structure to guarantee the similarity calculation credible as well as feasible.
Clustering is a technique of data compression: data ob-jects in the same cluster can be treated collectively because they are more similar to each other than those from dif-ferent clusters. From another point of view, the clustering procedure preserves the significant similarity entries within the dataset, by distributing pairs of highly similar objects into the same cluster. Like DBSCAN [4] that explicitly con-trols the intra-cluster similarity by specifying the expected data density, we use variance , a criterion of evaluating the diameter of clusters, to meet the requirement. The formal definition of variance will be given in Section 3.1. Roughly speaking, greater variance me ans the derived clusters are more compact and higher similarity values between objects are preserved by the clustering procedure.

Based on the above discussion, our DIVA algorithm is designed as follows: First, divide the whole dataset into a number of clusters so that the variance of each cluster is greater than a particular threshold value  X  . Based on these clusters, a hierarchical dendrogram is built using an agglom-erative approach. Finally the appropriate level of the den-drogram that satisfies the variance requirement is selected to construct the clustering result. Table 1 summarizes the main framework of DIVA, consisting of two parts: a recursive di-visive step to partition the dataset and an agglomerative step to build the dendrogram based on the clusters. After defining some fundamental concepts in Section 3.1, we pro-vide more details for these two steps in Section 3.2 and 3.3. The computational complexity is analyzed in Section 3.4.
As presented in the related work (Section 5), traditional clustering approaches are usually categorized as partitional and hierarchical. The classical k -Medoids algorithm, as an example of partitional clustering, defines the medoid of a
DIVA (dataset D 0 ,numberofROs r ,variance  X  ) cluster as the data object that has the maximum average similarity (or minimum average of distance) with the other objects in the same cluster. This requires the calculation of the similarity values between every pair of data objects in the given cluster. On the other hand, the hierarchical clustering is composed of two sub-categories: divisive and agglomerative. In the former one, a common criterion to decide whether or not a cluster should be divided is its di-ameter, which is determined by the distance value of two data objects in the cluster that are the farthest away from each other. Again we are faced with the quadratic compu-tational complexity. Similarly, the traditional agglomera-tive clustering paradigms also has the quadratic complexity when searching for two sub clusters that are closest to each other in order to derive the super one. Due to the compli-cated structure of relational data objects, relational cluster-ing with quadratic computational complexity is restrained in many applications.

Is there any efficient way to delimit the shape of clusters and hence accelerate the division and agglomeration proce-dures? We develop the concept Representative Object (RO) to achieve this goal. The ROs are defined as a set of r maximum-spread objects in the data space, given r  X  2. More strictly, after a start object x s is randomly chosen, the i -th RO is determined by the following formula: ro i = The reason we do not use x s as an RO is: x s will reside at the center part of the data space with high probability, and thus not satisfy the maximum-spread requirement for ROs. Additionally, we can also reduce the impact of randomly selected x s on the final clustering result. In Section 3.4 we analyze how the application of ROs successfully reduces the total computational complexity of DIVA to be linear with the size of dataset.

We use { ro ( D ) i } (1  X  i  X  r )todenotethesetofROsfor the dataset D . Because they are maximum-spread from each other, the distance between the farthest pair of ROs approx-imates to the diameter of D . More formally, the variance of the dataset D is defined as: Hence, greater variance means data objects reside in a smaller data space and thus are more similar to each other, which means the data are of higher homogeneity.
Divisive-Step(dataset D 0 ,numberofROs r ,variance  X  )
The divisive step starts by assuming all the data objects belong to the same cluster. Here we use D 0 to denote the whole dataset as well as the initial cluster it forms. Equa-tion 3 is applied to find a set of ROs for D 0 and thus its variance  X ( D 0 ) is determined by Equation 4.  X ( D 0 )isless than the pre-specified variance threshold  X  , so the division procedure is launched. Two ROs that are farthest away from each other, i.e. the pair of ROs determining the di-ameter of D 0 , are used as the absorbent objects of two sub-clusters respectively. The other data objects are allo-cated to the appropriate sub-cluster based on their similar-ities to the absorbent objects. Finally, the original cluster D 0 is replaced by its derived sub-clusters in the cluster list L . Since the similarity values of all the non-RO objects to every RO object have been obtained when determining { ro ( D ) 0 i } by Equation 3, the division operation for D performed without extra effort of similarity calculation. If either of the newly formed sub-clusters remains unsatisfied with the required threshold  X  , the above division process is recursively performed. Finally we get a set of clusters {D k } ( k D k = D 0 , D k 1 D k 2 =  X  ) with variance equals or
Agglomerative-Step(cluster set {D k } ) is greater than  X  , which are used as the input of th e agglom-erative step. The divisive step is summarized in Table 2.
Like classical agglomerative clustering approaches, in this step we will build a hierarchical dendrogram T in a bottom-up fashion. The cluster set {D k } obtained from the divisive step constitute the leaf nodes of the dendrogram. In each iteration, the most similar pairwise clusters (sub nodes) are merged to form a new super-cluster (parent node). Because each cluster in the agglomerative step is related to a unique node in the dendrogram, the words  X  X luster X  and  X  X ode X  are used interchangeable in this section.

Various similarity metrics for agglomerating the nodes within a dendrogram have been discussed in [2], among which we adopt the complete-linkage similarity. Since each cluster is represented by a set of ROs, the similarity between two nodes t l ,t l  X  T is defined as follows: where { ro ( t l ) i } is the set of ROs contained in node t { ro ( t l ) j } is the set of ROs contained in node t l . Without loss of generality, we assume that the super node t p is formed based on two sub-nodes t l and t l , then the top-r maximum-t .

The agglomerative step is summarized in Table 3. It is worth noting here that constructing the hierarchy in this step is not a reverse reproduction of the divisive step in Section 3.2. As shown in [16], the agglomeration can remedy the inaccurate partitioning generated by the divisive step.
After the dendrogram T is built, we need to determine the appropriate level in T and use the corresponding nodes to construct the clustering result. A common strategy is to se-lect the level at which the variance of each node equals or is greater than  X  . Alternatively, we can record the variance of newly generated node for each level, find the largest gap be-tween variances of two neighboured levels and use the lower level as the basis to construct clusters [2]. When the number of clusters is fixed, as in the experiments in Section 4, we select the level which contains the exactly required number of nodes to construct clusters.
In this section, we will briefly analyze the computational complexity for each step in our DIVA algorithm, given the whole dataset D 0 of size N , the number of iteration in the divisive step is R and the size of {D k } is K : Therefore, the total computational complexity of the DIVA algorithm is O ( rN + RrN + r 2 K 2 ).

We must point out that both R and K above are controlled by the variance threshold  X  :higher  X  leads to more recursive divisions and thus generates more clusters. When  X   X  1, the recursive division will generate many tiny clusters, each of which will only contains the RO itself. In this extreme case, our DIVA algorithm will behave like the complete agglom-erative approach RDBC with quadratic complexity. Nev-ertheless, by choosing moderate values for r and  X  to keep rK N , the computational complexity of our DIVA algo-rithm would be linear to the size of the dataset. Usually r will be set a fairly small value, such as 3 or 4. Like BIRCH [16], we can gradually increase the value of  X  to improve the homogeneity of the generated clusters, until their quality meets our requirement. In order to evaluate the effectiveness and efficiency of our DIVA algorithm for clustering multi-relational datasets, we compare it with the following approaches: (1) ReCoM [13], which uses relationships among data objects to improve the cluster quality of interrelated data objects through an iter-ative reinforcement clustering process. Because there is no prior knowledge about the authoritativeness in the datasets, we treat all data objects as equally important. Additionally, k -Medoids is incorporated as the meta clustering approach in ReCoM. (2) FORC [9], which is the natural extension of k -Medoids in the field of relational clustering. (3) LinkClus [15], which uses a new hierarchical structure, SimTree, to represent the similarity values between pairwise data objects and facilitate the iterative clustering process. All the exper-iments were carried out on a workstation with a 2.8GHz P4 CPU, 1GB memory and RedHat Operating System. All approaches are implemented by Java.
 The experiments are conducted on two relational datasets. The first one is a synthetic dataset that simulates the users X  browsing products on the website www.amazon.com .The second is the real movie dataset mentioned in Section 1. To evaluate the accuracy of the clustering result, we use the Related Minimum Variance Criterion [2] to measure the similarity between pairs of objects in the same cluster: where n k is the size of cluster D k and When the class labels of data objects are available, we can also use an entropy-based measure [13] to evaluate the clus-tering result. The measure reflects the uniformity or purity of a cluster. Formally, given a cluster D k and category labels of data objects in it, the entropy of cluster D k is: where P h is the proportion of data objects of class h in the cluster. The total entropy is defined as: Generally, larger intra-cluster similarity and smaller entropy values indicate higher accuracy of clustering result.
In the following experiments, we calculate the above cri-teria on a fixed number of clusters. For ReCoM and FORC, the number of clusters is a pre-specified input parameter. For LinkClus, we use the method mentioned in [15] to ob-tain the fixed number of clusters, i.e. first find the level at which the number of nodes is most close to the pre-specified number, then adopt the operation of merging to satisfy the requirement. For DIVA, when the number of merged clusters reaches the pre-specified requirement during the agglomer-ative step, the algorithm is terminated.
In this section, we test each clustering approach on a syn-thetic dataset. The dataset is generated by the following steps, as in [13]: 1. The product taxonomy of the online shop Amazon is 2. We randomly generate 2,000 users and uniformly dis-3. Each user X  X  browsing action is generated according to 4. In order to test the robustness of different clustering
Figure 4 is the ontology of the synthetic dataset, Concepts  X  X ser X  and  X  X roduct X  are interrelated by the link browse ac-tion and Concept  X  X roduct X  has property  X  X ategory X  as its content feature. When creating objects for users or prod-ucts, we set the depth bound Depth = 1 for FORC/DIVA and Depth = 0 for LinkClus/ReCoM, because the latter two apply the reinforcement clustering manner. For all ap-proaches, the numbers of clusters to be generated within datasets  X  X ser X  and  X  X roduct X  were specified as 100 by de-fault. LinkClus was executed with a series of c values and the best clustering result was used to compare the perfor-mance of LinkClus with that of other approaches. Since FORC, LinkClus and ReCoM launch an iterative procedure of clustering data objects until convergence, we set the maxi-mum number of iterations to be 10 because these algorithms converge very quickly in most cases. (a) Intra-cluster similarity Figure 5: Synthetic Dataset -Clustering users with different  X  (a) Intra-cluster similarity Figure 6: Synthetic Dataset -Clustering products with different  X 
Variance  X  is the most important parameter for the DIVA algorithm, so we first test its impact. Figure 5 and 6 show the evaluation results of clustering users and products re-spectively, ranging  X  from 0.3 to 0.6 and fixing r =3. In general, the quality of the clustering result generated by DIVA improves as  X  increases. When  X   X  0 . 4, DIVA out-performs all the other algorithms, especially when evaluated by the entropy criterion, which means DIVA is more capa-ble of discovering the inherit category of products as well as the hidden groups of users. Furthermore, we found that the accuracy of LinkClus is far worst than those of the other approaches. The reason is that LinkClus builds an initial SimTrees by applying Frequent Pattern Mining, which only exploits the link structure of the relational dataset. The con-tent features of the data objects, for example the category information for product, are completely ignored in the pro-cedure of clustering. Due to such information loss, LinkClus cannot generate clusters of high quality. As a result, we will not test LinkClus in the next experiments.

Figure 7 shows that FORC is always the most time con-suming algorithm. This result is not surprising since its computational complexity is O ( N 2 ). On the other hand, time spent by ReCoM, LinkClus and DIVA are comparable. When clustering products DIVA even outperforms ReCoM. The reason is: the average number of users related to each product is less than that of products related to each user, so the most expensive operation in similarity calculation, Equation 2, is used less for clustering products than that for users. Generally, as  X  increase, DIVA will spend more time to generate smaller clusters in the divisive step and com-bine them again in the agglomerative step. When  X &gt; 0 . 6 for this dataset, time spent by DIVA sharply increases be-cause many single-object clusters are generated. As we have discussed in Section 3.4, such over-estimation downgrades DIVA into RDBC with quadratic complexity.

Next we examine the parameter r , the number of ROs for each cluster. r isvariedfrom2to17instepsof compare two fixed values of  X  here for the reason of clarity, but the conclusion is also valid for other values. Curves  X  X ser-04 X  and  X  X ser-05 X  in Figure 8 are for clustering users with variance 0.4 and 0.5 respectively, and curves  X  X roduct-04 X  and  X  X roduct-06 X  for clustering products with variance 0.4 and 0.6 respectively. We can see that the running time grows very quickly while the accuracy does not change a lot. Therefore, a fairly small value of r , such as 3 or 4, is enough for providing high accuracy as well as keeping short processing time. Figure 8: Synthetic Dataset -Clustering users and products by DIVA with different r
In many applications very small clusters (in the extreme case, the singleton cluster that only contain one data object) are meaningless, so it is necessary to investigate the struc-ture of the derived clusters. Since the cluster number in our experiments has been fixed to 100, resulting in the same av-erage size of the derived clusters for all approaches, we con-sider the standard deviation of the clusters X  sizes here. The result is shown in Figure 9. Roughly speaking, the standard deviation of the clusters X  sizes generated by DIVA decreases as the variance increase, meaning that our approach does not tend to generate singleton clusters. Figure 9: Synthetic Dataset -Standard deviation of the cluster sizes
Figure 10 illustrates the robustness of all approaches un-der different noise ratios of browsing actions, ranging from 20% to 100%. The parameters for DIVA are set as:  X  =0 . 5 and r = 3. Generally, the accuracy of all approaches are re-duced as noise ratio increases. When evaluated by the crite-rion of intra-cluster similarity, ReCoM is slightly better than DIVA and FORC is the worst among three. Yet the entropy-based criterion might be more p referable here, because the intra-cluster similarity is calculated based on not only the informative browsing actions but also the noise ones, while the entropy is calculated only based on the class labels of the data objects. Evaluated by the latter one, DIVA ex-ceeds ReCoM and FORC when the noise ratio is below 80% and their performance are very close when the noise ratio is above 80%. (a) Intra-cluster similarity Figure 10: Synthetic Dataset -Accuracy of cluster-ing users along with different noise ratio
The clustering approaches were also evaluated on a real-world dataset, a movie knowledge base defined by the ontol-ogy in Figure 1. After data pre-processing, there are 62,955 movies, 40,826 actors and 9,189 directors. The dataset also includes a genre taxonomy of 186 genres. Additionally, we have 542,738 browsing records included in 15,741 sessions from 10,151 users. The number of sessions made by differ-ent users ranges from 1 to 814.

The evaluation result based on the intra-cluster similarity is shown in Figure 11(a), in which DIVA performs better than ReCoM and FORC. The entropy-based criterion de-fined by Equation 7 can not be applied, because there is no pre-specified or manually-labelled class information for movies in the dataset. We have to utilize the visit informa-tion from users to evaluate the clustering results indirectly. The traditional collaborative filtering algorithm constructs a user X  X  profile based on all items he/she has browsed across sessions, then the profile of active user u a is compared with those of other users to form u a  X  X  neighbourhood and the items visited by the neighbourhoods but not by u a are re-turned as the recommendation [10]. Hence, two items are (b) User-based coefficient Figure 11: Real Dataset -Clustering movies with different  X   X  X abelled X  into the same category if they are co-browsed by the same user, which reflects the partitioning of the dataset from the viewpoint of users. Accordingly, we can construct the evaluation criterion as in [15]: two objects are said to be correctly clustered if they are co-browsed by at least one common user. The accuracy of clustering is defined as a variant of Jaccard Coefficient: the number of objects pairs that are correctly clustered over all possible objects pairs in the same clusters. Another criterion is similar but of higher granularity, based on the assumption that a user sel-dom shifts his/her interest within a session, so two objects are said to be correctly cluste red if they are included in at least one common session. Therefore we have two new crite-ria for evaluating the accuracy of clustering: user-based and session-based coefficient. As discussed in [15], higher accu-racy tends to be achieved when the number of clusters in-creases, so we set to generate 100 clusters for all approaches.
Figure 11(b) and 11(c) show the evaluation results based on the above two criteria respectively. Since the user brows-ing data are very sparse (a common phenomenon in the sce-nario of recommender systems), no clustering approach can achieve very high coefficient value. Despite that, DIVA still outperforms both ReCoM and FORC when  X &gt; 0 . 3, indi-cating the clusters generated by DIVA are more accordant with the user browsing patterns, i.e. the partitioning of the dataset derived by DIVA is more acceptable by users. It in-dicates that DIVA will have higher usefulness than ReCoM or FORC when applied into a recommender system.
As a widely-applied technique for data analysis and knowl-edge discovery, clustering tries to separate a dataset into a number of finite and discrete subsets so that the data objects in each subset share some common trait [7]. Roughly speak-ing, traditional clustering methods can be divided into two categories: hierarchical and partitional. Hierarchical clus-tering algorithms recursively agglomerate or divide existing clusters in a bottom-up or top-down manner respectively, so the data objects are organized within a hierarchical struc-ture. On the contrary, partitional algorithms construct a fixed number of clusters from the beginning, distribute each data object into its nearest cluster and update the cluster X  X  mean or medoid iteratively.

In recent years, many innovative ideas have been proposed to address various issues of traditional clustering algorithms [6, 14]. For example, BIRCH [16] utilizes the concept clus-tering feature (CF) to efficiently summarize the statistical characteristics of a cluster and distribute the data objects in the Euclidean space. Because the CF vectors are addi-tive, they can be updated easily when a cluster absorbs a new data object or two sub-clustered are merged. By scan-ning the dataset, BIRCH incrementally builds a CF-tree to preserve the inherent clustering structure of the data ob-jects that have been scanned. Finally, in order to remedy the problems of skewed input order or undesirable splitting, BIRCH applies a traditional agglomerative clustering algo-rithm to improve the CF-tree. BIRCH is very efficient be-cause of its linear computational complexity, but it is not applicable for non-Euclidean datasets, such as the relational data object shown in Figure 2. DBSCAN [4], a density-based clustering algorithm, is proposed to find clusters of sophisticated shapes, which re quires two parameters to de-fine the minimum density of clusters: Eps (the radius of the neighbourhood of a point) and MinPts (the minimum num-ber of points in the neighbourhood). Clusters are dynam-ically created from an arbitrary point and then all points in its neighbourhood are absorbed. Like the dilemma of pre-specifying parameter k in k -Means/ k -Medoids, the pa-rameters Eps and MinPts in DBSCAN are difficult to be determined. Another problem is that DBSCAN needs R*-tree to improve the efficiency of region queries. Since such a structure is not available in a relational dataset, the compu-tational complexity of DBSCAN will be degraded to O ( N 2
In contrast to traditional clustering algorithms that only exploit flat datasets and search for an optimal partition-ing in the Euclidean space, relational clustering algorithms try to incorporate the relationships between data objects as well. Neville et al. [12] provided some preliminary work of combining traditional clustering and graph partitioning techniques to solve the problem. They used a very sim-ple similarity metric, called matching coefficient ,toweight the relations between data. Kirten and Wrobel developed RDBC [8] and FORC [9] as the first-order extensions of clas-sical hierarchical agglomerative and k -partitional clustering algorithms respectively. Both of them adopt the distance measure RIBL2 to calculate the dissimilarity between data objects, which recursively compares the sub-components of the first-order objects until they can finally fall back on propositional comparisons on elementary features. There-fore, RDBC and FORC inherit the disadvantage as their propositional antecedent: When the relational datasets are very large, such algorithms will be infeasible due to the quadratic computational complexity. Enlightened by the idea of mutual reinforcement, Wang et al. [13] proposed a general framework for clusterin g multi-type relational data objects: Initially, clustering is performed separately for each dataset based on the content information. The derived clus-ter structure formulates a reduced feature space of the cur-rent dataset and is then propagated along the linkage struc-ture to impact the re-clustering procedure of the related datasets. More specifically, the relationships between data objects of different types are transformed into the linkage feature vectors of these objects, which can be easily handled by the traditional clustering algorithms just like the con-tent feature vectors. The re-clustering procedure continues in an iterative manner for all types of datasets until the re-sult converges. Additional improvement would be achieved by assigning different importance among data objects in the clustering procedure according to their hub and authorita-tive values. Long et al. presented another general frame-work for multi-type relational clustering in [11]. Based on the assumption that the hidden structure of a data matrix can be explored by its factorization, the multi-type rela-tional clustering is converted into an optimization problem: approximate the multiple relation matrices and the feature matrices by their corresponding collective factorization. Un-der this model a spectral clustering algorithm for multi-type relational data is derived, which updates one intermediate cluster indicator matrix as a number of leading eigenvectors at each iterative step until the result converges. Finally the intermediate matrices have to be post-processed to extract the meaningful cluster structure. This spectral clustering framework is theocratically sound, but it is not applicable for the semantic information, such as  X  X enre X  for concept  X  X ovie X  in Figure 1.

According to the definition, the clustering approaches try to partition a dataset so that data objects in the same clus-ters are more similar to each other while objects in different clusters are less similar. Since data objects in the same clus-ter can be treated collectively, the clustering result can be considered as a technique of data compression [6]. There-fore, we can use the clustering model to approximate the similarity values between pairs of objects. Yin et al. pro-pose a hierarchical data structure SimTree to represent the similarities between objects, and use LinkClus to improve the SimTree in an iterative way [15]. However, since they only utilized frequent pattern mining techniques to build the initial SimTrees and use path-based similarity in one SimTree to adjust the similarity values and structures of other SimTrees, information contained in the property at-tributes of data objects are not adopted in their clustering framework, which definitely degrades the accuracy of the final clustering result.
In this paper we propose a new variance-based clustering approach for multi-type relational datasets. The variance is a criterion to control the compac tness of the derived clusters and thus preserve the most significant similarity entries of the object pairs. Our approach combines the advantages of divisive and agglomerative paradigms to improve the qual-ity of clustering results. By incorporating the idea of Repre-sentative Object, our approach has linear time complexity. Since the multi-type relational information is considered in the procedure of constructing data instances, the problem of skewness propagation within reinforcement clustering ap-proaches can be avoided. Experimental results show our al-gorithm outperforms some well-known relational clustering approaches in accuracy, efficiency and robustness. [1] S. S. Anand, P. Kearney, and M. Shapcott. Generating [2] R.O.Duda,P.E.Hart,andD.G.Stork. Pattern [3] S. Dzeroski and N. Lavrac. Relational Data Mining . [4] M.Ester,H.P.Kriegel,J.Sander,andX.Xu.A [5] P. Ganesan, H. Garcia-Molina, and J. Widom.
 [6] J. Han and M. Kamber. Data Mining: Concepts and [7] A.K.Jain,M.N.Murty,andP.J.Flynn.Data [8] M. Kirsten and S. Wrobel. Relational distance-based [9] M. Kirsten and S. Wrobel. Extending k-means [10] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, [11] B. Long, Z. M. Zhang, X. Wu;, and P. S. Yu. Spectral [12] J. Neville, M. Adler, and D. Jensen. Clustering [13] J.Wang,H.Zeng,Z.Chen,H.Lu,T.Li,andW.-Y.
 [14] R. Xu and D. Wunsch. Survey of clustering [15] X. Yin, J. Han, and P. S. Yu. Linkclus: efficient [16] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH:
