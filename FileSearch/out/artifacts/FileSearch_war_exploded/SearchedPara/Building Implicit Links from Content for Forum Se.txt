 The objective of Web forums is to create a shared space for open communications and discussions of specific topics and issues. The tremendous information behind forum sites is not fully-utilized yet. Most links between forum pages are automatically created, which means the link-based ranking algorithm cannot be applied efficiently. In this paper, we proposed a novel ranking algorithm which tries to introduce the content information into link-based methods as implicit links. The basic idea is derived from the more focused ran-dom surfer: the surfer may more likely jump to a page which is similar to what he is reading currently. In this manner, we are allowed to introduce the content similarities into the link graph as a personalization bias. Our method, named Fine-grained Rank (FGRank), can be efficiently computed based on an automatically generated topic hierarchy. Not like the topic-sensitive PageRank, our method only need to compute single PageRank score for each page. Another contribution of this paper is to present a very efficient algorithm for au-tomatically generating topic hierarchy and map each page in a large-scale collection onto the computed hierarchy. The experimentalresultsshowthattheproposedmethodcanim-prove retrieval performance, and reveal that content-based link graph is also important compared with the hyper-link graph.
 H.3.3[ Information Storage and Retrieval ]: Information Search and Retrieval; H.5.4 [ Information Interfaces and Presentation ]: Hypertext/Hypermedia Algorithms, Performance, Experimentation Forum Search, PageRank, Hierarchy Generation, Cluster-ing, Categorization Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.
Web forum, also named bulletin or message board, is an online portal for open discussions on specific topics and is-sues. It also can be considered as the upgraded Web version of traditional newsgroups. Not like the common Web sites that are managed by editors, innumerable new postings are created each day on Web forums that cover almost any con-ceivable topic. Actually, Web forums created some kind of knowledge database which took millions of internet users many years. Commercial search engines, e.g. Google [1] and Yahoo! [2], have begun to leverage a small part of valu-able information gleaned from such forums. The interesting fact here is that, if one makes a query on Google regard-ing a specific problem such as  X  X y camera got wet X , most answers will come from forum pages. Now, the tremendous information is hidden behind a large number of forum sites, distributed on the whole internet. How to utilize the in-formation and make it accessible for all the internet users becomes a valuable research topic.

Xi [26] proposed a learning-based approach to optimize the ranking function for newsgroup search. However, Web forums, derived from newsgroups, have distinct characteris-tics that make the search problem different. In newsgroups, eachmessageissentinindividualemailwithsemi-structured headers. Web forums are composed of HTML pages. They are well-formatted for a better human readability, but not for machine parsing. Multiple postings, user profiles, and artificial links are often integrated into one page. For the variety of forums, it becomes a challenging task to automat-ically extract the structured information from these pages, such as splitting different postings, merging postings into a thread tree. But better than newsgroups, the link informa-tion is available in Web forums. The most useful links are  X  X eference X  links in user postings, e.g. a link preceded by  X  X ee my previous posting X . Moreover, important postings, like  X  X ticky X  messages and popular threads, always can be easily reached from any other pages within several clicks. Even at the site level, there are still some cross-forum links which give strong  X  X ecommendations X . Therefore in this paper, we try to utilize the link-base ranking methods to improve the search results in Web forums.

Link-based ranking techniques have been proven effective for modern Web search engines. Commonly, the link-based ranking algorithms consider that the hyperlink from one page to another is some kind of recommendation. But this assumption is not always correct. Navigational (for most of in-site links) and Ad links are common on the internet. And even two hyperlinked pages are often irrelevant [5]. In Webforums, thelinkinformationbecomesmorenoisy. Most links in forums are automatically generated and only for navigations and operations (e.g. replying a thread or sort-ing threads by dates), rather than recommendations. On the other side, the useful links, including cross-forum links, are relatively rare. Therefore, completely relying on these noisy links may not be a good solution.

In this paper, besides using hyperlinks only, we try to build an implicit graph from the content and further com-bine it with the original link graph to improve the search performance. As a result, we proposed a novel rank algo-rithmnamedFine-grainedRank(FGRank), whichisderived from PageRank. PageRank assumes that the random surfer has no preference when choosing a  X  X eset X  page, i.e. jump-ing to any page with an equal probability. Some later work on PageRank [12, 14], consider that the random surfer has a uniform preference when taking a jump, regardless of his interest context. Actually, surfers X  activities often are task-based and topic-specific. That is, the surfer may more likely jump to a page which is similar to what he is reading cur-rently. To model the dynamic preferences, we assume that any surfer X  X  interests can be represented by the nodes on a topic hierarchy. Most likely, the surfer, who focuses himself on a specific topic, will continue to stay on this topic or drill down to a sub-topic. In this manner, the surfer will have different preferences on each page, determined by the topic of this page. In particular, the proposed approach also can be interpreted as a multi-level smoothing on the link graph. Another contribution of this paper is to present a very effi-cient algorithm for automatically generating topic hierarchy and map each page in a large-scale collection onto the com-puted hierarchy. The basic idea is derived from the fact that text categorization can be efficiently conducted based on only a few discriminative words.

The rest of this paper is organized as follow. In Section 2, we give a review on link-based ranking algorithms and also introduce background knowledge on our formulation. Then, wepresentourFGRankalgorithm,includingautomatichier-archy generation and page categorization, in Section 3. The experimental results are given in Section 4. Finally, Section 5 concludes this paper and discusses the future work.
Comparing with traditional document retrieval, link-base ranking like HITS [16] and PageRank [7, 21], is a distinct technique in modern Web search systems. Google X  X  PageR-ank algorithm is widely applied in both academy and indus-try. It assigns each Web page an  X  X mportant X  score based on the stationary distribution of a random walk on the di-rected hyperlink graph. Also, PageRank can be intuitively interpretedassimulatingtheactionsofrandomsurfers: con-sider a random surfer that starts from a random page, and keeps clicking on the links of the current page with a uni-form probability, but eventually gets bored and starts again on another random page. More specifically, let P be the stochastictransitionmatrixoflinkgraphwithoutsinknodes [21], where P ( i, j )=1 /deg ( i ) if there is a link from the i th page to the j th page, and deg ( i ) is the out-degree of the i th page. Therefore, the PageRank vector v PR is defined as follow: where c is the damping factor, which is usually set to 0 . 85, n isthenumberofpagesinthelinkgraph, E isreferredastele-portation containing the artificial jumps, and v bias is named personalization vector and often uniformly set to 1 /n .Non-uniform v bias will result the biased PageRank vector with the preferences on certain kinds of pages. The PageRank vectorcanbecomputedbytheiterativePowerMethod, with uniform initial ranks. Based on this basic formulation, most of extensions can be classified into three categories from dif-ferent aspects of PageRank.
 Damping factor c is not only a smoothing factor of the PageRank vector, but also a useful cue for link-spam detec-tion [6]. Without using a constant damping factor, Dirich-let PageRank [25] employ dynamic damping factors based on the Dirichlet smoothing to improve the retrieval perfor-mance. The basic assumption is that the random surfer would more likely follow the out-links of the current page, if the page has many out-links.

Personalization vector v bias can bias PageRank scores ac-cording to user X  X  preferences. Topic-sensitive PageRank [12] uses 16 different personalization vectors, built from the top-level topics of the ODP data, to compute 16 PageRank scores for each page. The final PageRank scores are com-puted at query time as the linear combinations of the pre-computed scores. This method cannot scale up because it need more computations for fine-grained topic basis, i.e. more personalization vectors. Jeh [14] later improved it to use hub vectors to construct biased PageRank scores at query time. The algorithm scales well because its cost only depends on the number of non-zero elements in the person-alization vector. Our proposed FGRank also utilizes modi-fied personalization vectors to enhance the ranking perfor-mance. But not like topic-sensitive PageRank, our method only needs to compute one PageRank score for each page without overhead at query time. Our algorithm is efficient in both computation and storage and also fits our task well. BlockRank [15] use the link structure hierarchy, e.g. do-main, hosts, and pages, to build the personalization vectors. The drawback of the method is no considering of the content of the pages.

In the traditional PageRank, all the links are equally con-sidered. As aforementioned, the in-site links are different from off-site ones in terms of link properties. Based on the observation, Xue [27] and Kamvar [15] try to leverage the hierarchical structure on Web graph and employ mod-ified stochastic transition matrices to compute the PageR-ank scores. Again, the page content is ignored. Richardson [22] proposed the Directed Surfer Model which weights links based on the page relevance to given queries. The direct in-terpretation to the model is a more  X  X ntelligent X  surfer, who can selectively click out-links according to his information need. Directed Surfer Model will need to pre-compute dif-ferent PageRank scores for each term. Focused Rank [10] is another work trying to combine the content information with links. This algorithm also does not scale well because it will need to compute PageRank scores at query time. These methodologies are similar to ours but our method can be efficiently computed. In the next section, we will introduce the proposed algorithm in details and also show the connec-tions with some existing methods.
Link information is not always reliable, especially in the forum context. Combining content with links reveals a good direction to improve link-based ranking algorithms. How-ever, most methods in literature are not practical and can-not scale well. The proposed FGRank, which is based on non-uniform personalization vectors built from content sim-ilarities, is efficient and has no any overhead at query time.
User X  X  information need is often task-oriented and topic-specific. It is reasonable to assume that the random surfer more likely  X  X eleports X  to these pages that are relevant to the current page. Intuitively, we actually need to compute a distinct personalization vector for each page, based on its content similarities to other pages. But the straightfor-ward solution is totally impractical for both computation and storage costs. We will need to compute a full similarity matrix and also heavily increase the cost for each PageRank iteration step. To tackle this problem, we conduct the cal-culation at a higher level, i.e. the topic level, rather than the page level. Because the number of topics are much small than that of pages, we can greatly reduce the computation and storage complexity in this manner.

Topics can naturally be organized in a tree-like structure, namelytopichierarchy. Weactuallyallowtoefficientlycom-pute the bias vector based on the hierarchical structure. Given two nodes N i and N j on the topic hierarchy tree, the path from N i to N j is written as N p (1) ,N p (2) ,...,N where N p (1) = N i and N p ( m ) = N j . Therefore, the potential (we use rather  X  X otential X  here than  X  X robability X  because the computed values are not normalized to one) of jumping from N i to N j is computed by:
P ( N i ,N j )= where Pa ( N i ) is the direct parent node of N i , Pr ( N denotes the conditional probability of N j when given N i (computed by equation (5)), and z, 0 &lt;z&lt; 1, is called  X  X hift X  factor. It controls the probability that the random surfer shifts his interests from the current topic to another topic. The smaller z will lead to a more focused surfer. Topic hierarchy bias can be intuitively explained as: when a surfer is reading a page under certain topic (e.g. food), he more likely jumps to another page related to the same topic or sub-topics (e.g. cake), but less likely teleports to thepagesaboutbroadertopics(e.g. living)ordistincttopics (e.g. sports). A specific sample on topic hierarchy bias is illustrated in Fig. 1.

Assume that each page is only assigned to single node on the hierarchy tree, and let N ( i ) be the topic node of the i th page. Therefore, the j th element of the i th page X  X  personalization vector is the probability of teleporting from the i th page to the j th page, calculated by:
In the forum context, we have another intuitive explana-tion on the hierarchy bias. Assume that we have built a meta-forum site by aggregating distributed forum sites, and mapped all the pages onto a topic hierarchy, or directory. So, the surfers have two choices: browse the information at the original sites (link information), or at our new site (con-tent similarity information). Our method is to combine the two random walks respectively on graph P and E in (1).
Topic-sensitivePageRank[12]separatelycomputesPageR-ank scores for each topic, and each topic has its own person-alization vector. Our FGRank algorithm also uses different personalization vectors for distinct topics but only needs to compute PageRank once. If there are two pages P i and P ,where P i belongs to the query topic but P j does not, ideally the topic-sensitive PageRank will gives P i ahigher score than that of P j . Our method is query independent. If P i and P j are highly referenced within each own topic, both of them will obtain high ranking scores. However, af-ter combining with the relevance scores, P i and P j still can be correctly ranked. If the given query is relevant to sev-eral topics, FGRank will be prone to return the pages in hot topics. A known issue of PageRank is of suppressing newly created page. Similarly, our method may also suppress the pages in less popular topics, for example, emerging topics.
Richardson[22]andDiligenti[10]employcontentinforma-tion to re-weight the links according to queries. The ideas are promising but all the algorithms are computationally in-tensive. Our FGRank will boost the links pointing to the pages within the same topic but suppress cross-topic links. At the same time, out method also introduces strong im-plicit links between the pages in one topic, which may cause a smoothing effect on the scores.
To compute the topic hierarchy bias, we need a topic hi-erarchy and each page is assigned to a topic. The direct way is to conduct a hierarchical clustering [13, 24] or cate-gorization [11, 17, 4, 9] on the whole collection. Clustering based methods are often hard to scale up, especially for hier-archical clustering algorithms. While, categorization based methods need the generated hierarchy and some training data. It is less possible to apply these methods directly on our problem. Mostly, user discussions in forum sites are topic-centric. So, we can easily confine diversity of topics in our collection. It means that we are allowed to only use a small number of pages, sampling from the whole collection, to generate the topic hierarchy and then categorize the rest of pages based on the computed hierarchy. A similar idea also can be found in [19].

The widely applied vector-space model represents docu-ments as high-dimensional and sparse vectors. Feature se-lection is helpful for some text categorization methods. In many cases, we can even only use a small set of discrimi-Figure 2: Find document clusters and associated discriminative words by transforming the document-by-word matrix into a block-diagonal matrix. native words to efficiently categorize documents [17]. With this observation, we expect a clustering algorithm which can not only generate document clusters but also give associ-ated word clusters. And the word clusters should be the most discriminative for each cluster and can be efficiently used for later categorization. The supervised methods also employ similar approaches to enhance the feature selection [4, 9]. Consider a document-by-word matrix A such that A i,j is the frequency of the j th word in the i th document. Our purpose actually tries to transform the matrix A into a block-diagonal matrix, as shown in Fig. 2. This cluster-ing method is called co-clustering which is derived from the problem of bipartite graph partition [8]. For completeness, we also include the algorithm (also with our small modifica-tions) in this paper, shown in Fig. 3.

Co-clustering algorithm simultaneously groups the docu-ments and words into clusters. Each cluster contains both documents and associated words. But it is still a flat clus-tering algorithm. In this paper, we extend the algorithm to generate the hierarchical clusters. Basically, we can sim-ply use the co-clustering algorithm recursively to obtain the topic hierarchy. However, to locate the most discrimina-tive word clusters for efficient categorization, we need to continuously refine the word clusters after each round of co-clustering. Assume mapping function C ( x ) gives the cluster label of x ,where x can either be a document or a word. We define the discriminative power to judge the quality of each word, computed as: where w i and d j denote the i th word and j th document in matrix A ,and | S | is the cardinality of set S . Power ( w ) gives the probability that a document belongs to the cluster C ( w )ifitcontainstheword w . To maximize the dissimi-larity between clusters, we filter out the ambiguous words in each cluster whose discriminative power is lower than a given threshold. Actually the refinement step can be consid-ered as the feature (i.e. word) selection widely used in text categorization. For our purpose, we care more about speed rather than accuracy. We use the formula (4) for efficiency, and other feature selection techniques [28] can also be used for the purpose. A complete description to the hierarchical co-clustering algorithm is given in Fig. 3.

The conditional probability Pr ( N j | N i ) ,N i = Pa ( N aforementioned in Section 3.1, is estimated in this step. Without loss of generality, we may assume that N i is the root node of the topic hierarchy. Therefore the maximum
Hierarchical Co-clustering for Hierarchy Generation Figure 3: The hierarchical co-clustering algorithm for automatically generating topic hierarchy. likelihood estimation of Pr ( N j | N i )isgivenby: where D is the set of documents, and | D | is the total number of documents.
When the topic hierarchy is generated, we can also ob-tain a set of discriminative words on each node. Then we may build efficient classifiers based these words without the training process. Given an unlabeled document d ,westart from the root node, and recursively categorize the document in a decision-tree-like mode. Without loss of generality, let N be the root node, and Ch ( N )bethesetoftheroot X  X  children. The document likelihood to a node N i  X  Ch ( N ) is computed by summing the evidence from words: where DW ( N ) is the set of discriminative words associated with node N , Power ( w | N ) is the discriminative power of word w given node N ,and Exist ( d, w ) is a binary indicator. The binary indicator equals 1 when word w is in document d , but 0 otherwise.

Suppose the document d gives the maximum likelihoods ontwonodes N p and N q in Ch ( N ),and L ( d, N p )  X  L ( d, N Figure 4: Categorize page on the computed topic hierarchy. W i denotes a word and its discriminative power is given below the word.
 L ( d, N i ) ,i = p, q . We define the topic purity of document d on node N by: When the topic purity is close to zero, it is no need to con-tinue classifying the given document d at the lower level because it becomes ambiguous on topic N p and N q . Par-ticularly, if the purity value is smaller than a threshold, we label document d as node N ; otherwise we choose node N p and continue to classify document d at the lower level. A specific sample is given in Fig. 4 to illustrate the categoriza-tion process.

Additionally, the proposed categorization method can be efficiently applied in the context of indexing. Our approach only depends on the existences of discriminative words. It is already available in the phase of building the inverted index. For each document, we can first calculate document likelihoods to all the topic nodes in the indexing process, and then categorize the document later. So the overhead is very limited.
Asaforementioned,ourmethodonlyproducessinglePageR-ank score for each page. So, at query time, FGRank will not bring in any overhead to the retrieval engine. Although we employ non-uniform personalization vectors, the teleporta-tion matrix E is not full rank. Therefore, in this section, we introduce how to compute FGRank efficiently.

Let H = { N 1 ,N 2 ,...,N k } be the generated topic hierar-chy and each node N i represents a topic on the hierarchy. Given the i th page d i in the collection, C ( d i ) is the topic label of page d i and v i bias is the personalization vector of d computed by (3). Actually, it is easy to proof that there are only k distinct personalization vectors. Suppose two page d and d j satisfies C ( d i )= C ( d j )= N , they will share same personalization vector, denoted as v N bias . Therefore, we can rewrite (1) according to FGRank: Because v N bias canbepre-computed, theoverheadofFGRank is approximately equal to appending k non-sparse columns into the stochastic transition matrix P . In particular, to deal with the sink nodes, we can modify (8) as:
Figure 5: The results of two-level co-clustering. where s i is a indicator. It equals 0 when d i has no out-links, but1otherwise. Thedetailed proofisgivenintheappendix.
The experiments are divided into two parts. First of all, we evaluate the algorithms for automatic topic hierarchy generation and hierarchical categorization. Then, we com-pare the proposed ranking algorithm with PageRank on real data gleaned from Web forums.
To nicely proof our algorithm in the context of Web fo-rums, we used the 20-Newsgroups ( NG20 ) dataset [18] to evaluate our algorithm. The NG20 dataset consists of ap-proximately 20,000 messages evenly collected from 20 dif-ferent usenet newsgroups. The pre-processing steps include: removing stop words (based on a general stop word list) and very-low-frequency noise words, ignoring the message headers except the subject lines. We use 70% data to build a two-level topic hierarchy and then classify the remaining 30% data based on the computed hierarchy. Also, to facil-itate the result evaluation, we ignore the purity check and force each message only assigned to the leave nodes.
Fig. 5 gives the generated hierarchy and associated news-groups in NG20 . The association is calculated by find-ing the maximum overlap between cluster nodes and NG20 newsgroups. It is easy to see that the computed hierarchy matches the newsgroup topics very well. Our method could notonlysuccessfullydistinguishbetween X  X ec.sport.baseball X  and  X  X ec.sport.hockey X , but also indicate their similarity by grouping them at the upper level. In particular, we also give the confusion matrix at the first level in Table 1. Themessagesin X  X ci.electronics X  X nd X  X isc.forsale X  X recon-fused on two generated clusters. However, these results are understandable because their boundaries may not be very clear, e.g.  X  X ci.electronics X  may be overlapped with  X  X omp.*.hardware X . Overall, we employ the categorization measures, macro-averaged and micro-averaged F1, to eval-uate the overall performance on clustering algorithm, given in Table 2. Evidently, our results are promising and may approach to some supervised methods. And even at the second-level, where 13 classes are considered, the F1 values are still higher than 0.76. Table 2: The overall performances on hierarchical clustering and categorization Hierarchical Hierarchical Categorization The categorization performance is also shown in Table 2. When only considering the first level of topics, i.e. 6 cat-egories, the categorization performance is pretty good and approaches to the clustering X  X . But the performance drops evidently at the second level. Even so, the F1 values are still higherthan0.7. Ifweignorethetwoambiguousnewsgroups, namely  X  X ci.electronics X  and  X  X isc.forsale X , the F1 perfor-mance rises to around 0.76, shown as  X  X evel 2 (removed) X  in Table 2.

Although our algorithm is not as good as state-of-art su-pervised methods, it can be directly derived from our clus-tering results, and classify pages on-the-fly. It is at a good balancepointonboththespeedandtheperformance. More-over, our methods are unsupervised, which can be done full-automatically without human labels.

In this collection, i.e. 14,000 messages for clustering and 5,997 messages for categorization, we only need no more than 30 CPU seconds for generating the two-level hierar-chy and no more than 10 CPU seconds for classification. Figure 6: Node and categorized page distributions on each level of the topic hierarchy.
 All times were computed on a standard 2GHz Pentium PC running Windows XP.
The experimental data are collected from four Web fo-rums, which all focus on specific topic, i.e. mobile devices. After the pages are crawled and stored locally, we conduct a pre-processing step to remove noise pages, e.g. user pro-file, error, login, and reply pages. Finally, we obtain the cleaned collection containing more than one million pages. The average size of these forum pages is about 60.7k, which is much larger than common web pages. We also found that the average number of links on each page is 85.9. But there are only 24.6% links pointing to the pages in our cleaned collection. It means that more than 3/4 links are leading to noise pages. So, the clean processing is a critical step in our context.
The topic hierarchy is built from 60,000 pages randomly sampled from the whole collection. The generated hierar-chy contains 68 nodes spanning 5 levels. Fig. 6 shows the distributions of node and categorized pages on each level of the topic hierarchy. The threshold for topic purity is set to 0.3. If the classifier built from only 60,000 pages cannot be generalized to the whole collection, for most pages, the cat-egorization algorithm will stop early at top levels. However, it is easily to find from Fig. 6 that a large number of pages are categorized to the nodes at deeper levels, i.e. 3 and 4. These statistics indicates that our strategy of combining clustering with categorization is successfully applied to the forum page collection.
Wefirstcollectsomefrequentlyaskedquestionsfromthese forumsites, andthenrefinethesequestionsintoqueryterms. Finally, we obtain 25 representative queries (e.g.  X  X IM card problem X ,  X  X ow delete ringtone X ) which cover different com-ponents on mobile devices, for example Bluetooth and SIM card, and manufactures. We first use BM2500 [23] to find the top 100 pages for each query, and then re-order the 100 pages according to the PageRank and FGRank respectively. We keep top 30 pages from the three ordered lists and man-Figure 7: The performance comparison between BM2500, PageRank and FGRank. ually judge the relevance of page on the queries. Overall, we obtain 1744 pages with relevance judgments.

We linearly combine the relevance score, BM2500, with the link-based importance scores, namely PageRank and FGRank. The parameters are tuned to achieve the best per-formance on annotated queries. We take P@10 and Means Average Precision (MAP) as the evaluation metrics which are widely used in TREC. The details can be found in [3]. The overall performance is given in Fig. 7.

The discussions in forums are highly topic-specific. Im-portant terms are often frequently mentioned on the whole page. It makes the relevance scores very effective to do re-trieval. So we can see that BM2500 achieves much higher performance than the general Web search task. Even so, FGRank still outperform BM2500 and PageRank by 8.1% and 5.9% on MAP, and by 7.4% and 3.6% on P@10. We also conduct a t -test on the results. It shows that our improve-ments on MAP are significant (0.04 for PageRank, and 0.03 for BM2500). Because most of P@10 scores approach to 1, our improvements failed to pass the t -test on this measure.
The damping factor can be considered as a combining factor for link and content information in FGRank. We also explore how the factor impact on the performance. We use 6 different factor values and evaluate their performance, as shown in Fig. 8. The best combination is achieved when c takes 0.3. It is clear to see that the implicit links built from the content is more important than the explicit link information. But, the link information is still useful. If we continue to increase the weight on content, the performance dropped eventually.
Most links in forums are not for  X  X ecommendations X , but for navigations and operations. In this paper, we proposed a link-based ranking algorithm, named Fine-grained Rank (FGRank), to overcome the problem. The basic idea can be interpreted to be that the random surfer may more likely jump to a page that is relevant to his current page. We build implicit links from content and further combine them with hyperlinks through the non-uniform bias. Experimental re-Figure 8: The performance changes with the damp-ing factor. sults indicate that FGRank outperforms PageRank in our context. Also, our algorithms, proposed for topic hierarchy generation and hierarchical categorization, are proven to be efficient and effective.

Although the algorithms proposed in this paper are moti-vated from the problems in forum search, the basic assump-tion is still applicable to general Web search. The work in [15] and [27] tries to improve link-based ranking algo-rithm by utilizing the underlying hierarchical structure on Web graph. We also argue that such kind of hierarchical structure also exists from the perspective of content. So the future work includes: 1) introducing the content hierarchy to improve general web search; 2) combining two types of hierarchical structure respectively from link and content. [1] Google search engine. http://www.google.com . [2] Yahoo! search engine. http://search.yahoo.com . [3] R. Baeza-Yates, F. Saint-Jean, and C. Castillo. Web [4] L. D. Baker and A. McCallum. Distributional [5] D. Bergmark, C. Lagoze, and A. Sbityakov. Focused [6] P. Boldi, M. Santini, and S. Vigna. Pagerank as a [7] S. Brin and L. Page. The anatomy of a large-scale [8] I. S. Dhillon. Co-clustering documents and words [9] I. S. Dhillon, S. Mallela, and R. Kumar. Enhanced [10] M. Diligenti, M. Gori, and M. Maggini. Web page [11] S. Dumais and H. Chen. Hierarchical classification of [12] T. H. Haveliwala. Topic-sensitive pagerank. In Proc. [13] A. K. Jain and R. C. Dubes. Algorithms for clustering [14] G. Jeh and J. Widom. Scaling personalized web [15] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and [16] J. Kleinberg. Authoritative sources in a hyperlinked [17] D. Koller and M. Sahami. Hierarchically classifying [18] K. Lang. News weeder: Learning to filter netnews. In [19] T. Li, S. Zhu, and M. Ogihara. Topic hierarchy [20] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] M. Richardson and P. Domingos. The intelligent [23] S. E. Robertson. Overview of the okapi projects. [24] S. Vaithyanathan and B. Dom. Model-based [25] X. Wang, A. Shakery, and T. Tao. Dirichlet pagerank. [26] W. Xi, J. Lind, and E. Brill. Learning effective [27] G. R. Xue, Q. Yang, H. J. Zeng, Y. Yu, and Z. Chen. [28] Y. Yang and J. O. Pedersen. A comparative study on
In this appendix we give the proof on the conclusion given in (9). Let s be n -dimensional column vector identifying the nodes with out-links: where deg ( i )isthenumberofout-linksofpage i . Tosimplify the notions, let  X  s =1  X  s . So we construct P from P as follow: Because the row sum of E always equals 1, P isagraph without sink nodes. Replace P with the modified graph P and rewrite (1) as follow: v PR = c ( P ) T v PR +(1  X  c ) E T v PR Follow the proof presented in (8), and then we can easily get the conclusion on (9)
