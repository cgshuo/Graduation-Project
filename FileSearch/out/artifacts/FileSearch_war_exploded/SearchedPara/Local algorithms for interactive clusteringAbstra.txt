 Carnegie Mellon University, Pittsburgh, USA Maria-Florina Balcan NINAMF @ CC . GATECH . EDU Georgia Institute of Technology, Atlanta, USA Konstantin Voevodski KVODSKI @ GOOGLE . COM Google, NY, USA Clustering is usually studied in an unsupervised learn-ing scenario where the goal is to partition the data given pairwise similarity information. Designing provably-good clustering algorithms is challenging because given a sim-ilarity function there may be multiple plausible cluster-ings of the data. Traditional approaches resolve this am-biguity by making assumptions on the data-generation pro-cess. For example, there is a large body of work that fo-cuses on clustering data that is generated by a mixture of Gaussians (Achlioptas &amp; McSherry, 2005; Kannan et al., 2005; Dasgupta, 1999; Arora &amp; Kannan, 2001; Brubaker &amp; Vempala, 2008; Kalai et al., 2010; Moitra &amp; Valiant, 2010; Belkin &amp; Sinha, 2010). Although this helps de-fine the  X  X ight X  clustering one should be looking for, real-world data rarely comes from such well-behaved proba-bilistic models. An alternative approach is to use limited user supervision to help the algorithm reach the desired an-swer. This approach has been facilitated by the availabil-ity of cheap crowd-sourcing tools in recent years. In cer-tain applications such as search and document classifica-tion, where users are willing to help a clustering algorithm arrive at their own desired answer with a small amount of additional prodding, interactive algorithms are very useful. Hence, the study of interactive clustering algorithms has become an exciting new area of research.
 In many practical settings we already start with a fairly good clustering computed with semi-automated tech-niques. For example, consider an online news portal that maintains a large collection of news articles. The news ar-ticles are clustered on the  X  X ack-end, X  and are used to serve several  X  X ront-end X  applications such as recommendations and article profiles. For such a system, we do not have the freedom to compute arbitrary clusterings and present them to the user, which has been proposed in prior work. But it is still feasible to get limited feedback and locally edit the clustering. In particular, we may only want to change the  X  X ad X  portion revealed by the feedback without changing the rest of the clustering. Motivated by these observations, in this paper we study the problem of designing local algo-rithms for interactive clustering.
 We propose a theoretical interactive model and provide strong experimental evidence supporting the practical ap-plicability our algorithms. In our model we start with an initial clustering of the data. The algorithm then interacts with the user in stages. In each stage the user provides limited feedback on the current clustering in the form of split and merge requests. The algorithm then makes a local edit to the clustering that is consistent with user feedback. Such edits are aimed at improving the problematic part of the clustering pointed out by the user. The goal of the al-gorithm is to quickly converge (using as few requests as possible) to a clustering that the user is happy with -we call this clustering the target clustering.
 In our model the user may request a certain cluster to be split if it is overclustered (intersects two or more clusters in the target clustering). The user may also request to merge two given clusters if they are underclustered (both intersect the same target cluster). Note that the user may not tell the algorithm how to perform the split or the merge; such in-put is infeasible because it requires a manual analysis of all the objects in the corresponding clusters. We also restrict the algorithm to only make local changes at each step, i.e., in response we may change only the cluster assignments of the points in the corresponding clusters. If the user requests to split a cluster C i , we may change only the cluster assign-ments of points in C i , and if the user requests to merge C and C j , we may only reassign the points in C i and C j . The split and merge requests described above are a nat-ural form of feedback. It is easy for users to spot over/underclustering issues and request the corresponding splits/merges (without having to provide any additional in-formation about how to perform the edit). For our model to be practically applicable, we also need to account for noise in the user requests. In particular, if the user requests a merge, only a fraction or a constant number of the points in the two clusters may belong to the same target cluster. Our model (See Section 2) allows for such noisy user responses. We study the complexity of algorithms in the above model (the number of edits requests needed to find the target clustering) as a function of the error of the initial cluster-ing. The initial error may be evaluated in terms of under-clustering error  X  u and overclustering error  X  o (See Sec-tion 2). The initial error may be fairly small: given two k -clusterings,  X  u and  X  o is at most k ( k  X  1) . Therefore we would like to develop algorithms whose complexity de-pends polynomially on  X  u ,  X  o and only logarithmically on n , the number of data points. We show that this is indeed possible given that the target clustering satisfies a natural stability property (see Section 2). We also develop algo-rithms for the well-known correlation-clustering objective function (Bansal et al., 2004), which considers pairs of points that are clustered inconsistently with respect to the target clustering (See Section 2).
 As a pre-processing step, our algorithms compute the average-linkage tree of all the points in the data set. Note that if the target clustering C  X  satisfies our stability as-sumption, then the average-linkage tree must be consistent with C  X  (see Section 3). However, in practice this average-linkage tree is much too large to be directly interpreted by the users. Still, given that the edit requests are somewhat consistent with C  X  , we can use this tree to efficiently com-pute local edits that are consistent with the target clustering. Our analysis then shows that after a limited number of edit requests we must converge to the target clustering. Our Results In Section 3 we study the  X  -merge model. We assume that the user may request to split a cluster C i only if C i contains points from several ground-truth clusters. The user may request to merge C i and C j only if an  X  -fraction of points in each C i and C j are from the same ground-truth cluster. For this model for  X  &gt; 0 . 5 , given an initial clustering with overclustering error  X  o and underclustering error  X  we present an algorithm that requires  X  o split requests and 2(  X  u + k ) log 1 tering, where n is the number of points in the dataset. For  X  &gt; 2 / 3 , given an initial clustering with correlation-clustering error  X  cc , we present an algorithm that requires at most  X  cc edit requests to find the target clustering. In Section 4 we relax the condition on the merges and al-low the user to request a merge even if C i and C have a single point from the same target cluster. We call this the unrestricted-merge model. Here the requirement on the accuracy of the user response is much weaker and we need to make further assumptions on the nature of the requests. More specifically, we assume that each merge re-quest is chosen uniformly at random from the set of feasi-ble merges. Under this assumption we present an algorithm that with probability at least 1  X  requires  X  o split requests and O (log k  X  2 u ) merge requests to find the target clustering. We develop several algorithms for performing the split and merge requests under different assumptions. Each algo-rithm uses the global average-linkage tree T avg to com-pute a local clustering edit. Our splitting procedure finds the node in T avg where the corresponding points are first split in two. It is more challenging to develop a correct merge procedure, given that we allow  X  X mpure X  merges, where one or both clusters have points from another tar-get cluster (other than the one that they both intersect). To perform such merges, in the  X  -merge model we develop a procedure to extract the  X  X  X ure X  subsets of the two clus-ters, which must only contain points from the same tar-get cluster. Our procedure searches for the deepest node in T avg that has enough points from both clusters. In the unrestricted-merge model, we develop another merge pro-cedure that either merges the two clusters or merges them and splits them. This algorithm always makes progress if the proposed merge is  X  X mpure, X  and makes progress on average if it is  X  X ure X  (both clusters are subset of the same target cluster).
 In Section 5 we demonstrate the effectiveness of our al-gorithms on real data. We show that for the purposes of splitting known over-clusters, our splitting procedure com-putes the best splits, when compared to other well-known techniques. We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods (Telgarsky &amp; Dasgupta, 2012; Heller &amp; Ghahramani, 2005; Dasgupta &amp; Hsu, 2008; Dai et al., 2010; Boulis &amp; Ostendorf, 2004; Zhong, 2005). Still, we find that our algorithms perform fairly well; for larger settings of  X  we are able find the tar-get clustering after a limited number of edit requests. Related work Interactive models for clustering studied in previous works (Balcan &amp; Blum, 2008; Awasthi &amp; Zadeh, 2010) were inspired by an analogous model for learning under feedback (Angluin, 1998). In this model, the algorithm can propose a hypothesis to the user (in this case, a clustering of the data) and get some feedback regarding the correct-ness of the current hypothesis. As in our model, the feed-back considered is split and merge queries. The goal is to design efficient algorithms which use very few queries to the user. A critical limitation in prior work is that the al-gorithm has the freedom to choose any arbitrary clustering as the starting point and can make arbitrary changes at each step. Hence these algorithms may propose a series of  X  X ad X  clusterings to the user to quickly prune the search space and reach the target clustering. Our interactive clustering model is in the context of an initial clustering; we are restricted to only making local changes to this clustering to correct the errors pointed out by the user. This model is well-motivated by several applications, including the Google application described in the experimental section.
 Basu et al. (Basu et al., 2004) study the problem of mini-mizing the k -means objective in the presence of limited su-pervision. This supervision is in the form of pairwise must-link and cannot-link constraints. They propose a variation of the Lloyd X  X  method for this problem and show promis-ing experimental results. The split/merge requests that we study are a more natural form of interaction because they capture macroscopic properties of a cluster. Getting pair-wise constraints among data points involves much more ef-fort on the part of the user and is unrealistic in many sce-narios.
 The stability property that we consider is a natural gen-eralization of the  X  X table marriage X  property (see Defini-tion 2.2) that has been studied in a variety of previous works (Balcan et al., 2008; Bryant &amp; Berry, 2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict thresh-old separation (Balcan et al., 2008; Krishnamurthy et al., 2012). This property is known to hold for real-world data. In particular, (Voevodski et al., 2012) observed that this property holds for protein sequence data, where similarities are computed with sequence alignment and ground truth clusters correspond to evolutionary-related proteins. Given a data set X of n points we define C = { C 1 ,C 2 ,...C k } to be a k -clustering of X where the C represent the individual clusters. Given two clusterings C and C 0 , we define the distance between a cluster C i  X  C and the clustering C 0 as: This distance is the number of additional clusters in C 0 contain points from C i ; it evaluates to 0 when all points in C i are contained in a single cluster in C 0 . Naturally, we can then define the distance between C and C 0 as: dist( C,C 0 ) = P C tion of clustering distance is asymmetric: dist( C,C 0 ) 6 = dist( C 0 ,C ) . Also note that dist( C,C 0 ) = 0 if and only if C refines C 0 . Observe that if C is the ground-truth cluster-ing, and C 0 is a proposed clustering, then dist( C,C 0 ) can be considered an underclustering error , and dist( C 0 ,C ) an overclustering error .
 An underclustering error is an instance of several clusters in a proposed clustering containing points from the same ground-truth cluster; this ground-truth cluster is said to be underclustered . Conversely, an overclustering error is an instance of points from several ground-truth clusters con-tained in the same cluster in a proposed clustering; this proposed cluster is said to be overclustered . In the fol-lowing sections we use C  X  = { C  X  1 ,C  X  2 ,...C  X  k } to refer to the ground-truth clustering, and use C to refer to the initial clustering. We use  X  u to refer to the underclustering error of the initial clustering, and  X  o to refer to the overcluster-ing error. In other words, we have  X  u = dist( C  X  ,C ) and  X  = dist( C,C  X  ) .
 We also observe that we can naturally define the distance between two clusterings using the correlation-clustering objective function. Given a proposed clustering C , and a ground-truth clustering C  X  , we define the correlation-clustering error  X  cc as the number of (ordered) pairs of points that are clustered inconsistently with C  X  : where c ( u,v ) = 1 if u and v are in the same cluster in C , and 0 otherwise; c  X  ( u,v ) = 1 if u and v are in the same cluster in C  X  , and 0 otherwise. Note that we may divide the correlation-clustering error  X  cc into overclustering compo-nent  X  cco and underclustering component  X  ccu :  X  cco = |{ ( u,v )  X  X  X  X : c ( u,v ) = 1 and c  X  ( u,v ) = 0 }|  X  ccu = |{ ( u,v )  X  X  X  X : c ( u,v ) = 0 and c  X  ( u,v ) = 1 }| In our formal analysis we model the user as an oracle that provides edit requests.
 Definition 2.1 (Local algorithm) . We say that an interac-tive clustering algorithm is local if in each iteration only the cluster assignments of points involved in the oracle re-quest may be changed. If the oracle requests to split C i the algorithm may only reassign the points in C i . If the or-acle requests to merge C i and C j , the algorithm may only reassign the points in C i  X  C j .
 We next formally define the stability property of a cluster-ing that we study in this work.
 Definition 2.2 (Stability) . Given a clustering C = { C 1 ,C 2 ,  X  X  X  C k } over a domain X and a similarly func-tion S : X  X  X 7 X  &lt; , we say that C satisfies stabil-ity if for all i 6 = j , and for all A  X  C i and A 0 S ( A,C i \ A ) &gt; S ( A,A 0 ) , where for any two sets A,A In the following sections we will assume that the ground-truth clustering of the data set satisfies this stability prop-erty, and we have access to the corresponding similarity function. In order for our algorithms to make progress, the oracle requests must be somewhat consistent with the target clustering.
 Definition 2.3 (  X  -merge model) . In the  X  -merge model the oracle requests have the following properties split ( C i ) : C i contains points from two or more target clus-ters. merge ( C i ,C j ) : At least an  X  -fraction of the points in each C i and C j belong to the same target cluster.
 Definition 2.4 (Unrestricted-merge model) . In the unrestricted-merge model the oracle requests have the following properties split ( C i ) : C i contains points from two or more target clus-ters. merge ( C i ,C j ) : At least 1 point in each C i and C j to the same target cluster.
 Note that the assumptions about the nature of the split re-quests are the same in both models. In the  X  -merge model, the oracle may request to merge two clusters if both have a constant fraction of points from the same target cluster. In the unrestricted-merge model, the oracle may request to merge two clusters if both have some points from the same target cluster. In this section we describe and analyze the algorithms in the  X  -merge model. As a pre-processing step for all our algorithms, we first run the average-linkage algorithm on all the points in the data set to compute the global average-linkage tree, which we denote by T avg . The leaf nodes in this tree contain the individual points, and the root node contains all the points. The tree is computed in a bottom-up fashion: starting with the leafs in each iteration the two most similar nodes are merged, where the similarity be-tween two nodes N 1 and N 2 is the average similarity be-tween points in N 1 and points in N 2 .
 We assign a label  X  X mpure X  to each cluster in the initial clustering; these labels are used by the merge procedure. Given a split or merge request, a local clustering edit is computed from the average-linkage tree T avg as described in Figure 1 and Figure 2.
 To implement Step 1 in Figure 1, we start at the root of T and  X  X ollow X  the points in C i down one of the branches until we find a node that splits them. In order to implement Step 2 in Figure 2, it suffices to start at the root of T and perform a post-order traversal, only considering nodes that have  X  X nough X  points from both clusters, and return the first output node.
 The split procedure is fairly intuitive: if the average-linkage tree is consistent with the target clustering, it suffices to find the node in the tree where the corresponding points are first split in two. It is more challenging to develop a cor-rect merge procedure: note that Step 2 in Figure 2 is only correct if  X  &gt; 0 . 5 , which ensures that if two nodes in the tree have more than an  X  -fraction of the points from C i C , one must be an ancestor of the other. If the average-linkage tree is consistent with the ground-truth, then clearly the node equivalent to the corresponding target cluster (that C i and C j both intersect) will have enough points from C i and C j ; therefore the node that we find in Step 2 must be this node or one of its descendants. In addition, because our merge procedure replaces two clusters with three, we require pure/impure labels for the merge requests to termi-nate:  X  X ure X  clusters may only have other points added to them, and retain this label throughout the execution of the algorithm.
 We now state the performance guarantee for these split and merge algorithms.
 Theorem 3.1. Suppose the target clustering satisfies sta-bility, and the initial clustering has overclustering error  X  and underclustering error  X  u . In the  X  -merge model, for any  X  &gt; 0 . 5 , the algorithms in Figure 1 and Figure 2 re-quire at most  X  o split requests and 2(  X  u + k ) log 1 requests to find the target clustering.
 In order to prove the theorem, we observe that if the target clustering satisfies stability, then every node of the average-linkage tree must be laminar (consistent) with respect to the ground-truth clustering. We next formally state these observations.
 Definition 3.2 (Laminar) . A node N is laminar with re-spect to the ground-truth clustering C  X  if for each clus-ter C  X  i  X  C  X  we have either N  X  C  X  i =  X  , N  X  C  X  i , or C i  X  N .
 Lemma 3.3. Suppose the ground-truth clustering C  X  over a domain X satisfies stability with respect to a similarity function S . Let T avg be the average-linkage tree for X constructed with S . Then every node in T avg is laminar w.r.t. C  X  .
 It follows that the split computed by the algorithm in Fig-ure 1 must also be consistent with the target clustering; we call such splits clean .
 Definition 3.4 (Clean split) . A partition (split) of a cluster C i into clusters C i, 1 and C i, 2 is said to be clean if C C i, 2 are non-empty, and for each ground-truth cluster C  X  such that C  X  j  X  C i 6 =  X  , either C  X  j  X  C i = C  X  j  X  C C j  X  C i = C We now prove the correctness of the split/merge proce-dures.
 Lemma 3.5. If the ground-truth clustering satisfies stabil-ity and  X  &gt; 0 . 5 then, a. The split procedure in Figure 1 always produces a b. The new cluster added in Step 4 in Figure 2 must Proof. a. For purposes of contradiction, suppose the re-turned split is not clean: C i, 1 and C i, 2 contain points from the same ground-truth cluster C  X  j . It must be the case that C i contains points from several ground-truth clusters, which implies that w.l.o.g. C i, 1 contains points from some other ground-truth cluster C  X  l 6 = j . This implies that N not laminar w.r.t C  X  , which contradicts Lemma 3.3. b. By our assumption, at least 1 2 | C i | points from C i and points from C j are from the same ground-truth cluster C  X  Clearly, the node N 0 in T avg that is equivalent to C  X  l contains all the points in C  X  l and no other points) must con-tain enough points from C i and C j , and only ascendants and descendants of N 0 may contain more than an  X  &gt; 1 / 2 fraction of points from both clusters. Thus the node N that we find with a depth-first search must be N 0 or one of its descendants, and will only contain points from C  X  l . Using the above lemma, we can prove the bounds on the split and merge requests stated in Theorem 3.1.
 Proof Sketch: To bound the number of split requests, we observe that each clean split reduces the overcluster-ing error by exactly 1. Let us call a requested merge impure if one or both of the clusters are marked  X  X m-pure, X  and pure otherwise. To bound the number of im-pure merge requests, consider the pure sets P = { C i C j | C i is marked  X  X mpure X  and C i  X  C respond to the pure subsets of clusters that are marked  X  X m-pure X . We observe that each impure merge must reduce the size of one of the sets in P by an  X  -fraction. The number of pure merges is upper bounded by the number of pure clus-ters, which is at most the number of impure merge requests. To bound the number of edit requests with respect to the correlation clustering objective, we must use a different merge procedure, which is described in Figure 3. Here in-stead of creating a new  X  X ure X  cluster, we add these points to the larger of the two clusters in the merge. Using this merge procedure and the split procedure presented earlier gives the following performance guarantee.
 Theorem 3.6. Suppose the target clustering satisfies sta-bility, and the initial clustering has correlation-clustering error of  X  cc . In the  X  -merge model, for any  X  &gt; 2 / 3 , using the split and merge procedures in Figures 1 and 3 requires at most  X  cc edit requests to find the target clustering. Proof Sketch: We can verify that splitting C i may not in-crease  X  ccu , but it must decrease  X  cco by at least | C the merge, given that the reassigned points must be  X  X ure X  (all come from the same target cluster), we can verify that  X  cc must decrease if  X  &gt; 2 / 3 .
 When the data satisfies stronger stability properties we may simplify the presented algorithms and/or obtain better per-formance guarantees. In particular, if the data satisfies the strict separation property from (Balcan et al., 2008), we may change the split and merge algorithms to use the local single-linkage tree (constructed from only the points in the edit request). In addition, if the data satisfies strict thresh-old separation , we may remove the restriction on  X  and use a different merge procedure that is correct for any  X  &gt; 0 . In this section we further relax the assumptions about the nature of the oracle requests. As before, the oracle may re-quest to split a cluster if it contains points from two or more target clusters. For merges, now the oracle may request to merge C i and C j if both clusters contain only a single point from the same ground-truth cluster. We note that this is a minimal set of assumptions for a local algorithm to make progress, otherwise the oracle may always propose irrel-evant splits or merges that cannot reduce clustering error. For this model we propose the merge algorithm described in Figure 4. The split algorithm remains the same as in Fig-ure 1. To provably find the ground-truth clustering in this setting we require that each merge request must be chosen uniformly at random from the set of feasible merges. This assumption is consistent with the observation in (Awasthi &amp; Zadeh, 2010) that in the unrestricted-merge model with arbitrary request sequences, even very simple cases (ex. union of intervals on a line) require a prohibitively large number of requests. We do not make additional assump-tions about the nature of the split requests; in each itera-tion any feasible split may be proposed by the oracle. In this setting our algorithms have the following performance guarantee.
 Theorem 4.1. Suppose the target clustering satisfies sta-bility, and the initial clustering has overclustering error  X  and underclustering error  X  u . In the unrestricted-merge model, with probability at least 1  X  , the algorithms in Fig-ure 1 and Figure 4 require  X  o split requests and O (log k merge requests to find the target clustering.
 The above theorem is proved in a series of lemmas. We first state a lemma regarding the correctness of the Algorithm in Figure 4. We argue that if the algorithm merges C i and C , it must be the case that both C i and C j only contain points from the same ground-truth cluster. The proofs of the lemmas below are omitted due to space constraints. Lemma 4.2. If the algorithm in Figure 4 merges C i and C j in Step 3, it must be the case that C i  X  C  X  l and C j  X  C for some ground-truth cluster C  X  l .
 The  X  o bound on the number of split requests follows from the observation that each split reduces the overclustering error by exactly 1 (as before), and the fact that the merge procedure does not increase overclustering error.
 Lemma 4.3. The merge algorithm in Figure 4 does not increase overclustering error.
 The following lemmas bound the number of impure and pure merges. Here we call a proposed merge pure if both clusters are subsets of the same ground-truth cluster, and impure otherwise.
 Lemma 4.4. The merge algorithm in Figure 4 requires at most  X  u impure merge requests.
 Lemma 4.5. The probability that the algorithm in Figure 4 requires more than O (log k  X  2 u ) pure merge requests is less than . We perform two sets of experiments: we first test the pro-posed split procedure on the clustering of business listings maintained by Google, and also test the proposed frame-work in its entirety on the much smaller newsgroup docu-ments data set. 5.1. Clustering business listings Google maintains a large collection of data records rep-resenting businesses. These records are clustered using a similarity function; each cluster should contain records about the same distinct business; each cluster is summa-rized and served to users online via various front-end ap-plications. Users report bugs such as  X  X ou are display-ing the name of one business, but the address of another X  (caused by over-clustering), or  X  X  particular business is shown multiple times X  (caused by under-clustering). These bugs are routed to operators who examine the contents of the corresponding clusters, and request splits/merges ac-cordingly. The clusters involved in these requests may be quite large and usually contain records about several busi-nesses. Therefore automated tools that can perform the re-quested edits are very helpful.
 In particular, here we evaluate the effectiveness of our split procedure in computing correct cluster splits. We consider a binary split correct iff the two resulting sub-clusters are  X  X lean X  using Definition 3.4. Note that a clean split is suffi-cient and necessary for reducing the clustering error (mea-sured by  X  u +  X  o , see Section 2). To compute the splits, we use a  X  X ocal X  variation of the algorithm in Figure 1, where we use the average-linkage tree built only from the points in the cluster (referred to as Clean-Split ). This variation is easier to implement and run, but still gives a provably  X  X lean X  split for stronger assumptions on the data. For comparison purposes, we use two well-known tech-niques for computing binary splits: the optimal 2-median clustering ( 2-Median ), and a  X  X weep X  of the second-smallest eigenvector of the corresponding Laplacian ma-trix. Let { v 1 ,...,v n } be the order of the vertices when sorted by their eigenvector entries, we compute the par-tition { v 1 ,...,v i } and { v i +1 ,...,v n } such that its con-ductance is smallest ( Spectral-Balanced ), and a partition such that the similarity between v i and v i +1 is smallest ( Spectral-Gap ). We compare the split procedures on 25 over-clusters that were discovered during a clustering-quality evaluation (anonymized data available upon re-quest). The results are presented in Table 1. We observe that the Clean-Split algorithm works best, giving a correct split in 22 out of the 25 cases. The well-known Spectral-Balanced technique usually does not give correct splits for this application. The balance constraint usually causes it to put records about the same business on both sides of the partition (especially when all the  X  X lean X  splits are not well-balanced), which increases clustering error. As expected, the Spectral-Gap technique improves on this limitation (be-cause it does not have a balance constraint), but the result often still increases clustering error. The 2-Median algo-rithm performs fairly well, but it may not be the right tech-nique for this problem: the optimal centers may be records about the same business, and even if they represent distinct businesses, the resulting partition is still sometimes incor-rect.
 In addition to using the clean-split criteria, we also eval-uated the computed splits using the correlation-clustering error. We found that using this criteria Clean-Split also computes the best splits. Note that a clean split is suffi-cient to improve the correlation-clustering objective, but it is not necessary. 5.2. Clustering newsgroup documents In order to test our entire framework (the iterative appli-cation of our algorithms), we perform computational ex-periments on newsgroup documents data. 1 The objects in these data sets are posts to twenty different online forums (newsgroups). We sample these data to get data sets of manageable size (labeled A through E in the figures). We compute an initial clustering by perturbing the ground-truth. In each iteration, we compute the set of all feasible splits and merges: a split of a cluster is feasible if it con-tains points from 2 or more ground-truth clusters, and a merge is feasible if at least an  X  -fraction of points in each cluster are from the same ground-truth cluster. Then, we choose one of the feasible edits uniformly at random, and ask the algorithm to compute the corresponding edit. We continue this process until we find the ground-truth clus-tering or we reach 20000 iterations. Our initial clusterings have over-clustering error of about 100, under-clustering error of about 100; and correlation-clustering error of about 5000.
 We notice that for newsgroup documents it is difficult to compute average-linkage trees that are very consistent with the ground-truth. This observation was also made in other clustering studies that report that the hierarchical trees con-structed from these data have low purity (Telgarsky &amp; Das-gupta, 2012; Heller &amp; Ghahramani, 2005). These observa-tions suggest that these data are quite challenging for clus-tering algorithms. To test how well our algorithms can per-form with better data, we prune the data sets by repeatedly finding the outlier in each target cluster and removing it, where the outlier is the point with minimum sum-similarity to the other points in the target cluster. For each data set, we perform experiments with the original (unpruned) data set, a pruned data set with 2 points removed per target clus-ter, and a pruned data set with 4 points removed per target cluster, which prunes 40 and 80 points, respectively (given that we have 20 target clusters). We first experiment with local clustering algorithms in the  X  -restricted merge setting. Here we use the algorithm in Figure 1 to perform the splits, and the algorithm in Fig-ure 2 to perform the merges. We show the results of running our algorithm on data set A in Figure 5. We find that for larger settings of  X  , the number of edit requests (necessary to find the target clustering) is very favorable and is con-sistent with our theoretical analysis. The results are better for pruned datasets, where we get very good performance regardless of the setting of  X  . The results for algorithms in Figure 1 and Figure 3 (for the correlation-clustering objec-tive) are very favorable as well. We also experiment with algorithms in the unrestricted merge model. Here we use the same algorithm to perform the splits, but use the algorithm in Figure 4 to perform the merges. We show the results on dataset A in Figure 5. We find that for larger settings of  X  our results are better than our theoretic analysis (we only show results for  X   X  0 . 5 ), and performance improves further for pruned datasets. Our investigations show that for unpruned datasets and smaller settings of  X  , we are still able to quickly get close to the tar-get clustering, but the algorithms are not able to converge to the target due to inconsistencies in the average-linkage tree. We can address some of these inconsistencies by con-structing the tree in a more robust way, which indeed gives improved performance for unpruned data sets. We also consider a setting where the initial clustering is already very accurate. In order to simulate this scenario, when we compute the initial clustering, for each document we keep its ground-truth cluster assignment with probabil-ity 0.95, and otherwise reassign it to one of the other clus-ters, which is chosen uniformly at random. This procedure usually gives us initial clusterings with over-clustering and under-clustering error between 5 and 20, and correlation-clustering error between 500 and 1000. As expected, in this setting our interactive algorithms perform much better, es-pecially on pruned data sets. Figure 6 displays the results; we can see that in these cases it often takes less than one hundred edit requests to find the target clustering in both models. In this work we motivated and studied new models and al-gorithms for interactive clustering. Several directions come out of this work. It would be interesting to relax the condi-tion on  X  in the  X  -merge model, and the assumption about the request sequences in the unrestricted-merge model. It is important to study additional properties of an interactive clustering algorithm. In particular, it is often desirable that the algorithm never increase the error of the current cluster-ing. Our algorithms in Figures 1, 3 and 4 have this prop-erty, but the algorithm in Figure 2 does not. It is also rele-vant to study more generalized notions of clustering error. In particular, we can define a small set of intuitive proper-ties of a clustering error, and then prove that the algorithms in Figure 1 and Figure 2 are correct for any clustering error that satisfies these properties; a similar analysis is possible for the unrestricted-merge model as well. Achlioptas, D. and McSherry, F. On spectral learning of mixtures of distributions. In Proceedings of the 18th An-nual Conference on Learning Theory , 2005.
 Angluin, D. Queries and concept learning. Machine Learn-ing , 2:319 X 342, 1998.
 Arora, S. and Kannan, R. Learning mixtures of arbitrary
Gaussians. In Proceedings of the 33rd ACM Symposium on Theory of Computing , 2001.
 Awasthi, Pranjal and Zadeh, Reza Bosagh. Supervised clustering. In NIPS , 2010.
 Balcan, Maria-Florina and Blum, Avrim. Clustering with interactive feedback. In ALT , 2008.
 Balcan, Maria-Florina, Blum, Avrim, and Vempala, San-tosh. A discriminative framework for clustering via sim-ilarity functions. In Proceedings of the 40th annual ACM symposium on Theory of computing , STOC  X 08, 2008. Bansal, Nikhil, Blum, Avrim, and Chawla, Shuchi. Corre-lation clustering. Machine Learning , 56(1-3), 2004. Basu, Sugato, Banjeree, A., Mooney, ER., Banerjee,
Arindam, and Mooney, Raymond J. Active semi-supervision for pairwise constrained clustering. In In
Proceedings of the 2004 SIAM International Conference on Data Mining (SDM-04 , pp. 333 X 344, 2004.
 Belkin, Mikhail and Sinha, Kaushik. Polynomial learning of distribution families. In FOCS , 2010.
 Boulis, Constantinos and Ostendorf, Mari. Combining multiple clustering systems. In In 8th European confer-ence on Principles and Practice of Knowledge Discovery in Databases(PKDD), LNAI 3202 , 2004.
 Brubaker, S. Charles and Vempala, Santosh. Isotropic PCA and affine-invariant clustering. CoRR , abs/0804.3575, 2008.
 Bryant, David and Berry, Vincent. A structured family of clustering and tree construction methods. Adv. Appl. Math. , 27(4), November 2001.
 Dai, Bo, Hu, Baogang, and Niu, Gang. Bayesian max-imum margin clustering. In Proceedings of the 2010
IEEE International Conference on Data Mining , ICDM  X 10, 2010.
 Dasgupta, S. Learning mixtures of Gaussians. In Proceed-ings of the 40th Annual Symposium on Foundations of Computer Science , 1999.
 Dasgupta, Sanjoy and Hsu, Daniel. Hierarchical sampling for active learning. In ICML , 2008.
 Heller, Katherine A. and Ghahramani, Zoubin. Bayesian hierarchical clustering. In ICML , 2005.
 Kalai, Adam Tauman, Moitra, Ankur, and Valiant, Gregory.
Efficiently learning mixtures of two Gaussians. In STOC , 2010.
 Kannan, R., Salmasian, H., and Vempala, S. The spectral method for general mixture models. In Proceedings of the 18th Annual Conference on Learning Theory , 2005. Krishnamurthy, Akshay, Balakrishnan, Sivaraman, Xu,
Min, and Singh, Aarti. Efficient active algorithms for hierarchical clustering. ICML , 2012.
 Moitra, Ankur and Valiant, Gregory. Settling the polyno-mial learnability of mixtures of gaussians. In FOCS , 2010.
 Telgarsky, Matus and Dasgupta, Sanjoy. Agglomerative Bregman clustering. ICML , 2012.
 Voevodski, Konstantin, Balcan, Maria-Florina, R  X  oglin,
Heiko, Teng, Shang-Hua, and Xia, Yu. Active clustering of biological sequences. Journal of Machine Learning Research , 13:203 X 225, 2012.
 Zhong, Shi. Generative model-based document clustering: a comparative study. Knowledge and Information Sys-
