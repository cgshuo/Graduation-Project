 Conjoint analysis is one of the most popular market research methodologies for assessing how customers with heteroge-neous preferences appraise various objective characteristics in products or services, which provides critical inputs for many marketing decisions, e.g. optimal design of new prod-ucts and target market selection. Nowadays it becomes prac-tical in e-commercial applications to collect millions of sam-ples quickly. However, the large-scale data sets make tra-ditional conjoint analysis coupled with sophisticated Monte Carlo simulation for parameter estimation computationally prohibitive. In this paper, we report a successful large-scale case study of conjoint analysis on click through stream in a real-world application at Yahoo!. We consider identifying users X  heterogenous preferences from millions of click/view events and building predictive models to classify new users into segments of distinct behavior pattern. A scalable con-joint analysis technique, known as tensor segmentation, is developed by utilizing logistic tensor regression in standard partworth framework for solutions. In offline analysis on the samples collected from a random bucket of Yahoo! Front Page Today Module, we compare tensor segmentation against other segmentation schemes using demographic information, and study user preferences on article content within tensor segments. Our knowledge acquired in the segmentation re-sults also provides assistance to editors in content manage-ment and user targeting. The usefulness of our approach is further verified by the observations in a bucket test launched in Dec. 2008.
 H.1.0 [ Models and Principles ]: General; H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.5 [ Online Information Services ]: Web-based services; H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Experiments, Designs, Performance Conjoint Analysis, Classification, Clustering, Segmentation, Tensor Product, Logistic Regression
Since the advent of conjoint methods in marketing re-search pioneered by Green and Rao [9], research on theoret-ical methodologies and pragmatic issues has thrived. Con-joint analysis is one of the most popular marketing research methodologies to assess users X  preferences on various ob-jective characteristics in products or services. Analysis of trade-offs, driven by heterogeneous preferences on benefits derived from product attributes, provides critical input for many marketing decisions, e.g. optimal design of new prod-ucts, target market selection, and product pricing. It is also an analytical tool for predicting users X  plausible reactions to new products or services.

In practice, a set of categorical or quantitative attributes is collected to represent products or services of interest, while a user X  X  preference on a specific attribute is quantified by a utility function (also called partworth function). While there exist several ways to specify a conjoint model, additive models that linearly sum up individual partworth functions are the most popular selection.

As a measurement technique for quantifying users X  prefer-ences on product attributes (or partworths), conjoint anal-ysis always consists of a series of steps, including stimu-lus representation, feedback collection and estimation meth-ods. Stimulus representation involves development of stim-uli based on a number of salient attributes (hypothetical profiles or choice sets) and presentation of stimuli to appro-priate respondents. Based on the nature of users X  response to the stimuli, popular conjoint analysis approaches are ei-ther choice-based or ratings-based. Recent developments of estimation methods comprise hierarchical Bayesian (HB) methods [15], polyhedral adaptive estimation [19], Support Vector Machines [2, 7] etc.

New challenges on conjoint analysis techniques arise from emerging personalized services on the Internet. A well-known Web site can easily attract millions of users daily. The Web content is intentionally programmed that caters to users X  needs. As a new kind of stimuli, the content could be a hy-perlink with text or an image with caption. The traditional stimuli, such as questionnaires with itemized answers, are seldom applied on web pages, due to attention burden on the user side. Millions of responses, such as click or non-click, to content stimuli are collected at much lower cost compared to the feedback solicited in traditional practice, but the im-plicit feedback without incentive-compatibility constraints are potentially noisy and more difficult to interpret. Al-though indispensable elements in the traditional settings of conjoint analysis have been changed greatly, conjoint anal-ysis is still particularly important in identifying the most appropriate users at the right time, and then optimizing available content to improve user satisfaction and retention.
We summarize three main differences between Web-based conjoint analysis and the traditional one in the following: In this paper, we conduct a case study of conjoint analy-sis on click through stream to understand users X  intentions. We construct features to represent the Web content, and collect user information across the Yahoo! network. The partworth function is optimized in tensor regression frame-work via gradient descent methods on large scale samples. In the partworth space, we apply clustering algorithms to identifying meaningful segments with distinct behavior pat-tern. These segments result in significant CTR lift over both the unsegmented baseline and two demographic segmenta-tion methods in offline and online tests on the Yahoo! Front Page Today Module application. Also by analyzing charac-teristics of user segments, we obtain interesting insight of users X  intention and behavior that could be applied for mar-ket campaigns and user targeting. The knowledge could be further utilized to help editors for content management.
The paper is organized as follows: In Section 2, we de-lineate the scenario under consideration by introducing the Today Module application at Yahoo! Front Page; In Sec-tion 3, we review related work in literature; In Section 4, we present tensor segmentation in detail. We report exper-imental results in Section 5 and conclude in Section 6. Figure 1: A snapshot of the default  X  X eatured X  tab in the Today Module on Yahoo! Front Page, de-lineated by the rectangular. There are four articles displayed at footer positions, indexed by F1, F2, F3 and F4. One of the four articles is highlighted at the story position. At default, the article at F1 is highlighted at the story position.
In this section, we first describe our problem domain and our motivations for this research work. Then we describe our data set and define some notations. Today Module is the most prominent panel on Yahoo! Front Page, which is also one of the most popular pages on the Internet, see a snapshot in Figure 1. The default  X  X eatured X  tab in Today Module highlights one of four high-quality articles selected from a daily-refreshed article pool curated by human editors. As illustrated in Figure 1, there are four articles at footer positions, indexed by F1, F2, F3 and F4 respectively. Each article is represented by a small picture and a title. One of the four articles is highlighted at the story position, which is featured by a large picture, a title and a short summary along with related links. At default, the article at F1 is highlighted at the story position. Auser can click on the highlighted article at the story position to read more details if the user is interested in the article. The event is recorded as a  X  X tory click X . If a user is interested in the articles at F2  X  F4 positions, she can highlight the article at the story position by clicking on the footer position.
A pool of articles is maintained by human editors that in-carnates the  X  X oice X  of the site, i.e., the desired nature and mix of various content. Fresh stories and breaking news are regularly acquired by the editors to replace out-of-date ar-ticles every a few hours. The life time of articles is short, usually just a few hours, and the popularity of articles, mea-sured by their click-through rate (CTR) 1 , changes over time. Yahoo! Front Page serves millions of visitors daily. Each visit generates a  X  X iew X  event on the Today Module, though the visitor may not pay any attention to the Today Mod-ule. The users of the Today Module, a subset of the whole
CTR of an article is measured by the total number of clicks on the article divided by total number of views on the article in a certain time interval. traffic, also generate a large amount of  X  X tory click X  events by clicking at story position for more details of the stories they would like to read. Our setting is featured by dynamic characteristics of articles and users. Scalability is also an important requirement in our system design.

One of our goals is to increase user activities, measured by overall CTR, on the Today Module. To draw visitors X  atten-tion and increase the number of clicks, we would like to rank available articles according to visitors X  interests, and to high-light the most attractive article at the F1 position. In our previous research [1] we developed an Estimated Most Pop-ular algorithm (EMP), which estimates CTR of available articles in near real-time by a Kalman filter, and presents the article of the highest estimated CTR at the F1 position. Note that there is no personalized service in that system. i.e. the article shown at F1 is the same to all visitors at a given time. In this work we would like to further boost overall CTR by launching a partially personalized service. User segments which are determined by conjoint analysis will be served with different content according to segmental interests. Articles with the highest segmental CTR will be served to user segments respectively.

In addition to optimizing overall CTR, another goal of this study is to understand users X  intention and behavior to some extend for user targeting and market campaigns. Once we identify users who share similar interests in conjoint analy-sis, predictive models can be built to classify users (including new visitors) into segments. For example, if we find a user segment who like  X  X ar &amp; Transportation X  articles, then we can target this user segment for Autos marketing campaigns. Furthermore, with knowledge of user interests in segments, we can provide assistance to the editors for content manage-ment. For example, if we know that most users in a segment who like articles of News visit us in the morning while most users in another segment who like articles about TV come in the evening. Then editors can target these two segments by simply programming more News articles in the morning and more TV-related articles in the evening.

Note that if we only consider the first goal of maximizing overall CTR, personalized recommender systems at individ-ual level might be an appropriate approach to pursue. In our recent research [4], we observed that feature-based models yield higher CTR than conjoint models in offline analysis. However, conjoint models significantly outperform the  X  X ne-size-fits-all X  EMP approach on the metric of CTR, and also provide actionable management on content and user target-ing at segment level. The flexibility supplied by conjoint models is valuable to portals such as Yahoo!.
We collected three sets of data, including content features, user profiles and interactive data between users and articles.
Each article is summarized by a set of features, such as topic categories, sub-topics, URL resources, etc. Each visi-tor is profiled by a set of attributes as well, e.g. age, gender, residential location, Yahoo! property usage, etc. Here we simply selected a set of informative attributes to represent users and articles. Gauch et al. [8] gave an extensive review on various profiling techniques.

The interaction data consists of two types of actions: view only or story click for each pair of a visitor and an article. One visitor can only view a small portion of available arti-cles, and it is also possible that one article is shown to the same visitor more than one time. It is difficult to detect whether the visitors have paid enough attention to the ar-ticle at the story position. Thus in data collection a large amount of view events are false non-click events. In another words, there is a relatively high rate of false negative (false non-click) in our observations.

There are multiple treatments on users X  reactions in mod-eling the partworth utility, such as In the Today Module setting, Poisson-based and metric-based responses might be vulnerable by the high rate of false negative observations. Thus we follow the choice-based re-sponses only in this work.
Let index the i -th user as x i ,a D  X  1 vector of user fea-tures, and the j -th content item as z j ,a C  X  1 vector of article features. We denote by r ij the interaction between the user x i and the item z j ,where r ij  X  X  X  1 , +1 } for  X  X iew X  event and  X  X tory click X  event respectively. We only observe inter-actions on a small subset of all possible user/article pairs, and denote by O the set of observations { r ij } .
In very early studies [21], homogeneous groups of con-sumers are entailed by a priori segmentation. For exam-ple, consumers are assigned to groups on the basis of demo-graphic and socioeconomic variables, and the conjoint mod-els are estimated within each of those groups. Clearly, the criteria in the two steps are not necessarily related: one is the homogeneity of customers in terms of their descrip-tive variables and another is the conjoint preference within segments. However, this segmentation strategy is easy to implement, which is still widely applied in industry.
Traditionally, conjoint analysis procedures are of two-stage: 1) estimating a partworth utility function in terms of at-tributes which represents customers X  preference at individual-level, e.g. via ordinary least squares regression; 2) if seg-mentation is of interest to marketing, through hierarchi-cal or non-hierarchical clustering algorithms, customers are grouped into segments where people share similar individual-level partworths.

Conjoint studies, especially on the partworth function, de-pend on designs of stimuli (e.g. product profiles or choices sets on questionnaires) and methods of data collection from respondents. One of challenges in the traditional conjoint analysis is to obtain sufficient data from respondents to es-timate partworth utilities at individual level using relatively few questions. The theory of experimental design is adapted for constructing compact profiles to evaluate respondents X  opinion effectively. Kuhfeld et al. [14] studied orthogonal designs for linear models. Huber and Zwerina [11] brought two additional properties, minimal level overlap and util-ity balance, into choice-based conjoint experiments. Sandor and Wedel [17] developed experimental designs by utilizing prior information. More references are reviewed by Rao [16] recently.

Hierarchical Bayesian (HB) methods [15] are developed to exploit partworth information of all respondents in modeling the partworth function. The HB models relate the variation in a subject X  X  metric-based responses and the variation in the subjects X  partworths over the population as follows: where  X  i is a C -dimensional partworth vector of the user i and W is a matrix of regression coefficients that relates user attributes to partworths. The error terms { ij } and {  X  i } in eq(1) and eq(2) are assumed to be mutually in-dependent and Gaussian random variables with zero mean and covariance matrix {  X  2 j } and  X  respectively, i.e. ij N (0 , X  2 j )and  X  i  X  X  (0 ,  X  ). 2 Together with appropriate prior distributions over the remaining variables, the poste-rior analysis yields the HB estimator of the partworths as a convex function of an individual-level estimator and a pooled estimator, in which the weights mainly depend on the noise levels {  X  2 j } and  X  in eq(1) and eq(2). The estimation on these noise levels usually involves the Monte Carlo simu-lation, such as the Gibbs sampling or Metropolis-Hastings algorithms [20], which is computationally infeasible for our applications with millions of samples.

Huber and Train [10] compared the estimates obtained from the HB methods with those from classical maximum simulated likelihood methods, and found the average of the expected partworths to be almost identical for the two meth-ods in some applications on electricity suppliers. In the past decade, integrated conjoint analysis methods have emerged that simultaneously segment the market and estimate segment-level partworths, e.g. the finite mixture conjoint models [6]. The study conducted by [20] shows the finite mixture con-joint models performed well at segment-level, compared with the HB models.

Toubia et al. [19] developed a fast polyhedral adaptive conjoint analysis method that greatly reduces respondent burden in parameter estimation. They employ X  X nterior point X  mathematical programming to select salient questions to re-spondents that narrow the feasible region of the partworth values as fast as possible.

A recently developed technique on parameter estimation is based on the ideas from statistical learning and support vector machines. The choice-based data can be translated as a set of inequalities that compare the utilities between two selected items. Evgeniou et al. [7] formulated the partworth learning as a convex optimization problem in regularization framework. Chapelle and Harchaoui [2] proposed two ma-chine learning algorithms that efficiently estimate conjoint models from pairwise preference data.

Jiang and Tuzhilin [12] experimentally demonstrated both 1-to-1 personalization and segmentation approaches signifi-cantly outperform aggregate modeling. Chu and Park [4] re-2 N (  X ,  X  2 ) denotes a Gaussian distribution with mean  X  variance  X  2 . cently proposed a feature-based model for personalized ser-vice at individual level. On the Today Module application, the 1-to-1 personalized model outperforms several segmen-tation models in offline analysis. However, conjoint models are still indispensable components in our system, because of the valuable insight on user intention and tangible control on both content and user sides at segment level.
In this section, we employ logistic tensor regression cou-pled with efficient gradient-descent methods to estimate the partworth function conjointly on large data sets. In the users X  partworth space, we further apply clustering tech-niques to segmenting users. Note that we consider the cases of millions of users and thousands of articles. The number of observed interactions between user/article pairs could be tens of million.
We first define an indicator as a parametric function of the tensor product of both article features z j and user attributes x as follows: where D and C are the dimensionality of user and content features respectively, z j,a denotes the a -th feature of z x i,b denotes the b -th feature of x i . The weight variable is independent of user and content features, which represents affinity of these two features x i,b and z j,a in interactions. In matrix form, eq(3) can be rewritten as where W denotes a D  X  C matrix with entries { w ab } .The partworths of the user x i on article attributes is evaluated as W x i , denoted as  X  x i , a vector of the same length of z
The tensor product above, also known as a bilinear model, can be regarded as a special case in the Tucker family [5], which have been extensively studied in literature and appli-cations. For example, Tenenbaum and Freeman [18] devel-oped a bilinear model for separating  X  X tyle X  and  X  X ontent X  in images, and recently Chu and Ghahramani [3] derived a probabilistic framework of the Tucker family for mod-eling structural dependency from partially observed high-dimensional array data.
 The tensor indicator is closely related to the traditional HB models as in eq(1) and eq(2). Respondent heterogene-ity is assumed either to be randomly distributed or to be constrained by attributes measured at individual level.
Conventionally the tensor indicator is related to an ob-served binary event by a logistic function. In our particular application, we found three additions in need: Based on the considerations above, the logistic function is defined as follows, where  X  s ij = s ij  X   X  i  X   X  j and s ij is defined as in eq(3).
Together with a standard Gaussian prior on the coeffi-cients { w ab } , i.e. w ab  X  X  (0 ,c ), 3 the maximum a posteriori estimate of { w ab } is obtained by solving the following opti-mization problem, where  X  denotes {  X  i } and  X  denotes {  X  i } .Weemploya gradient descent package to find the optimal solution. The gradient with respect to w ab is given by and the gradient with respect to the bias terms can be de-rived similarly. The model parameter c is determined by cross validation.
With the optimal coefficients W in hand, we compute the partworths for each training user by  X  x i = W x t .Thevector  X  x represents the user X  X  preferences on article attributes.
In the partworth space spanned by {  X  x i } , we further apply a clustering technique, e.g. K-means [13], to classify training users having similar preferences into segments. The number of clusters can be determined by validation in offline analy-sis.

For an existing or new user, we can predict her partworths by  X  x t = W x t ,where x t is the vector of user features. Then her segment membership can be determined by the shortest distance between the partworth vector and the centroids of clusters, i.e. where { o k } denote the centroids obtained in clustering.
We collected events from a random bucket in July, 2008 for training and validation. In the random bucket, articles are randomly selected from a content pool to serve users. An event records a user X  X  action on the article at the F1 position, which is either  X  X iew X  or  X  X tory click X  encoded as  X  1 and +1 respectively. We also collected events from a random bucket in September 2008 for test.

Note that a user may view or click on the same article multiple times but at different time stamps. In our train-ing, repeated events were treated as a single event. The distinct events were indexed in triplet format, i.e. (user id, article id, click/view). Appropriate priors can be specified to bias terms too.
We split the July random bucket data by a time stamp threshold for training and validation. There are 37 . 806 mil-lion click and view events generated by 4 . 635 million users before the time stamp threshold, and 0 . 604 million click events happened after the time stamp for validation. In the September test data, there are about 1 . 784 million click events.
 The features of users and items were selected by  X  X upport X . The  X  X upport X  of a feature means the number of samples having the feature. We only selected the features of high support above a prefixed threshold, e.g. 5% of the popula-tion. Each user is represented by a vector of 1192 categorical features, which include: Each article is profiled by a vector of 81 static features. The 81 static features include:
Categorical features are encoded as binary vectors with non-zero indicators. For example,  X  X ender X  is translated into two binary features, i.e.,  X  X ale X  is encoded as [0 , 1],  X  X emale X  is encoded as [1 , 0] and  X  X nknown X  is [0 , 0]. As the num-ber of non-zero entries in a binary feature vector varies, we further normalized each vector into unit length, i.e., non-zero entries in the normalized vector are replaced by 1 / where k is the number of non-zero entries. For article fea-tures, we normalized URL and Editor categories together. For user features, we normalized behavioral categories and the remaining features (age, gender and location) separately, due to the variable length of behavioral categories per user. Following conventional treatment, we also augmented each feature vector with a constant attribute 1. Each content item is finally represented by a feature vector of 82 entries, while each user is represented by a feature vector of 1193 entries.
For each user in test, we computed her membership first as in eq(4), and sorted all available articles in descending or-der according to their CTR in the test user X  X  segment at the time stamp of the event. On click events, we measured the rank position of the article being clicked by the user. The performance metric we used in offline analysis is the num-ber of clicks in top four rank positions. A good predictive model should have more clicks on top-ranked positions. We computed the click portion at each of the top four rank po-the metric  X  X ift X  over the baseline model, which is defined as
We trained our logistic regression models on the training data with different values of the trade-off parameter c where Figure 2: Click portion at the top rank position with different cluster number on the July validation data. c  X  X  0 . 01 , 0 . 1 , 1 , 10 , 100 } , and examined their performance (click portion at the top position) on the validation data set. We found that c = 1 gives the best validation performance.
Using the model with c = 1, we run K-means cluster-ing [13] on the partworth vectors of training users computed as in eq(4) to group users with similar preferences into clus-ters. We varied the number of clusters from 1 to 20, and presented the corresponding results of the click portion at the top rank position in Figure 2. Note that the CTR esti-mation within segments suffers from low-traffic issues when the number of segments is large. We observed the best val-idation performance at 8 clusters, but the difference com-pared with that at 5 clusters is not statistically significant. Thus we selected 5 clusters in our application.

To verify the stability of the clusters we found in the July data, we further tested on the random bucket data collected in September 2008. The EMP approach, described as in Section 2.1, was utilized as the baseline model. We also im-plemented two demographic segmentations for comparison purpose: We estimated the article CTR within segments by the same technique [1] used in the EMP approach. A user will be served with the most popular article in the segment which she belongs to.

We computed the lifts over the EMP baseline approach and presented the results of the top 4 rank positions in Fig-ure 3. All segmentation approaches outperform the baseline model, the EMP unsegmented approach. AgeGender seg-mentation having 11 clusters works better than Gender seg-mentation with 3 clusters in our study. Tensor segmentation with 5 clusters consistently gives more lift than Gender and AgeGender at all the top 4 positions.
We collected some characteristics in the 5 segments we discovered. On the September data, we identified cluster Figure 3: A comparison on lift at top 4 rank posi-tions in the offline analysis on the September data. The baseline model is the EMP unsegmented ap-proach. Figure 4: Population distribution in the 5 segments. membership for all users, and plotted the population distri-bution in the 5 segments as a pie chart in Figure 4. The largest cluster takes 32% of users, while the smallest cluster contains 10% of users. We further studied the user composi-tion of the 5 clusters with popular demographic categories, and presented the results in Figure 5 as a Hinton graph. We found that We also observed that c1 and c2 contains a small portion of users above age 55, and c3 has some young female users as well. Here, cluster membership is not solely determined by demographic information, though the demographic informa-tion gives a very strong signal. It is users X  behavior (click pattern) that reveals users X  interest on article topics.
We utilized the centroid of each cluster as a representa-tive to illustrate users X  preferences on article topics. The centroids in the space of article topics are presented in Fig-ure 6 as a heatmap graph. The gray level indicates users X  preference, from like (white) to dislike (black). We found the following favorite and unattractive topics by comparing the representatives X  scores across segments: Figure 5: Distributions of users with a specific de-mographic category in the 5 clusters. In the Hinton graph, each square X  X  area represents user percentage in a cluster. Each column represents users having a particular demographic category, while each row is of a cluster. Figure 6: Users X  preferences on selected article top-ics in the 5 clusters. Each square X  X  gray level indi-cates the preference of a segment on the correspond-ing article topic, from white (like) to black (dislike). Some topics are preferable for any user, such as Celebrity, but the discrepancy on interests is significant on most of top-ics. The discrepancy between clusters can be exploited by editors in content management to enhance user engagement.
One interesting finding in our conjoint analysis is that vis-iting patterns of some segments are quite different from the http://omg.yahoo.com/, a web site of celebrity gossip, news, photos, etc.
 Figure 7: Fraction of views in each user segment over a week. The first day is a US holiday and the sixth and seventh day are Saturday and Sunday.
 Table 1: Bucket test results of three segmentation methods, Gender, AgeGender and Tensor-5. All buckets have served the almost same amount of page views.
 others, as shown in Figure 7. Yahoo! Front Page is visited by more older users (c3 and c4) in the morning while by more younger users (c1 and c2) in the late afternoon. Most users around the midnight are international users (c5). Por-tion of traffic from older male users significantly decreases during weekend/holiday while traffic from other segments remains almost the same level for the entire week. This finding suggests some tips for content management, such as programming more articles related with News, Politics and Finance in the morning of weekdays, more entertainment articles of Sports and Music in the late afternoon, and more articles relevant to international users around midnight.
We can also monitor user activities within segments and remind editors to target underperformed segments when the CTR within the segments runs below their average level.
To validate the tensor segmentation we proposed, we launched a bucket test in December 2008. Three segmentation meth-ods, Gender, AgeGender and Tensor-5, were implemented in our production. From 8:00am 12 December to 0:00am 15 December, each of the three schemes and the control (EMP) bucket served about several million page views respectively. The numbers of page views in these four buckets are almost the same in the three days. We computed the story CTR and reported the corresponding lifts over the EMP control bucket for the three segmentation schemes in Table 1. The tensor segmentation with 5 clusters yields the most lift in the bucket test. We also observed the AgeGender segmentation outperforms the Gender segmentation. The observations in the online bucket test are consistent with our results in of-fline analysis. Although the bucket test only lasted about 3 days around a weekend, the CTR lift we observed in sig-nificant amount of traffic gives another strong empirical ev-idence.
In this study, we executed conjoint analysis on a large-scale click through stream of Yahoo! Front Page Today Module. We validated the segments discovered in conjoint analysis by conducting offline and online tests. We analyzed characteristics of users in segments and also found different visiting patterns of segments. The insight on user intention at segment level we found in this study could be exploited to enhance user engagement on the Today Module by assisting editors on article content management. In this study, a user can belong to only one segment. We would like to exploit other clustering techniques, such as Gaussian mixture mod-els, that allow for multiple membership, and then a user X  X  preference might be determined by a weighted sum of sev-eral segmental preferences. We plan to pursue this direction in the future.
We thank Raghu Ramakrishnan, Scott Roy, Deepak Agar-wal, Bee-Chung Chen, Pradheep Elango, and Ajoy Sojan for many discussions and helps on data collection. [1] D. Agarwal, B. Chen, P. Elango, N. Motgi, S. Park, [2] O. Chapelle and Z. Harchaoui. A machine learning [3] W. Chu and Z. Ghahramani. Probabilistic models for [4] W. Chu and S.-T. Park. Personalized recommendation [5] R. Coppi and S. Bolasco, editors. Multiway data [6] W.S.DeSardo,M.Wedel,M.Vriens,and [7] T. Evgeniou, C. Boussios, and G. Zacharia.
 [8] S.Gauch,M.Speratta,A.Chandranouli,and [9] P. E. Green and V. R. Rao. Conjoint measurement for [10] J. Huber and K. Train. On the similarity of classical [11] J. Huber and K. Zwerina. On the importance of utility [12] T. Jiang and A. Tuzhilin. Segmenting customers from [13] T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko, [14] W. F. Kuhfeld, R. D. Tobias, and M. Garratt. [15] P.J.Lenk,W.S.DeSardo,P.E.Green,andM.R.
 [16] V. R. Rao. Developments in conjoint analysis. [17] Z. Sandor and M. Wedel. Designing conjoint choice [18] J. B. Tenenbaum and W. T. Freeman. Separating [19] O. Toubia, J. R. Hauser, and D. I. Simester. [20] M. Vriens, M. Wedel, and T. Wilms. Metric conjoint [21] Y. Wind. Issue and advances in segmentation research.
