 We evaluate a framework for BM25F-based XML element re-trieval. The framework gathers contextual information asso-ciated with each XML element into an associated field, which we call a characteristic field . The contents of the element and the contents of the characteristic field are then treated as distinct fields for BM25F weighting purposes. Evidence supporting this framework is drawn from both our own ex-periments and experiments reported in related work. H.3.3 [ Information Search and Retrieval ]: Retrieval models Theory, Experimentation XML retrieval, BM25, BM25F, Wikipedia, book search
INEX [1], the annual Initiative for the Evaluation of XML retrieval, includes experiments on ad hoc focused XML el-ement retrieval, where the task is to return a ranked list of document elements (e.g., paragraphs, sections, abstracts) in response to a previously unseen query. Elements are re-quired to be non-overlapping, so that no returned element contains another, but otherwise any document element may be returned.

While various participating groups have reported attempts to exploit XML structure in order to improve performance on this task, none of these efforts have consistently outper-formed the simple approach of applying Okapi BM25 [12] to score individual XML elements and then filtering the re-sulting ranked list to remove overlap. Under this approach, each element is scored as if it were an independent docu-ment. The context of the element  X  such as information appearing in the elements that surround it  X  is ignored. Runs using this basic approach ranked third in 2004, third in 2007, and first in 2008 [4,5,9].

Okapi BM25 is a well-established ranking formula, which has proven its value across a wide range of domains and ap-
For INEX 2005, Robertson et al. applied an earlier ver-sion of BM25F to XML element retrieval [8, 11], reporting 65% improvements over BM25 measured by nxCG(10) on INEX IEEE collection with a different task where overlap is allowed. In that work, an element X  X  score is computed from multiple fields, which may include the body of the element, the document X  X  title, the document X  X  abstract, and ancestral section titles.
 Trotman describes another effort to extend BM25F to XML element retrieval [15] on TREC Wall Street Journal collection, but showed that improvement obtained is 0.64% computed by mean average precision. BM25F has also been used for XML-encoded book retrieval, where the task was to return books not elements [7,16] producing 9.09% improve-ment measured by NDCG@1.
In order to simplify the application of BM25F to XML element retrieval, we propose a framework with only two fields for each element. The body field contains the element body, and the characteristic field contains any contextual or background information that characterizes the element. The precise contents of the characteristic field may vary from element to element. While this approach is similar to that of Robertson et al. [11] and Lu et al. [8] it avoids the complexity of multiple field types and allows a consistent approach to be applied across heterogenous elements. Ad Hoc Retrieval . We first report the results of runs on INEX 2009 ad hoc task. We trained on a 5.9GB INEX 2008 Wikipedia corpus [3] with 659,387 articles and 70 assessed topics and tested on a 50.7GB INEX 2009 Wikipedia cor-pus [14] with 2,666,190 articles and 68 assessed topics. Our training optimized the official metric of iP[0.01]. For these runs, we used a characteristic field formed from the titles of the article and the sections in which an element occurs.
Table 1 shows the official INEX results. The BM25F run that ranked first gives a 6.62% improvement over the BM25 run that ranked 12th.

Book Page Retrieval . We used INEX 2008 Book Track data [6] of 50239 books of size 37GB after pre-processing. Only 25 out of 70 topics had relevance judgements, thus we used 17 of them for training, and 8 for testing. The corpus comes with a file, machine readable cataloging (MARC) for-mat [10], that contains information such as book category and library of congress classification (LCC) code.
The Book track task required to group the pages by the books and rank the books. Thus all of our runs did so and ranked the books by the highest scoring page returned for the book. Training maximized mean average precision.
Table 2 shows the results of our experiments. The runs with the plus signs indicate information used in the charac-teristic field. We see that using characteristic information gives up to 48.92% and 35.45% improvement over BM25 during training and testing respectively.

