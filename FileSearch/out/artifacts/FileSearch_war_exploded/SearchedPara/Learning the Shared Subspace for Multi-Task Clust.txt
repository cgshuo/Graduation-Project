
Clustering has a long history in the machine learning literature. It aims to partition data points into groups, so that the data points in the same group are relatively similar, while the data points in different groups are relatively dis-similar. In the past decades, incorporating prior knowledge into clustering has witnessed increasing interest, e.g. semi-supervised clustering [1] [2] [3] [4] [5] and co-clustering [6] [7] [8] [9]. However, all these methods are limited to a single task, where i.i.d. assumption of the data samples holds. We refer them as single-task clustering .

There are many different but related data sets in real appli-cations. For example, we have web pages from 4 universities, e.g. Cornell, Texas, Wisconsin and Washington. And we are going to cluster the web pages of each university into 7 cate-gories, e.g. student, faculty, staff, department, course, project and the other. In this scenario, clustering the web pages of each university can be seen as a task. Our intuition tells us that the 4 clustering tasks are related, since the sources and contents of their data are similar. However, the distributions of their data should be different, since different universities exhibit different features. Imagine that if we have limited web pages in one university, typical clustering methods may fail to discover the correct clusters. In this case, one may argue to use the web pages from the other universities as an auxiliary data to inform the correct clusters. However, simply combining them together followed with traditional single-task clustering approach does not necessarily lead to performance improvement, because their distributions are different, which violates the i.i.d. assumption in single-task clustering. To address this problem, new clustering paradigm is imperative, which can utilize the relation of different tasks to enhance clustering as well as overcome the non i.i.d. problem.

In this paper, based on the observations mentioned above, we will study a novel clustering paradigm, namely multi-task clustering , which can exploit the knowledge shared by multiple related tasks. It falls in the field of multi-task learning [10] [11] [12] [13] [14] [15], which says learning multiple related tasks together may achieve better performance than learning these tasks individually, provided that we can exploit the underlying relation. The assumption of our multi-task clustering is that there is a common underlying subspace shared by the multiple related tasks. This underlying subspace can be seen as a new feature representation, in which the data distributions of the related tasks are close to each other. Hence single-task clustering algorithm can be applied in this shared subspace. Similar assumption has also been made in several multi-task classi-fication approaches [11] [13] [14] [15]. Based on the above assumption, we propose a multi-task clustering method. It aims to learn a subspace shared by all the tasks, through which the knowledge of one task can be transferred to another. And the objective of our approach consists of two parts: (1) Within-task clustering: clustering the data of each task in its input space individually; and (2) Cross-task clustering: simultaneous learning the shared subspace and clustering the data of all the tasks. Our approach not only utilizes the knowledge in each individual task as traditional clustering method does, but also make use of the knowledge shared by the related tasks which may benefit the clustering performance. We will show that it can be solved via alternating minimization, and its convergence is theoretically guaranteed. To the best of our knowledge, this is the first work addressing multi-task clustering.

Furthermore, we will show that provided with the labels of one task, our multi-task clustering method turns out to be a transductive transfer classification method (a.k.a. cross-domain classification or domain adaption), in which the label of a source task (in-domain) is available as prior knowledge, and we aim to utilize this prior knowledge to predict the labels of the data in a related target task (out-of-domain). Ex-periments on several cross-domain text data sets demonstrate that the proposed multi-task clustering method outperforms traditional single-task clustering methods greatly. And the transductive transfer classification method is comparable to or even better than several existing transductive transfer classification approaches.
 The remainder of this paper is organized as follows. In Section II, we will review some related works. In Section III we will propose the multi-task clustering algorithm. In Section IV, we will extend the multi-task clustering method to transfer clustering setting. Experiments on text data sets are demonstrated in Section V. Finally, we draw a conclusion in Section VI and point out the future works.

In this section, we will review some works related with ours.
 A. Multi-Task Learning
Empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance, relative to learning these tasks independently. This motivates multi-task learning [10] [11] [12] [13] [14] [15]. However, existing multi-task learning methods all tackle classification, in which each task has both labeled and unlabeled data, and the goal is to predict the class labels of unlabeled data in each task by utilizing within-task and cross-task knowledge. In this paper, we consider multi-task clustering, where the data in each task are all unlabeled, and it aims at predicting the cluster labels of the data in each task.
 B. Transfer Learning
Transfer learning [16] [17] is closely related with multi-task learning. It tackles the transfer of knowledge across tasks, domains, categories and distributions that are similar but not the same. In this paper, we refer task and domain as the same thing. Transfer learning is closely related with multi-task learning, with the difference that in multi-task learning, the learner focuses on enhancing the performance of all the tasks, while in transfer learning, the learner only focuses on improving the performance of a so-called target task (out-of-domain) by using the knowledge from a so-called source task (in-domain). Transfer learning can be categorized as (1) inductive transfer: there are a few labeled data in the target task, while there are a large amount of labeled [18] [19] [20] or unlabeled [21] data in the source task, (2) transductive transfer: there are no labeled data in the target task, while there are large amount of labeled data in the source task [22] [23] [24] [25], this is also called cross-domain classification or domain adaption, and (3) Unsupervised transfer: there are no labeled data in the target task, while there are large amount of unlabeled data in the source task [26]. In our study, we focus on transductive transfer classification, which belongs to the second category. C. Clustering with Background and Prior Knowledge
Improving clustering performance using the background and prior knowledge has witnessed increasing interest in the past decade. One direction is co-clustering [6] [7] [8] [9], which clusters the data and features simultaneously to enhance the clustering performance. Another direction is semi-supervised clustering [1] [2] [3] [4] [5], which incorporates pairwise constraints, e.g. must-link and cannot-link constraints, to assist clustering. Both co-clustering and semi-supervised clustering use either the background or prior knowledge within a single task. However, multi-task clustering exploits both in-task and out-of-task knowledge. D. Semi-Supervised Learning
In many practical machine learning problems, the acquisi-tion of sufficient labeled data is often expensive and/or time consuming. On the contrary, in many cases, large number of unlabeled data are far easier to obtain. Consequently, semi-supervised learning [27] [28] [29], which aims to learn from both labeled and unlabeled data points, has received signif-icant attention in the past decade. Semi-supervised learning is different from transductive transfer classification. In semi-supervised learning, the labeled and unlabeled samples are drawn from the same task, so their distributions are the same. However, in transductive transfer classification, the labeled samples are from the source task, while the unlabeled samples are from the target task. So their distributions are different.

In this section, we first present the problem setting of multi-task clustering. Then we propose a multi-task cluster-ing method and the optimization algorithm, followed with its convergence analysis.
 A. Problem Formulation
Suppose we are given m clustering tasks, each with a R d , 1  X  k  X  m , where n k is the number of data points in the k -th task. The goal of multi-task clustering is to {C j } c j =1 . Note that we assume the dimensionality of the feature vector of all the tasks is the same, i.e. d . It is appropriate since we could augment the feature vectors of all the tasks to make the dimensionality same. In fact, the bag-of-words document representation used in our experiments implicitly does the augmentation. Moreover, we assume that the number of clusters in each task is the same, i.e. c = c 2 = . . . = c m = c , which is also assumed in existing multi-task learning literature.
 B. Objective Let us consider the case of single-task clustering first. Take the k -th task for example. We are going to partition the k -th data set into c clusters. The classical K-means algorithm achieves this goal by minimizing the following objective where || X || 2 is 2 -norm and m ( k ) j is the mean of cluster C matrix, which represents the clustering assignment, such that P ij = 1 if x otherwise. This is also known as hard clustering, i.e. the cluster assignment is binary.

When it comes to multi-task clustering setting, we are going to learn a shared subspace, which is obtained by an tasks, in which we perform all the clustering tasks together. This shared subspace can be seen as a new feature space, in which the data distribution from all the tasks are similar with each other. As a result, we can cluster them together in this shared subspace using traditional single-task clustering algorithm, i.e. K-means. Furthermore, we add a constraint that the clustering result of each task in the shared subspace is the same as that in the input subspace, which intertwines the clustering in the input space and clustering in the shared subspace. Then it is formulated as minimizing where  X   X  [0 , 1] is a regularization parameter balancing the clustering in the input space and the clustering in the shared subspace, I is an identity matrix, and M = [ m 1 , . . . , m R m  X  c with m j is the mean of cluster C j of all the tasks in the shared subspace.

The objective in Eq.(3) consists of two terms. The first term is Within-task clustering, which includes k independent k-means clustering of each task in the input space. The second term is Cross-task clustering, which simultaneously learns the shared subspace and clusters the data of all the tasks together in the shared subspace. It is worth noting that the second term is similar with clustering the data of all the tasks together via Adaptive Subspace Iteration (ASI) clustering method [30]. The first term and the second term are intertwined through the partition matrices. When letting  X  = 1 , Eq.(3) degenerates to m independent K-means clustering. And When letting  X  = 0 , Eq.(3) turns out to be clustering data of all the tasks via ASI. In general case, the more related the tasks are, the smaller  X  we will set. values, which makes the minimization in Eq.(3) very dif-domain. Then the objective of multi-task clustering in Eq.(3) turns out to be We call Eq.(4) Learning the Shared Subspace for Multi-Task Clustering (LSSMTC).
 C. Optimization As we see, minimizing Eq.(4) is with respect to solution. In the following, we will present an alternating minimization algorithm to optimize the objective. In other words, we will optimize the objective with respect to one variable when fixing the other variables. 1) Computation of M : Given W and P ( k ) , optimizing Eq.(4) with respect to M is equivalent to optimizing where X = [ X (1) , . . . , X ( m ) ] and P = Setting  X  X  1  X  M = 0 , we obtain iterative solution. We introduce the Lagrangian multiplier
Using the Karush-Kuhn-Tucker condition [31]  X  ij P ( k ) ij 0 , we get Introduce A = A +  X  A  X  and B = B +  X  B  X  where A + ij = Eq.(13) leads to the following updating formula Eq.(4) with respect to W is equivalent to optimizing where X = [ X (1) , . . . , X ( m ) ] and P =
Substituting M = W T XP ( P T P )  X  1 into the above equation, we obtain It is easy to show that the optimal W minimizing Eq.(16) is composed of the eigenvectors of X ( I  X  P ( P T P )  X  1 P T ) X T corresponding to the l smallest eigenvalues.
 In summary, we present the algorithm of optimizing Eq.(4) in Algorithm 1.
 Algorithm 1 Learning the Shared Subspace for Multi-Task Clustering (LSSMTC)
Input: m tasks, { X ( k ) } m k =1 , the dimensionality of the shared subspace l , maximum number of iterations T ; Initialize t = 0 and P ( k ) , 1  X  k  X  m using K-means; while not convergent and t  X  T do end while D. Convergence Analysis In the following, we will investigate the convergence of Algorithm 1. We use the auxiliary function approach [33] to analyze the multiplicative updating formulas. Here we first introduce the definition of auxiliary function [33].
 Definition III.1. [33] Z ( h, h 0 ) is an auxiliary function for F ( h ) if the conditions are satisfied.
 Lemma III.2. [33] If Z is an auxiliary function for F , then F is non-increasing under the update Lemma III.3. [32] For any nonnegative matrices A  X  symmetric, then the following inequality holds Theorem III.4. Let Then the following function monotonically decrease the value of the objective in Eq.(4), the objective is invariant under the updating if and only if theorem.
 In addition to Theorem III.5, since the computation of W in Eq.(16) also monotonically decreases the value of the objective in Eq.(4), Algorithm 1 is guaranteed to converge.
In this section, we will show that given the labels of one task, the proposed multi-task clustering method turns out to be a transductive transfer classification method.
 { x 1 , x of data points in the k -th task. Without loss of generality, we assume the label of the 1 st task is given, and we are going to predict the labels of the data in the 2 nd task. This problem is exactly transductive transfer classification, which is also known as domain adaption or cross-domain classification. We call the 1 st task source task (in-domain), [ x 1 , . . . , x of classes in each task is the same, i.e. c 1 = c Note that it is trivial to generalize our transductive transfer classification method from 1 source task to more than 1 source task.

Based on the above discussion, our multi-task clustering method can be extended to transductive transfer classifica-tion as follows, where the first term is clustering in the input space of the target task, the second and the third term are clustering of the source and the target task together in the shared subspace,  X   X  [0 , 1] is a regularization parameter balancing the clustering in the input space and the clustering in the shared subspace. It should be noted that P (1) in Eq.(19) is a constant since the label of the source task has been known as prior knowledge.
 Again, we relax P (2) into nonnegative continuous domain. Then the objective of transductive transfer classification in Eq.(19) turns out to be We call Eq.(20) Learning the Shared Subspace for Trans-ductive Transfer Classification (LSSTTC).

Since the optimization of Eq.(20) is very similar with that of Eq.(4), we omit the derivation of the optimization algorithm, and present it in Algorithm 2 directly.

The convergence of Algorithm 2 is also theoretically guaranteed. The proof of convergence is very similar with that of Algorithm 1.
 Algorithm 2 Learning the Shared Subspace for Transductive Transfer Classification (LSSTTC)
Input: X (1) , P (1) , X (2) , the dimensionality of the shared subspace l , maximum number of iterations T ; Initialize P (2) using K-means;
Initialize W using any orthonormal matrix. while not convergent and t  X  T do end while
In our experiments, we will evaluate the proposed meth-ods on several cross-domain text data sets.
 A. Evaluation Metrics
To evaluate the clustering results, we adopt the perfor-mance measures used in [34]. These performance measures are the standard measures widely used for clustering.
Clustering Accuracy: Clustering Accuracy discovers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. Clustering Accuracy is defined as follows: where r i denotes the cluster label of x i , and l i denotes the true class label, n is the total number of documents,  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map ( r i ) is the permutation mapping function that maps each cluster label r i to the equivalent label from the data set.

Normalized Mutual Information: The second measure is the Normalized Mutual Information (NMI), which is used for determining the quality of clusters. Given a clustering result, the NMI is estimated by where n i denotes the number of data contained in the cluster C (1  X  i  X  c ) ,  X  n j is the number of data belonging to the L (1  X  j  X  c ) , and n i,j denotes the number of data that are in the intersection between the cluster C i and the class L The larger the NMI is, the better the clustering result will be.

To evaluate the classification results, we use the classifi-cation accuracy.
 B. Data Sets
In order to evaluate the proposed methods, we use 2 text data sets, which are widely used in cross-domain classifica-tion literature [22] [23] [25].
 ered from university computer science departments (Cornell, Texas, Washington, Wisconsin). There are about 8280 docu-ments and they are divided into 7 categories, and we choose student, faculty, course and project these four most popu-lous entity-representation categories for clustering, named WebKB4. We consider clustering the web pages of each university as one task. Therefore, we have 4 tasks. proximately 20000 newsgroup documents, partitioned across 20 different newsgroups nearly evenly. We generate 2 cross-domain data sets, i.e. Rec.vs.Talk and Comp.vs.Sci, for evaluating multi-task clustering and transductive transfer classification methods. In detail, two top categories are chosen, one as positive and the other as negative. Then the data are split based on sub-categories. The task is defined as top category classification. The splitting ensures the data in different tasks are related but different, since they are drawn from the same top category but different sub-categories. The detailed constitutions of the 2 data sets are summarized in Table I.

Table.II summarizes the characteristics of the 3 data sets used in this experiment.
 C. Experiment 1: Multi-Task Clustering
In this experiment, we study multi-task clustering. We assume that the labels of all the tasks in each data set are unknown. We compare the proposed multi-task clus-tering method with typical single-task clustering meth-ods, e.g. Kmeans (KM), Principal Component Analysis (PCA)+Kmeans (PCAKM), Normalized Cut (NCut) [35] and adaptive subspace iteration (ASI) [30]. Note that Kmeans can be seen as a special case of the proposed method with  X  = 1 . We also present the experimental results of clustering the data of all the tasks together using Kmeans, PCA+Kmeans, NCut and ASI. Note that clustering the data of all the tasks together via ASI corresponds to the proposed method with  X  = 0 . 1) Methods &amp; Parameter Settings: We set the number of clusters equal to the true number of classes for all the clus-tering algorithms. For NCut, the scale parameter of Gaussian For PCAKM, the reduced dimension of PCA is set to the minimal number that preserves at least 95% of the information. For LSSMTC, we set l by searching the grid { 2 , 2 2 , 2 3 , 2 4 } . And the regularization parameter  X  is set by searching the grid { 0 . 25 , 0 . 5 , 0 . 75 } . Under each parameter setting, we repeat clustering 5 times, and the mean result as well as the standard deviation is computed. We report the mean and standard deviation result corresponding to the best parameter setting for each method to compare with each other. Since our algorithm is iterative, in our experiments, we prescribe the maximum number of iterations as T = 20 . 2) Clustering Results: We repeat each experiment 5 times, and the average results are shown in Table III, Table IV and Table V.  X  X ll X  refers to clustering the data of all the tasks together. We can see that LSSMTC indeed improves the clustering result, and outperforms Kmeans greatly, which is its single-task degeneration. This improvement owes to exploiting the relation among the tasks by learning the shared subspace. In Task 2 of WebKB4 data set, NCut achieves better clustering result than our method. This is because NCut considers the geometric structure in the data, which is suitable for data sampled from manifold, while our method does not take this into account.

In addition, it is worthwhile noticing that although our method involves combining all the tasks together and doing dimensionality reduction, it far exceeds these simple opera-tions. As we see, simply clustering the data of all the tasks together does not necessarily improve the clustering result, because the data distributions of different tasks are different, and combining the data together directly will violate the i.i.d. assumption in single-task clustering. Moreover, the clustering result of doing dimensionality reduction followed with clustering is also not as good as LSSMTC, because it treats learning the subspace and clustering independently, while learning the subspace and clustering could benefit from each other.
 D. Experiment 2: Transductive Transfer Classification
In this experiment, we study transductive transfer classi-fication. We do experiments on any two tasks of each data set. One task is used as source task, in which the class labels of all the data are known. The other is used as target task, where the class labels of all the data are unknown and to be predicted. We compare the proposed transductive transfer classification method with support vector machine (SVM) [36], three semi-supervised learning methods, i.e. Gaussian Field Harmonic Function (GFHF) [27], Learning with Local and Global Consistency (LLGC) [28] and transductive SVM (TSVM) [29]. We also compare it with several existing transductive transfer classification methods, Co-Clustering based Classification (CoCC) [22] and Cross-Domain Spec-tral Classification (CDSC) [23]. 1) Methods &amp; Parameter Settings: For SVM, TSVM, CDSC, since they are designed originally for binary clas-sification, we address the multi-class classification via 1-vs-rest strategy. For SVM, it is trained on the source task, and tested on the target task. For TSVM, GFHF, LLGC and our method, they are trained using both labeled (source task) and unlabeled (target task) data, and are tested on kernel is used. The implementation of GFHF is the same as in [27]. The width of the Gaussian similarity is set via the is the mean distance between any two samples in the training set. And the size of neighborhood is searched by the grid { 5 , 10 , 50 , 80 , n  X  1 } . The implementation of LLGC is the same as in [28], in which the width of the Gaussian similarity and the size of neighborhood are also determined the same as that in GFHF, and the regularization parameter is set by searching the grid { 0 . 1 , 1 , 10 , 100 } . The implementation and parameter settings of CoCC and CDSC are the same as that in their papers. For LSSTTC, we set l by searching the grid { 100 , 200 , . . . , 900 , 1000 } . And the regularization parameter  X  is set by searching the grid { 0 . 25 , 0 . 5 , 0 . 75 } . Under each parameter setting, we repeat LSSTTC 5 times, and the mean result is computed. 2) Classification Results: The classification results are reported in Table VI, Table VII and Table VIII. It is obvious that the proposed transductive transfer classification method outperforms traditional single-task classification methods, e.g. SVM, TSVM, GFHF and LLGC, greatly on most transfer settings. This improvement is due to the prior knowledge, i.e. label information, in the related source task which is transferred to the target task by our method. It is also comparable to or even better than existing transductive transfer classification methods, e.g. CoCC and CDSC. Note that in Task 4  X  Task 1 and Task 4  X  Task 2 settings of WebKB4 data set,  X  X egative transfer X  [17] occurred, where transfer learning lowers the learning performance.

The contribution of this paper includes the following aspects. First of all, we initiate a novel clustering paradigm, i.e. multi-task clustering, which utilizes the relation among multiple clustering tasks and outperforms traditional single-task clustering methods greatly. As far as we know, this is the first work addressing multi-task clustering. Secondly, we extend our multi-task clustering method to transductive transfer classification, which is comparable to or even better than existing methods.

In our future work, we will extend our method to take into account geometric structure as in [34] [38].

This work was supported by the National Natural Sci-ence Foundation of China (No.60721003, No.60673106 and No.60573062) and the Specialized Research Fund for the Doctoral Program of Higher Education. We would like to thank the anonymous reviewers for their helpful comments. And we especially thank one of the anonymous reviewers for pointing out a recent work [39] which also considers ex-ploring the relation among multiple unsupervised domains. By Lemma III.3, we have To obtain the lower bound for the remaining terms, we use the inequality that z  X  1 + log z,  X  z &gt; 0 , then which is a diagonal matrix with positive diagonal elements. which we can get Eq.(18).

