 Crowdsourcing is the de-facto standard for gathering annotated data. While, in theory, data annotation tasks are assumed to be attempted by workers independently, in practice, data annotation tasks are of-ten grouped into batches to be presented and annotated by workers together, in order to save on the time or cost overhead of providing instructions or necessary background. Thus, even though indepen-dence is usually assumed between annotations on data items within the same batch, in most cases, a worker X  X  judgment on a data item can still be affected by other data items within the batch, leading to additional errors in collected labels. In this paper, we study the data annotation bias when data items are presented as batches to be judged by workers simultaneously. We propose a novel worker model to characterize the annotating behavior on data batches, and present how to train the worker model on annotation data sets. We also present a debiasing technique to remove the effect of such an-notation bias from adversely affecting the accuracy of labels ob-tained. Our experimental results on synthetic and real-world data sets demonstrate that our proposed method can achieve up to +57% improvement in F 1 -score compared to the standard majority voting baseline.
 H.2.8 [ Database Applications ]: Data mining Crowdsourcing; annotation bias; worker accuracy model
Crowdsourcing provides an efficient method to annotate data on a large scale for various machine learning tasks, by employing a massive workforce drawn from global Internet users. Popular on-line crowdsourcing platforms include Amazon Mechanical Turk and CrowdFlower 2 . However, while crowdsourcing is relatively https://www.mturk.com/ http://www.crowdflower.com/ c  X  cheap compared to employing experts, getting large quantities of labeled data annotated by crowds (say thousands, or millions of data items) can be rather expensive.

A key mechanism, often employed in practice for reducing costs, is batching , i.e., grouping multiple data items (to be annotated to-gether) into one single task as a batch. Batching can save significant monetary costs, since the necessary instructions and background for completing the task needs to be provided just once for the en-tire batch. Thus, the worker will spend less time on reviewing these instructions, and more time on annotating data items, and therefore will be able to annotate more data items within the same time. For instance, consider a scenario where a worker has to judge whether a comment is relevant to a document. Here, making a judgment for each comment requires reading through the entire document. Instead, with batching, the worker only needs to read the entire document once, and then make a judgment for all the comments in the batch. In fact, even from the workers X  point of view, it is also more attractive to label batches of data items as they can save time on switching between different tasks.

However, even though batching is an attractive option in practice due to its cost and time savings, having workers annotate batches can lead to severe correlation between annotations within batches. For example, say we have a task of annotating whether a review of the movie  X  X he Imitation Game X  crawled from IMDb is posi-tive. As illustrated in Figure 1(a), if we only show one review to be judged as part of each crowdsourcing unit task, workers will have to spend some time looking up the movie before they can make a single judgment on a review. Although judgments are likely to be independent, this way of assigning work is too costly to be prac-tical. Instead, if we assemble multiple reviews of the same movie into a batch, as shown in Figure 1(b), workers can make multiple judgments after they look up a movie. Nevertheless, in this case, the annotation of different reviews might interfere with each other. For example, the review  X  X verage In The Extreme X  does not seem like a positive review per se (Cf. top right in Figure 1(a)), while grouped with the review  X  X tack of Lies X , it looks much more like a positive review (Cf. top in Figure 1(b)). Similarly, when the review  X  X ood enough but historically sketchy X  looks quite positive by it-self (Cf. bottom left in Figure 1(a)), it does not look as positive as a strongly effusive review simply saying  X  X reat movie X , as shown in the bottom of Figure 1(b). Thus, overall these effects might be undesirable and misleading as it is inconsistent with the case when workers make independent judgments. Therefore, it is challenging to ascertain true labels of data items in batches.

So far, there has been little to no work in exploring the the possi-ble annotation error introduced by grouping data items into batches. Although batching data items has been adopted in many crowd-sourced tasks such as sorting [17], object recognition [32] or clus-tering [10], and anecdotally very widely used in practice, the as-sumption is often that the annotations are collected independently, which is not the case. While there is limited work on judging data items in sequence [18, 26, 27], it is not directly applicable to our setting where a batch of data items are presented and annotated in parallel. Our previous research [36] identified batch annotation bias, and demonstrated that in a binary classification, annotations on about 35% of positive items and 5% of negative items might be biased. Here, we focus on developing debiasing techniques. We defer the detailed discussion of the related work to Section 7. There are several research challenges in solving this problem. First, how do we model workers X  behavior when they make judg-ments in batches? Second, how do we leverage the model to debias the crowdsourced annotation of data batches? We make the follow-ing contributions in answering these questions: 1. Proposing an interpretable worker annotation model on 2. Debiasing annotation data obtained as batches. Based on 3. Conducting experiments on a real-world crowdsourcing
The rest of this paper is organized as follows: Section 2 intro-duces the basic concepts and formalizes the research problem; Sec-tion 3 proposes the worker model for annotating batches of data; Section 4 presents a strategy to debias batch annotations; Section 5 describes experimental results; Section 6 discusses extensions of our proposed method; Section 7 presents related work and Sec-tion 8 concludes.
In this section, we formally define the concepts and notations we use in this paper; we then formalize the problem of debiasing crowdsourced batches.
First we need to formalize several basic concepts in a crowd-sourcing platform. Suppose we are given a set of data items X = { x i } , where i = 1 ,...,n . Each data item is associated with a label y i  X  Y , and we thereby define Y = { y i } n lowing discussion, we focus on a binary classification task, where Y = { 0 , 1 } , but our framework generalizes to multi-class or rating cases seamlessly (Cf. Section 6). According to a standard for-malization in learning theory for binary classification, we suppose each ( x i ,y i ) is generated from a joint probability distribution P We define an inherent score  X  x i to be the conditional probability P ( y i = 1 | x i ) . For simplicity, we denote the inherent score as  X 
In a job or task submitted to a crowdsourcing platform, we can assemble several data items into a batch. Each batch b represented by a set of indices of data items in the batch, de-strict, data items in the batch should be represented by x { x b j 1 ,...,x b jk } . However, for simplicity, we denote data items in the batch specified by b j as { x j 1 ,...,x jk } . Similarly, we define y j = { y j 1 ,...,y jk } to be true labels associated with data items in x , where y jl is the true label of x jl according to Y ,  X  1  X  l  X  k . In CrowdFlower language, a batch corresponds to a single  X  X nit X , where a worker has to judge the entire unit at the same time; in Mechanical Turk language, a batch corresponds to a single  X  X IT X  (short for Human Intelligence Task). Usually, data items in the same batch might share the same context, background, or the same instruction, in order to reduce the overhead. For example, if one is asked to judge whether a review about a restaurant is positive or negative, it might save time for workers by grouping reviews of the same restaurant into the same batch, as they only need to read the description of the restaurant once before they can make multiple judgments on different reviews.

As we assemble data items into batches, each worker has to judge the entire batch as a single judgment. Given a batch b the judgment provided by a worker can be represented as y { y j 1 ,...,y 0 jk } , where y 0 jl  X  Y is the annotation of data item cor-responding to x jl , provided by the worker. Noting that the worker annotation y 0 j can be different from the true label y j worker annotation as  X  annotation  X , while the ground-truth label is referred to as simply the  X  label  X .

In CrowdFlower, as a judgment can only be made based on a unit, workers are not allowed to submit partial results on a batch (as with Mechanical Turk). However, one can always add an  X  X n-known X  option for every data item, so that the workers can provide partial results on a batch. For simplicity, we consider no partial judgments in the rest of the paper.

Now, we are in a position to give a formal definition for a batch of data items:
D EFINITION 1 (B ATCH ). Given a data set ( X,Y ) , a batch of data items with size k extracted from the given data set can be represented as ( b j , x j , y j , y 0 j ) , where b j = ( b set of indices for X and Y ; x j = { x j 1 ,...,x jk } is a set of all the data items, indexed by b j ; y j = { y j 1 ,...,y jk } consists of the corresponding true labels of data items in x j ; y 0 j = { y is the worker annotation on the set of the batch.
 Additionally, a set of batches can be defined as:
D EFINITION 2. Given a data set ( X,Y ) , a set of batches extracted from the given data set is denoted as A = ( B,X B ,Y B ,Y 0 B ) , where B = { b j } m j =1 consists of the indices of each batch; X B = { x j } m j =1 is the set of data item batches, with their corresponding true labels Y B = { y j } m j =1 and worker anno-Remarks. 1) Notice that a data item x i  X  X may certainly ap-pear in multiple batches in A . That is, x jl and x to the same data item as long as b jl = b j 0 l 0 ; 2) For the sake of fully utilizing the workforce of crowds, without loss of generality, we focus on the scenario when all batches have the identical size k . However, our model generalizes to the case when batches have different sizes; 3) In some real world crowdsourcing platforms, a batch can actually be judged by multiple workers, which means there could be multiple y 0 j  X  X  associated to a single ( b for instance, this is referred to as multiple assignments on Mechani-cal Turk. However, for the purposes of debiasing, it is equivalent to regard a single batch as multiple batches with identical ( b but associated with judgments made by different workers y
Based on the concepts described thus far, we can formalize the problem of debiasing crowdsourced batches as the following: pose we have a labeled data set ( X L ,Y L ) with Y L known, as well as its extracted batches and their crowdsourced annotation ( B
L ,X B L ,Y B L ,Y 0 B L ) . If we are then given another unlabeled data set X U , as well as its extracted batches and crowdsourced an-notation ( B U ,X B U ,Y 0 B U ) , the objective is to infer the true labels Y U associated with X U from the crowdsourced annotation.
 Notice that our problem formulation as described above requires as input labeled and annotated data items for training purposes. In practice, the labeled data for training can be collected from the  X  X est questions X  with ground-truth labels, inserted by the crowdsourcing platform for the purpose of quality control and monitoring of work-ers. The usage of test questions is standard practice: As an exam-ple, in CrowdFlower, all workers have to attempt a certain number Notation Description b j A set of data item indices { b jl } k l =1 x j A data item batch { x jl } y j A label batch consists of true labels { y jl } y 0 j Worker annotation collected from a crowdsourcing X B Set of all the data item batches
Y B Set of all the true labels associated with data item
Y 0 B Set of all the worker annotation from crowds on data of test questions with correct labels and need to achieve an accu-racy over a certain threshold ( e.g. 70%) before they can proceed to work on the regular task(s). Also, additional hidden data items with known labels can be inserted into the regular tasks to monitor their accuracy. In our setting, worker behavior on these test questions or labeled data can additionally be used for training purposes.
Also notice that in this version of our problem formulation, we assume identical worker behavior. This is a more standard setting in crowdsourcing practice as there is usually not enough work done by each worker to ascertain individual behavior. Also, it is straight-forward to extend our model when different workers have different behavior when working on tasks.
In this section, we first describe our model for workers X  anno-tation behavior on a batch of data items; then we introduce how to train the model based on a training data set. The basic idea is, workers might be biased when they make annotations by ranking data items in the same batch.
 Plackett-Luce model. Before we delve into our model, we first re-cap a probability model for generating rankings based on scores as-sociated with items, namely the classical Plackett-Luce model [15, 21] introduced in the 70s. Without loss of generality, suppose we are given a set of items x 1 ,...,x k . Each item x with a certain score s ( x i ) &gt; 0 . Here the score s ( x the tendency of ranking x i higher in a randomly generated rank-ing and can be viewed as a measure of the inherent  X  X oodness X  of the item. A ranking of these items can be represented as a bijec-i -th position in the ranking. The corresponding ranking list can be represented as  X  (1)  X  X  X   X  ( k ) . In Plackett-Luce model, the probability of generating a ranking  X  is: The equation above can be interpreted as the following process: Initially, we have a pool A of all the data items. Each time one picks an item x i from a pool A of data items with a probability proportional to its score, namely: This item is then removed from the pool A and placed at the next position in the ranking. Repeat this operation until A becomes empty. The probability of generating a ranking list according to this process is equivalent to the probability described in the Plackett-Luce model.
 Worker model. We now introduce our worker model for an-notating batches of data items. Again, without loss of generality, suppose we are given a batch x j where x jl = x l , namely the given data item batch can be denoted as x j = { x 1 ,...,x k } . Also, recall that for each data item x i , we denote P ( y i = 1 | x i score  X  i , which is not explicitly known.

When a worker starts to work on a certain batch of data items, they may choose to use one of two strategies: To combine these two different scenarios, we suppose the worker chooses to make independent judgments with a certain probability 0 &lt;  X  &lt; 1 , while with probability (1  X   X  ) the worker makes relative judgments.

The intuition of this model is to capture two behavior patterns of workers. In the independent judging scenario, workers can re-main independent in judging different data items in the same batch, with each data item being judged based on its inherent score  X  Nevertheless, sometimes workers might judge data items within a batch by comparison. In the relative judging scenario, workers sim-ply judge the relative relationships between data items in the same batch, which is captured by the Plackett-Luce model for generating the ranking. In order to determine the labels of data items, they have an expectation of label distribution, which is reflected by the distribution of generating  X  , as it characterizes the probability of having  X  positives within k data items. For instance, if workers expect there to be few positive items, then the probability of  X  be-ing low is high, while if workers expect the batches to be balanced, then the probability of  X  being close to k/ 2 is high comparing to other values of  X  . However, this distribution does not necessarily reflect the correct label distribution. When they try to apply their expectation of the label distribution on the batch, bias might occur.
We summarize the process of generating annotation for a batch of data items in our proposed model as below: 1. Toss a coin Z  X  Bernoulli (  X  ) .
 2. For each x i , generate y 0 i  X  Bernoulli (  X  i ) .
 3. Generate a ranking  X  based on Plackett-Luce model for data 4. Draw  X   X  Mult ( p  X  ) . 5. For the top- X  items in ranking  X  , generate y 0 i = 1 ; Model learning. The parameters that need to be determined in this worker model include: the probability of making independent judgments  X  , and the distribution of the number of positive anno-tation when making relative judgments, represented by p 0 where 0  X  p  X   X  1 and P p  X  = 1 . We assume these parameters are fixed for each new application of our techniques. However, for different applications, these parameters might be different  X  for instance, these parameters for content moderation may be different from the same parameters for sentiment analysis.
 Suppose we are given a set of n L items X L with their true labels Y , or more ideally, their inherent scores {  X  i } x i  X  X L ent score of a data item  X  i is not given, but only the binary label y is known, we can define  X  i = ( y i + ) / (1 + 2 ) where is a small constant, which is set to 10  X  3 in our experiments. Then, we form them into m L batches represented by B L , send them to the crowds, and obtain their annotation from workers, denoted as Y 0 B
For each batch b j  X  B L , we denote the set of items annotated annotated as negative as X 0 j = { x jt | y 0 jt = 0 } .
We train the model by maximum likelihood estimation. The like-lihood of the obtained annotation can be written as: L = where  X  j = | X 1 j | is the number of positive annotation in batch b P ( X 1 j X 0 j ) denotes the probability of generating any rankings  X  that rank items in X 1 j higher than any items in X 0 j where R ( X 1 ,X 0 ) = {  X  |  X   X  1 ( x 0 ) &gt;  X   X  1 ( x X } ; and P (  X  ) is defined by the Plackett-Luce model, as presented in (1). Notice that the calculation of the exact value of P ( X X j ) is hard when k is large. In our experiments, k is small enough to enumerate entire set R ( X 1 j ,X 0 j ) . If k is large, we can apply Monte Carlo method to estimate the value of P ( X 1 j X 0
Applying an EM-algorithm, where at E-step, we can have  X   X 
And at M-step, we update the parameters  X   X  and  X  p  X  by where  X  Z = P m L j =1 (1  X   X   X  j ) .
In this section, we introduce our method that debiases annota-tions collected for batches of data given the trained worker model. More precisely, given a set of n U unlabeled data items X bled into m U batches represented by B U , as well as their annota-tions obtained from the crowds Y 0 B U , how do we infer their true labels Y U ?
The basic idea is, based on the given worker model, we infer  X  for each x i  X  X U . Then, we simply apply the Bayes classifier to determine the inferred label, which yields  X  y i = 1 if  X   X  y = 0 if  X  i  X  0 . 5 .

We again adopt a maximum likelihood estimation techique. The log-likelihood of the obtained annotation is: Notice that  X   X  and  X  p  X  j are parameters learned from Section 3, and P ( X 1 j X 0 j ) is also a function of  X  i  X  X . Similar to the previous section, we apply an EM-algorithm here by first calculating each batch at the E-step according to (3) but replacing  X  and p the value we learned during the training step. Then we have: log L (  X  )  X  where the second term includes log P ( X 1 j X 0 j ) , which is hard to optimize. We apply the idea of the EM-algorithm again here. We use notation R j to represent R ( X 1 j ,X 0 j ) . For each  X   X  R can calculate its conditional probability given X 1 j X 0 as  X  q  X  by: which is the E-step. According to Jensen X  X  inequality we have: where the last inequality yields the objective function we want to optimize. The correctness of EM-algorithm guarantees the conver-gence of optimizing this function.

Furthermore, according to the minorization-maximization (MM) algorithm used in [11], we obtain the lower bound for log P (  X  ) , which is defined by the Plackett-Luce model, by: where  X   X  i is the estimated parameter of last iteration.
By combining (6), (8) and (9), we obtain the objective function to optimize as:
Q (  X  ) = Notice that Q (  X  ) is actually a lower-bound of the original log-likelihood function (5). Moreover, for two EM-step and one MM-step we apply in deriving Q (  X  ) , it is proven that by improving Q (  X  ) from this iteration Q ( X   X  ) , the improvement of the log-likelihood is no less than the improvement we achieve on Q (  X  ) . Therefore opti-mizing Q (  X  ) can also optimize the log-likelihood.

Take the derivative, we obtain where M 1 ( i ) and M 0 ( i ) are defined as M y ( i ) = { j : x for y  X  { 0 , 1 } . The updating rule can be obtained by solving  X  X  (  X  ) / X  X  i = 0 , namely where
By iteratively updating the scores to optimize the likelihood of the annotation on test data, we can obtain the inferred  X   X  item. Based on this, we can determine the inferred binary label for each data item by assigning y 0 i = 1 if  X   X  i &gt; 0 . 5 , or y otherwise. Notice that we do not further tune the threshold in this step, as the scores we learned here are expected to be a reason-able estimate of the true  X  i  X  X . Therefore, if the inherent scores are known, learning theory guarantees us that by using Bayes classifier (namely to take 0.5 as threshold) is supposed to achieve the best expected performance in terms of square loss.

The entire process of training model and leveraging the model to debias the obtained annotations are summarized in Algorithm 1.
In this section, we conduct experiments on a synthetic data set and a real data set to verify the effectiveness of our proposed worker model and debiasing technique.
We first introduce the data sets we used in this experiments. A summary is provided in Table 2.
 Synthetic data set. We construct synthetic data sets following the worker annotation model we propose in Section 3. Suppose we have n items in X , we first generate their inherent scores  X 
Algorithm 1 : Debiasing crowdsourced annotation on batches of data items. x  X  X from a Beta distribution Beta (  X , X  ) , then generate the true labels Y by drawing y i from a Bernoulli distribution parameterized by  X  i for each i . In our synthetic data set, we set  X  = 2 and  X  = 4 to simulate the case when negative data items overwhelm positive data items.

Then, we generate m batches of size k by sampling without re-placement for each batch. Notice that by the phrase  X  X ithout re-placement X  we mean there are no identical data items within the same batch, while the same item can still appear in multiple batches as we do replace the items back into the pool after a batch is gener-ated. Thereby we obtain the set of batches B . For each b generate the workers X  annotation y 0 j from our proposed batch an-notation model. The probability of making independent judgments  X  is set as 0 . 5 . The distribution of determining number of positive annotations p  X  is also assigned to be: where  X  is positive constant, set as 2 in our experiments. Comments data set. We utilize a real world crowdsourcing data set for annotating comments, which is used in [36]. The origi-nal crowdsourcing task was to identify inappropriate comments on LinkedIn posts published by companies or LinkedIn influencers. Inappropriate comments are defined as comments containing pro-motional, profane, blatant soliciting, random greeting comments, as well as comments with only web links and contact informa-tion. In order to collect annotation of comments, for each post, k comments are sampled and sent to CrowdFlower as a batch (unit). Workers are also provided with a codebook (i.e., a sequence of in-structions) explaining how to annotate the data items. Each com-ment is regarded as a data item and can be annotated as positive (inappropriate comment) or negative (acceptable comment). Each batch is annotated by 5 or more workers.

In order to provide test questions and track the performance of each worker, some of the batches are annotated by 9 trained LinkedIn employees (experts) with the same codebook and inter-face as used for crowd workers. The average Cohen X  X  kappa for all expert pairs is 0.7881. For this experiments, we only adopt the batches with all of their data items annotated by both crowds and experts as we can use the experts X  annotation as ground truth (aggregated by majority voting). Out of these batches, the 1,099 batches that are annotated before a worker actually starts on the job are utilized as training data set B L . while the other 5,267 batches are utilized as the test data set B U to infer the 651 data items the 5,267 batches covered. Methods evaluated. We compare the performance of our pro-posed method with several baselines: Evaluation methodology. For baselines without training, we di-rectly apply them on the test data set; for our proposed method as well as MVT, we first train the worker model on the training data set, then apply the debiasing strategy based on the trained worker model on the test data set. We compare the inferred labels to the ground-truth and evaluate the performance in terms of accuracy, precision, recall and F 1 -score.
 Trials and setup. For our proposed model, in training phase, we randomly initialize  X  and p  X  ; in debiasing phase, we initialize the all the inferred scores as 0 . 1 . For training the worker model, we set a fixed number of iteration as 100. Our experimental re-sults presented later show the model converges within a number of iterations much fewer than 100. For debiasing, we calculate the log-likelihood of the model and stop when the relative change of log-likelihood is within 10  X  5 .
Now we present the experimental results. We first verify the learning algorithm of our model on the synthetic data set, then present the learned model parameters on a real data set; we also evaluate the effectiveness of our debiasing strategy on both syn-thetic data set and real data set, which demonstrates an improve-ment in terms of F 1 -score; finally we conduct a study on differ-ent configurations of experiments as a guideline for setting up a batched crowdsourcing task. (a) Parameter comparison of p Figure 2: Learning worker model from the synthetic training data set. Figure 3: Analysis of estimation error of parameters in the worker model under different configurations.
 Worker model learning. We first verify the effectiveness of learn-ing our proposed worker model. On our synthetic data set, the  X  X rue X  value of probability of making independent judgments  X  is set to 0.5. We learn the model from the synthetic training data and obtain the inferred  X   X  as 0.4998, which reasonably recovers the orig-inal value. We also compare the original model parameters p the inferred parameters in Figure 2(a). The black dashed line repre-sents the original parameters used for generating synthetic annota-tion data, while the red solid line shows the inferred parameters of worker model, which seems as a precise fit of the original param-eter. We also show the curve of log-likelihood of the training data set, which seems to converge within 20 iterations.

To further confirm the robustness of our learning method, we modify the configuration of synthetic data generation, and train the worker model on different data sets to check if they can recover the original parameters. We still take the same configuration of n
L = 1 , 000 and m L = 10 , 000 . The estimation error analysis is shown in Figure 3. Figure 3(a) shows the difference between the inferred parameter  X   X  and the  X  X rue X  parameter  X  , given the annota-tion data generated by  X  varying from 0.1 to 0.9. It can be observed that the error is reasonable small, basically within 0.1. Figure 3(b) shows the ` 2 norm of the difference between the estimated distri-bution  X  p  X  and the  X  X rue X  distribution p  X  , when p  X  is generated with respect to different  X  varying from 1 to 3. In most of the settings, the error is below 2  X  10  X  3 , which is fairly low. Although we only instantiate p  X   X  X  using a power-law distribution, as the learning method does not confine the learned distribution to be parametric, it can be directly applied to any other type of distributions. Learned model on real data set. Given the effectiveness of our learning method verified, we apply the worker model trying to fit the data set of annotating inappropriate comments. The learned probability of a worker making independent judgments  X   X  on com-ments data set is 0.7877. The learned distribution for determin-ing the number of positive annotations in a batch is presented in Figure 4(a). It shows that a worker tends to annotate the entire batch as negative ( i.e. acceptable comment) with a probability over (a) Parameter analysis of p  X   X  X  Figure 4: Learning worker model from the comments training data set.
 Table 3: Performance comparison of Majority Voting (MV), Plackett-Luce Model (PL) and Batch Annotation Model (BAM). All results are shown as percents.
 0.6, while picking only 1 of them as positive ( i.e. inappropriate comment) also occurs with a relative high probability around 0.25. The workers tend to annotate no more than 1 comments in a size-5 batch. This is coherent with most people X  X  intuition that inappro-priate comments are rare comparing to the entire set of comments.
The convergence analysis is shown in Figure 4(b). The model converges within 50 iterations.
 Performance comparison. We proceed to evaluate the perfor-mance of different aggregation strategies on both data sets. The overall performance results are shown in Table 3. In both data sets, our proposed debiasing strategy is a clear winner in terms of F score, and also achieves the best accuracies.

In synthetic data set, majority voting, without tuning the thresh-old (default set to 0.5), fails to identify most of the positive data items, and therefore achieves an extremely low recall. Only after the threshold is tuned on a training data set can it achieve a reason-able F 1 -score of 71%. PL-model, in contrast, achieves a relatively low precision of 52%. Our proposed method is able to achieve the best overall performance in terms of F 1 -score and accuracy. Notice that we do not directly apply any threshold tuning for our method and simply takes the threshold as 0.5.

In comments data set, the na X ve majority voting strategy again obtains a poor recall below 80%. After tuning the threshold, its re-call rises to around 85%, but still lower than our proposed method. The scores learned by PL-model yield a comparable recall to major-ity voting with tuned threshold, but fail to achieve a high precision. Our proposed method achieves a comparable precision of 93% and a higher recall of 87%, and therefore beat all the other baselines in terms of F 1 -score (90%).
 Batch number m vs. item number n . An interesting ques-tion to study is, for a certain number of items, how many (random) batches of data items does one need to label to obtain an aggre-gated result accurate enough. We study this question by generating synthetic data sets with different settings of number of batches m and m U while number of data items n L and n U are fixed. In this (a) Accuracy with different m/n ratio Figure 5: Performance of debiasing strategies on synthetic data sets generated by setting both m L /n L and m U /n U as 2 , 5 , 10 , 20 re-spectively. (a) Accuracy with different sizes of training data set Figure 6: Performance of debiasing strategies on synthetic data sets generated by different size of training data set n L ( m L while the size of testing data set remains n U = 5 , 000 and m 50 , 000 . experiments, we set n L = 1 , 000 and n U = 5 , 000 , and gener-ate synthetic data sets with m L /n L = m U /n U = 2 , 5 , 10 , and 20 . We then apply all the strategies on these data sets. To min-imize randomness, for each setting we repeat the data generation and debiasing for 10 times, then report the average performance.
Results are shown in Figure 5. As we can observe, under all the different settings, the proposed method consistently outperforms other baselines, in terms of both accuracy and F 1 -score. Major-ity voting with tuned threshold (MVT) is able to achieve compa-rable results to our proposed method when m/n are large enough ( e.g. m/n = 20 ). However, when m/n is relatively small, our pro-posed method can achieve much better results than most of other baselines. When m/n = 2 , it achieves an accuracy approximately 9% higher than MVT, and an F 1 -score around 5% more than MVT. An exception is the na X ve majority voting strategy that achieves the best accuracy when m/n = 2 . This is due to the skewed distri-bution of data labels, and by simply labeling all the data items as negative can get an accuracy of approximately 80% . In compari-son, the F 1 -score of MV is only around 30% .

Another observation that we can make about Figure 5(b) is that the performance of majority voting drops as m/n increases. This result indicates when workers are biased and no debiasing tech-niques are applied, increasing the quantity of annotations collected does not help.
 Size of training data set. As our method requires a small set of training data, there might be some concerns about how large a training data set is sufficient. We test the performance of two methods that rely on training data sets  X  MVT and our proposed method  X  on synthetic data sets and the comments data set. For the synthetic data set, we keep the size of test data set as n and m U = 50 , 000 , and vary the size of training data set by setting n L as 10 , 20 , 50 , 100 , 200 , 500 , and 1 , 000 , while setting m (a) Accuracy with different sizes of training data set Figure 7: Performance of debiasing strategies on comments data set where the training data set is randomly sampled from the original remains the same. 10 n L . For each configuration, we generate synthetic data sets 10 times and utilize the average performance on these 10 data sets to evaluate the debiasing performance. For the comments data set, we randomly sample m L batches from the training data set, where m
L is set to 100 , 200 ,..., 1000 . Again, for each configuration of training data size, we repeat the random sampling for 10 times and report the average performance.

The results of synthetic data set are shown in Figure 6. As ob-served, when training data set is small ( e.g. n L = 10 ), the perfor-mance of MVT drops in terms of both accuracy and F 1 -score (73% and 56% respectively). As the size of training data set increases, the performance of MVT becomes comparable to our proposed method. In comparison, the performance of our proposed method is relatively stable even when there are only 10 items and 100 batches as training data. The results imply our proposed method can obtain very high performance with a small cost of labeling ground-truth data for collecting training data.
 The results for the comments data set are shown in Figure 7. Again, when training data size is extremely small ( e.g. m the performance of MVT drops substantially (89% in accuracy and 75% in F 1 -score), while its performance gets more and more com-parable to our method as the training data size increases. In con-trast, our proposed method maintains a fairly stable performance (96% in accuracy and 90% in F 1 -score) for different sizes of the training data set. This verifies again the ability of our proposed method to yield high-quality results with a sufficiently small train-ing data set.
In this section, we discuss several straightforward extensions of our proposed worker model and debiasing strategies, with respect to some useful applications other than binary classification: rating estimation and multi-class classification. We also discuss how to extend our model to the more general case when different workers have different biases.
 Rating estimation. In rating estimation, each data item x longer associated with a discrete label from a finite set of labels, but instead, a real value y i  X  R . Although we do not explicitly formalize our problem for a rating task, with some straightforward modifications, our techniques can still be applied if the workers are asked to rate data items in batches.

Without loss of generality, we can assume 0 &lt; y i &lt;  X  . If the actual rating can be negative, we can always apply a certain sig-moid function to normalize the scores to be positive values. For independent judging, we can design a distribution w.r.t. y where a worker draws a rating y 0 i for x i , e.g. a Gaussian distri-bution N ( y i , 1) . For relative judging, we can still assume that the worker generates a ranking from Plackett-Luce model with param-eters y i  X  X , and introduce another distributions w.r.t. the ranking of each data item  X   X  1 ( x i ) from where a worker generates the ratings. For example, y 0 i  X  N (  X   X  1 ( x i ) , 1) . Once the design of model is accomplished, it is straightforward to apply the same technique described in this paper to derive the debiasing strategy. Multi-class classification. In a multi-class classification problem, the label set Y may contain more than 2 possible labels. Workers are usually requested to assign data items with different labels. This is a natural extension from binary classification problem.
If the labels in Y are ordinal, for example, judging whether a review is  X  X ery helpful X ,  X  X elpful X  or  X  X ot helpful X , the problem reduces to a rating estimation problem, where the possible values of rating are discrete values. We can simply apply the extended strategy described above. If the labels in Y do not have an order, the problem can be reduced to several binary classification prob-lems, which is straightforward to apply our strategy for debiasing workers X  annotations.
 Personalized worker model. To address different behaviors of different workers, we may want to employ different worker model separately for different workers, instead of using an identical model for all the workers. This can be achieved by rewriting (5) as log L (  X  ) = where each w corresponds a worker and each worker annotates m batches.
In this section, we first introduce existing studies on annotation bias of crowds, when data items are presented either independently, or in a sequence or batches; we then introduce rank aggregation techniques and their application on crowdsourced ranking or rating. Annotation bias in independent judgments. A number of stud-ies have been conducted on verifying and quantifying annotation bias of crowd workers. Snow et al. [29] explore the performance of annotations by non-expert workers for several NLP tasks. De-meester et al. [8] discuss the disagreement between different users on assessment of web search results.
 There are also extensive studies on modeling worker behaviors. Raykar et al. [23, 24, 25] study how to learn a model with noisy labeling. Specifically, they employ a logistic regression classi-fier, and insert hidden variables indicating whether a worker tells the truth. Karger et al. [12] propose an iterative algorithm to in-fer workers X  reliability and aggregating their answers. Whitehill et al. [35] model the annotator ability, data item difficulty, and infer the true label from the crowds in a unified model. Most of these studies also propose various generative model to capture worker behavior. However, they assume judgments on different data items are independent, which is not necessarily true when data items are grouped into batches.

Venanzi et al. [33] propose a community-based label aggregation model to identify different types of workers, and correct their labels correspondingly. Das et al. [7] address the interactions of opinions between people connected by networks. They focus on another as-pect of dependencies, which is the dependencies between workers, while in our studies, we are more concerned about dependencies between data items and their judgments.
 Annotation bias in sequential and batch judgments. A few re-searchers also notice the correlation between judgments on differ-ent data items, but their work are mainly developed in the setting when data items are reviewed in a sequence. Scholer et al. [26, 27] study the annotation disagreements in a relevance assessment data set. They discover correlations between annotations of similar data items. They also explore  X  X hreshold priming X  in annotation, where the annotators tend to make similar judgments or apply sim-ilar standard on consecutive data items they review. However, their work focuses on the scenario when data items are organized in a long sequence. It confines the dependencies to exist only between consecutive data items. Also, they focus more on qualitative con-clusions, without a quantitative model to characterize and measure the discovered factors. Carterette et al. [4] provide several assessor models for the TREC data set. Mozer et al. [18] study the similar  X  X elativity of judgments X  phenomenon on sequential tasks instead of batches. Again, their focus is more on data items presented as a long sequence, while we focus more on data items presented in batches simultaneously.

Our recent work [36] also considers a similar setting when data items are organized in batches; we verify the existence of annota-tion bias caused by batching data items. Our focus in that paper was to design an active learning algorithm to smartly assemble batches, aiming to improve the performance of the classifier trained on their annotations. Our focus was not on improving the quality of labels collected, and we still used majority voting to obtain labels for data items. In this paper, we focus on debiasing the obtained annota-tions, which can trigger a broader range of application including both training and evaluating classifiers.
 Crowdsourced ranking and rating. In our model, we employ the Plackett-Luce model to capture worker behavior, and aggre-gate worker annotations on batches as rankings in order to infer true labels. There is a related thread of work on rank aggregation; however, to the best of our knowledge, we are the first to model crowds X  annotating behavior on batches by ranking, and propose a debiasing strategy.

Studies on aggregating multiple rankings into a consistent rank-ing can be dated back to the seminal work of Arrow [2]. Negah-ban et al. [19] study how to aggregate pairwise comparisons into a ranking by utilizing the Bradley-Terry model [3], which is a simpli-fied version of Plackett-Luce model utilized in this paper. Hunter et al. [11] propose the minorization-maximization (MM) algorithm to infer Plackett-Luce model from multiple partial orderings. Soufi-ani et al. [30] generalize Negahban et al.  X  X  work and propose a class of generalized method-of-moments (GMM) algorithm to infer pa-rameters of Plackett-Luce model from multiple orderings, and com-pare the performance against MM-algorithm. They then further ex-tend their algorithm to be applied to a more general class of rank-ing models called random utility models (RUMs) [31]. In addition, the technique for rank aggregation has also been studied in context of information retrieval [9, 13, 14, 22, 34]. These studies do not explicitly address the crowdsourcing settings to actually model the worker behavior. Directly applying their techniques ( e.g. [11]) may not lead to better performance, as shown in our experiments.
There is related research on aggregating multiple rankings or leveraging crowds X  power to obtain ranking of data items. Chen et al. [6] study aggregating crowdsourced annotation on pairwise comparison to obtain a ranking on data items. Mao et al. [16] show how aggregated results of noisy voting obtained from crowdsourc-ing platform may differ by using different aggregating strategies. However, their objective is just to obtain a ranking, while our model incorporates a ranking model but the ultimate goal is still to collect labels for data items. Several papers also consider crowdsourced rating.
 Parameswaran et al. [20] focused on crowdsourced rating on items, and applied their system on a peer evaluation data set of a MOOC course. Crowdsourcing has also been utilized for rating multimedia content quality [5] and relevance assessment [1]. However, they do not explicitly study the scenario when data items are grouped into batches.
In this work we study a specific type of annotation bias in crowd-sourcing, which occurs when data items are grouped into batches and submitted to workers to be judged simultaneously. We pro-pose a novel worker model designed to capture this type of bias, and show how to train the worker model on annotation data. We also present how to debias the label obtained from crowds given a trained worker model. We conduct experiments on both synthetic data and real world data, and observe that our proposed debiasing technique, with a fairly small training data set, can achieve up to +57% F 1 -score improvement in the synthetic data set compared to the na X ve majority voting strategy, and +17% improvement over a tuned majority voting strategy. On a real data set, our technique can also achieve up to +15% improvement on F 1 -score over the tuned majority voting strategy.

The observation of batch annotation bias might exist in many scenarios other than crowdsourcing, and therefore the debiasing strategy can trigger a broad range of applications. For example, the conference paper review system where each reviewer is assigned a batch of papers can also be regarded as a batch annotation.
There are several interesting directions to extend this work. For example, one can extend the model to further incorporate workers X  different demographical attributes and adjust the debiasing strategy accordingly. Also, it would be interesting to see if it is possible to improve the efficiency of debiasing by actively assemble a batch of data items to collect the desired labels, instead of sending randomly formed batches to the crowds.

