 REGULAR PAPER Ji Zhang  X  Hai Wang Abstract In this paper, we identify a new task for studying the outlying de-gree (OD) of high-dimensional data, i.e. finding the subspaces (subsets of fea-tures) in which the given points are outliers, which are called their outlying sub-spaces. Since the state-of-the-art outlier detection techniques fail to handle this new problem, we propose a novel detection algorithm, called High-Dimension Outlying subspace Detection (HighDOD), to detect the outlying subspaces of high-dimensional data efficiently. The intuitive idea of HighDOD is that we mea-sure the OD of the point using the sum of distances between this point and its k nearest neighbors. Two heuristic pruning strategies are proposed to realize fast pruning in the subspace search and an efficient dynamic subspace search method with a sample-based learning process has been implemented. Experimental results show that HighDOD is efficient and outperforms other searching alternatives such as the naive top X  X own, bottom X  X p and random search methods, and the existing outlier detection methods cannot fulfill this new task effectively.
 Keywords Outlying subspace  X  High-dimensional data  X  Outlier detection  X  Dynamic subspace search 1 Introduction Outlier detection is a classic problem in data mining that enjoys a wide range of applications such as the detection of credit card frauds, criminal activities, and exceptional patterns in databases. Outlier detection problem can be formulated as follows: Given a set of data points or objects, find a specific number of objects that are considerably dissimilar, exceptional, and inconsistent with respect to the remaining data [ 9 ].
 the outlier detection problem defined earlier. They can broadly be divided into distance-based methods [ 13 , 14 , 19 ] and local density-based methods [ 7 , 12 , 17 ]. However, many of these outlier detection algorithms are unable to deal with high-dimensional data sets efficiently as many of them only consider outliers in the entire space. This implies that they will miss out the important information about the subspaces in which these outliers exist.
 search method [ 3 ] where outliers are detected by searching for sparse subspaces. Points in these sparse subspaces are assumed to be the outliers. While knowing which data points are the outliers can be useful, in many applications, it is more important to identify the subspaces in which a given point is an outlier, which motivates the proposal of a new technique in this paper to handle this new task. consider the example in Fig. 1, in which 3 two-dimensional views of the high-dimensional data are presented. Note that point p exhibits different ODs in these three views. In the leftmost view, p is clearly an outlier. However, this is not so in the other two views. Finding the correct subspaces so that outliers can be detected is informative and useful in many practical applications. For example, in the case of designing a training program for an athlete, it is critical to identify the specific subspace(s) in which an athlete deviates from his or her teammates in the daily training performances. Knowing the specific weakness (subspace) allows a more targeted training program to be designed. In a medical system, it is useful for the Doctors to identify from voluminous medical data the subspaces in which a par-ticular patient is found abnormal and therefore a corresponding medical treatment can be provided in a timely manner.
 search algorithm, called High-Dimension Outlying subspace Detection (High-DOD), that utilizes a sample-based learning process to efficiently identify the sub-spaces in which a given point is an outlier . Note that, instead of detecting outliers in specific subspaces, our method searches from the space lattice for the associated subspaces whereby the given data points exhibit abnormal deviations. To our best knowledge, this is the first such work in the literature so far. The main features of HighDOD include the following: 1. The outlying measure, OD, is based on the sum of distances between a data 2. Two heuristic pruning strategies are proposed to aid in the search for outlying 3. A fast dynamic subspace search algorithm with a sample-based learning pro-4. The heuristic on the minimum sample size based on the hypothesis testing related work in the classic outlier detection problem. Section 3 discusses the basic notions and problem to be solved. In Sect. 4, we present our outlying subspace detection technique, called HighDOD, for high-dimensional data. Section 5 dis-cusses the detailed algorithms of HighDOD. Experimental results are reported in Sect. 6. Section 7 concludes this paper. 2 Related work Detecting outlying subspaces is a new task in the OD analysis for high-dimensional data, which has rarely been treated so far. However, we have wit-detecting outliers in a given space or subspace. In this section, we will present on the techniques used, i.e. distribution-based methods, distance-based methods, density-based methods, and clustering-based methods.
 dancy/outlier tests have been developed for different circumstances, depending on the parameter of data set (such as the assumed data distribution) and parameter of distribution (such as mean and variance), and the expected number of outliers [ 9 , 13 ]. However, distribution-based methods suffer from some key drawbacks. First, they cannot be applied in a multi-dimensional scenario because they are univariate in nature. In addition, the lack of any prior knowledge regarding the un-derlying distribution of the data set makes the distribution-based methods difficult to use in practical applications. Finally, the quality of results cannot be guaranteed because they are largely dependent on the distribution chosen to fit the data. dmin )-Outlier, which defines an object in a data set as a DB( pct, dmin )-Outlier if at least pct % of the objects in the data sets have the distance larger than dmin from this object. The notion of distance-based outlier is extended and the distance to the k th nearest neighbors of a point p , denoted as Dk ( p ) , is proposed to rank the point so that outliers can be more efficiently discovered and ranked [ 19 ]. The Dk ( p ) -outlier is further extended by considering for each point the sum of its k nearest neighbors [ 4 ]. Unlike distribution-based methods, distance-based methods do not rely on any assumed distribution to fit the data.
 This formulation ranks the OD of the points using Local Outlier Factor (LOF) . LOF of an object intuitively reflects the density contrast between its density and those of its neighborhood. Note that LOF ranks points by only considering the neighborhood density of the points, thus it may miss the potential outliers whose densities are close to those of their neighbors. The efficiency of this algorithm by proposing an efficient micro-cluster-based local outlier mining algorithm [ 12 ], but it still use LOF to mine outliers in data set.
 there are numerous studies on clustering, and a number of them are equipped with some mechanisms to detect outliers, such as CLARANS [ 16 ], DBSCAN [ 8 ], BIRCH [ 23 ], WaveCluster [ 22 ]. Clustering techniques tailored to subspace clustering for high-dimensional data includes DenClue [ 11 ], CLIQUE [ 1 ]andthe technique proposed in [ 21 ]. Strictly speaking, clustering algorithms should not be considered as outlier detection methods because their objective is only to group the objects in data set such that clustering functions can be optimized. The aim to eliminate outliers in data set using clustering is only to dampen their adverse effect on the final clustering result.
 applicable to high-dimensional data. Ref. [ 3 ] is the first work in high-dimensional method by first finding all the lower-dimensional projections that are locally sparse. The sparsity of a k -dimensional projection is measured by the so-called Sparse Coefficient of the corresponding k -dimensional cube. However, this Sparse Coefficient is not characterized by any upward or downward closure in the set of dimensions; hence, no pruning mechanism can be performed to find the sparse subspaces quickly.
 Remark 1 All the existing outlier detection algorithms, regardless of in low-or high-dimensional scenario, invariably fall into the framework of detecting outliers in a specific data space, either in full space or subspace. We term these methods as  X  space  X  outliers  X  techniques. For instance, outliers are detected by first find-ing locally sparse subspaces [ 3 ], and the so-called Strongest/Weak Outliers are discovered by first finding the Strongest Outlying Spaces [ 14 ]. Our technique is novel in that it approaches the outlying analysis from a different prospective: find-ing the associated subspaces for each point in which this point is regarded as an outlier, which we can call it an  X  outlier  X  spaces  X  technique. 3 Outlying degree measure and problem formulation Before we formally discuss our outlying subspace detection technique, we start with introduction of the OD measure that will be used in this paper and formula-tion of the new problem of outlying subspace detection we identify. 3.1 Outlying degree (OD) For each point, we define the degree to which the point differs from the majority of the other points in the same space, termed the outlying degree (OD in short). OD is defined as the sum of the distances between a point and its k nearest neighbors in a data space [ 4 ]. Mathematically speaking, the OD of a point p in space s is computed as: where KNNSet ( p , s ) denotes the set composed by the k nearest neighbors of p in s . Note that the OD measure is applicable to both numeric and nomi-nal data: for numeric data we use Euclidean distance, while for nominal data we use the simple match method. Mathematically, the Euclidean distance be-tween two numeric points p 1 and p 2 is defined as Dist ( p 1 , p 2 ) =[ (( p 1 i  X  p mum data value of the i th dimension. The simple match method measures the dis-of attributes. 3.2 Problem formulation We now formulate the new problem of outlying subspace detection for high-dimensional data as follows: given a data point or object, find the subspaces in spect to the remaining points or objects. These points under study are called query points , which are usually the data that users are interested in or concerned with. ates significantly from its neighboring points. We call a subspace s is an outlying subspace of data point p if OD s ( p )  X  T . Note that the distance threshold T can either be a uniform threshold for all the subspaces or a different one for each of the subspaces. We call the former as the global distance threshold and the later as the local distance threshold. 3.3 Applicability of existing high-dimensional outlier detection techniques given subspaces, are theoretically applicable to solve the new problem identified in this paper. To do this, we have to detect outliers in all subspaces and a searching in all these subspaces is needed to find the set of outlying subspaces of p ,which are those subspaces in which p is in their respective set of outliers. Obviously, the computational and space costs are both in an exponential order of d ,where d is the number of dimensions of the data point. Such an exhaustive space searching is rather expensive in high-dimensional scenario. In addition, they usually only return the top-k outliers in a given subspace; thus, it is impossible to check whether or not p is an outlier in this subspace w.r.t the given distance threshold if p is not in this top-k list. This analysis provides an insight into the inherent difficulty of using the existing high-dimensional outlier detection techniques to solve the new outlying subspace detection problem. 4HighDOD In this section, we present an overview of our HighDOD method (shown in Fig. 2). It mainly consists of four modules. The X-tree Indexing module performs X-tree [ 6 ] indexing of the high-dimensional data set to facilitate k NN search in every subspace. Sample-based Learning module randomly samples the data set and per-forms dynamic subspace search to estimate the downward and upward pruning probabilities of subspaces from 1 to d dimensions. Outlying Subspace Detection module uses the probabilities obtained in the Learning module to carry out a dy-namic subspace search to find the outlying subspaces of the given query data point. 4.1 X-tree indexing We utilize X-tree (eXtended node tree) [6] to index the high-dimensional data set in order to speedup the k NN queries in different subspaces (see Fig. 3). X-tree is an index structure that is designed to support efficient query processing of high-dimensional data. The introduction of X-tree is motivated by the problem with the R-tree-based index structures that the overlap of bounding boxes in the directory will increase as the dimension grows. The basic idea of X-tree is to use overlap-minimizing split and supernodes to keep the directory as hierarchical as possible and at the same time to avoid splits that will result in high overlap. It is a hybrid of linear array-like and R-tree like directory. The hierarchical organization is good for low dimensions, while in high dimensions, a linear organization is more efficient. For medium number of dimensionality, a dynamic organization of the tree such that the data producing high overlap are organized in a linear format, while those data can be organized hierarchically without causing too much overlap are organized in a hierarchy. X-tree has been shown experimentally outperform the well-known R*-tree and TV-tree for high-dimensional data by up to two orders of magnitude. 4.2 Subspace pruning To find the outlying subspaces of a query point, we make use of the heuristics we devise to quickly detect the subspaces in which the point is not an outlier or the subspaces in which the point is an outlier. All these subspaces can be removed from further consideration in the later stage of the search process.
 searching process, i.e. Global-T pruning and Local-T pruning . In the Global-T pruning, a global distance threshold T is used for delimiting outlying and non-outlying subspaces in the space lattice for a query data point. While in the Local-T pruning, different local distance thresholds are used for different subspaces re-quired to evaluate in the searching process. 4.2.1 Global-T pruning strategy OD maintains two interesting monotonic properties that allow the design of an ef-ficient outlying subspace search algorithm in Global-T pruning. These two prop-erties are based on the fact that the OD value of a point in a subspace cannot be less than that in its subset spaces. Mathematically, we have OD s 1 ( p )  X  OD s 2 ( p ) if s 1  X  s 2 . We first present the proof of this proposition as follows. Proposition 1 OD s 1 ( p )  X  OD s 2 ( p ) if s 1  X  s 2 .
 Proof Let a k and b k be the k th nearest neighbors of p in the an m -dimensional subspace s 1 and n -dimensional subspaces s 2 , respectively (1  X  n  X  m  X  d and s  X  s in the subspace s 2 .  X  Max Dist therefore conclude: OD s 1 ( p )  X  OD s 2 ( p ) .
 erties given later.
 Property 1 If a point p is not an outlier in a subspace s , then it cannot be an outlier in any subspace that is a subset of s .
 Property 2 If a point p is an outlier in a subspace s , then it will be an outlier in any subspace that is a superset of s .
 to quickly prune away those subspaces in which the point cannot be an outlier. distance threshold. In the upward Global-T pruning strategy, Property 2 of OD is utilized to detect those subspaces in which the point is definitely an outlier. The reason is that if OD s 2 ( p )  X  T ,then OD s 1 ( p )  X  T .
 where OD e i denotes the averaged OD value of points in the one-dimensional subspace e i and C is a constant factor ( C &gt; 1). This specification stipulates that, in any subspace, only those points whose OD values are significantly larger than the average level in the full space are regarded as outliers. The average OD level in the full space is approximated by d i = 1 OD 2 e is specified by the constant factor C , normally we set C = 2or3. 4.2.2 Local-T pruning strategy Let T s be the local distance threshold for subspace s ,and s 1 and s 2 be two subspaces that satisfy s 1  X  s 2 . In the downward Local-T pruning strategy, is satisfied: pruned only when the following condition is satisfied: where C is the same constant factor used in the Global-T pruning. Being more conservative than the Global-T pruning, the two requirements mentioned earlier intuitively mean that the subspace s which is a superset/subset of the current subspace s can be pruned for data point p only when the OD ( p ) value in each of the one-dimensional subspaces that belong to s or the difference of the two subspaces (i.e. s  X  s ) is significantly smaller or larger than the corresponding average OD level.
 T , the distance threshold for a subspace s , as follows: The part of OD 2 e space s .
 Remark 2 By comparing the previous two pruning strategies, we can learn their pros and cons. Since the Global-T pruning strategy performs subspace pruning in a more aggressive way and may be able to prune a larger number of subspaces in each step than the Local-T pruning strategy, therefore it is faster to execute and more scalable to high-dimensional data set. However, the Global-T pruning strategy is limited in the situation that the average OD values of the points in dif-ferent subspaces differ significantly, which makes it inaccurate to use a uniform distance threshold for all the subspaces. Adopting different distance thresholds for different subspaces is, therefore, more advantageous by taking into account the varied denseness/sparsity of points in different subspaces, which enables the Local-T pruning strategy to detect outlying subspaces with a higher level of accu-racy. Still, it is comparatively slow and less scalable to high-dimensional data set. The selection between these two pruning strategies involves a tradeoff between speed/scalability and accuracy. 4.2.3 Saving factors of subspaces pruning Now, we will compute the savings obtained by applying the pruning strategies during the search process quantitatively. Before that, let us first give three defini-tions.
 Definition 1 Downward Saving Factor (DSF) of a Subspace.
 savings obtained by pruning all the subspaces that are subsets of s .Inotherwords, the Downward Saving Factor of s , denoted as DSF( s ), is computed as DSF ( s ) = out of a total of m items.
 Definition 2 Upward Saving Factor (USF) of a Subspace.
 USF( s ), is defined as the savings obtained by pruning all the subspaces that are supersets of s . It is computed as USF ( s ) = d  X  m i = 1 [ C i d  X  m  X  ( m + i ) ] . Definition 3 Total Saving Factor (TSF) of a Subspace.
 point p , denoted as TSF( m, p ), is defined as the combined savings obtained by applying the two pruning strategies during the search process. It is computed as follows: where we have the following: (1) f down ( m ) and f up ( m ) are the percentages of the remaining subspaces to be (2) pr up ( m , p ) and pr down ( m , p ) are the probabilities that upward and downward 4.3 Sampling-based learning We adopt a sample-based learning process to obtain some knowledge about the data set before subspace search of the query points are performed. This is desirable when the data set is large so that learning the whole data set becomes prohibitive. The task of performing this sampling-based learning is twofold: first, we will have to estimate OD s i which will be used in specifying the global and local distance thresholds and the requirements in the Local-T pruning. Secondly, we will have a small number of points are randomly sampled from the data set.
 spaces s i on all the sampling data and OD s i is computed as the average OD values of all sampling points in subspace s i ,i.e.
 where S is the number of sampling points and sp j denotes the i th sampling point. on the sampling data. For each sampling point sp , we have the following initial specifications regarding the two priors pr up ( m , p ) and pr down ( m , p ) : downward pruning in the subspaces of any dimension, except 1 and d , for each sampling point at the beginning. After all the m -dimensional subspaces have been ages of m -dimensional subspaces s in which OD s ( sp )  X  T and OD s ( sp )&lt; T , respectively. The average pr up and pr down values of subspaces from 1 to d dimen-sions can be obtained as follows: wherewehave pr down ( 1 ) = pr up ( d ) = 0.
 pr Remark 3 There might be a misunderstanding that the sampling technique will fail here because the outliers are rare in the data set. Recall that we are trying to detect outlying subspaces of query points, not outliers. Every point can become query point and every query point will have its outlying subspaces, if its set of outlying subspaces is not empty. Hence, the outlying subspaces can be regarded as a global property for all the points and a sample of sufficient size will make sense in the learning process. 4.4 Dynamic subspace search In HighDOD, we use a dynamic subspace search method to find the subspaces in which the sampling points and the query points are outliers. The basic idea of the dynamic subspace search method is to commence search on those subspaces with the same dimension that has the highest TSF value. As the search proceeds, the TSF of subspaces with different dimensions will be updated and the set of sub-spaces with the highest TSF values are selected for exploration in each subsequent step. The search process terminates when all the subspaces have been evaluated or pruned. Note that the only difference between the dynamic subspace search method used on the sample points and query points lies in the decision of values of pr up ( m , p ) and pr down ( m , p ) : For sample points, we assume an equal proba-bility of upward and downward pruning (referring to Section 4.3), while for query points we use the averaged probabilities obtained in the learning process . Example 1 Now, we give an example to illustrate how the TSF of a subspace is computed at each step of the search process using Global-T pruning strategy. Without loss of generality, we suppose, for a sample point sp ,the OD ( sp )values of the subspaces in the boxes are larger than T , while the OD ( sp ) values of the subspaces underlined are smaller than T , as shown in Fig. 4. We compute the TSF for subspaces with different dimensions (1 X 4) in each step of the search process (as shown in Table 1(a) X (c)). In each step, we select the dimension that has the maximum TSF value, which are highlighted in each of the tables. The order of subspaces in the search process is 4  X  1  X  3  X  2, meaning that the four-dimensional subspaces are searched first, followed by one-and three-dimensional subspaces. The two-dimensional subspaces are searched at last.
 4.5 Minimum sampling size for training data set Recall that the sampling method is utilized to obtain a training data set that can be used to pre-compute the prior probabilities of upward and downward pruning, namely pr up ( m ) and pr down ( m ) (1  X  m  X  d ). As such, samples of different sizes will only affect the pruning efficiency of the algorithm. They will not change the number of subspaces found.
 accurately predict pr up ( m ) and pr down ( m ) with certain degree of confidence. We denote X as the sample point that can be expressed as an S -dimensional vector as X =[ x of j th dimension of i th data in the sample. Applying dynamic subspace searching on sampling points, for each dimension m ,weobtain Y estimate the mean of pr down ( m ) . We estimate the sample size by constructing the confidence interval of the mean of pr down ( m ) . Specifically, to obtain a ( 1  X   X ) -confidence interval, the minimum size of a random sample is given as follows [ 15 ]: where  X  m denotes the estimated standard deviation of pr down in the m th dimension using the training points that is defined as:  X  denotes the half-width of the confidence interval.
 m  X  d ) , the minimum sample size S size requirement of each dimension is computed as: 5 Algorithms The algorithm of dynamic subspace search is presented in Fig. 5. In this algo-rithm, SetDim stores the number of dimension of the subspaces (from 1 to d ). A non-empty SetDim indicates that there are still some subspaces to be searched. The new TSF of the remaining dimensions are computed (calling function Com-puteDynaTSF() ) and an additional pass of the subspace search is performed. The dimension of the subspace with the maximum TSF in the current step is stored in the variable CurrDim (calling function MaxiDynaTSF() ). All the subspaces de-tected (calling function SubspaceSearch() ) are saved in SetDetectSS . The current dimension of subspaces is deleted from SetDim and the whole program terminates when SetDim becomes empty.
 the query points. The only difference lies in the function ComputeDynaTSF() . Here, the sampling points use the pre-defined probabilities as the priors, while the query points use the probabilities as the priors obtained in the sample-based learning process. The function of ComputeDynaTSF() dynamically computes the TSF values in the subspace searching process.
 of the CurrDim dimension. It displays a generic skeleton of the function Subspace Search() for subspace searching using either the Global-T or the Local-T pruning strategy. Note that there are two differences between the exact implementation of the function for the two pruning strategies: (1) T represents the global distance threshold in the Global-T pruning strategy and the local distance threshold for each subspace in the Local-T pruning strategy; (2) In the Local-T pruning strat-egy, the algorithm has to check the satisfiability of the pruning requirements within the functions PruneExist SS ( superset ( ss )) and PruneExist SS ( subset ( ss )) before the pruning is performed, while the Global-T pruning strategy does not have to. 6 Experimental results In this section, we will carry out extensive experiments to test the efficiency of outlying subspace detection and the effectiveness of outlying subspace compres-sion in HighDOD. Synthetic data sets are generated using a high-dimensional data set generator and four real-life high-dimensional data sets from the UCI machine learning repository, which have been used in [ 3 ] for performance evaluation of their high-dimensional outlier detection technique, are also used.
 ples) ( N ) and dimensions ( d ) of the data set generated. We will further specify the number of the intervals ( N I ) and the maximum ( max ) and minimum ( min ) values for each dimension of the data set. The length for each interval will be ( max  X  min )/( N I ) . For each dimension, N / N I distinct instances are generated based on the Gaussian distribution in each of the N I intervals. (The projection of high-dimensional data within small intervals in each dimension can be reasonably assumed to fit Gaussian distribution [ 3 ].) In this way, we will be able to obtain N distinct instances for each dimension. The data set generated generally displays dense and sparse regions in the data space, which serves as an ideal test-bed for our experiments. Specifically, we specify N I = 10, min = 0, max = 100 for each dimension in our experimental setting.
 of main memory running on Windows 2000. In all the experiments, we set k ,the number of neighbors we are interested in, to 10. 6.1 Efficiency study Since the existing high-dimensional outlier detection techniques fail to handle the new outlying subspace detection problem, we thus choose to compare the effi-ciency of several subspace search methods, i.e. top X  X own, bottom X  X p, random and dynamic subspace search, instead.
 data using various searching strategies. The top X  X own search method only em-ploys a downward pruning strategy, while the bottom X  X p search method only uses an upward pruning strategy. The random search method, the  X  X eadless chicken X  replacement in each step. The dynamic search method, a hybrid of upward and downward search, computes the TSF of all subspaces of different dimensions and selects the best layer of subspaces for search. To evaluate the efficiency of the sample-based learning process, we run the dynamic search algorithm with and without incorporating the sample-based learning process. We implement the pre-vious five searching methods with each of the two pruning strategies, which results in a total of 10 searching methods. Note that the execution times shown in this sec-tion are the average time spent in processing each point in the learning and query process. 6.1.1 Effect of dimensionality First, we investigate the effect of dimensions on the average execution time of HighDOD (see Fig. 7). We can see that the execution time of all the 10 methods increase at an exponential rate since the number of subspaces increases exponen-tially as the number of dimension goes up, regardless of which searching and pruning strategy is utilized. On a closer examination, we see that (1) the execu-tion time of top X  X own and bottom X  X p search methods increase much faster than the dynamic search method; (2) when using the sample-based learning process, the dynamic search method performs better than the method without using the sample-based learning process; (3) the execution times of the methods using the Global-T pruning strategy increase much less slowly compared with the those methods using the Local-T pruning strategy. This is because the Local-T pruning strategy is more conservative than the Global-T pruning strategy and therefore fewer subspaces can be pruned in each step of the searching process.
 6.1.2 Effect of data set size from 100 k to 1000 k . Figure 8 shows that the average execution times using the 10 methods to process each query point are approximately linear with respect to the size of the data set. Similar to results of the first experiment, the dynamic search method with sample-based learning process and Global-T pruning strategy gives the best performance. 6.1.3 Effect of number of query points Next, we vary the number of query points N q . Figure 9 shows the results of the five searching method using the Local-T pruning strategy only. It is interesting to note that when N q is large, dynamic search method with sample-based learning process gives the best performance. However, when N q is small, it is better to use dynamic search without sample-based learning. The reason is because when the number of query points is small, the saving in computation by using the learning process is not sufficient to justify the cost of the learning process itself. 6.1.4 Effect of sample size We also investigate the effect of the number of sampling points, S ,usedinthe learning process. A large S gives a more accurate estimation of the possibilities of upward and downward pruning in subspaces, which in turn, helps to speedup the search process. However, a large S also implies an increase in the computa-tion during the learning process, which may increase the average time spent in the whole detection process. As shown in Fig. 10, the execution time is first decreased when the number of sampling points is small, this is because the prediction of pos-sibility is not accurate enough, which cannot greatly speedup the later searching process. When the sample size increases, the prediction of the possibilities are suf-ficiently accurate, therefore any larger size of sample will no longer contribute to the speedup of the search process, but only increase the execution time as a whole. The horizontal dot-line in Fig. 10 indicates the execution time when dynamic sub-space search without sample-based learning is employed. 6.1.5 Results on real-life data sets Finally like [ 3 ], we evaluate the practical relevance of HighDOD by running ex-periments on five real-life high-dimensional data sets in the UCL machine learning repository. The data sets range from 8 to 160 dimensions. Table 2 shows the results of the five search methods using the Local-T pruning strategy. It is obvious that dynamic search with sampling-based learning process works best in all the real-life data sets. Furthermore, using dynamic subspace search alone is faster than top X  X own bottom X  X p or random search methods by approximately 20%, while in-corporating sample-based learning process into dynamic subspace search further reduces the execution time by about 30%. 6.2 Effectiveness study In this part, we will show that the existing outlier detection techniques (we term them  X  space  X  outli er s  X  techniques) will break down when trying to perform the task of finding the subspaces in which given points are outliers. We will first present two definitions that will be used in the experiment evaluation in this part. Definition 4 Outlying Strength of points.
 centage of number of subspace in which p is a outlier against the total number of subspaces, i.e.
 Definition 5 Detecting ability of  X  space  X  outli er s  X  techniques.
 spaces in which given points are outliers, denoted as  X  , by computing the percent-age of the number of given points that appear in the top n subspaces obtained by the method against the total number of given points, i.e. ber of points (we select 50 in our experiment) for each different group of Outlying Strength ranging from 10 to 80%. (The information of Outlying Strength of points can be obtained using HighDOD in advance.) We then apply the evolutionary-based search method, the latest member of the  X  space  X  outli er s techniques, on all these selected points and compute the value of  X  based on points in each group. We adopt two ways for selecting the subspaces for study, i.e. randomly selecting n subspaces from the whole space lattice and selecting the top-k subspaces returned by the evolutionary-based search method. We vary the value of n , the number of subspace selected, from 20 to 100 in this experiment. The objective of this experi-ment is to test the effectiveness of the evolutionary-based search method in finding the subspaces of query points under different Outlying Strengths of given points and different number of subspaces returned by this method.
 the evolutionary-based search method performs poorly in finding the subspaces in which points are outliers. For overwhelming majority of the points, this method cannot return any subspaces in which these points are true outliers. Even when a point appears in the top n subspaces obtained by this method, the method can only find the subspaces within the n subspaces returned rather than the whole spec-trum of subspaces. Increasing the number of subspaces returned can theoretically improve the performance, but such increasing in the number of subspaces will invariably increase the computation exponentially that render it almost infeasible in practice. The reason that the evolutionary-based search method, also includ-ing other  X  space  X  outli er s  X  techniques, performs poorly in this experiment is because the subspaces that are spare are not necessarily, in most cases, the right subspaces in which the given points are outliers.
 7 Conclusions In this paper, we propose a novel algorithm, called HighDOD, to address the new problem of detecting outlying subspaces for high-dimensional data. In HighDOD, two heuristics for fast pruning in the subspace search and a dynamic subspace search method with a sample-based learning process are used. Experimental re-HighDOD, in comparison with other search alternatives and the existing outlier detection techniques. We believe that HighDOD is useful in revealing new and interesting knowledge in outlying analysis of high-dimensional data and can be potentially used in many practical applications.
 outli er s  X  techniques cannot effectively deal with the new task identified in this paper. Actually, by proposing the new task of outlying subspace detection and HighDOD, we are able to have more options when handing outliers in high-dimensional space: when we want to detect outliers in a certain subspace, we can use the evolutionary-based search method, while when we are interesting in getting insights on the subspaces in which points are outliers, our technique can come into play.
 heuristics introduced in this paper have been applied. To further lower the hardness of this problem and make HighDOD more efficient, future research efforts will be taken in the following two directions. First, we will study the compress scheme for compacting the voluminous resultant outlying subspaces detected to make them more space efficient. Second, we are interested in exploring the parallel computing architecture to render HighDOD workable in a parallel paradigm so as to achieve a higher level of efficiency.
 References
