 Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algo-rithm should be developed for sequential pattern mining so that mining can be adapted to incremental database up-dates. However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the gener-ation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan , for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan out-performs some previously proposed incremental algorithms as well as a non-incremental one with a wide margin. H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Performance, Experimentation incremental mining, buffering pattern, reverse pattern match-ing, shared projection  X  The work was supported in part by U.S. National Science Foundation NSF IIS-02-09199, University of Illinois, and Mi-crosoft Research. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the au-thors and do not necessarily reflect the views of the funding agencies.

Sequential pattern mining is an important and active re-search topic in data mining [1, 5, 4, 8, 13, 2], with broad applications, such as customer shopping transaction analy-sis, mining web logs, mining DNA sequences, etc.
There have been quite a few sequential pattern or closed sequential pattern mining algorithms proposed in the previ-ous work, such as [10, 8, 13, 2, 12, 11], that mine frequent subsequences from a large sequence database efficiently. These algorithms work in a one-time fashion : mine the entire database and obtain the set of results. However, in many applications, databases are updated incrementally. For ex-ample, customer shopping transaction database is growing daily due to the appending of newly purchased items for ex-isting customers for their subsequent purchases and/or in-sertion of new shopping sequences for new customers. Other examples include Weather sequences and patient treatment sequences which grow incrementally with time. The exist-ing sequential mining algorithms are not suitable for han-dling this situation because the result mined from the old database is no longer valid on the updated database, and it is intolerably inefficient to mine the updated databases from scratch.

There are two kinds of database updates in applications: (1) inserting new sequences (denoted as INSERT), and (2) appending new itemsets/items to the existing sequences (de-noted as APPEND). A real application may contain both.
It is easier to handle the first case: INSERT. An im-portant property of INSERT is that a frequent sequence in DB = DB  X   X  db must be frequent in either DB or  X  db (or both) . If a sequence is infrequent in both DB and  X  db , it cannot be frequent in DB , as shown in Figure 1. This property is similar to that of frequent patterns, which has been used in incremental frequent pattern mining [3, 9, 14]. Such incremental frequent pattern mining algorithms can be easily extended to handle sequential pattern mining in the case of INSERT.

It is far trickier to handle the second case, APPEND, than the first one. This is because not only the appended items may generate new locally frequent sequences in  X  db , but also that locally infrequent sequences may contribute their occurrence count to the same infrequent sequences in the original database to produce frequent ones. For example, in the appended database in Figure 1, suppose | DB | =1000 and |  X  db | =20, min sup =10%. Suppose a sequence s is in-Figure 1: Examples in INSERT and APPEND database frequent in DB with 99 occurrences ( sup =9 . 9%). In ad-dition, it is also infrequent in  X  db with only 1 occurrence ( sup = 5%). Although s is infrequent in both DB and  X  db , it becomes frequent in DB with 100 occurrences. This problem complicates the incremental mining since one can-not ignore the infrequent sequences in  X  db , but there are an exponential number of infrequent sequences even in a small  X  db and checking them against the set of infrequent sequences in DB will be very costly.
 When the database is updated with a combination of IN-SERT and APPEND, we can treat INSERT as a special case of APPEND  X  treating the inserted sequences as appended transactions to an empty sequence in the original database. Then this problem is reduced to APPEND. Therefore, we focus on the APPEND case in the following discussion.
In this paper, an efficient algorithm, called IncSpan ,is developed, for incremental mining over multiple database increments. Several novel ideas are introduced in the al-gorithm development: (1) maintaining a set of  X  almost fre-quent  X  sequences as the candidates in the updated database, which has several nice properties and leads to efficient tech-niques, and (2) two optimization techniques, reverse pattern matching and shared projection , are designed to improve the performance. Reverse pattern matching is used for matching a sequential pattern in a sequence and prune some search space. Shared projection is designed to reduce the number of database projections for some sequences which share a common prefix. Our performance study shows that IncSpan is efficient and scalable.

The remaining of the paper is organized as follows. Sec-tion 2introduces the basic concepts related to incremental sequential pattern mining. Section 3 presents the idea of buffering patterns, several properties of this technique and the associated method. Section 4 formulates the IncSpan al-gorithm with two optimization techniques. We report and analyze performance study in Section 5, introduce related work in Section 6. We conclude our study in Section 7.
Let I = { i 1 ,i 2 ,...,i k } be a set of all items. A subset of I is called an itemset .A sequence s = t 1 ,t 2 ,...,t ( t i  X  I )isanorderedlist. The size , | s | , of a sequence is the number of itemsets in the sequence. The length , l ( s ), is the total number of items in the sequence, i.e., l ( s )= i =1 | t i | . A sequence  X  = a 1 ,a 2 ,...,a m is a sub-sequence of another sequence  X  = b 1 ,b 2 ,...,b n , denoted as  X   X  (if  X  =  X  , written as  X   X   X  ), if and only if  X  i 1 ,i 2 such that 1 i 1 &lt;i 2 &lt;...&lt;i m n and a 1  X  b i 1 ,a b ,..., anda m  X  b i m .

A sequence database, D = { s 1 ,s 2 ,...,s n } ,isasetofse-quences. The support of a sequence  X  in D is the number of sequences in D which contain  X  , support (  X  )= |{ s | Dand X  s }| . Given a minimum support threshold, min sup , a sequence is frequent ifitssupportisnoless than min sup ; given a factor  X   X  1, a sequence is semi-frequent if its support is less than min sup but no less than  X   X  min sup ; a sequence is infrequent if its support is less than  X   X  min sup .Thesetof frequent sequential pattern , FS , includes all the frequent sequences; and the set of semi-frequent sequential pattern SFS , includes all the semi-frequent sequences.

EXAMPLE 1. The second column of Table 1 is a sam-ple sequence database D .If min sup =3, FS = { ( a ) : 4 , ( b ) :3 , ( d ) :4 , ( b )( d ) :3 } .
 Table 1: A Sample Sequence Database D and the Appended part
Given a sequence s = t 1 ,...,t m and another sequence s a = t 1 ,...,t n , s = s s a means s concatenates with s . s is called an appended sequence of s ,denotedas s  X  a s .If s a is empty, s = s , denoted as s  X  = a s .An appended sequence database D of a sequence database D is one that (1)  X  s i  X  D ,  X  s j  X  D such that s i  X  a s  X  = a s j ,and(2)  X  s i  X  D ,  X  s j  X  D such that s j  X  a s s i.e., LDB is the set of sequences in D which are appended with items/itemsets. We denote ODB = { s i | s i  X  D and s  X  a s j } , i.e., ODB is the set of sequences in D which are appended with items/itemsets in D . We denote the set of frequent sequences in D as FS .

EXAMPLE 2. The third column of Table 1 is the ap-pended part of the original database. If min sup =3, FS = { ( a ) :5 , ( b ) :4 , ( d ) :4 , ( b )( d ) :3 , ( c ) : 3 , ( a )( b ) :3 , ( a )( c ) :3 } .

A sequential pattern tree T is a tree that represents the set of frequent subsequences in a database. Each node p in T has a tag labelled with s or i . s means the node is a starting item in an itemset; i means the node is an interme-diate item in an itemset. Each node p has a support value which represents the support of the subsequence starting from the root of T and ending at the node p .

Problem Statement. Given a sequence database D , a min sup threshold, the set of frequent subsequences FS in D , and an appended sequence database D of D ,the problem of incremental sequential pattern mining is to mine the set of frequent subsequences FS in D based on FS instead of mining on D from scratch.
In this section, we present the idea of buffering semi-frequent patterns, study its properties, and design solutions of how to incrementally mine and maintain FS and SFS . Figure 2: The Sequential Pattern Tree of FS and SFS in D
We buffer semi-frequent patterns, which can be consid-ered as a statistics-based approach. The technique is to lower the min sup by a buffer ratio  X   X  1andkeepaset SFS in the original database D . This is because since the sequences in SFS are  X  almost frequent  X , most of the fre-quent subsequences in the appended database will either come from SFS or they are already frequent in the original database. With a minor update to the original database, it is expected that only a small fraction of subsequences which were infrequent previously would become frequent. This is based on the assumption that updates to the original database have a uniform probability distribution on items. It is expected that most of the frequent subsequences intro-duced by the updated part of the database would come from the SFS .The SFS forms a kind of boundary (or  X  buffer zone  X ) between the frequent subsequences and infrequent subsequences.

EXAMPLE 3. Given a database D in Example 1, min sup =3,  X  =0 . 6. The sequential pattern tree T representing FS and SFS in D is shown in Figure 2. FS are shown in solid line and SFS in dashed line.
 When the database D is updated to D ,wehavetocheck LDB to update support of every sequence in FS and SFS . There are several possibilities: 1. A pattern which is frequent in D is still frequent in D ; 2. A pattern which is semi-frequent in D becomes frequent 3. A pattern which is semi-frequent in D is still semi-frequent 4. Appended database  X  db brings new items. 5. A pattern which is infrequent in D becomes frequent in 6. A pattern which is infrequent in D becomes semi-frequent
Case (1) X (3) are trivial cases since we already keep the information. We will consider case (4) X (6) now.

Case (4) : Appended database  X  db brings new items. For example, in the database D , ( c ) is a new item brought by  X  db . It does not appear in D .

Property : An item which does not appear in D and is brought by  X  db has no information in FS or SFS . Solution : Scan the database LDB for single items.
For a new item or an originally infrequent item in D ,if it becomes frequent or semi-frequent, insert it into FS or SFS . Then use the new frequent item as prefix to construct projected database and discover frequent and semi-frequent sequences recursively. For a frequent or semi-frequent item in D , update its support.

Case (5) : A pattern which is infrequent in D becomes frequent in D . For example, in the database D , ( a )( c ) is an example of case (5). It is infrequent in D and becomes frequent in D . We do not keep ( a )( c ) in FS or SFS , but we have the information of its prefix ( a ) .

Property : If an infrequent sequence p in D becomes frequent in D , all of its prefix subsequences must also be frequent in D . Then at least one of its prefix subsequences p is in FS .

Solution : Start from its frequent prefix p in FS and construct p -projected database, we will discover p .
Formally stated, given a frequent pattern p in D , we want to discover whether there is any pattern p with p as prefix where p was infrequent in D but is frequent in D .Ase-quence p which changes from infrequent to frequent must have  X  sup ( p ) &gt; (1  X   X  ) min sup .

We claim if a frequent pattern p has support in LDB sup LDB ( p )  X  (1  X   X  ) min sup , it is possible that some sub-sequences with p as prefix will change from infrequent to frequent. If sup LDB ( p ) &lt; (1  X   X  ) min sup , we can safely prune search with prefix p .
 Theorem 1. For a frequent pattern p , if its support in LDB sup LDB ( p ) &lt; (1  X   X  ) min sup , then there is no se-quence p having p as prefix changing from infrequent in D to frequent in D .
 Proof : p was infrequent in D ,so
If sup LDB ( p ) &lt; (1  X   X  ) min sup ,then
Since sup LDB ( p )= sup ODB ( p )+ X  sup ( p ) .Thenwe have (2)
Since sup D ( p )= sup D ( p )+ X  sup ( p ) , combining (1) and (2), we have sup D ( p ) &lt;min sup .So p cannot be frequent in D .

Therefore, if a pattern p has support in LDB sup LDB ( p ) &lt; (1  X   X  ) min sup , we can prune search with prefix p .Other-wise, if sup LDB ( p )  X  (1  X   X  ) min sup , it is possible that some sequences with p as prefix will change from infrequent to fre-quent. In this case, we have to project the whole database D using p as prefix. If | LDB | is small or  X  is small, there are very few patterns that have sup LDB ( p )  X  (1  X   X  ) min sup , making the number of projections small.

In our example, sup LDB ( a )=3 &gt; (1  X  0 . 6)  X  3, we have to do the projection with ( a ) as prefix. And we discover  X  ( a )( c ) : 3 X  which was infrequent in D . For another ex-ample, sup LDB ( d )=1 &lt; (1  X  0 . 6)  X  3, there is no sequence with d as prefix which changes from infrequent to frequent, sowecanprunethesearchonit.

Theorem 1 provides an effective bound to decide whether it is necessary to project a database. It is essential to guar-antee the result be complete.

We can see from the projection condition, sup LDB ( p )  X  (1  X   X  ) min sup , the smaller  X  is, the larger buffer we keep, the fewer database projections the algorithm needs. The choice of  X  is heuristic. If  X  is too high, then the buffer is small and we have to do a lot of database projections to discover sequences outside of the buffer. If  X  is set very low, we will keep many subsequences in the buffer. But mining the buffering patterns using  X   X  min sup would be much more inefficient than with min sup . We will show this Figure 3: The Sequential Pattern Tree of FS and SFS in D tradeoff through experiments in Section 5.

Case (6) : A pattern which is infrequent in D becomes semi-frequent in D . For example, in the database D , ( be ) is an example of case (6). It is infrequent in D and becomes semi-frequent in D .

Property : If an infrequent sequence p becomes semi-frequent in D , all of its prefix subsequences must be either frequent or semi-frequent. Then at least one of its prefix subsequences, p ,isin FS or SFS .

Solution : Start from its prefix p in FS or SFS and con-struct p -projected database, we will discover p .
Formally stated, given a pattern p ,wewanttodiscover whether there is any pattern p with p as prefix where p was infrequent but is semi-frequent in D .

If the prefix p is in FS or SFS , construct p -projected database and we will discover p in p -projected database. Therefore, for any pattern p from infrequent to semi-frequent, if its prefix is in FS or SFS , p can be discovered.
In our example, for the frequent pattern ( b ) ,wedothe projection on ( b ) and get a semi-frequent pattern ( be ) :2 which was infrequent in D .

We show in Figure 3 the sequential pattern tree T includ-ing FS and SFS after the database updates to D .Wecan compare it with Figure 2to see how the database update affects FS and SFS .
In this section, we formulate the IncSpan algorithm which exploits the technique of buffering semi-frequent patterns. We first present the algorithm outline and then introduce two optimization techniques.
Given an original database D , an appended database D , a threshold min sup , a buffer ratio  X  , a set of frequent se-quences FS and a set of semi-frequent sequences SFS ,we want to discover the set of frequent sequences FS in D . Step 1: Scan LDB for single items, as shown in case (4).
Step 2: Check every pattern in FS and SFS in LDB to adjust the support of those patterns.
 Step 2.1: If a pattern becomes frequent, add it to FS . Then check whether it meets the projection condition. If so, use it as prefix to project database, as shown in case (5). Step 2.2: If a pattern is semi-frequent, add it to SFS .
The algorithm is given in Figure 4.
Reverse pattern matching is a novel optimization tech-nique. It matches a sequential pattern against a sequence from the end towards the front. This is used to check sup-Algorithm. IncSpan( D , min sup ,  X  , FS , SFS )
Input: An appended database D , min sup ,  X  ,frequent sequences FS in D , semi-frequent sequences SFS in D .

Output: FS and SFS . 1: FS =  X  , SFS =  X  2:Scan LDB for single items; 3: Add new frequent item into FS ; 4: Add new semi-frequent item into SFS ; 5: for each new item i in FS do 6: PrefixSpan( i , D | i ,  X   X  min sup , FS , SFS ); 7: for every pattern p in FS or SFS do 8: check  X  sup ( p ); 9: if sup ( p )= sup D ( p )+ X  sup ( p )  X  min sup 10: insert( FS , p ); 11: if sup LDB ( p )  X  (1  X   X  ) min sup 12: PrefixSpan( p , D | p ,  X   X  min sup , FS , SFS ); 13: else 14: insert( SFS , p ); 15: return; port increase of a sequential pattern in LDB .Sincethe appended items are always at the end part of the original sequence, reverse pattern matching would be more efficient than projection from the front.

Given an original sequence s , an appended sequence s = s s a , and a sequential pattern p , we want to check whether the support of p will be increased by appending s a to s . There are two possibilities: 1. If the last item of p is not supported by s a ,whether p 2. If the last item of p is supported by s a ,wehavetocheck Figure 5 shows the reverse pattern matching.
Shared Projection is another optimization technique we exploit. Suppose we have two sequences ( a )( b )( c )( d ) and ( a )( b )( c )( e ) , and we need to project database using each as prefix. If we make two database projections individu-ally, we do not take advantage of the similarity between the two subsequences. Actually the two projected databases up From D | ( a )( b )( c ) , we do one more step projection for item d and e respectively. Then we can share the projection for ( a )( b )( c ) .

To use shared projection, when we detect some subse-quence that needs projecting database, we do not do the projection immediately. Instead we label it. After finishing checking and labelling all the sequences, we do the projec-tion by traversing the sequential pattern tree. Tree is natural for this task because the same subsequences are represented using shared branches.
A comprehensive performance study has been conducted in our experiments. We use a synthetic data generator pro-vided by IBM. The synthetic dataset generator can be re-trieved from an IBM website, http://www.almaden.ibm.com /cs/quest. The details about parameter settings can be re-ferredin[1].

All experiments are done on a PowerEdge 6600 server with Xeon 2.8 , 4G memory. The algorithms are writ-ten in C++ and compiled using g++ with -O3 optimiza-tion. We compare three algorithms: IncSpan ,anincremental mining algorithm ISM [7], and a non-incremental algorithm PrefixSpan [8].

Figure 6 (a) shows the running time of three algorithms when min sup changes on the dataset D10C10T2.5N10, 0.5% of which has been appended with transactions. IncSpan is the fastest, outperforming PrefixSpan by a factor of 5 or more, and outperforming ISM even more. ISM even cannot finish within a time limit when the support is low.
Figure 6 (b) shows how the three algorithms can be af-fected when we vary the percentage of sequences in the database that have been updated. The dataset we use is D10C10T2.5N10, min sup =1%. The buffer ratio  X  =0 . 8. The curves show that the time increases as the incremental portion of the database increases. When the incremental part exceeds 5% of the database, PrefixSpan outperforms IncSpan . This is because if the incremental part is not very small, the number of patterns brought by it increases, mak-ing a lot overhead for IncSpan to handle. In this case, mining from scratch is better. But IncSpan still outperforms ISM a wide margin no matter what the parameter is.
 Figure 6 (c) shows the memory usage of IncSpan and ISM. The database is D10C10T2.5N10, min sup varies from 0.4% to 1.5%, buffer ratio  X  =0 . 8. Memory usage of IncSpan in-creases linearly as min sup decreases while memory used by ISM increases dramatically. This is because the number of sequences in negative border increases sharply as min sup decreases. This figure verifies that negative border is a memory-consuming approach.

Figure 7 (a) shows how the IncSpan algorithm can be af-fected by varying buffer ratio  X  . Dataset is D10C10T2.5N10, 5% of which is appended with new transactions. We use PrefixSpan as a baseline. As we have discussed before, if we set  X  very high, we will have fewer pattern in SFS , then the support update for sequences in SFS on LDB will be more efficient. However, since we keep less information in SFS , we may need to spend more time on projecting databases. In the extreme case  X  =1, SFS becomes empty. On the other hand, if we set the  X  very low, we will have a large number of sequences in SFS , which makes the support up-date stage very slow. Experiment shows, when  X  =0 . 8, it achieves the best performance.

Figure 7 (b) shows the performance of IncSpan to handle multiple (5 updates in this case) database updates. Each time the database is updated, we run PrefixSpan to mine from scratch. We can see from the figure, as the increments accumulate, the time for incremental mining increases, but increase is very small and the incremental mining still out-performs mining from scratch by a factor of 4 or 5. This experiment shows that IncSpan can really handle multiple database updates without significant performance degrad-ing.

Figure 7 (c) shows the scalability of the three algorithms by varying the size of database. The number of sequences in databases vary from 10,000 to 100,000. 5% of each database is updated. min sup =0.8%. It shows that all three algo-rithms scale well with the database size.
In sequential pattern mining, efficient algorithms like GSP [10], SPADE [13], PrefixSpan [8], and SPAM [2] were devel-oped.

Partition [9] and FUP [3] are two algorithms which pro-mote partitioning the database, mining local frequent item-sets, and then consolidating the global frequent itemsets by cross check. This is based on that a frequent itemset must be frequent in at least one local database. If a database is updated with INSERT, we can use this idea to do the incre-mental mining. Zhang et al. [14] developed two algorithms for incremental mining sequential patterns when sequences are inserted into or deleted from the original database.
Parthasarathy et al. [7] developed an incremental mining algorithm ISM by maintaining a sequence lattice of an old database. The sequence lattice includes all the frequent se-quences and all the sequences in the negative border. How-ever, there are some disadvantages for using negative border: (1) The combined number of sequences in the frequent set and the negative border is huge; (2) The negative border is generated based on the structural relation between se-quences. However, these sequences do not necessarily have high support. Therefore, using negative border is very time and memory consuming.

Masseglia et al. [6] developed another incremental mining algorithm ISE using candidate generate-and-test approach. The problem of this algorithm is (1) the candidate set can be very huge, which makes the test-phase very slow; and (2) its level-wise working manner requires multiple scans of the whole database. This is very costly, especially when the sequences are long.
In this paper, we investigated the issues for incremen-tal mining of sequential patterns in large databases and addressed the inefficiency problem of mining the appended database from scratch. We proposed an algorithm IncSpan by exploring several novel techniques to balance efficiency and reusability. IncSpan outperforms the non-incremental method (using PrefixSpan ) and a previously proposed incre-mental mining algorithm ISM by a wide margin. It is a promising algorithm to solve practical problems with many real applications.
 There are many interesting research problems related to IncSpan that should be pursued further. For example, in-cremental mining of closed sequential patterns, structured patterns in databases and/or data streams are interesting problems for future research. [1] R. Agrawal and R. Srikant. Mining sequential [2] J.Ayres,J.E.Gehrke,T.Yiu,andJ.Flannick.
 [3] D. Cheung, J. Han, V. Ng, and C. Wong. Maintenance [4] M. Garofalakis, R. Rastogi, and K. Shim. SPIRIT: [5] H. Mannila, H. Toivonen, and A. I. Verkamo.
 [6] F. Masseglia, P. Poncelet, and M. Teisseire. [7] S. Parthasarathy, M. Zaki, M. Ogihara, and [8] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, [9] A. Savasere, E. Omiecinski, and S. Navathe. An [10] R. Srikant and R. Agrawal. Mining sequential [11] J. Wang and J. Han. Bide: Efficient mining of [12] X. Yan, J. Han, and R. Afshar. CloSpan: Mining [13] M. Zaki. SPADE: An efficient algorithm for mining [14] M. Zhang, B. Kao, D. Cheung, and C. Yip. Efficient
