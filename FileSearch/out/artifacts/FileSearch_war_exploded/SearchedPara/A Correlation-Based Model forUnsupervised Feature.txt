 We propose a new model for feature evaluation and selection that assesses the propensity of the features to support two-set classification. For each item of the data set, the collection of features induce a ranking (ordered list) of the remaining items. The evaluation criterion favors features that result in the most consistent discrimination between relevant and non-relevant items within these ranked lists. The discrimi-nation boundaries within a single list are determined combi-natorially, according to the degree of correlation among the relevant sets of its members. The model makes no special assumptions on the nature of the data. A selection heuristic based on the model is also proposed using sequential forward generation, and an experimental comparison is made with other unsupervised feature selection methods.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Systems]: Information Storage and Retrieval  X  Infor-mation Search and Retrieval General Terms: Performance Keywords: Feature Selection, Clustering
For many applications on very large data sets, such as data mining, machine learning, pattern recognition, and search, the choice of data features can have a considerable impact on both quality and performance. Given an initial set of candidate features, the selection of a smaller, effective subset not only reduces the dimensionality of the feature space, but can also lead to a substantial improvement in process speed, eliminate noise from the feature set, and lead to simpler and more comprehensible models of the data.

The feature selection problem can be defined as that of determining a minimal subset of a collection of features so that the reduced feature space optimizes a given evaluation criterion. Finding an optimal feature set is typically difficult, with many problems related to feature selection known to be NP-hard.

A great number of heuristics for feature selection have been proposed in the context of supervised learning [6, 11, 12], where the goal of the selection process is to maximize the predictive accuracy. Supervised feature selection requires that class-labeled training sets be available, which may not always be the case due to the cost and expertise required to produce them. However, relatively few heuristics have yet been proposed for unsupervised feature selection. Sev-eral have been proposed within the wrapper framework, in which the search process is wrapped around an (unsuper-vised) clustering method rather than a (supervised) induc-tion method [3, 4, 10]. Filter-style methods for unsupervised feature selection are also known [2, 13, 15].

In this paper, we present a new filter-style model for un-supervised feature selection, whose evaluation criterion is based on the Relevant Set Correlation (RSC) model for clus-tering [8]. The model, which we call Relevant Set Correla-tion for Features (RSCF), assesses a collection of features according to its ability to form aggregations of points whose local neighborhoods (the  X  X elevant sets X ) are strongly corre-lated. The model presupposes that the data set S has a natural (yet unknown) partition P = { P 1 ,P 2 ,...,P m } disjoint classes, for which a set of features would ideally allow for discrimination between any class P i and the re-mainder of the set. More precisely, the best set of features would have the property that any similarity query based at an item q  X  P i would rank any item of the partition set P ahead of any item not in P i . The evaluation criterion can be viewed as a two-stage procedure: for a given q  X  S ,the first stage is to identify the most  X  X ignificant X  neighborhood based at q according to a discriminative correlation criterion adapted from RSC. The second stage involves aggregating a score for each significant neighborhood according to an internal correlation criterion, again adapted from RSC.
The feature evaluation model proposed in this paper has its origins in the Relevant-Set Correlation (RSC) model for clustering. In this section, we briefly describe only those elements of the RSC model we require; for a description of the full model and its application to clustering, see [8, 9].
Let S be a dataset drawn from some domain D . For every item q  X  S , we assume the existence of a unique ordering  X  ( q )=( q 1 ,q 2 ,...,q | S | ) of the items of S ,where i&lt;j that q i  X  S is deemed more relevant or similar to q than For any 1  X  i  X | S | , we will denote the i -th element in the ordering by  X  ( q, i ). The ranking  X  ( q ) also induces a collection of relevant sets q ( q, k )= {  X  ( q, i ) | 1  X  i  X  k } each choice of set size 1  X  k  X | S | . With respect to the ranking, if a dataset query-by-example operation were to be based at item q , q ( q, k ) would represent the top-k relevant set.

In practice the ordering function  X  typically results from some combination of data features together with a similar-ity measure, and would expected to be consistent with some spatial embedding of the data set. However, for the pur-poses of the discussion to follow, we will allow  X  ( q )tobe any arbitrary permutation of the items of S , not necessarily reflecting a spatial embedding of the data, and not neces-sarily dependent on any other permutation  X  ( q ), q  X  S . Consider any two subsets A and B drawn from data set S , each associated with some underlying concept relevant to the domain. Every item of S can be associated with a coordinate of a vector space whose dimension is equal to the size of S . A subset A of S can be represented by a zero-one characteristic vector in this space, where a coordinate value of 1 indicates that the corresponding item is a member of and a value of 0 indicates that the item does not belong to A . Even if no additional information is available regarding the nature of A and B , the relationship between A and B (and their underlying concepts) can be quantified in terms of the correlation between corresponding coordinates of their characteristic vectors.

For sequences ( x 1 ,x 2 ,...,x n )and( y 1 ,y 2 ,...,y n ables with means  X  x and  X  y , respectively, the standard Pearson correlation is given by the following formula: Applying the formula to the pairs of coordinates of the char-acteristic vectors of sets A and B , after simplification one obtains the following inter-set correlation formula:
RSC assesses the internal association of set A according to correlations involving relevant sets based at the members of
A . Provided that the ranking function  X  adequately cap-tures the relevancies among the data items, if an item v  X  A is strongly associated with the remaining items of A ,itis likely that the items of S that are highly relevant to v also belong to set A . This intuition forms the motivation for the (first-order) RSC self-correlation measure proposed in [8], defined as the average of the inter-set correlations between A and relevant sets of size | A | taken over all items of A An intra-set correlation value of 1 indicates perfect associa-tion among the members of A , whereas a value approaching 0 indicates little or no internal association within A .
The self-correlation statistic can be used to assess and compare the level of internal association of two subsets of S , provided that their sizes are equal. However, when the two subsets are of very different sizes, the comparison is likely to favor the smaller of the two sets  X  for this reason, as with the Pearson correlation statistic upon which it is based, the degree of significance of the self-correlation value relative to set size must be taken into account. As is argued in [8], the standard tests for the significance of the Pearson correlation cannot be applied to the self-correlation of A since the number of variable pairs to be correlated is equal to | S | (a fixed quantity) and not | A | .

Instead, we test against the assumption that each rele-vant set contributing to the correlation score is indepen-dently generated by means of uniform random selection from among the available items of S . In practice, of course, the relevant sets are far from random. However, this situation serves as a convenient reference point from which the signif-icance of observed correlation values can be assessed. Under the randomness hypothesis, the mean and standard devia-tion of the correlation score can be calculated. Standard scores (also known as Z -scores) can then be generated and compared with one another. The more significant relation-ship would be the one whose standard score is highest  X  that is, the one whose correlation exceeds its expected value by the greatest number of standard deviations. This stan-dard score is derived as in [8], and is referred to as the significance of A .
Clustering heuristics based on the RSC model can assess the relative importance of two candidate cluster sets accord-ing to the significance of their internal association values, as described above. However, the use of RSC presupposes that the underlying ranking function  X  is fixed. When  X  is al-lowed to vary, pathological choices of the function can lead to unfortunate interpretations under the RSC model. As an extreme example, consider the case where  X  ( q )=  X  ( v )for all choices of q  X  S and v  X  S  X  X hatis,if  X  produces the same ordering of | S | for every query item in S . Such a ranking function would clearly be useless for the purposes of clustering or retrieval. However, under the original RSC model, the subset of | S | attaining the maximum significance score is S itself, with a self-correlation score of sr ( S and a significance value of Z ( S )= tunately, this significance value is the largest that can be attained over all possible choices of S and  X  .

To address this problem, we propose an extension of the self-correlation measure that takes into account not only the intra-set association, but the associations with items exter-nal to the set. We define the distinctiveness of A  X  S as: This quantity can be viewed as the difference between two average correlations, the first being the self-correlation and the second being the average correlation between A and same-sized relevant sets based at items outside A .Itcan easily be seen that for the extreme case where  X  returns the same ordering for every possible query, the value of dr ( is zero for all choices of A .

The significance of A can be computed with respect to the distinctiveness, under the hypothesis of randomness em-ployed by the original RSC model. The derivation is analo-gous to that of the original RSC significance, and is omitted in this version.

Z dr ( A )=
Let P = { P 1 ,P 2 ,...,P m } be any disjoint partition of the items of data set S , for some m  X  1. For item q  X  S , let be the index of the unique partition set to which it belongs. A ranking function  X  will be said to be ideal for partition if for all q  X  S , its associated relevant sets satisfy in other words,  X  is ideal if all elements of the partition set containing q appear ahead of any other items of S in the ordering  X  ( q ).
We now state the RSCF evaluation criterion that will be used to guide the unsupervised feature selection process. Let k  X  and k + be minimum and maximum bounds on the allow-able size of a partition set of S ,with1 &lt;k  X   X  k + &lt; | S |
It is possible for Z dr ( q ( q, k )) to achieve its maximum at more than one value of k . Throughout the paper, we shall assume that argmax returns the smallest index in the range at which the maximum value is attained.

The following properties of the evaluation criterion  X  (  X  prevent highly redundant feature sets, or highly noisy sets, from achieving high scores:
On the other hand, for sufficiently-large data sets, and disjoint partitions whose class sizes lie within a certain wide range, it turns out that there always exists an ideal partition for which the evaluation criterion can achieve the optimum value 1. The sufficient conditions on the data set are stated more formally as Theorem 1 below (proof omitted in this version). Informally, the theorem indicates the usefulness of the RSCF evaluation criterion in guiding a feature selection process towards an ideal feature set, even when the target data partition is unknown.

Theorem 1. Let P = { P 1 ,P 2 ,...,P m } be a disjoint par-tition of S such that 1 &lt; | P i | X  | S | 2 for all 1  X  i  X  m n = | S | . For every q  X  S , define the function If for all 1  X  i  X  m we have 4  X | P i | &lt; n 4 ,andif (10
In this section, we shall discuss how the RSCF evaluation criteria can be wrapped within a search procedure so as to select f effective features from an initial set of p features.
Although the evaluation criterion  X  has many desirable qualities as a model for feature selection, to explicitly com-pute its value would require time quartic in the size of the data set. A more practical alternative is to estimate the value of  X  , by disregarding the contributions to dr arising from correlations with external items (the negative terms in the formula). The external correlations have an expected contribution of zero to the total score; in practice, the vast majority of external items will have correlations close to zero. One option is to limit the computation of external correlations to a restricted number of items (the closest ex-ternal items to q ), where the number is dependent upon the size of the internal set ( k ), rather than the full data set ( Another is to simply ignore the external contributions to-gether. This allows reducing, in practice, the distinctiveness computation of a set A to the computation of its intra-set association, namely dr ( A )  X  1 | A |
The above approximation taken alone would result in a drop in the time complexity, but only from quartic in the data set size to cubic. However, for the method to be prac-tical, further reductions would be needed. For the origi-nal RSC greedy clustering heuristic, scalability was achieved by computing significance values with respect to fixed-size neighborhoods within a hierarchy of data samples of sizes , n 4 , .... A value of  X  achieved at a set of size k within a sample of size m = n 2 l serves to estimate the value of one would expect for a set of size k 2 l within the original set. The strategy used by RSCF to evaluate its significance scores Z dr is identical to the one used by the GreedyRSC heuristic to evaluate the original RSC significance measure Z . For further details, we refer the reader to [9].
Although the RSCF feature evaluation criterion can be wrapped within any of the search strategies commonly used for feature extraction, the cost of computing estimates of essentially rules out the more expensive (yet robust) strate-gies such as tabu search. For the experiments of this paper, a hill-climbing search technique was employed, in which a fixed-sized feature set was improved by iteratively consider-ing swaps of one feature in the current set with one feature outside the set. Any swaps leading to an improvement in the criterion value are confirmed, whereas any that do not lead to an improvement are rejected. Formally, a hill climb-ing strategy with cost function  X  selects a swap m i at each iteration i if  X  (state i  X  m i )  X   X  (state i ). In our experiments, a state is a set of f features drawn from the original set, and a move is a feature swap. With  X  f denoting the ordering function induced by f , the cost function is the evaluation criterion  X  (  X  f ). The hill climbing is initialized by selecting a random state and then proceding to explore the search space by means of random swaps. In order to accelerate the search process, bounds can be placed on the maximum number of search iterations and the maximum number of consecutive non-improving iterations. The process can also be halted after a predetermined number of iterations. Figure 1: Performance of different feature selection strategies for COIL (left) and ALOI (right).

To illustrate the effectiveness of our selection and eval-uation strategies, we implemented two other unsupervised feature selection methods: projection onto a vector basis of reduced dimension as determined via principal compo-nent analysis (PCA) [5, 6], and sequential forward genera-tion (SFG) using an entropy-based criterion [2, 11].
Our experimental comparison was performed on sets of images selected from the COIL-100 and ALOI databases. Our choice was motivated by the fact that feature selection is a particularly challenging and important problem in mul-timedia contexts. The COIL-100 dataset consists of 7200 images belonging to one of 100 classes, each class compris-ing 72 different views of the same physical object [14]. The full ALOI dataset consists of 110,250 images of 1000 differ-ent objects, taken from a variety of views under differing illumination conditions [7]. From the first 80 objects of the full set, we generated a collection with image classes of vary-ing sizes, by selecting the first 100  X  i images from object This yielded a data set of 4760 images with object class sizes ranging uniformly from 20 to 99. For both datasets, each image was (initially) described by a collection of 641 color histogram, texture, and shape features as devised by Boujemaa et al. [1].

For the COIL dataset, each algorithm was given the task of selecting 160 features out of the original 641, a 4-fold re-duction, with a ratio of 1.6 features per object class. For the reduced ALOI dataset, the task was to select 120 features, a more than 5-fold reduction, with a ratio of 1.5 features per class. Figure 1 depicts the classical precision/recall diagrams corresponding to our technique RSCF, the two unsupervised feature selection SFG and dimensionality reduction PCA, and the original feature set FULL. For these diagrams, the precision is plotted for every possible recall value, expressed as a proportion of the class size. For the COIL set, RSCF required an average of 30s per iteration, and for ALOI, 17s per iteration. Execution for both COIL and ALOI was ter-minated after 500 iterations.

The results of the experiments demonstrate the effective-ness of RSCF in identifying features that allow better over-all retrieval performance, relative to well-established unsu-pervised feature selection methods based on information-theoretic criteria or dimension reduction techniques. In the case of COIL, where the classes are of uniform size and are well-distinguished, the performance essentially matches that of the full feature set. For the more difficult case of ALOI, where the classes are of variable size and the objects are less-well distinguished, RSCF still managed to be competi-tive with the full feature set while substantially outperform-ing both PCA and SFG. The results support our belief that the RSCF criterion can be an effective analytical tool for data discovery applications in which the characteristics of the data are largely unknown.
