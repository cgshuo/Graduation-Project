 In this paper, we describe the utilization of text encoding and prediction by partial matching langua ge modeling to identify gene functions within abstracts of biomedical papers. The National Center for Biotechnology Information has  X  X eneRIF X   X  a collection of the best possible functional representations for a subset of abstracts from PubMed. We use GeneRIF to test the efficiency of our technique. We discuss the methodology adopted to construct models necessary to enable the Text Mining Toolkit to distinguish between gene functi ons and the rest of the abstract (non gene functions). We also de scribe the similarity based approach we deploy on the list of automatically annotated functions to generate the most li kely gene function representative of the paper. The results indicate that our combined approach to identify gene functions in scientific abstracts performs very well on both precision and recall, and therefore presents exciting opportunities for use in extrac ting other entities embedded in scientific text. I.2.7 [ Natural Language Processing ]: Language generation , Language models , Language parsing and understanding , Machine translation , Text analysis . Algorithms, Measurement, Document ation, Performance, Design, Experimentation, Languages, Theory, Verification. Prediction by partial matching (PPM), Text Mining, Gene function identification, Entropy. The PubMed collection [1] is incontestably considered the largest data source for biomedical articl es X  citations. With this large amount of abstracts made available for researchers, it is often time consuming to manually search th rough an abstract to identify target entities such as genes and gene information such as gene functions. Providing automatic support for annotating these abstracts provides tremendous opportunities to leverage the wealth of information that they comprise. Several research studies have been reported that aim to extract  X  X seful information X  from these studies are dedicated to identifying gene name entities. The main feature that characterizes the existing approach is the tokens); and the use of techniqu es varying from syntactic based rules to probabilistic and machin e learning techniques for token identification. Recently, studies have been reported that aim to extract information beyond just protein/gene names; targeting biological features that characterize these entities such as protein interactions, pathways, and gene functions. In the domain of gene function identification, we distingui sh research work such as [9, 10, 11, 12]. Studies cited in [9, 10 , 11] make use of Gene ontology [13], as the basis for discovering biological processes that help in the easy identification of gene function. The method described in [9] relies on the ability to automatically assign GO ids of a biological process to each gene or protein and then utilize shallow processing and sentence structure analysis. A preliminary run resulted in a precision of 91-94% and a recall of 54-64%. The low recall rate is explained to be due to insufficient GO terms to associate with the biological process terms in the abstract. The MKE system described in [10] performs the identification of functions of gene products base d on indexing techniques that make use of GO database. In [12], the authors combine rule based and statistical techniques using surface clues such as occurrence of Roman numerals to identify name fragments. The reported results provide a precision of 25.5% and a recall of 88.4%. The low precision is attributed to th e presence of a large number of under annotated gene terms w ithin the training data. In this paper, we describe the utilization of language models from text encoding  X  specifically, pred iction by partial matching (PPM)  X  for the identification of gene functions within abstracts of scientific papers. The National Center for Biotechnology Information, NCBI [14] has  X  X  eneRIF X   X  a collection of best possible functional representations for a subset of abstracts from PubMed. We use GeneRIF collection [15] to test the performance of our technique. We discus s the methodology adopted to construct the models necessary fo r the Text Mining Toolkit [16] to distinguish between gene functi ons and the rest of the abstract. We also describe the similarity based approach we deploy on the list of automatically annotated functions to generate the most likely gene function representative of the paper. The rest of the paper is organi zed as follows. In section 2 we describe the methodology used to address the gene function extraction in scientific abstracts. Evaluation criteria, experiments performed and results obtained are discussed in section 3. Sections 4 and 5 discuss results, concludes this study and outline directions for future work. PPM. Then we describe the enc oding perspective that we adopt for text mining, and also the rele vant details of the text mining algorithm we use to perform segmentation of the text. Next we describe the text resources used to train the models, and the data pre-processing that was performed prior to the training phase. This is followed by a description of how the gene function candidates were identified, and how the gene function representative was finally selected for each abstract. The Prediction by Partial Matching (PPM) text compression scheme has consistently set the standard in lossless compression of text since it was originally published in 1984 by [17]. Compression methods encode a text string according to a given model. Formally, the average number of bits per symbol to encode a text string n x x T K 1 = can be considered to be the cross-language L [ 19 ] , and is given by: In the case of the PPM family of models, the individual symbols are encoded within the context provided by the preceding symbols appearing in a text sequence. Taking the above equation as a starting point, we can achieve op timal compression for the model 
P by applying the Chain Rule: Thus each symbol is encoded according to its information content within the context provided by all the preceding symbols. In practice, PPM uses a Markov ap proximation and assumes a fixed order context. A fixed order context of five is found to perform well, and increasing it further does not generally improve compression [20]. PPM uses an approximate blending technique called exclusion based on the escape mechanism to exclude lower order predictions from the final pr obability estimate. This includes an order 0 model which predicts symbols based on their unconditioned probabilities, and a default model which ensures that a finite probability (however small) is assigned to all possible symbols. Prediction probabilities fo r each context in the model are calculated from frequency counts, and the symbol that actually occurs is encoded relative to its predicted distribution. The PPM method is character-b ased, although the blending mechanism can equally be applied to other classes of symbols such as words and parts of speech. Compression experiments using a wide range of English [20] show that word-based models consistently outperform the char acter-based methods, although the difference in performance is only a few percent. A fuller description of the PPM algorithm can be found in [21] and in [20]; two variants of the algorithm that perform well in compression experiments are PPMC [18] a nd PPMD [22]. These methods differ in the way they estimate the escape probability when a model uses a lower order contex t to make a prediction. PPMC uses the number of times a novel symbol has occurred before as the basis for this probability. PPM D is a slight variation of PPMC that usually provides a small but noticeable im provement in prediction in many cases. Data and text compression refe r to the use of encoding to represent the input stream usi ng fewer bits (or any suitable metric). Compression techniques aim to achieve high fidelity communication in the presence of noise and distortion. The compression method must also eval uate the actual overheads in obtaining sophisticated equipment at the receiver's end and the actual cost of transmitting the message as is.  X  X ossless X  data compression, used for compressing text files, excel spreadsheets and data of this genre, recom pose the data without any loss of fidelity whereas  X  X ossy X  data co mpression, used for compressing image files, videos etc.., allow a certain loss in the information content during data compression. Th e loss is practically invisible when the data is recomposed . Certain encoding schemes for compression use a fixed length c ode that is devoid of any statistical frequency of the actual symbol. For example, use of ASCII code to compress text data into bits generates a fixed length code for symbols that appear in the input stream. Most other encoding schemes take advantage of the statistical frequency of the input stream. This is ba sed on an observation that longer words tend to appear less freque ntly in common sentences as opposed to shorter words. For exam ple, words like  X  X s X ,  X  X ave X  appear more often than words like  X  X conomics X ,  X  X im ulation X  etc. Both Huffman encoding and Arithmetic encoding are statistical encoding techniques that minimi ze the overall code length by using the set of input symbols an d the frequency of appearance of each symbol. Text mining may be defined as th e process of analyzing text to extract information that is usef ul for particular purposes [23]. What is  X  X seful X  depends on th e application  X  for example, it might be finding words in Chinese text, finding names and addresses in bibliographic refere nces, or classifying text by language, authorship, or genre. Kni ght [24] defines text mining as the process of  X  X utomated or semi-automated knowledge acquisition from linguistic resour ces X . Text mining is possible because you do not have to understand the text in order to extract useful information from it. The standard approach to te xt mining is based on manually designing tokenizers and grammars for the particular data being extracted. Witten et al . [23] have shown how text compression can alternatively be used as a key technology for text mining. Their method uses supervised trai ning to detect sub-languages of text instead of explicit pr ogramming. Language modeling techniques based on the text comp ression scheme PPM are used to extract meaningful low-level in formation about the location of semantic tokens such as names, email addresses, locations, URLs and dates. The type of algorithm commonly used in natural language processing (NLP) applications is based on a statistical framework called the  X  X oisy channel model X  [25]. This has as its basis a theory developed by Shannon [26] to model a noisy communication channel. The key idea is that the target text can be considered to be a corruption of the source text. Here, the application is formulated as a communication process  X  a message is sent down a communication channel by the sender, and the receiver tries to recover the original message in the presence of noise by correcting the message that is received. This is usually done by finding the target text with maximum probability given the observed source text. The pro cess of correcting the text is often referred to as decoding (see Figure 1a). This is usually performed using a dynamic programming search algorithm such as the Viterbi [27] algorithm. The noisy channel model may seem a rather arbitrary and contentious characterization for NL P, but leads to more robust and accurate performance than other methods. For our purposes, where we wish to exploit languag e models based on state-of-the-art text compression techniques which have been found to be highly effective in many text mi ning and NLP applications [20], we wish to alter the perspective of the noisy channel model statistical framework. Rather than characterize the process as one of noisy communication  X  and therefore requiring a decoding process to recover the original message  X  our approach is to think of the process (perhaps more naturally) as noiseless communication, and th e emphasis is on encoding rather than decoding. Here, the sender will fi rst find the best encoding of the target message, so that it can be sent with lossless property to the receiver down a noiseless communica tion channel. In this case, the decoding process is of secondary importance, mainly to verify the encoding process  X  the emphasis instead is placed on how efficiently the target message can be encoded. Also encoded is additional information on how to transform the source message into the target message. In this case, the search process  X  that of finding the best transformation (i.e. the target message that optimizes the cost of encoding the target message, plus the information required to transform between it and the source message)  X  occurs prior to the encoding phase rather than during the decoding phase. (See Figure 1b). Witten et al. [23] apply this approach to text mining. They show how a broad range of patterns such as names, email addresses, and URLs can be located in text. With their approach, the training data for the model of the target text contains text that is already marked up in some manner. A correction process is then performed to  X  X ecover X  the most probable targ et text (with special markup symbols inserted) from the unma rked-up source text. Yeates &amp; Witten [28] show how many text mining applications can be recast as  X  X ag insertion X  problems  X  that of re-inserting tags back into a sequence to reveal the meta-data implicit in it (hence the name for the model in Figure 1b). Some sample applications they explore are word segmentation and identifying acronyms in text [29]. Teahan &amp; Harper [30] describe a competitive text compression scheme which also a dopts this tag insertion model. The scheme works by switching between different PPM models, both static (i.e. do not change once trained) and dynamic (adapt to the text as it is being encoded). Tags are inserted to indicate where the encoder has switched between models. The purpose of the classification search stage is to find the sequence of text plus inserted tags that have the best encoding. An example of this is shown in Figure 2. Note that in these approaches, the source text can easily be recovered from the target text simply by removing the tags. Tag insertion is not the only solution that can be considered here, though. A broader class of applic ations is possible by using different methods to encode the target text. Here the search process is a generation phase rather than a classification phase  X  the purpose is to generate alternative target texts to find which has the best encoding. The Text Mining Toolkit [16] described in the next section implements this approach, and is therefore able to address a broader range of applica tions than those covered by the tag insertion model. The Text Mining Toolkit (TMT ) is a toolkit for modeling sequential text based on text compression models and the encoding perspective described in the previous section. The toolkit is available for download at: 0.08.tar.gz Figure 1a.
 The noisy channel model The toolkit provides a powerful method for transforming text. The essential differences between quite diverse applications can be characterized by a few transforma tions that uniquely distinguish each application. These are the transformations that are performed when the search for the most probable target text (the one with the best encoding) given the source te xt is being performed during the encoding phase. These  X  X MT transformations X  can be specified by the form: source text  X  target text The toolkit uses a similar conven tion to regular expressions to represent (and match) the source and target texts specified in the transformation. For example, a Chinese word segmentation application can be specified by just two TMT transformations: this will match any symbol in the source text;  X  space symbol. In other words, the Chinese word segmentation requires one transformation to keep the symbol unchanged, and another which inserts an extra space after it. Language segmentation concerns the problem of identifying where in the text different languages occur  X  for example, documents that might contain multiple languages such as Welsh and English e.g. those produ ced by the Welsh Assembly government; or in the applica tion described here, documents which contain different  X  X tyles X  or genres of text  X  such as biomedical abstracts which cont ain text that describe gene function interspersed with othe r English text that does not. The language segmentation applica tion used in the text mining phase for this paper can be specified using the following TMT transformations: braces  X  {...}  X  these signal to the transformation process to insert a special sentinel symbol into the encodi ng stream (to terminate the coding of the prior context) after which all subsequent symbols will be coded using the new model (up until the next sentinel symbol is encountered). The two transformations above sp ecify that the language models used to encode future symbol s will switch between an English model and a Function model for every symbol in the source sequence. (These la nguage models have been trained on representative English and gene f unction text respectively). Note that for both of the applications above, the transformation  X   X   X  is required to ensure that the unalt ered source text sequence and all its subsequences will always be included in the search for the marked up sequence with the best encoding. As well as implementing these basic transformations, the toolkit provides the developer with a much more powerful way of specifying the transformations us ing method calls to directly specify the matching conditions and generated target text for each transformation. This is where th e toolkit diverges substantially from the tag insertion model of Y eates et al. [29] and arguably also the noisy channel model approach. However, space precludes describing this in further detail. For this and the Toolkit X  X  API plus sample applications, the in terested reader should visit the above URL. The language segmentation problem is easily specified in the Toolkit using the two TMT tr ansformation described above. Teahan [31] showed how a compression-based method can be applied successfully to this problem (with over 99% accuracy). The solution implemented in the Toolkit is essentially the same solution but specified within the framework of the toolkit. The toolkit also implements seve ral search algorithms which can be used here. The number of po ssible languages determines the branching factor of the search tree. If there are just two possible languages in the text (e.g. English and gene function as in Figure 2) then the search either switch es to use the other language model to encode the text by encoding a special sentinel symbol or continues to encode with the same language model. Figure 2 shows the search tree for th e language segmentation problem applied to the short text sequence  X  X h X . Within each node in the tree is the current state of the marked up sequence  X  this is represented by the use of subscripts i.e. the sequence  X  X  the node directly above the one in the bottom right of the diagram indicates that the letter  X  X  X  wa s first encoded using the Function model, then the encoding switched for the letter  X  X  X  to using the English model. Additionally, it is possible to specify Viterbi [27] and stack searching variations [31], and co mbined with the variations of language models that are possible, this provides a range of options for the experimenter to choose from to explore the best setup for this type of application. The Viterbi algorithm guarantees that the best possible segmentation will be found by using a trellis-based search  X  all possible segmentation search paths are extended at the same time and the poorer performing alternatives that lead to the same conditioning contex t are discarded. For the experiments described in section 3, we used the Viterbi algorithm, and the metric we used to decide which search paths were  X  X etter X  was the comp ression code length of the segmentation sequence in terms of the order 5 PPMD model  X  in other words, the cross-entropy of the model currently being used to encode the sequence as specified by formula (1) in Section 2.1. This step mainly involves preparing the data to be used during the training phase of the PPM approach. This involves building the models necessary to separate be tween gene function and the rest of the abstract. For this purpose, we built two models, a Function model that includes sentences that describe gene function available through GeneRIF, and an English model for which the purpose is to identify non function text that is part of the abstract. The approach we used to build the models is to leverage on the GeneRIF collection including more than 191622 functions, and use it to prepare the data. More precisely, we used a collection of abstracts that map to the GeneRI F functions we extracted from PubMed [1] collection. The co llection was divided into two distinct subsets. A small subset of the abstracts was used for testing purposes while the rest wa s dedicated to the training phase. For each pair of GeneRIF function and PubMed abstract, mapped through PMID unique identifier, we extracted the part of the abstract that does not include the GeneRIF function. The concatenation of the extracted text from all processed abstracts is used to form the basis of the English model. The basis of the Function model corresponds to the concatenation of the corresponding GeneRIF functions in the abstracts. Several variations of the data used to build the models are proposed (see section 3), adding more data to the basis of the two models; the main objective is to improve the performance (recall and precision) of the proposed technique. The PPM approach consists of tw o main steps: a training phase and a text mining phase. During the first step, models necessary to separate between differe nt text categories are constructed. This corresponds to building the Functi on and English model using the data prepared during the pre-pr ocessing phase. We used static order 5 PPMD character models as implemented by the Text Mining Toolkit. For the second pha se, the annotation of the text was performed by the markup utility also provided by the toolkit. The text mining approach adopted by this utility has been described in section 2.2. The abstracts used for testing are randomly selected from the list of abstracts having a corresponding GeneRIF function. The GeneRIF functions are used as a reference during the validation of the sentences selected by the PPM technique. Note that the validation process cannot be au tomatically performed as the GeneRIF function does not always exactly match a sentence of the abstract. In addition some functions predicted by the PPM approach are actually real gene functions even if they are not selected as part of GeneRIF (see section 3). The result of applying the PPM language segmentation technique to markup an abstract included in the testing data is a new file that includes markup tags, namely &lt;Func tion&gt; or &lt;English&gt; tags, used to delimit the parts of the abstracts that were better compressed with one of the models. Figure 3 is an extract of the output generated by running the markup utility provided by the Text Mining Toolkit. was marked up using this procedure. Since the evaluation is at the sentence level, the inclusion of certain English words within the sentence may be tolerated if the import of the statement is a gene function or its description. In the above example, both sentence 2 and sentence 3 have a reference to "mice" within them. However sentence 2 is not really a gene function whereas sentence 3 does talk about the role of a gene. Thus sentence 3 would actually be a true positive and sentence 2 -a false positive. into several parts, where each part being identified as part of an entity (English or function). This is due to the nature of the segmentation algorithm adopted by the markup utility. We chose to search for all possible segmentations of the text  X  whether into English or gene function  X  and performed on a character by character basis. That is, the util ity will find the best segmentation as measured by the minimum compression code length by swapping or not swapping between, using the two PPM models to encode each character. The ma rkup utility has the option to perform this at word boundaries using regular expression type syntax to identify the boundary. However, we chose not to use this option as it is not obvious what constitutes a  X  X ord X  boundary in texts, especially in the biomedical domain. We wished initially to keep to the simplest of approaches by adopting character-based models as well as character-based segmentation to see how well it would perform, with perhaps more complicated models and segmentation methods being adopted at a later date. The excellent results we present in Section 3 show that this decision was a good one. Since gene names and biological en tities are often composed of special character combinations and are most often abbreviations of longer frequencies better. For exampl e LFA-1, BRCA-1, TNFRSF1B are gene names and by using a char acter based encoding, we can improve the precision of identifying related entities. By using a character based approach, it would be easier to recognize sentences that contain such symbol combinations as opposed to using a direct word based approach . However, the idea of using a character based approach for the gene function and a word based approach for the English mode l seems very interesting and requires further investigation. The result of the text mining proce ss applied to each abstract is a set of extracts that are predicted either as gene function or regular text. For the purpose of this study, only the predicted gene functions extracts are consid ered. Although an abstract may actually include more than one ge ne function sentence as shown through the study of a sample set of abstracts, we proposed selecting one representative gene function. The aim is twofold. First we propose to include this appr oach as part of the efforts of GeneRIF initiative that aims to represent each article that describes a gene function by a GeneRIF function; and second, finding the best representative gene function for a paper saves researchers time, especially when sifting through the large amounts of available abstract. The proposed method we use to se lect the representative gene function leverages on the availability of the title of the paper, in addition to the abstract, and exploits its role as the representative of the paper as follows. We use the similarity between each sentence predicted by the PPM model and the title sentence to representative gene function. An example of similarity measure we deployed is based on the number of common words between the title sentence and the function sentence. The candidate with most common words with the title is selected. A preliminary step to computing the similarity valu es consists (1) of expanding function extracts detected by the text mining approach into full sentences and (2) removing stop wo rds such as prepositions (e.g. the) from both the title sentence a nd each predicted gene function. Note that the expansion process may lead to one or more sentences depending on how many sentence parts are included between the function tags. The final result of the proposed approach is a set of function sentences ranked based on their similarities to the title function. The approach we propose in this paper is heavily reliant on the PPM model. Therefore, its performance depends to a great extent on selecting the appropriate para meters of the PPM model. The main parameters include the context order, the escape method, the selection of the data used for building the models, and the processing unit (i.e. character, word or sentence based) [16, 20]. For the first two parameters, we used order five and escape method D as the two values were proven to provide the best results overall [20]. For the last parameter, we consider only the original character based approach . The other two variations will be part of future work. Therefore the central parameter we used to evaluate the performance of the approach is the data used to building the Function and English models. Two main challenges were faced when collecting the appropriate training data for the markup language models.  X  First, the richness of the text in terms of scientific words like  X  The second challenge is related to the nature of the collection Taking into consideration these conditions, we devised a conservative and incremental approach towards building the language models. Each time, we analyzed the performance results generated by the current variant of the training setup, before proposing the next variant. We stopped the enhancement process as we reached very satisfactory recall and precision ratios. More precisely we devised four variants of the training setup used to build the English and Function models, as follows: which the corresponding GeneRIF function exactly matches a sentence in the abstract. The part of the abstracts containing the gene function was used to build the Function model, and the remaining text was used to build the English model. including an abstract (with its English and Function parts) if there exists a sentence in the abstract that partially matches (we consider the first twenty char acters) the corresponding GeneRIF Function. enhanced by concatenating onto the end of the previous English text a corpus of general English text not taken from the biomedical abstracts training data . The English corpus used was the King James Bible which comes with the TMT toolkit [16] that we used to run the text mining analysis. The aim is to increase the ability to distinguish between the two types of data by decreasing and therefore controlling the impact of similarity that exists between the gene function sentence and the rest of the abstract. but we enhanced the size of gene functions data by including all gene functions available in GeneRIF except for the ones that are part of the testing data. results from previous experiment s, we applied a post-processing phase to the results that were obtained, to select for each abstract a representative gene function from the list of candidate gene function returned by the TMT toolkit (see section 2.6). developed by W. Teahan [16] based on an earlier API [33]. We used the train utility to build the data models and the markup utility to annotate the abstracts using the English and Function models generated by train utilit y. The same static order five character PPMD models (i.e. usi ng escape method D) were used for training and testing purposes. The markup utility takes a list of models as input, and applies th e language segmentation solution described in Section 2.3 using the Viterbi search option. We applied the text mi ning software to the following two tasks:  X  Task I: The purpose of this task was to test the ability of the model to identify gene functi ons as annotated by NCBI X  X  GeneRIF. That is, while abstract content may include more than one gene function, the GeneRIF contains for each abstract the  X  X ain X  or  X  X ost representative X  ge ne function as annotated by the curators.  X  Task II: The purpose of this task was to consider the ability of the method to identify any gene function in the abstract including the one selected as part of GeneRI F. In order to perform this task, a manual annotation of all gene f unctions in the test data was performed a priori as GeneRIF functions are just a subset of the target gene functions. Independent of the task, the results for Variant#1 are quite poor  X  this was expected as this is the simplest of the models, where only the abstracts for which there was an exact match of GeneRIF function to a function in the text were used. The size of the training data is therefore an important parameter towards boosting the results performance of the pr oposed approach as supported by the other variants results (starting from variant#2). Table 1: Task I results  X  identi fication of GeneRIF functions Variant#1 58/91 58/100 63.76 58.00 Variant#2 79/115 79/100 68.69 79.00 Variant#3 89/120 89/100 74.17 89.00 Variant#4 97/125 97/100 77.60 97.00 Variant#5 93/95 93/96 97.89 96.87 Interestingly, there is a big jump in both recall and precision between Variant 2 and 3 where the text from the King James X  Version of the Bible was added to the training data used to train the English model. This is surpri sing in that this text is not representative of mode rn English usage. Table 2: Results for Task II  X  Id entification of gene functions Method 
Descripti on Variant#1 72/103 72/142 69.90 50.70 Variant#2 105/125 105/142 80.00 73.94 Variant#3 120/135 120/142 88.90 84.50 Variant#4 134/158 134/142 84.81 94.36 Compression experiments have shown that a model built from this text performs poorly at compressi ng modern English text. Despite this, there is a significant improvement indicating that even better results could be achieved in esta blishing the difference between gene function and the rest of th e abstract with more standard balanced English corpora such as the Brown Corpus. 
Figure 4: Comparison of precision values for tasks 1 and 2 When increasing the size of the Function model as in the variant#4 experiment, we notice that the recall increased by the same order for both tasks (9% for task 1 and 9.86 for task 2). On the other hand, the precision has s lightly decreased. This confirms the tradeoff that exists between r ecall and precision, which in this method is manifested in the build ing of the English and Function models. Figure 5: Comparison of reca ll values for tasks 1 and 2 across Overall, the proposed approach ha s performed slightly better at identifying gene functions in genera l, when compared to its ability to identifying the  X  X ost representative X  gene function (figure 4). On the other hand, this will generate more noise in the results data (figure 5). Furthermore, as the aim for task 1 is to generate one representative gene function to be eventually part of GeneRIF, by performing a post-processing phase as in variant#5 (see section 2.6), the precision has drama tically improved (table 1). While GeneRIF is considered as a high quality subset of annotated sequences, it includes only a very small subset of gene sequences hosted by NCBI. The new annotation approach we propose in this paper, based on the PPM compressi on technique and refined by a similarity based approach, has pr ovided very satisfactory results both in terms of recall and precision. Therefore, the proposed method shows great potential to be used as a means for annotating NCBI gene sequences with PubM ed publication; thus increasing the coverage of GeneRIF collection with high quality annotations. 
Encouraged by these results we are currently working on extending the application of the proposed method to the extraction of other entities and features, such as protein-protein interaction. PubMed; http://www.ncbi.nlm.nih.gov/entrez/ (2008). [2] Fukuda, K., Takagi, T. (2004)  X  X  Pathway Editor for [3] Bickel, S. , Brefeld, U., Faul stich, L., J X rg , et al. (2004) [4] Ono T, Hishigaki H, Tani gami A, Takagi T. (2001) [5] Mika, S., Rost, B. (2004)  X  X ro tein names precisely peeled off [6] Hanisch, D., Fluck, J., Me vissen, H. and Zimmer,R. (2003) [7] Yamamotoy, K., Kudoz, T. K onagayay, A., Matsumoto, Y. [8] Chang, J.T., Sch X tze, H, Altman, R.B. (2004)  X  X APSCORE: [9] Koike A, Niwa Y, Takagi T. (2005)  X  X utomatic extraction of [10] Chiang, J.H., Yu, H.C. (2003)  X  X eKE: Discovering the [11] Raychaudhuri S., Chang J.T. , Sutphin P.D., Altman R.B. [12] Seki, K., Mostafa, J. (2003)  X  X owards database curation in [13] Gene ontology. (2007) http://www.geneontology.org/ . [14] NCBI. (2008) http://www.ncbi.nlm.nih.gov/. [15] GeneRIF.(2007) [16] Teahan, W.J. (2006) The Text Mining Toolkit . [17] Cleary, J and Witten, I. (1984)  X  X ata compression using [18] Moffat, A. (1990)  X  X mplementing the PPM data compression [19] Teahan, W. J., &amp; Harper, D. J. (2003)  X  X sing compression-[20] Teahan, W.J. (1998)  X  X odelling English Text X , Ph.D. thesis, [21] Bell, T., Cleary, J. and Witten, I. (1990) Text Compression . [22] Howard, P.G. (1993) The Desi gn and Analysis of Efficient [23] Witten, I.H., Bray, Z., Mahoui, M. &amp; Teahan ,W.J. (1999) [24] Knight, K. (1999)  X  X ining on-line text X . Comm. of the ACM , [25] Jelinek, F. (1985)  X  X elf-organized Language Modeling for [26] Shannon, C.E. (1948)  X  X  mathematical theory of [27] Viterbi, A.J. (1967)  X  X rror bounds for convolutional codes [28] Yeates, S. and Witten, I.H. (2000)  X  X n tag insertion and its [29] Yeates, S., Bainbridge, D. and Witten, I.H. (2000)  X  X sing [30] Teahan, W. J., &amp; Harper, D. J. (2001)  X  X ombining PPM [31] Teahan, W. (2000)  X  X ext Classification and Segmentation [32] GeneRIF help. (2007) [33] Cleary, J.G. and Teahan WJ (1999)  X  X n Open Interface for 
