 Michael Piotrowski (Leibniz Institute of European History) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 17), 2012, ix+157 pp; paperbound, ISBN 978-1608459469 Reviewed by Laurent Romary Inria &amp; Humboldt University, Berlin The publication of a scholarly book is always the conjunction of an author X  X  desire (or need) to disseminate their experience and knowledge and the interest or expectations of a potential community of readers to gain benefit from the publication itself. Michael Piotrowski has indeed managed to optimize this relation by bringing to the public a compendium of information that I think has been heavily awaited by many scholars having to deal with corpora of historical texts. The book covers most topics related to the acquisition, encoding, and annotation of historical textual data, seen from the point of view of their linguistic content. As such, it does not address issues related, for instance, to scholarly editions of these texts, but conveys a wealth of information on the various aspects where recent developments in language technology may help digital humanities projects to be aware of the current state of the art in the field. on the experience acquired by the author within the corpus development projects he has been involved in, and reflects in particular the specific topics on which he has made more in-depth explorations. It is thus written more as a series of returns on experience than a systematic resource to which one would want to return after its initial reading. the book and provides an overview of the reasons why natural language processing (NLP) has such an entrenched position in digital humanities at large and the study of historical text in particular. Citing several prominent projects and corpus initiatives that have taken place in the last few decades, Piotrowski defends the thesis, which I share, that a deep understanding of textual documents requires some basic knowledge of language processing methods and techniques. Chapter 2 in particular ( X  X LP and Digital Humanities X ) could be read as an autonomous position paper, which, independently of the following chapters, presents the current landscape of infrastructural initiatives and scholarly projects that shape this convergence between the two fields.
 lated to spelling variations in historical text. It shows how difficult it may be to deal with both diachronic (e.g., in comparison to modern standardized spellings) and synchronic (degree of stabilization of historical spellings) variations, especially in the context of the uncertainty brought about by the transcription process itself. This is particularly true for historical manuscripts and Piotrowski goes deeply into this, showing some concrete examples of the kind of hurdles that a scholar may fall into. This is the kind of short introduction I would recommend for anyone, in particular students, wanting to gain a first understanding in the domain of historical spelling.
 52) and covers various aspects of the digitization workflow that needs to be set up to create a corpus of historical texts. The chapter is quite difficult to read as a single unit because of its intrinsic heterogeneity. Indeed, it covers quite a wide range of topics: presentation of existing digitization projects worldwide, technical issues related to scanning, comparison of various optical character recognition systems for various types of scripts, the potential role of lexical resources, crowdsourcing for optical character recognition (OCR) post-processing, and manual or semi-automatic keying. Getting an overview of the various topics is even more difficult because of the way the author has followed his own personal experience, and alternates between general considerations and in-depth presentations of concrete results. Pages 34 X 40, for instance, is one single subsection on the comparison of OCR outputs that goes into so much detail that it breaks the continuity of the argument, although in itself this subsection could be really interesting for a specialized reader. This chapter illustrates the point that the content of this book would benefit from being published in a more modern and open setting.
Annotation Schemes, X  pp. 53 X 68), which tackles two specific issues, namely, character and document encoding. On these two, the author presents what could be considered best practices. For character encoding, the book rightly focuses on the advantages that the move towards ISO 10646/Unicode has brought to the community. The corresponding sub-section actually covers three different aspects: It first makes an extensive presentation of the history of character encoding standards (from ASCII/ISO 646 to Unicode/ISO 10646), it provides insights into the current coverage and encoding principles (e.g., UTF-8 vs. UTF-16) of ISO 10646, and finally, it focuses on the specific difficulties occurring in historical texts both from the point of view of legacy ASCII-based transcription languages and the management of characters that are not present in Unicode. Although well documented, these three topics should have been more clearly separated so that readers interested in one or the other could directly refer to it. This is a typical case where, given the great expertise of the author on the subject,
I can imagine the corresponding texts being published on-line as separate entries in a blog. The second half of the chapter focuses on the role of the Text Encoding Initiative (TEI) guidelines for the transcription and encoding of historical text. It covers the various representation levels that may be concerned (metadata, text structure, surface annotation) and insists on the current difficulty of linking current NLP tools to TEI encoded documents. Although this is indeed still an issue in general, it might have been interesting to refer to standards (ISO 24611 X  MAF) and initiatives (Textgrid core encoding at the token level; the TXM platform for text mining) that have started to provide concrete sustainable answers to the issue.
 of short studies describing possible methods for dealing with OCR errors or spelling variations as described in Chapter 3. Independent of the fact that I find it strange to see the two chapters set quite far from one another, Chapter 6 distinguishes itself by its profound heterogeneity. Whereas several sections do have the most appropriate level of detail and topicality for historical texts (in particular those on canonicalization), some sections seem to be completely off topic (Section 6.2,  X  X dit Distance, X  describes what I would consider as background knowledge for such a book). It is all the more disappointing that the author shows here a very high level of expertise and, as in the 232 case of Chapter 3, I would strongly recommend the reading of the relevant sections to newcomers in the field.
 Languages, X  pp. 85 X 100) is more coherent and focused. It mainly addresses the morpho-syntactic analysis of historical text and presents, through concrete deployment scenar-ios, possible methods to constrain the appropriate parsers, in a context where hardly any existing tools can be simply re-used. The chapter is very well documented and refers to most of the relevant initiatives in the domain of morphology for historical text, at least on the European scene. This focus may also be misleading because recent work on named entity recognition on historical texts are not at all mentioned and are probably, to my view, one of the most promising direction for enhanced digital scholarship. language, of the major historical corpora available worldwide. It shows the dynamic that currently exists in the community and is an essential background resource to both understanding who is active in maintaining historical corpora and discerning the most relevant resources. The chapter as a whole provides an interesting  X  X istorical X  per-spective on the progress made by most text-based projects in using the TEI guidelines as their reference standard. It seems quite difficult now to imagine an initiative which would not take TEI for granted, and would not build inside the TEI framework. On another issue, namely, copyright, Piotrowski also provides an interesting analysis on the difficulty of re-using old editions which have been recently re-edited on paper, and thus fall into some publisher X  X  copyright restrictions. The conclusion could have been a little tougher here though, and probably should have recommended putting a hold on any paper publication of historical sources by a private publisher unless it is guaranteed that the electronic material can be used freely, under an appropriate open license. disappointment. Enthusiasm, because the content is so rich that it should serve as back-ground reference (and indeed be quoted) for any further work on the creation, manage-ment, and curation of historical corpora. Still, I cannot help thinking that the editorial setting as a book is not the most appropriate setting for such content. The variety of topics that are addressed as well as the heterogeneous level of detail provided through the different chapters would benefit from a more fragmented treatment. Indeed, this blog such as those on the hypotheses.org platform) which in turn would allow an interested reader to discover exactly the topics they want information about and cite the corresponding entries. With the bibliography in Zotero and relevant pointers to the corresponding on-line corpora or tools, I could imagine the resulting content soon becoming one of the most cited on-line resources. I am sure the author would gain more visibility in doing so than having the material hidden on a library shelf or behind a paywall. Not knowing the exact copyright transfer agreement associated with the book, I cannot judge if it is too late for the author to think in these terms, but this could be a lesson for scholars who are now planning to write such an introductory publication. Is the book still the best medium?

Pierre M. Nugues (Lund University, Sweden) Springer (Cognitive technologies series, edited by A. Bundy et. al), 2014,
Second Edition, xxv+662 pp; hardcover, ISBN 978-3-642-41463-3, $89.99; ebook, ISBN 978-3-642-41464-0, $69.99; doi 10.1007/978-3-642-41464-0 Reviewed by Wei Lu Singapore University of Technology and Design
Although Prolog was intended to be used in building natural language processing (NLP) applications, the language has often been neglected in many modern NLP intro-ductory courses and texts. Because of the popularity of machine learning and statistical approaches to language processing problems over the past decades, researchers and practitioners tend to use other modern programming languages such as C++ and Java for developing NLP applications. However, with a growing interest in semantic process-ing and knowledge base construction in recent years, researchers have found Prolog to be an indispensable tool for tasks such as searching databases and performing symbolic reasoning. We have already seen recent successful deployment of Prolog-based NLP systems in industry. An example would be the use of Prolog in IBM X  X  DeepQA project to express pattern-matching rules. 1 Pierre M. Nugues X  comprehensive textbook Language Processing with Perl and Prolog provides a timely, in-depth introduction to the field of
NLP using Prolog and, to a lesser extent, Perl. This book is suitable for anyone wanting to enter NLP through learning to p rototype NLP modules in Prolog. moves on to concepts such as words, syntax, and semantics, and concludes with a discussion on the more advanced and open topics of discourse and dialogues.
 and 3 then provide a technical discussion of corpus processing and data representations. cepts of entropy and perplexity are introd uced first. They are followed by a discussion on the learning of a decision tree model. One minor quibble is that general machine learning concepts such as supervised and unsupervised learning are presented under the subsection  X  X achine Learning X  within  X  X  ntropy and Decision Trees. X  A brief intro-duction to  X  X inear Models X  is given before moving on to regression and classification models, including perceptrons, support vect or machines, and logistic regression. I liked the way that such concepts were elucidated X  X  thought the use of automatic language detection as an example to illustrate how the different models worked was a nice touch. Although advanced readers might find this section unsatisfying since it does not cover more sophisticated models and algo rithms, I believe this section is ideal for beginners, as it covers the most fundamental and essential topics for computational linguistics.
 mental issues related to word-level processing, including how to count the words in sentences, tokenization issues, and how to model word sequences. Next, the concepts of parts of speech (POS), lexicon, and morphology are introduced in Chapter 6. An in-depth treatment of morphology is also given in this chapter, which includes a review of finite state automata and finite state transducers. The chapter mainly focuses on
European languages, but I believe addition al discussions on other language families would be helpful. Chapter 7 presents rule-based techniques for performing POS tag-ging. In this chapter, Nugues describes Br ill X  X  tagger, one of the earliest successful approaches to POS tagging. Recent developm ents in multilingual POS tag sets are also introduced. In Chapter 8, statistical approaches to POS tagging are discussed. Methods based on noisy-channel models, hidden Markov models (HMMs), perceptrons, and decision trees are presented. HMMs, in particular, are illustrated with detailed inference and decoding algorithms and well-chosen examples. Although HMMs are influential graphical models, I feel that its discriminat ive counterpart, linear-chain conditional random fields (CRFs), which are now widely used in the NLP community, could also be covered in the chapter. At the end of the chapter, two applications of the noisy-channel model are given. One is on spe llchecking, and the other on machine translation. The treatment of machine translation in this example constitutes most of the discussions of this topic in the text.
 of the definite clause grammar (DCG) used in Prolog to define grammars. DCG is then linked to phrase-structure rules used fo r syntactic parsing of natural languages. Extensive Prolog code examples on parsing based on DCG are presented in this chapter.
A small section in Chapter 9 is devoted to the interesting topic of semantic parsing, where the  X  -calculus as a semantic formalism is i ntroduced and its implementation discussed. The topics of the next few chapters include partial parsing, constituency parsing, and dependency parsing.

In this chapter, Nugues explains the many concepts related to information extraction, with a focus on named entities and multiwor d expressions. Real examples taken from the CONLL shared tasks are used when introducing such concepts.
 troduction to Chomsky X  X  ideas on constituen cy parsing, followed by unification-based grammars. Next, dependency grammars are introduced in detail. At the end of the chapter, the connections between consti tuency grammars and dependency grammars are highlighted.
 as the CYK algorithm and the Earley algori thm are discussed. These algorithms are treated in a non-probabilistic manner and are illustrated extensively in Prolog. How-ever, not much is written about probabilistic constituency parsing apart from the sub-section  X  X dding Probabilities to the CYK Parser, X  which contains no code examples.
Nugues also writes about lexicalized PCFG pa rsing algorithms, with special attention given to Charniak X  X  parser. In his discu ssion of dependency parsing in Chapter 13,
Nugues covers Nivre X  X  shift-reduce style parser, Covington X  X  non-projective parser, as well as Eisner X  X  cubic-time projective parser. 698 and 15. The former focuses on formal semantics and the latter on lexical semantics. Chapter 14 starts with predicate logic, which can be naturally implemented with Prolog.
The  X  -calculus expressions are then revisited, followed by a wide range of technical issues related to representing words and phrases with logical forms. The issue of us-ing semantic representations to interact wi th databases for question-answering is also covered in this chapter. The area of semantic parsing X  X apping natural languages to their semantic representations or denotations X  X as recently received a lot of attention.
Although some recent developments in the field are not covered in these sections, the discussion should already serve as a good introduction to readers interested in this growing area.
 are discussed. Some useful resources such a s WordNet, FrameNet, and Propositional Bank are also introduced here.
 ing beyond the sentence level. Chapter 16 f ocuses on discourse analysis. Topics under this theme include coreference, rhetoric, as well as event and temporal information processing. The final chapter focuses on the more advanced topic of dialogues. This can be regarded as a shift from a single, static monologue to dynamic, interacting dialogues.
Many open issues remain to be addressed on such a topic, but some discussions on building simple dialogue systems are given in this chapter.
 contained in the Appendix is necessary to understand the examples within the text, itmightbebettertohaveitatthefrontofthebook.
 adequate depth and breadth. The book presents a nice synthesis of actionable code and theory and its well-considered structure allows different concepts in NLP to be strung together. This text would serve as a solid stepping-stone for NLP beginners, and is therefore suitable for graduate or senior undergraduate students who are interested in entering into the field. It would also serve as an excellent reference for researchers as well as a good guide for practitioners who are i nterested in building NLP applications.
 Bing Liu (University of Illinois at Chicago) Cambridge University Press, 2015, 381 pp.; hardcover, ISBN 9781107017894, $80 Reviewed by Jun Zhao, Kang Liu, and Liheng Xu Institute of Automation, Chinese Academy of Sciences With the increasing development of Web 2.0, such as social media and online businesses, the need for perception of opinions, attitudes, and emotions grows rapidly. Sentiment analysis, the topic studying such subjective feelings expressed in text, has attracted significant attention from both the research community and industry. Although we have known sentiment analysis as a task of mining opinions expressed in text and analyzing the entailed sentiments and emotions, so far the task is still vaguely defined in the research literature because it involves many overlapping concepts and sub-tasks. vagueness and define various directions and aspects in detail, especially for students, scholars, and developers new to the field. In fact, the field includes numerous natural language processing tasks with different aims (such as sentiment classification, opinion information extraction, opinion summarization, sentiment retrieval, etc.) and these have multiple solution paths. Bing Liu has done a great job in this book in providing a thorough exploration and an anatomy of the sentiment analysis problem and conveyed a wealth of knowledge about different aspects of the field. contributions to the understanding of opinions and sentiments expressed in text, but he also has significantly influenced the design of real-life sentiment analysis algorithms and the building of practical sentiment analysis systems. This book has at least three significant merits and meets the needs of different types of readers. 1. First, it is praiseworthy that the book gives detailed definitions of opinions 2. This book not only presents the main sub-tasks of sentiment analysis, 3. It is also commendable that the book gives a balanced treatment of both and define the sentiment analysis problem. Chapters 3 X 9 discuss the core sentiment analysis tasks (e.g., sentiment classification, aspect analysis, and opinion summariza-tion) and their current solution methods. Chapters 10 X 13 investigate the emerging themes from recent research and applications (e.g., analysis of debates, intentions, fake opinions, and review quality).
 describes the expression of sentiment as one of the most important and complicated phenomena of human language. The goal of sentiment analysis is to computationally extract sentiments, opinions, and emotions expressed in text, which is different from the goal of traditional linguistic studies aimed at understanding the human language. Tech-nically, analysis of sentiment can be divided into several levels according to different 596 discourse granularity, such as document, sentence, and aspect or sentiment-and-target levels. Liu took a structured approach to write this book.
 key concepts such as subjectivity, affect, emotion, and mood. An opinion can either be defined as a quadruple or quintuple, and sentiment involves the type, orientation, and intensity about an opinion. Many succinct examples are also given to make the concepts easily understandable.
 most studied problem in sentiment analysis. It usually involves direct applications of (supervised or unsupervised) machine learning algorithms. Although the problem is called sentiment classification, it could also be taken as a regression problem. Most sentiment classification approaches categorize documents into only positive, negative, and objective types, whereas emotion classification involves more categories with over-lapping meanings and is more difficult to perform accurately.
 cations. More elaborate analysis is applied to sentence and aspect levels, as introduced in Chapters 4 and 5. These two problems are the most practically useful research topics of sentiment analysis. In particular, aspect-level analysis forms the core of applications of sentiment analysis as it aims to identify the atomic unit of information contained in sentiment, opinion, and emotion expressions, which is the pair of sentiment and its target. For sentiment identification, the book shows that beyond positive and negative orientations of sentiment expressions, many sophisticated language phenomena also need to be considered in analysis (e.g., conditional expressions, sarcasm, sentiment composition, and negation). To deal with such sophisticated language phenomena, term-level features are no longer sufficient. One has to incorporate deeper knowledge of syntax, semantics, and discourse. Although significant efforts have been made, the problem is still far from being solved.
 respectively. The two topics are inter-related because sentiment lexicons are often used to express opinions on aspects or targets. Therefore, many researchers have studied the two tasks together. Furthermore, aspects and sentiment expressions also have common characteristics, such as, for example, the fact that they are both domain-specific. Although the two problems are challenging, these chapters provide valuable resources and practical algorithms for their solutions.
 challenging because of the flexible usage of comparatives and the difficulty of identify-ing the preferred entity set. Limited research attention has been paid to this problem, but it is very useful in applications.
 ion summarization is quite different from conventional single-document or multi-document summarization, because it is centered on opinion targets and produces quantitative sentiment ratings for targets. The output of opinion summarization can be interactively visualized, which gives really interesting and easily understandable results that can be seen by users in a single glance.
 study of debates, intentions, deceptive opinions, and quality of reviews. These are emerging themes from recent research. We believe there will be significant research activities and applications on these topics in the years to come.
 with in-depth discussions of linguistic phenomena related to sentiments, opinions, and emotions. Although many sentiment analysis methods are based on machine learning as in other NLP tasks, sentiment analysis is much more than just a classification or regression problem, because the natural language constructs used to express opinions, sentiments, and emotions are highly sophisticated, including sentiment shift, implicated expression, sarcasm, and so on. Liu has described these issues and problems very clearly. Readers will find this book to be inspiring and it will arouse their interests in sentiment analysis.

 Partha Niyogi (University of Chicago) The MIT Press, 2006, xviii+482 pp; hardbound, ISBN 0-262-14094-2, $42.00,  X 27.95 Reviewed by Tony Belpaeme University of Plymouth Darwin already remarked that evolutionary thinking also applies to the study of lan-guage. Language is heritable, in the sense that the language of offspring will likely resemble that of the parents, and during language learning variation is inevitably introduced. If on top of this a selection mechanism is operating that allows individuals using a particular language to have more descendants, some languages are more likely to spread through the population than others. The evolutionary nature of language change has been extensively studied in diachronic or historical linguistics, but in The Computational Nature of Language Learning and Evolution Niyogi takes a fresh approach by providing a formal study of evolutionary language change. In this he focuses on the population instead of on the individual language users, and provides a thorough analysis of the population dynamics resulting from individuals learning and using a language.
 analysis. The expertise built up in theoretical biology, game theory, information theory, statistical physics, and complex systems research seems to be particularly well-suited to study and report on language dynamics at a macroscopic level. In addition, these disciplines rely on a set of trusted analytical tools that can be employed to study dynamical aspects of language learning and evolution.
 grammar. For this the language learner uses a particular learning mechanism, and the first part of the book is concerned with the  X  X ogical problem of language acqui-sition X  (Gold 1967), and a number of formal learning mechanisms are presented that will be used in later chapters. Niyogi makes no commitment to the representation of grammars. Grammars can be generative grammars, but can equally be phonological rules, probabilistic grammars, sentence X  X eaning pairs, or any other combinatorial and compositional structure. However, he does assume that the learner has some sort of bias in its learning mechanism, a universal grammar if you will, that constrains the grammar acquisition process so that a stable language is maintained in a community.
 including the memory-less learner, which bases its next hypothesis only on its current hypothesis and the current sentence, and the batch learner, which waits until all exam-ple sentences have been received and then chooses the most likely hypothesis. If the learner is exposed to more example sentences, the hypothesis of the learner will home in on the target grammar of the teacher. Only if an infinite number of sentences are presented will the learner be able to acquire the exact same grammar as its teacher. In reality, no child ever hears an infinite number of sentences, so it is bound to learn a grammar that varies slightly from that of its caretakers. Exactly this variation drives diachronic change in languages. From this very simple formal framework, it already follows that language-learning in a population exhibits complex dynamics. These might result in chaos, but more often than not in the case of language, individuals chaotic regimes typically steer clear of. If bifurcations do arise from the non-linear dynamics of language acquisition, these might help explain major transitions in language, such as rather abrupt changes in word-order structure.
 stability. If language learners form a distributed system without coordination, how can coherence in the language arise and how can it be maintained? Niyogi shows that coherence does not imply that all learners need to have the same grammar; they only need a grammar that produces sentences that are correctly interpreted by others.
Language change, as with most changes in a large population, has often been believed to follow a logistic S-shaped curve. However, the nature of the change depends, among other things, on the learning algorithm and distributional assumptions of sentences offered to learners. Niyogi shows how under certain circumstances language change diverges from the typical logistic curve, and populations end up in alternative absorbing states of the dynamical system. He explores a wide range of parameter settings of his system, changing the number of languages in the population, the structure of the population, and introducing maturation in the population. Language change is highly non-linear, and a slight variation in a parameter setting can result in very different evolutionary trajectories. In Chapter 7 this framework is applied to the historical change of the placement of clitics in Portuguese, and Chapter 8 does the same for phonological change in the Wu dialects in China. Interestingly, Niyogi fails to explain how the phono-logical change occurred, but at same time X  X hrough presenting possible evolutionary scenarios X  X anages to discount a number of alternative hypotheses for the change. language is transmitted from generation to generation. Chapter 9 extends the formal models to handle horizontal, or cultural, evolution. These are built upon the theories of Cavalli-Sforza and Feldman (1981), in which a set of cultural traits with a limited number of parameter settings is passed on between individuals. Niyogi explores the competition between linguistic traits and illustrates this with a study of the evolution of English syntax from Old over Middle to Modern English. Chapter 10 explores some variations on the model. Until now Niyogi considered only infinite populations, making the models X  predictions insensitive to interaction dynamics. Having finite populations and a spatial organization between individuals introduces stochasticity in the inter-action dynamics, which X  X lthough more realistic X  X akes it more difficult to make generalizing conclusions.
 ular with the conditions under which communication systems are selected for. Niyogi defines a measure for the fitness of an individual, the communicative efficiency, which measures how mutually intelligible a language is. Interestingly, in a case study using phonemic contrast in English, where it is shown that there is quite some perceptual confusion in English, Niyogi warns that communicative efficiency might not have been very important in the evolution and origins of language. Chapter 12 is an extended version of Nowak, Komarova, and Niyogi (2001), studying populations of interacting linguistic agents where communicative efficiency forms a selective Darwinian pressure.
It is formally shown that the learning fidelity in the model, which is the probability of a child learning the parent X  X  language, should be sufficiently high for coherence to emerge in a population. If the fidelity is not high enough, the population will not end up speaking one majority language and linguistic communication will not take off. 430 Interestingly, Niyogi remarks that this predicts that having only one or a few languages is optimal, but given the many thousands of languages spoken today, this result is rather counterintuitive. In Chapter 13, Niyogi looks at circumstances under which linguistic coherence might emerge without natural selection on linguistic performance. Instead, social learning is used, and individuals now learn from the entire population, as opposed to learning from a single parent as in previous models. It is shown that under a large range of conditions, the evolutionary trajectory bifurcates and the population ends up speaking one language. An interesting insight is that if a learner only takes input from a single parent, natural selection is needed on communicative fitness to allow a shared linguistic system to emerge (this might, for example, be the case for bird song). However, if the learner receives input from a community, natural selection is not a necessary requirement for linguistic coherence to emerge.
 ments in history where linguistic evolution suddenly takes a sharp turn. These bifurca-tions result from the nonlinear interactions between language users, and could explain major transitions in linguistic history. Niyogi also points to future directions for this type of work, mostly involving more complicated versions of the current models. language evolution has also received equal if not more attention. Whereas formal analysis allows insights in the macroscopic regime of an evolutionary system X  X hrough, for example, state-space analysis X  X omputational modeling allows for more realistic models (albeit still rather simple compared to the actual phenomena that are being modeled). Niyogi for the most part prefers formal analysis, and although this requires abstraction to the extreme, the book clearly shows how this can still provide intriguing insights. The main reason for this is that formal analysis (and computational modeling) brings issues into focus in a way that verbal arguments cannot. Niyogi uses the language of statistical physics to give an evolutionary account of language change and language origins, which for some readers might be a little daunting. However, an effort has been made to reach out to different disciplines and together with a peppering of practical examples, The Computational Nature of Language Learning and Evolution will not only be of interest to researchers modeling the evolution of language, but also deserves attention from adventurous historical linguists.
 References
 Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch  X  utze (Stanford University, Yahoo! Research, and University of Stuttgart) Cambridge: Cambridge University Press, 2008, xxi+482 pp; hardbound, ISBN 978-0-521-86571-5, $60.00 Reviewedby OlgaVechtomova UniversityofWaterloo IntroductiontoInformationRetrieval by Manning, Raghavan, and Sch  X  utze is an up-to-date, thorough, and systematic introduction to information retrieval (IR) from a com-puter science perspective. Written as a textbook, its main audience is graduate and senior undergraduate students taking IR courses. The book will also be valuable to researchers in other computer science fields, such as computational linguistics, as well as to professional practitioners wishing to delve into the IR field.
 mation retrieval, starting with the fundamentals (such as Boolean retrieval, document indexing, vector-space model, and evaluation in IR) and moving on to more advanced topics (such as probabilistic models, XML retrieval, text classification, machine learning for IR, document clustering, and Web retrieval). Pedagogical features of the book in-clude short exercises at the end of each section and brief overviews of related research literature at the end of each chapter. Some of the major strengths of the book are its accessibility, clarity, and good balance between theory and practice. There are many concrete examples throughout the book that facilitate understanding of complex topics. topics in IR, it largely bypasses two important subjects, in my opinion: natural language processing techniques in IR and interactive information retrieval. Although the authors refer to some research done in these areas in various chapters, they do not give them the same thorough treatment given to other topics in the book. To compensate, in the preface the authors provide references to the detailed coverage of these and some other topics in other textbooks. It also might have been useful if the authors introduced some specialized IR tasks, such as opinion retrieval or enterprise search, which might benefit from more advanced NLP techniques.
 such as term, index, document, query, recall, precision, and so on. It outlines the main principles of Boolean retrieval, briefly criticizes it, and compares it to ranked retrieval. The authors also present a good real-world example of a commercial Boolean retrieval system.
 indexing process that include tokenization, stemming and lemmatization, stopwords removal, and approaches to dealing with phrases at the indexing stage, namely bi-gram indexing and the use of positional indexes. In this chapter the authors discuss some linguistic aspects of these processes. For example, when examining tokenization, they discuss various morphological and other aspects of languages that complicate this process (e.g., hyphenation in English, compound nouns in German, and word sequences in East Asian languages). A brief outline of word segmentation approaches is provided. The authors also give a very good summary of the key relevant research works in these areas at the end of the chapter.
 struction, and compression. These three chapters will most likely be of least interest to the computational linguistics community. However, two topics that might be of interest are the use of wildcard queries by users and spelling correction of queries, discussed in Chapter 3. In their discussion of wildcard queries, the authors examine only the use of wildcards in queries to represent different morphological variants of a word (e.g., American vs. British), the user X  X  uncertainty in the correct spelling of a word, and stemmed words (e.g., judicia * to represent both judicial and judiciary ). It would have been interesting if the authors also discussed the use of wildcards to represent entire words, since some commercial search engines started to provide this functionality, allowing users to search for a phrase with a user-specified number of words in the middle (e.g., the use of fine*me to represent finebyme , finewithme ,and fineforme ).
 frequency and inverse document frequency, and the vector-space model. The authors briefly touch upon phrase queries, and how they can be handled by the vector-space model. Phrase or proximity-based retrieval is an important problem in IR, but unfor-tunately, the authors do not discuss in detail different approaches to proximity-based term weighting.
 works, such as the Text Retrieval Conference (TREC) and classical evaluation measures, such as mean average precision, precision at different cutoff points, as well as the more recently developed measure NDCG (normalized discounted cumulative gain) for evaluation with graded (non-binary) relevance judgments. The penultimate section in the chapter discusses various approaches to presenting retrieved documents in the ranked list shown to the user, such as snippets, and query-independent and query-biased document summaries.
 following relevance feedback is one of the most effective techniques in IR. The authors provide an overview of the main types of QE: local , whereby the query is modified on the basis of retrieved documents, and global , which is query independent. Among the local methods, they introduce here the classic Rocchio algorithm. Probabilistic approaches to query expansion following relevance feedback are discussed in detail in Chapter 11 after the authors introduce probabilistic models of IR. Query expansion following relevance feedback can be either automatic (AQE), whereby the system selects terms and adds them to the query, or interactive (IQE), whereby the selected terms are shown to the user for further selection. The authors only discuss AQE in the context of relevance feedback. Among the global QE methods, they mention the use of manual and automatically generated thesauri, as well as approaches to QE on the Web, such as suggestion of related queries.
 for XML retrieval and INEX (Initiative for the Evaluation of XML retrieval), the main evaluation framework for XML retrieval.
 eling, respectively. Chapter 11 introduces the theoretical underpinnings of probabilistic IR models, and describes the Robertson and Sp  X  arck Jones probabilistic model and the
BM25 term weighting function. Chapter 12 starts by describing the basic approach to language modeling in IR and then reviews some of its extensions. 308 Bayes classification, and then moving on to vector-space classification and support vec-tor machines. All topics are presented in sufficient detail, supplemented with references to the key papers in these areas.
 methods ( K -means and expectation maximization) and clustering evaluation methods are discussed in Chapter 16, and Chapter 17 is devoted to hierarchical clustering. Here, the authors present different agglomerative clustering algorithms, such as single-link, complete-link, group-average, and centroid similarity, as well as top X  X own (divisive) hierarchical clustering. An important problem in clustering is the labeling of clusters. The authors discuss and compare two approaches to labeling: differential cluster la-beling, where label terms are selected on the basis of their distribution in one cluster compared to the others, and cluster-internal labeling, where a label is selected only on the basis of the cluster being labeled.
 and readers might have benefited from a more extensive discussion of the use and practical applications of LSI.
 discussed are spam, types of user information needs, Web crawling and indexing, link-based approaches to document ranking such as PageRank, Markov chains, and hubs and authorities.
 and well-written overview of the main topics in IR. The book offers a good balance of theory and practice, and is an excellent self-contained introductory text for those new to IR. Although the book does not cover advanced NLP techniques for IR, it is recommended for experts in computational linguistics who wish to learn about IR. Although many computational linguists are familiar with the material covered in the chapters on text classification, they will most certainly find chapters on different IR models and methods very useful.


Daniel Jurafsky and James H .Martin (Stanford University and University of Colorado at Boulder) Pearson Prentice Hall, 2009, xxxi+988 pp ;hardbound, ISBN 978-0-13-187321-6, $115.00 Reviewed by Vlado Keselj Dalhousie University
Speech and Language Processing is a general textbook on natural language processing, with an excellent coverage of the area and an unusually broad scope of topics. It includes statistical and symbolic approaches to NLP, as well as the main methods of speech processing. I would rank it as the most appropriate introductory and reference textbook for purposes such as an introductory fourth-year undergraduate or graduate course, a general introduction for an interested reader, or an NLP reference for a researcher or other professional working in an area related to NLP.
 natural language processing. After the introductory chapter 1, there are five parts: similar size and structure. The structure has been changed by breaking the old part  X  X ords X  into two parts  X  X ords X  and  X  X peech, X  merging two old parts  X  X emantics X  and  X  X ragmatics X  into one  X  X emantics and Pragmatics, X  and introducing one new part  X  X pplications. X  I considered the old edition also to be the textbook of choice for a course in NLP, but even though the changes may not appear to be significant, the new edition is a marked improvement, both in overall content structure as well as in presenting topics at a finer-grained level. Topics on speech synthesis and recognition are significantly expanded ;maximum entropy models are introduced and very well explained ;and statistical parsing is covered better with an explanation of the principal ideas in probabilistic lexicalized context-free grammars.
 computation, explaining various methods such as n -grams and smoothing. As another example, maximum entropy modeling is a popular topic but in many books explained only superficially, while here it is presented in a well-motivated and very intuitive way.
The learning method is not covered, and more details about it would be very useful. The new edition conveniently includes the following useful reference tables on endpapers: regular expression syntax, Penn Treebank POS tags, some WordNet 3.0 relations, and major ARPAbet symbols.
 ing ;symbolic and stochastic approaches ;and algorithmic, probabilistic, and signal-processing methodology) and a wide audience: computer scientists, linguists, and engineers. This has a positive side, because there is an educational need, especially in computer science, to present NLP in a broad, integrated way ;this has seemed to be always very challenging and books with wide coverage were rare or non-existent. For example, Allen X  X  (1995) Natural Language Understanding presented mostly a symbolic approach to NLP, whereas Manning and Sch  X  utze X  X  (1999) Foundations of Statistical
Natural Language Processing presented an exclusively statistical approach. However, there is also a negative side to the wide coverage X  X t is probably impossible to present material in an order that would satisfy audiences from different backgrounds, in par-ticular, linguists vs. computer scientists and engineers.

Processing in 2002 at Dalhousie University, which later became a combined graduate/ undergraduate course. My goal was to present an integrated view of NLP with an emphasis on two main paradigms: knowledge-based or symbolic, and probabilistic. Not being aware of Jurafsky and Martin X  X  book at the time, I was using Manning and Sch  X  utze X  X  book for the probabilistic part, and Sag and Wasow X  X  (1999) book Syntactic Theory :A Formal Introduction for the symbolic part. I was very happy to learn about
Jurafsky and Martin X  X  book, since it fitted my course objectives very well. Although I keep using the book, including this new edition in Fall 2008, and find it a very good match with the course, there is quite a difference between the textbook and the course in order of topics and the overall philosophy, so the book is used as a main supportive reading reference and the course notes are used to navigate students through the mate-rial. I will discuss some of the particular differences and similarities between Jurafsky and Martin X  X  book and my course syllabus, as I believe my course is representative of the NLP courses taught by many readers of this journal.
 introduces context-free grammars in Chapter 12, followed by some general discussion about formal languages and complexity in Chapter 16. This is a somewhat disrupted sequence of topics from formal language theory, which should be covered earlier in a typical undergraduate computer science program. Of course, it is not only a very good idea but necessary to cover these topics in case a reader is not familiar with them ;however, they should be presented as one introductory unit. Additionally, a presentation with an emphasis on theoretical background, rather than practical issues of using regular expressions, would be more valuable. For example, the elegance of the definition of regular sets, using elementary sets and closure of three operations, is much more appealing and conceptually important than shorthand tricks of using practical regular expressions, which are given more space and visibility. As another example, it is hard to understand the choice of discussing equivalence of deterministic 464 and non-deterministic finite automata in a small, note-like subsection (2.2.7), yet giv-ing three-quarters of a page to an exponential algorithm for NFSA recognition (in
Figure 2.19), with a page-long discussion. It may be damaging to students even to mention such a poor algorithm choice as the use of backtracking or a classical search algorithm for NFSA acceptance. Context-free grammars are described in subsection troductory background review, more space should be given to this important formal-ism. In addition to the concepts of derivation and  X  X yntactic parsing, X  the following concepts should be introduced as well: parse trees, left-most and right-most deriva-tion, sentential forms, the language induced by a grammar, context-free languages, grammar ambiguity, ambiguous sentences, bracketed representation of the parse trees, and a grammar induced by a treebank. Some of these concepts are introduced in other parts of the book. More advanced concepts would be desirable as well, such as pumping lemmas, provable non-context-freeness of some languages, and push-down automata.
 with words and speech, then syntax, and ending with semantics and pragmatics, fol-lowed by applications. From my perspective, having applications at the end worked well ;however, while levels of NLP are an elegant and important view of the NLP domain, it seems more important that students master the main methodological ap-proaches to solving problems rather than the NLP levels of those problems. Hence, my course is organized around topics such as n -gram models, probabilistic models, naive
Bayes, Bayesian networks, HMMs, unification-based grammars, and similar, rather than following NLP levels and corresponding problems, such as POS tagging, word-sense disambiguation, language modeling, and parsing. For example, HMMs are introduced in Chapter 5, as a part of part-of-speech tagging ;language modeling is discussed in Chapter 4 ;and naive Bayes models are discussed in Chapter 20.
 structures in Chapter 15, including discussion of unification, implementation, modeling some natural language phenomena, and types and inheritance. The unification algo-rithm (Figure 15.8, page 511) is poorly chosen. A better choice would be a standard, elegant, and efficient algorithm, such as Huet X  X  (e.g., Knight 1989). The recursive algo-rithm used in the book is not as efficient, elegant, nor easy to understand as Huet X  X , and it contains serious implementational traps. For example, it is not emphasized that the proper way to maintain the pointers is to use the UNION -FIND data structure (e.g.,
Cormen et al. 2002). If the pointers f 1 and f 2 are identical, there is no need to set f to f 2 . Finally, if f 1 and f 2 are complex structures, it is not a good idea to make a recursive call before their unification is finished, since these structures may be accessed and unified with other structures during the recursive call. The proper way to do it is to use a stack or queue (usually called sigma) in Huet X  X  style, add pointers to structures to be unified on the stack, and unify them after the unification of current feature structure nodes is finished. Actually, this is similar to the use of  X  X genda X  earlier in the book, so it would fit well with previous algorithms.
 order, starting from classical unification and resolution, followed by definite-clause grammars, and then following with feature structures. The Prolog programming lan-guage is a very important part in the story of unification, and should not be skipped, as it is here. More could be written about type hierarchies and their implementation, especially because they are conceptually very relevant to the recent popular use of ontologies and the Semantic Web. to present all needed linguistic background at the beginning, such as English word classes (in Chapter 5), morphology (in Chapter 3), typical rules in English syntax (in
Chapter 12), and elements of semantics (in Chapter 19), and even a bit of pragmatics. As can be seen, these pieces are placed throughout the book. The introduction of English syntax in Chapter 12 is excellent and better than what can be typically found in NLP books, but nonetheless, the ordering of the topics could be better: Agreement and other natural language phenomena are intermixed with context-free rules, whereas in my course those two were separated. The point should be that context-free grammars are a very elegant formalism, but phenomena such as agreement, movement, and sub-categorization are the issues that need to be addressed in natural languages and are not handled by a context-free grammar (cf. Sag and Wasow 1999).
 emphasis on speech synthesis. The book was a useful reference, but the coverage was sufficient for only a small part of the course.
 ing, X  is unusual because normally parsing is considered to be a synonym for syntactic processing. The chapter describes the classical parsing algorithms for formal languages, such as CKY and Earley X  X , and the next chapter describes statistical parsing. Maybe a title such as  X  X lassical Parsing, X   X  X ymbolic Parsing, X  or simply  X  X arsing X  would be better. The Good X  X uring discounting on page 101 and the formula (4.24) are not well explained. The formula (14.36) on page 479 for harmonic mean is not correct ;the small fractions in the denominator need to be added.
 find that the order of material was the best possible. Nonetheless, the book is recom-mended as first on the list for a textbook in a course in natural language processing. References

Cyril Goutte  X  , Nicola Cancedda  X  , Marc Dymetman  X  , and George Foster (  X 
Institute for Information Technology, National Research Council Canada; Research Centre Europe)
Cambridge, MA: The MIT Press, 2009, xii+316 pp; hardbound, ISBN 978-0-262-07297-7, $45.00,  X 29.95 Reviewed by Phil Blunsom The University of Edinburgh
Attending recent computational linguistics conferences, it is hard to ignore the phe-nomenal amount of research devoted to statistical machine translation (SMT). Driven by the wide availability of open-source translation systems, corpora, and evaluation tools, a research area that was once the preserve of large research groups has become accessible to those of more modest resources. Although the current state-of-the-art SMT systems have matured into robust commercial systems, capable of providing reasonable quality translations for a variety of domains, they remain limited by naive modeling assumptions and a heavy reliance on heuristics. These limitations have led researchers to ask the question of whether the adoption of techniques from the machine learning literature could allow more complex translations to be modeled effectively. As such, this book, focused on the application of machine learning to SMT, is particularly timely in capturing the current interest of the machine translation community.

Technologies, X  focuses on research peripheral to machine translation. Topics covered include the acquisition of parallel corpora, cross-language named-entity processing, and language modeling. The second part covers core machine translation system building, presenting a number of approaches applying discriminative machine learning tech-niques within a SMT decoder.

Access Workshop held at the Neural Information Processing conference in 2006. As SMT is not a frequent topic at that conference, the bridging of research from the mainstream machine learning community with research on MT is particularly promising. A fine example of this cross-over is Chapter 9,  X  X ernel-Based Machine Translation, X  in which a novel approach to estimating translation models is presented. However, this promise is not entirely fulfilled, as some contributions either fail to make use of machine learning or are somewhat obscure, unlikely to impact on the mainstream SMT community. 1. Chapter 1: A Statistical Machine Translation Primer
In the first chapter,  X  X  Statistical Machine Translation Primer, X  the editors seek to both introduce the concept of the book as well as give a brief tutorial on current SMT techniques. In these aims they succeed, describing the elements of current approaches to
SMT succinctly. Although those foreign to the field would not come away from reading this chapter able to implement a translation model, pointers to research publications that contain that level of detail are provided and the authors avoid highlighting obscure research that may mislead.
 with structured outputs, an active area of research in the machine learning community.
However, I can X  X  help but feel an opportunity was missed to lay out a clear agenda for research seeking to leverage machine learning techniques in SMT. The issue is not that machine learning can be applied to SMT, but why we would want to do so. Here it is necessary to identify problems with the current approach which could be addressed by a more rigorous statistical treatment: in particular, the lack of structural conditioning and theoretical analysis. Conversely it would seem prudent to highlight the properties of the current approach that have led to its success: the ability to scale to very large cor-pora and represent phrasal translation units. Trading either of these for more principled learning frameworks is unlikely to yield improvements in performance. 2. Part I (Chapters 2 X 6): Enabling Technologies
The first section features a collection of five chapters dealing with technologies which are related to, but not core, SMT. Chapter 2,  X  X ining Patents for Parallel Corpora, X  by
Masao Utiyama and Hitoshi Isahara, describes the application of standard techniques for collecting parallel corpora to a Japanese X  X nglish patent data corpus. Lacking any particular novel insights or applications of machine learning, this will mostly be of interest to those seeking data in that particular domain. The problem of constructing dictionaries of named entities and their translation is tackled by Bruno Pouliquen and
Ralf Steinberger in Chapter 3,  X  X utomatic Construction of Multilingual Name Dictio-naries. X  This is an interesting problem with relevance for commercial MT systems which must avoid nonsensical literal translations of named entities. However, the treatment takes the form of a system description and fails to make use of any machine learning, thus feeling somewhat out of place in this book.
 tilingual Corpora, X  by Alexandre Klementiev and Dan Roth, again dealing with named entities but this time making novel use of their temporal occurrence distributions. The authors are able to learn both entity alignments and transliterations with an iterative procedure using the observation that, in temporally aligned parallel corpora, a named entity and its translation will appear co-located in time. This is an interesting technique and should be equally applicable to the alignment of other word types.
 processing Schemes, X  by Jakob Elming, Nizar Habash, and Josep M. Crego, tackles the problem of word alignment for morphologically rich languages such as Arabic. To avoid the issue of having to choose a single morphological tokenization, the authors create alignments from a range of tokenizations which are then combined using a binary clas-sifier trained on hand-aligned data. Although of particular interest for those working with Arabic, this chapter fails to go beyond other works on supervised training for word alignment which have consistently shown that it X  X  easy to achieve large gains in alignment accuracy while much more difficult to impact on end-to-end translation performance (Fraser and Marcu 2007).
 those before. In  X  X inguistically Enriched Word-Sequence Kernels for Discriminative
Language Modeling, X  Pierre Mah  X  e and Nicola Cancedda demonstrate the use of string kernels for language modeling, evaluating a number of kernels including one able to integrate a range of factors (surface form, lemma, part-of-speech). This is interesting 638 work, showing that complex machine learning techniques can be brought to bear on basic NLP tasks, although scaling issues limit the evaluation to small artificial data sets. 3. Part II (Chapters 7 X 13): Machine Translation
Part II presents a collection of works more directly addressing the title of the book. It is often the case that research seeking to apply machine learning techniques to SMT can neatly be divided into two categories: those that simplify and decompose the translation problem into subtasks that fit existing classification models; and those that maintain the structure of state-of-the-art models and develop new machine learning algorithms specifically for them.
 lem into subproblems, particularly focusing on lexical choice as classification. In Chap-ter 7,  X  X oward Purely Discriminative Training for Tree-Structured Translation Models, X 
Benjamin Wellington, Joseph Turian, and I. Dan Melamed seek to transduce source syntax trees into target strings by learning local classifiers for the nodes in the trees.
Although such an approach allows SMT to be viewed as learning local classifiers, the trade-offs made seem to significantly limit the model, something encountered in other works on local tree transduction (Yamada and Knight 2002). In Chapter 10,  X  X tatistical Machine Translation through Global Lexical Selection, X  Srinivas Bangalore,
Stephan Kanthak, and Patrick Haffner take a bag-of-words approach, ignoring ordering information and learning classifiers that predict the presence of target lexical items given an entire source sentence. This chapter takes quite a novel finite X  X tate transducer approach to SMT; however, again the simplifying modeling assumptions seem limiting.
Based Machine Translation, X  by Zhuoran Wang and John Shawe-Taylor. This work directly addresses the aim of the book: applying powerful state-of-the-art machine learning approaches to machine translation. The authors describe a class of bilingual string kernels capable of modeling phrase-based SMT without constraining phrase extraction with word alignments, instead modeling unrestricted phrase co-occurrence.
The learning objective chosen is to minimize the squared loss of the n -gram overlap of candidate translations given the reference, a close fit to the evaluation metric BLEU. The inevitable scaling problems are tackled with a novel information retrieval approach. For each test sentence the algorithm sub-selects training samples based on lexical overlap and decodes using a regression model based on this subset. The results achieved are surprisingly competitive with a standard phrase-based model, an encouraging outcome given that no explicit language model is present in the kernel-based decoder. by Kenji Yamada and Ion Muslea; 11:  X  X iscriminative Phrase Selection for SMT X  by tion X  by Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar) cover relatively well-trodden ground, taking standard SMT models and applying common machine learning algorithms to a sub-part of the system (re-ranking, discriminative phrase selection, and semi-supervised learning, respectively). These chapters provide solid descriptions of applying these techniques and the performance gains that can be achieved, a useful contribution for anyone seeking to augment their existing decoder. However, a caveat here is the evaluation in Chapter 11. Although the authors must be commended on their thoroughness, the vast number of metrics used (one table includes 37!) provides more confusion than clarity when seeking to understand the performance of their system.
Matusov, Gregor Leusch, and Hermann Ney introduce a novel approach to learning system combination models based on confusion networks. This chapter provides a nice treatment of this topic with an evaluation demonstrating the consistent performance gains that can be achieved; it will be of particular interest for those involved in multi-site evaluation campaigns. 4. Summary In an age in which most research publications can be readily accessed for free via the
Web, a collected-works publication such as this stands on its ability to bring together articles which compactly summarize and define a direction of research. In this respect, this book falls short of being a must-buy for the SMT researcher, as many of the works tend towards the esoteric, making it hard for someone seeking familiarity with the field to separate core contributions from those unlikely to represent its future. However, the high degree of novelty and range in the collected articles, with many authors proposing new structures for translation models, still make this a worthwhile read with great potential to inspire future research.
 References

Philipp Koehn (University of Edinburgh) Cambridge University Press, 2010, xii+433 pp; ISBN 978-0-521-87415-1, $60.00 Reviewed by Colin Cherry National Research Council Canada
Statistical Machine Translation provides a comprehensive and clear introduction to the most prominent techniques employed in the field of the same name (SMT). This text-book is aimed at students or researchers interested in a thorough entry-point to the field, and it does an excellent job of providing basic understanding for each of the many pieces of a statistical translation system. I consider this book to be an essential addition to any advanced undergraduate course or graduate course on SMT.

Foundations (75 pages) covers an introduction to translation, working with text, and probability theory. Core Methods (170 pages) covers the main components of a standard phrase-based SMT system. Advanced Topics (125 pages) covers discriminative training and linguistics in SMT, including an in-depth discussion of syntactic SMT. The text as a whole assumes a certain familiarity with natural language processing; though the
Foundations section provides an effort to fill in the gaps, the book X  X  focus is decidedly translation. As such, students unfamiliar with NLP may sometimes need to consult a general NLP text.
 translation system, and it definitely succeeds in doing so. Supplementing this core material for each chapter is a highly inclusive Further Reading section. These sections provide brief narratives highlighting many relevant papers and alternative techniques for each topic addressed in the chapter. I suspect many readers will find these literature pointers to be quite valuable, from students wishing to dive deeper, to experienced SMT researchers wishing to get started in a new sub-field. Each chapter also closes with a short list of exercises. Many of these are very challenging (accurately indicated by a star-rating system), and involve getting your hands dirty with tools downloaded from the Web. The usefulness of these exercises will depend largely on the instructor X  X  tastes;
I view them as a bonus rather than a core feature of the book. 1. Chapters 1 X 3: Foundations
The first three chapters provide foundational knowledge for the rest of the book. In-troduction provides an overview of the book and a brief history of machine transla-tion, along with a discussion of applications and an expansive list of resources. The overview X  X  structure takes the form of a summary of each chapter. This structure pro-vides an effective preview of what will be covered and in what order, but it does not focus on typical introduction material; for example, there is no one place set aside to con-vince the reader that SMT is a good idea, or to introduce concisely the main philosophies behind the field. The history section is enjoyable, and I was glad to see a cautionary note regarding machine translation X  X  history of high hopes and disappointments. The applications section provides an excellent overview of where SMT sees actual use, and helps the reader understand why translations do not always need to be prefect. on a broad set of topics including Zipf X  X  law, parts-of-speech, morphology, and a num-ber of grammar formalisms. To give an idea of just how brief coverage can be, the section on grammar covers four formalisms in five pages. Nonetheless, these descriptions should be helpful when the concepts re-appear later in the book. This chapter closes with a discussion of parallel corpora and sentence alignment. As these are central to the business of SMT, I feel they might have been better placed in a translation-focused chapter.
 out the book. This chapter is clear, and provides strong intuitions on important issues such as conditional probability. There is a surprisingly large emphasis on binomial and normal distributions, considering SMT X  X  heavy reliance on categorical distributions; however, these are needed to discuss significance testing and some language modeling techniques covered later. 2. Chapters 4 X 8: Core Methods
The next five chapters provide detailed descriptions of each of the major components of a phrase-based SMT system. Word-based Methods discusses the five IBM translation models, with a brief detour to discuss the noisy channel model that motivates the
IBM approach. This chapter is best taken as a complement to Brown et al. (1993) and Kevin Knight X  X  (1999) tutorial on the same subject, rather than a replacement. It provides strong intuitions on what each IBM model covers and how each model works, including the clearest descriptions I have seen of IBM:3 X 5. However, it does sometimes make them seem a little mysterious. For example, there is no attempt to explain why
IBM:1 always arrives at a global maximum, or to generalize when one can apply the mathematical simplification that reduces IBM:1 X  X  exponential sum over products to a polynomial product over sums. One glaring omission from this chapter is a discussion of the alignment HMM (Vogel, Ney, and Tillmann 1996). This elegant model is widely used and widely extended, and I had expected to see it covered in detail.
 the popular phrase-based SMT paradigm. They are clear and fairly complete; this book could easily serve as an effective reference for these topics. Phrase-based Models motivates the use of phrases, and then covers phrase extraction along with the calculation of phrase features, such as lexical weighting and lexicalized re-ordering models. This chap-ter also marks the beginning of a careful dance, where log-linear models are introduced without having yet covered SMT evaluation or discriminative training. These topics are covered in Chapters 8 and 9, respectively. This division of modeling and training is a reasonable strategy, given the amount of material required to understand the full pipeline, but a student may need some extra guidance to understand the complete picture. The Decoding chapter focuses on stack decoding, and it is extremely well-written, with great explanations of search and pruning strategies. Alternative decoding formalisms, such as A* or finite-state decoding, are given short but effective summaries. This chapter makes phrasal SMT decoding feel easy.
 depth coverage in other NLP texts, I was surprised to see it covered quite thoroughly here as well. This chapter covers a number of smoothing techniques, as well as some 774 practical tips for handling large models. As usual, the exposition is exceptionally clear, and each new method X  X  advantages are demonstrated with predicted counts or perplex-ity scores on Europarl data, which I found to be very useful. For many SMT courses, this chapter will be sufficient to stand alone as both an introduction and a reference for language modeling.
 discusses human evaluation, motivates automatic evaluation, and then covers the major contenders: word error rate, BLEU, and METEOR. The discussion of BLEU X  X  shortcom-ings is very even-handed, perhaps a little pessimistic, and acknowledges all of the major concerns regarding the metric. 3. Chapters 9 X 11: Advanced Topics
The final three chapters cover advanced topics, which include recent or not universally adopted advances. So at this point, one might expect that all of the major components of a baseline phrase-based SMT system have been covered, but the final piece of the puzzle does not come until Discriminative Training , which includes a discussion of minimum error rate training (MERT) for the log-linear models introduced in Chapter 5. This chapter also covers n -best list extraction, n -best list re-ranking, and posterior methods such as Minimum Bayes Risk Decoding. It also devotes a surprisingly large amount of time to large-scale discriminative training, where thousands of parameter values can be learned. There is a lot of ground to cover here; consequently, much of the material will need to be supplemented with research papers or other texts if the instructor wants to cover any one topic in depth. The sections covering the learning methods used in parameter tuning (maximum entropy, MERT) did not feel as clear as the rest of the book. I suspect that a newcomer to the field will require some guidance to pick out the essential parts.
 covering linguistic pre-processing, syntactic features, and factored translation models.
The pre-processing discussion includes transliteration, morphological normalization, compound splitting, and even syntactically motivated re-ordering of the input sen-tence. The syntactic features section mostly covers n -best list re-ranking as done in the
Smorgasbord paper (Och et al. 2004). Each of these topics is well motivated, and the text provides a clear description of a prominent, recent solution.
 first describing synchronous context-free grammars, and then describing both for-mally syntactic hierarchical grammars and linguistically syntactic synchronous-tree-substitution grammars in terms of this common formalism. This is a very nicely presented chapter. It draws a lot of interesting connections between formalisms; for example, tree-to-tree rule extraction and tree-to-string rule extraction are presented as simple constraints on hierarchical phrase extraction. The description of chart parsing for decoding is also very clear, and it draws many useful analogies to the material presented earlier for phrasal decoding. I get the impression that many insights gained while adding syntactic SMT into the Moses translation system have found their way into this chapter. 4. Summary
This book X  X  existence indicates that the field of SMT has reached a point of maturity where it makes sense to discuss core and foundational techniques. This book provides a clear and comprehensive introduction to word, phrase, and tree-based translation modeling, along with the decoding, training, and evaluation algorithms that make these models work. The text X  X  stated goal is to provide a thorough introduction, but I would also recommend it as an effective reference for anyone interested in writing their own
SMT decoder, be it phrasal or syntactic. Most importantly, this book makes the prospect of teaching a course devoted to SMT much less daunting, and it should provide a valuable resource to researchers or students looking to teach themselves.
 References
 Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang (Chinese University of Hong Kong, Hong Kong Polytechnic University, City University of Hong Kong, and San Diego State University) Princeton, NJ: Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 4), 2010, x+148 pp; paperbound, ISBN 978-1-59829-932-8, $40.00; e-book, ISBN 978-1-59829-933-5, $30.00 or by subscription Reviewed by Min Zhang Institute for Infocomm Research This introductory book is a systematic and up-to-date overview of fundamental and focused knowledge of Chinese NLP for both practitioners and a general audience. The Chinese language has the largest number of native speakers in the world, with over one billion people speaking some style of Chinese. The globalization and the development of the Internet have significantly increased the participation of Chinese-speaking users in global business and social life, accounting for the highest growth rate in on-line population over the past decade.
 processing of the language distinctive and challenging. With a clear awareness of many differences between Chinese and other languages in morphology, syntax, and seman-tics, the authors focus the subject matter on morphological analysis, which means they choose to pay close attention to the essential processing techniques that lay down the foundation of any advanced Chinese NLP system. For many readers who are puzzled by  X  X hy on earth we have to bother with Chinese NLP given the availability of English-language processing technologies, X  this book is suitable puzzle-solving material; for undergraduate and postgraduate students interested in the field, knowledge regarding the fundamentals of computer processing of Chinese language can be acquired; for a veteran already who knows everything in Chinese NLP, this is a useful and lightweight reference book. Overall, this book has a large potential audience of readers who are interested in Chinese NLP at all levels.
 resources, and a bibliography. Chapter 1 gives an introduction by raising the unique characteristics of Chinese. A couple of interesting examples are used to explain the problem of ambiguity caused by the lack of clear delimiters between words in Chi-nese text. Then it proceeds to illustrate how other types of difficulties are produced at morphological, syntactic, and semantic levels. From these discussions, Chapter 1 highlights and concludes that  X  X he main difference in NLP between Chinese and other languages takes place in the first stage. This lays down the objective of the book, namely, to introduce the basic techniques in Chinese morphological analysis. X  written script, providing the necessary preparation for morphological analysis in the later chapters. It introduces the concepts of characters, morphemes, and words, as well as the typical morphological processes of word formation. In particular, the authors provide sufficient details on the nature of the compounding that plays a predominant role in Chinese word formation and appears much more prevalent than that in English.
A number of examples make the content easy to follow, especially for international learners who are not familiar with Chinese.
 lenges that arise from a wide spectrum of morphological problems of Chinese. Com-pared to an alphabetic system like English, the difficulties of Chinese morphological processing not only result from its representation and writing conventions, such as a large character set, multiple co-existing variants of character sets and encoding stan-dards, dialectal variations, special genres of punctuation, and so on, but also from its underlying linguistic heritage and distinctions. For example, the lack of formal morphological markers may render part-of-speech tagging troublesome due to few part-of-speech clues and multiple parts of speech for the same word; the indetermi-nacy may be worsened by homophony and homography. The authors emphasize the influences of different kinds of ambiguities and out-of-vocabulary (OOV) words and suggest that external knowledge and contextual information are important to ambiguity resolution.
 nese and a few other Asian languages. A tree-based taxonomy is used to classify segmentation algorithms into character-and word-based approaches. Besides the rudi-mentary methods, emphasis is given to those methods using statistics and machine learning that take the advantage of large-scale annotated corpora to solve the segmen-tation ambiguity issue. Three main Chinese word segmentation standards are intro-duced, together with the performance measures and benchmark data widely used for evaluation.
 identification (UWI) is discussed separately in Chapter 5, probably because of its larger impact on accuracy and greater difficulty. Unknown words account for 60% of word segmentation errors. Proper nouns representing person, place, and organization names that are missing from the dictionary are common sources of unknown words. New names may appear on a daily basis, tending to intensify the OOV problem. This chapter describes various means for UWI from both generic and specific point of views. The general statistical methods based on co-occurrence statistics are depicted, followed by the techniques for identifying specific types of proper nouns based on their particular formation patterns. Two general methods worthy of review but missing are class-based language models (Fu and Luke 2004) and discriminative Markov models (Zhou 2005), both of which make significant improvements by leveraging various types of features.
 izes them with the gist of the basic semantic concepts, relations, and resources, and helps build up knowledge for resolving deeper NLP problems such as word sense disam-biguation, parsing, and language understanding. Three major contemporary Chinese thesauri, namely CILIN, HowNet, and CCD, are surveyed in comparison with the well-known English WordNet. For example, HowNet has three unique features compared to WordNet: (1) its concept definitions are based on sememes, which are specified in a structured mark-u planguage; (2) the semantic role relations can be connected across part-of-speech categories such as between nouns and verbs; and (3) the concepts are represented in both Chinese and English. Though structurally compatible, CCD differs from WordNet with some important extensions, such as more types of nouns, finer relations, and examples of collocations selected from real corpora with quantitative 778 descriptions. The three major Chinese linguistic resources at the lexical semantic level are valuable to Chinese NLP applications.
 cal convention using the customary combination of words in expressions. Starting with the interesting example of historical burden as contrasted with historical luggage ( burden and luggage can be expressed with the same Chinese word), Chapter 7 places emphasis on the concepts, distinguishing features, categorization, and data resources. Although widely used and easily comprehensible, collocation is defined in various ways and is therefore subject to controversy in the literature. After a review of English collocation, the authors accentuate the major differences between Chinese and English collocations, and then define Chinese collocation as a lexically restricted word combination with emphasis on semantic and syntactic considerations.
 categorizes the existing methods into statistical, syntactic, semantic, and categorical approaches. Given a headword and a context window, a statistical approach identifies the word combinations from the context inclusive of the head word with significant lexical statistics as the collocations; a syntactic approach resorts to syntactic knowledge acquired from a parser to refine the candidates, assuming that collocation words are syntactically dependent; a semantic approach exploits semantic relations such as syn-onyms obtained from WordNet to overcome the inaccuracies caused by insufficient statistics; a categorical approach adopts a set of carefully tailored rules to identify collocations of different types, which may potentially achieve better results, but the method turns out to be less generalizable. This language-specific approach corresponds to the characteristics of Chinese collocations. The chapter would have been more helpful if it had included the work on collocativity measures based on limited modifiability with only shallow parsing (Wermter and Hahn 2004), statistical methods using accurate collocational information based on full parsing (Seretan and Wehrli 2006), and the more recent monolingual word alignment method (Liu et al. 2009).
 The references are generally complete and helpful to readers interested in further studies.
 critical issues in Chinese NLP. As the foundation of any NLP technology, morphological analysis (where Chinese morphology differs most apparently from other languages) is the first crucial procedure prior to syntactic and semantic processing. Although it would be more integrated to include syntactic and semantic analysis, I do believe the authors have made this choice wisely. Given the limited number of pages, it makes sense to focus on the essence with sufficient details to draw quick attention from many NLP readers who are sensitive to the most salient and distinctive subject matters. Instead of elaborating the Chinese part alone, the authors adopt a helpful writing style so that, whenever applicable, comparative discussions are made on the corresponding issues in other languages, especially English. This bilingual perspective sheds more light on what constitutes the noteworthy features in Chinese and the reason why customary techniques have to be or not be employed in Chinese NLP. The reader might have hoped for more polish and a better editorial quality in the book. For instance, a repeatedly referred-to concept OAS is not pre-defined (Section 4.4); not all the extracted co-words shown in Tables 8.1 X 8.6 are true collocations, which should have been boldface as stated in the text. In addition, one more chapter is needed to close the book by summarizing the book with some conclusions and discussions on further readings beyond the intro-duction at Chinese morphological level. This book review was edited by Pierre Isabelle.
 References

Vladimir Pericliev (Bulgarian Academy of Sciences) London: Equinox, 2010, ix+330 pp; hardbound, ISBN 978-1-84553-660-2, $90.00,  X 60.00 Reviewed by Eric J. M. Smith University of Toronto
The subtitle of Vladimir Pericliev X  X  book, AnIntroductionandSomeExamples , is a succinct and accurate description of its contents. Pericliev argues briefly for the usefulness of computer-aided techniques in linguistic discovery, contrasting it with the intuitionist approach which has characterized linguistic discovery throughout much of its history.
The bulk of the book is devoted to examples of software-aided linguistic discovery drawn from his own work.
 guistic theory, categorizing scientific discovery into three main approaches: the intui-tionist approach, the chance approach, and the problem-solving approach. Discoveries by intuition and by chance remain the purview of humans, but clearly the problem-solving approach can benefit from the application of computational techniques. nation X  in order to determine the minimal set of features which are necessary to dis-criminate all of a language X  X  kinshi pterms. The program is used to discover feature geometries, superior to existing human-discovered ones, which describe the kinship terminology of languages like English and Bulgarian.
 parsimonious discrimination), which is then applied to a variety of other tasks, some of which are unconnected to linguistics. Of these applications, the most interesting is the use of MPD to determine the segment profiles which uniquely identify languages in the UPSID-451 database (consisting of segment inventories from 451 languages, selected to provide broad coverage of the world X  X  language families) (Maddieson and
Precoda 1991). Although Pericliev discusses his results at considerable length, it is not clear what the theoretical usefulness of these profiles might be. What does it really tell us about French to know that it is the only language in the database to contain the phoneme [  X   X ]? Of more practical interest was Pericliev X  X  discussion of the process of converting the UPSID data into a featural representation to make it amenable to processing, describing how to represent underspecified segments and how to deal with transcription variations. This sort of necessary preprocessing constitutes an important and underemphasized part of the process of machine-aided linguistic discovery. The study of UPSID does produce some interesting, though not unexpected, results. For instance, when a profile contains more than one unique segment, the majority of these segments share a common feature, and 85.8% of the unique segments have some sort of secondary articulation.
 and AUTO. The UNIV software is inspired by Greenberg X  X  universals (Greenberg 1966), and automates the haphazard process by which universals have been identified in the past. Given a vector of features for each language being studied, UNIV identifies all universal patterns which hold above a user-specified threshold. Such universals can be unrestricted or statistical and they can be stand-alone or implicational. Once a set of universals has been identified, the results are fed through AUTO (for "AUthoring
TOol"), which assembles boilerplate text into a journal article; given the vast number of (often trivial) universals which UNIV discovers, this can be useful. UNIV is first applied to two data sets: one of kinship terms, and the other the word-order data used by Greenberg himself. The most interesting result is that Greenberg X  X  set of word-order universals was neither complete nor fully supported by the data.
 of previously unnoted universals, most of which are rather low-level and of little in-herent theoretical interest. However, the low-level machine-discovered generalizations can then be used as the basis for more interesting manually created generalizations. The UNIV analysis also serves to refine earlier claims made by Maddieson (1984) and Gamkrelidze (1978).
 typology to account for an observed set of universals. The search for such typologies is discussed by Greenberg (1966) and by Hawkins (1983). MINTYP takes a system of universals and a set of logically admissible types, and eliminates any superfluous uni-versals which can be implied by stronger universals in order to determine the smallest set of universals which still accounts for the observed data. This approach is able to distill Greenberg X  X  set of universals into as few as four composite universals. Like UNIV and KINSHIP, MINTYP follows Pericliev X  X  basic approach: Reduce the data to a set of features, and then find the patterns which most economically cover the observed feature distribution.
 cation with the RECLASS software. Pericliev extends the featural approach to include
Swadesh-type word-lists, for which he describes a method for calculating a similarity metric based on phonological features of words in the list. A set of languages from different families is selected for study, a similarity metric is calculated for pairs of lan-guages, and unrelated languages whose similarity is significantly greater than expected are given further attention. In Pericliev X  X  test case, the initial feature data consists of kinshi pterminology, which revealed an unex pected similarity between the Kaingang languages of Brazil and various Polynesian languages. He pursues this similarity first by using features based on word-list similarities and then by looking at other structural features, arguing at length for the plausibility of a genetic connection. Of all the results described in the book, this is probably the most interesting, because it represents a discovery made by Pericliev X  X  machine-aided approach which is unlikely ever to have been found by the haphazard manual process of discovery.
 was developed or co-developed by Pericliev himself, so the various programs all risk seeming like variations on a single theme. The book would have benefited by including examples of software from others working in the field, which might differ from the feature-coverage approach favored by Pericliev. That being said, Pericliev X  X  essential point is a valid one: Machine-aided discovery has a tremendous untapped potential for analyzing data sets which are too large to be amenable to human inspection.
The success of this approach is best exemplified by his machine-aided discovery of a possible genetic relationship which would otherwise have eluded human discovery. 786 References
 Luc Steels (editor) Universitat Pompeu Fabra and Sony Computer Science Laboratory, Paris Amsterdam: John Benjamins Publishing Company (Constructional Approaches to
Language series, edited by Mirjam Fried and Jan-Ola  X  Ostman, volume 11), 2012, xi+332 pp; hardbound, ISBN 978-90-272-0433-2, e 99.00, $149.00 Reviewed by
Nathan Schneider, Carnegie Mellon University and Reut Tsarfaty, Uppsala University
In computational modeling of natural language phenomena, there are at least three modes of research. The currently dominant statistical paradigm typically prioritizes instance coverage : Data-driven methods seek to use as much information observed in data as possible in order to generalize linguistic analyses to unseen instances. A second approach prioritizes detailed description of grammatical phenomena, that is, forming and defending theories with a focus on a small number of instances. A third approach might be called integrative : Rather than addressing phenomena in isolation, different approaches are brought together to address multiple challenges in a unified framework, and the behavior of the system is demonstrated with a small number of instances. Design
Patterns in Fluid Construction Grammar ( DPFCG ) exemplifies the third approach, intro-ducing a linguistic formalism called Fluid Construction Grammar (FCG) that addresses parsing, production, and learning in a single computational framework.
 tive paradigms that can be traced back to Generalized Phrase Structure Grammar (GPSG) (Gazdar et al. 1985), Lexical Functional Grammar (LFG) (Bresnan 2000), Head-Driven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory
Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal meta-framework allows computational linguists to formalize their hypotheses and intuitions about a language X  X  grammatical behavior and then explore how these representational choices affect the processing of natural language utterances. Many of the aforemen-tioned approaches have engendered large-scale platforms that can be used and reused to provide formal description of grammars for different languages, such as Par-Gram for LFG (Butt et al. 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger, and Oepen 2002). of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG treats constructions as the basic units of grammatical organization in language. The con-structions are viewed as learned associations between form (e.g., sounds, morphemes, syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG does not impose a strict separation between lexicon and grammar X  X ndeed, it is per-haps best known as treating semi-productive idioms like  X  X he X-er, the Y-er X  and  X  X  let alone Y X  on equal footing with lexemes and  X  X ore X  syntactic patterns (Fillmore,
Kay, and O X  X onnor 1988; Kay and Fillmore 1999). FCG, like other CxG formalisms X  namely, Embodied Construction Grammar (Bergen and Chang 2005; Feldman, Dodge, and Bryant 2009) and Sign-Based Construction Grammar (Boas and Sag 2012) X  X s unification-based. 1 The studies in this book describe constructions and how they can be combined in order to model natural language interpretation or generation as feature structure unification in a general search procedure.
 processing matters, advanced case studies, and, finally, features of FCG that make it fluid and robust. Each chapter identifies general strategies ( design patterns ) that might merit reuse in new FCG grammars, or perhaps in other computational frameworks.
Construction Grammar X  (by Luc Steels) presents the aims of the FCG formalism. FCG was designed as a framework for describing linguistic units (constructions X  X heir form and meaning), with an emphasis on language variation and evolution ( X  X luidity X ). The constructionist approach to language is described and the argument for applying it to study language variation and change is defended. Psychological validity is explicitly ruled out as a modeling goal ( X  X he emphasis is on getting working systems, and this is difficult enough X ; page 4). The architects of FCG set out to include both sides of the processing coin, however X  X arsing (interpretation) and production (generation).
The concept of search in processing is emphasized, though some of the explanations of processing steps are too abstract for the reader to comprehend at this point. A further desideratum X  X obustness to noisy input containing disfluencies, fragments, and errors X  X s given as motivating a constructionist approach.
 (by Steels), describes the mechanisms of FCG in detail. In FCG, a working analysis hypothesized in processing is known as transient structure ; the transduction of form to meaning (and vice versa) selects a sequence of constructions that apply to the transient structure to gradually expand it until reaching a final analysis. Identifying construc-tions that may apply to a transient structure presents a non-trivial search problem, also addressed by the architects of FCG. The sheer number of technical details make this chapter somewhat overwhelming. Most of the chapter is devoted to the low-level feature structures and the operations manipulating them. Templates X  X  practical means of avoiding boilerplate code when defining constructions X  X re then introduced, and do most of the heavy lifting in the rest of the book.
 practice.  X  X  Design Pattern for Phrasal Constructions X  (by Steels) illustrates how con-structions are used to describe the combination of multiple units into higher-level, typed phrases. Skeletal constructions compose with smaller units to form hierarchical struc-tures (essentially similar to the Immediate Constituents Analysis of Bloomfield [1933] and follow-up work in structuralist linguistics [Harris 1946]), and a range of additional constructions impose form (e.g., ordering) constraints and add new meaning to the newly created phrases. This chapter is of a tutorial nature, illustrating the step-by-step application of four kinds of noun phrase constructions to expand transient structures in processing. Over twenty templates are introduced in this chapter; they encapsulate design patterns dealing with hierarchical structure, agreement, and feature percolation. 448
An aspect of phrasal constructions that is not yet dealt with is the complex linking of se-mantic arguments and the morphosyntactic categorizations of the composed elements. then builds on the formal machinery presented in the previous chapter to explicitly address the complex mappings between semantic arguments (agent, patient, etc.) and syntactic arguments (subject, object, etc.). This mapping is a complex matter due to language-specific conceptualization of semantic arguments and different means of morphosyntactic realization used by different languages. In FCG, each lexical item introduces its linking potential in terms of the different types of semantic and syntactic arguments that it may take, with no particular mapping between them. Each argument structure construction imposes a partial mapping between the syntactic and semantic arguments to yield a particular argument structure instantiation, one of the multiple alternatives that may be available for a single lexical item. This account stands in sharp contrast to the lexicalist view of argument-structure (the view taken in LFG, HPSG, and
CCG) whereby each lexical entry dictates all the necessary linking information. The construction-based approach is defended for its ability to deal with unknown words and constructional coercion 3 (Goldberg 1995). The argument structure design pattern allows FCG to crudely recover a partial specification of the form-meaning mapping of these elements, which is important for robust processing (see subsequent discussion). string and a meaning representation, where the two directions (parsing and production) share a common declarative representation of linguistic knowledge (the grammar). This entails assembling an analysis incrementally on the basis of the grammar, the input, and any partial analysis that has already been created. With FCG (and unification grammars more broadly) this search is nontrivial, and streamlining search (i.e., minimizing non-determinism and avoiding dead ends) is a key motivator of many of the grammar design patterns suggested in the book.
Beule) deals mainly with the problem of choosing which of multiple compatible con-structions to apply next. Whereas the default heuristic search in FCG is a greedy, depth-first search (which can backtrack if the user-defined end-goal has not yet been achieved) the FCG framework allows for a guided search through scores that reflect the relative tendency of a construction to apply next. The authors suggest that such scoring can be informed by general principles, for instance: (i) specific constructions are preferred to more general ones, and (ii) previously co-applied constructions are preferred. Choosing appropriate constructions to apply early on dramatically reduces the time needed for processing the utterance. the next level, and proposes to organize the different constructions in networks of conditional dependencies. A conditional dependency links two constructions where one provides necessary information for the application of the other. These dependency networks can be updated whenever an input is processed so that the system learns to search more efficiently when the same constructions are encountered in the future.
Using these networks to guide the search thus significantly reduces the search for compatible constructions. An empirical effort to quantify this effect indeed shows a sharp reduction in search time; unlike the held-out experimental paradigm accepted in statistical NLP, however, the parsed/produced sentence is assumed to have been seen already by the system.
  X  X eature Matrices and Agreement X  (by van Trijp) on German case offers a new unification-based solution to the problem of feature indeterminacy. For instance, in the sentence Er findet und hilft Frauen  X  X e finds and helps women X , the first verb requires an accusative object, whereas the second requires a dative object; the coordination is allowed only because Frauen can be either accusative or dative. Kindern  X  X hildren X , which can only be dative, is not licensed here. Encoding case in a single feature on the Frauen construction wouldn X  X  work because the feature would have to unify with contradictory values (from the verbs X  inflectional features). Instead, case restrictions specified lexically for a noun or verb can be expressed with a distinctive feature matrix , with each matrix slot holding a variable or the value + or -. Unification then does the right thing X  X llowing Frauen and forbidding Kindern  X  X ithout resorting to type hierarchies or disjunctive features. agreement models a phenomenon whereby morphosyntactic, semantic, and phono-logical factors affect the choice between poly-and mono-personal agreement X  X hat is, the decision whether a Hungarian transitive verb should agree with its object or just with its subject. The case and definiteness of the object and the person hierarchy rela-tionship between subject and object determine which kind of agreement obtains, and phonological constraints determine its form. To make the different levels of structure interact properly, constructions are grouped into sets (lexical, morphological, etc.) and those sets are considered in a fixed order during processing. Construction sets also allow for efficient handling of unmarked forms (null affixes) X  X hey are considered only after the overt affixes have had the opportunity to apply, thereby functioning as defaults.
Martin Loetzsch) on German spatial phrases models the German spatial terms for front , back , left ,and right . To model spatial language in situated interaction with robots, two problems must be overcome. The first is syntactic indeterminacy : Any of these spatial relations may be realized as an adjective, an adverb, or a preposition. The second is semantic ambiguity , specifically when the perspective (e.g., whose  X  X eft X ?) is implicit.
Both are forms of underspecification which could cause early splits in the search space if handled na  X   X vely. Much in the spirit of the argument structure constructions (see above), the solutions (which are too technical to explain here) involve (a) disjunctive representations of potential values of a feature, and (b) deferring decisions until a more opportune stage.
 features of the system that ensure robustness in the face of variation, disfluencies, and noise. Natural language is fluid and open-ended. There is variation between speakers, there are disfluencies and speech errors, and noise may corrupt the speech signal. All of these may jeopardize the interpretability of the signal, but human listeners are adept at processing such input. In the spirit of usage-based grammar (Tomasello 2003), FCG emphasizes the attainment of a communicative goal , rather than ensuring grammaticality of parsed/produced utterances. This is accomplished with a diagnostic-repair process that runs in parallel to parsing/production. Diagnostics can test for unknown words, unfamiliar meanings, missing constructions, and so on.
Diagnostic tests are implemented by reversing the direction of the transduction process: a speaker may assume the hearer X  X  point of view to analyze what she has produced 450 in order to see whether communicative success has been attained. Likewise, a hearer may produce a phrase according to his own interpretation of the speaker X  X  form, and check for a match. If a test fails, repair strategies such as proposing new constructions, relaxing the matching process for construction application, and coercing constructions to adapt to novel language use are considered.
 framework has been used in experiments that assume embedded communication in robotic agents. This research program is developed at length by Steels (2012b).
Discussion. Like the legacy of the GPSG book (Gazdar et al. 1985), this book X  X  main merit is not necessarily in its technical details or computational choices, but in demonstrating the feasibility of implementing the constructional approach in a full-fledged computa-tional framework. We suggest that the CxG perspective presents a formidable challenge to the computational linguistics/natural language processing community. It posits a different notion of modularity than is observed by most NLP systems: Rather than treat different levels of linguistic structure independently, CxG recognizes that multiple formal components (phonological, lexical, morphological, syntactic) may be tied by convention to a specific meaning or function. Systematically describing these  X  X ross-cutting X  constructions and their processing, especially in a way that scales to large data encompassing both form and meaning and accommodates both parsing and generation, would in our view make for a more comprehensive account of language processing than our field is able to offer today. Thus, we hope this book will be provocative even outside of the grammar engineering community.
 terse, which can be daunting for readers new to FCG. Contextualization with respect to other strands of computational linguistics and AI research is, for the most part, lacking, though a second FCG book (Steels 2012a) picks up some of the slack on this front.
DPFCG does not address the feasibility of learning constructions directly from data, does it discuss the expressive power of the formalism in relation to learnability results (such as that of Gold [1967]). As admitted by the authors, much more work would be needed to build life-size grammars. Still, we hope that readers of DPFCG will appreciate the authors X  vision for a model of linguistic form and function that is at once formal, computational, fluid, and robust.
 References 452
 Inderjeet Mani* and James Pustejovsky  X  (*Children X  X  Organization of Southeast Asia and  X  Brandeis University) Oxford University Press (Explorations in Language and Space series, edited by Emile Van Der Zee), 2012, xiii+166 pp; hardbound, ISBN 978-0-19-960124-0,  X 60.00 Reviewed by Thora Tenbrink Bangor University, Wales Inderjeet Mani and James Pustejovsky present a documentation of the state of the art with respect to the formal and computational representation of motion concepts expressed in language (mostly English). Starting from the conceptual properties rep-resented in the linguistic repertory of motion, they provide an overview of existing for-malisms and annotation approaches, ultimately moving towards automatic approaches and computational applications. The book is timely in its representation of the current understanding of motion concepts in language, and will therefore be of great interest in the computational and cognitive linguistics communities.
 that motion is just one of many human concepts expressed by a number of linguistic terms, which are adequately described by their lexical entries in anybody X  X  dictionary. Mani and Pustejovsky X  X  analysis of the linguistic representation of motion suggests a very different idea, however. Far from representing just one marginal aspect of human language, the conceptualization and verbalization of motion turns out to be central to human life X  X nd, as a consequence, central to communication. Motion combines the two fundamental human concepts of space and time. Space without time is, for humans, as meaningless as time without space. Both are inextricably linked X  X nd this link is most notably and systematically represented in language via expressions of motion. Motion is represented whenever aspects of life are described, reflecting its deep relevance for human thinking. Any computational approach towards interpreting natural language representation will, sooner or later, need to deal with motion concepts. Interpreting motion, therefore, turns out to be one of the most fundamental research issues for a variety of purposes both in basic (or cognitive) and applied (or computational) research. conceptual domains is characteristically divided into two fairly distinct communities dealing with either space or time in language and cognition. The combination of both, adding dynamic aspects, appears to pose too many challenges, adding too many complexities to the already puzzling diversity with respect to human representations of space and time. With their book Interpreting Motion , Mani and Pustejovsky are at the forefront of research that aims to bridge this gap by systematically bringing together findings and formalisms from both directions. The effort, as such, is laudable. The formal detail provided to explicate the representational patterns considerably adds to the value of this book. Formalization serves computational purposes just as well as providing a more precise conceptual grasp of the identified linguistic phenomena.
Nevertheless, it is noted that more elaborate explanations and consistent informal glosses might have supported the general audience of the Oxford University Press  X  X an-guage and Space X  series. Some readers may be more interested in the identified concepts than in the abundance of formalizations, which may be felt to hamper readability. readers may, at particular points, feel slightly less than satisfied with the mechanisms provided for dealing with motion in language. The book is unusually clear and honest in highlighting limitations in the current understanding of crucially relevant concepts, including existing formalization techniques. Consequently, the book is not only a valu-able summary of the currently available tools for interpreting motion, but also a useful starting point for further research that aims to fill various gaps identified by Mani and Pustejovsky X  X  exploration of the field.
 linguistic formalisms such as ontologies or calculi to linguistic expressions. Such a mapping is essential, because language reflects how humans naturally represent just those concepts that formalisms attempt to capture. The lack of systematic mapping mechanisms between linguistic structures and widely used formalization categories is by no means the authors X  fault, but reflects a longstanding research desideratum well-known in the relevant research communities X  X ith scattered attempts to provide solutions here and there, several of which are represented in this book. Mani and
Pustejovsky contribute to this urgently needed research by laying out the available tools in an accessible way and in many cases going several steps further ahead, suggesting mapping solutions wherever and to the extent possible.
 mainly serves to motivate the complexity involved in interpreting motion, highlighting key insights taken from cognitive linguistic theory as well as earlier psycholinguistic experimentation. Subsequent chapters outline linguistic observations supplemented by non-linguistic calculi, ontologies, and representations, dealing with space and time sep-arately. The main innovative contribution of the book emerges with a proposal for the formal representation of motion in Chapter 4. Here, previous approaches and mecha-nisms are combined to model the topological changes over time introduced by motion verbs. The remaining two chapters provide annotation specifications and application prospects. Extraction of motion information from natural language descriptions is pro-posed in terms of manual annotation; computational implementations are currently still very limited. Nevertheless, these chapters set the stage for subsequent machine learning and other automatic approaches, adopting methodologies already successfully established for other formalisms, to which the newly proposed motion formalism is a successor. In general, the described actual applications concern mostly other related work; the book describes the relevance of the current framework to such applications and represents their goals.
 approach is to capture the spatial implications carried by lexical items in terms of their consequences in the real world. For instance, the verb to fly implies a disconnection be-tween the flying figure and the ground below it. Although this is an essential condition for flying, in other cases implications can be context dependent, which is why corpus-based investigation is essential. For instance, to establish the spatial situation conveyed by the verb to cross it is necessary to consider what exactly is being crossed. In the case of a field, there is constant contact with the ground, whereas in the case of a river being crossed via a bridge, there is no such contact X  X n fact, contact with the river will be 456 avoided. Spatial implications such as these are implicitly understood by humans and need to be made available for computational approaches.
 to systematic patterns in the spatial domain, rather than making any claims about con-ceptually prominent aspects of lexical items in a cognitive linguistic (or psychological) sense. In this, the present approach is unique. For example, the linguistic ontology proposed by Bateman et al. (2010) captures the differences in meaning distinguished by the linguistic system, pointing to conceptual patterns reflected in (or made prominent by) language. In contrast, the framework presented in this book aims at identifying the spatial content carried across by the use of lexical items in context.
 procedure for interpreting the language of motion than has been available before. As a matter of fact, the authors basically reject all previous approaches for being too vague in fundamental respects. I believe that there may be limits to the specifiability of linguistic terms, however, due to their phenomenal flexibility. Cognitive linguists generally assume that language serves to trigger associations in people X  X  minds, or build them up on the spot, based on sketchy and flexible semantics along with discourse context. As Talmy (1988, page 165) puts it,  X  X e take a sentence (or other portion of discourse) to evoke in the listener a particular kind of experiential complex, here to be termed a  X  X ognitive representation. X  X  In other words, linguistic terms may not in the first place describe or represent meanings as such, but rather serve as triggers for activating concepts of human experience, which are far richer and more flexible than any lexical entry or formalization could possibly represent. These considerations resonate with current efforts to capture more adequately what has been characterized as  X  X mbodied cognition X  (Anderson 2003). To the extent that human cognition operates in a non-symbolic way, formalizations may ultimately remain inadequate in capturing human understanding of motion.
 cations that can be gained from the language of motion has led to a considerable step forward in a much-needed direction. It reaches far beyond traditional formal semantics approaches that basically leave the import of conceptual elements altogether untouched. The formal representation of motion is in this book carried further than ever before, combining decades of previous effort ingeniously towards extraction of motion information from natural language descriptions, to the extent possible given the current state of the art X  X nd maybe ultimately limited by the nature of human language. References

Markus Dickinson*, Chris Brew  X  , and Detmar Meurers  X  (*Indiana University,  X  Educational Testing Service, and  X 
Wiley-Blackwell, 2013, xviii+232 pp; paperbound, ISBN 978-1-4051-8305-5, $34.95; hardbound, ISBN 978-4051-8306-2, $87.95; e-book, ISBN 978-1-1183-2316-8, $22.99 Reviewed by Mats Wir  X  en Stockholm University
Any textbook on computational linguistics today must position itself relative to Jurafsky and Martin (2009), which, by virtue of its depth and comprehensiveness, is the standard introduction and reference for speech and language processing. Readers without sub-stantial background may find that book too demanding, however, and then Language and Computers , by Dickinson, Brew, and Meurers, may be a better choice. Roughly, this is how Language and Computers distinguishes itself from Jurafsky and Martin (2009): 1. It does not presuppose any computing background. More specifically, as 2. It is structured according to major applications (treated in six chapters), 3. Its topic is processing of text. There is some discussion in the first chapter 4. It is introductory, typically aimed at a quarter-length course according to written and spoken language are represented in computers. First, the basic writ-ing systems are described: alphabetical (roughly, one character X  X ne phoneme), syl-labic (roughly, one character X  X ne syllable), logographic (roughly, one character X  X ne morpheme/word), and major encodings are addressed (in particular, Unicode). A de-scription of the nature of speech and its representation in waveforms and spectrograms follows, including sections on how to read spectrograms and on language modeling using n -grams. The two latter sections come under the heading of  X  X nder the Hood. X 
Sections so headed (typically, one or two per chapter) provide digressions into selected technical material that provide bonuses for the interested reader, but which can be omitted without losing the gist of each chapter.
 parts: checking spelling and checking grammar. The first part includes an overview of different kinds of spelling errors, and methods for detection and correction of errors.
These include a description of dynamic programming for calculating the minimum edit distance between a misspelled word and a set of candidate corrections. The second tion of context-free grammar for the purpose of specifying the norm (the well-formed sentences) of the language. (The possibility of enriching the formalism with features is mentioned later in the same chapter.) It is then stated that  X  X w]hen a parser fails to parse a sentence, we have an indication that something is grammatically wrong with the sentence X  (page 58). At the same time, it is recognized that the most one can do is to build grammar fragments. So what about undergeneration, and the challenges of maintaining a large, manually encoded grammar? Instead of elaborating on this, the chapter goes on with a brief description of methods for correcting errors. These include relaxation-based techniques that discharge grammar rules, and special-purpose rules or n -grams that trigger directly on erroneous sequences of words. The reader is left wondering what one could expect from a grammar fragment. Although this may not be relevant to grammar checking, it would have been instructive to have a mention somewhere of the possibility of augmenting context-free rules or other formalisms with probabilities, and of how this has affected wide-coverage parsing (Clark and Curran 2007).
 tically aware. To this end, the concepts (but not the inner workings) of tokenization, part-of-speech tagging, and syntactic parsing are described. An example language tutoring system for learners of Portuguese is then addressed. Compared with traditional workbook exercises, the system gives immediate feedback on orthographic, syntactic, and semantic errors, and also contains audio.
 of the chapter is contained in two comprehensive sections that deal with searching in unstructured data (typically the Web) and semi-structured data (such as Wikipedia). The former section contains a relatively detailed description of search engines and
PageRank, as well as an overview of HTML. Evaluation of search results is mentioned, but the measures are described in more detail in the next chapter. The section on semi-structured data contains a description of regular expressions, Unix grep , and finite-state automata. The chapter also contains brief sections on searching of structured data (databases, using Boolean expressions) and of text corpora (mainly discussing corpus annotation). Given that many of the readers of this book will be linguists, it is perhaps surprising that the book has so little material related to corpus linguistics, but it could be argued that this topic would require a text of its own (and that it is not an  X  X pplication X ). 778
Anyway, the  X  X urther Reading X  section of the chapter includes some good suggestions on this topic.
 concepts in machine learning. Then there is a detailed description of how to measure success in classification, with precision/recall, true/false positives/negatives, and the related measures of sensitivity and specificity used in medicine. After this, two examples of document classifiers are described in some detail: naive Bayes and the perceptron. Finally, there is a short section on sentiment analysis. This chapter has a good balance between applications-oriented and theoretical material, and gives a good grasp of each of them.
 examines in detail an example spoken dialogue transcript from the Carnegie Mellon
University Let X  X  Go! Bus Information System, followed by a thorough description of dialogue moves, speech acts, and Grice X  X  conversational maxims. After this ambitious background, one would expect some (even superficial) material on the anatomy of a dialogue system founded on these principles, but instead there is a detailed description of Eliza . The motivation is that  X  X  Let X  X  Go! ] is . . . too complicated to be explained fully in this textbook X  (page 167). The ambition to explain a working system in full is admirable, but seems misdirected here. Why not try to work out the basic principles of a simpler question-answering system? As it stands, the applications-oriented material ( Eliza )is not connected to the theoretical parts of the chapter. It is also potentially misleading when the authors say:  X  X  Eliza ] works reasonably well using simple means, and this can be useful if your application calls for a straightforward but limited way of creating the illusion of an intelligent being at the other end of the wire X  (page 170). Here, the authors must be referring to chatbots, but for applications in general, it would have been valu-able to have a pointer to principles of user interface design, such as trying to maintain consistency (Cohen, Giangola, and Balogh 2004). On a different note, this chapter would profit from picking up the thread on speech from Chapter 1. For example, it might be instructive to have an explanation of word-error rate and the degradation of input typi-cally caused by a speech recognizer. This would also give an opportunity to elaborate on one of the problems mentioned at the outset of the chapter:  X  X f]ixing confusions and misunderstandings before they cause the conversation to break down X  (page 154). that gives a good grasp of both applications-oriented and theoretical material. Example-based translation and translation memories are briefly discussed, and, after some back-ground, word alignment, IBM Model 1, the noisy channel model, and phrase-based statistical translation are explained. Commercial translation is also addressed. In con-nection with the translation triangle, the possibility of an interlingua is discussed, the conclusion being that  X  X d]espite great efforts, nobody has ever managed to design or build a suitable interlingua X  (page 189) and  X  X e probably cannot have one any time soon X  (page 190). This is true for a universal interlingua in the sense of a fully abstract language-independent representation, but what might be more relevant is domain-specific interlinguas, which have proved to be feasible in recent years (Ranta 2011).
Interlinguas are also mentioned later in the chapter, at the end of  X  X nder the Hood 12 X  (page 205) on phrase-based statistical translation, where it is stated that  X  X t]here is certainly no interlingua in sight. X  Although this is different, Google Translate actually uses English as an interlingua for a large majority of its language pairs (Ranta 2010).
Surely, the reason for this is that Google typically has much more parallel data for language pairs where English is one of the members than for language pairs where this is not the case. pact of language technology on society (effects of automatization of natural-language tasks), the human communication system as opposed to animal communication, human self-perception when computers begin to use language to interact, and ethical considerations.
 showing the reader how the underlying techniques recur across applications. However, for numerous terms such as n-gram, part-of-speech tagging, parsing, supervised learning, and so forth, many (sometimes a majority) of the occurrences in the book are not covered by the index. It would be highly desirable to improve this for a future edition. between the applications-oriented and theoretical material, the overall impression is that Language and Computers successfully fills a niche: It serves its purpose well in being an introductory textbook in computational linguistics for people without a computing background. It is possible to debate both the applications chosen and the theory selected for presentation, but it is nonetheless good to see a general introduction with a clear ambition to explain some of the inner workings of the technology.
 in linguistics who lack a computing background. My experience is that the primary need of these students is corpus linguistics, because, above all, they need to learn how to practice linguistics as an empirical science. They soon reach a point, however, where they also need a broader introduction to the applications and methods of computational linguistics. Such a broad introduction is also needed by Bachelor X  X  students in linguis-tics, and by those studying to become language consultants, translators, and so on. This book would then be the natural choice.
 References
 Mapping Scientific Frontiers: The Quest for Knowledge Visualization by Chaomei Chen. New York: Springer Verlag; 2003, 256 pps, $79.95 (ISBN: 0-85233-494-0) Understanding the body of work in a research area, more specifically the key paradigms, under-explored areas, and contributors, is a necessary but arduous task, which requires ef-fort proportional to the size of the literature body. Mapping Scientific Frontiers describes a useful complement to the traditional literature review process X  X sing data mining, text min-ing, and visualization techniques to create graphical depictions or maps that can potentially illustrate the pioneers, paradigm shifts, and latent domain knowledge (i.e., under-explored aspects) in a research area. Although scientific frontier visualization is a relatively new area, there has been extensive work in related disciplines, such as scientometrics, biblio-metrics, and cartography. Chen discusses the influence of these related disciplines on the visualization of scientific frontiers and then describes the current state of the art, numerous real-world examples, and open research problems. For the real-world examples, Chen pro-vides some process guidance along with color visualizations and analyses to demonstrate how a researcher might create and analyze such maps to gain insight about a literature body.
The author outlines the following three goals for the book: 1. To provide a broad overview of similar ways of thinking and visualizing a variety of 2. To stimulate and foster interdisciplinary research between the information visualization 3. To introduce a specific way to operationalize the identification of scientific paradigms. Chen accomplishes the first objective by discussing and presenting visualizations of terres-trial, biological, concept, network, and other maps. He discusses the information science and information visualization disciplines and demonstrates links between them as a way to promote understanding and ideally collaboration among researchers in the two fields. He accomplishes the third objective by providing an in-depth discussion on how to con-struct and analyze visualizations to identify three aspects of a scientific frontier: pioneers, paradigm shifts, and latent domain knowledge. Another intention of the book is for it to be a resource for researchers in other related disciplines like cognitive psychology, philosophy, and social science; however, most of the discussion requires extensive domain knowledge on topics like information retrieval and data mining.

The book is organized around four major topics:  X 
Theories to describe how scientific knowledge grows (Chapter 1);  X 
A historical account of maps (Chapters 2 and 3); 506 IVORY  X 
T echniques or technology that make it possible to map scientific frontiers (Chapter 4); and  X 
Procedures for identifying the aforementioned aspects of a scientific frontier (Chapters 5 X 7).
 The book begins with a discussion of competing theories on the growth of scientific knowl-edge and an argument for visualization as a way of  X  X eeing X  science. The value of under-standing how knowledge grows is that ideally we can design visualizations to highlight growth patterns that are inherent in a body of literature and explore these growth patterns during our analyses. Chen mentions an interesting challenge for visualizations X  X here is a difference in novices X  and experts X  ability to comprehend them. The difference is not just due to visual analysis ability, it is also due to gaps in domain knowledge. As Chen points out later for several examples, extensive domain knowledge is needed to analyze a scientific frontier map. For an unfamiliar research area, where does scientific frontier mapping fit within the literature review process, given the required domain knowledge? Do we map the scientific frontier at the outset to get an overview of a research area before reading the literature? Do we map the scientific frontier after we have read some or all of the literature? Are there some questions that we can answer at the outset and some that we can only answer at the end? Although Chen considers this problem, he does not proffer any guidance on how or when to apply this approach.

Chapters 2 and 3 give a historical account on the use of maps to visualize the universe (ge-ography, star system, DNA, etc.) and the mind (similarity among concepts, images, patents, visualizations and following cartography guidelines like simplicity and clarity. Chen also discusses the value of depicting associations in visualizations. Unlike Chapter 2, Chapter 3 contains a highly technical discussion on dimensionality reduction techniques. This discus-sion seems more appropriate for the subsequent chapter which explores enabling techniques like information visualization, information retrieval, data mining, text mining, and related computer science approaches. Although Chapter 4 provides a broad overview of techniques for analyzing literature sources, it does not discuss software tools that are readily available for carrying out these analyses.

The last three chapters discuss procedures for identifying pioneers, paradigm shifts, and analysis. The innovations that Chen discusses have the potential to transform an arduous task into an exciting adventure, provided the right tools are available for doing so. Within each chapter, Chen presents several different analysis approaches that can be used, but there is no notion of the most insightful approach. Furthermore, given individual differences in visual thinking ability, one could argue that the example visualizations do not respect the cartography guidelines of simplicity and clarity. There is no evaluation of the visualizations; however, Chen does point out that visualization evaluation is still an open research problem.
Overall, the book is educational, enjoyable, and inspirational. Chen shares an important and compelling vision for scientific frontier mapping. He provides concrete guidance and e xamples on how to approach this problem. Furthermore, he provides a detailed synopsis of researchers who are interested in advanced applications of IR techniques. Because it BOOK REVIEW 507 provides insight on how to conduct a visual literature review, the book could also be a useful supplementary text in research methods courses, especially within information sci-ence programs where information retrieval and possibly data and text mining topics are addressed.

