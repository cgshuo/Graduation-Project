 Recent work introduced a probabilistic framework that mea-sures search engine performance information-theoretically. This allows for novel meta-evaluation measures such as In-formation Difference , which measures the magnitude of the difference between search engines in their ranking of doc-uments. for which we have relevance information. Using In-formation Difference we can compare the behavior of search engines X  X hich documents the search engine prefers, as well as search engine performance  X  X ow likely the search engine is to satisfy a hypothetical user. In this work, we a) extend this probabilistic framework to precision-oriented contexts, b) show that Information Difference can be used to detect similar search engines at shallow ranks, and c) demonstrate the utility of the Information Difference methodology by showing that well-tuned search engines employing different retrieval models are more similar than a well-tuned and a poorly tuned implementation of the same retrieval model. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and ef-fectiveness) Information Retrieval; Search Evaluation
One of the ways search engines are compared is by com-puting the magnitude of the difference between their perfor-mance using some IR evaluation measure. These measures attempt to quantify the satisfaction of a hypothetical user of a search engine. This is crucial information about a search engine, but it does not necessarily provide a great deal of insight into search engine behavior  X  X hich documents does the search engine prefer and why? For example it is possible for two search engines to retrieve wildly different documents, or to rank similar documents in a very different order, and yet receive the same score from an evaluation metric, even a diversity metric such as ERR-IA [3]. It is equally possi-ble for two ranked lists to be highly similar and yet for one system to have a much greater performance than the other.
Recently, Golbus and Aslam [2] introduced a probabilistic framework that measures performance information-theoretically. The advantage of this approach is that it pro-vides additional novel interpretations beyond simply esti-mating user satisfaction. For example, the authors defined Information Difference , which measures the magnitude of the difference between systems in their ranking of doc-uments for which we have relevance information. The au-thors demonstrated that Information Difference can be used to detect similar search engines whereas performance deltas cannot.

However, the probabilistic framework underlying Informa-tion Difference required a recall-oriented approach and was evaluated at rank 1000. This leads to the concerns that In-formation Difference detected the similarities between sys-tems based on the uninteresting long  X  X ail X  of the ranked lists. In this work, we a) adapt the probabilistic framework to precision-oriented tasks at shallow ranks. We demon-strate that b) even when evaluated at rank 20, Information Difference is still able to detect similar systems with high ac-curacy. Therefore, Information Difference relies on whether systems choose the same highly relevant documents at the top. Finally, we c) demonstrate the utility of the Infor-mation Difference methodology by showing that well-tuned search engines employing different retrieval models are more similar than a well-tuned and a poorly-tuned implementa-tion of a retrieval model, i.e. that a well-tuned instantiation of BM25 is more similar to a well-tuned instantiation of a language model than it is to a poorly tuned instantiation of BM25. In this section, we provide an overview of the Information Difference methodology described in [2].

Mathematically, one can view a search system as providing a total ordering of the documents ranked and a partial order-ing of the entire collection, where all ranked documents are preferred to unranked documents but the relative preference among the unranked documents is unknown. Similarly, one can view relevance assessments X  X ommonly referred to as QRELs X  X s providing a partial ordering of the entire collec-tion: in the case of binary relevance assessments, for exam-ple, all judged relevant documents are preferred to all judged non-relevant and unjudged documents, but the relative pref-erences among the relevant documents and among the non-relevant and unjudged documents is unknown. Thus, math-ematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial ordering of the collection induced by the relevance assessments .

Golbus and Aslam described a probabilistic framework within which to compare two such orderings, defined in terms of three things: (1) a sample space of objects, (2) a dis-tribution over this sample space, and (3) random variables over this sample space. For example, Golbus and Aslam de-fined a new evaluation measure, Relevance Information Correlation in the following way. Let the sample space, pairs of judged documents with different relevance grades. Let P = U be the uniform probability distribution over all such pairs of documents. We define a QREL variable Q over ordered pairs of documents as The ranked list variable R is computed by truncating the list at the last retrieved relevant document. Let r i represent the rank of document d i .

R [( d i ,d j )] = Relevance Information Correlation is the mutual informa-tion between the QREL variable Q and the truncated ranked list variable R .
 This quantity is estimated via Maximum Likelihood for a given QREL and system.

This definition is inherently recall-oriented. In this work, we propose a precision-oriented version, RIC@ k . RIC@ k dif-fers from RIC in two ways. First, we normalize with respect to the maximum possible RIC @ k of an ideal ranked list, as with nDCG. Second, we alter the probability distribution so as to give more weight to documents with higher relevance grades. To do so, we begin by observing that evaluation met-rics can be viewed as inducing probability distributions over ranks. For example, Carterette [1] defines the probability of stopping at a rank k according to nDCG as Imagine a QREL with R gmax documents relevant at the highest grade. According to the QREL these documents are equally likely to appear at ranks one through R gmax have zero probability of appearing anywhere else. There-fore, in any ideal ranked list, the probability associated with one of these documents will be P DCG ( k ) for some k with 1  X  k  X  R gmax . Therefore, we define the probability of a document as the average probability of the ranks at which the document can appear in an ideal list. If R g is the num-ber of documents that are relevant at grade g , then for a document d with such that rel ( d ) = g , the minimum rank Figure 1: Information Difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space. for this document in an ideal list i.e. after all of the documents with higher relevance grades, and the maximum rank is Then the probability associated with the document is where  X  is a normalizing constant. Note that the proba-bility of non-relevant documents is non-zero , and that this definition can also be used for binary relevance.

RIC requires us to define a probability distribution over document pairs, whereas Equation 7 defines a probability for documents. To create the appropriate distribution, we assume that each document in the pair is chosen indepen-dently, where  X  is a normalizing constant that ensures that P ( d forms a distribution.

We define RIC @ k by normalizing by the ideal ranked list, as in nDCG, and computing mutual information with re-spect to the probability distribution defined in 8.
Information Difference is inspired by the Boolean Alge-bra symmetric difference operator as applied to information space, corresponding to the symmetric difference between the intersections of the systems with the QREL (see Fig-ure 1). For two systems S 1 and S 2 , with Q and R defined as in Equations 1 and 2 respectively. We also define id @ k by using the probability distribution described in Equation 8, and by normalizing with respect to an ideal system. id @ k ( S 1 ,S 2 ) = I ( R S 1 ; Q | R S 2 ) + I ( R S 2 In this section, we employ the framework described in Section 2 to demonstrate the utility of the Information Dif-ference framework. In Section 3.1, we show that Information Difference can be be used to determine whether systems are similar, even at shallow ranks, whereas performance deltas cannot. In Section 3.2, we demonstrate the use of Informa-tion Difference as a tool for meta-evaluation by performing a simplistic experiment concerning the similarity between search engines.
We wish to detect whether two systems are similar. As a proxy for similarity, we will consider two systems submitted to TREC 1 to be  X  X imilar X  iff they were submitted by the same group. It is reasonable to assume that the majority of these systems were different instantiations of the same un-derlying technology, although there will be many instances where this is not the case at all. We repeat the experi-ment first performed by Golbus and Aslam [2] to show that the results also hold when performed in a precision-oriented fashion (at rank 20), and are therefore not dependent on the long and uninteresting  X  X ail X  of the ranked lists.
Using the same construction, we sorted all the systems submitted to TREC 8 and TREC 9 by RIC and RIC@20, and separated them into twenty equal-sized bins. By con-struction, each bin contained systems with small differences in performance. All systems within each bin are compared to one another using Information Difference and performance delta. Figure 2 shows the resulting ROC curves when Infor-mation Difference and performance delta are used to predict whether two systems meet our proxy for similarity described above. We also report the area under the ROC curves av-eraged over all four conditions (TREC 8 vs TREC 9; RIC
Th e annual, NIST-sponsored Text REtrieval Conference (TREC) creates test collections commonly used in academic research. vs RIC@20). It is quite evident that Information Difference, with an average AUC greater than 0.9, is quite capable of detecting similarities as reported by our proxy, even at shal-low ranks. It is equally evident that with an average AUC less than 0.6, performance delta is not capable of detecting similarities. This result is obvious X  X nformation Difference was constructed for just this purpose, whereas performance delta is not. However, we believe that this result is also important. Since performance delta has traditionally been used to measure the similarities between search engines, our notions of which systems are similar may be false.
In this section, we will attempt to determine whether the choice of retrieval model has a bigger impact on the behav-ior (rather than the performance ) of a search engine than does parameter tuning. To perform this experiment, we use a standard, state-of-the-art search engine, in this case the Terrier search engine [4], to create highly simple search en-gines, i.e. without query expansion, pseudo-relevance feed-back, etc., and analyze the results when our systems are run over TRECs 8 and 9. We compare 1) a query-prediction language model (LM) with Dirichlet smoothing, 2) BM25, and 3) Divergence from Randomness (DFR) models, 2 across a range of 21 different, evenly spaced,  X  X easonable X  param-eter values (see Figure 3), and the  X  X est X  of these observed parameter values which achieves the maximum performance (see Table 1). 3 As we can see from Table 1, the three models perform relatively consistently with one another. Figure 3 shows that, with the exception of BM25 and DFR at rank 20, each model has reasonably consistent performance with itself as parameters are tuned.
W e use In e B2 for RIC and PL2 for RIC@20.
Our goal is to compare search engines during the evaluation phase , when relevance assessments have already been used. Therefore we employ the best parameters for these queries, rather than optimal parameters applicable to future queries. Figure 4: Cumulative histograms of Information Difference between parameterizations of a standard retrieval model.

Using Information Difference we can now measure the sim-ilarity of these models are in terms of their behavior , rather than their performance . Recalling that a small Information Difference implies a high degree of similarity, consider Ta-ble 2, which shows the Information Difference between the best performing retrieval models. As a point of reference, using an Information Difference threshold of 0.1 would have achieved a roughly 92% average accuracy on the classifica-tion task described in Section 3.1. Therefore, it is quite likely that Information Difference would have failed in this case and considered these retrieval models to be the same system.

Now we consider the difference between instantiations of a single model. We instantiate each model with all 21 param-eter values and compute the Information Difference between each pair of instantiations. Figure 4 shows cumulative his-tograms of the Information Difference between all 21 choose RIC TREC8 TREC8@20 TREC9 TREC9@20 BM25 0.292 0.325 0.344 0.295 LM 0.314 0.294 0.358 0.277 DFR 0.323 0.302 0.349 0.296 Table 1: Best observed performance of standard re-trieval models.
 T able 2: Information Difference between stan-dard retrieval models with  X  X est X  parameters (Sec-tion 3.2). 2 pairs of these model instantiations. These histograms show that there is far more difference in behavior within a model across parameterizations than their is across models with the best parameterization. For example, consider the largest In-formation Difference between models, which is between our language model and BM25 on TREC 8 at rank 20. The In-formation Difference of 0.149 is smaller than roughly 40% of the pairs of BM25 models. The smallest Information Dif-ference is between BM25 and DFR on TREC 9 at rank 20. The Information Difference of 0.056 is smaller than all but roughly 45% of the pairs of LM models.
The probabilist framework developed by Golbus and Aslam leads to interesting, novel, and highly interpretable meta-evaluations within the same context as traditional evalua-tion. As originally introduced, this framework was only ap-plicable in deeply judged, recall-oriented contexts, greatly reducing its practical value. In this work, we extended this probabilistic framework to precision-oriented contexts, which are far more prevalent. We also showed two novel ap-plications of Information Difference, a tool developed within this framework. We showed that Information Difference can be used to detect similar search engines at shallow ranks. We also showed that Information Difference can be used as a tool for meta-evaluation by showing that well-tuned search engines employing different retrieval models are more simi-lar than a well-tuned and a poorly-tuned implementation of the same retrieval model. [1] Ben Carterette. System effectiveness, user models, and [2] Peter B. Golbus and Javed A. Aslam. A mutual [3] Peter B. Golbus, Javed A. Aslam, and Charles L.A. [4] I. Ounis, G. Amati, V. Plachouras, B. He,
