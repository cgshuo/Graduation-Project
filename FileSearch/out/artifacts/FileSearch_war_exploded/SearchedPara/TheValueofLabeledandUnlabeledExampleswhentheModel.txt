 Dept. of Computer Science and Engineering semi-supervised learning.
 distrib utions needs to be made. available for co-training ([4 , 8]).
 trib ution p ( x; y ) is a mixture of two parametric distrib ution p few labeled examples.
 underlying probability distrib ution.
 data and represent our ndings in Fig 1. P classication error of the optimal classier . The quantity P cal details to simplify the exposition. The classier , for which P underlying model.
 equipr obable class densities p probability of err or P unlabeled examples is P rates.
 frame work.
 Theor em 2.2. (Co ver and Castelli [5]) In a two class mixtur e model, let p wher e D = log f 2 p (1 ) R p p see pp-2103 [5]. The effect of dimensionality is not captured in their result. tainty , which we will refer to as Type-B.
 with labels f 1 ; 1 g . True class densities are always represented by p case of Type-A uncertainty the y are simply p p denote the mixing parameter by t and the indi vidual parametric class densities by f respecti vely and the resulting mixture density as tf mixture density is inde xed by a 2 d dimensional vector = [ represent the standard Euclidean norm in R d and by jjjj &gt; 0 , jjjj d inte ger variable and A; B are constants. 2.1 Type-A Uncertainty : Perfect Model Imperfect Inf ormation Type-I uncertainty inherently gives rise to a perturbation size dened by u denes a perturbation size con vergence of such estimation procedure is O q d number of labeled examples (follo wing [11 ]) but then it reduces only at a rate O q d belo w.
 cation rule.
 Strategy 1: p ples, P at least (1 P repr esent the reduction rate of this excess err or ( P R ee ( l ) After using l labeled examples P 2.2 Type-B Uncertainty: Imperfect Model model. The uncertainty in this case is specied by the perturbation size densities p assume that best approximations of p for i 2 f 1 ; 2 g ; ( f that jj p parametric model densities and in particular , if jj f jj f Thus, using only a very small number of labeled examples P number of labeled examples to P the presence of perturbation size, P only up to P decision boundary . For any such nonparametric technique P This trend is roughly what the follo wing theorem says. Here f not necessarily Gaussians. In what follo ws we assume that p for non parametric classication (see [14 ]) is O 1 condition is not satised.
 Theor em 2.4. In a two class mixtur e model with individual class densities p ver sion of f and P After using l labeled examples P (exponentially in the number of labeled examples) until P u analyze the effect of these examples and also of perturbation size of error .
 p Gaussian approximation is at most No w suppose we use the follo wing strate gy to use labeled and unlabeled examples. Strategy 2: Theor em 2.5. In a two class mixtur e model with equipr obable class densities p mixtur e density of the best tting par ametric model be 1 d dimensional spherical Gaussians with means f ; f 2 , suc h that for a perturbation size 2 &gt; 0 , jj f 1 p 1 jj d with u = O d 2 then for l l , P with probability at least (1 ) . If mor e labeled examples l &gt; l are provided, P asymptotically con ver ges to zer o at most at a rate O 1 rate of this excess err or ( P can compactly repr esented as, After using l labeled examples, P until P exponentially fast in the number of labeled examples until P that O ( ing P Corollary states this important fact.
 Cor ollary 2.6. For a perturbation size matrices. If using u = O d 2 tting model can be estimated O ( examples u &gt; u does not help in reducing the excess err or. In this section we discuss the effect of perturbation size number of labeled and unlabeled examples give rise to four dif ferent regions where P examples and the y axis corresponds to the number of labeled examples. model O ( Theorem 2.5, u = O d 3 P parameters of the best tting model O ( label the estimated decision regions so that P graphical representation of dif ferent regions where P Figure 1: The Big Pictur e. Beha vior of P ples 3.1 Beha vior of P In this region u u unlabeled examples estimate the decision regions and l which depends on u , are required to correctly label these estimated regions. P at a rate O (exp( l )) + O d of the decision regions will be bad and and corresponding l happens in region III. Thus in region I, l is restricted to l &lt; l and P exp ( O ( l )) + O d 3.2 Beha vior of P parameters of the best tting model can be estimated O ( not help reducing P and for small number of labeled examples l l , P 3.3 Beha vior of P In this region u u and hence model parameters have not been estimated O ( u parameters. Note that once the parameters have been estimated O ( is the number of such labeled examples, then in this region l &lt; l l on number of unlabeled examples u u , l , and l u . In presence of labeled examples alone, using Theorem 2.3, P effecti ve rate at which P two. 3.4 Beha vior of P In this region when u &gt; u ; l &gt; l and when u u ; l &gt; l the best tting model have been estimated O ( ( P then this rate is O ( 1 is much slo wer .
 Ackno wledgements This work was supported by NSF Grant No 0643916. [13] V. N. Vapnik. Statistical Learning Theory . Wiley, Ne w York, 1998.
