 Information extraction (IE) systems discover structured in-formation from natural language text, to enable much richer querying and data mining than possible directly over the unstructured text. Unfortunately, IE is generally a com-putationally expensive process, and hence improving its ef-ficiency, so that it scales over large volumes of text, is of critical importance. State-of-the-art approaches for scaling the IE process focus on one text collection at a time. These approaches prioritize the extraction effort by learning key-word queries to identify the  X  X seful X  documents for the IE task at hand, namely, those that lead to the extraction of structured  X  X uples. X  These approaches, however, do not at-tempt to predict which text collections are useful for the IE task X  X nd hence merit further processing X  X nd which ones will not contribute any useful output X  X nd hence should be ignored altogether, for efficiency. In this paper, we focus on an especially valuable family of text sources, the so-called deep web collections, whose (remote) contents are only ac-cessible via querying. Specifically, we introduce and study techniques for ranking deep web collections for an IE task, to prioritize the extraction effort by focusing on collections with substantial numbers of useful documents for the task. We study both (adaptations of) state-of-the-art resource se-lection strategies for distributed information retrieval, and IE-specific approaches. Our extensive experimental eval-uation over realistic deep web collections, and for several different IE tasks, shows the merits and limitations of the alternative families of approaches, and provides a roadmap for addressing this critically important building block for efficient, scalable information extraction.
A large body of today X  X  knowledge is embedded in natural language text documents. Information extraction (IE) sys-tems identify and extract information from text into a struc-tured form, such as relational tables. For example, an IE sys-tem to extract an Occurs -in ( Natural Disaster , Location )re-c lation would extract the tuple earthquake, Mexico from the text fragment  X  X  powerful earthquake shook a wide area of Mexico. X  Much richer querying and data mining are possi-ble over structured information than over the unstructured text where it originates. Unfortunately, IE is generally a computationally expensive process [22], and hence improv-ing its efficiency, so that it scales to large volumes of text, is of critical importance.

In this paper, we focus on IE over deep web collections , which are text document collections that are accessible only through a query interface and are hence not  X  X rawlable X  through traditional search engine crawlers [7,17,27,28]. Ex-amples of such collections include the Federal Emergency Management Agency (FEMA) collection 1 , a key up-to-date resource for natural disasters and other hazards in the United States; and PubMed 2 , a well-known resource for life sciences and biomedical research with over 22 million abstracts and references to research papers. Deep web collections often host high-quality content and together span a broad range of topics. Furthermore, the collective content in deep web collections may exceed in volume, according to some esti-mates, that of the crawlable, or  X  X urface X  web [18,24].
To run an IE system over a deep web collection, a key chal-lenge is effectively and efficiently retrieving its useful doc-uments, namely, the documents from which the IE system manages to extract tuples. To address this challenge, tech-niques such as QXtract [2], PRDualRank [12], FactCrawl [8], and RSVM-IE and BAgg-IE [6] learn a collection-specific set of text queries (e.g., consisting of words and phrases) to target the useful documents and ignore the rest, thus reduc-ing the overall extraction cost drastically. These techniques, however, do not attempt to predict which text collections are useful for the IE task X  X nd hence merit further processing X  and which ones will not contribute any useful output X  X nd hence should be ignored, for efficiency.

In this paper, we introduce and address the problem of ranking deep web collections for an IE task, to prioritize the extraction effort by focusing on collections with sub-stantial numbers of useful documents for the IE task. An approach for this task should rightfully conclude, for exam-ple, that FEMA is better for extracting Occurs -in tuples than PubMed. This collection ranking problem is related to the problem of resource selection in distributed informa-tion retrieval [29, Chapter 3], to identify topically relevant collections for a given user query. Unlike in distributed IR, though, our IE scenario requires that we identify collections http://www.fema.gov/ http://www.ncbi.nlm.nih.gov/pubmed with useful documents for the IE task, rather than docu-ments that are topically relevant for a given query. Despite this difference in focus, we can adapt resource selection ap-proaches to our IE scenario, as we will see, as well as develop alternative, IE-specific approaches.

To prioritize deep web collections for an IE task, we must identify the most useful collections, namely, the collections with the largest numbers of useful documents for the IE task. Therefore, we need to estimate the number of useful documents in each collection and, importantly, we need to do so efficiently (e.g., by issuing a relatively small number of queries to each collection). For this estimation problem, we could exploit state-of-the-art techniques for measuring certain deep web collection properties (e.g., their number of documents) [4, 34, 35]. Unfortunately, as we will see, such techniques can be prohibitively expensive for IE, because they may need to issue many queries to sufficiently cover the (often rare) useful documents for an IE task of interest.
To address the limitations of generic estimation techniques, and to effectively rank deep web collections for a given IE task, we develop approaches that target the useful docu-ments for the IE task in question. We compare both (adap-tations of) state-of-the-art resource selection strategies and IE-specific approaches in an extensive experimental evalua-tion over realistic deep web collections, and for several dif-ferent IE tasks. In summary, our key contributions are:  X  We introduce the problem of ranking deep web collec-tions for efficient and scalable IE (Section 2).  X  We study traditional as well as IE-specific approaches for estimating, for each deep web collection, the number of useful documents for a given IE task (Section 3).  X  We report the results of an extensive evaluation of both (adaptations of) traditional approaches for distributed infor-mation retrieval and IE-specific approaches over real-world deep web collections and for several different IE tasks. Our results show the merits and limitations of the alternative families of approaches, and provide a roadmap for addressing this critically important building block for efficient, scalable information extraction (Sections 4 and 5).
Our focus is on the efficiency and scalability of the IE pro-cess over deep web text collections, whose contents can only be accessed via querying and cannot be retrieved using tra-ditional Web  X  X rawlers. X  To run an IE system over the avail-able deep web collections, a na  X   X ve, expensive approach could resort to state-of-the-art approaches for efficient query-based IE execution (e.g., [2,6,8,12]) over each deep web collection individually. 3 Such a na  X   X ve approach would be unnecessar-ily expensive, because not all collections contain any useful documents, or documents from which the IE system at hand manages to extract tuples. Therefore, to prioritize the IE effort, for efficiency, we focus on the problem of ranking deep webcollectionsforanIEtaskofinterest.

A related problem, resource selection , has been studied in the context of distributed information retrieval, to rank col-lections according to their topical relevance to a given user query [29]. Resource selection approaches generally consist of two steps: (1) descriptor generation: in an offline step,
Our approach is not applicable over open information ex-traction scenarios (e.g., [3]) where documents frequently con-tribute tuples to the open-ended extraction task.
Figure 1: Collection ranking for IE over the deep web. build a compact, representative collection summary (e.g., consisting of word frequency vectors [11, 15] or document samples [30,32]); (2) relevance estimation: to process a given query, use the collection descriptors to estimate the number of topically relevant documents in each collection, and rank the collections accordingly.

Unlike in distributed IR, our IE scenario requires that we identify collections with useful documents for the IE task, rather than collections with documents that are topically relevant to a given query. As a result, the collection descrip-tors will need to effectively capture the characteristics of the useful documents, a challenging proposition because of two critical reasons. First, the notion of document usefulness is, by definition, specific to a given IE task, so our collec-tion ranking approaches X  X nd the collection descriptors on which they rely X  X ill have to be flexible to adapt to each given IE task. In particular, the  X  X ne-size-fits-all X  descrip-tors adopted by resource selection approaches for distributed IR would not be appropriate for our IE scenario. Second, the fraction of documents in a collection that are useful for an IE task can many times be very small, so our collec-tion ranking approaches X  X nd the estimation techniques on which they rely X  X ill have to effectively target the useful documents, to keep the ranking overhead to manageable lev-els. In particular, state-of-the-art techniques for estimating certain deep web collection properties [4,34,35], as discussed above, would be prohibitively expensive for our IE scenario. We summarize the problem that we address as follows:
Problem Definition 1. Consider a set of deep web col-lections and an information extraction task T with its corre-sponding (previously trained) information extraction system. Our goal is to rank the collections according to their num-ber of useful documents for the IE task T (see Figure 1). Furthermore, the ranking process should be efficient (e.g., in terms of the number of queries issued to each collection), to keep its overhead to reasonable levels.

Earlier efforts to identify collections for an extraction task (e.g., [1, 21]) have focused on examining the quality of the extraction output, rather than its volume. The (comple-mentary) methods described in this paper can be adapted to consider quality (see Section 6).
To prioritize the IE effort and rank deep web collections for an IE task, we need to estimate the number of useful documents for the IE task in each collection. Specifically, for each collection C , we will estimate the cardinality of C the set of useful documents in C for the IE task at hand. In this section, we first provide an overview of three families of state-of-the-art estimation approaches that we can adapt forthetask(Section3.1)andthendescribeeachmethodin detail (Sections 3.2 through 3.4). Figure 2: An example (query, document)-graph: the esti-mate contribution f ( d 1 ) from document d 1 has a 1 3 weight (since its sampled degree is 3) and f ( d 3 ) is counted twice, each time with weight 1 2 .
Given an IE task, we can cast the problem of estimating the number of useful documents for the task in a deep web collection as an instance of the generic task of estimating a  X  X roperty X  of interest for a deep web collection, which has been studied extensively in the research literature. Such a property F of a collection C is typically defined as an aggregate over a document-level function, say, f ( d ). A sim-ple example is the estimation of the number of documents in a deep web collection C :inthiscase, f ( d )=1forev-ery document d in C ,and F ( C )= d  X  C f ( d ). In our IE scenario, to estimate the number of useful documents we should define f ( d )= 1 { d is useful } , that is, as the indica-tor function that returns 1 if d is useful and 0 otherwise. Various methods have been proposed to estimate proper-ties of (queryable) document collections (e.g., collection size, number of documents relevant to a query, average document length) [4,19,34,35], and these methods can be classified in three broad classes: (i) surrogate-based methods, (ii) query pool-based methods, and (iii) query pool-free methods.
Surrogate-based methods construct an approximate repre-sentation of the entire collection, and then use that surrogate to estimate the metric of interest, without further accessing the actual collection. A surrogate typically comprises a (rel-atively small) document sample (e.g., [30,32]), or document frequency estimations for the terms occurring in the collec-tion (e.g., [11,15]). In resource selection for distributed IR, such representations have been widely used to estimate the number of documents relevant to a query (e.g., CORI [11], GlOSS [15], ReDDE [32], and Relax [30]). For example, ReDDE builds a random document sample S foreachcol-lection C , once and for all. Simply stated, to judge the rel-evance of C to a query q , ReDDE extrapolates the number of documents relevant to q in S to the entire collection.
Query pool-based methods pick queries from a predefined query pool Q (e.g., a dictionary of words or n -grams col-lected from extensive web crawls) to retrieve X  X rom the col-lection at hand X  X ocuments from which to estimate the met-ric of interest. Unlike the collection-specific surrogates, the query pool can be shared across collections and can also be targeted specifically to the estimation task at hand. The query pool-based method in [23] aims at estimating collec-tion size effectively but is inefficient: for large collections, the samples required to produce accurate estimates are very large. Subsequent (query pool-based) methods addressed this limitation, and sample random edges from a (query, document)-graph, as sketched in Figure 2: the graph ver-tices are queries q  X  Q and documents d  X  C ,andasolid Table 1: Summary of the characteristics of the baseline and IE-specific methods of Section 3. edge ( q,d )meansthat q retrieves d .Foreachsampled( q,d ) pair, the measure f ( d ) contributes to the estimation of F ( C ) with a weight that is proportional to the probability of hav-ing sampled d (see Section 3.3). Pool-based methods work well, provided the query pool Q retrieves all documents of interest for the metric (i.e., for which f ( d ) =0).
Query pool-free methods avoid relying on a query pool, and rather find the queries to issue  X  X n the fly. X  These methods use a seed query (e.g., a common word or phrase) to re-trieve a first set of documents and select the next query to issue from these documents. Thus, they issue queries and retrieve documents to perform a random walk on a graph where nodes are either queries or documents. To properly weigh the particular f ( d ) value derived from visiting a node in such graph, these methods use the fact that the probabil-ity of visiting a node during a random walk is proportional to its number of incident edges.

In our IE scenario, we need to estimate the number of use-ful documents in a deep web collection for an IE task of in-terest. Unfortunately, only very few or no useful documents might be present in a truly random document sample from the collection, given that useful documents might be rare for an IE task. This poses a problem for all three families of es-timation methods summarized above. The next subsections describe the existing approaches in detail and derive IE-specific estimators that effectively target useful documents. By aiming to collect documents for which f ( d ) = 0, our IE-specific methods are designed to have more non-zero terms in their estimation, thus alleviating the limitations of existing estimation techniques for other tasks. Each subsection first discusses a state-of-the-art baseline method and how we can apply it to our IE scenario; the second part introduces an IE-specific method, designed specifically for collection use-fulness estimation. Table 1 summarizes the requirements of each method to handle collections and IE tasks. Baseline (ReDDE): Our first (baseline) estimator is an adaptation of ReDDE, a resource selection technique for distributed IR [31]. To estimate the number of topically relevant documents for a query q in a collection C , ReDDE predicts the relevance of a representative sample S  X  C and scales it to the entire collection with a factor SF = | C to obtain a collection relevance metric Rel( q, C ). This met-ric relies on estimating (i) the collection size | C | and (ii) the relevance of sample documents in S to the given query q . The size | C | is query-independent and thus computed once and for all, while the query relevance for d is based on issuing q to a centralized global sample unifying all individual collec-tion samples. To apply ReDDE to our IE scenario, we model the IE task as a set of high-performing queries Q ie ,which we can automatically learn (e.g., see QXtract [2]). We thus calculate collection relevance for the IE task as a sum of the ReDDE relevance metric, taken over the top-k queries from IE-specific (Surrogate): An alternative, IE-specific esti-mation approach is to collect a random sample S from a collection C , run the IE system over the documents in S , and extrapolate the number of useful documents to the full collection C as in ReDDE, namely,  X  | C u | = SF  X | S u | scaling factor SF = | C | / | S | . Unfortunately, a random sam-ple might have very few or no useful documents because, as discussed, useful documents for an IE task are often rare. To address this problem, the document sample S should be then biased towards useful documents. This implies that we subsequently have to correct the scaling factor SF for that usefulness bias of S . The main idea is to look at doc-ument frequency differences of certain terms between the sample S and the full collection C .Foragiventerm t ,let df ( t, X ) be the fraction of documents in X that contain t , and define the frequency ratio in the sample vs. in the full this ratio over useful terms , T u , as a heuristic scaling fac-tor: SF = E T u 1 / X  S t , where useful terms are those that are (i) biased towards usefulness, in contrast to  X  X eutral X  terms that would appear equally in useful and useless documents; and (ii) overrepresented in the sample (i.e.,  X  S t &gt; 1). The rationale for this choice of SF is that the overrepresentation of such useful terms in S compared to in C is a good proxy for the overrepresentation of useful documents. (We can find T u through standard statistical significance tests.) Impor-tantly, we assume that we know the document frequencies df ( t, C ) in the complete collection, as well as the collection size | C | . We can estimate these values reliably once and for all for each collection (e.g., see [20] and [31]). Baseline (PB): We consider the method of Bar-Yossef et al. [4], which set the foundations for subsequent query pool-based estimators (e.g., [34]) and allows estimating any generic metric that can be expressed as a discrete integral over the documents in C :Int  X  ( f ) the target function of interest and  X  D weighs documents as needed (e.g.,  X  D ( d ) = 1 if all documents contribute equally). As indicated in Section 3.1, documents d  X  C are obtained by issuing queries from a pool Q .Bar-Yossef et al.  X  X  method relies on two core ideas: (i) extend  X  D to a measure over the (query, document)-space Q  X  C , and (ii) apply importance sampling to use a practical sampling strategy, selecting a (query, document)-pair with probability p ( q,d ), rather than from the probability distribution induced by  X  D ,whichis unfeasible to sample from. For (i), the extended measure is: where C q is the set of documents that q retrieves from C and  X  ( d ) is the degree of document d , defined as the number of queries in Q that retrieve d . For (ii), the practical sampling strategy is: (1) pick a random query q from the set of queries that return at least one document, noted as Q + ,andthen (2) randomly pick one of the documents it retrieves: In importance sampling, p ( q,d )and  X  ( q,d ) induce probabil-ity distributions referred to as the trial and target distribu-tions, respectively, and define the importance weight as: Bar-Yossef et al. use an efficient estimator u ( q,d )of w ( q,d ), defined as u ( q,d )=  X  D ( d )  X  PSE  X | C q | X  IDE( d ), with a pool size estimator PSE for | Q + | and an inverse degree estimator IDE( d )for1 / X  ( d ), in turn, to calculate the approximate importance sampler (AIS): AIS( q,d ) f ( d )  X  u ( q,d ). The authors show that if u approximates w well, and if the ratio u/w is uncorrelated with f , AIS remains largely unbiased. A variant of the above estimator for sums [4] computes factor cancels out) and obtains the estimator by multiply-ing Avg( f ) by the size of the collection. This collection size estimation can be done once and for all, and reused for dif-ferent metrics (e.g., for the usefulness for different IE tasks), to amortize its cost.

However, directly using AIS in our IE scenario is prob-lematic: (i) the number of queries to issue and subsequent IE-processing of returned documents to determine f ( d ), to find some useful documents, may be high; and (ii) the esti-mation will have high variance because of the few non-zero f ( d )values. 4 To address the first limitation for the related problem of counting the frequency of a given word (e.g.,  X  X ports X ) in a collection, Zhang et al. identify the queries that are positively correlated with the word (e.g., query  X  X olf X ) [34]. The query sampling process is then stratified over correlated and uncorrelated queries. Unfortunately, this approach still requires issuing a large number of queries. IE-specific (PB-W): We adapt the Bar-Yossef et al. ap-proach, with target and trial distributions that are aligned with f ( d ) for usefulness: Our target distribution assigns probabilities greater than 0 only to useful documents: where C u q is the set of useful documents that q retrieves from C .Our trial distribution , accordingly, should (i) re-trieve useful documents with high recall and precision, and (ii) be efficient to sample from. Specifically, for recall, we learn queries from a large, external text collection E that we can process once and for all. For precision, we learn the usefulness of queries with respect to the given IE task: (1) process E with the IE system at hand, (2) query E with all words in the documents, and (3) count the useful and useless documents within the top-k results for each query q , noted as | E u q | and | E n q | , respectively. Finally, our trial dis-tribution assigns a selection weight to each query q , defined as w  X | E u q | + | E n q | . 5
Given these query weights, our trial process consists of two steps: (1) pick a useful query q proportionally to its weight,
The variance can be reduced via Rao-Blackwellization, as suggested in [4], which requires running IE over all retrieved documents. We evaluate this version of the algorithm later in the experimental section.
E n q | in the selection weight operates as a smoothing factor. and (2) pick a document from q  X  X  useful results uniformly at random. This yields the following trial distribution: of the probability distribution induced by the queries that retrieve at least one useful document, Q u + . We can now obtain our importance weight function as: which we need to compute only over the useful documents. Similarly to [4], we rely on an efficient estimator u u of w defined as: to keep estimation costs to reasonable levels. Here, NFE and UE( q ) are estimators of normalization factor Z w and number of useful documents retrieved by a query.

The key challenge in computing NFE is to account for the number of queries | Q u + | and the distribution of their selec-tion weight. We can compute them both on the fly while sampling during our trial process: Both factors can be com-puted by sampling according to the selection weight, instead of uniformly at random, as in Bar-Yossef et al.  X  X  PSE. We compute NFE by keeping track of the fraction of sampled queries that retrieve at least one useful document, defined as  X  , and computing  X   X  q  X  Q ( w  X | E u q | + | E n q | UE( q ), we proceed similarly to Bar-Yossef et al.  X  X  IDE com-putation: we sample documents uniformly at random from C q until we find a useful one; if we find the useful document after processing n documents, then UE( q )= | C q | n . Baseline (PF): We focus on the method introduced by Zhang et al. [35], using a query graph: (i) the nodes are h -grams 7 q that retrieve at least u documents 8 and (ii) undi-rected edges connect a node pair ( q,q )if q matches (i.e., appears in the text of) at least one of the documents that q retrieves, and vice versa. Since a random walk implies that anode q is visited with a probability proportional to its de-gree d ( q ), each per-query estimate is weighted with 1 /d ( q ) to agree with uniform sampling. The estimation of a func-tion F ( C )= d  X  C f ( d ) from sampled queries S in (query) graph Q is:
The | E u q | and | E n q | values in this formula are computed once and for all during the generation of the queries.
Zhang et al. argue that h = 1 works well in practice. Parameter u controls the size and connectivity of the graph. Zhang et al. propose u =3.
 Here,  X  ( d ) is the number of queries in Q that retrieve doc-ument d . Furthermore, the size of the query graph  X  | Q | estimated from a startup query collection V C , which is a sample (e.g., obtained from the documents of another ran-dom walk, independent of S ) of the vocabulary of h -grams appearing in the collection, and its estimated fraction  X  that retrieve at least u documents when issued as a query. While eliminating a potential coverage issue by avoiding an a priori query pool, the resulting estimation may still re-quire many queries in the IE scenario, to find sufficiently many useful documents.
 IE-specific (PF-W): To estimate the number of useful documents, we could restrict the graph to only useful queries (i.e., queries that retrieve at least one useful document and for which f ( d ) = 1). However, such graph could be largely disconnected and the random walk would be unable to fully explore it. Instead, we keep the original graph and modify the random walk process to favor visiting useful queries: We define a weighted graph , on which we perform a  X  X eighted X  random walk (i.e., edge e with weight w ( e ) is selected from an edge set E with probability w ( e ) / e  X  E w ( e )). We de-fine an edge ( q,q )tobe useful if and only if both queries it connects retrieve at least one useful document. We assign useful edges weight w and the others 1. In Equation (1) we thus replace the original (unweighted) degree d ( q )withthe weighted counterpart: d w ( q )= w  X  N u + N n , where N u number of q  X  X  useful incident edges (i.e., useful neighbors q ) and N n = N all  X  N u (with N all = d ( q )). The definition of  X  ( d ) (i.e., the number of queries that retrieve it) remains unchanged, thus we still estimate it using the method in [35].
To estimate the weighted degree d w ( q )ofasampledquery q we need (i) the number of all incident edges N all ,and (ii) the number of useful edges N u .For N all we proceed as in [35]. For N u we proceed similarly, now counting the number of sampling attempts n u we need to find a q that both matches q and is useful, to estimate  X  N u = | Q | /n Thus, we calculate  X  d w ( q )= w  X   X  N u +(  X  N all  X   X  N
In this section, we described the baseline and IE-specific approaches that we study in this paper. We now describe the settings for our experimental evaluation of these techniques (Section 4) and report our results (Section 5). Collections: Our test set consists of 96 real web collections across different topics, collected using an approach similar to that in [16] over the Open Directory Project (ODP) 11 . Specifically, we first selected the 8 top-level ODP categories with the largest number of entries, namely, Business, Soci-ety, Arts, Science, Computers, Recreation, Shopping, and Sports. We then selected the 5 most popular subcategories in each of the 8 initial categories. In turn, we also picked the 5 most popular subsubcategories from each subcategory, for a total of 200 subsubcategories. For each subsubcategory, we then randomly chose 7 unique web collections that have a text search interface. Finally, we collected their contents using a state-of-the-art query-based sampler [10], issuing at most 20,000 queries and retrieving up to 1,000 documents for each. In our test set, we kept the collections that produced
We correct an erroneous 1 / | S | factor from [35, eq. (5)].
Implementation-wise, we can get n all , the sampling at-tempts for N all , from the same sampling sequence as n u http://www.dmoz.org/ Table 2: Fraction of useful documents found in the TREC 1-5 collections for relations extracted using two IE systems, SSK and BONG. We use (*) only during tuning. at least 10,000 documents following this method, to focus the evaluation on collections with a substantial number of documents. Our tuning set , which we use for tuning param-eters of the various techniques, consists of 40 collections se-lected randomly from among the collections under the above subsubcategories and not in the test set. We collected doc-uments from these tuning collections using the Nutch Web crawler 12 . We indexed each collection, in both the tuning set and the test set, with the text retrieval toolkit Lucene to emulate the query-only behavior of deep web collections and only access them through their query interface. We exhaustively processed the collections with our IE systems (see below) to obtain the real number of useful documents in each collection. We also used TREC 1-5 collections 14 for different operations (e.g., query pool construction), which we describe as needed throughout this section.
 Information Extraction Systems: We evaluated a va-riety of IE systems and components for all relations in our experiments (see below) via 5-fold crossvalidation over a set of training documents, and selected the two best-performing combinations, namely, Subsequence Kernel (SSK) [9] and Bag of n -Grams (BONG) [14]. We implemented them using REEL 15 [5]. We also considered different named entity tag-gers and selected: for person and location entities, the pre-trained conditional random fields (CRF) [26] from the Stan-fordNLP package 16 ;for natural disasters , CRFs from the etxt2db framework 17 ; for the remaining entities, maximum entropy markov models (MEMM) [25], also from etxt2db. Relations: For a robust evaluation, we include 5 substan-tially different relations in the experiments (see Table 2). Four such relations, namely, Natural Disaster X  X ocation, Man Made Disaster X  X ocation, Person X  X harge, and Election X  X in-ner, are sparse, in that very few documents tend to be useful for them. In contrast, relation Person X  X areer is a dense re-lation. (Table 2 shows the percentage of useful documents for each relation in the TREC 1-5 collections for the two IE systems. Also, Figure 3 shows the distribution of useful documents over the collections in our test set.) Technique Tuning: We tuned each technique over the tuning set and using the SSK IE system over Man Made Disaster X  X ocation.
 Surrogate-based methods (Section 3.2): (1) Baseline: We evaluated different query sets Q and numbers of queries k . For Q , we generated one-word queries using the SVM ap-http://nutch.apache.org/ http://lucene.apache.org/ http://trec.nist.gov/data.html http://reel.cs.columbia.edu/ http://nlp.stanford.edu/software/CRF-NER.shtml http://web.ist.utl.pt/rui.lageira/ Figure 3: Fraction of useful documents for each relation across our 96 test collections. The box boundaries are the 25th and 75th percentiles, the bold horizontal line inside each box is the median, and the dots denote outliers. proach in [2] and two effective feature selection methods, namely, Information Gain and  X  2 test [36], over 10,000 doc-uments (50% useful and 50% useless) from TREC 1-5; we kept the words that are discriminative of the useful class; also, and as suggested in [2], we evaluated a query set includ-ing all words in the documents and another one removing the tuple attribute values. We varied k  X  [10 , 200] in intervals of 10. We built the document samples using the query-based sampling technique in [10], which produces nearly-random document samples from words collected from retrieved doc-uments. (2) IE-specific: We compared (i) the query-based sampling technique in ReDDE; (ii) the bootstrapping-based sampling approach in [2], which starts from a set of seed tuples as queries that is iteratively expanded as it collects more useful documents and extracts tuples from them; (iii) a sampling technique that issues the queries in Q above based on their score. Finally, to find the useful terms and to asses term bias we used the Fisher exact test in [13] varying p -value  X  [0 . 01 , 0 . 1].
 Query pool-based methods (Section 3.3): We built two query pools of single words from TREC 1-5: (i) a generic query pool ( G ) of 4M words considering all documents; and (ii) an IE specific query pool ( S ) that only considers words from useful documents. We performed the estimation process with and without Rao-Blackwellization, suggested in the original paper for variance reduction [4]. We evaluated the sum and average approaches. For the IE-specific method, we also: (i) evaluated several different values for the weight w  X  [50 , 500000]; (ii) used the TREC 1-5 collections as our external collection; and (iii) varied the number of documents to retrieve k  X  [10 , 1000] for weight computation. Query pool-free methods (Section 3.4): We varied three pa-rameters common to the baseline and IE-specific approach: (i) the number of sampled queries | S | X  [10 , 500], with in-crements of 10, (ii) the length of h -grams, h  X  X  1 , 2 , 3 (iii) the length of the burn-in of the random walk b before collecting the samples, b  X  [50 , 500] with increments of 50. For the IE-specific method, we evaluated several different values for its weight w  X  [10 , 5000].
 Techniques for Comparison: We compared the following alternatives over the 96 collections in our test set, with the settings derived via tuning, as summarized below:  X  Baseline surrogate-based (ReDDE): We generate the samples with the query-based document sampler in [10], is-suing up to 300 queries and collecting (at most) 5 documents for each. We use the queries learned with  X  2 for Q and use the top-100 queries (i.e., k = 100).  X  IE-specific surrogate-based method (Surrogate): For sam-pling, we derive Q using the  X  2 method and excluding tuple attributes. We considered the useful queries in order of  X  Our sample S has at most 1000 documents. To select the useful terms T u ,weuse p -value = 0.05 in the Fisher test.  X  Baseline query-pool-based (PB): We use the sum (ABS) and average (AVG) methods using the G and Q query pools and applying Rao-Blackwellization. (The impact on effi-ciency of applying Rao-Blackwellization is low, because the number of documents processed with the IE system is small.)  X  IE-specific query-pool-based method (PB-W): As with the baseline, we evaluate sum (ABS) and average (AVG) estimators using the G and S query pools and applying Rao-Blackwellization. We index our external collection us-ing Lucene with default parameters. We use k = 1000 for the query weight computation and w = 5000.  X  Baseline query-pool-free (PF): We use an English dic-tionary to randomly find an initial query for the estimation process. We use single terms as queries (i.e., h =1)andonly accept queries that retrieve at least u =3documents,as suggested in [35]. We collect 100 queries for V c , the startup query path that can be shared across IE tasks, and at most 100 for the estimation sample queries S .  X  IE-specific query-pool-free method (PF-W): We use a similar configuration to that in PF, but with w = 1000. Additional Settings: Estimators issue at most 100 queries, and retrieve up to 50 documents per query. To account for randomness, we run each estimator five times and report av-erage values over the five runs. Finally, note that all estima-tors contain some form of (weighted) averaging of a metric over samples S (e.g., for the pool-free estimator, we calcu-late the individual contribution of each q in the summation in the denominator of Equation (1)). We filter outliers from this average, using the outlier detection algorithm in [33,  X  X ethod I X  X  as implemented in the R 18 package X  X xtremeval-ues X : a value x is an outlier if it is outside the limit where less than 1 observation is expected, based on observed data within quantile limits [  X , 1  X   X  ]. We evaluated every estima-tor with and without outlier removal (using  X   X  X  0 . 05 , 0 . 1 Removing outliers using  X  =0 . 1 performed best across all techniques. Thus, our final results include such removal. Evaluation Metrics: We measure ranking quality and estimation cost with the following metrics:  X  Cumulative Gain (CG@ k ): We measure the number of useful documents that we obtain by processing the collec-tions in ranking order. If u i is the number of useful doc-uments in the i th collection, we define CG@ k = k i =1 u This metric focuses on absolute values of useful documents, which makes the comparison across relations problematic, and also does not fully capture  X  X rrors X  in the ranking.  X  Normalized Discounted Cumulative Gain (nDCG@ k ): nDCG@ k alleviates the limitations above in a robust man-ner. Specifically, nDCG@ k is defined as the normalized version of Discounted Cumulative Gain DCG@ k = u 1 + der. 19 Now, to normalize DCG@ k  X  X nd obtain nDCG@ k  X , we need to calculate the DCG@ k of an ideal ranking, namely,  X  Processed Documents ( PD ) and Issued Queries ( IQ ): We measure the efficiency of the IE process in terms of the number of issued queries and processed documents, and not running time. The reasons for this are twofold: (1) many http://www.r-project.org/
Variants of DCG@ k exist in the literature; the version we use accounts for the distribution of useful documents. factors (e.g., network traffic, collection responsiveness) can distort running times in the distributed environments on which we focus and are difficult to capture reliably; and (2) the number of issued queries and processed documents are good indicators of expected running time. Finally, we re-port these values only for the actual estimation process and ignore the initial, once-and-for-all processing (e.g., collection size estimation or random document sample generation) re-quired by the techniques, which gets amortized over time.
We now evaluate the baseline and IE-specific ranking ap-proaches of Section 3, with the settings of Section 4. Quality of collection ranking approaches: We evalu-ate the ranking approaches over all relations and IE systems of Section 4. Figure 4 shows nDCG@ k of all techniques over the entire rank of collections (i.e., k  X  [1 , 96]) for Natural Disaster X  X ocation using the BONG IE system, and issuing at most 100 queries. (Other relations and IE systems X  X ith the exception of Person X  X areer, which we study in detail later X  X ielded similar results. Also, we later vary the num-ber of issued queries.) Figure 4a, for the surrogate and query pool-based methods, shows that the PF baseline outper-forms PF-1000, its IE-specific counterpart, by almost 75%. PF-1000 requires on average more queries than PF to walk the random graph; for this reason, PF-1000 will rarely find useful documents at such small query budget. As we will see, when PF-1000 finds useful documents, its performance improves considerably, always overcoming its baseline coun-terpart. In contrast, among the surrogate methods, Surro-gate outperforms ReDDE by almost 50%: the IE-specific document sample, although small, manages to include use-ful documents; also, ReDDE X  X  collection descriptors do not accurately characterize the useful documents.

Figures 4b and 4c, for the query pool-based methods for sums and averages, show that the IE-specific versions also outperform the baseline counterparts. For sum, this differ-ence is mainly based on the number of useful documents sampled during the estimation, because there are more non-zero components to include in the estimation. For this rea-son, PB-S-ABS-5K is best, with its weighted specific query pool highlighting potentially useful queries. For average, the quality of the ranking also depends on finding queries that retrieve a combination of useful and useless documents, as both types are crucial for computing the average in ques-tion. PB-S-AVG and PB-G-AVG-5K sample such queries and thus exhibit the highest quality in this family. Overall, and across families, the top contenders are Surrogate and the average pool-based estimators, which effectively exploit both useful and useless documents during estimation.
To understand the actual number of useful documents ob-served as we process collections in ranking order, Figure 5 shows CG@ k for Natural Disaster X  X ocation and Person Ca-reer using the BONG IE system, for a selection of high-quality techniques according to our experiments. For refer-ence we also show the CG@ k of an ideal ranking (labeled Ideal) and of a random ranking (noted as a dashed line). Figure 5 reveals substantial gains from prioritizing the col-lections for extraction. For Natural Disaster X  X ocation, for example, Surrogate effectively identifies the collections con-taining 95% of the total useful documents within the top-10 collections. This would translate to an efficiency improve-ment of almost 90% if we were to only process these top-10 collections, because we would ignore 86 (out of 96) test collections. A noticeable benefit is also shown for Person X  Career, which we discuss in detail later in this section, for Surrogate, ReDDE, and PB-G-AVG-5K.
 Efficiency of collection ranking approaches: To under-stand the efficiency of the different approaches, we analyze the extraction cost and achieved ranking quality at differ-ent stages in the estimation process. Figures 6 and 7 show, respectively, the number of documents processed with the IE system and nDCG@10 for different numbers of issued queries, for Person X  X harge using the SSK IE system. (Dif-ferent relations and IE systems yielded analogous conclu-sions.) We show the same technique splits as in the above analysis, for clarity. Figure 6 shows that IE-specific tech-niques process on average more documents than their base-line counterpart. ReDDE is an exception: it does not incur querying or extraction costs during the estimation process.
The reasons as to why IE-specific approaches process more documents on average are manifold and differ for each fam-ily of techniques. Notably, for Surrogate (Figure 6a), the number of extracted documents grows with the size of the document sample, because all sampled documents need to be processed. For the pool-free family (also in Figure 6a), PF-1000 may need to process several documents before choosing the next  X  X op, X  due to its weighted walking strategy. In con-trast, PF does not need this operation, since it navigates the graph only based on the document contents, without incur-ring additional extractions. However, these techniques may retrieve several hundred documents from each collection for graph construction, to extract h -grams from the documents.
Finally, for the pool-based families (Figures 6b and 6c), the extraction cost grows with the number of observed useful documents (from the sampled edges). In fact, after obtain-ing a useful document further operations take place: (i) for sum, the inverse degree estimator issues additional queries to the collection, although it avoids processing the docu-ments with the IE system; the average estimators do not need inverse degree estimation and, therefore, more docu-ments are often processed for the same query budget; and (ii) the Rao-Blackwellization method for variance reduction processes all documents that the current query retrieves; this increases the extraction cost, but this cost is compara-ble across the techniques we evaluated.

After analyzing the extraction cost, we also study the ranking quality associated with such costs. Figure 7 shows that the quality of the ranking improves with the number of issued queries: The estimators learn more X  X nd more reliably X  X bout the collections as the estimation progresses, Figure 5: CG@ k for Natural Disaster X  X ocation (left) and Person X  X areer (right) for the BONG IE system and issuing (at most) 100 queries. whichisreflectedinbetterestimations. Theseimprove-ments, though, vary considerably across techniques: Sur-rogate, for instance, produces high-quality estimates even when issuing a small number of queries, whereas the re-maining techniques improve their quality progressively, with a steeper gain for IE-specific than for baseline approaches. Furthermore, some IE-specific techniques yield higher qual-ity rankings than other techniques at a fraction of their cost. For example, after issuing 25 queries, PB-G-ABS-5K and PB-G-AVG-5K exhibit comparable quality to PB-S-ABS and PB-S-AVG after issuing 100 queries, respectively. Impact of collection characteristics: Deep web collec-tions are rather heterogeneous, with substantial differences in size and contents. Notably, large collections are prob-lematic for baseline approaches, as Figure 5a shows for Nat-ural Disaster X  X ocation, where the most useful collections were among the largest collections (350,000 documents on average). We showed the effectiveness of our IE-specific ap-proaches for this case. Similarly, the collection contents also affect some of the other approaches: Pool-based approaches retrieved substantially fewer documents than pool-free ap-proaches (see Figure 6), because many queries in the query pool were not topically relevant to the contents of the col-lections; pool-free approaches do not exhibit this problem. Impact of IE-task characteristics: To understand the impact of relation characteristics, we now focus on a dense relation. Figure 8 shows nDCG@ k over the entire rank of collections (i.e., k  X  [1 , 96]) for Person X  X areer using the BONG IE system, and for all techniques. We show the same technique splits as in the above analysis, for clarity. Fig-ure 8a shows that Surrogate, PF, and PF-1000 exhibit com-parable (low) quality: these techniques failed to correctly rank large collections with a large number X  X ut a relatively low fraction X  X f useful documents. Specifically, Surrogate was unable to obtain discriminative terms for the estima-tion, since most sampled documents were useful. The pool-free approaches, on the other hand, were unable to reach many useful documents, because the useful documents in the top collections were a small fraction in each collection. The pool-based sum estimators in Figure 8b exhibit a trend similar to that of pool-free approaches. However, two sets of techniques performed relatively well, namely, ReDDE (Fig-ure 8a) and the IE-specific pool-based AVG estimators (Fig-ure 8c). These techniques benefited from the scaling to the entire collection, because the largest collections were among the top most useful collections.
 Additional discussion: An orthogonal, interesting aspect to the discussion above concerns the overhead incurred by a ranking approach when new collections or IE tasks arrive. Table 1 summarized the collection-and IE-specific require-ments of each ranking technique. For new collections, almost all techniques require some preprocessing (see collection col-umn in Table 1). Deciding the approach to adopt will not only depend on the performance and overhead of the tech-niques but also on the number of IE systems that we will run over each (new) collection. Because size estimation and graph construction may take up to several thousand queries, approaches that rely on these will only be reasonable when many IE systems are involved, as the cost will amortize over time. In other cases, though, estimators that do not require additional information from the collection, such as pool-based estimators for sums, may be the best choice.
Similarly, for new IE tasks, almost all techniques also re-quire some preprocessing (see IE column in Table 1). The most expensive process is, by far, producing a specific query pool (with or without query weights). The remaining pro-cesses, namely, learning queries for document sampling or for querying the descriptor in ReDDE and producing a query sample for the pool-free techniques, are relatively inexpen-sive. Therefore, and given our quality and efficiency results, surrogate methods (i.e., ReDDE or Surrogate) seem to be the most reasonable choice for new IE tasks. However, if new collections are expected to appear at high rates, it may be worthwhile building X  X nd amortizing the construction of X  X  query pool, so that the estimation starts without overhead.
In this paper, we introduced and addressed the problem of ranking deep web collections for an IE task, to prioritize the extraction effort by focusing on collections with substantial numbers of useful documents for the IE task. Specifically, the problem is that of effectively and efficiently ranking a set of deep web collections according to their number of useful documents for a given IE task. We studied both (adapta-tions of) state-of-the-art resource selection strategies, and IE-specific approaches. We performed an extensive exper-imental evaluation over realistic deep web collections, and for several different IE tasks. Our evaluation focused on the quality and efficiency characteristics of the ranking ap-proaches, with the following conclusions: (1) we found top contenders for each of these characteristics and provided in-sight on how to choose among them; (2) we analyzed which techniques are better suited for certain characteristics of the collections, such as their size and contents, and of the IE tasks, such as their sparsity; and (3) we discussed the over-head incurred by each technique in dynamic domains, where new collections or IE tasks may arrive. Overall, this paper provides a roadmap for addressing this critically important building block for efficient, scalable information extraction.
As interesting future work, we will consider other factors for collection ranking, such as the quality, diversity, and nov-elty of the extraction output from the collections. Some of these factors (e.g., quality) just require defining an appro-priate target measure f (e.g., as a function of the confidence scores of the tuples extracted from a document), as sug-gested in [4] and [35]. However, other factors (e.g., diversity and novelty) would also require analyzing the contents of multiple collections simultaneously, because the extraction output of one collection may be included in that of others. Acknowledgments: This material is based upon work sup-ported by the National Science Foundation under Grant IIS-08-11038 and by Bloomberg L.P. C. Develder was supported by a travel grant from the Research Foundation  X  Flanders (FWO). This work was also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Depart-ment of Interior National Business Center (DoI/NBC) con-tract number D11PC20153. The U.S. Government is autho-rized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily repre-senting the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.
