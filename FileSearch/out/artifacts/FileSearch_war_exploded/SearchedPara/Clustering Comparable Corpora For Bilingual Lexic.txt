 Bilingual lexicons are an important resource in mul-tilingual natural language processing tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Balles-teros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to dif-ferent domains, researchers have tried to automat-ically extract bilingual lexicons from various cor-pora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different lan-guages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McK-eown, 1997; Fung and Yee, 1998; Rapp, 1999; D  X  ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lex-icon extraction from comparable corpora is a dis-tributional hypothesis, stating that words which are translation of each other are likely to appear in simi-lar context across languages. On top of this hypoth-esis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across lan-guages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010) an approach based on improving the comparability of the cor-pus under consideration, prior to extracting bilingual lexicons. This approach is interesting since there is no point in trying to extract lexicons from a corpus with a low degree of comparability, as the probabil-ity of finding translations of any given word is low in such cases. We follow here the same general idea and aim, in a first step, at improving the compara-bility of a given corpus while preserving most of its vocabulary. However, unlike the previous work, we show here that it is possible to guarantee a cer-tain degree of homogeneity for the improved corpus, and that this homogeneity translates into a signifi-cant improvement of both the quality of the resulting corpora and the bilingual lexicons extracted. We first introduce in this section the comparability measure proposed in former work, prior to describ-ing the clustering-based algorithm to improve the quality of a given comparable corpus. For conve-nience, the following discussion will be made in the context of the English-French comparable corpus. 2.1 The Comparability Measure In order to measure the degree of comparability of bilingual corpora, we make use of the measure M developed in (Li and Gaussier, 2010): Given a com-parable corpus P consisting of an English part P e and a French part P f , the degree of comparability of P is defined as the expectation of finding the trans-lation of any given source/target word in the tar-get/source corpus vocabulary. Let  X  be a function indicating whether a translation from the translation set T w of the word w is found in the vocabulary P v of a corpus P , i.e.: and let D be a bilingual dictionary with D v e denoting its English vocabulary and D v f its French vocabulary. The comparability measure M can be written as:
M ( P e , P f ) (1) = where # w ( P ) denotes the number of different words present in P . One can find from equa-tion 1 that M directly measures the proportion of source/target words translated in the target/source vocabulary of P . 2.2 Clustering Documents for High Quality If a corpus covers a limited set of topics, it is more likely to contain consistent information on the words used (Morin et al., 2007), leading to improved bilin-gual lexicons extracted with existing algorithms re-lying on the distributional hypothesis. The term ho-mogeneity directly refers to this fact, and we will say, in an informal manner, that a corpus is homogeneous if it covers a limited set of topics. The rationale for the algorithm we introduce here to enhance corpus comparability is precisely based on the concept of homogeneity. In order to find document sets which are similar with each other (i.e. homogeneous), it is natural to resort to clustering techniques. Further-more, since we need homogeneous corpora for bilin-gual lexicon extraction, it will be convenient to rely on techniques which allows one to easily prune less relevant clusters. To perform all this, we use in this work a standard hierarchical agglomerative cluster-ing method. 2.2.1 Bilingual Clustering Algorithm The overall process retained to build high quality, homogeneous comparable corpora relies on the fol-lowing steps: 1. Using the bilingual similarity measure defined 2. Pick high quality sub-clusters by threshold-3. Combine all these sub-clusters to form a new 4. Use again steps (1), (2) and (3) to enrich the The first three steps aim at extracting the most com-parable and homogeneous subpart of P . Once this has been done, one needs to resort to new corpora if one wants to build an homogeneous corpus with a high degree of comparability from P L . To do so, we simply perform, in step (4), the clustering and thresholding process defined in (1), (2) and (3) on two comparable corpora: The first one consists of the English part of P L and the French part of an ex-ternal corpus P T ; The second one consists of the French part of P L and the English part of P T . The two high quality subparts obtained from these two new comparable corpora in step (4) are then com-bined with P H to constitute the final comparable corpus of higher quality. 2.2.2 Similarity Measure Let us assume that we have two document sets (i.e. clusters) C 1 and C 2 . In the task of bilingual lexi-con extraction, two document sets are similar to each other and should be clustered if the combination of the two can complement the content of each single set, which relates to the notion of homogeneity. In other words, both the English part C e 1 of C 1 and the French part C f 1 of C 1 should be comparable to their counterparts (respectively the same for the French part C f 2 of C 2 and the English part C e 2 of C 2 ). This leads to the following similarity measure for C 1 and C : sim ( C 1 , C 2 ) =  X   X  M ( C e 1 , C f 2 ) + (1  X   X  )  X  M ( C where  X  (0  X   X   X  1) is a weight controlling the importance of the two subparts ( C e 1 , C f 2 ) and ( C C ). Intuitively, the larger one, containing more in-formation, of the two comparable corpora ( C e 1 , C f 2 ) and ( C e 2 , C f 1 ) should dominate the overall similar-ity sim ( C 1 , C 2 ) . Since the content relatedness in the comparable corpus is basically reflected by the re-lations between all the possible bilingual document pairs, we use here the number of document pairs to represent the scale of the comparable corpus. The weight  X  can thus be defined as the proportion of possible document pairs in the current comparable corpus ( C e 1 , C f 2 ) to all the possible document pairs, which is: where # d ( C ) stands for the number of documents in C . However, this measure does not integrate the rel-ative length of the French and English parts, which actually impacts the performance of bilingual lexi-con extraction. If a 1-to-1 constraint is too strong (i.e. assuming that all clusters should contain the same number of English and French documents), having completely unbalanced corpora is also not desirable. We thus introduce a penalty function  X  aiming at penalizing unbalanced corpora: The above penalty function leads us to a new simi-larity measure sim l which is the one finally used in the above algorithm: The experiments we have designed in this paper aim at assessing (a) whether the clustering-based algo-rithm we have introduced yields corpora of higher quality in terms of comparability scores, and (b) whether the bilingual lexicons extracted from such corpora are of higher quality. Several corpora were used in our experiments: the TREC 1 Associated Press corpus ( AP , English) and the corpora used in the CLEF 2 campaign including the Los Ange-les Times ( LAT94 , English), the Glasgow Herald ( GH95 , English), Le Monde ( MON94 , French), SDA French 94 ( SDA94 , French) and SDA French 95 ( SDA95 , French). In addition, two monolingual cor-pora Wiki-En and Wiki-Fr were built by respectively retrieving all the articles below the category Society and Soci  X  et  X  e from the Wikipedia dump files 3 . The bilingual dictionary used in the experiments is con-structed from an online dictionary. It consists of 33k distinct English words and 28k distinct French words, constituting 76k translation pairs. In our ex-periments, we use the method described in this pa-per, as well as the one in (Li and Gaussier, 2010) which is the only alternative method to enhance cor-pus comparability. 3.1 Improving Corpus Quality In this subsection, the clustering algorithm described in Section 2.2.1 is employed to improve the quality of the comparable corpus. The corpora GH95 and SDA95 are used as the original corpus P 0 (56k En-glish documents and 42k French documents). We consider two external corpora: P 1 T (109k English documents and 87k French documents) consisting of the corpora LAT94 , MON94 and SDA94 ; P 2 T (368k English documents and 378k French documents) consisting of Wiki-En and Wiki-Fr .
After the clustering process, we obtain the result-ing corpora P 1 (with the external corpus P 1 T ) and P 2 (with P 2 the method described in (Li and Gaussier, 2010) on the same data, producing resulting corpora P 1 0 (with P 1 T ) and P 2 0 (with P 2 T ) from P 0 . In terms of lexical coverage, P 1 (resp. P 2 ) covers 97.9% (resp. 99.0%) of the vocabulary of P 0 . Hence, most of the vocabulary of the original corpus has been preserved. The comparability score of P 1 reaches 0.924 and that of P 2 is 0.939. Both corpora are more comparable than P 0 of which the comparabil-ity is 0.881. Furthermore, both P 1 and P 2 are more comparable than P 1 0 (comparability 0.912) and P 2 0 (comparability 0.915), which shows homogeneity is crucial for comparability. The intrinsic evaluation shows the efficiency of our approach which can im-prove the quality of the given corpus while preserv-ing most of its vocabulary. 3.2 Bilingual Lexicon Extraction Experiments To extract bilingual lexicons from comparable cor-pora, we directly use here the method proposed by Fung and Yee (1998) which has been referred to as the standard approach in more recent studies (D  X  ejean et al., 2002; Gaussier et al., 2004; Yu and Tsujii, 2009). In this approach, each word w is rep-resented as a context vector consisting of the words co-occurring with w in a certain window in the cor-pus. The context vectors in different languages are then bridged with an existing bilingual dictionary. Finally, a similarity score is given to any word pair based on the cosine of their respective context vec-tors. 3.2.1 Experiment Settings In order to measure the performance of the lexi-cons extracted, we follow the common practice by dividing the bilingual dictionary into 2 parts: 10% of the English words (3,338 words) together with their translations are randomly chosen and used as the evaluation set, the remaining words being used to compute the similarity of context vectors. En-glish words not present in P e or with no translation in P f are excluded from the evaluation set. For each English word in the evaluation set, all the French words in P f are then ranked according to their sim-ilarity with the English word. Precision and recall are then computed on the first N translation candi-date lists. The precision amounts in this case to the proportion of lists containing the correct translation (in case of multiple translations, a list is deemed to contain the correct translation as soon as one of the possible translations is present). The recall is the proportion of correct translations found in the lists to all the translations in the corpus. This evaluation procedure has been used in previous studies and is now standard. 3.2.2 Results and Analysis In a first series of experiments, bilingual lexicons were extracted from the corpora obtained by our ap-proach ( P 1 and P 2 ), the corpora obtained by the approach described in (Li and Gaussier, 2010) ( P 1 0 and P 2 0 ) and the original corpus P 0 , with the fixed N value set to 20 . Table 1 displays the results ob-tained. Each of the last two columns  X  P 1 &gt; P 0  X  and  X  P 2 &gt; P 0  X  contains the absolute and the rel-ative difference (in %) w.r.t. P 0 . As one can note, the best results (in bold) are obtained from the cor-pora P 2 built with the method we have described in this paper. The lexicons extracted from the enhanced corpora are of much higher quality than the ones ob-tained from the original corpus . For instance, the increase of the precision is 6.9% (30.5% relatively) in P 1 and 23.5% (104.0% relatively) in P 2 , com-pared with P 0 . The difference is more remarkable with P 2 , which is obtained from a large external cor-pus P 2 T . Intuitively, one can expect to find, in larger corpora, more documents related to a given corpus, an intuition which seems to be confirmed by our re-sults. One can also notice, by comparing P 2 and P 2 0 as well as P 1 and P 1 0 , a remarkable improve-ment when considering our approach and the early methodology.

Intuitively, the value N plays an important role in the above experiments. In a second series of ex-periments, we let N vary from 1 to 300 and plot the results obtained with different evaluation measure in Figure 1. In Figure 1(a) (resp. Figure 1(b)), the x -axis corresponds to the values taken by N, and the y -axis to the precision (resp. recall) scores for the lexi-cons extracted on each of the 5 corpora P 0 , P 1 0 , P 2 P 1 and P 2 . A clear fact from the figure is that both the precision and the recall scores increase accord-ing to the increase of the N values, which coincides with our intuition. As one can note, our method con-sistently outperforms the previous work and also the original corpus on all the values considered for N . As previous studies on bilingual lexicon extrac-tion from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extract-ing bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original cor-pus, whereas our work aims at preserving most of the vocabulary of the original corpus.

We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extract-ing information from it. However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which fi-nally leads to comparable corpora of higher quality. In turn such corpora yield better bilingual lexicons extracted.
 This work was supported by the French National Re-search Agency grant ANR-08-CORD-009.

