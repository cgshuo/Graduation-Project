 There has been much recent interest in adapting data mining algorithms to time series databases. Most of these algorithms need to compare time series. Typically some variation of Euclidean distance is used. However, as we demonstrate in this paper, Euclidean distance can be an extremely brittle distance measure. Dynamic time warping (DTW) has been suggested as a technique to allow more robust distance calculations, however it is computationally expensive. In this paper we introduce a modification of DTW which operates on a higher level abstraction of the data, in particular, a Piecewise Aggregate Approximation (PAA). Our approach allows us to outperform DTW by one to two orders of magnitude, with no loss of accuracy.
 Time series, similarity measures, Dynamic Time Warping. Time series are a ubiquitous form of data occurring in virtually every scientific discipline and business application. There has been much recent work on adapting data mining algorithms to time series databases. For example, Das et al attempt to show how association rules can be learned from time series [5]. Debregeas and Hebrail [6] demonstrate a technique for scaling up time series clustering algorithms to massive datasets. Keogh and Pazzani introduced a new, scaleable time series classification algorithm [12]. Almost all algorithms that operate on time series data need to compute the similarity between time series. Euclidean distance, or some extension or modification thereof, is typically used. However, Euclidean distance can be an extremely brittle distance measure. Consider the clustering produced by Euclidean distance sequence 4, yet it appears more similar to 1 or 2.
 The reason why Euclidean distance may fail to produce an intuitively correct measure of similarity between two sequences is Consider Figure 2.A. The two sequences have approximately the same overall shape, but the shapes are not aligned in the time axis. The nonlinear alignment shown in Fig 2.B would allow a more sophisticated distance measure to be calculated.
 A method for achieving such alignments has long been known in the speech processing community [20]. The technique, Dynamic Time Warping (DTW), was introduced to the data mining community by Berndt and Clifford [3]. Although they demonstrate the utility of the approach, they acknowledge that the algorithms time complexity is a problem and that "...performance on very large databases may be a limitation".
 As an example of the utility of DTW compare the clustering shown in Figure 1 with Figure 3.
 In this paper we introduce a technique which speeds up DTW by a large constant. The value of the constant is data dependent but is typically one to two orders of magnitude. The algorithm, Piecewise Dynamic Time Warping (PDTW), takes advantage of the fact that we can efficiently approximate most time series by a piecewise aggregate approximation.
 The rest of this paper is organized as follows. Section 2 contains a review of the classic DTW algorithm. Section 3 introduces the Piecewise Aggregate Approximation and PDTW algorithm. In Section 4 we experimentally compare DTW, PDTW and Euclidean distance on several real world datasets. Section 5 contains a discussion of related work. Section 6 contains our conclusions. 1 2 4 3 Figure 1. An unintuitive clustering produced by the Euclidean distance measure. Sequences 1 to 3 are astronomical time series [7]. Sequence 4 is simply a straight line with the same mean and variance as the other sequences.
 Suppose we have two time series Q and C, of length n and m respectively, where: To align two sequences using DTW we construct an n-by-m distance d ( q distance, d ( q to the alignment between the points q below) set of matrix elements that defines a mapping between Q and C . The k th element of W is defined as w
The warping path is typically subject to several constraints.  X  Boundary conditions: w  X  Continuity: Given w  X  Monotonicity: Given w
There are exponentially many warping paths that satisfy the above conditions, however we are interested only in the path which minimizes the warping cost: warping paths may have different lengths.
 programming to evaluate the following recurrence which defines current cell and the minimum of the cumulative distances of the adjacent elements: The Euclidean distance between two sequences can be seen as a special case of DTW where the k th element of W is constrained such that w special case where the two sequences have the same length. The time complexity of DTW is O( nm ).
 This review of DTW is necessarily brief; we refer the interested reader to [16] for a more detailed treatment. In this section we introduce the piecewise aggregate approximation and a DTW algorithm for the representation. We denote a time series query as X = x dimensionality of the transformed time series we wish to work with (1  X  N  X  n ). For convenience, we assume that N is a factor of n . This is not a requirement of our approach, however it does simplify notation.
 = . The i th element of X is calculated by the following equation: 1 2 3 4 Figure 3. When the dataset used in Figure. 1 is clustered using DTW the results are much more intuitive.
 Simply stated, to reduce the data from n dimensions to N dimensions, the data is divided into N equi-sized "frames". The vector of these values becomes the data reduced representation. Figure 5 illustrates this notation. The complicated subscripting in Eq. 6 is just to insure that the original sequence is divided into the correct number and size of frames.

Figure. 5: An illustration of the data reduction technique utilized in this paper. A time series consisting of eight ( n ) points is projected into two ( N ) dimensions. The time series is divided into two ( N ) frames and the mean of each frame is calculated. A vector of these means becomes the data reduced representation. Two special cases worth noting are when N = n the transformed representation is identical to the original representation. When N = 1 the transformed representation is simply the mean of the original sequence. More generally the transformation produces a piecewise constant approximation of the original sequence, we therefore call our approach Piecewise Aggregate Approximation (PAA). Figure 6 illustrates a natural time series and its PAA approximation. We denote the ratio of the length of the original time series to the length of its PAA representation, the compression rate c . In choosing a value for c there is a classic tradeoff between memory savings and fidelity. In this work we do not address the problem of choosing the  X  X est X  compression rate. The  X  X est X  compression rate depends on the structure of the data itself and the applications the best approach may be to have an expert interact with the data and choose this parameter, although automated approaches to similar problems have been suggested [22,15]. In Section 2 we showed how to perform dynamic time warping on two sequences Q and C . Here we will show how to perform dynamic time warping using the reduced dimensionality versions of Q and C , which we denote we call the algorithm defined on the reduced dimensionality representation Piecewise Dynamic Time Warping (PDTW).
 To align two sequences using PDTW we construct an N -by-M distance d ( distance between two elements is defined as the square of the distance between them: Apart from this modification the matrix-searching algorithm is essentially unaltered. Equation 5 is modified to reflect the new distance measure: When reporting the DTW distance between two time series (Eq. 4) we compensated for different length paths by dividing by K , the length of the warping path. We need to do something similar for PDTW but we cannot use K directly, because elements in the warping matrix now correspond to aggregate segments of data and we would like PDTW to be measured in the same units as DTW to facilitate comparison between the two measures. To compensate for this we can use a distance measure that is similar to Eq. 4 but where the denominator is the square root of the compression rate. Because the length of the warping path is measured in the same units as DTW we have: Figure 7 shows strong visual evidence that SDTW finds alignments that are very similar to those produced by DTW. In the next section we will provide strong experiment evidence to the same effect.
 The time complexity for a PDTW is O( NM ), where M = m / c and N = n / c . The time complexity for the original DTW algorithm is O( nm ). So the speedup obtained by PDTW should be O( nm )/O( MN ) which is O( c 2 ).
B) A) warping as DTW. We are interested in two properties of the proposed approach. The speedup obtained over the classic DTW algorithm and the quality of the alignment. In general, the quality of the alignment is subjective, so we designed experiments that indirectly, but objectively measure it. For our clustering experiments we utilized two datasets, one natural and one synthetic. 1) The Australian Sign Language (ASL) dataset from the UCI
The Cylinder-Bell-Funnel (CBF) synthetic dataset as used in Figure 8 shows some examples of the Cylinder and Funnel class (members of the Bell class look like mirror images of the Funnel class).
 For every possible pairing of the ten words in the ASL dataset, we clustered the 10 corresponding sequences, using group-average hierarchical clustering. At the lowest level of the corresponding dendrogram, the clustering is subjective. However, the highest level of the dendrogram (i.e. the first bifurcation) should divide the data into the two classes. Any dendrogram that correctly partitions the data in this fashion we consider correct and any other partition we consider incorrect. There are 34,459,425 possible ways to cluster 10 items, of which 11,025 of them correctly separate the two classes, so the default rate for an algorithm which guesses randomly is only 0.031%.
 We performed the same experiments for the CBF dataset, with every possible pairing of the three classes. Figure 8 shows the results of one experiment with the Cylinder and Funnel classes. experiment 100 times and averaged the results.
 We compared four distance measures: 1) DTW : The classic dynamic time warping algorithm as 2) PDTW : The piecewise dynamic time warping algorithm 3) Euclidean : We also tested Euclidean distance measure to 4) PEuclidean : Because it might argued that any increased Table 1 summarizes the results .
 Although the Euclidean distance can be quickly calculated, it smoothing effect of the PAA representation does help slightly for the CBL dataset, both of the Euclidean based metrics have great difficulty differentiating between two classes in both datasets. Both DTW and PDTW have essentially the same high accuracy, but PDTW faster by a factor of 47 for the ASL dataset and a factor of 21.5 for the CBL dataset. Dynamic time warping has enjoyed success in many areas where recognition [9], robotics [21], speech processing [18], manufacturing [10] and medicine [4].
 Conventional DTW, however, is much too slow for searching large databases. For this problem, Euclidean distance, combined with an indexing scheme is typically used. Faloutsos et al, extract the first few Fourier coefficients from the time series and use these to project the data into multi-dimensional space [8]. The data can then be indexed with a multi-dimensional indexing structure such as a R-tree. Keogh and Pazzani address the problem by de-clustering the data into bins, and optimizing the data within the bins to reduce search times [12]. . The most important contribution of this paper is to show that to Euclidean distance metric, although popular, is an extremely brittle distance measure that degrades rapidly in the presence of time axis distortion. We reintroduced DTW to the KDD community and demonstrated a modification of DTW that exploits a higher level representation of time series data to produce one to two orders of magnitude speed-up with no decrease in accuracy. We experimentally demonstrated our approach on several real world datasets and showed a speedup of one to two orders of magnitude. c( t ) = (6+  X  )  X  X [a,b] (t) +  X  ( t ) b( t ) = (6+  X  )  X  X [a,b] (t)  X  ( t -a )/( b -a ) +  X  ( t ) f( t ) = (6+  X  )  X  X [a,b] (t)  X  ( b -a )/( b -t ) +  X  ( t ) Where  X  and  X  (t) are drawn from a standard normal distribution
N(0,1), a is an integer drawn uniformly from the range [16,32] and (b-a) is an integer drawn uniformly from the range [32, 96]. Distance Measure Mean DTW 174.4 48.8 519.2 92.1 PDTW 3.7 51.1 24.1 93.3 Euclidean 2.1 4.4 0.49 3.2
PEuclidean 2.3 4.4 0.62 4.8 [1] Agrawal, R., Lin, K. I., Sawhney, H. S., &amp; Shim, K. (1995). [2] Bay, S. (1999). UCI Repository of Kdd databases [3] Berndt, D. &amp; Clifford, J. (1994) Using dynamic time warping [4] Caiani, E.G., Porta, A., Baselli, G., Turiel, M., Muzzupappa, [5] Das, G., Lin, K., Mannila, H., Renganathan, G. &amp; Smyth, P. [6] Debregeas, A. &amp; Hebrail, G. (1998). Interactive [7] Derriere, S. (1998) D.E.N.I.S strip 3792: [http://cdsweb.u-[8] Faloutsos, C., Ranganathan, M., &amp; Manolopoulos, Y. [9] Gavrila, D. M. &amp; Davis,L. S.(1995). Towards 3-d model-[10] Gollmer, K., &amp; Posten, C. (1995) Detection of distorted [11] Kadous, M. W. (1999) Learning comprehensible descriptions [12] Keogh, E., &amp; Pazzani, M. (1998). An enhanced [13] Keogh, E., &amp; Pazzani, M. (1999). An indexing scheme for [14] Keogh, E., &amp; Pazzani, M. (2000). A simple dimensionality [15] Keogh, E., Smyth, P. (1997). A probabilistic approach to fast [16] Kruskall, J. B. &amp; Liberman, M. (1983). The symmetric time [17] Manganaris, S. (1997). Supervised classification with [18] Rabiner, L. &amp; Juang, B. (1993). Fundamentals of speech [19] Saito, N. (1994). Local feature extraction and its application [20] Sakoe, H. &amp; Chiba, S. (1978) Dynamic programming [21] Schmill, M., Oates, T. &amp; Cohen, P. (1999). Learned models [22] Shatkay, H., &amp; Zdonik, S. (1996). Approximate queries and
