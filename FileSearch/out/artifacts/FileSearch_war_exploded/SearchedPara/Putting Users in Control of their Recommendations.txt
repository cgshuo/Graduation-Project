 The essence of a recommender system is that it can recom-mend items personalized to the preferences of an individual user. But typically users are given no explicit control over this personalization, and are instead left guessing about how their actions a  X  ect the resulting recommendations. We hy-pothesize that any recommender algorithm will better fit some users X  expectations than others, leaving opportunities for improvement. To address this challenge, we study a rec-ommender that puts some control in the hands of users. Specifically, we build and evaluate a system that incorpo-rates user-tuned popularity and recency modifiers, allowing users to express concepts like  X  X how more popular items X . We find that users who are given these controls evaluate the resulting recommendations much more positively. Fur-ther, we find that users diverge in their preferred settings, confirming the importance of giving control to users. H.5.3 [ Group and Organization Interfaces ]: Computer-supported cooperative work; H.1.2 [ User/Machine Sys-tems ]: Human factors; H.3.3 [ Information Search and Retrieval ]: Information filtering recommender systems; collaborative filtering; social com-puting; user control; personalization; user study; simulation study; MovieLens Recommender systems are usually not user-configurable. Recommendation algorithms personalize their responses for users by measuring behaviors (ratings, clicks, purchases, sur-vey questions, etc.), sometimes matching these with con-tent attributes (popularity, price, author, etc.) Typically, users are given no explanation of how their behaviors af-fect their recommendations 1 ; the system is a  X  X lack box X . Therefore, while users may want a recommender to behave di  X  erently ( X  X y recommendations are too obscure X , or  X  X y recommendations show too many expensive things X ), they are not given any means to tell the system to behave di  X  er-ently.

In this research, we explore the idea of giving users con-trol over their recommender. What if the user could tell the recommender to de-emphasize obscure content, or to priori-tize a  X  ordable content? We hypothesize that such a system could leave users feeling more satisfied with their recommen-dations and more in control of the process.

To this end, we evaluate a system that gives users control over a single variable that is external to (or subsumed by) the recommendation algorithm. By examining a single variable, we allow users to control the recommender through simple actions like  X  X how less expensive items X .

Our recommendation method uses a linear weighted com-bination of a personalization variable (the output of a rec-ommender algorithm such as item-item collaborative filter-ing) and a blending variable (a non-personalized content at-tribute such as price); we give users control over the weights. This approach has a number of advantages. It can be incor-porated into any existing item-ranking recommender sys-tem by simply re-ranking result lists. It is computationally cheap, both to recommend items, and to change the recom-mender X  X  weights. It is simple to understand, both from a user perspective and from a data analytics perspective.
We structure this work around the following research ques-tions:
We focus our analysis on two attributes available in most recommender systems: item popularity , and item age . Item popularity  X  the number of users who have interacted with the item  X  may be operationalized by measuring data such as clicks, ratings, and mentions. Item age may be oper-ationalized by the time when the item was added to the system, or when the item was created/released.
Some users will try to reverse engineer the algorithm [12]
To deepen our understanding of the impact of using pop-ularity and age in item recommendations, we conduct an of-fline simulation study and an online user study in MovieLens ( http://movielens.org ), a movie recommendation web site with several thousand active monthly users. MovieLens members rate movies on a 5 star scale (with half-star in-crements). They expect that rating more movies will help the system deliver better recommendations.

We make several research contributions. We describe a functional, computationally e cient recommendation method that can add user-configurability into any existing item rec-ommendation framework. Through a simulation study, we develop several insights concerning popularity and age  X  two widely-available, easily understood variables for user manipulation. We determine, via a user study, that users who are given control over their recommender evaluate the resulting recommendations more positively.
There has been substantial research e  X  ort on optimiz-ing recommender algorithms for system-determined outcome measures such as prediction accuracy, click-through rate, purchase dollars, and return visits. Some of the best-known approaches are based on nearest-neighbor (e.g., item-item collaborative filtering [19]) or matrix factorization approaches (e.g., SVD++ [14]).

Recently, there has been an emphasis on learning to rank [15], a family of machine learning techniques that are trained on a ranking-oriented loss function. There are several related approaches to this idea that seek to maximize ranking out-comes while remaining computationally feasible (e.g., [6, 4]). Learning to rank has the potential for optimizing recommen-dation lists by examining many sources of behavioral input, including implicit feedback such as clicks and purchases [18]. Algorithms will continue to improve in their ability to opti-mize system-determined outcomes. We build on this fact by seeking a method that lets users decide their own outcomes. Our recommendation method can extend any algorithm that scores or ranks items.

In this work, we develop a hybrid recommender  X  an ensemble of multiple recommender algorithms. Hybrid rec-ommenders have been widely applied to improve prediction and recommendation accuracy [3, 13, 23, 21]. Weighted en-sembles  X  linear combinations of multiple recommendation algorithms  X  are one of the most popular methods in this area [3]. For example, [1] describes introducing a simple lin-ear ensemble model in Netflix, incorporating predicted rat-ings with many other features. We build on this literature by allowing the user to vary the weights, and by conducting a user study to measure user perceptions of di  X  erent weights.
We are interested in more than just o  X  ine optimizations; we want happy users. Statistical accuracy metrics (e.g., ac-curacy, recall, and fallout) are not necessarily a good approx-imation of the recommendation quality perceived by users [8]. Other factors, such as transparency [22] and user control [16, 5] also a  X  ect the user experience [9].

Especially relevant to this work are systems that give users more control over their recommended items. Critiquing rec-ommender systems [7, 17] allow users to quickly build a highly-customized, task-specific, transparent taste profile. These systems rely on content attributes and are task-specific; in this work we aim to provide users with control over longer-lived, general-purpose, content-agnostic recommendation al-gorithms. Research on Tasteweights [2], an interactive rec-ommender system that allows users to directly manipulate many attributes, found that users were more satisfied when they were given control. We extend this idea by evaluat-ing a recommendation method that works on top of existing recommendation methods, and that allows simple user ma-nipulation.
To build a top-N recommendation list for each user, we build a personalized blending score ( s u,i ) for each item, then sort by that score. The mechanism for computing these scores is kept simple so that we can learn from the results, and flexible so that we can experiment with di  X  erent blend-ing strategies.

Equation 1 shows the general form of our method  X  a weighted linear combination of input variables. r u,i is the recommender X  X  score of item i for user u ; f 1 i ,...,f n meric representations of item features, and w 0 u ,...w n weights that determine the relative importance of the fea-tures.
We operationalize this general method as shown in Equa-tion 2.
The specific variables used are:
Weights { w 0 ,w 1 ,w 2 } may take any value; we experiment with weights in the range of 0-1. In addition, we examine only one of { w 1 ,w 2 } at a time, which we accomplish by setting the other variable X  X  weight to zero.

We scale pred u,i , pop i ,and age i to the range of 0-1 by computing their percentile rank before combining them, in order to make their weights comparable. For example, to scale pred u,i , we first compute the user X  X  predicted rating for each item on a 0.5-5 scale, then calculate the percentile of each of those scores. In this way, we flatten the distri-butions in a consistent manner (as compared with simple normalization, which would preserve the existing shape of the distribution). This flattening step is critical for ratings data, such as those found in MovieLens, where the normal-shaped distribution of rating values (  X  =3 . 53 , =1 . 05) causes many closely-clustered predicted rating values, thus dramatically over-weighting di  X  erences in other parameters
In our o  X  ine analysis, we varied the popularity window across 3, 6, 12, and 24 months, and found no major di  X  er-ences in outcomes. for movies in this prediction range. We note that percentile-adjusted distributions are not perfectly flat, since percentiles may have ties.
To establish the feasibility of our recommendation method, and to guide the design of our user-facing experiment, we use an o  X  ine simulation study to understand the impact of di  X  erent weighting values.
This study is based on a dataset of user ratings and movie rating statistics drawn from the MovieLens database on April 2, 2015. We sample users who joined MovieLens in the six months preceding that date. We include users with at least 40 ratings, to ensure that all users could have at least 20 ratings in both their training and test sets. The resulting sample consists of 4,976 users; these users had visited Movie-Lens a median of 35 times and had rated a median of 253 movies.

We conduct an o  X  ine simulation study using a rating-holdout methodology [11] to simulate users X  top-N recom-mendation lists. Specifically, we conduct 5-fold cross-validation experiments, using the LensKit evaluation framework [10], which partitions by user. In each fold, we use four groups of users to train the item-item similarity model; we measure outcomes for the remaining group. For users in that group, we choose 20 ratings at random as their training data (their simulated ratings profile), while their remaining ratings are treated as their test data.

We run two groups of simulations, one to measure the impact of each of the two blending variables: popularity ( X  X op X ) and age. For both pop and age, we test simu-lated outcomes with di  X  erently configured recommenders that span the range of weighting. The recommenders X  weights span the following values, where the first number in the tu-ple is the weight assigned to predicted rating, and the second number is the weight assigned to the blending variable (pop or age): We only experiment with one blending variable at a time (the other blending variable is held constant at 0) to isolate its e  X  ect.

For each configuration, and for each user, we simulate generating a top-20 list of recommendations. Using the test data for that user, we can then evaluate their ability to in-teract with those recommendations (e.g., to add the item to their wishlist). All reported metrics only concern these top-20 recommendations (e.g., RMSE is measured on rated items in users X  top-20 simulated recommendations).
See Figure 1 for a visualization of the results of this sim-ulation. Each plot in this figure shows the simulated results in both experiments  X  popularity and age  X  on a single outcome variable, averaged across users. The x-axis repre-sents the full range of relative weighting: the far-left point is the result of sorting by predicted rating, while the far-right point is the result of sorting by the blending variable (pop or age). The intermediate points on the x-axis span the range of weighting values as discussed above. Though the far-right point represents a recommendation algorithm that is 0% predicted rating, it is still semi-personalized because we assume in this experiment, as we do in the live system, that we do not display already-rated movies to users.
Manipulation checks . Blending a small amount of popu-larity strongly increases the average pop percentile of top-N lists; blending a small amount of age strongly increases the average age percentile . These e  X  ects are expected, and are included mostly as sanity checks. However, these charts also show that even small weights on the blending variable will have a dramatic influence on the resulting rec-ommendations.

Actionable items . MovieLens users add items to a  X  X ish-list X  to express interest, and  X  X ide X  items to express a lack of interest. We simulate the presence of these items in users X  top-20 lists with #inwishlist and #hidden . We find, interestingly, that increasing popularity simultaneously in-creases both actions, though they express opposite responses to the content. Possibly, this is due to increased familiar-ity with the items in the list  X  a user may have stronger preferences for items they have heard of. Increasing the weighting of age has the property of increasing the num-ber of wishlisted items and decreasing the number of hidden items, though the e  X  ect is more subtle than with popularity.
Item quality . Average rating is a non-personalized proxy for item quality, which we measure on a 0-5 star scale. Adding a small amount of popularity dramatically increases the average rating, while adding age slightly lowers the av-erage rating of items in users X  top-20 lists.

Recommender quality . RMSE is a prediction accuracy met-ric; nDCG is a rank quality metric [20]. We find that both pop and age have worsening e  X  ects on both metrics, though the e  X  ects are minimal at low blending values.

Impact on personalization . By blending non-personalized factors into a recommender algorithm, we expect the system to become less personalized. We quantify this cost using global unique , which shows the number of unique movies recommended across all users in our sample. We find that even small weights cause a large decline in this metric  X  even blending in 5% popularity drops the number of unique top-20 movies from 14.5k to 6.8k, while blending 5% age drops the number to 7.7k.
O  X  ine analysis gives us some general understanding of the e  X  ects of blending on top-N recommendations, but does not tell us how users perceive these e  X  ects and what amount of popularity or age (if any) they choose to blend in. We therefore conduct a user experiment in which users are able to control the blending themselves.

Our user experiment has two main parts. First, we ask users to  X  X une X  a list of recommendations using a small set of controls. Then, we ask users to complete two surveys (in random order): one about their original list of recommen-dations, and another (with the same questions) about their adjusted list.
We invited 2,023 existing MovieLens users to participate in the user study between March 26 and April 16, 2015. This set of users was chosen randomly among a larger set of users who (a) had logged in during the previous six months, (b) had rated at least 15 movies, and (c) had consented to receive emails from MovieLens.

Our study has a 2x2 design with random assignment. The first variable, condition , determines if the user will be given control over the popularity ( X  X op X ) variable or the age vari-able in their recommendations. The second variable, order , determines the order of the surveys shown, to test and con-trol for order e  X  ects.

Subjects first see the recommender tuner interface  X  a top-24 list of movies and several buttons for changing the contents of the list (see Figure 2). The instructions read: Figure 2: A screenshot of the experimental recom-mender tuner interface. Users click left or right to explore di  X  erent recommender configurations. Each click changes the list by four movies  X  these are em-phasized visually in the  X  X hat changed X  section. These instructions intentionally do not reveal how their ac-tions control the recommendations in order to remove any cognitive bias. E.g., a user who believes  X  X  want to watch new releases X  may use the feature di  X  erently if the controls are labelled X  X lder X  X nd X  X ewer X  X ather than the more neutral  X  X eft X  and  X  X ight X . Note that the o  X  ine analysis above eval-uates top-20 lists; we choose 24 for this experiment because it matches the system interface.

By clicking the left , right , and reset buttons, the user is able to change the movies shown in the list. At the time the page is loaded (i.e., the origin ), the top-24 list is the result of 100% predicted rating from MovieLens X  X  item-item CF algorithm. Each subsequent click  X  X arther X  from the ori-gin increases the weight of the blending variable (pop or age) relative to the personalization variable. Values to the  X  X ight X  of the origin represent positive weights on the blend-ing variable. As in the o  X  ine analysis, moving farther from the origin simultaneously increases the weight on the blend-ing variable and decreases the weight on the personalization variable (to a maximum of 1.0 blending, 0.0 personalization). Values to the left of the origin represent negative weights (to a maximum of -1.0 blending, 0.0 personalization). Note that the o  X  ine analysis above does not model negative weights; we include these so that users can express  X  X ore obscure X  or  X  X lder X  if they wish.

Each step  X  X  right or left click  X  changes four items in the user X  X  top-24 list. This means that the specific amount Figure 3: A screenshot of the survey that asks users to evaluate their top-24 list. Subjects take this sur-vey twice in random order  X  once for their origi-nal recommendations, and once for their hand-tuned recommendations. by which the weights change per step varies by user. Early testing of the interface revealed that it was di cult to set an increment that worked well for all users (a given change in weights may a  X  ect half of the items in the recommendation list for one user, and not change the list for another user). Thus, we pre-computed a set of weights for each subject that would correspond to replacing four items  X  enough to feel the list changing, but not enough to be overwhelming. We locate a value for each step using a binary search algorithm.
To assist users in determining whether the list is better before or after the change, we show two boxes, one showing the movies that were just removed, and one showing the movies that were just added. We also call attention to the new items in the top-24 list by highlighting them.
After clicking I X  X  done! , we ask survey questions about the subject X  X  original and tuned recommendations (see Fig-ure 3). These questions are designed to assess their per-ceptions of the di  X  erences between the two lists. Several of these questions are derived from [5]. We conclude the survey with several questions concerning the general feasibility of this interface as a permanent feature in MovieLens.
We consider two groups for analysis. First, for examining use of the recommender tuner interface, we look at users who (a) used both the left and the right buttons, (b) took three or more actions, and (c) clicked I X  X  done! (N=168, 8.3% of those emailed). Second, for examining survey results, we look at users from the first group who also completed the survey (N=148, 7.3% of those emailed). Both groups come from a pool of 381 users (19% of those emailed) who clicked on the email link and were assigned to an experimental con-dition. These samples skew towards highly-active users (e.g., users who finished the survey have a median of 186 logins and 626 ratings vs. the broader sample of emailed users who have a median of 48 logins and 254 ratings).

Overall, subjects used a median of 10 actions in tuning their list of recommendations (pop condition: median=12; age condition: median=10). 15% of subjects chose the orig-inal recommender as their favorite configuration, while the remaining 85% chose a configuration one or more steps from the initial setting. See Figure 4 for a histogram of these Figure 4: A histogram with a smoothed density overlay (red line) of users X  final recommender se-lections, represented in the number of steps (i.e., clicks) away from the origin of 100% item-item CF. Each step represents a change of four movies out of the 24 shown. Negative values represent final se-lections with negative weights for the variable (i.e., biasing the list towards obscure or old content). subjects X  final choices (measured in number of clicks left or right from the initial configuration) and Table 1 for descrip-tive statistics of three outcome measures.

See Table 2 for an overview of how the top-24 movies themselves changed in terms of popularity, age, and average rating. Subjects in the pop condition tuned their recommen-dations to become more popular (average ratings, last year: 22 ) 100), newer (average release date: 1993 ) 1998), and more highly-rated (average rating: 3.58 ) 3.85). Subjects in the age condition tuned their recommendations to become newer (average release date: 1995 ) 2002); there were not statistically significant di  X  erences in the other metrics.
Of the 148 subjects who completed the survey, 85 are in the pop condition, 63 are in the age condition. 72 users took the original list survey first, while 76 users took the tuned list survey first. We found no order e  X  ects in any of the statistical tests described below.

Subjects in both the pop condition and the age condi-tion responded more favorably concerning the properties of the tuned list, as compared with the original list. These results are summarized in Figure 5 and Figure 6. All di  X  er-ences shown in those figures concerning the distributions of responses between original and tuned are statistically sig-nificant (p &lt; 0.001) using a Wilcoxon rank sum test.
Subjects responded that the tuned list contained more movies they had heard of, and more movies they wanted to watch (Wilcoxon test, p &lt; 0.001). See Table 3 for descrip-tive statistics. These di  X  erences remain statistically signif-icant when looking only at subjects in the pop condition (p &lt; 0.001) and the age condition (p &lt; 0.001).
RQ1: Do users like having control over their recommenda-tions? Subjects strongly preferred their top-24 recommen-Table 1: Descriptive statistics of users X  tuned list of recommendations, by condition. Movies changed from original indicates how many of the original 24 movies were not present in the tuned list. Final step indicates the number of clicks from the origin of the final choice (negative numbers are  X  X eft X  of the ori-gin). Final coe cient indicates the user X  X  preferred blending weight (this number was not exposed to users). movies changed from original final step final coe cient dation lists after using the experimental controls to adjust the popularity or age. Subjects in the popularity condition were more likely to say that their recommendations were  X  X ust right X  in terms of popularity; subjects in the age con-dition were more likely to say that their recommendations were X  X ust right X  X n terms of recency. Across both conditions, subjects reported that their adjusted lists better represent their preferences, and would better help them find movies to watch.

Subjects responded positively to a survey question asking if they would use a feature like this if it were a permanent part of the system (median  X  X gree X  on a likert scale). How-ever, subjects responded negatively to a survey question ask-ing if they found the interface easy to use (median X  X isagree X  on a likert scale). Based on user feedback, we attribute a large part of this usability problem to our decision to obfus-cate the experimental controls (e.g., labeling a button with  X  X ight X  instead of  X  X ewer X , asking them to pick  X  X  position in a spectrum of values X  instead of  X  X  popularity setting X ). Given the strongly positive feedback towards the tuned lists of recommendations, we feel it is worth investing e  X  ort in Table 2: Average statistics of subjects X  original and tuned top-24 recommendation lists by condition. * denotes a statistically significant di  X  erence (p &lt; 0.001) using a paired t-test. Figure 5: User survey responses from the popularity condition. Note that the first plot uses a custom scale where the best values are in the middle ( X  X ust right X ), while the other two plots use a conventional agree/disagree scale. All di  X  erences between origi-nal and tuned are statistically significant (Wilcoxon test, p &lt; 0.001). building usable interfaces for recommender tuning, but this remains future work.

RQ2: Given control, how di  X  erent are users X  tuned recom-mendations from their original recommendations? Di  X  erent users used the controls in di  X  erent ways. The median user in the pop condition changed out 12 (50%) of their origi-nal top-24 recommendations, while the median user in the age condition changed out 7 (29%). There were some users who changed out all of their movies (N=13, 7.7%), and some users who did not change any movies (N=13, 7.7%).
O  X  ine simulation predicts several e  X  ects of our recom-mendation method on resulting lists. We are able to measure several of these same e  X  ects using data from the user study  X  popularity percentile, age percentile, and average rating  X  and find that subjects X  recommendation lists moved in the predicted direction. These results show some striking di  X  er-ences in the tuned recommendation. For example, users in Table 3: User responses concerning their familiarity with and desire to watch the movies in the original and tuned top-24 lists. The di  X  erence between orig-inal and tuned is statistically significant (Wilcoxon test, p &lt; 0.001). Figure 6: User survey responses from the age con-dition. Note that the first plot uses a custom scale where the best values are in the middle ( X  X ust right X ), while the other two plots use a conventional agree/disagree scale. All di  X  erences between origi-nal and tuned are statistically significant (Wilcoxon test, p &lt; 0.001). the popularity condition tuned their lists to contain movies that were rated nearly five times as often in the past year (100 vs. 22), on average.

RQ3: Do users converge to a common  X  X est tuning X  set-ting? There does not appear to be a  X  X ne size fits all X  tun-ing value where users converge in their preferences, in ei-ther condition. In fact, 15% of users in the pop condition chose weights below 0.0 (to the  X  X eft X  of the origin, in the interface) to encourage less popular items to appear; 17% of users in the age condition chose negative weights to encour-age older items to appear. Given that the majority of user feedback in MovieLens (outside this experiment, in day-to-day operation) indicates recommendations that skew too old or too obscure, the number of users who chose negative value weights is surprising, which underscores the importance of giving the users control.
Our algorithm uses item-item collaborative filtering as the baseline personalization algorithm. This algorithm, opti-mized for accuracy in the prediction task, may behave ex-tremely di  X  erently from other algorithms [9], content-based or machine-learning algorithms in particular. It is possible that user controls would have a more subtle e  X  ect in the con-text of a recommender that was on average better oriented with user goals (e.g., optimized to predict movies that will be added to the wishlist).

Our experiments examine just two attributes  X  popular-ity and age  X  in a single context  X  movie recommenda-tions. Therefore, our findings may or may not generalize to a broader pool of entity attributes in di  X  erent domains; di  X  erent systems will probably need to experiment to find the most useful attributes for providing users with control. In addition, our method lends itself most naturally to at-tributes that may be linearly scaled; categorical variables (e.g., movie genre) do not fit naturally into this framework. Our user study was taken by power users of MovieLens. These users may have responded more favorably to our ex-perimental interface than more typical users would have, but we cannot know. We must consider the possibility that a typical user would not be nearly as sensitive to the di  X  er-ences in top-24 recommended items.

Our experimental interface was poorly rated for its usabil-ity, and we received several comments about the di culty of the task. Again, this biases the experimental sample to-wards users who have a higher tolerance for complexity or navigating unclear tasks.
In this research, we address only item-level recommenda-tions. Our method does not naturally generalize to di  X  erent types of recommendation tasks, such as recommending cate-gories ( X  X op dark comedies X ) or recommending similar items ( X  X ore movies like this X ). Developing and evaluating the fea-sibility of user-tuned recommenders for these tasks is future work.

We make the assumption that users have relatively static preferences, but in reality, a user X  X  context is always chang-ing (e.g., changing moods, social contexts, or physical lo-cations). A recommender tuning that works one day may be less appropriate the next day. Understanding the rela-tionship between context-sensitive recommenders and user-tuned recommenders  X  and developing interfaces that merge the two  X  is an interesting next step.
In this paper, we build and evaluate a recommender sys-tem that incorporates user-tuned popularity and recency modifiers. We find that users who are given these controls evaluate the resulting recommendations much more posi-tively than their original recommendations: they rated the tuned recommendations to be more personalized, and iden-tified more movies that they hoped to watch. Further, we find that there is no globally optimal setting that works for all users. Some used the experimental controls to change out every movie in their recommendation list, while others responded that their original list was optimal.

These results underscore the importance of user control  X  based on these data, any globally-optimized hybrid rec-ommender we deploy will not match the desires of a large fraction of our users. Recommender systems traditionally o  X  er no explicit control to users, and leave users guessing about the relationship between their actions and the result-ing recommendations. Based on the results of this study, we want this to change  X  users will be happier if they are given control.
We would like to thank Joseph Konstan and Daniel Kluver for their contributions, and MovieLens users for their par-ticipation. This material is based on work supported by the National Science Foundation under grants IIS-0808692, IIS-0964695, IIS-0968483, and IIS-1111201. This project was supported by the University of Minnesota X  X  Undergraduate Research Opportunities Program. [1] X. Amatriain. Mining Large Streams of User Data for [2] S. Bostandjiev, J. O X  X onovan, and T. H  X  A  X ullerer. [3] R. Burke. Hybrid Web Recommender Systems. In [4] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. [5] S. Chang, F. M. Harper, and L. Terveen. Using [6] O. Chapelle and S. S. Keerthi. E cient algorithms for [7] L. Chen and P. Pu. Critiquing-based recommenders: [8] P. Cremonesi, F. Garzotto, S. Negro, A. V.
 [9] M. D. Ekstrand, F. M. Harper, M. C. Willemsen, and [10] M. D. Ekstrand, M. Ludwig, J. A. Konstan, and J. T. [11] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [12] M. Honan. I Liked Everything I Saw on Facebook for [13] M. Jahrer, A. Toscher, and R. Legenstein. Combining [14] Y. Koren. Factorization Meets the Neighborhood: A [15] T.-Y. Liu. Learning to Rank for Information Retrieval. [16] S. M. McNee, S. K. Lam, J. A. Konstan, and J. Riedl. [17] P. Pu and L. Chen. User-Involved Preference [18] S. Rendle, C. Freudenthaler, Z. Gantner, and [19] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [20] G. Shani and A. Gunawardana. Evaluating [21] J. Sill, G. Takacs, L. Mackey, and D. Lin.
 [22] R. Sinha and K. Swearingen. The Role of [23] L. Tang, Y. Jiang, L. Li, and T. Li. Ensemble
