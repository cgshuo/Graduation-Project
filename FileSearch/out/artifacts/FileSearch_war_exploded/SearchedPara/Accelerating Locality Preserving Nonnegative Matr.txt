 Matrix factorization techniques have been frequently ap-plied in information retrieval, computer vision and pattern recognition. Among them, Non-negative Matrix Factoriza-tion (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in the human brain. Locality Preserving Non-negative Matrix Factorization (LPNMF) is a recently proposed graph-based NMF extension which tries to preserves the intrinsic geomet-ric structure of the data. Compared with the original NMF, LPNMF has more discriminating power on data representa-tion thanks to its geometrical interpretation and outstanding ability to discover the hidden topics. However, the computa-tional complexity of LPNMF is O ( n 3 ), where n is the num-ber of samples. In this paper, we propose a novel approach called Accelerated LPNMF (A-LPNMF) to solve the com-putational issue of LPNMF. Specifically, A-LPNMF selects p ( p  X  n ) landmark points from the data and represents all the samples as the sparse linear combination of these landmarks. The non-negative factors which incorporates the geometric structure can then be efficiently computed. Experimental results on the real data sets demonstrate the effectiveness and efficiency of our proposed method. I.5.3 [ Clustering ]: Algorithm; D.2.8 [ Applications ]: Text processing Non-negative Matrix Factorization, Speedup
Data representation is one of the fundamental problems in various data processing fields such as machine learning, data mining and pattern recognition. Researchers have long sought effective and efficient representations of data. For a give database, we may have thousands of distinct features. However, the degree of the freedom of each sample could be far less. Instead of the original feature space, one might hope to find a hidden semantic  X  X oncept X  space to represent the data. The dimensionality of this  X  X oncept X  space will be much smaller than the feature space. To achieve this goal, matrix factorization based approaches have attracted considerable attention in the last decades [2, 5, 9, 7, 8].
Among various methods, Non-negative Matrix Factoriza-tion (NMF) [9] has received considerable attentions for its theoretical interpretation and practical performance. NMF enforces the factorizations to be non-negative that leads to a parts-based representation. It has been shown to be superior to SVD in face recognition and document clustering [2]. One shortcoming of the original NMF is that it only considers the Euclidean structure of the data [2]. To address this limita-tion, Locality Preserving Non-negative Matrix Factorization (LPMNF) [3] has been proposed to learn the parts based representations which incorporating the intrinsic geometric structure of the data. LPNMF uses a nearest neighbor graph to model the geometric structure and aims at finding a hid-den concept space in which two data points are sufficiently close to each other if they are connected in the graph. The intuition is that if two points are sufficiently close they may have the similar topic representations [2]. Despite its good performance, LPNMF has to pay high computational cost in graph construction and subsequent matrix calculations, which limits the method X  X  applicability to large-scale prob-lems.

Inspired by the recent progress on scalable semi-supervised learning [10] and large scale spectral clustering [4], we pro-pose a novel solution to speed up LPNMF in this paper. Specifically, given a data set with n samples, we select p ( p  X  n ) landmark points and represents all the samples as the sparse linear combination of these landmarks. The non-negative factors which incorporates the geometric struc-ture can then be efficiently computed. Experimental results on the real data sets demonstrate the effectiveness and effi-ciency of our proposed method.

The rest of the paper is organized as follows: Section 2 provides a review of LPNMF. Section 3 introduces our method. Extensive experimental results on clustering are presented in Section 4. Finally, we provide some concluding remarks in Section 5.
Given a data matrix X = [ x ij ] = [ x 1 ;  X  X  X  ; x n ]  X  R m  X  n where each column x i represents a sample vector, NMF aims to find two non-negative matrices U  X  R m  X  t and V  X  R n  X  t which minimize the following objective function: where Y = [ y ij ] = UV T . The above objective function is lower bounded by zero, and vanishes if and only if X = Y . It is usually referred as X  X ivergence X  X f X from Y . It reduces to the Kullback-Leibler divergence, or relative entropy, when  X  normalized probability distributions. 1
The original NMF algorithm only considers the Euclidean structure of the feature space. Recent studies have shown that many data are probably sampled from a sub-manifold of the ambient Euclidean space [2]. Therefore, the intrin-sic manifold structure needs to be considered while per-forming the matrix factorization. Motivated by this, Cai et al . [3] proposed a novel matrix factorization algorithm called Locality Preserving Non-negative Matrix Factoriza-tion (LPNMF). LPNMF uses the following term to measure the smoothness of the low dimensional representation along the geodesics in the intrinsic geometry of data: where W  X  R n  X  n is a nearest neighbor graph defined on the data points. The objective function of LPNMF is a natural combination of Eq. (1) and Eq. (2): where  X  0 is the regularization parameter. By minimizing R , LPNMF gets a conditional probability distribution which is sufficiently smooth on the data manifold. The solution of LPNMF can be obtained using the following updating rules [3]: v where v k is the k -th column of V and L is the Laplacian matrix of W [3].

The effectiveness of LPNMF has been demonstrated on real world problems [3], however, it is computational expen-sive. The matrix ( Thus, we need to inverse an n  X  n matrix in each iteration which cost us O ( n 3 ).
One can use other cost functions ( e.g ., Frobenius norm) to measure how good UV T approximates X . Please refer [9, 2] for more details.
In this section, we will introduce our A-LPNMF (denote for Accelerated LPNMF) algorithm which dramatically re-duces the computational cost of LPNMF. Specifically, A-LPNMF compresses the data using an efficient landmark-based approach [4, 10]. With the landmark-based compres-sion, the updating equations in LPNMF can be efficiently computed.
Given n samples x 1 ;  X  X  X  ; x n  X  R m , we try to compress the data by representing x i as a sparse linear combination of several landmarks [4, 10]. In other words, for any data point x i , our goal is finding its approximation  X  x i as where { l j } p j =1 are p landmarks. A good set of landmarks should be able to cover the data set well. Thus, we use Kmeans clustering 2 to generate the landmarks (taking the cluster centers as the landmarks). Then we can use z i = [ z 1 i ;  X  X  X  ; z li ] T as a compression of x i .

A natural assumption here is that z ji should be larger if l is closer to x i . We can emphasize this assumption by setting the z ji to zero as l j is not among the r (  X  l ) nearest neighbors (for all the landmarks) of x i . This restriction naturally leads to a sparse compression.

Let N  X  i  X  denote the index set which consists r indexes of r nearest landmarks of x i , i.e ., l j is among the r nearest landmarks of x i if j  X  N  X  i  X  , we compute z ji as wh ere K (  X  ) is a kernel function. One can simply choose the most commonly used Gaussian kernel, where is the kernel width parameter.

Thus, we compressed the data matrix X  X  R m  X  n as a sparse matrix Z  X  R p  X  n . According to Eq. (7), it is impor-tant to note that each column of Z sums up to 1.
Now we have the compressed data matrix Z  X  R p  X  n for the input X  X  R m  X  n , to avoid the high computational cost, instead of constructing a nearest neighbor graph as LPNMF does, we simply compute the affinity matrix as where b Z = D  X  1 = 2 Z and D is a p  X  p diagonal matrix whose entries are the row sums of Z ( d ii =
The advantages of using this graph instead of a nearest neighbor graph are as follows:
There is no need to wait the Kmeans converge and we can stop the Kmeans after t iterations, where t is a parameter (5 is usually enough). Table 1: Time complexity of accelerating method 1. Constructing a nearest neighbor graph requires O ( n 2 2. The inversion of the Laplacian matrix L can be effi-
Recall that each column of Z sums up to 1, it is easy to verify that the degree matrix of W in Eq. (9) is I [4]. We can then rewrite the matrix inversion operation part in Eq.5 as follows: For simplicity, define bury [6] formula on the Eq. (10), we have: Since  X  Z  X  R p  X  n , by Eq. (11), the inversion part changes from an n  X  n matrix to a p  X  p matrix. When p  X  n , it can significantly speed up the calculation in each iteration during the solving progress. Substitute the Eq. (11) into Eq. (5), we get the new updating form of v k in Eq. (12): Note that, suppose the data contains n points with m di-mensionality and we use p landmarks, our method spends O ( pnm ) to construct the adjacency matrix W (our method only needs to calculate matrix Z ), while the original ap-proach costs O ( n 2 m ). Besides, the Kmeans for selecting the landmarks needs O ( tpnm ), where t is the number of itera-tions in Kmeans. In practice, we have not used the W di-rectly, instead, we only need to save the Z in memory which avoids the waste of the storage. The summary of computa-tional complexity of our method and LPNMF is in Table 1. Figure 1: The running time of LPNMF and A-LPNMF versus clustering number on TDT2 data set Figure 2: The running time of LPNMF and A-LPNMF versus clustering number on Reuters data set
In this section, we show the experimental results and com-parisons to evaluate the efficiency and effectiveness of our proposed approach on real-world text databases. All algo-rithms in our experiments are implemented in MATLAB2010 and run on a computer with 2.0 GHZ CPU, 64GB RAM.
Assume that a document corpus contains c clusters, each corresponds to a coherent topic. The NMF is very powerful for clustering, especially in the document clustering. It can project the document into a c -dimensional semantic space where each dimensionality denotes a particular topic and represent each document as a linear combination of the c topics. In this paper, we also evaluate our A-LPNMF on document clustering problems. We use the largest 30 cat-egories in TDT2 and Reuters-21578 as our test data sets. In the experiments, those documents appearing in two or more categories were removed. Finally, we have 9 ; 394 doc-uments in TDT2 and 8 ; 076 in Reuters, In both of the two data sets, the stop words are removed and each document is represented as a tf-idf vector.
The experimental setting is described in the following. For each data set, given the cluster number c , 10 tests are con-ducted on the randomly chosen c classes from the whole Table 2: Clustering performance on TDT2 (%NMI) Table 3: Clustering performance on Reuters (%NMI) data set (except the case when the entire data set is used). Then, the average performance and time are computed as the results.

There are three parameters in our approach: the num-ber of landmark points p , the number of nearest landmark neighbors r and the regularization parameter . Through-out our experiments, we empirically set the number of p to 1000, r to 5 and the value of to 100.
In order to demonstrate the effectiveness of our method, we compare the time and efficiency with the following ap-proaches:
We use normalized mutual information (NMI) [1] to mea-sure the clustering performance. The NMI is a probabilities metric which computes the correlation of two data sets. It evaluates the results by comparing the obtained label of each sample with the label provided by the data set. The detailed definition can be refereed to [1].
Table.2 and Table.3 show the detailed clustering perfor-mance of algorithms by projecting the c clusters documents into a c -dimensional semantic space (results under Kmeans are the clustering performance over all the features). The last row in each table records average clustering performance over all the clusters. We can see that the LPNMF ap-proach gets significantly better performance than the ordi-nary NMF. While A-LPNMF has a comparable results with LPNMF. Fig.1 and Fig.2 are the time comparison of LP-NMF and A-LPNMF over all the clusters. Compared with LPNMF, the A-LPNMF spend much less time. As the num-ber of cluster grows, the time of our algorithm has a linear growth while LPNMF has a rapid rate of growth.
In this paper, we have introduced a novel efficient matrix factorization approach based on Locality Preserving Non-negative Matrix Factorization called A-LPNMF. It uses p  X  n landmark points representing the original data to con-struct the graph matrix W , and utilizes the Woodbury for-mula to reduce the time of matrix computation in each iter-ation. The method has significant time reduction comparing with LPNMF, and extensive experiments on clustering show the effectiveness and efficiency of our approach in processing large scale text data. This work was supported in part by National Natural Science Foundation of China under Grants 91120302 and 60905001, National Basic Research Program of China (973 Program) under Grant 2011CB302206. [1] D. Cai, X. He, and J. Han. Document clustering using [2] D. Cai, X. He, J. Han, and T. S. Huang. Graph [3] D. Cai, X. He, X. Wang, H. Bao, and J. Han. Locality [4] X. Chen and D. Cai. Large scale spectral clustering [5] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [6] G. H. Golub and C. F. V. Loan. Matrix computations . [7] N. Guan, D. Tao, Z. Luo, and B. Yuan. Manifold [8] N. Guan, D. Tao, Z. Luo, and B. Yuan. Non-negative [9] D. D. Lee and H. S. Seung. Learning the parts of [10] W. Liu, J. He, and S.-F. Chang. Large graph
