 Personalization of search results offers the potential for significant improvement in information re trieval performance. User interactions with the system and documents during information-seeking sessions provide a wealth of information about user methods for analyzing and modeling user search behavior in search sessions to predict document usefulness and then using information to personalize search results. We generate prediction models of document usefulness from behavior data collected in a controlled lab experiment with 32 participants, each completing uncontrolled searching for 4 tasks in the Web. The generated models are then tested with a nother data set of user search sessions in radically different search tasks and constrains. The documents predicted useful and not useful by the models are used to modify the queries in each search session using a standard relevance feedback technique. The re sults show that application of the models led to consistent ly improved performance over a baseline that did not take account of user interaction information. These findings have implicati ons for designing systems for personalized search and improvi ng user search experience. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval  X  relevance feedback. Measurement, Human Factor Implicit feedback, search behavi ors, document usefulness, task type, personalization User search interactions are complex, but they are also coherent over segments of a search session, for example when evaluating results from a query. Overall, one expects the actions of a user to reflect their task goal and the process they carry out to achieve that goal. It is reasonable, then, to think that evidence of the user's search interests and task goal may be available in observations of their behaviors during the search session. In this study, we investigate the effectiveness of using the observation of people X  X  behaviors during information-seeking sessions for improving and personalizing their interactions with information. We generated document usefulness predicti on models from a laboratory experiment of the influence of different types of tasks on task search session behaviors, includ ing a general model and several specific models tailored to different types of tasks. Then these prediction models were applied to TREC 2011 Session Track data to evaluate their retrieval performance. Our study makes several contributions to the literature. First, we examined user behavioral measur es as predictors of document usefulness and built prediction m odels. Second, we demonstrate the prediction models perform well on an entirely different dataset of user interactions. Third, we found that the prediction accuracy of document usefulness and some of users X  search behaviors are performance by the pe rsonalizati on model. Users of information retrieva l systems can provide explicit information about their interests and task intent via relevance feedback. However, users may not be willing to provide explicit relevance feedback, so research has concentrated on techniques to gain relevance feedback informati on via implicit processes. These implicit relevance feedback (IRF) techniques observe user search behaviors unobtrusively and infe r the relevance/usefulness of documents and other information objects from the user interactions. Evidence for IRF comes from various measures of user search behaviors while interacting with individual content pages, search result pages, and other behaviors during a search session [11] [9]. Document usefulness for IRF may be learned from behaviors such as dwell time on each content page, the number of clicks and scrolling on each content page, num ber of visits to each content page, and further usage of conten t pages [9]. User behaviors on search result pages have been found to be good indicators of user document preferences [9]. These behaviors include the time on a search result page before the first click, total time on a search result page, click-through and clic k order of each content page on the search result page, and so on [1] [7]. There is also evidence that the nature of query reformul ation during a search episode can be predictive of document useful ness, and perhaps of task type been investigated individually, with little work integrating multiple user behaviors over all types of Web pages to predict document usefulness. 
Copyright 2012 ACM 978-1-4503-1472-5/12/08...$15.00. Correlation of individual measures of user behavior with users X  document preferences have been found to vary for different task types [10] [16] and in different search stages [14]. White and Kelly [16] analyzed retrieval pe rformance after implementing an IRF system based on dwell time on content pages when task type information was considered and when the user information was considered. They found that dwe ll time combined with knowledge of task information improved re trieval performance. White, Ruthven and Jose [18] examined the effect of search stages on the utility of implicit relevance feed back (IRF) and explicit relevance feedback (ERF) in two separate sy stems. Their results showed that in the IRF system, IRF is used more in the middle of the search than at the beginning or end, wh ereas in ERF system, ERF is used more towards the end. Liu and Belkin [14] examined the interaction between decision time on content pages, search stage, and document usefulness, and found knowledge of search stage could help improve the interpretati on of decision time as IRF, as could knowledge of task and topic. This work suggests that a high performance search personali zation model should predict document usefulness taking account of the users' search context, including task type, user task and topic knowledge, search stage, and so on. Generally, the performance of a personalization algorithm should be improved if it incorporates the ability to predict aspects of the user's task and knowledge. For our initial study, we recrui ted 32 participants from a domain relevant to our work/search tasks. They were informed in advance that they would receive $20 for participation. To ensure they treated their assigned tasks seriously, they were told that the top 25% who saved the best set of pages for all four tasks, as judged by an external expert, would receive an additional $20. The participants were undergraduates at Rutgers university majoring in journalism and were between 18 and 27 years old (26 female, and 6 male). Most were native English speakers (78%) with the balance indicating a high proficiency in English. Participants had an average search experience of 8.85 years using a range of browsers (IE, Firefox, Safari, Chrome, and others). Generally, participants rated their search experience high with more Web search experience as compared to online library catalog search. They were generally positive about their average success when conducting online searches. Each participant was given a tutorial as a warm-up task and then performed four Web search tasks (described in section 3.2). A pre-search questionnaire elicited th eir prior familiarity with each task, and their knowledge of the task topic. Participants were asked to search using IE 6.0 on our lab computer, were free to go anywhere on the Web to search for information, and were asked to continue the search until they had gathered enough information to accomplish the task. There was a time limit of 20 minutes for each task. For each task, participants were asked to save content pages that were useful for accomplishing the assignment, and could delete participants decided they had found and saved enough information objects for purposes of the task, they were then asked to evaluate the usefulness of the information objects they saved, or saved and then deleted, through replaying th e search using a screen capture program. An online questionnaire was then administered to ask about their searching experience, including their subjective evaluation of their performance, and reasons for that evaluation. The order of the four tasks was systematically rotated for each participant following a Latin Square design. After completing four different tasks, an exit ques tionnaire was administered asking about their overall search experience. The four work tasks and associated search tasks that we identified are presented below. These tasks follow the normal scenario practice as proposed by Borlund [4], and are couched in assignment, and an associated task to complete. Each task was constructed using the faceted task classification scheme proposed tasks systematically by facet values. In the task descriptions, These are also specified in Table 1. Background Information Collection (BIC) Your assignment: You are a journalist for the New York Times, working with several others on a story about  X  X hether and how changes in US visa laws after 9/11 have reduced enrollment of international students at universi ties in the US X . You are supposed to gather background information on the topic, sp ecifically, to find what has already been written on this topic. Your task : Please find and save all the stories and related materials that have already been published in the last two years in the Times on this topic, and also in five other important newspapers. The BIC task is a Mixed Product because identifying  X  X mportant X  newspapers is intellectual, but finding topical documents is factual. It is Document Level because whole stories are judged. It has the Specific Goal of finding documents on a well-defined topic, but Unnamed because the search targets are not specifically identified (compare with CPE, below). Interview Preparation (INT) Your assignment: Your assignment editor asks you to write a news story about  X  X hether state budget cuts in New Jersey are affecting financial aid for college and university students. Your Task: Please find the names of two people with appropriate expertise that you are going to interview for this story and save just the pages or sources that describe their expertise and how to contact them. INT is a Mixed Product , because defining expertise is intellectual, and contact information is a fact. It is at the Document Level , because expertise is determined by a whole page. The Goal Quality is Mixed , because determining e xpertise is amorphous but contact information is specific. It is Unnamed because the search targets are not specifically identified in the task. Copy Editing (CPE) Your assignment: You are a copy editor at a newspaper and you have only 20 minutes to check the accuracy of the three underlined statements in the excerpt of a piece of news story below. Topic: New South Korean President Lee Myung-bak takes office Body: Lee Myung-bak is the 10th man to serve as South Korea X  X  president and the first to come from a business background. He won a landslide victory in last December X  X  election. He pledged to make the economy his top prio rity during the campaign. Lee promised to achieve 7% annu al economic growth, double the country X  X  per capita income to US$4,000 over a decade and lift the country to one of the topic se ven economies in the world. Lee, 66, also called for a stronger al liance with top ally Washington and implored North Korea to forg o its nuclear ambitions and open impoverished nation. Lee said he would launch massive investment and aid projects in the North to increase its per capita income to US$3,000 within a decade  X  X nce North Korea abandons its nuclear program and chooses the path to openness. X  Your Task: Please find and save an authoritative page that either confirms or disconfirms each statement. CPE is a Factual Product , because facts have to be identified. It is at the Segment Level , because items within a document need to be found. It has the Specific Goal of confirming facts, and it is Named because the search targets are specified. Advance Obituary (OBI) Your assignment : Many newspapers commonly write obituaries of important people years in advance, before they die, and in this assignment, you are asked to write an advance obituary for a information you will need to write an advance obituary of the artist Trevor Malcolm Weeks. OBI is a Factual Product , because facts about the person are needed. It is at the Document Level because entire documents need to be examined. It is Unnamed because the search targets are Amorphous because  X  X ll the information X  is undefined. Task Naming Product Level Goal (quality) BIC Unnamed Mixed Document Specific INT Unnamed Mixed Document Mixed OBI Unnamed Factual Document Amorphous During the search, all of the participants X  interactions with the computer system were logged during the searches on the client side: eye-tracking; mouse move ment capture; clicks and URL requests; scrolling; keystrokes; que ries and other behaviors. This logging required multiple systems, including Tobii eyetracking software, UsaProxy, the Morae screen-capture program [3], as well as our own software. From th ese logs, we extracted a variety of measures of the users X  search behaviors on search result pages (SERPs) and content pages throughout the session, and during query intervals. A query interval is defined as the interval between two successive queries issued in one search session; and the last query interval is the period after issuing the last query till the completion of the task. The behavioral variables used in the work reported here are listed in Table 2. We classified these behaviors into two groups: behavioral meas ures on the clicked documents and behavioral measures during the query intervals. time_to_first_click the time before first click on pages after We used binary recursive partiti oning analysis [5] to identify the most important predictors of useful pages. Recursive partitioning is a stochastic learning techniqu e for non-parametric classification problems. It grows decision tr ees by examining all independent variables and splits a node using the variable that best distinguishes the remaining data in the collection. The tree is grown until all of the data has been assigned to a node. Recursive to classify the dataset. An exam ple (illustrative only) might be to the first click &lt; 34 seconds and there are fewer than 5 SERPs in the query interval X . This met hod has not been used much in identifying predictors of document usefulness or user interests. Recursive partitioning may be superi or to logistic regression when the goal is to correctly classify members rather than optimize overall accuracy. Recursive partitioning can take account of the possible interactions between pred ictor variables natively in the algorithm. We used recursive partitioning analysis to generate prediction models of document usefulness based on the potentially important behavioral variables listed in Table 2. Decision trees were generated using these predictors, and we identified the cutoff points based on the observed distri bution of the variables in the dataset. To better learn the be havioral measures, we made a training collection that balanced the number of saved pages and not-saved pages by sampling the larger not-saved pages pool. In this way, ten balanced training sets were constructed, each sharing the same saved pages pool. The recursive partitioning method was applied to each sample to generate the prediction model. We then compared the variables and cutoff values in these generated models to identify the repeated variables and cutoff values for the decision-tree based prediction model. We first built the general prediction model when considering all tasks in the experiment, without taki ng account of the task type. In this model, three behavioral measures were identified as the important factors: visit_id , dwell time and time to first click . Among these three variables, the dwell time on documents is the most important variable in the general prediction model. Examining the distribution of dwell time in the dataset, we found that the model cutoff (17.45 sec onds) is the third quarter of the dwell time in all the data. At the same time, we found users saved reasonable to infer that the cutoff point for the dwell time variable depends upon the proportion of saved documents among all visited documents. If the relative frequency of saved documents is not clear in a new dataset, the me dian dwell time can be adopted as used in White and Kelly [17]. Not many documents were visited more than once by users in one search session, so the model specifies that if a page has been visited more than once, even though the dwell time is shorte r than the median, the page is predicted to be useful. Finally, the cutoff time for time to first We will apply these cutoff points in our test dataset to evaluate the prediction model. We also conducted 10 random sampling and recursive partitioning analyses on each of the individual tasks to see if tasks of different types would lead to different comb inations of important predictors and prediction models. Following the procedure in section 4.1, for each task, we made balanced pools of saved and non-saved pages with the total number of saved pages, and then generated the prediction models using recursive partitioning. Because the CPE task and OBI task in our user experiment differ the most from one another as task types, we present the specific prediction models for CPE (Figure 2) and OBI (Figure 3). These tree models show that dwell time on documents is still the most important predictor. However, the cutoff point for each task is very different, i.e. 30.65 s econds in CPE and 17.5 seconds in needed for different types of tasks, as White and Kelly [16] suggested. As for the general model, we suggest the median as the cutoff point for each task. The CPE model also identifies visit time of the page, number of mous e clicks on the page, and the proportion of time spent on content pages during the query interval ( prop_content ) as important variables. Among them, the number of mouse clicks was nega tively related to the saved documents, and the prop_content was positively related to the saved documents. The OBI model looks very similar to the general model, except that it specifies a range for the variable time to first click , which is close to the first quarter to the median value for that variable in the dataset. In order to evaluate the predictio n models (both the general model and the specific models) generate d from the user experiment, we applied our models to the data collected for the TREC 2011 Session Track. The Session Track [8] aims to provide test collections and evaluation measures for studying information retrieval based on previous user interactions during a search session, rather than one-time queries. The track used the ClueWeb09 collection (English only) 1 , and the topics were lemurproject.org/clueweb09 defined in the usual TREC ad hoc sense, with a title, description, and narrative. Sessions of real us er interactions were recorded, including users X  queries, clicks on search result pages, and associated time stamps. There were search sessions for 76 topics in total. Before implementing the prediction models, we first classified the tasks in the TREC 2011 Session Track using our modification of From Table 3 we can see that all of the 76 tasks required searching for factual information, so the product for them is Factual . In addition, 66 of the 76 se ssions can be classified as searching for information on the Segment level, with Specific goal, with search targets Named (SSN). There are very few examples of three ot her types of tasks (i .e. SSU, DSN, DAN) in the data set. When we tried to match these task types with the task classification in our user experime nt (Table 1), we found that the SSN type of task was mostly s imilar to CPE, and DSN and DAN types of tasks were very similar to OBI, so we used these specific models for those tasks. We applied the General model for the SSU task type. Table 3 Task classification of the search tasks in TREC 2011 The general model for the TREC dataset was based on the general model we built from the user expe riment and the threshold was determined based on the targeted dataset. The implementation of specific models in the TREC dataset is based on our task cl assification (Table 3). The CPE specific model is used for SSN type of task, the OBI specific model is used for DSN and DAN types of task , and the other task types use the general model. Since the interaction log for the TREC 2011 Session Track was collected on the server side, and did not include any user interactions on the documents (i.e. content pages), the variable number of mouse clicks was not included in the specific models.. For evaluation of the various prediction models in this paper, we use only one of the three types of query modification specified in the TREC 2011 Session Track, which uses the maximum amount of behavioral data obtained duri ng the search session [8]. Since the Session Track X  X  basic concept was to compare the search results of the final query in a search session using a baseline system with no behavi oral data, to the results of a final query using such data, we used the results of our document usefulness prediction models to modify the last query but one ( last -1 ) in the session, using both positive and (positive + negative) relevance feedback (RF). The TREC 2011 Session Track database, Clueweb09 English only [8] was th e searched collection, and was the source for documents and terms for RF. From the prediction of document usefulness for each of our models, we generated a pool of  X  X elevant X  documents that occurred during each search session, and calculated the term frequency for each term in the pool, and in the equivalent corpus of non-useful documents. The observed term frequency was then discounted by the prior of the e xpectation of appearance in a random document in the Englis h language using the Brown corpus [15]. With respect to the number of useful and non-useful terms for query expansion, we used the approach described in Belkin et al. [2], in which a negative RF sy stem was implemen ted. The number of suggested feedback terms wa s determined by the formula: The query was parsed as a weighted sum, using the default Lemur 2 weighting for RF term addition for positive terms, and adding the negative terms under the InQuery  X  X OT X  operator, with 0.6 weight. lemurproject.org 
The specific model (decision tree) is: if (visit_id &gt; 1), then useful pages if (task type = SSN) type of task) task) else if (task type = SSU) type of task) else if (task type = DSN or DAN, these types of task) else if (time_to_first_click &gt; 6.33 &amp; time_to_first_click &lt; 14.55) 
The general model (decision tree) is: if (visit_id &gt; 1),{then it is a useful page}; else if (dwell time &gt; 28.55 seconds), { then it is a useful page}; else if (6.33 seconds &lt; time-to-first-click &lt;14.55 seconds), {then it is a useful page}; else { non-useful pages}. Two relevance feedback me thods were implemented: Positive relevance feedback only : In the runs with positive RF only, the predicted  X  X seful X  docume nts were used to calculate the term frequency, the top 25 terms were selected to expand the last -1 query in the session. Both positive and negat ive relevance feedback : In the runs with both positive and negative RF, the predicted  X  X seful X  documents were used to calculate the term frequency for  X  X seful X  terms and the top 15 terms were selected to be  X  X seful X  terms; the predicted  X  X on-useful X  documents were used to calculate the term frequency for  X  X on-useful X  terms and the top 10 terms were selected to be  X  X on-useful X  terms. We then combined the last-1 queries with the 15  X  X seful X  terms (with positive weight 1.0), and the 10  X  X on-useful X  terms (with negative wei ght 0.6) using the Indri query language. Therefore, we have four pr ediction models of document usefulness generated and evaluated in this study: Our baseline model used Pseudo Relevance Feedback on the last queries issued by the users in eac h session. The defa ult parameters in the Indri Retrieval System were used as follows: We compare the retrieval performan ce of our pers onalized models with the baseline system, to evaluate whether the personalized models based on users X  interactio ns in the search session improve results over pseudo relevance feedb ack that does not take account of users X  interaction behaviors. We evaluated the prediction accuracy of document relevance against the TREC assessors X  relevance judgments [8]. The relevance judgments ranged from -2 to 3 (-2 stands for spam document, 0 stands for not relevant, 1 for relevant, 2 for highly relevant, and 3 means the document is a key to the information need). Since we used binary re levance ratings in our prediction models, we grouped TREC assesso rs X  judgments into two groups: ratings that were -2 or 0 were grouped as  X  X ot-relevant X , and ratings of 1 to 3 were grouped as  X  X elevant X . Table 4 and Table 5 show the prediction accuracy of the general model and the specific models on the document relevance. They demonstrate that the general m odel achieved better prediction performance than the specific model for the TREC 2011 Session Track data. General model predicted Overall accuracy 61% Specific model predicted Overall accuracy 53% We first calculated the mean of the retrieval performance over 76 sessions by each of the models (Table 6). Our results demonstrate that all of the four models improve over the baseline, on almost all the measures provided by TREC 2011 Session Track [8]. ERR 3 0.20 0.28 0.29 0.27 0.28 ERR@10 0.19 0.28 0.29 0.26 0.28 nERR 4 0.29 0.44 0.45 0.41 0.42 nERR@10 0.27 0.43 0.45 0.40 0.42 nDCG 5 0.30 0.24 0.25 0.24 0.25 AP 6 0.09 0.09 0.09 0.08 0.09 GAP 7 0.08 0.09 0.09 0.09 0.09 Expected Reciprocal Rank (ERR) [6] was chosen for the following analysis because the user model underlying the ERR measure matches the user model underlying our prediction models. When examining results by search sessions, we found, as usual, ERR: Expected Reciprocal Rank; nERR: ERR normalized by the maximum ERR per query; nDCG: normalized discounted cumulative gain; AP: Average Precision GAP: Graded Average Precision; 
Parameters for Pseudo Relevance Feedback: int fbDocs = _param.get( "fbDocs" , 10 ); int fbTerms = _param.get( "fbTerms" , 10 ); double fbOrigWt = _param.get( "fbOrigWeight", 0.5 ); double mu = _param.get( "fbMu", 0 ); that there is great variety in evaluation results for any single technique between sessions. For example, the GP model had better retrieval performance on E RR in 42 sessions, but had worse performance on ERR in 32 sessions. Table 7 Retrieval performance change over baseline on ERR Run Measures Mean ERR 0.29 0.28 0.28 0.27 Percent improvement over baseline (ERR=0.20) 44% 38% 38% 32% Number of sessions that improve the baseline 40 40 36 36 Number of session that decrease from the baseline 29 32 32 34 Number of session that did not change the baseline 7 4 8 6 Among the 76 sessions, there were 12 sessions in which users did not click on any documents. We first compare the retrieval performance and the change over baseline between the sessions that did not contain any clicke d documents (N=12) with the sessions that contained at leas t one clicked document (N=64). Models Sessions that do GPN 0.08 (0.15) 0.32 (0.25) &lt;0.05 GPN change over baseline -0.07 (0.3) 0.11 (0.31) GP 0.15 (0.28) 0.32 (0.25) &lt;0.05 GP change over baseline 0.01 (0.13) 0.11 (0.31) SPN 0.08 (0.15) 0.31 (0.27) &lt;0.05 SPN change over baseline -0.07 (0.3) 0.09 (0.32) SP 0.15 (0.28) 0.31 (0.27) &lt;0.05 SP change over baseline 0.01 (0.13) 0.09 (0.32) Table 8 shows that the retrieval performance of our four models and the improvement of our models over baseline (except SP) is significantly better in the sessions that contain clicked documents than in sessions that did not c ontain any clicked documents. This result shows that our models n eed clicked document data to improve search results. Since we are using users X  interac tions during sessions to build the prediction models of document usefulness, in the following analysis, we present results for sessions that contain at least one clicked document (N=64) to ev aluate the performance of our prediction models. We then ex amine the relati onship between prediction accuracy and retrieval performance, by three types of performance compared with the baseline. We compare the amount of user in teraction in each session, and the prediction accuracy of document usefulness by the general model among the three groups (Table 9). The amount of user interactions is defined as the sum of the number of queries, the number of search result pages, and the number of content pages visited. Our results show that th e prediction accuracy of document usefulness in the sessions for which use of the general model increased the retrieval performance over the baseline was as high as 75%. In contrast, the prediction accuracy was only 51% in the sessions that where personalization decreased the retrieval performance. The Kruskal Wallis test revealed that the differences number of clicked documents nor the number of interactions was significantly different among three groups. Sessions Number of clicked documents in the session Number of interaction s in the session Overall accuracy of the general model We also compared the amount of us er interaction in each session, and the prediction accuracy of document usefulness by the specific model among the three groups (Table 10). As with the general model, we found that the sessions that increased retrieval performance over the baseline by the specific model had significantly higher prediction accuracy of document usefulness (64%) with the specific model as compared to the sessions that decreased the retrieval performance (34%). Again, neither the number of clicked documents nor the number of interactions was significantly different among three groups. Number of clicked documents in the session Number of interaction s in the session Overall accuracy of the general model In general, when applying our prediction models on the TREC Session Track 2011 dataset, our results indicate that the better the prediction accuracy, the greater the improvement of retrieval performance over the baseline. Figure 4 and Figure 5 show the ERR values for each search session for the baseline and for each of our personalization methods. It is clear there are specific search sessions which will benefit from our personalization methods, and others for which such personalization is either not worth the effort, or harmful. Figure 4 When to personal ize using the general model (sessions ordered according to increasing positive difference between GPN and baseline) Figure 5 When to personal ize using the specific model (sessions ordered according to increasing positive difference between SPN and baseline) The question is when should one apply personalized models? (cf. [16]) In practice we need to identify these sessions from the observable user interactions during the search session. To accomplish this we focus on interaction behaviors that are associated with sessions where ou r models were particularly good and particularly bad. Specifically , we look at the behaviors in sessions where our models improved retrieval performance by greater than 0.20 on ERR and for sessions where the models did worse than-0.05 on ERR. We compare the behavioral variables in the search sessions between the two extremes in retrieval performance defined by the GP model in Table 11. For the sessions in which retrieval perfo rmance benefited a lot from the personalization model, users spen t significantly more dwell time on each content page (51.25 seconds) than in the other group of sessions where the performance was decreased (34.61 seconds). They also spent a relatively greater proportion of task time on reading content pages (43%) than in the other group of sessions (29%). These results, in particular the former, suggest that it is possible to identify search sessions which would benefit from personalization relatively early in th e course of the search session. 
Table 11 Comparison of behavioral measures between two Behavioral measures Decrease Total time on task (seconds) 354.27 317.35 1 Total time on content pages (seconds) Total time on SERPs 223.03 141.73 .18 Mean dwell time on each content page (seconds) Mean dwell time on each SERPs (seconds) Number of query 4.23 3.96 .37 Time to first click (seconds) 33.47 22.44 .19 Query interval time (seconds) Proportion of time spent on content pages in the session In this study, we generated document usefulness prediction models based on a laboratory experiment of the influence of different types of tasks on task session behaviors. We constructed a general prediction model and several specific prediction models tailored to different types of tasks. Recursive partitioning analysis was used to generate decision tree models based on users X  interaction behaviors during search sessions. Our models identified three main behavioral m easures as important predictors of document usefulness: dwell time on document, the number of times a page has been visited in the session ( visit_id ), and the time before first click after issuing a query ( time to first click ). Besides the positive relationship between dwell time and document usefulness, we found that, in all but one case, the visit_id and time_to_first_click to be positively related to document usefulness. The specific models fo r different types of tasks show major differences by task type for both the predictors and the rules. For example, in the specific prediction model for CPE, number of mouse clicks on the document was negatively related to document usefulness, and the proportion of time spent on content pages during query interval ( prop_content ) was positively related to document usefulness. However, neither of these two measures and associated rules was useful in the specific model for OBI. Furthermore, although all models included dwell time as a usefulness differed between the different tasks, and from that of the general model. A primary focus of this study was to evaluate whether the prediction models generated from our user experiment could improve retrieval performance, so we applied our models to the TREC 2011 Session Track data. The comparison of the prediction by the different models against TREC assessors X  relevance judgments showed the general model had higher accuracy than the specific models. One reason for this is that there is a great imbalance in task type in the TREC 2011 Session Track data. The task in our experiment. Our specific model for CPE contains number of mouse clicks on the documents as one of the predictors, but such data was not available in the server-side logs in the TREC 2011 Session Track. The task type imbalance also meant that not enough data was available (10 sessions in total) for three of the four types of tasks ( SSU, DSN, DAN). Consequently, one the individual task types. In addi tion, other facets of the task types may present in the TREC Session Track that differ from those in the user experiment in our lab. Such facets could influence users X  behaviors in different ways. For example, users were given 20 minutes for searching in our experiment, whereas in the crowdsourced search sessions in the TREC Session Track, users were given only 5 minutes for sear ching. Users were likely to have searched with an awareness of time-pressure which might not have been the case during our experiment. With respect to retrieval perform ance, all of our models improved a great deal from the baseline which used pseudo relevance feedback on the last queries users issued in each session. Our general model had somewhat better retrieval performance than the specific model on both of the two relevance feedback methods. The models with positive relevance feedback performed only slightly better than the models with both positive and negative relevance feedback. This suggest s that using negative IRF may not be worth the effort, although perhaps the limited size of the data set for the TREC Session Track mitigates this conclusion. In addition, we found that the better the prediction accuracy of document usefulness, the more likely that the model could improve retrieval performance. Teevan, Dumais &amp; Horvitz [16] examined users X  clicks and explicit judgments for the same queries and found they differed greatly from one another. They proposed that the potential for personalization could be defined by th e g ap between how well search engines could perform if they were to tailor results to the individual, and how well they currently perform by returning addressed the question of when to apply personalization models by presenting the retrieval difference between our models and the baseline. We found our prediction models consistently improved retrieval performance over the baseline in some sessions, but consistently decreased the perfor mance of the baseline line in some other sessions. We compar ed some users X  behavioral measures during these sessions, and found significant differences between the two extreme groups. In particular, users spent significantly more dwell time on each content page and a greater proportion of task time on content pages for the sessions where the personalization m odels we applied we re beneficial. Dwell time on content pages has been proven to be positively related to document relevance, and one explanation for our results is that users were visiting more useful documents in the sessions where the models performed well and that query expansion based on the useful documents helped users to articulate their information needs. Therefore, IRF based on the predicted useful documents achieved better results than using the top n returned documents, as in pseudo relevance feedback. Further work is needed to investigate when the system shoul d apply personal ization models to improve retrieval performance. Because the data that we used for our personalization models should be applicable to any gene ral retrieval system, one might expect that our levels of impr ovement would be applicable to techniques with much higher baseline performance, resulting in higher absolute performance levels. It is also the case that our usefulness prediction models were us ed as input to quite standard, and rather simple relevance feedback techniques. More sophisticated use of the models could result in better overall performance improvement. In this study, we used searchers X  interaction behavioral measures derived from a laboratory experime nt to build prediction models of document usefulness. We th en evaluated the prediction accuracy and retrieval performance of these models by applying them to the TREC 2011 Session Track data set, where users search interactions were generated in radically different search sessions. We found severa l behavioral measures, e.g. dwell time, visit time, time to first click, pr oportion of time on content pages, and others could be potential predictors of document usefulness. We also found different combinations of variables and rules of prediction models based on these measures for different types of tasks. The results demonstrate that the prediction models we generated from our user experi ment improve the performance over a baseline that did not ta ke account of user interaction information in search sessions with quite different characteristics than those from which the prediction models were developed. The positive  X  X ransfer X  effect leads us to believe the models we have developed may be used for personali zation of retrieval in a variety of searching circumstances, and that we could expect even greater performance benefit when richer, client-side data that our prediction models depend upon are applied. The research that led to this work was funded by the IMLS, under grant number LG-06-07-0105-07. We thank all of the members of the Personalization of the Digita l Library Experi ence (PoODLE) research team at Rutgers Univer sity, without whose efforts this work could not have been accomp lished. Thanks to Si Sun for assistance in TREC 2011 Session Track. Thanks to Jingjing Liu for validating task classification as the third coder. Thanks to David Pane at CMU, who helped us greatly and generously in performing the Indri runs. [1] Agichtein, E., Brill, E., Du mais, S., &amp; Ragno, R. (2006). [2] Belkin, N. J., Carballo, J. P., Cool, C., Lin, S., Park, S. Y., [3] Bierig, R., Cole., M.J., &amp; Gwizdka, J. (2009). A user [4] Borlund, P. (2003). The IIR ev aluation model: A framework [5] Breiman, L. (1984). Classification and Regression Trees . [6] Chapelle, O., Metzler, D., Zhang, Y., and Grinspan, P. [7] Fox, S., Karnawat, K., Mydland, M., Dumais, S., &amp; White, [8] Kanoulas, E., Hall, M. Clough, P. Carterette, B., &amp; [9] Kelly, D. (2005). Imp licit feedback: Using behavior to infer [10] Kelly, D. &amp; Belkin, N.J. (2004). Display time as implicit [11] Kelly, D. &amp; Teevan, J. (2003). Implicit feedback for inferring [12] Li, Y. &amp; Belkin, N.J. (2008). A faceted approach to [13] Liu, C., Gwizdka, J., &amp; Liu, J. (2010). Helping identify when [14] Liu, J. &amp; Belkin, N.J. (2010). Personalizing information [15] Loper, E. and Bird, S. (2002). NLTK: The Natural Language [16] Teevan, J., Dumais, S.T., and Horvitz, E. (2010). Potential [17] White, R. W. &amp; Kelly, D. (2 006). A study on the effects of [18] White, R.W., Ruthven, I., and Jose, J.M. (2005). A study of 
