 Word alignment is a key component of most end-to-end statistical machine translation systems. The standard approach to word alignment is to construct directional generati ve models (Bro wn et al., 1990; Och and Ne y, 2003), which produce a sentence in one language given the sentence in another lan-guage. While these models require sentence-aligned bite xts, the y can be trained with no further super -vision, using EM. Generati ve alignment models do, howe ver, have serious dra wbacks. First, the y require extensi ve tuning and processing of lar ge amounts of data which, for the better -performing models, is a non-tri vial resource requirement. Second, condi-tioning on arbitrary features of the input is dif ficult; for example, we would lik e to condition on the or-thographic similarity of a word pair (for detecting cognates), the presence of that pair in various dic-tionaries, the similarity of the frequenc y of its two words, choices made by other alignment systems, and so on.

Recently , Moore (2005) proposed a discrimina-tive model in which pairs of sentences ( e, f ) and proposed alignments a are scored using a linear combination of arbitrary features computed from the tuples ( a, e, f ) . While there are no restrictions on the form of the model features, the problem of find-ing the highest scoring alignment is very dif ficult and involv es heuristic search. Moreo ver, the param-eters of the model must be estimated using averaged perceptron training (Collins, 2002), which can be unstable. In contrast, Taskar et al. (2005) cast word alignment as a maximum weighted matching prob-lem, in which each pair of words ( e tence pair ( e, f ) is associated with a score s reflecting the desirability of the alignment of that pair . Importantly , this problem is computationally tractable. The alignment for the sentence pair is the highest scoring matching under constraints (such as the constraint that matchings be one-to-one). The scoring model s ture set defined on word pairs ( e text, including measures of association, orthogra-phy, relati ve position, predictions of generati ve mod-els, etc. The parameters of the model are estimated within the frame work of lar ge-mar gin estimation; in particular , the problem turns out to reduce to the solution of a (relati vely) small quadratic program (QP). The authors sho w that lar ge-mar gin estimation is both more stable and more accurate than percep-tron training.

While the bipartite matching approach is a use-ful first step in the direction of discriminati ve word alignment, for discriminati ve approaches to com-pete with and eventually surpass the most sophisti-cated generati ve models, it is necessary to consider more realistic underlying statistical models. Note in particular two substantial limitations of the bipartite matching model of Taskar et al. (2005): words have fertility of at most one, and there is no way to incor -porate pairwise interactions among alignment deci-sions. Mo ving beyond these limitations X  X hile re-taining computational tractability X  X s the next major challenge for discriminati ve word alignment.
In this paper , we sho w how to overcome both lim-itations. First, we introduce a parameterized model that penalizes dif ferent levels of fertility . While this extension adds very useful expressi ve power to the model, it turns out not to increase the computa-tional comple xity of the aligner , for either the pre-diction or the parameter estimation problem. Sec-ond, we introduce a more thoroughgoing extension which incorporates first-order interactions between alignments of consecuti ve words into the model. We do this by formulating the alignment problem as a quadratic assignment problem (QAP), where in ad-dition to scoring indi vidual edges, we also define scores of pairs of edges that connect consecuti ve words in an alignment. The predicted alignment is the highest scoring quadratic assignment.

QAP is an NP-hard problem, but in the range of problem sizes that we need to tackle the problem can be solv ed efficiently . In particular , using standard off-the-shelf inte ger program solv ers, we are able to solv e the QAP problems in our experiments in under a second. Moreo ver, the parameter estimation prob-lem can also be solv ed efficiently by making use of a linear relaxation of QAP for the min-max formu-lation of lar ge-mar gin estimation (Taskar , 2004).
We sho w that these two extensions yield signif-icant impro vements in error rates when compared to the bipartite matching model. The addition of a fertility model impro ves the AER by 0 . 4 . Model-ing first-order interactions impro ves the AER by 1 . 8 . Combining the two extensions results in an impro ve-ment in AER of 2 . 3 , yielding alignments of better quality than intersected IBM Model 4. Moreo ver, including predictions of bi-directional IBM Model 4 and model of Liang et al. (2006) as features, we achie ve an absolute AER of 3 . 8 on the English-French Hansards alignment task X  X he best AER re-sult published on this task to date. We begin with a quick summary of the maximum weight bipartite matching model in (Taskar et al., 2005). More precisely , nodes V = V s  X  V t cor -respond to words in the  X  X ource X  ( V s ) and  X  X ar -get X  ( V t ) sentences, and edges E = { jk : j  X  V , k  X  V t } correspond to alignments between word pairs. 1 The edge weights s to which word j in one sentence can be translated using the word k in the other sentence. The pre-dicted alignment is chosen by maximizing the sum of edge scores. A matching is represented using a set of binary variables y j is assigned to word k in the other sentence, and 0 otherwise. The score of an assignment is the sum of edge scores: s ( y ) = P us begin by assuming that each word aligns to one or zero words in the other sentence; we revisit the issue of fertility in the next section. The maximum weight bipartite matching problem, arg max be solv ed using combinatorial algorithms for min-cost max-flo w, expressed in a linear programming (LP) formulation as follo ws: where the continuous variables z ation of the corresponding binary-v alued variables y hence optimal) solutions for any scoring function s ( y ) (Schrijv er, 2003). Note that although the abo ve LP can be used to compute alignments, combina-torial algorithms are generally more efficient. For Figure 2: An example fragment that requires fertility greater than one to correctly label. (a) The guess of the baseline M model. (b) The guess of the M+F fertility-augmented model. example, in Figure 1(a), we sho w a standard con-struction for an equi valent min-cost flo w problem. Ho we ver, we build on this LP to develop our exten-sions to this model belo w. Representing the predic-tion problem as an LP or an inte ger LP pro vides a precise (and concise) way of specifying the model and allo ws us to use the lar ge-mar gin frame work of Taskar (2004) for parameter estimation described in Section 3.

For a sentence pair x , we denote position pairs by x for some user pro vided feature mapping f and ab-bre viate w &gt; f ( x , y ) = P include in the feature vector the identity of the two words, their relati ve positions in their respecti ve sen-tences, their part-of-speech tags, their string similar -ity (for detecting cognates), and so on. 2.1 Fertility An important limitation of the model in Eq. (1) is that in each sentence, a word can align to at most one word in the translation. Although it is common that words have gold fertility zero or one, it is cer -tainly not always true. Consider , for example, the bite xt fragment sho wn in Figure 2(a), where bac k-bone is aligned to the phrase  X  epine dor sal . In this figure, outlines are gold alignments, square for sure alignments, round for possibles, and filled squares are tar get alignments (for details on gold alignments, see Section 4). When considering only the sure alignments on the standard Hansards dataset, 7 per -cent of the word occurrences have fertility 2, and 1 percent have fertility 3 and abo ve; when considering the possible alignments high fertility is much more common X 31 percent of the words have fertility 3 and abo ve.

One simple fix to the original matching model is to increase the right hand sides for the constraints in Eq. (1) from 1 to D , where D is the maximum allo wed fertility . Ho we ver, this change results in an undesirable bimodal beha vior , where maximum weight solutions either have all words with fertil-ity 0 or D , depending on whether most scores s are positi ve or negative. For example, if scores tend to be positi ve, most words will want to collect as man y alignments as the y are permitted. What the model is missing is a means for encouraging the common case of low fertility (0 or 1), while allo wing higher fertility when it is licensed. This end can be achie ved by introducing a penalty for having higher fertility , with the goal of allo wing that penalty to vary based on features of the word in question (such as its frequenc y or identity).

In order to model such a penalty , we introduce indicator variables z meaning: node j has fertility of at least d (and node k has fertility of at least d ). In the follo wing LP , we introduce a penalty of P of node j , where each term s increment for increasing the fertility from d  X  1 to d : max s . t . X We can sho w that this LP always has inte gral so-lutions by a reduction to a min-cost flo w problem. The construction is sho wn in Figure 1(b). To ensure that the new variables have the intended semantics, we need to mak e sure that s so that the lower cost z cost z from source and to sink, s s Figure 3: An example fragment with a monotonic gold alignment. (a) The guess of the baseline M model. (b) The guess of the M+Q quadratic model. plies that the penalty must be monotonic and con vex as a function of the fertility .

To anticipate the results that we report in Sec-tion 4, adding fertility to the basic matching model mak es the tar get alignment of the bac kbone example feasible and, in this case, the model correctly labels this fragment as sho wn in Figure 2(b). 2.2 First-order interactions An even more significant limitation of the model in Eq. (1) is that the edges interact only indi-rectly through the competition induced by the con-straints. Generati ve alignment models lik e the HMM model (Vogel et al., 1996) and IBM models 4 and abo ve (Bro wn et al., 1990; Och and Ne y, 2003) directly model correlations between alignments of consecuti ve words (at least on one side). For exam-ple, Figure 3 sho ws a bite xt fragment whose gold alignment is strictly monotonic. This monotonicity is quite common  X  46% of the words in the hand-aligned data diagonally follo w a pre vious alignment in this way. We can model the common local align-ment configurations by adding bonuses for pairs of edges. For example, strictly monotonic alignments can be encouraged by boosting the scores of edges of the form  X  ( j, k ) , ( j + 1 , k + 1)  X  . Another trend, common in English-French translation (7% on the hand-aligned data), is the local inversion of nouns and adjecti ves, which typically involv es a pair of language is often translated as a phrase (consecuti ve sequence of words) in the other language. This pat-tern involv es pairs of edges with the same origin on three of these edge pair patterns are sho wn in Fig-ure 1(c). Note that the set of such edge pairs Q = { jklm : | j  X  l |  X  1 , | k  X  m |  X  1 } is of linear size in the number of edges.

Formally , we add to the model variables z which indicate whether both edge jk and lm are in the alignment. We also add a corresponding score s the correlations we described are positi ve. (Ne ga-tive scores can also be used, but the resulting for -mulation we present belo w would be slightly dif fer -ent.) To enforce the semantics z use a pair of constraints z Since s or 1 ), then z lowing LP as an inte ger linear program will find the optimal quadratic assignment for our model: max s . t . X Note that we can also combine this extension with the fertility extension described abo ve.
 To once again anticipate the results presented in Section 4, the baseline model of Taskar et al. (2005) mak es the prediction given in Figure 3(a) because the two missing alignments are atypical translations of common words. With the addition of edge pair features, the overall monotonicity pushes the align-ment to that of Figure 3(b). To estimate the parameters of our model, we fol-low the lar ge-mar gin formulation of Taskar (2004). Our input is a set of training instances { ( x where each instance consists of a sentence pair x and a tar get alignment y parameters w that predict correct alignments on the training data: y where Y pair x
In standard classification problems, we typically measure the error of prediction, ` ( y simple 0 -1 loss. In structured problems, where we are jointly predicting multiple variables, the loss is often more comple x. While the F-measure is a nat-ural loss function for this task, we instead chose a sensible surrog ate that fits better in our frame work: weighted Hamming distance, which counts the num-ber of variables in which a candidate solution  X  y dif-fers from the tar get output y , with dif ferent penalty for false positi ves ( c + ) and false negatives ( c  X  ) : ` ( y ,  X  y ) = X
We use an SVM-lik e hinge upper bound on the loss ` ( y ` f encourages the true alignment y respect to w for each instance i : min where  X  is a regularization parameter .

In this form, the estimation problem is a mixture of continuous optimization over w and combinato-rial optimization over y into a more standard optimization problem, we need a way to efficiently handle the loss-augmented in-fer ence , max mization problem has precisely the same form as the prediction problem whose parameters we are trying to learn  X  max tional term corresponding to the loss function. Our assumption that the loss function decomposes over the edges is crucial to solving this problem. We omit the details here, but note that we can incorporate the loss function into the LPs for various models we de-scribed abo ve and  X  X lug X  them into the lar ge-mar gin formulation by con verting the estimation problem into a quadratic problem (QP) (Taskar , 2004). This QP can be solv ed using any off-the-shelf solv ers, such as MOSEK or CPLEX. 2 An important dif fer -ence that comes into play for the estimation of the quadratic assignment models in Equation (3) is that inference involv es solving an inte ger linear program, not just an LP . In fact the LP is a relaxation of the in-teger LP and pro vides an upper bound on the value of the highest scoring assignment. Using the LP re-laxation for the lar ge-mar gin QP formulation is an approximation, but as our experiments indicate, this approximation is very effecti ve. At testing time, we use the inte ger LP to predict alignments. We have also experimented with using just the LP relaxation at testing time and then independently rounding each fractional edge value, which actually incurs no loss in alignment accurac y, as we discuss belo w. We applied our algorithms to word-le vel alignment using the English-French Hansards data from the 2003 NAA CL shared task (Mihalcea and Pedersen, 2003). This corpus consists of 1.1M automatically aligned sentences, and comes with a validation set of 37 sentence pairs and a test set of 447 sentences. The validation and test sentences have been hand-aligned (see Och and Ne y (2003)) and are mark ed with both sur e and possible alignments. Using these align-ments, alignment err or rate (AER) is calculated as: Here, A is a set of proposed inde x pairs, S is the sure gold pairs, and P is the possible gold pairs. For example, in Figure 4, proposed alignments are sho wn against gold alignments, with open squares for sure alignments, rounded open squares for possi-ble alignments, and filled black squares for proposed alignments.

The input to our algorithm is a small number of labeled examples. In order to mak e our results more comparable with Moore (2005), we split the origi-nal set into 200 training examples and 247 test ex-amples. We also trained on only the first 100 to mak e our results more comparable with the exper -iments of Och and Ne y (2003), in which IBM model 4 was tuned using 100 sentences. In all our experi-ments, we used a structured loss function that penal-ized false negatives 10 times more than false posi-tives, where the value of 10 was pick ed by using a validation set. The regularization parameter  X  was also chosen using the validation set. 4.1 Featur es and results We parameterized all scoring functions s s feature sets. The features were computed from the lar ge unlabeled corpus of 1.1M automatically aligned sentences.

In the remainder of this section we describe the impro vements to the model performance as various features are added. One of the most useful features for the basic matching model is, of course, the set of predictions of IBM model 4. Ho we ver, computing these features is very expensi ve and we would lik e to build a competiti ve model that doesn X  t require them. Instead, we made significant use of IBM model 2 as a source of features. This model, although not very accurate as a predicti ve model, is simple and cheap to construct and it is a useful source of features. The Basic Matching Model: Edge Featur es In the basic matching model of Taskar et al. (2005), called M here, one can only specify features on pairs of word tok ens, i.e. alignment edges. These features include word association, orthograph y, proximity , etc., and are documented in Taskar et al. (2005). We also augment those features with the predictions of IBM Model 2 run on the training and test sentences. We pro vided features for model 2 trained in each direction, as well as the intersected predictions, on each edge. By including the IBM Model 2 features, the performance of the model described in Taskar et al. (2005) on our test set (trained on 200 sentences) impro ves from 10.0 AER to 8.2 AER, outperforming unsymmetrized IBM Model 4 (but not intersected model 4).
 As an example of the kinds of errors the baseline system mak es, see Figure 2 (where multiple fer -tility cannot be predicted), Figure 3 (where a prefer -ence for monotonicity cannot be modeled), and Fig-ure 4 (which sho ws several multi-fertile cases). The Fertility Model: Node Featur es To address errors lik e those sho wn in Figure 2, we increased the maximum fertility to two using the parameter -ized fertility model of Section 2.1. The model learns costs on the second flo w arc for each word via fea-tures not of edges but of single words. The score of taking a second match for a word w was based on the follo wing features: a bias feature, the proportion of times w  X  X  type was aligned to two or more words by IBM model 2, and the buck eted frequenc y of the word type. This model was called M + F . We also in-cluded a lexicalized feature for words which were common in our training set: whether w was ever seen in a multiple fertility alignment (more on this feature later). This enabled the system to learn that certain words, such as the English not and French verbs lik e aur ait commonly participate in multiple fertility configurations.

Figure 5 sho w the results using the fertility exten-sion. Adding fertility lowered AER from 8.5 to 8.1, though fertility was even more effecti ve in conjunc-tion with the quadratic features belo w. The M + F set-ting was even able to correctly learn some multiple fertility instances which were not seen in the training data, such as those sho wn in Figure 2.
 The First-Order Model: Quadratic Featur es With or without the fertility model, the model mak es mistak es such as those sho wn in Figure 3, where atypical translations of common words are not cho-sen despite their local support from adjacent edges. In the quadratic model, we can associate features with pairs of edges. We began with features which identify each specific pattern, enabling trends of monotonicity (or inversion) to be captured. We also added to each edge pair the fraction of times that pair X  s pattern (monotonic, inverted, one to two) oc-curred according each version of IBM model 2 (for -ward, backw ard, intersected).

Figure 5 sho ws the results of adding the quadratic model. M + Q reduces error over M from 8.5 to 6.7 (and fix es the errors sho wn in Figure 3). When both the fertility and quadratic extensions were added, AER dropped further , to 6.2. This final model is even able to capture the diamond pattern in Figure 4; the adjacent cycle of alignments is reinforced by the quadratic features which boost adjacenc y. The ex-ample in Figure 4 sho ws another interesting phe-nomenon: the multi-fertile alignments for not and d  X  eput  X  e are learned even without lexical fertility fea-tures (Figure 4b), because the Dice coef ficients of those words with their two alignees are both high. Ho we ver the surf ace association of aur ait with have is much higher than with would . If, howe ver, lexi-cal features are added, would is correctly aligned as well (Figure 4c), since it is observ ed in similar pe-riphrastic constructions in the training set.
We have avoided using expensi ve-to-compute fea-tures lik e IBM model 4 predictions up to this point. Ho we ver, if these are available, our model can im-pro ve further . By adding model 4 predictions to the edge features, we get a relati ve AER reduction of 27%, from 6.5 to 4.5. By also including as features the posteriors of the model of Liang et al. (2006), we achie ve AER of 3.8, and 96.7/95.5 precision/recall.
It is comforting to note that in practice, the burden of running an inte ger linear program at test time can be avoided. We experimented with using just the LP relaxation and found that on the test set, only about 20% of sentences have fractional solutions and only 0.2% of all edges are fractional. Simple rounding 3 of each edge value in the LP solution achie ves the same AER as the inte ger LP solution, while using about a third of the computation time on average. We have sho wn that the discriminati ve approach to word alignment can be extended to allo w flexible fertility modeling and to capture first-order inter -actions between alignments of consecuti ve words. These extensions significantly enhance the expres-sive power of the discriminati ve approach; in partic-ular , the y mak e it possible to capture phenomena of monotonicity , local inversion and contiguous fertil-ity trends X  X henomena that are highly informati ve for alignment. The y do so while remaining compu-tationally efficient in practice both for prediction and for parameter estimation.

Our best model achie ves a relati ve AER reduc-tion of 25% over the basic matching formulation, beating intersected IBM Model 4 without the use of any compute-intensi ve features. Including Model 4 predictions as features, we achie ve a further rela-tive AER reduction of 32% over intersected Model 4 alignments. By also including predictions of an-other model, we dri ve AER down to 3.8. We are currently investig ating whether the impro vement in AER results in better translation BLEU score. Al-lowing higher fertility and optimizing a recall bi-ased cost function pro vide a significant increase in recall relati ve to the intersected IBM model 4 (from 88.1% to 94.4%), with only a small degradation in precision. We vie w this as a particularly promising aspect of our work, given that phrase-based systems such as Pharaoh (K oehn et al., 2003) perform better with higher recall alignments.

