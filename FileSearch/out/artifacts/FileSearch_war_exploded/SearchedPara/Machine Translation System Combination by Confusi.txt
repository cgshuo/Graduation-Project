 System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Hen-derson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Banga-lore et al., 2001).

Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypothe-ses are aligned against the skeleton, forming a lattice with each arc representing alternative word candi-dates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothe-sis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is mea-sured by an evaluation metric, such as translation er-ror rate (TER) (Snover et al., 2006). The new trans-lation hypothesis is generated by selecting the best path through the network.

We present a novel method for system combina-tion which exploits the syntactic similarity of system outputs. Instead of constructing a string-based con-fusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes ex-ponentially many parse trees in a polynomial space. The packed forest, or confusion forest , is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, sys-tem outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed for-est is generated using a variant of Earley X  X  algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to those found in the parse trees. Spuri-ous ambiguity during the generation step is further reduced by encoding the tree local contextual infor-mation in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley X  X  algorithm.
Experiments were carried out for the system combination task of the fifth workshop on sta-tistical machine translation (WMT10) in four di-rections, f Czech, French, German, Spanish g -to-English (Callison-Burch et al., 2010), and we found comparable performance to the conventional con-fusion network based system combination in two language pairs, and statistically significant improve-ments in the others.

First, we will review the state-of-the-art method which is a system combination framework based on confusion networks ( x 2). Then, we will introduce a novel system combination method based on con-fusion forest ( x 3) and present related work in con-sensus translations ( x 4). Experiments are presented in Section 5 followed by discussion and our conclu-sion. The system combination framework based on confu-sion network starts from computing pairwise align-ment between hypotheses by taking one hypothe-sis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) in-troduced TER (Snover et al., 2006) to measure the edit-based alignment.

Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hy-potheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network con-structed from the four hypotheses in Figure 1(a), as-suming the first hypothesis is selected as our skele-ton. The network consists of several arcs, each of which represents an alternative word at that position, including the empty symbol,  X  .

This pairwise alignment strategy is prone to spu-rious insertions and repetitions due to alignment er-rors such as in Figure 1(a) in which  X  X reen X  in the third hypothesis is aligned with  X  X orest X  in the skele-ton. Rosti et al. (2008) introduces an incremental method so that hypotheses are aligned incremen-tally to the growing confusion network, not only the . . . . . . . . . . . . . . . . . . Figure 1: An example confusion network construc-tion skeleton hypothesis. In our example,  X  X reen trees X  is aligned with  X  X lue forest X  in Figure 1(c).
The confusion network construction is largely in-fluenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a pas-sive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously in-serted words, as in  X  X  saw the blue trees was found X  in Figure 1(c). Rosti et al. (2007b) partially re-solved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. The confusion network approach to system com-bination encodes multiple hypotheses into a com-pact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypothe-ses into a confusion forest, which is a packed forest which represents multiple parse trees in a polyno-mial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharing tree frag-. . . . . . I . Figure 2: An example packed forest representing hy-potheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chi-ang, 2007).

More formally, a hypergraph is a pair  X  V; E  X  where V is the set of nodes and E is the set of hy-peredges. Each node in V is represented as X @ p where X 2 N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsu-lates each node id relative to its parent. The root node is given the address  X  and the address of the first child of node p is given p: 1 . Each hyperedge e 2 E is represented as a pair  X  head ( e ) ; tails ( e )  X  where head ( e ) 2 V is a head node and tails ( e ) 2 V is a list of tail nodes, corresponding to the left-hand side and the right-hand side of an in-stance of a rule in a CFG, respectively. Figure 2 presents an example packed forest for the parsed hypotheses in Figure 1(a). For example, VP @2 has two hyperedges,  X  VP @2 ;  X  derivations where the former takes the grammatical construction in passive voice while the latter in ac-tive voice.

Given system outputs, we employ the following grammar based approach for constructing a confu-sion forest: First, MT outputs are parsed. Second, Initialization: Scan: Predict: Complete: Goal: Figure 3: The deductive system for Earley X  X  genera-tion algorithm a grammar is learned by treating each hyperedge as an instance of a CFG rule. Third, a forest is gen-erated from the unique root symbol of the extracted grammar through non-terminal rewriting. 3.1 Forest Generation Given the extracted grammar, we apply a variant of Earley X  X  algorithm (Earley, 1970) which can gener-ate strings in a left-to-right manner from the unique root symbol, TOP. Figure 3 presents the deductive inference rules (Goodman, 1999) for our generation algorithm. We use capital letters X 2 N to denote non-terminals and x 2 T for terminals. Lowercase Greek letters , and are strings of terminals and non-terminals ( T [N ) . u and v are weights asso-ciated with each item.

The major difference compared to Earley X  X  pars-ing algorithm is that we ignore the terminal span in-formation each non-terminal covers and keep track of the height of derivations by h . The scanning step will always succeed by moving the dot to the right. Combined with the prediction and completion steps, our algorithm may potentially generate a spu-riously deep forest. Thus, the height of the forest is constrained in the prediction step not to exceed H , which is empirically set to 1.5 times the maximum height of the parsed system outputs. 3.2 Tree Annotation The grammar compiled from the parsed trees is lo-cal in that it can represent a finite number of sen-tences translated from a specific input sentence. Al-though its coverage is limited, our generation algo-rithm may yield a spuriously large forest. As a way to reduce spurious ambiguities, we relabel the non-terminal symbols assigned to each parse tree before extracting rules.

Here, we replace each non-terminal symbol by the state representation of Earley X  X  algorithm corre-sponding to the sequence of prediction steps starting from TOP. Figure 4(a) presents an example parse tree with each symbol replaced by the Earley X  X  state in Figure 4(b). For example, the label for VBD is replaced by S + NP : VP + VBD : NP which corresponds to the prediction steps of TOP ! S, S ! NP VP and VP ! VBD NP. The context represented in the Earley X  X  state is further limited by the vertical and horizontal Markovization (Klein and Manning, 2003). We define the vertical order v in which the label is limited to memorize only v pre-vious prediction steps. For instance, setting v = 1 yields NP : VP + VBD : NP in our example.
 Likewise, we introduce the horizontal order h which limits the number of sibling labels memorized on the left and the right of the dotted label. Limiting h = 1 implies that each deductive step is encoded with at most three symbols.
 No limits in the horizontal and vertical Markovization orders implies memorizing of all the deductions and yields a confusion forest representing the union of parse trees through the grammar collection and the generation processes. More relaxed horizontal orders allow more reorder-ing of subtrees in a confusion forest by discarding the sibling context in each prediction step. Like-wise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F , new k -best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k -best parsing (Huang and Chiang, 2005). We use a linear combi-Figure 4: Label annotation by Earley X  X  alsogirhtm state nation of features as our objective function to seek for the best derivation  X  d : where h ( d; F ) is a set of feature functions scaled by weight vector w . We use cube-pruning (Chiang, 2007; Huang and Chiang, 2007) to approximately intersect with non-local features, such as n -gram language models. Then, k -best derivations are ex-tracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). Consensus translations have been extensively stud-ied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hy-potheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly ex-ploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from pars-ing errors such as the confusion network construc-tion influenced by alignment errors. Even with pars-ing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for simi-lar hypotheses.

Rosti et al. (2007a) describe a re-generation ap-proach to consensus translation in which a phrasal translation table is constructed from the MT outputs aligned with an input source sentence. New transla-tions are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a re-generation approach in which an off-the-shelf mono-lingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of gener-ating forests from semantic representations (Langk-ilde, 2000), we generate forests from a CFG encod-ing the consensus among parsed hypotheses.

Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is ei-ther translation-level in which no new translation is generated, or derivation-level in that the rules shar-ing the same left-hand-side are used in both sys-tems. While our work is similar in that a new forest is constructed by sharing rules among systems, al-though their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). # of systems 6 16 8 14 avg. words tune 10.6K 10.9K 10.9K 11.0K sentences tune 455 Table 1: WMT10 system combination tuning/testing data 5.1 Setup We ran our experiments for the WMT10 sys-tem combination task usinge four language pairs, f
Czech, French, German, Spanish g -to-English (Callison-Burch et al., 2010). The data is summa-rized in Table 1. The system outputs are retok-enized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased.

We implemented our confusion forest sys-tem combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are ex-tracted and a new forest is generated as described in Section 3. Our baseline, also implemented in ci-cada , is a confusion network-based system combi-nation method ( x 2) which incrementally aligns hy-potheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon re-moval, the network is transformed into a forest by parsing with monotone rules of S ! X, S ! S X and X ! x . k -best translations are extracted from the forest using the forest-based algorithms in Sec-tion 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009).
We use three lower-cased 5-gram language mod-els h i lm ( d ) : English Gigaword Fourth edition 1 , the English side of French-English 10 9 corpus and the news commentary English data 2 . The count based features h t ( d ) and h e ( d ) count the number of ter-minals and the number of hyperedges in d , respec-tively. We employ M confidence measures h m s ( d ) for M systems, which basically count the number of rules used in d originally extracted from m th system hypothesis (Rosti et al., 2007a).

Following Macherey and Och (2007), BLEU (Pa-pineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e ::: e M , M BLEU scores are computed for d using each of the system outputs e m as a reference h b ( d ) = BP ( e ; e m ) exp where e = yield( d ) is a terminal yield of d , BP ( ) and n ( ) respectively denote brevity penalty and n -gram precision. Here, we use approximated un-clipped n -gram counts (Dreyer et al., 2007) for com-puting n ( ) with a compact state representation (Li and Khudanpur, 2009).

Our baseline confusion network system has an ad-ditional penalty feature, h p ( m ) , which is the total edits required to construct a confusion network us-ing the m th system hypothesis as a skeleton, normal-ized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Pa-pineni et al., 2002). We vary the horizontal orders, h = 1 ; 2 ; 1 with vertical orders of v = 3 ; 4 ; 1 . Systems without statistically significant differences from the best result ( p &lt; 0 : 05 ) are indicated by bold face. Setting v = 1 and h = 1 achieves compa-rable performance to CN. Our best results in three languages come from setting v = 1 and h = 2 , which favors little reordering of phrasal structures. In general, lower horizontal and vertical order leads to lower BLEU. Table 2: Translation results in lower-case BLEU. CN for confusion network and CF for confusion forest with different vertical ( v ) and horizontal ( h ) Markovization order.
Table 3 presents oracle BLEU achievable by each combination method. The gains achievable by the CF over simple reranking are small, at most 2-3 points, indicating that small variations are encoded in confusion forests. We also observed that a lower horizontal and vertical order leads to better BLEU potentials. As briefly pointed out in Section 3.2, the higher horizontal and vertical order implies more faithfulness to the original parse trees. Introducing new tree fragments to confusion forests leads to new phrasal translations with enlarged forests, as pre-sented in Table 4, measured by the average number lang cz-en de-en es-en fr-en
CN 2,222.68 47,231.20 2,932.24 11,969.40 CF v =4 254.45 651.10 302.01 477.51 CF v =3 286.01 802.79 349.21 575.17 Table 4: Hypegraph size measured by the average number of hyperedges ( h = 1 for CF).  X  X attice X  is the average number of edges in the original CN. of hyperedges 3 . The larger potentials do not imply better translations, probably due to the larger search space with increased search errors. We also conjec-ture that syntactic variations were not captured by the n -gram like string-based features in Section 5.2, therefore resulting in BLEU loss, which will be in-vestigated in future work.

In contrast, CN has more potential for generat-ing better translations, with the exception of the German-to-English direction, with scores that are usually 10 points better than simple sentence-wise reranking. The low potential in German should be interpreted in the light of the extremely large confu-sion network in Table 4. We postulate that the di-vergence in German hypotheses yields wrong align-ments, and therefore amounts to larger networks with incorrect hypotheses. Table 4 also shows that CN produces a forest that is an order of magnitude larger than those created by CFs. Although we can-not directly relate the runtime and the number of hyperedges in CN and CFs, since the shape of the forests are different, CN requires more space to en-code the hypotheses than those by CFs.

Table 5 compares the average length of the min-imum/maximum hypothesis that each method can produce. CN may generate shorter hypotheses, whereby CF prefers longer hypotheses as we de-crease the vertical order. Large divergence is also observed for German, such as for hypergraph size. We presented a confusion forest based method for system combination in which system outputs are merged into a packed forest using their syntactic Table 5: Average min/max hypothesis length pro-ducible by each method ( h = 1 for CF). similarity. The forest construction is treated as a generation from a CFG compiled from the parsed outputs. Our experiments indicate comparable per-formance to a strong confusion network baseline with smaller space, and statistically significant gains in some language pairs.

To our knowledge, this is the first work to directly introduce syntactic consensus to system combina-tion by encoding multiple system outputs into a sin-gle forest structure. We believe that the confusion forest based approach to system combination has future exploration potential. For instance, we did not employ syntactic features in Section 5.2 which would be helpful in discriminating hypotheses in larger forests. We would also like to analyze the trade-offs, if any, between parsing errors and confu-sion forest constructions by controlling the parsing qualities. As an alternative to the grammar-based forest generation, we are investigating an edit dis-tance measure for tree alignment, such as tree edit distance (Bille, 2005) which basically computes in-sertion/deletion/replacement of nodes in trees. We would like to thank anonymous reviewers and our colleagues for helpful comments and discussion.
