 BBN Technologies University of Colorado University of Colorado
Most semantic role labeling (SRL) research has been focused on training and evaluating on training to the particular corpus. This article describes the operation of art SRL system, and analyzes the robustness of the system when trained on one genre of data and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data.Experimentsarethenpresentedtoevaluatetheportabilityofthesystemtoanothersourceof data. These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and features are dominant in the argument identification task. 1.Introduction
Automatic, accurate, and wide-coverage techniques that can annotate naturally oc-curring text with semantic structure can play a key role in NLP applications such as information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering (Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is one method for producing such semantic structure. When presented with a sentence, a semantic role labeler should, for each predicate in the sentence, first identify and then label its semantic arguments. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. In the bulk of recent work, this problem has been cast as a problem in supervised machine learning. Using these techniques with hand-corrected syntactic parses, it has been possible to achieve accuracies within the range of human inter-annotator agree-ment. More recent approaches have involved using improved features such as n -best parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argu-ment interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypothesesfromdifferentlabelingsystemsusinginference(M ` arquezetal.2005);aswell as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information.Somehavealsotriedtojointlydecodethesyntacticandsemanticstructures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M ` arquez 2004; Carreras and M ` arquez 2005).
Although all of these systems perform quite well on the standard test data, they show significant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data.
 state-of-the art SRL system. Results are presented for training and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data.
 genre of data. These experiments are based on comparisons of performance using
PropBanked WSJ data and PropBanked Brown corpus data. The results indicate that whereas syntactic parses and identification of the argument bearing nodes transfer relatively well to a new corpus, role classification does not. Analysis of the reasons for this generally point to the nature of the more lexical/semantic features dominating the classification task, as opposed to the more structural features that are relied upon for identifying which constituents are associated with arguments. 2.SemanticAnnotationandCorpora
In this article, we report on the task of reproducing the semantic labeling scheme used bythePropBankcorpus(Palmer,Gildea,andKingsbury2005).PropBankisa300k-word corpus in which predicate argument relations are marked for almost all occurrences of non-copula verbs in the WSJ part of the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993). PropBank uses predicate independent labels that are sequential sitive verb) and A RG 1istheP ROTO -P ATIENT (usually its direct object). In addition to these corearguments , additional adjunctivearguments , referred to as A RG Ms,are also marked. Some examples are A RG M-L OC , for locatives, and A RG M-T MP , for temporals. Table 1 shows the argument labels associated with the predicate operate in PropBank. tree representation along with the argument labels is shown in Figure 1. [ aligns with one or more nodes in the hand-corrected Treebank parses. Although most frequently the arguments are identified by one node in the tree, there can be cases where the arguments are discontinuous and more than one node is required to identify parts of the arguments. 290 do not have any words associated with them. These can also be marked as arguments.
As traces are typically not reproduced by current automatic parsers, we decided not to consider them in our experiments X  X hether or not they represent arguments of a predicate. None of the previous work has attempted to recover such trace arguments. PropBank also contains arguments that are coreferential.
 shared task on semantic role labeling. The first part of a discontinuous argument is labeled as it is, and the second part of the argument is labeled with a prefix  X  C- X  appended to it. All coreferential arguments are labeled with a prefix  X  R- X  appended. set, Section 00 as the development set, and Section 23 as the test set. The training set comprises about 90,000 predicates instantiating about 250,000 arguments and the test set comprises about 5,000 predicates instantiating about 12,000 arguments. 3.TaskDescription
In ASSERT , the task of semantic role labeling is implemented by assigning role labels to constituents of a syntactic parse. Parts of the overall process can be analyzed as three different tasks as introduced by Gildea and Jurafsky (2002): 1. Argument Identification  X  X his is the process of identifying parsed 2. Argument Classification  X  X iven constituents known to represent 3. Argument Identification and Classification  X  X  combination of the two tasks.
N ULL node because it does not correspond to a semantic argument. The node NP that dominates about 20 minutes is a NON -N ULL node, because it does correspond to a semantic argument X  X  RG M-T MP . 4.ASSERT(AutomaticStatisticalSEmanticRoleTagger) 4.1SystemArchitecture
ASSERT 1 produces a separate set of semantic role labels for each candidate predicate in a sentence. Because PropBank only annotates arguments for non-copula/non-auxiliary verbs, those are also the predicates considered by ASSERT . ASSERT performs constituent-based role assignment. The basic inputs are a sentence and a syntactic parse of the sentence. For each constituent in the parse tree, the system extracts a set of features and uses a classifier to assign a label to the constituent. The set of labels used are the
PropBank argument labels plus NULL , which means no argument is assigned to that constituent for the predicate under consideration.
 perform well on text classification tasks, where data is represented in a high dimen-sional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000;
Lodhi et al. 2002). We formulate the problem as a multi-class classification problem using an SVM classifier. We employ a O NE vs A LL (OVA) approach to train n classifiers for a multi-class problem. The classifiers are trained to discriminate between examples 292 of each class, and those belonging to all other classes combined. During testing, the classifier scores on an example are combined to predict its class label.

Matsumoto 2000, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C =1; and, tolerance of the termination criterion, e = 0.001. SVMs output distances from the classification hyperplane, not probabilities. These distances may not be comparable
These raw SVM scores are converted to probabilities by fitting to a sigmoid function as done by Platt (2000).
 is made independently, without considering other arguments assigned to the same predicate. This ignores a potentially important source of information: that a predicate is likely to instantiate a certain set of arguments. To represent this information, a backed-off trigram model is trained for the argument sequences. In this model, the predicate is considered as an argument and is part of the sequence. This model represents not only what arguments a predicate is likely to take, but also the probability of a given sequence of arguments. During the classification process the system generates an argument lattice using the n -best hypotheses for each node in the syntax tree. A Viterbi search through the lattice uses the probabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to find the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL . The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2Features The feature set used in ASSERT is a combination of features described in Gildea and
Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identified. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classified.
 represented with the string NP  X  S  X  VP  X  VBD.  X  and  X  represent upward and downward movement in the tree, respectively. 4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent. 4.2.4 Position. Whether the constituent is before or after the predicate. 4.2.5 Voice. Whether the predicate is realized as an active or passive construction. A set of hand-written tgrep expressions operating on the syntax tree is used to identify passives. 4.2.6SubCategorization. Thisisthephrasestructureruleexpandingthepredicate X  X parent node in the parse tree. For example, in Figure 3, the subcategorization for the predicate  X  X ent X  is VP  X  VBD-PP-NP. that verbs with similar semantics will tend to have similar direct objects. For example, verbs such as eat , devour ,and savor will tend to all occur with direct objects describing food. The clustering algorithm uses a database of verb X  X irect-object relations extracted by Lin (1998). The verbs were clustered into 64 classes using the probabilistic co-occurrence model of Hofmann and Puzicha (1998). We then use the verb class of the current predicate as a feature. 4.2.8 Head Word. Syntactic head of the constituent. 4.2.9 Head Word POS. Part of speech of the head word. 4.2.10 Named Entities in Constituents. Binary features for seven named entities
IdentiFinder (Bikel, Schwartz, and Weischedel 1999). 4.2.11 Path Generalizations. 1. PartialPath X  Path from the constituent to the lowest common ancestor 2. Clause-basedpathvariations X  Position of the clause node (S, SBAR) 294 3. Pathn-grams X  This feature decomposes a path into a series of trigrams. 4. Singlecharacterphrasetags X  Each phrase category is clustered to a 4.2.12 Predicate Context. We added the predicate context to capture predicate sense variations. Two words before and two words after were added as features. The POS of the words were also added as features. 4.2.13Punctuation. Punctuation plays an particularly important role for some adjunctive arguments, so punctuation on the left and right of the constituent are included as feature value. 4.2.14 Head Word of PP. Many adjunctive arguments, such as temporals and locatives, occur as prepositional phrases in a sentence, and it is often the case that the head words of those phrases, which are prepositions, are not very discriminative; for example, in the city and in a fe wminutes both share the same head word in and neither contain a named entity, but the former is A RG M-L OC , whereas the latter is A RG M-T MP . The head word of the first noun phrase inside the prepositional phrase is used for this feature.
Preposition information is represented by appending it to the phrase type, for example,  X  X P-in X  instead of  X  X P. X  4.2.15 First and Last Word/POS in Constituent. The first and last words in a constituent along with their parts of speech. 4.2.16 Ordinal Constituent Position. In order to avoid false positives where constituents far away from the predicate are spuriously identified as arguments, we added this feature which is a concatenation of the constituent type and its ordinal position from the predicate. 4.2.17ConstituentTreeDistance. This is a more fine-grained way of specifying the already present position feature. This is the number of constituents that are encountered in the path from the predicate to the constituent under consideration. 4.2.18 Constituent Relative Features. These are nine features representing the phrase type, head word, and head word part of speech of the parent, and left and right siblings of the constituent. 4.2.19 Temporal Cue Words. There are several temporal cue words that are not captured by the named entity tagger and were added as binary features indicating their presence.
The BOW toolkit was used to identify words and bigrams that had highest average mutual information with the A RG M-TMP argument class. 4.2.20SyntacticFrame. Sometimes there are multiple children under a constituent having the same phrase type, and one or both of them represent arguments of the predicate. In such situations, the path feature is not very good at discriminating between them, and the position feature is also not very useful. To overcome this limitation, Xue and Palmer (2004) proposed a feature which they call the syntacticframe . For example, if the sub-categorization for the predicate is VP  X  VBD-NP-NP, then the syntactic frame feature for the first NP in the sequence would be,  X  X bd NP np, X  and for the second it would be  X  X bd np NP. X  4.3Performance
Table2illustratestheperformanceofthesystemusingTreebankparsesandusingparses produced by a Charniak parser (Automatic). Precision (P), Recall (R), and F-scores are given for the identification and combined tasks, and Classification Accuracy (A) for the classificationtask.ClassificationperformanceusingCharniakparsesisonly1%absolute worse than when using Treebank parses. On the other hand, argument identification performance using Charniak parses is 10.9% absolute worse. About half of the ID errors are due to missing constituents in the Charniak parse. Techniques to address the issue of constituents missing from the syntactic parse tree are reported in Pradhan, Ward et al. (2005). 4.4FeatureSalience
In Pradhan, Hacioglu et al. (2005) we reported on a series of experiments to show the relative importance of features to the Identification task and the Classification task.
The data show that different features are more salient for each of the two tasks. For the Identification task, the most salient features are the Path and Partial Path. The Predicate was not particularly salient. For Classification, the most salient features are Head Word, First Word, and Last Word of a constituent as well as the Predicate itself. For Classification, the Path and Phrase Type features were not very salient. whereas more specific lexical or semantic features are important for Classification. As 296 across genres. 5.RobustnesstoGenreofData
Most work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set. Both the Treebank on which the syntactic parser is trained, and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the Wall Street Journal . Improvements to the system may reflect tuning to the specific data set rather than real progress. For this technology to be widely accepted it is critical that it perform reasonably well on text with styles different from the training data. The availability of PropBank annotation for another corpus of a very different style than WSJ makes it possible to evaluate the portability of SRL techniques, and to understand some of the factors affecting performance. 5.1TheBrownCorpus
The Brown Corpus is a standard corpus of American English that consists of about one million words of English text printed in the calendar year 1961 (Ku  X  cera and Francis 1967). The corpus contains about 500 samples of 2,000+ words each. The motivation for creating this corpus was to create a heterogeneous sample of English text useful for comparative language studies. Table 3 lists the sections in the Brown corpus. 5.2SemanticAnnotation
Release 3 of the Penn Treebank contains hand-corrected syntactic trees from a subset of the Brown Corpus (sections F, G, K, L, M, N, P, and R). Sections belonging to the newswire genre were not included because a considerable amount of similar material was already available from the WSJ portion of the Treebank. Palmer, Gildea, and Kingsbury (2005) annotated a significant portion of the Treebanked Brown corpus with PropBank roles. The PropBanking philosophy is the same as described earlier.
In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we use the release of the Brown PropBank dated September 2005. 6.RobustnessExperiments on the WSJ corpus to performance on the Brown corpus. The intent is to understand how well the algorithms and features transfer to other sources and to understand the nature of any problems. 6.1Cross-GenreTesting
The first experiment evaluates the performance of the system when it is trained on annotateddatafromonegenreoftext(WSJ)andisusedtolabelatestsetfromadifferent genre (the Brown corpus). The ASSERT system described earlier, trained on WSJ Sec-tions 02 X 21, was used to label arguments for the PropBanked portion of the Brown corpus. As before, the Charniak parser was used to generate the syntax parse trees. fication for each of the eight different text genres as well as the overall performance on Brown. As can be seen, there is a significant degradation across all the various sections of Brown. In addition, although there is a noticeable drop in performance for the Identification task, the bulk of the degradation comes in the combined task. degradation: 1. Syntactic parsing errors X  X he semantic role labeler is completely 2. The Brown corpus may in fact be fundamentally more difficult than the 298 The following discussion explores each of these possibilities in turn.
 parse leading to an Identification error. The syntactic parser deletes 6.2% of the argu-ment bearing nodes in the tree when it is trained and tested on WSJ. When tested on
Brown, this number increases to 8.1%, a relative increase of 30%. This effect goes some way toward explaining the decrease in Identification performance, but does not explain the large degradation in combined task performance.
 syntactic trees from the Treebanks for both corpora. This permits an analysis of other factors affecting the performance difference. For this experiment, we evaluated per-formance for all combinations of training and testing on WSJ and Brown. A test set for the Brown corpus was generated by selecting every tenth sentence in the corpus.
The development set used by Bacchiani et al. (2006) was withheld for future parameter tuning. No parameter tuning was done for these experiments. The parameters used for the data reported in Table 2 were used for all subsequent tests reported in this article. This procedure results in a training set for Brown that contains approximately 14k predicates. In order to have training sets comparable in size for the two corpora, stratified sampling was used to create a WSJ training set of the same size as the Brown training set. Section 23 of WSJ is still used as the test set for that corpus. when the system is trained on the 14k predicate WSJ training. Testing on Brown vs. WSJ results in a modest reduction in F-score from 95.3 to 93.0 for argument identification.
Although there is some reduction in Identification performance in the absence of errors in the syntactic parse tree, the effect is not large. However, argument classification shows a large drop in accuracy from 86.1% to 72.9%. These data reiterate the point that syntactic parse errors are not the major factor accounting for the reduction in performance for Brown.
 for testing results on WSJ and Brown. The first row of Table 7 shows the performance when ASSERT is trained on the full WSJ training set of Sections 2 X 21 (90k predicates).
The second row shows performance when it is trained on the reduced set of 14k pred-icates. Whereas the F1 score for Identification dropped by 1.5 percentage points (from 96.8% to 95.3%) the Classification rate dropped by 6.9% percent absolute. Classification seemingly requires considerable more data before its performance begins to asymptote. 300 performance of argument Identification is essentially the same as when training and testing on WSJ. However, argument Classification is 6 percentage points worse (80.1% vs. 86.1%) when training and testing on Brown than when training and testing on WSJ.
This pattern is consistent with our third hypothesis given previously: Brown may be an intrinsically harder corpus for this task.
 1. More unique predicates or head words than are seen in the WSJ set, so 2. More predicate sense ambiguity in Brown; 3. Less consistent relations between predicates and head words; 4. A greater preponderance of difficult semantic roles in Brown; 5. Relatively fewer examples of predictive features such as named entities. The remainder of this section explores each of these possibilities in turn. predicate sense information as a feature in ASSERT . Because only about 60% of the
PropBanked Brown corpus was tagged with predicate sense information, these results
Brown training and test sets are subsets of the earlier ones, with about 10k predicates in training and 1k in testing. For comparison, we used the same size WSJ training data. Table 8 shows the performance when trained on WSJ and Brown, and tested on Brown, with and without predicate sense information, and for both Treebank parses and
Charniak parses. We find that there is a small increase in the combined identification and classification performance when trained on Brown and tested on Brown. the training data. Because we know that Predicate and Head Word are two particularly salient features for classification, the percentages of a combination of these features in the Brown test set that are seen in both the training sets should be informative. This information is shown in Table 9. In order to get a cross-corpus statistic, we also present the same numbers on the WSJ test set.
 However, a cross comparison shows that there is about a 15% drop in coverage from
WSJ/WSJ to WSJ/Brown. It is also interesting to note that for WSJ, the drop in coverage for predicate lemmas is almost the same as that for individual predicate senses. This fur-ther confirms the hypothesis that WSJ has a more homogeneous collection of predicates. about the same drop in coverage for predicate lemmas, but a much more significant drop for the senses. This variation in senses in Brown is probably the reason that adding sense information helps more for the Brown test set. In the WSJ case, the addition of word sense as a feature does not add much information, and so the numbers are not much different than for the baseline. Similarly, we can see that percentage of head words seen across the two genres also drop significantly, and they are much lower to begin with. Finding the coverage for the predicate lemma and head word combination is still worse, and this is not even considering the sense. Therefore, data sparseness is another potential reason that the importance of the predicate sense feature does not reflect in the performance numbers.
 bution of PropBank arguments in this corpus. Table 10 shows the classification perfor-manceforeachargument,foreachofthefourconfigurations(trainonBrownorWSJand test on WSJ or Brown). Among the two most frequent arguments X  X  RG 0andA RG 1 X 
A RG 1 seems to be affected the most. When the training and test sets are from the same genre, the performance on A RG 0 is slightly worse on the Brown test set. A RG 1onthe other hand is about 5% worse on both precision and recall, when trained and tested on
Brown. For core-arguments A RG 2 X 5 which are highly predicate sense dependent, there is a much larger performance drop.
 named entities in the corpus. Table 11 shows the frequency of occurrence of name entities in 10k WSJ and Brown training sets. It can be seen that number of organizations talked about in Brown is much smaller than in WSJ, and there are more person names.
Also, monetary amounts which frequently fill the A RG 3andA RG 4 slots are also much more infrequent in Brown, and so is the incidence of percentages. This would definitely have some impact on the usability of these features in the learned models. 302 7.EffectofImprovedSyntacticParses
Practical natural language processing systems will always use errorful automatic parses, and so it would be interesting to find out how much syntactic parser errors hin-der performance on the task of semantic role labeling. Fortunately, recent improvements to the Charniak parser provided an opportunity to test this hypothesis. We use the latest version of the Charniak parser that does n -best re-ranking (Charniak and Johnson 2005) and the model that is self-trained using the North American News corpus (NANC). ThisversionadaptsmuchbettertotheBrowncorpus(McClosky,Charniak,andJohnson 2006a, 2006b). We also use another model that is trained on the Brown corpus itself. The performance of these parsers is shown in Table 12.
 1. ASSERT is trained on features extracted from automatically generated 2. ASSERT is trained on features extracted from automatically generated 3. ASSERT is trained on features extracted from automatically generated 4. ASSERT is trained on features extracted from automatically generated 5. ASSERT is trained on features extracted from automatically generated experiment. Table 13 shows the results. For simplicity of discussion we have labeled the five conditions as A, B, C, D, and E. Comparing conditions B and C shows that when the featuresusedtotrain ASSERT areextractedusingasyntacticparserthatistrainedonWSJ it performs at almost the same level on the task of identification, regardless of whether it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This, however, is about 5 X 6 F-score points lower than when all the three (the syntactic parser training set, ASSERT training set, and ASSERT test set) are from the same genre X  X SJ or
Brown, as seen in A and D. For the combined task, the gap between the performance for conditions B and C is about 10 F-score points apart (59.1 vs. 69.8). Looking at the argument classification accuracies, we see that using ASSERT trained on WSJ to test
Brown sentences results in a 12-point drop in F-score. Using ASSERT trained on Brown 304 using the WSJ-trained syntactic parser reduces accuracy by about 5 F-score points.
When ASSERT is trained on Brown using a syntactic parser also trained on Brown, we get a quite similar classification performance, which is again about 5 points lower than what we get using all WSJ data. Finally, looking at conditions C and D we find that the difference in performance on the combined task of identification and classification using the Brown corpus for training ASSERT is very close (69.8 vs. 68.9) even though the syntactic parser used in C has a performance that is about 3.2 points worse than that used in D. This indicates that better parse structure is less important than lexical semantic coverage for obtaining better performance on the Brown corpus. 8.AdaptingtoaNewGenre
One possible way to ameliorate the effects of domain specificity is to incrementally add small amounts of data from a new domain to the already available out-of-domain training data. In the following experiments we explore this possibility by slowly adding data from the Brown corpus to a fixed amount of WSJ data.
 tated. Therefore, we will take six different scenarios X  X wo in which we will use correct
Treebank parses, and the four others in which we will use automatically generated parses using the variations used before. All training sets start with the same number of examples as that of the Brown training set. The part of this section used as a test set for the CoNLL 2005 shared task was used as the test set for these experiments. This test set contains 804 predicates in 426 sentences of Brown section K. identification and classification improves gradually until about 5,625 examples of sec-tion K, which is about 75% of the total added, above which it adds very little. Even when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding 7,500 instances of this new genre achieves almost the same performance as when all three are from the same genre (67.2 vs. 69.9). For the task of argument identification, the incremental addition of data from the new genre shows only minimal improvement. The system that uses a self-trained syntactic parser performs slightly better than other 306 versions that use automatically generated syntactic parses. The improvement on the identification performance is almost exclusively due to recall. The precision numbers are almost unaffected, except when the labeler is trained on WSJ PropBank data. 9.Conclusions
In this article, we have presented results from a state-of-the-art Semantic Role Labeling system trained on PropBank WSJ data and then used to label test sets from both the WSJ corpus and the Brown corpus. The system X  X  performance on the Brown test set exhibited a large drop compared to the WSJ test set. An analysis of these results revealed that the subtask of Identification, determining which constituents of a syntax tree are arguments of a predicate, is responsible for only a relatively small part of the drop in performance.
TheClassificationtask,assigninglabelstoconstituentsknowntobearguments,iswhere the major performance loss occurs.
 mance difference: significantly worse than training and testing on WSJ, even when the amount of training data is controlled for. Training and testing on Brown showed performance intermediate between training and testing on WSJ and training on WSJ and testing on Brown. This leads to our final hypothesis.
 different data. The more homogeneous training data allows the system to rely heavily on specific features and relations in the data. It is usually the case that training on a more heterogeneous data set does not give quite as high performance on test data from the same corpus as more homogeneous data, but the heterogeneous data ports better to other corpora. This is seen when training on Brown compared to WSJ. The observation with this explanation. For the Identification task, structural features such as path and partial path tend to be the most salient while the Classification task relies more heavily on lexical/semantic features such as specific predicate-head word combinations. the training data and on test sets of the same genre as the training data. But they would be more likely to lead to better generalization across genres. Training on very homogeneoustrainingsetsandtestingonsimilartestsetsgivesamisleadingimpression of the performance of a system.
 Acknowledgments References 308
