 The modularization of the driving design concerns is a key factor to achieve maintainable information systems [16, 21]. A concern is any important property or area of interest of a system that we want to treat in a modular way [23]. Business rules, distribution, persistence, and security are examples of typical concerns found in many information systems and that are important, albeit hard, to achieve full modularization. The inadequate separation of concerns degrades design modularity and may lead to maintainability-related design flaws [6, 11]. Detection of these design flaws by programmers is far from trivial and requires effective support. 
Software metrics are the key means for assessing the maintainability of information systems [3, 7]. The community of software metrics has traditionally explored quantifiable module properties, such as class coupling, cohesion, and interface size, in order to identify maintainability problems in a software project [3, 8, 19, 20]. More specifically, software measurement is also seen as a pragmatic solution to find symptoms of particular design flaws, such as code smells [17, 19]. Code smells are symptoms that something may be wrong in the system code [12]. 
Marinescu [19], for instance, relies on traditional metrics to compose strategies aiming to detect code smells. However, some code smells are often a direct result of poor separation of concerns, and traditional module-driven measurement cannot be tailored to quantify properties of concern modularity. Whereas traditional metrics quantify the properties of modules, the concern metrics quantify properties of concerns, such as scattering and tangling [10]. A growing number of concern metrics have been proposed [5, 6] aiming to quantify key characteristic s of concerns X  implementation. Indeed, concern metrics have been applied with different purposes and used in several empirical studies. They are used, for instance, to compare aspect-oriented and object-oriented programming techniques [4, 11, 13, 14] and to identify crosscutting concerns that should be refactored [6]. However, we still lack empirical knowledge on the effectiveness of concern metrics to support code smell detection in information systems. 
To fill this gap, this paper presents an empirical investigation of the effectiveness of concern metrics compared with traditional metrics on the identification of code smells. We report the results of a series of experiments relying on two benchmark information systems, named Health Watcher [14] and MobileMedia [11]. This study focuses on a two-dimension analysis comparing the trade-offs on the recall and time efficiency of code smell detection. To analyze the recall, we compare classes identified as suspects of exhibiting a code smell with the reference list of code smells provided by the actual developers in each information system. We also assess time efficiency based on the recorded time spent by each subject in the experimental tasks. This empirical study involved 54 subjects, which were divided into three groups. metrics: (i) only traditional metrics , (ii) only concern metrics , and (iii) both traditional and concern metrics, called hybrid metrics from now on. These metrics were previously applied to the source code of both target information systems. Subjects then analyzed the values of metrics aimi ng to detect three specific code smells, namely Divergent Change [12], Shotgun Surgery [12], and God Class [22]. Our overall results confirmed that concern metrics, in fact, contribute to improve the detection of these code smells. More specifically, this study shows that (i) concern metrics are clearly useful to detect Divergent Change and God Class; (ii) the subject X  X  level of experience does not have significant impact on detection rates; (iii) time explains most of the variations observed in detection rates; and (iv) recall of each metric suite is largely dependent on the adequacy of each metric to quantify a property explicitly mentioned in the smell definition. 
The rest of this paper is organized as follows. Section 2 summarizes the concepts of software metrics and code smells. Section 3 describes the study procedures. Sections 4 and 5 discuss the main results of this empirical study. Section 6 discusses the study limitations and related work. Section 7 concludes this paper and points out directions for future work. Software metrics have played an important role in understanding and analyzing information systems [3, 7, 17]. For the purpose of this study, software metrics can be divided into three sets: traditional metrics (Section 2.1), concern metrics (Section 2.2), and hybrid metrics; i.e., a combination of both traditional and concern metrics. Section 2.3 describes the three code smells that we investigate in this study. 2.1 Traditional Metrics We selected a set of the most widely used metrics to be a baseline in this study. The selected set includes object-oriented (OO) metrics proposed by Chidamber and Kemerer [3] and well-documented metrics in the software engineering literature [7]. Table 1 summarizes the metrics used in this study, while detailed definitions can be found elsewhere [3, 7]. 
We selected the most common and widely used traditional metrics for several reasons. First, it is still not well known whether some particular combinations of these metrics can precisely detect specific code smells. Hence, finding combinations involving either concern or traditional metrics might be a relevant result of this paper. Second, we aim to select a reduced number of metrics since many metrics could make the analysis harder and with redundant measurements. Finally, the selected metrics have been used in previous work [8, 11, 19] and they seem to assist developers in software maintenance tasks. 2.2 Concern Metrics Concern metrics have been defined aiming to capture modularity properties associated with the realization of concerns in software artifacts [10]. Their goal is the identification of specific design flaws [6] or design degeneration caused by poor modularization of concerns [9]. Some recent studies [6, 8] have also shown that concern metrics can be useful indicators of defect-prone modules. Concern is something that you may want to treat as a modular unit, including non-functional requirements and programming language idioms [23]. Concern metrics rely on a mapping between concerns and design elements [9, 10]. The mapping consists of assigning a concern to the corresponding de sign elements that realize it. Table 2 presents a brief definition of the concern metrics evaluated in this paper. 
A more detailed description and discussion of these metrics can be found elsewhere [4, 6, 10, 13]. These metrics were selected for evaluation in this paper because they have been successfully used in a number of studies related to software maintainability [11, 13, 14]. However, no systematic study has been performed to evaluate whether these concern metrics support code smell detection. 2.3 Code Smells Code smells were proposed by Kent Beck in Fowler X  X  book [12] as a mean to diagnose symptoms that may be indicative of something wrong in the system code. This paper investigates the use of concern metrics to detect three code smells, namely Divergent Change [12], Shotgun Surgery [12] and God Class [22], which are described below. These code smells were chosen because they recurrently appear in information systems and are related to poor modularization of concerns [2, 19]. Divergent Change. This smell occurs when one class is often changed in different ways for different reasons [12]. For example, we have to change three methods of a class every time we get a new database or we have to change other four methods every time there is a new financial instrument. Depending on the number of assignments of a given class, it may undergo unrelated changes. The fact that a class undergoes various kinds of changes can be associated with a symptom of concern tangling [2]. Shotgun Surgery. This code smell is somehow the opposite of Divergent Change. We identify a Shotgun Surgery instance every time we make a kind of change that code smell can lead to small changes in classes that have a common concern [2]. God Class. This code smell describes an object that knows too much or does too God Class implements too many concerns and, so, it has too many responsibilities [2]. This study aims at evaluating the effectiveness of concern metrics in detecting code smells. Our study relies on traditional metrics as baseline. Therefore, we perform a comparative analysis between traditional and concern metrics in order to identify whether the latter supports the former in detecting three specific code smells. Section 3.1 introduces the two target information systems. Sections 3.2 and 3.3 present, respectively, part in this study. Finally, Section 3.4 explains the tasks assigned to each subject. 3.1 Target Systems Our study involved two information systems: Health Watcher [14] and MobileMedia [11]. These systems were selected because th ey have been previously used in other maintainability-related studies [4, 8, 11, 18], and we have access to their developers and experts. Therefore, we were able to recover a reference list of actual code smells for each analyzed information system (see Section 3.2). A brief description of the Health Watcher and MobileMedia functionalities and their key concerns are described below. Most of these concerns recurrentl y appear in typical information systems. Health Watcher. It is a Web-based information system that supports the registration and management of complaints to the public health system [14]. This system has about 6 KLOC. Some concerns implemented in Health Watcher that we used are: Business , Concurrency , Distribution , Exception Handling , Persistence , and View . MobileMedia. Our study also involved the 7th version of the MobileMedia system applications that manipulate photo, music, and video on mobile devices. The concerns of our interest in MobileMedia are: Sorting , Favorites , Exception Handling , Security , and Persistence . 3.2 Code Smells Reference List Before conducting the study, we performed a systematic code analysis of Health Watcher and MobileMedia aiming to determine which classes were affected by the relevant code smells. We also relied on two experts in each information system to help us building the reference lists. These experts participated of the development, maintenance, or assessment of the systems. Our goal was to detect actual instances of each code smell in both systems. Table 3 presents classes in the final reference list of each code smell per system. Reference List Protocol. Each expert was instructed to individually use their own strategy for detecting code smells in the system classes. As a result, different strategies were used. One expert focused on code inspection following more traditional code analysis. Following a different path, another expert used a complementary set of automated detection strategies [18] to identify candidate instances of the three code smells. For each code smell, the sets of potential instances  X  one set from each expert  X  were not exactly the same, although they have many classes in common (approximately 80% and 75% for Health Watcher and MobileMedia, respectively). In order to achieve a consensus, we promoted discussions among experts of the same system. The result of their discussion was recorded as a joint decision and double-checked by ourselves. 3.3 Background of Subjects This study involved a set of 54 subjects, named S1 to S54, from two different institutions (UFMG/Brazil and Lancaster/UK). Subjects from the 1st institution were 11 young IT professional taking an advanced SE course, 4 PhD candidates, and 12 undergraduate students. Subjects from the 2nd institution were 14 PhD candidates and 13 undergraduate students. We organized subjects in such a way that each group worked with only one set of metrics: traditional metrics, concern metrics, or hybrid metrics. The study was performed using the OO designs of both information systems. We conducted 13 rounds of the experiment in different dates. Subjects were organized as follows: (i) 24 subjects detected Divergent Change in 6 rounds, (ii) 20 subjects detected Shotgun Surgery in 5 rounds, and (iii) 10 subjects detected God Class in 2 rounds. Health Watcher was used by subjects of Lancaster to detect all three code smells, while MobileMedia was used by subjects of UFMG to detect Divergent Change and Shotgun Surgery. Further details about the distribution of subjects are available at the project website [1]. 
Before running the experiment, we used a background questionnaire (also available at [1]) to balance previous knowledge of each subject. Table 4 summarizes knowledge that subjects claimed to have in the background questionnaire. Although the subjects were asked to answer the questionnaire, it was not compulsory. Therefore, some subjects annotated in the last column (No Answer) in Table 4 have not answered the questionnaire. In fact, we asked subject to indicate their level of knowledge by choosing one of the following options: none, few, moderate, and high experience. The other columns list subjects who claimed to have moderate or high knowledge in a particular skill. Subjects answered questions about their level of knowledge with respect to Class Diagrams, Java Programming, and Software Metrics. Furthermore, they indicated their previous academic and work experience. Some subjects do not appear in a row because they have few or none experience in that particular topic. For instance, with respect to work experience in detecting Divergent Change, subjects S1 to S3 (and others) have not answered the questionnaire, while subjects S4, S27, S39-S44, and S47 claimed to have none or little knowledge in Java Programming. In general, excluding 13 subjects who have not answered the background questionnaire, we have observed that (i) about 60% of the subjects have moderate to high knowledge in Class Diagram and Java Programming; and (ii) 70% of the subjects have moderate to high knowledge in at least one topic. Therefore, in general, all subjects have at least basic knowledge required to perform the experimental tasks, and subjects are fairly distributed among the groups of metrics. 3.4 Experimental Tasks The study was preceded by a 30-minute training session to allow subjects to familiarize themselves with the evaluated metrics and the target code smells. After the training session, each subject received a doc ument containing: (i) a brief explanation and a partial view of the system design as a Class Diagram, and (ii) a description of the concerns involved in the respective information systems. The document also described steps and guidelines that subjects should follow, the questions they should answer, and information they should register. In addition, we provided subjects with the results of the metrics in the respective information system under analysis. In order to identify the classes with code smells, we asked subjects to reason about the metrics and identify which of them (alone or combined with other metrics) provide relevant indicators based on the code smell description. We also asked subjects to register the time taken to conclude the experimental tasks and to explain which metrics they used or not to detect each code smell. Each group of subjects (traditional, concern or hybrid) only had access to the results of metrics to which they were assigned. Subjects had no access to source code of the information systems. This section presents the results of our experiments. Section 4.1 introduces the recall and precision metrics, while Sections 4.2 to 4.4 report the results per code smell. 4.1 Evaluation Metrics: Recall and Precision We rely on three metrics, namely True Positive (TP), False Positive (FP), and False Negative (FN), collected based on the reference lists (Section 3.2). True Positive and False Positive quantify the number of correctly and wrongly identified code smells by a subject. False Negative, on the other hand, quantifies the number of code smells a subject missed out. Based on these metrics, we quantify recall and precision, presented below, to support our analysis. Recall measures the fraction of relevant classes listed by a subject. Relevant classes are classes that appear in the reference list (TP + FN). Precision measures the ratio of correctly detected smells by the total classes a subject listed (TP + FP). We focus our discussion mainly on recall because it is a measure of completeness. That is, high recall means that the subject was able to identify most code smells in the system. High precision, on the other hand , means that a subject indicated more relevant (TP) than irrelevant (FP) code smells. For code smell detection, a large number of false positives are preferred over a large number of false negatives, because manual inspection, which is inevitable, tends to uncover false positives. 4.2 Concern Metrics Support Divergent Change Detection Table 5 presents the results for the identification of Divergent Change. Rows in this table present three pieces of data: Recall (R), Precision (P), and the Time (T) in minutes used by subjects to complete their tasks. In total, 24 subjects had to identify Divergent Change in the target systems. Table 5 shows that subjects in the concern and hybrid groups achieved better results than those in the traditional group. The average recall of the concern group was 62%. Two out of five subjects in this group identified all code smells (100% of recall) . On the other hand, the best achievement by a subject using only traditional metric wa s 33% of recall. Results of subjects in the hybrid group vary from 0% to 100% of recall (S19 and S16) being on average 41%. These results reveal that, even when analy zed in isolation, concern metrics are an effective means for Divergent Change detection. 4.3 Hard to Detect Shotgun Surgery with Metrics Table 6, which follows the same structure of Table 5, presents the results for Shotgun Surgery. Note that no group of subjects stands out with good results in this scenario. In fact, only one subject in each group achieved more than 60% of recall: S28 scored 67% analyzing traditional metrics, S30 scored 75% in the concern group, and S35 scored 67% of recall analyzing hybrid metrics. The concern group performed a little better: all subjects scored more than 25% of recall and the average recall was 44%. However, the poor detection rates for almost all subjects suggest that the used metrics cannot properly indicate Shotgun Surgery instances. 
In addition to a poor recall, almost all su bjects (except S34) also had low precision rates. In fact, more than half of the Shot gun Surgery instances detected by the subjects were incorrect, regardless of the metrics used. Interestingly, the subjects detecting Shotgun Surgery in general spent less time (on average) in their tasks than the subjects assigned to detect other code sm ells. That is, although subjects could not succeed detecting Shotgun Surgery, they did not take much longer to conclude their tasks. This result might indicate that, if developers do not have appropriate means to detect a code smell, they give up with their duties soon. 4.4 Joint Data Analysis Favor God Class Detection metrics when used in isolation do not offer appropriate means to detect God Class. Two subjects (S45 and S46) in the traditional group scored only 33% of both recall and precision. This low performance is much worse than the one achieved by the concern and hybrid groups. For example, two out of three subjects in the concern group and three out of four subjects in the hybrid group scored 100% of recall. Subjects S49 and S51 are exceptions. In addition, S52 in the hybrid group achieved full precision and recall. Therefore, joint analysis of concern and traditional metrics seems to succeed in detecting this particular code smell. This section aims to answer three research questions. We focus on the most interesting results, but the complete raw data can be found on the project website [1]. Section 5.1 analyzes the recall of concern me trics compared to the traditional metrics. Section 5.2 discusses to which extent the background of subjects and the time spent impact the recall of code smell detection. Section 5.3 analyzes possible combinations of metrics that increases the recall of identifying each code smell. 5.1 Comparing Concern Metrics and Traditional Metrics The main goal of this paper is to evaluate the effectiveness of concern metrics to detect code smells. Towards that goal, this section aims to answer the following specific research question: RQ1. How accurate do concern metrics perform in comparison with traditional metrics to detect code smells? We start by investigating whether the type of system (Health Watcher and MobileMedia) influences the detection of code smells. Table 8 shows average recall results for traditional, concern and hybrid metrics, along with corresponding values of variance, sample size (i.e., number of subjects who participated in the experiment) and 90% confidence intervals. Results are presented separately for each system -Health Watcher and MobileMedia -and for each type of code smell. We also show results for all code smells combined (row All ). In order to check for statistically significant differences across systems, metrics and/or types of code smell, we perform an unpaired t-test 1 with 90% confidence level [15]. 
Focusing first on the use of concern metrics to detect code smells in general (i.e., row All ); we note that the confidence intervals computed for subjects who used concern metrics for the two systems do not overlap. Therefore, we can state that the results for the two systems are significantly different at the 90% confidence. As shown in Table 8, the results for the system analyzed  X  Health Watcher  X  are significantly better (75% higher recall, on average). In other words, detection of code smells using the concern metrics leads to higher recall while using the Health Watcher system. On the other hand, the two confidence intervals computed for the group of subjects who used traditional metrics do overlap. This fact indicated that the results for both systems are not statistically different, with 90% confidence. The same behavior is observed for the group of subjects who used hybrid metrics. In other words, whereas the system used does impact the detection of code smells using concern metrics, the detection using traditional and hybrids metrics is not significantly influenced by it. 
Next, we applied the unpaired t-test (90% confidence level) to evaluate whether the concern metrics lead to significantly differen t results compared to the other groups of metrics for a fixed system, considering all code smells combined (row All ). We found that the concern metrics produce significantly higher recall, compared to traditional metrics for the Health Watcher system. For the MobileMedia system, there is a statistical tie, at 90% confidence, though average results are better for the concern metrics. Moreover, we also found that the concern metrics outperform the hybrid best ones, among those analyzed, for the detection of three types of code smells as hybrid metrics, they are not likely to obtain better accuracy compared to the concern metrics, since the quantity of metrics could hinder the detection of the code smell. We may argue that concern metrics would be more time efficient because (i) the set only includes four metrics, and (ii) their definitions capture concerns properties that might be related to the code smells. 
We also examine whether the type of code smell detected influences the recall of concern metrics in comparison with traditional ones. We restrict our analysis to two code smells, Divergent Change and Shotgun Surgery, because God Class was not analyzed on MobileMedia. Our results indicate that there is no significant difference between the two systems in terms of recall, for any code smell. In other words, subjects were able to recover around the same rates of code smells, regardless of the analyzed system. This is an interesting result because it supports the claims that metrics abstract most of the system complexi ty [7]. Therefore, metric-based detection of code smells is expected to scale up to larger systems. 
After ascertaining that the difference between the systems in terms of recall is not significant, we applied t-tests (90% confidence level) to compare concern metrics against traditional and hybrid metrics for each of three code smells separately, considering the results for both systems together (Table 8). Our results show that the superiority of the concern metrics varied according to the type of code smell. We observed that the use of concern metrics was consistently better in comparison with traditional metrics in the Divergent Change and God Class detection cases. However, the difference between both types of metrics for Shotgun Surgery is not statistically significant (with 90% confidence). Additionally, we observed that the difference between concern and hybrid metrics is not significant, independently of the type of code smell to be identified. These results indicate that the accuracy of the metric suite is largely dependent on the adequacy of each metric to quantify a property explicitly mentioned in the smell definition. For instance, God Class is characterized by the  X  X igh amount of class members with the realization of multiple responsibilities X  [12]. 
This property is better captured by concern metrics. This probably explains why the concern metrics outperformed the traditional ones for God Class detection. Data also suggests that detecting Divergent Change with only traditional metrics seems harder when compared to the support of concern metrics. The explanation could be that this code smell is closely related to poor separation of concerns. Divergent Change often occurs when several concerns are tangled into a module [2]. Therefore, this module is likely to be changed by different reasons. Focusing on subjects that used concern metrics (concern and hybrid groups), it is interesting to note that 10 out of 18 subjects in either groups achieved 68% of recall on average. 5.2 Background of Subjects Our goal in this section is to analyze whether the background of subjects can impact the results. In other words, we aim to answer the following research question: RQ2. Does background of subjects impact the efficiency of the detected code smell?
To answer RQ2, we evaluate the impact of both the background of subjects and the time spent by them on the effectiveness of the detection when using concern metrics. To that end, we apply a 2 k full factorial design with k=2 factors, namely the developers' work experience and the time spent in detected code smells [15]. As discussed in Section 3.3, all subjects have at least basic knowledge in the relevant topics of software development, namely UML Class Diagram, Java Programming, and Measurement. Therefore, we decided to draw this analysis with respect to work experience of subjects which varied a lot among subjects [1]. In this analysis, we excluded subjects that did not answer th e background questionnaire (Section 3.3). 
We focus on the recall of the detected code smells using the concern metrics as the response variable. For this analysis, we consider the results for all code smells and both systems together. Since we did not observe statistical difference in the recall of detection across systems when using concern metrics (Section 5.1), we grouped the results for both systems together for this analysis. Moreover, we also consider all three code smells indistinctly. 
We divided subjects into two categories acco rding to their work experience: (i) no experience indicates those subjects who never worked, or worked for fewer than 6 months, and (ii) some experience identifies those subjects who worked for at least 6 months in software development industry. Additionally, we also divided subjects into two categories according to the time spent in detected code smells: (i) short time indicates those subjects who took less than 33 minutes (overall average) to detect the code smells, and (ii) long time indicates those subjects who took at least 33 minutes. 
In general, results show that the recall tends to increase with the work experience and the time spent in the detection, as one might expect. In order to quantify the relative impact of each of these factors on the subjects X  recall, we compute the percentage of the variation in the measured recall that can be credited to each factor in variation explained by a factor/interaction, the more important it is to the response variable [15]. 
Out of the total variation observed in our measurements, 96% can be attributed to the time spent in the detection, whereas only 4% is due to variations in the subjects X  work experience and 1% can be attributed to the interaction of these two factors. Thus, both the work experience factor and the interaction seem of little importance to the final recall, compared to the time subjects spent in detecting the code smells. The latter has a major impact on the final recall. Indeed, the results clearly show that the subjects who spent more time to analyze the concern metrics achieved the better results in terms of recall. One possible explanation is the complexity of concern metrics, which require more time from subjects to successfully perform the detection. Additionally, even the subjects who have no experience tend to obtain a higher recall when they spend a longer time to detect the smell. 5.3 Metrics Flocking Together In this section, we analyze possible metrics that might be useful to detect specific code smells and answer the following research question. RQ3. Is there a combination of metrics that increases recall of code smell detection?
As explained in Section 3.4, subjects reported the metrics they considered useful for each code smell. Based on their answers, we analyzed in this section the metrics that were considered useful by at least three subjects. In order to determine which metrics were used together to detect code smells, we performed analysis of subjects who used the same metrics and scored high in terms of recall. Table 9 shows the metrics that at least three subjects claimed to have used for Divergent Change. In this case, we also restricted our analyzes to metrics with average of recall higher than 30%. Both the Number Concern per Component (NCC) and Lack of Cohesion in Methods (LCOM) metrics were considered useful to detect Divergent Change by eleven subjects. Subjects that considered these metrics useful achieved 60% and 34% of recall in average, respectively. Additionally, the concern metric Concern Diffusion over Components (CDC) was considered useful by 3 subjects. It is interesting to observe that subjects that considered concern metrics NCC and CDC useful achieved better results in terms of recall. 
In particular, NCC seems the most effective metric (among the analyzed ones) to detect Divergent Change. For instance, S7, S8, S12 and S16 used NCC -solo or in combination with other metrics -and achieved 94% of recall. We also observed that subjects who indicated NCC as not being useful achieved less than 11% of recall; as it is the case of S10, S13 and S19. Additionally, subjects who indicated NCC and LCOM as being useful achieved 50% of recall in average. For instance, we observed that the metrics were used together by subjects S12 and S15. These subjects achieved 75% and 50% of recall respectively. Interestingly, while S12 had 100% of precision, S15 had only 25%. We also observed that subjects who indicated NCC and LCOM as not being useful achieved 0% of recall; as it is the case of S19. 
Since most subjects had poor performance for detecting Shotgun Surgery, Table 10 presents metrics considered useful by at least three subjects when detecting this code smell. Coupling between Object (CBO) was considered useful by eleven subjects. However, these subjects achieved only 15% of recall in average. On the other hand, five subjects indicated Concern Diffusion over Components (CDC) as being useful and achieved 23% of recall in average. In addition, Number Concern per Component (NCC) was considered useful by four subjects who achieved 42% of recall in average. Hence, the concern metrics NCC achieved better results in terms of recall. A combination of these metrics, i.e.., NCC and CBO, was used together by subject S37 who achieved 33% of recall. In fact, all subjects that used NCC, solo or in combination with other metrics, scored higher than 30% of recall. This is the case of subjects S32 (33%), S35 (67%), S36 (33%), and S37 (33%). However, a combined analysis of Tables 6 and 10 does not allow us to conclude that these metrics (and any other) are good to detect Shotgun Surgery du e to the global symptoms associated with this code smell. 
Table 11 shows for God Class the metrics (i) considered useful by at least three subjects and (ii) with average of recall for these subjects higher than 60%. Coupling between Object (CBO) and Lack of Cohesion in Methods (LCOM) were considered useful to detect God Class by at least four subjects. Subjects using these metrics achieved about 67% of recall in average. On the other hand, three metrics also considered useful achieved recall rates above 85%, namely Weighted Methods per Class (WMC), Lines of Code (LOC), and Concern Diffusions over LOC (CDLOC). This result suggests that size metrics, such as LOC and WMC, and the concern metric CDLOC are good indicators of God Class. Additionally, we observed some cases of metrics that were used together. WMC with LOC seems the best combination of metrics. It was used by S52 and S53 an d worked well since both subjects achieved 100% of recall. In addition, the combination of WMC and CBO, was used by S47 and S53 and worked well since subjects achieved 67% and 100% of recall respectively. Another case was the combination of CBO with LCOM used by the subjects S51, S53 and S54. These Subjects achieved 78% of recall in average. The conclusions obtained here are restricted to the involved metrics, code smells, and Although we acknowledge these limitations, we note that our study fills a gap in the literature by reporting original analyses on the benefits of using concern metrics for detecting code smells. Additionally, this paper describes an experimental framework that can be used in further rounds of this study. 
Ultimately, the recall of concern metrics depends on how accurate the mapping (assignment) of each concern to code elements was. Fortunately, we observed in a previous study [9] that, apart from Concern Diffusion over Lines of Code (CDLOC), the mapping process does not significantly impact the concern metrics assessed in this paper. Additionally, in order to mitigate this threat, we relied on concern mappings produced by the original developers. Whet her the concern mapping was fully correct or not, it just reflects how concern metrics would be used in practice. 
Detection strategies of code smells have been the subject of recent studies reported in the literature. They are usually based on exploiting information that is extracted from the source code [6, 8, 9, 11, 14, 17, 19] and rely on the combination of metrics. Metrics has been historically used to det ect code smells [17, 19]. Marinescu [19] proposed the use of strategies composed of traditional metrics for detecting code smells. He observed that multiple metrics are required to capture all factors in the code smell definition. He relied on several traditional metrics also used in their study, but have not used concern metrics. 
Several studies have used traditional and concern metrics to assess diverse maintainability attributes of information systems, such as instability [11, 14] and error-proneness [6, 8]. Some of these studies [11, 14] rely on concern metrics to support the comparison of aspect-oriente d and object-oriented decompositions. Unlike our work, these studies implicitly assume that concern metrics are reliable indicators of the respective quality attribute assessed. This paper, on the other hand, aims to verify whether concern metrics can provide appropriate means to detect code smells. 
Eaddy and his colleagues [6] have carried out three experiments to evaluate the usefulness of concern metrics to identify error-prone modules. Their experiments evaluated six concern metrics; two of them are also used in our experiment, namely CDC and CDO. They found a moderate to strong correlation between the concern metrics and defects in modules for all three experiments. The purpose of our study is different, due the fact that we are not focused on error-proneness analysis. Our work complements and extends Eaddy X  X  findings since we observed that concern metrics could also serve as reliable indicators of code smells. The evaluation of software maintainability is largely dependent on the availability of metrics that accurately detect code smells. Concern metrics are increasingly being effectiveness of concern metrics to detect code smells. Our results revealed that concern metrics are clearly useful to detect Divergent Change and God Class and that experience of developers does not have influence on the effectiveness of code smell detection. Additionally, we observed that the effectiveness of each metric suite is largely dependent on the adequacy of each metric to quantify a property explicitly mentioned in the smell definition. For instance, we observed that the concern metric Number of Concerns per Component (NCC) was efficient to detect Divergent Change even when used alone because it seems to quantify a dimension of module cohesion that is not captured by other metrics. 
This study represents a first stepping-stone towards the evaluation of concern metrics to detect code smells. We are currently working on strategies to detect code smells based on the concern metrics we found useful. We also plan to perform further empirical studies to analyze the role of concern metrics at different levels of abstraction, such as architectural and detailed design. Acknowledgments. This work was partially supported by FAPEMIG, grants APQ-02376-11 and APQ-02532-12, and CNPq grant 485907/2013-5. 
