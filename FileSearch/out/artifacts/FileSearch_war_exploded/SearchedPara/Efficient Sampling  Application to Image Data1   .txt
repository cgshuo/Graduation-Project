 As the size of stored data is increasing day-by-day thanks to cheaper storage devices and increasing number of information sources such as Internet, the need for scalability is intensifying. Often sampling is used to reduce the data size while remaining the underlying structure. Use of a simple random sample, however may lead to unsatisfactory results. The problem is that such a sample may not adequately represent the entire data set due to random fluctuations in the sampling process. The difficulty is particularly apparent at small sample sizes. outputs a sample. It starts with a relatively large simple random sample of trans-actions and deterministically trim the sample to create a final subsample whose distance from the complete database is as small as possible. For reasons of com-putational efficiency, it defines the subsample as close to the original database if the high-level aggregates of the subsample normalized by the total number of data points are close to the normalized aggregates in the database. These normalized aggregates typically correspond to 1-itemset or 2-itemset supports in the association-rule setting or, in the setting of a contingency table, relative marginal or cell frequencies. The key innovation of ease lies in the method by which the final subsample is obtained. Unlike fast [2], which obtains the final subsample by trimming away ou tliers in a process of quasi-greedy descent, ease uses an approximation method to obtain the final subsample by repeated halv-ing. Unlike fast ,the ease provides a guaranteed upper bound on the distance between the initial sample and final subsample. In addition, ease can process transactions on the fly, i.e., a transaction is examined only once to determine whether it belongs to the final subsample. Moreover, the average time needed to process a transaction is proportional to the number of items in that transac-tion. We showed that ease leads to much better estimation of frequencies than srs . Experiments in the context of both association-rule mining and classical contingency-table analysis indicated that ease outperforms both fast and srs .  X  Due to its halving nature, ease has certain granularity in sample ratio. In [1]  X  ease was built especially for categorical count data, e.g., transactional data. We validate and compare the output samples of ease 2 , srs and easier by performing two important data mining tasks: classification and association rule mining. Support vector machine ( svm ) is used as the classifier to image data due to its high classification accuracy and strong theoretical foundation [3]. svm classifier results show that easier samples outperforms srs samples in accuracy and ease samples in time. easier achieves the same or even better accuracy than ease . Similar results are obtained for association rule mining. We also report the association rule mining results for ibm quest data set [4] in order to compare and contrast with the earlier reported results [1].  X  -approximation method and ease algorithm. In Section 3 we introduce the new easier algorithm and analyze the performance vis-a-vis ease . The application of image processing is discussed in Section 4. In Section 5 experimental results are presented. Conclusion and future work are given in Section 6.
 Notation. Denote by D the database of interest, by S a simple random sample drawn without replacement from D ,andby I the set of all items that appear in
D .Let N = | D | , n = | S | and m = | I | . Also denote by I ( D ) the collection of itemsets that appear in D ; a set of items A is an element of I ( D ) if and only if the items in A appear jointly in at least one transaction t  X  D .If A contains exactly k (  X  1) elements, then A is sometimes called a k -itemset. In particular, the 1-itemsets are simply the original items. The collection I ( S ) denotes the itemsets that appear in S ;ofcourse, I ( S )  X  X  ( D ). For k  X  1 we denote by I ( D )and I k ( S ) the collection of k -itemsets in D and S , respectively. of transactions in T that contain A . The support of A in D and in S is given threshold s&gt; 0, an item is frequent in D (resp., in S ) if its support in D (resp., in S ) is no less than s . We denote by L ( D )and L ( S ) the frequent itemsets in D and S ,and L k ( D )and L k ( S ) the collection of frequent k -itemsets in D and S , respectively. Specifically, denote by S i the set of all transactions in S that contains item A i ,andby r i and b i the number of red and blue transactions in S i respectively. Red means the transactions will be kept in final subsample and blue means the transactions will be deleted. Q is the penalty function of r i and b . f r denotes the ratio of red transactions, i.e., the sample ratio. Then the ratio of blue transactions is given by f b =1  X  f r . In order to obtain a good representation of a huge database,  X  -approximation method is used to find a small subset so that the supports of 1-itemset are close to those in the entire database. The sample S 0 of S is an  X  -approximation if its discrepancy satisfies Dist ( S 0 ,S )  X   X  . The discrepancy is computed as the distance of 1-itemset frequencies between any subset S 0 and the superset S : Given an  X &gt; 0, Epsilon Approximation Sampling Enabled ( ease ) algorithm is proposed to efficiently obtain a sample set S 0 which is an  X  -approximation of S . S is obtained from the entire dataset D by using srs . A repeated halving method keeps about half of the transactions in each round. Each halving iteration of ease works as follows: 1. In the beginning, uncolor all transactions. 2. Color each transaction in S as red or blue. Red means the transaction is 3. The coloring decision is based on a penalty function Q i for item A i . Q i is low transaction will be colored blue and deleted. Otherwise, it will be colored red and added to the sample. The initial value of  X  i is 1  X  exp (  X  ln(2 m ) /n ) where m is the number of items in original dataset and n is the initial sample size. The details can be found in [1]. ease is a good sampling algorithm that outperforms srs , but it has some dis-advantages. In this section we analyzed the problems of ease andproposedthe new algorithm easier to avoid these problems. 3.1 Without Halving In ease the halving process has certain granularity. It can only compute a subset approximately half the size of S . If a different sample ratio is wanted other than half the size, we have to run the halving procedure several times with a proper initial random sample set S of data set D . This will consume more time and memory due to multiple halving iterations. In order to directly obtain a sample set of any sample ratio in one pass, the halving round is modified to select red transactions with a probability which is proportional to the desired final sample size. This will remove the need to store several levels of penalties. If we want to obtain a sample set from S with sample ratio r s directly, the ratio of red transactions is f r = r s and the ratio of blue transactions is f b =1  X  r s . Then we have r i = f r  X | S i | and b i = f b  X | S i | .So r i f and r i + b i = | S i | for each item i , our new method will be modified to minimize penalty function Q ( j || r ) i (or Q ( j || b ) i ) in Equation 4 is changed to: are computed with a similar procedure and the results are shown in Equation 8. The overall penalty is calculated as described in section 2.
 to [1] the value of r i 2 f Therefore, the same  X  i , as in section 2, is used in easier . In algorithm 1 the completed easier algorithm is given. The penalty for each item i of a transaction is calculated only once. So it does not need to store the penalty for each halving iteration, and thus results in a reduction of memory from O ( mh )for ease to O ( m ). The time for processing one transaction is bounded by O ( T max )for easier whereas ease requires O ( hT max ) where T max denotes the maximal transaction length in T . Thus, unlike ease , easier is independent of sample size. Algorithm 1. easier Sampling In [1] we showed the application of ease over association rule mining and clas-sical contingency table analysis, and results showed that ease performs better than srs . In this paper we include another important application, image pro-cessing, mainly to show that ease and easier are applicable to continuous data. cameras, we are experiencing a high increase in the amount of multimedia in-formation. The need to analyze and manage the multimedia data efficiently is essential. For applications like machine learning and classification of multime-dia, the training data is important and the use of a good sampling set would influence the final results significantly. easier is a suitable sampling algorithm for such application. This fast and efficient process can select a representative sample set from a large database based on a set of visual features dynamically. 4.1 Color Structure Descriptor The tremendous growth of multimedia content is driving the need for more effi-cient methods for storing, filtering and retrieving audiovisual data. MPEG-7 is a multimedia standard, which improves multimedia management by providing a rich set of standardized descriptors and description schemas for describing mul-timedia content. Based on one or several visual descriptors, users can efficiently browse the image database or retrieve similar images. Color Structure Descriptor ( csd ) is one such descriptor in MPEG-7. It is defined to represent images by both the color distribution of the images (like a color histogram) and the local spatial structure of the color. csd can distinguish the images which have similar histograms and different color spatial distribution. Figure 2 [5] illustrates this using a pair of images. Figure 2(a) is a highly structured color image and Figure 2(b) is a un-structured color image. As the number of foreground color pixels is the same, they cannot be distinguished with the traditional color histograms. But the csd s of these two images are very different because the distributions of color are different. Compared with other color descriptors, csd has the detailed color information and can achieve better retrieval results [6]. The format of csd is identical to a color histogram. It is a 1-D array of eight bit-quantized values and can be 256-, 128-, 64-or 32-bin. In our experiments, 256-bins csd is used. 4.2 Application of EASIER We apply easier to find the representative samples from huge image databases according to csd . Because easier is based on the calculation of the frequency of each item, the format of csd is changed. Each csd descriptor has 256 bins and each bin has an 8-bits numerical value. To handle non-binary data, the numerical value in a bin is converted into an indication on which bit position is set to one in a string of 256 binary bits (allocated for each bin) which has been initialized to zero. After mapping, there will be a total of 256  X  256 = 65536 (one-bit) items. The positions of each non-zero bit in this 65536 items list is subsequently converted into a vector and this vector is usually of a length of 100. of the original csd is re-quantized into a 4-bit quantized representation. The re-quantization is non-uniform and the suggested csd amplitude quantization table in MPEG-7 standard [5] is used. This effectively reduces the number to 16  X  256 = 4096 items for each csd descriptor. A smaller number of bits to represent the data will result in a loss of accuracy in the percentage of each color representation but this action will result in a significant reduction in the number of items for each csd descriptor. Experimental results have shown that the retrieval accuracy of the quantized data is close to the original csd data. In this section we compare the performance of easier , ease and srs in the context of classification and association rule mining. Both real-world data ( csd s of image database) and synthetic data ( ibm quest data) are used for testing. tion time and memory requirement. Sampling time is the time taken to obtain the final samples. Since association rule mining [4, 7] is focused on finding the fre-quent itemsets, i.e., the itemsets satisfying the minimum support, we have used a metric to measure the accuracy of our data reduction methods. In particular: where, as before, L ( D )and L ( S ) denote the frequent itemsets from the database D and the sample S , respectively. We used Apriori 3 [4] to compare the three algorithms in fair manner by computing the frequent item sets.
 is used to test the classification performance of different training sample sets. For the choice of classification algorithm, svm is a good candidate for the image classification applications [8]. The training set of svm is selected by ease , easier and srs . Other remaining images are used for testing the svm . 5.1 Image Processing (CSD) We use COIL-100 4 as the image database. It has 7200 color images of 100 objects: for each object there are 72 images where each image is taken at pose intervals of 5 degrees. So the descriptor database includes 7200 csd descriptors. All three algorithms, easier , ease and srs start from the whole dataset. As the halving process has a certain granularity and ease cannot achieve the specific sample ratio, in each iteration we first ran ease with a given number of halving times. Then we use the actual sample ratio from the ease samples to generate easier samples. As easier is probabilistic, it does not guarantee the exact sample size. Hence, the actual easier sample size is used to generate srs sample. Note that although ease and easier sample sizes are not exactly the same, the difference is very little. For image classification, because ease is based on halving, the final sample ratios of training sets are predetermined to be 0.5, 0.25, 0.125 and 0.0625. For association rule mining, we applied halving one to seven times, hence the sample ratios are 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625 and 0.0078125. each sample ratio we run easier and ease for 50 times and in each iteration the input descriptors are shuffled randomly. srs is also run 50 times over this shuffled data. The association rule mining results of easier , ease and srs are computed as an average over these 50 runs. The minimum support value for Apriori is set to 0.77% and we evaluated only the 1-item frequent set for csd data. Otherwise there are too many frequent itemsets. However, for ibm transaction data, we use all frequent itemsets. The image classification results are average of 10 runs. 3(a) and Figure 3(b) shows the correct rate of image classification and the accu-racy of association rule mining respectively. easier achieves similar accuracy as ease which is better than srs for both classification and association rule mining, especially when the sample ratio is very small. For a sample ratio 0.0625, easier achieves 83.5% classification rate, while ease achieves 82.3% and srs achieves only 59.7%. Figure 3(c) shows the sampling time. EASIER outperforms EASE for all sample ratios smaller than 0.5. It requires an almost fixed amount of time whereas EASE requires more time as the sample ratio falls.
 Accuracy of easier is similar to ease with less running time. For example, for a sample ratio of 0.000785, easier achieves 85.7% accuracy of association rule mining, while the accuracy of ease is 85.4% and srs achieves only 71.5%. The sampling time of easier does not change with the sample ratio. As the number of items is reduced, the running time of easier is reduced and closer to srs . the penalty function increases with the halving times. For example, when we applied seven halvings to csd data, the required memory for storing the penalty of csd items is about 9MB. But for easier , the memory is only about 3MB. 5.2 IBM Transaction Data In order to further compare the performance of the three algorithms, the ibm quest data [1] is used to test the performance of association rule mining. The dataset has total 98,040 transactions and the total number of items is 1000. The average length of these transactions is 10, and the average length of potentially frequent item sets is 4. The minimum support value is set to 0.77%. All three algorithms start from a 20% simple random sample S of the original database. One to five halvings are applied for ease . Thus the final sample ratios is 0.1, 0.05, 0.025, 0.0125 and 0.00625 of the whole dataset. The three algorithms gen-erate samples using the described setting. All three algorithms run 50 times for each sample and the results are the average over these 50 runs. For eas-ier and ease , in each iteration a different random sample is used as the initial sample.
 sample ratio. For ratio 0.00625, the accuracy of easier is about 86.2% while ease has only 71.2% accuracy. The srs gives the worst accuracy of 41.2%. The sampling time of the three methods are very similar as shown in Figure 5(b). In this paper we proposed a new sampling algorithm, easier . The algorithm is similar to its predecessor ease but it reduces the requirements for time and memory. In ease , the sampling time and memory are increased when the sam-ple ratio is reduced. However, in easier the running time is almost fixed and the memory is independent of the sample ratio. Another improvement is, due to its halving nature, in ease we must change the size of initial sample to obtain some specific ratios. But using easier , any sample ratio can be obtained directly from the orginal set. We have evaluated the performance of easier using both real-world and synthetic data. Experiments show that easier is a good approx-imation algorithm which can obtain better sampling results with almost fixed time and even better accuracy than ease .
 features. As easier can flexibly generate representative samples of huge image database, it is used to select the training sets for an svm classifier in image do-main. The performance shows that an easier sample represents the original data much better than a simple random sample. easier is an online algorithm where the incoming transactions are processed once and a decision is taken regarding its participation in the final sample. This scheme is very conducive for stream data processing. The idea is to maintain a sample for the stream data dynam-ically. Just like reservoir sampling [9], each incoming transaction is processed in a given amount of time. But unlike reservoir sampling where this decision is made based solely on probability, easier makes informed decisions. The early idea is to maintain a ranking among the selected transactions in the reservoir sample. When a new streaming transaction arrives, its rank is determined by calculating the change in distance of the reservoir sample from the actual data. If its rank is higher than the lowest rank among the reservoir transactions, it is selected.

