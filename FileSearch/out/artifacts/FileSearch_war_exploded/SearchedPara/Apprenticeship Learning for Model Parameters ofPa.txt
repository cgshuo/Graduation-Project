 Takaki Makino mak@sat.t.u-tokyo.ac.jp Johane Takeuchi johane.takeuchi@jp.honda-ri.com Learning from Demonstration (LfD) is a framework for learning to perform a complex task by observing demonstration (task execution) by an expert ( Argall et al. , 2009 ). LfD is particularly useful for domains where the expert knowledge of the domain is limited or difficult to represent, because demonstrations are much easier than designing a controller for the task. Apprenticeship Learning via Inverse Reinforcement Learning ( Abbeel &amp; Ng , 2004 ), which is an applica-tion of LfD for reinforcement learning, is an algorithm that learns the reward function of the environment un-der the assumption that the expert is trying to maxi-mize the reward. The idea is that, although reinforce-ment learning can produce an optimal policy with re-spect to a given reward function, designing a reward function that captures the desired task behavior is not always obvious and requires expert knowledge of the domain. Moreover, learning the reward function from demonstration requires much less amount of demon-stration compared to learning the policy directly from the demonstration, because the reinforcement learning combines the reward function with the environment model for optimizing the policy for future rewards. Inverse reinforcement learning is successfully applied to tasks where the environment is fully observable, including aerobatic helicopter ight ( Abbeel et al. , 2010 ), robot hand control ( Boularias et al. , 2011 ) and prediction of linguistic structures ( Neu &amp; Szepesvari , 2009 ). Inverse reinforcement learning in partially ob-servable environments when an exact model is avail-able has also been studied ( Ziebart et al. , 2010 ; Henry et al. , 2010 ; Choi &amp; Kim , 2011 ).
 However, the design bottleneck is not limited to the reward function. In many tasks, how to model the en-vironment is not obvious as well, and requires expert knowledge of the domain, especially when the environ-ment is partially observable. For example, dialogue system tasks are often represented as a Partially Ob-servable Markov Decision Process (POMDP) in which the user's mental state is situated as a hidden state ( Williams et al. , 2005 ; Kim et al. , 2008 ; Meguro et al. , 2010 ), but designing such a model requires a consider-able amount of work by domain experts, such as an-notating dialogue corpus. Thus, there is a need for a way to estimate uncertain parameters of an environ-ment model from non-annotated demonstration data. One obvious way to estimate environmental parame-ters from the demonstration is to extract the environ-mental reaction to the expert's action ( Thomson et al. , R O
POMDP
Parameters apprenticeship learning (proposed).
 2010 ). In case of POMDP environment, This reduces the problem into a parameter estimation of an Input-Output Hidden Markov Model (IO-HMM) ( Bengio &amp; Frasconi , 1996 ) (Fig. 1 (a)). However, this approach assumes nothing about the demonstrator, and it is ap-plicable to cases where the demonstration is generated from the learning agent or even from a naive random policy. Our claim is that demonstration by an expert contains much richer information about the environ-ment that comes from the expert's knowledge, and by extracting this information, we can reduce the burden of designing a model suitable for the task.
 Our proposal is to apply the framework of apprentice-ship learning to estimate uncertain parameters of the environment (Fig. 1 (b)) 1 . Assuming that the expert's behavior is based on a stochastic optimal policy with knowledge of the perfect POMDP model for the target environment, we can extract the expert's knowledge regarding the POMDP parameters from his demon-stration. The extracted information of expert knowl-edge can be combined with IO-HMM estimation from the environmental response, to provide a better esti-mate of the POMDP parameters.
 We present two straightforward estimation algorithms, maximum a posteriori (MAP) estimator and poste-rior sampler by Markov chain Monte Carlo (MCMC), combined with planning algorithms to achieve model-parameter apprenticeship learning In the experiments with short demonstrations, we show that our al-gorithms can achieve more accurate estimates of POMDP parameters and better policies than can ex-isting methods based on IO-HMM estimation. An Input-Output Hidden Markov Model (IO-HMM) ( Bengio &amp; Frasconi , 1996 ) is a framework for repre-senting environments consisting of hidden states, in-puts (actions that may affect the states), and outputs (observations from the states). Formally, an IO-HMM is de ned as a tuple  X  S; A; Z; T; O; b 0  X  , where S is the nite set of states, A is the nite set of actions, Z is the nite set of observations, T is the state transition func-tion such that T ( s; a; s  X  ) denotes probability P ( s  X  j of changing to state s  X  by taking action a at state s , O is the observation function such that O ( a; s; z ) denotes probability P ( z j a; s ) of perceiving observation z as a result of taking action a and arriving in state s , and b 0 is the vector of initial state distribution such that b ( s ) denotes the probability of starting in state s . Since the true state is hidden, we construct a belief about the state. We denote belief by a vector b where b ( s ) denotes the probability that the state is s at the current time step. The following update formula can be used to calculate the belief b a z for the next time step from the belief at the current time step, given the action a at the current time step and the observation z at the next time step: A partially observable Markov decision process (POMDP) is a formulation of an action selection prob-lem on an IO-HMM. A POMDP is de ned as a tuple P =  X  S; A; Z; T; O; b 0 ; R;  X  , where S; A; Z; T; O; b 0 de ned as in the IO-HMM, R is the reward function so that R ( s; a ) denotes the immediate reward of taking action a in state s , and 2 [0 ; 1) is the discount factor. The goal of an agent is to maximize the expected dis-counted total reward E [ a policy.
 Since the true state is hidden, a policy of agent action must be de ned over past actions and observations. If a POMDP is speci ed, we can use a belief as a suffi-cient statistic of past actions and observations, where ( b;a ) = P ( a j b ) is a probability of taking action a at belief b . A policy induces a value function V ( b ) that represents the expected discounted total reward of executing policy starting from b . It is known ( Smallwood &amp; Sondik , 1973 ) that V , the value func-tion associated with the optimal greedy policy , can be approximated with an arbitrary accuracy by a con-vex, piecewise-linear function
Q ( b ; a ) = max where ( a ) is a nite set of vectors called -vectors associated to action a , and b is the inner product of a -vector and vector b . We consider soft-max policy for a given set of -vectors: where is the inverse temperature parameter that con-trols the orderedness of the policy. We denote the soft-max policy from the optimal action-value function Q as soft-max optimal policy ~ .
 In general, computing an approximately optimal solu-tion within a given error bound  X  is NP-hard. How-ever, it is known that given a set of balls of radius space, an approximated solution can be computed in a polynomial time ( Hsu et al. , 2008 ). SARSOP ( Kur-niawati et al. , 2008 ) is one of approximated POMDP solvers that implements elaborated point selection and a pruning algorithm.
 In this paper, we consider situations in which some part of the environment model is uncertain. We introduce a K-element parameter vector with its prior distribution p ( ), and consider a POMDP P =  X 
S; A; Z; T ; O ; b 0 ; ; R ;  X  , where T , O , b 0 and R are determined according to the given parameter . An L -length sequence D = ( a 1 z 1 a L z L ) of demonstra-tion by an expert is given, assuming that the expert knows true , the true parameter of the environment, and is following a soft-max optimal policy ~ POMDP P true with inverse temperature . 2 What we want is to calculate p ( j D ), the posterior distribution of the parameter, and to nd an optimal policy over the posterior. Bayes' theorem gives posterior distribution p ( j D ) of parameter given demonstration D = ( a 1 z 1 a L z L ): Likelihood p ( D j ) of the demonstration is the result of marginalizing expert's policy : Note that, from our assumption, the expert's policy POMDP P with parameter , thus p ( = ~ j ) = 1. We can further refactor the likelihood p ( D j ; ) as follows: p ( D j ; ) = p ( a 1 j ; ) p ( z 1 j ; a 1 ) p ( a 2 j The rst factor of Eq. 7 corresponds to the likelihood that the expert performs action a i given the policy : p ( a 1 a L j ; z 1 z L 1 ) = where b i; ;D is the belief at time step i in a POMDP P with history D , calculated by applying Eq. 1 re-peatedly to b 0 ; .
 On the other hand, the second factor corresponds to the likelihood that the environment responds with ob-servation z i to the performed actions: p ( z 1 z L j ; a 1 a L ) = To our knowledge, previous studies that use the ben-e t of the rst factor in the inference of parameter only consider the change of the reward function. In cases where only the rewards are uncertain, the infer-ence is relatively easy since the value function V for a given policy is given as a linear function of the reward values ( Ramachandran &amp; Amir , 2007 ). However, if we consider cases where transition and observation proba-bilities are uncertain, the inference is complex because of the nonlinear dependence between the parameters and the value function. 3.1. Maximum A Posteriori Inference Maximum a posteriori (MAP) inference is to nd that maximizes the posterior (Eq. 4 ). Unfortunately, it is not easy to use sophisticated optimization tech-niques using gradients because changes in beliefs com-plicates obtaining gradients for either factor in Eq. 7 . This is the major difference from the setting of inverse reinforcement learning, in which we can evaluate the gradient of expert action likelihood, and the observa-tion likelihood is constant given D .
 We take a straightforward approach to optimization by using the COBYLA algorithm ( Powell , 1998 ), which does not require gradients. For each candidate param-eter value , we call a POMDP solver for POMDP P to obtain the optimal action-value function Q , which gives the soft-max optimal policy ~ for the POMDP that is used for evaluating expert action likelihood (Eq. 8 ). As for the observation likelihood (Eq. 9 ), we apply the standard forward algorithm for IO-HMM to POMDP P and sequence D .
 This algorithm has no guarantee to nd the MAP pa-rameter because it is based on a local search. However, in practice it seems to nd a reasonably good solution, and calculation is quick compared to the sampling ap-proach which we will describe next. 3.2. Inference by Sampling We also employ a Markov chain Monte Carlo (MCMC) sampling approach ( Gilks et al. , 1996 ) to infer the pos-terior distribution of . Unlike MAP inference, The approximation calculated by MCMC can be arbitrar-ily accurate with a sufficient computational time. The traditional way of sampling parameters for IO-HMM is to use the Markov chain Monte Carlo ap-proach; that is by alternately sampling the hidden state sequence s given parameter , and given s . By using a conjugate prior for the parameters, we can easily sample from the posterior given s .
 To make the sample distribution follow the expert ac-tion likelihood, we introduce the Metropolis algorithm ( Metropolis et al. , 1953 ), which accepts the proposed sample  X  with the probability min(1 ; p  X  =p ), where p  X  and p are the expert action likelihood for the proposed sample and for the previous sample, respectively. Algorithm 1 MCMC sampler for posterior p ( j D ) Require: D : demonstration 1: sample from the prior 2: p := in nitesimal positive value 3: loop 4: sample s = ( s 1 s L ) from p ( s j D; ) 5: for k := 1 to K , in random order do 7: replace  X  k by a sampled value from IO-HMM 8: call POMDP solver to nd ~  X  9: p  X  := p ( a 1 a L j ~  X  ; z 1 z L ) f Eq. 8 g 10: if with probability min(1 ; p  X  =p ) then 11: p := p  X  , :=  X  f accept the sample g 12: end if 13: end for 14: end loop Algorithm 1 shows the sampler for the posterior of model parameters. The algorithm is similar to the sampling algorithm for model parameters from IO-HMM posterior, which deals with only the likelihood of an environmental response (the second term in Eq. 7 ). The difference lies in lines 8{12, that performs Metropolis algorithm for the expert action likelihood (the rst term in Eq. 7 ). We run the algorithm until speci ed number M of samples are collected, excluding burn-ins and interval samples. Our goal is to achieve model-parameter apprenticeship learning; that is, to make an optimal policy for the learned posterior of POMDP model parameters. In this section, we describe how to produce an optimal policy based on the sampled results. Note that, in case of a MAP estimate, we can obtain a policy by applying a solver to POMDP P ^ with estimated parameter ^ . Existing planning methods for POMDP with Bayesian uncertainty ( Ross et al. , 2008 ) are not applicable, be-cause they require that the uncertainty be represented in conjugate priors, which cannot represent the poste-rior distribution of parameters after observing demon-stration. Instead, we employed a method to develop a POMDP policy based on the sampled parameters. The idea is to extend the hidden state of POMDP with a variable m , which is an index of the sampled parameters m ( m = 1 ; : : : ; M ). At the beginning m is uniformly distributed, and never changes. This ex-tended POMDP can be solved by a standard POMDP solver. We expect that the belief over sample index m converges to the index of the most likely parame-ter while the agent interacts with the environment. In case the target POMDP is episodic, we want to retain belief over m beyond episodes, so we convert the target POMDP into non-episodic POMDPs before extension. Formally, given M sampled parameters 1 ; : : : ; M for the target POMDP, we create an extended POMDP ~ P =  X  ~ S; A; Z; ~ T ; ~ O; ~ b 0 ; ~ R;  X  , where ~ b ([ s; m ]) = b 0 ; m ( s ) =M ~ R ([ s; m ] ; a ) = R m ( s; a ) Note that the optimal policy of the extended POMDP becomes a good policy in the target POMDP only if the samples represent the target well. If we need a agent that learns by exploring the uncertainty in the target POMDP, we will need scheduled resampling as has been done in fully observable environments by the BOSS algorithm ( Asmuth et al. , 2009 ). In this pa-per we chose not to resample, because our purpose is to evaluate the posterior distribution p ( j D ) obtained from demonstration. To evaluate the proposed model-parameter apprentice-ship learning algorithms, we performed experiments with two tasks: one is a simple environment based on the well-known Tiger problem ( Kaelbling et al. , 1998 ), and the other is a task designed for a dia-log system. In the following experiments, we used APPL Toolkit which implements the SARSOP algo-rithm ( Kurniawati et al. , 2008 ) as a POMDP solver. We used COBYLA implementation in the NLopt li-brary ( Johnson , 2008 ). 5.1. Bayesian Tiger Problem We introduced four unknown parameters to the Tiger problem, whose prior is represented as p i Beta(3 ; 3), p ; p r Beta(5 ; 3), r t N ( 50 ; 50 2 ) as follows. An agent is standing in front of two doors. A tiger is hidden behind the left door with probability p i and behind the right door with probability 1 p i . The agent can open one of the doors, and obtain reward r if the agent sees the tiger and reward 10 otherwise. Alternatively, the agent can choose to listen with re-ward 1: if the tiger is behind the left door, the agent hears the tiger from the left with probability p l and from the right with probability 1 p l ; if the tiger is behind the right door, the agent hears it from the right with probability p r and from the left with probability 1 p r .
 We set the true environment as p i = 0 : 6, p l = p r = 0 : 85 and r t = 100, and we used = 0 : 9. We gener-ated 100 demonstrations by the experts with soft-max policy = 0 : 3, each consisting of 100 steps of ac-tions and observations (which contained 22 episodes on average). For each demonstration, we applied one of the learning algorithms to the demonstration to es-timate the posterior. From the estimated posterior, an optimal greedy policy was derived, and tested by simulating 100,000 steps in the true environment, and the average reward was measured. For sampling al-gorithms, 1,000 MCMC steps were performed includ-ing 100-step burn-in, and parameters were sampled for every 10 steps (total M = 90 samples) to generate a greedy policy.
 Table 1 shows the distribution of the estimated param-eters. Both of the proposed methods produce more ac-curate estimates of parameters compared to the meth-ods based on IO-HMM 3 . We can see that the proposed methods provide better RMSE than do the IO-HMM methods. The estimates from IO-HMM methods are closer to the prior mean, suggesting that the provided demonstration is too short to obtain an accurate es-timate. On the other hand, the estimates from the proposed methods are closer to the true value, which indicates that the proposed methods provide a bet-ter estimate using the same length of demonstration. We can also see that the proposed sampler produces a narrower posterior distribution (i.e., smaller standard deviation of the samples) than that of the IO-HMM sampler.
 As shown in Figure 2 , having an accurate estimate leads to better results in simulation by the learned policy. The results of the policies based on estimated posterior with our methods are not much worse than those of the expert policy who knows the true param-eter values. On the other hand, policies based on IO-HMM estimation occasionally result in very bad poli-cies, as shown in \Less" average rewards in the g-ure. Considering that the demonstration is short and noisy, these results indicate that the model-parameter apprenticeship learning methods prevent agents from critical failures in learning to follow the demonstrated task. 5.2. Dialog System To show the effectiveness of our methods in a more re-alistic scenario, we developed a new task of dialog man-agement for a ticket-vending system. A user asks the agent for a ticket with a certain origin and destination via an unreliable voice recognition interface; the task of the agent is to repeat the order correctly, and issue the ticket. We expect that the expert demonstration is useful to determine parameters, which represents user's preferred ticket routes and way of talking. The task consists of 13 observations from voice recog-nition, including three place names and SIL (silence). The agent can choose from 11 actions, consisting of uttering one of 9 words, waiting for next word from the user, or issuing a ticket. The dialog is managed by a 32-state POMDP (Fig. 3 ) for each of 3 2 = 6 ticket routes, resulting in the total of 192 hidden states. The POMDP is parameterized with a 15-dimensional vector ; 4 parameters are assigned to route prefer-ences (initial state distribution), 9 to ways of talk-ing (transition probabilities), and 2 to voice recogni-tion errors (observation probabilities). The agents are required to estimate the parameters from a 300-step demonstration generated by an expert. In the exper-iments we didn't use samplers since they require too much computational resources.
 We generated 12 demonstrations by the experts (the result of solving the true POMDP model) with soft-max policy = 0 : 4, each consisting of 300 steps of actions and observations (which contained 27 episodes on average). Using the learned parameters, we applied SARSOP POMDP solver to obtain a greedy policy, and measured average reward by testing the policy on the original environment. Since calculating the exact solution of the POMDP is too expensive, we set the timeout of 40 CPU seconds for each parameter can-didate during MAP search, and 600 CPU seconds for calculating the expert policy and the nal policy based on the estimated parameters; Figure 4 shows the results. We found that the agents based on the parameters estimated by the proposed MAP algorithm perform signi cantly better than the agents based on the parameters estimated by IO-HMM ( P &lt; : 05). However, in this setting, we couldn't ob-tain the expert-level performance by the apprentice-ship learning. One possible reason is that the opti-mization algorithm is disturbed by the approximation error of expert action likelihood, which is caused by the short timeout of the POMDP solver and random searching strategy of SARSOP. We believe that the result can be improved if we use more computational resources; or, if we use a POMDP solver that can be started from the result of a similar POMDP, we may be able to improve the optimization process. We have shown that the apprenticeship learning ap-proach can be used to estimate parameters of an un-known POMDP environment. Assuming that an ex-pert knowing the perfect POMDP model of the tar-get environment will try to maximize the reward, we can extract the expert's knowledge about the environ-ment from his demonstration in terms of the posterior distribution of unknown parameters. Our proposed algorithms are simple but are capable of estimating POMDP parameters accurately even if the demon-stration is short. We also showed that the extracted knowledge can be used to develop a policy that can act reasonably well in the target environment. Our approach is a generalization of inverse reinforce-ment learning, in a sense that the unknown parameters are not limited to those for the reward function but can also be for transition and observation functions. This approach can be particularly useful in the do-main of applications that interact with human beings, whose model is unknown but demonstration by experts is available. One direct extension of the approach is to estimate other parameters, such as the discount factor of the expert, from the demonstration. Future work should also include the development of more efficient algorithms as has been done in the context of inverse reinforcement learning.
 This research is supported by the Aihara Innova-tive Mathematical Modelling Project, the Japan So-ciety for the Promotion of Science (JSPS) through the \Funding Program for World-Leading Innovative R&amp;D on Science and Technology (FIRST Program)," initi-ated by the Council for Science and Technology Policy (CSTP), and by JSPS Grant-in-Aid for Young Scien-tists (B) (20700126).
 Abbeel, P. and Ng, A. Apprenticeship learning via inverse reinforcement learning. In Greiner, R. and Schuurmans, D. (eds.), Proc. of 21st International Conference on Machine Learning (ICML 2004) . ACM Press, 2004.
 Abbeel, P., Coates, A., and Ng, A. Y. Autonomous helicopter aerobatics through apprenticeship learn-ing. International Journal of Robotics Research , 29 (13):1608{1639, 2010.
 Argall, B. D., Chernova, S., Veloso, M., and Browning, B. A survey of robot learning from demonstration.
Robotics and Autonomous Systems , 57(5):469{483, 2009.
 Asmuth, J., Li, L., Littman, M., Nouri, A., and
Wingate, D. A bayesian sampling approach to ex-ploration in reinforcement learning. In Proc. of the 25th Annual Conference on Uncertainty in Arti cial Intelligence (UAI'09) , pp. 19{26. AUAI Press, 2009. Bengio, Y. and Frasconi, P. Input-output HMM's for sequence processing. IEEE Transactions on Neural Networks , 7(5):1231{1249, 1996.
 Boularias, A., Kober, J., and Peters, J. Relative en-tropy inverse reinforcement learning. Journal of Ma-chine Learning Research: Workshop and Conference Proceedings (AISTATS 2011) , 15:182{189, 2011. Choi, J. and Kim, K.-E. Inverse reinforcement learn-ing in partially observable environments. Journal of Machine Learning Research , 12:691{730, 2011. Gilks, W., Richardson, S., and Spiegelhalter, D.
Markov Chain Monte Carlo in Practice . Chap-man&amp;Hall/CRC, 1996.
 Henry, P., Vollmer, C., Ferris, B., and Fox, D. Learn-ing to navigate through crowded environments. In Proc. of 2010 IEEE International Conference of
Robotics and Automation (ICRA 2010) , pp. 981{ 986, 2010.
 Hsu, D., Lee, W. S., and Rong, N. What makes some POMDP problems easy to approximate? In Platt, J., Koller, D., Singer, Y., and Roweis, S. (eds.),
Advances in Neural Information Processing Systems 20 , pp. 689{696. MIT Press, Cambridge, MA, 2008. Johnson, S. G. The NLopt nonlinear-optimization package. http://ab-initio.mit.edu/nlopt, 2008. Kaelbling, L. P., Littman, M. L., and Cassandra,
A. R. Planning and acting in partially observable stochastic domains. Arti cial Intelligence , 101:99{ 134, 1998.
 Kim, K., Lee, C., Jung, S., and Lee, G. G. A frame-based probabilistic framework for spoken di-alog management using dialog examples. In Proc. of the 9th SIGdial Workshop on Discourse and Di-alogue , pp. 120{127, 2008.
 Kurniawati, H., Hsu, D., and Lee, W. S. SARSOP:
Efficient point-based POMDP planning by approx-imating optimally reachable belief spaces. In Proc. Robotics: Science and Systems , 2008.
 Meguro, T., Higashinaka, R., Minami, Y., and Doh-saka, K. Controlling Listening-oriented Dialogue using Partially Observable Markov Decision Pro-cesses. In Proc. of the 23rd International Conference on Computational Linguistics (COLING 2010) , pp. 761{769. ACL, 2010.
 Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
Teller, A. H., and Teller, E. Equations of state cal-culations by fast computing machines. Journal of Chemical Physics , 21:1087{1092, 1953.
 Neu, G. and Szepesvari, C. Training parsers by inverse reinforcement learning. Machine Learning , 77(2-3): 303{337, 2009.
 Powell, M. J. D. Direct search algorithms for opti-mization calculations. Acta Numerica , 7:287{336, 1998.
 Ramachandran, D. and Amir, E. Bayesian inverse re-inforcement learning. In Proc. of International Joint
Conference of Arti cal Intelligence (IJCAI-2007) , pp. 2586{2591, 2007.
 Ross, S., Chaib-draa, B., and Pineau, J. Bayes-adaptive POMDPs. In Platt, J. C., Koller, D., Singer, Y., and Roweis, S. T. (eds.), Advances in Neural Information Processing Systems 20 , 2008. Smallwood, R. and Sondik, E. The optimal control of partially observable Markov processes over a nite horizon,. Operations Research , 21:1071{1088, 1973. Thomson, B., Jurccek, F., Gasic, M., Keizer, S.,
Mairesse, F., Yu, K., and Young, S. Parameter learning for POMDP spoken dialogue models. In
Proc. of the 3rd IEEE Workshop on Spoken Lan-guage Technology (SLT 2010) , pp. 271{276. IEEE, 2010.
 Williams, J. D., Poupart, P., and Young, S. Partially observable Markov decision processes with continu-ous observations for dialogue management. In Proc. of the 6th SIGdial Workshop on Discourse and Di-alogue , pp. 25{34. 2005.
 Ziebart, B., Bragnell, J. A., and Dey, A. K. Model-ing interaction via the principle of maximum causal entropy. In F X urnkranz, J. and Joachims, T. (eds.),
Proc. of the 27th International Conference on Ma-chine Learning (ICML 2010) , pp. 1255{1262. Om-
