 We describe a novel simple and highly scalable semi-supervised method called Word-Class Distribution Learning (WCDL), and ap-ply it the task of information extraction (IE) by utilizing unlabeled sentences to improve supervised classification methods. WCDL iteratively builds class label distributions for each word in the dic-tionary by averaging predicted labels over all cases in the unla-beled corpus, and re-training a base classifier adding these distribu-tions as word features . In contrast, traditional self-training or co-training methods add self-labeled examples (rather than features) which can degrade performance due to incestuous learning bias. WCDL exhibits robust behavior, and has no difficult parameters to tune. We applied our method on German and English name en-tity recognition (NER) tasks. WC DL shows improvements over self-training, multi-task semi-supervision or supervision alone, in particular yielding a state-of-the art 75.72 F1 score on the German NER task.
 Categories and Subject Descriptors: I.2.7 [Artificial Intelligence]: Natural Language Processing -text analysis ; M.4 [Knowledge Man-agement]: Knowledge modeling General Terms: Algorithms Keywords: Semi-Supervised Learning, Semi-Supervised Feature Learning, Name Entity Recogniti on, Information Extraction
Words are a fundamental building block in language and fea-tures based on words are a fundamental building block of natural language processing (NLP) systems. Indeed, many tasks such as named entity recognition (NER), par t-of-speech (POS) tagging and chunking involve sequence modeling with word-level evaluation. For other tasks, such as document classification or sentiment ex-traction, that are evaluated at the document-level individual words still carry significant label information.

Supervised techniques using such features have yielded great success in the NLP community, but are restricted by the expense of annotating data. Popular semi-supervised methods such as self-training [18, 14, 12, 13] or co-training [2, 4] that utilize large un-labeled corpora try to improve over supervised methods by itera-tively adding self-labeled examples predicted by the current model. However, they are vulnerable to the incestuous training bias prob-lem [19, 21], i.e. examples may be consistently mislabeled making the model even worse on the next iteration. To combat this sev-eral authors have proposed schemes for only adding examples that meet a selection criterion [13, 19, 6], but these heuristic choices still might yield unreliable results.

In this paper we propose a novel semi-supervised strategy that works by providing semi-supervision at the level of words rather than examples . Under the assumption that words carry label infor-mation we measure the class label distribution for each word on a large unlabeled corpus. These features are then used to retrain the model in an iterative fashion. As noisy self-labeled examples are not added (as in self-training), our model exhibits robust behavior, and moreover has no difficult parameters (e.g. selection criteria) to tune.
 We applied this strategy on two CoNLL-2003 [8] shared tasks, German NER and English NER. Using a state-of-the-art neural network model [5] in various setups, we observed improvements from using our method, called Word-Class Distribution Learning (WCDL), compared to the baseline classifier and to self-training, whenever we applied it. In particular we achieved a state-of-the art result of 75.72 F1 on the German NER task.
Unlike most popular semi-supervised approaches (details in Sec-tion 3), we propose to induce features from a large corpus of unan-notated examples in a supervised fashion, and then use these fea-tures to augment the feature space of the labeled set.
We consider the setting where one is given labeled training ex-amples { ( x i , y i ) } i =1 ,...,L  X  X  X Y and an unlabeled set of ex-amples { x  X  i } i =1 ,...,U  X  X  where U L . In particular set of all sequences composed of elements which take on a finite set of possible values, e.g. sequences of words (but in the general case this could include other discrete types of feature as well, e.g. POS tags, stem-ends, etc.). That is, we will assume an input se-quence x =( x 1 ,...,x | x | ) ,where x j  X  X  , a dictionary of size |D| . The labels Y X  X  1 ,...,K } are the K classes a sequence can be assigned to.

We define the word-class distribution for a given word w  X  X  a vector wcd ( w )  X  R K where That is, the i th dimension measures the probability of label being assigned given that word w is present in the input sequence This distribution is of course unknown but can be estimated from the training set or, critically, can be re-estimated using unlabeled data by applying a trained classifier. We thus define the empirical word-class distribution as: where f (  X  ) is a classifier trained to predict y  X  X  given
We hence propose the following iterative semi-supervised train-ing algorithm: 1. Define the feature representation  X  ( w ) for a word w 2. Train a classifier f (  X  ) on training examples ( x i , y 3. Augment the representation of words with their word-class 4. Iterate steps 2 and 3.
In this work we consider sequence labeling tasks where inputs are windows of a fixed size, and the middle word in the window is the word to be tagged. In this case one may want to consider the modified word class distribution where we are interested in class distributions only for the words to be labeled: where we only count matches to word w with the middle word with index m =( | x  X  j | X  1) / 2+1 . However, we still augment all words in the window with wcd (  X  ) features to capture local patterns.
Like self-training and co-training our algorithm (i) iteratively tries to improve its model; and (ii) is a wrapper approach that can use a supervised (or semi-supervised) classifier as a  X  X ase learner X . However, our algorithm also has the following benefits:
We have already mentioned self-training [18] (also called  X  X oot-strapping X  in the traditional NLP field) and co-training [2]. These methods augment the training set with labeled examples from the unlabeled set which are predicted by the model itself. This may give improvements in a model, but care must be taken as the pre-dictions are prone to noise.

Many other semi-supervised learning algorithms exist, including tranductive SVMs [11], graph-based regularization [22], entropy regularization [10] and EM with generative mixture models [16], see [3] for a review. Apart from self-training and co-training, many other semi-supervised methods ha ve scalability problems for real-istic language modeling tasks, which normally involve hundreds of thousands of labeled examples.

Beyond the above approaches of semi-supervised learning with small amounts of labeled data and larger sets of unlabeled data, there has been a growing interest in the use of human-provided as-sociations of features to particular classes for augmenting standard supervised learning. Most of this type of work has focused on using prior class-bias based features (called "labeled features") to gener-ate labeled pseudo-examples or make feature selections [17, 20]. Further, the authors of [7] introduced a generalization expectation criterion to softly constrain the model X  X  predictions on unlabeled examples with labeled features directly.

Finally, there are some methods that use auxiliary tasks on a large unlabeled corpus for training sequence models (often through multi-task learning). Ando et al . [1] proposed a method based on defining multiple tasks using unlabeled data that are multi-tasked with the task of interest, which they showed to perform very well on several natural language tagging tasks. Similarly, Collobert et al. [5] proposed a related method for deep neural networks where each word in the dictionary is represented by a vector (a represen-tation which is shared between the multiple tasks). They multi-task with an unsupervised language model (LM), predicting the missing word in the middle of a text window, again resulting in good per-formance. In this work we follow the setup of [5] and measure the performance of WCDL as a wrapper on their approach.
We test our approach on the English and German NER datasets provided by the CoNLL-2003 shared task [8]. NER systems label atomic elements in the sentence into categories such as  X  X ERSON X ,  X  X OMPANY X , or  X  X OCATION X , an important sub-task of informa-tion extraction (reviewed in [15]). For each language, a training set, a development set (for parameter tuning), a test set are provided. For both languages, more than 200,000 training tokens exist in the provided training sets (Table 1). The large unannotated ECI data file provided by CoNLL-2003 is used as our unlabeled corpus for the German NER. A sampled set of English Wikipedia web pages is used for WCDL on the English NER (size listed in Table 1). Table 1: Number of (labeled) and unlabeled tokens used in our experiments in two CoNLL-2003 [8]) NER tasks .
 As a  X  X ase classifier X  for the tagging task, we use the unified Neural Network (NN) framework of [5] where the input sentence is processed by several layers of feature extractions. The first layer maps words to 50-dimensional vectors (one vector for each word in the dictionary), the parameters of which are automatically trained during the learning process using backpropagation. The second layer is a classical layer of H hidden units (where H is optimized on the development set), and the final layer outputs probabilities of the class labels. In [5], the authors described its application to sev-eral well known NLP tasks including POS tagging and semantic role labeling, but do not report results for NER. They report us-ing multi-taskin g with an unsupervised task of learning a language model (LM) yields good results for other sequence labeling tasks. We hence tried using the LM as well.

We train our NER labeling system using a sliding window, op-tionally followed by a viterbi decoding of the entire sentence given the class probabilities from the NN predictions. This viterbi decod-ing could capture the local dependencies between targeted NER classes, which improves the NN performance effectively. The pro-posed WCDL (under sequence labeling) functions similarly as the viterbi decoding, since the learned class-distributions of surround-ing words should obey local dependencies as well. Hence we com-pare WCDL with the viterbi step.

In all cases, it is straightforward to use WCDL. For all words in the text window centered at the target word, WCDL input features are concatenated along with the other word feature vectors.
Our baseline model uses the following word features:
We compare WCDL over multiple baselines, including NER by supervision alone, supervision with viterbi decoding, semi-supervision with LM, and with self-training.

Comparison with Supervision &amp; Semi-supervision: Table 2 lists the test set performance on the German NER task using the F1 measure when applying WCDL as a wrapper to various systems: using only word features (with and without a viterbi decoding step), and using all features plus the language model (LM) based semi-supervised learning. In all cases WCDL improves over the base-line. Our best performance of 75.72 (using all features + WCDL) beats the state-of-the-art German NER performance of 75.27 which was reported in [1]. The best result during the CoNLL-2003 com-petition was 74.17 [9].

We also considered taking our best model, and adding the WCDL features predicted by it to a basic word-features only model. This Table 2: F1 score on the test set for German NER. For each choice of baseline (left column) applying word-class distribu-tion learning (WCDL) improves over it (right column). LM means using language model semi-supervision.
 improved its accuracy from 50.61 to 64.1. Using the LM as well yields 72.45 (words+LM on their own are 69.05). This is interest-ing because these results do not require POS, chunk, stem or caps features any more, but are close to the state-of-the-art. Table 3: F1 score on the test set for English NER. WCDL im-proves over each baseline.

Table 3 provides results for the English NER task. Again, WCDL improves over all baselines; our best result was 88.69. In contrast, the best performing method during the competition was 88.76, and [1] have since reported 89.31 using multi-task semi-supervision. Here, our slightly worse performance seems to be due to our weaker baseline method (before even applying WCDL) compared to these approaches.

Comparison with Self-training We applied self-training to the same baseline methods to compare the performance of WCDL. There are numerous variants of self-training. We adopt the fol-lowing weighting scheme: given L training examples, we choose L/R ( R is a parameter to choose) unlabeled examples to add in the next round X  X  training. By varying R , we get a range of impacts from self-training.
 Table 4 and Table 5 give the results of the English and German NER. Self-training only helped marginally, or not at all, depending on the parameters.

The above comparison indicates that WCDL has better behavior than self-training with a random selection strategy. Since there ex-ist many selection strategies for self-training, other selection tech-niques might bring improvements, see e.g. [6, 19] for other strate-gies. Still, these heuristic choices are difficult and need careful tun-ing [19]. In contrast, the proposed WCDL method does not seem to suffer from these issues.

Further, the perform ance in multiple rounds of self-training might oscillate because of degradation by noisy labels (see e.g. [19, 21]). We observed that WCDL X  X  iterative training gives stable results. Figure 1 shows the test F1 from the iterations (as a wrapper for the  X  X ll features + LM + Viterbi X  baseline) for the German NER set. It appears to converge in only a few iterations.
In this work we proposed a novel semi-supervised algorithm called word class-distribution learning and applied it to the task of sequence labeling. Our method is highly scalable, contains no difficult parameters to tune, and we found it to be empirically ro-bust, improving over every supervised and semi-supervised base-line method we applied it to.

The proposed method can easily be extended to other cases or domains. For example, instead of calculating predicted class dis-tributions for each word, we could consider n -gram distributions instead. Moreover, one can generalize beyond word-level evalua-tion tasks. For instance in text categorization problems (document classification or sentiment analysis) a word X  X  class distribution is the distribution of labels of documents that contained that word. [1] R. K. Ando and T. Zhang. A framework for learning [2] A. Blum and T. Mitchell. Combining labeled and unlabeled [3] O. Chapelle, B. Sch X lkopf, and A. Zien, editors.
 [4] M. Collins and Y. Singer. Unsupervised models for named [5] R. Collobert and J. Weston. A unified architecture for nlp: [6] H. Daum X  III. Cross-task knowledge-constrained self [7] G. Druck, G. Mann, and A. McCallum. Learning from [8] Erik and F. De Meulder. Introduction to the conll-2003 [9] R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. Named [10] Y. Grandvalet and Y. Bengio. Semi-supervised learning by [11] T. Joachims. Transductive inference for text classification [12] R. J. Kate and R. J. Mooney. Semi-supervised learning for [13] Z. Kozareva, B. Bonev, and A. Montoyo. Self-training and [14] D. McClosky, E. Charniak, and M. Johnson. Effective [15] D. Nadeau and S. Sekine. A survey of named entity [16] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text [17] R. E. Schapire, M. Rochery, M. G. Rahim, and N. Gupta. [18] H. Scudder. Pr obability of error of some adaptive [19] H. Shan and D. Gildea. Self-training and co-training for [20] X. Wu and R. Srihari. Incorporating prior knowledge with [21] T. Zhang, F. Damerau, and D. Johnson. Text chunking using [22] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised
