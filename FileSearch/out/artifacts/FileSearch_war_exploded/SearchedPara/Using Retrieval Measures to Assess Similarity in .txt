 While scalable data mining methods are expected to cope with mas-sive Web data, coping with evolving trends in noisy data in a c on-tinuous fashion, and without any unnecessary stoppages and recon-figurations is still an open challenge. This dynamic and sing le pass setting can be cast within the framework of mining evolving d ata streams. In this paper, we explore the task of mining mass use r profiles by discovering evolving Web session clusters in a si ngle pass with a recently proposed scalable immune based cluster ing approach (TECNO-STREAMS), and study the effect of the choic e of different similarity measures on the mining process and o n the interpretation of the mined patterns. We propose a simple si milarity measure that has the advantage of explicitly coupling the pr ecision and coverage criteria to the early learning stages, and furt hermore requiring that the affinity of the data to the learned profiles or sum-maries be defined by the minimum of their coverage or precisio n, hence requiring that the learned profiles are simultaneousl y precise and complete, with no compromises. In our expriments, we stu dy the task of mining evolving user profiles from Web clickstrea m data (web usage mining) in a single pass, and under different tren d se-quencing scenarios, showing that compared oto the cosine si milar-ity measure, the proposed similarity measure explicitly ba sed on precision and coverage allows the discovery of more correct pro-files at the same precision or recall quality levels.
 Categories &amp; Subject Descriptors: H.2.8 [ Information Systems ]: Database Management: Database Applications data mining.
 General Terms: Algorithms, Performance, Design.
 artificial immune systems, web mining, personalization, cl ustering, stream data mining, mining evolving data Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
The proliferation of massive data sets has recently put even higher demands on clustering algorithms. They now must handle very large data sets, leading to some scalable clustering techni ques. How-ever, most scalable clustering techniques such as BIRCH [24 ] and the scalable K-Means (SKM) [4] assume that clusters are clea n of noise, hyper-spherical, similar in size, and span the whole data space. Robust clustering techniques have recently been proposed to handle noisy data. Another limitation of most clustering algo-rithms is that they assume that the number of clusters is know n. However, in practice, the number of clusters may not be known . This problem is called unsupervised clustering . A recent explosion of applications generating and analyzing data streams has added new unprecedented challenges for clustering algorithms if they are to be able to track changing clusters in noisy data streams us ing only the new data points because storing past data is not even an option [2, 1, 5, 7].

Web usage mining [21, 23, 17, 6, 18, 3, 16, 19, 15, 14, 22] has re -cently attracted attention as a viable framework for extrac ting use-ful access pattern information, such as user profiles, from m assive amounts of Web log data for the purpose of Web site personaliz a-tion and organization. Most efforts have relied mainly on cl ustering or association rule discovery as the enabling data mining te chnolo-gies. For example, user sessions (or clicks) can be extracte d from Web log files and then submitted, offline, to clustering to dis cover typical user trends or profiles in the Web clickstream data. A pro-file can consist of a set of URLs that are relevant to the sessio ns assigned to a given cluster [16]. These mass profiles are diff erent from other explicit profiles that require user input of perso nal infor-mation, such as demographic profiles, because they are based en-tirely on a group of similar users X  behavior or access histor y which is considered as an implicit way of rating the visited web pag es. Once these profiles are discovered, they can be exploited as p art of an automated personalization on the website, by treating them as summarized user models against which all future user sess ions are compared. In this user-to-user based recommendation st rategy, also known as collaborative filtering , the recommendation engine suggests URLs that are deemed to be relevant to a new user X  X  in ter-ests by first comparing this user session to the pre-discover ed user profiles, and recommending the URLs that are relevant to the c los-est profile. Since the profiles constitute an essential part ( the user model) of the personalization strategy, it is essential tha t the system always has access to the most current/up to date profiles. For this purpose, typically, data mining has to be completely re-app lied pe-riodically and offline on newly generated Web server logs in o rder to keep the discovered profiles up to date. This periodical up date framework, which is common in most profile based recomemnder systems, can make it hard to synchronize the profile discover y and the ultimate recommendations, and raises several open chal lenges, such as when is the optimal time to update the profiles? and how long does it take to discover these profiles?. If it takes too l ong to discover the profiles, then by the time they are ready to be u sed for recommendations, some rapidly changing profiles may hav e al-ready become obsolete, while some newly emerging profiles ma y be completely missed. Hence, it is important to be able to dis cover perpetually current user profiles in a continuous and scalab le man-ner, that does not impose any ungraceful stoppages or delays on the recommendation system. These requirements call for a pr ofile discovery method that gleams inspiration from stream data m in-ing, where the input data is unleashed in massive quantities that make it imperative to process the data in sequential forward order, or in a single pass. Furthermore, it is desired that the strea m min-ing framework allows the notion of adaptation to the dynamic s of profile emergence and extinction, by being able to detect eme rging trends and to forget old trends. This in turn requires an Evolving Stream Mining framework for mining Web clickstreams. Finally to add even more open challenges to an already difficult problem , the stream mining framework should have provisions to resist ou tliers or noise in the input data, as is typical in most Web usage data , and to automatically discover a resonable (and unknown) num ber of user trends, since it is obviously impossible to know the n um-ber of user trends or clusters in advance, when these are cons tantly morphing as the stream data mining unfolds.

In [11], we proposed a new immune system inspired approach for clustering noisy multi-dimensional stream data, calle d TECNO-STREAMS ( T racking E volving C lusters in NO isy S treams), that has the advantages of scalability, robustness, and automatic scale estimation . TECNO-STREAMS is a scalable clustering methodol-ogy that gleams inspiration from the natural immune system t o be able to continuously learn and adapt to new incoming pattern s by detecting an unknown number of clusters in evolving noisy da ta in a single pass, in the same way that the immune system learns ho w to recognize and respond to external antigens such as bacter ia and viruses.

In this paper, we study the possibility of mining evolving us er profiles from Web clickstream data (web usage mining) in a sin -gle pass, and under different usage trend sequencing scenar ios us-ing TECNO-STREAMS. while studying the effect of the choice o f different similarity measures on the mining process and on t he in-terpretation of the mined patterns. We propose a simple simi larity measure that has the advantage of explicitly coupling the pr ecision and coverage criteria to the early learning stages, and furt hermore requiring that the affinity of the data to the learned profiles or sum-maries be defined by the minimum of their coverage or precisio n, hence requiring that the learned profiles are simultaneousl y precise and complete, with no compromises.

The rest of the paper is organized as follows. In Section 2, we de-scribe the TECNO-STREAMS algorithm. and compare it to some existing scalable clustering algorithms. In Section 3, we d escribe how we can use TECNO-STREAMS to track evolving clusters in Web usage data, and illustrate using it for mining real Web cl ick-stream data, while studying the effect of the choice of diffe rent similarity measures on mining and interpreting the evolvin g pro-files. Finally, in Section 4, we present our conclusions.
The immune system (lymphocyte elements) can behave as an al-ternative biological model of intelligent machines, in con trast to the conventional model of the neural system (neurons). In pa rticu-lar, the Artificial Immune Network (AIN) model is based on Jer ne X  X  Immune Network theory [9]. The system consists of a network o f B cell lymphocytes that summarize the learned model. The immu ne network consists of a set, X B , of artificial B-cells, as well as stimu-lating and suppressing links between them. Learning takes a s input a set of input data point training data, X a , and tries to learn an opti-mal immune network consisting of linked B-Cells based on clo ning operations as in nature. Each B-Cell represents a learned pa ttern that could be matched to or validated by an input data point/d ata item or another B-Cell in the network. A link between two B-Ce lls gets stronger if they are more similar. Data from the input da ta point training set is matched against a B-Cell based on a prop erly chosen similarity measure.

Here, we summarize the TECNO-STREAMS approach omitting some of the details and proofs that can be found in [11]. In a dy -namic environment, the objects from a data stream X sented to the immune network one at a time, with the stimulati on and scale measures re-updated with each presentation. It is more convenient to think of the input data point index, j , as monotoni-cally increasing with time. That is, the input data points ar e pre-sented in the following chronological order: x 1 , x 2 ,  X  X  X  , x Dynamic Weighted B-Cell ( D-W-B-cell ) represents an influence zone over the domain of discourse consisting of the training data set. However, since data is dynamic in nature, and has a tempo ral aspect, data that is more current will have higher influence c om-pared to data that is less current. Quantitatively, the influ ence zone is defined in terms of a weight function that decreases not onl y with distance from the input data point to the D-W-B-cell prototy pe, but also with the time since the input data point has been present ed to the immune network. It is convenient to think of time as an add i-tional dimension that is added to the D-W-B-Cell compared to the classical B-Cell, traditionally statically defined in inpu t data point space only [12].
 Definition 1: (Robust Weight/Activation Function) For the D-W-B-cell, DW B i , i = 1 ,  X  X  X  , N B , we define the robust weight/ sactivation function caused by the j th input data point, after data points have been presented, as where  X  controls the time decay rate of the contribution from old input data points, and hence how much emphasis is placed o n the currency of the immune network compared to the sequence o f input data points encountered so far. d 2 put data point x j (which is the j th input data point encountered by the immune network) to D-W-B-cell, DW B i .  X  2 meter that controls the decay rate of the weights along the sp atial dimensions, and hence defines the size of an influence zone aro und a cluster prototype. Data samples falling far from this zone are con-sidered outliers. The weight functions decrease exponenti ally with the order of presentation of an input data point, j , and therefore, will favor more current data in the learning process.
 Definition 2: (Influence Zone) The i th D-W-B-cell represents a soft influence zone, IZ i , that can be interpreted as a robust zone of influence, consisting of all the data points that succeed in a cticating this cell.

Each D-W-B-cell is allowed to have is own zone of influence with radial size proportional to  X  2 Hence, outliers are easily detected as data falling outside the in-fluence zone of all D-W-B-cells or through their weak activat ions ( w ij &lt; w min ,  X  i ).
 Definition 3: (Pure Stimulation) The pure stimulation level, after J input data points have been presented to DWB i , is defined as the density of the input data point population around DWB i : Lemma 1: (Optimal Scale Update) The equations for optimal scale updates [11] are given by
For the purpose of computational efficiency, however, we con vert the above equations to incremental counterparts as follows . Lemma 2: (Incremental Update of Pure Stimulation and Opti-mal Scale) After J input data points have been presented to pure stimulation and optimal scale can be updated using the f ollow-ing approximate incremental equations, respectively, where W i,J previous input data points, x 1 , x 2 ,  X  X  X  , x J
We propose incorporating a dynamic stimulation factor,  X  ( t ) the computation of the D-W-B-cell stimulation level by addi ng a compensation term that depends on other D-W-B-cells in the n et-work [8, 20]. In other words, a group of co-stimulated D-W-B-cells can self-sustain themselves in the immune network, even aft er the input data point that caused their creation disappears from the envi-ronment. However, we need to put a limit on the time span of thi s memory to forget truly outdated patterns. This is done by all owing D-W-B-cells to have their own stimulation coefficient, and t o have this stimulation coefficient decrease with their age:  X  ( t ) = We also incorporate a dynamic suppression factor,  X  ( t ) = to control the proliferation and redundancy of the D-W-B-ce ll pop-ulation.

The number of possible internal interactions (between diff erent cells in the network) can be a serious bottleneck in the face o f all ex-isting immune network based learning techniques [8, 20]. Su ppose that the immune network is compressed by clustering the D-W-B-cells using a linear complexity approach such as K Means. The n the immune network can be divided into K subnetworks that form a parsimonious view of the entire network. For global low res o-lution interactions, such as the ones between D-W-B-cells t hat are very different, only the inter-subnetwork interactions are germane. For higher resolution interactions such as the ones between similar D-W-B-cells, we can drill down inside the corresponding sub net-work and afford to consider all the intra-subnetwork interactions . The proposed AIS based clustering model can achieve scalabi lity at a finite compression rate ( K  X   X  N B ). Instead of taking into account all possible ( N B ) 2 interactions between all the immune network, only the intra-subnetwork interaction s with the N i network to which this B cell is assigned) are taken into accou nt. In case K-Means is used, this representative as well as the or gani-zation of the network into subnetworks is a by-product. For m ore complex data structures, a reasonable best representative /prototype (such as a medoid) can be chosen. Taking these modifications i nto account, the stimulation and scale values that take advanta ge of the compressed network are given by where s a i,J is the pure input data point stimulation after encoun-tering J input data points, given by (5 ) for D-W-B-cell i is the number of B-cells in the subnetwork that is closest to t he DWB-cell. This will modify the D-W-B-cell scale update equa tions to become where D 2 w
Recently, data mining techniques have been applied to extra ct usage patterns from Web log data [21, 23, 17, 6, 18, 16, 19, 3, 15, 14, 22]. In the context of this paper, a usage pattern is a pro-file that summarizes the characteristics of a group/cluster of similar user sessions, and it consists of a set of URLs that are releva nt to thi profile.. In [16, 15], we have proposed new robust and fuzz y relational clustering techniques that allow Web usage clus ters to overlap, and that can detect and handle outliers in the data s et. A new subjective similarity measure between two Web sessions , that captures the organization of a Web site, was also presented a s well as a new mathematical model for  X  X obust X  Web user profiles [16 ] and quantitative evaluation means for their validation. Un fortu-nately, the computation of a huge relation matrix added a hea vy computational and storage burden to the clustering process . In [14], we presented a quasi-linear complexity technique, called Hierarchical Unsupervised Niche Clustering (H-UNC), for m ining both user profile clusters and URL associations in a single step. More recently, we have presented a new approach to mining use r profiles that is inspired by concepts from the natural immune sys-tem [12]. This approach proved to be successful in mining clu sters and frequent itemsets from large web session data. This kind of data, which is extremely sparse, presents a real challenge t o con-ventional clustering and frequent itemset mining techniqu es. Many data sets share this sparsity with clickstream data: these i nclude text data as well as a large number of transactional databases. Un fortu-nately, all the above methods assume that the entire preproc essed Web session data could reside in main memory. This can be a dis -advantage for systems with limited main memory in case of hug e web session data, since the I/O operations would have to be ex -tensive to shuffle chunks of data in and out, and thus compromi se scalability. Today X  X  web sites are a source of an exploding a mount of clickstream data that can put the scalability of any data m ining technique into question.

Moreover, the Web access patterns on a web site are very dy-namic in nature, due not only to the dynamics of Web site conte nt and structure, but also to changes in the user X  X  interests, a nd thus their navigation patterns. The access patterns can be obser ved to change depending on the time of day, day of week, and accordin g to seasonal patterns or other external events in the world. A s an al-ternative to locking the state of the Web access patterns in a frozen state depending on when the Web log data was collected and pre -processed, we propose an approach that considers the Web usa ge data as a reflection of a dynamic environment which therefore re-quires dynamic learning of the access patterns. An intellig ent Web usage mining system should be able to continuously learn in t he presence of such conditions without ungraceful stoppages, recon-figurations, or restarting from scratch. In this section, we illustrate using TECNO-STREAMS to continuously and dynamically learn evolving Web access patterns from non-stationary Web usage envi-ronments.
For many data mining applications such as clustering text docu-ments and other high dimensional data sets, the Euclidean distance measure is not appropriate. This is due mainly to the high dim en-sionality of the problem, and the fact that two documents may not be considered similar if keywords are missing in both docume nts. More appropriate for this application, is the cosine simila rity mea-sure between data item x i and a learned B-Cell profile p in the simplest case, can both be defined as binary vectors of l ength n , the total number of items/URLs or keywords, [10],
We note that it is easy to show that the cosine similarity is re lated to the well known information retrieval measures of precisi on and coverage as follows: where the precision in the learning phase, P rec L ij describes the accuracy of the learned B-cell profiles p j in representing the data x , or the ratio of the number of matching items (URLs or terms) between the learned profile and the data (session or document ) to the number of items in the learned profile: while the coverage in the learning phase, Covg L completeness of the learned B-cell profiles p j in representing the data x i , or the ratio of the number of matching items (URLs or terms) between the learned profile and the data (session or do cu-ment) to the number of items in the data: In light of (10), we can see that the cosine similarity tries t o op-timize both precision and coverage simultaneously and equa lly by combining them through the geometrical average. However, w e no-ticed that when learning in a single pass framework, this ten ds to fa-vor longer profiles that tend to match more data, while compro mis-ing precision. Without loss of generality, if we confine ours elves to the simplest type of recommendation strategy or informat ion re-trieval scheme, we can see that compromising precision can h ave a pernicious effect on the learned profiles, especially when t hese are viewed as the cluster or profile summaries that will be used la ter in a recommendation system based on recommending the neares t profile, or in an information retrieval system based on match ing a user query to the nearest cluster representative centroid. In order to circumvent this problem, one can simply disregard the cov erage component from the cosine similarity, hence using only prec ision as a similarity measure. However, we noticed that this would tend to suffer from the other extreme, resulting in very short pro files that completely ignore coverage. For this reason, we propos e to use different combination strategies of precision and cove rage, not necessarily limited to the geometrical average. It can be sh own that the most conservative aggregation that places harsh demand s on both precision and coverage simultaneously must be given by the following pessimistic aggregation,
Therefore, we will compare learning the profiles using the co -sine similarity S cos in computing robust weights, scales, and stim-ulation function, to learning using the most pessimistic ag gregation of precision and coverage, called Min-Of-Precision-Coverage or MinPC , S min ij given by (13). In the next section, we explain how this comparison is performed and how the results are validat ed.
In evaluating the goodness of the learned B-Cell profiles tha t make up the immune network model, we recall that the B-cell pr o-files should represent the ground-truth trends as accuratel y as pos-sible, and as completely as possible, and that the distribut ion of the learned repertoire of B-cell profiles should mirror the i ncoming stream of evolving data as represented by the ground truth pr o-files/topic representatives. Accuracy can be measured base d on the precision of the learned B-cell profiles, p L ground truth profiles p GT based on coverage of the learned B-cell profiles, p L the ground truth profiles p GT phase, P rec v ij describes the accuracy of the B-cell profiles in representing the ground truth profiles p GT number of matching items (URLs or terms) between the learned profile and the ground truth profiles to the number of items in t he learned profile: while the coverage in the validation phase, Covg v the completeness of the B-cell profiles p j in representing the data x , or the ratio of the number of matching items (URLs or terms) between the learned profile and the data (session or document ) to the number of items in the data:
These are the measures that are computed as TECNO-STREAMS continuously learns the profiles from the incoming stream of web sessions or text documents.
Profiles were mined from the 12-day clickstream data (from 19 98) with 1704 sessions and 343 URLs from the website of the depart -ment of Computer Engineering and Computer Science at the Uni -versity of Missouri. This is a benchmark data set used in [13, 14]. The profiles that were discovered using TECNO-STREAMS in a single pass are comparable to the ones previously obtained u sing a variety of less scalable techniques [13, 14]. The maximum po p-ulation size was 50, the control parameter for compression w as K = 10 , with periodical compression every T = 10 sessions. The activation threshold was w min = 0 . 375 , and  X  = 100 illustrate the continuous learning ability of the proposed technique using the following simulations: Scenario 1: We partition the Web sessions into 20 distinct sets of sessions, each one assigned to the closest of 20 profiles prev iously discovered and validated using Hierarchical Unsupervised Niche Clustering (HUNC) [14], and listed in Table 1. In this table, the profile number, | P T i | denotes the number of sessions assigned closest to this profile ( P T i ), and the last column lists the top URLs in the profile. Each URL is preceded by its relevance weight in this profile, as explained in in [13, 14]. Then we presented th ese sessions to TECNO-STREAMS one profile at a time: sessions as-signed to trend 0, then sessions assigned to profile 1,  X  X  X  Scenario 2: We used the same session partition as scenario 1, but presented the profiles in reverse order: sessions assigned t o trend 19, then sessions assigned to trend 18,  X  X  X  , etc, ending with trend 0.
 Scenario 3: The Web sessions are presented in their natural chrono-logical order exactly as received in real time by the web serv er.
For each of the above scenarios, we repeated the experiment u s-ing cosine similarity S cos ij in learning as given by (9), and then again using the MinPC similarity S min ij as given by (13). Table 1: Summary of some usage trends previously discovered using Hierarchical Unsupervised Niche Clustering (only UR Ls with top 3 to 4 relevance weights shown in each profile)
We track the number of B-cells that succeed in learning each o ne of the 20 ground truth profiles after each session is presente d, by counting the number of B-cells registering a sufficient matc h (i.e., above a certain threshold) with each ground truth profile bas ed on one of the following criteria: (i) precision P rec v ij , measuring the accuracy of the learned profiles compared to the ground truth pro-files as given by (14), (ii) coverage Covg v pleteness of the learned profiles compared to the ground trut h pro-files as given by (15). These two measures provide an evolving number of hits per profile relative to each of the above criter ia, as shown in Figures 2 -7, for the two different learning similar ity options, and the three above scenarios respectively. The y-axis is split into 20 intervals, with each interval devoted to the tr end/profile number indicated by the lower value (from 0 to 19). A hit for th e i th profile for session No. t is shown in these figures at location ( t, i ) , and indicates the presence of at least one B-cell profile tha t achieved the desired threshold in the validation measures o f preci-sion or coverage.

The proposed immune clustering algorithm can learn the user profiles in a single pass. A single pass over all 1704 Web user s es-sions (with non-optimized Java code) took less than 7 second s on a 2 GHz Pentium 4 PC running on Linux. With an average of 4 mil-liseconds per user session , the proposed profile mining system is suitable for use in a real time personalization system to con stantly and continuously provide a fresh and current list of an unkno wn number of evolving user profiles. Old profiles can be handled i n a variety of ways. They may either be discarded, moved to secon dary storage, or cached for possible re-emergence. Even if disca rded, older profiles that re-emerge later, would be re-learned fro m scratch just like new profiles. Hence the logistics of maintaining ol d pro-files are less crucial compared to existing techniques.
Figures 2 and 3 show the evolving hits per usage trend for the cosine similarity and the MinPC similarity, respectively when sce-nario 1 is deployed for sequencing the usage trends. They bot h exhibit an expected staircase pattern proving the gradual learning of emergent usage trends as these are experienced by the immu ne network in the order from trend 0 to 19. The plot shows some pe-culiarities, for example at trend 15 since it records hits at the same time as trends 0, 2, 3, and 5. Table 1 and the examination of the user sessions in each of these trends show that these trends d o in-deed share many similarities with trend 15, especially in te rms of overlap. Typical cross reactions between similar patterns are actu-ally desired and illustrate a certain tolerance for inexact matching.
Figures 2(a) and 3(a) show that the number of learned profiles satisfying more than 0 . 5 precision evolves in synchrony with the usage trends being presented. Furthermore, Figure 3(a) sho ws that the MinPC similarity allows learning and maintaining high-precision profiles longer than cosine similarity in Figure 2(a). For in stance, compare the top 3 profiles in each figure corresponding to tren ds 17, 18, and 19 that are presented last in that sequence. Simil arly, Figure 3(b) shows that the MinPC similarity allows learning more high-coverage profiles and can keep them longer than the plain co-sine similarity in Figure 2(b). This can be seen in the top 5 pr ofiles corresponding to trends 15, 16, 17, 18, and 19 that are the las t to be encountered in that sequence.

Figures 4 and 5 show the evolving hits per usage trend for the cosine similarity and the MinPC similarity, respectively when sce-nario 2 is deployed for sequencing the usage trends. They sho w an interesting inverted staircase pattern due to the reverse presentation order. Again, comparing Figures 4(a) with Figure 5(a) shows that the MinPC similarity allows learning more high-precision profiles and can maintain them longer than cosine. Similarly, by cont rasting Figure 5(b) and Figure 4(b), we can infer that the MinPC similar-ity allows learning more high-coverage profiles and can keep them longer.

Finally Figures 6 and 7 show the evolving hits per usage trend for the cosine similarity and the MinPC similarity, respectively when the sessions are presented in their original chronological order cor-responding to scenario 3. In this case, the order of presenta tion of the trends is no longer sequenced in straight or reverse or der of the trend number. Instead, the user sessions are presented i n com-pletely natural (chronological) order, exactly as in real t ime. So we cannot expect a staircase pattern. In order to visualize t he ex-pected pattern , we simply plot the distribution of the original input sessions, but with all the noise sessions excluded, in Figur e 1 to further test the robustness to noise. This figure shows that t he ses-sion data is quite noisy, and that the arrival sequence and pa ttern of sessions belonging to the same usage trend may vary in a way that makes incremental tracking and discovery of the profile s even more challenging than in a batch style approach, where the se s-sions can be stored in memory, and a standard iterative appro ach is used to mine the profiles. It also shows how some of the usage trends (e.g: No. 13, 14, 15) are not synchronized with others , and how some of the trends (No. 5, 9, 13, 14) are weak and noisy. Such weak profiles can be even more elusive to discover in a rea l time web mining system. While Figures 6 and 7 show the high precision and high-coverage B-cell distribution with time , Figure 1 shows the distribution of the input data with time. The fact t hat all these figures show a striking similarity in the emergence pat terns of the trends, attests to the fact that the immune network is a ble to form a reasonable dynamic synopsis of the usage data, even af ter a single pass over the data, for both types of similarity measures (co-sine or MinPC ). Again, even here, we notice that MinPC succeeds slightly better than cosine similarity in learning high-pr ecision and high-coverage profiles. This can be seen for example by the fa ct that profiles 10 and 19 end up lost with the cosine similarity i n Fig-ures 6, because their corresponding learned profiles fall be low the precision and coverage threshold.

We notice furthermore that the gap between the MinPC and co-sine similarities, in the number and fidelity of learned high -precision and high-coverage profiles compared to the incoming stream o f evolving trends, gets wider when the trends are presented on e at a time (scenarios 1 and 2) as opposed to when they are presente d in a more random, alternating order (scenario 3). Note that sce narios 1 and 2 are much more challenging than scenario 3, and they wer e simulated intentionally to test the ability of TECNO-STREA MS to learn completely new and unseen patterns (usage trends, t opics, fore. In other words, these scenarios represent an extreme t est of the adaptability of the single-pass web mining system.

It is interesting to note that the memory span of the network is affected by the parameter  X  which affects the rate of forgetting in the immune network. A low value will favor faster forgetting , and therefore a more current set of profiles that reflect the most r ecent activity on a website, while a higher value will tend to keep o lder profiles in the network for longer periods.
In this paper, we studied the effect of the similarity measur e on the quality of the evolving cluster profiles or trends in a n oisy Web data stream, detected by using a recently proposed robus t and scalable algorithm (TECNO-STREAMS) . Even though the co-sine similarity has been prevalent in the majority of web clu ster-ing approaches, it may fail to explicitely seek profiles that achieve high coverage and high precision simultaneously . The Min-Of-Precision-Coverage or MinPC similarity, proposed and investigated in this paper, overcomes these drawbacks. Our simulations c on-firmed that the MinPC similarity does a better job than cosine in learning from a stream of evolving data in a single pass setti ng, re-gardless of the order of presentation. This is because the MinPC similarity has the advantage of explicitely coupling the pr ecision and coverage criteria to the early learning stages, and furt hermore requiring that the affinity of the data to the learned profiles or sum-maries be defined by the minimum of their coverage or precisio n, hence requiring that the learned profiles are simultaneousl y precise and complete, with no compromises. Our approach is modular a nd generic enough that it can be extended to handle richer Web ob ject models, such as more sophisticated web user profiles and web u ser sessions, or more elaborate text document representations . The only module to be extended would be the similarity measure th at is used to compute the stimulation levels controlling the su rvival, interaction, and proliferation of the learned B-cell profil es. This work is supported by National Science Foundation CA-REER Award IIS-0133948 to O. Nasraoui. [1] S. Babu and J. Widom. Continuous queries over data [2] D. Barbara. Requirements for clustering data streams. ACM [3] J. Borges and M. Levene. Data mining of user navigation [4] P. Bradley, U. Fayyad, and C. Reina. Scaling clustering [5] Y. Chen, G. Dong, J. Han, B. W. Wah, and J. Wang.
 [6] R. Cooley, B. Mobasher, and J. Srivastava. Data preparat ion [7] S. Guha, N. Mishra, R. Motwani, and L. O X  X allaghan.
 [8] J. Hunt and D. Cooke. An adaptative, distributed learnin g [9] N. K. Jerne. The immune system. Scientific American , [10] R. R. Korfhage. Information Storage and Retrieval . Wiley, [11] O. Nasraoui, C. Cardona-Uribe, and C. Rojas-Coronel. [12] O. Nasraoui, D. Dasgupta, and F. Gonzalez. An artificial [13] O. Nasraoui, H. Frigui, R. Krishnapuram, and A. Joshi. [14] O. Nasraoui and R. Krishnapuram. One step evolutionary [15] O. Nasraoui, R. Krishnapuram, H. Frigui, and A. Joshi. [16] O. Nasraoui, R. Krishnapuram, and A. Joshi. Mining web [17] M. Perkowitz and O. Etzioni. Adaptive web sites: [18] C. Shahabi, A. M. Zarkesh, J. Abidi, and V. Shah.
 [19] J. Srivastava, R. Cooley, M. Deshpande, and P.-N. Tan. W eb [20] J. Timmis, M. Neal, and J. Hunt. An artificial immune [21] T. Yan, M. Jacobsen, H. Garcia-Molina, and U. Dayal. Fro m [22] H. Yang, S. Parthasarathy, and S. Reddy. On the use of [23] O. Zaiane, M. Xin, and J. Han. Discovering web access [24] T. Zhang, R. Ramakrishnan, and M. Livny. Birch: An
