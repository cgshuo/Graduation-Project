 The application of kernel methods to link analysis is explored. In particular, Kandola et al. X  X  Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg X  X  HITS importance. These popular measures of re-latedness and importance correspond to the Neumann kernels at the extremes of their parameter range, and hence these kernels can be interpreted as defining a spectrum of link analysis measures inter-mediate between co-citation/bibliographic coupling and HITS. We also show that the kernels based on the graph Laplacian, including the regularized Laplacian and diffusion kernels, provide relatedness measures that overcome some limitations of co-citation relatedness. The property of these kernel-based link analysis measures is exam-ined with a network of bibliographic citations. Practical issues in applying these methods to real data are discussed, and possible so-lutions are proposed.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Co-citation coupling, graph kernel, HITS, link analysis
Link analysis aims to find useful information from the structure of graphs. In particular, much effort has been devoted in quanti-fying two types of information: importance of individual nodes in the graph, and relatedness between them. HITS [7] and PageRank [1] are the popular methods of evaluating importance of web pages.  X  Present address: Google Japan, Inc., Cerulean Tower 6F, 26-1 Sakuragaoka-cho, Sibuya-ku, Tokyo 150-8512, Japan.
 Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
 Co-citation [11] and bibliographic coupling [6] are two classic but still widely used measures of relatedness.

The objective of this paper is to show that the positive semidef-inite kernels on graph nodes, proposed in the context of machine learning, provide a link analysis framework that enjoy many attrac-tive properties.

As an instance of such a framework, we show that Kandola et al. X  X  Neumann kernels [5] subsume not only the co-citation and bibliographic coupling relatedness at an extreme of the parame-ter range, but also the HITS importance at the other extreme. This enables us to treat these popular link analysis measures uniformly in a parameterization scheme. Accordingly, the Neumann kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation/bibliographic coupling and HITS.
A formulation based on different kernels overcomes the limita-tions of co-citation (and bibliographic coupling) relatedness. The co-citation relatedness between documents is defined only on the basis of the number of joint citations made to them. It follows that co-citation coupling is not capable of computing relatedness if the documents are not jointly cited by any document. Moreover, it can be argued that the number of non-joint citations that are ignored by co-citation coupling is also a factor determining relatedness.
We show that the kernels based on the graph Laplacian [2, 8, 12] yield relatedness measures consistently over their parameter range, and these measures do not suffer from the above limitations. By in-troducing a new parameterization, we obtain link analysis measures that are intermediate between HITS importance and the relatedness measure given by the Laplacian-based kernels.

We also discuss the practical issues that may be encountered in the application of these kernels, including parameter tuning and approximation methods.

Due to lack of space, all the theorems in this paper are presented without proofs. These proofs can be found in [10] together with additional experimental results.
In this section, we review the link analysis measures of related-ness and importance that are relevant to the subsequent discussions. Throughout the paper, we denote matrices by capital letters, and column vectors by boldface letters. For a matrix A , A ( sents its ( i , j ) -element. Likewise, v ( i ) represents the nent of vector v .Let  X  ( A ) denote the spectral radius of
A general assumption underpinning link analysis is that in the target network structure such as a bibliographic citation graph and the web, an edge (a citation or hyperlink) between a pair of nodes (papers or web pages) signifies the nodes being in some sense re-lated. Hence the degree of relatedness can be inferred from the node proximity induced by the existence of edges.

Co-citation [11] and bibliographic coupling [6] are the standard methods of computing relatedness between documents in a citation network. Co-citation coupling defines relatedness between docu-ments as the number of other documents citing them both. Bib-liographic coupling defines relatedness between documents as the number of common references cited by them. Given an adjacency matrix A , the number of co-citations between nodes i and by the ( i , j ) -element of the co-citation matrix A T A liographic coupling matrix AA T gives the values of bibliographic coupling. These matrices are symmetric, so their graph counter-parts, the co-citation graph and bibliographic coupling graph ,are undirected. See Figure 1 for illustration.
Because of the difficulty in computing the importance of docu-ments from their contents, citation counts have long been used as the index of document importance.

Kleinberg X  X  HITS [7], along with PageRank, is a more recent and sophisticated method for evaluating document importance. HITS assigns two scores to each document (node), called the authority and hub scores. Let A be the adjacency matrix of a graph. HITS computes the following recursion over n = 0, 1, . . . starting with a ) = h ( 0 ) = 1 , the vector of all 1 X  X . The limit lim n  X   X  a ( tor whose i -th component represents the authority score of node Similarly, the hub vector lim n  X   X  h ( well known that when the dominant eigenvalue of A T A (and is simple, the authority and hub vectors exist and equal the domi-nant eigenvectors of A T A and AA T , respectively.
In this section and the next, we present some formulations of link analysis measures that are intermediate between importance of nodes and their relatedness. These formulations are based on the family of symmetric positive semidefinite kernels [9] defining an inner product of nodes in a graph.

Importance is a measure defined on individual nodes and is nat-urally represented as a vector, whereas relatedness is defined be-tween nodes, and hence forms a matrix. To define intermediate measures between these two extremes, we use the symmetric ma-trix vv T instead of an importance score vector v . When all the components of v is positive, every row (and column) vector in yields the node ranking identical to that of v .
Kandola et al. [5] proposed the Neumann kernels for computing document similarity from terms occurring in documents, in a spirit analogous to Latent Semantic Analysis. We discuss the interpreta-tion of these kernels in the context of link analysis.

The Neumann kernel in its original form is defined in terms of the term-by-document matrix X whose ( i , j ) -element is the frequency of the i -th term occurring in document j .From X , document cor-relation matrix K = X T X and term correlation matrix M = are first constructed.
 Definition 3.1 Let X be a term-by-document matrix, and let X T X and M = XX T .The Neumann kernel matrices with diffusion factor  X  (  X  0 ) , denoted by  X  K  X  and  X  M  X  , are defined as the solution to the following system of equations. The similarity between documents i and j isgivenbythe ( element of  X  K  X  , and the term similarity is given by  X  implies an alternative representation based on the Neumann series. Hence, when  X  &lt;  X  ( K )  X  1 (=  X  ( M )  X  1 ) , the solution exists and is given by  X  K  X  = K ( I  X   X  K )  X  1 and  X  M  X  = M ( I  X   X  M
The recurrence over  X  K and  X  M in eq. (2) implies that the Neumann kernels evaluate similarity between documents from term similar-ity, and vice versa. This complementary relation is reminiscent of the recursion (1) between the authorities and hubs in HITS. We apply the Neumann kernels to link analysis on the basis of this par-ticular similarity to HITS. Specifically, we use the adjacency matrix A of a citation graph in place of the document-by-term matrix Thus we have K = A T A and M = AA T , which coincide with the co-citation and bibliographic coupling matrices, respectively. Plug-ging them into eq. (3) yields the Neumann kernels based solely on citation information. For convenience, we introduce the shorthand and write Likewise,  X  M  X  = N  X  ( AA T ) , but since  X  M  X  can be obtained simply by transposing A in eq. (5), we focus on  X  K = N  X  ( A T A
The Neumann kernels thus obtained from a citation graph pos-sess much deeper relationship with HITS than just the superficial resemblance of their recursive forms. We will show that when viewed as a ranking method, the Neumann kernels subsume the HITS importance ranking as a special case.
Eq. (5) shows that the Neumann kernel matrix N  X  ( A T A ) is a weighted sum of ( A T A ) n over every n = 1, 2, . . . the ( i , j ) -element of the term ( A T A ) n represents the number of paths of length n between nodes i and j in the co-citation graph, we see that each element of the kernel matrix equals the sum of the number of paths between nodes weighted by a factor decaying exponentially with path length.

To grasp the meaning of the path counting and weight summa-tion in the Neumann kernels, let us first examine what each term (
A T A ) n , or the number of paths of length n , represents in terms of link analysis.

At n = 1 , ( A T A ) n = A T A is the co-citation matrix giving a relatedness measure. It is not so obvious what ( A T A ) n represents when n is larger. With n sufficiently large, however, it can be shown that the number of paths of length n emanating from a node is an indicator of the importance of the node, as the following theorem asserts. Theorem 3.1 Let  X  be the dominant eigenvalue of a nonnegative symmetric matrix A T A .If  X  is a simple eigenvalue, there exists an eigenvector v corresponding to  X  such that ( A T A /  X  ) n  X  n  X   X  .
 An implication of this theorem is that for every row (and column) vector of ( A T A ) n , the node ranking induced by the magnitude of its components tends towards the HITS authority ranking (given by the dominant eigenvector v ), if the co-citation graph is connected.
Summing ( A T A ) n over n = 1, 2, . . . as in eq. (5) can thus be interpreted as the mixture of relatedness (when n is small) and im-portance (when n is large). As a special case, the Neumann kernels subsume co-citation and bibliographic coupling at  X  = 0 .Onthe other hand, at the ceiling of the parameter range, the rankings in-duced by the Neumann kernels are also identical to the HITS im-portance, as stated by the following theorem.
 Theorem 3.2 Let  X  be the dominant eigenvalue of a nonnegative symmetric matrix A T A .If  X  is a simple eigenvalue, there exists a unit eigenvector v corresponding to  X  such that
In this section, we present different link analysis measures based on kernels, with the intention of overcoming the limitations of co-citation and bibliographic coupling relatedness. The two limita-tions we address are as follows.
 Limitation 1 Co-citation coupling assigns a non-zero relatedness score to a pair of documents only if they are commonly referenced by a document.
 In Figure 1(a), documents n 1 and n 3 are not jointly cited by any document, resulting in the absence of edge ( n 1 , n 3 ) in the co-citation graph (Figure 1(b)). Accordingly, they are not related to each other in terms of co-citation coupling. Because real-world networks are typically sparse, it is often desirable to even capture weak relation-ship between nodes such as n 1 and n 3 in this case. The relationship between these nodes might not be as strong as n 1 and n 2 n , but the fact that they are both co-cited with n 2 by other nodes still conveys a valuable piece of information.
 Limitation 2 Co-citation coupling determines the relatedness be-tween nodes i and j only on the basis of the number of nodes com-monly citing the two. Nodes citing only one of i and j are neglected, and the number of citations from those nodes does not affect the re-latedness between i and j in any way. Figure 2: A citation graph illustrating the limitation of co-citation relatedness.
 To see why Limitation 2 is an issue, consider the graph of Figure 2. In this graph, n 3 represents a frequently linked web page such as Google and Yahoo. Node n 1 is a much less popular page cited only by n 4 . Our intuition dictates that n 2 is more related to n as one would not conclude a page is related (or similar) to Google or Yahoo just because it is jointly cited with them. However, each effect, n 1 and n 3 are estimated as equally related to n co-citation relatedness.
 The Neumann kernels do not give a solution to these limitations. Limitation 1 does not appear to be a problem for the Neumann kernels, as they count paths of any length if parameter  X  &gt; However, an increase in  X  , no matter how small, biases the induced measures towards importance at the same time, making them un-suitable for evaluating relatedness. This bias also incurs Limita-tion 2. When applied to the graph of Figure 2, the Neumann kernels with non-zero  X  regards n 2 as more related to n 3 than to contradicts our intuition even further than co-citation coupling.
The submatrix of the Neumann kernel with  X  = 0.18 0.9  X   X  for nodes n 1 through n 3 is shown below, where  X  is the spectral radius of the co-citation coupling matrix.
 Note that N 0.18 ( 2, 1 )= 3.05 &lt; 15.49 = N 0.18 ( 2, 3 holds for smaller  X  as well. At  X  = 0.02 0.1  X   X  1 , for instance, N 0.02 ( 2, 1 )= 1.06 &lt; 1.13 = N 0.02 ( 2, 3 ) .
We show that the kernels based on the graph Laplacian [2] over-comes the limitations of Section 4.1. Let G be an undirected graph with positive edge weights, and B be its adjacency matrix. The Laplacian of G is defined as L ( B )= D ( B )  X  B , where diagonal matrix with D ( B )( i , i )=  X  define the regularized Laplacian kernels [12]) as follows. Definition 4.1 Let B be a nonnegative symmetric matrix, G induced undirected graph, and let  X   X  0 . The matrix is called the regularized Laplacian kernel on G with diffusion fac-tor  X  .
 To ensure the symmetry of B ,wetakeas B the co-citation matrix A T A or the bibliographic coupling matrix AA T .

Provided that  X  &lt; 1/  X  ( L ( B )) , the right-hand side of (6) is the closed form solution to the series It is also obtainable by using  X  L ( B ) in place of the adjacency ma-trix B and dropping the first factor B from eq. (4). Note however that but R  X  ( B ) in eq. (6) may exist even if  X   X  1/  X  ( the infinite series in eq. (7) does not converge. In practice, restrict-ing the parameter range to  X  &lt; 1/  X  ( L ( B )) has a merit in that an approximate computation method based on infinite series represen-tation is applicable (see Section 5.1).

In addition, the infinite series representation allows the interpre-tation of kernel computation as path counting, parallelling the dis-cussion of Section 3.3. In the case of the regularized Laplacian kernels, counting takes place not in co-citation or a bibliographic coupling graphs, but in the graph induced by taking the negative of their Laplacian as the adjacency matrix. The difference is that self-loop edges in the latter graph have negative weights.
The regularized Laplacian kernels remain a relatedness measure even if diffusion factor  X  is increased, by virtue of negative weights assigned to self-loop edges. During path counting, paths through these loops are also taken into account. As a result, authoritative nodes receive larger discounting, as the loops at authoritative nodes typically have a heavier weight; as seen from the definition of the Laplacian, the weight of a loop is the negated sum of the weight of the (non-loop) edges incident to the node.

Recall the graph depicted in Figure 2. As argued previously, it is more natural to regard n 2 as more related to n 1 than to regularized Laplacian kernel matches this intuition and assigns a greater relatedness score to n 1 than to n 3 relative to n
Below is the regularized Laplacian kernel with  X  = 0.18 0.9  X   X  1 ,where  X  is the spectral radius of the negative Laplacian of the co-citation matrix.
 Again, only the submatrix of the kernel for nodes n 1 through shown. Here, we have R 0.18 ( 2, 1 ) &gt; R 0.18 ( 2, 3 ) .
If discounting high-degree nodes is all that is needed, one may argue that there should be simpler ways. Even though some of these straightforward discounting methods may work at a relatively small  X  ,as  X  gets larger, they are either biased towards importance, or give a measure inconsistent with our intuition on relatedness. For instance, simply normalizing the Neumann kernel matrices by  X  N ( i , j )= N ( i , j ) / N ( i , i ) N ( j , j ) gives  X  N induced feature spaces does not work either.

Another possibility may be to use the column transition matrix  X  A , obtained by normalizing the adjacency matrix A so that its col-umn sums equal to one, and apply the Neumann kernels to the ma-trix  X  A T  X  A . Again, this method works at relatively small  X  , but in-creasing  X  towards its ceiling yields a strange measure which does not appear to be either importance or relatedness.

At  X  = 0.68 0.9  X   X  1 (  X  is different from above due to reweight-ing), this method gives It evaluates n 2 as more related to n 1 than to n 3 as desired. Note however that N 0.68 (  X  A T  X  A )( 3, 1 ) &gt; N 0.68 (  X  that n 3 is more related to n 1 than to n 2 . This is against intuition since the relatedness between n 3 and n 1 must be deduced from the paths from n 3 to n 1 in the co-citation graph, all of which pass through n 2 on the way. Indeed, Theorem 3.2 shows that N  X  in the limit  X   X  1/  X  associates identical ranking n 1 &gt; to all nodes n 1 , n 2 ,and n 3 .

By contrast, such anomaly is not present in the regularized Lapla-cian kernels in the limit of  X  . The following theorem states that these kernels in the limit assign a uniform score to all the nodes in the same connected component of the graph induced by B . Theorem 4.1 Let B  X  R m  X  m be a nonnegative symmetric irre-ducible matrix. The regularized Laplacian kernel R  X  ( B ) converges to ( 1/ m ) 11 T as  X   X   X  .

A question at this point is whether the regularized Laplacian ker-nels are nonnegative so that one can use the vectors in the kernel matrices as a score vector, in a manner similar to the Neumann kernels. The proof does not seem so straightforward as the latter because of the negative elements in the Laplacian, but the proof for the case where  X  &lt; 1/  X  ( B ) can be obtained as a corollary to Theorem 4.2 we present in the next section.
In the regularized Laplacian kernels,  X  cannot be used for con-trolling the bias between importance and relatedness, as they re-main a relatedness measure regardless of the value of  X  . To control bias in these kernels, we introduce a new parameterization scheme. Definition 4.2 Let G be an undirected graph with positive weights, and B be its adjacency matrix, and let 0  X   X   X  1 . We define the modified Laplacian L  X  ( B ) of G as L  X  ( B )=  X  D ( B )  X  D (
B ) is a diagonal matrix with D ( B )( i , i )=  X  j B ( i , j Definition 4.3 Let B be a nonnegative symmetric matrix, and its induced graph. For  X   X  0 and 0  X   X   X  1 ,iftheseries is convergent, we call R  X  ,  X  ( B ) the modified regularized Laplacian kernel on G .
 At  X  = 1 , R  X  ,  X  ( B ) reduces to the (original) regularized Laplacian kernel R  X  ( B ) representing relatedness between nodes. As  X  de-creases towards 0 , each row (column) vector of the kernel matrix bears more and more the character of an importance vector, pro-vided that  X  is sufficiently large. In particular, at  X  = reduces to I +  X  N  X  ( B ) ,where N  X  ( B ) isgivenbyeq.(4). The following theorem states the property of the modified regularized Laplacian kernels.
 Theorem 4.2 For any nonnegative symmetric matrix B , the modi-fied regularized Laplacian kernel R  X  ,  X  ( B ) , if it converges, is doubly nonnegative, i.e., (element-wise) nonnegative and symmetric posi-tive semidefinite. Nonnegativity means that the vectors in the kernel matrices can be interpreted as score vectors. Positive semidefiniteness implies that the kernels define an inner product in some feature space and hence they are compatible with Support Vector Machines and other state-of-the-art kernel-based machine learning tools.
All the kernels presented above are based on the Neumann series, but other series can be used. Using the matrix exponential in place of the Neumann series yields the so-called diffusion (heat) kernels , originally developed in the context of spectral graph theory [2]. It was first introduced to machine learning community by Kondor and Lafferty [8].
 Definition 4.4 Let G be an undirected graph with positive weights, and B be its adjacency matrix. The diffusion kernel matrix G with diffusion factor  X   X  0 is given by It can be shown that H  X  ( B ) also converges to a uniform matrix as  X   X   X  .

The modified Laplacian L  X  ( B ) can be used in place of the Lapla-cian L ( B ) with diffusion kernels as well. The resulting kernel ,  X  ( B )= exp (  X   X  L  X  ( B )) is positive semidefinite, and allows for controlling the bias between relatedness and importance just like the modified regularized Laplacian kernels.
 In parallel to Theorem 3.2, it can be shown that when v is the HITS authority vector of a graph whose adjacency matrix is and  X  is the dominant eigenvector of A T A , H 0,  X  ( A T A converges to vv T as  X   X   X  .
In this section, we discuss some issues that may be encountered in the practical application of the kernel-based link analysis. Em-pirical results demonstrating the effectiveness of the methods pro-posed below are presented in the companion technical report [10].
Computing the entire kernel matrix requires matrix inversion or exponentiation, and hence its computational complexity is roughly O ( | V | 3 ) where | V | is the number of nodes in the graph, and this may be a computational burden with large graphs. However, the standard techniques for matrix computation [4,  X  11.2] allow ap-proximating kernel computation with the sum of the first k the infinite series in eqs. (5), (6), and (8). The approximation error is bounded by ( | V | / k ! )((  X  X  )  X  1  X  1 )  X  1/2 ,where  X  is the spectral radius of the co-citation matrix.

Furthermore, if one is concerned with the importance of nodes relative to a single node i rather than the entire kernel matrix, or if the entire kernel matrix cannot be kept on memory, we can re-duce the space requirement by summing  X  A T A n u i over n 1,..., k ,where u i is a unit vector with only 1 at the i nent; the computation now reduces to that of vector sums and the matrix-vector multiplication similar to HITS. All the kernels in the previous sections are parameterized. In the Neumann kernels, the parameter  X  controls the tradeoff between re-latedness and importance. Setting the right parameter value hence emerges as an issue in practical application of these kernels. Unfor-tunately, the optimality condition according to which the parameter must be tuned seems highly dependent on individual tasks. Con-sider a paper recommendation system, for example. Given a small list of the  X  X oot X  papers the user considered interesting, the system should recommend other papers that may be of interest to the user. The degree of the user X  X  acquaintance with the field of the root papers should affect how much the system should bias (through parameter tuning) its decision towards authoritative papers in the field, but such knowledge on the user is often outside the scope of link analysis.

If parameter setting requires external knowledge (e.g., user mod-eling), a practical alternative should be to present the user (or the external user-modeling module) kernel matrices with various pa-rameter settings, and let them choose the most suitable one. This approach requires a way to choose sample points efficiently; as we will see in Section 6, the character of the link analysis measures in-duced by these kernels is far from linear to the parameters, making sampling at uniform intervals a non-viable option.

We point out that the derivatives of kernel matrices with respect to the bias parameter can be used to efficiently determine sample points. For some kernels, derivatives can be analytically computed from kernel matrices at a given point  X  as follows.
Let us take the Neumann kernel N  X  ( B ) as an example. Suppose we have N  X  ( B ) for some  X  at hand. We can compute the first order approximation  X  N  X  +  X   X  ( B ) of the matrix N  X  +  X   X  ( where  X  N  X  ( B ) /  X  X  is given by eq. (10). By comparing the rank-ings induced by  X  N  X  +  X   X  ( B ) and N  X  ( B ) , we can estimate how likely the change may occur in a given range [  X  ,  X  +  X   X  ] . The cost of this estimation is that of matrix multiplication and summation in eqs. (10) and (11); there is no need to compute N  X  +  X   X  from scratch, until a suitable sampling interval  X   X  is determined.
To evaluate the characteristics of the kernel-based link analysis measures introduced in the previous sections, we applied them to a co-citation graph of papers on natural language processing, which is a connected graph consisting of 2280 nodes (papers).

Each kernel matrix was treated as a ranking method by taking the i -th row vector of the matrix as the score vector for the i Given the ranking induced by the i -th row vector, we call the node as the root node of this ranking.

Following [13], we use the minimizing Kendall (K-min) distance [3] between the top-10 items to evaluate the (dis)similarity of rank-ings. A small K-min distance means the two top-10 rankings are similar. It is equal to 0 if all top-10 items are identical, and takes the maximum value of 100 if there are no common items in the top-10 lists.
Table 1 shows the K-min distance between the top-10 lists in-duced by the Neumann kernels and HITS, averaged over all 2280 root nodes. The diffusion factor  X  for the kernels is shown as a Table 1: K-min distance between HITS and the Neumann ker-nels. Table 2: K-min distance between HITS and the regularized Laplacian kernels. normalized factor relative to 1/  X  ,where  X  is the spectral radius of the co-citation matrix; thus the admissible parameter range is 0  X   X  X  &lt; 1 . Table 1 indicates that the rankings induced by the Neumann kernels are biased towards the HITS ranking as  X  is in-creased. Table 2 lists the average K-min distance among the rankings of HITS and the regularized Laplacian kernels with various  X  , with the average taken over 2280 root nodes.

All over the parameter range shown in the table, the K-min dis-tance consistently exceeds 95, meaning that the induced rankings do not resemble that of HITS. Recall that the distance between the rankings of HITS and the Neumann kernel at  X  = 0.01 was 87.4 (see Table 1), which is also large, but not as large as that of the reg-ularized Laplacian kernels. This result suggests that bias towards importance persists in the Neumann kernels even with a small  X  .
The regularized Laplacian kernels are extremely stable over the parameter range of 0.01  X   X  &lt; 1 ; the difference in K-min distance between the rankings at  X  = 0.01  X   X  1 and 0.999  X   X  1 is less than 1 . The increase in the distance of rankings between  X  = 0.01 and  X   X  10  X   X  1 is not because the measure is inclined towards importance as  X  is increased, but because increased  X  makes the kernel more and more  X  X niform, X  as asserted in Theorem 4.1.
To see if the regularized Laplacian kernels indeed give a relat-edness measure, we need to measure the correlation between these kernels and co-citation coupling. However, this time we cannot use the K-min distance as the index of dissimilarity of their rank-ing lists, because the co-citation ranking often includes a number of ties which cannot be handled by the K-min distance. Instead, we have verified that for every root paper in the dataset, all the pa-pers that are co-cited with the root paper are ranked topmost by the regularized Laplacian kernels with  X  = 0.1  X   X  1 and 0.01
Although omitted for the lack of space, the rankings of diffu-sion kernels show tendencies similar to the regularized Laplacian kernels.
To verify parameter  X  controlling the tradeoff between related-ness and importance in the modified regularized Laplacian kernels, we compared the rankings induced by these kernels with those of HITS and the regularized Laplacian kernel with  X  = 0.1  X   X  latter two are used as the benchmark of the importance and related-ness measures, respectively. Again, we did not use co-citation cou-pling as the baseline for relatedness because of ties in its ranking. The regularized Laplacian kernel was used instead, on the basis of Table 3: K-min distance between the modified regularized Laplacian kernels and other link analysis measures: HITS (as a baseline measure of importance), and the unmodified regu-larized Laplacian kernel with  X  = 0.1  X   X  1 (as a measure of re-latedness). the result presented in Section 6.2, which showed that this kernel is a legitimate alternative to co-citation. The diffusion factor  X  is set to 0.99999  X   X  1 ,where  X  is the spectral radius of the Laplacian.
The result is shown in Table 3. The modified regularized Lapla-cian kernels tend to be more similar to HITS as  X  is decreased.
We conclude this section with an example illustrating the char-acters of the kernels. Table 4 shows the part of the ranking lists for the root paper  X  X mpirical studies in discourse X  by M. A. Walker andJ.D.Moore, Computational Linguistics 23(1):1 X 12, 1997. The table lists all the 22 papers that are ranked as top 10 in at least one of the 9 ranking lists shown in the right-hand side of the table.
An interesting observation with this root paper is that it makes a real-world example of the toy graph of Figure 2; the root paper is n 2 , and the most authoritative paper (Penn Treebank) is the other co-cited papers concern discourse just like the root paper, and presumably, they are more related to the root paper than the Penn Treebank paper is. As a result, each of these co-cited papers on discourse corresponds to n 1 .

Compare the ranking lists of the Neumann kernel (NK) and the regularized Laplacian (RLK) with  X  X  = 0.1 . In the two lists, the set of the top seven papers (including the root paper itself) is identi-cal, with all the seven papers being those with non-zero co-citation (CC) scores.

Looking inside the rankings of these seven papers, we find that both kernels place the root paper at the top of the ranking, but for the other six papers, the rankings they produce are the inverse of each other. The Neumann kernel X  X  ranking of these six matches that of HITS, while that of the regularized Laplacian kernel is in the opposite order of their HITS rankings.
Relative importance is a new link analysis measure recently pro-posed by White and Smyth [13]. This measure is defined as the  X  X m-portance of nodes in a graph relative to one or more root nodes. X  In this view, HITS and PageRank are  X  X lobal X  importance algorithms. White and Smyth made a convincing argument that simply apply-ing global importance algorithms to the subgraph surrounding the root nodes does not yield a precise estimate of relative importance, because the root nodes are not given any special preference during importance computation.

Our kernel-based link analysis measures fit naturally as relative importance, and as a bonus clarify the relationship between rela-tive importance and relatedness (namely, the co-citation and bibli-ographic coupling relatedness), an issue not addressed previously.
Smola and Kondor [12] pointed out the connection between graph kernels and importance computation methods including HITS, in a formulation different from ours. There are two differences between was not co-cited with the root paper.
 their formulation and ours. (1) They state that for a given node in a regular graph, its HITS score is given by the length of its corre-sponding vector in the feature space induced by Laplacian-based kernels. By contrast, we interpret the elements of kernel matrices, not just the diagonals, as indicating the score of importance (or re-latedness, or relative importance). (2) Our formulation obtains the HITS importance scores as an extremum of the Neumann kernels, which does not rely on the Laplacian. In addition, their argument is solely concerned with importance, and relatedness and interme-diates between these are left out of discussion.
This paper has investigated the properties of graph kernels viewed as link analysis measures. The Neumann kernels provide a unified framework that accounts for the co-citation/bibliographic coupling relatedness and the HITS importance. Laplacian-based kernels de-fine relatedness measures that are free from the limitations of co-citation and bibliographic coupling. We have also proposed an al-ternative parameterization for the Laplacian-based kernels to con-trol the tradeoff between the Laplacian-based relatedness and HITS importance.

In future work, we plan to investigate the property of the ker-nels based on the normalized version of the Laplacian [2] as link analysis measures. [1] S. Brin and L. Page. The anatomy of a large-scale [2] F. R. K. Chung. Spectral Graph Theory . Am. Math. Soc., [3] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k [4] G.H.GolubandC.F.VanLoan. Matrix Computation . Johns [5] J. Kandola, J. Shawe-Taylor, and N. Cristianini. Learning [6] M. M. Kessler. Bibliographic coupling between scientific [7] J. M. Kleinberg. Authoritative sources in a hyperlinked [8] R. Kondor and J. Lafferty. Diffusion kernels on graphs and [9] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels .MIT [10] M. Shimbo and T. Ito. Application of kernels to link [11] H. Small. Co-citation in the scientific literature: a new [12] A. J. Smola and R. Kondor. Kernels and regularization of [13] S. White and P. Smyth. Algorithms for estimating relative
