 Rating scales have been used for measuring hu-man perception of various stimuli for a long time, at least since the early 20th century (Freyd, 1923). First used in psychology and psychophysics, they are now also common in a variety of other disci-plines, including NLP . Discrete scales are the only type of scale commonly used for qualitative assess-ments of computer-generated language in NLP (e.g. in the DUC / TAC evaluation competitions). Contin-uous scales are commonly used in psychology and related fields, but are virtually unknown in NLP .
While studies assessing the quality of individual scales and comparing different types of rating scales are common in psychology and related fields, such studies hardly exist in NLP , and so at present little is known about whether discrete scales are a suitable rating tool for NLP evaluation tasks, or whether con-tinuous scales might provide a better alternative.
A range of studies from sociology, psychophys-iology, biometrics and other fields have compared discrete and continuous scales. Results tend to dif-fer for different types of data. E.g., results from pain measurement show a continuous scale to outperform a discrete scale (ten Klooster et al., 2006). Other results (Svensson, 2000) from measuring students X  ease of following lectures show a discrete scale to outperform a continuous scale. When measuring dyspnea, Lansing et al. (2003) found a hybrid scale to perform on a par with a discrete scale.

Another consideration is the types of data pro-duced by discrete and continuous scales. Parametric methods of statistical analysis, which are far more sensitive than non-parametric ones, are commonly applied to both discrete and continuous data. How-ever, parametric methods make very strong assump-tions about data, including that it is numerical and normally distributed (Siegel, 1957). If these as-sumptions are violated, then the significance of re-sults is overestimated. Clearly, the numerical as-sumption does not hold for the categorial data pro-duced by discrete scales, and it is unlikely to be nor-mally distributed. Many researchers are happier to apply parametric methods to data from continuous scales, and some simply take it as read that such data is normally distributed (Lansing et al., 2003).
Our aim in the present study was to system-atically assess and compare discrete and continu-ous scales when used for the qualitative assess-ment of computer-generated language. We start with an overview of assessment scale types (Section 2). We describe the experiments we conducted (Sec-tion 4), the data we used in them (Section 3), and the properties we examined in our inter-scale com-parisons (Section 5), before presenting our results (Section 6), and some conclusions (Section 7). With Verbal Descriptor Scales ( VDS s), partici-pants give responses on ordered lists of verbally de-scribed and/or numerically labelled response cate-gories, typically varying in number from 2 to 11 (Svensson, 2000). An example of a VDS used in NLP is shown in Figure 1. VDS s are used very widely in contexts where computationally generated language is evaluated, including in dialogue, summarisation, MT and data-to-text generation.

Visual analogue scales ( VAS s) are far less com-mon outside psychology and related areas than VDS s. Responses are given by selecting a point on a typically horizontal line (although vertical lines have also been used (Scott and Huskisson, 2003)), on which the two end points represent the extreme values of the variable to be measured. Such lines can be mono-polar or bi-polar, and the end points are labelled with an image (smiling/frowning face), or a brief verbal descriptor, to indicate which end of the line corresponds to which extreme of the vari-able. The labels are commonly chosen to represent a point beyond any response actually likely to be cho-sen by raters. There is only one examples of a VAS in
NLP system evaluation that we are aware of (Gatt et al., 2009).

Hybrid scales, known as a graphic rating scales, combine the features of VDS s and VAS s, and are also used in psychology. Here, the verbal descriptors are aligned along the line of a VAS and the endpoints are typically unmarked (Svensson, 2000). We are aware of one example in NLP (Williams and Reiter, 2008); we did not investigate this scale in our study.
We used the following two specific scale designs in our experiments:
VDS-7 : 7 response categories, numbered (7 = best) and verbally described (e.g. 7 =  X  X erfectly flu-ent X  for Fluency, and 7 =  X  X erfectly clear X  for Clar-ity). Response categories were presented in a verti-cal list, with the best category at the bottom. Each category had a tick-box placed next to it; the rater X  X  task was to tick the box by their chosen rating.
VAS : a horizontal, bi-polar line, with no ticks on it, mapping to 0 X 100. In the image description tests, statements identified the left end as negative, the right end as positive; in the weather forecast tests, the positive end had a smiling face and the label  X  X tatement couldn X  X  be clearer/read better X ; the neg-ative end had a frowning face and the label  X  X tate-ment couldn X  X  be more unclear/read worse X . The raters X  task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. Weather forecast texts: In one half of our evalua-tion experiments we used human-written and auto-matically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative sys-tem evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The fol-lowing are examples of weather forecast texts from the data: Image descriptions: In the other half of our eval-uations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUNA Project (van Deemter et al., 2006). The following is an ex-ample of an item from the corpus, consisting of a set of images and a description for the entity in the red frame: 4.1 Evaluation criteria Fluency/Readability : Both the weather forecast and image description evaluation experiments used a quality criterion intended to capture  X  X ow well a piece of text reads X , called Fluency in the latter, Readability in the former.

Adequacy/Clarity : In the image description ex-periments, the second quality criterion was Ade-quacy, explained as  X  X ow clear the description is X , and  X  X ow easy it would be to identify the image from the description X . This criterion was called Clarity in the weather forecast experiments, explained as  X  X ow easy is it to understand what is being described X . 4.2 Raters In the image experiments we used 8 raters (native speakers) in each experiment, from cohorts of 3rd-year undergraduate and postgraduate students doing a degree in a linguistics-related subject. They were paid and spent about 1 hour doing the experiment.
In the weather forecast experiments, we used 22 raters in each experiment, from among academic staff at our own university. They were not paid and spent about 15 minutes doing the experiment. 4.3 Summary overview of experiments Weather VDS-7 (A) : VDS -7 scale; weather forecast data; criteria: Readability and Clarity; 22 raters (uni-versity staff) each assessing 22 forecasts.
 Weather VDS-7 (B) : exact repeat of Weather VDS -7 (A), including same raters.

Weather VAS : VAS scale; 22 raters (university staff), no overlap with raters in Weather VDS -7 ex-periments; other details same as in Weather VDS -7.
Image VDS-7 : VDS -7 scale; image description data; 8 raters (linguistics students) each rating 112 descriptions; criteria: Fluency and Adequacy.
Image VAS (A) : VAS scale; 8 raters (linguistics students), no overlap with raters in Image VAS -7; other details same as in Image VDS -7 experiment.
Image VAS (B) : exact repeat of Image VAS (A), including same raters. 4.4 Design features common to all experiments In all our experiments we used a Repeated Latin Squares design to ensure that each rater sees the same number of outputs from each system and for each text type (forecast date/image set). Following detailed instructions, raters first did a small number of practice examples, followed by the texts to be rated, in an order randomised for each rater. Eval-uations were carried out via a web interface. They were allowed to interrupt the experiment, and in the case of the 1 hour long image description evaluation they were encouraged to take breaks. Validity is to the extent to which an assessment method measures what it is intended to measure (Svensson, 2000). Validity is often impossible to as-sess objectively, as is the case of all our criteria ex-cept Adequacy, the validity of which we can directly test by looking at correlations with the accuracy with which participants in a separate experiment identify the intended images given their descriptions. A standard method for assessing Reliability is Kendall X  X  W, a coefficient of concordance, measur-ing the degree to which different raters agree in their ratings. We report W for all 6 experiments.

Stability refers to the extent to which the results of an experiment run on one occasion agree with the results of the same experiment (with the same raters) run on a different occasion. In the present study, we assess stability in an intra-rater, test-retest design, assessing the agreement between the same participant X  X  responses in the first and second runs of the test with Pearson X  X  product-moment correla-tion coefficient. We report these measures between ratings given in Image VAS (A) vs. those given in Im-age VAS (B), and between ratings given in Weather VDS -7 (A) vs. those given in Weather VDS -7 (B).
We assess Interchangeability , that is, the extent to which our VDS and VAS scales agree, by comput-ing Pearson X  X  and Spearman X  X  coefficients between results. We report these measures for all pairs of weather forecast/image description evaluations.
We assess the Sensitivity of our scales by de-termining the number of significant differences be-tween different systems and human authors detected by each scale.

We also look at the relative effect of the differ-ent experimental factors by computing the F-Ratio for System (the main factor under investigation, so its relative effect should be high), Rater and Text Type (their effect should be low). F-ratios were de-termined by a one-way ANOVA with the evaluation criterion in question as the dependent variable and System, Rater or Text Type as grouping factors. 6.1 Interchangeability and Reliability for Interchangeability : Pearson X  X  r between the means per system/human in the three image description evaluation experiments were as follows (Spearman X  X   X  shown in brackets): For both Adequacy and Fluency, correlations be-tween Image VDS -7 and Image VAS (A) (the main VAS experiment) are extremely high, meaning that they could substitute for each other here.
 Reliability : Inter-rater agreement in terms of Kendall X  X  W in each of the experiments: W was higher in the VAS data in the case of Fluency, whereas for Adequacy, W was the same for the VDS data and VAS (B), and higher in the VDS data than in the VAS (A) data. 6.2 Interchangeability and Reliability for Interchangeability : The correlation coefficients (Pearson X  X  r with Spearman X  X   X  in brackets) between the means per system/human in the image descrip-tion experiments were as follows: For both Adequacy and Fluency, correlations be-tween Weather VDS -7 (A) (the main VDS -7 experi-ment) and Weather VAS (A) are again very high, al-though rank-correlation is somewhat lower.
 Reliability : Inter-rater agreement in terms of Kendall X  X  W was as follows: This time the highest agreement for both Clarity and Readability was in the VDS -7 data. 6.3 Stability tests for image and weather data Pearson X  X  r between ratings given by the same raters first in Image VAS (A) and then in Image VAS (B) was .666 for Adequacy, .593 for Fluency. Between ratings given by the same raters first in Weather VDS -7 (A) and then in Weather VDS -7 (B), Pearson X  X  r was .656 for Clarity, .704 for Readability. (All sig-nificant at p &lt; . 01 .) Note that these are computed on individual scores (rather than means as in the cor-relation figures given in previous sections). 6.4 F-ratios and post-hoc analysis for image The table below shows F-ratios determined by a one-way ANOVA with the evaluation criterion in question (Adequacy/Fluency) as the dependent variable and System/Rater/Text Type as the grouping factor. Note that for System a high F-ratio is desirable, but a low F-ratio is desirable for other factors.
 Out of a possible 28 significant differences for Sys-tem, the main factor under investigation, VDS -7 found 8 for Adequacy and 14 for Fluency; VAS (A) found 7 for Adequacy and 15 for Fluency. 6.5 F-ratios and post-hoc analysis for weather The table below shows F-ratios analogous to the pre-vious section (for Clarity/Readability).
 Out of a possible 55 significant differences for Sys-tem, VDS -7 (A) found 24 for Clarity, 23 for Read-ability; VAS found 25 for Adequacy, 26 for Fluency. 6.6 Scale validity test for image data Our final table of results shows Pearson X  X  correla-tion coefficients (calculated on means per system) between the Adequacy data from the three image description evaluation experiments on the one hand, and the data from an extrinsic experiment in which we measured the accuracy with which participants identified the intended image described by a descrip-tion: The correlation between Adequacy and ID Accuracy was strong and highly significant in all three image description evaluation experiments, but strongest in VAS (B), and weakest in VAS (A). For comparison, Pearson X  X  between Fluency and ID Accuracy ranged between .3 and .5, whereas Pearson X  X  between Ade-quacy and ID Speed (also measured in the same im-age identfication experiment) ranged between -.35 and -.29. Our interchangeability results (Sections 6.1 and 6.2) indicate that the VAS and VDS -7 scales we have tested can substitute for each other in our present evaluation tasks in terms of the mean system scores they produce. Where we were able to measure va-lidity (Section 6.6), both scales were shown to be similarly valid, predicting image identification ac-curacy figures from a separate experiment equally well. Stability (Section 6.3) was marginally better for VDS -7 data, and Reliability (Sections 6.1 and 6.2) was better for VAS data in the image descrip-tion evaluations, but (mostly) better for VDS -7 data in the weather forecast evaluations. Finally, the VAS experiments found greater numbers of statistically significant differences between systems in 3 out of 4 cases (Section 6.5).

Our own raters strongly prefer working with VAS scales over VDS s. This has also long been clear from the psychology literature (Svensson, 2000)), where raters are typically found to prefer VAS scales over VDS s which can be a  X  X onstant source of vexation to the conscientious rater when he finds his judg-ments falling between the defined points X  (Champ-ney, 1941). Moreover, if a rater X  X  judgment falls be-tween two points on a VDS then they must make the false choice between the two points just above and just below their actual judgment. In this case we know that the point they end up selecting is not an accurate measure of their judgment but rather just one of two equally accurate ones (one of which goes unrecorded).
 Our results establish (for our evaluation tasks) that VAS scales, so far unproven for use in NLP , are at least as good as VDS s, currently virtually the only scale in use in NLP . Combined with the fact that raters strongly prefer VAS s and that they are regarded as more amenable to parametric means of statisti-cal analysis, this indicates that VAS scales should be used more widely for NLP evaluation tasks.
