 Dual coordinate descent method is one of the most effec-tive approaches for large-scale linear classification. However, its sequential design makes the parallelization difficult. In environment. After pointing out difficulties faced in some existing approaches, we propose a new framework to paral-lelize the dual coordinate descent method. The key idea is to make the majority of all operations (gradient calculation here) parallelizable. The proposed framework is shown to be theoretically sound. Further, we demonstrate through experiments that the new framework is robust and efficient in a multi-core environment.
 dual coordinate descent, linear classification, multi-core com-puting
Linear classification such as linear SVM and logistic re-gression is one of the most used machine learning methods. However, training large-scale data may be time-consuming, so the parallelization has been an important research issue. In this work, we consider multi-core environments and study parallel dual coordinate descent methods, which are an im-portant class of optimization methods to train large-scale linear classifiers.

Existing optimization methods for linear classification can be roughly categorized to the following two types: 1. Low-order optimization methods such as stochastic gra-dient or coordinate descent (CD) methods. By using only the gradient information, this type of methods runs many cheap iterations. 2. High-order optimization methods such as quasi Newton or Newton methods. By using, for example, second-order information, each iteration is expensive but fewer itera-tions are needed to approach the final solution.
 These methods, useful in different circumstances, have been parallelized in some past works. To be focused here, we re-strict our discussion to those that are suitable for multi-core environments. Therefore, some that are mainly applicable in distributed environments are out of our interests.
For Newton methods, recently we have shown that with careful implementations, excellent speedup can be achieved in a multi-core environment [11]. Its success relies on par-allelizable operations that involve all data together. In con-trast, stochastic gradient or CD methods are inherently se-quential because each time only one instance is used to up-date the model. Among approaches of using low-order infor-mation, we are particularly interested in the CD method to solve the dual optimization problem. Although such tech-recent introduction to linear classification [5], dual CD has become one of the most efficient methods. Further, in con-trast to primal-based methods (e.g., Newton or primal CD) that often require the differentiability of the loss function, a dual-based method can easily handle some non-differentiable losses such as the l 1 hinge loss (i.e., linear SVM). Several works have proposed parallel extensions of dual CD methods (e.g., [6, 10, 13, 14, 15]), in which [6, 13, 14] focus more on multi-core environments. We can further cat-egorize them to two types: 1. Mini-batch CD [13]. Each time a batch of instances are selected and CD updates are parallelly applied to them. 2. Asynchronous CD [6][14]. Threads independently update different coordinates in parallel. The convergence is often faster than synchronous algorithms, but sometimes the algorithm fails to converge.
 In Section 2, we detailedly discuss the above approaches for parallel dual CD, and explain why they may be either in-efficient or not robust. Indeed, except the experiment code in [6], so far no publicly available packages have supported parallel dual CD in multi-core environments. In Section 3, we propose a new and simple framework that can effectively take the advantage of multi-core computation. Theoretical properties such as asymptotic convergence and finite ter-mination under given stopping tolerances are provided in Section 4. In Section 5, we conduct thorough experiments and comparisons. Results show that our proposed method is robust and efficient.

Based on this work, parallel dual CD is now publicly avail-able in the multi-core extension of our LIBLINEAR package: Because of space limitation, proofs and some additional ex-perimental results are left in supplementary materials at the same address. Code for experiments is also available there.
In this section, we begin with introducing optimization problems for linear classification and the basic concepts of dual CD methods. Then we discuss difficulties of the paral-lelization in multi-core environments.
Assume the classification task involves a training set of instance-label pairs ( x i ,y i ) ,i = 1 ,...,l, x i  X  R { X  1 , +1 } , a linear classifier obtains its model vector w by solving the following optimization problem. parameter. Commonly used loss functions include  X  ( w ; x ,y )  X 
In this work, we focus on l 1 and l 2 losses (i.e., linear SVM), primal problem, then a dual CD method solves the following dual problem: where  X  Q = Q + D , D is a diagonal matrix, and Q ij y for the l 2 loss, U =  X  and D ii = 1 / (2 C ) ,  X  i . Notice that l 1 loss is not differentiable, so solving the dual problem is generally easier than the primal.

We briefly review dual CD methods by following the de-others are fixed. Specifically, if the current  X  is feasible for (2), we solve the following one-variable sub-problem: where e i = [0 ,..., 0 where If  X 
Q ii &gt; 0, 1 the solution of (3) can be easily seen as
It has been pointed out in [5] that  X  Q ii = 0 occurs only when x i = 0 and the l 1 loss is used. Then  X  Q ij = 0 ,  X  j and the optimal  X  i = C . This variable can thus be easily removed before running CD.
 Algorithm 1 A dual CD method for linear SVM 1: Specify a feasible  X  and calculate w = P j y j  X  j x j 2: while  X  is not optimal do 3: for i = 1 ,...,l do 4: G  X  y i w T x i  X  1 + D ii  X  i 5: d  X  min(max(  X  i  X  G/  X  Q ii , 0) ,U )  X   X  i 6:  X  i  X   X  i + d 7: w  X  w + dy i x i Algorithm 2 Mini-batch dual CD in [13] 1: Specify  X  = 0 , batch size b , and a value  X  b &gt; 1 . 2: while  X  is not optimal do 3: Get a set B with | B | = b under uniform distribution 4: w = P j y j  X  j x j 5: for all i  X  B do in parallel 6: G = y i w T x i  X  1 + D ii  X  i 7:  X  i  X  min(max(  X  i  X  G/ (  X  b  X   X  Q ii ) , 0) ,U ) The main computation in (5) is on calculating  X  i f (  X  ) . One crucial observation in [5] is that If is maintained, then  X  i f (  X  ) can be easily calculated by
Note that we slightly abuse the notation by using the same symbol w of the primal variable in (1). The reason is that w in (6) will become the primal optimum if  X  converges to a dual optimal solution. We can then update  X  and maintain the weighted sum in (6) by This is much cheaper than calculating the sum of l vectors in (6). The simple CD procedure of cyclically updating  X  i ,i = 1 ,...,l is presented in Algorithm 1. We call each iteration of the while loop as an outer iteration. Thus each outer iteration contains l inner iterations to sequentially update all  X   X  X  components. Further, the main computation at each inner iteration includes two O ( n ) operations in (7) and (8).
The above O ( n ) operations are by assuming that the data plexity discussion in this paper should be replaced by O (  X  n ), where  X  n is the average number of non-zero feature values per instance.
We point out difficulties to parallelize dual CD methods by discussing two types of existing approaches.
Algorithm 1 is inherently sequential. Further, it contains many cheap inner iterations, each of which cost O ( n ) oper-ations. Some [13] thus propose applying CD updates on a batch of data simultaneously. Their procedure is summa-rized in Algorithm 2
Algorithms 1 and 2 differ in several places. First, in Algo-rithm 2 we must select a set B . In [13], this set is randomly selected under a distribution, so the algorithm is a stochastic dual CD. If we would like a cyclic setting similar to that in Algorithm 1, a simple way is to split all data { x 1 ,..., x blocks and then update variables associated with each block in parallel. The second and also the main difference from Algorithm 1 is that (5) cannot be used to update  X  i ,  X  i  X  B . The reason is that we no longer have the property that all but one variable are fixed. To update all  X  i ,i  X  B in par-allel but maintain the convergence, the change on each co-ordinate must be conservative. Therefore, they consider an approximation of the one-variable problem (3) by replacing  X  Q ii in (4) with a larger value  X  b  X   X  Q ii ; see line 7 of Algo-rithm 2. By choosing a suitable  X  b that is data dependent, [13] proved the expected convergence. One disadvantage of using conservative steps is the slower convergence. There-fore, asynchronous CD methods that will be discussed later aim to address this problem by still using the sub-problem (3).

An important practical issue not discussed in [13] is the calculation of w . In Algorithm 2, we can see that they re-calculate w at every iteration. This operation becomes the bottleneck because it is much more time-consuming than the update of  X  i ,  X  i  X  B . Following the setting in Algorithm 1, what we should do is to maintain w according to the change of  X  . Therefore, lines 5-7 in Algorithm 2 can be changed to 1: for all i  X  B do in parallel 2: G  X  y i w T x i  X  1 + D ii  X  i 3: d i  X  min(max(  X  i  X  G/ (  X  b  X   X  Q ii ) , 0) ,U )  X   X  4:  X  i  X   X  i + d i We notice that both the for loop (line 1) and the update the for loop can at best half the running time. Updating w in parallel is possible, but we explain that it is much more difficult than the parallel calculation of d i The main issue is that two threads may want to update the same component of w simultaneously. The following example shows that one thread for x i and another thread for x j both would like to update w s : The recent work [11] has detailedly studied this issue. One way to avoid the race condition is by atomic operations, so each w s is updated by only one thread at a time: 1: for all i  X  B do in parallel 2: Calculate G , obtain d i and update  X  i 3: for ( x i ) s 6 = 0 do 4: atomic: w s  X  w s + y i d i ( x i ) s Unfortunately, in some situations (e.g., number of features is small) atomic operations cause significant waiting time so that no speedup is observed [11]. Instead, for calculating the sum of some vectors the study in [11] shows better speedup by storing temporary results of each thread in the following vector and parallelly summing these vectors in the end. This ap-proach essentially implements a reduce operation in parallel computation. However, it is only effective when enough vec-tors are summed because otherwise the overhead of main-taining all  X  u p vectors leads to no speedup. Unfortunately, B is now a small set, so this approach of implementing a reduce operation may not be useful.

In summary, through the discussion we point out that the update of w may be a bottleneck in parallelzing dual CD. To address the conservative updates in parallel mini-batch CD, a recent direction is by asynchronous updates [6], [14]. Under a stochastic setting to choose variables, each thread independently updates an  X  i by the rule in (5): 1: while  X  is not optimal do 2: Select a set B 3: for all i  X  B do in parallel 4: G  X  y i w T x i  X  1 + D ii  X  i 5: d i  X  min(max(  X  i  X  G/  X  Q ii , 0) ,U )  X   X  i 6:  X  i  X   X  i + d i 7: for ( x i ) s 6 = 0 do 8: atomic: w s  X  w s + d i y i ( x i ) s To avoid the conflicts in updating w , they consider atomic operations. From the discussion in Section 2.2.1, one may worry that such operations cause serious waiting time, but [6], [14] report good speedup. A detailed analysis on the use of atomic operations here was in [11, supplement], where we point out that practically each thread updates w (line 8 of the above algorithm) by the following setting: 1: if d i 6 = 0 then 2: for ( x i ) s 6 = 0 do 3: atomic: w s  X  w s + d i y i ( x i ) s For linear SVM, some  X  elements may quickly reach bounds (0 or C for l 1 loss and 0 for l 2 loss) and remain the same. The corresponding d i = 0 so the atomic operation is not needed after calculating G =  X  i f (  X  ). Therefore, the atomic op-erations that may cause troubles occupy a relatively small portion of the total computation. However, for dense prob-lems because most x i  X  X  elements are non-zero, the race situa-tion more frequently occurs. Hence experiments in Section 5 show worse scalability.

The major issue of using an asynchronous setting is that the convergence may not hold. Both works [6], [14] assume that the lag between the start (i.e., reading x i ) and the end (i.e., updating w ) of one CD step is bounded. Specifically, if we denote the update by a thread as an iteration and order these iterations according to their finished time, then the resulting sequence {  X  k } should satisfy that where  X  k is the iteration index when iteration k starts, and  X  is a positive constant.

Both works require  X  to satisfy some conditions for the convergence analysis. Unfortunately, as indicated in Fig-ure 2 of [14], these conditions may not always hold, so the asynchronous dual CD method may not converge. In our experiment, this situation easily occurs for dense data (i.e., most feature values are non-zeros) if more cores are used. To avoid the divergence situation, [14] further proposes a semi-asynchronous dual CD method by having a separate thread to calculate (6) once after a fixed number of CD updates. However, they do not prove the convergence under such a semi-asynchronous setting. Algorithm 3 A practical implementation of Algorithm 1 considered by LIBLINEAR , where new statements are marked by  X  / new X  1: Specify a feasible  X  and calculate w = P j y j  X  j x j 2: while true do 3: M  X  X  X  X  4: for i = 1 ,...,l do 5: G  X  y i w T x i  X  1 + D ii  X  i 6: Calculate PG by (11) / new 7: M  X  max( M, | PG | ) / new 8: if | PG | X  10  X  12 then / new 9: d  X  min(max(  X  i  X  G/  X  Q ii , 0) ,U )  X   X  i 10:  X  i  X   X  i + d 11: w  X  w + dy i x i 12: if M &lt;  X  then / new 13: break
Based on the discussion in Section 2, we set the following design goals for a new framework. 1. To ensure the convergence in all circumstances, we do not consider asynchronous updates. 2. Because of the difficulty to parallelly update w (see Sec-tion 2.2.1), we run this operation only in a serial setting.
Instead, we design the algorithm so that this w update takes a small portion of the total computation. Further, we ensure that the most computationally intensive part is parallelizable.
To begin, we present Algorithm 3, which is the practical version of Algorithm 1 implemented in the popular linear classifier LIBLINEAR [2]. A difference is that a stopping con-dition is introduced. If we assume that one outer iteration contains the following inner iterates, then the stopping condition 2 is where  X  is a given tolerance and  X  P i f (  X  ) is the projected gradient defined as Notice that for problem (2),  X  is optimal if and only if
Another important change made in Algorithm 3 is that at line 8, we check whether  X  i f P (  X  )  X  0 to see if the current  X  is close to the optimum of the single-variable optimization problem (3). If that is the case, then we update neither  X  nor w . Note that updating  X  i is cheap, but the check at
Note that LIBLINEAR actually uses max i  X  f (  X  k,i consider (10). Therefore, in practice we may have the following situation Clearly, the calculation of is wasted. However, we know these values are close to zero only if we have calculated them.

The above discussion shows that between any two updated  X  components, several unchanged elements may exist. In fact we may deliberately have more unchanged elements. For example, if at line 8 of Algorithm 3 we instead use the following condition then many elements may be unchanged between two up-dated ones. Note that  X  is typically larger than 0.001 (0.1 is the default stopping tolerance used in LIBLINEAR ) and  X   X  (0 , 1) can be chosen not too small (e.g., 0.5). 3
A crucial observation from (12) is that because we can calculate their projected gradient values in parallel. Unfortunately, the number s is not known in advance. One solution is to conjecture an interval { 1 ,...,I } so we parallely calculate all corresponding gradient values, This approach ends up with the following situation After  X  k s is updated, gradient values become different and Because guessing the size of the interval is extremely diffi-cult, we propose a two-stage approach. We still calculate gradient values of I elements, but select a subset of candi-dates rather than one single element for CD updates: Stage 1: We calculate  X  i f (  X  k ) ,i = 1 ,...,I in parallel and then select some elements for update. The following exam-ple shows that after checking all I elements, three of them, { s 1 ,s 2 ,s 3 } , are selected; see the difference from (13). Stage 2: We sequentially update selected elements (e.g.,  X  1 , X  s 2 , and  X  s 3 in the above example) by regular CD up-dates.
 The standard CD greedily uses the latest  X  i f (  X  ) to check if  X  i should be updated. In contrast, our setting here relies elements should be updated. When  X  is close to the opti-mum and is not changed much, the selection should be as good as the standard CD. Algorithm 4 shows the details of
Note that we need  X   X  (0 , 1) to ensure from the stopping condition (10) that at each outer iteration at least one  X  updated. Algorithm 4 A parallel dual CD method 1: Specify a feasible  X  and calculate w = P j y j  X  j x j 2: Specify a tolerance  X  and a small value 0 &lt;  X   X   X  3: while true do 4: M  X  X  X  X  5: Split { 1 ,...,l } to  X  B 1 ,...,  X  B T 6:  X  t  X  0 7: for  X  B in  X  B 1 ,...,  X  B T do 8: Calculate  X  f  X  B (  X  ) in parallel 9: M  X  max( M, max i  X   X  B | X  P i f (  X  ) | ) 10: B  X  X  i | i  X   X  B, | X  P i f (  X  ) | X   X  X  } 11: for i  X  B do 12: G  X  y i w T x i  X  1 + D ii  X  i 13: d  X  min(max(  X  i  X  G/  X  Q ii , 0) ,U )  X   X  i 14: if | d | X   X   X  then 15:  X  i  X   X  i + d 16: w  X  w + dy i x i 17:  X  t  X   X  t + 1 18: if M  X   X  or  X  t = 0 then 19: break Algorithm 5 A framework of parallel dual CD methods, where Algorithms 4 and 6 are special cases 1: Specify a feasible  X  2: while true do 3: Select a set  X  B 4: Calculate  X   X  B f (  X  ) in parallel 5: Select B  X   X  B with | B ||  X  B | 6: Update  X  i ,i  X  B our approach. Like the cyclic setting in Algorithm 1, here we split { 1 ,...,l } to several blocks. Each time we parallely calculate  X  i f (  X  ) of elements in a block  X  B and then select a subset B  X   X  B for sequential CD updates. Note that line 14 is similar to line 8 in Algorithm 3 for checking if the change of  X  i is too small and w needs not be updated.
 A practical issue in Algorithm 4 is that the selection of B depends on the given  X  . That is, the stopping tolerance specified by users may affect the behavior of the algorithm. implementations.
The idea in Section 3.1 motivates us to have a general framework for parallel dual CD in Algorithm 5, where Algo-rithm 4 is a special case. The key properties of this frame-work are: 1. We select a set  X  B and calculate the corresponding gradi-ent values in parallel . 2. We then get a much smaller set B  X   X  B and update  X  B Assume that updating  X  B costs O ( | B | n ) operations as in Algorithm 4. Then the complexity of Algorithm 5 is see that parallel computation can significantly reduce the running time.

One may argue that Algorithm 5 is no more than a typi-cal block CD method and question why we come a long way to get it. A common block CD method selects a set  X  B at a time and solve a sub-problem of the variable  X   X  B . If we consider Algorithm 5 as a block CD method, then it has a very special setting in solving the sub-problem of  X   X  B gorithm 5 spends most efforts on further selecting a much smaller subset B and then (approximately or accurately) solving a smaller sub-problem of  X  B . Therefore, we can say that Algorithm 5 is a specially tweaked block CD that aims for multi-core environments.
In Algorithm 4, while the second stage is to cyclically update elements in the set B , the first stage is a gradient-and gradient-based settings are the two major ways in CD to select variables for update. The use of gradient motivates us to link to the popular decomposition methods for kernel SVM (e.g., [3, 8, 12]), which calculate the gradient and select a small subset of variables for update. It has been explained in [5, Section 4] why a gradient-based rather than a cyclic variable selection is useful for kernel classifiers, so we do not repeat the discussion here. Instead, we would like to discuss the BSVM package [7] that has recognized the importance of maintaining w for the linear kernel; 4 see also [9, Section 4]. After calculating  X  f (  X  ), BSVM selects a small set B (by default | B | = 10) by the following procedure. Let r be be the number of elements to be selected, and The set B includes the following indices. 1. The largest min( | B | / 2 ,r ) elements in v that correspond to  X   X  X  free elements. 2. The smallest ( | B | X  min( | B | / 2 ,r )) elements in v . BSVM then updates  X  B by fixing all other elements and solving the following sub-problem. where N = { 1 ,...,l }\ B and (14) is reduced to the single-variable sub-problem in (3). We present a parallel implementation of the BSVM algorithm in Algorithm 6, which is the same as the current BSVM implementation except the parallel calculation of  X  f (  X  ) at line 3. Clearly, Algorithm 6 is a special case of the framework in Algorithm 5 with  X  B = { 1 ,...,l } .

While both Algorithms 4 and 6 are realizations of Algo-rithm 5, they significantly differ in how to update  X  B after selecting the working set B (line 6 of Algorithm 5). In [9], the sub-problem (14) is accurately solved by an optimization algorithm that costs
Maintaining w is not possible in the kernel case because it is too high dimensional to be stored. Algorithm 6 A parallel implementation of the BSVM al-gorithm [7] for linear classification 1: Specify a feasible  X  and calculate w = P j y j  X  j x j 2: while true do 3: Calculate  X  i f (  X  ) ,  X  i = 1 ,...,l in parallel 4: if  X  is close to an optimum then 5: break 6: Select B by the procedure in Section 3.3 7: Find d B by solving (14) 8: for i  X  B do 9: if | d i | X   X   X  then 10:  X  i  X   X  i + d i 11: w  X  w + d i y i x i operations, where | B | 2 n is for constructing the matrix and | B | 3 is for factorizing  X  Q BB several times. In contrast, from line 11 to line 17 in Algorithm 4, we very loosely solve the sub-problem (3) by conducting | B | number of CD up-dates. As a result, we can see the following difference on the two algorithms X  complexity: Here an inner iteration in Algorithm 4 means to handle one in Algorithm 6 because they both update elements in a set B eventually. Note that in (15) we slightly favor Algorithm 6 by assuming that solving the sub-problem (14) can be fully parallelized.

The complexity comparison in (15) explains why in the serial setting the cyclic CD [5] is much more widely used than the BSVM implementation [7]. When a single thread is used, Algorithm 4 is reduced to Algorithm 1 with P = 1 ,T = l and | B | = 1. Then (15) becomes Clearly the ln term causes each iteration of Algorithm 6 to be extremely expensive. Thus unless the number of itera-more than that of Algorithm 1. Now for the multi-core en-vironment, Algorithm 4 parallelizes the evaluation of |  X  l/T gradient components, and to use the latest gradient in-formation, |  X  B | cannot be too large (we used several hundreds or thousands in our experiments; see Section 4 for details.). In contrast, Algorithm 6 parallelizes the evaluation of all l components. Because the scalability is often better for the situation of a higher computational demand, we expect that Algorithm 6 benefits more from multi-core computa-tion. Therefore, it is interesting to see if Algorithm 6 be-comes practically viable. Unfortunately, in Section 5.2 we slower than Algorithm 4.
In this section we investigate theoretical properties and implementation issues of Algorithm 4, which will be used for subsequent comparisons with existing approaches. First we show the finite termination.
 Theorem 1 Under any given stopping tolerance  X  &gt; 0 , Al-gorithm 4 terminates after a finite number of iterations. Because of the space limitation, we leave the proof in Section I of supplementary materials.

Besides the finite termination under a tolerance  X  , we hope that as  X   X  0, the resulting solution can approach an opti-mum. Then the asymptotic convergence is established. Note that Algorithm 4 has another parameter  X   X   X  , so we also need  X   X   X  0 as well. Now assume that  X   X ,  X   X  is the solution after running Algorithm 4 under  X  and  X   X  , and The following theorem gives the asymptotic convergence. Theorem 2 Consider a sequence {  X  k ,  X   X  k } with If w  X  is the optimum of (1) , then we have
Next we discuss several implementation issues.
An effective technique demonstrated in [5] to improve the efficiency of dual CD methods is shrinking. This technique, originated from training kernel classifiers, aims to remove some elements that are likely bounded (i.e.,  X  i = 0 or U ) in the end. For the proposed Algorithm 4, the shrinking lated, we can apply conditions used in [5] to remove some elements in  X  B . After the stopping condition is satisfied on the remaining elements, we check if the whole set satisfies the same condition as well. A detailed pseudo code is given in Algorithm I of supplementary materials.
In Algorithm 4, the set  X  B is important because we paral-lelize the calculation of  X   X  B f (  X  ) and then select a set B  X  for CD updates. Currently we cyclically get  X  B after splitting { 1 ,...,l } to T blocks, but the size of  X  B needs to be decided. We list the following considerations. -|  X  B | cannot be too small because first the overhead in par-allelizing the calculation of  X   X  B f (  X  ) becomes significant, and second the set B selected from  X  B may be empty. current solution to select too many elements at a time for
CD updates. Without using the latest gradient informa-tion, the convergence may be slower.
 Fortunately, we find that the training time is about the same imental results in Section 5.1.2.

To avoid that | B | = 0 happens frequently, we further de-sign a simple rule to adjust the size of |  X  B | : if | B | = 0 then else if | B | X  init  X  B then The idea is to check the size of B for deciding if |  X  B | needs to be adjusted: If | B | = 0, to get some elements in B for Table 1: Data statistics: Density is the average ratio of non-zero features per instance. Ratio is the percentage of running time spent on the gradient calculation (line 8 of Algorithm 4); we consider the l 1 loss by using one core (see also the discussion in Section 5.1).
 Data set #data #features density ratio rcv1 677,399 47,236 0.15% 89% yahoo-korea 368,444 3,052,939 0.01% 86% yahoo-japan 176,203 832,026 0.02% 96% webspam (trigram) 350,000 16,609,143 0.02% 91% url combined 2,396,130 3,231,961 0.004% 86% KDD2010-b 19,264,097 29,890,095 0.0001% 86% covtype 581,012 54 22.12% 66% epsilon 400,000 2,000 100% 80% HIGGS 11,000,000 28 92.11% 85% CD updates, we should enlarge  X  B . In contrast, if too many elements are included in B , we should reduce the size of  X  upper bound. In our experiments, we set init  X  B = 256 and max  X  B = 4 , 096. Because in general 0 &lt; | B | &lt; init seldom changed in practice. Hence our rule mainly serves as a safeguard.
Algorithm 4 is  X  -dependent because of the condition users pick a very small  X  , then in the beginning of the algo-rithm almost all elements in  X  B are included in B . To make Algorithm 4 independent of the stopping tolerance, we have a separate parameter  X  1 , that starts with a constant not too we make the following changes: 1.  X  1 = 0 . 1 in the beginning. 2. The set B is selected by 3. The stopping condition is changed to Therefore, the algorithm relies on a fixed sequence of  X  values rather than a single value  X  specified by users.
We consider nine data sets, each of which has a large num-ber of instances. 5 Six of them are sparse sets with many features, while the others are dense sets with few features. Details are in Table 1.

In all experiments, the regularization parameter C = 1 is used. We have also considered the best C value selected by cross-validation. Results, presented in supplementary mate-rials, are similar. All implementations, including the one in
All sets except yahoo-japan and yahoo-korea are available at http://www.csie.ntu.edu.tw/  X cjlin/libsvmtools/datasets/. For covtype , both scaled and original versions are available; we use the scaled one. [5], are extended from the package LIBLINEAR version 2.1 [2] by using OpenMP [1], so the comparison is fair. The ini-tial  X  = 0 is used in all algorithms. In Algorithm 4, we set are conducted on Amazon EC2 m4.4xlarge machines, each of which is equivalent to 8 cores of an Intel Xeon E5-2676 v3 CPU.
We investigate various aspects of Algorithm 4. Some more results are in supplementary materials.
Our idea in Algorithm 4 is to make the gradient calcula-tion the most computationally expensive yet parallelizable step of the procedure. In Table 1, we check the percentage of total training time spent on this operation by using a single core and the stopping tolerance  X  = 0 . 1. 6 The l 1 loss is con-sidered. Results indicate that in general more than 80% of time is used for calculating the gradient. Hence the running time can be effectively reduced in a multi-core environment. 5.1.2 Size of the Set  X  B
An important parameter to be decided in Algorithm 4 is the size of the set  X  B ; see the discussion in Section 4.2. To and II of supplementary materials, we compare the running the adaptive rule in Section 4.2 in order to see the effect of different |  X  B | sizes.

Results show that |  X  B | = 64 is slightly worse than 256 and 1 , 024. For a too small |  X  B | , the parallelization of  X  is less effective because the overhead to conduct parallel op-erations becomes significant. On the other hand, results of using |  X  B | = 256 and 1,024 are rather similar, so the selection of |  X  B | is not difficult in practice.
We briefly compare Algorithms 4 and 6 because they are two different realizations of the framework in Algorithm 5. We mentioned in Section 3.3 that Algorithm 6 use all gradi-ent elements to greedily select a subset B , but Algorithm 4 is closer to the cyclic CD.

In Table 2, we compare them by using two sets. The subset size | B | = 10 is considered in Algorithm 6, while for Algorithm 4, the initial | B | = 256 is used for the adaptive rule in Section 4.2. A stopping tolerance  X  = 0 . 001 is used for both algorithms, although in all cases Algorithm 4 reaches that increasing the number of cores from 1 to 8 leads to more significant improvement on Algorithm 6. However, the overall computational time is still much more than that of Algorithm 4. This result is consistent with our analysis in Section 3.3. Because Algorithm 4 is superior, subsequently we use it for other experiments. We compare the following approaches. -Mini-batch CD [13]: See Section 2.2.1 for details. -Asynchronous CD [6]: We directly use the implementa-tion in [6]. See details in Section 2.2.2. The value 0.1 is the default stopping tolerance used in LIBLINEAR . Table 2: A comparison between Algorithms 4 and 6 on the training time (in seconds). A stopping toler-ance  X  = 0 . 001 is used.
 -Algorithm 4: the proposed multi-core dual CD algorithm in this study. -LIBLINEAR [2]: It implements Algorithm 1 with the shrink-ing technique [5]. This serial code serves as a reference to compare with the above multi-core algorithms.
 To see how the algorithm behaves as training time increases, we carefully consider non-stop settings for these approaches; see details in Section VII of supplementary materials.
We check the relation between running time and the rel-ative difference to the optimal objective value: where f (  X  ) is the objective function of (2). Because  X  not available, we obtain an approximate optimal f (  X   X  ) by running LIBLINEAR with a small tolerance  X  = 10  X  6 .
Before presenting the main comparisons, by some experi-ments we rule out mini-batch CD because it is less efficient in compared with other methods. Details are left in Supple-mentary Section III.
 We present the main results of using l 1 and l 2 losses in Figures 1 and 2, respectively. To check the scalability, 1, 2, 4, 8 cores are used. Note that our CPU has 8 cores and all three approaches apply the shrinking technique. Therefore, the result of asynchronous CD may be slightly different from that in [6], where shrinking is not applied. From Figures 1 and 2, the following observations can be made. -For some sparse problems, asynchronous CD gives excel-lent speedup as the number of cores increases. However, it fails to converge in some situations ( url combined and covtype when 8 cores are used). For all three dense prob-lems with l 1 or l 2 loss, it diverges if 16 cores are used. -Algorithm 4 is robust because it always converges. Al-though the scalability may not be as good as asynchronous
CD in the beginning, in Figure 1 it generally has faster final convergence. Further, Algorithm 4 achieves much better speedup for problems url combined and covtype , where asynchronous CD may diverge. -In compared with the serial algorithm in LIBLINEAR , we can see that Algorithm 4 using one core is slower. How-ever, as the number of cores increases, Algorithm 4 often becomes much faster. This observation confirms the im-portance of modifying the serial algorithm to take the advantage of multi-core computation, where our discus-sion in Section 3 serves as a good illustration. -Results for l 1 and l 2 losses are generally similar though we can see that for all approaches, the final convergence for the l 2 loss is nicer. The curves of training time versus the objective value are sometimes close to straight lines. -The resulting curve of Algorithm 4 may look like a piece-wise combination of several curves. This situation comes from the reduction of the  X  1 parameter; see Section 4.3.
We have also compared these methods without applying the shrinking technique. Detailed results are in Section V of supplementary materials.

It is mentioned in Section 2.2.2 that to address the con-vergence issue of the asynchronous CD method, the study in [14] considers a semi-asynchronous setting. We modify the code in [6] to have that w is recalculated by (6) after each cycle of using all x i ,  X  i = 1 ,...,l . The computational time is significantly increased, but we observe similar behavior. For problems where the asynchronous CD method fails, so does the new semi-asynchronous implementation. Therefore, it is unclear to us yet how to effectively modify the asynchronous CD method so that the convergence is guaranteed.
Before making conclusions we discuss issues including lim-itation and future challenges of the proposed approach. Our current development is for the environment of a single CPU with multiple cores. We find that if multiple CPUs are used (i.e., the NUMA architecture in multi-processing), because of the communication between CPUs. Assume two CPUs are available: CPU-1 and CPU-2. When  X   X  B f (  X  ) is x i may be loaded into the cache of CPU-2 for calculating  X  f (  X  ). Later if x i is selected to the set B and CPU-1 is utilized to sequentially conduct CD updates on elements in B (line 11 of Algorithm 4), then x i must be loaded from memory or transferred from the cache of CPU-2. How to design an effective parallel dual CD method for multi-CPU environments is an important future issue.

Our recent study [11] on parallel Newton methods for the primal problem with l 2 and LR losses easily achieves ex-cellent speedup in multi-CPU environments. In compared with Algorithm 4, a Newton method possesses the following advantages for parallelization. -For every operation the Newton method uses the whole data set, so it is like that the set  X  B in Algorithm 4 be-comes much bigger. Then the overhead for parallelization is relatively smaller. -In the previous paragraph we discussed that in a loop of Algorithm 4 an x i may be accessed in two separate places.
Such situations do not occur in the Newton method, so the issue of memory access or data movement between CPUs is less serious.

Nevertheless, parallel dual CD is still very useful because of the following reasons. First, in the serial setting, dual CD is in some cases much faster than other approaches includ-ing the primal Newton method, so even with less effective parallelization, it may still be faster. Second, for the l 1 loss, the primal problem lacks differentiability, so solving the dif-ferentiable dual problem is more suitable. Because the dual problem possesses bound constraints 0  X   X  i  X  U,  X  i, un-constrained optimization methods such as Newton or quasi-Newton cannot be directly applied. In contrast, CD meth-ods are convenient choices for such problems.
It is known that both projection operation defined as These two conditions are respectively used in lines 9-10 and lines 13-14 of Algorithm 4. An interesting question is why we do not just use one of the two.

In optimization,  X   X  P [  X   X   X  f (  X  )] is often considered more suitable because it gives a better measure about the optimality when  X  i is close to a bound. For example, Clearly  X  i cannot be moved much, so  X  i  X  P [  X  i  X  X  X  i f (  X  )] rightly indicates this fact. Therefore, it seems that we should project gradient mainly because of historical reasons. The dual CD in LIBLINEAR currently relies on project gradient for implementing the shrinking technique and so does the asynchronous CD [6] used for comparison. Hence we follow them for a fair comparison. Modifying Algorithm 4 to use  X   X  P [  X  i  X  X  X  i f (  X  )] is worth investigating in the future.
In this work we have proposed a general framework for parallel dual CD. For one specific implementation we estab-lish the convergence properties and demonstrate the effec-tiveness in multi-core environments.
 Acknowledgements This work was supported in part by MOST of Taiwan via the grant 104-2221-E-002-047-MY3. [1] L. Dagum and R. Menon. OpenMP: an industry [2] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [3] R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set [4] C. Hildreth. A quadratic programming procedure. [5] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, [6] C.-J. Hsieh, H.-F. Yu, and I. S. Dhillon. PASSCoDe: [7] C.-W. Hsu and C.-J. Lin. A simple decomposition [8] T. Joachims. Making large-scale SVM learning [9] W.-C. Kao, K.-M. Chung, C.-L. Sun, and C.-J. Lin. [10] C.-P. Lee and D. Roth. Distributed box-constrained [11] M.-C. Lee, W.-L. Chiang, and C.-J. Lin. Fast [12] J. C. Platt. Fast training of support vector machines [13] M. Tak  X a X c, P. Richt  X arik, and N. Srebro. Distributed [14] K. Tran, S. Hosseini, L. Xiao, T. Finley, and [15] T. Yang. Trading computation for communication:
