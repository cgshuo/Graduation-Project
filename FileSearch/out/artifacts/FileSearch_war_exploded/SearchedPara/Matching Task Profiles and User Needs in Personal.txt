 Personalization has been deemed one of the major challenges in information retrieval with a significant potential for pro-viding better search experience to individual users. Espe-cially, the need for enhanced user models better capturing elements such as users X  goals, tasks, and contexts has been identified. In this paper, we introduce a statistical language model for user tasks representing different granularity levels of a user profile, ranging from very specific search goals to broad topics. We propose a personalization framework that selectively matches the actual user information need with relevant past user tasks, and allows to dynamically switch the course of personalization from re-finding very precise in-formation to biasing results to general user interests. In the extreme, our model is able to detect when the user X  X  search and browse history is not appropriate for aiding the user in satisfying her current information quest. Instead of blindly applying personalization to all user queries, our approach refrains from undue actions in these cases, accounting for the user X  X  desire of discovering new topics, and changing in-terests over time. The effectiveness of our method is demon-strated by an empirical user study.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  relevance feedback, retrieval models Experimentation, Human Factors, Algorithms
Search personalization has been pursued in many ways, in order to provide better result rankings and better over-all search experience to individual users. However, blindly applying personalization to all user queries, for example, by a background model derived from the user X  X  long-term query-and-click history, is not always appropriate for aiding the user in accomplishing her actual task. User interests change over time, a user sometimes works on very different categories of tasks within a short timespan, and history-based personalization may impede a user X  X  desire of discov-ering new topics. In this paper we propose a personalization framework that is selective in a twofold sense. First, it se-lectively employs personalization techniques for queries that are expected to benefit from prior history information, while refraining from undue actions otherwise. Second, we intro-duce the notion of tasks representing different granularity levels of a user profile, and base our reasoning selectively on query-relevant user tasks. Modeling the whole spectrum of past user tasks, ranging from very specific tasks such as find-ing a hotel in Borneo to the rather general user interest into traveling, allows to adapt the scope of the employed person-alization scheme to the user X  X  current search mode. Elbas-suoni et al. [12, 23] identified three major search modes: re-finding known information , finding out about topics of user interest , and following an ad-hoc information need . By dy-namically varying the scope of personalization based on the granularity of the best-matching past user task, our method inherently supports and balances the different search goals a user might pursue. It may assist the user on re-finding a previously searched hotel address, or ameliorate ambiguous search results by respecting her interest in traveling. Our reasoning is cast into a sound theoretical model based on statistical language models for tasks and queries.
The outline of the paper is as follows. In Section 2 we review related work. We introduce the architecture of our client-side personalization system in Section 3.1, and detail our personalization framework in Section 3.2. In Section 4 we present experimental results from an empirical user study showing that our approach achieves statistically significant gains in performance over both opponents, Google and a non-selective personalization approach. Also, we back the benefit of structuring the user profile in terms of tasks by comparing our task-aware personalization scheme to a per-sonalization which is based on a rather monolithic user pro-file. In Section 4.3 we study a variant of our personalization approach which emphasizes the immediate session context whenever possible. In addition to basing personalization on a re-ranking of the top-50 Google results, we consider apply-ing query expansion when appropriate in Section 4.4. We conclude by pointing out ways of learning optimal param-eters to our model in a running system (Section 4.5), and discussing the computational complexity of our method at query-processing time (Section 5).
Our work touches various fields. In the following we re-view related work in personalization, query performance pre-diction, and query expansion.
 sonalizing Web search. Due to space limitations we give a high-level categorization of what has been done along with some exemplary references which are, however, not meant to be exhaustive.

One way of personalizing search is by means of implicit user relevance feedback. Approaches along these lines in-clude [29, 26] which also inspired the work presented in this paper. They achieve personalization by a client-side re-ranking of Web search results based on the previous user search and browse behavior. However, each one tackles a single facet of personalization, either biasing search results to general user-interests [29] or respecting the current search session X  X  context [26], while we unify both aspects and dy-namically switch between these two search modes. Elbas-suoni et al. [12] also aim at distinguishing the various user search modes, and do so by following a simplistic rule-based decision strategy. While their experimental results show im-provements for the search modes of re-finding known infor-mation and satisfying an ad-hoc information need, their per-sonalization approach shows query-dependent variations and less impressive performance gains when the user is exploring topics of user interest. This user search mode is especially hard as imposing some general topic bias on queries is error-prone and its usefulness is highly query-dependent. Our ap-proach improves on theirs by more carefully differentiating when a biasing of search results towards general user inter-ests is beneficial. Kim and Chan [19] have published yet another personalization approach employing re-ranking of search results from the client side. Our approach and theirs have in common that both methodologies view the user X  X  profile as a hierarchy of general to specific interests. While we reason on tasks based on the user X  X  search history, they employ a hierarchical clustering of terms based on the user X  X  bookmarks. A recent work by Teevan et al. [30] performs a large-scale query-log analysis on implicit measures for pre-dicting query ambiguity. They correlate implicit ambiguity measures with the level of variation found in explicit rele-vance assessments of users on the same queries, and point out the potential for personalization on ambigious queries.
Another path of addressing personalization is by the cat-egorization of both user interests and search results and a biasing of search results according to some similarity mea-sure on these categories. Approaches along these lines in-clude [22, 8, 31]. Personal biases inside the state-of-the-art link analysis algorithms such as PageRank [3] and HITS [20] provide a further means to shift search results according to user interests. E.g., Haveliwala [15] has been the first to propose biasing the random jumps inside the PageRank al-gorithm towards pages of user interests. Qiu and Cho [24] further extend this idea by automatically learning topic pref-erences from the user search behavior. Some personalization techniques not only consider a single user, but also take the actions of a surrounding group of users into account, e.g., Sugiyama et al. [27] follow a collaborative filtering approach. a query might benefit from personalization, we are actu-ally interested in predicting the performance difference be-tween the non-personalized and the personalized search re-sult ranking. Reasonings in similar vein occur in the tradi-tional web search setting when estimating the difficulty or ambiguity of a query. Cronen-Townsend et al. [9] introduce query clarity defined as the Kullback-Leibler divergence be-tween the query and the collection language model. The rationale behind is that a query with high divergence from the collection model is of high coherence and little ambigu-ity, a query having a similar term distribution to the collec-tion is, however, most probably on more than one topic. A direct translation of query clarity to the setting of a client-side personalization would measure the divergence of the query from the user history profile. Then a high divergence score might be either due to the query being on a topic not strongly represented in the profile, or the query being in fact very coherent. Still, such an approach would reason on the user profile in its entirety, not distinguishing between the possibly highly diverse user interests.

A comparative study on the effectiveness of query clarity, query length, and measures reasoning on the inverse docu-ment frequencies of query terms for predicting query per-formance has been done by He and Ounis [16]. They find query clarity and the standard deviation of the idf X  X  of query terms to perform best. Grivolla et al. [13] view query per-formance prediction as a classification problem. As features they consider, e.g., IR ranking scores, entropy and mean cosine similarity of the top result set documents. None of the features shows a perfect correlation with average pre-cision, however, still most of them are useful, especially in combination with other features.
 process of adding terms to the original query. However, in a more general sense, it also refers to methods of query re-formulation, i.e., any kind of transformation applied to a query to facilitate a more effective retrieval. Based on the data that serves as the knowledge base for transforming the query, existing methodologies can be categorized based on whether they rely on relevance feedback , either explicit, im-plicit or pseudo, collection-based co-occurrence statistics , or thesaurus information . Similar to personalization, the per-formance of query expansion is highly query-dependent. It might improve some queries, while harming others. This lately led to the emergence of approaches that try to auto-matically determine when query expansion is worthwhile [1, 21, 5, 4]. E.g., Cronen-Townsend et al. [10] study a selective query expansion approach that automatically decides when it is beneficial to expand a query based on a comparison be-tween the ranked lists for the original and expanded query. Chirita et al. [7] consider an adaptive personalized query expansion by varying the number of expansion terms based on an empirically determined function of query scope with respect to the user profile and the query clarity on the web. However, this approach always interprets a user profile in its entirety, thus adapting only the number of expansion terms dynamically instead of the whole process of term selection.
To the best of our knowledge we are the first to devise a full-fledged personalization model encompassing adaptivity as an integral part of the model.
Before we introduce our personalization framework, we in-troduce the key characteristics of our personalization archi-tecture. As especially the browsing activities beyond search are outside the reach of a search engine, client-side solutions are favorable. Moreover, as all user data is kept locally, user privacy is not violated. We therefore set up a client-side search personalization with the use of a proxy which is run-ning locally. It intercepts all HTTP traffic, extracts queries, query chains , i.e., subsequently posed queries, result sets, clicked result pages, as well as the whole clickstream of sub-sequently visited web pages, and stores this information to a local database file which we refer to as the user profile in the following. Accordingly, searches with Google (the same approach can be easily applied to any other search engine as well) are intercepted and search results are re-ranked ac-cording to personal preferences. We preferred a proxy over implementing a plugin for browser-independence. Moreover, the proxy is broader applicable as it may bundle several users and thus achieve biasing of search towards community interests. The proxy we are using relies on the UsaProxy im-plementation [2] that enhances all html files passed through by some Javascript code that sends logging information on events such as the load of page, mouse movements, etc back to the proxy.

For the following discussion we define our notion of a search session which is based on heuristics about the user X  X  timing as well as the relatedness of subsequent users X  ac-tions. User actions are (1) queries, (2) result clicks, and (3) other page visits. All successive actions are considered to be within the same search session as long as their similarity ex-ceeds a certain predefined threshold. When computing the similarity of subsequent actions, a query is represented by the centroid of the top-50 result snippets.
 than the typical user is likely to view (50 results). When-ever a user action allows to update the query representation, unseen results are re-ranked. E.g., this is the case when the user submits a query or when she presses the  X  X ext X  link to view more results. Yet we refrain from re-ranking when the user returns to a seen result list using the  X  X ack X  button, as we perceived this more as irritating than as advantageous. to rewrite the query sent to the search engine, thus giving personalization most probably a larger impact by not only re-ordering the original results, but also retrieving a poten-tially very different result set.
 to incorporate the query-independent web page importance, personalized result ranks and original web ranks (as an ap-proximation for the real page rank) are aggregated to form the final result ranking. Inspired by rank aggregation meth-ods for the web presented in [11], we use Borda X  X  method to combine the two result rankings. Thereby each result item is assigned a score corresponding to the number of re-sults ranked below it. Then the total score of a result is a weighted sum of its scores with respect to each ranking. Figure 1: Selective and task-aware personalization framework.
We propose a framework based on fine-grained language models for units of a user X  X  past search and browse behav-ior, coined tasks . They reflect the non-homogeneous, various aspects of user interests which might, e.g., range from the specific task of searching a hotel in Borneo to the general interest in traveling. We obtain tasks by means of a hierar-chical clustering of the user X  X  profile. The atomic units to be clustered are past sessions which consist of cohesive query chains, i.e., subsequently posed queries including their re-sult clicks and further browsed documents within the same session, or query-independently browsed documents. Thus we represent the user profile as the full dendogram of tasks which reaches from small tasks consisting only of a single ses-sion, to the largest task encompassing the whole user search and browse history.

Similarly, for each query, we perform a hierarchical clus-tering of the query X  X  result set items (each represented by its title and snippet information) to obtain candidate query facets F 1 , . . . , F m which represent the different aspects the query might span. Then our approach indirectly determines the ambiguity level of a query, and its presence in the user profile as follows (also see Figure 1 for visualization). cases: either (1) the current query is the first query in the current session, or (2) there exists some query history al-ready, and the current query is a refinement of a previously issued query in the same search session. In the first case we retrieve the top-k tasks T 1 , . . . , T k most similar to the query from the user X  X  profile. In the latter case the tasks present in the user profile are accompanied by a current task made up by all the actions of the currently active session, and represented by the language model T k +1 being just a special incarnation of a task language model.
Each obtained query facet and each task is represented by a unigram language model. Considering the Kullback-Leibler (KL) divergence between a query facet F i and a task T we determine the facet-task pair ( F  X  i , T  X  j ) with the lowest Kullback-Leibler (KL) divergence (we approximate KL ( F i by considering only those terms present in F i  X  T j ). That way we learn the query facet F  X  i closest to the user X  X  interests, which represents the most probable meaning of the query in case of ambiguity. At the same time, the task T  X  j represents the best-matching part of the user profile to the query facet F . The Kullback-Leibler divergence between between F  X  i and T  X  j further characterizes the strength of their similarity. If KL ( F  X  i , T  X  j ) is larger than a threshold  X  , we conclude that the current query goes for a previously unexplored task, and thus refrain from biasing the search results. Otherwise we might either reformulate the query sent to Google or re-rank the original search results as described in the next section. question how to personalize when personalization has been deemed useful for the current query, that is we are given a facet of the current query F  X  i and a task T  X  j most similar to each other. We update the query representation with terms best discriminating the query facet F  X  i from all other query facets, while being most similar to the task T  X  j , i.e., terms which have the largest impact on the KL-divergence between the union of the chosen facet-task pair and the remaining query facets, for which thus with holds. Based on the KL divergence between the new query representation and the result items X  language models esti-mated from their title and snippet information, we re-rank the original results. Thus in case of ambiguity, the query is biased towards the facet with the best-matching counter-part in the user X  X  profile. We introduce a third threshold  X  to allow for an automatic reformulation of the query sent to Google whenever we find strong terms. Terms for which v ( w ) &lt;  X  qualify for query expansion, while terms with  X   X  v ( w ) &lt;  X  and P ( w | S i F i ) &gt; 0 qualify for re-ranking the original top-50 search results. That way the use of the more invasive query expansion is restricted to queries for which there is a certain level of confidence in its success.
Alternatively to selecting terms based on their contribu-tion to the KL-divergence as just described, we consider ranking terms based on their probability in the facet-task pair, i.e., v ( w ) = P ( w | F i  X  T j ), as well as, thresholding the number of terms to be selected.
The language model of a user task is a weighted mixture of its components: queries, result clicks, clickstream docu-ments and query-independent browsed documents. Let Q be a query language model, and B a language model of query-independently browsed documents. Then the task language model T is obtained as where w denotes a word in the vocabulary. While B is the average of the individual browsed documents X  language mod-els, Q is the uniform mixture of the task X  X  query chains as follows. Let QC denote a query chain Q 1 , Q 2 , . . . , Q Q k is the last query in the chain. Then P ( w | Q ) = 1 , i.e., a task X  X  queries X  language model is the average of all query chains X  models. Each query chain is modeled by a weighted sum of its constituent queries, weighted in such a manner that queries later in the chain are considered more important than queries early in the chain. The rationale behind this is the assumption that queries submitted later in the session, are better matches to the user information need, and thus better characterize the search task X  X  intention. Let r ( ) denote the rank of a search result document, and CR be the set of clicked result items of the total set of result items R. Among the non-clicked result items, we further dis-tinguish intentionally non-clicked documents ranked above a clicked one, N R = { d  X  R,  X  c  X  CR : r ( d ) &lt; r ( c ) } , and unseen results ranked below the lowest-ranked clicked item, U R = { d  X  R,  X  c  X  CR : r ( d ) &gt; r ( c ) } . Furthermore, let CS be the set of clickstream documents beyond the result docu-ments. Then the individual query models can be estimated as , i.e., a single query is represented by a mixture of its ac-tual query terms (q denotes the query string), its clicked documents, its intentionally non-clicked documents ranked above clicked ones, its unseen result documents ranked be-low all clicked ones, and all the documents the user visited starting from clicked result set documents. The constituent language models for both clicked and clickstream documents are a uniform mixture of their document language models, and similarly the language models for unseen and intention-ally non-clicked results are uniform mixtures of their indi-vidual result models estimated from their title and snippet text. All constituent language models employ Dirichlet prior smoothing, i.e., where c ( w, ) denotes the frequency of word w in ( ), Coll is short for collection which amounts to the user profile in our setting, | . | is the overall length of the document or the collection, and  X  is typically 2000 (see [32] for more details).
User #Queries #Browsed documents #Sessions
Similarly, the facet language model F is the uniform mix-ture of the result snippets s  X  F present in the facet, i.e.,
In order to evaluate the effectiveness of our proposed ap-proach, we asked 7 volunteers (computer scientists at our institute) to install our proxy on their local machines to log their search and browsing activities for a period of 2 months. Participants were free to opt out of the logging to protect their privacy. Due to holiday seasons, the period actually covered is approximately 6 weeks. The range of users spans from highly active to only sporadically searching and brows-ing the web. Thus, also the amount of logged user history varies accordingly. Table 1 shows the respective log sizes in terms of number of distinct query strings issued, number of web documents viewed, and number of sessions. These numbers already show the high divergence of web interaction behavior between users, e.g., user 1 uses primarily search to access web content, whereas user 5 has a stronger emphasis on direct or browsing-based navigation.

During a one-week evaluation phase, each participant eval-uated 8 self-chosen search tasks, among which at least two were asked to reflect topics searched during the logging pe-riod. A search task is a sequence of queries, click and brows-ing actions until the user X  X  information need is satisfied. We assisted users in remembering information they searched be-fore by presenting them the frequency-ordered terms of their logged query strings. During evaluation, we tried to mimic natural queries by having the participants evaluate their common-day queries at their own pace. The performed eval-uation search tasks consisted of 2 . 3 queries on average, from which the first, the last, and the next to last query if avail-able were subject to evaluation. For each task, the partic-ipant was presented with the top-50 Google results for the selected queries, merged and placed in random order in or-der to avoid result X  X  position bias. Then the participant was asked to mark each result as highly relevant, relevant or com-pletely irrelevant. Furthermore, we asked users to group the top-10 results of each query by giving labels to them. This was to generate the optimal clustering of search results as perceived by the individual user, and to decouple the evalu-ation of our personalization strategy from the quality of the search result set clustering. In total our experiment com-prises 59 search tasks, and 98 individual evaluation queries. We removed queries whose search results were lacking hu-man labels and queries that had no matching tasks in the user profile as no personalization could be carried out on these queries. These considerations left us with a set of 89 evaluation queries.
 To measure the ranking quality, we use the Discounted Cumulative gain (DCG) [17], which is a measure that takes into consideration the rank of relevant documents and al-lows the incorporation of different relevance levels. DCG is defined as follows where i is the rank of the result within the result set, and G( i ) is the relevance level of the result. We used G ( i ) = 2 for highly relevant documents, G ( i ) = 1 for relevant ones, and G ( i ) = 0 for non-relevant ones. Dividing the obtained DCG by the DCG of the ideal ranking we obtain a normalized DCG which accounts for the variance in performance among queries. Table 2: Considered parameter settings for the task query language model.

During our experiments we studied various degrees of free-dom of our framework, such as the mean of selecting expan-sion terms, i.e., whether terms were ranked based on their probability in the chosen facet-task pair as opposed to their contribution to the KL-divergence between the chosen facet-task pair and the remaining query facets. Other parameters we varied are whether to enforce the inclusion of the origi-nal query terms in the course of query rewriting, as well as, the weight for aggregating original and personalized results using Borda X  X  method. All these general parameters were fixed among all users during all experiments.

Besides these general variations of our personalization ap-proach, we also experimented with different values for the weights in the task language model for each user separately to account for the diverse user styles of interacting with the Web. We varied the weight of queries with respect to browsed documents. We chose  X   X  X  0 . 5 , 0 . 7 , 1 } , thus test-ing the extreme points of giving equal importance to queries and independently-browsed documents, as opposed to giv-ing higher weight to queries up to ignoring independently-browsed documents when  X  = 1. Similarly we changed the influence of the individual components of the task query lan-guage model by giving different weights to  X  q ,  X  c ,  X  n  X  . These regulate the importance of query strings, clicked documents, non-clicked documents, unseen documents and query-dependent clickstream documents with respect to each other. Table 2 gives a summary of the parameter combina-tions in the task query language model we studied. In total, we restricted ourselves to 18 parameter combinations. Fi-nally, the threshold  X  which determines when personaliza-tion is employed, as well as, the threshold  X  which regulates the number of expansion terms on a query-to-query basis, or alternatively, a parameter fixing the number of expansion terms to be selected, could be tuned.
The rest of this section is divided as follows. We first present the experimental results we obtain when restricting our means of personalization to result re-ranking only. In ad-dition, we study another incarnation of our approach where we enforce the use of the current session task whenever pos-sible to account for the particular importance of the current session context. Then we present the results we obtain by also employing query expansion within our personalization framework. As a first proof-of-concept and as an indicator of the potential of our approach, we report in the following results based on the best performing parameters setting. We conclude by proposing a method to learn and adjust the op-timum values for these parameters automatically from the user X  X  profile.
We computed the NDCG at top-10 for the original Google results, enforced personalization results where personaliza-tion has been carried out for each query, and our selective personalization results where personalization has been per-formed only for queries whose KL divergence is below the threshold  X  . We experimented with two incarnations of our selective personalization approach. In the first case, which we refer to as Fixed , we adopted the same fixed number of expansion terms among all user queries, whereas in the other, coined Flexible , we determined the optimal threshold  X  which varies the number of expansion terms between dif-ferent queries. In both cases, in order to give the enforced personalization a fair chance and make results comparable, we chose the user-specific parameters that would maximize the improvement in NDCG between enforced personalized and original rankings. This would determine all user-specific parameters of the selective personalization approach except for the thresholds  X  and  X  which we chose such that the performance of selective personalization is maximized while keeping all other parameters fixed.
 Table 3: Average NDCG@10 for best parameter set-ting per user aggregated over all users with human generated query facets.
 Table 4: Average NDCG@10 for best parameter set-ting per user aggregated over all users with automat-ically generated query facets.

Tables 3 and 4 present the aggregated average perfor-mance. We chose the best performing parameters setting for each individual user independently, and then averaged the computed NDCG values over our 7 participants. Table 3 represents the results where we relied on the  X  X deal X  user clustering to construct query facets. Consequently, Table 4 represents the case where the automatic clustering of the search results was used instead.

Our selective personalization outperforms both the orig-inal ranking and the enforced personalization in the case of fixed number of expansion terms. We report significantly large improvements over the original ranking with one-tailed paired t-test p-values of 0.026 for human-labeled facets, and 0.024 for automatically generated facets. Also, the improve-ments over the enforced personalization are statistically sig-nificant with p-values of 0.023 (human) and 0.021 (auto-matic). Similarly, we obtained statistically significant im-provements for flexible number of expansion terms over the original Google ranking. The enforced personalization with flexible number of expansion terms already benefits from the selectivity in the number of expansion terms, and also outperforms the original Google ranking significantly with p-values of 0.048 (automatic) and 0.022 (human). We find only small differences between the results obtained when em-ploying a selective personalization with a fixed as opposed to a flexible number of expansion terms. Comparing the hierar-chical clustering approach for the generation of query facets to  X  X deal X  query facets as perceived by users, we find the au-tomatic approach to slightly outperform a human grouping indicating the validity of the hierarchical clustering approach for determining query facet candidates.

To evaluate the merit of a task-aware personalization ap-proach, we compare three variants of enforced personaliza-tion. As a baseline, we consider a personalization method-ology that is unaware of both query facets and tasks, which we refer to as  X  X nforced (no tasks) X . As an enhancement thereof, we also consider a variant that serves as an inter-mediate step and distinguishes between history tasks but still treats a query always in its entirety which we refer to as  X  X nforced (no facets) X . Comparing the enforced person-alization that utilizes both our notion of query facets and search history tasks to these two baselines indeed proves the concepts of user tasks and query facets beneficial. Table 5, we report the correlation between the improvement in NDCG of the enforced personalization with fixed number of expansion terms over the original ranking and the KL-divergence between the chosen facet-task pair KL ( F  X  i , T A negative correlation indicates that queries with more rel-evant information in the local index (i.e., with a low KL di-vergence) experience higher improvement in NDCG as com-pared to those that have less relevant information in the local index (i.e., with a high KL divergence). We find that users who benefit the most from personalization, i.e., with strongest improvement of selective personalization over the original ranking, also show a medium negative correlation between the improvement in NDCG of the enforced person-alization over the original ranking and KL ( F  X  i , T  X  j indicates the validity of our approach for predicting whether a query benefits from personalization.
 6 and 7 show the chosen user-specific parameters setting when using fixed and flexible number of terms, respectively. We only report the case where query facets were automat-Table 5: Pearson correlation (  X  ) between KL ( F  X  i , T and the improvement of enforced personalization (E) over the original ranking (O). Selective person-alization (S) and the percentage (%) of times it de-cides right. We report NDCG@10 for fixed num-ber of expansion terms and automatically-generated facets. ically generated. For the majority of users, browsed docu-ments play a role as  X  equals either 0.7 or 0.5 in most cases. Regarding the task query language models, the weights of its components varied among the different users supporting our intuition that these weights are indeed user-dependent. Table 6: Chosen parameter settings per user with automatically generated query facets and flexible number of terms Table 7: Chosen parameter settings per user with automatically generated query facets and fixed num-ber of expansion terms
For the general parameters that were fixed among all the users, we found the exclusion of the original query terms, term selection according to terms X  facet-task probabilities and the aggregation of original and personalized results with an equal weight of 0.5 to be the best performing parameter setting for selective personalization with both fixed and flex-ible number of expansion terms.
When comparing the performance of queries across the various time points in a session (see Figure 2), we find a large gap between the first query in a session and both the last and the query before the last in terms of performance of the original Google ranking (first: 0 . 33, last: 0 . 7 average NDCG@10). The difference in performance between the first and the last query in a session is significant with a one-tailed paired t-test p-value of 0.005. This indicates that indeed the user succeeds in improving result quality solely based on her query reformulation skills which motivates us to study a stronger consideration of session context within our personalization framework.
 Figure 2: Query performance gaps with respect to the query X  X  position within a search session.

Here, we report the results for a variant of our model where we enforce the use of the current session context as the chosen task whenever available. We again report the results where for each user, we chose the setting that op-timizes the enforced personalization NDCG, restricting the set of evaluated queries to those that have some current session context associated with. Table 8 represents the av-erage performance aggregated over all users. We find sta-tistically non-significant improvements of selective person-alization over both the original ranking and the enforced personalization for both the variant with fixed and the one with flexible number of expansion terms. The correlation be-tween the improvement in NDCG and the KL divergence is negative in both cases. We again highlight the importance of considering the different query facets by comparing the enforced personalization that takes into consideration these query facets, with the one that treats a query always in its entirety. As indicated in Table 8, the former enforced per-sonalization outperforms the personalization without facets. Comparing the selective personalization that freely chooses an appropriate user profile task to the one that enforces the use of the current session task on the reduced set of queries for which current session information is available, shows clearly the benefit of this simple heuristic to aid the selective personalization in selecting the right user task. Table 8: Average NDCG@10 for best parameter set-ting per user aggregated over all users with current session enforced and automatically generated query facets.
The results we have presented so far all rely on result re-ranking of the original user X  X  query result set. One drawback of result re-ranking is the high dependence on the perfor-mance of the original query. A query with a small recall, would not stand a high chance of improvement. On the other hand, automatic query expansion, if not applied care-fully, could harm the query performance. Our framework allows for judicious query expansion, reasoning on how well the expansion terms selected from the user X  X  facet-task pair are believed to present the current user X  X  search actions.
Revisiting Section 3.2, our personalization strategy allows for automatic query expansion, by augmenting the original user X  X  query with additional terms before forwarding it to the search engine. We only expand queries that are believed to benefit from personalization, i.e., queries for which the kl-divergence between the chosen facet-task pair is below the threshold value  X  as set during the re-ranking phase. For each qualifying query, we experimented with two variants of expansion methods. The first method, which we refer to as  X  X nforced expansion X  expands each qualifying query with kl-divergence below  X  with terms for which v ( w ) &lt;  X  . Next, we evaluate our selective personalization scheme as described in Section 3.2, where we combine both the mer-its of query expansion and result re-ranking. Queries that have expansion terms with scores v ( w ) &lt;  X  qualify for query expansion. If there are no such terms, we re-rank the orig-inal results using terms that have v ( w ) &lt;  X  , otherwise we refrain from personalization completely. We contrast these two variants employing query expansion, with the selective personalization which employs only re-ranking.

To evaluate query expansion, we only considered queries that were chosen for personalization by our selective ap-proach. We expanded each such qualifying query adding terms in incremental manner, and fetching the top-5 results for the expanded query. We only added terms whose v ( w ) were below the chosen threshold  X  , i.e, terms that were cho-sen by our selective re-ranking approach with flexible num-ber of terms, and presented the user with the merged set of results in random order. Similar to the case of re-ranking, each user had to assess each result being highly relevant, irrelevant or completely irrelevant. We restricted ourselves to only 5 results per query, in order to avoid overwhelming users with a large number of results. We also believe that it is sufficient to test the effect of query expansion on top-5, since query expansion substantially changes the result set.
Table 9 represents the performance of query expansion ag-gregated over all 7 users. The reported NDCG values are all at top-5. Both our selective personalization scheme, and the selective re-ranking outperform enforcing query expansion and the original Google results, yet the selective re-ranking has the largest gains.
 Table 9: Average NDCG@5 for query expansion with
In a running system, it is crucial to automatically deter-mine the values of the different system parameters. In [28], Taylor et al. discuss the lack of efficient learning algorithms for retrieval functions, and compare a greedy line search ap-proach that directly optimizes NDCG to a gradient descent approach optimizing the order of document pairs induced by the ranking function. Both of these methods are approxima-tions to doing an exhaustive search in the parameter space which becomes infeasible with a large number of parameters. Our case is even more involved since some of the parameters we want to learn are not a direct component of the ranking function, but indirectly influence the retrieval function by influencing the number and choice of terms added to the query representation.

The experimental results we presented so far are based on doing an exhaustive search on our set of test queries, thus providing us with an upper bound on the performance of our approach indicating its potential. In the following we discuss methods for determining optimal parameters in a running system. Most of the aforementioned work assumes that we know an objective performance function, such as NDCG, for each parameter setting we might want to con-sider. This, however, requires to ask users for explicit rel-evance assessments which the user usually is reluctant to give. Thus methods for implicitly determining the optimal parameters from the user X  X  search behavior are needed.
In the following, we present our experimental results for learning parameters from explicit user relevance assessments, as well as, implicit user feedback. For each participant, we extracted the last 8 queries that were recorded during the logging period, and used these queries as a training set. For each selected query, we ensured that the user had clicked on at least one result ranked below the first one. The ra-tionale behind this is to ensure that there is a potential for improvement over the original ranking. We also asked our participants for relevance assessments of the top-10 results for each query in the training set.
 rameter values which maximize the improvement in NDCG between enforced personalized and original rankings on the training set. Applying the parameters learnt on this training set on our test queries set, we obtain evaluation results as reported in Table 10. Even though, the learnt parameters do not fully generalize, selective personalization still improves over enforced personalization when using fixed (p-value = 0.046) or flexible (p-value = 0.055) number of terms. Table 10: Average NDCG@10 for best parameter setting per user aggregated over all users with ex-plicitly learnt parameters. information to learn parameters implicitly on the same train-ing set. As most queries occur only once, methods such as the one by Carterette et al. [6] which estimate DCG based on click rates of a significant number of users are not appli-cable to our client-side personalization setting. Thus we ex-periment with a naive heuristic which chooses parameters in such a way that clicked results would be ranked the highest. We obtain an estimate of the expected performance of this heuristic by comparing the NDCG of the original rankings in our training set, with the ones we obtain by re-shuffling results in such a way that either (1) all clicked results are ranked highest followed by the remaining ones in original or-der, or (2) clicked results are ranked top, followed by unseen results, and intentionally non-clicked results at the bottom. Averaging the NDCGs over the training queries of all users yields an average NDCG of 0.863 for the original ranking, 0.897 for the first re-ordering heuristic and 0.857 for the sec-ond one. The differences between original and heuristic 1 are statistically significant with a one-tailed paired t-test p-value of 0.031, and also the performance gap between the two heuristics are statistically significant with a p-value of 0.004. This led us to choose the first heuristic for learning parameters. Its performance improvement over the original ranking opens up potential for learning parameters that im-prove over the original ranking. At the same time, these numbers raise awareness of the much smaller margin of po-tential improvement as opposed to what might be learnt from explicit ratings which optimize parameters towards a targeted NDCG of 1. This indeed indicates the challenge of deducing relevance assessments from clicks, which suffers from result position bias [18], and noisy user click behavior [25].

Table 11 shows the results of using the parameter set-tings learnt by relying on implicit user feedback on our test set. Similarly as with explicit learning, our selective ap-proach does not fully generalize to the test evaluation set, however, still outperforms enforced personalization in both cases when using fixed (p-value = 0.013) or flexible (p-value = 0.002) number of terms. Increasing the size of the train-ing set, we expect to be less susceptible to such problems. However, the moderate sizes of our user profiles limited us to experiment with a rather small training set.
 Table 11: Average NDCG@10 for best parameter setting per user aggregated over all users with im-plicitly learnt parameters.
Tasks are computed offline. In a running system, the hierarchical clustering of past user sessions would be re-computed periodically. To fold in new sessions without mak-ing a complete re-computation necessary, incremental clus-tering approaches such as [14] could be employed. To an-alyze the complexity of our approach at query-processing time, we empirically compare the relative contribution of each query-processing step to the overall latency incurred by our personalization approach. We picked an exemplary user, and averaged the runtime of each processing step over her evaluation queries (see Table 12).

Clearly, the hierarchical clustering of the top-10 search results, the retrieval of the top-k tasks best matching the query string and the construction of their language mod-els, as well as, the pairwise KL-divergence computations between all candidate facet-task pairs are the largest bot-tlenecks. We considered replacing the hierarchical cluster-ing here with a flat clustering approach which requires to specify an optimal number of clusters n. We experimented with methods for determining the optimal number of clus-ters. These, however, mostly involve heavy computations as well such as computing clusterings for all possible values of n. Also, the predictions of n we obtained exhibited only medium Pearson correlation with the number of clusters ob-tained from human labellings.

The time complexity of the retrieval and construction of the top-k tasks can be traded against additional space com-plexity for fully precomputing all possible task language models. In our experiments we pre-computed term scores only at the session level, and at query-time computed the task language model by aggregation over the constituent ses-sions.

The complexity of the pairwise KL-divergence computa-tion can be reduced by limiting the number of candidate pairs to be considered. This might be either achieved by choosing a smaller k or reducing the number of query facet candidates. At query-time we perform a hierarchical cluster-ing of the top-10 result snippets. Instead of considering the full hierarchical cluster dendogram, we could remove some of its levels. Yet another option would be to reduce the number of terms in the summation of the KL-divergence computa-tion by approximating the task and facet language models by their most contributing terms, and ignoring terms be-low some probability threshold. We would, however, need to carefully study the effect of these approximations on per-sonalization performance.

We have proposed a thorough language modeling frame-work that explicitly addresses user tasks and carefully matches the current user needs with appropriate past user tasks for an amelioration of search results. Our novel task model takes into consideration more user centered feedback infor-mation, which does not only rely on past viewed documents, but also incroporates past queries, as well as clickstream be-havior. We introduced the notion of selectivity that enables us to avoid the pitfalls of most personalization systems that omit irregularities in users needs. Our framework also com-bines both result re-ranking and query expansion, selectively choosing the appropriate means of personalization. We pre-sented an empirical user study where our method achieved statistically significant gains over both the original Google ranking and traditional personalization approaches.
During our experiments, we relied on single terms to present the vocabulary of both the task and the facet language model. However, we encountered limits of a mere bag-of-words model to truly capture user interests. Currently, we are analyzing term correlations, and experimenting with ways of augmenting the language models with correlated compounds of terms to capture dependencies between words to further improve the selection process of expansion terms.
Furthermore, we currently always look for a single best-matching task-facet pair. However, we actually could also consider how close the next most matching pairs are and al-low more than one pair to qualify. For example, consider the following scenario: a user is both a Java programmer and has traveled to the island Java. However, the programming makes up a larger part of the history. Then in our frame-work, the query  X  X ava X  is biased towards the programming interest, where it actually should recognize that the user has two diverse almost equally strong interests. [1] G. Amati, C. Carpineto, and G. Romano. Query [2] R. Atterer, M. Wnuk, and A. Schmidt. Knowing the [3] S. Brin and L. Page. The anatomy of a large-scale [4] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. [5] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. [6] B. Carterette and R. Jones. Evaluating search engines [7] P.-A. Chirita, C. S. Firan, and W. Nejdl. Personalized [8] P.-A. Chirita, W. Nejdl, R. Paiu, and C. Kohlsch  X  utter. [9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [10] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar. [12] S. Elbassuoni, J. Luxenburger, and G. Weikum. [13] J. Grivolla, P. Jourlin, and R. de Mori. Automatic [14] K. Hammouda and M. Kamel. Incremental document [15] T. H. Haveliwala. Topic-sensitive pagerank. In WWW , [16] B. He and I. Ounis. Inferring query performance using [17] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [18] T. Joachims. Optimizing search engines using [19] H. Kim and P. Chan. Personalized ranking of search [20] J. Kleinberg. Authoritative sources in a hyperlinked [21] G. Kumaran and J. Allan. Selective user interaction. [22] F. Liu, C. Yu, and W. Meng. Personalized web search [23] J. Luxenburger, S. Elbassuoni, and G. Weikum. [24] F. Qiu and J. Cho. Automatic identification of user [25] F. Scholer, M. Shokouhi, B. Billerbeck, and A. Turpin. [26] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [27] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [28] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, [29] J. Teevan, S. T. Dumais, and E. Horvitz.
 [30] J. Teevan, S. T. Dumais, and D. J. Liebling. To [31] Y. Xu, B. Zhang, Z. Chen, and K. Wang.
 [32] C. Zhai and J. Lafferty. A study of smoothing
