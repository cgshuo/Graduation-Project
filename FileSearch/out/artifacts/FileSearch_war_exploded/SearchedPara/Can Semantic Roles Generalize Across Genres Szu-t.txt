 Correctly identifying semantic entities and success-fully disambiguating the relations between them and their predicates is an important and necessary step for successful natural language processing applica-tions, such as text summarization, question answer-ing, and machine translation. For example, in or-der to determine that question (1a) is answered by sentence (1b), but not by sentence (1c), we must de-termine the relationships between the relevant verbs ( eat and feed ) and their arguments. (1) a. What do lobsters like to eat? An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the con-stituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, Prop-Bank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Mos-chitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M  X  arquez, 2005). However, there is still little con-sensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic la-bels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims about how any one verb X  X  arguments relate to other verbs X  arguments, or about general distinctions be-tween verb arguments and adjuncts.

However, there are several limitations to this ap-proach. The first is that it can be difficult to make inferences and generalizations based on role labels that are only meaningful with respect to a single verb. Since each role label is verb-specific, we can not confidently determine when two different verbs X  arguments have the same role; and since no encoded meaning is associated with each tag, we can not make generalizations across verb classes. In con-trast, the use of a shared set of role labels, such as thematic roles, would facilitate both inferencing and generalization.

The second issue with PropBank X  X  verb-specific approach is that it can make training automatic se-mantic role labeling (SRL) systems more difficult. A vast amount of data would be needed to train the verb-specific models that are theoretically mandated by PropBank X  X  design. Instead, researchers typically build a single model for the numbered arguments (Arg0, Arg1, . . . , Arg5). This approach works sur-prisingly well, mainly because an explicit effort was made to use arguments Arg0 and Arg1 consistently across different verbs; and because those two argu-ment labels account for 85% of all arguments. How-ever, this approach causes the system to conflate different argument types, especially with the highly overloaded arguments Arg2-Arg5. As a result, these argument labels are quite difficult to learn.
A final difficulty with PropBank X  X  current ap-proach is that it limits SRL system robustness in the face of verb senses, verbs or verb constructions that were not included in the training data, and the training data is all Wall Street Journal corpora. If a PropBank-trained SRL system encounters a novel verb or verb usage, then there is no way for it to know which role labels are used for which argument types, since role labels are defined so specifically. This is especially problematic for Arg2-5. Similarly, PropBank-trained SRL systems can have difficulty generalizing when a known verb is encountered in a novel construction. These problems can happen quite frequently if the training data comes from a different genre than the test data. This issue is re-flected in the relatively poor performance of most state-of-the-art SRL systems when tested on a novel genre, the Brown corpus, during CoNLL 2005. For example, the SRL system described in (Pradhan et al., 2005b; Pradhan et al., 2005a) achieves an F-score of 81% when tested on the same genre as it is trained on (WSJ); but that score drops to 68.5% when the same system is tested on a different genre (the Brown corpus). DARPA-GALE is funding an ongoing effort to PropBank additional genres, but better techniques for generalizing the semantic role labeling task are still needed.

In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking ad-vantage of VerbNet X  X  more consistent set of labels, we can generate more useful role label annotations with a resulting improvement in SRL performance on novel genres. 2.1 PropBank PropBank (Palmer et al., 2005) is an annotation of one million words of the Wall Street Journal por-tion of the Penn Treebank II (Marcus et al., 1994) with predicate-argument structures for verbs, using semantic role labels for each verb argument. In or-der to remain theory neutral, and to increase anno-tation speed, role labels were defined on a per-verb-sense basis. Although the same tags were used for all verbs, (namely Arg0, Arg1, ..., Arg5), these tags are meant to have a verb-specific meaning.
Thus, the use of a given argument label should be consistent across different uses of that verb, in-cluding syntactic alternations. For example, the Arg1 (underlined) in  X  X ohn broke the window X  is the same window that is annotated as the Arg1 in  X  The window broke X , even though it is the syntactic sub-ject in one sentence and the syntactic object in the other. However, there is no guarantee that an argu-ment label will be used consistently across different verbs. For example, the Arg2 label is used to des-ignate the destination of the verb  X  X ring; X  but the extent of the verb  X  X ise. X  Generally, the arguments are simply listed in the order of their prominence for each verb. However, an explicit effort was made when PropBank was created to use Arg0 for argu-ments that fulfill Dowty X  X  criteria for  X  X rototypical agent, X  and Arg1 for arguments that fulfill the cri-teria for  X  X rototypical patient. X  (Dowty, 1991) As a result, these two argument labels are significantly more consistent across verbs than the other three. But nevertheless, there are still some inter-verb in-consistencies for even Arg0 and Arg1. 2.2 VerbNet VerbNet (Schuler, 2005) consists of hierarchically arranged verb classes, inspired by and extended from classes of Levin 1993 (Levin, 1993). Each class and subclass is characterized extensionally by its set of verbs, and intensionally by a list of the arguments of those verbs and syntactic and seman-tic information about the verbs. The argument list consists of thematic roles (23 in total) and pos-sible selectional restrictions on the arguments ex-pressed using binary predicates. The syntactic infor-mation maps the list of thematic arguments to deep-syntactic arguments (i.e., normalized for voice alter-nations, and transformations). The semantic predi-cates describe the participants during various stages of the event described by the syntactic frame.
The same thematic role can occur in different classes, where it will appear in different predicates, providing a class-specific interpretation of the role. VerbNet has been extended from the original Levin classes, and now covers 4526 senses for 3769 verbs. A primary emphasis for VerbNet is the grouping of verbs into classes that have a coherent syntactic and semantic characterization, that will eventually facil-itate the acquisition of new class members based on observable syntactic and semantic behavior. The hi-erarchical structure and small number of thematic roles is aimed at supporting generalizations. 2.3 Mapping PropBank to VerbNet Because PropBank includes a large corpus of man-ually annotated predicate-argument data, it can be used to train supervised machine learning algo-rithms, which can in turn provide PropBank-style annotations for novel or unseen text. However, as we discussed in the introduction, PropBank X  X  verb-specific role labels are somewhat problematic. Fur-thermore, PropBank lacks much of the information that is contained in VerbNet, including information about selectional restrictions, verb semantics, and inter-verb relationships.
 We have therefore created a mapping between VerbNet and PropBank (Loper et al., 2007), which will allow us to use the machine learning tech-niques that have been developed for PropBank anno-tations to generate more semantically abstract Verb-Net representations. Additionally, the mapping can be used to translate PropBank-style numbered ar-guments (Arg0. . . Arg5) to VerbNet thematic roles (Agent, Patient, Theme, etc.), which should allow us to overcome the verb-specific nature of PropBank.
The mapping between VerbNet and PropBank consists of two parts: a lexical mapping and an in-stance classifier . The lexical mapping is responsible for specifying the potential mappings between Prop-Bank and VerbNet for a given word; but it does not specify which of those mappings should be used for any given occurrence of the word. That is the job of the instance classifier, which looks at the word in context, and decides which of the mappings is most appropriate. In essence, the instance classi-fier is performing word sense disambiguation, de-ciding which lexeme from each database is correct for a given occurrence of a word. In order to train the instance classifier, we semi-automatically anno-tated each verb in the PropBank corpus with Verb-Net class information. 1 This mapped corpus was then used to build the instance classifier. More de-tails about the mapping, and how it was created, can be found in (Loper et al., 2007). In order to confirm our belief that PropBank roles Arg0 and Arg1 are relatively coherent, while roles Arg2-5 are much more overloaded, we performed a preliminary analysis of how argument roles were mapped. Figure 1 shows how often each PropBank role was mapped to each VerbNet thematic role, cal-culated as a fraction of instances in the mapped cor-pus. From this figure, we can see that Arg0 maps to agent-like roles, such as  X  X gent X  and  X  X xperiencer, X  over 94% of the time; and Arg1 maps to patient-like roles, including  X  X heme, X   X  X opic, X  and  X  X atient, X  over 82% of the time. In contrast, arguments Arg2-5 get mapped to a much broader variety of roles. It is also worth noting that the sample size for arguments Arg3-5 is quite small in comparison with arguments Arg0-2, suggesting that any automatically built clas-sifier for arguments Arg3-5 will suffer severe sparse data problems for those arguments. An important issue for state-of-the-art automatic SRL systems is robustness: although they receive high performance scores when tested on the Wall Street Journal (WSJ) corpus, that performance drops significantly when the same systems are tested on a corpus from another genre. This performance drop reflects the fact that the WSJ corpus is highly spe-cialized, and tends to use genre-specific word senses for many verbs. The 2005 CoNLL shared task has addressed this issue of robustness by evaluating par-ticipating systems on a test set extracted from the Brown corpus, which is very different from the WSJ corpus that was used for training. The results sug-gest that there is much work to be done in order to improve system robustness.

One of the reasons that current SRL systems have difficulty deciding which role label to assign to a given argument is that role labels are defined on a per-verb basis. This is less problematic for Arg0 and Arg1, where a conscious effort was made to be consistent across verbs; but is a significant problem for Args[2-5], which tend to have very verb-specific meanings. This problem is exacerbated even fur-ther on novel genres, where SRL systems are more likely to encounter unseen verbs and uses of argu-ments that were not encountered in the training data. 4.1 Addressing Current SRL Problems via By exploiting the mapping between PropBank and VerbNet, we can transform the data to make it more consistent, and to expand the size and variety of the training data. In particular, we can use the map-ping to transform the verb-specific PropBank role labels into the more general thematic role labels that are used by VerbNet. Unlike the PropBank labels, the VerbNet labels are defined consistently across verbs; and therefore it should be easier for statisti-cal SRL systems to model them. Furthermore, since the VerbNet role labels are significantly less verb-Figure 1: The frequency with which each PropBank numbered argument is mapped to each VerbNet the-matic role in the mapped corpus. The numbers next to each PropBank argument reflects the num-ber of occurrences of that numbered argument in the mapped corpus. dependent than the PropBank roles, the SRL X  X  mod-els should generalize better to novel verbs, and to novel uses of known verbs. In order to verify the feasibility of performing se-mantic role labeling with VerbNet thematic roles, we re-trained our existing SRL system, which originally used PropBank role labels, with a new label set that makes use of VerbNet thematic role information. 5.1 The SRL System Our SRL system is a Maximum Entropy based pipelined system which consists of four compo-nents: Pre-processing, Argument Identification, Ar-gument Classification, and Post Processing. The Pre-processing component pipes a sentence through a syntactic parser and filters out constituents which are unlikely to be semantic arguments based on a constituents location in the parse tree. The Argu-ment Identification component is a binary MaxEnt classifier, which tags candidate constituents as ar-guments or non-arguments. The Argument Classifi-cation component is a multi-class MaxEnt classifier which assigns a semantic role to each constituent. The Post Processing component further selects the final arguments based on global constraints. Our ex-periments mainly focused on changes to the Argu-ment Classification stage of the SRL pipeline, and in particular, on changes to the set of output tags. For more information on our SRL system, see (Yi and Palmer, 2004; Yi and Palmer, 2005).

The evaluation of SRL systems is typically ex-pressed by precision, recall and the F1-measure. Precision is the number of correct arguments pre-dicted by a system divided by the total number of arguments proposed. Recall is the number of cor-rect arguments divided by the number of the total number of arguments in the Gold Standard Data. F1 computes the harmonic mean of precision and recall. 5.2 SRL Experiments on Mapped VerbNet Since PropBank arguments Arg0 and Arg1 are al-ready quite coherent, we left them as-is in the new label set. But since arguments Arg2-Arg5 are highly Figure 2: Thematic Role Groupings for the exper-iments on linked lexical resources; and for Arg2 in the experiments on arguments with different verb in-dependency. overloaded, we replaced them by mapping them to their corresponding VerbNet thematic role. We found that mapping directly to individual role labels created a significant sparse data problem, since the number of output tags was increased from 6 to 23. We therefore grouped the VerbNet thematic roles into five coherent groups of similar thematic roles, shown in Figure 2. 2 Our new tag set therefore in-cluded the following tags: Arg0 ( agent ); Arg1 ( pa-tient ); Group1 ( goal ); Group2 ( extent ); Group3 ( predicate/attrib ); Group4 ( product ); and Group5 ( instrument/cause ).

Training our SRL system using these thematic role groups, we obtained performance similar to the original SRL system. However, it is important to note that these performance figures are not directly comparable, since the two systems are performing different tasks: The Original system labels Arg0-5,ArgA and ArgM and the Mapped system labels Arg0, Arg1, ArgA, ArgM and Group1-5. In partic-ular, the role labels generated by the original system are verb-specific, while the role labels generated by the new system are less verb-dependent. 5.2.1 Results
For our testing and training, we used the portion of Penn Treebank II that is covered by the mapping, and where at least one of Arg2-5 is used. Training was performed using sections 2-21 of the Treebank (10,783 instances of argument); and testing was per-formed on section 23 (859 instances). Table 1 dis-plays the performance score for the SRL system us-ing the augmented tag set ( X  X apped X ). The per-formance score of the original system ( X  X riginal X ) is also listed, for reference; however, as was dis-Table 1: Overall SRL System performance using the PropBank tag set ( X  X riginal X ) and the augmented tag set ( X  X apped X ) Table 2: SRL System performance evaluated on only Arg2-5 (Original) or Group1-5 (Mapped). cussed above, these results are not directly compara-ble because the two systems are performing different tasks.

The results indicate that the performance drops when we train on the new argument labels, espe-cially on precision when we evaluate the systems on only Arg2-5/Group1-5 (see Table 2). However, it is premature to conclude that there is no benefit from the VerbNet thematic role labels. Firstly, we have very few mapped Arg3-5 instances (less than 1,000 instances); secondly, we lack test data gen-erated from a genre other than WSJ to allow us to evaluate the robustness (generality) of SRL trained on the new argument labels.

We therefore redesigned our experiments by lim-iting the scope to mapped instances of Arg1 and Arg2. By doing this, we should be able to accom-plish the following: 1) we can map new argument la-bels back to the original PropBank labels; therefore we can directly compare results; 2) With the ability of testing our systems on other test data, we can eval-uate the influence of the mapping on SRL robust-ness; 3) We can validate our original hypothesis that the behavior of Arg1 is primarily verb-independent while Arg2 is more verb-specific. 5.3 SRL Experiments on Arguments with We conducted two further sets of experiments: one to test the effect of the mapping on learning Arg2; and one to test the effect on learning Arg1. Since Arg2 is used in very verb-dependent ways, we ex-pect that mapping it to VerbNet role labels will in-Figure 3: Thematic Role Groupings for Arg1 in the experiments on arguments with different verb inde-pendency. crease our performance. However, since a conscious effort was made to keep the meaning of Arg1 consis-tent across verbs, we expect that mapping it to Verb-Net labels will provide less of an improvement.
Each experiment compares two SRL systems: one trained using the original PropBank role labels; the other trained with the argument role under consid-eration (Arg1 or Arg2) subdivided based on which VerbNet role label it maps to. In order to prevent the training data from these subdivided labels from becoming too sparse (which would impair system performance) we grouped similar thematic roles to-gether. For Arg2, we used the same groupings as the previous experiment, shown in Figure 2. The argu-ment role groupings we used for Arg1 are shown in Figure 3.

The training data for both experiments is the por-tion of Penn Treebank II (sections 02-21) that is cov-ered by the mapping. We evaluated each experi-mental system using two test sets: section 23 of the Penn Treebank II, which represents the same genre as the training data; and the PropBank-ed portion of the Brown corpus, which represents a very different genre. 5.3.1 Results and Discussion
Table 3 describes the results of SRL overall per-formance tested on the WSJ corpus Section 23; Ta-ble 4 demonstrates the SRL overall system perfor-mance tested on the Brown corpus. Systems Arg1-Original and Arg2-Original are trained using the original PropBank labels, and show the baseline performance of our SRL system. Systems Arg1-Mapped and Arg2-Mapped are trained using Prop-Bank labels augmented with VerbNet thematic role groups. In order to allow comparison between the system using the original PropBank labels and the systems that augmented those labels with VerbNet Table 3: SRL System Performance on Arg1 Map-ping and Arg2 Mapping, tested using the WSJ cor-pus (section 23) . This represents performance on the same genre as the training corpus.
 Table 4: SRL System Performance on Arg1 Map-ping and Arg2 Mapping, tested using the PropBank-ed Brown corpus . This represents performance on a different genre from the training corpus. thematic role groups, system performance was eval-uated based solely on the PropBank role label that was assigned.

We had hypothesized that with the use of thematic roles, we would be able to create a more consis-tent training data set which would result in an im-provement in system performance. In addition, the thematic roles would behave more consistently than the overloaded Args[2-5] across verbs, which should enhance robustness. However, since in practice we are also increasing the number of argument labels an SRL system needs to tag, the system might suf-fer from data sparseness. Our hope is that the en-hancement gained from the mapping will outweigh the loss due to data sparseness.
 From Table 3 and Table 4 we see the F1 scores of Arg1-Original and Arg1-Mapped are statistically in-different both on the WSJ corpus and the Brown cor-pus. These results confirm the observation that Arg1 in the PropBank behaves fairly verb-independently so that the VerbNet mapping does not provide much benefit. The increase of precision due to a more co-herent training data set is compensated for by the loss of recall due to data sparseness.
 The results of the Arg2 experiments tell a differ-ARG2-ARG0 53 50 -Mapped ARG1 -716 -Table 5: Confusion matrix on the 1,539 instances which ARG2-Mapped tags correctly and ARG2-Original fails to predict. ent story. Both precision and recall are improved significantly, which demonstrates that the Arg2 label in the PropBank is quite overloaded. The Arg2 map-ping improves the overall results (F1) on the WSJ by 6% and on the Brown corpus by almost 10%. As a more diverse corpus, the Brown corpus provides many more opportunities for generalizing to new us-ages. Our new SRL system handles these cases more robustly, demonstrating the consistency and useful-ness of the thematic role categories. 5.4 Improved Argument Distinction via The ARG2-Mapped system generalizes well both on the WSJ corpus and the Brown corpus. In or-der to explore the improved robustness brought by the mapping, we extracted and observed the 1,539 instances to which the system ARG2-Mapped as-signed the correct semantic role label, but which the system ARG2-Original failed to predict. From the confusion matrix depicted in Table 5, we discover the following:
The mapping makes ARG2 more clearly defined, and as a result there is a better distinction be-tween ARG2 and other argument labels: Among the 1,539 instances that ARG2-Original didn X  X  tag correctly, 233 instances are not assigned an argu-ment label, and 1,252 instances ARG2-Original con-fuse the ARG2 label with another argument label: the system ARG2-Original assigned the ARG2 la-bel to 50 ARG0 X  X , 716 ARG1 X  X , 1 ARG3 and 482 ARGM X  X , and assigned other argument labels to 3 ARG2 X  X . In conclusion, we have described a mapping from the annotated PropBank corpus to VerbNet verb classes with associated thematic role labels. We hy-pothesized that these labels would be more verb-independent and less overloaded than the PropBank Args2-5, and would therefore provide more consis-tent training instances which would generalize better to new genres. Our preliminary experiments confirm this hypothesis, with a 6% performance improve-ment on the WSJ and a 10% performance improve-ment on the Brown corpus for Arg2.
 In future work, we will map the PropBank-ed Brown corpus to VerbNet as well, which will allow much more thorough testing of our hypothesis. We will also examine back-off to verb class membership as a technique for improving performance on out of vocabulary verbs. Finally, we plan to explore the ef-fect of different thematic role groupings on system performance.

