 Traditional supervised learning works on the single label scenario. That is, each in-stance is associated with one single label within a finite set of labels. However, in many applications, each instance can be associated with more than one label simultaneously. For example, in text categorization, one document can belong to multiple categories [Yang et al. 2009]; in image classification, an image is usually associated with multiple labels which are characterized by different regions in the image [Zha et al. 2008]; in bioinformatics, one gene sequence may serve multiple functions [Elisseeff and Weston 2002]; in video annotation, an video can be tagged with multiple labels simultaneously [Zha et al. 2009]. This setting is called multi-label classification, which corresponds to the problem of classifying each instance with a set of labels. Multi-label classification has been drawing increasing attention from the machine learning and data mining communities in the past decade [Dembczy  X  nski et al. 2010a; Petterson and Caetano 2010; Zhang and Zhang 2010].

Conventional multi-label classification approaches focus on the single objective set-ting, where the learning algorithm trains one model that optimizes over one single objective. The objective can be a performance evaluation criterion (e.g., Hamming Loss [Tsoumakas et al. 2010]) or a heuristic function (e.g., the posteriori principle in ML-KNN [Zhang and Zhou 2007]). The basic assumption of single-objective multi-label classifi-cation is that one single objective can evaluate the overall performance of a multi-label classifier. Thus, the optimization over one single objective can comprehensively im-prove the classifier X  X  performance. However, in multi-label classification, many criteria are proposed to evaluate the classification performance from different perspectives, and some criteria are inconsistent [Gao and Zhou 2011] or even conflict [Dembczy  X  nski et al. 2010b]. Gao and Zhou [2011] prove that no convex surrogate loss is consistent with the ranking loss. Dembczy  X  nski et al. [2010b] elaborate the connection among these criteria and point out that some loss functions are essentially conflicting, such as Hamming Loss [Tsoumakas et al. 2010] and Subset 0/1 Loss [Ghamrawi and McCallum 2005]. So the optimization over one single objective may not lead to the performance improvement on the other objectives. For example, in a multi-label classification task where the performances on Hamming Loss [Tsoumakas et al. 2010] and Micro F1 [Ghamrawi and McCallum 2005] are concerned, one may minimize Hamming Loss, maximize Micro F1 (i.e., minimize 1  X  Micro F 1), or optimize both of them simulta-neously. An example of results is shown in Figure 1. Due to the inconsistency existing in these two objectives in some conditions, only optimizing over Hamming Loss may lead to bad performance on Micro F1 (e.g., solution B ), or vice versa (e.g., solution A ). However, it is obvious that solution C is better than A and B when we concern the classification performances on both Hamming Loss and Micro F1. As a consequence, it is necessary to simultaneously optimize over multiple objectives for multi-label classi-fication in such conditions where the concerned objectives are inconsistent or potential conflicting. This helps to balance the trade-off among these objectives and compre-hensively improve performances of multi-label classification, not limiting to one single criterion. In addition, the simultaneous optimization over multiple objectives is also practically needed in many multi-label classification tasks [Tsoumakas et al. 2010]. For example, in a news-filtering application, users must be presented with those in-teresting articles, but it is also important to only see the most interesting one. So the performances of the multi-label classifier on One Error [Tsoumakas et al. 2010] and Micro F1 [Ghamrawi and McCallum 2005] both need to be considered.

In conventional multi-label classification (i.e., single-objective multi-label classifica-tion, as shown in Figure 2(a)), one single solution is usually returned to satisfy the requirements of all users. However, it is often the case that users in different applica-tion scenarios can have very different expectations on a multi-label classifier [Petterson and Caetano 2010]. With multiple optimization objectives employed, there is usually no single best solution for this multi-label classification task, but instead, a set of non-dominated solutions that correspond to different trade-offs among those objectives so that users can flexibly select appropriate solutions in items of their different applica-tions. For example, in Figure 1, one can select A in a Hamming Loss-aware application, or select C in a Hamming Loss and Micro F1 -aware application.

Formally, the multi-objective multi-label classification (as shown in Figure 2(b)) cor-responds to simultaneously optimizing over multiple objectives and obtaining a set of multi-label classification models. Despite its value and significance, the multi-objective multi-label classification has not been studied in this context so far, due to the following research challenges. (1) Most evaluation objectives in multi-label classification cannot be directly optimized even in the single objective setting [Gao and Zhou 2011]. The loss functions in multi-label classification are usually difficult to optimize directly be-cause of non-convexity and discontinuity. Many multi-label classification approaches work with surrogate loss functions, such as Ranking Loss [Tsoumakas et al. 2010] and Hamming Loss [Tsoumakas et al. 2010]). (2) Multi-objective optimization is much more difficult than single objective optimization. It is not easy to effectively trade-off mul-tiple objectives in multi-label classification. Multi-objective optimization can be con-verted into single objective optimization with the scalarization method (e.g., weighted sum method [Furnkranz and Flach 2003]) and the trade-offs among objectives can be exploited by tuning weights. However, it is hard to choose the weights in real appli-cations and cannot discover the solutions in the concave Pareto front [Freitas 2006]. For example, the weighted sum method can find A and B in Figure 1, but it cannot discover C .

In this article, we study the problem of multi-objective multi-label classification and propose a novel solution, called M OML (multi-objective-based multi-label algorithm). Different from conventional multi-label classification approaches, the proposed M
OML can simultaneously optimize over multiple objectives based on evolutionary multi-objective optimization (EMO). EMO has unique properties to effectively solve these challenges. (1) EMO does not require the optimization objectives to be differ-entiable, and thus any evaluation metric in multi-label classification can be used as optimization objectives in our M OML . (2) It can automatically balance the trade-offs among multiple objectives with population optimization. Due to multiple optimization objectives, M OML returns a set of classification models with different preferences on these objectives, so we propose two model selection strategies to make full use of these models and make predictions on the testing data. And thus, users can flexibly apply these model selection strategies in different applications. Experiments on seven real-world multi-label classification tasks justify the effectiveness of our M OML with nine popular performance evaluation criteria. Results show that M OML can compre-hensively boost the multi-label classification performance on most of the performance criteria. Moreover, in comparison experiments of model selection strategies, M OML can effectively adapt to the user X  X  preferences in different applications by achieving better performances on the preferred objectives. Multi-label classification has been well developed in the past decade. There are two basic ways to solve this problem: problem transformation and algorithm adaptation. In problem transformation, a multi-label problem is transformed into multiple single-label problems. For each single-label problem, a single-label classifier is learnt, and then these single-label classifiers are combined for the original multi-label problem. Many base learners have been employed in problem transformation approaches, such as Support Vector Machines [Godbole and Sarawagi 2004], Naive Bayes [Ji et al. 2008], and k -Nearest-Neighbor methods [Zhang and Zhou 2007]. In algorithm adaptation, it modifies specific learning algorithms to solve multi-label data directly. The represen-tative approaches involve decision trees [Vens et al. 2008], AdaBoost [Schapire and Singer 2000], and BP-MLL [Zhang and Zhou 2006]. These algorithms usually optimize only one evaluation metric explicitly or implicitly, whereas our MOML explicitly opti-mizes multiple objectives at the same time.

Since ensemble learning can effectively improve learners X  generalization perfor-mances, it has been widely applied in multi-label learning to build a set of base learn-ers [Read et al. 2008, 2009; Shi et al. 2011; Tsoumakas et al. 2008; Tsoumakas and Vlahavas 2007]. For example, RAKEL [Tsoumakas and Vlahavas 2007] trains each single-label base learner for the prediction of each element in the powerset of the label set, and the single-label base learner in EPS [Read et al. 2008] is built for a pruning label subset. Similar to these approaches, M OML also employs the ensemble method in the model selection phase, whereas M OML generates the solution set through evolutionary multi-objective optimization. Recently, some researches began to be aware of conflict existing in measure criteria [Dembczy  X  nski et al. 2010b; Petterson and Caetano 2010; Xu and Xu 2010]. Petterson and Caetano [2010] point out the evaluation measures are as diverse as the applications. However, their method still optimizes a single criterion by appropriate surrogate. Different from ML-2OKM [Xu and Xu 2010] which also optimizes two particular objectives with an existing EMO, M
OML  X  X  optimization objectives can be any evaluation metrics and its base model is a multi-label classifier. Dembczy  X  nski et al. [2010b] analyze the connection between loss functions in multi-label classification, which helps to select appropriate optimization objectives in M OML . In addition, there is an increasing attention on the consistency of multi-label learning [Gao and Zhou 2011; Kotlowski et al. 2011; Dembczy  X  nski et al. 2012]. Since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity, the surrogate loss functions are widely used in multi-label classification. However, Gao and Zhou [2011] find that no convex surrogate loss is consistent with the ranking loss. Then Dembczy  X  nski et al. [2012] prove that com-mon convex surrogates used for binary classification are consistent for the minimiza-tion of rank loss. These theoretical analysis further disclose the inconsistency among surrogate loss functions, which implies the importance of multi-objective multi-label classification.

Multi-objective optimization is the process of simultaneously optimizing two or more conflicting objectives subject to certain constraints, which is widely existing in many fields (e.g., decision and optimization). Many methods have been proposed to solve this problem (e.g., weighted sum method [Furnkranz and Flach 2003]), among which evo-lutionary algorithm [Goldberg 1989] has been proven to be an effective solution. This kind of solutions is also called EMO technique [Deb 2001]. EMO simultaneously opti-mizes multiple objectives through population evolution, in which individuals reproduce through evolutionary operation (e.g., crossover and mutation) and obey the Darwinian evolution: survival of the fittest. Traditional EMO focuses on numerical optimization problems [Deb et al. 2002]. However, EMO begins to be applied in data mining problems in recent years [Freitas 2006], such as data clustering [Handle and Knowles 2007] and click prediction [Agarwal et al. 2011]. Shi et al. [2011] use EMO to generate a set of classifiers, while their work focuses on the ensemble of classifiers. Chen and Yao [2010] employ the multi-objective neural network ensemble to improve classification performances, whereas it focuses on the single-label classification problem. Let  X  = R d be the d -dimensional input space and L ={ 1 , 2 ,..., L } be the finite set of L is an instance and Y i  X  L is the label set associated with x i . The task of multi-label learning is to learn a multi-label classifier h :  X   X  2 L from D , which predicts a set of labels for each unseen instance.

Conventional multi-label classification approaches can be roughly classified into two categories: (1) one type of approach trains one single model by explicitly or implicitly optimizing a performance criterion. For example, ML-RBF [Zhang 2009] explicitly op-timizes the Hamming Loss, while Ranking Loss is optimized in BP-MLL [Zhang and Zhou 2006] and R ANK -SVM [Elisseeff and Weston 2002]. (2) The second type of approach does not explicitly optimize those performance criteria, but implicitly optimizes one single heuristic function which is not directly related to any performance criteria. For example, E CC [Read et al. 2009] and L EAD [Zhang and Zhang 2010] optimize the gen-eralization risk for multi-label predictions by encoding label correlations, and ML-KNN [Zhang and Zhou 2007] maximizes the posteriori principle in multi-label learning. In both types of approaches, the multi-label learning is regarded as a single objective optimization problem (SOP), which can be defined as follows.
 through optimizing one single objective function. is the set of feasible models, M is a predictive model in . O 1 :  X  R is an objective function, which can be a performance criterion (e.g., metrics in Section 5.1.2) or any other implicit heuristic function. Without loss of generality, we assume O 1 is to be minimized. Most of conventional algorithms are based on solving this SOP. Different algorithms may vary in the objective function O 1 and optimization techniques.
This article first formulates multi-label learning as a multi-objective optimization problem (MOP) [Deb 2001], which can be defined as follows.

Definition 2 . Multi-objective multi-label classification. It determines models M  X  through simultaneously optimizing multiple objective functions. t is the number of objectives, and O i represents the i th objective.

For the MOP, each objective corresponds to an optimal solution. We have to incorpo-rate the different trade-offs among the multiple objectives. One fundamental difference between SOP and MOP is that, for a MOP, we can find a set of optimal solutions where no single solution can be said to be better than any other. Solving a MOP often im-plies to search for the set of optimal solutions as opposed to one single solution for a SOP. Here, we define the concept of domination relation to compare the performance of multi-label classification models, similar to Deb [2001].

Definition 3 . Domination. For two models M 1 , M 2  X  , M 1 dominates M 2 (denoted as M 1 M 2 )ifandonlyif Similarly, if M 1 M 2 and M 2 M 1 , M 1 is non-dominated with M 2 . A model M  X  is said to be Pareto optimal [Deb 2001] if and only if M is not dominated by any other model in . The set of all Pareto-optimal models is called the Pareto-optimal set, or Pareto front. An example is shown in Figure 1. Model C dominates the model D ,and C is non-dominated with A and B . A , B ,and C are the Pareto-optimal set or Pareto front. In order to solve the multi-objective multi-label classification problem, a simple ap-proach is to convert multiple objectives into a single objective by using certain schemes and user-specified parameters, such as the weighted sum method [Furnkranz and Flach 2003]. However, this method cannot be directly applied to multi-label classifica-tion problem, since many objectives may not be easily optimized even in SOP setting and the parameter settings are very difficult for these methods. Here we apply EMO to solve the multi-objective multi-label classification problem. Although EMO has been successfully applied in many numeric optimization problems and some data mining problems, it is seldom applied in classification. The reason lies in these two difficulties: (1) the classifier model is difficult to be effectively encoded in evolutionary algorithm; (2) it is far more difficult to trade off the self-learning of classifiers and information exchange among classifiers in EMO.

This article, proposes a method based on EMO to solve the multi-objective multi-label classification problem. The method is called multi-objective multi-label algorithm (M OML ) which includes two phases: model training and selection. Briefly, M OML designs an effective multi-objective optimization mechanism and a novel method of generating new solutions based on a modified ml-RBF base model in the model training phase. In the model selection phase, two model selection strategies are proposed to help users flexibly select their preferred models in terms of their application scenarios. A good EMO algorithm needs to generate a set of solutions that uniformly distribute along the Pareto front [Veldhuizen and Lamont 2000], which includes two key issues: (1) solutions prone to converge to the Pareto front and maintain diversity in the evolu-tionary process; (2) generating promising solutions in each generation. In order to make EMO fit for multi-label learning, we design many novel mechanisms in the following two sections.
 4.1.1. Multi-Objective Optimization Mechanism. Since a good solution is expected to con-verge to the Pareto front and maintain diversity, the fitness of the solution can be determined by its convergence and diversity. We apply the non-dominated-sort and diversity-estimate process to effectively evaluate these two measures. Furthermore, the proposed select-individuals process selects the best solutions as the next genera-tion population in terms of these measures.

Non-dominated-sort. The non-dominated-sort process sorts solutions according to their raw fitness (i.e., objective value O i ). The different value range of objectives (e.g., Co v erage &gt; 1and HammingLoss &lt; 1) may lead to the situation that some base models reproduce too rapidly. Instead of the raw fitness, this article employs the rank-based fitness assignment [Goldberg 1989] to reassign the fitness (i.e., a rank value) to the solutions, because this method behaves in a more robust manner. In the rank-based fitness assignment, the solution set is divided into different fronts with different ranks. The solutions in the same front are non-dominated to each other and solutions in the higher front are always dominated by some solutions in the lower front. Figure 3 shows an example that 12 solutions are divided into three fronts according to their domination relations. In this way, each solution (i.e., model) M i in a front F a has a rank value M rank i = a . It is evident that solution M i is better than solution M j when the minimization problem is considered in this article.

Diversity-estimate. Along with convergence to the Pareto front, it is also desired that an evolutionary algorithm maintains a good spread of solutions. So the solution in the crowded region is more likely to be deleted. To get a diversity estimate of solutions surrounding a particular solution in the population, we design the diversity-estimate process that calculates the average Euclidean distance of two solutions on either side of this solution along each of objectives. It is simple and effective to estimate the diversity of solutions. The diversity estimation of solution M i , M distance i , serves as the perimeter of the cuboid formed by using the nearest neighbors as the vertices. As shown in Figure 3, the diversity of this i th solution in its front is the average side length of the cuboid. The small M distance i means solution M i is in a more crowded region, which implies a bad diversity.

Select-individuals. Every solution M i in the population has two feature values: (1) non-domination rank M rank i ; (2) diversity estimation M distance i . We define a partial order  X  to compare two solutions, which comprehensively considers both of features.
Definition 4 . Partial Order  X  . For two solutions M i and M j , M i  X  M j ,ifandonly if That is, between two solutions with different non-domination ranks, we prefer the so-lution with the lower rank. Otherwise, if both solutions belong to the same front, then we prefer the solution that is located in a less crowded region. After sorting the pop-ulation with  X  ,the select-individuals process selects top solutions, which guarantees that good solutions (with low rank and high diversity) will be kept. In the meantime, those promising solutions are also likely to be contained in the population. 4.1.2. Base Model and Evolutionary Operations. In the framework of M OML , many classi-fication models can be used, such as decision tree [Schietgat et al. 2010], Back Propa-gation (BP) [Zhang and Zhou 2006], and Radical Basis Function (RBF) [Zhang 2009] neural network. Different base models will lead to different genetic representation and operation. Because the structure can be effectively encoded and the weights can be ef-ficiently calculated in close form, the ml-RBF neural network in ML-RBF [Zhang 2009] is selected as the base model in M OML , however with an additional regularization term added to reduce overfitting risks as explained later.

The architecture of ml-RBF is shown in Figure 4(a). It can be briefly summarized as follows: (1) the input of a ml-RBF corresponds to a d -dimension feature vector. (2) The hidden layer of ml-RBF is composed of L sets of prototype vectors, that is, L l = 1 C l . Here, C l consists of k l prototype vectors k -means clustering is performed on the set of instances U l with label l . Thereafter, k l clustered groups are formed for class l and the j th centroid (1  X  j  X  k l ) is regarded as a prototype vector c l j of basis function  X  l j (  X  ). (3) Each output neuron is related to a possible class. In the hidden layer of ml-RBF, the number of clusters k l is settled to be a fraction  X  of the number of instances in U l : The scale coefficient  X  controls the structure and complexity of ml-RBF model.
Different from the error function in the original ml-RBF, we add a regularization term into the error function. The regularization term greatly reduces the overfitting risk and improves the stability of solutions as observed in the experiments. of instance i on label l , K = L l = 1 k l ,and  X  is the regularization coefficient. Similar to the derivation of minimizing the error function by scaled-conjugate-gradient descent in Chen and Yao [2010], the optimal output weights W can be computed in closed form by and T = [ t il ] m  X  L with elements t il = t i l . Through extensive experiments, the regulariza-tion coefficient  X  is fixed at 0 . 1 in this article.

Genetic representation. According to the structure of ml-RBF, we propose a novel genetic representation that is the sequence of prototypes &lt; bias , c 1 1 , c 2 1 ,... c L k ample is shown in Figure 4(a). The genetic representation has the following advantages. (1) When the prototypes ( c ) are determined, the basis functions (  X  ) and the weights ( W ) can be efficiently computed, which means the performance of RBF mostly depends on the selection of the prototypes. (2) It is easy to design the crossover and mutation operators by tuning these prototypes.

Initialization. When the base model is ml-RBF, the initialization operation of M OML generates a set of ml-RBF models with different scale coefficient  X  . As suggested in Zhang [2009],  X  is randomly selected from [0.01, 0.02] in the experiments. An advantage of this Initialization operation is that it generates a set of ml-RBF models with different structures, which contributes to the population diversity.

Generate-individuals. Generating new solutions is realized by the generate-individuals process. The basic idea is to randomly select parent solutions from the cur-rent population based on the roulette wheel selection [Baker 1985] and do crossover and mutation operation to generate new solutions with the ratio of cro Rat and 1  X  cro Rat , respectively. In this article, cro Rat is fixed at 0.8, which helps to converge to the Pareto front and maintain the appropriate diversity of the population. M OML applies the roulette wheel selection [Baker 1985] to assign each solution with an appropriate selection pressure. That is, the solutions in the lower front have a higher selection probability. It guarantees that the better solution has a high yet appropriate selection probability.

Since different ml-RBFs may have different numbers of prototypes, We adapt the cut and splice crossover [Goldberg et al. 1993] which randomly chooses a crossover point for two ml-RBFs and swaps their prototypes beyond this point. Different from the traditional cut and splice crossover, the crossover point in M OML is randomly selected between two prototype vectors, rather than in an arbitrary position. Figure 4(b) shows such an example, in which the crossover point j is selected between the prototype vector &lt; c i 1 ,..., c i a the newly generated ml-RBF is unabridged cluster centroid. The width of the centroid of the new ml-RBF is recalculated as in Zhang [2009]. The weights are calculated following Equation (7).

According to the structure of ml-RBF, two mutation operations are designed. The mutation operator randomly selects some prototype vectors in a ml-RBF and does the following two structural mutation operations with the same probability. (1) Delete one prototype. Randomly select one prototype and delete it. (2) Add one prototype. The center of the new prototype is determined by a random combination of all centroids in this prototype vector.
 ALGORITHM 1 :M OML -Training
Although the crossover and mutation operations may not generate the optimal combi-nation of prototypes, they provide an effective method to search the prototypes space of ml-RBF. The crossover operator reassembles the prototypes of parent solutions, which not only maintains the good genes but also generates new combinations. The mutation operator deletes and adds new prototypes, which helps to extend the search space and maintain diversity. Once a good solution is found in the space of prototypes, it will be kept in population until it becomes a bad one. 4.1.3. Algorithm Framework. The training phase of M OML is described in Algorithm 1. M
OML transforms the t optimization objectives to a fitness measure by the creation of a number of fronts, sorted according to non-dominated-sort . After the fronts have been created, diversity-estimate assigns its members density value later to be used for diver-sity maintenance. In each generation, N new solutions are generated with generate-individuals .Ofthe2 N solutions, select-individuals selects the N best solutions for the next generation. In this way, a huge elite can be kept from generation to generation.
In M OML , the multi-objective optimization mechanism guides the solutions to con-verge to Pareto front and maintain the diversity. The genetic operations effectively search the prototypes space of ml-RBF and generate promising solutions. A particular advantage of M OML is that any function can be used as the optimized objective, only if the function can be calculated, without the requirement of being differentiable. The model training phase of M OML returns a solution set, which is a unique feature of the multi-objective multi-label classification. The user can make full use of these solutions in terms of their applications. For example, users can select one good model according to some criteria, such as AUPRC [Vens et al. 2008]. Here we design two strategies to select a set of prediction models according to users X  preferences.
The dynamic model selection strategy (called DYN) selects the top-k models on the preference objective and then makes predictions with a majority vote. Assume that instances are independently and identically distributed, these selected models will also perform well on the corresponding objective on the testing data. This dynamic model selection strategy not only can flexibly select the preferred models in terms of users X  applications but also can improve the generalization performances with en-semble learning. Note that the preference objective may be or not be the optimization objects. As we know, the model training process is expensive and is not often done. The optimization objectives are usually fixed ahead of time. In different applications, users have diverse preference on performances, so they can flexibly determine their ALGORITHM 2 :M OML -Testing-DYN ALGORITHM 3 :M OML -Testing-EN preference objectives. Since the prediction process is fast, it can be done online accord-ing to different user preferences. The DYN strategy is shown in Algorithm 2, in which M i ( x
The ensemble model selection strategy (called EN) combines all models and then makes predictions with a majority vote, which can be seen in Algorithm 3. On the one hand, this strategy can be used for users without obvious preferences. The EN strategy ensembles all models, so it may have no preference on a certain objective. On the other hand, it is promising to uniformly promote the performances on all criteria, since the ensemble learning is employed. Let d be the number of features of instances, m and n be the number of training and testing instances respectively, L be the number of labels. We consider the time complexity of ml-RBF first. Two main time-consuming components of ml-RBF are the k -means clustering and calculating = [  X  ij ] m  X  ( K + 1) for all training instances. For simplicity, suppose each label has the same number of instances m L , and thus the number of centroid is  X  m L . The complexity of a k -means clustering is O (  X  ( m L ) 2 ) (the iteration number in k -means is fixed, so it is omitted here). Lk -means clustering are needed, so the total complexity is O (  X  m 2 / L ).  X  ij needs to calculate the distance to each prototype vector c j for each instance x i , and thus its complexity is O (  X  dm 2 ). In all, the ml-RBF has the following complexity:
For M OML , it needs to generate N ml-RBFs and evaluate NG new ml-RBFs. The complexity of M OML in ml-RBF is O (  X  Nm 2 / L +  X  NGdm 2 ). The complexity of the genetic operation in M OML is O ( GN 2 ). Since N m , the total time complexity of M OML in the training phase is There are k models to make predictions on the testing data ( k is N for the EN strategy), so the time complexity of the testing phase is Since k NG , the testing phase is much faster than the training phase. 5.1.1. Data Collection. We tested our algorithm on seven real-world multi-label datasets from three different domain, as summarized in Table I. The first dataset is Yeast [Read et al. 2009; Zhang 2009; Zhang and Zhang 2010; Zhang and Zhou 2006] in biology, where the task is to predict the gene functional classes of the Yeast Saccharomyces cerevisiae. The second dataset Image [Read et al. 2009; Zhang 2009; Zhang and Zhang 2010; Zhang and Zhou 2006] involves the task of automatic image annotation for scene images. The other five datasets RCV1-1 X  X CV1-5 are the subsets of RCV1 [Yang et al. 2009; Zhang and Zhang 2010], where the task is to predict topic categories of each text document. These five datasets have different multi-label distributions, such as label cardinality and density [Zhang and Zhang 2010]. 5.1.2. Evaluation Metrics. The performance evaluation for multi-label learning is much more complicated than single-label problems. Here, we adopt nine state-of-the-art multi-label evaluation metrics which are most popular in the literature. To the best of our knowledge, few works on multi-label learning have conducted experimental eval-uation on such comprehensive comparisons over the nine metrics. These metrics are briefly summarized in Table II, where  X   X   X  indicates the smaller the value, the better the performance;  X   X   X  indicates the larger the value, the better the performance. Assume we have a multi-label dataset U containing n multi-label instances ( x i , y i ), where y i  X  { 0 , 1 } L ( i = 1 ,..., n ). Let h ( x 5.1.3. Compared Methods. We compare our method with four baseline methods which optimize over different single objectives. In M OML , any subset of metrics listed here can be used as the optimization objectives. Here, we employ two pairs representa-tive subsets of evaluation metrics, that is, { HL , RL } and { MicF 1 , AP } . The { HL , RL } objective subset includes two popular objectives that have already been directly opti-mized in previous single objective approaches [Elisseeff and Weston 2002; Zhang 2009; Zhang and Zhou 2006]. The { MicF 1 , AP } objective subset includes two most useful performance criteria which are not often been directly optimized before. In addition, these two pairs of objectives are potentially conflicting. Here the DYN model selection strategy is employed. These compared methods are summarized as follows.  X  X  OML { HL , RL } . The proposed M OML approach with the first objective subset ( { HL , RL } ), which outputs a set of models with different preferences on each objective. In order to verify the quality of the outputted solution set, we report two versions of the DYN model selection based on the top k models in terms of HL and RL , respectively. The corresponding algorithms are called M OML { HL , RL } and M OML { HL , RL } . These two com-bined models correspond to the two application preferences over the two optimization objectives.  X  X  OML { MicF 1 , AP } . The proposed M OML approach with the second objective subset { MicF 1 , AP } . Similarly, we report two versions of the DYN model selection in terms of MicF 1and AP and the corresponding algorithms are called M OML { MicF 1 , AP } and
M OML { MicF 1 , AP } , respectively. Note that, in order to be fit for the minimization prob-lem, 1  X  MicF 1and1  X  AP are used in M OML .  X  X L-RBF [Zhang 2009]. Based on ml-RBF neural network, the method explicitly opti-mizes the HL criterion.  X  X P-MLL [Zhang and Zhou 2006]. This method is based on BP neural network, which explicitly optimizes the RL criterion.  X  X L-KNN [Zhang and Zhou 2007]. The KNN based lazy multi-label learning method optimizes a posterior principle which is not directly related to any single performance criterion.  X  X  CC [Read et al. 2009]. It is an ensemble of classifier chains which encode the multi-label correlations in the multi-label classification process.

The population size and running generation of M OML are set as 30 and 10. k is 9 (i.e., 30% of the population size) in the top-k model selection. ML-RBF is implemented with fixed parameters of  X  = 0 . 01 and  X  = 1 . 0, as suggested in the literature [Zhang 2009]. For BP-MLL , as indicated in the literature [Zhang and Zhou 2006], the number of hidden neurons is set to be 20% of the number of input neurons, and the number of training epochs is fixed at 100 with learning rate of 0.05. For ML-KNN , the number of nearest neighbors considered is set to 10 and Euclidean distance is used as the distance measure [Zhang and Zhou 2007]. For E CC , the ensemble size is set to 10 and sampling ratio is set to 67% [Read et al. 2009]. Ten-fold cross-validation is performed on each experimental dataset. On each dataset, we report the average values of each algorithm with the ranks based on its results. All experiments are conducted on machines with Intel Xeon Quad-Core CPUs of 2.26GHz and 24GB RAM.

Due to the limited space, we only show the results of the average values of nine metrics on Yeast, Image and RCV1-1 in Tables III X  X , where  X * X  indicates the best result on each criterion and  X   X  indicates the performance of M OML on its preference objective. The other four datasets on RCV1 have similar results with RCV1-1. From these tables, we can observe that the four versions of the M OML method rank among the first four on most metrics and they always have the best average ranks on each dataset. Furthermore, Table VI summarizes the mean and standard deviation of the rank values for each method over nine metrics on all seven datasets. To statistically measure the significance of performance improvement, pairwise t -test at 5% significance level are conducted between M OML and other compared algorithms for each dataset. Here the M
OML  X  X  performances are the average performances of four versions of M OML . Table VII illustrates the number of win/tie/loss of M OML against other compared algorithms on all seven datasets. The results indicate that, although M OML only optimizes two objectives, the performances of M OML are significantly better than the baselines on most metrics. Moreover, Table VI shows that each variant of the four M OML algorithms does provide the best average rank on its primary objective, such as M OML { HL , RL } on HL ,M OML { HL , RL } on RL , etc. Other methods may occasionally outperform our approach on some of the metrics in a few of the datasets, but not consistently. These results validate our intuition that the multi-objective optimization in our M OML can effectively tradeoff among multiple objectives and avoid the local optimal to improve the overall performance almost on all metrics.

Table VIII shows the average running time. We only show one result of four versions of M OML , since the four versions have the same time complexity. Although M OML is slower than ML-RBF ,ML-KNN and E CC , it is still faster than BP-MLL in the training phase. In the testing phase, M OML is faster than BP-MLL and E CC .
 There are two genetic operation related parameters governing the M OML ,thatis,the population size N and the running generations G . Figure 5 illustrates the evolutionary characteristics of M OML { HL , RL } on the Yeast data with ten-fold cross-validation, under different parameter configurations. Specifically, when the population size N increases from 10 to 60 with an interval of 10, we report the average of performances, running time and weights (the sum of absolute value of W ) by combining all the models in the population.

It is evident from Figure 5 that when the population size N is fixed, the performance (i.e., Hamming Loss and Ranking Loss) of M OML consistently improves as the running generation increases. In the meantime, the weights of ml-RBF and running time also increase. Figure 5 also clearly shows that the large population size usually leads to better performances accompanying with the increase of weights and running time. Observing the trend of weight curves in Figure 5(c), we can find that although the weights consistently increase, the rate of increase becomes small. If we do not add the regularization term in the error function of ml-RBF (see Equation (6)), the weights will increase sharply, which means these models are overfitting. Figure 5(d) illustrates that the running time of M OML increases linearly with the population size N and running generation G , which validates the time complexity of M OML in Equation (9).
In addition, the number of top models k also affects the performance of M OML .In order to observe its effect on performance, we do experiments on Image data with M Note that M OML { HL , RL } has the same parameter setting with that in Section 5.2 and k is the ratio of selected models. Figure 6(a) clearly illustrates that M OML { HL , RL } has the best performance on criteria Hamming Loss and Ranking Loss when k are 0.3 and 0.4. The same phenomenon is also shown in Figure 6(b). When k is small, there are only few classifiers to make a prediction. When k becomes larger, more classifiers will be ensembled, which usually leads to better performances. However, ensembling more classifiers makes the prediction have less preference to the optimization objective, so the performance of M OML { HL , RL } on criteria Hamming Loss and Ranking Loss will degrade for large k . This is the reason why the performances of M OML { HL , RL } increase first and then decrease as k increases. The experiments also imply that M OML will achieve better performance when k is 0.3 and 0.4. In this setting, the model selection process not only has the benefit of ensembling classifiers but also keeps the preference on optimization objectives. Our previous experiments only show the cases with a pair of objectives. However, more objective functions also can be included in M OML . In order to study the performances of M OML with different numbers of objectives, here we consider four objective functions (i.e., HL, RL, MicF1, and AP) and three versions of M OML which optimize the first 2, 3, and 4 objectives, respectively. The corresponding algorithms are called M OML -{ HL , RL } , M
OML -{ HL , RL , MicF 1 } ,andM OML -{ HL , RL , MicF 1 , AP } . We also consider a special case of M OML , called M OML -{ Ens } , where the running generation of M OML is 0. That is, M OML -{ Ens } does not do any genetic operation and multi-objective optimization. So it is just the ensemble of multiple ml-RBFs, and its performances are constant along the evolutionary process. Considering M OML -{ Ens } as baseline, we observe the improvement rate of performances of other algorithms against M OML -{ Ens } . Ten-fold cross-validation are reported on the Yeast data.

The results are shown in Figure 7. It is obvious that the three versions of M OML achieve the consistent and steady performance promotion against M OML -{ Ens } on all four objectives. It illustrates that the genetic operation and multi-objective optimization in M OML is really helpful to train better models. We can also find that M OML has better performances on the optimization objectives than on non-optimization objectives. However, when more objectives are included in M OML , the performance of M OML on the optimization objectives will degrade. For example, compared to M OML -{ HL , RL } ,M OML -{
HL , RL , MicF 1 , AP } can achieve better performances on MicF1 and AP by including them in its objective set, while its performances are slightly worse than M OML -{ HL , RL } on HL and RL . As the number of optimization objectives increases, the space of Pareto frontier is greatly extended, which results in the exponential increase of the number of non-dominated solutions [Saxena et al. 2013]. So the domination-based selection operators in EMO do not work well in this case [Saxena et al. 2013]. As we noted, a regularization term is added in the error function of M OML (see Equation (6)), which is different from the error function in the original RRF [Zhang 2009]. This section will validate the effect of the regularization term on M OML .Werun M
OML { HL , RL } with and without the regularization term on all seven datasets. The same parameters are set with that in Section 5.2. Note that we only need to set  X  with 0 (see Equation (6)) for M OML { HL , RL } without the regularization term.

The experiment results are shown in Figure 8. It is clear that M OML with the regularization term is better than that without the regularization term for almost all the datasets. Particularly, the superiority is more obvious for the text data D3 X  D7 (i.e., RCV1(1 X 5)). The experiments illustrate the importance of the regulariza-tion term for M OML . Compared to ML-RBF [Zhang 2009], M OML has more overfitting risk, since M OML trains models more times due to its evolution process. The regular-ization term effectively reduces the overfitting risk, which helps M OML achieve good performances. We know that a direct approach for the multi-objective multi-label classification prob-lem is the weighted-sum method [Furnkranz and Flach 2003]. This section will compare the M OML { HL , RL } with the weight-sum method. The weight-sum method optimizes the objective: The optimization technique employs the same evolutionary algorithm framework with M
OML { HL , RL } . The experiments are done on Image data and the two methods have the same parameters setting with that in Section 5.2. The weighted-sum method varies w 1 from 0 to 1 with the interval 0.1. For each weight setting, it obtains a best solution and employs the solution to make a prediction. So the weighted-sum method generates 11 results for varying weights. After one run, M OML { HL , RL } returns a set of solutions, from which we select the Pareto optimal solutions to make predictions. Figure 9 shows the prediction performance on Hamming Loss and Ranking Loss. It clearly shows that the solutions generated by M OML overwhelmingly dominate those of the weighted-sum method. Moreover, the M OML  X  X  solutions are widely spread, which implies that users can flexibly select prediction models in terms of their preferences. The experiment not only illustrates M OML  X  X  potential to generate better solutions compared to the weighted-sum method but also shows M OML  X  X  advantages in computational efficiency. M OML only needs to run once to generate these solutions. However, the weighted-sum method needs to be run many times through varying weights. As a consequence, M OML is much more convenient and efficient than the weighted-sum method. This section will compare different model selection strategies and clarify their char-acteristics and application scenarios. In experiments, we use the same model training phase with the optimization objectives HL and RL (see Algorithm 1) and different model selection strategies (see Algorithm 2 X 3). These compared strategies and methods are summarized as follows.  X  X YN(HL) and DYN(RL). These two dynamic model selection strategies select the optimization objective as the preference objective. The DYN(HL) denotes the strat-egy selects the top models according to the optimization objective HL . Similarly, the DYN(RL) selects models according to RL . In fact, DYN(HL) and DYN(RL) are  X  X YN(MicF1) and DYN(AP). The preference objective in these two dynamic model selection strategies is not the optimization objective. The DYN(MicF1) and DYN(AP) denotes the strategy selects the top models according to the preference objective
MicF 1and AP , respectively.  X  X N. This is the ensembling model selection strategy.  X  X L-RBF [Zhang 2009]. This is the base model in our algorithm, which is used as baseline.
 We do experiments on all seven datasets with the same parameters in Section 5.1.3. Similarly, we test the average values of all strategies on nine metrics and summa-rize their rank values. We only show the average rank results in Table IX due to the space limitation. Generally, these strategies achieve their best performances on differ-ent criteria and they consistently and significantly outperform the baseline ML-RBF .In addition, we can find that M OML usually achieves best performances on its preference objective no matter whether the preference objective is or not the optimization objec-tive. For example, DYN(HL) and DYN(RL) perform best on their preference objectives HL and RL , DYN(MicF1) and DYN(AP) have the performance improvement on the preference objectives (i.e., MicF 1and AP ) as well as other objectives (e.g., SL , Co v , Acc ). Compared to the baseline ML-RBF , the ensembling model selection strategy (i.e., EN) overall improves performances on almost all objectives, but it cannot achieve best performances on any special objectives. As we have noted, the time-consuming model training phase can be done offline, whereas the model selection phase is fast, which can be done online according to user X  X  preference. If a user has apparent preference, he can employ the preference objective in the DYN strategy to make better classification on his preference. If users have no obvious preferences, the EN strategy can be adopted. In this article, we first studied the multi-objective multi-label classification problem and proposed a novel algorithm M OML .M OML can simultaneously optimize over multiple objectives and return a set of solutions. In applications, users can flexibly select models in terms of their preferences. Experiments show that M OML not only achieves the better performances on the optimization objectives, but also improves the performances on most of the other state-of-the-art criteria for multi-label classification.
