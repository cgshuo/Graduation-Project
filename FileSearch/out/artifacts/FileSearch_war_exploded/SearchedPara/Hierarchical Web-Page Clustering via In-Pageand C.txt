 Web page clustering has been studied extensively in the literature as a means to group pages into homogeneous topic clusters. However, much of the existing study [1] [7] [18] [9] is based on any arbitrary set of pages, e.g. , pages from mul-tiple websites. Limited work has been done on clustering pages from a specific website of an organization. Despite of the wide diversity of webpages, webpages residing in a particular organization, in most cases, have some semantically hi-erarchic structures. For ex ample, the website of a com puter science department may contain a large set of pages about its people, courses, news and research, among which pages of people can be categorized into the ones of faculty, staff and students, and pages of research may dive rsify into different areas. Uncovering such hierarchic structures could supply users a convenient way of comprehen-sive navigation, accelerate other search ing and mining tasks, and enables us to provide value-added services.

This is, however, a challenging task due to the semantic and structural het-erogeneity of the webpages. Nevertheless, one can observe that the information in a site is usually organized according to certain logical relationships, e.g. ,re-lated items are often organized together in a way that is easier for users to find relevant information items. For example, in a university department, there is usually a page about its faculty, a page about its courses, etc. .Exploringsuch site organizational information, i.e. , information item togetherness , will help us cluster the items of the same type.

From an implementation point of view, such togetherness is typically man-ifestedinHTMLcodethroughtwomeans: in-page structures and cross-page hyper-links . Information items of the same type are usually coded as sibling nodes of the same parent in the HTML tag tree ( i.e. , DOM tree ), and links that represent similar items often reside together in a page as siblings, forming paral-lel links of a page. Such page structure and parallel links provide an important clue in the design of similarity functions for meaningful clustering.
Based on this idea, we develop a novel method, HSClus , for hierarchical site clustering of webpages in order to discover the inherent semantic structure of an organization X  X  website. Our major contributions include: 1. Deriving from DOM trees, a novel concept called parallel links is proposed, 2. A new clustering algorithm called HSClus is designed to group densely linked Our experiments show that HSClus is efficient and effective at uncovering web-page structures at some organization X  X  website, which sets a foundation for fur-ther mining and exploring web semantic structures. Spectral partitioning [5] is a group of one-level network clustering algorithms, which targets to cutting a graph into a set of sub-graphs, i.e. , clusters, with an object function that minimizes the numbe r of cross-cluster edges and maximizes the number of in-cluster edges. Because its solution relies on the calculation of eigen values, the time complexity is square to the number of edges. Agglomerative hierarchical clustering [11] [3] treats each data point as a singleton cluster, and then successively merges clusters until all points have been merged into a single remaining cluster. However, these methods are sensitive to outliers. DOM tree structures have been widely used for webpage segmentation and partitioning. As the correspondence of in-page parallel links in this paper, [14] enhances web page classification by utilizing labels and contents information from sibling pages. Web patterns [10] are formalized descriptions of common features of objects on web pages. Each page is presented by a vector of pattern weights, which record the extent of importance of the pattern for the web page. Based on pattern vectors, similarity between pages is defined. To automatically extract main classes of pages offered by a website, [4] compares structures of DOM trees. In order to improve search results via text contents [1] uses the path length and [7] uses weighted path between two pages to adjust c lusters. [19] combines out-links, in-links and terms in page contents to improve the clustering quality on web search results. [2] finds dense units by density-based algorithms, and then merges units by agglomerative hierarchical clustering. This section takes a set of pages P = { p 0 ,...,p n  X  1 } at an organization X  X  website as input objects , and outputs a similarity matrix among pages in P for the clustering algorithm introduced in latter sections.

Web pages contain abundant information about link structures that can help discovering web clusters, which is mainly in two categories: cross-page link-structures and in-page link-structures . The former one refers to the link graph among webpages, while the latter one refers to the organization of links inside an individual page. If we regard cross-page link-structures as web structures at the macro-level, then in-page link-structures are the one at the micro-level. Combining macro-and micro-levels of web structures will gain great power for link-based web page clustering. 3.1 Cross-Page Link-Structures Co-citation [15] and bibliography-coupling [8] are two popular measures in the analysis of link graph. Concretely, for pages p i and p j , their co-citation C ( i, j ) and bibliography-coupling B ( i, j ) are defined as the frequencies of common in-and otherwise E ( i, j )=0.Weuse Cosine function to calculate the similarity Sim CB ( i, j ) gained from C ( i, j )and B ( i, j ) for pages p i and p j 1 : 3.2 In-Page Link-Structures The DOM (Document Object Model) is a platform-and language-independent standard object model for representing HTML or XML documents. Building DOM trees from input web pages is a necessary step for many data extraction algorithms. Furthermore, nodes in DOM trees are written in the form of tags, indicating the structure in a web page and a way of hierarchically arranging text-based contents. Formally, we use DOM ( i ) to denote the DOM tree extracted from the source code of a particular web page p i with trivial HTML tags removed. For a tree node  X  in DOM ( i ), the sub-tree rooted at  X  is denoted by DOM  X  ( i ).
In this sub-section, we will introduce Parallel Link as a novel concept derived from DOM trees. Note parallel links are independently extracted from each page of the targeting website, which reasonably assumes the homogeneity of the layout and the contents inside one particular page, e.g. , the homepage of a laboratory may list hyper-links to its professors together and the link of each professor is followed by the professor X  X  name and then by the email. Here the consecutive positions of these hyper-links and the homogeneous organization of each profes-sor X  X  information are good examples of in-page link-structures, which give strong hints of the semantic meaning of these professors X  pages. It is necessary to un-derstand that it does not make any assumptions about the homogeneity of pages among the whole website.
 become a group of Parallel Sub-Trees if DOM  X  s ( i )and DOM  X  t ( i ) for any s, t  X  1 ..k are exactly the same (including the tree structures and the HTML tags). Tree nodes  X  1 ,  X  2 ,  X  X  X  ,  X  k form a group of Parallel Nodes if they locate in the same position of DOM  X  1 ( i ), DOM  X  2 ( i ),  X  X  X  , DOM  X  k ( i ), respectively, and furthermore become a group of Parallel Links if their HTML tags are  X  X yper-of parallel links with the size no less than 4. The similarity Sim P ( i, j ) of pages p and p j gained from in-page link-structures equals to how many times p i and p j appear in a group of parallel links. 3.3 Consolidating with Content-Based Similarities The final similarity Sim ( i, j ) for pages p i and p j is: Here SIM content ( i, j ) can be obtained by any kind of content-based similarity functions [6] [20]. w 1 and w 2 are parameters that tunes linear weights among the three parts. Different values of w 1 and w 2 express different emphasis to structure-based and content-based similarities. There could be more than one good answers for a page clustering task. It is not a competition between two runners ( i.e. , structure-based and content-based similarities) to see which one has the better performance, instead we are installing two engines for more effective similarity functions as well as clustering results. Although there have been many clusteri ng algorithms developed for web ap-plications, we choose to further develop the density-based approach with the following reasoning. 1. Web clusters may have arbitrary shapes and the data points inside a cluster 2. Web datasets are usually huge. Density -based clustering can be linear to the 3. Web clusters may vary a lot in size, and w eb datasets contain noises. Spectral
SCAN [17] is a one-level density-based network clustering algorithm, of which one clear advantage is its linear time complexity that out-performs other meth-ods. However, SCAN requires two parameters, and the optimal parameters that lead to the best clustering performance are given by human via visualization. In this section, we extend SCAN to a hierarchical clustering algorithm, called HSClus .
 Algorithm Framework. It is natural to derive HSClus from SCAN by itera-tively applying SCAN to each cluster obtained by SCAN in the previous step. However, since different sets of pages may have different optimal parameters, it is infeasible to select two fixed parameters as the input for each call of SCAN. To solve this problem, HSClus (i) tests SCAN with different pairs of parameters, (ii) uses a scoring function to evaluate the clustering results under different parame-ters, and (iii) finally clusters pages by the optimal parameters. For each resulting cluster, HSClus repeats the same procedure until termination conditions are met. Because of the space limitation, details are omitted.
 Complexity Analysis. Usually, the number of levels L in a clustering hierarchy is small ( e.g. , no more than 10), and we select a constant number (say K )of parameters to test. Since SCAN is linear to the number of edges m ,thetime complexity of HSClus is also linear, which is O ( LKm ). In this section, we evaluate the e fficiency and the effectiveness of HSClus on both synthetic and real datasets. All algorithms are implemented in Java Eclipse and Microsoft Visual Studio 2008, conduc ted in a PC with 1.5GHz CPU and 3GB main memory. We compare HSClus with two algorithms: (i) k -medoids [13] and (ii) FastModularity [3]. 5.1 Effectiveness A real dataset UIUC CS is the complete set of pages in the domain of cs.uiuc.edu crawled down by Oct. 3, 2008. It has 12 , 452 web-pages and 122 , 866 hyper-links. The average degree of each page is 19 . 7, and 33 , 845 groups of parallel links are discovered.

To evaluate the usefulness of parallel links, Figue. 1 and 2 show the clus-tering results with and without similarities gained from in-page link structure. As observed, the two figures are generally the same at high levels; in low lev-els, Figue. 2 may mix pages that are in different kinds but have close se-mantic meanings, e.g. , Research/F aculty and Research/Area alternate, and Undergraduate/T ransfer is in the middle of Undergraduate/Course .

Figue. 3 and 4 are the results generated by FastModularity and k -medoids (some parts are omitted), respectively. We can observe that, the clustering qual-ity is much lower than HSClus .
 5.2 Efficiency To verify that HSClus is as fast as linear against networks of different sizes, we generate 8 synthetic graphs, whose numbers of edges range from 2 , 414 to 61 , 713 , 102, to test corresponding running times. We can see in Figue. 5that the running time (in second) of HSClus is linear against the i nput size (number of edges); FastModularity increases more quickly than linear; and k -medoids rises dramatically. This paper develops a novel method for hierarchical clustering of webpages in an organization in order to discover the inherent semantic structure of the website. Both cross-page link structure and in-page link organizations are explored to produce a new similarity function, and a new density-based clustering algorithm is developed to group densely linked webpages into semantic clusters and iden-tifies their hierarchical relationships. Our experiments show that this method is efficient and effective at uncovering we bpage structures at some organizations websites and sheds light on mining and exploring web semantic structures.
