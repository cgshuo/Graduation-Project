 YOUXI WU, ZHENYU LU, TIANYU CAO, and XINDONG WU , University of Vermont Impact craters, the structures formed by collisions of meteoroids with planetary sur-faces, are among the most studied geomorphic features in the solar system because they yield information about the past and present geological processes and provide the only tool for measuring relative ages of observed geologic formations [Crater Analysis Techniques Working Group 1979; Tanaka 1986]. However, advances in surveying craters present in images gathered by planetary probes have not kept up with the advances in collection of images at ever-higher spatial resolutions. Today, as in the past, efficient crater detection in planetary images remains as a daunting task due to the following challenges [Kim et al. 2005]. (1) Challenge 1: Lack of distinguishing features. Craters, as a landform formation, (2) Challenge 2: Heterogeneous morphology in images. Planetary surfaces are not homo-(3) Challenge 3: Huge amount of subkilometer craters in high-resolution planetary
As a result, comprehensive catalogs of craters are restricted to only large craters using manual inspection of images, for example, 42 , 283 Martian craters with diameters larger than 5 km [Barlow 1988], and 8 , 497 named lunar craters with diameters larger than a few kilometers [Andersson and Whitaker 1982]. There are millions of smaller craters waiting to be identified in a deluge of high-resolution planetary images but no means for their efficient identification and comprehensive analysis. If left to manual surveys, the fraction of cataloged craters to the craters actually present in the available and forthcoming imagery data will continue to drop precipitously. Crater autodetection techniques are needed, especially to catalog small subkilometer craters that are most abundant. Surveying such craters is ill-suited for visual detection, due to their shear numbers, but well-suited for an automated technique. In summary, automating the process of small crater detection is the only practical solution to a comprehensive surveying of such craters.

This article partially addresses Challenges 1 X 3 by designing an innovative frame-work that uses feature extraction, feature selection, and supervised boosting ensemble learning. The three key components of proposed method are as follows.  X  Utilizing mathematical morphology on shape detection for efficient identification of regions indicative for craters. Due to the shear number of small sub-km craters discussed in Challenge 3, a practical crater detection tool must use computational time wisely. We adapt the concept of crater candidates introduced by Urbach and
Stepinski [2009]. Crater candidates are the regions of an image that can potentially contain craters. The benefits of identifying crater candidates at an early stage are two-fold: (i) Significant computational time is reduced at later stages of complicated calculations on feature extraction and classification, where crater candidates are used instead of pixel-based image blocks that are calculated from exhaustive search of the entire image. (ii) The number of false positive detections is reduced at the stage of classification, because a large portion of the image, including background, is removed from being classified.  X  Using a combination of image texture features and a family of supervised boosting ensemble learning algorithms to yield a highly accurate classifier. Targeting at Chal-lenge 1, we are the first research team that contruct image gradient texture features from crater candidates for rapid feature extraction. Those gradient texture features can efficiently capture the underlying image gradient structure without requiring prior domain knowledge. A set of base learners are built from those texture features and combined to build a strong classifier using boosting ensemble learning.  X  Applying transfer learning to feature selection and classifier induction, in order to minimize training for the application of a crater detection tool to a heterogeneous planetary surface. As discussed in Challenge 2, an unseen test site may contain craters that are different from those in the training site. A set of transfer learning algorithms are newly designed to transfer knowledge from an old training site to a new unseen test site. We propose TL-Random, TL-Max, TL-Min, and TL-MaxMin algorithms to sample new test instances and add them into the existing training set, using random sampling, sampling of maximum, minimum, and combined maximum and minimum distributions, respectively.

The entire framework is evaluated on a large, high-resolution image of Martian sur-face (37 , 500  X  56 , 250 m 2 ), featuring high density of small sub-km craters and spatial variability of crater morphology. The proposed boosting ensemble learning algorithms with transfer learning achieve an F1 score above 0.85 on crater detection, a significant improvement over the other crater detection algorithms. The transfer learning algo-rithms have proved powerful on regions where surface morphology differs as charac-terized by the training set. The experimental results demonstrate robustness and good accuracy that validate our approach and make it feasible to construct a robust and reli-able crater autodetection framework that can be widely adopted for planetary research.
The rest of the article is organized as follows. Section 2 discusses the proposed crater detection framework: Sections 2.1 and 2.2 explain how to construct crater candidates and image texture features from those candidates. Section 2.3 provides a brief review on unsupervised versus supervised crater detection methods. Section 3 introduces our ensemble boosting algorithms used for crater detection with and without using trans-fer learning. Section 4 presents our empirical study on finding craters in a large, high-resolution planetary image. Section 5 summarizes our work and discusses future directions. The flow chart indicating components of our method is shown in Figure 1. A key in-sight behind our method is that a crater can be recognized as a pair of crescent-like highlight and shadow regions in an image (see Figure 2). Pairwise crescent-like shapes are identified from images using a shape detection method based on mathematical morphology [Urbach et al. 2007], and those that can be matched are used to construct crater candidates [Urbach and Stepinski 2009], the locations where craters likely re-side. The input objects of supervised learning are derived from image blocks containing crater candidates and the classification is performed on feature vectors based on image texture features. In order to reduce the load on the classification module, we first identify crater can-didates: parts of an image that contain crescent-like pairs of shadows and highlights. Identification of crater candidates is achieved using an image processing method based on mathematical morphology proposed by Urbach et al. on object detection in Urbach and Stepinski [2009] and Urbach et al. [2007]. Figure 3 shows a flow diagram of the method used for identification of crater candidates. The highlight and shadow shapes are processed in parallel using inverted image to process the shadow shapes. The goal is to eliminate all the shapes that are not indicative of craters while keeping the high-light and shadow shapes. The step of Background Removal deletes shapes, such as mountains, that are too large to be part of the craters; the Power Filter removes shapes that lack sufficient contrast; the Area Filter removes shapes that are too small for reliable crater detection; the Shape Filter uses shape attributes that are invariant to translation, rotation, and scaling to preserve or remove regions of an image exclusively on the basis of their shapes. Utilization of the Shape Filter, that requires only a single parsing of an image, improves performance by a factor of 5 to 9 in comparison with other shape detection methods [Urbach et al. 2007]. In the final step, highlight and shadow regions are matched so that each pair corresponds to a single crater candidate. This method does not have high enough accuracy to constitute a stand-alone crater de-tection technique, but is ideal for identification of crater candidates. More calculations must be performed to discriminate craters from noncraters in those crater candidates. Compared to the shape features used in Urbach and Stepinski [2009] that results less satisifying results on crater detection (experimental results will be given in Section 4.6), in this article, we construct image texture features from the crater candidates to be used by the classification algorithms. We use image texture features reminiscent of Haar basis functions which were first proposed in Papageorgiou et al. [1998] for detection of objects and later popularized by Viola and Jones [2004] in the context of face detection. These features can be thought of as image masks consisting of black and white sectors. Different from vertical and horizontal rectangle features used in face detection [Viola and Jones 2004], we specially design nine square mask-features shown in Figure 4. A symmetric square mask is used because a crater to be identified is in a symmetric shape. A mask in different scales is scanned through the region of a crater candidate. Each position of the mask produces a single feature value. The value of a feature is the difference between the sum of gray scale values in pixels located within the white sectors and the black sectors. The number of features is equal to the number of masks used multiplied by the number of positions overlaid by those masks. All features can be calculated very efficiently in one image scan, using an integral image data structure [Viola and Jones 2004].

To represent a crater candidate in terms of Haar-like features, we first extract square image blocks around each crater candidate. In our experiments, we use the size twice that of the candidate in order to include regions surrounding crater rims. The un-derlying texture information of each crater candidate is encoded in the set of nine mask-features in different scales, having various granularities and positioned at finely sampled locations. Thus an image containing a crater candidate and its immediate surroundings is described by thousands of texture features. Those features are not independent from each other and those over-complete features compensate the limited texture information a single square mask-feature can capture. Underlying gradient texture information is encoded by those features without the requirement of prior do-main knowledge. If a single simple feature can be viewed as a weak learner, that is, only using this feature to classify crater candidates by constructing a single-node de-cision tree, it is a natural choice to build a strong ensemble classifier out of thousands of weak learners, using the boosting approach. Salamuniccar and Loncaric [2007] provided an extensive review of all previous research on crater detection algorithms. Existing efforts on detecting craters in planetary images can be divided into two general categories: unsupervised approaches and supervised approaches.

The unsupervised methods identify crater rims in an image as circular or elliptical features [Leroy et al. 2001; Honda et al. 2002; Cheng et al. 2002; Bandeira et al. 2007; Kim et al. 2005]. In particular, the original image is preprocessed [Leroy et al. 2001; Bandeira et al. 2007; Kim et al. 2005] to enhance the edges of rims, and the actual detection is achieved by means of the Hough transform [Hough V 1962] or genetic algorithms [Honda et al. 2002]. Unsupervised methods have the advantage of being fully autonomous but the performance is usually at least one magnitude less accurate than supervised methods.

The supervised methods [Burl et al. 2001; Vinogradova et al. 2002; Wetzler et al. 2005] take advantage of domain knowledge in the form of labeled training sets that guide classification algorithms. In Burl et al. [2001] and Vinogradova et al. [2002], a continuously scalable template model technique was used to achieve detection. In Wetzler et al. [2005], a number of algorithms were tested and the Support Vector Machine algorithm was shown to achieve the best rate of crater detection. More recent methods [Kim et al. 2005; Martins et al. 2009] incorporated techniques originally developed [Viola and Jones 2004] for the purpose of face detection. These methods concentrated on the classification component of crater detection and did not incorporate identification of crater candidates or transfer learning, as what has been extensively studied in this article.

Notice that previous research on crater detection algorithms X  X upervised and unsupervised methods X  X ocused predominantly on partially addressing Challenge 2, in which morphologically identical craters exhibiting different appearances in different images [Leroy et al. 2001; Honda et al. 2002; Cheng et al. 2002; Bandeira et al. 2007; Kim et al. 2005; Vinogradova et al. 2002; Wetzler et al. 2005; Burl et al. 2001; Martins et al. 2009]. In addition, the bulk of previous work relies on inefficient exhaustive search of the entire image using pixel-based approaches. This may work for finding a small number of large craters in low-resolution images, but not for finding a very large number of small craters in high resolution-images. Billions of pixels in a high-resolution planetary image inevitably become a bottleneck of scalability of those crater detection methods.

The problem of finding crater candidates has only recently been raised in Urbach and Stepinski [2009], but the relatively low crater detection rates using a decision tree J48 are reported due to the use of less discriminative geometric shape features. Urbach and Stepinski X  X  method uses a small set of features (16 features used in their experiments) to describe the shapes of the shadow and high regions of crater candidates. However, other noncrater landforms in similar shapes makes using shape features an unideal choice on crater detection. It is well known that the classification performance is primarily controlled by the quality of features. In this article, we use a large set of texture features (1089 features used in our experiments) in combination of boosting ensemble learning algorithms to achieve better accuracy on crater detection. Detailed comparison will be presented in Section 4.6.

To the best of our knowledge, the problem of transfer learning in the context of autodetection of craters has not been previously addressed. This omission renders most existing approaches impractical for planetary research as the benefit of automation decreases significantly if new training sets need to be established for every new image or even for various segments of the same image. In the next section, we will design several supervised algorithms, some of which integrate transfer learning. To classify crater candidates into craters and noncraters on the basis of texture fea-tures, we have designed and implemented three supervised learning algorithms. These algorithms simultaneously select subset features necessarily for accurate classification and train the final ensemble classifier based on the supplied training set. The first is the Boost algorithm, a variant of the AdaBoost algorithm inspired by the methodology of face detection [Viola and Jones 2004]. The second is the Naive algorithm, a drastic simplification of the Boost algorithm using a greedy approach instead of the boosting method. The third is the TL algoritm, a transfer learning algorithm using four different sampling methods. Table I gives a brief summary of the three algorithms. A crater candidate at this stage is represented as a feature vector x = f 1 ,..., f N . Each feature f i , i = 1 ... N , is produced by a square mask-feature in a particular position overlaying the cater candidate.

The Boost algorithm (see Algorithm 1) generates a sequence of weak classifiers h t ( f ) and combines them through a weighted boosting approach to build a strong ensemble classifier H ( x ): where T is the number of iterations, t = 1 ,..., T ; f , f  X  X  f 1 ,..., f N } , is the single feature selected at each boosting iteration to construct a weak classifier h t ( f ),and  X  t is the learned weight of hypothesis h t ( f ) when adding the newly selected weak classifier into the ensemble. The Boost algorithm (Algorithm 1) iteratively selects one feature at a time and stops when reaching T iterations; note that T N . Different from the traditional AdaBoost algorithm that usually uses the entire feature set, Boost at each iteration selects only one best feature at one time. Thus feature selection is integrated into the boosting iteration. Three core steps are required to complete one boosting iteration. (1) Weak Classifier Learning . The construction of a weak classifier h t ( f )ona (2) Feature Selection . Calculate the weighted error sum of each weak classifier and (3) Weight Updating . Update weights using the same method proposed in AdaBoost As depicted in Algorithm 1, steps 2 X 4 are used for Weak Classifier Learning and Feature Selection, and step 5 is for Weight Update. The number of craters is usually less than the number of noncraters. The initial weight of each training instances is designed to cope with imbalance data by using different group average weights in the positive and negative classes, respectively. The weights of positive examples are not necessarily the same as those of negative examples, whereas every positive example (a true crater) in the training set has the same weight and every negative example (a noncrater) shares the same weight.

In order to reduce the computational cost of the Boost algorithm, we design a simpli-fied greedy version of the algorithm and call it the Naive algorithm (see Algorithm 2). The Naive classifier uses the same Weak Classifier Learning step and selects the top T best features using the weighted error sum in the step of Feature Selection as a criterion without any further iterations on the step of Weight Updating.

Time Complexity Analysis. The time complexity of the Boost algorithm is O ( TNn ), where n is the number of training examples, N is the number of total features, and T is the number of boosting iterations. In particular, each feature produces n weak classifiers, based on each feature value for every training example according to the threshold  X  ; N features produce Nn classifiers; it takes O ( Nn ) time to find the weak classifier that produces the minimum error; and it takes O ( TNn ) time to select the top T features after T boosting iterations.

The time complexity of the Naive algorithm is O ( Nn ) as no boosting iterations are performed. Interestingly, the Naive classifier performs decently well in some circum-stances during our real-world case study (see Section 4). Boost and Naive assume that both training and testing instances are drawn indepen-dently and identically from the same underlying distribution. What if training and test instances are from different distributions? We have designed a transfer learning based algorithm, inspired by the TrAdaBoost algorithm [Dai et al. 2007], which is capable of transferring knowledge from the old training data to the new test data. We refer to it as the TL algorithm. In principle, transfer learning algorithms are often used when the training set and test set are not in the same feature space or have the same distribution [Pan and Yang 2010]. The TL algorithm (Algorithm 3) has the same three steps as the Boost algorithm, but the Weight Updating step is different as it attempts to transfer knowledge from the original training set to the new test data. As the Boost algorithm is not expected to perform well if the test data has a different distribution from the training data, because the critical set of features that best serves to distinguish craters in the training set may not be the same as that in the test set.

We denote the previous original training data as the diff-distribution training data; and here we are uncertain about the similarity and usefulness of this data for the new task. We denote the additional small portion of labeled test data, which is a repre-sentative of the new set of crater candidates, as the same-distribution training data. During the training process, we apply the Boost algorithm to the same-distribution training data to build a model; the weights of misclassified examples are increased during the next iteration while the weights of correctly classified examples are de-creased. The key component is that we transfer knowledge from the old training data to the new test data by modifying the weights of misclassified examples from the diff-distribution training data. Those misclassified examples are considered as the ones that are dissimilar to the same-distribution examples and should be deemphasized. Accord-ingly, we decrease (not increase) the weights of those examples in order to weaken their impact. The weight-changing mechanism selects good examples (similar to the labeled test data) from the old training data to compensate the insufficient training examples in the same-distribution data. The change of weight factor  X  = 1 misclassified examples from diff-distribution and the threshold voting T t = T the diff-distribution converges to zero [Dai et al. 2007; Freund and Schapire 1995]. There are two major differences between the TL algorithm and the existing algorithm TrAdaBoost [Dai et al. 2007]. (1) Feature Selection . We use an embedded approach in feature selection (steps 3 X 4 (2) Sampling method from the test set . TrAdaBoost uses random sampling to choose
For the TL algorithm, we extracted some samples from a test set to compose the same-distribution set. When a training set and a test set are in different distributions, the quantity of the diff-distribution set, a.k.a. the original training set, is inadequate to train a transferable classifier. After combining the diff-distribution set with the same-distribution set, the quality of newly selected samples may have a great influence on classifier training. Apparently, randomly selecting samples to construct the same-distribution set cannot guarantee the quality of the same-distribution. We take into consideration of the distribution divergence when constructing the same-distribution set. Normally, the samples that distribute significantly differently with the training samples should have more contribution for classifier induction. However, the samples which are greatly different from the main trend distribution could be outliers thus lead to wrong training results. Furthermore, the test samples which have very similar distribution with the training samples may also be useful, as those samples may not be in the same class with those in the training set. For example, the sample in the test set is a crater but the samples in the original training set which share a similar distribution may not be craters. Therefore, the testing samples in large and small distribution differences to the training samples have their own benefits and deficiencies.
In order to make the training process geared to the new knowledge gained in the same-distribution, we propose to use three new methods, TL-Min, TL-Max, and TL-MinMax, to build a same-distribution set, considering the closest distribution, farthest distribution, and combined cases, respectively. The detailed construction method is in Algorithm 4. To calculate the divergence of the samples, we firstly quantize all the training samples and testing samples with a certain bin number and rerepresent all the samples by a probability distribution (step 1). The quantization range is deter-mined by the minimum and maximum value of input samples. Kullback-Leibler(KL) divergence [Kullback and Leibler 1951] 1 is applied for the probability distribution divergence calculation (step 2). We use Min(Max)-distribution divergence vectors to find the test instances closest (farthest) to the instances in the diff-distribution set (steps 3 X 4). A TL-Min filter is constructed to select same-distribution samples with the minimum distribution difference, a TL-Max filter for the maximum distribution difference, and a TL-MinMax fileter for the combination of these two filters to form a same-distribution set. After the same-distribution set is constructed, feature selection and classifier induction are conducted using the TL algorithm described in Algorithm 3.
Time Complexity Analysis. The time complexity of the TL algorithm is the combination of the same-distribution construction and the boosting calculation. Same as the Boost algorithm, the boosting step takes O ( TNn ), where n is the number of training examples, N is the number of total features, and T is the number of boosting iterations. It takes O ( mn ) on the construction of the same-distribution, where m is the number of testing examples, because each test instance needs compare to each training instance. Thus the TL algorithm has a time complexity of O ( TNn + mn ). We have selected a portion of the High-Resolution Stereo Camera (HRSC) nadir panchromatic image h0905 [HRSC Data Browser 2009], taken by the Mars Express spacecraft, to serve as the test set. As illustrated in Figure 15, the selected image has a A domain expert manually marked  X  3 , 500 craters in this image to be used as the ground truth to which the results of autodetection are compared. The image repre-sents a significant challenge to automatic crater detection algorithms because it covers a terrain that has spatially variable morphology and because its contrast is rather poor (which is most noticeable when the image is inspected at a small spatial scale). We di-vide the image into three sections denoted as the west region, the central region, and the east region (see Figure 15). The central region is characterized by surface morphol-ogy that is distinct from the rest of the image. The west and east regions have similar morphology but the west region is much more heavily cratered than the east region. In the first stage of our method, we identify 13,075 crater candidates in the image using the pipeline depicted in Figure 3. The dataset is imbalanced as the majority objects are noncrater candidates. 1,089 image texture features are constructed using the 9 square-mask features described in Figure 4. The training set for the Boost and Naive algorithms consists of 204 true craters and 292 noncrater examples selected randomly from amongst crater candidates located in the northern half of the east region. Thus, the training set uses only 3.75% of the total dataset. Note that we have purposely restricted the locations of examples in a training set to a specific sector of the image in order to mimic actual planetary research; it is likely that in current studies such craters are identified in a specific region and are in need of identification by a supervised learning algorithm in the rest of the image. For the TL algorithm results shown in Figure 5, we have constructed an additional training set (same-distribution set), using random sampling(TL-Random), consisting of 253 crater candidates (102 true craters and 153 noncraters) selected from random locations throughout the entire image. The ratio between the false and true examples in the The original training set consisting of 496 examples from the northeastern section of the image serves as the diff-distribution set.
 The table in Figure 5 summarizes the performance results of crater detection by the three algorithms: Boost, Naive, and TL. The ground truth of the entire image serves as an external criterion to evaluate the performance of the three algorithms on the unseen test set. Of the three algorithms, the number of features used to construct a strong classifier and the values of the threshold  X  are selected to maximize the performance of each classifier.

The candidate data has an imbalanced class distribution and the successful detection of true craters is more significant than the detection of noncraters. Hence we use recall ( r = TP TP + FN ) and precision ( p = TP TP + FP ) and F1 as the evaluation metrics, where TP stands for the number of true positive detections (detected craters that are actual craters), FP stands for the number of false positive detections (detected craters that are actually not), and FN stands for the number of false negative  X  X etections X  (nondetection of real craters). F1 measures the harmonic mean between precision and recall 2 1 values of precision, recall, and F1 are listed, and the best performance of each measure is highlighted in bold. A precision score of 1.0 means that every object classified as a crater is indeed a crater but says nothing about the number of craters that are not recognized by classifiers as such. A recall score of 1.0 means that every true crater is classified as such but says nothing about how many other landforms were incorrectly classified as craters. An F1 score of 1.0 means that all the existing craters are correctly identified and all the objects classified as craters are true craters.
 Of the three algorithms compared, the TL classifier using random sampling (TL-Random) yields the best precision in all regions and the Naive classifier yields the worst precision in all regions. On the other hand, the Naive classifier has the highest recall in all regions whereas the TL classifier has the lowest value of recall, except in the east region, where the Boost classier has the lowest value of recall. Overall, the TL classifier has the highest value of F1 in all regions except the west region where the Naive classifier has the highest value of F1.

The Naive classifier performs surprisingly well considering its simple nature and low computational cost. We take an in-depth look into the performance of the Boost and Naive classifiers on the northeastern section of the image containing 1406 crater candidates of which 496 constitute a training set for both algorithms. Figure 6 shows the precision, recall, and F1 for these classifiers as a function of the number of features selected to construct a strong classifier. The Boost classifier clearly outperforms the Naive classifier on F1 and precision measures if more than 100 features are selected. However, the recall measures of the two classifiers remain comparable regardless of the number of selected features. Thus, the Boost classifier is superior to the Naive classifier on crater candidates that closely resemble those in the training set, but that disadvantage decreases and/or disappears when classifying crater candidates that are less similar to those in the training set. We link the relatively small advantage (or lack of advantage) of the Boost classifier over the naive classifier to the peculiarity of image texture features in the context of crater detection. Top features (weak classifiers) are actually quite strong performers by themselves capable of achieving an F1 score as high as 0.81. These features limit the advantage of the boosting algorithm that works best with an ensemble of weak classifiers. In order to better understand the results of the three proposed algorithms Boost, Naive, and TL, it is useful to assess dissimilarity between the set of feature vectors in the original training set and those in the west, central, and east regions. Figure 7(a) shows such dissimilarity as measured by the KL-divergence; Figure 7(b) plots the F1 scores graphically of the three regions. Clearly, the central region is most dissimilar to the training set, whereas the east region is the most similar (since the training set was selected from the northeastern portion of the image). This is why the TL classifier performs best (relatively to the other classifiers) in the central region. It is expected that the TL classifier would have the least advantage in the east region, as it is the region best characterized by the training set, but the results show that the TL classifier has the smallest gain (if any) in the west region. This can be explained by the fact that the west region has a similar character to the east region, but is much more heavily cratered, so in fact, relatively fewer additional training samples come from these regions resulting in no sufficient information gain to be exploited by the TL classifier.

Randomly selecting samples from the test set cannot always guarantee the quality of the selected samples. Thus, we apply the distribution divergence analysis filters to select the cotraining samples. We test the TL-Min, TL-Max, TL-MinMax filters, and TL-Random on the north half the west region (denoted as Region 1) and the north half of the central region (denoted as Region 2). Region 1 is selected as a site that closely resembles the training set, and the region is also featured with high-density subkilometer craters. Region 2 is used as a site that has a heterogeneous surface with different morphology from the training set.

The distributions of these two test sets Regions 1 and 2, and the training set are reported in Figures 8 and 9. In each figure, all the samples from the test set and training set are quantized into 1 to 50 bins. The bin sizes of different figures may be different due to different distributions of the two test sets, thus the training set curves may vary in those two figures. In Figures 8(a) and 9(a), we can find that the blue curve is very similar to the red curve, where the blue/red curve denotes positive test/training examples (craters). However, In Figures 8(b) and 9(b), the blue curve always has big differences with the red curve, where in those figures the blue/red curve denotes negative test/training examples (noncraters). This illustrates that the craters (positive samples) are always similar and the noncraters (negative samples) are different with each other in their own ways. Furthermore, the test samples in Figure 8 distribute significantly differently from the training set than those test samples in Figure 9, which means the model trained from the training set may be more suitable in Region 1 than Region 2 because the significantly different surface morphology in Region 2. Figure 10 shows the KL-divergence and probability distributions, between positive and negative examples in the training set and Regions 1 and 2, respectively. The divergence between Region 2 and the training set is almost 3 times larger than the divergence between Region 1 and the training set.
 The experimental results of the four algorithms, TL-Random, TL-Min, TL-Max, and TL-MinMax, are reported in Figure 11 for Region 1 and Figure 12 for Region 2. Figure 11 indicates that when the samples are not sufficient, TL-MinMax slightly outperforms the TL-Random and achieves its peak F1 score 0.8532 with 90 same-distribution samples. Because Region 1 is similar to the training set, the TL-Min has less contribution to improve classification performance. And TL-Max achieves better results when there are sufficient samples to select. In Figure 12, TL-MinMax and TL-Random are comparable and TL-MinMax is slightly better than TL-Random most of the time. Because of the difference between the training set and the test set, TL-Max can select the samples which have significant difference to help reconstruct the model and capture the main trend of the sample distribution. But the performance of TL-Max is limited if there is no sufficient test samples to select. It is instructive to compare top features (weak classifiers) selected by each of the three classification algorithms (Naive, Boost, and TL). Figure 13 shows six top features selected by each algorithm. The top two features selected by the three algorithms con-centrate on the transition between the shadow and the highlight which best define the characteristics of a crater, but there are significant differences between other selected top features. Features selected by the Naive algorithm are relatively strong by them-selves. Most of them utilize the transition between the shadow and the highlight to distinguish craters from no craters, while the next best feature selected by the Boost algorithm always attempts to correct mistakes done by the previous feature. Figure 14 illustrates how the second best feature selected by the Boost algorithm corrects the mistakes by the first best feature, and we can observe that this feature performs well on candidates with shifted shadow regions. Not all top features selected by the TL al-gorithm utilize the transition between shadows and highlights, but rather crater rims. This indicates the new test data has different characteristics on crater edges.
Figure 15 displays the results of the TL algorithm, using top 150 features and the threshold  X  = 0 . 500. Notice that the large craters  X  5000-meter in diameter are in-tentionally not detected as we set the parameters of our algorithm to target small subkilometer craters (large craters on Mars have already been identified manually [Barlow 1988]).
 Table II provides a in-depth evaluation of the TL method with the crater detection method proposed by Urbach and Stepinski [2009]. Our method outpeforms their method on precision, recall, and F1 measure on all regions and each individual region.
We have also tested three representative algorithms for the purpose of a thorough comparative performance study: AdaBoost [Freund and Schapire 1995] with C4.5 as the base leaner for an example of boosting algorithms, SVM [Boser et al. 1992; Joachims 2002] with a linear kernel as an example of kernel-based learning algorithms, and TrAdaBoost [Dai et al. 2007] with C4.5 as the base leaner for an example of transfer learning algorithms. Using all the 1089 features, the F1 score of SVM on all regions is 0.202, AdaBoost is 0.302, TrAdaBoost is slightly better than 0.4. As we can see from Figure 9, the three algorithms designed in this article can achieve an F1 score above 0.85.

The huge performance gain by the three algorithms (Boost, Naive, and TL) is because the proposed algorithms intelligently select and integrate subset of best features out of all 1089 features to build a strong ensembled classifiers using boosting. The 1089 features are overcompleted by contructing 9 maskes in different scales, stepwise, and positions. Without a built-in mechanism on feature selection to remove irrelevant and redunt features, the AdaBoost, SVM, and TrAdaBoost classifiers cannot perform well. Comparable results would be obtained on the crater detection, if similar feature set is used on those classifiers. However, this approach is less desirable as feature selection and classifier induction have already been simultaneously integrated into the learning process of the three proposed algorithms. The aim of this article is to present a robust and reliable framework for autodetection of small craters in high-resolution images of planetary surfaces. This is one of the most challenging problems in planetary science: effective and automatic crater detec-tion from extremely large orbiter images. The framework uses an innovative method that integrates improved techniques on embedding feature selection with supervised classification, and transfer learning. First, we have demonstrated that our method identifies craters with high accuracy. The test site is an HRSC image of Martian scene that presents a heterogeneous region of 37 , 500  X  56 , 250 m 2 , and detecting craters in various forms is challenging using regular algorithms. Our approach can achieve an F1 score above 0 . 85, and provides a reliable mechanism for planetary research. Second, we have demonstrated that a consistently accurate detection can be achieved through transfer learning. Without transfer learning the performance of our algorithms (Boost and Naive) decreases in the central region of the image where surface morphology differs as characterized by the training set. However, using the TL algorithm partially restores the level of performance. Third, we noticed that the Naive algorithm can per-form well in the context of crater detection for a fraction of the computational cost of the Boost algorithm.

We contend that the robustness and reliability of our methodology make it an effec-tive tool for planetary research. If adopted, our approach has great potential to produce surveys of small craters over entire surfaces of planets, thus revolutionizing certain as-pects of planetary science. Our future research will address means of efficient selection of additional training samples for construction of the same-distribution for transfer learning. The goal is to intelligently select samples that exemplify differences between the existing training sets and new candidate sets.

