 Testing algorithms and systems involves trying different sets of parameter values on different domains or data sets. Even for a moderate number of parameters and domains the num-ber of possible experiments can get very large due to the combinatorial explosion. Evaluating the outcome of these experiments requires comparing the results, which is often done by writing a script or inspecting the result files man-ually. For a new algorithm or version, the work has to be done over again. With hundreds, thousands, or even more possible experiments, both the preparation and the evalua-tion can become complex and tedious. In this demonstrator we present a software tool, called ET, for evaluating the pa-rameters of an algorithm or system, either automatically or controlled by the user. It allows to launch large numbers of experiments in just a few clicks, visually explore the results and analyze the performance of the algorithm.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  performance mea-sures Experimentation Structured Testing, Evaluation, Framework
Designers of both algorithms and systems strive to find the best parameter values in terms of performance and ro-bustness, either in general or for a specific domain or data sets. For some algorithms, theory or empirical experimen-tation provide suggestions for parameterization. In many cases, such theory is not available. Testing various options is in practice mostly unavoidable. Consider kernel density estimation as an example. To determine a good kernel band-width, several heuristics are suggested in the literature, some of which require their own parameters. In practice, the choice of a heuristic and its corresponding parameterization has to be validated experimentally.

The nature of the parameters might be numeric, for exam-ple defining a cardinality or setting a weight, or categorical, for example deciding between different strategies for a given algorithm or components of a system. Even with only a few parameters and a moderate number of test data sets, the number of possible experiments quickly exceeds hundreds, thousands, or even higher orders of magnitude.

In practice, experiments are mostly launched individually or using a script. Similarly, the result files are either opened and inspected manually or another script is written, for ex-ample to filter out the best result or to combine several re-sults into a single file. For evaluation, initially one might be interested in the best performing parameter values. Later in the evaluation one might focus on the influence of the parameters on the performance, or the comparison between different strategies or approaches. Another important as-pect is the robustness of a parameter setting over different contexts. For these analyses scripts have to be adjusted or rewritten. Finally, for the next algorithm the work has to be done over again.

In this demonstrator we present a software tool for auto-matic evaluation of algorithms, called ET (evaluation tool). The user provides an executable of his program, defines its objective functions and declares its parameters. ET then automatically generates and executes experiments with dif-ferent parameter settings. The results are plotted in charts and statistics are provided to analyze the performance of the algorithm w.r.t. the objective functions. Unlike exist-ing software frameworks, like KNIME [1] or WEKA [3], for example, our focus is not on providing a collection of al-gorithms and measures. Our goal is to help researchers, algorithm designers and system designers to perform struc-tured testing and evaluation for a large number of experi-ments. We provide details on ET in the next section and discuss application scenarios and the demonstration plan in Section 3. Section 4 contains information on the ET website, video and tutorial, Section 5 concludes the paper.
The steps listed below illustrate the basic workflow of ET, we detail the single steps in the following subsections.
Prepare your program  X  Create an executable of your algorithm and define its parameters. Run jobs automat-ically  X  Test thousands of parameter settings in just a few clicks or let the framework find an optimal parameter set-ting. Explore results  X  Explore the results and see the parameter influences and their robustness.
The main entity in ET is a program . A program cor-responds to an executable of an algorithm, for example a runnable jar file or an executable from C code. It is associ-ated with a set of parameters . For each parameter the user can define a set of allowed values. Several programs can be combined in a program graph . The first node can, for ex-ample, be a data generator which is followed by (connected to) a second node corresponding to a learning algorithm. After creating a new program graph in the GUI, the user can add programs to the graph per drag-and-drop from the list of programs and connect the programs using the context menu.

Table 1 shows an example algorithm for nearest neighbor classification. The five parameters and their corresponding types and values are provided by the user. The user can start the resulting 1200 (= 10  X  5  X  2  X  3  X  4) experiments in a single click. To control the choice and the number of experi-ments, parameter values can be selected and deselected using check boxes in the GUI. This GUI element is called filter and is used as well for exploration and robustness analysis (see below). Selecting only one distance function and one data set reduces the number of experiments to 100 in this exam-ple. The second option is to let ET search for the best pa-rameter setting automatically. The optimization heuristics currently available in ET are steepest descent, simulated an-nealing, and genetic optimization. We are currently working on adding Gibbs sampling [2] and optimization using cov-ering arrays [4], which heuristically avoid the exponential number of experiments due to the combinatorial explosion.
The results of the finished experiments can be explored in the explore perspective (see Figure 1). The workflow in this perspective consists of four main steps as indicated in the screen shot. (1) First a program graph is selected from the list on the top left. (2) In the bottom left part of the explore perspective the filter view is shown, in which the available parameters and parameter values of the programs in the selected program graph are listed. For each program the filter is represented in a separate tab. An additional tab lists all objectives (for example accuracy, runtime, etc.) that are available in the selected program graph. By using the check boxes in this filter view the user can select a subset of the experiments that are explored, for example focusing on a specific strategy or data set. (3) The bottom right corner contains the list of currently displayed results. The user can add results to the list in several ways, for example picking a single setting manually or retrieving the best results with respect to the defined filter. (4) The selected results are then plotted in the chart on the top right. Each experiment from the list below is shown as a diamond in the plot. From each Table 1: Parameters of a nearest neighbor classifier. of these parameter settings, the parameter that is shown on the x-axis is varied (while all other parameters are fixed) and the available results are plotted using a solid line to connect them. This allows to easily analyze the influence of this parameter. The user can customize the plot, for example by changing the parameter that is shown on the x-axis or zooming into an area of interest. The plot data can be exported for example as png, pdf, or as a csv file.
Consider the nearest neighbor classifier from Table 1. As-sume that from the exploration perspective we know that the best result was achieved on the kr-vs-kp data set with k =5, s =0.4, majority =false, and distance =L 2. We now want to investigate whether the parameter values for k and s also deliver competitive results on other data sets and for other distance functions. We call this the robustness ; we will explain how it can be investigated in ET and then continue the example.

To investigate the robustness using ET the user separates the parameters into investigated parameters and context pa-rameters. For each investigated parameter a specific value is fixed and the set of fixed values is called the reference setting . For each context parameter the user selects a set of values using the filter (cf. Sections 2.1 and 2.2). The contexts are formed by all possible combinations of the val-ues from the filter. ET then computes for each context the difference (absolute or relative) between the performance of the reference setting and the best available result for this context, called  X  best . For n contexts this results in n  X  best values, from which ET computes statistics like median ( X  best ), 95-percentile ( X  95 best ), and 98-percentile ( X  98
As above, assume that based on exploration we deem k =5 and s =0.4 good parameter choices. Now we want to in-vestigate whether these are robust w.r.t. different data sets and distance functions (cf. Table 1). k and s become our investigated parameters, (5 , 0 . 4) becomes the reference set-ting, and the set of context parameters is { majority, dis-tance, dataset } . We fix majority to false and end up with 12 contexts (3 distances  X  4 data sets). The above men-tioned percentiles now provide hints to the robustness of the reference setting. For example, for relative differences,  X  best = 0 . 02 indicates a very robust setting (w.r.t. the con-texts), since in 95% of the contexts the reference setting is maximally 2% worse than the best setting. As another ex-ample,  X  90 best = 0 . 2 states that in 10% of the contexts the performance is at least 20% worse than the best setting, which points to a low robustness of the reference setting.
Apart from the main workflow described above there are several additional options offered by ET:
ET is based on Eclipse RCP and uses a relational database to store and handle all data, either a built in SQLite database
Figure 1: Explore perspective of the evaluation tool. First a program graph is chosen (1) and a filter for the contained programs is defined (2). The selected results (3) are then displayed on the top right (4) and the influence of the cur-rently chosen x-axis parameter is visualized. or a MySQL database (remote or locally). The experiments can be executed locally (default) or send as jobs to a com-pute cluster head node. ET currently offers direct support for algorithms written in Java, C, C++, C# or Python. It is possible to use the framework for algorithms written in other programming languages, as long as the executable can read the xml parameter files that are generated by ET and provides the results in the csv format expected by ET. Readers and writers for the above mentioned languages are available on the project homepage along with example pro-grams, examples for xml input and csv output formats, and further details.
A typical scenario for an information retrieval system could contain the following three components. The first compo-nent performs the feature extraction. From various bench-mark data sets it extracts commonly used features (Sift, color histograms, signatures, etc.) and sets the parameters for the feature extraction, e.g. number of bins etc. The sec-ond component performs ranking queries using these fea-tures. Parameters for the queries can be the choice of dis-tance measure and the corresponding parameters. The third component then computes quality measures for the gener-ated rankings, e.g. AUC values or the Spearman rank co-efficient. ET allows then to explore the influence, e.g., of feature extraction parameters on the performance, or the robustness of a certain distance measure over different data sets and feature sets.

As an example from data mining consider the following two components. At first a data generator is parameter-ized with the desired data set size, its dimensionality, and possibly several distribution parameters such as cluster cen-ters or the noise level. The second component constitutes a Bayesian classifier based on kernel density estimation. The parameters can define the employed heuristic for bandwidth estimation and its parameters, a sampling strategy and a sampling rate, etc. Exploring the results can yield insights on the influence of dimensionality or data set size on the performance (for example accuracy or runtime of the clas-sifier). Alternatively, the robustness of a parameter setting for the classifier over all tested data generator settings can be investigated.

We will demonstrate the functionality of ET using exam-ple programs and various use cases. We would like to discuss both the workflow and possible use cases with the audience to discover useful features for improving ET. The software will be interesting for many attendees, especially researchers, algorithm and system designers, since it can greatly help in structured testing and efficiently finding good and robust parameter settings.
ET is a free software tool which can be downloaded from the ET website dme.rwth-aachen.de/et . The site con-tains an introductory video, a user guide and a tutorial with several lessons for basic and advanced features. It also pro-vides additional material such as the database schema, ex-ample programs, and examples for input and output formats.
Finding good and robust parameter settings often requires to run and evaluate large numbers of experiments. In this paper we presented a software tool that allows to test large numbers of parameter settings in just a few clicks and that supports visual exploration of the results. We described the main components and the workflow of the framework and discussed example applications. The software can be down-loaded for free from the project homepage, which also pro-vides a tutorial, videos, examples, and further material. This work has been supported by the UMIC Research Centre, RWTH Aachen University, Germany. [1] M. R. Berthold, N. Cebron, F. Dill, T. R. Gabriel, [2] G. Casella and E. I. George. Explaining the gibbs [3] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [4] N. J. A. Sloane. Covering arrays and intersecting codes.
