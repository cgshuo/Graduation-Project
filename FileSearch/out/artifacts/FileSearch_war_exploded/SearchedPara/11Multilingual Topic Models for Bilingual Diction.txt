 XIAODONG LIU, KEVIN DUH, and YUJI MATSUMOTO , Nara Institute of Science A machine-readable bilingual dictionary plays a very important role in many natural language processing tasks. In statistical machine translation (SMT), dictionaries can help in the domain adaptation setting [Daume III and Jagarlamudi 2011]. In cross-lingual information retrieval (CLIR), dictionaries serve as efficient means for query translation [Resnik et al. 2001]. Many other multilingual applications also rely on bilingual dictionaries as integral components. For example, Volkova et al. [2013] use a bilingual dictionary to analyze multilingual sentiment in social media; Zhang et al. [2010] incorporate a Chinese-English dictionary into a probabilistic topic model to ex-plore bilingual latent topics in Chinese and English texts.

One typical approach for building a bilingual dictionary resource uses parallel sentence-aligned corpora. This is often done in the context of SMT, using word align-ment algorithms such as the IBM models [Brown et al. 1993; Och and Ney 2003]. Unfortunately, parallel corpora may be scarce for certain language pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilin-gual dictionary extraction has become an active research topic [Haghighi et al. 2008; Vuli  X  c et al. 2011; Liu et al. 2013]. Here, a comparable corpus is defined as a collection of document pairs written in different languages, but talking about the same topic [Koehn 2010], such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment meth-ods developed for parallel corpus cannot be directly applied because of assumptions of sentence alignment. We solve this problem by converting comparable corpora, which are aligned at the document level, to topic -aligned corpora, then extracting the dictio-nary by conventional word alignment methods. This general framework is shown in Figure 1.

Intuitively, our framework works as follows: Let w e be an English word and w French word, our goal is to obtain ( w e , w f ) pairs in which the probability p ( w p ( w f | w e ) is high. It is difficult to reliably estimate p ( w directly because the alignment is only at the document level, so the number of transla-tion choices is high. Our approach differs by modeling p ( w where t is a topic. The key idea is that it is easier to tease out the translation of a word possible translation choices.

Unlike most previous works in bilingual dictionary extraction from comparable cor-pora, which are based on the context vector idea [Rapp 1995; Fung and Lo 1998], our topic modeling framework requires absolutely no seed dictionary to bootstrap the ex-traction process. Thus our framework is advantageous in low-resource scenarios and can be used as a way to obtain high-precision seed dictionaries for other bilingual dic-tionary extraction frameworks.

A second advantage of our topic modeling approach is that we can exploit compa-rable corpora in more than two languages, a situation which is becoming increasingly prevalent due to the spread of the multilingual Web. We show how we can improve the extraction of Japanese-English dictionaries using comparable data not only from Japanese and English, but also from other languages such as Chinese and French.
The article is organized as follows. In the next section, we review previous works on dictionary extraction using comparable corpora. We then describe our multilingual topic model in Section 3 and introduce our overall framework in Section 4. In Section 5, we show how our method extracts a high-precision Japanese-English dictionary from a large-scale comparable Wikipedia corpus consisting of Japanese, English, French, and Chinese. The numerous works on bilingual lexicon from comparable corpora can be divided into two broad categories: context vector approaches (Section 2.1) and projection-based approaches (Section 2.2). We also briefly touch upon research on pivot languages (Section 2.3) and multilingual word representation learning (Section 2.4); to the best of our knowledge, these are promising approaches but have not yet been employed for bilingual lexicon extraction. The context vector approach, starting with the seminal works of Rapp [1995] and Fung and Lo [1998], is built on the assumption that a word and its corresponding translation tend to appear in similar contexts across languages, also known as the distributional hypothesis . A typical context vector approach for the bilingual dictionary extraction consists of three steps. (1) Represent contexts of a word using an existing seed dictionary. This ranges from (2) Measure similarity/distance between words in this common space, for example, (3) Extract word pairs with high similarity.

Methods differ in how the seed dictionary is acquired [D  X  ejean et al. 2002; Koehn and Knight 2002] and how similarity is defined [Fung and Cheung 2004; Tamura et al. 2012]. It is important to note that all these methods critically rely on a seed dictionary to ensure that word in different languages are represented in the same space. To al-leviate the dependence on the size of the seed dictionary, Tamura et al. [2012] use an unsupervised label propagation method to improve robustness. Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches [Gaussier et al. 2004]; for example, Haghighi et al. [2008] use canonical correlation analysis (CCA) to map vec-tors in different languages into the same latent space. Laroche and Langlais [2010] presents a good summary for the project-based approaches.

Vuli  X  c et al. [2011] pioneer a new approach to bilingual dictionary extraction. The main idea is to first map words in different languages into the same semantic space using multilingual topic models. Then, several statistical measures, such as Kullback-Leibler divergence, are used to compute similarity between words in cross-languages; finally, extract lexicons with high resulting probability. This method is a totally unsu-pervised learning style and does not require any seed dictionary.

Our approach is motivated by Vuli  X  c et al. [2011]. However, we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments as we do, and as a result, their approach requires training topic models with a large number of topics, which may limit the scalability of the approach. Further, we explore extensions of multilingual topic models in more than two languages.

Recently, there has been much interest in multilingual topic models (MLTM) [Jagarlamudi and Daume 2010; Mimno et al. 2009; Ni et al. 2009; Boyd-Graber and Blei 2009]. Many of these models give p ( t | e ) and p ( t bilingual lexicon. Although topic models can group related e and f in the same topic cluster, the extraction of a high-precision dictionary requires additional effort. One of our contributions here is an effective way to do this extraction using word alignment methods.
 Multilingual corpora in more than two languages have been exploited to various de-grees. This is often done using a pivot language: for example, given a Japanese-English dictionary and an English-Chinese dictionary, one can exploit transitive properties with English serving as the pivot to find Japanese-Chinese translations. When avail-able, such multilingual information has been shown to improve the quality of bilingual lexicon [Aker et al. 2014; Kwon et al. 2013; Sadat et al. 2002].

This pivot language idea has proven beneficial in applications such as cross-lingual information retrieval [Gollins and Sanderson 2001] and machine translation [Paul et al. 2009; Wu and Wang 2009]. It is also used for bootstrapping the construction of WordNet for low-resource languages (c.f. [Bond et al. 2008]) and for directly creat-ing multilingual lexical resources [de Melo and Weikum 2009; Magnini et al. 1994; Mausam et al. 2009].

Our multilingual topic model approach differs in that there is no concept of pivot: data from all languages are treated equally. In this respect, extension to many lan-guages is straightforward as long as computation efficiency issues can be solved. Multilingual word representation learning, which is an extension of monolingual word representation learning, is a set of deep learning algorithms that enables new ways to do cross-lingual processing. It works by mapping words in different languages into the same low-dimensional space in order to capture syntactic and semantic similar-ities across languages. For instance, Klementiev et al. [2012] propose training bilin-gual word representations by jointly training monolingual neural language models together with a regularizer that enforces seed translations to have similar representa-tions. Chandar et al. [2014] propose a novel autoencoder algorithm for learning bilin-gual word representations; importantly, their algorithm only depends on bag-of-words representations of aligned sentences and does not rely on word alignments.
These works focus on cross-lingual classification tasks, but conceivably, their results could be adapted to our comparable lexicon extraction task. For example, the vectors might be used as seed within context-based models like with Rapp [1995]. Alterna-tively, these vectors could be used within our framework to generate something like the topic-aligned corpora; that is, words with similar vectors are grouped together and given to our word aligner, analogously to how we group words with the same topic together. We believe the use of vector representations is an interesting area of future work. We adopt the Multilingual Topic Model (MLTM) proposed by Ni et al. [2009] and Mimno et al. [2009], which extends the monolingual Latent Dirichlet Allocation model [Blei et al. 2003]. MLTM learns word-topic distributions and topic-document distribu-tions from comparable corpora. In the original works, it is assumed that each docu-ment tuple t m in the comparable document is fully-connected ; for example, if we have a quad-lingual comparable corpus consisting of Chinese (c), English (e), French (f), and Japanese (j), it is assumed that all document tuples in the collection contains docu-ments in all four languages (i.e., t m = [ d c m , d e m , d in the collection and d c m represents a Chinese document, d document, etc.

However, such fully-connected comparable corpora are rare in practice. Taking the entire Wikipedia as an example, Arai et al. [2008] show that among all Japanese docu-ments, only 64.4% have links to English entries and only 22.5% have links to Chinese entries. In our own Wikipedia crawl (described in Section 5.1), we find that within our target set of 14K Japanese documents, the proportion of linked English, Chinese, and French documents is only around 20 X 30%; if we restrict to tuples that contain documents in all four languages, this number drops to 12%. In some cases, this is be-cause the interlanguage-link information is missing, but in most cases, this disparity is largely the result of different human editors contributing independently in different languages [Duh et al. 2013].

We assume that a tuple does not necessarily contain documents in all languages and call such comparable corpora partially-connected . In the following, we extend MLTM [Mimno et al. 2009; Ni et al. 2009] to handle partially-connected corpora with a maxi-mum of L languages per tuple. The generative process of our proposed partially-connected multilingual topic model is this: First, we define our comparable corpus as a collection of M tuples in dif-ferent languages (i.e., t m = [ d 1 m ,..., d l m ,..., d [ y m ,..., y or absence of documents in the language l in tuple m . The value of the l th of vector m , y of documents which may contain Chinese, English, and Japanese can be represented by a 3-dimensional binary vector: [ 1,1,1] denotes that it contains all of the three lan-guages; while [0,1,0] denotes that it only contains English document. It is not difficult to see that it is very flexible to encode the relationship of a tuple of a document for the multilingual topic model.

The generative story is shown in Algorithm 1, and a graphical representation is shown in Figure 2.
 Here, language-specific topic word distributions  X  l are drawn from symmetric Dirichlet distributions with prior  X  l ;  X  m is the topic proportion of a tuple of documents t m drawn from symmetric Dirichlet distribution with prior  X  ; z language l ; words w l are drawn from language-specific distributions p ( w where l  X  X  1,..., L } .
 The central computational problem for a partial multilingual topic model is approxi-mating the posterior given a tuple of documents. In general, it is hard to estimate the posterior of a Bayesian model using exact inference methods. Therefore, approximate inference algorithms are always selected to deal with these kind of models. One of the approximate inference methods is based on Markov Chain Monte Carlo (MCMC) [Blei and Jordan 2006; Mimno et al. 2009], a sampling approach. The basic idea of MCMC is that first a Markov chain is constructed, and then its stationary distribution, which is the posterior of interest, is computed.

In this article, we develop a collapsed Gibbs sampling [Heinrich 2004; Mimno et al. 2009; Ni et al. 2009], a type of MCMC, to estimate the posterior given a tuple of docu-ments. Concretely, given a tuple of documents m , the possibility of topic k of the i word in the language l yields: Here, the document in language l denotes w l ={ w l i = v , w topic states z l ={ z l i = k , z l  X  i } ; the counts n ( v ) the corresponding document l in the tuple; the counts ( n excluded from the corresponding topic k when l = l is held in the tuple; V vocabulary in language l .

Finally, we compute the multinomial parameter sets of and B : Direchlet hyperparameters  X  and  X  can be optimized by a simple and stable fixed-point iteration for a maximum likelihood estimator [Minka 2000]. The general idea of our proposed framework is sketched in Figure 1: First, we run a multilingual topic model to convert the comparable corpora to topic-aligned corpora; second, we run a word alignment algorithm on the topic-aligned corpora in order to extract translation pairs. The innovation is in how this topic-aligned corpora is defined and constructed X  X he link between the two stages. We describe how this is done in Section 4.1 and show how existing approaches are subsumed in our general framework in Section 4.2. Suppose the original comparable corpus has D document pairs [ d multilingual topic model with K topics, where K is user-defined (Section 3). The topic-aligned corpora is defined hierarchically as a set of sets : on the first level, we have a set of K topics, { t 1 , ... , t k , ... , t K } ; on the second level, for each topic t D  X  X ord collections X  { C k ,1 , ... , C k , i , ... , C k , D English and foreign words that occur simultaneously in topic t
For clarity, let us describe the topic-aligned corpora construction process with a flow chart in Figure 3. (1) Train a multilingual topic model (Section 3). (2) Infer a topic assignment for each token in the comparable corpora and generate a (3) Rearrange the word collections such that C k , i belonging to the same topic are (4) For each topic t k , we run IBM Model 1 [Brown et al. 1993; Och and Ney 2003] on (5) To extract a bilingual dictionary, we find pairs ( w e
In practice, we compute the probabilities of Equation (4) in both directions: p ( w as in Eq. (4) and p ( w f | w e ) = k p ( w f | w e , t k conceivable for extracting a bilingual lexicon: option (a) is to set a threshold  X  and extract all pairs (  X  e ,  X  f ) with p ( w f =  X  f | w thresholds  X  1 ,  X  2 and extract lexicons based on the following bidirectional constraint that a pair (  X  e ,  X  f ) is extracted only if
We show results from both options in our experiments. Option (a) is useful for gen-erating a ranked list and computing precision-recall curves, since  X  can be adjusted to allow for different numbers of extracted pairs. Option (b) gives very high precision ex-tractions, since it takes the intersection from both p ( w is not easy to tune  X  1 ,  X  2 to extract a given number of pairs, since the intersection is not known beforehand. In our experiments, we  X  X et X   X  1 ,  X  2 to retrieve only one candidate translation per model, extracting a pair (  X  e ,  X  f ) if the following holds: To the best of our knowledge, Vuli  X  c et al. [2011] is the only work that focuses on using topic models for bilingual lexicon extraction like ours, but they exploit the topic model results in a different way. Their Cue Method computes
This can be seen as a simplification of our Eq. (4), where Eq. (9) replaces p ( w with the simpler p ( w e | t k ) . This is a strong assumption which essentially claims that the topic distribution t k summarizes all information about w formulation can be considered more realistic because we do not have the assumption that w e is independent of w f given t k ; we model p ( w parameters with word alignment methods.

Another variant proposed by Vuli  X  c et al. [2011] is the so-called Kullback-Leibler (KL) method. It scores translation pairs by KL ( w e , w f ) = Divergence ( p ( t k | w e ) || p ( t k The information content is the same as the Cue Method (Eq. (9)); it is simply a different scoring equation. In our experiment, we find that a symmetric version of KL, known as Jensen X  X hannon Divergence, gave better results: where w ef denotes the average of the word-topic distributions of both e and f ,that is, w ef = [ p ( t | w e ) + p ( t | w f ) ] / 2. 2 First, we describe our experiment setup in Section 5.1. Section 5.2 compares our method with previous works, and Section 5.3 shows how our method improves given additional languages in the comparable data. Section 5.4 discusses practical issues, such as hyper-parameter selection and runtime, while Section 5.5 provides detailed analyses of the results. Finally, Section 5.6 demonstrates how our approach can be used to provide high-precision dictionaries to bootstrap existing context vector methods. We perform experiments based on the Kyoto Wiki Corpus. 3 We choose the Kyoto Wiki Corpus because it is a parallel corpus, where the Japanese edition of Wikipedia is translated manually into English sentence by sentence (14K document pairs, 472K sentences). This enables us to use standard word alignment methods to create a  X  X old-standard X  lexicon for large-scale automatic evaluation. First, we run IBM Model 4 on requirement of Eq. (7), with threshod  X  1 =  X  2 = 0.3. We refer to these pairs as a  X  X old standard X  bilingual lexicon. Due to the large data size and the strict bidirectional requirement, these  X  X old standard X  bilingual dictionary items are of high quality (92% precision by a manual check on 500 random items). Note that sentence alignments are used only for creating this gold standard and are not used in subsequent experiments. To evaluate the proposed framework, we use a real comparable corpus crawled from Wikipedia (denoted as Wiki). We keep the Japanese side of the original Kyoto Wiki Corpus but crawl the online English, Chinese, and French editions by following the inter-language links from the Japanese page. The statistics of the crawl are shown in Table I. Observe that this is a partially-connected comparable corpus: the number of corresponding articles in English, Chinese, and French is much smaller, consisting of only 20 X 30% of the original Japanese. The number of fully-connected tuples in all four languages is only 12%, as seen in the intersection ( j  X  e
For preprocessing, we did word segmentation on Japanese and Chinese using Kytea [Neubig et al. 2011] and Porter stemming on English and French using NLTK Version 3.0. 4 Finally, we remove the 2,000 rarest words and stop-words from each language as shown in Table II. To facilitate future work in this area, our stop-word lists for these four languages are available at https://bitbucket.org/allenLao/stopwords . We begin by comparing with previous topic-modeling approaches to bilingual lexicon extraction, namely, Vuli  X  c et al. [2011]. Using the automatically-created  X  X old-standard X  lexicon, we evaluate methods by precision, defined as |{ Gold ( e , f ) Table III shows the precision of our proposed method compared with the baseline Cue and JS methods [Vuli  X  c et al. 2011]. All these methods first run our MLTM with K 400 topics 5 on the partially-connected Japanese-English Wiki dataset, which consists of 14, 033 + 4, 087 = 18, 120 documents.
 Our method extracts a total of 1,457 pairs using the bidirectional constraint in Eq. (8). This achieves a precision of 0.742. For comparison, we adjust the threshold  X  (Option (a) discussed in Section 4.1), such that the Cue (Eq. (9)) and JS (Eq. (11)) methods give roughly the same number of extracted pairs as our proposed method. The resulting precision of Cue and JS are very poor, at 0.073 and 0.091, respectively. Vuli  X  c et al. [2011] reports that a large number of topics is necessary for good results, so we re-ran the baselines with K = 2,000, the suggested value in Vuli  X  c et al. [2011]. Despite the long runtime of MLTM for large K , the precision only increased to 0.104 and 0.123 for Cue and JS, respectively.
It can be seen that our proposed method is much more effective at extracting bilin-gual lexicon, in particular in large-vocabulary datasets (the vocabulary size in [Vuli  X  c et al. 2011] is 7K and 9K in Italian and English, respectively). We have a hypothesis as to why Cue and JS depend on large K . Eq. (4) is a valid expression for p ( w makes little assumptions. We can view Eq. (9) as simplifying the first term of Eq. (4) so the same number of parameters is needed in reality to describe this distribution. By throwing out w f , which has large cardinality, t k needs to grow in cardinality to compensate for the loss of expressiveness.
 As an additional baseline, we directly run IBM Model 1 on the fully-connected Japanese-English comparable corpora, treating each document pair as a sentence pair. This IBM-1 baseline does not employ MLTM, and the score of a pair ( e , f ) is defined as the average lexical probabilities obtained from IBM Model 1 in both directions. In-terestingly, this baseline achieves a precision of 0.52, better than Cue and JS. But our proposed method still performs better, implying that the combination of existing word alignment models and MLTM attains good synergy. We now examine the effects of adding additional languages to the Japanese-English lexicon extraction. Table IV shows how precision improves as we add Chinese (3,494 comparable documents in addition to the original 18,120 Japanese-English corpora), as well as both Chinese and French (3,494+2,871=6,365 comparable documents). Since the number of extractions changes (because probability value changes affect the bidi-rectional constraint of Eq. (8)), we also manually evaluated precision (ManualPrec) on a fixed random set of 100 pairs.

From Table IV, we see that adding Chinese documents improves the (automatic) precision from 0.742 to 0.761. Adding both Chinese and French documents further improves results, with (automatic) precision gaining 3% (0.742 precision gaining 9% (0.62  X  0.71). We observe these improvements because adding more languages and data improves the estimation of the MLTM. Specifically, in our bilingual extraction equation (Eq. (4)), more data can directly improve the estimation of the topic distribution p ( t k | w f ) ; further, more data may also indirectly improve the estimation of the the topic-dependent bilingual lexicon p ( w inference results for input into the word alignment step. Note that the word alignment part is the same for the various systems in Table IV, so improvements come from better MLTM.

We also show results using only the fully-connected Japanese-English compara-ble corpus (Full-Japanese-English). This system only runs MLTM on 4,087 document pairs, and as a result, the precision is lower than the partially-connected case (0.612 vs. 0.742). This demonstrates our MLTM is effective in exploiting monolingual documents in estimating its parameters.

Finally, we also compare the systems not by using the bidirectional constraint, but by varying the threshold  X  (as discussed in Option (a) at the end of Section 4.1. Figure 4 shows how precision varies as we lower the threshold. Figure 5 plots the num-ber of extracted pairs versus threshold on the same data. We observe a large overall gain in precision regardless of threshold as we move from fully-connected to partially-connected data, which corroborates the results in Table IV. The number of extractions are roughly similar for the various partially-connected systems, while fully-connected has slightly larger numbers (but lower precision). The differences between the vari-ous systems using partially-connected corpora does not seem very large, but this is not surprising given the large amount of monolingual Japanese documents (140,033) in our dataset compared to additional Chinese and French documents (around 3,000). Nevertheless, we do observe that the Japanese-English-Chinese-French system does indeed have the best precision curve. The most important parameter in our approach is the number of topics K in MLTM. As K goes to one, the proposed approach becomes equivalent to running word alignment directly on comparable documents, treating each document pair as a sentence pair. As K increases, the topic-aligned corpora become more fine grained, and the lexicon extraction precision improves. However, if K is too large, then each word collection C k , i in the topic-aligned corpora becomes too small, and if the topic model incorrectly assigns translation pairs to different topics, it becomes impossible to extract it in the subsequent word alignment step.

First, we show how precision varies with different values of K in Figure 6. We ob-serve that for low values of K (e.g., 100, 200), the precision is relatively low, around 0.4 X 0.6. The best precision is achieved with K = 400, followed closely by K K = 800, all in the 0.7 X 0.8 range.

While it is expected that results vary somewhat by K , the important question is whether the best K can be selected a priori in an unsupervised manner. Now we show that the per-word log-likelihood on the held-out dataset is effective for model selection. Per-word log-likelihood , which is widely used in the machine learning and statistics community, is defined as the geometric mean of the inverse marginal probability of each word in the held-out (dev) set of documents D dev : Here, n t denotes the number of words for the t th tuple of documents in dev corpus. Following Teh et al. [2006], we estimate p ( t | D and Eq. (3). A higher per-word log-likelihood score indicates better performance [Blei et al. 2003; Hoffman et al. 2010; Teh et al. 2006].

Figure 7 summarizes our results for the model selection, plotting the per-word likeli-hoods of a 100-tuple held-out dev set. We observe that per-word likelihood successively picks out K = 400 as the best model for various setups, which generally corresponds to the best precision results in Figure 6.
 Finally, we show the runtime of MLTM on a 2.4GHz Opteron CPU for varying K in Figure 8. As expected, runtime increases with K : on datasets as large as ours, training with K = 400 takes approximately 4 days, and K = 800 takes 8 days. Time complexity of MLTM is O ( NK M m = 1 L l = 1 w l m ) , where N indicates the number of iterations; K, M, L denotes the number of the topics, size of corpus, and numbers of the languages; w denotes the number of words in tuple m written in language l .

The overall time for various systems is shown in Table V. First, note that MLTM time dominates the overall time for all systems, so the training time does not differ much among methods if we use the same number of topics in MLTM, but in practice, Proposed requires fewer numbers of topics, so it is much faster to train. Second, assum-ing the same number of topics, the breakdown of training time shows that Proposed is still relatively fast because both GIZA++ and Eq. (4) are fast. Comparing Eq. (9) of Cue to Eq. (4) of Proposed, we see that both need to compute p ( t tends to be faster because p ( w e | w f , t k ) in Eq. (4) tends to be sparse, while p ( w Eq. (9) is dense. First, we present some interesting examples of bilingual lexicon found by our method. In particular, due to the topic-dependent translation probabilities p ( w able to tease out the translation of polysemous words. In Table VI, we look at poten-tially polysemous English words w e and list the highest-probability Japanese transla-tions w f conditioned on different t k . We found many promising cases where the topic identification helps divide the different senses of the English word, leading to the cor-rect Japanese translation achieving the highest probability.

Second, we perform an error analysis on the results of the Full-Japanese-English system. We find that the proposed method makes several types of incorrect lexicon ex-tractions. First, word segmentation  X  X rrors X  on Japanese could make it impossible to find a proper English translation (e.g.,  X   X   X   X  should translate to  X  X rince-Takechi, X  but the system proposes  X  X akechi X ). Second, an unrelated word pair ( w incorrectly placed in the same topic, leading to an incorrect topic error. Third, even if ( w e , w f ) intuitively belong to the same topic, they may not be direct translations; an extraction in this case would be a correct topic, incorrect alignment error (e.g.,  X  X  X  X  X  X   X   X  , a particular panfried snack, is incorrectly translated as  X  X anfry X ).
Table VII shows the distribution of error types by manual classification. Incorrect alignment errors are the most frequent, implying the topic models are doing a rea-sonable job of generating the topic -aligned corpus. The amount of incorrect topic is not trivial, though, so we would still imagine more advanced topic models to help. seg-mentation errors are in general hard to solve, even with a better word segmenter, since one-to-one cross-lingual word correspondence is not consistent. We believe the solution is a system that naturally handles multiword expressions [Baldwin 2011].
Since word alignment errors were frequent, we conduct an additional experiment to compare several popular word alignment methods in statistical machine trans-lation: (a) Giza-vb, a modification of IBM Models training using Variational Bayes EM learning [Riley and Gildea 2010]; (b) Giza-L0, a modification of IBM Models to generate sparser alignments using approximate L0-norm optimization [Vaswani et al. 2012]; and (c) Berkeley Aligner, which enforces agreement in bidirection word alignment [Liang et al. 2006]. Figure 9 shows the lexicon extraction results using different alignment tools. While the differences are in general not very large, we observe that the Berkeley Aligner appears slightly better than all the other IBM model variants, implying that bidirectional constraints may be helpful in this kind of topic-aligned data.

Finally, we attempt to quantitatively explain why our approach of topic-aligned cor-pus is more effective than directly extracting bilingual lexicon from topic model param-eters, as suggested by Mimno et al. [2009] and executed by Vuli  X  c et al. [2011]. To do so, we first take the word-topic distribution from MLTM ( K number of topics each word type w may appear in it (i.e., nonzero probabilities accord-ing to p ( w | t ) ). Figure 10 shows that the plot of the number of word types that have x number of topics behaves like a power-law. This suggests that it is not easy to directly extract a lexicon by taking the cross-product ( w f , w e and p ( w e | t k ) for the same topic t k , as suggested by Mimno et al. [2009]. The majority of words are grouped in the same few topics, making it difficult to discern the actual translations. When we attempt to do this using top-2 words per p ( w we could only obtain precision of 0.37 for 1,600 extractions. The methods of Vuli  X  cetal. [2011] are essentially based on similar information, albeit with probability weighting. This skewed distribution similarly explains the poor performance of the Cue and JS baselines.

On the other hand, after constructing the topic-aligned corpora (Step 3 of Figure 3), we compute the ratio of distinct English word types versus distinct Japanese word types for each topic. If the ratio is close to one, this means the partition into topic-aligned corpora effectively separates the skewed word-topic distribution of Figure 10 into more uniformly-distributed word collections. We found that the mean ratio aver-aged across topics is low at 1.721 (variance is 1.316), implying that within each topic, word alignment is relatively easy.

Some examples of how the proposed multilingual model reduces translation errors are shown in Table VIII. Taking  X  X usic X  as an example, if we only use the English-Japanese corpus, we erroneously find that  X   X  (to sing) has high translation probabil-ity; this is understandable, though, because the words are roughly in the same topic. However, with additional language data (Chinese, French), the topic distributions be-come more precise, so the error disappears and the correct translation left with higher probability. Context vector approaches are one of the most established ways to extract lexicon from comparable corpora. These approaches require a seed dictionary, which could be provided by our approach. We evaluate this hybrid approach as follows.
  X  First, the 1,457-pair high-precision dictionary extracted by our proposed method in  X  As comparison, we run the same [Rapp 1995] method with different amounts of
From Figure 11, we find that WikiSeeds outperforms both 500 and 1,000 gold seeds in precision across all numbers of extracted pairs. As expected, a roughly equal num-ber of gold seeds (1,500) should outperform WikiSeeds, but the differences are not large. Such observations imply that our extracted seeds can be used in the context vector approach when there are no large seeds existing in certain language pairs. The code of the context vector approach is available at https://bitbucket.org/allenLao/ context based model for dic/src. We propose an effective way to extract bilingual dictionaries using a novel combination of topic modeling and word alignment techniques. The key innovation is the conversion of a comparable document -aligned corpus into a parallel topic -aligned corpus, which allows word alignment techniques to learn topic-dependent translation models of the any bilingual seed dictionary, and (2) it can effectively exploit comparable corpora con-sisting of documents in more than two languages.

Our large-scale experiments demonstrate that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. Further, we show improvements in the precision of our Japanese-English lexicon as we include more languages (i.e., Chinese and French) to the comparable corpora. To facilitate further work in this area, all preprocessed data and topic modeling code is available at https://bitbucket.org/allenLao/topic-modeling-gibbs .

While our framework is purely unsupervised in the sense that it requires no seed dic-tionary, we can imagine several interesting extensions if such a seed dictionary were available. First, the seeds could be used as a prior for the multilingual topic model, for instance by employing the Dirichlet tree prior [Andrzejewski et al. 2009; Hu et al. 2014]. Second, the seed translation could also be incorporated into the word align-ment step (as supervised alignments) to improve performance of the topic-dependent flexible to incorporate additional resources and knowledge into the lexicon extraction process.

