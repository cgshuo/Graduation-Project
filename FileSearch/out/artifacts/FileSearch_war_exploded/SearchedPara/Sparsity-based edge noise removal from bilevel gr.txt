 ORIGINAL PAPER Thai V. Hoang  X  Elisa H. Barney Smith  X  Salvatore Tabbone Abstract This paper presents a new method to remove edge noise from graphical document images using geomet-rical regularities of the graphics contours that exist in the images. Denoising is understood as a recovery problem and is accomplished by employing a sparse representation frame-work in the form of a basis pursuit denoising algorithm. Directional information of the graphics contours is encoded by atoms in an overcomplete dictionary which is designed to match the input data. The optimal precision parameter used in this framework is shown to have a linear relationship with the level of the noise that exists in the image. Experi-mental results show the superiority of the proposed method over existing ones in terms of image recovery and contour raggedness.
 Keywords Image degradation model  X  Noise spread  X  Bilevel image denoising  X  Sparse representation  X  Dictionary learning  X  Directional denoising 1 Introduction The scanning and binarization processes that produce binary document images introduce noise that is concentrated on the edges of image objects [ 4 ]. This edge noise causes dif-ficulties in document reading and interpretation. It also has influence on later steps in the chain of automatic document processing. For example, it could affect feature measurement inrecognition,reduceimageredundancyincompression,and distort skeletons in vectorization. For the accurate analysis and recognition of document images, edge noise needs to be removed.

Removing noise from images could be considered as an inverse problem where  X  X riginal X  images are reconstructed from available noisy images. Like many other inverse prob-lems of similar nature, such as signal reconstruction from noisy and sampled measurements, if there exists no clue about the original image other than its noisy version and no information about the process that introduces noise to the image, then noise removal is an ill-posed or ill-conditioned problem that has many trivial solutions. For example, any image of the same size as the noisy image could be used as a solution to this problem. In order to reduce the set of solutions so that it contains acceptable ones, existing works usually rely on assumptions and noise models.

Assumptions are commonly used in the literature to recast the original inverse problem in various discrete forms, and each requires a specific tool to find solutions. Usually, the original image is assumed to be  X  X imilar X  to its noisy ver-sion, and the geometrical regularities are  X  X ell represented X  in the original image, in conformity with natural patterns. For example, similarity could be represented by means of the distance between the two points that represent the orig-inal and noisy images in a high-dimensional space, where the number of dimensions is equal to the number of pixels in each image. Since a small distance does not always guaran-tee a high visual quality of the denoised image, regulariza-tion is often used to prevent over-fitting and also to introduce other assumptions. On the other hand, mathematical model-ing can be used to characterize the additive noise that results from a number of degradation factors in the image formation process. Intuitively, different levels of noise demand differ-ent  X  X evels of denoising, X  or different levels of regularization. Thus, knowledge about the level of additive noise may be useful in controlling the aforementioned regularization.
This paper revisits the denoising problem in the context of graphical document images in light of recent research in document image degradation modeling [ 4 ] and recent devel-opments in signal representational theories [ 27 ] in order to achieve the two preferred performance criteria of image recovery and contour smoothness. It is a thorough extension of a preliminary work presented in [ 28 ] and a continuation of a previous work that uses sparsity for text/graphics separa-tion [ 29 ]. The remainder of this paper is organized as follows. Section 2 gives some background on edge noise modeling, followed by a detailed review of related works in Sect. 3 .The proposed edge noise removal algorithm using sparse repre-sentation is presented in Sect. 4 . Experimental results and discussions are given in Sect. 5 , and conclusions are drawn in Sect. 6 . 2 Image degradation model A scanner model which is based on the physics in the doc-ument acquisition process provides a theoretical platform for the analysis of that process. The scanner model that is described below is a portion of the model presented in [ 3 ] and is schematically described in Fig. 1 . It is assumed in this model that when a spatially continuous image o is converted to digital form using either a digital camera or a document scanner, the value of each pixel in the scanned image before quantization, s [ i , j ] , depends on the light collected at the corresponding discrete sensor. This collected light in turn depends on the reflectance in the original image in the neigh-borhood around that sensor, that is a function of the optics and the sensors. The contribution of the source reflectance to the sensor value is usually described by a function of the dis-tance from the sensor center called the point spread function (PSF). Thus, the signal value that is received at each sensing element ( i , j ) is modeled as s [ i , j ]= PSF ( x In order to model the noise that would occur during the acqui-sition process, it is generally assumed that additive white Gaussian noise (AWGN) n of standard deviation  X  noise is added to these values as a [ i , j ]= s [ i , j ]+ n [ i , j ] .
 Moreover, since document and graphical images are usually processed in bilevel, this image is then thresholded, usually with a global threshold value  X  as f [ i , j ]= While the AWGN is evenly distributed over the whole grayscale image, the effect of AWGN after thresholding is concentrated along the edges. The process of turning a smooth edge into a rough one and the analysis of this rough edge are discussed as follows. 2.1 Edge without noise In graphical document images, the image content contains large regions of white ( 0 ) background, with foreground image features displayed in black ( 1 ) . When documents are scanned in grayscale, the edges change from step functions to functions sloped in the shape of the edge spread func-tion (ESF), which is the cumulative marginal of the PSF. The slope of the edge functions in turn causes changes in the edge locations after thresholding [ 5 ]. For a PSF parameterized by a width w , the amplitude of the ESF is s ( x ) = ESF An example of an ESF is shown in Fig. 2 a, and an image that corresponds to this is shown in Fig. 3 c. When there is no noise, the edge location after thresholding occurs at the point where the amplitude is equal to the threshold  X  and would be at the location x = X   X  c , where  X  = X  w ESF  X  1 (  X  ) .
 The shift  X  c in edge location that depends on s and  X  in Fig. 2 a is depicted in Fig. 2 b. This effect in images is shown in Fig. 3 e. 2.2 Edge with noise and noise spread When additive noise n is considered, the edge intensity after blurring a will fluctuate around s as illustrated in Fig. 2 c. This variation in an image is shown in Fig. 3 d. This fluctuation results in a small region near the edge of the step function in which the value of the pixel could be above or below the threshold. So after thresholding, the thresholded signal has a noisy edge that may be at any position in that region, Fig. 3 f. The breadth of that region is defined as noise spread (NS) (Fig. 2 d). It has been shown recently [ 36 ] that NS is not just dependentonthestandarddeviation  X  noise ofAWGN,butalso on the width w of the PSF, which determines the slope of the grayscale edges, and the level of the binarization threshold  X  through the relation NS = where LSF is the line spread function, or 1D PSF.
 Some examples of edges with noise are shown in Fig. 4 . The standard deviation  X  noise of the additive noise is kept fixed in the three images in Fig. 4 a X  X . Images with a common  X  noise are conventionally thought of as having the same noise level. However, it can be seen that these three images have edges with distinctly different amounts of perceptual noise. Even with a constant value of  X  noise , an increase in NS makes the image noisier in an amount proportional to the increase in the value of NS. In Fig. 4 d, a significantly larger NS is shown, and its effect on the edge can be easily observed: There are generally two rows of pixels affected by the additive noise when NS = 2 . 0. In this manner, NS provides a measure that can numerically quantitate the amount of edge noise and also relates to the noise visually observed on bilevel images. 2.3 Relationship between NS and Hamming distance The real benefit of determining the NS of a scanned object is that NS provides an effective measure of how noisy a bilevel object is. However, for two bilevel images of the same size, the Hamming distance [ 26 ], which is defined as the number of substitutions required to change one image into the other, is often used as a measure of difference between them. It is very useful and hence is usually used for the analysis of noise in bilevel images when the noise-free tem-plate is known. The formula that relates the expected Ham-ming distance E { H } to NS for straight edges was developed [ 36 ]as E { H }= NS where  X  is a constant equal to the length of the edge seg-ment. The above equation shows that Hamming distance is directly proportional to NS, leading to the possibility of using NS as a predictor of the Hamming distance between a degraded image and the predicted noise-free image, and vice versa. In addition to the theoretical result in Eq. ( 1 ), experiments have also been carried out to validate this lin-ear relationship between NS and the expected Hamming distance.

The ability of NS to provide a quantitative measure that also qualitatively describes the amount of noise and its lin-ear relationship with the Hamming distance led to a poten-tial that NS could be used as an input to a denoising algo-rithm that works on bilevel images, in a similar fashion to how the standard deviation of the AWGN is often used as an input to denoising algorithms that work on grayscale images. 3 Related bilevel denoising work Let the noisy image f be the result of scanning and then global thresholding a noise-free input image f 0 of size w  X  it will contain edge noise of a certain NS. Denoising is viewed as an inverse problem, i.e., one needs to find an estimated image  X  f from f which is close to f 0 and, at the same time, has some preferred properties like contour smoothness for graphical document images. Many methods exist for remov-ing noise from digital images [ 8 ], and each has its own prop-erties that make it suitable for some particular situations. The aim of this section is thus not to give a long list of existing methods, but to review some ones relevant to the problem of noise removal from bilevel graphical document images based on the preferred criteria of image recovery and con-tour smoothness.

Bilevel image denoising For bilevel document images, the most famous and frequently used denoising methods are contour smoothing using chain codes, median filtering, mor-phological operators, and kFill filtering. Contour smoothing based on chain codes [ 52 ] replaces a chain code sequence by a simpler sequence, usually a shorter one that corresponds to the shortest path. The simplicity in the code is enforced by minimizing the total change in the code sequence, which in turnisdonebyrecursivelyreplacingtwoconsecutivechanges by a single one. Even though this method can produce a smooth contour from a jagged one, it cannot be performed on an image such as the one that is shown in Fig. 5 a where the noisy pixels are not only distributed along the contours, but also over the whole image region.

The main idea of the median filter [ 2 ] is to run through the image pixel by pixel, replacing each pixel X  X  value with the median of neighboring pixels X  values. Due to its nature, the median filter is particularly effective at removing outliers, such as  X  X alt &amp; pepper X  noise or noise whose probability density has heavier tails than the Gaussian. Morphological operators [ 35 ] like erosion, dilation, and their combinations opening and closing have their root in set and lattice theo-ries. The popularity and efficiency of the simple morpholog-ical openings and closings to suppress positive and negative impulse noise have theoretical supports. The kFill algorithm [ 38 ] is designed to remove  X  X alt &amp; pepper X  noise iteratively while maintaining the readability of text by using a filter that retains text corners. The value of the parameter k of the fil-ter can be chosen adaptively based on text size and image resolution.

Among the aforementioned denoising methods, median filtering, morphological operators, and kFill filtering perform isotropic and geometric local smoothing and thus may not be sufficient for denoising tasks that need directional smooth-ing or contour preservation. These methods are known to be unable to preserve fine image details and may unintention-ally remove thin lines and corners. This is because they are general-purpose denoising methods which are not designed specifically for edge noise and do not exploit the directional information in their operations. Contours denoised by these methods are jagged and sometimes shifted from their origi-nal positions. Another shortcoming of existing binary image denoising methods is that they do not take into account the information about the level of noise that exists in the bina-rized document images. Denoising is performed in a  X  X lind, X  nonadaptive way.

Total variation denoising Noise removal by minimiz-ing the total variation (TV) of an image f , TV ( f ) = | X  f ( x ) | d x , while preserving some fit to the original mea-sured data was first proposed in [ 45 ]. The method is based on the principle that images with excessive and possibly spuri-ous details have a high TV, that is, the integral of the absolute gradient of the image is high. According to this principle, the problem of removing noise from a noisy image f based on TV could be posed as the following optimization problem:  X  f = argmin where the parameter TV , which is related to the estimated noise level, determines the sharpness or smoothness of the restored image. It has been proven that [ 47 ] for a general tex-tureimage,thisnoiseremovalmethodhasanedge-preserving property, which is better than simple methods such as linear smoothing or median filtering, which reduce noise, but at the sametimesmoothawayedges.However,theedge-preserving effects of TV regularization is somewhat local; that is, the effect on one edge in the image has little or no correlation with the effect on another edge. This local property results in the inability of the TV-based denoising method to exploit the long global contours that exist in graphical document images in order to produce smooth ones.

Anisotropic diffusion In image processing, anisotropic diffusion aims at suppressing image noise without remov-ing significant parts of the image content. It is motivated by minimizing the energy functional of an image f defined by E where g is a real-valued function. The gradient descent equa-tion is given by  X  f  X  t Thus by letting c = g , the anisotropic diffusion equation becomes  X  f  X  t where denotes the Laplacian,  X  denotes the gradient, and div (  X  ) is the divergence operator.

In the original formulation [ 42 ], the diffusion coefficient c ( x , y , t ) that controls the diffusion rate was proposed to be c where K is a constant that controls the sensitivity to edges. Thefilterisinfactisotropic,butdependsontheimagecontent in the way that it approximates an impulse function close to edges and other image X  X  structures that need to be preserved over the different levels of the resulting scale-space. A more general formulation allows the filter to adapt locally to be truly anisotropic near linear structures such as edges or lines. The filter has an orientation similar to that of the structure such that it is elongated along the structure and narrow across. Suchamethodisreferredtoas coherence enhancing diffusion [ 50 ]. As a consequence, the resulting images preserve linear structures, while at the same time, smoothing is made along these structures.

Orthogonal wavelet denoising Wavelet-based image denoising has been used widely, and its success is due to the tendency of images to have a compact representation in the wavelet transform domain [ 43 ]. The efficiency of image approximation based on a small subset of wavelets also led to the adoption of the wavelet transform in JPEG-2000 image compression and coding systems. In denoising problems, it is legitimate to assume that only a few large wavelet coef-ficients contain information about the underlying images, while small coefficients are attributed to the noise. Thus, the common procedure is to first apply the discrete wavelet trans-form (DWT) (analysis operator T ) to the noisy image f , then to use a nonlinear estimation rule O to the transform coef-ficients, and finally to compute the inverse DWT (synthesis operator T T = T  X  1 ) to get an estimate  X  f of the noisy image f . This procedure can be described symbolically as  X  f = T T O ( T f ). (3) This approach has already proven to be very successful on both practical and theoretical sides [ 31 ]. Many thresholding or shrinkage rules were proposed for the operator O with hard-thresholding and soft-thresholding certainly being the most well known. For the 1D variable t , these thresholding operators are defined as follows.  X  Hard-thresholding [ 49 ] consists of setting to zero all coefficients whose magnitudes are less than a threshold (Fig. 6 a):  X  Soft-thresholding [ 20 ] is defined as the kill-or-shrink rule with the coefficients above a threshold are shrunk toward the origin, and those with a magnitude smaller than that threshold are set to zero (Fig. 6 b): where (  X  ) + = max (  X  , 0 ) .

Noise modeling It can be easily seen that most of the denoising methods mentioned above are parametric, and they require knowledge of noise to set their parameters in order to have good performance. For example, TV denoising needs to set the value for TV ; multiscale denoising has to do the same for  X  . The knowledge of noise is thus a crucial factor in applying noise removal methods. In practical situations, the noise level is usually obtained from image noise models [ 7 ], which are built from the knowledge of noise generation processes or from some measured values. For the aforemen-tioned edge noise which can be measured quantitatively, the availability of NS could pave the way for the design of a new more efficient method to remove edge noise.

The following section describes a new parametric method for edge noise removal in bilevel graphical document images that exploits the directional information of graphics contours. Information about the level of edge noise represented by NS is used as an input to the denoising process in order to have better performance. Directional denoising is facilitated by using a sparse representation framework. This is done by promoting the sparse representation of graphical docu-ment images in an overcomplete dictionary using a basis pursuit denoising algorithm with the dictionary defined from curvelets or learned from data. The images reconstructed from their sparse representations are grayscale ones, which can be simply thresholded to obtain the final bilevel denoised images. 4 Sparsity-based edge noise removal Let x 0 , x , and  X  x  X  R p ( p = w h ) be the vectors generated by stacking the columns of f 0 , f , and  X  f , respectively, then x = x edge noise. This section is devoted to the finding of  X  x from x by using the recent idea of directional representation in order to achieve the two preferred criteria of image recovery and contour smoothness. 4.1 Multiscale directional denoising Although applications of wavelets in image processing have become increasingly popular, traditional wavelets represent well only point singularities since they ignore the geometric properties of structures and do not exploit the regularity of edges. Images denoised by using traditional wavelets usu-ally have unfavorable blocky artifacts. Therefore, wavelet-based denoising becomes inefficient for geometric line-like features and surface singularities.

To overcome the missing directional selectivity of con-ventional 2D DWTs, there have been several developments of wavelet frames in recent years. Steerable wavelets [ 24 ], Gabor wavelets [ 32 ], brushlets [ 37 ], beamlets [ 19 ], ridgelets [ 16 ], curvelets [ 9 ], contourlets [ 17 ], shearlets [ 25 ], wave atoms [ 15 ], and surflets [ 11 ] were proposed independently withthegoal of better representationof directional features in images. Among these X-lets, curvelets have the highest pub-licity and have found applications in several domains [ 33 ]. In the 2D case, the curvelet transform allows an almost opti-mal sparse representation of objects with singularities along smooth curves. For a smooth object f with piecewise C 2 singularities, the best N -term approximation  X  f N ,thatis,a linear combination of only N elements of the curvelet frame, obeys f  X   X  f N 2  X  CN  X  2 ( log N ) 3 , while for wavelets, the decay rate is only N  X  1 .

The curvelet transform has a property that the coefficients of those curvelets whose essential support does not overlap with or overlaps with, but are not tangent to an edge are small and negligible. For example, in Fig. 5 b, the coefficients of curvelets of types A and B are negligible; most of the energy of the graphics is localized in just a few coefficients of type C. In other words, the curvelet transform produces a sparse rep-resentation of objects, and most of the energy of the objects is localized in just a few coefficients of curvelets which overlap and are nearly tangent to the object contours. Based on this property, the application of the curvelet transform for image denoising is straightforward; it could simply be done by hard-thresholding of curvelet coefficients as in Eq. ( 4 )[ 46 ]. The images reconstructed by curvelets exhibit higher perceptual quality than those from wavelets, have visually sharper and, in particular, higher quality recovery of edges and of linear and curvilinear features. 4.2 Thresholding as a minimization problem SupposethatdenoisingisperformedbyusingEq.( 3 )where T is orthonormal like in the case of the DWT. Let X  X  investigate the following optimization problem to obtain an estimated image  X  x = T T  X   X  :  X   X  = argmin where D = T T = T  X  1 . Due to the unitarity of T : x  X  D  X  2 2 = x  X  T T  X  2 2 = Tx  X   X  2 2 , and thus, the optimization translates into  X   X  = argmin where  X  = Tx is the transform of the noisy image x . As both terms  X   X   X  2 2 and  X  q q are separable, the optimization problem in Eq. ( 7 ) decouples into a set of n independent scalar problems of the form  X   X  = argmin It is not difficult to prove that the unique closed-form solu-tion of this problem in the two notable cases q = 0 and q = 1 is actually the two thresholding operators defined in Eqs. ( 4 ) and ( 5 ), respectively, with  X  i = O ( X  i , X ) for the case of orthonormal transforms, thresholding-based denoising could be viewed as solving an optimization of the form in Eq. ( 6 ) with q = 0 for hard-thresholding and q = for soft-thresholding. 4.3 Basis pursuit denoising (BPDN) The thresholding operators defined in Eqs. ( 4 ) and ( 5 )are the exact solutions to the optimization problem in Eq. ( 6 )for the two cases q = 0 and q = 1 only if D is orthonormal. When a redundant transform like the curvelet transform is used, the corresponding overcomplete dictionary D has more columns than rows and is not unitary ( DD T = I but D T D where I is the identity matrix). The problem in Eq. ( 6 ) thus does not have a simple and closed-form solution, even in the two notable cases q = 0 and q = 1. This is because the presence of D destroys the separability that allows the solving of Eq. ( 8 ) instead of Eq. ( 7 ).

However, for the graphical document image denoising problem, the formulation in Eq. ( 6 ) is still adopted with the overcomplete dictionary D being defined as the synthesis operator of the curvelet transform in order to obtain smooth graphical contours. Moreover, to ease the investigation of the dependence of the framework X  X  parameter on the noise level, the optimization problem is written as  X   X  = argmin where is the precision parameter depending on z . Note that, the above formulation is a slightly modified version of the BPDN problem in [ 12 ] where the squared Euclidean norm is replaced by the Euclidean norm. This modification in fact does not change the nature of the problem as the value of could be changed accordingly: x  X  D  X  2  X   X  x  X  D  X  2 2  X  2 .
 The BPDN problem has been well studied by optimization specialists, and there are many practical methods for solv-ing it. For large-scale applications, the following special purpose optimizers are frequently used: iterative reweighted least squares [ 13 ], iterative shrinkage-thresholding [ 6 ], and least angle regression [ 21 ]. Interestingly, it has been shown that simple shrinkage could be interpreted as the first iteration of an algorithm that solves BPDN [ 23 ]. By solving the prob-lem in Eq. ( 9 ), the estimated image can be obtained from the sparse reconstruction as  X  x = D  X   X  .  X  x is of course in grayscale due to reconstruction from curvelets and is then converted to bilevel by a simple thresholding operation  X  x = T  X  x where T is the thresholding operator.

In the above BPDN problem, the 1 -norm is used instead of a more general q -norm to avoid the NP-hard problem [ 14 ] when q = 0, nonconvexity, when q &lt; 1, nonsparse and over-fitting solutions when q &gt; 1. In addition, a sparse solu-tion, which guarantees directional denoising by curvelets, is still obtained if the solution of the following 0 -norm opti-mization problem is sufficiently sparse [ 18 ]:  X   X  = argmin As the overcomplete dictionary D is constructed from curvelets and the images to be processed contain mainly graphical contours, this requirement is easily satisfied. Illus-trationofthedistributionofthemagnitudeofthe5,000largest curvelet coefficients of the image in Fig. 5 a obtained from the curvelet transform and BPDN is given in Fig. 7 . It is observed that BPDN results in a sparse representation; many elements of  X   X  have almost zero value. The sparsity in  X   X  is, in some respects, better than that in the coefficients D T x resulting from the curvelet transform. In addition, the shape of the dis-tribution of  X   X  resembles that of a Laplacian distribution, and this agrees with the Bayesian formulation of sparse coding [ 51 ].

It should also be noted that the problem in Eq. ( 9 ) and that in Eq. ( 2 ) are similar: They are both minimization problems with a fidelity constraint. The main difference between them is that TV denoising pursues an estimation that is sparse in the spatial domain (sparse gradient), whereas BPDN denoising has a desire for sparsity in the transform domain. Fusing TV with BPDN amounts to solving [ 10 ]  X   X  = argmin However, as the gradient of  X  is not clearly defined, imposing piece-wise smoothness in  X  by TV regularization in this way usually produces images with nondeterministic artifacts. 4.4 The precision parameter The BPDN problem in Eq. ( 9 ) has a nonnegative precision parameter that describes the desired fidelity of the recon-structed  X  x to the noisy x . It is the only parameter, besides the selected dictionary D , that controls the quality of denoised images. It is straightforward that when = 0, the BPDN problem reduces to a simple curvelet transform: x = D  X   X   X  X  X   X   X  = D T x and one easily has x =  X  x =  X  x .Asthevalueof increases, the measure of sparsity  X   X  1 of the solution  X   X  must monotoni-cally decrease, since the feasible set of solutions S ={  X  : x  X  D  X  2  X  } gets wider, taking the feasible set of solu-tions of a smaller as a subset: &lt; A sparser solution means a better alignment of curvelets with graphics contours, which consequently increases denoising performance. However, when has a reasonably large value, the solution  X   X  of the BPDN problem may be overly sparse in terms of the 1 -norm, and the estimated image  X  x gets overly blurred. In addition, due to the thresholding operation to get the binary image  X  x from  X  x , deformation in  X  x will appear. Illustration of the influence of the value of on the estimated images is given in Fig. 8 using the noisy image in Fig. 5 a at = 30 , 40 , 50 , 60. It can be seen that for both threshold-ing operations using a fixed threshold of 0 . 5 or using Otsu X  X  threshold [ 40 ]:  X  A small value of = 30 results in insufficient blurring in the estimated images. The binarized images still have noise along the contours.  X  A large value of = 60 causes over blurring in the esti-mated images. Deformation can be observed in the bina-rized images.

The selected value of should depend on the level of noise that exists in the images. In the literature, there exists no work that discusses in detail the dependence of on an image X  X  noise level. For zero-mean white and homogeneous Gaussian noise with a known standard deviation  X  ,thevalue of is usually chosen as cn  X  2 , with 0 . 5  X  c  X  1 . 5[ 22 , Chap-ter 14]. For graphical document images, the theory of edge noise presented in Sect. 2 sheds light on this problem by the established linear relationship between NS and the expected Hamming distance as given in Eq. ( 1 ). It is thus fair to conjec-ture that the relation ( NS ) should also be linear and is of the form = k NS. This is because x  X  T ( D  X  ) 2 is essentially the Hamming distance between the binary denoised image  X  x = T ( D  X  ) and its corresponding noisy one x . Experimental evidence of the linear relationship between and NS will be shown in the experimental section (Sect. 5 ). The remainder of this paper uses a fixed level of 0.5 for the thresholding oper-ation since it gives better bilevel images than using Otsu X  X  method. 4.5 BPDN with a learned dictionary The above formulations use the synthesis operator of the curvelet transform as the overcomplete dictionary D . Thus, D is fixed regardless of the input image x that needs to be represented. Allowing D to change leads to the problem of dictionary learning, where D is learned to optimally adapt to a certain class of images. Dictionary learning for natural images under the sparsity assumption [ 39 ] aims at maximiz-ing the likelihood that natural images have efficient, sparse representations in an overcomplete dictionary. Mathemati-cally speaking, for a given image x , the goal of learning is to find an overcomplete dictionary D  X  such that D  X  = argmax In order to solve the above difficult optimization problem, two main assumptions were introduced. First, the coefficients  X  i are independent, and each has a Laplacian distribution, which nicely fits the probability distribution of  X  i when the decomposition is sparse. The second assumption is that the reconstruction error x  X  D  X  can be modeled as the addi-tive white Gaussian noise. Under these two assumptions, the problem in Eq. ( 10 ) transforms into the minimization problem: D  X  = argmin When a set of n images X =[ x 1 ,..., x n ] is used to learn the dictionary D , the above problem becomes D  X  = min where A =[  X  1 ,...,  X  n ] . This problem could also be viewed as a matrix factorization problem using 1 regularization where the matrix X is factored into the two matrices D and A . In the literature, this problem is usually solved by alternating between two steps:  X  Sparse coding D is kept constant, and the energy function is minimized with respect to A .  X  Dictionary update A is kept constant, and the energy func-tion is minimized with respect to D .
 The algorithm alternates between sparse coding and dictio-nary update until convergence. Different dictionary learning methods use different strategies to perform these two steps, of which the sparse coding step is essentially the problem in Eq. ( 6 ). For example, the original method [ 39 ]usesconvex optimization for sparse coding and gradient descent for dic-tionary update. A fast online learning algorithm was recently proposed [ 34 ] by using a subset of the training data for the optimization problem in Eq. ( 11 ) and then augmenting the subset with new training samples to compute a new solution using the previous solution as initialization.

Provided that D has been learned by solving the problem in Eq. ( 11 ), denoising X leads to solving  X  A = argmin which is in fact an aggregate of Eq. ( 9 ) for all images x Accordingly, the denoised images are  X  X = D  X  A . Sincelearning D directlyfromdataisveryexpensivewhen D has a large size, it is common in practice to learn D from a set of some small-sized representative  X  X atches X  extracted from the noisy image. When D has been learned, a denoised version of each patch from the noisy image is then computed from D using Eq. ( 9 ). The final denoised image in grayscale is formed by tiling and averaging all the denoised patches according to their extraction pattern from the noisy image. In this work, the patch size is chosen to be 16  X  16 to balance performance and complexity. Moreover, due to the adoption of patch-based denoising when D is learned from data to avoid high computational complexity, the linear relationship conjecture in Sect. 4.4 becomes incorrect. This is because of the locality of patch data, or the locality of dictionary atoms, that D cannot represent the noisy image as a whole. Figure 9 illustrates some atoms of size 16  X  16 in a dictionary learned from some patches extracted from the noisy image in Fig. 5 a. It can be observed that the learned dictionary contains atoms that represent oriented edges well in the graphical document images.

For convenience, in the remainder of this paper BPDN with the dictionary defined as curvelets is denoted as BPDN-fixed, whereas BPDN with the dictionary learned from data is denoted as BPDN-learned. 4.6 Algorithm summary A summary of the sparsity-based denoising algorithm is given in Algorithm 1 . The sparse coding step to solve Eq. ( 9 ) is carried out by using the orthogonal matching pursuit [ 41 ]. The dictionary learning step is performed by using the K-SVD algorithm [ 1 ]. For BPDN-fixed, the learning step is not needed since the dictionary D is defined analytically as curvelets. In practice, we use an efficient implementation of K-SVD for dictionary learning and batch-OMP for sparse coding [ 44 ]. A thorough analysis of the complexity of sparse Algorithm 1 Sparsity-based image denoising coding and dictionary learning steps has also been given in [ 44 ].

It should be noted here that the size of the dictionary D in Eq. ( 9 ) depends on the length w h of x : D has w h rows and at least w h columns in order to make D redundant. When BPDN-fixed is used, w h is the number of pixels in the noisy image f and the size of D grows quadratically with the image size. Thus, BPDN-fixed rapidly becomes pro-hibitivelyexpensivewhendealingwithnoisyimagesoflarger size. In contrast, BPDN-learned works with image patches of size 16  X  16 and x has a constant length of 256. The size of D in BPDN-learned remains almost the same, regard-less of the size of original noisy images. Thus, the size of the noisy image f only slightly affects the complexity of BPDN-learned. In practice, in order to denoise a large-sized image using BPDN-fixed, the noisy image is first divided into sub-images of small size (for example 256  X  256). BPDN-fixed is then used to denoise these sub-images, and finally, the denoised sub-images are merged together to obtain the denoised image. 5 Experimental results In order to demonstrate the effectiveness of bilevel graphical document image denoising using BPDN with the overcom-plete dictionary D defined as curvelets or learned from data, three types of experiments have been carried out: one for the validation of the linear relationship between the parameter and NS; one to show the qualitative effectiveness of the methods over a range of degradation levels and graphical symbol shapes; and the third to demonstrate the superiority, both qualitatively and quantitatively, of the proposed method over comparison ones.

Two metrics will be used for quantitative analysis: image recovery and contour smoothness. Image recovery in this work is measured with the normalized cross correlation between the original image and the resulting image. This is calculated using the formula: where A and B are 2D input data of size m  X  n and  X  A and  X  B are the mean values of A and B . The normalized cross correlation takes on values in the range [ X  1 , 1 ] , with the value 1 occurring when the two images A and B are the same (i.e., no error, or perfect reconstruction).

While the use of image recovery as an evaluation criteria is straightforward, contour smoothness is adopted to quan-titatively evaluate the capability of comparing methods in producing denoised images of good visual quality. Contour smoothness is a measurement based on raggedness defined in [ 30 ]. Raggedness is the standard deviation of the distance of the edge pixels from a line fitted to the edge threshold of the line. In its original form, it was applied to straight lines. Here, we have adapted it to be a moving average of the raggedness measure over an edge segment 15 pixels long. 5.1 The relation ( NS ) A dataset of noisy images has been generated from a ground-truth and noise-free image  X  X ymbol017 X  to be used to evalu-ate the relationship ( NS ) .  X  X ymbol017 X  is a graphical sym-bol image of size 256  X  256 taken from the GREC2005 database [ 48 ], and its noisy version is given in Fig. 12 . This image was degraded at 10 values of NS ranging from 0 . 2to2 . 0 with increments of 0 . 2. Assuming that the parameter of BPDN-fixed in Eq. ( 9 ) takes the value  X  that corresponds to the peak in denoising performance in terms of a measure of fidelity between  X  x and x 0 , and the relation  X  ( NS ) thus needs to be established exper-imentally. The measure of fidelity, denoted by  X   X  x , x employed in this work is the normalized cross correlation defined as  X   X  x , x 0 () = corr2 (  X  x , x 0 ) , where corr2 in Eq. ( 12 ).

Illustration of the determination of  X  by means of  X   X  x is given in Fig. 10 a where the noisy image in Fig. 12 p with NS = 2 . 0 is taken as the input image. To compute the value of  X  lem in Eq. ( 9 )issolvedfor  X   X  and then, the bilevel denoised image  X  x = T  X  x = T D  X   X  can be easily obtained. The plot of  X   X  x , x 0 () in the case of a fixed thresholding of 0 has its maximum value of 0 . 9439 at  X  = 48. This means that if the input noisy image has NS = 2 . 0, in Eq. ( 9 ) should take the value 48 in order to have the  X  X est X  denois-ing performance. It should be noted here that, due to the blunt maxima in  X   X  x , x 0 () , a small deviation of the selected value of from  X  has almost no effect on the performance of BPDN-fixed.

Having determined the value of  X  for each image of a certain NS, the relation  X  ( NS ) is established. This was repeated three more times, by generating three more sets of 10 degraded images, with different instances of noise. These four relations are plotted separately in Fig. 10 b. It can be seen that an image of higher NS requires a larger value of  X  for optimal performance. In addition,  X  has a nearly linear relationship with NS for all four subsets. A narrow band formed by  X  ( NS ) also means that the stan-dard deviation of  X  is reasonably small. Combining this fact with the blunt maxima in  X   X  x , x 0 () , it thus can be con-cluded that if the parameter is estimated from the relation ( NS ) , the proposed method with curvelets as the dictio-nary performs almost at its full potential. Similar results were seen for other symbol images from the GREC2005 dataset. 5.2 Illustration of results The proposed method for denoising bilevel graphical docu-ment images using BPDN was evaluated on two datasets. The first, shown in the previous subsection, was designed to show that results hold over multiple instances of the same levels of noise. The second is designed to illustrate the effective-ness over different shapes of symbols. It is generated from ground-truth and noise-free graphical symbol images 011, 016, 024, 047, 048, 058, 081, 104, 107, and 147, in addi-tion to symbol017 also used in the previous subsection, all from the GREC2005 database. These symbols are shown in Fig. 11 . These images are selected due to the existence of all possible graphic contour directions and various configu-rations of contours that may cause difficulties in denoising.
Figures 12 and 13 provide examples of denoised graphi-cal symbols by BPDN-fixed and BPDN-learned. Figure 12 shows symbol017 at four different noise levels NS = 0 . 8 , 1 . 2 , 1 . 6 , 2 . 0. Figure 13 shows noisy versions of three different ground-truth images, all at noise levels NS = 2 In the case of BPDN-fixed, =  X  for each case in order to have optimal performance. For BPDN-learned, has a fixed value of 0 . 75 determined experimentally. In these fig-ures, the original noisy images are given in the first column. The corresponding estimated images in grayscale obtained by using BPDN-fixed and BPDN-learned are given in the second and fourth columns, respectively. These images after being converted back to bilevel through thresholding at a 0 . 5 level are shown in columns three and five. Evidence of directional denoising along noisy contours exists in the esti-mated images by BPDN-fixed in the second column: Edge noise is smoothed out in the direction that is perpendicular to the noisy contour. It is like the images have been filtered locally along the noisy contours by anisotropic filters, and each has its direction coincident with the local direction of the nearest contour. Due to this directional filtering phenom-ena, the denoised images in binary by BPDN-fixed in the third column are clean and have smooth contours. For BPDN-learned, because of the locality of patch-based denoising, the estimated images by BPDN-learned in the fourth column do not have clear evidence of directional denoising along noisy contours as the estimated images in the second column by BPDN-fixed. However, it can be seen that BPDN-learned does remove noise along the contours and produces grayscale images of very high visual quality. This is because the dictio-nary used in BPDN is learned directly from the noisy images so that the learned atoms adapt more to the graphical con-tours. As a result, the denoised images in binary by BPDN-learned in the last column are also clean and have smooth contours. 5.3 Comparison with existing methods To demonstrate the effectiveness of the proposed method based on BPDN, comparison with seven frequently used methods has been carried out:  X  Bilevel denoising median filtering uses a 3  X  3 neighbor-hood; kFill filtering sets the parameter k = 3; and closing then opening and opening then closing both use a 3  X  3 structuring element.  X  Total variation TV takes the value  X  optimal performance in terms of normalized cross corre-lation. The selection of TV is similar to the selection of in the proposed method.  X  Shrinkage hard-thresholding of curvelet coefficients with one threshold value  X  jl is used for all curvelets of scale j andangle l .  X  jl iscomputedbyapplyingaforwardcurvelet transform on an image containing a delta function at its center.  X  Diffusion iterative applications of anisotropic diffusion and coherence enhancing diffusion in sequence with para-meters determined by experience.
 The proposed method using BPDN with fixed and learned dictionaries and all comparison methods are applied to each degraded image, and then, normalized cross correlation and raggedness (described at the start of Sect. 5 ) are measured for each resulting denoised image. Samples of denoised images from comparison methods using an image of NS = 2 . 0are given in Fig. 14 . It can be seen that the images resulting from kFill filtering, median filtering, morphology-based methods, and total variation have ragged edges and bad visual qual-ity with the worst images resulting from morphology-based methods. Shrinkage, diffusion, and BPDN-based methods produce denoised images of good visual quality. How-ever, the diffusion method has more difficulty in restoring the sharp corners of the contours; shrinkage and BPDN-fixed methods produce graphical strokes of nonuniform width.

For a quantitative comparison, the performance of each method per noise level is computed as the average perfor-mance over noisy images of the same noise level. At each noise level, the first dataset has 4 noisy images of symbol017, and the second dataset has 10 noise images of each of the graphical symbols 011, 016, 017, 024, 047, 048, 058, 081, 104, 107, and 147. Thus, the averaging in the second dataset is performed over 110 noisy images. The comparison results are shown in Fig. 15 for the two restoration criteria over a range of noise levels. The left column shows the average over multiple noise instances of the same shape (symbol017), and the right column shows the average over a range of different shapes. It is observed that as the noise level (NS) increases, the ability to recover the original images decreases and the contour raggedness of the denoised images increases for all methods. The performance of kFill filtering, morphology-based methods, and total variation are similarly bad with open-closing breaking down when the noise level is reason-ably high ( NS &gt; 1 ) . Median filtering has its performance in the middle. Top performance belongs to shrinkage, diffu-sion, and BPDN-based methods. The decrease in the image recovery and increase in the contour raggedness of these four best methods are nearly constant and are the smallest among all methods for both datasets. Nevertheless, diffusion has slightly worse performance than that of shrinkage and BPDN. BPDN-fixed outperforms shrinkage when the noise level is small to moderate ( NS &lt; 1 . 5 ) , and their performance are comparable when the noise level is high ( NS  X  1 . 5 ) BPDN-learned results in a denoised image of best recovery with smoothest contours at all values of NS. It should also be noted from the comparison results that for noisier images, in terms of image recovery and contour raggedness, BPDN-learned produces the most significant improvements.
The superiority of BPDN-learned over BPDN-fixed can be explained by the locality of dictionary atoms. The dictio-nary of BPDN-fixed is constructed from curvelets, and each curvelet has a fixed set of location, orientation, and scale due to its analytical definition. On the contrary, the dictionary of BPDN-learned is learned directly from the data, and thus, its atoms will have location, orientation, and scale so as to  X  X est X  represent the data. The superiority of BPDN-learned over BPDN-fixed and other comparison methods provides clear evidence for the need to carefully design the dictio-nary in a sparsity-based denoising framework. Figure 16 shows the 95 % confidence interval for the averaged perfor-mance of the proposed methods and the two best compari-son methods (shrinkage and diffusion) using the aforemen-tioned dataset. The figure shows that the confidence inter-vals of BPDN-learned are well separated from that of other methods, meaning that BPDN-learned stably outperforms other methods. These results consolidate our conclusion on the superiority of BPDN-learned for graphical document images.

In addition to the results shown in Figs. 12 , 13 , 14 , 15 , 16 , additional experiments were run on symbols of different sizes: 128  X  128, 256  X  256, and 512  X  512 (Fig. 17 ). Five degradation levels were utilized that implemented NS levels of 0.2, 1, and 2. The BPDN-fixed and BPDN-learned meth-ods were applied as well as the comparison methods. Image recovery and contour smoothness were measured for each case. For all the degradation levels at all the resolutions, the sparsity-based methods continued to exhibit lower ragged-ness and higher image recovery than the other comparison methods, with results very similar to those in Figs. 12 , 13 , 14 , 15 . The performance of each denoising method improves as the resolution increases. At each noise level and for each denoising method, when the image size increases the image recovery value increases and the contour raggedness value decreases. This phenomena can be explained by the alias-ing effect that exists when the graphical contours are dis-cretized. At higher resolution, this effect becomes less seri-ous and denoising methods have less difficulty in recovering the original contours. However and more importantly, the relative performance of denoising methods on this dataset is similar to that on previous datasets. Sparsity-based denois-ing methods still provide the best performance. This demon-strates the robustness of sparsity-based denoising methods to resolution.

Performance of sparsity-based denoising algorithms has also been evaluated on some degraded images of engineer-ing drawings of size 1,950  X  2,700, and the example results are given in Fig. 18 . Shrinkage, diffusion, and sparsity-based methods still produce images of good visual quality, whereas the other methods result in images having ragged edges. Although shrinkage produces denoised images that have smooth contours, it fails in recovering dashed lines and contourintersectionpoints.Dashedlinesbecomecontinuous, and some intersecting lines get disconnected after denoising. In addition, the graphics and text components recovered by shrinkage usually have distorted shapes. BPDN-fixed pro-duces denoised images that are somewhat similar to the ones produced by shrinkage. This can be explained because the same dictionary is used by both BPDN-fixed and shrink-age. Diffusion is better than shrinkage when dealing with dashed lines and intersection points. However, when com-pared with BPDN-learned, the visual quality of denoised image obtained by BPDN-learned is better than that obtained by diffusion. These experimental results demonstrate clearly the performance and usefulness of BPDN-learned for large-sized scanned graphical document images. 6 Conclusion A new parametric method for edge noise removal from graphical document images has been presented in this paper using sparse representation by means of basis pursuit denois-ing. When the overcomplete dictionary is defined based on curvelets, noise spread can be used as the input parameter to select the precision parameter of the method due to the linear relationship between and NS . In the case of the learned dictionary, due to the locality of small-sized patch data, this linear relationship does not hold anymore. How-ever, it is anticipated that if the patch size is equal to the image size, or in other words, D is learned directly from the whole image data, the relation  X  ( NS ) becomes linear again. This is because in that case is simply the upper bound of the Hamming distance between the binary denoised image and its corresponding noisy one. The superiority of the proposed method over comparison ones in terms of image recovery and contour raggedness has been validated by experimental results.

It is possible that other image restoration methods, such as those discussed in this paper and others, might also benefit from measurement of NS and include it in their parameter setting. Investigating this is planned as the future work. References
