 We describe an optimal randomized MapReduce algo-rithm for the problem of triangle enumeration that re-quires O E 3 / 2 / ( M pected memory size of a reducer and M the total available space. This generalizes the well-known vertex partitioning approach proposed in (Suri and Vassilvitskii, 2011) to multi-ple rounds, significantly increasing the size of the graphs that can be handled on a given system. We also give new theo-retical (high probability) bounds on the work needed in each reducer, addressing the  X  X urse of the last reducer X . Indeed, our work is the first to give guarantees on the maximum load of each reducer for an arbitrary input graph. Our ex-perimental evaluation shows the scalability of our approach, that it is competitive with existing methods improving the performance by a factor up to 2  X  , and that it can signifi-cantly increase the size of datasets that can be processed. H.2.8 [ Database Management ]: Database Applications X  Data Mining Design, Experimentation, Algorithms Triangle Enumeration, MapReduce, Graph Mining
We are living in a flood of data. These data can be usu-ally represented as graphs such as social networks, interac-tion networks, road networks and so on. For example, in the most famous social network service Facebook, there are 1.23 billion monthly active users and they form a huge friendship network [12]. Because of the enormity of these networks, it is hard to mine meaningful information from them. Recently, several graph data mining problems have been intensively studied. Especially, the triangle enumeration problem is re-garded as one of the fundamental graph mining problems because of its various applications. In a social network, for example, the triangle enumeration problem is used for de-tecting sybil accounts and measuring content quality [2, 41]. On the web, it is used for finding spam pages and uncovering hidden thematic layers [2, 11]. Also, the triangle enumer-ation problem is a tool for solving the following problems: truss decomposition which finds k -trusses of a graph for all k where a k -truss is a subgraph such that every edge is in ( k -2) triangles [36]; triangular vertex connectivity which finds all triangularly connected vertex pairs [31]. The importance of these problems and their applications have been introduced in many previous works [6, 8, 11, 37].

Triangle enumeration has been widely studied in the last years, in particular in sequential platforms [17, 28, 7, 5]. However, as the graph size continues to increase, it is effec-tively impossible to enumerate triangles in massive graphs. One of the methods to deal with such large graphs is to ex-ploit recent parallel programming paradigms. In particular MapReduce [10], and its open source version Hadoop [16], has emerged as a de facto standard framework for process-ing massive data sets on large scale parallel platforms, such as clusters of commodity PCs. Informally, a MapReduce al-gorithm transforms an input set of key-value pairs into an output set of key-value pairs in a number of rounds , where in each round each pair is first individually transformed into a set of new pairs ( map step ), then grouped by key ( shuffle step ), and finally all values associated with the same key are processed, separately for each key, producing the next new set of key-value pairs ( reduce step ).

The triangle enumeration problem has then been studied in MapReduce [29, 1, 34]. The main objective of these re-sults is to derive efficient MapReduce algorithms requiring a very small number of MapReduce rounds. However, as shown in [1], a MapReduce algorithm using a small number of rounds must generate large amount of intermediate data that travel over the network during the shuffle operation. Since the amount of this intermediate data can be much larger than the input size, issues related to the performance of the network and to system failure may arise with mas-sive input graphs. Indeed, the network may be subject to congestion since a large amount of data is created and sent over the network in a small time interval (i.e., during the shuffle step), reducing the scalability and fault tolerance of the network. It is then desirable to design algorithms that tradeoff round number and decrease the amount of data ex-changed during a round, even at the cost of a larger number of rounds. Moreover, we observe that distributing a large computation among different rounds may help to bookkeep the computation and thus to restore it if the system com-pletely fails or if, in the case of cloud services, the computing cost exceeds a given threshold.

In this paper, we address this issue by proposing a new multi-round MapReduce randomized algorithm for enumer-ating all triangles that, by increasing the number of rounds, reduces the maximum amount of intermediate data required during each round. The algorithm exhibits a tradeoff among the number R of rounds, the maximum amount m of mem-ory words required by each reducer, and the total amount M of space required in a round. Specifically, we have R = O E 3 / 2 / ( M in the input graph. Our algorithm is based on a vertex partitioning as other previous MapReduce algorithms [34, 1, 29]. However, our partitioning relies on a 4-wise inde-pendent hash function which allows us to improve previous analyses that assume a random input graph: we guarantee the claimed results for any input graph. Moreover, 4-wise hash functions can be easily and efficiently implemented us-ing the tabulation based technique in [35].
Our main contributions are summarized as follows: We remark that the stated bounds apply for each input graph, and do not assume the input to be a random graph as in previous works.

We provide binary file of CTTP and datasets used for the experiments in http://kdm.kaist.ac.kr/cttp . The remaining part of the paper is organized as follows. In Section 2, previous works related to the triangulation prob-lem and MapReduce are reviewed. In Section 3 we formal-ize the problem and describe the MapReduce computational model. In Section 4, the CTTP algorithm is described and analyzed. In Section 5, we show the experimental results. In Section 6, some final comments are provided.
The enumeration problem has been recently targeted in the external memory model, that is on a disk-memory hier-archy where the memory has size m and data are trans-ferred in blocks of size B . In particular, Chu et al. and Hu et al. [7, 17] have proposed deterministic algo-rithms requiring O E 2 / ( mB ) I/Os, while Pagh and Sil-vestri [28] proposed an optimal randomized algorithm re-quiring O E 3 / 2 / ( B described an efficient algorithm requiring O (( E/B ) log I/Os for graphs with constant arboricity, while Chiba and Nishizeki have proposed a general RAM algorithm requiring O (  X E ) work for graphs with arboricity  X  . The relations between listing and other problems have also been widely investigated, see for instance Williams and Williams [39] for a reduction to matrix multiplication, and Jafargholi and Vi-ola [18] for 3SUM/3XOR. Triangle listing in certain classes of random graphs has been addressed recently by Berry et al. [3] to explain the empirically good behavior of simple tri-angle listing algorithms. For the related problem of counting the number of triangles in a graph, we refer to [24] and ref-erences therein.
 Triangle enumeration has been explicitly addressed in the MapReduce framework by Afrati et al. [1]. However, the MapReduce algorithms for triangle counting proposed by Suri and Vassilvitskii [34] and by Park and Chung [29] can be easily adapted to triangle enumeration. All these algorithms require just O (1) MapReduce rounds, but replicate each edge  X  p E/m times on the average, where m denotes the available space of a reducer. All these algorithms rely on a vertex partitioning that split the problem into ( E/m ) subproblems, each one solved in a reducer of size O ( m ) in a single round. We observe that the vertex partitions pro-posed by the previous works provide balanced subproblems only assuming an input random graph, and fail for some spe-cial graphs such as complete graphs. Finally, we recall that MapReduce algorithms exhibiting tradeoffs between round number, reducer size and total memory have been studied in [4, 30] for sparse and dense matrix multiplication.
MapReduce is a popular distributed programming framework [10] for processing very large amount of data. MapReduce has several advantages: (a) it allows program-ming for distributed systems easy; (b) it provides fault-tolerance; (c) it is highly scalable; and (d) it requires a relatively cheap cost to build and maintain a cluster. MapReduce , and its open source version Hadoop [16], have been used for many important graph mining tasks, like ra-dius/diameter [23], graph queries [22], triangle [21], and vi-sualization [20].
Our algorithms are designed and analyzed in the computa-tional model for MapReduce, named MR( m,M ), proposed in [30]. The model is defined in terms of two parameters m and M , characterizing the maximum amount of mem-ory available to a map/reduce function, and the maximum amount of memory available in the whole system, respec-tively.
 An algorithm in this model specifies a sequence of rounds . The computation within each round is defined by map and reduce functions whose input and output are multisets of key-value pairs. A pair is denoted as  X  k ; v  X  , where k is the key and v the value. Function map takes as input one pair and outputs a multiset of new pairs. Function reduce re-ceives as input pairs with the same key and outputs a mul-tiset of new pairs. We use the keyword emit (  X  k ; v  X  ) within a map/reduce function for specifying that the pair  X  k ; v  X  is an output pair.
 The r -th round, for each r  X  0, is organized as follows. The input of the round is a multiset I r of pairs. A new mul-tiset W r is then generated by applying the map function to each pair in I r ( map step ); we call mapper each application of the map function. Subsequently, pairs in W r with the same key are grouped together ( shuffle step ). Finally, each group of pairs with the same key is processed by the reduce function, and the multiset O r containing the output of the applications of the reduce function to each group is the final output of the round ( reduce step ). We denote with reducer a single application of the reduce function. Multiset O r be the input of the next round r + 1. Let m k,r be the space required for computing the reduce function on the group de-fined by key k in round r , and let K r be the set of distinct keys in W r . Then, the model requires that m k,r  X  m for each k  X  K r and r  X  0, and that P k  X  K r  X  0. Similar constraints are required for the map function.
The complexity of an algorithm in the MR( m,M ) model is the number of rounds R that it executes in the worst case. The goal is to minimize the round number for given values of m and M . We also define the total work of an algorithm in MR( m,M ) as the sum of the work requirement of all mappers and reducers.
We consider a simple, undirected graph G (no self loops, no parallel edges) with vertex set V and edge set E . The enumeration problem requires to enumerate all triangles within the graph G . We do not require the algorithm to emit a pair for each triangle, but we simply assume that for each triangle ( u,v,w ) the algorithm calls a local function enum (  X  ) with the triangle vertexes as input parameters.
For notational convenience and consistency with earlier papers, whenever the context is clear we use E as a short-hand for the size of a set E (and similarly for other sets). We will assume that the elements of V are ordered accord-ing to degree, breaking ties among vertices of the same de-gree arbitrarily. We assume that each edge { u,v } requires one memory word, and is initially represented by the pair a dummy key. Following [17], for a triangle ( v 1 ,v 2 ,v v the vertex v 1 its cone vertex . For any integer n , we denote with [ n ] the set { 0 ,...,n  X  1 } .

For the sake of simplicity, we assume there are no very high degree vertexes, that is, vertexes with degree larger than  X 
Em . We observe that triangles with at least one high de-gree vertex can be enumerated using sorting. Specifically, for each very high degree vertex v , all triangles containing v are found by suitably sorting three times the edge sets E as shown in [28, Lemma 1]. The sorting operation can be car-ried out using the MapReduce algorithm proposed in [14], which requires O (1) rounds as soon as the reducer size m is larger than finding all triangles with a given very high degree vertex, M/E very high degree vertexes can be processed in parallel. Thus all triangles with at least one very high degree vertex are enumerated in O E 3 / 2 / ( M ers of size m , constant size mappers, aggregate space M and total work O E 3 / 2 . We observe that the round complexity and the total work for removing very high degree vertexes are asymptotically negligible compared with the ones of the CTTP algorithm described in the next section.
In this section we propose Colored Triangle Type Partition (CTTP), a MapReduce algorithm for triangle enumeration that exhibits a tradeoff between the number of rounds R , the space m required by each mapper or reducer, and the total aggregate space M . For arbitrary input graph, CTTP re-quires R = O E 3 / 2 / ( M each mapper and reducer requires space M/E and expected space m , respectively. We then show how to get rid of the  X  X urse of the last reducer X  in high probability with minor changes to the CTTP algorithm. As already mentioned we suppose for simplicity that there are no very high degree vertexes (i.e., vertexes with degree larger than refer to Section 3.2 for an approach that enumerates all tri-angles with at least one very high degree vertex with the same asymptotic round complexity of the CTTP algorithm.
The CTTP algorithm is based on vertex partitioning as many other previous MapReduce algorithms [34, 1, 29]. CTTP, howerver, partitions vertexes according with a col-oring function randomly selected from a 4-wise independent family of functions, as suggested in [28] for triangle enu-meration in external memory. By just requiring 4-wiseness to the coloring function, our technique overcomes previous analyses that assume a random input graph, and guarantees that the claimed result applies to any input graph.
The vertexes are colored with  X  = d p 6 E/m e colors us-ing a function  X  : V  X  [  X  ] chosen uniformly at random from a 4-wise independent family of functions. We let E i,j with i  X  j and i,j  X  [  X  ], denote the set { ( u,v )  X  E | i = angle ( u,v,w ) is classified type-3 if no two vertexes of the triangle have the same color, type-2 if there are exactly two vertexes with the same color, and type-1 if all vertexes have the same color.

The CTTP algorithm decomposes the enumeration prob-lem into K =  X  3 +  X  2 =  X  (  X  2  X  1) / 6 subproblems. The subproblems are of two types: 1. ( i,j,k ) -subproblem , with i &lt; j &lt; k and i,j,k  X  [  X  ]: 2. ( i,j ) -subproblem , with i &lt; j and i,j  X  [  X  ]: the algo-
The CTTP algorithm solves each subproblem within a single reducer, by evenly distributing the K subproblems among R =  X E/M rounds. The pseudocode of the r -th round, for each 0  X  r &lt; R , is given in Algorithm 1. The input of each round is a set of pairs  X   X  ; ( u,v )  X  where ( u,v ) is an edge in E and  X  is a dummy symbol.

In the r -th round, CTTP solves the ( i,j,k )-subproblem, for each 0  X  i &lt; j &lt; k &lt;  X  with i + j + k  X  r mod R , in the reducer associated with key ( i,j,k ), and the ( i,j )-subproblem, for each 0  X  i &lt; j &lt;  X  with i + j  X  r mod R , in the reducer associated with key ( i,j,  X  1). Mappers are re-sponsible for forwarding each edge to the right reducers. For each input pair  X   X  ; ( u,v )  X  , the mapper sends the following 1. Case i 6 = j . For each 0  X  k &lt;  X  such that k  X  2. Case i = j . For each 0  X  k &lt;  X  such that k  X  r  X  i
We observe that the proposed distribution of subproblems among rounds guarantees that each mapper emits the same number of pairs (i.e.,  X /R =  X  ( M/E ) emitted pairs per mapper in each round). Therefore, the work of the map step can be evenly distributed among the available process-ing units by the runtime schedule, avoiding that the compu-tation is delayed by a few slow mappers. In contrast, other distributions may require some mappers to emit much more pairs than others. For instance, by lexicographically sort-ing the triplets/pairs denoting subproblems and then solv-ing each chunk of consecutive E/m subproblems in a round, we get that a few mappers emit up to E/m pairs each, while the remaining mappers emit just a constant number of pairs.
Algorithm 1 : Map and reduce functions in the r -th round of CTTP.
 2: if i 6 = j then 5: for ` = 0 to  X /R  X  1 do 10: else 12: for ` = 0 to  X /R  X  1 do 1: if k  X  0 then 3: else Let K =  X  (  X  2  X  1) / 6 be the total number of subproblems. It is easy to see that O ( K/R ) subproblems are solved in each round. However, the next lemma shows that exactly K/R subproblems are solved in each round if R is not a multiple of 2 or 3, and that the deviation from K/R is at most 7  X / (6 R ) otherwise.

Lemma 1. Let K =  X  (  X  2  X  1) / 6 be the total number of subproblems and let K r be the number of subproblems solved in the r -th round. Then, K r = K/R if R 6 X  0 mod 2 and R 6 X  0 mod 3 , and K/R  X  5  X / (6 R )  X  K r  X  K/R + 7  X / (6 R ) otherwise.

Proof. Let K 0 r be the number of ( i,j,k )-subproblems solved in a round. We recall that a ( i,j,k )-subproblem is solved in the r -th round if the following congruence is veri-fied: It is easy to see that K 0 r = K  X  r / 3!, where K  X  r be the number of triplets ( i,j,k ) with i,j,k  X  [  X  ] and i 6 = j 6 = k (i.e., no assumption on the order) satisfying Equation 1: indeed all and only the 3! permutations of each triplet in K 0 r in K  X  r . For any given values of i and j in [  X  ], there are  X /R values of k that verify the congruence k  X  r  X  i  X  j mod R . Since there are  X  2 combinations of i,j we get that there are  X  3 /R triplets satisfying Equation 1. However, this number also contains triplets with two or three equal terms. We now count the number of triplets with only two or one with i,j  X  [  X  ] satisfying Equation 1, that is 2 i + j  X  r mod R , is 3  X  2 /R . Note that this number contains also the triplets ( i,i,i ) satisfying the congruence. The number of triplets ( i,i,i ) satisfying Equation 1, that is 3 i  X  r mod R , depends on the value of R . If R 6 X  0 mod 3, then 3 i  X  r mod R has  X /R solutions for any r . If R  X  0 mod 3, then 3 i  X  r mod R has no solution when r 6 X  0 mod 3, and 3  X /R solutions when r  X  0 mod 3.
By the above arguments, we have that K 0 r = (  X  3 3(  X  2 /R  X   X /R )  X   X /R ) / 3! =  X  3 /R if R 6 X  0 mod 3. On the other hand, we have for R  X  0 mod 3 that K 0 r = (  X  3 /R  X  3(  X  2 /R  X  3  X /R )  X  3  X /R ) / 3! =  X  3 /R + 2  X / (3 R ) if r  X  0 mod 3 and K  X  r = (  X  3 /R  X  3  X  2 /R ) / 3! =  X  3 /R  X   X / (3 R ) if r 6 X  0 mod 3.

Consider now the number K 00 r of ( i,j )-subproblems. Us-ing an argument similar to the previous one we get that K r =  X  (  X   X  1) / (2 R ) =  X  2 /R if R 6 X  0 mod 2, K 00 r  X  (  X   X  2) / (2 R ) =  X  2 /R  X   X / (2 R ) if R  X  0 mod 2 and r  X  0 mod 2, and K 00 r =  X  2 / (2 R ) =  X  2 /R +  X / (2 R ) if R  X  0 mod 2 and r  X  0 mod 2.

The lemma then follows by summing up K 0 r and K 00 the different cases.

We are now ready to prove the claimed performance of the CTTP algorithm.

Theorem 1. The CTTP algorithm correctly enumerates all triangles of the input graph in ( E/M ) l p 6 E/m m rounds. In each round, each mapper and reducer requires space M/E and expected space m , respectively, and the aggregate space is M . The total expected work is O E 3 / 2 .

Proof. It is easy to see that each ( i,j,k )-subproblem is evaluated once during round r = i + j + k mod R . Moreover, all edges E 0 = E i,j  X  E j,k  X  E i,k required for solving the subproblem are available to the reducer. Indeed, for each edge ( u,v )  X  E i,j the mapper receiving the pair  X   X  ; ( u,v )  X  creates a message  X  ( i,j,k 0 ); ( u,v )  X  with k 0 = k (Line 9 of Algorithm 1). Similarly for edges in E j,k and E i,k equivalent argument shows that all ( i,j )-subproblems are correctly solved.

Since each mapper emits M/E pairs, a mapper clearly requires M/E space. On the other hand, the expected size of each reducer is m since it receives three sets of expected size m/ 3 each. At any time, the aggregate space is M since there are at most M/E copies of each edge.

We now upper bound the work for solving ( i,j,k )-subproblems. Let E i,j , E j,k and E i,k be the random variables representing the input size of the three sets re-ceived by the reducer with key ( i,j,k ). Using any work-efficient sequential algorithm (see e.g. [17, 28]) within each reducer, we get that the work required by the reducer The total work T can thus be upper bounded as follows:
Let L (resp., S ) be the set of pairs ( i,j ) for which E (resp., E i,j  X  m ). Thus The expected work of T is then By [28, Lemma 2], E P ( i,j )  X  L E 2 i,j is upper bounded by O ( Em ) when the maximum degree of the graph is Thus E ( T ) = O ( E 3 / 2 ). Since the total work for solving ( i,j )-subproblems is negligible, the claim follows.
Finally, we observe that our algorithm is optimal by de-riving a lower bound on the round number required by an algorithm for triangle enumeration. We assume that each edge or vertex requires at least one memory word: that is, at any point in time there can be at most m edges/vertexes in a reducer of size m . This assumption is verified by our al-gorithm, and is similar to the indivisibility assumption which is usually required for deriving lower bounds on a memory hierarchy.

Theorem 2. Any algorithm using reducers of size m and aggregate space M requires, even in the best case, b t/ ( M CTTP algorithm is then asymptotically optimal in the worst case.

Proof. Without loss of generality, we ignore mappers since they would decrease the lower bound by just a constant factor (in fact, as already noticed in [26], a reduce step can clearly embed the subsequent map step so that a MapRe-duce computation can be simply seen as a sequence of rounds of reduce steps). A reducer with size m cannot enumerate more than m 3 / 2 triangles [28] by the initial assumption on the memory words required by each edge/vertex. Then the maximum number of triangles that can be enumerated in round and m i is the size required by reducer with key i . Since P i  X   X  m i  X  M and m i  X  m , we get that the summa-tion is upper bounded by M reducers of size m ). Then, at least b t/ ( M required for enumerating t distinct triangles. Since there ex-ists a graph with t =  X  E 3 / 2 (a complete graph with vertexes) the worst-case optimality of CTTP follows.
We observe that the lower bound proposed by Afrati et al. [1] for triangle enumeration in MapReduce does not apply in our settings. Indeed, they show that a one-round algo-rithm requires in the worst case at least aggregate memory M =  X  V 3 / bound can also be derived as a special case of Theorem 2.
We now show how the vertex coloring in the CTTP algo-rithm can be improved in order to get strong guarantees on the maximum load of each reducer. As in the previous sec-tion, we assume vertex degree to be not larger than For the sake of simplicity, the results in this section are pro-vided in asymptotic notation, although exact bounds can be derived as in the previous section with simple but tedious derivations.

We use two different coloring techniques for high degree vertexes (i.e., with degree in ( vertexes (i.e., with degree in [0 , are colored using a coloring function  X  : V  X  [  X  ] randomly chosen among a set of log E -wise functions. On the other hand, the coloring of high degree vertexes is deterministi-cally computed as suggested in [32] for subgraph enumera-tion in external memory. Namely, high degree vertexes are colored using  X  colors in such a way that the sum of the degrees of vertexes with the same color is  X ( easy to see that vertex degree can be computed by sorting the edge set E , while the coloring of the at most 2 degree vertexes can be computed within a single reducer as soon as m =  X (
Theorem 3. The CTTP algorithm with the above color-ing technique enumerates all triangles of the input graph in O ( E/M ) l p E/m m rounds. The algorithm uses O ( M/E ) space per mapper and O ( M ) aggregate total space. More-over, if m =  X  E 3 / 4 at least 1  X  1 /E , for any constant &gt; 0 , that each reducer requires O ( m ) space and the total work is O E 3 / 2 .
Proof. In [32, Theorem 3], it is proved that the prob-1  X  1 /E as soon as m =  X   X  colored by assigning a random color in [  X  ] independently and uniformly. The proof relies on a result by [19] for providing strong bounds on the sum of dependent random variables. However, by using a result in [15] instead of the one by [19] within the proof of [32, Theorem 3], it is possible to extend the result to the case low degree vertexes are colored with a log E -wise hash function assuming m =  X  E 3 / 4
Consider the edge set E 0 containing edges adjacent to only low degree vertexes, and let Y e for e  X  E 0 be an indicator variable set to 1 if e is in E i,j and 0 otherwise. We clearly have | E i,j  X  E 0 | = P e  X  E 0 Y e . Since Y e and Y e dent if e and e 0 share a vertex in G and vertexes are col-ored using log E -wise functions, we use the result by Grad-wohl and Yehudayoff [15, Theorem 3.1] for providing a devi-ation bound on the sum of the dependent random variables Y . It follows that | E i,j  X  E 0 | = O ( m ) with probability we use log E -wise hash functions and a random variable Y depends on the at most 2 edges).

Consider now the edge set E 00 containing edges connecting high degree vertexes of colors i or j to low degree vertexes. By the coloring of high degree vertexes, we have that E 00 O set to 1 if the low degree vertex of e has color i or j . We have | E i,j  X  E 00 | X  P e  X  E 0 Z e (note that Z e = 1 even if vertexes in e have both color i ). Since these variables are dependent, we use again [15, Theorem 3.1], getting that | E i,j  X  E 00 | = O ( m ) with probability 1  X  O ( E 3 / 4 from [15] since we are using log E -wise hash functions and a random variable Z e depends on the at most 2 variables of edges adjacent on the low degree vertex of e ).
Finally, consider edges connecting only high degree ver-texes. There cannot be more than O ( m ) of these edges in E i,j since there are at most  X  ( with the same colors.
 Therefore, the set E i,j for a given color pair ( i,j ) has size O ( m ) with probability 1  X  O ( E 3 / 4 log E ) /m log E By applying an union bound, we get that all edge sets E i,j have size O ( m ) with probability 1  X  ( E/m ) O ( E 3 / 4 by setting m  X   X E 3 / 4 the size of each reducer if O ( m ) with probability at least 1  X  1 /E , the upper bound on the work complexity easily follows.

In this section, we experimentally evaluate the proposed algorithm and compare it to recent MapReduce algorithms for triangulation. We aim to answer the following questions from the experiments.
 Q1 How does the number of round affect the performance Q2 Is CTTP scalable in terms of the size of data and the Q3 Does CTTP work well with real world datasets?
We first introduce the datasets which are used for the ex-periments, and we specify how to implement the algorithms in Section 5.2. After that, we answer the questions in sec-tion 5.3 by presenting the result of the experiments.
We use real-world datasets listed in Table 2 to evaluate the proposed algorithm. They are brought from various sources. LiveJournal is an online community for sharing journals, and the LiveJournal dataset is the friendship network of the com-munity [33]. The PhoneCall dataset is a list of phone call records on December 2007 and on January 2008. It is of-fered by an anonymous telecommunication service provider. The Twitter dataset is the  X  X ollowing X  network of Twitter, a famous social network service [25]. The SubDomain dataset contains links among subdomains on the Web [27]. The Ya-hooWeb dataset is a page level hyperlink graph [40]. The largest one used in the experiments, the ClueWeb09 dataset is another page level hyperlink graph [38].

Each dataset is preprocessed to be a simple undirected graph. If a pair of vertexes has two edges ( a,b ) and ( b,a ), one of them is deleted from the graph. If an edge appears multiple times in a dataset, all of the duplicate edges are removed except one. We also remove all self-loop edges of which source and destination vertexes are the same. This procedure can be simply implemented on MapReduce and the running time is O ( E ) which is dominated by the running time of the main algorithms. We exclude the preprocessing time when we analyze the running time of all algorithms because they need the same preprocessing before execution.
We compare the proposed algorithm (CTTP) with the recent MapReduce algorithms (TTP [29] and GP [34]) for triangulation. The proposed algorithm and the compared algorithms are written in Java programming language and they are executed on Hadoop which is regarded as the de facto standard implementation of MapReduce. All experi-ments are conducted on a Hadoop cluster at KAIST with 40 machines where each machine has 4GB of memory. The default number of reducers is set to 80.

The number  X  of partitions (or colors) for GP and TTP is set to p 9 E/m and p 6 E/m , respectively, for the following reason. Let us assume that a dataset is divided into  X  parti-tions. When we suppose that edges are evenly distributed, the probability that vertexes incident to an edge belong to a specific partition is 1 / X  2 ; this kind of edge is called inner-edge . Similarly, the probability that the vertexes incident to an edge belong to two specific partitions is 2 / X  2 ; this type of edge is called outer-edge . It indicates that the probabil-ity that an edge belongs to a subproblem ( i,j,k ) of GP is 3  X  1 / X  2 + 3  X  2 / X  2 = 9 / X  2 because there are three cases of inner-edge, i , j , and k , and three cases of outer-edge, ( i,j ), ( i,k ), and ( j,k ); thus the expected number of edges in a subproblem of GP is 9 E/ X  2 . All edges in a subproblem should fit in the memory of a reducer, i.e. 9 E/ X  2  X  m , and it leads to set  X  to p 9 E/m . TTP processes two types of subproblems, ( i,j,k ) and ( i,j ). In contrast with GP, a sub-problem ( i,j,k ) in TTP has no inner-edge, and it indicates that the expected number of edges in a subproblem ( i,j,k ) is 6 E/ X  2 . The expected number of edges in a subproblem ( i,j ) is 4 E/ X  2 which is smaller than a subproblem ( i,j,k ). Thus, we set  X  to p 6 E/m for TTP. The number  X  of colors for CTTP is set to p 6 E/m , the same as that of TTP, because CTTP and TTP process exactly the same subproblems.
As we mentioned in Section 4, the number R of rounds of CTTP is O (  X E/M ). In our experiments, we suppose a poor condition where the total available space M is only the input size E , i.e. M = E , so R is set to  X  in Section 5.3.2, 5.3.3, and 5.3.4. We make an exception if the  X  is smaller than mentioned, the number K of subproblems is  X  (  X  2  X  1) / 6. If R =  X  , the number K r of subproblems solved in a round is (  X  2  X  1) / 6. If K r &lt; r , CTTP cannot utilize all reducers since several reducers do nothing and just waiting until the other reducers finish. In fact, such situation occurs when the input dataset is so small compared to M . In this case, we set R to 1 as in the previous algorithms.
In this section, we present the experimental evaluation an-swering the questions listed in the beginning of Section 5. The result tells us that 1) the number of rounds affects the performance of CTTP with a small factor, and the addi-tional cost is much smaller than the total cost, 2) CTTP is very scalable in terms of data size, and its machine scal-ability is close to the optimum, and 3) CTTP works very well with real world datasets, and outperforms the recent algorithms.
In CTTP, the number R of rounds is O ( E 3 / 2 /M While the number E of edges are determined by the in-Figure 1: Effect of the number of rounds. It shows the running time and the shuffled data size per round with various total number of rounds on the Twitter dataset. The dotted line in (a) is the ex-pected result from the cost function of Equation 6. With R  X  4 , the result is almost the same as ex-pected. With R &lt; 4 , however, the result shows much higher running time than our expectation because of the massive intermediate data which deplete the disk I/O performance and network speed. (b) shows the trade-off relationship between the total number of rounds and the size of shuffled data per round. put data, the memory space m of a reducer and the total available space M of a cluster is determined by the system. Besides, even in the same system, M changes from time to time. It means that R can vary on the same input data and the same system. Then our question is that how the performance of CTTP is affected by the number of rounds. We, first, set the simplified cost function of CTTP focus-ing on the number of rounds. After that, we analyze and compare the experimental result with the cost function. In the cost function, we suppose that the disk I/O speed and the network speed are not affected by the amount of pro-cessed data. The details of the cost function is described in Section 4 of [9].

A MapReduce job of CTTP consists of a map step, a shuf-fle step, and a reduce step. We let CostM (  X ,s ), CostS (  X ,s ), and CostR (  X ,s,K r ) be the cost of each step for a round where  X  and  X  are the numbers of mappers and reducers, respectively, s is the input data size of each step, and K the number of subproblems solved in the job. CostM (  X ,s ) contains the cost for starting  X  mappers and loading the input data into memory. It is formulated as follows: where D s is disk I/O speed. CostS (  X ,s ) is the cost for trans-ferring data between different machines. It is given by: where D r is the ratio of data transferred between different machines and N s is the network speed. CostR (  X ,s ) contains the cost for starting  X  reducers and loading the data trans-ferred from the shuffle step, and also contains the plugged in method X  X  original time complexity. It is given by: Figure 2: Average running time of each step. When R &lt; 4 , the running time of map and shuffle step is much longer than in the case of R  X  4 while the running time of reduce step increases linearly as ex-pected.
 Then, the total cost of CTTP with R rounds is given by:
For each round, the whole input data are loaded during the map step. The shuffle step ships (  X   X  1) E/R data be-tween different machines, and reducers receive (  X   X  1) E/R from the shuffle step. By applying Equations 2, 3 and 4 to Equation 5, we get the following cost: It indicates that the number of round affects only the factor CostM (  X ,E ) + startUpCost (  X  ). In order to measure the running time of the factor, we design a DoNothingJob whose mappers load the input data but does not emit any data, and reducers are explicitly executed but do nothing. The cost of the job is exactly the factor of additional cost. The running time of the DoNothingJob on Twitter dataset is measured about 45 seconds on average.

Now we are ready to explain the result in Figure 1. Fig-ure 1(a) shows the running time of CTTP with various round numbers from 1 to 62. If R &gt; 62, fewer subproblems than reducers are processed; in this case, some reducer does not work and only wait for other reducers. The dotted line in Figure 1(a) shows the expected result from the cost function of Equation 6. With R  X  4, the experimental result is al-most the same as expected. However, the result shows much higher running time than our expectation with R &lt; 4. The reason is that CTTP generates massive intermediate data in a round when R is small, so the data are overloaded to mappers in the map step and overburden the network in the shuffle step. Figure 2 explains the phenomenon well. When R &lt; 4, the running time of map and shuffle step is much longer than the case of R  X  4 while the running time of re-duce step increases linearly as expected. Note that Figure 2 shows the average running time of each step, thus the sum of the running time in this figure differs a little from the real running time in Figure 1(a).

Figure 1(b) shows the shuffled data size of CTTP with various total round numbers. As we mentioned in Section 4.1, there is a trade-off relationship between the total num-Figure 3: Data scalability of three algorithms. (a) shows the running time of CTTP, TTP and GP, and (b) shows the shuffled data size per round on ran-dom subgraphs with k edges of YahooWeb graph, varying k from 0.2 to 6.4 billion. Only CTTP suc-cessfully processed all datasets while GP and TTP failed to process datasets containing more than 1.6 billion edges. As the graph size increases, the shuf-fled data size of TTP and GP rapidly increases while that of CTTP increases linearly with the number of edges. The enormous shuffled data make TTP and GP fail due to the lack of space. ber of rounds and the shuffled data size per round. When R = 1, the shuffled data size is 985GB; it is almost 30 times larger than the input file size (32GB). If the shuffled data is generated enormously in a round, the system will fail due to the lack of space and heap memory. CTTP can easily avoid the system failure by increasing the number of rounds while it takes little more time.
Data scalability describes how the running time of an algo-rithm increases as data grow, and how large data can be pro-cessed by the algorithm. We compare and analyze CTTP to recent MapReduce triangulation algorithms, TTP and GP, in terms of data scalability. We run the three algorithms on random subgraphs with k edges of YahooWeb graph where k is the number of randomly selected edges from the original graph. For k , 6 . 4  X  10 9 , 3 . 2  X  10 9 , 1 . 6  X  10 9 and 2 . 0  X  10 8 are used.

The experimental results are depicted in Figure 3. It shows the running time of three algorithms and the shuf-fled data size per round on random subgraphs of YahooWeb graph. Only CTTP successfully processes all the subgraphs while GP and TTP fail to process datasets containing more than 1.6 billion edges. Figure 3(b) shows the reason of the failure well. As the graph size increases, the shuffled data size of TTP and GP rapidly increases, and they cause  X  X ut of space X  error when k &gt; 1 . 6  X  10 9 . On the contrary, CTTP generates small amount of intermediate data whose size in-creases linearly with the number of edges; thus, it is able to process much larger datasets compared to TTP and GP.
Machine scalability is the degree of performance improve-ment when the number of machines increases. In order to evaluate CTTP and competitors with regard to machine scalability, we run the algorithms on the Twitter datasets varying the number of mappers and reducers from 10 to 80. The experimental results are depicted in Figure 4. It shows the speedup factors of three algorithm on the Twitter dataset. The speedup factor is defined as t 20 /t r , where t Figure 4: Machine scalability of three algorithms on the Twitter dataset. The dotted diagonal line is the optimum. CTTP is closest to the optimum among three algorithms. The speedup factor is defined as t /t r , where t r is the running time of an algorithm with r reducers. (a) Figure 5: The running time of all algorithms on all real-world datasets. (a) shows the relative running time to CTTP and (b) is the list of the running time. CTTP outperforms the other algorithms in all datasets and shows more than 2x faster performance in SubDomain dataset compared to TTP. YahooWeb and ClueWeb09 datasets are not presented in (a) because GP and TTP failed on the datasets. the running time of an algorithm with r reducers. We set the reference point to the case of 20 reducers instead of 10 reducers since GP failed to run with 10 reducers because of the lack of space. The dotted diagonal line is the optimum and its slope is 1/20. The result shows that CTTP pro-vides the best performance, closest to the optimum among three algorithms. Furthermore, the result implies that the performance gap will increase as the number of machines increases.
Figure 5 presents the running time of all algorithms on all datasets. Figure 5(a) shows the relative running time to CTTP on four datasets where all algorithms run successfully, and Figure 5(b) shows the running time for every case. In CTTP, the number R of rounds is set to 1 on LiveJournal and PhoneCall datasets, and to  X  on the other datasets as we mentioned in Section 5.2. Only CTTP completed to enumerate all triangles in the YahooWeb and ClueWeb09 datasets; TTP and GP failed due to the lack of space. In this figure, we can see that CTTP outperforms the other algorithms on all datasets, and shows more than 2  X  faster performance with SubDomain dataset compared to TTP. Figure 6: The shuffled data size of all algorithms on real-world datasets. In CTTP, the round num-ber R is 1 on LiveJournal and PhoneCall datasets, and  X  on the other datasets as we mentioned in Sec-tion 5.2. CTTP generates the smallest amount of shuffled data per round. In contrast, TTP and GP generate a huge amount of shuffled data on Twit-ter dataset; and they failed to run on YahooWeb and ClueWeb09 datasets because of the enormous shuffled data.

Figure 6 shows the shuffled data size of all algorithms for real-world datasets. It indicates that TTP and GP generate a huge amount of shuffled data when the input graph is very large (e.g. Twitter and SubDomain graphs). The algorithms failed to run on YahooWeb and ClueWeb09 datasets because of the enormous intermediate data. The shuffled data sizes of CTTP on LiveJournal and PhoneCall dataset are similar with that of TTP because R is 1 on the datasets. On the other datasets, CTTP generates much smaller amount of the shuffled data compared to TTP and GP due to the effect of multiple round approach.
In this paper, we propose CTTP, a multi-round MapRe-duce randomized algorithm for triangle enumeration. CTTP requires O E 3 / 2 / ( M uses M/E words per mapper, m words in expectation per reducer, and M words as total aggregate space. More-over, the algorithm requires O E 3 / 2 total work in expec-tation, matching the best-known sequential algorithms. The claimed bounds are also shown to be optimal.

We further improve CTTP to get strong guarantees on the maximum load of each reducer with high probability. Specif-ically, we show that the aforementioned expected bounds on the space requirements and total work apply with probabil-ity at least 1  X  1 /E when m =  X  E 3 / 4 shows how to get rid with high probability of the  X  X urse of the last reducer X  [34], that is of an uneven distribution of the total work among the reducers.

Experiments show that CTTP outperforms the state of the art MapReduce algorithms on large real world graphs. Specifically, CTTP outperforms competitors by 2  X  on a phone call dataset with 30 million vertexes and 230 million edges; furthermore, CTTP enumerates 31 billion triangles of the ClueWeb09 graph with 4.8 billion vertexes and 7.9 billion edges in 23 hours, while competitors fail to run on the data. This work was partly supported by the IT R&amp;D program of MSIP/IITP. [10044970, Development of Core Technology for Human-like Self-taught Learning based on Symbolic Ap-proach]. Silvestri is supported by the University of Padova under Project CPDA121378, and by MIUR of Italy under project AMANDA; this work was done while Silvestri was visiting the IT University of Copenhagen. Pagh is sup-ported by the Danish National Research Foundation under the Sapere Aude program. [1] Foto N. Afrati, Anish Das Sarma, Semih Salihoglu, [2] Luca Becchetti, Paolo Boldi, Carlos Castillo, and [3] Jonathan Berry, Luke Fostvedt, Daniel Nordman, [4] Matteo Ceccarello and Francesco Silvestri.
 [5] Norishige Chiba and Takao Nishizeki. Arboricity and [6] Shumo Chu and James Cheng. Triangle listing in [7] Shumo Chu and James Cheng. Triangle listing in [8] Jonathan Cohen. Graph twiddling in a mapreduce [9] Robson Leonardo Ferreira Cordeiro, Caetano Traina [10] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: [11] Jean-Pierre Eckmann and Elisha Moses. Curvature of [12] Facebook. Facebook newsroom: [13] Michael T. Goodrich and Pawel Pszona.
 [14] Michael T. Goodrich, Nodari Sitchinava, and Qin [15] Ronen Gradwohl and Amir Yehudayoff. t-wise [16] Hadoop information. http://hadoop.apache.org/. [17] Xiaocheng Hu, Yufei Tao, and Chin-Wan Chung. [18] Zahra Jafargholi and Emanuele Viola. 3SUM, 3XOR, [19] Svante Janson. Large deviations for sums of partly [20] U Kang, Jay-Yoon Lee, Danai Koutra, and Christos [21] U Kang, B. Meeder, E. Papalexakis, and C. Faloutsos. [22] U Kang, Hanghang Tong, Jimeng Sun, Ching-Yung [23] U Kang, Charalampos E. Tsourakakis, and Faloutsos [24] Mihail N. Kolountzakis, Gary L. Miller, Richard Peng, [25] Haewoon Kwak, Changhyun Lee, Hosung Park, and [26] Silvio Lattanzi, Benjamin Moseley, Siddharth Suri, [27] Robert Meusel, Oliver Lehmberg, Christian Bizer, [28] Rasmus Pagh and Francesco Silvestri. The [29] Ha-Myung Park and Chin-Wan Chung. An efficient [30] Andrea Pietracaprina, Geppino Pucci, Matteo [31] Thomas Schank. Algorithmic aspects of [32] Francesco Silvestri. Subgraph enumeration in massive [33] SNAP. Stanford network analysis project, [34] Siddharth Suri and Sergei Vassilvitskii. Counting [35] Mikkel Thorup and Yin Zhang. Tabulation based [36] Jia Wang and James Cheng. Truss decomposition in [37] Duncan J Watts and Steven H Strogatz. Collective [38] ClueWeb09 Wiki. Clueweb12 web graph, [39] Virginia Vassilevska Williams and Ryan Williams. [40] Yahoo! Webscope -yahoo! labs, [41] Zhi Yang, Christo Wilson, Xiao Wang, Tingting Gao,
