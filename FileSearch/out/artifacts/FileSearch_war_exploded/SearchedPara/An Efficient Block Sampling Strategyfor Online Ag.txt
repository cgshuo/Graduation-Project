 Data-driven activities are rapidly growing in various applications, including Web access logs, sensor data, scientific data, etc. Distilling the meaning from these data has never been in such urgent demand. All these big data have brought in great challenges to traditional data management in terms of both data size and significance. As the growth of sheer volume of data, performing analysis and delivering exact result on big data can be extremely expensive, sometimes even impossible. For example, suppose one petabyte data are stored in the database and we want to find a particular record from the database. If indices have been built on the attribute we want to query, the answer will be returned soon. But if we do not build indices, how long will it take? The only way we can do without indices is full scan. Now for the fastest Solid State Drives (SSD) with disk scan-ning speed of about 6GB/s, a linear scan of SSD with one petabyte data will take  X  We implement our method in COLA, a system for Cloud Online Aggrega-rize the related work. We provide an overview of our problem in Section 3. In Section 4, we describe the stage of preprocessing. Our adaptive block sampling is presented in Section 5. In Section 6, we use COLA to implement our approach. Experimental results are presented in Section 7, followed by conclusions. In general, our work in this paper is related to two fields: online aggregation and data sampling. OLA was first introduced in RDBMS [ 1 ], which focuses on single-table queries involving  X  X roup by X  aggregations. The work in [ 2 ] improves theapproachin[ 1 ] by providing the large-sample and deterministic confidence interval computing methods in the case of single-table and multi-table queries. The query processing and estimate algorithms for OLA were studied in the context of joins over multi-tables [ 3 ]. A family of join algorithms called ripple joins were presented in [ 3 ]. Wu et al. [ 6 ] proposed a new OLA system called COSMOS to process multiple aggregation queries efficiently. All the work above is in the context of RDBMS. In fact, these centralized OLA methods or sys-tems can not be extended to MapReduce-based cloud systems straightforward. Hadoop Online Prototye (HOP) [ 8 ] is a modified version of the original MapRe-duce framework, which is proposed to construct a pipeline between Map and Reduce. COLA [ 11 , 28 ] realizes the estimation of confidence interval based on HOP. A Bayesian framework based approach is used to implement OLA over MapReduce [ 13 ] based on the open source project Hyracks [ 9 ]. The approach X  X  estimation method is complex, which is hard to be implemented in the MapRe-duce framework. In the work of [ 15 ], the authors focus on the optimization for running OLA over MapReduce-based cloud system.
 in the existing sampling techniques: row-level sampling [ 19 , 21 ] and block-level sampling [ 20 , 22 ]. Row-level sampling provides true uniform-randomness, which is the basis of many approximate algorithms. The work in [ 20 ] proposes statis-tical estimators with block-level samplings. Several special sampling approaches for group-by queries are also proposed. A method called Outlier Indexing [ 23 ] is proposed to improve sampling-based approximations for aggregation queries. Congressional sampling [ 24 ] stratifies the database by considering the set of queries involving all possible combinations of grouping columns and provides general-purpose synopses. Another approach to solve the small group problem has been developed by Babcock et.al. [ 25 ] and is called small group sampling. It generates multiple sample tables and selects an adequate subset of them at query evaluation time. Philipp et.al. [ 26 ] proposed a novel sampling scheme for con-structing memory-bounded group-aware sample synopses. All above sampling approaches for group-by queries are row-level sampling. In the context of online should be completed: randomization of data blocks and acquisition of necessary data information. The performance of online aggregation highly depends on the sampling, and the accuracy of sampling depends on the data distribution. If the data are fully random, the result will be good. Unfortunately, this is not always true for real world data. For tuple-level sampling, there are several ways to achieve random sampling from disk. If the data are distributed randomly, we just need sequential access. Otherwise, we must do random disk access to obtain random sampling. For block-level sampling, the case can be worse. We still can not get random sampling by random disk access because the data layout inside the block may be not random. Besides, completely random disk access can be five orders of magnitude slower than sequential access [ 27 ]. If we want to ensure samples from different groups can match the data distribution, some additional data information must be obtained. In this section, we introduce one approach to randomize the blocks and obtain the data information simultaneously. assumption in this paper. Here, static dataset does not mean we never update the dataset. In real-life applications, if the dataset is relatively static or update rarely, we can view it as a static dataset. The cost of preprocessing can be amor-tized by subsequent massive queries. This stage can be done by one MapReduce job and the basic procedure is as follows: by N. values within each column. The concept of column here is similar to the column of RDBMS. In fact, there is no column in the data block, but different values in one line which are seperated by some predefined characters can be viewed as different columns. Then Map tasks generate a random number between 1 and N, and assign the random number to the header of the key of current key-value pair. according to the header of the key. At the same time, reduce tasks merge the frequency of different values from all the map tasks. These frequencies and cor-responding location (i.e. the data block they belong to) will be stored on the master as data information.
 any MapReduce-based cloud system without any modification. Its performance will be demonstrated in the subsequent experiments. Unlike query processing in RDBMS, OLA must return the estimated result con-tinuously with accuracy guarantees. In this section, we will elaborate the query processing of OLA, which includes a new adaptive block sampling and the esti-mation of results.
  X  step5 : Repeat step 1 to 4 until users terminate the process actively. If users 5.2 Estimation and Confidence Interval particular group k , and set exp k = n k i =1 exp k ( t ij ). The estimation of aggrega-tion result is given by considered as the average of Y , where Y = N  X  exp k . The sampled data are retrieved in random order, so Y are identically distributed and independent to each other. According to the central limited theorem, the average of Y in samples obeys the normal distribution, so we can obtain the half-width interval with specified confidence level 100p%:  X  n = z p  X  n / in the standard normal distribution, and  X  n is the standard deviation of n k varibles in the sample. The final 100 p % confidence interval of the aggregation result is [  X   X   X  n , X  +  X  n ]. Cloud is different from RDBMS, and the major problem of online aggregation in the cloud is that naive MapReduce does not support pipeline operations. Several online aggregation systems have tackled the problem of pipeline, e.g., HOP, COLA. We finally choose COLA, which is described in the paper [ 11 ], rather than HOP, to implement our method. Our method is easier to be implemented on COLA because it supports more operations for online aggregation. The basic steps of online aggregation with our method is described as follows:  X  step1 : The user sends a query to the master and the master determines the  X  step2 : Read data from relative slaves and get the initial samples using our  X  step3 : Compute the estimation and confidence interval;  X  step4 : Output the result and continue sampling;  X  step5 : Repeat step 1 to 4 until users terminate the process actively. If users mentation only involves one MapReduce job. In the Map function shown in Algorithm 1, tuples of every block are filtered out according to the predicate and transformed into key-value pairs.
 for grouping. We generate about 20GB data and C contains 10 groups. Three of the ten groups are relatively small, and they are defined as  X  X malll group X  in this paper. Table 1 summarizes settings used in the experiments: Sum with following example queries Q.
 of estimated aggregation result is measured by relative error , which is computed by following equation: 7.2  X  ,  X  and  X  These three parameters are very important for our sampling. We want to make our samlping faster, so  X  can not be too large. At the same time, we need to ensure the accuary of the result, so  X  can not be too small. We set  X  to 10%, 20%, 30%, 40% and 50% respectively. For each  X  ,weset  X  to 20%, 30%, 40%, 50%, 60% and 70%, so we have thirty kinds of combination. We record the time when the relative error is no more than 5%. In order to choose the parameters accurately, one small group in C is used to show the result. Figure 2 illustrates the result. X-axis means  X  , Y-axis means  X  . The area of the bubbles in the figure represents the time. The smaller the bubble is, the shorter the time it takes. We 7.3 Effect of Data Randomization We compare the trends of relative error between original data and data after randomization. We only choose one large group from the ten groups in C to show the result due to limited space. Figure 3 shows that our method of data randomization really takes effect. We can get the accurate result earlier than the dataset without data randomization. 7.4 Query Error of Small Group Our method mainly focuses on the problem of small group, so we compare the trends of query error of the three small groups in our experiment. Because there is no work of block sampling for the problem of small group, we just compare our method with simple random sampling. S1, S2 and S3 are the results of small group 1, 2 and 3 by using our adaptive block samping. OS1, OS2 and OS3 are the results of small group 1, 2 and 3 by using simple random sampling. Group 1 is the samllest one of the three groups while group 3 is the largest. tively. From all these figures, our method can get high accuary with less time. We can also find that the samller the group is, the better our method performs.  X  X mall group X  is a very important problem for online aggregation. Uniform ran-dom sampling is not suitable for this problem. We propose a new adaptive block sampling to solve the problem of  X  X mall group X . This method can adjust the probability of acceptance dynamically and the experiments have shown its effieciency.

