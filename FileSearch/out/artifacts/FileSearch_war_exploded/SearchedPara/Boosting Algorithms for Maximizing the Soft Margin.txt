 Boosting methods have been used with great success in many ap plications like OCR, text classifi-[7] it was frequently observed that the generalization erro r of the combined hypotheses kept de-generalization error of ensemble methods was bounded by the sum of two terms: the fraction of training points which have a margin smaller than some value  X  plus a complexity term that depends most difficult examples. This idea is implemented in many alg orithms including AdaBoost with compared to the original AdaBoost algorithm on high noise da ta. AdaBoost generates a combined hypothesis with a large margi n, but not necessarily the maximum hard margin [15, 18]. This observation motivated the develo pment of many Boosting algorithms rithms have worse or no known convergence rates. However, su ch margin-maximizing algorithms even more problematic for such algorithms than for the origi nal AdaBoost algorithm [1, 8]. least the optimum soft margin minus  X  . BrownBoost [6] does not always optimize the soft margin. SmoothBoost and MadaBoost can be related to maximizing the s oft margin, but while they have the maximum soft margin. From a theoretical point of view the optimization problems underlying bounds [19].
 may help to increase the convergence speed: We will give exam ples where LPBoost converges much more slowly than our algorithm X  X inear versus logarithmic g rowth in N .
 problem. In Section 3 we discuss LPBoost and give a separable setting where N/ 2 iterations are SoftBoost algorithm and prove its iteration bound. We provi de an experimental comparison of the algorithms on real and synthetic data in Section 5, and concl ude with a discussion in Section 6. In the boosting setting, we are given a set of N labeled training examples ( x where the instances x the end of this section.
 One measure of the performance of a base hypothesis h with respect to distribution d is its edge,  X  just an affine transformation of the weighted error  X  maximum edge of the set.
 a convex combination of base hypotheses f added at iteration t and w  X  set.
 the labels y hypothesis becomes d u t and the margin of the n -th example w.r.t. a convex combination w of the first t  X  1 hypotheses is P t  X  1 now allow examples to lie below the margin but penalize them l inearly via slack variables  X  dual problem (2) minimizes the maximum edge when the distrib ution is capped with 1 / X  , where  X   X  { 1 ,...,N } : s.t.
 By duality,  X   X  long been exploited by the SVM community [3, 20] and has also b een used before for Boosting in Assumption on the weak learner We assume that for any distribution d  X  1 Adding a new constraint can only increase the value  X   X  therefore  X   X  hypothesis set from which the oracle can choose. Clearly  X   X  be lower than  X   X  (  X  ) and in that case the optimum soft margin we can achieve is g . programming problem (1) based on the t hypotheses received so far.
 The goal of the boosting algorithms is to produce a convex com bination of T hypotheses such that  X  Although the guarantee g is typically not known, it is upper bounded by b  X  and therefore LPBoost uses the more stringent stopping crit erion  X  of the optimizer, LPBoost can require  X ( N ) iterations: Theorem 1 There exists a case where LPBoost requires N/ 2 iterations to achieve a hard margin that is within  X  = . 99 of the optimum hard margin.
 Proof. Assume we are in the hard margin case (  X  = 1 ). The counterexample has N examples and Algorithm 1 LPBoost with accuracy param.  X  and capping parameter  X  precision parameter that is an arbitrary small number.
 Figure 1 shows the case where N = 8 and T = 5 , but it is trivial to generalize this example to any even N .
 There are 8 examples/rows and the five columns are the u  X  X  of the five available base hypotheses. The examples are separable because if we put half of the weight on the first and last hypothesis, then the margins of all examples are at least  X / 2 .
 We assume that in each iteration the oracle will return the re maining hypothesis with maximum  X  enough, then after N/ 2 iterations LPBoost is still at least .99 away from the optima l solution. Another attempt might be to modify LPBoost so that at each ite ration a base hypothesis is chosen Algorithm 2 SoftBoost with accuracy param.  X  and capping parameter  X  of [22]. SoftBoost takes as input a sequence of examples S = h ( x d t by minimizing the relative entropy  X ( d , d 0 ) := P in a feasibility problem for linear programming where the ed ges are upper bounded by b  X  we remove the relative entropy and minimize the upper bound o n the edges, then we arrive at the optimization problem of LPBoost, and logarithmic growth in the number of examples is no longer is put on the examples with low soft margin, which are the exam ples that are hard to classify. 4.1 Iteration bounds for SoftBoost only in the additional details related to capping.
 Theorem 2 SoftBoost terminates after at most  X  2 that is at most  X  below the optimum value g .
  X  of the simplex. Hence,  X   X  Let C b  X   X   X  . Notice that C 0 is the N dimensional probability simplex where the components are c apped Because adding a new hypothesis in iteration t results in an additional constraint and b  X  we have C the set C d C || d constraints on the optimization problem assure that d t u t  X  b  X  b  X  over the first T  X  1 iterations, we obtain Since the left side is at most ln( N/ X  ) , the bound of the theorem follows. When  X  = 1 , then capping is vacuous and the algorithm and its iteration bound coincides with the When  X  = N , then the distribution stays at d 0 and the iteration bound is zero. sidered algorithms. 2 We generated a synthetic data set by starting with a random ma trix of 2000 case, we flipped the sign of a random 10% of the data set. We then chose a random 500 examples as which has the largest edge with respect to the current distri bution on the examples. We have trained LPBoost and SoftBoost for different values o f  X  and recorded the generalization not easily separable, even when allowing  X  wrong predictions. Hence the algorithm may mistakenly labeled examples are likely to be identified as margin errors (  X  performance decays again. The generalization performance s of LPBoost and SoftBoost are very similar, which is expected as they both attempt to maximize t he soft-margin. Using the same data set, we analysed the convergence speed of several algorithms: LPBoost, Soft-Boost, BrownBoost, and SmoothBoost. We chose  X  = 10  X  2 and  X  = 200 . 3 For every iteration we record all margins and compute the soft margin objective ( 1) for optimally chosen  X  and  X   X  X . no theoretical evidence is known for this observed converge nce. Among the three remaining algo-rithms, LPBoost and SoftBoost converge in roughly the same n umber of iterations, but SoftBoost result. Finally, we present a small comparison on ten benchmark data sets derived from the UCI benchmark repository as previously used in [15]. We analyze the perfor mance of AdaBoost, LPBoost, Soft-Boost, BrownBoost [6] and AdaBoost Reg [15] using RBF networks as base learning algorithm. 5 This leads to 100 estimates of the generalization error for e ach method and data set. The means Boost and LPBoost are very similar. However, the soft margin algorithms outperform AdaBoost on most data sets. The genaralization error of BrownBoost lies between that of AdaBoost and Soft-Boost. AdaBoost algorithm.
 LPBoost i.t.o. generalization error.
 We prove by counterexample that LPBoost cannot have an O (ln N ) iteration bound. This counterex-ond, the iteration bound essentially says that column gener ation methods (of which LPBoost is a maximum hard margin. We believe that similar methods can be u sed to show that  X ( N/ X  ) examples may be needed to get  X  close. However the real challenge is to prove that LPBoost ma y require  X ( N/ X  2 ) examples to get  X  close.

