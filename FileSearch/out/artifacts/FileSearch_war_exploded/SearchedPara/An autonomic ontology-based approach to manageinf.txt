 1. Introduction
Data integration has emerged in recent decades as an interesting research challenge. This interest has been mainly motivated by the need for seamless access to data provided by heterogeneous information sources that are expected to work together (exchanging where a varietyof heterogeneous devices havingtheirownformats torepresentand exchange data are used together to provide users with services such as patients' telemonitoring or home-automation applications [3
Home Gateway) whose main purpose is to collect information from the different data sources. In general, in a typical home scenario all these devices are part of an architecture that usually includes two sites: the home site and a remote site (where an external manager (EM) is located) linked by a communication network (see Fig. 1 ). At the home site, data provided by the different devices and also sometimes user feedback are transferred and collected in the HG. Subsequently, the data collected in the HG is generally management tasks also need to be integrated in the HG. In the remote site, a server device designated as the Telemonitoring Server (TS) is commonly used to manage information provided and collected from all the HGs linked to it. In summary, in addition to providing a common understanding of the terminologies and concepts used, it is as important to provide the HG with common knowledge about how actions should be performed to collect, process and remotely access data. Although in recent years standards have been proposed to address these integration and interoperability challenges at different points of this communication scenario order to provide more than a common format to exchange data. A common knowledge to deal with both integrations is needed, thus relating all the sources that take part in the management process. Hence, the development of knowledge oriented frameworks is thus necessary to address this challenge and consequently provide effective and shareable services.

Such integration in the HG should be addressed from both theoretical and practical points of view [7]. First, from a theoretical point of view, a formal model is required in the HG to achieve a common understandable knowledge for all the sources that share and exchange data not only at the home site but also at the remote site. Hence, ontologies constitute the perfect theoretical approach to deal with data integration in the proposed scenario. They provide a knowledge oriented framework in which different data sources can be easily mapped and, in contrast to other data structure technologies, they provide mechanisms to reason upon data defined in the model. In fact, three popular environments where ontology-based approaches have been successfully applied to solve interoperability problems are clinical environments [7 network management applications [16,17] . The development of a specific tool to manipulate and map incoming data into the new model is required. Apart from developing the theoretical model, practical applications to integrate data are required [18,19] .
It should be noted that the HG element works in a dynamic environment where over time new devices can be added to or removed from it. In order to avoid continuous human intervention to manage applications supported by the HG, clearly some kind of self-management capabilities should be introduced into the HG (e.g. to perform updating tasks and provide services according to the configured scenario). Hence, this scenario seems to be an appropriate area for the application of some ideas based on the so-called autonomic computing paradigm [20] . According to this paradigm, an autonomic element implements an intelligent control loop that consists of collecting data, analyzing the data, determining a plan or sequence of actions in response to the analysis and finally executing the plan/sequenceof actions. Consequently, the combination of an ontology-based approach and the autonomic computing paradigm is proposed in this paper to deal with the data and management procedure integration challenge in home-based scenarios.
The idea proposed and implemented is to develop an ontology to clearly describe the managed data and explicitly describe in the data the representation of the management procedures, i.e. the tasks that comprise the control loop explained above. Furthermore, rules, managed elements are provided with a profile where all the tasks to be performed for the management procedure are clearly described and personalized management services can be provided. It is argued by the authors that the proposed approach constitutes a formal and generic solution that can be used not only to represent but also to manage any type of information in a home-based of this model, a specific case study using the proposed approach for technical management in home-based telemonitoringscenarios is paper thus addresses both the theoretical aspects and the practical applicability of the proposed approach.

The remainder of thepaper isstructuredasfollows: Section 2 provides background informationabout thetechnologiesinvolvedin the solution and related work is also discussed. Section 3 describes in detail the theoretical ontology model proposed. Section 4 Section 5 .
 2. Background and related work
Ontologies, rules and the autonomic computing paradigm are the main concepts involved in this work. The following subsections provide background information about these related topics and the reasons for selecting the technologies used in the ontology-based approach presented in this paper. Furthermore, related work is also discussed. 2.1. Ontologies in home-based scenarios
According to one of the most popular definitions in semantic web literature [21] , an ontology can be defined as a specification of a shared conceptualization  X  . In thelast decade ontologies have become popular torepresentknowledgeand they have also been successfully applied to achieve data integration in scenarios such as clinical or home-smart environments where heterogeneous sources work together. Regarding clinical scenarios, [7] is a good example. It presents a context management middleware architecture to support health information exchange and alerts handling for the supervision of patients with chronic conditions in telemonitoring scenarios. Other good examples of using ontologies for patient's remote supervision can be found in [9  X  runs successfully on a typical smart phone without consuming many resources. Regarding other home-smart applications, see for environment where service personalization is done by reasoning over the context expressed in an ontology model.
All of them are based on ontologies to achieve data integration thus describing clearly managed data in the scenario and involved entities. However, these approaches lack the provision of a clear methodology to describe and explain within the ontology the process. However, our ontology-based approach was designed to represent not only data modeling but also modeling of the management procedure execution. This constitutes one key difference in contrast to other ontology-based solutions presented in home-based scenarios [7,9,11,12,14] . 2.1.1. OWL to describe ontologies
While there are various languages for expressing ontologies, the standardized RDF (Resource Description Framework) schema and OWL (Ontology Web Language) have been gaining popularity in the semantic web [22,23] . OWL language is a vocabulary extension of RDF and describes the structure of a domain in terms of classes and properties and provides a set of axioms to assert assumptions or equivalence with respect to classes or properties. Specifically, the OWL-DL language (an OWL sublanguage) was class and property descriptions while guaranteeing total computational capacity were the main reasons for choosing this language proposed by the W3C-endorsed [23] . 2.2. Autonomic systems
As a consequence of growing communication networks and the complexity of required management tasks, IBM started in 2001 an alternative paradigm for system design and application called the autonomic computing paradigm. Inspired by the human autonomic nervous system, the goal of this paradigm is to achieve computing systems that can manage themselves in accordance with high-level guidance from humans [24,25] . This IBM solution is based on the division of actions that are taken when trying to provide autonomy to a process.

These autonomic systems are able to self-control their internal functions and operations thanks to the autonomic element which constitutes the main core of the autonomic system. The autonomic element implements a closed-control loop that dictates the work element would include a monitoring module, an analysis module, a planning module, an execution module and a knowledge module containing the necessary description of the domain that the autonomic system requires (MAPE-K). The control loop only handles known environmental conditions and is based on the knowledge module embedded in the element. This knowledge engine is supported by ontologies in many experiences. An autonomic system implements self-management capabilities by using the closed control loop described above to collect information, make decisions, and adapt and react if necessary [25] .

The idea of using ontologies to provide knowledge formalization in combination with the autonomic computing paradigm to organize processes has also been proposed in other works such as [13,27] for different application fields (personalized services in home environments or data warehouse improvement respectively). Nevertheless, in contrast to these approaches that use ontologies as a core knowledge describing the knowledge upon which the autonomic system is to work, the idea presented in this paper is to provide an ontology for describing both the data to be managed but also the tasks that make up the MAPE process. In this way, data integration is achieved but a clear explanation is also provided about how to process data providing an abstract view about the business model. This idea is in line with the work presented in [28,29] where a generic framework based on the autonomic loop and ontologies is proposed for self-management of ubiquitous M2M (machine to machine) networks. 2.3. Rules for dynamic knowledge
Ontologies are used to express knowledge in a static and declarative way as a set of things that are true. Generally, developed solutions combine knowledge presented in ontologies with dynamic knowledge presented by the use of rules [13,30] . A system based apply them. By using rules the behavior of individuals can be expressed inside a domain. For this reason, rule-based systems have been extensively used in applications that require personalized services [7,13,30] . 2.3.1. SPARQL language to de fi ne rules Most solutions that include rules are based on the use of SWRL (Semantic Web Rule Language) [31] in combination with OWL.
Nevertheless, there are other popular languages such as Rule-ML language [32] or DL Safe Rules [33] and choosing between them is conditioned by the characteristics of the ontology and the practical application.

The use of rules in combination with ontology instances is proposed in this paper to provide personalized management tasks. In this work, SPARQL (SPARQL Protocol and RDF Query Language) has been studied to express the rules to provide personalized services range of business rules using SPARQL expressiveness [36] . Furthermore, the inclusion of some SPARQL queries in the ontology model has been proposed in this work to express functions.

SPARQL is established as the standard query language for obtaining information from RDF graphs. It should be noted that there are many tools and frameworks to deal with them. Many RDF APIs and databases come with SPARQL support (such as Jena [37] ). From a of using a SPARQL query is by a structure of SELECT  X  FROM to be extended. SPARQL also provides the means to check whether certain conditions currently hold in an RDF model (ASK) or to derive new triples from existing triples (CONSTRUCT). In fact, SPARQL can be used as a rule language by combining CONSTRUCT clause and FILTER restrictions [35] . On the one hand, the CONSTRUCT query form returns an RDF graph. This graph is built based on those which the filter expression considers as TRUE. A SPARQL query FILTER can even be used to restrict on arithmetic expressions. the WHERE and FILTER clauses). In our work, these rules have been defined to personalize the execution of the tasks according to specific individual behaviors. In this way, these SPARQL queries turn to be IF 3. Theoretical approach 3.1. MAPE-inspired ontology structure
The developed ontology has been named HOTMES (Home Ontology for inTegrated Management in homE-based Scenarios). This ontology model is adopted as a unified common information model to provide two levels of seamless integration: data source to provide a generic framework to define what has been termed in this paper as a management profile .A management profile can be seen as a collection of tasks that should be performed in the HG to provide a management service. For example, in the case of a the HG (and even its own technical features) providing in this way personalized technical management for each specific HG. patient's disease characteristics, providing personalized clinical management. An overview of this ontology and these potential applications was introduced in [39] and its clinical application has been presented in [40] .

The design of the management profile has been inspired by the MAPE loop from the autonomic computing initiative. Thus, a management profile will be composed of 4 sections that define 4 different tasks: monitoring task, analysis task, planning task and execution task. By using this idea, the HG is provided with some level of autonomic behavior capability. Depending on the management profile content, a HG will be enabled to monitor its state or information provided by the different sources, analyze acquired information, react to abnormal findings and inform about changes in its contextual environment. Based on the statements domain, some initial questions were formulated. These competency questions had to be answered with the ontology model and served later to evaluate the design. This methodology was useful enough to define the HOTMES ontology. Later, within the case of was to consider reusing vocabularies from other ontologies. Some ontologies and standards were reviewed in order to reuse vocabularies and already defined conceptual schemas instead of having to reinvent all the ontology. In the same way as was proposed the overhead. By reusing existing vocabularies, the mapping task with other conceptual models is made easier.
 The HOTMES ontology has been written in OWL-DL using the Prot X g X -OWL v.4.0.2 ontology editor and checked using the FACT ++ reasoner [43] . The main components of this proposed ontology are presented in Fig. 3 . As can be seen from Fig. 3 ,the
HOTMES ontology has 5 root classes: the management profile class and 4 classes that describe the tasks that compose a management profile . Detailed information about each root class is provided in the following subsections.
 3.2. Rules
We propose SPARQL to define a set of rules that together with an instance of the management profile dictates what the agent needs to do in order to execute the closed loop (monitoring, analysis, planning and execution) and take some management decisions. These rules will not be generic. The EM will define a specific set of rules for each management profile instance. Each defined management profile will be composed of at least one instance of each type of task. Then, the relation among the tasks will be achieved by some conditions, expressed using SPARQL rules. Applying these rules, the agent will execute, for example, the appropriate analysis task result.

The SPARQL language has been chosen as it can be used to efficiently filter OWL individuals, compliment OWL with arithmetic of information at a certain specific moment during the closed loop performance. A set of examples of these rules is provided in the case study section.

Second, the use of SPARQL has also been studied to complement OWL expressivity by expressing arithmetic functions that will be included in the model. Describing functions and including them in the ontology model provides a clear explanation about how data should be processed during the analysis phase. In this way, the ontology provides all the information required to execute described actions and releases developers from the implementation of the described functions. Finally, the same SPARQL engine can be used to functions included in the ontology may be extended without having to modify the agent implementation and different personalized action management can be defined. 3.3. Data integration
To provide data integration, the ontology describes within the monitoring task structure of the information that is of interest for management purposes. Data provided by the different sources included in the managed scenario will be mapped into these classes study section later). It is interesting to note that the monitoring, analysis, planning and execution tasks defined in the ontology
Modeling a domain is not an easy task and different methodologies available in the literature that aim to explain how to build an ontology such as the METHONTOLOGY approach can be used [41,44 implementation and maintenance are general stages commonly included in these methodologies [45]. These were also followed in our approach. Then, as we did, evaluation should be considered along the whole design process and different levels of the ontology should be evaluated taking into account both quality attribute-based evaluation criteria and task-oriented approaches [47,48] . Different data model, standards and other external sources should be reviewed in order to provide a generalized view where all sources that are to be mapped could be represented. The selection of vocabulary and terminology (it should be closed to the domain described), the evaluation of taxonomy thoroughness and the correct usage of the ontology language are issues to be addressed too.

As an alternative to manual ontology engineering, automatic or semi-automatic approaches have been proposed for constructing ontologies from existing related documents in intranet resources or the WWW (world wide web) [49,50] . Ontology learning techniques aim to release developers from the consuming task of manual ontology engineering while making easier the maintaining of the ontology [51] . Most of the semi-automatic approaches that aims to ease the ontology development burden, combine user's knowledge by using forms in their methods [52] . Accuracy in defining ontologies becomes a more challenging task when the experts defining the ontology are not experts in the domain they are describing, thus needing collaborations and further refinements of the ontology. Nowadays, new approaches for the development of ontologies even facilitate different groups of experts the concurrent development of the same ontology making easier track changes and allowing ontology version management and conflict detection [53] . The HOTMES ontology was described manually by means of the Prot X g X  tool. It describes a new concept, what we termed in the paper as a management profile which comprises different management tasks and their relations. Because of these reasons (novelty of the concept and diversity in the tasks included), it was considered that describing the HOTMES manually would lead to a much more accurate definition. 3.4. Management procedure integration
Management procedure integration is undertaken by managing data according to the tasks described in the management profile (see Fig. 3 ). 3.4.1. Monitoring task class To perform a monitoring task generally means to collect information from a system, to test it and to be aware of its state [54].
Defining an instance of a monitoring task will allow control and observation of any changes that may occur over time in the state network management for collecting information:  X 
Polling task : A polling-based monitoring method consists of periodically polling devices for information retrieval.  X 
Event task : An event can be defined as something that happens during the course of a flow process. Therefore, all information provided in an asynchronous manner should be monitored by instances of the event task.

The following ontology properties (depicted in Table 1 ) have been defined for the Polling , Event and Monitoring task class. As is depicted in Table 1 , each monitoring task will be characterized by 3 main properties: the information that is being monitored, the monitoring schedule and the transmission policy. Details of this task have been shown in Fig. 4 .  X   X 
Monitored Data : This is a generic class that refers to the information that the agent should collect. When extending the monitoring task for each application, the new classes that describe the monitored resources will be added as subclasses of the
MonitoredData class.  X 
Transmission Policy : This class indicates if collected data should be transferred to a remote site (e.g. in real time) or not. 3.4.2. Analysis task class
An analysis task provides a set of processes and rules that enable to observe a situation in order to determine if something has changed or needs to be changed. The analysis task described in the proposed ontology explains how to reason with the monitoring retrieved at a certain time) and also describes a set of functions that will be used to manipulate information.
Some terms from OWL-S ontology have been borrowed to describe the analysis task ontology [55] . The OWL-S is an ontology for describing Semantic Web Services. It has three main parts that describe what the service does (service profile), how a client can initially proposed to describe semantic web services, its use has already been studied in other applications such as in network interact with a service. Similarly, in our ontology, the Process refers to the way the agent interacts with data (manipulates) and analyses it. Acquired data will be the input in the process and the result of the analysis will be determined as consequence of the output.Someoftermsandmodelsusedarerelatedtothe Atomic Process , Composite Process , Construct Component (Process Component) , If  X  Then  X  Else , Sequence , Perform , Condition and List classes described in OWL-S.

Considering the type of data that can be acquired in a home-based scenario, the analysis task has been subdivided into two only numerical data (integer and decimal data types) and simple data strings are expected to be received and then processed in the have been defined as follows:  X 
Syntactical Analysis : This detects specific tag values when comparing string data types in the analysis task.  X 
Mathematical Analysis : By defining an instance of the mathematical analysis task, the associated input data will be evaluated after being mathematically manipulated.

The main properties relating to the analysis task are defined in Table 2 . As can be seen in Table 2 ,the Analysis Schedule class dictates when the analysis task should be run. It can be configured as a day (including in the analysis all the data acquiredduringthedayuntil the set time).The Process class describes how theacquireddata (specified in the monitoring task) is to be manipulated in order to subsequently compare the acquired result with the criteria expressed through SPARQL. After processing acquired data according to the defined Process instance, these configured rules will be class is divided into two subclasses: atomic and composite process.  X  will do something and then will return an output. The main property of this class is hasFunction ( d: AtomicProcess
An Atomic Process will process inputdata accordingto a mathematical function ( Function class)toprovideanoutput valuethat will be evaluated according to a specified criterion.  X 
Composite Process : This is associated with actions that are required to be performed in multi-steps. Inspired by OWL-S, the ontology defines 3 types of executing process combinations ( ProcessComponent from OWL-S): Perform class, Sequence class and If
Then  X  Else class. OWL-S defines further types of ProcessComponent , thus if more complex execution processes are deemed necessary for performing an analysis task, the HOTMES ontology could be extended with such definitions.
 an instance of the Evaluation Criterion class. 3.4.2.1. Function class. This class indicates how information is to be mathematically processed. A total of six functions have been to express methods and function definitions. Nevertheless, this limitation can be compensated for by using some SPARQL queries the information is explicitly represented, some SPARQL queries that express arithmetic functions have been included in the model to indicate how the information is modified with each defined function. Therefore, the hasSparql-Xpathfunction property indicates for
For example, Fig. 6 corresponds to a SPARQL function associated with average and division. It indicates that the output value of a monitoring datum expressed as an operator (X or Y) will be processed as the select clause indicates. The last acquired datum of the correspondent type will be evaluated for real time executed operations and non-average or sum functions. See for example the average function. First, this SPARQL query indicates that the result variable of the function will contain an average of values  X   X  Select avg(?a) as ?result  X  . Then, restrictions are presented within the WHERE clause in order to apply the function type of individuals that are going to be processed with this function the output value property of individuals whose class name is the same as that pointed out by the operator of the function which will be included and then processed into this mathematical function.

In this way, it is clearly indicated how the agent should manipulate monitoring information included in the ontology when can be defined in the ontology without changing the code in the agent that is going to execute the task in the HG. The second output value of other mathematical processes executed previously, hence providing enhanced flexibility of the model. 3.4.3. Planning task class
A planning task can be defined as a detailed formulation of a sequence of tasks and activities for solving a sequential decision problem [57]. The output of the planning task will be a plan specified according to an application solution criterion. The Planning
Task model described in the ontology answers four questions: 1) What strategy are we going to follow for dealing with the detected analysis outcome? 2) Who is going to execute the plan? 3) What is the schedule for the actions of the plan? 4) Under what conditions will the plan be executed? Vocabularies and schemas used to describe the planning task object properties have been inspired by other ontologies and models that define the concept of planning [57 our ontology include terms defined in the ontology for formalizing the planning task presented in [57]. For example, the hasPreCondition property was defined in this ontology as constraints to activate a plan (the same as in the PLANET ontology [58]) and borrowed in ours. The concept of the PlanModel and Agent who executes the task was also borrowed from the first ontology.
These ontologies provide more details for describing a plan (e.g. temporal definition for the execution of actions) which were not required for our proposal. The main properties defined for the planning task are shown in Table 3 .

Pre-Conditions and Post-Conditions will be defined as instances of the Condition class. An instance of this class will be the agent together with the management profile . After performing an analysis task, the agent should execute the pre-condition and post-condition rules in order to execute the corresponding planning task if any is activated.
 3.4.4. Execution task class
An execution task can be defined as a set of actions performed by a specific agent [58]. In this model the Execution task refers to actions to be performed by the agent or the EM in order to react to a certain situation or solve a detected problem. Therefore the execution tasks will be used to comprise the planning task. Each execution task can be decomposed into small actions, thus each activity and optionally they can include some required parameters. For example, for the action of sending a mobile text message, a mobile number will be required as a parameter. Other possible actions would be to send an e-mail or to show a pop-up to the user. These will be configured as instances of the Action class. Table 4 shows the properties associated to the ActionList class. 3.5. Work fl ow procedure
The whole management workflow procedure will be as follows. First, an instance of the management profile will be configured by each particular management purpose. Then, once the management profile is in the HG, the management procedure will be performed know which information should be collected. These data will be transferred from different data sources (e.g. MDs, sensors, HG different sources included in the managed scenario will be mapped into the ontology model and thus transformed into ontology instances. In this way data integration is achieved.

Then, according to the analysis task description included in the management profile , these monitored data will be analyzed. This to the activation of a planning task. By applying the corresponding rules, the activation of planning tasks (described under the management profile ) will be performed and consequently the specified execution tasks that it comprises will be executed. By representing a unified view of how to monitor, analyze and evaluate, integration of the data management procedure is achieved. 4. Case study: technical management in home-based telemonitoring scenarios
Toshow theapplicabilityof our approach from a practical pointof view, it has beenspecifically applied tomanagetechnical data in remote supervision of the patient's health status and appropriate feedback provision is made possible [60] .

Technical management is an important task to be addressed in addition to clinical management. Guaranteeing that clinical data is telemonitoring scenario (MDs and HG) are working properly and to ensure that any technical problem that could prevent or compromise patient supervision is detected. Despite its importance, there is no standard proposal or unified common model to specifically manage devices used in telemonitoring scenarios in a remote manner where different MDs from different manufacturers a perfect case study as heterogeneous sources with different functionalities and program models can be found working together.
While a wide variety of studies are available in the literature regarding clinical management issues in this scenario [7,9 report concerns about technical management issues [61,62] . For example, in [61] a traffic aware model is proposed to monitor a mobile state used to gather (and later transfer) large amounts of data coming from sensors in WBAN (Wireless Body Area Networks). As another example, this problem is addressed in [62] by means of its integration into the SNMP (Simple Network Management
Protocol) architecture. On the other hand, there are a plethora of works based on ontologies for home smart environments which aim tomodel devices,sensors (medical or not) orequipment's and services for whichthey are used [13 are involved in the telemonitoring scenario to guarantee that the patient is supervised correctly.

The main purpose of this practical application was to study how the proposed model could be used to define a profile with a set of beendefined asa subclass of the management profile class, new classes under themonitoringclass have been included and a prototype as the Semantic Autonomic Agent. This technical management profile class will be associated to the element that is to be managed according to the profile, that is to say the HG in this case. 4.1. Technical data integration
The HOTMES ontology has been extended to describe all the information that may need to be managed in relation to the technical of these technical features can be controlled in both devices (HG and MD), such as the battery level (when the HG works on battery
For example, the NumberLinkedMD class which refers to thenumber of MDsthat are connected tothe HGwill be a monitored resource associated only to the HG.
 The terms used to describe these technical resources have been inspired by the X73 standard [6] and the MIB II (Management
Information Base) from the SNMP architecture [54,63] . Specifically, the system, interface and host-resource MIB groups were considered. This last MIB provides specific information about host features to be managed. On the one hand, the X73 family of standards is a reference frame for MD interoperability. It describes how information can be exchanged between personal MDs (X73 agents) and a concentrator device or gateway (where the X73 manager is installed) used to collect data transferred from different agents. TheX73 wasreviewed in orderto modeltechnical information thatcouldbemanaged from MDs. Onthe other hand, SNMPis a primitives to ease the communication between network devices and the EM and with structured data through MIB-module definition. The relations with both reference sources (X73 and SNMP) have been included in the ontology as annotations and have been gathered in Table 5 in the description column. Specifically, its mapping to the concepts included in the standard is shown. By relating concepts from the ontology to widely known standards, the nature of the terms is clearer, and the mapping task process for class ( Monitored Data subclass within the monitoring task section, Section 3.4.1).

Information used to describe the previous technical concepts (situated within the monitoring task) has been organized into two
An instance of these classes should be configured by the EM when defining the technical management profile . For example, regarding the classes displayed in Fig. 9 (memory resource), in order to monitor the memory used in the HG, the type of memory should be to a previous resource description. This data will be provided by the different sources involved in the management procedure.
Therefore, this second level describes all pieces of data eventually retrieved from the MDs and HG. In the example given above, the size of memory capacity of the device ( MemoryMaxCapacity class), the date, the time and the output value of the measurement, together with the storage units ( StorageUnits class), will characterize the memory datum related to the previous configured information.

In addition to the root classes that compose a technical management profile ,a Device class has been included in the ontology in ordertolinkthe technical management profile to the HG and provide a description of the devices presented in the scenario. As can be also be specified in the technical management profile . 4.2. Example of instance de fi nition
To illustrate the usage of the technical management profile and the instance task definition, a simple example is presented in detail. Let us assume that we want to monitor when the battery level of the thermometer is under the 15%. Table 6 gathers the list of steps followed to generate the individuals related to this management task and that in general should be followed to configure a task inside the management profile .

Following the steps depicted in Table 6 , first, a monitoring task (event type) associated to collect data regarding battery level absolute value of the battery level datum (function). The evaluation criterion associated to the analysis will indicate: ontology generated for the case of study section were developed by means of the Prot X g X  tool as well. It is our recommendation to are currently studying cognitive methods and usability testing techniques to develop this tool.
 4.3. Prototype implementation
The Semantic Autonomic Agent has been developed using Java technologies. Specifically, the Jena framework (version 2.8.6) has been used to process the ontology and create new instances for monitoring, data acquisition and manipulation when the SPARQL store the ontology model and the technical management profile instance in the HG [64] .

The architecture of the Semantic Autonomic Agent comprises the following modules: 1) the ontology knowledge base which contains the ontology knowledge model and the instance of its technical management profile , 2) the rule processor module which extracts and executes the appropriate rules during the management procedure (they will be transferred and stored all together by that provides the integrated management procedure (see Fig. 2 ). According to the workflow explained in Section 3.5 and considering there is any monitoring task (event type) associated to it and evaluates that type of data. This task is done in the MAPE module by in Fig. 2 ) using different communication protocols. For example, let's consider the case of an MD using the X73 standard reporting information about its battery level (see example from Section 4.2 ). This MD will establish a communication with its corresponding  X  data manager  X  (see Fig. 2 ) installed in the HG, it means the X73 manager. Then, communication should be established between this module and the semantic agent to notify about the incoming data. After checking the evaluation of the incoming data within the management profile , a new instance of the ontology will be created regarding the received data, and the tasks described in Table 7 would be executed within the MAPE module. 4.4. Evaluation and assessment applicability of our approach and detect and correct errors in consequence, the HOTMES technical ontology was evaluated by defining different tasks for a technical management profile to configure different management actions (simple, medium and complex). These of the Semantic Autonomic Agent prototype has been evaluated in terms of three common cost metrics: storage size, memory consumption and computational cost. The storage size refers to the database size used during the management procedure to hold the ontology and the instances. Memory consumption refers to both static and dynamic memory. Computational cost refers to the processing activity in the agent entity. 4.4.1. Evaluation of applicability of ontology: management actions execution tasks.

Table 7 shows representative configured actions used to evaluate the ontology and the agent management capabilities (from simple to complex actions). All these actions were included in a technical management profile and were run together for the agent evaluation performance. The number of required tasks and rules to configure each action has been included in the action description demonstrated that bydefininganinstanceof the management profile itwaspossible to configure theagent towarn the patientwhen it detects that a registered MD is not connected to the HG or, for example, when the battery level of a MD is under 15%.
In the simple action examples, only atomic processes are involved in the configured mathematical analysis. Furthermore, planning perform the analysis at a certain programmed hour. As shown in this example, the system would support temporal queries if an this task, an atomic process associated to an AVG function should be configured and the analysis schedule should be executed once per day or week (as desired).

Medium management actions require configuring more complex tasks. For example, to add several pre and post conditions to the planning task and composite process, (If then Else) is required to perform the last task. The number of tasks and rules are also increased. Finally, Table 8 includes complex managementtasksthat can be configured with the technical management profile ontology. (atomic or composite).

Note that during the initial tests, the problems emerged to configure these actions (and conflicts with rules) were solved by introducing additional properties in the ontology. After that, there were not any additional problems regarding the working mode of the agent related to the execution of the tasks to perform an action or conflicts between the rules.

The HOTMES ontology was extended to manage the technical information for the evaluation. The HOTMES technical ontology configure the actions is given. Regarding the schema evaluation proposed in the OntoQA framework [65] , the relationship richness was measured as 0.51, attribute richness was measured as 0.71 and the inheritance richness as 0.66. According to the mentioned framework these results lead to the assertion that the number of is high information is provided about each class and that the ontology represents a wide range of knowledge (actually it does: management tasks and managed data). The HOTMES technical ontology includes both cardinality (165) and value restrictions (owl: allValuesfrom,23), 158 disjoint axioms and enumeration axioms. Regarding instance metrics, it should be noted that new instances been measured in terms of the total annotations properties included in the ontology. The 53 detected annotations are rdfs:comments and 22 of them were used to clarify technical information and relate terms of the ontology with the X73 standard and the MIB from
SNMP. At most, when a class has a comment and just 35, 8% has been annotated. 4.4.2. Agent evaluation
A technical management profile containing all the action tasks set out in Table 6 was used to test the agent performance. For this evaluation the agent was run during short periods of high activity. Polling tasks were configured to retrieve data each minute and events were manually simulated in periods with irregular activity (5 min of active periods every 1 min and every 5 min). The agent ran on a 1.6 GHz i7 Intel Core running Windows 7 for the evaluation phase. MD and HG data were simulated using developed test-bench software.

Initially, the Semantic Autonomic Agent application downloads the OWL file into the TDB store whose initial size (without the should be included in the database. After 1 h of running the agent with periods of high and low activity, the maximum increase in memory was 54.000 kB (based on the initial 62.000 kB) and the maximum CPU observed was 1.57%.

The results show that this prototype implementation does not consume many resources when running the tasks specified in the technical management profile . Although lightweight ontology processing and also reasoning technologies usually consume significant memory resources (compared to other non-semantic tools used for similar purposes), the size of this ontology is not very large.
Moreover, selected technologies optimize the prototype implementation performance. Nevertheless, semantic tools need to be optimized in terms of resource consumption as device constraints and hardware resource limitations increase when working with small devices such as smartphones [66] . Our prototype shows a proof of concept leading us to conclude that the ontology-system could beused fortechnical managementtasksin telemonitoringscenarios.Future researchshould bedonein the implementation and evaluation of the prototype in a real scenario and to assess its adequacy when involving small and mobile devices. 5. Conclusions
The solution presented in this paper offers a generic ontology-based solution to integrate data and its management procedure in home-based scenarios. Hence, this approach could easily be used in other applications involving remote management tasks at home complex task since it involves not only clinical measurements but also the patient's context. It also requires the collaboration of physicians, and the assessment of its applicability needs to be undertaken taking into account various types of disease [40] .
The combination of ontologies and the autonomic computing paradigm is an interesting approach that takes advantage of the main benefits that both solutions offer in terms of knowledge representation, workflow organization and self-management capabilities. Moreover, the idea of defining management profiles in combination with SPARQL rules offers a flexible solution to personalize management tasks, which in fact is a key difference compared with similar solutions presented for home-based scenarios therefore able to provide an explicit representation of all the actions involved in the management procedure.

In addition to the theoretical approach, the proposed solution has been studied from a practical view point for managing technical data in telemonitoring scenarios. This is of interest given that few studies have been conducted into technical management in home-based telemonitoring scenarios. Most published solutions deal only with clinical data [7] . Furthermore, to the best of our constitutes an innovative management solution in the field of healthcare.
 Appendix Example: Evaluate a clinical measurement value defined in the profile (see Fig. A.1 ). It will be determined that a clinical measurement out of the range 32 temperature datum must be included in the profile. This analysis will be related to two conditions expressed by two SPARQL rules from Fig. A.1 ) to indicate that the execution tasks that compose the planning task should only be performed if the out-of-range measurement is detected 3 times. This planning task composed of two execution tasks (send an e-mail to the manager and show a pop-up to the patient) will be included in the technical management profile . Therefore, as depicted in Fig. A.1 ,aninstanceofthe technical management profile to run this test will be composed of 5 tasks and 4 rules will have to be defined.
References
