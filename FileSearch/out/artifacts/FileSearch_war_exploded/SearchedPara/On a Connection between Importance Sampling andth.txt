 Policy gradient methods have been some of the most effective learning algorithms for dynamic con-trol tasks in robotics. They have been applied to a variety of complex real-world reinforcement manoid robotic motion planning [2], and learning gaits for legged robots [3, 4, 5]. For such robotics tasks real-world trials are typically the most time consuming factor in the learning process. Making efficient use of limited experience is crucial for good performance.
 In this paper we describe a novel connection between likelihood ratio based policy gradient methods and importance sampling. Specifically, we show that the likelihood ratio policy gradient estimate estimated using only data from the current policy. This insight indicates that likelihood ratio policy gradients are quite naive in terms of data use, and suggests an opportunity for novel algorithms which use all past data more efficiently by working with the importance sampled expected return function directly.
 importance sampled expected return function, allowing us to make more progress for a given amount of experience. Our approach uses estimates of the importance sampling variance to constrain the search in a principled way. Second, we derive generalizations of optimal policy gradient baselines which are applicable to the importance sampled expected return function.
 Section 2 describes preliminaries on Markov decision processes (MDPs), policy gradient methods and importance sampling. Section 3 describes the novel connection between importance sampling and likelihood ratio policy gradients, and Section 4 examines our novel minimum variance baselines. Section 5 outlines our proposed method. Section 6 relates our method to prior work. Section 7 demonstrates the effectiveness of the proposed methods on standard reinforcement learning testbeds. Markov Decision Processes. A Markov decision process (MDP) is a tuple ( S,A,T,R,D, X ,H ) , A 7 X  R is the reward function; D is a distribution over states from which the initial state s 0 is drawn; 0 &lt;  X  &lt; 1 is the discount factor; and H is the horizon time of the MDP, so that the MDP terminates after H steps 1 . A policy  X  is a mapping from states S to a probability distribution over the set of actions A . We will consider policies parameterized by a vector  X   X  R n . We denote the expected return of a policy  X   X  by be the (discounted) sum of rewards accumulated along the state-action trajectory  X  .
 Likelihood Ratio Policy Gradient. Likelihood ratio policy gradient methods perform a (stochas-tic) gradient ascent over the policy parameter space  X  to find a local optimum of U (  X  ) . One well-known technique called REINFORCE [6, 7] expresses the gradient  X   X  U (  X  ) as follows: where the rightmost expression provides us an unbiased estimate of the policy gradient from  X   X  log P (  X  ( i ) ;  X  ) = P to compute an unbiased estimate of the policy gradient. REINFORCE has been shown to be moder-ately efficient in terms of number of samples used [6, 7].
  X  optimized to minimize variance) to the REINFORCE gradient estimate without biasing it [8, 9]. Past work often used a scalar b , resulting in:  X   X  U (  X  ) = E P (  X  ;  X  ) [  X   X  log P (  X  ;  X  )( R (  X  )  X  b )]  X   X  g = 1 Importance Sampling. For a general function f and a probability measure P , computing a quantity of interest of the form can be computationally challenging. The expectation is often approximated with a sample-based es-timate. However, samples from P could be difficult to obtain, or P might have very low probability where f takes its largest values. Importance sampling provides an alternative solution which uses samples from a different distribution Q . Given samples from Q , we can estimate the expectation w.r.t. P as: In the above, we assume Q ( x ) = 0  X  P ( x ) = 0 . Hence, one can sample from a different distri-bution Q and then simply re-weight the samples to obtain an unbiased estimate. This can be readily leveraged to estimate the expected return of a stochastic policy [10] as follows: the return of a policy  X   X  from sample paths obtained from acting according to a policy  X   X  0 . Evaluat-empirical distribution Q (  X  ) = 1 m P m j =1 P (  X  ;  X  ( j ) ) to enable use of all past data [10]. We now outline a novel connection between policy gradients and importance sampling. A set of we have: This analysis shows that the standard likelihood ratio policy gradient can be interpreted as forming an importance sampling based estimate of the expected return based on the runs under the current policy  X   X   X  and then using this estimate of the expected return function only to estimate a gradient b informed importance sampling based estimate that uses all past data.
 Instead of only using local information from a single policy to drive our learning, we can use global information provided by b U (  X  ) using trials run under all past policies. Such importance sampling based methods (as have been proposed in [10]) should be able to learn from fewer trial runs than the currently widely popular likelihood ratio based methods.
 Generalization to G(PO)MDP / Policy Gradient Theorem formulation. The observation that past rewards do not depend on future states or actions is leveraged by the G(PO)MDP [8] and the Policy Gradient Theorem [11] variations on REINFORCE to reduce the variance on their gradient estimates. This same observation can also be leveraged when estimating the expected return function itself. Let  X  1: t denote the state action sequence experienced from time 1 through time t , then we have For simplicity of notation we will continue to describe our approach in terms of the expression for U (  X  ) given in Equation (2.1), but our generalization of baselines, and our policy search algorithm are equally applicable when using the expression for U (  X  ) we present in Equation (3.2). Previous work has shown that the REINFORCE gradient estimate benefits greatly from the addition of an optimal baseline term [12, 9, 8]. In this section, we show that policy gradient baselines are special cases of a more general variance reduction technique. Our result generalizes policy gradient baselines in three ways: (i) It applies to estimating expectations of any random quantity, not just policy gradients; (ii) It allows for baseline matrices and higher-dimensional tensors, not just vec-tors; and (iii) It can be applied recursively to yield baseline terms for baselines since baselines are themselves expectations.
 Minimum Variance Unbiased Baselines. Given a random variable X  X  P  X  ( X ) , where P  X  b E g ( X ) = h ( X )  X  b T  X   X  logP  X  ( X ) is minimized. This variance is given by: term with respect to b equal to zero yields the minimum variance baseline The baselines commonly employed with REINFORCE, GPOMDP, and other likelihood ratio policy gradient methods can be derived as special cases of this generalized baseline [12]. Minimum Variance Unbiased Baselines with Importance Sampling. When using im-portance sampling with x ( i ) drawn from Q , we have an unbiased estimator of the form
Baselines. The minimum variance technique is naturally extended to vector-valued or matrix-valued random variables h ( X ) . For each entry in h ( X ) we can compute a minimum variance baseline vector b using Equation (4.1) or (4.2). In general, if h ( X ) is an n -dimensional tensor, we can stack these baseline vectors into a n + 1 -dimensional tensor. Indeed, in the case of REINFORCE we would obtain a baseline matrix, rather than a baseline scalar (as in the original work [7]) and rather than a vector baseline (as described in later work, such as [12]). The baselines themselves are estimated from sample data. Using standard policy gradient methods, it can be impractical to run enough trials to accurately fit such baselines. By using importance sampling to reuse data we can use richer baseline terms in our estimators.
 Recursive Baselines. The baselines are themselves composed of expectations. It is possible to reduce the variance on the baseline estimates. However, the number of baseline parameters being estimated increases rapidly in this recursive process. Moreover, if we estimate multiple expectations from the same set of samples, these estimates become correlated and the final result is no longer unbiased. In practice, these baselines can be regularized to match the amount of available data. In Section 8 we empirically investigate the performance of several different baseline schemes. We propose the algorithm outlined in Figure 1. It uses importance sampling with optimal generalized This estimator allows to search for a  X  which improves the expected return. It maintains a list of candidate policy parameters from which it searches for improvements. Memory-based search allows backtracking away from unpromising parts of the search space without taking additional, costly trials on the real platform.
 Estimate of Expected Returns: We use weighted importance sampling, and add a baseline to Equation (2.2): Optimal Baseline: Applying Equation (4.2) we get the following sample based estimate of the optimal baseline b for the estimate of the expected return function: 2
ESS Search Region: As our policy search steps away from areas of  X  where we have gathered sample data, the variance of our estimator b U increases and our function estimate becomes unreli-able. The effective sample size ESS = m 1+Var( w importance sampled estimate [13]. Here w i are the normalized importance weights and M is the number of trials. Our policy search only considers parameter values  X  with sufficiently high ESS. Step Direction: We use the finite-difference gradient of b U as the step direction for the inner loop of the policy search. In theory, since every outer iteration searches for a local optimum within the ESS region, the choice of step direction affects only the amount of computation and not the number of trials required for convergence. 3 Line Search: One issue with gradient based optimization methods is the need to choose the right step size. One solution is to use adaptive line search-based step size rules like the Armijo rule [15]. 4 For traditional likelihood ratio policy search methods this would require additional trials. By con-trast, no new trials are required when using importance sampling. 5 Various past approaches use the idea of constructing a model of the system from sample data, which can be used to search for the optimal policy, e.g., [16], [10], [17]. In contrast to Sutton X  X  DYNA, our method attempts to directly optimize the expected return function by varying policy parameters rather than building a model for the environment. Cao [17] also uses importance sampling to reuse past data for estimating policy gradients, but focuses on estimating local gradient information rather than global surface information. The work of Peshkin and Shelton [10] is most similar in spirit to our policy search method. They use importance sampling to construct a  X  X roxy X  environment from sampled data which can be used to evaluate the expected return at arbitrary policies. They apply a hill-climbing policy search to this  X  X roxy X  surface. This technique does not use estimates of the importance sampling variance to restrict the search, does not use generalized minimum variance baselines, and does not use memory. Our experiments show that these improvements are necessary to outperform standard policy gradient methods across our test domains.
 Our general approach of estimating and optimizing the expected return function instead of the gradi-ent of the expected return function allows for non-local policy steps. Recent EM-based policy search methods [18, 14] are able to make larger steps by optimizing a local lower bound on the expected return function. These methods can use importance sampling to make better use of data. This lower bound objective function and update step could be used in our memory based approach instead of following the finite difference gradient step.
 We explained throughout the paper the relationship with earlier methods such as REINFORCE [7, 6] and GPOMDP [8, 9]. PEGASUS [19] is an efficient alternative policy search method but can only be used if a simulation model is available.
 Recent work has suggested following the natural gradient direction [20, 21, 22]. The natural gradi-ent approach is a parameterization invariant second order method which finds the direction which maximize the ratio of the improvement of the objective function over the change in distribution over trajectories. Our approach exploits a similar intuition through consideration of variance through the effective sampling size (ESS) X  X referring regions for which the past experience gives a good estimate.
 Natural actor critic (NAC) approaches have enjoyed substantial success on real-life robotics tasks [1, 23]. In the episodic setting, which we consider in this paper, the only difference between episodic NAC and natural gradient is in the estimate of the baseline. Episodic NAC computes a scalar base-line by solving an LSTD-Q type regression rather than, e.g., using a minimum variance baseline criterion. 6 We present experiments on four testbeds: LQR, cartpole, mountaincar, and acrobot. The details of each experimental testbed can be found in the appendix. Though the systems are simulated, the learning algorithms cannot make use of the simulation dynamics except by gathering trials. For each testbed we randomly generated a pool of initial policies until one is found that does not achieve the worst case return We then used our policy gradient algorithms to optimize performance. The same set of initial policies is used across learning algorithms. We focus on an analysis of performance when only allowed for a small number of trials: In each of the following experiments we run 50 iterations of policy search, running M trials for each policy at each iteration. policy search algorithm. We then break down the effectiveness of each component of our algorithm: memory based search, optimal baselines, and ESS search region. Our policy search outperform likelihood ratio methods on two of the testbeds and performs equally well on the two remaining ones. Performance is reported as the expected return versus the number of sampled trials. The expected return is plotted on the y-axis. Error bars are shown based on running each instance with 10 initial policies. The number of trials is plotted on the x-axis.
 Generalized Baseline Experiments: There are a variety of choices in our generalized baseline technique: We can vary the dimensionality of the baseline terms to add, the depth of the recursive baseline, and what (if any) regularization to use.
 baseline, a matrix baseline, and a recursive tensor baseline on the matrix baseline. Figure 2 (a) shows the average reward received plotted against the number of trials run for the matrix (MAT) and recursive tensor (REC) baselines. The vector baseline was not able to improve the initial policies. The matrix baseline outperforms the other baselines and we use it going forward.
 Components of Our Approach: Figure 3 examines each of the central contributions of our al-gorithm (memory based search, baselines, and ESS). We tested our approach without any of the three components, which is equivalent to Peshkin and Shelton X  X  algorithm [10], which we label PS. We added each one of the three components individually, labeled PS+M, PS+B, PS+E. We also tested the performance with two out of three components, labeled OUR-M, OUR-B, and OUR-E respectively. Finally we tested the performance of our approach with all three components. The results indicate that each of the three components is improving performance with ESS and memory based being the most important components. Without any one of the components our approach has difficulty outperforming importance sampled GPOMDP.
 Comparison With Likelihood Ratio Policy Gradients: We have compared several episodic likeli-hood ratio algorithms against our global policy search algorithm. We run M = 10 trials per iteration, and repeat each trial 10 times. For the likelihood ratio algorithms, we use the appropriate optimal baselines [12] and hand-tune the step size. As a comparison, we have also implemented policy gra-dient algorithms which use importance sampling to estimate the gradient of b U . Figure 2 plots the reward received as a function of the number of real trials sampled from the system. We plot our global search approach against GPOMDP, an importance sampled GPOMDP (IS GPOMDP), and an implementation of Peshkin and Shelton X  X  global search. 7 Our approach is consistently able to improve its initial policy, outperforming likelihood ratio policy gradient methods on both the cart-pole and LQR testbeds. In general, importance sampling based methods outperform non-importance sampling based algorithms, which work poorly when given few trials. All algorithms in considera-tion performed poorly on the mountaincar and acrobot testbed X  X one of them showing significant improvement in performance through learning. We have shown that policy gradient methods are a special case of gradient descent over the im-portance sampled expected return function b U . Since our approach provides a full approximation of the expected return function, we can use global information in addition to gradient information to achieve faster learning. We have also shown that optimal baselines for standard policy gradient methods can be seen as special cases of a more general variance reduction technique. Our impor-tance sampling approach allows us to leverage more data to fit generalized baseline terms in our estimators. Our experiments show our algorithm requires fewer trials than current policy gradient methods on several testbeds and no more trials on the remaining testbeds, making it appealing for robotic learning tasks for which trials are expensive. Acknowledgments The authors thank Jan Peters and Hamid Reza Maei for insightful discussions and the anonymous reviewers for their feedback. This work was supported in part by NSF under award IIS-0931463. Jie Tang is supported by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program.
 (i) LQR: We use the formulation given in [21]. We use a linear parameterized policy with param-The initial state is drawn from x (0)  X  N (0 . 3 , 0 . 1) , and the dynamics are given by x ( t + 1) = Each episode was 20 time steps. (ii) Cartpole: This task consists of a cart moving along a track while balancing a pole. The goal pole upright. Following the formulation given in [24], our control input is drawn from the policy dynamics using a fourth order Runge-Kutta method. We run each episode for 200 time steps, though balanced and satisfies | x | &lt; 0 . 05 , and  X  1 otherwise. (iii) Mountain Car: The mountain car testbed [25] models a simulated car, which starts in a valley where t  X  N (0 , X  ) . Our initial acceleration is f 0 = +1 ; f t +1 = u t f t . The dynamics are given by  X  x t +1 =  X  x t + 0 . 001 f t  X  0 . 0025 cos(3( x t  X  0 . 5)) , and x t +1 = x t +  X  x t We run for 200 time steps, though the episode terminates once the mountaincar reaches its target at x = 1 . 0 . The reward function is 0 if the car is at its target and  X  1 otherwise. (iv) Acrobot: The acrobot [25] is a robot with 2 rotational links connected by an actuated motor. It be close to [  X , 0 , 0 , 0] (pointing straight up), and the goal is to keep the acrobot balanced upright for as long as possible. Our control input is drawn from the policy u  X  N ( Lx + K &gt;  X  ( x ) , X  ) . Here L is the optimal LQR controller for acrobot linearized around the stationary point, and  X  ( x ) = d m We solve the dynamics using a fourth order Runge-Kutta method. Each episode is run for 400 time steps, though the episode terminates once the acrobot has failed (defined as whenever the height of after the failure occurs, and  X  (1  X  (  X  cos (  X  1 )  X  cos (  X  1 +  X  2 )) / 2) 2 otherwise.
