 In unsupervised problems where observed data has sequential, recursive, spatial, relational, or other kinds of structure, we often employ statistical models with latent variables to tease apart the under-lying dependencies and induce meaningful semantic parts. Part-of-speech and grammar induction, word and phrase alignment for statistical machine translation in natural language processing are ex-amples of such aims. Generative models (graphical models, grammars, etc.) estimated via EM [6] are one of the primary tools for such tasks. The EM algorithm attempts to maximize the likeli-hood of the observed data marginalizing over the hidden variables. A pernicious problem with most models is that the data likelihood is not convex in the model parameters and EM can get stuck in local optima with very different latent variable posteriors. Another problem is that data likelihood may not guide the model towards the intended meaning for the latent variables, instead focusing on explaining irrelevant but common correlations in the data. Very indirect methods such as clever initialization and feature design (as well as ad-hoc procedural modifications) are often used to affect the posteriors of latent variables in a desired manner.
 By allowing to specify prior information directly about posteriors of hidden variables, we can help avoid these difficulties. A somewhat similar in spirit approach is evident in work on multivariate information bottleneck [8], where extra conditional independence assumptions between latent vari-ables can be imposed to control their  X  X eaning X . Similarly, in many semisupervised approaches, assumptions about smoothness or other properties of the posteriors are often used as regulariza-tion [18, 13, 4]. In [17], deterministic annealing was used to to explicitly control a particular feature of the posteriors of a grammar induction model. In this paper, we present an approach that effec-tively incorporates rich constraints on posterior distributions of a graphical model into a simple and efficient EM scheme. An important advantage of our approach is that the E-step remains tractable in a large class of problems even though incorporating the desired constraints directly into the model would make it intractable. We test our approach on synthetic clustering data as well as statistical word alignment and show that we can significantly improve the performance of simple, tractable models, as evaluated on hand-annotated alignments for two pairs of languages, by introducing intu-itive constraints such as limited fertility and the agreement of two models. Our method is attractive in its simplicity and efficiency and is competitive with more complex, intractable models. We are interested in estimating the parameters  X  of a model p  X  ( x , z ) over observed variables X taking values x  X  X  and latent variables Z taking values z  X  X  . We are often even more interested in the induced posterior distribution over the latent variables, p  X  ( z | x ) , as we ascribe domain-specific semantics to these variables. We typically represent p  X  ( x , z ) as a directed or undirected graphical model (although the discussion below also applies to context free grammars and other probabilistic models). We assume that computing the joint and the marginals is tractable and that clique potentials or conditional probability distributions.
 Given a sample S = { x 1 , . . . , x n } , EM maximizes the average log likelihood function L S (  X  ) via an auxiliary lower bound F ( q,  X  ) (cf. [14]): to 1 over z for each x . The lower bound above is a simple consequence of Jensen X  X  inequality for the log function. It can be shown that the lower bound can be made tight for a given value of  X  by maximizing over q and under mild continuity conditions on p  X  ( x , z ) , local maxima ( q  X  ,  X   X  ) of F ( q,  X  ) correspond to local maxima  X   X  of L S (  X  ) [14].
 Standard EM iteration performs coordinate ascent on F ( q,  X  ) as follows:
E : q t +1 ( z | x ) = arg max
M :  X  t +1 = arg max of the latent variables given the observed variables and current parameters. The M step uses q to  X  X ill in X  the values of latent variables z and estimate parameters  X  as if the data was complete. This step is particularly easy for exponential models, where  X  is a simple function of the (expected) sufficient statistics. This modular split into two intuitive and straightforward steps accounts for the vast popularity of EM. In the following, we build on this simple scheme while incorporating desired constraints on the posteriors over latent variables. 2.1 Constraining the posteriors Our goal is to allow for finer-level control over posteriors, bypassing the likelihood function. We propose an intuitive way to modify EM to accomplish this and discuss the implications of the new procedure below in terms of the objective it attempts to optimize. We can express our desired constraints on the posteriors as the requirement that p  X  ( z | x )  X  X  ( x ) . For example, in dependency grammar induction, constraining the average length of dependency attachments is desired [17]; in statistical word alignment, the constraint might involve the expected degree of each node in the alignment [3]. Instead of restricting p directly, which might not be feasible, we can penalize the distance of p to the constraint set Q . As it turns out, we can accomplish this by restricting q to be constrained to Q instead. This results in a very simple modification to the E step of EM, by constraining the set of q over which F ( q,  X  ) is optimized (M step is unchanged): Note that in variational EM, the set Q ( x ) is usually a simpler inner bound (as in mean field) or outer bound (as in loopy belief propagation) on the intractable original space of posteriors [9]. The situa-tion here is the opposite: we assume the original posterior space is tractable but we add constraints to enforce intended semantics not captured by the simple model. Of course to make this practical, the set Q ( x ) needs to be well-behaved. We assume that Q ( x ) is convex and non-empty for every x so that the problem in Eq. (5) becomes a strictly convex minimization over a non-empty convex set, guaranteed to have a unique minimizer [1]. A natural and general way to specify constraints on q is by bounding expectations of given functions: E q [ f ( x , z )]  X  b (equality can be achieved by adding E [  X  f ( x , z )]  X   X  b ). Stacking functions f () into a vector f () and constants b into a vector b , the minimization problem in Eq. (5) becomes: In the next section, we discuss how to solve this optimization problem (also called I-projection in information geometry), but before we move on, it is interesting to consider what this new procedure in Eq. (5) converges to. The new scheme alternately maximizes F ( q,  X  ) , but over a subspace of the original space of q , hence using a looser lower-bound than original EM. We are no longer guaranteed that the local maxima of the constrained problem are local maxima of the log-likelihood. However, we can characterize the objective maximized at local maxima as log-likelihood penalized by average KL divergence of posteriors from Q : Proposition 2.1 The local maxima of F ( q,  X  ) such that q ( z | x )  X  X  ( x ) ,  X  x  X  S are local maxima of where KL( Q ( x ) || p  X  ( z | x ) = min q ( z | x ))  X  X  ( x ) KL( q ( z | x ) || p  X  ( z | x )) . Proof: By adding and subtracting E S [ P z q ( z | x ) log p  X  ( z | x )] from F ( q,  X  ) , we get:
Since the first term does not depend on q , the second term is minimized by q  X  ( z | x ) = This proposition implies that our procedure trades off likelihood and distance to the desired posterior subspace (modulo getting stuck in local maxima) and provides an effective method of controlling the posteriors. 2.2 Computing I-projections onto Q ( x ) The KL-projection onto Q ( x ) in Eq. (6) is easily solved via the dual (cf. [5, 1]): given by q  X   X  ( z | x ) .
 Such projections become particularly efficient when we assume the constraint functions decom-pose the same way as the graphical model: f ( x , z ) = P  X  f ( x  X  , z  X  ) . Then q  X  ( z | x )  X  Q straint functions do not decompose over the model cliques but require additional cliques, the re-sulting q  X  will factorize over the union of the original cliques and the constraint function cliques, Figure 1: Synthetic data results. The dataset consists of 9 points drawn as dots and there are three clusters represented by ovals centered at their mean with dimensions proportional to their standard deviation. The EM algorithm clusters each column of points together, but if we introduce the con-straint that each column should have at least one of the clusters, we get the clustering to the right. Figure 2: An example of the output of HMM trained on 100k the EPPS data. Left: Baseline model. Middle: Substochastic constraints. Right Agreement constraints. potentially making inference more expensive. In our experiments, we used constraint functions that decompose with the original model. Note that even in this case, the graphical model p  X  ( x , z ) can not in general satisfy the expectation constraints for every setting of  X  and x . Instead, the constrained EM procedure is tuning  X  to the distribution of x to satisfy these constraints in expectation. The dual of the projection problem can be solved using a variety of optimization methods; perhaps the simplest of them is projected gradient (since  X  is non-negative, we need to simply truncate negative values as we perform gradient ascent). The gradient of the objective in Eq. (11) is given by: b  X  E q  X  [ f ( x , z )] = b  X  P  X  E q computing marginals of q  X  ( z | x ) , which is of the same complexity as computing marginals of p ( z | x ) if no new cliques are added by the constraint functions. In practice, we do not need to solve the dual to a very high precision in every round of EM, so several (about 5-10) gradient steps suffice. When the number of constraints is small, alternating projections are also a good option. A simple but common problem that employs EM is clustering a group of points using a mixture of Gaussians. In practice, the data points and Gaussian clusters have some meaning not captured by the model. For example, the data points could correspond to descriptors of image parts and the clusters could be  X  X ords X  used for later processing of the image. In that case, we often have special knowledge about the clusters that we expect to see that is difficult to express in the original model. For example, we might know that within each image two features that are of different scales should not be clustered together. As another example, we might know that each image has at least one copy of each cluster. Both of these constraints are easy to capture and implement in our framework. Let z ij = 1 represent the event that data point i is assigned to cluster j . If we want to ensure that data point i is not assigned to the same cluster as data point i 0 then we need to enforce the constraint to it from an instance I we need to enforce the constraint E P i  X  I z ij  X  1 ,  X  j . We implemented this constraint in a mixture of Gaussians clustering algorithm. Figure 1 compares clustering of synthetic data using unconstrained EM as well as our method with the constraint that each column of data points has at least one copy of each cluster in expectation. Statistical word alignment, used primarily for machine translation, is a task where the latent variables are intended to have a meaning: whether a word in one language translates into a word in another language in the context of the given sentence pair. The input to an alignment systems is a sentence aligned bilingual corpus, consisting of pairs of sentences in two languages. Figure 2 shows three machine-generated alignments of a sentence pair. The black dots represent the machine alignments and the shading represents the human annotation. Darkly shaded squares with a border represent a sure alignments that the system is required to produce while lightly shaded squares without a border represent possible alignments that the system is optionally allowed to produce.
 We denote one language the  X  X ource X  language and use s for its sentences and one language the  X  X arget X  language and use t for its sentences. It will also be useful to talk about an alignment for a particular sentence pair as a binary matrix z , with z ij = 1 representing  X  X ource word i generates target word j . X  The generative models we consider generate target word j from only one source word, and so an alignment is only valid from the point of view of the model when P i z ij = 1 , so we can equivalently represent an alignment as an array a of indices, with a j = i  X  z ij = 1 . Figure 2 shows three alignments performed by a baseline model as well as our two modifications. We see that the rare word  X  X onvivial X  acts as a garbage collector[2], aligning to words that do not have a simple translation in the target sentence. Both of the constraints we suggest repair this problem to different degrees. We now introduce the baseline models and the constraints we impose on them. 4.1 Baseline models We consider three models below: IBM Model 1, IBM Model 2 [3] and the HMM model proposed by [20]. The three models can be expressed as: 1 assumes that the positions of the words are not important and assigns uniform distortion prob-model assumes that the only the distance between the current and previous source word are impor-word to the source sentence. The likelihood of the corpus, marginalized over possible alignments is concave for Model 1, but not for the other models [3]. 4.1.1 Substochastic Constraints A common error for our baseline models is to use rare source words as garbage collectors [2]. The models align target words that do not match any of the source words to rare source words rather than to the null word. While this results in higher data likelihood, the resulting alignments are not desirable, since they cannot be interpreted as translations. Figure 2 shows an example. One might consider augmenting the models to disallow this, for example by restricting that the alignments are at most one-to-one. Unfortunately computing the normalization for such a model is a ] P complete problem [19]. Our approach is to instead constrain the posterior distribution over alignments during the E-step. More concretely we enforce the constraint E q [ z ij ]  X  1 . Another way of thinking of this constraint is that we require the expected fertility of each source word to be at most one. For our hand-aligned corpora Hansards [15] and EPPS [11, 10], the average fertility is around 1 and 1 . 2 , respectively, with standard deviation of 0 . 01 . We will see that these constraints improve alignment accuracy. 4.1.2 Agreement Constraints Another weakness of our baseline models is that they are asymmetric. Usually, a model is trained in each direction and then they are heuristically combined. [12] introduce an objective to train the two models concurrently and encourage them to agree. Unfortunately their objective leads to an intractable E-step and they are forced to use a heuristic approximation. In our framework, we can Table 1: Test Corpus statistics. Max and Avg. refer to sentence length. Fertility is the number of words that occur at least twice and have on average at least 1.5 sure alignment when they have any. Avg. F. is the average word fertility. All average fertilities have a standard deviation of 0.01. also enforce agreement in expectation without approximating. Denote one direction the  X  X orward X  direction and the other the  X  X ackward X  direction. Denote the forward model  X  X  X  p with hidden variables  X  X  X  z  X  0 . Define a mixture p ( z ) = 1 2  X  X  X  p ( z )+ 1 2  X  X  X  p ( z ) for z  X  in this setup are E q [ f ( x , z )] = 0 with We evaluated our augmented models on two corpora: the Hansards corpus [15] of English/French and the Europarl corpus [10] with EPPS annotation [11]. Table 1 presents some statistics for the two corpora. Notably, Hansards is a much easier corpus than EPPS. Hansards test sentences are on average only half as long as those of EPPS and only 21% of alignments in Hansards are sure and hence required compared with 69% for EPPS. Additionally, more words in EPPS are aligned to multiple words in the other language. Since our models cannot model this  X  X ertility X  we expect their performance to be worse on EPPS data. Despite these differences, the corpora are also similar in some ways. Both are alignments of a Romance language to English and the average distance of an alignment to the diagonal is around 2 for both corpora.
 The error metrics we use are precision, recall and alignment error rate (AER), which is a weighted combination of precision and recall. Although AER is the standard metric in word alignment is has been shown [7] that it has a weak correlation with the standard MT metric, Bleu, when the alignments are used in a phrase-based translation system. [7] suggest weighted F-Measure 1 as an alternative that correlates well with Bleu, so we also report precision and recall numbers. Following prior work [16], we initialize Model 1 translation table with uniform probabilities over word pairs that occur together in same sentence. Model 2 and Model HMM were initialized with the translation probabilities from Model 1 and with uniform distortion probabilities. All models were trained for 5 iterations. We used a maximum length cutoff for training sentences of 40. For the Hansards corpus this leaves 87.3% of the sentences, while for EPPS this leaves 74.5%. Following common practice, we included the unlabeled test and development data during training. We report results for the model with English as the  X  X ource X  language when using posterior decoding [12]. Figures 3 shows alignment results for the baselines models as well as the models with additional constraints. We show precision, recall and AER for the HMM model as well as precision and recall for Model 2. We note that both constraints improve all measures of performance for all dataset sizes, with most improvement for smaller dataset sizes.
 We performed additional experiments to verify that our model is not unfairly aided by the standard but arbitrary choice of 5 iterations of EM. Figure 4 shows AER and data likelihood as a function of the number of EM iterations. We see that the performance gap between the model with and without agreement constraints is preserved as the number of EM iterations increases. Note also that likelihood increases monotonically for all the models and that the baseline model always achieves higher likelihood as expected. Figure 3: Effect of posterior constraints on learning curves for IBM Model 2 and HMM. From left to right: Precision/Recall for IBM Model 2, Precision/Recall for HMM Model and AER for HMM Model. Top: Hansards Bottom: EPPS. Both types of constraints improve all accuracy measures across both datasets and models. In this paper we described a general and principled way to introduce prior knowledge to guide the EM algorithm. Intuitively, we can view our method as a way to exert flexible control during the execution of EM. More formally, our method can be viewed as a regularization of the expectations of the hidden variables during EM. Alternatively, it can be viewed as an augmentation of the EM objective function with KL divergence from a set of feasible models. We implemented our method on two different problems: probabilistic clustering using mixtures of Gaussians and statistical word alignment and tested it on synthetic and real data. We observed improved performance by introduc-ing simple and intuitive prior knowledge into the learning process. Our method is widely applicable to other problems where the EM algorithm is used but prior knowledge about the problem is hard to introduce directly into the model. Acknowledgments J. V. Grac  X a was supported by a fellowship from Fundac  X   X  ao para a Ci  X  encia e Tecnologia (SFRH/ BD/ 27528/ 2006). K. Ganchev was partially supported by NSF ITR EIA 0205448.
 [1] D. Bertsekas. Nonlinear Programming . Athena Scientific, Belmont, MA, 1999. [2] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J. Goldsmith, J. Hajic, R. L. Mercer, and [3] Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The math-[4] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. Semi-Supervised Learning . MIT Press, [5] I. Csiszar. I-divergence geometry of probability distributions and minimization problems. The [6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data [7] Alexander Fraser and Daniel Marcu. Measuring word alignment quality for statistical machine [8] Nir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate information [9] Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence K. Saul. An introduc-[10] Philipp Koehn. Europarl: A multilingual corpus for evaluation of machine translation, 2002. [11] P. Lambert, A.De Gispert, R. Banchs, and J. B. Mari  X  no. Guidelines for word alignment eval-[12] Percy Liang, Ben Taskar, and Dan Klein. Alignment by agreement. In Proc. HLT-NAACL , [13] Gideon S. Mann and Andrew McCallum. Simple, robust, scalable semi-supervised learning [14] R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justifies incremental, [15] Franz Josef Och and Hermann Ney. Improved statistical alignment models. In ACL , 2000. [16] Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment [17] Noah A. Smith and Jason Eisner. Annealing structural bias in multilingual weighted grammar [18] Martin Szummer and Tommi Jaakkola. Information regularization with partially labeled data. [19] L. G. Valiant. The complexity of computing the permanent. Theoretical Computer Science , [20] Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word alignment in statis-
