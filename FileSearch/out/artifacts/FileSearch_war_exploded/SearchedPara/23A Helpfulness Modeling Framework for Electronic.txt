 RICHONG ZHANG and THOMAS TRAN , University of Ottawa The Word-Of-Mouth (WOM) has long been shown to be an important source for con-sumers to make purchasing decisions. Due to the rapid development of Internet busi-ness, the number of electronic commerce Web sites has grown exponentially in recent years. Electronic word-of-mouth has emerged as a new source for consumers to refer to while studies have shown that eWOM affects consumer behaviors [Hu et al. 2006] and is helpful for decision-making on purchases [Park et al. 2007].

As mentioned in Schindler [2005], consumers seek for online eWOM as information input to specify purchase decisions and eWOM could help consumers decide whether a product should be purchased. According to a survey with the responses of 1,000 online shoppers [Palmer 2009], 47% of consumers surveyed look to onsite consumer reviews when making a purchasing decision. In another survey [Nielsen Online 2008], 71% of consumers agree that consumer reviews make them more comfortable with buying the right product. Furthermore, 81% of online holiday shoppers read online customer reviews according to Nielsen Online [2008].

One of the characteristic features of an electronic shopping environment is that there exists a vast amount of information available when making purchase decisions [Parra and Ruiz 2009]. User-Generated Contents (UGC) is more and more prevalent in most electronic commerce Web sites, like Amazon.com, that provide not only products and services, but also online reviews or eWOM messages, composed by previous purchasers. By taking advantage of the increasing availability of rich information, consumers en-joy a greater number of experiences than ever before. Still, the quality of the online reviews varies significantly. Especially with the explosive growth of the number of Web blogs, forum posts, and online reviews, potential consumers have to spend an immense amount of time retrieving reviews that assist them in better understanding products.
Considering the large amount of available online product reviews, too much infor-mation may overburden any consumer and due to the unavailability of the helpfulness of eWOM messages, consumers need to search across these contents for ones that may helpful in their purchasing decisions. In Duan et al. [2008], the authors pointed out that consumers need to find trustable experiences from all kinds of eWOM contents. Currently, many opinion sharing platforms provide a function to let readers vote for helpful or not about a review or opinion post. Potential consumers can make use of this information to estimate the helpfulness of the review and decide to read it or not. As the vote collection process takes time and newly posted reviews always get fewer votes, a system that can easily discover helpful online reviews is much needed to reduce the time of acquiring helpful reviews.

A number of helpfulness assessment approaches have been developed to model the helpfulness value of eWOM messages [Kim et al. 2006; Liu et al. 2008; Zhang and Varadarajan 2006]. These helpfulness assessment approaches utilize the positive vote fraction as the helpfulness benchmark and focus on applying different machine learning tools to learn a helpfulness function which is a mapping Score : D  X  R , where D is the space of all review documents and R represents the set of real numbers. The major limitation of conventional approaches is the utilization of the positive vote fraction-based benchmarking methodologies. Particularly when only a limited number of votes are available, the positive vote fraction is unable to significantly represent the true helpfulness of review documents.

Aware of such a limitation, most existing approaches solely attempt to analyze those eWOM messages for which a relatively large number of votes are available. However, even when a large number of voters X  opinions are available for each eWOM message, the same helpfulness value of positive vote fraction can come from different sizes of voter population and this fraction fails to capture the confidence of the helpfulness estimates. Furthermore, in reality, most online consumer reviews receive very few votes (see Figure 1 for the histogram of the number of votes on a set of review documents taken from Amazon.com). Since most online consumer reviews are only associated with a small number of voters, the applicability of existing approaches is significantly limited to a small part of available information.

In this article, we present a more robust formulation of review helpfulness with precise mathematical meaning, and a more principled probabilistic helpfulness assess-ment framework for inferring the helpfulness distribution of eWOM messages using the distribution of helpfulness conditioned on the feature of the document to charac-terize the dependency of the helpfulness and the review. Furthermore, we introduce a new quantitative helpfulness benchmark to capture the uncertainty of the helpful-ness estimates and a helpfulness bias formulation, based on which our model can rank user-generated reviews by the obtained helpfulness distribution.

Moreover, there are currently no standard evaluation metrics to judge the perfor-mance of review helpfulness assessment algorithms. In this article, we also discuss the evaluation aspects of the review helpfulness modeling framework and propose a uniformed evaluation measurement for assessing the success of helpfulness prediction models. Experiments on review datasets of two product categories from Amazon.com suggest that our algorithm in fact outperforms the previously reported prediction algo-rithms and show that the algorithm can effectively predict the helpfulness distribution of eWOM and recommend the most helpful contents to potential consumers.

The remainder of this article is organized as follows. Section 2 introduces our frame-work to estimate the helpfulness distribution of a review and how to evaluate the performance of a helpfulness assessing model. Section 3 delivers a graphical model-based approach under our framework. Section 4 presents experimental evaluation of our framework. Finally, Section 5 will discuss and conclude the article. An increasing number of literature illustrate the effect of online product reviews and eWOM on product sales and consumer X  X  behavior. Park et al. [2007] find that the quality of reviews has a positive effect on product sales as consumers purchase intentions in-crease with the quantity of product reviews. Hu et al. [2008] mentioned that consumers not only consider review ratings, but also contextual information like the reviewer X  X  reputation. They also find that the impact of online reviews on sales diminishes over time.

Park and Kim [2008] investigated the relationship between different types of reviews and consumers. They find that consumer concerns vary at each stage of the product life-cycle, and suggest marketers develop different strategies for different types of consumers. Lee et al. [2008] examined the effect of the quality of negative online reviews on product attitude and discover that high-involvement consumers consider the quality of negative reviews and low-involvement consumers tend to conform to other reviewer attitudes regardless of the quality.

In Vermeulen and Seegers [2009], authors studied the effect of online hotel reviews on consumer consideration and conclude that positive reviews have a positive impact on consumer behavior. In Park and Lee [2008], the authors analyzed the two roles of online consumer reviews (an informant and a recommender). When information over-load occurs, it suggests that low-involvement consumers mainly focus on perceived popularity and high-involvement consumers focus on the product information of re-views. Based on the findings of these researches, it can be confirmed that eWOM plays an important role in the purchase decision making process of consumers as consumers are willing to consider another person X  X  experiences before deciding to make a purchase.
Some works have been done in the area of review mining and summarizing. Zhuang et al. [2006] mined and summarized movie reviews based on a multi-knowledge ap-proach that included WordNet, statistical analysis, and movie knowledge. Hu and Liu [2004] summarized product reviews by mining opinion features. Mayzlin and Chevalier [2003] examined the effect of consumer reviews on relative book sales on Amazon and Barns and Noble Web sites. Hatzivassiloglou and McKeown [1997] proposed a method to predict the semantic orientation of adjectives by a supervised learning algorithm. Turney [2002] presented an unsupervised learning algorithm to classify reviews as recommended or not recommended by analyzing the semantic orientation based on mutual information. In Yu and Hatzivassiloglou [2003], the authors proposed a classi-fication approach to separate sentences as positive or negative. In Pang et al. [2002], the authors classified movie reviews as positive or negative by several machine learn-ing methods that were naive Bayes, maxium entropy, and support vector machines and they also used different features like unigram, bigram, position, and a combination of these features. The results showed that the unigram presence feature was the most effective and SVM performed the best for sentiment classification. These studies focus on sentiment classification and opinion mining for eWOM, however, only little research considers the usefulness of eWOM contents.

Kim et al. [2006] developed an SVM-based method to assess review helpfulness, where review lengths, unigrams, and product ratings are taken as the discriminating features. Weimer et al. [2007b] developed an algorithm to assess the quality of posts in Web forums using a variety of features including surface, lexical, and syntactic features, as well as some forum-specific features and certain similarity features. In a follow-up, Weimer et al. [2007a] extended the method into three datasets and found that the SVM classification performed the best.

Liu et al. [2008] presented a nonlinear regression model for the helpfulness predic-tion. Three groups of factors that might affect the value of helpfulness were analyzed and the model was built upon on these three groups of factors. The results of applying their model showed that the performance was better than the SVM regression model. Zhang and Varadarajan [2006] incorporated a diverse set of features to build a regres-sion model to predict the utility of online product reviews. As we discussed in Section 1, these modeling approaches lack principles and can only learn helpfulness from a small part of available eWOMs. Our study focuses on proposing a generalized framework for analyzing the helpfulness of eWOM and helping consumers find the most helpful ones more efficiently. In this section, we introduce a probabilistic framework for the inference of review helpfulness. Throughout this article, a random variable will be denoted by a bold-font capitalized letter, for example, V , D ; and any value the random variable may assume will be denoted by its corresponding lower-cased letter, for example, v , d . The distri-bution (either probability mass function or probability density function) of a random variable, say, X , will be denoted by p X . Following the aforementioned notational conven-tion, when p X is treated as a function, we also write as p X ( x ) when necessary, namely using the lower-cased letter  X  x  X  as the argument of the function.

We will often encounter a collection of random variables, say, { X k : k  X  K } for some index set K. In this case, we may write the set of random variables collectively as X K , and its corresponding (vector) value as x K .
 3.1.1. Probabilistic Formulation of Helpfulness. We will use D to denote the space of all eWOMs and use W to denote the population of readers who may vote on the documents in D . We argue that the  X  X elpfulness X  of any document d  X  D is not only a property of document d itself, but it in fact also depends on W . Specifically, a given document d may be more helpful with respect to one population W than it is with respect to another population W .

To rigorously define  X  X elpfulness, X  we characterize W as a pair ( R , P ), where R is a family of functions mapping D to { 0 , 1 } and P is a probability measure on R . Here, each function r  X  R specifies a rule of voting, whereby r ( d ) = 1 indicates that the document d will be voted positively (i.e., as HELPFUL ) under rule r . In this setting, we define the helpfulness of a document d , denoted by  X  ( d ), as the probability that document d will be voted positively by a random reader in W , namely, where R is drawn at random from probability space ( R , P ). Equivalently, the helpful-ness of document d can be characterized by the conditional probability distribution p | D ( v | d ), where D is a random document from D and V is the { 0 , 1 } -valued opinion of a random voter from ( R , P ). We are particularly concerned with the probability measure on D from which the random document D is chosen since D is always con-ditioned upon throughout the article. It is then clear that p V | D ( v = 1 | d ) =  X  ( d )and p | D ( v = 0 | d ) = 1  X   X  ( d ). 3.1.2. The Helpfulness Inference Problem. Under this formulation of helpfulness, the ob-on a sample of documents drawn from D and a collection of votes on these documents.
Let D I be a random set of documents drawn from D , where each element in I is the index of a document in D I . For each i  X  I ,let ( i )  X  W denote the set of readers who have voted on document i . Denote by : = X  i  X  I ( i ) the set of all voters that have voted on at least one document in I .

In the helpfulness inference problem, we wish to determine p V | D ( v | d )when D I is given as some d I and V is given as some v . 3.1.3. Probabilistic Formulation of Helpfulness inference. We now present a probabilistic framework of this problem. Essential to this framework is the creation of a model that describes the relationship between documents, their helpfulness, and voter opinion. Here, a model is a sensible family V | D of conditional distributions of V given D . The objective of helpfulness inference is to select a distribution p V | D from the family V | D such that p V | D where the latter equality is due to the fact that, given document i , voters opinion on the document is independent their opinions on any other documents.

Furthermore, we assume voters in each ( i ) are chosen independently, giving rise to
Applying our definition of helpfulness, when a document D is drawn from D (under any probability measure) and a random voter V is chosen from W under P , it is easy to see that random variables D , H ,and V form Markov chain D  X  H  X  V , where Specifically, where [ P ] (known as Iverson X  X  convention) for any Boolean proposition P evaluates to 1if P is true, and to 0 otherwise.

Putting together Eq. (1), Eq. (2), and Eq. (4), we have where the final equality follows from
We note Eq.(6) also implies that determining p  X  V | D ( v | d ) boils down to determining p and may induce the family V | D under Eq.(6). Then we reformulate the helpfulness inference problem as finding for some properly defined family H | D .

It is worth noting that H depends on D functionally (as given in (3)). The nature of the problem is that neither V nor P is known. This lack of knowledge necessarily implies that an appropriate dependency model of H on D is to assume that H depends on D probabilistically. This need is further amplified when a numerous amount of votes are not available for some documents. 3.1.4. Helpfulness Ranking. Once helpfulness distribution p H | D ( h i | d i ) in the aforemen-tioned framework is determined, a question which naturally arises is how to rank the document according to p H | D ( h i | d i ). We define the probability of a review being helpful is greater than 0.5 as a ranking metric.
We label this metric as helpfulness bias , which describes the helpfulness distribution quantitative metric to measure the helpfulness of eWOM.

Suppose the  X  X racle X  is accessible to the learning machine, the learning machine assumption we may make here is that without access to the  X  X racle X , p H ( h i ) is uniformly  X  X racle X  and it is clear that six different v ( i ) examples. It can be observed that with the same helpfulness value, the degree of certainty (the width of the distribution) is completely different. When a small number of voters are given, the distribution of helpfulness is highly uncertain. When large sufficient voters voted on a document, we expect the helpfulness distribution is narrow and highly concentrated on the value of positive vote fraction.

The helpfulness bias of the  X  X racle X  with the prior knowledge of voter opinion can be formulated as the integral of the posterior density over [0.5, 1], namely,
The helpfulness bias of the  X  X racle X  equals the area under the probability curve bounded by 0.5 and 1 (as indicated by the shaded area in Figure 2 which shows the helpfulness distribution in different circumstance). Figure 2(a) demonstrates the help-fulness values for eWOM that hold the same positive fraction of 3 5 . It can be observed that the helpfulness distributions of eWOM vary significantly although the positive vote fractions are same. Figure 2(b) demonstrates the helpfulness values for eWOM that hold the same positive fraction of 1. Conventional helpfulness formulations will fail to capture the difference between the helpfulness of those eWOMs and declare that they are equally helpful under this circumstance. In considering the problem of limitation of the positive vote fraction-based benchmarking methodology, we advocate the helpfulness bias of  X  X racle X  to serve as the learning target for machine learn-ing algorithms and regard it as a benchmark to compare against. With the proposed helpfulness bias as the ranking metric, the probability of the eWOM being helpful is identified and is ordered by this probability. 3.1.5. Feature as Sufficient Statistics for Helpfulness Inference. Since this function charac-terizing the dependency of helpfulness on an eWOM d i is difficult to determine in a learning framework, instead of considering helpfulness as depending on the eWOM functionally, we may consider helpfulness as depending on a set of extractable features of the review probabilistically .Let F denote the set of all features relevant to voter opinion. That is, there is a function  X  mapping D onto F such that for every d  X  D and domly drawn document D from D (under any probability measure), random variables D , F ,and H form a Markov chain, where F : =  X  ( D ) namely, given the feature  X  ( D )of D , the helpfulness of D is independent of the document D itself.

We can regard features as a sufficient statistic of the document, that is,
The objective of determining the helpfulness of the eWOM then can be formulated as finding In other words, we seek to determine p H | F from properly defined family of H | F which maximize the logarithm of p V | F ( v | f I ). 3.1.6. Evaluation of Model and Estimated Parameters. Under our proposed helpfulness framework, we can design different algorithms/models to find the probability distribution of voters X  opinions given a feature set. It is necessary to introduce a crite-rion to evaluate the goodness-of-fit for these algorithms/models. As defined in Eq.(1), the logarithm of the likelihood of observing all votes collectively given the features of all documents can be seen as how the model fits the data. Therefore, the objective of building a helpfulness model in our framework is to choose parameter values that achieve the best representation of the observed data. In other words, the criterion for choosing parameters from candidate distribution parameters is to find the one that best fits the training set.

For the  X  X racle X  learning machine, which has the prior knowledge of voters X  opinions, p the  X  X racle X  learning machine can be formulated as
Accordingly, the difference between the logarithm of p H | V ( h i | v ( i ) ) of our algorithm and the Oracle estimation indicates the fitness of the algorithm for eWOM documents.
The log-likelihood can also assist us to measure the goodness-of-fit for different mod-els. The log-likelihood function is the logarithm of the model formulation. In our case, is the resulting marginal probability distribution learned by an algorithm.
For the other helpfulness evaluation algorithms/models not under our probabilistic framework which generate a point estimate of helpfulness values, the likelihood of observing voters X  opinions given a set of estimated helpfulness value  X  i , i  X  I is The algorithm under our framework proposed in this article makes use of a graphical model to model the dependency between the random variables of our model and the Expectation Maximization (EM) algorithm to learn the parameters. Figure 3 provides the graphical representations of the eWOM helpfulness model. f I denotes the feature set of eWOMs, h I denotes the set of helpfulness, and v denotes the votes submitted by the readers. We suppose that each feature appears independently in an eWOM document. Then, h i can be calculated by the following linear summation model where the variances of the Gaussian noise term z  X  N (0 , X  2 ).

Let us denote the parameters A and  X  2 by  X  . Then the helpfulness distribution can be written in the form of p H | F ( h i | f i ;  X  ). Consider the generative process for an eWOM containing features f i , the probability density function of h i is
From the graphical model specified in Figure 3(b), it can be easily discovered that and
By the integration over the continuous hidden variable h I , we obtain the marginal distribution in the form of
Derived from Figure 3(a) and Figure 3(b), and taking the product of the marginal probabilities of the helpfulness of single document, we obtain the probability of observ-ing all documents X  helpfulness collectively given the features of all document and a set of parameters.
Given the helpfulness of h I , the distribution of v ( i ) is given by
Considering the conditional joint distribution of votes v and helpfulness h I ,wehave
Based on Eq. (17), Eq. (18), and Eq. (19), for a given value of parameters  X  and features f I , the distribution of v is then obtained by integrating over h I .
Our goal in adapting the feature vectors is to obtain a model of the helpfulness distribution of eWOMs. As can be seen from the previous discussion the optimization problem leads to the following maximization problem. We make use of Expectation Maximization(EM) algorithm [Dempster et al. 1977] to proposed model.

The following statements show how to do the maximization of estimating the param-eters.

E-Step: { q ( h i ): i  X  I } .
M-step:
In summary, the procedure for our EM algorithm consists of the following steps. (2) Calculate q t i ( h i ) by following Eq.(23) with respect to  X  t  X  1 . (3) Update q t i ( h i ) by Eq.(22). (4) Update  X  t by Eq.(24). (5) Set t = t + 1. If convergence condition is not satisfied, go to step 2.
Note that in the M-step of the algorithm, we fixed  X  for each iteration and then  X  can be deducted during calculating the maximum  X  . For an evaluation at algorithm level, we choose the most probable  X  for each eWOM, namely, to find a  X   X  to maximize the and iteratively go through this algorithm. As a result, 10 pairs of  X , A is learned by the EM algorithm.

As discussed in , the parameter  X  can be globally optimized by 3.1.6. This forces the width of the distribution to be the same for every document. Therefore, we choose the  X  and its corresponding A which maximize the probability of observing the train-ing data. When a new eWOM content arrives, the parameter  X  which maximizes content. Afterwards, the helpfulness bias of eWOM contents will be calculated and eWOM contents are ranked according to their corresponding helpfulness bias. This section shows the experimental results by applying our framework to real eWOM set, in form of online product reviews obtained from Amazon.com. Our experiments explicitly focus on the categories of electronic products and books. We crawled 1002 HDTV reviews, 1492 camera reviews, and 2408 book reviews from Amazon.com. 1 These reviews have been evaluated by at least 10 consumers as helpful or not helpful. We utilized the bag-of-word approach to build the language model. Each feature is a nonstop stemmed word and the value of this feature is a boolean value of the occurrence of the word on the review. We make use of the vocabulary of  X  X ord X  or  X  X erms X  as the feature set of the model to build a document-term matrix. Each the matrix has been normalized to zero-mean.

There are more than 15,000 identical terms in each of the eWOM corpus, so the term-document matrix of the document set is sparse and we need to reduce the doc-ument dimensions. After normalizing the document-term matrix, we performed Prin-cipal Component Analysis (PCA) [Jolliffe 2002] to reduce the dimension of the term-document matrix. Therefore the top 200 components, which dominate about 70% of the total variance, are selected in the latter on experimentation. We compute the principle eigenvectors of the covariance matrix for each training set and project the original document-term vector to a 200-dimensional space. To validate the ranking performance of our model, we compare our probabilistic model with Support Vector Regression (SVR) [Burges 1998], Artificial Neural Network (ANN) [Bishop 1996], and linear regression.
The ANN is one of the most popular machine learning algorithms to solve modeling and predicting problems. We implement a three-layer error Back-Propagation (BP) ANN in this study. The number of neurons in its hidden layer is chosen to be 10. Each node utilizes the sigmoid transfer function. The output node makes use of the log-sigmoid transfer function. The training progress is set to be stopped after 1000 iterations of learning, or when a present error value less than 0.001 is reached.
The SVR is also a popular machine learning algorithm successfully used for assess-ing review helpfulness [Kim et al. 2006; Zhang and Varadarajan 2006]. The SVR is applied in this study to compare the effectiveness with our probabilistic model. A lin-ear regression model has also been used for scoring the utility of reviews in Zhang and Varadarajan [2006]. Basically, linear regression is to find parameters which minimize the sum of the square deviations of all observed data. We applied a multiple linear regression model to predict the helpfulness of reviews and examined the ranking per-formance of this technique.

For all of these models, we use the PCA projected review data as the training data and test data. The learning target for these algorithms is the helpfulness bias of the  X  X racle. X  The helpfulness predictions from these models are evaluated by computing and comparing the Spearman X  X  rank correlation coefficient between the predicted rank-ings and the  X  X racle X  rankings.

In order to investigate the effectiveness of models towards eWOMs with different voter population sizes, we separate each category of documents into two groups ac-cording to the number of voters associated with each document. Group 2 contains all eMOMs and the corresponding voter opinions. We randomly choose 5 opinions for each eWOM message from group 2 to form group 1.

To obtain a fair comparison, we perform 10-fold cross-validation on our probabilistic model, Support Vector Regression (SVR), 2 ANN, and linear regression algorithm and compare the cross-validation performances. 5.2.1. Goodness-of-Fit Tests. Figure 4 demonstrates the goodness-of-fit of the  X  X racle X  (straight line) and our model (curve) with varying  X  from 0 . 1 to 1 with the training data. As described in our proposed EM algorithm, the  X  which maximizes the likelihood of the model and the corresponding parameter set  X  is chosen to be the final parameter to build our helpfulness model. We make use of a Gaussian distribution to model the relationship between h i and f i . With this Gaussian distribution, if a document is associated with a small number of votes, the  X  will be greater than the one with large run out of the range of (0,1). Therefore, in practice, one more consideration in choosing appropriate  X  is desired. The value of  X  can not be too big, so that most of the Gaussian distribution stays between (0,1). In addition, based on the knowledge of the training dataset (as illustrated in Figure 4),  X  cannot be too small to maximize the likelihood of our model. We examine our model with three categories of eWOM contents by varying  X  from 0.1 to 1 and find that a value of  X  = 0 . 2 is a satisfactory choice that meets these requirements and is used for the following experimentation.

Table I demonstrates the difference of the loglikelihood between the  X  X racle, X  our algorithm, SVR, linear regression, and ANN with the testing data. It can be seen that the goodness-of-fit of our model consistently outperforms SVR, ANN, and linear re-gression for reviews for all three categories and both groups of eWOM contents. This result indicates that our model fits the observed data better than other algorithms, no matter whether the available voter population for the training purpose is big or small. The log-likelihood of the prediction results may be not perfect for evaluating the effec-tiveness of nonprobabilistic algorithms. Therefore, we introduce the rank correlation coefficient to evaluate the ranking performance of different algorithms, probabilistic and nonprobabilistic. 5.2.2. Ranking Performance. In order to compare the obtained helpfulness bias from different learning algorithms with the helpfulness bias of  X  X racle, X  we evaluate the prediction accuracy of helpful rankings in terms of Spearman X  X  ranking correlation coefficient, which is wildly used for comparing two ordinal correlations between two random variables. It is defined by the following formula.
The ranking performance of different approaches is analyzed by 10-fold cross-validation. Table II shows the experimental results of our probabilistic model, SVR, ANN, and linear regression algorithm on HDTV, camera, and book reviews. The results demonstrate the effectiveness of our probabilistic model and indicate that our model consistently outperforms SVR, ANN, and linear regression for reviews with any voter population size.

For the reviews from all three review categories, compared with the other existing approaches, a significant improvement of the Spearman X  X  rank correlation coefficient is obtained by our model. In each group of data, the results show a close correlation be-tween the  X  X racle X  and our probabilistic model. For instance, the correlation coefficients of our model and the  X  X racle X  for each group of reviews are 0.62 and 0.65, respectively, for HDTV reviews, 0.47 and 0.49 for camera reviews, and 0.50 and 0.51 for book re-views. This result shows that our probabilistic model has a significant performance advantage over SVR, ANN, and linear regression.

Furthermore, as the rank correlation coefficients listed are averaged values of the 10 runs of experimentations, we perform a t-test to evaluate the difference between the 10 resulting ranking correlations by our algorithm and other algorithms X  ranking correlations of 10 runs. We find that the difference is significant at the 0.05 proba-bilistic level. These results strongly indicate that our model can consistently learn the helpfulness model under any voter population size and outperforms SVM regression, ANN, and linear regression.

As we expected, SVR, ANN, and linear regression with the reviews that are voted by more than 10 voters for all the two types of reviews perform better than the reviews voted by 5 voters. This is because that the confidence of the point prediction is low when voter population size is small. When more votes are available for an eWOM, the benchmark helpfulness value, which is used for training the point estimate model, is more certain and the ranking performance is improved by the increased certainty of data. By contrast, our probabilistic approach, that estimates the distribution of the voter opinion and estimate the helpfulness bias based on the estimated helpfulness distribution, is less likely to be affected by the shortage of voter opinions. Today X  X  online customers are impatient and demanding, while wanting to ensure the best purchasing decisions. Meanwhile, they are not willing to spend much time and effort for their purchases. The available eWOMs provide an immense amount of infor-mation that assists consumers in making purchasing decisions. Consumers are willing to read others consumers X  experiences and opinions. However, there is no formal format for the eWOM content available on the Internet. These contents are free-styled and consist of unstructured text and information. This fact makes consumers unable to find useful content easily as they have to spend more time surfing for helpful opinions and experiences.

The helpful eWOMs recommendation framework proposed in this article can surely assist to make purchasing decisions. An online store providing a review filtering system will definitely help potential consumers to find there interested product reviews and reduce the purchase decision making time. An online community that incorporates this review recommendation system can significantly increase usability and attract more users. One important and interesting extension of our model would be to integrate eWOMs from different sources or online communities. This would provide possible consumers more opportunities to benefit from other people X  X  experience.

The main contribution of this article is a novel probabilistic framework for modeling the helpfulness of eWOMs. This probabilistic framework provides a theoretical inter-pretation and a mathematical estimation technique to model the helpfulness distribu-tion of online eWOM. We have utilized the graphical modeling and EM algorithm to build a helpfulness assessment model under the proposed framework. Both mathemat-ical inference and the experimental results validate our framework. Our framework is clearly explained by the helpfulness inference and the experimental results show that a model under our framework can efficiently rank online reviews, as our algorithm outperforms the existing helpfulness assessment approaches by a clear margin.
Previous studies simply make use of the positive vote fraction as the benchmark to evaluate the helpfulness of eWOMs. As this benchmark hardly represents the true helpfulness value when only a few voters have voted on an eWOM, existing approaches only take ones which have been voted by a large number (such as &gt; 10) of readers. How-ever, in reality, most of eWOMs are only associated with a small number of available voter opinions and the votes need time to be collected. The available data for training the helpfulness model will be limited to a small amount of eWOMs.

Furthermore, the traditional helpfulness definition will treat the fraction of 9 10 , 18 20 , and 27 30 as the same helpfulness value, whereas the confidence of these helpfulness estimates varies a lot depending on voter population size. Our approach solves this problem by estimating the helpfulness distribution instead of a single value. We pro-posed a helpfulness bias function as  X  X enchmarking X  to justify the helpfulness of an eWOM document from available voter opinions and evaluate the true helpfulness value from the helpfulness distribution. The experimental results show that our framework can effectively model the helpfulness of eWOM, and it will help consumers reduce the processing effort in making purchase decisions.

Although in our algorithm we only take the unigram features and a Gaussian distri-bution into account to simulate the helpfulness distribution, our framework is capable of making use of any features and any other distributions. We envision two main future research directions that will possibly improve the efficiency of helpfulness modeling algorithms under our framework. First, we want examine the possibility of incorporat-ing other factors which may affect the quality of an eWOM document, to improve the precision of recommendations, for example, when the eWOM content is published, how consumers rate the product, and which emotional terms and product features are used. Second, we want to apply other distributions or other combinations of distributions to model the relations between the features and helpfulness value to improve the ability and accuracy of the helpfulness distribution prediction.

