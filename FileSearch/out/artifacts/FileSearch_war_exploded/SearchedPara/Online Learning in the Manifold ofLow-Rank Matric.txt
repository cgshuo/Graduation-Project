 model. Low-rank matrix models could therefore scale to hand le substantially many more features and classes than with full rank dense matrices.
 been commonly used. Sometimes, a matrix W  X  R n  X  m of rank k is represented as a product of two be applied by repeatedly taking a gradient step and projecti ng back to the manifold of low-rank large matrices and cannot be computed after every gradient s tep. We use second order retractions to develop LORETA  X  an online algorithm for learning low rank one, a case which is relevant to numerous online learning pro blems as we show below. measure among pairs of text documents, where the number of fe atures (text terms) representing on a factorized model, and also improves retrieval precisio n by 33% as compared with training a full rank model over pre-selected most informative feature s, using comparable memory footprint. memory required. These two experiments suggest that low-ra nk optimization could become very useful for learning in high-dimensional problems.
 this paper. An embedded manifold is a smooth subset of an ambient space R n . For instance the set { x : || x || dimensional space R n . Here we focus on the manifold of low-rank matrices, namely, the set of in
R n  X  m , which we denote M n,m k . Embedded manifolds inherit many properties from the ambie nt manifolds is simply the Euclidean metric restricted to the m anifold.
 to minimize a loss function L over the manifold of low-rank matrices M n,m M and has to be mapped back onto the manifold. The most common ma pping operation is the in
M . Unfortunately, the projection operation is very expensiv e to compute for the manifold of Riemannian gradient of a function on a manifold.
 Riemannian gradient and the tangent space Each point x in an embedded manifold M has a tangent space associated with it, denoted T compute the linear projection P Given a manifold M and a differentiable function L : M X  R , the Riemannian gradient  X  X  ( x ) of
L on M at a point x is a vector in the tangent space T x M . A very useful property of embedded on the manifold), the Riemannian gradient of f at point x is simply the linear projection P ordinary gradient of f onto the tangent space T performing constrained optimization.
 and step size  X  t . We now examine how x t + 1 2 can be mapped back onto the manifold. Retractions T computationally expensive. A major insight from the field of Riemannian manifold optimization is Formally, for a point x in an embedded manifold M , a retraction is any function R which satisfies the following two conditions [4]: (1) Center ing: R curve defined by  X  conditions: P ambient space onto the tangent space T of a second order approximation to the exponential mapping, which can be replaced by any other second-order retractions, when computing the projection i s too costly.
 the loss L at point x t  X  X  : (1) Gradient step: Compute x t + 1 2 = x t +  X  t , with  X  t =  X  X  ( x t ) = P is the ordinary gradient of L in the ambient space. (2) Retraction step: Compute x t +1 = R Based on the retractions described above, we now present an o nline algorithm for learning low-by a retraction to the manifold M n,m discusses the very common case where the online updates indu ce a gradient of rank r = 1 . representations as kept in memory (taking n  X  m float numbers to store). We intermix the two rank k is denoted R n  X  k 3.1 The general LORETA algorithm We start with a Lemma that gives a representation of the tange nt space T supplemental material.
 Lemma 1. Let x  X  X  n,m R respectively, such that A T to T x M = [ A A  X  ] M N Let  X   X  X  n,m  X  can be decomposed in a unique manner into three orthogonal co mponents:  X  =  X  S +  X  P where  X  S = AM B T ,  X  P given a rank-r gradient matrix Z , and want to compute a step on M n,m a first step we wish to find its projection P find the three matrices M , N projecting onto A  X  X  columns, denoted P and P N material.
 Theorem 1. Let x  X  X  n,m since we assume A and B are of full column rank). Let  X   X  T described above, and let The mapping R to We now have the ingredients necessary for a Riemannian stoch astic gradient descent algorithm. Algorithm 1 : Naive Riemannian stochastic gradient descent Given a gradient in the ambient space  X   X  X  ( x ) , we can calculate the matrices M , N Algorithm 1 explicitly computes and stores the orthogonal c omplement matrices A complexity, we use the fact that the matrices A they are orthogonal, the matrix A and likewise for B equal to I equals O ( n + m )( k + r ) 2 .
 Algorithm 2 : General Riemannian stochastic gradient descent Algorithm 3 , Loreta-1: Rank-one Riemannian stochastic gradient descent 3.2 LORETA with rank-one gradients q is ( W p  X  q ) p T , which is again a rank-one matrix. We now show how to reduce th e complexity of each iteration to be linear in the model rank k when the rank of the gradient matrix r is one. All other operations are O (max( n, m ) k ) at most. For r = 1 the outputs Z The memory requirement of Loreta-1 is about 4 nk (assuming m = n ), since it receives four input in-place while destroying previously computed terms. trace norm (sum of singular values) and semi-definite progra mming. See also [2]. to compute an SVD. Multi-class ranking with a large number of features was studied in [3]. based on a multi-label annotated set, using the imagenet dataset [16]. 2 5.1 Learning similarity on the 20 Newsgroups data set by example , and finding related content on the web.
 using a bilinear form S the data even if features are selected in a discriminative wa y.
 model with 50 K features, and a rank-50 model with 10 K features.
 l
W ( q , p 1 , p 2 ) = [1  X  S W ( q , p 1 ) + S W ( q , p 2 )] + Data preprocessing and feature selection. We used the 20 newsgroups data set (peo-each. We removed stop words but did not apply stemming. We sel ected features that conveyed high words. Two documents were considered similar if they shared the same class label. Experimental procedure and evaluation protocol. The 20 newsgroups site proposes a split of at the top r ranked documents. We further compute the mean average precision (mAP), a widely used measure in the information retrieval community, which averages over all values of r . Comparisons. We compared Loreta with the following approaches. (1) A direct gradient descent gradient descent steps are computed over the factors A and B , for the same loss used by Loreta l the loss l A is computed to be  X  is a predefined parameter controlling the maximum magnitude of the step size. This procedure is numerically more stable because of the normalization by the norms of the matrices multiplied by (4) Full rank similarity learning models. We compared with two online metric learning methods, LEGO [20] and OASIS [8]. Both algorithms learn a full (non-fa ctorized) model, and were run with with batch approaches such as [13] Figure 2b shows the mean average precision obtained with the three measures. Loreta outperforms of the data into low dimensional space, which is tailored to t he pairwise similarity task. 5.2 Image multilabel ranking a large number of classes ( L = 1661) with multiple labels per image.
 implies that these sub-models are linear combinations of a s maller number of latent models. Online learning of label rankings with Loreta-1. At each iteration, an image p is sampled, and compared with the ground truth labeling y = { y multiclass hinge loss as follows. Let  X  y = argmax the highest score, where ( W p ) L ( W, p , y ) = P r i =1 [( W p )  X  y  X  ( W p ) y Loreta: for the set of indices i is in which case it is 0 . . (www.imagenet.org/challenges/LSVRC/2010/) containing images labeled with respect to the Word-mean average precision (mAP) criterion mentioned above.
 Comparisons. We compared the performance of Loreta on this task with three other approaches: conservative gradient descent (3) Group Multi-Class Perceptron a mixed (2,1) norm online mirror significantly improves over all other methods across all ran ks. We presented Loreta, an algorithm which learns a low-rank ma trix based on stochastic Riemannian annotation, where it scales well with the number of classes.
 for exploring high dimensional data, or extract relations b etween large number of classes. Acknowledgments This work was supported by the Israel Science Foundation (IS F) and by the European Union under the DIRAC integrated project IST-027787. [2] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and app lications in system theory. In [6] B. Vandereycken and S. Vandewalle. A Riemannian optimiz ation approach for computing low-[9] C.D. Meyer. Generalized inversion of modified matrices. SIAM Journal on Applied Mathe-[10] M. Journee, F. Bach, PA Absil, and R. Sepulchre. Low-Ran k Optimization on the Cone of [12] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Gua ranteed minimum-rank solutions of [21] Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for
