
DCC-FCUP, Universidade do Porto, LIAAD, INESC Porto L.A., Porto, Portugal Departamento de Inform  X  atica, Universidade do Minho, Braga, Portugal 1. Introduction
Association rules are an important technique for descriptive data mining. Especially, when the aim is to unveil properties of the data that can be potentially actionable. Fundamentally, association rules are concerned with sets of items or discrete attributes. The manipulation of numerical attributes in association rule discovery is not trivial. Numerical values cannot be directly treated as categorical at the cost of wasting generalization ability. On the other hand, pre-discretization forces early and sub-optimal decisions. The research literature has documented some non pre-discretization solutions for numerical values in the consequent of rules. For example, the target attribute can be summarized by showing its mean or median value, as it happens in the Quantitative Association Rules approach [2], or it can be graphically represented by its whole distribution, as it is the case of Distribution Rules [11]. Other approaches try to discover interval conditions on numerical attributes using association rule speci fi c pre-discretization [25], clustering [6,14,18], rule templates [28], speci fi c interest measures [12,13], bottom-up agglomeration of interval conditions [3], and genetic algorithms [24,27]. Some approaches are capableof discoveringcon fi denceoptimal rules for fi xedsupport and vice-versa [9,22] underprovided rule templates. This paper describes an optimal approach for dealing with numerical attributes in the consequent of the rules. This approach avoids pre-discretization. The number of attributes in the antecedent is arbitrary. We exploit the path of determining, for each rule, optimal in terval conditions on the numerical attribute in the consequent. All the other attributes in the antecedent are assumed to be categorical or previously discretized. It is not a concern of this paper the discretization of the variables in the antecedent. In particular, we look for conditions which are optimal under a well known notion of association rule interest: leverage , also known as Piatetsky-Shapiro measure [20]. Another related measure, added value [23], is also optimized. The rules formed with these optimal intervals are called  X  X aximal Leverage Numerical Rules X , or  X  X aximal Leverage Rules X  for short. This work develops from our previous work on Distribution Rules [11]. There, a rule was objectively considered conditions in the antecedent, when compared with the overall distribution. This was measured using a Kolmogorov-Smirnov statistical test to determine whether the two distributions were likely to be the same or not. What we observe is that an interesting distr ibution rule under the Kolmogorov-Smirnov setting may become an interesting rule with interval conditions in the consequent under the optimal leverage setting. Moreover, we can assure that we do not overlook any such interval rule. What we propose is the other hand, in this paper we are not so much concerned with algorithmic issues of rule discovery but rather with theoretical interestingness properties. We use the previously existing implementation for discovering distribution rules from data. The interval rules with maximal leverage are obtained by post processing each of the resu lting distributio n rules. Our clai ms are theoretically justi fi ed and illustrated with examples on well known datasets. In this paper we start by presenting an overview of interval discovery for association rule mining. We then present Maximal Leverage Rules and describe how they can be obtained from distribution rules. We theoretically demonstrate the optimality of the proposed methods and then describe techniques for im proving the readability of rules. In the following, completeness and complexity of the MLR generation are analyzed and some experiments are performed on regression data sets. 2. Related work
The simplest way to deal with numerical attributes in association rules is by employing some kind of pre-discretization, such as equal-widh or equal-depth. However, generic discretization techniques are not necessarily the most adequate to satisfy the aims of association rule discovery. When we discretize, we should know the impact in the resulting rules. We can also discretize so that optimal rules are found under well de fi ned criteria.

Early discretization approaches for association rule discovery were focused on the aim of fi nding all rules with support and con fi dence above given values. Under this framework, Srikant and Agrawal [25] proposed a principled pre-discretization of numerical attributes. Since the aggregation of values makes the rules less precise but more general, the trade-off is settled by asking the user how much precision one is willing to lose. This parameter, which is user provided, determines the width of the smallest intervals. This discretization approach fi ts into the support-con fi dence association rule discovery framework. Miller and Yang [18] tried to overcome some limita tions of Srikant and Agrawal X  X  proposal by taking into account the distribution of the values and their distance. Their proposal involved clustering the values of quantitative attributes. Their discovery framework was distance-based rather than based on support and con fi dence. Miller and Yang describe an algorithm for mining such rules but they do not provide any completeness property stating that this algorithm fi nds all interesting rules under their notion of interest. Lent et al. [14] also proposed clustering for generating rules of the form X 1  X  Int 1  X  X 2  X  Int 2  X  Class = K . Attributes X 1 and X 2 are numerical, Int i are intervals and Class is categorical. Their approach starts by binning the numerical attributes into equi-width intervals, after which association rules of the above form are generated for the base intervals (bins). Then, given a fi xed K , rules are clustered together by merging neighboring intervals. The clustering is guided by a simpli fi ed Minimum Description Length measure, which simultaneously tries to minimize the number of rules and the number of cases that are not covered by any rule. The resulting algorithm is shown to be for ef fi ciency is the heuristic search approach used to fi nd the clusters.

Wang et al. [28] use a bottom-up interval merging algorithm for fi nding rules. Rule generation is governed by pre-de fi ned templates which allow the use of quantitative and qualitative attributes. The merging of intervals is made on the basis of improving the pre-existing interestingness J -measure . Although experimental results with arti fi cial data show interesting capabilities of the approach, no optimal solution is guaranteed. Besides, the use of templates implies some previous knowledge about the rules which are to be found.

In QAR-miner, by Cheung et al. [6], data attributes are assumed to be all numerical. The multi-dimensional space de fi ned by the attributes is fi rst discretized into equal sized base regions (hyper-rectangles de fi ned by intervals on the attributes). Too small intervals are avoided by setting a minimum density . Density is the number of cases in the region (a hyper-rectangle) divided by the region X  X  volume. A minimum volume threshold is also user-de fi ned. Base regions are then merged through clustering. Rules are obtained by regarding the resulting regions as itemsets, and checking if their support and con fi dence are large enough. QAR-miner is also more ef fi cient than Skrikant and Agrawal X  X  approach, but we have no measure of what we are losing when we discretize. The guiding principle is that dense regions (intervals) are interesting. Although sharing some simila rities with the distance based approach, Cheung X  X  rules are characterized in terms of well known support and con fi dence. No completeness property of the algorithm is provided. Yiping Ke et al. [12,13] generate association rules with an arbitrary number of items, involving qualitative and quantitative attributes. Quantitative items have conditions on intervals. The key idea of their approach is the use of a speci fi c interest measure normalized mutual information ( NMI ) which applies to both qualitative and quantitative items. They handle numeric attributes by pre-discretizing them with equi-depth intervals. Then, in the second step, they identify all pairs of attributes that are independent according to NMI . In the third step, itemsets are formed from combinations of these pairs of attributes, considering their possible values. In the case of numerical values, base intervals are considered, as well as combinations of base intervals up to a user provided maximal support. The algorithm is ef fi cient and sound with respect to minimum support and con fi dence (all provided rules satisfy these criteria) but is not complete w.r.t. the same criteria (some rules may be missed). However, the method can be made complete with respect to the measure interest ( lift ).

Ayubi et al. [3] devise an algorithm for discovering association rules involving items X  X  v where the operator  X  can be one of = , = ,&gt;, . The procedure starts by fi nding frequent itemsets with operator = (numerical and categorical attributes are treated equally without necessarily pre-processing). These are called simple itemsets. Then, it combines simple itemsets in order to include items with other operators. This is done by joining consecutive values of numerical attributes. One important aspect of this approach is that the database is visited only once for counting the supports of simple itemsets. The algorithm is ef fi ciently implemented, produces compact rules when compared with boolean association rules, but is obviously not complete. 2.1. Optimal intervals Fukuda et al. [9] contributed with an ef fi cient algorithm that fi nds rules with the form X  X  Int  X  Cons ,where Cons is a fi xed boolean condition. It fi nds rules which have enough support and are optimal in terms of con fi dence, and rules which have enough con fi dence and are optimal in terms of support. This is done by looking for optimal interval bounds using a non-trivial ef fi cient procedure which is reduced to fi nding optimal slopes in support-con fi dence curves. The whole algorithm is relaxed in a number of ways for the sake of ef fi ciency: the numerical attribute is fi rst binned (which introduces some source of global sub-optimality); and the bins are determined using a sample of the whole data set if it is too large to fi t in memory. Fukuda X  X  proposal has t he advantage of precisely characterizing the resulting rules. They also show that the method can be trivially extended to rules of the form X  X  Int  X  Cond  X  Cons , where Cond is a boolean condition. In their earlier paper [8 ], Fukuda et al. also propose rules involving two numerical attributes in the antecedent, thus de fi ning rectangular regions. They show that looking for such rules is O (( N A  X  N B ) 1 . 5 ) ,where N A and N B are the number of bins for the attributes involved. This is regarded as computationally demanding.

Rastogi and Shim [22] extended the work of Fukuda et al. [9] by allowing the rules to have disjunctions and an arbitrary number of quantitative and qualitative attributes. Rule search is restricted by the use of a template. A template is a rule where attribute names and values may be undetermined. Rule search and generation proceeds by fi lling in (instantiating) these free slo ts. Disjunctions enable conditions of the form X 1  X  Int 1  X  ...  X  X k  X  Int k . Although in general this problem is NP-hard, Rastogi and Shim propose a non-trivial search procedure which considers candidate rules as ordered by a combination of support and con fi dence w 1  X  supp + w 2  X  conf . 2.2. Evolutionary discretization
Interesting intervals can also be found using evolutionary approaches, which start with a population of itemsets with conditions of the form X  X  Int . Then, appropriate operators for mutation and crossover are applied to improve the itemsets according to a pre-de fi ned fi tness function. Under this general setting, Mata et al. [27] propose GAR (Genetic Association Rules) for generating quantitative itemsets. The fi tness function favours high support and itemset size, penalizing itemset coverage overlap (two itemsets covering the same tuples) and wide intervals. The proposal has no guaranteed result, but simple experiments with arti fi cial datasets show some interesting capabilities of the approach.

Saleb-Aouissi et al. [24] have later noted that the GAR does not take rule con fi dence into account, since the evolutionary approach is only used for itemset generation. Their algorithm, QuantMiner, starts with a population of rules generated from user-provided templates. One important difference w.r.t. GAR The fi tness function also favors small intervals and penalizes low support. 3. Maximal leverage numerical rules
Our proposal for discovering association rules w ith interval conditions focuses on rules which have one numerical attribute in the consequent. This attribute is a speci fi c property of interest (POI). In our approach, this POI is dynamically di scretized according to the conditi ons in the antecedent of the rule. The discretization is optimal with respect to well de fi ned criteria.

Given a dataset D , with all attributes categorical except one ( A ), which is numerical, we want interesting rules (or rules that can be potentially interesting in practice) of the form Ant  X  A  X  I ,where Ant is a conjunction of conditions on the categorical attributes. I is an interval or union of intervals. For example I can have the form [ x l ,x u ) . If such an interval is unbound on one of the sides the condition becomes A&lt;x or A x . Let us also assume for now that we have some measure of interest on such rules. Under such a setting, the baseline approach discretizes attribute A and uses some existing association rule discovery engine. However, such a global pre-discretization does not guarantee optimal results, since the exact form of the interval may be different for each particular rule. Our aim is to avoid pre-discretization and the sub-optimal decisions that come with it. The interest measure we will use is leverage , also known as Piatetsky-Shapiro X  X  measure [20].

Supp ( X ) is the support as usually de fi ned [1], i.e., the relative frequence of X . Leverage takes values from the interval [  X  0.25,0.25] [26]. When antecedent and consequent are statistically independent it is zero. Therefore, for values close to zero, the rule is considered uninteresting. Commonly, negative values of leverage are not exploited either, since they indicate a negative association. Such rules, however, might be interesting for particular applications. In our case, we assume we are interested in rules with leverage above some positive threshold. Nevertheless, our results can be easily adapted for fi nding rules of minimal leverage. Although less popular, the added value (AV) interest measure [23] is very intuitive and will be useful in our analysis. It is the difference between the a posteriori and apriori values of con fi dence ( Conf ( . )).
 Below we show an example of a maximal leverage numerical rule , using the UCI dataset  X  X uto MPG X  [17]. In the fi rst line we give the value for the measures coverage (Cov), leverage (Lev), added value (AV) and con fi dence (Conf).
The above rule states that cars with horse power (HP) above 132.5, more than 5 cylinders (Ncy) and made (Year)) before 1980 tend to have a miles per galon (MPG) value below 18 when compared to a generic car. In fact, the rule says that a generic car will only have such a bad performance with much lower probability (0.653 lower to be precise, a s it is given by the added v alue AV). The condition MPG &lt; 18 is the interval condition of the form A&lt;x u or A&gt;x l that maximizes leverage (and added value) for that sub group of cars. The con fi dence of the rule is 0.922. This characterizes the sub group in a very precise way. At the same time, tells us much about the relation between the features in the antecedent and the numerical property of interest. Note that the same rule but with the consequent MPG &lt; 20 . 3 would have con fi dence equal to 1. However, its leverage would not be optimal.  X  X ov X  is the coverage of the rule, which is simply the support of its antecedent. In this case, the antecedent of the rule applies to 22.6% of the examples in the data set.

The use of leverage as an interest measure is motivated by two reasons. First, leverage is one of the interest measures that assess the unexpectedness of the rule. The other reason is that leverage has mathematical properties that allow us to make clear statements. Other popular interest measures, such as lift (also known as interest), conviction [5] or  X  2 [15] will apparently require a different approach. Added value has a very intuitive reading but is a less popular measure. Our results have implications on both leverage and added value. Whenever we say  X  X aximal leverage X  or  X  X ptimal leverage X  this also implies  X  X aximal X  or  X  X ptimal added value X .

We should stress that such an optimal rule would be very unlikely to obtain if pre-discretization of the target variable had been used. One possibility would be to exhaustively consider all possible intervals. However, this would be unfeasable, in general. Even less likely would be to fi nd all optimal rules given a speci fi c pre-discretization procedure.

This kind of rules can be potentially useful in practical applications where there is a numerical property of interest and the aim is to study its behavior. One example is travel time in urban public transports [11]. Managing schedules of buses and bus drivers is a very complex task that can be assisted through the use of predictive and descriptive data mining techniques [16]. A discovered maximal leverage rule can give a manager information about under which conditions (d ay of week, month, holiday season, driver, etc.) the intervals of travel time values that are more speci fi c of those conditions. Such discovered knowledge can be potentially useful for bus schedule adjustment and for explaining deviations from the planned schedule. 3.1. Stating the problem
Let us now state our knowledge discovery problem of fi nding rules with maximal leverage having interval conditions on a numerical attribute in the consequent (maximal leverage rules, for short). Given a dataset S with a numerical attribute A , a minimum coverage value Cov min and a level of signi fi cance  X  , Find all the rules r of the form Ant  X  A  X  I ,where I is an interval or a union of that de fi nes an interesting subgroup with a level of signi fi cance  X  .
 Cov ( r ) is the coverage of the rule. The condition on the interestingness of Ant will be verifyed by a goodness of fi t test comparing the distribution of A for the cases which satisfy Ant with the a priori distribution of A . Our proposal is to use the Kolmogorov-Smirnov test. 4. Distribution rules
Distribution rules (DR) have been introduced in [11] as a kind of association rules with a numerical attribute of interest A on the consequent. Whereas the antecedent of a DR is similar to the one of an AR, its consequent is a distribution of A under the conditions stated in th e antecedent. As we will see, we can discover maximal leverage rules from distribution rules (DR) with advantage over a direct approach. In the DR setting, all the antecedents Ant which correspond to an interesting distribution D A | Ant (read as  X  X he distribution of A given Ant  X ) are found. In this case, interesting means that the distribution of A under Ant is signi fi cantly different from the distribution of A without any constraints ( apriori ). By post-processing the interesting distributions, we can obtain optimally interesting intervals on A , as far as leverage is concerned.
 Let us review what distribution rules are and how they can be derived from data [11].
 De fi nition 1. A distribution rule ( DR ) is a rule of the form Ant  X  A = D A | Ant ,where Ant is a set of an empirical distribution of A for the cases where Ant is observed. This attribute A can be numerical in the sample and freq ( A j ) is the frequency of A j for the cases where Ant is observed.
In the context of this paper we will assume A is a numerical variable. However, the concept of distribution rules includes categoricalattributes in the consequentas well. The attributes on the antecedent are either categorical or are discretized.

In Fig. 1 we can see one distribution rule derived from the  X  X uto MPG X  data set. In fact this distribution rule is related to the same subgroup as the maximal leverage rule shown before. The antecedent is shown above the chart. The darker curve shows the density of MPG|Ant , the grey curve shows the density of MPG overall. In the chart we can also see some measures characterizing the rule: KS-interest, which is given by 1  X  p KS ,where p KS is the p value of the Kolmogorov Smirnov test; the support (Sup) of the antecedent, the mean of MPG|Ant and its standard deviation. There is also an indication of what the property of interest (POI) is. The represented densities are estimated using kernel density estimation [10]. Given a dataset S , the task of distribution rule discovery consists in fi nding all the DR Ant  X  A = D A | Ant ,where Ant has a support above a determined mininum  X  min and D A | Ant is statistically distribution is the one obtained with all the values of A for the whole dataset or a distribution obtained from a holdout data set. To compare the distributions we use Kolmogorov-Smirnov [7], a statistical goodness of fi t test. The value of the KS (Kolmogorov-Smirnov) statistic is calculated by maximizing |
F s ( x )  X  F ( x ) | ,where F ( x ) is the empirical cumulative distribution function for the whole domain of A and F s ( x ) is the cumulative distribution function for the cases covered by Ant . 5. Finding maximal leverage rules
We now get back to our problem of fi nding rules with maximal leverage. We will see how such rules can be obtained from distribution rules. We start with a special case of conditions on intervals: inequalities. We show that, given a distribution rule Ant  X  D A | Ant , we can obtain a maximal leverage rule Ant  X  A  X  I ,where I is one interval unbound on one of the sides . Such consequents can be written as A&lt;x u or A x l . Then we turn to the general case where conjunctions of interval conditions are used. 5.1. Inequality consequents
In the process of determining whether a distribution rule is interesting or not we calculate the value of the KS (Kolmogorov-Smirnov) statistic. Although not interesting for the KS test, the point x max where F ( x max ) &lt;F ( x max ) then it will be more interesting to look at the rule Ant  X  A x max with positive are assuming that a KS test was performed with success.
 with maximal added value, and hence leverage, with an inequality in the consequent, is one of the two above. If F s ( x max )  X  F ( x max ) is positive the optimal rule is Ant  X  A&lt;x max .Ifitisnegativethe optimal rule is Ant  X  A x max . Algorithm 1 describes the procedure for fi nding the optimal boundary of the inequality (threshold value). Theorem 1 formalizes the above line of reasoning.
 Theorem 1. Let Ant  X  D A | Ant be a distribution rule, F s ( A ) be the empirical pr obability distribution of
A under the subgroup de fi ned by Ant , F ( A ) be the a priori empirical distribution function A .The rule of maximal leverage of the form Ant  X  A&lt;x max or Ant  X  A x max , is the one where x is the set of points observed in D A .
 A&lt;x max is which is the same as ( F s ( x max )  X  F ( x max ))  X  Supp ( Ant ). Since this is the maximal value of the difference, any other point will give a rule with lower leverage, which proves it for the fi rst case. If F s ( x max ) &lt;F ( x max ) , then the leverage of the rule with A x max is ((1  X  F s ( x max ))  X  (1  X  F difference is maximal for x max , then any other point will give a rule of this form with lower leverage. Example 1. Consider the distribution rule in Fig. 1. If we take the empirical distributions D MPG | Ant the rule is the maximal leverage rule w ith that antecedent and with an in equality in the cons equent. We can observe that x = 18 is very close to the intersection point of the two estimated density curves. We will return to that later.
 Example 2. Some other maximal leverage rules with inequalities obtained from the  X  X uto MPG X  dataset are
We can observe that we automatically obtain the sensible direction of the inequality. The fi rst rule says that heavy old cars from the US tend to make less than 19 miles per galon. The second rule states that light cars tend to make 26 m.p.g or more. The rules have a very intuitive reading and clearly point a tendency. The rules also have high con fi dence. 5.2. The general case of intervals
Inequalities provide rules with a quite neat description of data. However, we must be open to the possibility of having rules with mo re general conditions of the form Ant  X  A  X  I ,where I is an interval on the right. This naturally subsumes inequalities.
 Such general rules can also be found by studying the empirical distribution difference F s ( x )  X  F ( x ) . Unfortunately, they tend to be less readable than inequalities especially when I is a union of intervals. The existence of more than one interval is mostly caused by the natural instability of the empirical distribution. Albeit the reduction in readability, the leverage of an interval rule tends to be higher than its inequality rule counter-part, and it is at least as high.
 Example 3. Consider the distribution rule shown in Fig. 2. We can see that the density of MPG is higher until about 10, and then from about 13 to twenty something. If we try to transform this DR into a MLR with an inequality, we obtain
However, the following rule would have higher leverage.
Such rules can also be automatically obtained from distribution rules. However, readability is a problem. We may notice in the above example the existence of many small intervals, such as [23 . 9 , 24) , which may be disposable at a possibly small cost. Other strategies may also increase readability at a small cost. We will look closer into this problem later on.

Let X  X  now see how interval bounds are found. The process generalizes the one seen for fi nding inequality consequents. Given a distribution rule, we can compare its consequent distribution function F s against the apriori distribution F of the property of interest. In the case of inequalities, we need the global maximum of the difference | F s ( x )  X  F ( x ) | . Now we will look for the local maxima and minima of
F s ( x )  X  F ( x ) . When this difference grows, leverage is increasing. When it stops growing (at a local maximum) we have found an interval right bound. When it starts growing again (at a local minimum) we have identi fi ed an interval left bound. All growing portions of the domain increase leverage. All decreasing portions reduce leverage. Algorithm 2 provides the outline for the procedure for fi nding the local minima and maxima of F s ( x )  X  F ( x ) , which become the lower and upper bounds of the intervals in the consequent. Theorem 1 proves the correctness of the algorithm.
 Theorem 2. Let Ant  X  D A | Ant be a distribution rule in the same conditions as in Theorem 1. The rule of maximal leverage of the form Ant  X  A  X  I where I is an interval or a union of intervals of the form [ x l ,x u ) , is the one whose bounds are obtained as described in Algorithm 2. The set of intervals are obtained by pairing the elements of LB with the ones of UB following their order. The leverage of the Proof. Since each interval in I is limited by a local mininum and a local maximum of F s ( x )  X  F ( x ) , any segment of the intervals, the overall leverage value reduces. Likewise, for any segment outside the given intervals F s ( x )  X  F ( x ) is monotonically decreasing. Analougously, if we add to I any external segment the overall value of leverage will decrease. Which proves that we have found the rule of maximal leverage of the required form.
 Example 4. Looking back at the maximal leverage rules with inequalities shown in Example 2, we now study the corresponding rules with generic intervals.
Leveragesare higher (0.18 against 0.17 in the fi rst rule and 0.15 against 0.13 in the second). Con fi dence is higher for both interval rules as compared against inequality rules.

In the example above, again we observe the existence of too many  X  X mall X  intervals which reduce the readability of the rules. Are they worthwhile keepi ng? What should our criteria be? That X  X  what we will deal with in the next section. 6. Improving readability
As we have seen, we obtain the bounds of the intervals for the consequents of maximal leverage observed when we study it closely. As a consequence,the empirical distributions have some  X  X urbulence X  areas and local optima of F s ( x )  X  F ( x ) may abound in these regions. Since the interval bounds directly correspond to local optima, we will frequently have large collections of intervals as a consequent. The problem with such large collections is reduced readability.

Different strategies may be devised to improve read ability. Alway s necessarily at the cost of reducing leverage, since we have proved how the optimal leverage is found. The simplest one is to use a tighter language bias, as it was the case with inequality rules. Another strategy tries to eliminate too small of Dom ( A ) which are potencially more interesting for scrutiny. Finally, we can smooth the distribution curves to reduce turbulence by using density estimation.

As a proxy for improved readability and leverage l oss, we will follow one rule about old large cars obtained with the  X  X uto MPG X  data set. The optim al rule, without any readability concern is shown below. Its leverage is 0.155 and it has 5 conditions on the consequent. 6.1. Using tighter language bias
We have already seen how this can be done with ine qualities. Instead of using general intervals, we use less expressive but more readable conditions. The question is, given a tighter bias, how much do we lose in terms of leverage? Without any constraints on the underlying distributions, it is hard to say. Empirically, we can observe the distribution of leverage loss from interval rules to inequality rules on particular discovery tasks. The rule for the same subgroup of old large cars is now as follows.
We will also study leverage loss on the whole  X  X uto MPG X  dataset. What we observe is that, for a minimal support of 0.2 and a KS signi fi cance (  X  ) value of 0.05, the mean leverage loss is of 0.022, reaching a maximum of 0.078. Median is 0.012. This means that, in this data set, the cost of using a Figure 3 gives a full picture of the leverage loss distribution. If we measure the reduction in the number of intervals, and we take this as a proxy for improved readability, the mean value for the  X  X uto MPG X  dataset is of 88%. In other words, the single interval in the consequent of the inequality rule is 12% of the number of intervals for the optimal rules (on average). 6.2. Interval smoothing
Interval smoothing consists in identifying intervals that have little impact on overall leverage and remove them. Likewise, we can try to remove  X  X egative X  intervals, by joining/bridging two consecutive intervals with a small gap in between. Again, the gap (or negative interval) is removed if its removal has little impact. One direct strategy we have tried is to remove all gaps and intervals whose individual impact is below a (user provided) maximal impact threshold max.imp . Other strategies for measuring the impact of interval removal could be considered. Since the interval/gap removal decisions are independent of one another, the accumulated leverage loss for a given rule may reach max.imp  X  n.int ,where n.int is the number of intervals in the consequent. In an extreme situation, this strategy may remove all the intervals, which is not desirable. In the case of the rule for old large cars we obtain Empirically, we observe that the impact on leverage of removing intervals with a max.imp of 0.02 on the MPG set of rules we have been using is relatively low, with a maximum of 0.062 and a mean of 0.017 (median is 0.011). On the other hand, the reduction in the number of intervals is signi fi cant (improved readability). This direct strategy reduced, in average and for this data set, the number of intervals in the consequent by 58%. Preceding interval removal by gap removal, degrades results both in terms of leverage loss (mean 0.029) and readability improvement (mean 32%). The gap removal strategy tries to delete each of the interval gaps, and accepts the removal of those which do not degrade leverage more than max.imp (0.02). The example rule is shown below. By chance, this strategy found a rule with con fi dence 1. 6.3. Restricting the domain
This strategy is based on restricting the pool of points considered as candidate interval bounds. In the A (the property of interest). By restricting the Dom ( A ) for a given rule (subgroup) to the set of values of A observed in that subgroup, we get more readable rules. Again, necessarily, these are sub-optimal. Here is the observed example rule:
An experiment with the  X  X uto MPG X  data set indicates that this strategy can be effective in terms of improved readability. The mean reduction in the number of intervals is of 64%, which is better than interval smoothing for the same dataset. Mean leverage loss is slightly better (0.013) but its distribution has a long right tail, which means that it can have (with low probability) high val ues (maximum is 0.089). 6.4. Density estimation
In this case, readability is improved by using density estimations instead of empirical densities. So far, maximal leverage numerical rules have been determined on the basis of empirical distributions, as observed in the data set. Using kernel density estimation [10] we can smooth the behavior of F ( x )) and fi nd approximate maximal leverage rules with fewer interval conditions. The rules obtained using density estimation will be sub-optimal. However, they may also be more robust to the arrival of new observations. In the current implementation we have employed the R built-in function  X  X ensity X  with a gaussian kernel for density estimation.

The procedure for fi nding interval bounds with dens ities (which we will refer to as the MLR generation method  X  X stimation X ) is equivalent to the one for fi nding optimal intervals for the empirical distributions. Given the two distributions D s and D , we estimate the densities d s ( x ) and d ( x ) . The local optima of the distribution functions correspond to the points where the two density curves intersect. Therefore, the interval bounds are points from the domain of A (even if not observed in the data) where the difference d ( x )  X  d ( x ) changes sign.

Let us see the result of one example. In Fig. 1 we can see that the density within the subgroup seems to be higher than the apriori density for for MPG values below approximately 18. The obtained rule, however, provides more detail that is not easily visible in Fig. 1. In particular the interval MPG =[7 . 98 , 8 . 01) seems redundant. Nevertheless, it is objectively important for increasing leverage. This interval appears in this rule because of some instability in th e density curves.
Experiments with the  X  X uto MPG X  data set indicate that this estimation method greatly improves readability without sacrifycing too much leverage. On average, for the Auto data set, leverage loss is 0.02 and readability improves 85%. Moreover, as we will see in the experimental validation section of this paper, this method is very ef fi cient. 7. Completeness
Our aim was to fi nd all optimal rules with interval conditions on A in the consequent with signi fi cant interest given a level of signi fi cance  X  , and with coverage above given Cov min . The general process for This will be the coverage of the maximal leverage rule. It is different from its support because the interval I will have to exclude some points. Therefore we satisfy the minimum coveragerequirement for maximal leverage rules if we make Sup min = Cov min in the DR discovery process. Hence, given the completeness of the maximal leverage rule discovery process is as complete as the process for discovering distribution rules. We should note that some fi lters can be used in the process of the discovery of distribution rules. These make the process faster but may cause incompleteness. However, this is empirically tolerated. 8. Complexity
As in the discovery of association rules, the time taken by the discovery of distribution rules is mainly affected by the number of cases N and the minimal support/coverage  X  . Time spent grows linearly as N grows, and tends to grow exponentially as  X  decreases. Other aspects are data set dependent and are related to the distribution of the cases [11]. The number of different numerical values V of the property of interest is also important, since, for each rule, we must compare two distributions involving at most V values for determining the value of the KS statistic and its p-value. This comparison is done by running through the sequence of values once and is therefore O ( V ) . Memory spent is also O ( V ) .
Obtaining maximal leverage rules from DRs implies going through the pair of distributions for each I also O ( V ) . 9. Experiments
Our experiments have two aims. One is to study the scalability of the proposed algorithms with respect to the number of cases and the number of distinct values of the property of interest. The other is to perform a more general assessment of the impact of sub-optimal strategies on leverage. These experiments are performed on a number of regression datasets. 9.1. Running versions Our current running version of the methods for generating MLR from data are implemented in java and R [21]. The distribution rule generation phase is implemented in java as the program CAREN ,version 2.6 [4], also available as the R package carenR . 1 The MLR methods are currently prototyped in R, but will be integrated in CAREN in the future. 9.2. Data sets
The data sets used come from the UCI repository [17] and from the repository of Lu  X   X  s Torgo. 2 All the attributes except the property of interest are pre-discretized. The pre-discretization technique used is not of particular interest. Table 1 summarizes the data sets. 9.3. Scalability
Here, we want to con fi rm the theoretical complexity analysis that says that MLR generation time grows linearly with the number of different POI values. Moreover, we con fi rm that the generation of distribution rules also grows linearly with the number of examples. The data sets used for these experiments have different sizes, ranging from small to medium size. We have generated rules for 3 values of minimal coverage, 0.1, 0.05 and 0.01. The three maximal leverage methods used were inequalities , intervals and estimation . Distribution rules were generated with an  X  (KS-test level of signi fi cance) of 0.05. Table 2 shows the execution times for the DR generation, and MLR generation, for each of the three methods. We can see that execution times are higher as the number of different POI values increases. The  X  X balone X  data set, for example, despite being about 10 times larger than  X  X ousing X  and  X  X uto X , takes much less time. The number of rules generated also plays a role. However, it does not dominate, as we can see with  X  X al. Housing X . There, only 72 rules are generated and it is the second most time consuming data set.

Regarding the MLR methods, the most time consuming is the optimal one (intervals). Using in-equalities is considerably faster, but the most stable runtimes are obtained with the estimation method. Although these data sets give an overall perspective of the computational effort taken, it is hard to devise trends due to too many varying factors. Therefore, we will engage in more focused experiments that help to clarify these trends.
To study the in fl uence of the number of different POI values in the runtimes, we have generated 4 variants of the  X  X art-example X  data set, by rounding the property of interest to 1, 2, 3 and 4 decimal places. This simple process produces different numbers of distinct values of the POI. As we can observe in Fig. 4, the computation time per rule grows linearly with the number of different values on the property of interest for the  X  X nequality X  MLR met hod. Figure 5 shows that the estimation MLR method is sublinear. This happens because the values of the density functions are estimated for a fi xed number of points (512 in our experiments, which is the default parameter of R X  X  density estimation function). In practice, this has an effect similar to reducing the number of different POI values considered as candidates for interval bounds. This particular experiment con fi rms our complexity analysis. The time spent for fi nding MLR grows at most linearly with the number of values. This implies that we can apply the method to other domains with a large number of different values in the POI.
 We now con fi rm the linear scalability of distribution rul e generation. We have randomly splitted  X  X al. Housing X , in 5 parts, and ran the DR generation on a growing sample of the data set. The results can be seen in Fig. 6. 9.4. Re adability
In Section 6 we have already empirically measured the impact of different MLR generation methods on leverage and on readability. However, this has been done for one particular data set for illustrative purposes. In this section we extend the empirical assessment to other data sets and study in more depth the impact of minimal converage and different POI values on MLR readability. The experimental measures used are  X  X everage loss X  and  X  X mproved readability X . The leverage loss of one rule r for a method m is the difference between the optimal leverage for rule r , as obtained by the method  X  X nterval X , and the leverage of the corresponding rule obtained with method m . Improved readability of a rule r for a method m is 1 minus the quotient between the number of interval conditions in the consequent of r with method m and the number of interval conditions in the consequent of the corresponding optimal rule r . In Fig. 7 we can observe the relative performance of methods  X  X nequality X  and  X  X stimation X  on the 5 data sets. As we can see, no method is overall better.  X  X stimation X  loses less leverage but has lower readability than  X  X nequality X . This is perfectly expected since the  X  X nequality X  method generates rules with 1 interval condition. Given these results, we can claim that the method  X  X stimation X  is a very good compromise. In particular, we can see the nearly extreme improvement for the data sets with many different POI values ( X  X al. housing X  and  X  X art X ). In fact, for these data sets we obtain values very close to 1. This happens because the optimal rule ha s tens of interval conditi ons. These experiments do not include the other presented techniques for improving readability (interval smoothing and domain restriction) due to poor computational performance of their current implementation. 10. Discussion
We have empirically con fi rmed the scalability of the optimal MLR generation methods proposed:  X  X nequality X  and  X  X ntervals X . However, this second method is clearly computationally more expensive. The  X  X stimation X  method shows ver y good scalability features as the number of different POI values grows. We believe that the current implementation of the  X  X ntervals X  method, despite essentially linear, can still be algorithmically optimized. The proposed in terval smoothing technique, with interval and gap elimination, can certainly be much improved. The current proposal is a very simple one and relies on auserde fi ned parameter min.imp . This parameter has a local impact on leverage, but the user has no control on the effect of the global leverage loss caused by interval and gap removal.

Another relevant issue regarding scalability and ef fi ciency is the possibility of limiting the number of different values in the POI. Instead of the full set of values, we would use some sort of fi ne grained clustering. This would yield approximate results in terms of placing interval bounds. However, we have preliminary indications that these would be very good approximations. The main advantage of such an approximation is that computation time would be considerably limited since, as we have shown, MLR generation time grows linearly with the number of different values of the POI.

The interest measure  X  X everage X  has convenient properties that enable the ef fi cient algorithmic dis-covery of maximal leverage rules with dynamic discretization. In the case of leverage, the optimality criterion is easily de fi ned. Although lift is probably the most intuitive interest measure for association rules, it does not seem to have a direct mathematical relation with the KS statistic. However, maximal lift rules can be discovered using either a post-processing step or by implementing directly an interval determination step within the discovery procedure. The problem, in the case of lift, is that maximal lift rules may have very low support (coverage). This is similar to what happens with optimal con fi dence rules which can only be found for a fi xed value of support [9].

With respect to the state of the art, our proposal of Maximum Leverage Rules discovery is the only that optimizes leverage and added value . Most approaches presented in section 2 are heuristic. Some of these are able to discover rules in a general form, involving conditions on categorical and numerical attributes, but don X  X  guarantee any optimal results. The approaches of Fukuda et al. [9] and [22] provide optimal rules under the support/con fi dence setting. In these cases, however, discovery is restricted to certain forms of rules. In the case of [9] only one inte rval condition is allowed i n the antecedent. In the case of Rastogi and Shim [22] the user must supply rule templates to restrict search. 11. Conclusion
We have seen how to generate maximal (optimal) leverage rules with interval conditions in the consequent. Our proposal is highly scalable and avoids pre-discretization of the property of interest. We have proposed two basic methods:  X  X ntervals X  and  X  X nequalities X  that dynamically fi nd optimal interval bounds. We have proved that  X  intervals X  is the optimal method for positive interval conditions in general. The method  X  X nequalities X  is optimal for a speci fi c kind of interval conditions. A third method,  X  X stimation X , is sub-optimal but is a very good approximation with some advantages over the optimal method. These advantages are faster rule generation and more easily readable rules (with less interval conditions). The empirically observed leverage loss with respect to the optimal method is not important in general, as we have shown in our experiments (Fig. 7). To improve readability we have also proposed two other techniques:  X  X nterval smoothing X  and  X  X omain restriction X . The fi rst technique, when combined with the method  X  X ntervals X  is worse than  X  X stimation X  both on leverage loss and improved readability and is therefore not interesting. The  X  X omain restriction X  technique is less lossy but yields more complex rules than  X  X stimation X . We have proposed MLR approaches that take distribution rules as input. This enables modularity of the processes and allows MLR generation to immediatly bene fi t from DR concepts and fi lters. However, a more direct reimplementation would certainly result in a more ef fi cient knowledge discovery program.

In terms of scalability, it is important to notice that MLR generation time (per rule) grows linearly with the number of examples and the number of different values of the property of interest. Our implemented versions are quite capable of dealing with tens of thousands of cases and tens of thousands of different POI values for a minimum coverage of 1%.
 Future developments of the concept and algorithms for maximal leverage rules are as follows: most MLR methods are implemented in R and should be migra ted to java. Algorithmic optimizations are still possible. Conceptually, we intend to parameterize the language bias so that we can fi nd optimal rules with exactly or at most k conditions, for a given k . Another important research issue is the dynamic discretization of the attributes in the antecedent to avoid antecedent pre-discretization.
 Acknowledgements This work was partially supported by the FCT project MORWAQ (PTDC/EIA/68489/2006) and by Fundac  X   X  ao Ci  X  encia e Tecnologia, FEDER e Programa de Financiamento Plurianual de Unidades de I &amp; D. Special thanks to Brett Drury for giving some suggestions regarding the wording of two paragraphs. References
