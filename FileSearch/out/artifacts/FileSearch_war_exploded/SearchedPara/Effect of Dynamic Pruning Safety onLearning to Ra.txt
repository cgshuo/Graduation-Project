 A dynamic pruning strategy, such as Wand , enhances re-trieval efficiency without degrading effectiveness to a given rank K , known as safe-to-rank-K . However, it is also pos-sible for Wand to obtain more efficient but unsafe retrieval without actually significantly degrading effectiveness. On the other hand, in a modern search engine setting, dynamic pruning strategies can be used to efficiently obtain the set of documents to be re-ranked by the application of a learned model in a learning to rank setting. No work has examined the impact of safeness on the effectiveness of the learned model. In this work, we investigate the impact of Wand safeness through experiments using 150 TREC Web track topics. We find that unsafe Wand is biased towards docum-ents with lower docids, thereby impacting effectiveness. Categories &amp; Subject Descriptors: H.3.3 [Information Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Performance, Experimentation Keywords: Dynamic Pruning, Learning to Rank
A search engine deploying learning to rank techniques re-ranks the top K documents retrieved by a standard weight-ing model, known as the sample [3], as shown in Figure 1. To improve the efficiency of such a deployment, a dynamic pruning strategy such as Wand [1] could easily be used, which omits the scoring of documents that cannot reach the top K retrieved set. In doing so, Wand is safe-to-rank-K , which we denote as safe for short.
 Wand follows a Document-at-a-time retrieval strategy (DA-AT), whereby the posting lists for all constituent terms of a query are processed in parallel, allowing immediate deci-sions as to whether a document has scored high enough to make the current top K retrieved set. In particular, Wand repeatedly calculates a pivot term , by comparing the up-per bounds  X  ( t ) of each query term t to the current score of the K -th ranked document, known as the threshold  X  . The next document containing the pivot term is called the pivot document , which will be the next document to be fully scored. If the scored document exceeds the threshold  X  , then the current K -th ranked document is expelled from the re-trieved set, the new document inserted, and  X  updated. As the scoring for a query continues, the threshold  X  rises, such that more documents can be omitted from scoring.
However, Broder et al. [1] showed that Wand can be made more efficient by relaxing the safeness guarantee. This is a chieved by artificially increasing  X  by a factor F  X  1. F = 1 guarantees safe retrieval, while for F &gt; 1, increased effi-ciency can be achieved without much degradation in effec-tiveness. However, to the best of our knowledge, no previous work in the literature has addressed how such unsafe docu-ment rankings affect retrieval performance within a modern learning to rank setting. This paper provides a first study into the effect of safeness within a learning to rank setting, while providing explanations for the observed inherent bias in unsafe pruning that can improve effectiveness in some set-tings. Indeed, in contrast to a safe setting, unsafe Wand is dependent on the ordering of the collection, suggesting that further research into addressing the bias is needed.
Our experiments use the ClueWeb09 (cat. B) collection, which comprises 50 million English Web documents, and is aimed to represent the first tier index of a commercial search engine. We use the 150 corresponding topics and relevance assessments from the TREC Web tracks 2009-2011.

We index this collection using the Terrier information Following the three phase architecture of Figure 1, the top K = 1000 documents are ranked by Wand using the DPH Divergence from Randomness weighting model. We use a total of 33 standard query-dependent features (e.g. term weighting models, proximity features) and query-independent document features (e.g. link analysis, URL length, content quality). To re-rank the documents in the sample, we use the LambdaMART learning to rank technique [2, 4], which represents a state-of-the-art learning to rank technique, as per its recent performance in the 2011 Yahoo! learning to rank challenge. In particular, the 150 TREC topics are ran-domly split into three sets, namely training, validation and test. In the following, we experiment with the effectiveness of samples and LambdaMART models for various F values, while comparing and contrasting their retrieval effectiveness, in terms of NDCG@20 and relevant documents retrieved. h ttp://terrier.org Figure 2: NDCG@20 for the WAND sample, and L ambdaMART applied on the WAND sample.
 Table 1: Analysis of safe and unsafe samples and l earned models, as well as comparative statistics. Figure 2 shows the NDCG@20 effectiveness of both the Wand sample, and LambdaMART applied on the sample document rankings from Wand , as F is varied. Note that as the learned model obtained by LambdaMART may be sensi-tive to a given F setting, a different model is learned for each F value. From Figure 2, we observe that the effectiveness to rank 20 of the Wand sample is unchanged for 1  X  F  X  3, mirroring the original observations of Broder et al. [1]. How-ever, the LambdaMART performance is much less stable for different F values -indeed, the overall NDCG@20 trend is downward for larger F . This is explained in that the learned model ranks documents from deep in the sample, and hence is affected by degradations in the number of relevant doc-uments retrieved in unsafe samples. Indeed, on analysing F = 1, we find that of the top 20 documents ranked by LambdaMART, some were found as deep as rank 935 in the input sample ranking, while the mean rank in the sample of LambdaMART X  X  top 20 documents was 89.

However, for some small F values in Figure 2 (1 &lt; F &lt; 2), a learned model obtained from an unsafe sample could im-prove over the NDCG@20 of the learned model obtained from the safe F = 1 sample. To analyse this unexpected characteristic, in Table 1 we compare and contrast four set-tings: F = 1 and F = 1 : 75, with and without the application of LambdaMART. Indeed, F = 1 : 75 is an interesting setting as while it is unsafe, it does not degrade NDCG@20 of the sample ranking obtained from Wand , but significantly im-proves the effectiveness of LambdaMART, according to a paired t-test ( p &lt; 0 : 01). Moreover, F = 1 : 75 is an efficient setting (we find that it reduces the mean response time of 1000 queries from a query log by 19% compared to F = 1, while larger F values do not cause further time reductions).
Next, comparing the sample rankings obtained from Wand for F = 1 and F = 1 : 75, we note a decrease of 82 relevant documents retrieved across the 50 test queries. Moreover, we examined the docids (in the range 0..50M for ClueWeb09 cat. B) of the documents selected in the safe and unsafe sam-ples. We found that, on average, the safe sample retrieved documents from later in the posting lists (i.e. higher docids) than the unsafe sample (mean docids: 24.3M vs 21.9M). This observation is mirrored in the documents retrieved in one sample and not the other: the mean docid of documents Figure 3: Distribution of relevant documents across t he docid range, and for the safe &amp; unsafe samples. in the safe sample that are missing from the unsafe sample is 35M, while unsafe sample documents missing from safe have a mean docid of 11.2M. Finally, Figure 3 presents the distribution of relevant docids, as well as those retrieved in the safe and unsafe samples. This shows that while there is no docid bias for relevant documents, unsafe Wand is more biased towards low docids than safe Wand . Indeed, the mean docid of the relevant documents retrieved in the safe sample is higher than those found in the unsafe sam-ple (26.7M vs 25.6M in Table 1), explaining the change in retrieval effectiveness. Overall, this shows that aggressive, unsafe pruning by Wand can change the selected documents in a biased manner that is not present in safe pruning.
This behaviour of Wand is explained as follows: by ar-tificially increasing the threshold  X  by the factor F , the threshold for unsafe Wand causes more documents to be prevented from entering the top K . Early in the traversal of the posting lists, when the  X  is lower, documents can still enter into the retrieved set. However as  X  gets higher, more pruning occurs, even for documents that would have made the retrieved set for F = 1. This explains unsafe Wand  X  X  comparative preference for lower docid documents.
We contrasted the effectiveness of safe vs. unsafe rankings from Wand , and its impact on the effectiveness of a learning to rank technique, using ClueWeb09 cat. B and 150 TREC Web track topics. We found that while unsafe retrieval ef-fectiveness has little impact on the top ranked documents directly retrieved by Wand , it does impact deeper down, which can be to the detriment of a learned model applied on that sample. Some unsafe settings were even found to ben-efit the learned model. Through further analysis, we found that unsafe retrieval has an inherent bias towards documents with lower docids in the applied index ordering.
The observations in this paper can give rise to several fur-ther research lines. In particular, static collection orderings may be devised that counteract unsafe Wand  X  X  preference for lower docids. On the other hand, it may be possible to devise different manners of changing the threshold for in-creasing Wand  X  X  efficiency in a less biased manner. Craig Macdonald and Iadh Ounis acknowledge the support of EC-funded project SMART (FP7-287583).
