
Mining frequent trees is very useful in domains like bioinfor-matics, web mining, mining semi-structured data, and so on. 
We formulate the problem of mining (embedded) subtrees in a forest of rooted, labeled, and ordered trees. We present 
TREEMINER, a novel algorithm to discover all frequent sub-trees in a forest, using a new data structure called scope-list. 
We contrast TREEMINER with a pattern matching tree min-ing algorithm (PATTERNMATCHER). We conduct detailed experiments to test the performance and scalability of these methods. We find that TREEMINER outperforms the pat-tern matching approach by a factor of 4 to 20, and has good scaleup properties. We also present an application of tree mining to analyze real web logs for usage patterns. 
Frequent Structure Mining (FSM) refers to an important class of exploratory mining tasks, namely those dealing with extracting patterns in massive databases representing com-plex interactions between entities. FSM not only encom-passes mining techniques like associations [3] and sequences [4], but it also generalizes to more complex patterns like frequent trees and graphs [12, 14]. Such patterns typically arise in applications like bioinformatics, web mining, mining semi-structured documents, and so on. As one increases the com-plexity of the structures to be discovered, one e~tracts more informative patterns; we are specifically interested in mining tree-like patterns. 
As a motivating example for tree mining, consider the web usage mining [17] problem. Given a database of web access logs at a popular site, one can perform several mining tasks. 
The simplest is to ignore all link information from the logs, and to mine only the frequent sets of pages accessed by users. 
The next step can be to form for each user the sequence of links they followed, and to mine the most frequent user access paths. It is also possible to look at the entire forward accesses of a user, and to mine the most frequently accessed subtrees at that site. *This work was supported in part by NSF CAREER Award 
IIS-0092978, and NSF Next Generation Software Program grant EIA-0103708. permission and/or a fee. 
In recent years, XML has become a popular way of storing many data sets because the semi-structured nature of XML allows the modeling of a wide variety of databases as XML documents. XML data thus forms an important data min-ing domain, and it is valuable to develop techniques that can extract patterns from such data. Tree structured XML documents are the most widely occurring in real applica-tions. Given a set of such XML documents, one would like to discover the commonly occurring subtrees that appear in the collection. 
Tree patterns also arise in Bioinformatics. For example, re-searchers have collected vast amounts of RNA structures, which are essentially trees. To get information about a newly sequenced RNA, they compare it with known RNA structures, looking for common topological patterns, which provide important clues to the function of the RNA [19]. 
In this paper we introduce TREEMINER, an efficient algo-rithm for the problem of mining frequent subtrees in a for-est (the database). The key contributions of our work are as follows: 1) We introduce the problem of mining embedded subtrees in a collection of rooted, ordered, and labeled trees. 2) We use the notion of a scope for a node in a tree. We show how any tree can be represented as a list of its node scopes, in a novel vertical format called scope-list. 3) We develop a framework for non-redundant candidate subtree generation, i.e., we propose a systematic search of the possibly frequent subtrees, such that no pattern is generated more than once. 4) We show how one can efficiently compute the frequency of a candidate tree by joining the scope-lists of its subtrees. 5) Our formulation allows one to discover all subtrees in a forest, as well as all subtrees in a single large tree. Further-more, simple modifications also allow us to mine unlabeled subtrees, unordered subtrees and also frequent sub-forests (i.e., disconnected subtrees). 
We contrast TREEMINER with a base tree mining algorithm based on pattern matching, PATTERNMATCHER. Our ex-periments on several synthetic and one real dataset show that TREEMINER outperforms PATTERNMATCHER by a fac-tor of 4 to 20. Both algorithms exhibit linear scaleup with increasing number of trees in the database. We also present an application study of tree mining in web usage mining. 
The input data is in the form of XML documents that rep-resent user-session extracted from raw web logs. We show that the mined tree patterns indeed do capture more inter-esting relationships than frequent sets or sequences. 
A tree is an acyclic connected graph, and a forest is an acyclic graph. A forest is thus a collection of trees, where each tree is a connected component of the forest. A rooted tree is a tree in which one of the vertices is distinguished from others, and called the root. We refer to a vertex of a rooted tree = n, anti let {sl, s2 .... ,s~} be the nodes in S, with Figure 1: An Example Tree with Subtrees 
EXAMPLE 1. Consider Figure 1, which shows an example tree T with node labels drawn from the set L = {0, 1, 2, 3}. 
The figure shows for each node, its label (circled), its number of which support $1, i.e., there are two match labels for Sl, namely 134 and 135 (we omit set notation for convenience). 82 is also a valid sub,tee. $3 is not a (sub)tree since it is disconnected; it is a sub-forest.  X  
There are two mains steps for enumerating frequent sub-trees in D. First, we need a systematic way of generating candidate su.btrees whose frequency is to be computed. The candidate set should be non-redundant, i.e., each subtree should be generated as most once. Second, we need efficient ways of counting the number of occurrences of each candi-date in the database D, and to determine which candidates pass the minsup threshold. The latter step is data structure dependent, and will be treated later. Here we are concerned with the problem of non-redundant pattern generation. We describe below our tree representation and candidate gener-ation procedure. 
Representing Trees as Strings Standard ways of repre-senting a labeled tree are via an adjacency matrix or adja-cency list. For a tree with n nodes and m branches (note: m = n -1 for trees), adjacency matrix representation re-quires n+fn = n(f+ 1) space (f is the maximum fanout; n term is for storing labels and fn term for storing adjacency information), while adjacency lists require 2n + 2m = 4n-2 space (2n term is for storing labels and header pointers for adjacency lists, 2m is for storing label and next pointer per list node). Since f can possibly be large, we expect adja-cency lists to be more space-efficient. If we directly store a labeled tree node as a (label, child pointer, sibling pointer) triplet, we would require 3n space. 
For efficient subtree counting and manipulation we adopt a string representation of a tree. We use the following proce-dure to generate the string encoding, denoted T, of a tree T. 
Initially we set T = 0. We then perform a depth-first pre-order search starting at the root, adding the current node's label x to T. Whenever we backtrack from a child to its par-ent we add a unique symbol -1 to the string (we assume that -1 !i~ L). This format allows us to conveniently represent trees with arbitrary number of children for each node. Since each branch must be traversed in both forward and back-ward direction, the space usage to store a tree as a string is exactly 2m + 1 = 2n -1. Thus our string encoding is more space-efficient than other representations. Moreover, it is simpler to manipulate strings rather than adjacency lists or trees for pattern counting. We use the notation l(T) to refer to the label sequence of T, which consists of the node labels of T in depth-first ordering (without backtrack symbol -1), i.e., label sequence ignores tree topology. 
EXAMPLE 2. In Figure 1, we show the string encodings for the tree T as well as each of its subtrees. For example, subtree $1 is encoded by the string 1 1 -1 2 -1. That is, we start at the root of $1 and add 1 to the string. The next node in preorder traversal is labeled 1, which is added to the encoding. We then backtrack to the root (adding -1) and follow down to the next node, adding 2 to the encoding. Finally we backtrack to the root adding -1 to the string. 
Note that the label sequence of $1 is given as 112.  X  
We use the anti-monotone property of frequent patterns for efficient candidate generation, namely that the frequency of a super-pattern is less than or equal to the frequency of a sub-pattern. Thus, we consider only a known frequent pattern for extension. Past experience also suggests that an extension by a single item at a time is likely to be more efficient. Thus we use information from frequent k-subtrees to generate candidate (k + 1)-subtrees. 
Equivalence Classes We say that two k-subtrees X, Y are in the same prefix equivalence class iff they share a common prefix up to the (k -1)th node. Formally, let X, y be the string encodings of two trees, and let function p(,~', i) return the prefix up to the ith node. X, Y are in the same class iff p(X, k -1) = p(y, k -1). Thus any two members of an equivalence class differ only in the position of the last node. 
EXAMPLE 3. Consider Figure 2, which shows a class tem-plate for subtrees of size 5 with the same prefix subtree P of size 4, with string encoding P = 3 4 2 -1 1. Here x denotes an arbitrary label from L. The valid positions where the last node with label x may be attached to the prefix are no,n1 and na, since in each of these cases the subtree obtained by adding x to P has the same prefix. Note that a node at-tached to position n2 cannot be a valid member of class P, since it would yield a different prefix, given as 3 4 2 x. The figure also shows the actual format we use to store an equivalence class; it consists of the class prefix string, and a list of elements. Each element is given as a (x,p) pair, where x is the label of the last node, and p specifies the depth-first position of the node in P to which x is attached. For example (x, 1) refers to the case where x is attached to node nl at position 1. The figure shows the encoding of the subtrees corresponding to each class element. Note how each of them shares the same prefix up to the (k-1)th node. These subtrees are shown only for illustration purposes; we only store the element list in a class.  X  Let P be prefix subtree of size k -1; we use the notation [P]k-1 to refer to its class (we omit the subscript when there is no ambiguity). If (x, i) is an element of the class, we write it as (x, i) E [P]. Each (x, i) pair corresponds to a subtree of size k, sharing P as the prefix, with the last node labeled x, attached to node ni in P. We use the notation Px to refer to the new prefix subtree formed by adding (x,i) to P. 
LEMMA 1. Let P be a class prefix subtree and let nr be the right-most leaf node in P, whose scope is given as [r, r]. Let (x, i) E [P]. Then the set of valid node positions in P to which x can be attached is given by {i : ni has scope [i,r]}, where ni is the ith node in P. This lemma states that a valid element x may be attached to only those nodes that lie on the path from the root to the right-most leaf n~ in P. It is easy to see that if x is attached to any other position the resulting prefix would be different, since x would then be before n,. in depth-first numbering. Candidate Generation Given an equivalence class of k-subtrees, how do we obtain candidate (k+l)-subtrees? First, we assume (without loss of generality) that the elements, (x,p), in each class are kept sorted by node label as the primary key and position as the secondary key. Given a sorted element list, the candidate generation procedure we describe below outputs a new class list that respects that order, without explicit sorting. The main idea is to consider each ordered pair of elements in the class for extension, in-cluding self extension. There can be up to two candidates from each pair of elements to be joined. The next theorem formalizes this notion. 
THEOREM 1 (CLASS EXTENSION). Let P be a prefix class with encoding 7~, and let (x, i) and (y,j) denote any two el-ements in the class. Let P~ denote the class representing extensions of element (x, i). Define a join operator @ on the two elements, denoted (x,i)@(y,j), as follows: case I-(i = j): case II -(i &gt; j): add (y,j) to class [Px]. case III-(i &lt; j): no new candidate is possible in this case. 
Then all possible (k + 1)-subtrees with the prefix P of size k -1 will be enumerated by applying the join operator to each ordered pair of elements (x, i) and (y, j). PROOF: Omitted due to lack of space.  X  
EXAMPLE 4. Consider Figure 3, showing the prefix class ~o = (1 2), which contains 2 elements, (3, 1) and (4,0). The first step is to perform a self join (3, 1)@(3, 1). By case I a) this produces candidate elements (3, 1) and (3, 2) for the new class 'Pa = (1 2 3). That is, a self join on (3, 1) produces two possible candidate subtrees, one where the last node is a sibling of(3,1) and another where it is a child of(3, 1). The left-most two subtrees in the figure illustrate these cases. 
When we join (3,1)@(4, O) case II applies, i.e., the second element is joined to some ancestor of the first one, thus i &gt; j. 
The only possible candidate element is (4, 0), since 4 remains attached to node no even after the join (see the third sub(tee in the left hand class in Figure 3). We thus add (4, 0) to class [Pal. We now move to the class on the right with prefix "P4 = (1 2 -1 4). When we try to join (4, 0)@(3, 1), case 
III applies, and no new candidate is generated. Actually, if we do merge these two subtrees, we obtain the new subtree 1 2 3 -1 -1 4, which has a different prefix, and was already added to the class [Pal. Finally we perform a sclf-join (4, 0)@(4, 0) adding elements (4, 0) and (4, 1) to the class [P4] shown on the right hand side.  X  
Case I b) applies only when we join single items to pro-duce candidate 2-subtrees, i.e., we are given a prefix class [0] = {(xi,-1), i = 1,..., m}, where each xi is a label, and -1 indicates that it is not attached to any node. If we join (xl,-1)@(xj,-1), since we want only (connected) 2-subtrees, we insert the element (x j, 0) to the class of xi. 
This corresponds to the case where xj is a child of xl. If we want to generate sub-forests as well, all we have to do is to insert (xj, -1) in the class of xi. In this case x# would be a sibling of xi, but since they are not connected, they would be roots of two trees in a sub-forest. If we allow such class elements then one can show that the class extension theorem would produce all possible candidate sub-forests. However, in this paper we will focus only on subtrees. an prefix class with elements sorted according to the total or-or (x = y and: i &lt; j). Then the class extension method generates candidate classes [P]k with sorted elements. method correctly generates all possible candidate subtrees, and each candidate is generated at most once. Scope-List l:tepresentation Let X be a k-subtree of a tree 
T. Let Xk refer to the last node of X. We use the notation  X (X) to refer to the scope-list of X. Each element of the scope-list is a triple (t, m, s), where t is a tree id (tid) in which X occurs, m is a match label of the (k -1) length prefix of X, and s is the scope of the last item xk. Recall that the prefix match label gives the positions of nodes in 
T that match the prefix. Since a given prefix can occur multiple times in a tree, X can be associated with multiple match labels as well as multiple scopes. The initial scope-lists are created for single items (i.e., labels) i that occur in a tree T. Since a single item has an empty prefix, we don't have to store the prefix match label m for single items. We will show later how to compute pattern frequency via joins on scope-lists. with the horizontal format for each tree, and the vertical scope-lists format for each item. Consider item 1; since it occurs at noch~ position 0 with scope [0, 3] in tree To, we add (0, [0, 3]) to its scope list. Item 1 also occurs in T1 at position nl with scope [1,3], so we add (1,[1,3]) to /~(1). 
Finally, 1 occurs with scope [0, 7] and [4, 7] in tree T2, so we add (2, [0, 7]) and (2, [4, 7]) to its scope-list. In a similar manner, the scope lists for other items are created. 
Figure 5 shows the high level structure of TREEMINER. The main steps include the computation of the frequent items and 2-subtrees, and the enumeration of all other frequent subtrees via DFS search within each class [P]i E F2. We will now describe each step in some more detail. 
Computing F/ and F2: TREEMINER assumes that the 
To compute F1, for each item i  X  T, the string encoding of tree T, we increment i's count in a 1D array. This step also computes other database statistics such as the number of trees, maximum number of labels, and so on. All labels in FF1 belong to the class with empty prefix, given as [P]0 = [0] = {(i,-1), i  X  F1}, and the position -1 indicates that i is not attached to any node. Total time for this step is per tree, where n = ]T]. 
By Theorem 1 each candidate class [P]I = [i] (with i  X  F1) consists of'elements of the form (j, 0), where j &gt; i. For effi-cient F2 counting we compute the supports of each candidate by using a 2D integer array of size F~ x F1, where gives the count of candidate subtree with encoding (i j -1). Total time for this step is O(n 2) per tree. While computing 
F2 we also create the vertical scope-list representation for each frequent item i  X  El. 
Computing Fk(k &gt; 3): Figure 5 shows the pseudo-code for the depth-first search for frequent subtrees (ENUMERATE-
FREQUENT-SUBTREES). The input to the procedure is a set of elements of a class [P], along with their scope-lists. Fre-quent subtrees are generated by joining the scope-lists of all pairs of elements (including self-joins). Before joining the scope-lists a pruning step can be inserted to ensure that subtrees of the resulting tree are frequent. If this is true, then we can go ahead with the scope-list join, otherwise we can avoid the join. For convenience, we use the set R to de-note the up to 2 possible candidate subtrees that may result from (x, i) X (y,j), according to the class extension theorem, and we use E(R) to denote their respective scope-lists. The subtrees found to be frequent at the current level form the elements of classes for the next level. This recursive pro-cess is repeated until all frequent subtrees have been enu-merated. If [P] has n elements, the total cost is given as O(ln2), where l is the cost of a scope-list join (given later). 
In terms of memory management it is easy to see that we need memory to store classes along a path in DFS search. 
At the very least we need to store intermediate scope-lists for two classes, i.e., the current class [P], and a new candi-date class [Px]. Thus the memory footprint of TREEMINER is not much. 
Scope-list join for any two subtrees in a class [P] is based on interval algebra on their scope lists. Let s~ = [l~, u~] be a scope for node x, and Sy = [ly,uy] a scope for y. We say that s~ is strictly less than s~, denoted s~ &lt; sy, if and only if u~ &lt; ly, i.e., the interval s~ has no overlap with s~, and it occurs before sy. We say that s~ contains su, su is a proper subset of s=. The use of scopes allows us to compute in constant time whether y is a descendant of x 
EXAMPLE 6. Figure 6 shows an example of how scope-list 
While computing the new scope-list for the subtree (1 2 -1) the same match/abel. For example we find that s2 = [1, 1] most important elements of the in-scope and out-scope tests is to make sure that sy C s~ and s~ &lt; su, respectively. useful for resolving the prefix context when an item occurs more than once in a tree. Using this observation it is possible to reduce the space requirements for the scope-lists. We add l~ to the match label my if and only if x occurs more than once in a subtree with tid t. Thus, if most items occur only once in the same tree, this optimization drastically cuts down the match label size, since the only match labels kept refer to items with more than one occurrence. In the special case that all items in a tree are distinct, the match label is always empty, and each element of a scope-list reduces to a (rid, scope) pair. 
EXAMPLE 7. Consider the scope-list of (4, 0) in class [12] in Figure 6. Since 4 occurs only once in To and T1 we can omit the match label from the first two entries altogether, 
Opportunistic Candidate Pruning We mentioned above that before generating a candidate k-subtree, S, we perform a pruning test to check if its (k -1)-subtrees are frequent. While this is easily done in a BFS pattern search method like 
PATTERNMATCHER(see next section), in a DFS search we may not have all the information available for pruning, since some classes at level (k-l) would not have been counted yet. 
TREEMINER uses an opportunistic pruning scheme whereby it first determines if a (k -1)-subtree would already have been counted. ]If it had been counted but is not found in 
F~-i, we can safely prune S. How do we know if a sub-tree was counted? For this we need to impose an ordering on the candidate generation, so that we can efficiently per-form the subtree pruning test. Fortunately, our candidate extension method has the automatic ordering property (see 
Corollary 1). Thus we know the exact order in which pat-terns will be enumerated. To apply pruning test for a can-didate S, we generate each subtree X, and test if X &lt; S according to the candidate ordering property. If yes, we can apply the pruning test; if not, we test the next subtree. If 
S is not pruned, we perform scope-list join to get its exact frequency. 
PATTERNMATCHER serves as a base pattern matching algo-rithm to compare TREEMINER against. PATTERNMATCHER employs a breadth-first iterative search for frequent sub-trees. Its high-h~vel structure, as shown in Figure 7, is simi-lar to Apriori [3]. However, there are significant differences in how we count the number of subtree matches against an input tree T. For instance, we make use of equivalence classes throughout, and we use a prefix-tree data structure to index them, as opposed to hash-trees. The details of pattern matching are also completely different. PATTERN-
MATCHER assumes that each tree T in D is stored in its string encoding (horizontal) format (see Figure 4). F1 and 
F2 are computed as in TREEMINER. Due to lack of space we describe only the main features of PATTERNMATCHER; see [22] for details. PATTERNMATCHER (D, minsup): 1. F1 = { frequent 1-subtrees }; 2. F2 = { classes of frequent 2-subtrees }; 3. for (k=3;Fk-l#O;k=k+l) do 4. C~ --{ classes [P]k-1 of candidate k-subtrees }; 5. for all[ trees T in D do 6. Increment count of all S ~ T, S E [P]k-1 7. Ck = { classes of frequent k-subtrees }; 8. Fk ----( hash table of frequent subtrees in Ck}; 9. Set of all frequent subtrees --Us Fk; 
Pattern Pruning Before adding each candidate k-subtree to a class in Ck we make sure that all its (k -1)-subtrees are also frequent. To efficiently perform this step, during creation of F~-I (line 8), we add each individual frequent subtree into a hash table. Thus it takes O(1) time to check each subtree of a candidate, and since there can be k sub-trees of length k -1, it takes O(k) time to perform the pruning check for each candidate. 
Prefix Tree Data Structure Once a new candidate set has been generated, for each tree in D we need to efficiently find matching candidates. We use a prefix tree data struc-ture to index the candidates (C~) to facilitate fast support counting. Furthermore, instead of adding individual sub-trees to the prefix tree, we index an entire class using the class prefix. Thus if the prefix does not match the input tree 
T, then none of the class elements would match either. This allows us to rapidly focus on the candidates that are likely to be contained in T. Let [P] be a class in Ck. An internal node of the prefix tree at depth d refers to the dth node in 
P's label sequence. An internal node at depth d points to a leaf node or an internal node at depth d + 1. A leaf node of the prefix tree consists of a list of classes with the same label sequence, thus a leaf can contain multiple classes. For ex-ample, classes with prefix encodings (1 2 -1 4 3), (1 2 4 3), (1 2 4 -1 -1 3), etc., all have the same label sequence 1243, and thus belong to the same leaf. Storing equivalence classes in the prefix tree as opposed to individual patterns results in considerable efficiency im-provements while pattern matching. For a tree T, we can ignore all classes [P]k-1 where P 1~ T. Only when the prefix has a match in T do we look at individual elements. Sup-port counting consists of three main steps: 1) to find a leaf containing classes that may potentially match T, 2) to check if a given class prefix P exactly matches T, and 3) to check which elements of [P] are contained in T. Finding Potential Matching Leafs Let l(T) be the label sequence for a tree T in the database. To locate matching leafs, we traverse the prefix tree from the root, following child pointers based on the different items in liT), reach a leaf. This identifies classes whose prefixes have the same label sequence as a subsequence of lit ). This process focuses the search to some leafs of C~, but the subtree topol-ogy for the leaf classes may be completely different. We now have to perform an exact prefix match. In the worst case there may be !~) ~ n ~ subsequences of l(T) that lead to different leafs, however, in practice it is much smaller, since only a small fraction of the leafs match the label sequences, especially as pattern length increases. The time to traverse from the root to a leaf is O(klogm), where m is the average number of distinct labels at an internal node. Total cost of this step is thus O(kn k log m). Prefix Matching Matching the prefix P of a class in a leaf against the tree T is the main step in support counting. Let X[i] denote the ith node of subtree X, and let X[i,... ,j] denote the nodes from positions i to j, with j &gt; i. We use a recursive routine to test prefix matching. At the rth recursive call we maintain the invariant that all nodes in P[0, 1 .... , r] have been matched by nodes in i.e., prefix node P[0] matches T[i0], Pill matches so on, and finally P[r] matches T[ir]. Note that while nodes in P are traversed consecutively, the matching nodes in T can be far apart. We thus have to maintain a stack of node scopes, consisting of the scope of all nodes from the root i0 to the current right-most leaf ir in T. If ir occurs at depth d, then the scope stack has size d + 1. Assume that we have matched all nodes up to the rth node in P. If the next node Pit + 1] to be matched is the child of 
P[r], we likewise search for P[r + 1] under the subtree rooted at T[i,.]. If a match is found at position ir+l in T we push i~+1 onto the scope stack. On the other hand, if the next node P[r + 1] is outside the scope of P[r], and is instead attached to position l (where 0 &lt; l &lt; r), then we pop from the scope stack all nodes i~, where l &lt; k &lt; r, and search for P[r + 1] under the subtree rooted at process is repeated until all nodes in P have been matched. This step takes O(kn) time in the worst case. If each item occurs once it takes O(k + n) time. Element Matching If P -~ T, we search for a match in T for each element i x, k)  X  [P], by searching for x starting at the subtree T[ik-1]. (x, k) is either a descendant or em-bedded sibling of P[k -1]. Either check takes O(1) time. If a match is found the support of the element ix,k) mented by one. If we are interested in support iat least one occurrence in T), the count is incremented only once per tree, or else, if we are interested in weighted support In!! occurrences in T), we continue the recursive process unt|] all matches have been found. 
All experiments were performed on a 500MHz Pentium PC with 512MB memory running RedHat Linux 6,0. Timings are based on total wall-clock time, and include preprocessing costs (such as creating scope-lists for TREEMINER). Figure 8: Distribution of Frequent Trees by Length 
Synthetic Datasets We wrote a synthetic data generation program mimicking website browsing behavior. The pro-gram first constructs a master website browsing tree, ~4), based on parameters supplied by the user. These parame-ters include the maximum fanout F of a node, the maximum depth D of the tree, the total number of nodes M in the tree, and the number of node lab0]s N. We allow multiple nodes in the master tree to have tim same label. The master tree is generated using the following recursive process. At a given node in the tree 142, wc decide how many children to generate. The number of children is sampled uniformly at random from the range 0 to F. Before processing chil-dren nodes, we assign random probabilities to each branch, including an option of backtracking to the node's parent. 
The sum of all the probabilities for a given node is 1. The probability associated with a branch b = ix,y), indicates how likely is a visitor at x to follow the link to y. As long as tree depth is less than or equal to maximum depth 19 this process continues recursively. 
Once the master tree has been created we create as many subtrees of 142 as specified by the parameter T. To generate a subtree we repeat the following recursive process starting at the root: generate a random number between 0 and 1 to decide which child to follow, or to backtrack. If a branch has already been visited, we select one of the other unvis-ited branches, or backtrack. We used the following default values for the parameters: the number of labels N =-100, the number of nodes in the master tree M = 10,000, the maximum depth D = 10, the maximum fanout F = 10 and total number of subtrees T = 100,000. We use three syn-thetic datasets: D10 dataset had all default values, F5 had all values set to default, except for fanout F = 5, and for T1M we set T = 1,000, 000, with remaining default values. 
CSLOGS Dataset consists of web logs files collected over 1 month at the CS department. The logs touched 13361 unique web pages within our department's web site. After processing the raw logs we obtained 59691 user browsing subtrees of the CS department website. The average string encoding length for a user subtree was 23.3. 
Figure 8 shows the distribution of the frequent subtrees by length for the different datasets used in our experiments; all of them exhibit a symmetric distribution. For the lowest minimum support used, the longest frequent subtree in F5 and TIM had 12 and 11 nodes, respectively. For cslogs and D10 datasets the longest subtree had 18 and 19 nodes. 
Performance Comparison Figure 9 shows the performance of PATTERNMATCHER versus TREEMINER. On the real cslogs dataset, we find that TREEMINER is about 2 times faster than PATTERNMATCHER until support 0.5%. At 0.25% sup-lO port TREEMINEFt outperforms PATTERNMATCHER by more than a factor of 20! The reason is that cslogs had a max-imum pattern length of 7 at 0.5% support. The level-wise pattern matching used in PATTERNMATCHER is able to eas-ily handle such short patterns. However, at 0.25% support the maximum pattern length suddenly jumped to 19, and 
PATTERNMATCHER is unable to efficiently deal with such long patterns. Exactly the same thing happens for D10 as well. For supports lower than 0.5% TREEMINER outper-forms PATTERNMATCHER by a wide margin. At the low-est support the difference is a factor of 15. Both TiM and F5 have relatively short frequent subtrees. Here too 
TREEMINER outperforms PATTERNMATCHER, but for the lowest support shown, the difference is only a factor of 4. 
These experiments clearly indicate the superiority of scope-list based-method over the pattern matching method, espe-cially as patterns become long. 
Scaleup Comparison Figure 10 shows how the algorithms scale with increasing number of trees in the database D, from 10,000 to 1 million trees. At a given level of support, we find a linear increase in the running time with increasing number of transactions for both algorithms, though TREEM-INER continues to be 4 times faster than PATTERNMATCHER. 
Effect of Pruning In Figure 11 we evaluated the effect of candidate pruning on the performance of PATTERNMATCHER and TREEMINER. We find that PATTERNMATCHER (de-noted PM in the graph) always benefits from pruning, since the fewer the number of candidates, the lesser the cost of support counting via pattern matching. On the other hand 
TREEMINER (labeled TM in the graph) does not always ben-efit from its opportunistic pruning scheme. While pruning tends to benefit it at higher supports, for lower supports its performance actually degrades by using candidate prun-ing. TREEMINER with pruning at 0.1% support on D10 is 2 times slower than TREEMINER with no pruning. There are two main reasons for this. First, to perform pruning, we need to store Fk in a hash table, and we need to pay the cost of generating the (k -1) subtrees of each new k-pattern. This adds significant overhead, especially for lower supports when there are many frequent patterns. Second, the vertical representation :is extremely efficient; it is actually faster to perform scope-list joins than to perform pruning test. minsup No Pruning Full Pruning Opportunistic 1% 14595 2775 3505 0.5% 70250 10673 13736 0.1% 3555612 i 481234 536496 Table 12 shows the number of candidates generated on the D10 dataset with no pruning, full pruning (in PATTERN-MATCHER), and with opportunistic pruning (in TREEM-INER). Both full pruning and opportunistic pruning are ex-tremely effective in reducing the number of candidate pat-terns, and opportunistic pruning is almost as good as full pruning (within a factor of 1.3). Full pruning cuts down the number of candidates by a factor of 5 to 7! Pruning is es-sential thus for pattern matching methods, and may benefit scope-list method in some cases (for high support). To demonstrate the usefulness of mining complex patterns, we present below a detailed application study on mining usage patterns in web logs. Mining data that has been col-lected from web server log files, is not only useful for study-ing customer choices, but also helps to better organize web pages. This is accomplished by knowing which web pages are most frequently accessed by the web surfers. We use LOGML [16], a publicly available XML application, to describe log reports of web servers. LOGML provides a XML vocabulary to structurally express the contents of the log file information in a compact manner. LOGML docu-ments have three parts: a web graph induced by the source-target page pairs in the raw logs, a summary of statistics (such as top hosts, domains, keywords, number of bytes ac-cessed, etc.), and a list of user-sessions (subgraphs of the web graph) extracted from the logs. There are two inputs to our web mining system: 1) web site to be analyzed, and 2) raw log files spanning many days, or extended periods of time. The web site is used to populate a web graph with the help of a web crawler. The raw logs are processed by the LOGML generator and turned into a LOGML document that contains all the information we need to perform various mining tasks. We use the web graph to obtain the page URLs and their node identifiers. For enabling web mining we make use of user sessions within the LOGML document. User sessions are expressed as sub-graphs of the web graph, and contain complete history of the user clicks. Each user session has a session id (IP or host name), a list of edges (uedges) giving source and target node pairs, and the time (utime) when a link is traversed. An example user session is shown below: &lt;userSession name="pppO-69.ank2.isbank.net.tr" . ..&gt; &lt;uedge source="5938" target="16470" utime="7 : 53 : 46"/&gt; &lt;uedEe source="24755" target="47397" utime="7:57:28"/&gt; &lt;uedEe source="16470" target="24756" utime="7:58:30"/&gt; 
Itemset Mining To discover frequent sets of pages accessed we ignore all link information and note down the unique nodes visited in a user session. The user session above pro-duces a user "transaction" containing the user name, and the node set, as follows: (ppp0-69.ank2.isbank.net.tr, 5938 16470 24754 24755 47387 47397 24756). 
After creating transactions for all user sessions we obtain a database that is ready to be used for frequent set mining. We applied an association mining algorithm to a real LOCML document from CS web site (one day's logs). There were 200 user sessions with an average of 56 distinct nodes in each session. An example frequent set found is shown below. The pattern refers to a popular Turkish poetry site maintained by one of our department members. The user appears to be interested in the poet Akgun Akova. 
Sequence Mining If our task is to perform sequence min-ing, we look for the longest forward links [6] in a user ses-sion, and .generate a new sequence each time a back edge is traversed. We applied sequence mining to the LOGML document from the CS web site. From the 200 user sessions, we obtain 8208 maximal forward sequences, with an average sequence size of 2.8. An example frequent sequence (shown below) indicates in what sequence the user accessed some of the pages related to Akgun Akova. The starting page sair_listesi contains a list of poets. 
Tree Mining For frequent tree mining, we can easily ex-tract the forward edges from the user session (avoiding cy-cles or multiple parents) to obtain the subtree corresponding to each user. For our example user-session we get the tree: (ppp0-69.ank2.isbank.net.tr, 5938 16470 24754 -1 24755 47387 -1 47397-1 -1 24756-1 -1) 
We applied the TreeMiner algorithm to the CS logs. From the 200 user sessions, we obtain 1009 subtrees (a single user session can lead to multiple trees if there are multiple roots in the user graph), with an average record length of 84.3 (including the back edges, -1). An example frequent subtree found is shown below. Notice, how the subtree encompasses all the partial information of the sequence and the unordered information of the itemset relating to Akgun Akova. The mined subtree is clearly more informative, highlighting the usefulness of mining complex patterns. 
We also ran detailed experiments on logs files collected over 1 month at the CS department, that touched a total of 27343 web pages. After processing the LOGML database had 34838 user graphs. We do not have space to shows the results here (we refer the reader to [16] for details), but these results lead to interesting observations that support the mining of complex patterns from web logs. For exam-ple, itemset mining discovers many long patterns. Sequence mining takes longer time but the patterns are more useful, since they contain path information. Tree mining, tough it takes more time than sequence mining, produces very in-formative patterns beyond those obtained from set and se-quence mining. 
Tree mining, being an instance of frequent structure min-ing, has obvious relation to association [3] and sequence [4] mining. Frequent tree mining is also related to tree isomor-phism [18] and tree pattern matching [8]. Given a pattern tree P and a target tree T, with IP[ &lt; ITh the subtree iso-morphism problem is to decide whether P is isomorphic to any subtree of T, i.e., there is a one-to-one mapping from P to a subtree of T, that preserves the node adjacency rela-tions. In tree pattern matching the pattern and target trees are labeled and ordered. We say that P matches T at node v if there exists a one-to-one mapping from nodes of P to nodes of T such that: a) the root of P maps to v, b) if x maps to y, then x and y have the same labels, and c) if x maps to y and x is not a leaf, then the ith child of x maps to the ith child of y. Both subtree isomorphism and pattern matching deal with induced subtrees, while we mine embed-ded subtrees. Further we are interested in enumerating all common subtrees in a collection of trees. The tree inclusion problem was studied in [13], i.e., given labeled trees P and T, can P be obtained from T by deleting nodes? This problem is equivalent to checking if P is embedded in T. The paper presents a dynamic programming algorithm for solving or-dered tree inclusion, which could potentially be substituted for the pattern matching step in PATTERNMATCHER. How-ever, PATTERNMATCHER utilizes prefix information for fast subtree checking, and its three step pattern matching is very efficient over a sequence of such operations. There has been very little previous work in mining all fre-quent subtrees. In a recent paper, Asai et al. [5] presented FREQT, an apriori-like algorithm for mining labeled or-dered trees; they independently proposed a candidate gen-eration scheme similar to ours. Wang and Liu [20] devel-oped an algorithm to mine frequently occurring subtrees in XML documents. Their algorithm is also reminiscent of the level-wise Apriori [3] approach, and they mine induced sub-trees only. A related problem of accurately estimating the number of matches of a small node-labeled tree in a large labeled tree, in the context of querying XML data, was pre-sented in [7]. They compute a summary data structure, and then give frequency estimates based on this summary, rather than using the database for exact answers. In contrast we are interested in exact frequency of subtrees. Furthermore, their work deals with traditional (induced) subtrees, while we mine embedded subtrees. With the advent of XML as a data representation and ex-change standard, there has been active work in indexing and querying XML documents [15, 23, 2, 11], which are mainly tree (or graph) structured. To efficiently answer ancestor-descendant queries various node numbering schemes similar to ours have been proposed [15, 23, 1]. Other work has looked at path query evaluation that uses local knowledge within data graph based on path constraints [2] or graph schemas [11]. The major difference between these works and ours is that instead of answering user-specified queries based on regular path expressions, we are interested in finding all frequent tree patterns among the documents. There has been recent work in mining frequent graph pat-terns. The AGM algorithm [12] discovers induced (possibly disconnected) subgraphs. The FSM algorithm [14] improves upon AGM, and mines only the connected subgraphs. Both methods follow an Apriori-style level-wise approach. If one were to use ACM or FSM for tree mining one would dis-cover only induced subtrees. In contrast we discover em-bedded subtrees. Also there are important differences in graph mining and tree mining. Our trees are rooted, and thus have a unique ordering of the nodes based on depth-first traversal. In contrast graphs do not have a root, and allow cycles. For mining graphs the methods above first ap-ply an expensive canonization step to transform graphs into a uniform representation. This step is unnecessary for tree mining. Graph mining algorithms are likely to be overly general (thus not efficient) for tree mining. Our approach utilizes the tree structure for efficient enumeration. The work by Dehaspe et al [10] describes a level-wise Induc-tive Logic Programming based technique to mine frequent substructures (subgraphs) describing the carcinogenesis of chemical compounds. They reported that mining beyond 6 predicates was unfeasible due to the complexity of the subgraph patterns. The SUBDUE system [9] also discovers graph patterns using the Minimum Description Length prin-ciple. An approach termed Graph-Based Induction (GBI) was proposed in [21], which uses beam search for mining sub-graphs. However, both SUBDUE and GBI may miss some significant patterns, since they perform a heuristic search. We perform a complete (but not exhaustive) search, which guarantees that all patterns are found. In contrast to these approaches, we are interested in developing efficient algo-rithms for tree patterns. In this paper we introduced the notion of mining embed-ded subtrees in a (forest) database of trees. Among our novel contributions is the procedure for systematic candi-date subtree generation, i.e., no subtree is generated more than once. We utilized a string encoding of the tree that is space-efficient to store the horizontal dataset, and we use the notion of a node's scope to develop a novel vertical rep-resentation of a tree called scope-lists. Our formalization of the problem is flexible enough to handle several variations. For instance, if we assume the label on each node to be the same, our approach mines all unlabeled trees. A simple change in the candidate tree extension procedure allows us to discover sub-forests (disconnected patterns). Our formu-lation can find frequent trees in a forest of many trees or all the frequent subtrees in a single large tree. Finally, it is rel-atively easy to extend our techniques to find unordered trees (by modifying the out-scope test) or to use the traditional definition of a subtree. To summarize, this paper proposes a framework for tree mining which can easily encompass most variants of the problem that may arise in different domains. We introduced a novel algorithm, TREEMINER, for tree min-ing. TREEMINER uses depth-first search; it also uses the novel scope-list vertical representation of trees to quickly compute the candidate tree frequencies via scope-list joins based on interval algebra. We compared its performance against a base algorithm, PATTERNMATCHER. Experiments on real and synthetic data confirmed that TREEMINER out-performs PATTERNMATCHER from a factor of 4 to 20, and scales linearly in the number of trees in the forest. We stud-led an application of TREEMINER in web usage mining. For future work we plan to extend our tree mining frame-work to incorporate user-specified constraints. Given that tree mining, though able to extract informative patterns, is an expensive task, performing general unconstrained mining can be too expensive and is also likely to produce many pat-terns that may not be relevant to a give user. Incorporating constraints is one way to focus the search and to allow in-teractivity. We also plan to develop efficient algorithms to mine maximal frequent subtrees from dense datasets which may have very large subtrees. Finally, we plan to apply our tree mining techniques to other compelling applications, such as finding common tree patterns in RNA structures within bioinformatics, as well as the extraction of structure from XML documents and their use in classification, clus-tering, and so on. [1] S. Abitebou], H. Kaplan, and T. Milo. Compact labeling [2] S. Abiteboul and V. Vianu. Regular path expressions with [3] R. Agrawal et al. Fast discovery of association rules. In [4] R. Agrawal and R. Srikant. Mining sequential patterns. In [5] T. Asal, K. Abe, S. Kawasoe, H. Arimura, H. Satamoto, [6] M.S. Chen, J.S. Park, and P.S. Yu. Data mining for path [7] Z. Chen et al. Counting twig matches in a tree. In 17th [8] R. Cole, R. Hariharan, and P. Indyk. Tree pattern matching [9] D. Cook and L. Holder. Substructure discovery using [10] L. Dehaspe, H. Toivonen, and R. King. Finding frequent [11] M. Fernandez and D. Suciu. Optimizing regular path [12] A. Inokuchi, T. Washio, and H. Motoda. An apriori-based [13] P. Kilpelalnen and H. Mannila. Ordered and unordered tree [14] M. Kuramochi and G. Karypis. Frequent subgraph [15] Q. Li and B. Moon. Indexing and querying XML data for [16] J. Punin, M. Krishnamoorthy, and M. Zaki. LOGML: Log [17] R. Cooley, B. Mobasher, and J. Srivastava. Web Mining: [18] R. Shamir and D. Tsur. Faster subtree isomorphism. [19] B. Shapiro and K. Zhang. Comparing multiple rna [20] K. Wang and H. Liu. Discovering typical structures of [21] K. Yoshida and H. Motoda. CLIP: Concept learning from [22] M. J. Zaki. Efficiently mining trees in a forest. Tech. [23] C. Zhang et al. On supporting containment queries in 
