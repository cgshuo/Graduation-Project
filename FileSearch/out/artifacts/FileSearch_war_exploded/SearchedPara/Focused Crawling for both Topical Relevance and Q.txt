 Subject-specific search facilities on health sites are usually built using manual inclusion and exclusion rules. These can be expensive to maintain and often provide incomplete cov-erage of Web resources. On the other hand, health infor-mation obtained through whole-of-Web search may not be scientifically based and can be potentially harmful.
To address problems of cost, coverage and quality, we built a focused crawler for the mental health topic of depression, which was able to selectively fetch higher quality relevant in-formation. We found that the relevance of unfetched pages can be predicted based on link anchor context, but the qual-ity cannot. We therefore estimated quality of the entire link-ing page, using a learned IR-style query of weighted single words and word pairs, and used this to predict the quality of its links. The overall crawler priority was determined by the product of link relevance and source quality.
We evaluated our crawler against baseline crawls using both relevance judgments and objective site quality scores obtained using an evidence-based rating scale. Both a rel-evance focused crawler and the quality focused crawler re-trieved twice as many relevant pages as a breadth-first con-trol. The quality focused crawler was quite effective in re-ducing the amount of low quality material fetched while crawling more high quality content, relative to the relevance focused crawler.

Analysis suggests that quality of content might be im-proved by post-filtering a very big breadth-first crawl, at the cost of substantially increased network traffic. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  information filtering, retrieval models experimentation, performance, measurement quality health search, focused crawling, domain-specific search
A survey of US Internet users found that forty percent of respondents used the Internet to find advice or informa-tion about health or health care [2]. However, other studies have shown that medical information on the Internet can be fraudulent, of dubious quality and potentially dangerous [18, 23].

It is desirable that a search service over health web sites should return results which are not only relevant to the query but in accord with evidence-based medical guidelines. Health experts, based on either scientific evidence or ac-countability criteria, have developed protocols for manual assessment of medical web site quality [12, 8]. However, there is very little prior work on using automated quality assessments, either in determining what to index or how to rank potential search results. One exception, due to Price and Hersh [21], reranks results from general search engines based on automated ratings of relevance, credibility, absence of bias, content currency and value of links.

ANU X  X  Centre for Mental Health Research operates a web site 1 which publishes evidence-based information on depres-sive illness and also provides integrated search of over 200 depression sites. Currently, the set of indexed sites is man-ually maintained, using a seed list and URL-based inclusion rules that determine which parts of each site are indexed. Here we report our experiences in developing a fully auto-matic alternative, using a focused-crawler that takes into account relevance and quality. bluepages.anu.edu.au/
The ultimate measure of the quality of a health web site is its effect on health outcomes but it is not usually feasible for website publishers or visitors to obtain that information. Next best would be an assessment of the extent to which the content of the site is consistent with the best available scientific evidence  X  evidence-based medicine  X  but deter-mining this requires expert raters.

Therefore in the present study, experts rate our crawled sites on a 21-point scale derived by Griffiths and Christensen [13]. These ratings are based on a set of evidence-based depression guidelines published by the Centre for Evidence Based Mental Health (CEBMH) [5].

There are also rating schemes for non-experts such as Sil-berg [27] and DISCERN [8]. They focus on accountability criteria which could be measured by people without exten-sive medical expertise, such as whether the author is identi-fied and whether the site has been recently updated. How-ever, a study of depression web sites by Griffiths and Chris-tensen [12] found no correlation between Silberg scores and expert evidence-based ratings. The latter was found to be correlated with DISCERN scores [14], but carrying out such manual assessments is a lengthy process.
 In the Web search literature, link graph measures such as PageRank [4] have been promoted as indicators of quality, but how this type of quality might correlate with a medical definition has been little studied. A very recent study by Griffiths and Christensen [14] found only a moderate corre-lation between Google-reported PageRank and the 21-point rating scale. In this study we follow a content-based ap-proach.
Relevance feedback (RF) is a well-known IR approach of  X  X uery by example X . Given example sets of relevant docu-ments, the goal is to find more of the same. In this paper, we use this both to identify depression-relevant pages and high-quality depression-relevant pages. Our specific application of RF is described in more detail in Section 3.1. We applied Robertson X  X  approach to term selection [24]. In this approach, there are three ways to calculate the selec-tion value for a term: using the probability of the term oc-curring in a relevant document ( r/R ), rewarding terms that occur frequently in relevant documents ( the average of these. We used the third approach, comput-ing the selection value of a term Q t as: where R is the number of known relevant documents, r is the number of documents in R that contain term t and tf is the frequency of occurrence of the term within a document. The weight w was calculated using the Robertson-Sparck Jones weight [25]: where N is the number of documents in the collection, n is the number of documents containing a specific term; and R and r are defined as above.
First introduced by de Bra et al. [3], and subsequently studied by many others [6, 9, 16], focused crawlers are de-signed to selectively fetch content relevant to a specified topic of interest using the Web X  X  hyperlink structure. A focused crawler starts from a seed list of topical URLs. It estimates the likelihood that each subsequent candidate link will lead to further relevant content, and may priori-tise crawling order on that basis and/or reject low-likelihood links. Evidence such as link anchortext, URL words and source page relevance are typically exploited in estimating link value.

McCallum et al. [20] used Naive Bayes classifiers to cate-gorise hyperlinks while Diligenti et al. [11] used the context-graph idea to guide a focused crawler. Rather than examin-ing relevant nodes alone, both techniques trained a learner with features collected from paths leading up to the relevant nodes.

Chakrabarti et al. [6], on the other hand, used hypertext graphs including in-neighbours (documents citing the tar-get document) and out-neighbours (documents that target document cites) as input to some classifiers. According to these authors, a focused crawler can acquire relevant pages steadily while a standard crawler quickly indexes a large number of irrelevant pages and loses its way, even though they started from the same seed list.
In this section we introduce three crawlers. First we de-scribe our use of relevance feedback to estimate quality of pages, then our classifier to compute relevance scores for links. Finally we describe the crawlers: breadth-first, relevance-focused and quality-focused.
A quality-focused crawler needs some way of predicting the quality of uncrawled URLs, to set its priority. We tried various methods to predict this, using as training data quality-judged depression pages from a previous study [28]. We found it impossible to predict the quality of a link tar-get based on its anchor context alone, so we abandoned at-tempts to score each link separately. Instead we scored the quality of the whole page and applied this equally to the page X  X  outlinks.

We used relevance feedback to predict page quality. RF was a natural choice here, because a focused crawling frame-work needs to prioritise the crawling order, and RF gives us scores that can be used in ranking. We also made separate use of relevance feedback in scoring topic relevance for eval-uation purposes only. Both quality RF and relevance RF are described in this section. Both use the term selection meth-ods described in Section 2.2 to identify extra query words and phrases. Phrases usually include two adjacent words, but sometimes three words if the middle word is a preposi-tion, for example  X  X reatment of depression X .
Using relevance judgments from a previous experiment [28], we selected 347 relevant and 9000 irrelevant documents. We applied the Robertson selection value formula to obtain weights for all the terms in relevant documents. Past re-Table 1: Examples of terms in the relevance query.
Term Weight ( Q t ) Term Weight ( Q t ) depression 15 anxiety 2.6 health 6.9 medication 2.4 mental 5.4 cognitive 2.1 treatment 3.3 patient 1.8 therapy 2.7 symptoms 1.8 Table 2: Examples of terms in the quality query.
 search has suggested that the number of terms that could be usefully added to expand a query might range from 20 to 40 [15]. We arbitrarily selected 20 top weighted single words and 20 top weighted phrases. See examples in Table 1.
From the same previous experiment we identified 107 doc-uments relevant to depression and of high quality, and an-other set of 3002 documents which were either irrelevant or relevant but not of high quality.

We used the same technique as for the relevance query to produce two candidate term lists: one containing single words and the other containing phrases. However, we used a more sophisticated procedure to choose a term selection cutoff.

We first derived a list of words and phrases representing effective depression treatments from [13]. Multi-word treat-ments were divided into two-word phrases (e.g.  X  X ognitive behaviour therapy X  would become  X  X ognitive behaviour X  and  X  X ehaviour therapy X ) in order to match the two-word phrases in the relevance and quality queries described above. We then located these words and phrases in the candidate lists and cut off the lists just after the lowest-ranked occurrence of an effective treatment term. Surprisingly this gave us the same cutoff (20) for phrases and a similar cutoff for single words (29). Some example terms are shown in Table 2.
Note that the two queries include many terms in common, because both are on the topic of depression. High-quality depression-relevant documents are a subset of depression-relevant documents. The quality query contains more words relating to effective treatment methods such as  X  X ognitive therapy X  or antidepressant medications like  X  X oloft X  and  X  X axil X .
We used the Okapi BM25 weighting function [26] to score documents against the two weighted queries: where tf d is the number of times term t occurs in document d , N is the number of documents in the collection, n t is the number of documents containing t , dl is the length of the document and avdl is the average document length. Scores calculated with BM25 are collection dependent. Rather than assuming a collection of the documents crawled thus far, we chose to assume a more general web context and used values for the collection parameters ( N =2 , 376 , 673, avdl = 15 , 036 and n t ) which were derived from a large gen-eral crawl of Australian educational websites. The values for n t varied depending on what term t was used.

The final score was computed using the following equa-tion: where Q t is obtained from equation 1 and w t from equa-tion 2. These scores represented either quality or relevance depending on the query.
In our previous work we developed a classifier for predict-ing the relevance of a link target, based on features in the link X  X  source page [29]. We evaluated a number of learn-ing algorithms provided by the Weka package [30], such as k-nearest neighbor, Naive Bayes, and C4.5. Since then we also evaluated Perceptron. The C4.5 decision tree [22] was the best amongst those evaluated.

The classifier is based on words in the anchor text, words in the target URL and words in the 50 characters before and after the link (link context). If we found multiple links to the same URL, we included all available anchor contexts. This is a relatively standard approach [1, 9, 7].
To produce a confidence score at each leaf node of the decision tree we used a Laplace correction formula [19]: where N is the total number of training examples that reach the leaf; N k is the number of training examples from class k reaching the leaf; K is the number of classes and  X  k is the prior for class k and is usually set to be 1. In our case, K is 2 because we only had two classes, relevance and irrelevance.
We used the quality score of a page (computed using rele-vance feedback) to predict the quality of its outlinks. If more than one known page linked to the same URL, we took the mean quality score of the linking pages. Relevance scores computed from the decision tree were already aggregated across links.

To order the crawl queue for the quality crawler, we com-bined the quality and relevance scores. The overall score for aURLwasgivenby: where confidence level rel is the URL X  X  relevance score (equa-tion 4), DScore i using the quality query is a linking page X  X  quality score (equation 3), and m is the number of pages linking to the URL.

A side effect of taking the product is that if one of the two scores is zero, the overall priority score is zero. The decision to multiply the scores was taken arbitrarily. We plan to investigate different options for balancing rele-vance and quality in future work, including dispensing with the relevance component altogether. It is possible that this may lead to significant improvement.
We evaluated three crawlers: the breadth-first (BF) crawler, the relevance crawler, and the quality crawler. When a crawler encounters a new URL that URL is added to a crawl queue, and the crawler proceeds by taking URLs from that queue. The crawlers differ in how their crawl queues are prioritised.

The BF crawler serves as a baseline for comparison. It traverses the link graph in a breadth-first fashion, placing each newly discovered URL in a FIFO queue. This crawler is likely to find some depression pages since we start it from depression-relevant seeds, but we would expect the relevance of its crawl to fall as the crawl progresses.

The relevance crawler is designed to prefer domain-relevant pages, ordering its crawl queue using the relevance decision tree discussed in Section 3.2. The relevance RF score is not used, we reserve it for use in evaluation. By crawling the highest-scoring URLs first, we would expect the relevance crawler to maintain its overall relevance more successfully than the BF crawler.

The quality crawler is designed to prefer higher-quality domain-relevant pages. Each URL is given a score that was computed using equation 5. A major focus of this paper is to evaluate whether the quality crawler can successfully prioritise its queue to maintain the overall quality of its crawl and avoid pages with low quality, potentially harmful advice (with respect to depressive illness).
We used our RF relevance score (applying the relevance query in equation 3), and a score threshold to evaluate the overall relevance of our three crawls. The threshold was found using 1000 relevant and 1000 irrelevant pages from our previous study (these were separate from those used to generate the relevance query). A threshold at 25% of the theoretical maximum BM25 score (of 502.88 2 ) minimised the total number of false positives and false negatives, so in our crawls we labeled pages with RF relevance score greater than this threshold as RF-relevant.

Using RF scores rather than real relevance judgments al-lows us to get some idea of relevance without extensive rel-evance judging. However, to validate the accuracy of our RF-based  X  X udgments X , we employed two lay relevance as-sessors 3 to judge the relevance of 300 RF-relevant and 120 RF-irrelevant pages. These pages were randomly selected from all the RF results of all the crawled pages. As for the judging criterion, any page about the mental illness  X  X epres-sion X  was considered relevant.

The level of agreement between the two assessors was high (91.2%) indicating that judging for such a simple topic is easy. The RF-judgments had an accuracy of 89.3%, a
Corresponding to a hypothetical zero-length document containing infinite numbers of each of the query terms. university research assistants 90.9% success rate in predicting the relevance category, and a 84.6% success rate in predicting the irrelevance category. We concluded that these levels were high enough to present some RF-judgment-based results.

Note that this RF classifier was only used in evaluating the relevance of sets of pages returned by the various crawlers. None of these three crawlers used this classifier in deciding priorities of links for crawling.

We evaluated relevance of the three crawlers, each starting from a seed set of 160 URLs taken from the DMOZ depres-sion directory 4 . We evaluated the first 10,000 pages from each crawler according to RF-relevance.
Most of the models for assessing the quality of depression content on the Web refer to the entire sites, not individual pages [8, 17]. We therefore grouped all the pages in each crawl into sites. Pages originated from the same host names were considered to be from the same sites.

The quality of the sites was evaluated by a research as-sistant from the Centre for Mental Health Research using a rating scale derived by Griffiths and Christensen [13] from the CEBMH evidence-based clinical guidelines. Each site was assigned a quality score in the range 0 to 20.
Since judging took 4 hours per site on average, we could not use the full 160 page seed list. If we did, a large amount of effort would be needed just to judge seeds, and these are uninformative with respect to crawl strategy. Therefore we randomly selected 18 URLs from the 160 to use as our quality experiment seeds. We cut off each of our three crawls at 3,000 pages. For this small crawl size, we were able to judge the quality of any site with six or more crawled pages in all the crawls.

We propose three measures to compare crawl quality. Note that, in our measures, the quality score of a page is assigned the quality score of the site containing it. http://www.dmoz.org/Health/Mental_Health/ Disorders/Mood/Depression/ Figure 1: Comparison of the BF, relevance and qual-ity crawlers for relevance using the R Fclassifier. Figure 2: Quality score for each crawl based on all pages from judged sites.
Figure 1 depicts the relevance levels throughout each of our three crawls, based on RF relevance judgments. The rel-evance and quality crawls each stabilised after 3,000 pages, at about 80% and 88% relevant respectively. The breadth first crawler continued to degrade over time as it got further from the DMOZ depression seeds. At 10,000 pages it was down to 40% relevant and had not yet stabilised.

The quality crawler outperformed the relevance crawler, and this must be due to the incorporation of the quality RF score. Noticing this, we performed an additional crawl using relevance RF in place of quality RF, and achieved compa-rable results to the quality crawler. This indicates that RF scores can offer a small improvement in crawl relevance, on top of our relevance decision tree, with the caveat that, in this case only, we used RF techniques both to predict which links to follow and to evaluate relevance of crawled pages.
Our overall conclusion on relevance is simply that our fo-cused crawlers succeed in maintaining relevance as crawls progress.
The quality scores based on all pages from judged sites are shown in Figure 2. All three crawlers achieved positive quality scores. This means they crawled more pages from higher-quality sites than lower-quality ones. Although this is surprising in the case of the breadth first crawler, it may be because higher-quality sites are simply larger. To explore this, we fully crawled ten AAQ sites and ten BAQ sites, all of which were randomly selected. We found that, on average, a BAQ site had 56.6 pages while an AAQ site had 450.2 pages, about eight times higher.

The main finding is that the quality crawler, using the quality RF scores of known link sources to predict the qual-ity of the target, was able to significantly outperform the relevance crawler. Towards the end of the crawls its total quality was over 50% better than that of the relevance crawl.
Figure 3 shows the same total quality scores, but this time only counting pages judged relevant by our RF classifier. The results were similar to the previous figure, particularly for the quality crawler, so we concluded that the presence of irrelevant pages was not a major factor in quality evalua-tion. The relevance and quality crawlers suffered a little with the elimination of some irrelevant pages from higher-quality sites, whereas the breadth-first crawler benefited from the elimination of irrelevants from lower-quality sites. Now we focus on the AAQ and BAQ categories.

An interesting set of pages are those that are from AAQ sites and are RF-judged to be relevant. These are the pages we would expect to be most useful in our domain-specific engine. Figure 4 shows the number of these pages in each crawl over time. The quality crawler performed very well, with more than 50% of its pages being AAQ and relevant. The other two performed well too, with over 25% of their pages in that category.

Figure 5 shows the number of pages from BAQ sites, re-gardless of relevance. The breadth first crawler was much worse on this count than the other two, with two or three times more BAQ pages than the other two. In the quality crawl, only about 5% of the pages were from BAQ sites, and this in combination with the 50% AAQ result underlines the success of the crawler.

Note that the number of AAQ pages was higher than the number of BAQ pages even in the BF crawl. The BF crawler benefited from the seed list in its early stages  X  we found that the seed list has 4 BAQ but 7 AAQ URLs  X  and also from the relative sizes of AAQ and BAQ sites. However, in larger crawls the influence of the seed list would become less, and focus would become increasingly important.
We ran two additional experiments using our quality judg-ments. One measured the  X  X uality locality X  of linkage be-tween judged sites. The other considered what happens if we post-filter our crawls using our quality scoring formula (equation 3) on the text of the crawled pages, dropping low-quality pages from the system.
Topic locality experiments described in [10] indicated that pages typically link to pages with similar content. For a quality-focused crawler to function effectively we hope there is also  X  X uality locality X . More specifically it would be helpful if higher-quality sites tend to link to each other, making it easier for the crawler to identify more of the same.
We did a breadth first crawl of 100,000 pages starting from the 160 seed URLs on depression. Using these crawled Figure 3: Quality score for each crawl based on rel-evant pages from judged sites. Figure 4: Number of relevant and above-average-quality pages in each crawl. Figure 5: Number of all below-average-quality pages in each crawl.
 Table 3: Quality locality analysis according to the link structure between source sites and target sites for a 100,000 page B Fcrawl.
 Figure 6: Quality score for each crawl at different filtering points.
 Table 4: A comparison of quality scores between the quality crawl and each of the post-filtering B Fcrawls of different sizes. The number of judged pages were set to 2737, which was the number of pages from judged sites in the quality crawl.
 pages, we identified all links between sites, including links to URLs that were not yet crawled. We then analysed link-age between our 114 judged depression sites, in particular calculating the average number of sites of each type linking to sites of other types (Table 3). For example, on average each AAQ site had links from 2.53 AAQ sites, 1.92 AQ sites and 0.92 BAQ sites.

If quality locality were a direct analogue of topic locality, we might expect to see a cluster of AAQ sites linking to each other and another cluster of BAQ sites. What we observed in the linkage between judged sites was a tendency to link to AAQ sites, even amongst links from BAQ sites. This means that no matter which judged site is crawled, the crawler is most likely to find AAQ-site links. We also observed that higher-quality sites had more outlinks. We conclude that the observed link patterns are favourable for quality-focused crawling.
We observed pages from BAQ sites in all three crawls (Fig-ure 5). An alternate way of using our RF quality scores is to post-filter our crawls, removing pages with quality scores below some threshold. The question is whether filtering a crawl by RF quality score can improve its overall human-judged quality rating.

In our first post-filtering experiment we progressively ap-plied a stronger filter to our three main crawls (Figure 6). Because below-the-mean sites received negative scores in our scoring system, we expected an increase in total quality scores at certain thresholds where more low quality pages were filtered out. However, we were unable to improve the quality crawl or the relevance crawl by post-filtering. These crawls already had good overall quality, and our RF quality score was not sufficient to improve on that. We observed some improvement in the breadth first crawl, but it did not overtake the other crawlers.

Since the BF crawler was able to be improved by post-filtering, our second experiment filtered successively larger breadth-first crawls, to see if the quality-focused crawl could be surpassed. The quality crawl contained 2,737 pages from judged sites, so for each BF crawl we set the filtering thresh-old to give us 2,737 pages from judged sites. Note, this threshold also gave us a large number of pages from un-judged sites, adding some uncertainty to the quality rating.
Table 4 shows the results of the experiment. To surpass the quality rating of the quality crawler we had to increase the breadth-first crawl size to 25,000 pages, compared to 3,000 pages for the quality-focused crawl. This means that if an appropriate threshold can be set and a massive in-crease in crawl traffic and server load is acceptable, a filtered breadth first crawler is an alternative to a quality-focused crawler. However, certainly at an Australian university that pays over AUD20 per gigabyte of traffic, some focus is de-sirable.
 Finally, there are some experiments we did not perform. We did not consider how the quality score could be incorpo-rated as a ranking feature, at query time. We do not have the necessary per-query relevance and quality judgments to do this. Also we did not consider post-filtering using the RF relevance score. Again, we do not have the necessary human judgments to carry out this experiment. Furthermore, stan-dard IR systems are robust to having irrelevant documents in the crawl and the harm caused by retrieving one is low, so we believe quality filtering is the more important case.
Subject-specific search facilities on health sites are usu-ally built using manual inclusion and exclusion rules, which require a lot of human effort in building and maintenance. We have designed and built a fully automatic quality focused crawler for a mental health topic of depression, which was able to selectively crawl higher quality and relevant content. Our work has resulted in four key findings.

First, domain relevance on depression could be well pre-dicted using link anchor context. A relevance-focused crawler based on this information fetched twice as many relevant pages as a breadth-first control. A combination of link an-chor context and source-page relevance feedback improved the prediction slightly further.

Second, link anchor context alone was not sufficient to predict quality of Web pages. Instead, relevance feedback technique proved useful. We used this technique to learn and derive a list of terms representing high quality content from a small set of training data, which was then scored against crawled source pages to predict the quality of the targets. Compared to the relevance and BF crawls, a quality crawl using this approach obtained a much higher total quality score, significantly more relevant pages from high quality sites and fewer pages from low quality sites.

Third, analysis on quality locality suggested that above average quality depression sites tended to have more incom-ing links and outgoing links compared to other types of site. This observed link pattern is favourable for quality focused crawling, explaining in part why it was able to succeed.
Fourth, quality of content might be improved by post-filtering a very big breadth-first crawl if an appropriate fil-tering threshold is set. This leads to a trade-off decision be-tween cost and efficiency. The post-filtering approach could be adopted in cases where a massive increase in crawl traffic and server load is acceptable. Although we could not im-prove our other two crawlers by filtering, it might hypothet-ically be possible to do so in a larger-scale experiment, and this would be a less wasteful approach than all-out breadth first crawling.

Given the interesting results that we found, there is ob-vious follow-up work to be done on focused crawling. In particular, it would be interesting to compare our quality crawl with other depression-specific search portals and gen-eral search engines in terms of relevance and quality by run-ning queries against these engines and measuring the results.
Another question would be whether we could improve our quality focused crawler. The current approach evalu-ated links on page basis. Possibly, another quality focused crawler working on site basis, (by accumulating the qual-ity scores of all the crawled pages from the same sites, and crawling new pages according to the predicted quality score of the site containing them) could achieve even better re-sults.

Investigation of whether our findings generalise to other health domains (characterised by an evidence-based notion of quality) or more generally is left for future work.
We gratefully acknowledge the assistance of Alistair Ren-dell and Helen Christensen for seeking financial support for the project and the effort of our relevance and quality judges Sonya Welykyj, Michelle Banfield and Alison Neil. [1] C. C. Aggarwal, F. Al-Garawi, and P. S. Yu. On the [2] L. Baker, T. H. Wagner, S. Singer, and M. K.
 [3] P. D. Bra, G. Houben, Y. Kornatzky, and R. Post. [4] S. Brin and L. Page. The anatomy of a large-scale [5] CEBMH. A systematic guide for the management of [6] S. Chakrabarti, M. Berg, and B. Dom. Focused [7] S. Chakrabarti, B. Dom, P. Raghavan, [8] D. Charnock, S. Shepperd, G. Needham, and [9] J. Cho, H. Garcia-Molina, and L. Page. Efficient [10] B. D. Davison. Topical locality in the web. In Procs. [11] M. Diligenti, F. M. Coetzee, S. Lawrence, C. L. Giles, [12] K. Griffiths and H. Christensen. Quality of web based [13] K. Griffiths and H. Christensen. The quality and [14] K. Griffiths, H. Christensen, and S. Blomberg. [15] D. Harman. Towards interactive query expansion. In [16] M.Hersovici,M.Jacovi,Y.S.Maarek,D.Pellegb, [17] A. R. Jadad and A. Gagliardi. Rating health [18] R. Kiley. Quality of medical information on the [19] D. D. Margineantu and T. G. Dietterich. Improved [20] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [21] S. L. Price and W. R. Hersh. Filtering web pages for [22] J. R. Quinlan. C4.5: programs for machine learning . [23] A. Risk and J. Dzenowagis. Review of internet health [24] S. E. Robertson. On term selection for query [25] S. E. Robertson and K. S. Jones. Relevance weighting [26] S. E. Robertson, S. Walker, S. Jones, [27] W. M. Silberg, G. D. Lundberg, and R. A. Musacchio. [28] T. T. Tang, N. Craswell, D. Hawking, K. M. Griffiths, [29] T.T.Tang,D.Hawking,N.Craswell,andR.S.
 [30] I. H. Witten and E. Frank. Data Mining: Practical
