 morariu@umd.edu, balajiv@umiacs.umd.edu, vikas.raykar@siemens.com, Gaussian summations occur in many machine learning algorithms, including kernel density esti-mation [1], Gaussian process regression [2], fast particle smoothing [3], and kernel based machine learning techniques that need to solve a linear system with a similarity matrix [4]. In such algorithms, q is the weight associated with x i ; and h is the bandwidth . Straightforward computation of the above sum is computationally intensive, taking O ( M N ) time.
 To reduce the computational complexity, Greengard and Strain proposed the Fast Gauss Transform (FGT) [5], using two expansions, the far-field Hermite expansion and the local Taylor expansion , and a translation process that converts between the two, yielding an overall complexity of O ( M + N ) . However, due to the expensive translation operation, O ( p d ) constant term, and the box based data structure, this method becomes less effective for higher dimensions (e.g. d &gt; 3 ) [6]. Dual-tree methods [7, 8, 9, 10] approach the problem by building two separate trees for the source and target points respectively, and recursively considering contributions from nodes of the source tree to nodes of the target tree. The most recent works [9, 10] present new expansions and error control schemes that yield improved results for bandwidths in a large range above and below the op-timal bandwidth, as determined by the standard least-squares cross-validation score [11]. Efficiency across bandwidth scales is important in cases where the optimal bandwidth must be searched for. Another approach, the Improved Fast Gauss Transform (IFGT) [6, 12, 13], uses a Taylor expansion and a space subdivision different than the original FGT, allowing for efficient evaluation in higher dimensions. This approach also achieves O ( M + N ) asymptotic computational complexity. How-ever, the approach as initially presented in [6, 12] was not accompanied by an automatic parameter selection algorithm. Because the parameters interact in a non-trivial way, some authors designed simple parameter selection methods to meet the error bounds, but which did not maximize perfor-mance [14]; others attempted, unsuccessfully, to choose parameters, reporting times of  X   X   X  for IFGT [9, 10]. Recently, Raykar et al [13] presented an approach which selects parameters that mini-mize the constant term that appears in the asymptotic complexity of the method, while guaranteeing that error bounds are satisfied. This approach is automatic, but only works for uniformly distributed sources, a situation often not met in practice. In fact, Gaussian summations are often used because a simple distribution cannot be assumed. In addition, the IFGT performs poorly at low bandwidths because of the number of Taylor expansion terms that must be retained to meet error bounds. We address both problems with IFGT: 1) small bandwidth performance, and 2) parameter selection. First we employ a tree data structure [15, 16] that allows for fast neighbor search and greatly speeds up computation for low bandwidths. This gives rise to four possible evaluation methods that are chosen based on input parameters and data distributions: direct evaluation, direct evaluation using tree data structure, IFGT evaluation, and IFGT evaluation using tree data structure (denoted by direct , direct+tree , ifgt , and ifgt+tree , respectively). We improve parameter selection by removing the assumption that data is uniformly distributed and by providing a method for selecting individual source and target truncation numbers that allows for tighter error bounds. Finally, we provide an algorithm that automatically selects the evaluation method that is likely to be fastest for the given data, bandwidth, and error tolerance. This is done in a way that is automatic and transparent to the user, as for other software packages such as FFTW [17] and ATLAS [18].The algorithm is tested on several datasets, including those in [10], and in each case found to perform as expected. We briefly summarize the IFGT, which is described in detail [13, 12, 6]. The speedup is achieved by employing a truncated Taylor series factorization, using a space sub-division to reduce the number of terms needed to satisfy the error bound, and ignoring sources whose contributions are negligible. P i | q i | . The factorization that IFGT uses involves the truncated multivariate Taylor expansion e where  X  is multi-index notation 1 and  X  ij is the error induced by truncating the series to exclude terms of degree p and higher and can be bounded by Because reducing the distance || x i  X  x  X  || also reduces the error bound given above, the sources can be divided into K clusters, so the Taylor series center of expansion for source x i is the center of the cluster to which the source belongs. Because of the rapid decay of the Gaussian function, the c and r k x are cluster center and radius of the k th cluster, respectively.
 In [13], the authors ensure that the error bound is met by choosing the truncation number p i for each Because || y j  X  c k || cannot be computed for each  X  ij term (to prevent quadratic complexity), the error term  X  ij is maximized at d  X  jk = d ik + targets further than r k y from c k will not consider cluster k ). Figure 1: The four evaluation methods. Target is displayed elevated to separate it from sources. The algorithm proceeds as follows. First, the number of clusters K , maximum truncation number p max , and the cut-off radius r are selected by assuming that sources are uniformly distributed. Next, S , . . . , S k . Using the max cluster radius r x , the truncation number p max is found that satisfies worst-case error bound. Choosing p i for each source x i so that  X  ij  X   X  , source contributions are accumulated to cluster centers: from those clusters are evaluated: The clustering step can be performed in O ( N K ) time using a simple algorithm [19] due to Gonzalez, or in optimal O ( N log K ) time using the algorithm by Feder and Greene [20]. Because the number of values of  X  such that |  X  |  X  p is r pd = C ( p + d, d ) , the total complexity of the algorithm is O ( N + M n c )(log K + r ( p exponential. Searching for clusters within the cut-off radius of each target can take time O ( M K ) , but efficient data-structures can be used to reduce the cost to O ( M n c log K ) . One problem that becomes apparent from the point-wise error bound on  X  ij is that as bandwidth h decreases, the error bound increases, and either d ik = || x i  X  c k || must be decreased (by increasing the number of clusters K ) or the maximum truncation number p max must be increased to continue satisfying the desired error. An increase in either K or p max increases the total cost of the algorithm. Consequently, the algorithm originally presented above does not perform well for small bandwidths. However, few sources have a contribution greater than q i  X  at low bandwidths, since the cut-off radius becomes very small. Also, because the number of clusters increases as the bandwidth decreases, we need an efficient way of searching for clusters that are within the cut-off radius. For this reason, a tree data structure can be used since it allows for efficient fixed-radius nearest neighbor search. If h clusters within the cut-off radius can be found in O ( n c log K ) time [15, 16]. If the bandwidth is very low, then it is more efficient to simply find all source points x i that influence a target y j and perform exact evaluation for those source points. Thus, if n s source points are within the cut-off radius of y j , then the time to build the structure is O ( N log N ) and the time to perform a query is O ( n s log N ) for each target. Thus, we have four methods that may be used for evaluation of the Gauss Transform: direct evaluation, direct evaluation with the tree data structure, IFGT evaluation, and IFGT evaluation with a tree data structure on the cluster centers. Figure 1 shows a graphical representation of the four methods. Because the running times of the four methods for various parameters can differ greatly (i.e. using direct+tree evaluation when ifgt is optimal could result in a running time that is many orders of magnitude larger), we will need an efficient and online method selection approach, which is presented in section 5. Figure 2: Selecting p max and K using cluster radius, for M = N = 20000 , sources dist. as mixture of vs actual cluster radius for d = 3 . Right: Speedup from using actual cluster radius. As mentioned in Section 1, the process of choosing the parameters is non-trivial. In [13], the point-wise error bounds described in Eq. 1 were used in an automatic parameter selection scheme that is optimized when sources are uniformly distributed. We remove the uniformity assumption and also make the error bounds tighter by selecting individual source and target truncation numbers to satisfy cluster-wise error bounds instead of the worst-case point-wise error bounds. The first improvement provides significant speedup in cases where sources are not uniformly distributed, and the second improvement results in general speedup since we are no longer considering the error contribution of just the worst source point, but considering the total error of each cluster instead. 4.1 Number of Clusters and Maximum Truncation Number The task of selecting the number of clusters K and maximum truncation number p max is difficult because they depend on each other indirectly through the source distribution. For example, increas-ing K decreases the cluster radius, which allows for a lower truncation number while still satisfying the error bound; conversely, increasing p max allows clusters to have a larger radius, which allows for a smaller K . Ideally, both parameters should be as low as possible since they both affect compu-tational complexity. Unfortunately, we cannot find the balance between the two without analyzing the source distribution because it influences the rate at which the cluster radius decreases. The uni-formity assumption leads to an estimate of maximum cluster radius, r x  X  K  X  1 /d [13]. However, few interesting datasets are uniformly distributed, and when the assumption is violated, as in Fig. 2, actual r x will decrease faster than K  X  1 /d , leading to over-clustering and increased running time. Our solution is to perform clustering as part of the parameter selection process, obtaining the actual cluster radii for each value of K . Using this approach, parameters are selected in a way that the algorithm is tuned to the actual distribution of the sources.
 We can take advantage of the incremental nature of some clustering algorithms such as the greedy al-gorithm proposed by Gonzalez [19] or the first phase of the Feder and Greene algorithm [20], which provide a 2-approximation and 6-approximation of the optimal k -center clustering, respectively. We can then increment the value K , obtain the maximum cluster radius, and then find the lowest p that satisfies the error bound, picking the final value K which yields the lowest computational cost. Note that if we simply set the maximum number of clusters to K limit = N , we would spend O ( N log N ) time to estimate parameters. However, in practice, the optimal value of K is low relative to N , and it is possible to detect when we cannot lower cost further by increasing K or lowering p max , thus allowing the search to terminate early. In addition, in Section 5, we show how the data distribution allows us to intelligently choose K limit . 4.2 Individual Truncation Numbers by Cluster-wise Error Bounds Once the maximum truncation number p max is selected, we can guarantee that the worst source-target pairwise error is below the desired error bound. However, simply setting each source and target truncation number to p max wastes computational resources since most source-target pairs do not contribute much error. This problem is addressed in [13] by allowing each source to have its own truncation number based on its distance from the cluster center and assuming the worst placement of Figure 3: Speedup obtained by using cluster-wise instead of point-wise truncation numbers, for For d = 1 , the gain of lowering truncation is not large enough to make up for overhead costs. any target. However, this means that each cluster will have to compute r ( p i  X  1) d coefficients where p is the truncation number of its farthest point.
 We propose a method for further decreasing most individual source and target truncation numbers by considering the total error incurred by evaluation at any target where the left term on the r.h.s. is the error from truncating the Taylor series for the clusters that are within the cut-off radius, and the right term bounds the error from ignoring clusters outside the cut-off radius, r y . Instead of ensuring that  X  ij  X   X  for all ( i, j ) pairs, we ensure for all clusters. In this case, if a cluster is outside the cut-off radius, then the error incurred is no than Q k  X  . Summing over all clusters we have our desired error bound. The lowest truncation number that satisfies the cluster-wise error for each cluster is found in O ( p max N ) time by evaluating the cluster-wise error for all clusters for each value of p = { 1 . . . p max } . In addition, we can find individual target point truncation numbers by not only considering the worst case target distance r k y when computing cluster error contributions, but considering target errors for sources at varying distance ranges from each cluster center. This yields concentric regions around each cluster, each of which has its own truncation number, which can be used for targets in that region. Our approach satisfies the error bound tighter and reduces computational cost because: For any input source and target point distribution, requested absolute error, and Gaussian bandwidth, we have the option of evaluating the Gauss Transform using any one of four methods: direct , di-rect+tree , ifgt , and ifgt+tree . As Fig. 4 shows, choosing the wrong method can result in orders of magnitude more time to evaluate the sum. Thus, we require an efficient scheme to automatically choose the best method online based on the input. The scheme must use the distribution of both the source and target points in making its decision, while at the same time avoiding long computations that would defeat the purpose of automatic method selection.
 Note that if we know d , M , N , n s , n c , K , and p max , we can calculate the cost of each method: Figure 4: Running times of the four methods and our automatic method selection for M = N = 4000 , for d = 4 . Right: Ratio of automatic to fastest method and automatic to slowest method, showing that method selection incurs very small overhead while preventing potentially large slowdowns. Algorithm 1 Method Selection 1: Calculate  X  n s , an estimate of n s 2: Calculate Cost direct ( d, N, M ) and Cost direct+tree ( d, N, M,  X  n s ) 3: Calculate highest K limit  X  0 such that for some n c and p max 4: if K limit &gt; 0 then 5: Compute p max and K  X  K limit that minimize estimated cost of IFGT 6: Calculate  X  n c , an estimate of n c 7: Calculate Cost ifgt+tree ( d, N, M, K,  X  n c , p max ) and Cost ifgt ( d, N, M, K,  X  n c , p max ) 8: end if 9: return arg min i Cost i More precise equations and the correct constants that relate the four costs can be obtained directly from the specific implementation of each method (this could be done by inspection, or automatically offline or at compile-time to account for hardware). A simple approach to estimating the distribution dependent n s and n c is to build a tree on sample source points and compute the average number of neighbors to a sampled set of targets. The asymptotic complexity of this approximation is the same as that of direct+tree , unless sub-linear sampling is used at the expense of accuracy in predicting cost. However, n s and n c can be estimated in O ( M + N ) time even without sampling by using techniques from the field of database management systems for estimating spatial join selectivity[21]. of ifgt or ifgt+tree . Finally, we pick the method with lowest cost. As figure 4 shows, our method selection approach chooses the correct method across bandwidths at very low computational cost. Performance Across Bandwidths. We empirically evaluate our method on the same six real-world datasets as in [10] and compare against the authors X  reported results. As in [10], we scale the data to fit the unit hypercube and evaluate the Gauss transform using all 50K points as sources and targets, with bandwidths varying from 10  X  3 to 10 3 times the optimal bandwidth. Because our method satis-choose  X  (since we are doing this only to evaluate the running times of the two methods for the same relative errors) but we do include the time to automatically select the method and parameters. Since the code of [10] is not currently available, our experiments do not use the same machine as [10], and the CPU times are scaled based on the reported/computed the times needed by the naive approach on the corresponding machines. Figure 5 shows the normalized running times of our method versus the Dual-Tree methods DFD, DFDO, DFTO, and DITO. For most bandwidths our method is generally faster by about one order of magnitude (sometimes as much as 1000 times faster). For near-optimal bandwidths, our approach is either faster or comparable to the other approaches.
 Gaussian Process Regression. Gaussian process regression (GPR) [22] provides a Bayesian frame-work for non-parametric regression. The computational complexity for straightforward GPR is O ( N 3 ) which is undesirable for large datasets. The core computation in GPR involves the solution can be used to accelerate this solution for Gaussian processes with Gaussian covariance, given by new point x  X  , the training phase involves computing  X  = ( K +  X  2 I )  X  1 y , and the prediction of y  X  efficiently by a conjugate gradient method using IFGT for matrix-vector multiplication. Further, the accuracy of the matrix-vector product can be reduced as the iterations proceed (i.e.  X  is modified every iteration) if we use inexact Krylov subspaces [23] for the conjugate gradient iterations. We apply our method for Gaussian process regression on four standard datasets: robotarm , abalone , housing , and elevator 2 . We present the results of the training phase (though we also speed up the prediction phase). For each dataset we ran five experiments: the first four fixed one of the the fifth automatically selected the best method at each iteration (denoted by auto in figure 6). To validate our solutions, we measured the relative error between the vectors found by the direct method and our approximate methods; they were small, ranging from  X  10  X  10 to  X  10  X  5 . As expected, auto chose the correct method for each dataset, incurring only a small overhead cost. Also, for the abalone dataset, auto outperformed any of the fixed method experiments; as the right side of figure 6 shows, half way through the iterations, the required accuracy decreased enough to make ifgt faster than direct evaluation. By switching methods dynamically, the automatic selection approach outperformed any fixed method, further demonstrating the usefulness of our online tuning approach. Fast Particle Smoothing. Finally, we embed our automatic method selection in the the two-filter particle smoothing demo provided by the authors of [3] 3 . For a data size of 1000, tolerance set at 10  X  6 , the run-times are 18.26s, 90.28s and 0.56s for the direct, dual-tree and automatic ( ifgt was chosen) methods respectively. The RMS error for all methods from the ground truth values were observed as 2 . 904  X  10  X  04 . Figure 6: GPR Results. Left: CPU times. Right: Desired accuracy per iteration for abalone dataset. We presented an automatic online tuning approach to Gaussian summations that combines a tree data structure with IFGT that is well suited for both high and low bandwidths and which users can treat as a black box. The approach also tunes IFGT parameters to the source distribution, and provides tighter error bounds. Experiments demonstrated that our approach outperforms competing methods for most bandwidth settings, and dynamically adapts to various datasets and input parameters. Acknowledgments. We would like to thank the U.S. Government VACE program for supporting this work. This work was also supported by a NOAA-ESDIS Grant to ASIEP at UMD.

