 We consider the problem of data mining with formal privacy guarantees, given a data access interface based on the differ-ential privacy framework. Differential privacy requires that computations be insensitive to changes in any particular in-dividual X  X  record, thereby restricting data leaks through the results. The privacy preserving interface ensures uncondi-tionally safe access to the data and does not require from the data miner any expertise in privacy. However, as we show in the paper, a naive utilization of the interface to construct privacy preserving data mining algorithms could lead to inferior data mining results. We address this problem by considering the privacy and the algorithmic requirements simultaneously, focusing on decision tree induction as a sam-ple application. The privacy mechanism has a profound ef-fect on the performance of the methods chosen by the data miner. We demonstrate that this choice could make the difference between an accurate classifier and a completely useless one. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive imple-mentation but with an order of magnitude fewer learning samples.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; H.2.7 [ Database Management ]: Database Administration X  Security, Integrity and Protection Algorithms, Security Differential Privacy, Data Mining, Decision Trees
Data mining presents many opportunities for enhanced services and products in diverse areas such as healthcare, banking, traffic planning, online search, and so on. However, its promise is hindered by concerns regarding the privacy of the individuals whose data are being mined. Therefore, there is great value in data mining solutions that provide reliable privacy guarantees without significantly compromis-ing accuracy. In this work we consider data mining within the framework of differential privacy [7, 9]. Basically, dif-ferential privacy requires that computations be insensitive to changes in any particular individual X  X  record. Once an individual is certain that his or her data will remain pri-vate, being opted in or out of the database should make little difference. For the data miner, however, all these in-dividual records in aggregate are very valuable. Differential privacy provides formal privacy guarantees that do not de-pend on an adversary X  X  background knowledge or computa-tional power. This independence frees data providers who share data from concerns about past or future data releases and is adequate given the abundance of personal informa-tion shared on social networks and public Web sites. In addition, differential privacy maintains composability [12]  X  differential privacy guarantees can be provided even when multiple differentially-private releases are available to an ad-versary. Thus, data providers that let multiple parties access their database can evaluate and limit any privacy risks that might arise from collusion between adversarial parties or due to repetitive access by the same party.

We consider the enforcement of differential privacy through a programmable privacy preserving layer, similar to the Pri-vacy INtegrated Queries platform (PINQ) proposed by Mc-Sherry [15]. This concept is illustrated in Figure 1. In this approach, a data miner can access a database through a query interface exposed by the privacy preserving layer. The data miner need not worry about enforcing the privacy re-quirements nor be an expert in the privacy domain. The access layer enforces differential privacy by adding carefully calibrated noise to each query. Depending on the calculated function, the magnitude of noise is chosen to mask the influ-ence of any particular record on the outcome. This approach has two useful advantages in the context of data mining: it allows data providers to outsource data mining tasks with-out exposing the raw data, and it allows data providers to sell data access to third parties while limiting privacy risks.
However, while the data access layer ensures that privacy is maintained, the implementation choices made by the data miner are crucial to the accuracy of the resulting data min-Figure 1: Data mining with differential privacy (DP) access interface ing model. In fact, a straightforward adaptation of data mining algorithms to work with the privacy preserving layer could lead to suboptimal performance. Each query intro-duces noise to the calculation and different functions may require different magnitudes of noise to maintain the differ-ential privacy requirements set by the data provider. Poor implementation choices could introduce larger magnitudes of noise than necessary, leading to inaccurate results.
To illustrate this problem further, we present it in terms of Pareto efficiency [21]. Consider three objective functions: the accuracy of the data mining model (e.g., the expected ac-curacy of a resulting classifier, estimated by its performance on test samples), the size of the mined database (number of training samples), and the privacy requirement, represented by a privacy parameter . In a given situation, one or more of these factors may be fixed: a client may present a lower ac-ceptance bound for the accuracy of a classifier, the database may contain a limited number of samples, or a regulator may pose privacy restrictions. Within the given constraints, we wish to improve the objective functions: achieve better accuracy with fewer learning examples and better privacy guarantees. However, these objective functions are often in conflict. For example, applying stronger privacy guarantees could reduce accuracy or require a larger dataset to main-tain the same level of accuracy. Instead, we should settle for some tradeoff. With this perception in mind, we can eval-uate the performance of data mining algorithms. Consider, for example, three hypothetical algorithms that produce a classifier. Assume that their performance was evaluated on datasets with 50,000 records, with the results illustrated in Figure 2. We can see that when the privacy settings are high, algorithm 1 obtains on average a lower error rate than the other algorithms, while algorithm 2 does better when the privacy settings are low. A Pareto improvement is a change that improves one of the objective functions without harming the others. Algorithm 3 is dominated by the other algorithms: for any setting, we can make a Pareto improve-ment by switching to one of the other algorithms. A given situation (a point in the graph) is Pareto efficient when no further Pareto improvements can be made. The Pareto fron-tier is given by all the Pareto efficient points. Our goal is to investigate algorithms that can further extend the Pareto frontier, allowing for better privacy and accuracy tradeoffs.
We address this problem by considering the privacy and algorithmic requirements simultaneously, taking decision tree induction as a case study. We consider how algorithmic pro-cesses such as the application of splitting criteria on nominal and continuous attributes as well as decision tree pruning, and investigate how privacy considerations may influence the way the data miner utilizes the data access interface. Sim-Figure 2: Example of a Pareto frontier. Given a number of learning samples, what are the privacy and accuracy tradeoffs? ilar considerations motivate the extension of the interface with additional functionality that allows the data miner to query the database more effectively. Our analysis and exper-imental evaluations confirm that algorithmic decisions made with privacy considerations in mind may have a profound impact on the resulting classifier. When differential privacy is applied, the method chosen by the data miner to build the classifier could make the difference between an accurate clas-sifier and a useless one, even when the same choice without privacy constraints would have no such effect. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive implementation but with an order of magnitude fewer learning samples.
Most of the research on differential privacy so far focused on theoretical properties of the model, providing feasibility and infeasibility results [13, 10, 2, 8].

Several recent works studied the use of differential pri-vacy in practical applications. Machanavajjhala et al. [14] applied a variant of differential privacy to create synthetic datasets from U.S. Census Bureau data, with the goal of using them for statistical analysis of commuting patterns in mapping applications. Chaudhuri and Monteleoni [4] pro-posed differentially-private algorithms for logistic regression. These algorithms ensure differential privacy by adding noise to the outcome of the logistic regression model or by solv-ing logistic regression for a noisy version of the target func-tion. Unlike the approach considered in this paper, the algo-rithms require direct access to the raw data. McSherry and Mironov studied the application of differential privacy to col-laborative recommendation systems [16] and demonstrated the feasibility of differential privacy guarantees without a significant loss in recommendation accuracy.

The use of synthetic datasets for privacy preserving data analysis can be very appealing for data mining applications, since the data miner gets unfettered access to the synthetic dataset. This approach was studied in several works [2, 11, 10]. Unfortunately, initial results suggest that ensuring the usefulness of the synthetic dataset requires that it be crafted to suit the particular type of analysis to be performed.
Differential privacy [7, 8] is a recent privacy definition that guarantees the outcome of a calculation to be insensitive to any particular record in the data set.

Definition 1. We say a randomized computation M pro-vides -differential privacy if for any datasets A and B with symmetric difference A  X  B = 1 ( A and B are treated as mul-tisets), and any set of possible outcomes S  X  Range ( M ), The parameter allows us to control the level of privacy. Lower values of mean stronger privacy, as they limit further the influence of a record on the outcome of a calculation. The values typically considered for are smaller than 1 [8], e.g., 0.01 or 0.1 (for small values we have e  X  1 + ). The definition of differential privacy maintains a composability property [15]: when consecutive queries are executed and each maintains differential privacy, their parameters can be accumulated to provide a differential privacy bound over all the queries. Therefore, the parameter can be treated as a privacy cost incurred when executing the query. These costs add up as more queries are executed, until they reach an allotted bound set by the data provider (referred to as the privacy budget), at which point further access to the database will be blocked. The composition property also provides some protection from collusion: collusion between adversaries will not lead to a direct breach in privacy, but rather cause it to degrade gracefully as more adversaries collude, and the data provider can also bound the overall privacy budget (over all data consumers).

Typically, differential privacy is achieved by adding noise to the outcome of a query. One way to do so is by calibrat-ing the magnitude of noise required to obtain -differential privacy according to the sensitivity of a function [9]. The sensitivity of a real-valued function expresses the maximal possible change in its value due to the addition or removal of a single record:
Definition 2. Given a function f : D  X  R d over an arbi-trary domain D , the sensitivity of f is
Given the sensitivity of a function f , the addition of noise drawn from a calibrated Laplace distribution maintains -differential privacy [9]:
Theorem 1. Given a function f : D  X  R d over an arbi-trary domain D , the computation provides -differential privacy.

For example, the count function over a set S , f ( S ) = | S | , has sensitivity 1. Therefore, a noisy count that returns M ( S ) = | S | + Laplace (1 / ) maintains -differential privacy. Note that the added noise depends in this case only on the privacy parameter . Therefore, the larger the set S , the smaller the relative error introduced by the noise.
Another way to obtain differential privacy is through the exponential mechanism [17]. The exponential mechanism is given a quality function q that scores outcomes of a calcu-lation, where higher scores are better. For a given database and parameter, the quality function induces a probability distribution over the output domain, from which the expo-nential mechanism samples the outcome. This probability distribution favors high scoring outcomes (they are exponen-tially more likely to be chosen), while ensuring -differential privacy.

Definition 3. Let q : ( D n  X R )  X  R be a quality function that, given a database d  X  D n , assigns a score to each out-come r  X  R . Let S ( q ) = max r,A  X  B =1 k q ( A,r )  X  q ( B,r ) k Let M be a mechanism for choosing an outcome r  X  R given a database instance d  X  D n . Then the mechanism M , defined by M ( d,q ) = return r with probability  X  exp q ( d,r ) maintains -differential privacy.
PINQ [15] is a proposed architecture for data analysis with differential privacy. It presents a wrapper to C# X  X  LINQ lan-guage for database access, and this wrapper enforces differ-ential privacy. A data provider can allocate a privacy budget (parameter ) for each user of the interface. The data miner can use this interface to execute over the database aggregate queries such as count ( NoisyCount ), sum ( NoisySum ) and av-erage ( NoisyAvg ), and the wrapper uses Laplace noise and the exponential mechanism to enforce differential privacy.
Another operator presented in PINQ is Partition . When queries are executed on disjoint datasets, the privacy costs do not add up, because each query pertains to a different set of records. This property was dubbed parallel composition [15]. The Partition operator takes advantage of parallel composition: it divides the dataset into multiple disjoint sets according to a user-defined function, thereby signaling to the system that the privacy costs for queries performed on the disjoint sets should be summed separately. Conse-quently, the data miner can utilize the privacy budget more efficiently.

Note that a data miner wishing to develop a data mining algorithm using the privacy preserving interface should plan ahead the number of queries to be executed and the value of to request for each. Careless assignment of privacy costs to queries could lead to premature exhaustion of the privacy budget set by the data provider, thereby blocking access to the database half-way through the data mining process.
The input to the decision tree induction algorithm is a dataset T with attributes A = { A 1 ,...,A d } and a class attribute C . Each record in the dataset pertains to one individual. The goal is to build a classifier for the class C . The ID3 algorithm presented by Quinlan [19] uses greedy hill-climbing to induce a decision tree. Initially, the root holds all the learning samples. Then, the algorithm chooses the attribute that maximizes the information gain and splits the learning samples with this attribute. The same process is applied recursively on each subset of the learning samples, until there are no further splits that improve the information gain. Algorithm 1 presents a differential privacy adaptation of ID3, which evaluates the information gain using noisy counts (i.e., adding Laplace noise to the accurate count). It is based on a theoretical algorithm that was presented in the SuLQ framework (Sub-Linear Queries [1]), a predecessor of differential privacy, so we will refer to it as SuLQ-based ID3.
We use the following notation: T refers to a set of records,  X  = |T| , r A and r C refer to the values that record r  X  T takes on the attributes A and C respectively, T A j = { r  X  T : r A = j } ,  X  A j = |T A j | ,  X  c = | r  X  T : r C = c | , and  X  | r  X  T : r A = j  X  r C = c | . To refer to noisy counts, we use a similar notation but substitute N for  X  . All the log() expressions are in base 2.
 Algorithm 1 SuLQ-based ID3 1: procedure SuLQ ID3 ( T , A ,C,d,B ) 2: Input: T  X  private dataset, A = { A 1 ,...,A d }  X  a 4: Build SuLQ ID3( T , A ,C,d, ) 5: end procedure 6: procedure Build SuLQ ID3 ( T , A ,C,d, ) 7: t = max A  X  X  | A | 8: N T = NoisyCount ( T ) 9: if A =  X  or d = 0 or N T t | C | &lt; 10: T c = Partition ( T ,  X  c  X  C : r C = c ) 11:  X  c  X  C : N c = NoisyCount ( T c ) 12: return a leaf labeled with arg max c ( N c ) 13: end if 14: for every attribute A  X  X  do 15: T j = Partition ( T ,  X  j  X  A : r A = j ) 16:  X  j  X  A : T j,c = Partition ( T j ,  X  c  X  C : r C = c ) 20: end for 21:  X  A = arg max A V A 22: T i = Partition ( T ,  X  i  X   X  A : r  X  A = i ) 23:  X  i  X   X  A : Subtree i = Build SuLQ ID 3( T i , A \ 24: return a tree with a root node labeled  X  A and edges 25: end procedure
Given the entropy of a set of instances T with respect to the class attribute C , H C ( T ) =  X  P c  X  C  X  c  X  given the entropy obtained by splitting the instances with tion gain is given by InfoGain( A, T ) = H C ( T )  X  H C | A Maximizing the information gain is equivalent to maximiz-ing Hence, information gain can be approximated with noisy counts for  X  A j and  X  A j,c to obtain:
In ID3, when all the instances in a node have the same class or when no instances have reached a node, there will be no further splits. Because of the noise introduced by dif-ferential privacy, these stopping criteria can no longer be evaluated reliably. Instead, in line 9 we try to evaluate whether there are  X  X nough X  instances in the node to warrant further splits. While the theoretical version of SuLQ-based ID3 in [1] provided bounds with approximation guarantees, we found these bounds to be prohibitively large. Instead, our heuristic requirement is that, in the subtrees created af-ter a split, each class count be larger on average than the standard deviation of the noise in the NoisyCount . While this requirement is quite arbitrary, in the experiments it provided reasonable results.

Use of the overall budget B set by the data provider should be planned ahead. The SuLQ-based ID3 algorithm does that by limiting the depth of the tree according to a value set by the data miner and assigning an equal share of the budget for each level of the tree, including the leaves. Thanks to the composition property of differential privacy, queries on different nodes on the same level do not accumulate, as they are carried out on disjoint sets of records. Within each node, half of the allocated budget is used to evaluate the number of instances and the other half is used to determine the class counts (in leaves) or evaluate the attributes (in nodes). Class counts are calculated on disjoint sets, so each query can use the allocated . On the other hand, because each attribute evaluation is carried out on the same set of records, the budget must be further split among the attributes.

The suggested implementation in SuLQ-based ID3 is rel-atively straightforward: it makes direct use of the Noisy-Count primitive to evaluate the information gain criterion, while taking advantage of the Partition operator to avoid redundant accumulation of the privacy budget. However, this implementation also demonstrates the drawback of a straightforward adaptation of the algorithm: because the count estimates required to evaluate the information gain should be carried out for each attribute separately, the data miner needs to split the overall budget between those sepa-rate queries. Consequently, the budget per query is small, resulting in large magnitudes of noise which must be com-pensated for by larger datasets.
The main drawback of the SuLQ-based ID3 presented in the previous section is the wasteful use of the privacy bud-get when the information gain should be evaluated sepa-rately for each attribute. The exponential mechanism offers a better approach: rather than evaluating each attribute separately, we can evaluate the attributes simultaneously in one query, the outcome of which is the attribute to use for splitting. The quality function q provided to the exponential mechanism scores each attribute according to the splitting criterion. Algorithm 2 presents this approach. The budget distribution in the algorithm is similar to that used for the SuLQ-based ID3, except that for attribute selection, instead of splitting the allocated budget among multiple queries, the entire budget is used to find the best attribute in a single exponential mechanism query. Algorithm 2 Differential Private ID3 algorithm 1: procedure DiffPID3 ( T , A ,C,d,B ) 2: Input: T  X  private dataset, A = { A 1 ,...,A d }  X  a 4: Build DiffPID3( T , A ,C,d, ) 5: end procedure 6: procedure Build DiffPID3 ( T , A ,C,d, ) 7: t = max A  X  X  | A | 8: N T = NoisyCount ( T ) 9: if A =  X  or d = 0 or N T t | C | &lt; 10: T c = Partition ( T ,  X  c  X  [ C ] : r C = c ) 11:  X  c  X  C : N c = NoisyCount ( T c ) 12: return a leaf labeled with arg max c ( N c ) 13: end if 14:  X  A = ExpMech ( A ,q ) 15: T i = Partition ( T ,  X  i  X   X  A : r  X  A = i ) 16:  X  i  X   X  A : Subtree i = Build DiffPID 3( T i ,A \ 17: return a tree with a root node labeled  X  A and edges 18: end procedure
The next question is which quality function should be fed into the exponential mechanism. Although many studies have compared the performance of different splitting criteria for decision tree induction, their results do not, in general, testify to the superiority of any one criterion in terms of tree accuracy, although the choice may affect the resulting tree size (see, e.g., [18]). Things change, however, when the splitting criteria are considered in the context of algorithm 2. First, the depth constraint may prevent some splitting criteria from inducing trees with the best possible accuracy. Second, the sensitivity of the splitting criterion influences the magnitude of noise introduced to the exponential mech-anism, meaning that for the same privacy parameter, the exponential mechanism will have different effectiveness for different splitting criteria. We consider several quality func-tions and their sensitivity. The proofs for the sensitivity bounds are supplied in Appendix A: Information gain: following the discussion leading to equa-Gini index: this impurity measure is used in the CART Max operator: (based on the resubstitution estimate de-Gain Ratio: the gain ratio [20] is obtained by dividing
The sensitivity of the quality functions listed above sug-gests that information gain will be the most sensitive to noise, and the Max operator will be the least sensitive to noise. In the experimental evaluations we compare the per-formance of these quality functions, and the influence of the noise is indeed reflected in the accuracy of the resulting clas-sifiers.
One problem that may arise when building classifiers is overfitting the training data. When inducing decision trees with differential privacy, this problem is somewhat mitigated by the introduction of noise and by the constraint on tree depth. Nonetheless, because of the added noise, it is no longer possible to identify a leaf with pure class values, so the algorithm will keep splitting nodes as long as there are enough instances and as long as the depth constraint is not reached. Hence, the resulting tree may contain redundant splits, and pruning may improve the tree.

We avoid pruning approaches that require the use of a validation set, such as the minimal cost complexity pruning applied by CART [3] or reduced error pruning [20], because they lead to a smaller training set, which in turn would be more susceptible to the noise introduced by differential privacy. Instead, we consider error based pruning [20], which is used in C4.5. In this approach, the training set itself is used to evaluate the performance of the decision tree before and after pruning. Since this evaluation is biased in favor of the training set, the method makes a pessimistic estimate for the test set error rate: it assumes that the error rate has binomial distribution, and it uses a certainty factor CF (by default taken to be 0.25) as a confidence limit to estimate a bound on the error rate from the error observed on the training set. C4.5 estimates the error rate in a given subtree (according to the errors in the leaves), in its largest branch (pruning by subtree raising), and the expected error if the subtree is turned into a leaf. The subtree is then replaced with the option that minimizes the estimated error.
Since error based pruning relies on class counts of in-stances, it should be straightforward to evaluate the error rates using noisy counts. The error in the subtree can be evaluated using the class counts in the leaves, which were ob-tained in the tree construction phase. To evaluate the error if a subtree is turned into a leaf, the counts in the leaves can be aggregated in a bottom-up manner to provide the counts in upper level nodes. However, this aggregation would also add up all the noise introduced in the leaves (i.e., leading to larger noise variance). Moreover, subtrees split with multi-valued attributes would aggregate much more noise than those with small splits, skewing the results. Executing new NoisyCount queries to obtain class counts in upper level nodes could provide more accurate results. However, this would require an additional privacy budget at the expense of the tree construction phase. For similar reasons, error estimations for subtree raising would also incur a toll on the privacy budget.

As a compromise, we avoid making additional queries on the dataset and instead use the information gathered dur-ing the tree construction to mitigate the impact of the noise. We make two passes over the tree: an initial top-down pass calibrates the total instance count in each level of the tree to match the count in the parent level. Then a second bottom-up pass aggregates the class counts and calibrates them to match the total instance counts from the first pass. Finally, we use the updated class counts and instance counts to eval-uate the error rates just as in C4.5 and prune the tree. Al-gorithm 3 summarizes this approach. In the algorithm, N T refers to the noisy instance counts that were calculated in al-gorithm 2 for each node T , and N c refers to the class counts for each class c  X  C .
One important extension that C4.5 added on top of ID3 was the ability to handle continuous attributes. Attribute values that appear in the learning examples are used to determine potential split points, which are then evaluated with the splitting criterion. Unfortunately, when inducing decision trees with differential privacy, it is not possible to use attribute values from the learning examples as splitting points; this would be a direct violation of privacy, revealing at the least information about the record that supplied the value of the splitting point.

The exponential mechanism gives us a different way to determine a split point: the learning examples induce a probability distribution over the attribute domain; given a splitting criterion, split points with better scores will have higher probability to be picked. Here, however, the expo-nential mechanism is applied differently than in Section 4.1: the output domain is not discrete. Fortunately, the learn-ing examples divide the domain into ranges of points that have the same score, allowing for efficient application of the mechanism. The splitting point is sampled in two phases: first, the domain is divided into ranges where the score is constant (using the learning examples). Each range is con-sidered a discrete option, and the exponential mechanism is applied to choose a range. Then, a point from the range is sampled with uniform distribution and returned as the Algorithm 3 Pruning with Noisy Counts 1: Input: UT -an unpruned decision tree, CF -Certainty 2: procedure Prune ( UT ) 3: TopDownCorrect( UT , UT.N T ) 4: BottomUpAggregate( UT ) 5: C4.5Prune( UT , CF ) 6: end procedure 7: procedure TopDownCorrect ( T , fixedN T ) 8: T.N T  X  fixedN T 9: if T is not a leaf then 10: T i  X  subtree i ( T ) 11: for all T i do 13: TopDownCorrect( T i , fixedN T i ) 14: end for 15: end if 16: end procedure 17: procedure BottomUpAggregate ( T ) 18: if T is a leaf then 19: for all c  X  C do 21: end for 22: else 23: T i  X  subtree i ( T ) 24:  X  T i : BottomUpAggregate( T i ) 25: for all c  X  C do 26: T.N c  X  P i T i .N c 27: end for 28: end if 29: end procedure output of the exponential mechanism. The probability as-signed to the range in the first stage takes into account also the sampling in the second stage. This probability is ob-tained by integrating the density function induced by the exponential mechanism over the range. For example, con-sider a continuous attribute over the domain [ a,b ]. Given a dataset d  X  D n and a splitting criterion q , assume that all the points in r  X  [ a 0 ,b 0 ] have the same score: q ( d,r ) = c . In that case, the exponential mechanism should choose this range with probability In general, given the ranges R 1 ,...,R m , where all the points in range R i get the score c i , the exponential mechanism sets the probability of choosing range R i to be exp(  X  c i )  X | R where | R i | is the size of range R i . Note that this approach is applicable only if the domain of the attribute in question is finite, and in our experiments we define the domain for each attribute in advance. This range cannot be determined dynamically according to the values observed in the learning examples, as this would violate differential privacy.
A split point should be determined for every numeric at-tribute. In addition, this calculation should be repeated for every node in the decision tree (after each split, every child node gets a different set of instances that require different split points). Therefore, supporting numeric attributes re-quires setting aside a privacy budget for determining the split points. To this end, given n numeric attributes, the budget distribution in line 3 of algorithm 2 should be up-dated to = B (2+ n ) d +2 , and the exponential mechanism should be applied to determine a split point for each nu-meric attribute before line 14. An alternative solution is to discretize the numeric attributes before applying the de-cision tree induction algorithm, losing information in the process in exchange for budget savings.
In this section we evaluate the proposed algorithms using synthetic and real data sets. The experiments were executed on Weka [22], an open source machine learning software. Since Weka is a Java-based environment, we did not rely on the PINQ framework, but rather wrote our own differential privacy wrapper to the Instances class of Weka, which holds the raw data. The algorithms were implemented on top of that wrapper. We refer to the implementation of algorithm 2 as DiffPID3 , and to its extension that supports continuous attributes and pruning as DiffPC4.5 .
We generated synthetic datasets using a method adapted from [6]. We define a domain with ten nominal attributes and a class attribute. At first, we randomly generate a de-cision tree up to a predetermined depth. The attributes are picked uniformly without repeating nominal attributes on the path from the root to the leaf. Starting from the third level of the tree, with probability p leaf = 0 . 3 we turn nodes to leaves. The class for each leaf is sampled uniformly. In the second stage, we sample points with uniform distribu-tion and classify them with the generated tree. Optionally, we introduce noise to the samples by reassigning attributes and classes, replacing each value with probability p noise used p noise  X  { 0 , 0 . 1 , 0 . 2 } ). The replacement is chosen uni-formly, possibly repeating the original value. For testing, we generated similarly a noiseless test set with 10 , 000 records. In the first experiment we isolated a split on a single node. We created a tree with a single split (depth 1), with ten bi-nary attributes and a binary class, which takes a different value in each leaf. We set the privacy budget to B = 0 . 1, and by varying the size of the training set, we evaluated the success of each splitting criterion in finding the correct split. We generated training sets with sizes ranging from 100 to 5000, setting 5000 as a bound on the dataset size for determining information gain sensitivity. For each sam-ple size we executed 200 runs, generating a new training set for each, and averaged the results over the runs. Figure 3 presents the results for p noise = 0 . 1, and Table 1 shows the accuracy and standard deviations for some of the sample sizes. In general, the average accuracy of the resulting de-cision tree is higher as more training samples are available, reducing the influence of the differential privacy noise on the outcome. Due to the noisy process that generates the classi-fiers, their accuracy varies greatly. However, as can be seen in the cases of the Max scorer and the Gini scorer, the influ-ence of the noise weakens and the variance decreases as the number of samples grows. For the SuLQ-based algorithm, Figure 3: Splitting a single node with a binary at-tribute, B = 0 . 1 , p noise = 0 . 1 Figure 4: Splitting a single node with a continuous attribute, B = 0 . 1 , p noise = 0 . 1 when evaluating the counts for information gain, the budget per query is a mere 0 . 00125, requiring Laplace noise with standard deviation over 1000 in each query. On the other hand, the Max scorer, which is the least sensitive to noise, provides excellent accuracy even for sample sizes as low as 1500. Note that ID3 without any privacy constraints does not get perfect accuracy for small sample sizes, because it overfits the noisy learning samples.

Figure 4 presents the results of a similar experiment car-ried out with a numeric split. Three of the ten attributes were replaced with numeric attributes over the domain [0 , 100], and one of them was used to split the tree with a split point placed at the value 35. The results of the J48 algorithm (Weka X  X  version of C4.5 v8) are provided as a benchmark, with and without pruning. The relation between the differ-ent scores is similar for the numeric and the nominal case, although more samples are required to correctly identify the split point given the privacy budget.

In Figure 5 we compare the performance of DiffPC4.5 on a numeric split as opposed to discretizing the dataset and run-ning DiffPID3. The dataset from the former experiment was discretized by dividing the range of each numeric attribute to 10 equal bins. Of course, if the split point of a numeric attribute happens to match the division created by the dis-crete domains, working with nominal attributes would be Figure 5: Comparing numerical split to discretiza-tion, B = 0 . 2 , p noise = 0 . 0 preferable, because determining split points for numeric at-tributes consumes some of the privacy budget. However, we intentionally used discrete domains that mismatch the split point (placed at the value 35), to reflect the risk of reduced accuracy when turning numeric attributes to discrete ones. The results show that for smaller training sets, the budget saved by discretizing the dataset and switching to nominal attributes allows for better accuracy. For larger training sets, the exponential mechanism allows, on average, split points to be determined better than in the discrete case.
We conducted numerous experiments, creating and learn-ing trees of depths 3, 5 and 10, and setting the attributes and class to have 2, 3 or 5 distinct values. We used B = 1 . 0 over sample sizes ranging from 1000 to 50,000 instances, set-ting 50,000 as the bound on the dataset size for determining information gain sensitivity. For each tested combination of values we generated 10 trees, executed 20 runs on each tree (each on a newly generated training set), and averaged the results over all the runs. In general, the results exhibit behavior similar to that seen in the previous set of experi-ments. The variance in accuracy, albeit smaller than that observed for a single split, was apparent also when inducing deeper trees. For example, the typical standard deviation for the accuracy results presented in Figure 6 was around  X  5% and even lower than that for the results presented in Figure 7. When inducing shallower trees or using attributes with fewer distinct values, we observed an interesting pat-tern, which is illustrated in Figures 6 and 7. When the size of the dataset is small, algorithms that make efficient use of the privacy budget are superior. This result is similar to the results observed in the previous experiments. However, as the number of available samples increases, the restrictions set by the privacy budget have less influence on the accuracy Figure 6: Inducing a tree of depth 5 with 10 binary attributes and a binary class, B = 1 . 0 , p leaf = 0 . 3 Figure 7: Inducing a tree of depth 5 with 7 binary attributes, 3 numeric attributes and a binary class, B = 1 . 0 , p leaf = 0 . 3 of the resulting classifier. When that happens, the depth constraint becomes more dominant; the Max scorer, which with no privacy restrictions usually produces deeper trees than those obtained with InfoGain or Gini scorers, provides inferior results with respect to the other methods when the depth constraint is present. We conducted experiments on the Adult dataset from the UCI Machine Learning Repository [5], which contains cen-sus data. The data set has 6 continuous attributes and 8 nominal attributes. The class attribute is income level, with two possible values,  X  50K or &gt; 50K. After removing records with missing values and merging the training and test sets, the dataset contains 45,222 learning samples (we set 50,000 as the bound on the dataset size for determining information gain sensitivity). We induced decision trees of depth up to 5 with varying privacy budgets. Note that although the values Figure 8: Accuracy vs. privacy ( B ) in the Adult dataset considered for B should typically be smaller than 1, we used larger values to compensate for the relatively small dataset size 1 (the privacy budget toll incurred by the attributes is heavier than it was in the synthetic dataset experiments). We executed 10 runs of 10-fold cross-validation to evaluate the different scorers. Statistical significance was determined using corrected paired t-test with confidence 0.05. Figure 8 summarizes the results. In most of the runs, the Max scorer did significantly better than the Gini scorer, and both did significantly better than the InfoGain scorer. In addition, all scorers showed significant improvement in accuracy as the allocated privacy budget was increased. The typical mea-sured standard deviation in accuracy was  X  0 . 5%.
We conclude that the introduction of formal privacy guar-antees into a system requires the data miner to take a differ-ent approach to data mining algorithms. The sensitivity of the calculations becomes crucial to performance when dif-ferential privacy is applied. Such considerations are critical especially when the number of training samples is relatively small or the privacy constraints set by the data provider are very limiting. Our experiments demonstrated this tension between privacy, accuracy, and the dataset size. This work poses several future challenges. The large variance in the experimental results is clearly a problem, and more stable results are desirable even if they come at a cost. One solu-tion might be to consider other stopping rules when split-ting nodes, trading possible improvements in accuracy for increased stability. In addition, it may be fruitful to con-sider different tactics for budget distribution. Another in-teresting direction, following the approach presented in [14], is to relax the privacy requirements and allow ruling out rare calculation outcomes that lead to poor results.
In comparison, some of the datasets used by previous works were larger by an order of magnitude or more (census data with millions of records [14], Netflix data of 480K users [16], search queries of hundreds of thousands users [15]). [1] A. Blum, C. Dwork, F. McSherry, and K. Nissim. [2] A. Blum, K. Ligett, and A. Roth. A learning theory [3] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. [4] K. Chaudhuri and C. Monteleoni. Privacy-preserving [5] C. B. D.J. Newman, S. Hettich and C. Merz. UCI [6] P. Domingos and G. Hulten. Mining high-speed data [7] C. Dwork. Differential privacy. In ICALP (2) , volume [8] C. Dwork. Differential privacy: A survey of results. In [9] C. Dwork, F. McSherry, K. Nissim, and A. Smith. [10] C. Dwork and S. Yekhanin. New efficient attacks on [11] D. Feldman, A. Fiat, H. Kaplan, and K. Nissim. [12] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. [13] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, [14] A. Machanavajjhala, D. Kifer, J. M. Abowd, [15] F. McSherry. Privacy integrated queries: an extensible [16] F. McSherry and I. Mironov. Differentially private [17] F. McSherry and K. Talwar. Mechanism design via [18] J. Mingers. An empirical comparison of selection [19] J. R. Quinlan. Induction of decision trees. Machine [20] J. R. Quinlan. C4.5: Programs for Machine Learning . [21] R. E. Steur. Multiple criteria optimization: theory [22] I. H. Witten and E. Frank. Data Mining: Practical
In this section we show how the sensitivity of the quality functions presented in Section 4.1 was derived.

To evaluate the sensitivity of q IG , we will use the following property:
Claim 1. For a &gt; 0:
Proof. The function a log a +1 a does not have extremum points in the range a &gt; 0. At the limits, lim a  X  X  X  a log
Let r = { r 1 ,r 2 ,...,r d } be a record, let T be some dataset, and T 0 = T  X  X  r } . Note that in the calculation of q IG over the two datasets, the only elements in the sum that will differ are those that relate to the attribute j  X  A that the record r takes on the attribute A . Therefore, given this j we can focus on:
The addition of a new record r affects only one of the ele-ments in the left-hand sum (specifically, one of the elements  X  j,c increases by 1), and in addition,  X  A j increases by one as well.
 Hence we get that: of the form a log a +1 a . Therefore, we can apply Claim 1 and get that for a node with up to  X  elements,
Bounding the sensitivity of q IG ( T ,A ) requires an upper bound on the total number of training examples. In our ex-periments, we assume that such a bound is given by the data provider. An alternate approach is to evaluate the number of training examples with a NoisyCount before invoking the exponential mechanism. The downside of this approach is that negative noise may provide a value for  X  which is too small, resulting in insufficient noise. Therefore, in this ap-proach there is a small chance that the algorithm would violate -differential privacy.

Taking p ( j | t ) to be the fraction of records that take the value t on attribute A and the class t , the Gini index can be expressed as Gini = P j 6 = i p ( j | t ) p ( i | t ) = 1  X  P When determining the Gini index for a tree, this metric is summed over all the leaves, where each leaf is weighted according to the number of records in it. To minimize the Gini index, we observe that:
As in information gain, the only elements in the expression that change when we alter a record are those that relate to the attribute value j  X  A that an added record r takes on attribute A . So, for this given j , we can focus on: We get that: S ( q Gini ( T ,A ))  X  (  X  A j + 1)  X  to P  X  A j,c =  X  A j and the triangle inequality. In addition, since  X  j,c  X   X  A j , we get that 0  X 
This query function is adapted from the resubstitution estimate described in [3]. In a given tree node, we would like to choose the attribute that minimizes the probability of misclassification. This can be done by choosing the attribute that maximizes the total number of hits. Since a record can change the count only by 1, we get that S ( q Max ) = 1.
