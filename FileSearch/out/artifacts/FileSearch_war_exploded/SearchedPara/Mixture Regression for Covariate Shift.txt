 There is a common presumption in developing supervised methods that the distribution of training points used for learning supervised models will match the distribution of points seen in a new test scenario. The expectation that the training and test points follow the same distribution is explicitly implicit in the common practice of randomized splitting given data into a  X  X raining set X  and a  X  X est set X , where the latter is used in assessing performance [ 5 , p. 482-495].
 This paper, then, is concerned with the following issue. A set of real valued training data pairs of the form ( x , y ) is provided to train a model for a supervised learning problem. In addition data of the form x is provided from one (or more) test environments where the model will be used. The question to be addressed is  X  X ow should we predict a value of y given a value x from within that particular test environment? X  Cases where test scenarios truly match the training data are probably rare. The problem of mismatch has been grappled within literature from a number of fields, and has become known as covariate shift [ 14 ]. Specific examples of covariate shift include situations in reinforcement learning [c.f. 13 ] and bio-informatics [c.f. 1 ]. The common issue of sample selection bias [ 7 ] is a particular case of covariate shift.
 Much of the recent analysis of covariate shift has been made in the context of assessing the asymp-totic bias of various estimators [ 15 ]. In general it has been noted that in the case of mismatched models (i.e. where the model from which the training data is generated is not included in the training model class), some typical estimators, such as least squares approaches, produce biased asymptotic estimators [ 14 ]. It might appear that the presumption of matched models in Bayesian analysis means covariate shift is not an issue: failure or otherwise under situations of covariate shift is solved by valid choice for the prior distribution over conditional models. The difficulty with this dismissal of the subject is that modelling conditional distributions alone is not always valid. In fact we can categorise at least three different types of covariate shift: Let us presume that we are only interested in the quality of the conditional model P test ( y | x ) . Then Case 1 is the only one of the above where covariate shift will have no effect on modelling. Case 2 is the well known situation of class prior probability change and, for example, is considered in comparing the benefits of a naive Bayes model, which allows for class prior probability change, and discriminant models, which typically do not.
 Case 3 involves a more general assumption, and arguably can be used to cover most situations of covariate shift, by incorporating any known structural characteristics of the problem into some latent variable r . Change in the distribution of x points implicitly informs us about variation in the targets y via the shift in the latent variable r , which is the causal factor for the change. The purpose of this paper is to provide a generative framework for analysis of covariate shift. The main advantages of this new formulation over previous approaches are Outline. In Section 2 , related work is discussed, before the problem is formally specified and a general model is derived in Section 3 . A specific form of mixture regression model is formulated and an Expectation Maximisation solution is given in Section 3.1 . The specific relationship to Im-portance Weighted Least Squares is discussed in Section 3.1.2 . Test examples are given in section 4 . The results and methods are discussed in Section 5 . Covariate shift will be interpreted, in the context of this work, using mixture of regressor models, where the regression model is dependent on a latent class variable. Clustered regression models have been discussed widely [ 4 , 18 , 8 , 16 ]. The benefits of the mixture of regressor approach for heteroge-neous data was discussed in [ 17 ], but not formulated specifically for the problem of covariate shift. This paper establishes for the first time the relationship between the mixture of regressor model and the typical statistical results in the literature on covariate shift. The main differences of our approach from a standard mixture of regressor formalism is that we utilise the training and test distributions as part of the model and do not use only a conditional model, and we allow coupling of regressors across different mixture components. The main significance with regard to the literature on covari-ate shift is that we establish covariate shift within a general probabilistic modelling paradigm and hence extend the standard techniques to establish more general methods, which are also applicable when the training and test distributions are not explicitly given. The mixture of regressors form for ( x , y ) used in this paper is a specific from of mixture of experts [ 10 ]. Hence hierarchical extensions are also possible in the form of [ 11 ].
 The problem of sample selection bias is related to covariate shift. Sample selection bias has been discussed in [ 19 ], where they estimate the distribution determining the bias for a classification prob-lem. The problem of sample selection bias differs from the case in this paper as here there is no fundamental requirement of distribution overlap between the training and test sets. First, each can have zero density in regions the other is non-zero. Second, the presumption is different: rather than there being a sample rejection process that characterised the difference between training and test sets, there is a sample production process that differs. This paper follows most others in considering the restricted case of a single training and single test set. Each datum x is assumed to have been generated from one of a number of data sources using a mixture distribution corresponding to the source. The proportions of each of the sources varies across the training and test datasets. Hence, in the context of this paper, we understand covariate shift to be effected by a change in the contribution of different sources to the data.
 The motivation of the framework in this paper is that there is a latent feature set upon which each dataset is dependent, and the the variations between the two datasets are dependent upon variation of the proportions, but not the form, of those latent features. This is characterised by presuming each data source is a member of one of two different sets. Each of the two sets of sources is also associated with a regression model. The two sets of sources have the following characteristics: By taking this approach we note that we will be able to separate out effects that we expect to be only characteristics of the training data from effects that are common across training and test sets. The full generative model for the observed data consists of the model for the training data D and model for the test data T . The test data is just used to determine the nature of the covariate shift, and consists of only of the covariates x , and not any targets y . We emphasise that we do not presume to have seen the test data we wish to predict. Rather a prior model is built for the training and test data, and this is then conditioned on the information from the training data and the known covariates for the test data but not the unknown targets. 3.1 Mixture Regression for Covariate Shift In this section the full model is introduced. This significantly extends the previous work on covariate shift, in that the model allows for unknown training and test distributions, and utilises a mixture model approach for representing the relationship between the two. In Section 3.1.2 , we will show how the previous results on covariate shift are special cases of the general model. We will develop this formalism for any parametric form for the regressors P ( y | x ) . In fact this restriction is mainly for ease of explanation, and the method can be used with non-parametric models too, and will be The model takes the following form the relative proportions of each mixture from source set 1 in the training data, and  X  D 2 t are the relative proportions of each mixture from source set 2 in the training data. Finally  X  T 1 t are the proportions of each mixture from source set 1 in the test data. All these parameters are presumed unknown. At some points in the paper it will be presumed the mixtures are Gaussian, when the form N ( x ; m , K ) will be used to denote the Gaussian distribution function of x , with mean m and covariance K . For a parametric model, with the collection of mixture parameters denoted by  X  , the collection of regression parameters denoted by  X  , and the mixing proportions,  X  and  X  we have the full probabilistic model P ( { i  X  , y  X  , x  X  |  X   X  D } , { i  X  , x  X  |  X   X  T }|  X  ,  X  ,  X  ) = where s  X  denotes the source set used to generate the data point  X  , and t  X  denotes the particular mixture from that source set used to generate the data point  X  . In words, this says that the model for from that particular source set. Given these we then sample an x  X  from the relevant mixture and a except now there is only one source set to consider. 3.1.1 EM algorithm A maximum likelihood solution for the parameters (  X  ,  X  ,  X  ,  X  ) can be obtained for this model (given the training data and test covariate) using Expectation Maximisation (EM) [ 3 ]. The deriva-tions are standard EM calculations (see e.g. [ 2 ]), and hence are not reiterated here. Denote the responsibility of mixture i for data point  X  by  X   X  i . Then the application of EM involves maximisa-tion of with respect to the parameters through iteration of E and M steps. The E-step update uses current parameter values to compute the responsibility (denoted by  X  s) of each mixture 1 t and 2 t for each data point  X  in the training set and each data point  X  in the test set using We set  X   X  2 t = 0 for  X   X  T , as none of these mixtures are represented in the test set. The parameters of the mixture model distributions are then updated with the usual M steps for the relevant mix-ture component, and the regression parameters are updated using maximum responsibility-weighted likelihood. When each mixture component is a Gaussian of the form N ( x ; m st , K st ) , when we have a Gaussian regression error term, and denoting the (vector of) regression functions by f s for each source set s , these update rules are: Given the learnt model, inference is straightforward. The test data is associated with a single regres-for each point x i in the test set. 3.1.2 Importance Weighted Least Squares Previous results in modelling covariate shift can be obtained as special cases of the general approach taken in this paper. Suppose we make the assumptions that P D and P T are known, and that the source set 1 contains just the one component, which must be P T by definition. Suppose also that the two regressors have a large and identical variance  X  . In this simple case, we do not need to know the actual test points (in this framework these are only used to infer the test distribution, which is assumed given here). The M step update only involves update to the regressor. For the E step we case of infinite variance  X  . The resulting E and M steps are where we note that  X  1 is a common constant and can be dropped from the calculations. Hence we never need to learn  X  1 or the parameters associated with mixture 2 in this procedure. Also no iterative EM procedure is needed as the E step is independent of the M step results. Hence this is a one shot process. This is the Importance Weighted Least Squares estimator for covariate shift [ 14 ]. A simple extension of this model will allow the large variance assumption to be relaxed, so the model can use the regressor information for computing responsibilities. 4.1 Generated Test Data We demonstrate the mixture of regressors approach to covariate shift (MRCS) on generated test data: a one dimensional regression problem with two sources each corresponding to different linear regressors. Regression performance for MRCS with Gaussian mixtures and linear regressors is compared with three other cases. The first is an importance weighted least squares estimator (IWLS) given the best mixture model fit for the data, corresponding to the current standard for modelling covariate shift. The second uses a mixture of regressors model that ignores the form of the test data, but chooses the regressor corresponding to the mixtures which best match the test data distribution using a KL divergence measure (MRKL). This corresponds to recognising that covariate shift can happen, but ignoring the nature of the test distribution in the modelling process, and trying to choose the best of the two regressors. The third case is where the mixture of regressors is used simply as a standard regression model, ignoring the possibility of covariate shift (MRREG).
 The generative procedure for each of the 100 test datasets involves generating random parameter values for between 1 and 3 mixtures for each of two linear regressors. Test and training datasets of 200 data points each are generated from these mixtures and regression models, using different mixing proportions in each case. The various approaches were run 8 times with different random starting parameters for all methods. 80 iterations of EM were used. A fixed number of iterations was chosen to allow reasonable comparison. Analysis was done for fixed model sizes and for model choice using a Bayesian Information Criterion (BIC). Even though the regularity conditions for BIC do not hold for mixture models, it has been shown that BIC is consistent in this case [ 12 ]. It has also been shown to be a good choice on practical grounds [ 6 ].
 The results of these tests show the significant benefits of explicit recognition of covariate shift over straight regression even compared with the use of the same mixture of regressors model, but without reference to the test distribution. It also shows benefits of the approach of this paper over the current state of the art for modelling covariate shift. Table 1 gives the result of these approaches for various fixed choices of numbers of mixtures associated with each regressor. Independent of the use of any model order choice, the Mixture of Regressors for Covariate Shift (MRCS) performs better than the other approaches. Table 1 also gives the results when the Bayesian Information Criterion is used for selecting the number of mixtures. Again MRCS performs best, and consistently gives better performance on the test data for more than 70 percent of the test cases.
 To illustrate the difference between the methods, Figure 1 plots the results of training a MRCS model on some one dimensional data using a regularised cubic regressor. The fit to the test data is also shown. Once again this is compared with IWLS and MRKL. It can be seen that both IWLS and MRKL fail to untangle the regressors associated with the overlapping central clusters in the training data and hence perform badly in that region of the test data. Table 1: Average mean square error over all 100 datasets for each choice of fixed model mixture size. The 4.2 Auto-Mpg Test It is useful to see that the approach does indeed make a noticeable difference on data that takes the appropriate prior form, but that says nothing about how appropriate that prior is for real prob-lems. Here we demonstrate the method on the auto-mpg problem from the UCI dataset. This provides a natural scenario for demonstrating covariate shift. The auto-mpg data can be found at http://www.ics.uci.edu/  X  mlearn/MLSummary.html and involves the problem of predicting the city cycle fuel consumption of cars. One of the attributes is a class label dictating the origin of a particular car. To demonstrate covariate shift we can consider the prediction task trained on cars from one place of origin and tested on cars from another place of origin. Here we consider predicting the fuel consumption (attribute 1 ) using the four continuous attributes. We train the model using data on cars from origin 1 , and test on cars from origin 2 and origin 3 . We use the same test algorithms as the previous section, but now using a Gaussian process regressors for each regression function. The results of running this are in Table 2 . The Gaussian process hyper-parameters were optimised separately for each case. These are results obtained using a Bayesian Information Crite-rion for selecting the number of mixtures between 1 and 14 for each of the cases. We obtain similar results if we compare methods with various fixed numbers of mixtures. Critically, we note that all covariate shift methods performed better than a straight Gaussian Process predictor in this situation. The mixture of Gaussian processes did not perform as well as the methods which explicitly recog-nised the covariate shift, although interestingly did perform better than a straight Gaussian process predictor. Again the MRCS performed better overall. This paper establishes that explicit generative modelling of covariate shift can bring improvements over conditional regression models, or over standard covariate shift methods that ignore the depen-dent data in the modelling process. The method is also better than using an identical mixture of regressors model for the training data alone, as it utilises the positions of the independent test points to help refine the mixture locations and the separation of regressors.
 We expect significant improvements can be made with a fully Bayesian treatment of the parameters. This framework is currently being extended to the case of multiple training and test datasets using a fully Bayesian scheme, and will be the subject of future work. In this setting we have a Topic model, Table 2: Tests of methods on the auto-mpg dataset. These are the (standardised) mean squared errors for each approach. GP denotes the use of Gaussian Process regression for prediciton. Orgin 2, and Origin 3 denote the two different car origins used to test the model.
 similar to Latent Dirichlet Allocation, where each dataset is built from a number of contributing regression components, where each component is expressed in different proportions in each dataset. The model and tests of this paper show that this multiple dataset extension could well be fruitful. In this paper a novel approach to the problem of covariate shift has been developed that is demon-strably better than state of the art regression approaches, and better than the current standard for covariate shift. These have been tested on both generated data, and on a real problem of covariate shift, derived from a standard UCI dataset. Importance Weighted Least Squares is shown to be a special case. Specifically we provide explicit modelling of the covariate shift process by assuming a shift in the proportions of a number of latent components. A mixture of regressors model is used for this purpose, but it differs from standard mixture of regressors by allowing sharing of the regression functions between mixture components and explicitly including a model for the test set as part of the process.

