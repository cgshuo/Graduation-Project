 As spoken dialogue systems are designed to perform ever more elaborate tasks, the need for mixed-initiative interaction necessarily grows. Mixed-initiative interaction, where agents (both artificial and human) may freely contribute to reach a solution efficiently, has long been a focus of dialogue systems research (Allen et al., 1999; Guinn, 1996). Simple slot-filling tasks might not require the flexible environment that mixed-initiative interaction brings but those of greater complexity, such as collaborative task comple-tion or long-term planning, certainly do (Fergu-son et al., 1996). However, translating this interac-tion into working systems has proved problematic (Walker et al., 1997), in part to issues surround-ing turn-taking: the transition from one speaker to another.

Many computational turn-taking approaches seek to minimize silence and utterance overlap during transitions. This leads to the speaker con-trolling the turn transition. For example, systems using the Keep-Or-Release approach will not at-tempt to take the turn unless it is sure the user has released it. One problem with this approach is that the system might have important informa-tion to give but will be unable to get the turn. The speaker-centric nature of current approaches does not enable mixed-initiative interaction and results in inefficient dialogues. Primarily, these approaches have been motivated by smooth tran-sitions reported in the human turn-taking studies of Sacks et al. (1974) among others.

Sacks et al. also acknowledge the negotiative nature of turn-taking, stating that the  X  X he turn as unit is interactively determined X (p. 727). Other studies have supported this, suggesting that hu-mans negotiate the turn assignment through the use of cues and that these cues are motivated by the importance of what the conversant wishes to contribute (Duncan and Niederehe, 1974; Yang and Heeman, 2010; Schegloff, 2000). Given this, any dialogue system hoping to interact with humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking protocol. We believe that, by focus-ing on the rationale of human turn-taking be-havior, a more effective turn-taking system may be achieved. We propose the Importance-Driven Turn-Bidding (IDTB) model where conversants bid for the turn based on the importance of their utterance. We use Reinforcement Learning to map a given situation to the optimal utterance and bid-ding behavior. By allowing conversants to bid for the turn, the IDTB model enables negotiative turn-taking and supports true mixed-initiative interac-tion, and with it, greater dialogue efficiency.
We compare the IDTB model to current turn-taking approaches. Using an artificial collab-orative dialogue task, we show that the IDTB model enables the system and user to complete the task more efficiently than the other approaches. Though artificial dialogues are not ideal, they al-low us to test the validity of the IDTB model be-fore embarking on costly and time-consuming hu-man studies. Since our primary evaluation criteria is model comparison, consistent user simulations provide a constant needed for such measures and increase the external validity of our results. Current dialogue systems focus on the release-turn as the most important aspect of turn-taking, in which a listener will only take the turn after the speaker has released it. The simplest of these ap-proaches only allows a single utterance per turn, after which the turn necessarily transitions to the next speaker. This Single-Utterance (SU) model has been extended to allow the speaker to keep the turn for multiple utterances: the Keep-Or-Release (KR) approach. Since the KR approach gives the speaker sole control of the turn, it is overwhelm-ingly speaker-centric, and so necessarily unnego-tiative. This restriction is meant to encourage smooth turn-transitions, and is inspired by the or-der, smoothness, and predictability reported in hu-man turn-taking studies (Duncan, 1972; Sacks et al., 1974).

Systems using the KR approach differ on how they detect the user X  X  release-turn. Turn releases are commonly identified in two ways: either us-ing a silence-threshold (Sutton et al., 1996), or the predictive nature of turn endings (Sacks et al., 1974) and the cues associated with them (e.g. Gra-vano and Hirschberg, 2009). Raux and Eskenazi (2009) used decision theory with lexical cues to predict appropriate places to take the turn. Simi-larly, Jonsdottir, Thorisson, and Nivel (2008) used Reinforcement Learning to reduce silences be-tween turns and minimize overlap between utter-ances by learning the specific turn-taking patterns of individual speakers. Skantze and Schlangan (2009) used incremental processing of speech and prosodic turn-cues to reduce the reaction time of the system, finding that that users rated this ap-proach as more human-like than a baseline system.
In our view, systems built using the KR turn-taking approach suffer from two deficits. First, the speaker-centricity leads to inefficient dialogues since the speaker may continue to hold the turn even when the listener has vital information to give. In addition, the lack of negotiation forces the turn to necessarily transition to the listener af-ter the speaker releases it. The possibility that the dialogue may be better served if the listener does not get the turn is not addressed by current ap-proaches.

Barge-in, which generally refers to allowing users to speak at any time (Str  X  om and Seneff, 2000), has been the primary means to create a more flexible turn-taking environment. Yet, since barge-in recasts speaker-centric systems as user-centric, the system X  X  contributions continue to be limited. System barge-in has also been investi-gated. Sato et al. (2002) used decision trees to de-termine whether the system should take the turn or not when the user pauses. An incremental method by DeVault, Sagae, and Traum (2009) found pos-sible points that a system could interrupt without loss of user meaning, but failed to supply a rea-sonable model as to when to use such information. Despite these advances, barge-in capable systems lack a negotiative turn-taking method, and con-tinue to be deficient for reasons similar to those described above. We introduce the IDTB model to overcome the de-ficiencies of current approaches. The IDTB model has two foundational components: (1) The impor-tance of speaking is the primary motivation behind turn-taking behavior, and (2) conversants use turn-cue strength to bid for the turn based on this impor-tance. Importance may be broadly defined as how well the utterance leads to some predetermined conversational success, be it solely task comple-tion or encompassing a myriad of social etiquette components.

Importance-Driven Turn-Bidding is motivated by empirical studies of human turn-conflict res-olution. Yang and Heeman (2010) found an in-crease of turn conflicts during tighter time con-straints, which suggests that turn-taking is in-fluenced by the importance of task completion. Schlegoff (2000) proposed that persistent utter-ance overlap was indicative of conversants hav-ing a strong interest in holding the turn. Walker and Whittaker (1990) show that people will inter-rupt to remedy some understanding discrepancy, which is certainly important to the conversation X  X  success. People communicate the importance of their utterance through turn-cues. Duncan and Niederehe (1974) found that turn-cue strength was the best predictor of who won the turn, and this finding is consistent with the use of volume to win turns found by Yang and Heeman (2010).

The IDTB model uses turn-cue strength to bid for the turn based on the importance of the utter-ance. Stronger turn-cues should be used when the intended utterance is important to the overall suc-cess of the dialogue, and weaker ones when it is not. In the prototype described in Section 5, both the system and user agents bid for the turn after ev-ery utterance and the bids are conceptualized here as utterance onset: conversants should be quick to speak important utterances but slow with less important ones. This is relatively consistent with Yang and Heeman (2010). A mature version of our work will use cues in addition to utterance on-set, such as those recently detailed in Gravano and Hirshberg (2009). 1
A crucial element of our model is the judgment and quantization of utterance importance. We use Reinforcement Learning (RL) to determine impor-tance by conceptualizing it as maximizing the re-ward over an entire dialogue. Whatever actions lead to a higher return may be thought of as more important than ones that do not. 2 By using RL to learn both the utterance and bid behavior, the sys-tem can find an optimal pairing between them, and choose the best combination for a given conversa-tional situation. We build our dialogue system using the Informa-tion State Update approach (Larsson and Traum, 2000) and use Reinforcement Learning for action selection (Sutton and Barto, 1998). The system architecture consists of an Information State (IS) that represents the agent X  X  knowledge and is up-dated using a variety of rules. The IS also uses rules to propose possible actions. A condensed and compressed subset of the IS  X  the Reinforce-ment Learning State  X  is used to learn which pro-posed action to take (Heeman, 2007). It has been shown that using RL to learn dialogue polices is generally more effective than  X  X and crafted X  di-alogue policies since the learning algorithm may capture environmental dynamics that are unat-tended to by human designers (Levin et al., 2000).
Reinforcement Learning learns an optimal pol-icy, a mapping between a state s and action a , where performing a in s leads to the lowest ex-pected cost for the dialogue (we use minimum cost instead of maximum reward). An -greedy search is used to estimate Q-scores, the expected cost of some state X  X ction pair, where the system chooses a random action with probability and the argmin a Q ( s,a ) action with 1-probability. For Q-learning, a popular RL algorithm and the one used here, is commonly set at 0.2 (Sutton and Barto, 1998). Q-learning updates Q ( s,a ) based on the best action of the next state, given by the following equation, with the step size parameter  X  = 1 / p N ( s,a ) where N ( s,a ) is the number of times the s,a pair has been seen since the begin-ning of training.
 The state space should be formulated as a Markov Decision Process (MDP) for Q-learning to update Q-scores properly. An MDP relies on a first-order Markov assumption in that the transi-tion and reward probability from some s t ,a t pair is completely contained by that pair and is unaf-this assumption to be met, care is required when deciding which features to include for learning. The RL State features we use are described in the following section. In this section, we show how the IDTB ap-proach can be implemented for a collaborative slot filling domain. We also describe the Single-Utterance and Keep-Or-Release domain imple-mentations that we use for comparison. 5.1 Domain Task We use a food ordering domain with two partici-pants, the system and a user, and three slots: drink, burger, and side. The system X  X  objective is to fill all three slots with the available fillers as quickly as possible. The user X  X  role is to specify its de-sired filler for each slot, though that specific filler may not be available. The user simulation, while intended to be realistic, is not based on empirical data. Rather, it is designed to provide a rich turn-taking domain to evaluate the performance of dif-ferent turn-taking designs. We consider this a col-laborative slot-filling task since both conversants must supply information to determine the intersec-tion of available and desired fillers.

Users have two fillers for each slot. 3 A user X  X  top choice is either available, in which case we say that the user has adequate filler knowledge, or their second choice will be available, in which we say it has inadequate filler knowledge. This assures that at least one of the user X  X  filler is available. Whether a user has adequate or inadequate filler knowledge is probabilistically determined based on user type, which will be described in Section 5.2.

We model conversations at the speech act level, shown in Table 1, and so do not model the actual words that the user and system might say. Each agent has an Information State that proposes possi-ble actions. The IS is made up of a number of vari-ables that model the environment and is slightly different for the system and the user. Shared vari-ables include QUD , a stack which manages the questions under discussion; lastUtterance , the pre-vious utterance, and slotList , a list of the slot names. The major system specific IS variables that are not included in the RL State are availSlot-Fillers , the available fillers for each slot; and three slotFiller variables that hold the fillers given by the user. The major user specific IS variables are three desiredSlotFiller variables that hold an ordered list of fillers, and unvisitedSlots , a list of slots that the user believes are unfilled.

The system has a variety of speech actions: in-form [yes/no] , to answer when the user has asked a filler availability question; inform filler not avail-able , to inform the user when they have specified an unavailable filler; three query slot actions (one for each slot), a query which asks the user for a filler and is proposed if that specific slot is unfilled; three inform available slot fillers actions, which lists the available fillers for that slot and is pro-posed if that specific slot is unfilled or filled with an unavailable filler; and bye , which is always pro-posed.

The user has two actions. They can inform the system of a desired slot filler, inform slot filler , or query the availability of a slot X  X  top filler, query filler availability . A user will always respond with the same slot as a system query, but may change slots entirely for all other situations. Additional details on user action selection are given in Section 5.2.

Specific information is used to produce an in-stantiated speech action, what we refer to as an utterance. For example, the speech action inform slot filler results in the utterance of  X  X nform drink d1. X  A sample dialogue fragment using the Single-Utterance approach is shown in Table 2. Notice that in Line 3 the system informs the user that their first filler, d1 , is unavailable. The user then asks asks about the availability of its second drink choice, d2 (Line 4), and upon receiving an affirma-tive response (Line 5), informs the system of that filler preference (Line 6).

Spkr Speech Action Utterance 1 S: q. slot q. drink 2 U: i. slot filler i. drink d1 3 S: i. filler not avail i. not have d1 4 U: q. filler avail q. drink have d2 5 S: i. slot i. yes 6 U: i. slot filler i. drink d2 7 S: i. avail slot fillers i. burger have b1 Implementation in RL: The system uses RL to learn which of the IS proposed actions to take. In this domain we use a cost function based on dia-logue length and the number of slots filled with an available filler: C = Number of Utterances + 25  X  unavailablyFilledSlots. In the present implemen-tation the system X  X  bye utterance is costless. The system chooses the action that minimizes the ex-pected cost of the entire dialogue from the current state.

The RL state for the speaker has seven vari-ables: 4 QUD-speaker , the stack of speakers who have unresolved questions; Incorrect-Slot-Fillers , a list of slot fillers (ordered chronologically on when the user informed them) that are unavail-able and have not been resolved; Last-Sys-Speech-Action , the last speech action the system per-formed; Given-Slot-Fillers , a list of slots that the system has performed the inform available slot filler action on; and three booleans variables, slot-RL , that specify whether a slot has been filled cor-rectly or not (e.g. Drink-RL). 5.2 User Types We define three different types of users  X  Experts , Novices , and Intermediates . User types differ probabilistically on two dimensions: slot knowl-edge, and slot belief strength. We define experts to have a 90 percent chance of having adequate filler knowledge, intermediates a 50 percent chance, and novices a 10 percent chance. These proba-bilities are independent between slots. Slot belief strength represents the user X  X  confidence that it has adequate domain knowledge for the slot (i.e. the top choice for that slot is available). It is either a strong, warranted, or weak belief (Chu-Carroll and Carberry, 1995). The intuition is that experts should know when their top choice is available, and novices should know that they do not know the domain well.

Initial slot belief strength is dependent on user type and whether their filler knowledge is ade-quate (their initial top choice is available). Ex-perts with adequate filler knowledge have a 70, 20, and 10 percent chance of having Strong, War-ranted, and Weak beliefs respectfully. Similarly, intermediates with adequate knowledge have a 50, 25, and 25 percent chance of the respective belief strengths. When these user types have inadequate filler knowledge the probabilities are reversed to determine belief strength (e.g. Experts with inad-equate domain knowledge for a slot have a 70% chance of having a weak belief). Novice users al-ways have a 10, 10, and 80 percent chance of the respective belief strengths.

The user choses whether to use the query or inform speech action based on the slot X  X  belief strength. A strong belief will always result in an inform , a warranted belief resulting in an inform with p = 0 . 5 , and weak belief will result in an in-form with p = 0 . 25 . If the user is informed of the correct fillers by the system X  X  inform , that slot X  X  belief strength is set to strong. If the user is in-formed that a filler is not available, than that filler is removed from the desired filler list and the belief remains the same. 5 5.3 Turn-Taking Models We now discuss how turn-taking works for the IDTB model and the two competing models that we use to evaluate our approach. The system chooses its turn action based on the RL state and we add a boolean variable turn-action to the RL State to indicate when the system is performing a turn action or a speech action. The user uses belief to choose its turn action.
 Turn-Bidding: Agents bid for the turn at the end of each utterance to determine who will speak next. Each bid is represented as a value between 0 and 1, and the agent with the lower value (stronger bid) wins the turn. This is consistent with the use of utterance onset. There are 5 types of bids, highest, high, middle, low, and lowest , which are spread over a portion of the range as shown in Fig-ure 1. The system uses RL to choose a bid and a random number (uniform distribution) is gener-ated from that bid X  X  range. The users X  bids are de-termined by their belief strength, which specifies the mean of a Gaussian distribution, as shown in Figure 1 (e.g Strong belief implies a  X  = 0.35). Computing bids in this fashion leads to, on av-erage, users with strong beliefs bidding highest , warranted beliefs bidding in the middle , and weak beliefs bidding lowest . The use of the probabil-ity distributions allows us to randomly decide ties between system and user bids.
 Single-Utterance: The Single-Utterance (SU) approach, as described in Section 2, has a rigid turn-taking mechanism. After a speaker makes a single utterance the turn transitions to the listener. Since the turn transitions after every utterance the system must only choose appropriate utterances, not turn-taking behavior. Similarly, user agents do not have any turn-taking behavior and slot beliefs are only used to choose between a query and an inform .
 Keep-Or-Release Model: The Keep-Or-Release (KR) model, as described in Section 2, allows the speaker to either keep the turn to make multiple utterances or release it. Taking the same approach as English and Heeman (2005), the system learns to keep or release the turn after each utterance that it makes. We also use RL to determine which conversant should begin the dialogue. While the use of RL imparts some importance onto the turn-taking behavior, it is not influencing whether the system gets the turn when it did not already have it. This is an crucial distinction between KR and IDTB. IDTB allows the conversants to negotiate the turn using turn-bids motivated by importance, whereas in KR only the speaker determines when the turn can transition.

Users in the KR environment choose whether to keep or release the turn similarly to bid decisions. 6 After a user performs an utterance, it chooses the slot that would be in the next utterance. A number, k , is generated from a Gaussian distribution using belief strength in the same manner as the IDTB users X  bids are chosen. If k  X  0 . 55 then the user keeps the turn, otherwise it releases it. 5.4 Preliminary Turn-Bidding System We described a preliminary turn-bidding system in earlier work presented at a workshop (Selfridge and Heeman, 2009). A major limitation was an overly simplified user model. We used two user types, expert and novice, who had fixed bids. Ex-perts always bid high and had complete domain knowledge, and the novices always bid low and had incomplete domain knowledge. The system, using all five bid types, was always able to out bid and under bid the simulated users. Among other things, this situation gives the system complete control of the turn, which is at odds with the nego-tiative nature of IDTB. The present contribution is a more realistic and mature implementation. We now evaluate the IDTB approach by compar-ing it against the two competing models: Single-Utterance and Keep-Or-Release. The three turn-taking approaches are trained and tested in four user conditions: novice, intermediate, expert, and combined. In the combined condition, one of the three user types is randomly selected for each dia-logue. We train ten policies for each condition and turn-taking approach. Policies are trained using Q-learning, and  X  greedy search for 10000 epochs (1 epoch = 100 dialogues, after which the Q-scores are updated) with = 0 . 2 . Each policy is then ran over 10000 test dialogues with no exploration ( = 0 ), and the mean dialogue cost for that pol-icy is determined. The 10 separate policy values are then averaged to create the mean policy cost. The mean policy cost between the turn-taking ap-proaches and user conditions are shown in Table 3. Lower numbers are indicative of shorter dialogues, since the system learns to successfully complete the task in all cases.
 Table 3: Mean Policy Cost for Model and User condition 7 Model Novice Int. Expert Combined SU 7.61 7.09 6.43 7.05 KR 6.00 6.35 4.46 6.01 IDTB 6.09 5.77 4.35 5.52 Single User Conditions: Single user conditions show how well each turn-taking approach can op-timize its behavior for specific user populations and handle slight differences found in those pop-ulations. Table 3 shows that the mean policy cost of the SU model is higher than the other two mod-els which indicates longer dialogues on average. Since the SU system must respond to every user utterance and cannot learn a turn-taking strategy to utilize user knowledge, the dialogues are neces-sarily longer. For example, in the expert condition the best possible dialogue for a SU interaction will have a cost of five (three user utterances for each slot, two system utterances in response). This cost is in contrast to the best expert dialogue cost of three (three user utterances) for KR and IDTB in-teractions.

The IDTB turn-taking approach outperforms the KR design in all single user conditions ex-cept for novice (6.09 vs. 6.00). In this condi-tion, the KR system takes the turn first, informs the available fillers for each slot, and then releases the turn. The user can then inform its filler eas-ily. The IDTB system attempts a similar dialogue strategy by using highest bids but sometimes loses the turn when users also bid highest . If the user uses the turn to query or inform an unavailable filler the dialogue grows longer. However, this is quite rare as shown by small difference in perfor-mance between the two models. In all other single user conditions, the IDTB approach has shorter di-alogues than the KR approach (5.77 and 4.35 vs. 6.35 and 4.46). A detailed explanation of IDTB X  X  performance will be given in Section 6.1.
 Combined User Condition: We next measure performance on the combined condition that mixes all three user types. This condition is more realistic than the other three, as it better mimics how a system will be used in actual practice. The IDTB approach (mean policy cost = 5.52) outper-forms the KR (mean policy cost = 6.01) and SU (mean policy cost = 7.05) approaches. We also observe that KR outperforms SU. These results suggest that the more a turn-taking design can be flexible and negotiative, the more efficient the dia-logues can be.
 Exploiting User bidding differences: It fol-lows that IDTB X  X  performance stems from its ne-gotiative turn transitions. These transitions are dis-tinctly different than KR transitions in that there is information inherent in the users bids. A user that has a stronger belief strength is more likely to be have a higher bid and inform an available filler. Policy analysis shows that the IDTB system takes advantage of this information by using moderate bids  X  X either highest nor lowest bids X  to filter users based on their turn behavior. The distribu-tion of bids used over the ten learned policies is shown in Table 4. The initial position refers to the first bid of the dialogue; final position, the last bid of the dialogue; and medial position, all other bids. Notice that the system uses either the low or mid bids as its initial policy and that 67.2% of di-alogue medial bids are moderate. These distribu-tions show that the system has learned to use the entire bid range to filter the users, and is not seek-ing to win or lose the turn outright. This behavior is impossible in the KR approach.
 Table 4: Bid percentages over ten policies in the Combined User condition for IDTB Position H-est High Mid Low L-est Initial 0.0 0.0 70.0 30.0 0.0 Medial 20.5 19.4 24.5 23.3 12.3
Final 49.5 41.0 9.5 0.0 0.0 6.1 IDTB Performance: In our domain, performance is measured by dia-logue length and solution quality. However, since solution quality never affects the dialogue cost for a trained system, dialogue length is the only com-ponent influencing the mean policy cost.

The primary cause of longer dialogues are un-available filler inform and query (UFI X  X ) utter-ances by the user, which are easily identified. These utterances lengthen the dialogue since the system must inform the user of the available fillers (the user would otherwise not know that the filler was unavailable) and then the user must then in-form the system of its second choice. The mean number of UFI X  X  utterance for each dialogue over the ten learned policies are shown for all user con-ditions in Table 5. Notice that these numbers are inversely related to performance: the more UFI X  Q utterances, the worse the performance. For ex-ample, in the combined condition the IDTB users perform 0.38 UFI X  X  utterances per dialogue (u/d) compared to the 0.94 UFI X  X  u/d for KR users. While a KR user will release the turn if its planned Table 5: Mean number of UFI X  X  utterances over policies Model Novice Int. Expert Combined KR 0.0 1.15 0.53 0.94
IDTB 0.1 0.33 0.39 0.38 utterance has a weak belief, it may select that weak utterance when first getting the turn (either after a system utterance or at the start of the dialogue). This may lead to a UFI X  X  utterance. The IDTB system, however, will outbid the same user, result-ing in a shorter dialogue. This situation is shown in Tables 6 and 7. The dialogue is the same un-til utterance 3, where the IDTB system wins the turn with a mid bid over the user X  X  low bid. In the KR environment however, the user gets the turn and performs an unavailable filler inform, which the system must react to. This is an instance of the second deficiency of the KR approach, where Table 6: Sample IDTB dialogue in Combined User condition; Cost=6
Sys Usr Spkr Utt 1 low mid U: inform burger b1 2 h-est low S: inform burger have b3 3 mid low S: inform side have s1 4 mid h-est U: inform burger b3 5 mid high U: inform drink d1 6 l-est h-est U: inform side s1 7 high mid S: bye Table 7: Sample KR dialogue in Combined User condition; Cost=7
Agent Utt Turn-Action 1 U: inform burger b1 Release 2 S: inform burger have b3 Release 3 U: inform side s1 Keep 4 U: inform drink d1 Keep 5 U: inform burger b3 Release 6 S: inform side have s2 Release 7 U: inform side s2 Release 8 S: bye the speaking system should not have released the turn. The user has the same belief in both scenar-ios, but the negotiative nature of IDTB enables a shorter dialogues. In short, the IDTB system can win the turn when it should have it, but the KR system cannot.

A lesser cause of longer dialogues is an instance of the first deficiency of the KR systems; the lis-tening user cannot get the turn when it should have it. Usually, this situation presents itself when the user releases the turn, having randomly chosen the weaker of the two unfilled slots. The system then has the turn for more than one utterance, inform-ing the available fillers for two slots. However, the user already had a strong belief and available top filler for one of those slots, and the system has increased the dialogue length unnecessarily. In the combined condition, the KR system produces 0.06 unnecessary informs per dialogue, whereas the IDTB system produces 0.045 per dialogue. The novice and intermediate conditions mirror this (IDTB: 0.009, 0.076 ; KR: 0.019, 0.096 respect-fully), but the expert condition does not (IDTB: 0.011, KR: 0.0014). In this case, the IDTB system wins the turn initially using a low bid and informs one of the strong slots, whereas the expert user ini-tiates the dialogue for the KR environment and un-necessary informs are rarer. In general, however, the KR approach has more unnecessary informs since the KR system can only infer that one of the user X  X  beliefs was probably weak, otherwise the user would not have released the turn. The IDTB system handles this situation by using a high bid, allowing the user to outbid the system as its con-tribution is more important. In other words, the IDTB user can win the turn when it should have it, but the KR user cannot. This paper presented the Importance-Driven Turn-Bidding model of turn-taking. The IDTB model is motivated by turn-conflict studies showing that the interest in holding the turn influences conversant turn-cues. A computational prototype using Re-inforcement Learning to choose appropriate turn-bids performs better than the standard KR and SU approaches in an artificial collaborative dialogue domain. In short, the Importance-Driven Turn-Bidding model provides a negotiative turn-taking framework that supports mixed-initiative interac-tions.

In the previous section, we showed that the KR approach is deficient for two reasons: the speak-ing system might not keep the turn when it should have, and might release the turn when it should not have. This is driven by KR X  X  speaker-centric nature; the speaker has no way of judging the potential contribution of the listener. The IDTB approach however, due to its negotiative quality, does not have this problem.

Our performance differences arise from situa-tions when the system is the speaker and the user is the listener. The IDTB model also excels in the opposite situation, when the system is the listener and the user is the speaker, though our domain is not sophisticated enough for this situation to oc-cur. In the future we hope to develop a domain with more realistic speech acts and a more diffi-cult dialogue task that will, among other things, highlight this situation. We also plan on imple-menting a fully functional IDTB system, using an incremental processing architecture that not only detects, but generates, a wide array of turn-cues. We gratefully acknowledge funding from the National Science Foundation under grant IIS-0713698.
