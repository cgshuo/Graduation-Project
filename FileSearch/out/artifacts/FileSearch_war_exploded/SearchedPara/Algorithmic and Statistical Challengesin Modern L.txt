 We provide a report for the ACM SIGKDD community about the 2008 Workshop on Algorithms for Modern Mas-sive Data Sets (MMDS 2008), its origin in MMDS 2006, and future directions for this interdisciplinary research are a. The 2008 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2008) was sponsored by the NSF, DARPA, LinkedIn, and Yahoo! and was held at Stanford Univer-sity, June 25 X 28. The goals of MMDS 2008 were (1) to explore novel techniques for modeling and analyzing mas-sive, high-dimensional, and nonlinearly-structured scie ntific and internet data sets; and (2) to bring together computer scientists, statisticians, mathematicians, and data anal ysis practitioners to promote cross-fertilization of ideas. MMDS 2008 originally grew out of discussions about our vision for the next-generation of algorithmic, mathemati-cal, and statistical analysis methods for complex large-sc ale data sets. These discussions occurred in the wake of MMDS 2006, which was originally motivated by the complemen-tary perspectives brought by the numerical linear algebra and theoretical computer science communities to matrix al-gorithms in modern informatics applications [1]. As with the original 2006 meeting, the MMDS 2008 program gener-ated intense interdisciplinary interest: with 43 talks and 18 poster presentations from a wide spectrum of researchers in modern large-scale data analysis, including both senior re -searchers well-established as leaders in their respective fields as well as junior researchers promising to become leaders in this new interdisciplinary field, the program drew nearly 30 0 participants. Graph and matrix problems were common topics for dis-cussion, largely since they arise naturally in almost every aspect of data mining, machine learning, and pattern recog-nition. For example, a common way to model a large so-cial or information network is with an interaction graph  X 
Email: mmahoney@cs.stanford.edu  X 
Email: lekheng@math.berkeley.edu  X 
Email: gunnar@math.stanford.edu model , G = ( V, E ), in which nodes in the vertex set V repre-sent  X  X ntities X  and the edges (whether directed, undirecte d, weighted or unweighted) in the edge set E represent  X  X n-teractions X  between pairs of entities. Alternatively, the se and other data sets can be modeled as matrices, since an m  X  n real-valued matrix A provides a natural structure for encoding information about m objects, each of which is described by n features. Due to their large size, their ex-treme sparsity, and their complex and often adversarial noi se properties, data graphs and data matrices arising in modern informatics applications present considerable challenge s and opportunities for interdisciplinary research. These algo rith-mic, statistical, and mathematical challenges were the foc us of MMDS 2008.
 It is worth emphasizing the very different perspectives that have historically been brought to such problems. For exam-ple, a common view of the data in a database, in particu-lar historically among computer scientists interested in d ata mining and knowledge discovery, has been that the data are an accounting or a record of everything that happened in a particular setting. For example, the database might consis t of all the customer transactions over the course of a month, or it might consist of all the friendship links among members of a social networking site. From this perspective, the goal is to tabulate and process the data at hand to find inter-esting patterns, rules, and associations. An example of an association rule is the proverbial  X  X eople who buy beer be-tween 5 p.m. and 7 p.m. also buy diapers at the same time. X  The performance or quality of such a rule is judged by the fraction of the database that satisfies the rule exactly, whi ch then boils down to the problem of finding frequent itemsets. This is a computationally hard problem, and much algo-rithmic work has been devoted to its exact or approximate solution under different models of data access.
 A very different view of the data, more common among statisticians, is one of a particular random instantiation of an underlying process describing unobserved patterns in th e world. In this case, the goal is to extract information about the world from the noisy or uncertain data that is observed. To achieve this, one might posit a model: data  X  F  X  and mean( data ) = g (  X  ), where F  X  is a distribution that describes the random variability of the data around the deterministic model g (  X  ) of the data. Then, using this model, one would proceed to analyze the data to make inferences about the underlying processes and predictions about future observa -tions. From this perspective, modeling the noise component or variability well is as important as modeling the mean structure well, in large part since understanding the for-mer is necessary for understanding the quality of predictio ns made. With this approach, one can even make predictions about events that have yet to be observed. For example, one can assign a probability to the event that a given user at a given web site will click on a given advertisement presented at a given time of the day, even if this particular event does not exist in the database.
 The two perspectives need not be incompatible. For exam-ple, statistical and probabilistic ideas are central to muc h of the recent work on developing improved approximation algorithms for matrix problems; otherwise intractable opt i-mization problems on graphs and networks yield to approx-imation algorithms when assumptions are made about the network participants; much recent work in machine learning draws on ideas from both areas; and in boosting, a statisti-cal technique that fits an additive model by minimizing an objective function with a method such as gradient descent, the computation parameter, i.e., the number of iterations, also serves as a regularization parameter.
 Given the diversity of possible perspectives, MMDS 2008 was organized loosely around six hour-long tutorials that introduced participants to the major themes of the work-shop. On the first day of the workshop, participants heard tutorial s by Christos Faloutsos of Carnegie Mellon University and Edward Chang of Google Research, in which they presented an overview of tools and applications in modern large-scale data analysis.
 Faloutsos began his tutorial on  X  X raph mining: laws, gener-ators and tools X  by motivating the problem of data analysis on graphs. He described a wide range of applications in which graphs arise naturally, and he reminded the audience that large graphs that arise in modern informatics applica-tions have structural properties that are very different fro m traditional Erd  X os-R  X enyi random graphs. For example, due to subtle correlations, statistics such as degree distribu tions and eigenvalue distributions exhibit heavy-tailed behavi or. Although these structural properties have been studied ex-tensively in recent years and have been used to develop numerous well-publicized models, Faloutsos also describe d empirically-observed properties that are not well-reprod uced by existing models. As an example, most models predict that over time the graph should become sparser and the di-ameter should grow as O (log N ) or perhaps O (log log N ), where N is the number of nodes at the current time step, but empirically it is often observed that the networks den-sify over time and that their diameter shrinks. To explain these phenomena, Faloutsos described a model based on Kronecker products and also a model in which edges are added via an iterative  X  X orest fire X  burning mechanism. With appropriate choice of parameters, both models can be made to reproduce a much wider range of static and dynamic prop-erties than can previous generative models.
 Building on this modeling foundation, Faloutsos spent much of his talk describing several graph mining applications of recent and ongoing interest: methods to find nodes that are central to a group of individuals; applications of the Sin-gular Value Decomposition and recently-developed tensor methods to identifying anomalous patterns in time-evolvin g graphs; modeling information cascades in the blogosphere a s virus propagation; and novel methods for fraud detection. Edward Chang described other developments in web-scale data analysis in his tutorial on  X  X ining large-scale social networks: challenges and scalable solutions. X  After revie w-ing emerging applications X  X uch as social network analysis and personalized information retrieval X  X hat have arisen a s we make the transition from Web 1.0 (links between pages and documents) to Web 2.0 (links between documents, peo-ple, and social platforms), Chang covered four application s in detail: spectral clustering for network analysis, frequ ent itemset mining, combinatorial collaborative filtering, an d parallel Support Vector Machines (SVMs) for personalized search. In all these cases, he emphasized that the main per-formance requirements were  X  X calability, scalability, sc ala-bility. X  Modern informatics applications like web search afford easy parallelization X  X .g., the overall index can be partitione d such that even a single query can use multiple processors. Moreover, the peak performance of a machine is less impor-tant than the price-performance ratio. In this environment , scalability up to petabyte-sized data often means working i n a software framework like MapReduce or Hadoop that sup-ports data-intensive distributed computations running on large clusters of hundreds, thousands, or even hundreds of thousands of commodity computers. This differs substan-tially from the scalability issues that arise in traditiona l ap-plications of interest in scientific computing. A recurrent theme of Chang was that an algorithm that is expensive in floating point cost but readily parallelizable is often a bet ter choice than one that is less expensive but non-parallelizab le. As an example, although SVMs are widely-used, largely due to their empirical success and attractive theoretical foun -dations, they suffer from well-known scalability problems i n both memory use and computational time. To address these problems, Chang described a Parallel SVM algorithm. This algorithm reduces memory requirements by performing a row-based Incomplete Cholesky Factorization (ICF) and by loading only essential data to each of the parallel machines ; and it reduces computation time by intelligently reorderin g computational steps and by performing them on parallel ma-chines. Chang noted that the traditional column-based ICF is better for the single machine setting, but it cannot be parallelized as well across many machines. Milena Mihail of the Georgia Institute of Technology de-scribed algorithmic perspectives on developing better mod -els for data in her tutorial  X  X odels and algorithms for com-plex networks. X  She noted that in recent years a rich theory of power law random graphs, i.e., graphs that are random conditioned on a specified input power law degree distribu-tion, has been developed. With the increasingly wide range of large-scale social and information networks that are ava il-able, however, generative models that are structurally or syntactically more flexible are increasingly necessary. Mi -hail described two such extensions: one in which semantics on nodes is modeled by a feature vector and edges are added between nodes based on their semantic proximity; and one in which the phenomenon of associativity/disassociativit y is modeled by fixing the probability that nodes of a given degree d i tend to link to nodes of degree d j .
 By introducing a small extension in the parameters of a gen-erative model, of course, one can observe a large increase in the observed properties of generated graphs. This obser-vation raises interesting statistical questions about mod el overfitting, and it argues for more refined and systematic methods of model parameterization. This observation also leads to new algorithmic questions that were the topic of Mihail X  X  talk.
 An algorithmic question of interest in the basic power law random graph model is the following: given as input an N -vector specifying a degree sequence, determine whether there exists a graph with that degree sequence, and, if so, efficiently generate one (perhaps approximately uniformly randomly from the ensemble of such graphs). Such realiz-ability problems have a long history in graph theory and theoretical computer science. Since their solutions are in -timately related to the theory of graph matchings, many generalizations of the basic problem can be addressed in a strict theoretical framework. For example, motivated by associative/disassociative networks, Mihail described r ecent progress on the Joint-Degree Matrix Realization Problem: given a partition of the node set into classes of vertices of the same degree, a vector specifying the degree of each class , and a matrix specifying the number of edges between any two classes, determine whether there exists such a graph, and if so construct one. She also described extensions of thi s basic problem to connected graphs, to finding minimum cost realizations, and and to finding a random graph satisfying those basic constraints. A very different perspective was provided by Gunnar Carls-son of Stanford University, who gave an overview of geo-metric and topological approaches to data analysis in his tutorial  X  X opology and data. X  The motivation underlying these approaches is to provide insight into the data by im-posing a geometry on it. Whereas in certain applications, such as in physics, the studied phenomena support clean ex-planatory theories which define exactly the metric to use to measure the distance between pairs of data points, in most MMDS applications this is not the case. For instance, the Euclidean distance between DNA expression profiles in high-throughput microarray experiments may or may not capture a meaningful notion of distance between genes. Similarly, although a natural geodesic distance is associated with any graph, the sparsity and noise properties of social and infor -mation networks means that this is not a particularly robust notion of distance in practice.
 Part of the problem is thus to define useful metrics X  X n particular since applications such as clustering, classifi ca-tion, and regression often depend sensitively on the choice of metric X  X nd two design goals have recently emerged. First, don X  X  trust large distances X  X ince distances are often con-structed from a similarity measure, small distances reliab ly represent similarity but large distances make little sense . Second, trust small distances only a bit X  X fter all, similar ity measurements are still very noisy. These ideas have formed the basis for much of the work on Laplacian-based non-linear dimensionality reduction, i.e., manifold-based, m eth-ods that are currently popular in harmonic analysis and ma-chine learning. More generally, they suggest the design of analysis tools that are robust to stretching and shrinking of the underlying metric, particularly in applications suc h as visualization in which qualitative properties, such as h ow the data are organized on a large scale, are of interest. Much of Carlsson X  X  tutorial was occupied by describing thes e analysis tools and their application to natural image stati s-tics and data visualization. Homology is the crudest mea-sure of topological properties, capturing information suc h as the number of connected components, whether the data con-tain holes of various dimensions, etc. Importantly, althou gh the computation of homology is not feasible for general topo -logical spaces, in many cases the space can be modeled in terms of simplicial complexes, in which case the computa-tion of homology boils down to the linear algebraic compu-tation of the Smith normal form of certain data-dependent matrices. Carlsson also described persistent homology , an extension of the basic idea in which parameters such as the number of nearest neighbors, error parameters, etc., can be varied. A  X  X ar code signature X  can then be associated with the data set. Long segments in the bar code indicate the presence of a homology class which persists over a long range of parameters values. This can often be interpreted as corre -sponding to large-scale geometric features in the data, whi le shorter segments can be interpreted as noise. Statistical and machine learning perspectives on MMDS were the subject of a pair of tutorials by Jerome Friedman of Stanford University and Michael Jordan of the University of California at Berkeley. Given a set of measured values of attributes of an object, x = ( x 1 , x 2 , . . . , x n ), the basic pre-dictive or machine learning problem is to predict or estimat e the unknown value of another attribute y . The quantity y is the  X  X utput X  or  X  X esponse X  variable, and { x 1 , x 2 , . . . , x are the  X  X nput X  or  X  X redictor X  variables. In regression pro b-lems, y is a real number, while in classification problems, y is a member of a discrete set of unorderable categorical values (such as class labels). In either case, this can be viewed as a function estimation problem X  X he prediction takes the form of a function  X  y = F ( x ) that maps a point x in the space of all joint values of the predictor variables to a point  X  y in the space of response variables, and the goal is to produce an F ( ) that minimizes a loss criterion.
 In his tutorial,  X  X ast sparse regression and classification , X  Friedman began with the common assumption of a linear model, in which F ( x ) = P n j =1 a j x j is modeled as a linear combination of the n basis functions. Unless the number of observations is much much larger than n , however, empiri-cal estimates of the loss function exhibit high variance. To make the estimates more regular, one typically considers a constrained or penalized optimization problem where  X  L ( ) is the empirical loss and P  X  ( ) is a penalty term. The choice of an appropriate value for the regularization parameter  X  is a classic model selection problem, for which cross validation can be used. The choice for the penalty depends on what is known or assumed about the problem at hand. A common choice is P  X  ( a ) = k a k  X   X  = P n j =1 This interpolates between the subset selection problem (  X  = 0) and ridge regression (  X  = 2) and includes the well-studied lasso (  X  = 1). For  X   X  1, sparse solutions (which are of interest due to parsimony and interpretability) are obtain ed, and for  X   X  1, the penalty is convex.
 Although one could choose an optimal (  X ,  X  ) by cross vali-dation, this can be prohibitively expensive, even when the loss and penalty are convex, due to the need to perform computations at a large number of discretized pairs. In this case, path seeking methods have been studied. Consider the path of optimal solutions {  X  a (  X  ) : 0  X   X   X   X  X  , which is a one-dimensional curve in the parameter space R n . If the loss function is quadratic and the penalty function is piece -wise linear, e.g., with the lasso, then the path of optimal solutions is piecewise linear, and homotopy methods can be used to generate the full path in time that is not much more than that needed to fit a single model at a single parameter value. Friedman described a generalized path seeking al-gorithm, which solves this problem for a much wider range of loss and penalty functions (including some non-convex functions) very efficiently.
 Jordan, in his tutorial  X  X ernel-based contrast functions f or sufficient dimension reduction, X  considered the dimension-ality reduction problem in a supervised learning setting. Methods such as Principal Components Analysis, Johnson-Lindenstrauss techniques, and recently-developed Laplac ian-based non-linear methods are often used, but their applica-bility is limited since, e.g., the axes of maximal discrimin a-tion between two the classes may not align well with the axes of maximum variance. Instead, one might hope that there exists a low-dimensional subspace S of the input space X which can be found efficiently and which retains the sta-tistical relationship between X and the response space Y . Conventional approaches to this problem of Sufficient Di-mensionality Reduction (SDR) make strong modeling as-sumptions about the distribution of the covariate X and/or the response Y . Jordan considered a semiparametric for-mulation, where the conditional distribution p ( Y | X ) is treated nonparametrically and the goal is estimate the pa-rameter S . He showed that this problem could be formu-lated in terms of conditional independence and that it could be evaluated in terms of operators on Reproducing Kernel Hilbert Spaces (RKHSs).
 Recall that claims about the independence between two ran-dom variables can be reduced to claims about correlations between them by considering transformations of the random variables: X 1 and X 2 are independent if and only if for a suitably rich function space H . If H is L 2 and thus contains the Fourier basis, this reduces to a well-known fact about characteristic functions. More interesting fro m a computational perspective X  X ecall that by the  X  X eproduc-ing X  property, function evaluation in a RKHS reduces to an inner product X  X his also holds for suitably rich RKHSs. This use of RKHS ideas to solve this SDR problem can-not be viewed as a kernelization of an underlying linear al-gorithm, as is typically the case when such ideas are used (e.g., with SVMs) to provide basis expansions for regressio n and classification. Instead, this is an example of how RKHS ideas provide algorithmically efficient machinery to optimi ze a much wider range of statistical functionals of interest. In addition to other talks on the theory of data algorithms, machine learning and kernel methods, dimensionality re-duction and graph partitioning methods, and co-clustering and other matrix factorization methods, participants hear d about a wide variety of data applications, including movie and product recommendations; predictive indexing for fast web search; pathway analysis in biomolecular folding; func -tional MRI, high-resolution terrain analysis, and galaxy c las-sification; and other applications in computational geome-try, computer graphics, computer vision, and manifold lear n-ing. (We even heard about using approximation algorithms in a novel manner to probe the community structure of large social and information networks to test the claim that such data are even consistent with the manifold hypothesis X  they clearly are not.) In all these cases, scalability was a central issue X  X otivating discussion of external memory al -gorithms, novel computational paradigms like MapReduce, and communication-efficient linear algebra algorithms. In-terested readers are invited to visit the conference websit e, http://mmds.stanford.edu , where the presentations from all speakers can be found.
 The feedback we received made it clear that MMDS has struck a strong interdisciplinary chord. For example, near ly every statistician commented on the desire for more statist i-cians at the next MMDS; nearly every scientific computing researcher told us they wanted more data-intensive scienti fic computation at the next MMDS; nearly every practitioner from an application domain wanted more applications at the next MMDS; and nearly every theoretical computer scientist said they wanted more of the same. There is a lot of interest in MMDS as a developing interdisciplinary research area at the interface between computer science, statistics, appli ed mathematics, and scientific and internet data applications . Keep an eye out for future MMDSs! The authors are grateful to the numerous individuals (in particular, Mayita Romero, Victor Olmo, and David Gleich) who provided assistance prior to and during MMDS 2008; to Diane Lambert for providing an interesting perspective on these problems that we incorporated here; and to each of the speakers, poster presenters, and other participants, with out whom MMDS 2008 would not have been such a success. [1] G.H. Golub, M.W. Mahoney P. Drineas, and L.-H. Lim,
