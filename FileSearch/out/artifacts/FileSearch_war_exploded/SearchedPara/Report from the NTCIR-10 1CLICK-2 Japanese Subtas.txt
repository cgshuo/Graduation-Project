 The One Click Access Task (1CLICK) of NTCIR requires systems to return a concise multi-document summary of web pages in re-sponse to a query which is assumed to have been submitted in a mobile context. Systems are evaluated based on information units (or iUnits ), and are required to present important pieces of infor-mation first and to minimise the amount of text the user has to read. Using the official Japanese results of the second round of the 1CLICK task from NTCIR-10, we discuss our task setting and evaluation framework. Our analyses show that: (1) Simple baseline methods that leverage search engine snippets or Wikipedia are ef-fective for  X  X ookup X  type queries but not necessarily for other query types; (2) There is still a substantial gap between manual and auto-matic runs; and (3) Our evaluation metrics are relatively robust to the incompleteness of iUnits.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation evaluation, information units, mobile environment, NTCIR, nuggets, summaries, test collections NTCIR (NII Testbeds and Community for Information access Research) is a sesquiannual evaluation forum that focuses primarily on Asian language information access. The One Click Access Task (1CLICK) of NTCIR requires systems to return a concise multi-document summary of web pages in response to a query which is assumed to have been submitted in a mobile context 1 . Systems are evaluated based on information units (or iUnits ), and are required
However, in practice, we found it easier to construct iUnits by allowing them to entail multiple iUnits.
The number of documents per query k varied across queries as some documents could not be downloaded. The average of k over the query set is 390. Figure 2: Mean S -measure performances ( L = 500 )at 1CLICK-2. The x axis represents runs sorted by Mean S with the intersection iUnit match data.
Figure 2 shows the official mean S -measure performances ( L = 500 ) of runs submitted to the 1CLICK-2 Japanese subtask. The x axis represents run names sorted by mean S . Figure 3 breaks down Figure 2 by showing similar results per query type. For the four CELEBRITY query types, we also show graphs for specialised and non-specialised queries separately. Hereafter, we shall discuss statistical significance based on a randomised version of Tukey X  X  Honestly Significant Differences (HSD) test [1] at  X  =0 . 05 .
It can be observed that the three  X  X RG X  runs are the overall top performers in Figure 2. These are actually simple baseline runs submitted by the organisers X  team: ORG-J-D-MAND-1 is a DESK-TOP mandatory run that outputs a concatenation of search engine snippets from the baseline search results; ORG-J-D-MAND-2 is a DESKTOP mandatory run that outputs the first sentences of a top-ranked Wikipedia article found in the baseline search results; ORG-J-D-ORCL-3 is similar to ORGs-J-D-MAND-1 but uses the sources of iUnits instead of the search results (an oracle run). These three runs significantly outperform the other runs, and are signif-icantly indistinguishable from one another. Moreover, Figure 3 shows that these baseline runs outperform all participating runs with the four CELEBRITY query types as well as DEFINITION, while they are not as effective for FACILITY, GEO and QA. Fur-thermore, the graphs for CELEBRITY (where the runs have been sorted by the mean S over all CELEBRITY queries) reveals that while the baseline runs are effective for the non-specialised queries, they are not necessarily so for the specialised ones. Recall that specialised CELEBRITY queries seek specific information about a celebrity such as  X  X ichael jackson death . X  Also, note that FA-CILITY, GEO and QA queries also seek specific information, e.g. contact information of restaurants, sentences that directly answer the natural language questions, and so on.

In summary, the above results suggest that, while simple snippet-based and Wikipedia-based approaches are effective for  X  X ookup X  type queries (e.g. celebrity names and definitions), more sophis-ticated techniques are required to satisfy the user for other query types (e.g. queries that look for specific information).
We now address the following question: Given the 1CLICK eval-uation framework, what is the performance upperbound? .Tothis end, we hired four subjects and asked them to manually create an Figure 4: Comparison between MANUAL and submitted runs in terms of mean S -measure ( L = 500 ) over 73 queries. The x axis represents runs sorted by Mean S with the intersection iUnit match data.

Figure 4 shows the mean S -measure ( L = 500 ) over the 73 queries for all runs including the MANUAL ones. It can be ob-served that three of the four MANUAL runs far outperform the submitted automatic runs: these three runs are statistically signifi-cantly better than the other runs, and are statistically indistinguish-able from one another. These results suggest that there are a lot of challenges for advancing the state-of-the-art of 1CLICK systems: a highly effective 1CLICK system needs to (a) find the right docu-ments; (b) extract the right pieces from information from the doc-uments; and (c) synthesise the extracted information to form an understandable text. It should also be noted that S does not di-rectly take into account the readability of text: in fact, all of the runs were evaluated also in terms of readability (how easy it is for the user to read and understand the text), and the MANUAL runs outperformed the submitted runs in terms of this criterion as well.
In summary, the comparison with the MANUAL runs shows that our task setting is challenging, and that there is a lot of room for improvement for the automatic 1CLICK systems. We hope that the future rounds of the 1CLICK task will help close the performance gap.
Evaluating 1CLICK systems requires much manpower: for the 1CLICK-2 task, the organisers manually extracted over 6,000 iU-nits for the 100 queries in advance, and further added some new iUnits based on assessors X  feedback. In this section we address the following questions: Do we need to try to find iUnits for a query exhaustively? What happens to the system ranking if the sets of iU-nits were substantially incomplete? To this end, we follow a prac-tice from document retrieval evaluation for examining the effect of incomplete relevance assessments (e.g. [4]): we randomly down-sample from the official sets of iUnits and examine the changes in the system ranking in terms of Kendall X  X  tau rank correlation.
Figure 5 shows the effect of downsampling the iUnits on the sys-tem ranking for three of our evaluation metrics. It can be observed, for example, that even if we only have 50% samples of the offi-cial iUnit sets, the Kendall X  X  tau between the original system rank-ing and the new ranking is around 0.90 (about seven pairs of runs swapped) or higher for both S and weighted recall. Even with 10% samples, the tau is above 0.80. These results suggest that our eval-uation framework is fairly robust to the incompleteness of iUnits.
Since even randomly downsampled iUnits yield relatively reli-able evaluation results, exploring (semi)automatic approaches to iUnit extraction seems worthwhile. As we have mentioned earlier,
