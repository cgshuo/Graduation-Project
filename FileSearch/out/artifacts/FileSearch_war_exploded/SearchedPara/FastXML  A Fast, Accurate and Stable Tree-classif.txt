 The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking problems with certain advantages over existing formulations.
Our objective, in this paper, is to develop an extreme multi-label classifier that is faster to train and more accu-rate at prediction than the state-of-the-art Multi-label Ran-dom Forest (MLRF) algorithm [2] and the Label Partition-ing for Sub-linear Ranking (LPSR) algorithm [35]. MLRF and LPSR learn a hierarchy to deal with the large num-ber of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves sig-nificantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the pro-posed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Machine Learning, Optimization Multi-label Learning; Ranking; Extreme Classification
The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. For instance, there are more than a million categories on Wikipedia and one might wish to build a classifier that annotates a novel web page with the subset of most relevant Wikipedia categories. It should be emphasized that multi-label learning is distinct from multi-class classification which aims to predict a single mutually exclusive label.

Extreme classification is an important research problem not just because modern day applications have many cat-egories but also because it allows the reformulation of core learning problems such as recommendation and ranking. For instance, [2] treated search engine queries as labels and built an extreme classifier which, given a novel web page, returned a ranking of queries in decreasing order of relevance. Sim-ilarly, [35] treated YouTube videos as distinct labels in an extreme classifier so as to recommend a ranked list of videos to users. Both methods provided a fresh way of thinking about ranking and recommendation problems that led to significant improvements over the state-of-the-art.
Extreme classification is also a challenging research prob-lem as one needs to simultaneously deal with a large num-ber of labels, dimensions and training points. An obvious baseline is provided by the 1-vs-All technique where an inde-pendent classifier is learnt per label. Such a baseline has at least two major limitations. First, training millions of high dimensional classifiers might be computationally expensive. Second, the cost of prediction might be high since all the classifiers would need to be evaluated every time a predic-tion needed to be made. These problems could be amelio-rated if a label hierarchy was provided. Unfortunately, such a hierarchy is unavailable in many applications [2, 35].
State-of-the-art methods therefore learn a hierarchy from training data as follows: The root node is initialized to con-tain the entire label set. A node partitioning formulation is then optimized to determine which labels should be as-signed to the left child and which to the right. Nodes are recursively partitioned till each leaf contains only a small number of labels. During prediction, a novel data point is passed down the tree until it reaches a leaf node. A base multi-label classifier of choice can then be applied to the data point focusing exclusively on the leaf node label distri-bution. This leads to prediction costs that are sub-linear or even logarithmic if the tree is balanced.

Tree based methods can often outperform the 1-vs-All baseline in terms of prediction accuracy at a fraction of the prediction cost. However, such methods can also be expensive to train. In particular, the Label Partitioning by Sublinear Ranking (LPSR) algorithm of [35] can have even higher training costs than the 1-vs-All baseline since it needs to learn the hierarchy in addition to the base classifier. Similarly, the Multi-label Random Forest (MLRF) approach of [2] required a cluster of a thousand nodes as random for-est training was found to be expensive in high dimensional spaces. Such expensive approaches not only increase the cost of deploying extreme classification models and the cost of daily experimentation but also put such models beyond the reach of most practitioners.

Our objective, in this paper, is to tackle this problem and build a tree based extreme multi-label classifier, referred to as FastXML, that is faster to train as well as more accurate than the state-of-the-art MLRF and LPSR. Our key techni-cal contributions are a novel node partitioning formulation and an algorithm for its efficient optimization. In terms of training time, FastXML can be significantly faster than MLRF since the proposed node partitioning formulation can be optimized more efficiently than the entropy or Gini in-dex based formulations in random forests. At the same time, FastXML can be faster to train than LPSR which has to first learn computationally expensive base classifiers for accurate prediction. In terms of prediction accuracy, almost all ex-treme classification applications deal with scenarios where the number of relevant positive labels for a data point is orders of magnitude smaller than the number of irrelevant negative ones. Prediction accuracy is therefore not measured using traditional multi-label metrics, such as the Hamming loss, which give equal weightage to all labels whether posi-tive or negative. Instead, most applications prefer employ-ing ranking based measures such as precision at k which focuses exclusively on the positive labels by counting the number of correct predictions in the top k positive predic-tions. FastXML X  X  proposed node partitioning formulation therefore directly optimizes a rank sensitive loss function which can lead to more accurate predictions over MLRF X  X  Gini index or LPSR X  X  clustering error.

Experiments indicated that FastXML could be significantly more accurate at prediction (by as much as 5% in some cases) on benchmark data sets with thousands of labels where accurate models for MLRF, LPSR and the 1-vs-All baseline could be learnt. Furthermore, FastXML could efficiently scale to problems involving a million labels where accurate training of MLRF, LPSR and 1-vs-ALL models would re-quire a very large cluster. For instance, using a single core on a standard desktop, a single FastXML tree could be trained in under 10 minutes on a data set with about 4 M training points, 160 K features and 1 M labels. The entire ensem-ble could be trained in 8 hours using a single core and in 1 hour using multiple cores. MLRF and 1-vs-All could not be trained on such extreme multi-label data sets on a standard desktop. LPSR training could be made tractable by replac-ing the computationally expensive 1-vs-All base classifier by the cheaper Na  X   X ve Bayes but then its classification accuracy was found to lag behind by more than 20% on data sets such as Wikipedia.

Our contributions are: (a) We formulate a novel node par-titioning objective which directly optimizes an nDCG based ranking loss and which implicitly learns balanced partitions; (b) We propose an efficient optimization algorithm for the novel formulation; and (c) we combine these in a tree al-gorithm which can train on problems with a million labels on a standard desktop while increasing prediction accuracy. Code for FastXML can be downloaded by clicking here.
There has been much recent progress in extreme multi-label classification [2,4,8,10,12,15,19,20,22,29,34 X 36,38] and most approaches are either based on trees or on embeddings.
To clarify the discussion, it is assumed that the training set can be represented as { ( x i , y i ) N i =1 } with D dimensional real-valued feature vectors x i  X  R D with sparsity O ( and L dimensional binary label vectors y i  X  { 0 , 1 } y il = 1 if label l is relevant for point i and 0 otherwise. If one further assumes that the cost of training a linear binary classifier is O ( N  X  D ), then the training cost of the linear 1-vs-All baseline is O ( LN  X  D ) and the prediction cost is O ( L This is infeasible when L and N are in the range 10 10 6 . One can mitigate the training cost by sub-sampling the negative class for each binary classifier but the prediction cost would still be high.

Embedding methods [4, 8, 10, 12, 15, 19, 20, 22, 29, 34, 36, 38] exploit label correlations and sparsity to compress the num-ber of labels from L to  X  L . A low-dimensional embedding of the label space is found typically through a linear projection  X  y = Py where P is a  X  L  X  L projection matrix. The 1-vs-All strategy can now be applied and the cost of training and prediction in the compressed space reduces to O (  X  LN  X  D ) and O (  X 
L  X  D ) respectively. The compressed label predictions also need to be uncompressed at an additional cost of O ( or higher. Methods mainly differ in the choice of com-pression and decompression techniques such as compressed sensing [19, 22], Bloom filters [12], SVD [29], landmark la-bels [4, 8], output codes [38], etc . An interesting variant can be obtained by compressing the features  X  x = Rx to the same  X  L dimensional space as the labels [34]. Predictions can then be efficiently made in the embedding space via nearest neighbour techniques and no decompression is required.
Embedding methods have many advantages including sim-plicity, ease of implementation, strong theoretical founda-tions, the ability to handle label correlations, the ability to adapt to online and incremental scenarios, etc . Unfortu-nately, embedding methods can also pay a heavy price in terms of prediction accuracy due to the loss of information during the compression phase. For instance, none of the embedding methods developed so far have been able to con-sistently outperform the 1-vs-All baseline when  X  L  X  log( L ).
Tree methods that learn a hierarchy enjoy many of the same advantages as the embedding methods. In addition, they can outperform the 1-vs-All baseline even when the prediction costs are O (  X  D log L ). Our primary comparison is therefore with tree-based methods.

The Label Partitioning by Sub-linear Ranking (LPSR) method of [35] focussed on reducing the prediction time by learning a hierarchy over a base classifier or ranker. First, a base multi-label classifier was learnt for the entire label set. This step governs the overall training complexity and pre-diction accuracy. Generative classifiers such as Na  X   X ve Bayes are quick to train but have low accuracy whereas discrimina-tive classifiers such as 1-vs-All with linear SVMs [18], deep nets or Wsabie [34] are expensive to train but have higher accuracy. Next, a hierarchy was learnt in terms of a single binary tree. Nodes in the tree were grown by partitioning the node X  X  data points into 2 clusters, corresponding to the left and the right child, using a variant of k-means over the feature vectors. The tree was grown until each leaf node had a small number of data points and corresponding labels. Fi-nally, a relaxed integer program was optimized at each leaf node via gradient descent to activate a subset of the labels present at the node. During prediction, a novel point was passed down the tree until it reached a leaf node. Predic-tions were then made using the base classifier restricted to the set of active leaf node labels.

The Multi-label Random Forest (MLRF) approach of [2] did not need to learn a base classifier. Instead, an ensemble of randomized trees was learnt. Nodes were partitioned into a left and a right child by brute force optimization of a multi-label variant of the Gini index defined over the set of positive labels in the node. Trees were grown until each leaf node had only a few labels present. During testing, a novel point was passed down each tree and predictions were made by aggregating all the leaf node label distributions.

Both LPSR and MLRF have high training costs. LPSR needs to train an accurate base multi-label classifier, perform hierarchical k-means clustering and solve a label assignment problem at each leaf node. MLRF needs to learn an ensem-ble of trees where the cost of growing each node is high. In particular, while training in high dimensional spaces, ran-dom forests need to sample a large number of features at each node in order to achieve a good quality, balanced split (extremely random trees [17] and other variants do not work in extreme classification scenarios as they learn imbalanced splits). Furthermore, brute force optimization of the Gini index or entropy over each feature is expensive when there are a large number of training points and labels. All in all, accurate LPSR and MLRF training can require large clus-ters  X  with up to a thousand nodes in the case of MLRF.
Finally, care should be taken to note that our objective of learning a multi-label hierarchy is very different from the objective of learning a multi-class hierarchy [5, 11, 13, 16] or exploiting a pre-existing multi-label hierarchy [7, 9, 27].
Our primary objective, in developing FastXML, is to en-able training on a single desktop or a small cluster. At the same time, we aim to achieve greater prediction accuracy by optimizing a more suitable rank sensitive loss function as compared to MLRF X  X  Gini index and LPSR X  X  clustering error. We start this Section by presenting an overview of the FastXML algorithm, then go into the details of optimizing a loss function to learn a node partition and finally analyze FastXML X  X  cost of prediction.
FastXML learns a hierarchy, not over the label space as is traditionally done in the multi-class setting [5, 13, 16], but rather over the feature space. The intuition is similar to LPSR and MLRF X  X  and comes from the observation that only a small number of labels are present, or active, in each region of feature space. Efficient prediction can therefore be carried out by determining the region in which a novel point lies by traversing the learnt feature space hierarchy and then focusing exclusively on the set of labels active in the region. Like MLRF, and unlike LPSR, FastXML de-fines the set of labels active in a region to be the union of the labels of all training points present in that region. This speeds up training as FastXML does not need to solve the label assignment integer program in each region. Further-more, like MLRF, and unlike LPSR, FastXML learns an en-semble of trees and does not need to rely on base classifiers. parallel-for i = 1 ,..,T do end parallel-for return T 1 ,.., T T procedure grow-node-recursive ( n ) end procedure procedure process-leaf ( { x i , y i } N i =1 ,n ) end procedure Predictions are made by returning the ranked list of most frequently occurring active labels in all the leaf nodes in the ensemble containing the novel point. Algorithms 1 and 3 present pseudo-code for FastXML training and prediction respectively.
Training FastXML consists of recursively partitioning a parent X  X  feature space between its children. Such a node partition should ideally be learnt by optimizing a global mea-sure of performance such as the ranking predictions induced by the leaf nodes, Unfortunately, optimizing global measures can be expensive as all nodes in the tree would need to be learnt jointly [21]. Existing approaches therefore optimize local measures of performance which depend solely on pre-dictions made by the current node being partitioned. This allows the hierarchy to be learnt node by node starting from the root and going down to the leaves and is more efficient than learning all the nodes jointly.

MLRF and LPSR optimize the Gini index and clustering error as their local measure of performance. Unfortunately, neither measure is particularly well suited for ranking or extreme multi-label applications where correctly predicting the few positive relevant labels is much more important than predicting the vast number of irrelevant ones.

FastXML therefore proposes to learn the hierarchy by directly optimizing a ranking loss function. In particular, it optimizes the normalized Discounted Cumulative Gain (nDCG) [33]. This results in learning superior partitions due to two main reasons. First, nDCG is a measure which is sensitive to both ranking and relevance and therefore en-sures that the relevant positive labels are predicted with ranks that are as high as possible. This cannot be guaran-teed by rank insensitive measures such as the Gini index or the clustering error. Second, by being rank sensitive, nDCG
Id  X  n.Id  X  i [0]  X  X  X  1 , 1 } ,  X  i  X  Id # Random coin tosses w [0]  X  0 ,t  X  0 ,t w  X  0 , W 0  X  0 # Various counters repeat n +  X  new node ,n  X   X  new node n + .Id  X  i  X  Id : w [ t ] &gt; x i &gt; 0 n  X  .Id  X  i  X  Id : w [ t ] &gt; x i  X  0 return w [ t ] ,n + ,n  X  can be optimized across all L labels at the current node thereby ensuring that the local optimization is not myopic.
We stick to the notation introduced in the previous Sec-tion. it is assumed that the training set can be represented as { ( x i , y i ) N i =1 } with D dimensional real-valued feature vectors x  X  X  D and L dimensional binary label vectors y i  X  X  0 , 1 } with y il = 1 if label l is relevant for point i and 0 otherwise. Permutation indices i desc 1 ,...,i desc L that sort a real-valued vector y  X  R L in descending order are defined such that if returns the indices of the k largest elements of y ranked in descending order (with ties broken randomly), can then be defined as Let  X (1 ,L ) denote the set of all permutations of { 1 , 2 ,...,L } . The Discounted Cumulative Gain (DCG) at k of a ranking r  X   X (1 ,L ) given a ground truth label vector y with binary levels of relevance is Note that the log(1 + l ) term ensures that it is beneficial to predict the positive labels with high ranks. Thus, unlike precision (which would have been obtained had the log(1+ l ) term been absent), DCG is sensitive to both the ranking and the relevance of predictions. The normalized DCG, or for i = 1 ,..,T do end for r ( x ) = rank k 1 T P T i =1 P leaf i ( x ) return r ( x ) nDCG, is then defined as where I k ( y ) is the inverse of the DCG@ k of the ideal ranking for y obtained by predicting the ranks of all of y  X  X  positive labels to be higher than any of its negative ones. This nor-malizes L nDCG @ k to lie between 0 and 1 with larger values being better and ensures that nDCG can be used to com-pare rankings across label vectors with different numbers of positive labels.

FastXML partitions the current node X  X  feature space by learning a linear separator w such that where i indexes all the training points present at the node being partitioned,  X  i  X  { X  1 , +1 } indicates whether point i was assigned to the negative or positive partition and r + r  X  represent the predicted label rankings for the positive and negative partition respectively. C  X  and C r are user defined parameters which determine the relative importance of the three terms.

The first term in (5) is an ` 1 regularizer on w which en-sures that a sparse linear separator is used to define the partition. Given a novel point x , the FastXML trees can therefore be traversed efficiently during prediction depend-ing on the sign of w &gt; x at each node. The second term is the log loss of  X  i w &gt; x i . This term couples  X  and w as the op-timal solution of this term alone is  X   X  i = sign( w  X &gt; makes it likely that points assigned to the positive (nega-tive) partition, i.e. points for which  X  i = +1 (  X  i =  X  1), will have positive (negative) values of w &gt; x i . C  X  is relaxed to be a function of  X  i so as to allow different misclassification penalties for the positive and negative points. The third and fourth term in (5) maximize the nDCG@ L of the rankings predicted for the positive and negative partitions, r r  X  respectively, given the ground truth label vectors y i as-signed to these partitions. These terms couple r  X  to  X  and thus to w . As discussed, maximizing nDCG makes it likely that the relevant positive labels for each point are predicted with ranks as high as possible. As a result, points within a partition are likely to have similar labels whereas points across partitions are likely to have different labels.
Furthermore, it is beneficial to maximize nDCG@ L at each node even though the ultimate leaf node rankings will be evaluated at k L . This leads to non-myopic decisions at the root and internal nodes. For example, optimizing nDCG at k = 5 at the root node of the Wikipedia data set would be equivalent to finding a separator such that all the hundreds of thousands of Wikipedia articles assigned to the positive partition could be accurately labeled with just the five most frequently occurring Wikipedia categories in the positive partition and similarly for the negative parti-tion. Clearly this will not lead to good results at the root node and superior partitions can be learnt by considering all the Wikipedia categories rather than just the top five. Of course, as already pointed out, rank insensitive measures such as precision cannot be optimized at k = L as they be-come more and more uninformative with increasing k and L partitioning.

It is also worth noting that (5) allows a label to be as-signed to both partitions if some of the points containing the label are assigned to the positive partition and some to the negative. This makes the FastXML trees somewhat ro-bust as the child nodes can potentially recover from mistakes made by the parents [2, 13, 16, 35].

Finally, note that  X  and r  X  were deliberately chosen to be independent variables for efficient optimization rather than functions dependent on w . In particular, (5) could have been formulated as an optimization problem in just w by discard-ing the log loss term and defining  X  i ( w ) = sign( w &gt; r ( w ) = rank L P i 1 2 (1  X   X  i ( w )) I L ( y i ) y i . Such a formu-lation would also have been natural but intractable at scale. Direct optimization via efficient techniques such as stochas-tic sub-gradient descent would not be possible due to the sharp discontinuities in  X  ( w ) and r  X  ( w ). Furthermore, up-dates to w would necessitate expensive updates to  X  and r . We therefore decouple  X  and r from w by treating them as variables for efficient optimization but then couple their optimal values through the objective function. We develop the optimization algorithm for (5) in Section 4.
The objective function defined in (5) can be optimized efficiently and can lead to accurate predictions. A good objective function should, in addition, also lead to balanced partitions in order to ensure efficient prediction.
Given a novel point x  X  R D , FastXML X  X  top ranked k predictions are given by where T is the number of trees in the FastXML ensemble and P leaf t ( x )  X  P distribution and set of points respectively of the leaf node containing x in tree t . The average cost of prediction is up-per bounded by O ( TDH + T  X  L +  X  L log  X  L ) where H is the average length of the path traversed by x in order to reach the leaf nodes in the T trees and  X  L is the number of non-zero elements in the vector P T t =1 P leaf t ( x ). The cost is dom-inated by O ( TDH ) as  X  L DH . If the FastXML trees are balanced then H = log N  X  log L and the overall cost of prediction becomes O ( TD log L ) which is logarithmic in the total number of labels.

One might therefore be tempted to add a balancing term to (5) so as to get H as close to log N as possible. How-ever, this comes at the cost of reduced prediction accuracy as the objective function trades-off accuracy for balance. As it empirically turns out, our proposed nDCG based objec-tive function learns highly balanced trees and a balancing term does not need to be added to (5). Thus, FastXML X  X  predictions can be made accurately in logarithmic time.
It is well recognized in the learning to rank literature that nDCG is a difficult function to optimize [25,26,31,32] since it is sharply discontinuous with respect to w and hence stan-dard stochastic sub-gradient descent techniques cannot be applied. FastXML therefore employs an alternate strategy and optimizes (5) using an iterative alternating minimiza-tion algorithm. The algorithm is initialized by setting w = 0 and  X  i to be  X  1 or +1 uniformly at random. Each iteration, then, consists of taking three steps. First, r + and r  X  timized while keeping w and  X  fixed. This step determines the ranked list of labels that will be predicted by the positive and negative partitions respectively. Second,  X  is optimized while keeping w and r  X  fixed. This step assigns training points in the node to the positive or negative partition. The third step of optimizing w while keeping  X  and r  X  fixed is taken only if the first two steps did not lead to a decrease in the objective function. This is done to speed up training since optimizing with respect to  X  and r  X  takes only seconds while optimizing with respect to w can take minutes. This is the primary reason why (5) was formulated as a function of w ,  X  and r  X  rather than just w . The algorithm termi-nates when r  X  ,  X  and w do not change from one iteration to the next. We prove that the objective decreases strictly in each iteration and that the proposed algorithm terminates in a finite number of iterations. In practice, it was observed on all data sets that the algorithm made rapid progress and yielded state-of-the-art results as soon as a single update to w had been made. We now detail the steps in each iteration.
Given w and  X  , the first step in each iteration is to find the optimal rankings r + and r  X  that will be predicted by the positive and negative partition respectively. Fixing w and  X  simplifies (5) to which can be compactly expressed as two independent opti-mization problems where d  X  is an L -vector such that d  X  l = 1 / log(1 + r Since r + and r  X  are permutations of 1 , 2 ,...,L it is clear that (10) will be maximized if r  X  l is chosen as the index of the l th largest value in the vector P i :  X  Note that the optimal values of r  X  can be computed effi-ciently in time O ( n log L +  X  L log  X  L ) where n is the number of training points present in the node being partitioned, is the sparsity of the vector P i I L ( y i ) y i and it is assumed that y i is log L -sparse.
The next step in an iteration is to optimize (5) with re-spect to  X  while keeping w and r  X  fixed. This reduces to which decomposes over i . Thus, each  X  i can be optimized independently by seeing whether (12) is optimized by  X   X  i +1 or  X  1. This yields  X   X  i = sign( v  X  i  X  v + i ) where (13) The time complexity of obtaining  X   X  is O ( n  X  D + n log L ) as-suming that the feature vectors are  X  D -sparse and the label vectors are log L -sparse. This reduces to O ( n log L ) since w = 0 in the first iteration and w &gt; x i can be cached for all points in subsequent updates of w .
The final step of optimizing (5) with respect to w while keeping  X  and r  X  fixed is carried out only if the first two steps did not make any progress in decreasing the objective function. This can be efficiently determined in time O ( n ) by checking that  X  has remained unchanged. Optimizing (5) with respect to w while keeping  X  and r  X  fixed is equiva-lent to solving the standard ` 1 regularized logistic regression problem with the labels given by  X 
This problem has been extensively studied in the litera-ture [3,24,37]. FastXML optimizes (14) using the newGLM-NET algorithm [37] as implemented in the Liblinear pack-age [14]. No tuning of the learning rate parameter is re-quired since (14) is optimized in the dual. The algorithm is terminated after 10 passes over the training set in case it hasn X  X  converged already. The overall time complexity of the method is O ( n  X  D ) where n is the number of training points in the node being partitioned and  X  D is the average number of non-zero entries in a feature vector. This is the most time consuming of the three steps.
The formulation in (5) is non-convex, non-smooth and has a mix of discrete and continuous variables. Further-more, even the sub-problems obtained by optimizing with respect to only one block of variables might not be convex or smooth. It is well recognized that alternating minimization based techniques can fail to converge for such hard problems in general [6]. However, in our case, it is straightforward to show that FastXML X  X  alternating minimization algorithm for optimizing (5) will not oscillate and will converge in a finite number of iterations.

Theorem 1. Suppose Algorithm 2 has not yet terminated and let W =  X  X  1 , W 2 ,..., W i ,...  X  be the sequence of iter-ations at which w is updated. Let W i = W  X  W i be the sequence obtained by removing W i from W . Furthermore, let W i  X  t &lt; W i +1 . Then (a)  X  [ W i ] /  X  {  X  [ W j and (b)  X  [ t ] /  X  X   X  [ j ] |W i  X  j 6 = t &lt; W i +1 } . Proof. Let the objective value in (5), after iteration i, be O [ i ]. The individual sub-problems (10,12,14), that comprise a single iteration of 2, are all minimization problems, which minimize (5) w.r.t a single block of variables, and hence can never increase the objective value. In addition, algorithm 2 also ensures that any change in  X  is accompanied by a non-zero decrease in the objective. (a) Let W i be the iteration at which w is updated for the i th time. The algorithm ensures that, when  X  =  X  [ W w [ W i ] is the minimizer of (14), and r  X  [ W i ] is the minimizer of (10), which together imply that O [ W i ] is the unique min-imum value of (5) when optimized over w , r  X  , while fixing  X  =  X  [ W i ].
 Hence, for a given W i and a W j  X  W i Without loss of generality, let W i &lt; W j . Since by as-sumption, the algorithm has not yet terminated,  X  [ W  X  [ W ( i +1) ]. But, since there has been an update to  X  , O [ W O [ W ( i +1) ]  X  O [ W j ]. This, combined with (15) gives us  X  [ W i ] 6 =  X  [ W j ]. (b) Let u  X  { j : W i  X  j 6 = t  X  W i +1 } . Without loss of generality, assume t &lt; u . Since we are between two w we essentially solve the same optimization w.r.t r  X  in steps t + 1 and u + 1. Hence, O [ t + 1] = O [ u + 1]. But, absence of w updates imply that  X  [ t + 1] 6 =  X  [ t + 2], which further implies O [ t + 1] &gt; O [ t + 2]  X  O [ u + 1], contradict-ing the earlier equality. Hence,  X  [ t ] 6 =  X  [ u ].
Theorem 1 (b) states that  X  cannot repeat between two consecutive updates to w (in iterations W i and W i +1 ). Since  X  can only take a finite number of values, this implies the number of iterations between W i and W i +1 is bounded. Sim-ilarly, Theorem 1(a) states that  X  [ W i ] can never repeat for values W j  X  W . By a similar argument as above and The-orem 1(b), we conclude that W i is bounded for all i . Thus, the proposed alternating minimization algorithm cannot os-cillate and terminates in a finite number of iterations. Table 2: Results on small and medium data sets. FastXML was run with default hyper-parameter settings on all data sets. FastXML-T presents results when the parameters were tuned. A lgorithm P1 (%) P3 (%) P5 (%) F astXML-T 64 . 53  X  0 . 72 40 . 17  X  0 . 63 29 . 27  X  0 . 53 FastXML 63 . 26  X  0 . 84 39 . 19  X  0 . 66 28 . 72  X  0 . 48 MLRF 62 . 81  X  0 . 84 38 . 74  X  0 . 69 28 . 45  X  0 . 43
LPSR 62 . 95  X  0 . 70 39 . 16  X  0 . 64 28 . 75  X  0 . 45 1-vs-All 63 . 39  X  0 . 64 39 . 55  X  0 . 65 29 . 13  X  0 . 45 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 69 . 65  X  0 . 82 63 . 93  X  0 . 50 59 . 36  X  0 . 57 MLRF 67 . 86  X  0 . 70 62 . 02  X  0 . 55 57 . 59  X  0 . 43
LPSR 65 . 55  X  0 . 99 59 . 39  X  0 . 48 53 . 99  X  0 . 31 1-vs-All 65 . 42  X  1 . 05 59 . 34  X  0 . 56 53 . 72  X  0 . 50 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 87 . 35  X  0 . 27 72 . 14  X  0 . 20 58 . 15  X  0 . 15 MLRF 86 . 83  X  0 . 18 71 . 18  X  0 . 19 57 . 09  X  0 . 16
LPSR 82 . 33  X  2 . 15 66 . 37  X  0 . 35 50 . 00  X  0 . 20 1-vs-All 82 . 31  X  2 . 19 66 . 17  X  0 . 43 50 . 32  X  0 . 56 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 91 . 23  X  0 . 22 73 . 51  X  0 . 25 53 . 31  X  0 . 65 MLRF 87 . 66  X  0 . 46 69 . 89  X  0 . 43 50 . 36  X  0 . 74
LPSR 90 . 04  X  0 . 19 72 . 27  X  0 . 20 52 . 34  X  0 . 61 1-vs-All 90 . 18  X  0 . 18 72 . 55  X  0 . 16 52 . 68  X  0 . 57 This Section compares the performance of FastXML to MLRF, LPSR and the 1-vs-All baseline on some of the largest multi-label classification data sets.

Data sets : Experiments were carried out on data sets with label set sizes ranging from a hundred to a million to benchmark the performance of FastXML in various regimes. The data sets include two small scale data sets with hun-dreds of labels, BibTeX [23] and MediaMill [28], two medium scale data sets with thousands of labels, Delicious [30] and RCV1-X, and three large scale data sets with up to a million labels, WikiLSHTC [1], Ads430K and Ads1M. Table 1 lists the statistics of these data sets.
 Table 3: Results on large data sets comparing the perfor-mance of FastXML to LPSR trained with Na  X   X ve Bayes as the base classifier.
A lgorithm P1 (%) P3 (%) P5 (%) Train Time Test Time F astXML 49 . 78 33 . 06 24 . 40 9 . 14 5 . 10
LPSR-NB 27 . 91 16 . 04 11 . 57 1 . 59 3 . 52
A lgorithm P1 (%) P3 (%) P5 (%) Train Time Test Time F astXML 27 . 24 16 . 28 11 . 91 1 . 81 1 . 68
LPSR-NB 19 . 69 12 . 71 9 . 70 0 . 84 3 . 95
A lgorithm P1 (%) P3 (%) P5 (%) Train Time Test Time F astXML 23 . 45 14 . 21 10 . 41 8 . 09 6 . 26 LPSR-NB 17 . 08 11 . 38 8 . 83 3 . 78 19 . 32 Table 4: FastXML X  X  wall clock training time (in hours) vs the number of cores used on a single machine.

Co res WikiLSHTC (hr) Ads-430K (hr) Ads-1M (hr) Table 5: The variation in FastXML X  X  performance with the number of training iterations. W i denotes the iteration at which w is updated for the i th time at the root node on the Ads-430K data set. Precision values and training times are reported for the full ensemble.
Features and labels are publically available for all the data sets, apart from the two proprietary Ads data sets, and were used in the experiments. WikiLSHTC is a challenge data set for which the test set has not been released and we therefore partitioned the data provided into 75% for training and 25% for testing. The RCV1-X data set has the same features as the original RCV1 data set but its label set has been expanded by forming new labels from pairs of original labels. The two proprietary Ads data sets comprise of bag of words TF-IDF features extracted from expansions of queries from the Bing query logs. Other queries similar to a given query are used as labels for that query. Table 6: FastXML learns more stable and balanced trees than MLRF and LPSR leading to both faster training as well as faster prediction. Tree balance is measured as H/ log( N/ MaxLeaf), where H is the average length of the path traversed by a point in that tree and log( N/ MaxLeaf) is the average length of a path traversed in a perfectly bal-anced tree with at most MaxLeaf points at each leaf node. Smaller values of tree balance are better with a balance of 1 indicating a perfectly balanced tree.
 Figure 1: The variation in FastXML X  X  precision at 5 with the number of trees selected according to (1a) random order; and (1b) highest individual prediction accuracy on the training set. The training time can be halved on most data sets with a minimal decrease in prediction accuracy by training only 25 trees in random order.

Results are reported by averaging over ten random train and test splits for the small and medium data sets apart from RCV1-X for which only 3 splits were used. A single split was used for the large data sets.

Baseline algorithms : FastXML was compared to state-of-the-art tree based extreme multi-label methods such as MLRF and LPSR (see Section 2 for details) as well as the 1-vs-All baseline as implemented in M3L [18]. The 1-vs-All strategy was also used to learn the base classifier for LPSR on the small and medium data sets as it offered better performance as compared to other base classifiers such as Wsabie [34]. Unfortunately, M3L and Wsabie cannot be trained on the large data sets on a single desktop in a day. Na  X   X ve Bayes was therefore used as a base classifier for LPSR on these data sets.

Finally, note that we do not compare explicitly to embed-ding methods [4, 8, 10, 12, 15, 19, 20, 22, 29, 34, 36, 38] since none of these have been shown to consistently outperform the 1-vs-All base classifier.

Parameters : The following hyper-parameters settings were used for FastXML across all data sets: (a) Co-efficient of logistic-loss: C  X  = 1 . 0; (b) Co-efficient of negative-nDCG loss: C r = 1 . 0; (c) Number of trees: T = 50; (d) Maximum number of instances allowed in a leaf node: MaxLeaf = 10; (e) Number of labels in a leaf node whose probability scores are retained: k = 20; (f) Bias multiplier for Liblinear: 1 . 0; (g) Number of training iterations in Algorithm 2: 1; and Table 7: Results obtained by replacing the nDCG@ L loss function in FastXML with others such as nDCG@5 (FastXML-nDCG5) or precision at 5 (FastXML-P5) and by replacing the Gini index in MLRF with the proposed nDCG@ L loss function. A lgorithm P1 (%) P3 (%) P5 (%) F astXML 63 . 26  X  0 . 84 39 . 19  X  0 . 66 28 . 72  X  0 . 48 FastXML-P5 39 . 95  X  1 . 09 23 . 01  X  0 . 46 17 . 42  X  0 . 36 FastXML-nDCG5 51 . 75  X  1 . 28 30 . 87  X  0 . 63 22 . 70  X  0 . 42
MLRF-nDCG 58 . 41  X  1 . 20 36 . 45  X  0 . 65 26 . 95  X  0 . 49 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 69 . 65  X  0 . 82 63 . 93  X  0 . 50 59 . 36  X  0 . 57 FastXML-P5 60 . 11  X  0 . 91 53 . 97  X  0 . 53 49 . 81  X  0 . 58 FastXML-nDCG5 64 . 96  X  0 . 83 59 . 28  X  0 . 69 54 . 70  X  0 . 56
MLRF-nDCG 66 . 70  X  0 . 75 61 . 08  X  0 . 44 56 . 72  X  0 . 44 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 87 . 35  X  0 . 27 72 . 14  X  0 . 20 58 . 15  X  0 . 15 FastXML-P5 80 . 73  X  0 . 29 67 . 48  X  0 . 22 54 . 11  X  0 . 20 FastXML-nDCG5 86 . 66  X  0 . 27 70 . 81  X  0 . 21 56 . 51  X  0 . 20
MLRF-nDCG 86 . 67  X  0 . 26 71 . 13  X  0 . 22 57 . 24  X  0 . 21 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 91 . 23  X  0 . 22 73 . 51  X  0 . 25 53 . 31  X  0 . 65 FastXML-P5 66 . 62  X  0 . 23 56 . 41  X  0 . 40 40 . 83  X  0 . 14 FastXML-nDCG5 75 . 60  X  0 . 39 60 . 85  X  0 . 43 43 . 98  X  0 . 16
MLRF-nDCG 87 . 19  X  0 . 41 69 . 69  X  0 . 46 50 . 25  X  0 . 56 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 49 . 78 33 . 06 24 . 40 FastXML-P5 18 . 74 12 . 33 8 . 71
FastXML-nDCG5 20 . 50 12 . 51 8 . 80 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 27 . 24 16 . 28 11 . 91 FastXML-P5 25 . 06 14 . 36 10 . 27
FastXML-nDCG5 24 . 93 14 . 34 10 . 29 A lgorithm P1 (%) P3 (%) P5 (%) F astXML 23 . 45 14 . 21 10 . 41 FastXML-P5 21 . 09 12 . 32 8 . 87
FastXML-nDCG5 21 . 25 12 . 47 9 . 01 (h) Maximum number of (outer,inner) iterations of Liblinear before termination: (10 , 10). Using these default settings, it was observed that FastXML could outperform LPSR, MLRF and the 1-vs-All M3L. Tuning the hyper-parameters for each data set would lead to even superior prediction accuracies. The hyper-parameters for MLRF, LPSR and M3L were set using fine grained validation on each data set so as to achieve the highest possible prediction accuracy for each method.
Evaluation Metrics : Extreme multi-label classification data sets exhibit positive label sparsity in that each data point has only a few positive labels associated with it. It therefore becomes important to focus on the accurate pre-diction of the few positive labels per data point than on the vast number of negative ones. As such, most papers [2, 19, 22, 34 X 36] evaluate prediction accuracy using precision at k which counts the number of correct predictions in the top k positive predictions. More formally, the precision at k for a prediction  X  y  X  R L given the ground truth label vector y  X  X  0 , 1 } L is defined as All experiments were carried out on a single core of an Intel Core 2 Duo machine running at 3.3 GHz with 8 Gb of RAM. Training times were measured using the clock function available in C++. Parallelization experiments in Table 4 were carried out on multiple cores of Intel Xeon processors running at 2.1 GHz.

Results on small and medium data sets : Table 2 benchmarks the performance of FastXML on the small and medium data sets where accurate MLRF, LPSR and 1-vs-All models could be learnt. The focus is on prediction accuracy as neither training nor prediction present any challenge on these data sets  X  even the expensive MLRF, LPSR and 1-vs-All can be trained in seconds on BibTeX and Delicious, in minutes on MediaMill and in two hours on RCV1-X.
As compared to LPSR and 1-vs-All, FastXML could be up to 5% more accurate in terms of precision at 1 and up to 8% in terms of precision at 5. Large improvements were obtained on both Delicious and MediaMill. The smallest im-provements were obtained on BibTeX which had less than five thousand training points. FastXML with default param-eter settings gave more or less the same P1 as LPSR and 1-vs-All on BibTeX. A marginal improvement of 1% over the other methods could be obtained by tuning FastXML X  X  hyper-parameters (referred to as FastXML-T in Table 2). FastXML X  X  gains over MLRF were smaller as compared to the gains over LPSR and 1-vs-All but could still go up to 3% in terms of both precision at 1 and at 5 (on RCV1-X). All in all, these results indicate that FastXML could be significantly more accurate at prediction than highly tuned MLRF, LPSR and 1-vs-All classifiers.

Results on large data sets : Extreme classification is most concerned with large scale data sets having hundreds of thousands or even millions of labels. Training MLRF and 1-vs-All on such data sets was found to be infeasible without using a large cluster. LPSR training could be made tractable on a single core by replacing the 1-vs-All base clas-sifier with Na  X   X ve Bayes. Table 3 compares the performance of FastXML to LPSR-NB. FastXML X  X  improvements over LPSR-NB in terms of P1 ranged from 5% on Ads-1M to al-most 22% on WikiLSHTC and in terms of P5 ranged from approximately 1.5% on Ads-1M to almost 13% on WikiL-SHTC. FastXML could train in 1.81 hours on Ads-430K using a single core and in 8 to 9 hours on Ads-1M and Wik-iLSHTC with individual trees being grown in about 10 min-utes. This opens up the possibility of practitioners training accurate extreme classification models on commodity hard-ware. Finally, FastXML could be almost 1.5 to 3 times faster at prediction than LPSR-NB which could be a critical factor in certain applications.

Validating FastXML X  X  hyper-parameter settings and design choices : Table 4 lists the reduction in wall clock training time obtained by growing FastXML X  X  trees in paral-lel across multiple cores on a single machine. Training could be speeded up 12 to 13 times by utilizing 16 cores demon-strating that FastXML is trivially parallelizable. The entire ensemble could be trained in approximately an hour on Ads-1M and WikiLSHTC and in 12 minutes on Ads-430K.

All the FastXML results presented so far were obtained by terminating the proposed optimization algorithm after a single update to w . Table 5 shows the effects of allowing multiple updates to w while training on the Ads-430K data set. High precisions were reached after the first update to w which occurred after 15 updates to  X  and r . Subsequent updates to w ,  X  and r yielded a significant drop in the value of the objective function but little change in prediction ac-curacy. As such, it would appear that training time could be significantly reduced without much loss in precision by early termination.

Table 7 reports the effects of replacing the proposed nDCG@ L loss function in FastXML with others such as precision at 5 (FastXML-P5) and nDCG@5 (FastXML-nDCG5). As is evident, both these loss functions were inferior to nDCG@ L even when performance was measured using precision at 5. This demonstrates that rank sensitive loss functions such as nDCG are better suited to extreme multi-label classification as compared to rank insensitive ones such as precision. Fur-thermore, nDCG should be computed non-myopically over all labels rather than just the top few.

Finally, the key difference in FastXML X  X  formulation as compared to MLRF X  X  was the use of the nDCG based loss function and the use of a linear separator to partition each node. Table 7 demonstrates that both these ingredients were necessary as simply replacing the Gini index or en-tropy in MLRF with the nDCG based loss function (MLRF-nDCG) yielded significantly poorer results as compared to FastXML.
This paper developed the FastXML algorithm for multi-label learning with a large number of labels. FastXML learnt an ensemble of trees with prediction costs that were loga-rithmic in the number of labels. The key technical contribu-tion in FastXML was a novel node partitioning formulation which optimized an nDCG based ranking loss over all the labels. Such a loss was found to be more suitable for ex-treme multi-label learning than the Gini index optimized by MLRF or the clustering error optimized by LPSR. nDCG is known to be a hard loss to optimize using gradient descent based techniques. FastXML therefore developed an efficient alternating minimization algorithm for its optimization. It was proved that the proposed alternating minimization al-gorithm would not oscillate and would converge in a finite number of iterations. Experiments revealed that FastXML could be significantly more accurate than MLRF and LPSR while efficiently scaling to problems with more than a mil-lion labels. The FastXML code is publically available and should enable practitioners to train accurate extreme multi-label models without needing large clusters. We are very grateful to Purushottam Kar and Prateek Jain for helpful discussions. Yashoteja Prabhu is supported by a TCS PhD Fellowship at IIT Delhi. [1] Wikipedia dataset for the 4th large scale hierarchical [2] R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma. [3] G. Andrew and J. Gao. Scalable training of [4] K. Balasubramanian and G. Lebanon. The landmark [5] S. Bengio, J. Weston, and D. Grangier. Label [6] D. Bertsekas. Nonlinear Programming . Athena [7] W. Bi and J. T.-Y. Kwok. Multilabel classification on [8] W. Bi and J. T.-Y. Kwok. Efficient multi-label [9] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
 [10] Y.-N. Chen and H.-T. Lin. Feature-aware label space [11] A. Choromanska and J. Langford. Logarithmic time [12] M. Ciss  X e, N. Usunier, T. Arti`eres, and P. Gallinari. [13] J. Deng, S. Satheesh, A. C. Berg, and F. Li. Fast and [14] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [15] C.-S. Feng and H.-T. Lin. Multi-label classification [16] T. Gao and D. Koller. Discriminative learning of [17] P. Geurts, D. Ernst, and L. Wehenkel. Extremely [18] B. Hariharan, S. V. N. Vishwanathan, and M. Varma. [19] D. Hsu, S. Kakade, J. Langford, and T. Zhang. [20] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared [21] C. Jose, P. Goyal, P. Aggrwal, and M. Varma. Local [22] A. Kapoor, R. Viswanathan, and P. Jain. Multilabel [23] I. Katakis, G. Tsoumakas, and I. Vlahavas. Multilabel [24] K. Koh, S.-J. Kim, and S. Boyd. An interior-point [25] A. Kustarev, Y. Ustinovsky, Y. Logachev, [26] P. D. Ravikumar, A. Tewari, and E. Yang. On ndcg [27] J. Rousu, C. Saunders, S. Szedmak, and [28] C. Snoek, M. Worring, J. van Gemert, J.-M.
 [29] F. Tai and H.-T. Lin. Multi-label classification with [30] G. Tsoumakas, I. Katakis, and I. Vlahavas. Effective [31] H. Valizadegan, R. Jin, R. Zhang, and J. Mao. [32] M. N. Volkovs and R. S. Zemel. Boltzrank: Learning [33] Y. Wang, L. Wang, Y. Li, D. He, and T.-Y. Liu. A [34] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling [35] J. Weston, A. Makadia, and H. Yee. Label partitioning [36] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. [37] G.-X. Yuan, C.-H. Ho, and C.-J. Lin. An improved [38] Y. Zhang and J. G. Schneider. Multi-label output
