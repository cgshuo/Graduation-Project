
Association rules, empirical Bayes methods, garmna-Poisson model, market basket problem, shrinkage estimation. 
A common data mining task is the search for associations of items in a database of transactions. Each transaction, such as a list of items purchased, or a list of diagnoses and medications in a patient medical report, defines a subset of all possible items in the union of all transactions. If there are K total items, we define for each transaction, binary random variables X1 ..... XK that are 1 if the corresponding item is included in the transaction, and 0 otherwise, ff the total number of transactions in the database is 
N, then the data being modeled could be represented as an N x K the statistical model, we do not assume that the distribution of each row of X is identical. Rather, we assume that the transactions are stratified, for example by store location in the case of supermarket scanner data, or by patient age, sex, etc. in the ease of medical data. Time period of the transaction could also be a stratification variable. The purpose of stratification is to avoid finding spurious associations between two items merely because they both tend to be more frequent in the same strata. 
We assume that associations due to stratification variables (so called between strata associations) are not of interest, and instead require the methodology to identify items that are associated within strata. For example, certain upscale store locations might have greater sales volume of high-priced items, but we may only wish to say such items have interesting associations if they tend to be purchased together more frequently than their per-store marginal frequencies indicate, since an overall association would show up even if the items were purchased independently within each store. 
As the preceding paragraph indicates, we are departing from the majority of published treatments of the market basket problem by going beyond the enumeration of the support (proportion of transactions) of frequent item sets. No matter how frequently an item set occurs, we assume that it is of little interest to the analyst if its frequency is about the same as would be predicted if the members of the item set occur independently within each stratum. This comparison frequency, based on within-strata independence, will be called the baseline frequency of the particular item set, and our measure of interestingness is the ratio of actual to baseline frequency. Two earlier papers that focus on interestingness measures as deviations from expected frequencies predicted by independence for the market basket problem are Silverstein, Brin and Motwandi [14] and Aggarwal and Yn [1]. Considering the former paper first, [14] rightly criticizes the more common support-confidence framework for association rules and suggests using a chi-squared test statistic transactions) and we believe this describes the majority of market basket problems. 
Both the cited papers, as well as [6], refer to the simple ratio of observed to baseline frequencies of occurrence as the interest of an item set, which we denote R. The primary criticisms of R as a measure of dependence are 1. Since R is bounded above by 1/P(A)P(B) .... if all the items 2. Since R is symmetric in the items A, B, ..., it does not 3. Computing all item sets with greater than a specified value 
We believe that the first criticism is mitigated by the fact that the vast majority of market basket problems deal with items whose probabilities are not large. Data mining is, by definition, a search for low probability events of high value. We view the lack of directionality of R as more an advantage than a disadvantage. Once an item set is identified as unusually frequent, domain knowledge is required to identify mechanisms and potential causality among the items in the item set, something far beyond the presumption of a statistical algorithm. 
The third criticism above identifies both a computational and a statistical issue. We do not focus on computational issues here, which are already well treated in the association rule literature, but rather we focus on the statistical issues that arise for item sets with small support. 
A novel element of our approach to identifying interesting item sets is the ability to detect interesting associations even if they have relatively small support. To do so, we must explicitly allow for noise or spurious association due to small samples. Consider an item set observed in 200 transactions, even though the baseline (independence model) frequency is just 2. How should this 200:2 ratio of actual to baseline be compared to other item sets having the same ratio, such as 20:0.2, or even 2:0.02? Our answer involves an empirical Bayes model, described in detail in [8], that produces a posterior distribution fbr the true ratio of actual to baseline expected frequencies, assuming that the observed counts of every item set have separate Poisson distributions. Our conservative estimate of the ratio of actual to baseline is the lower 95% Bayesian confidence limit for this ratio, which we denote ~,.os. When observed counts are large, ~..0s will be almost identical to the naYve observed ratio, namely R = 100 in the 200:2 example. However, ~,.0s will be rather less than 100 for the case of 20:0.2, and will be much smaller, perhaps near 1, in the 2:0.02 example. This tendency of empirical Bayes estimates to reduce the observed effect when sample sizes are small gives rise to the label shrinkage estimate. 
To summarize, we introduce three improvements to the existing market basket methodology. First, we allow stratification adjustments to eliminate spurious association due to non-interesting stratification variables. Second, we focus on the ratio of item set frequencies to baseline frequencies computed via an independence assumption, rather than merely the support of each item set. And third, we replace the observed ratios of actual to baseline frequencies by empirical Bayes shrinkage estimates in 68 framework, where there are a great many similar estimation problems that can be fomaulated within a common model, is a natural application of empirical Bayes methodology, sometimes also called hierarchical modeling. See the books [7] or [13] for extensive expositions of the methodology. DuMouchel et al [9] and [8] introduced the application of empirical Bayes estimation to data mining of large sparse frequency tables in the domains of natural language processing and screening of adverse drug reaction reports, respectively. The latter paper is accompanied by the discussion of two FDA scientists, Drs. Ana Szarfman and Robert O'Neill, who describe the adoption and successful application of the methodology at the FDA. Those two papers considered the analysis of large sparse two-dimensional tables, whereas in this paper we extend the methodology to J dimensions, where J is the item set size being considered. This Section provides an overview of the rationale and properties of the empirical Bayes method applied to the market basket problem, while a more formal development is deferred to the Appendix. Empirical Bayes estimation enhances the simple use of the separate values of" R = n/e as the estimates for each separate 9, by adding the prior information that the many values of 9, are connected in that they can be treated as having arisen from a common super-population of ~,-values. The method assumes that the set of unknown ~,s are distributed according to the continuous parametric density function of a specific form rt(~,l 0). The parameter vector 0 is estimated from the data, the feature that distinguishes empirical Bayes methods from full Bayes methods. The estimation of 0 involves fitting the distribution of all the values of R, and once 0 has been estimated, the prior distribution of every ~, is assumed to be ~z(~.l d ). Since this allows us to improve over the simple use of each R as estimates of the corresponding Ls, we say that the values of R "borrow strength" from each other to improve every estimate. To summarize, the empirical Bayes algorithm involves the following series of assumptions and steps: 1. A collection of pairs (n, e), where each n &gt; n*, the 2. Assume that conditional on known ~,, each n is a Poisson 3. Assume that tbr some fixed 0, the ~,s are distributed 4. Compute the unconditional distribution of each n as f(n) = 5. Use the product of all expressions fin) in Step 4 to compute 6. For each (n, e) use Bayes rule to compute the posterior 7. For each ~,, use the results of Step 6 to obtain the posterior 69 70 71 ~,.05, with lightest gray corresponding to 2&lt; X.05&lt; 4 and the 2. Just over 8% of the country pairs are shaded; this means that in these cells, our lower confidence limit has at least twice as many accounts calling this pair of countries than we would expect under the assumption that countries are called independently of each other. An interesting aspect of Figure 2 is that ~,.05 is greatest for country pairs in the same part of the world --this is intuitively reasonable. LM sample drawn from a common distribution will have a smallest and a largest value --the question is "How big is big?". The quantile-quantile plot attempts to answer this question by displaying the quantiles (e.g., ordered values) of the observed measure against the quantiles of the assumed distribution of the measure. If the assumed distribution is correct, apart from random variation, the configuration of points should follow a straight line. Systematic departures from linearity indicate that the assumed distribution is not correct --common anomalies being systematic curvature or outliers at the extremes. 
Figure 4 is an example of a "standard normal quantile" plot, so-called because the assumed distribution is a normal (i.e., 
Gaussian) distribution with mean zero and unit standard deviation. In it we display the ordered values of the standardized change in the A measure (also referred to as "EBGM") from month one to month two. (Because our methods yield a posterior variance for every estimated log ~,, we can compute approximately standard normal test statistics tbr comparing the ratios of corresponding ~,s from one month to the next.) Mild have been identified with their country names and the observed n. We comment on a few of them: We introduced three variations on the market basket problem that are drawn from statistical considerations. First we introduced stratification of transactions by features that are known to correlate with item sets. Second we dramatically reduced the recommended minimal support size for item sets by introducing an empirical Bayes model that effectively takes into account variation associated with small frequencies. And finally we built on earlier work that considers interestingness measures that assess departures of observed frequencies from baseline frequencies. For item sets of size two, the independence model provides a natural baseline, but we argue that in larger item sets interpretations are sometimes aided by comparisons with fitted frequencies from other models, especially the log-linear model specifying "all two-factor interactions. In every case, our methods provide for a reliable ranking of item sets by "interestingness." Given an interesting item set, our methods do not explicitly choose a predictive association rule. We prefer to let a domain expert make judgments about potential cause and effect or directional association. For example, our discovery that adult entertainment lines had spread to St. Kitts in the Caribbean was easy to verify once the outlier in Figure 4 was presented to someone familiar with the situation in Vanuatu. We deliberately did not focus on algorithmic issues. In our experience the computing time to enumerate item sets and compute interestingness measures is negligible compared to the hours of analysis time spent perusing the voluminous output. We have developed and continue to develop visualization techniques to aid this phase of the problem. The figures in this paper are examples of some of the static displays that we feel are valuable. Yet the real power of visualization lies in interactive approaches, especially in high dimensions (i.e., large item sets). We are currently exploring such techniques and hope to report on this research at KDD2002. 74 This provides a "best" point estimate of each ratio ~,, in the sense of minimizing squared error loss in log(k). On the other hand, if a conservative estimate of ~ is desired, in which the risk of overestimating every ~, is constrained to be just 5%, then we can compute the 5 ~ percentile of the posterior distribution of each X, denoted ~,.0s, as the solution to the equation: where n( ) is given by (AI). The integral in (A8) is easily computed using standard computer approximations of the incomplete gamma function, while the solution ~..0s requires an iterative technique such as Newton's method. If e and n are both large, then A and ~..05 will both approach R = n/e, as can be seen from (A6), noting that for large arguments, ~t(x) ~ log(x), and log(or + n) -1og(13 + e) will approach log(n/e). However, when e or n are not large, then the effect of using A is to "shrink" R toward smaller values, and the shrinkage is even greater for ~,.0s. This is exactly the desired effect when sampling variation makes the true degree of association between the items in the item set uncertain. obtained by considering the marginal distribution of the set of all n for a given item set size, which is given in (A2). The negative binomial 'distributions f( ) in (A2) are derived as a mixture of Poisson distributions, where the Poisson means have a gamma distribution [i 1, p. 125]. The likelihood function for 0 is the product of mixtures of two negative binomial densities: The maximum likelihood estimate of 0 is the vector that maximizes (A9), where the product in (A9) is over all possible item sets of the given size. The maximization involves an iterative search in the five dimensional parameter space, where each iteration involves computing log[L(0)] and its first-and second-derivatives. Since log[L(0)] would be the sum of an astronomical number of terms if we included all possible item sets (e.g., including item sets that were not observed in the database and have n = 0), we use two modifications of (A9) to allow the procedure to scale up to large values of K, the number of unique items. First, we modify the likelihood to condition on n &gt; n*, where n* is the minimum support that we wish to consider for item sets of the given size. This allows us to merely tabulate and include in the likelihood item sets having n &gt; n*, but, to properly accommodate this, we must replace f(n; ~x, 13, e) Even choosing n* = 1 saves an enormous amount of computation compared to enumerating all possible item sets and computing the corresponding values of e and f(). As mentioned in the previous sections, we are focusing on applications where moderate values of n are of interest and so we take n* to be at least 1 but as small as computational resources allow, ff n* is as large as several hnndred, the noise in the counts for n &gt;_ n* will pass over the transaction database for each item set size, is a much greater computational burden than the empirical Bayes calculations described in this Appendix and the log-linear model calculations discussed in Section 4. Thus there is no computation-related reason not to compute A, ~,.05, EXCESS and EXCESS2 routinely. Table 1 shows the estimates and standard errors of the hyper-set sizes 2-5. Standard errors are based on the second derivatives of the log likelihood function. It can be seen that the parameter values are quite consistent across databases, but that increases. This indicates that the distribution of ~, becomes more and more dispersed as the dimension goes up, with more outlying item sets having very large frequencies compared to the baseline frequency based on the independence assumption. This also shows that the optimum amount of shrinkage varies depending on item set size. We have not performed extensive goodness of fit analyses to the prior distribution family used here. One approach is to fit different strata separately and see if they lead to similar sets of hyper-parameters. If not, it is easy to combine results alter separate fits to different subsets of the database or to different subsets of item sets. Classical hypothesis tests for goodness of fit to the observed distribution of (n, e) are possible but may not be useful, since massive data sets are bound to yield small "p-values" for rejecting almost any null hypothesis, even though the computed shrinkage estimates perform well as interestingness measures. 
