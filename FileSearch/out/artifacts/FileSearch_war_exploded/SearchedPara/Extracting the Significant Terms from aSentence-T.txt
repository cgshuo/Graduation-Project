 Traditional information retrieval systems usually use a noun which has some meaning of its own as an index term or a representative word for a logical view of the documents [1]. However, all of the nouns in the document may not always be the representatives. For instance, consider a document about fruit with its nouns,  X  X pple X ,  X  X anana X ,  X  X ear X ,  X  X trawberry X ,  X  X ruit X  and  X  X asket X , and suppose only four words are selected for the respresentatives; i.e., apple, banana, pear and basket. Here the word,  X  X asket X , might appear at the phrase like  X  X  fruit basket X . When  X  X asket X  is removed from the selected list and  X  X trawberry X  or  X  X ruit X  is added to, this revised word list can be more informative or effective to illustrate the example.
 above example and to extract the representative words or significant terms in the document, the [6] makes use of the principal component analysis (PCA) which is to reduce dimension of data by eige nvalue-eigenvector pairs [5]. In [6], however, performing PCA over the original sentence-term matrix which might have the noise of variability in term usage. Consequently, the unnecessary terms might be selected for the significant terms (called thematic words in [6]) by the only PCA.
 ably be involved in the orginal sentence-term matrix. The particular technique used is the singular value decompostion (SVD) [1,3,8]. Computing SVD of the original sentence-term matrix can be removed the noisy data by the proprotion of the singular values. As a result, the new or revised sentence-term matrix can be obtained, whose number of rows and columns is exactly equal to that of rows and columns of the original matrix. However, this matrix can have the smaller amount of the noise than the original. In contrast to [6], the PCA is performed over the revised matrix instead of the original. Actually the proposed methed is applied to the summarization for the evaluation.
 noise in term usage by SVD, and 3 briefly illustrates the way to extract the significant terms by PCA (more details in [6]). Section 4 reports experimental results. A brief conclusion is given in Section 5. 2.1 SVD Overview We will give an outline of SVD adapted from [3,8] and how to apply to remove the noise.
 and terms as shown in the next subsection. The matrix A can be written as the product of an s  X  r column-orthogonal matrix U 0 ,an r  X  r daigonal matrix W 0 with positive or zero elements (i.e., the singular values), and the transpose of a t  X  r orthogonal matrix V The SVD decompositon is shown in Eq. (1).
 where U 0 T U 0 = I , V 0 T V 0 = I ,and W 0 is the diagonal matrix of singualr values. by the cumulative ratio of the singular values. The cumulative ratio,  X  k ,can be calculated by Eq. (2). When the  X  k is more than 90%, k can be selected for the reduced dimensionality which can be large enough to capture most of the important underlying structure in association of sentences and terms, and also small enough to remove the noise of variability in term usage. dimension is euqal to that of A , s  X  t , without much loss of information. That is, we can write where U is an s  X  k matrix, W is a k  X  k martix, and V T is a k  X  t matrix. Consequently we can use  X  A instead of A for performing PCA to remove the noise of frequency distribution of the original sentence-term matrix, while keeping the significant structure of the orginal. 2.2 The Creation of the Revised Sentence-Term Matrix Now we will illustrate the way to create the revised sentence-term matrix by the sample article, and this matrix will be regarded as being removed the noise of variability in term usage in the document. Table 1 shows the extracted term list from one of the Korean newspaper articles composed of 8 sentencens and 61 unique nouns, and this article is about giving a prize on the protection of environment. Since we assume that the candidates of the significant terms are confined only to nouns occurred more than twice in the document, there are 10 terms in Table 1. Particularly, the term, X 4 , is not a correct noun resulted from the malfunction of our Korean morphological analyzer. Actually this term was not included in the list of the significant terms by both methos, that is, the only PCA and the proposed.
 frequency of each term within each sentence. For instance, the first column shows that X 1 was occurred twice in the first sentence, once in the second and twice in the fifth.
 (3), i.e., U  X  W  X  V T . The values of these four matrices is rounded to two decimal places as shown below. Actually U 0 is 8  X  10 matrix, W 0 is 10  X  10 and V 0 is 10  X  10. However, by using  X  k like Eq. (2), the dimension of these ones can be reduced without much loss of information on the frequency distribution in the sentence-term matrix. In our sample article, k is six, since the cumulative ratio of the singular values is more than 0 . 9 at the first six as shown Table 2. It is said that around ten-percent noise can be removed by using this cumulative ratio.  X 
A = V decreased by 0 . 1. Consequently, the matrix  X  A can be regarded as one removed the noise in term usage of the original matrix, A , by using SVD. And thus, the extracted significant terms by the revised matrix like  X  A can be more reasonable. 3.1 PCA Overview PCA is concerned with explaining the variance-covariance structure through a few linear combinations of the original variables [5]. PCA uses the covariance matrix instead of the obsevation-variable matrix (sentence-term matrix like A or  X  A above). An eigenvector and its corresponding eigenvalue can be obtained by applying PCA on the covariance matrix X  in other words, the covariance matrix ..., (  X  p ,e p )where  X  1  X   X  2  X  ... X  p . The cumulative proportion of total popu-lation variance due to the k th principal component(PC),  X  k ,is If most (  X  k is 0 . 8  X  0 . 9) of the total population variance, for large p ,canbe attributed to the first one, two, or three components, then these components can  X  X eplace X  the original p variables without much loss of information [5]. significant terms can be extracted by the pattern of statistical cooccurrence of the sentence-term matrix [6]. 3.2 Comparing the Extracted Significant Terms In this subsection, we will compare the significant terms by using only PCA [6] to those by using the proposed method. In order to extract the significant terms, there are two steps in [6]. In the first step, the PCs are selected by Eq. (4), i.e., the first k where  X  k  X  0 . 9. In the second step, the selected PCs are represented by using their coefficients (  X  0 . 5 or highest), i.e., their corresponding eigenvector. revised sentence-term matrix like  X  A instead of the original like A for PCA. Table 3 and 4 show the eigenvector and i ts corresponding eigenvalue of the orginal matrix A and the revised  X  A , respectively.
 spectively. This means that variance of the A and  X  A can be summarized very well by these four PCs, and the data from eight observations (sentences) on ten variables (terms) can be reasonably reduced to one from eight obsevations on 4 PCs. However, the degree of explanation of sample variance is slightly different between them X  X n other words, the selected PCs by the proposed method are more informative or reasonable.
 tracted from the original matrix, A , but X 2 is added to them by the revised matirx,  X  A . Since the content of our sample article is concerned with the protec-tion of environment, by adding the X 2 ,  X  X NEP X , the degree of information on its content can be increased.
 be more helpful to extract the significant terms concerned with the content of the document. For performance evaluation, the proposed approach was applied to the document summarization based on the occurrence of the extracted significant terms. The proposed method was compared to that of only PCA proposed in [6] which noted that the mothod by using PCA was to be preferred over term frequency[4,7] and lexical chain[2] for the document summarization.
 computed the score of each sentence by repeatedly summing 1 for each occurrence of terms, and second extracted the sentences by their scores in descending order, depending on compression rate.
 KISTI 1 for the evaluation. Each document consists of orginal article and manual summary amounting to 30% of the source text. And this manually extracted summary is regarded as the correct one.
 are defined respectively as follows.
 Table 5 shows that, by means of F-measure, the proposed method has improved the performance by around 3 . 3% over the method using the orginal sentence-term matrix for PCA. In this paper, we have proposed the way to extract the representative or signif-icant terms of the text documents by SVD and PCA.
 matrix is removed by using the cumulative ratio of the singular values by com-puting SVD of the original matrix. In the second step, the signifcant terms are extracted by performing PCA with the revised one. Consequently, the SVD is efficient to remove the noisy data of the frequency distribution, and the PCA to extract the significant terms by eigenvalue-eigenvector pairs. The experimental results on Korean newspaper articles show that the proposed method, SVD and PCA, is to be preferred over only PCA when the document summarization is the goal.
 for indexing, document categorization for feature selection, and automatic sum-marization for extracting terms and so on.

