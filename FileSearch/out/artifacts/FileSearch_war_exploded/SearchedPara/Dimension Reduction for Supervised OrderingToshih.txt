
Ordered lists of objects are widely used as representa-tional forms. Such ordered objects include Web search re-sults and best-seller lists. Techniques for processing such ordinal data are being developed, particularly methods for a supervised ordering task: i.e., learning functions used to sort objects from sample orders. In this article, we propose two dimension reduction methods specifically designed to improve prediction performance in a supervised ordering task.
Orders are sequences of objects sorted according to some property and are widely used to represent data. For exam-ple, responses from Web search engines are lists of pages sorted according to their relevance to queries. Best-seller lists, which are item sequences sorted according to sales volume, are used on many E-commerce sites. Processing techniques for orders have immediate practical value, and so research concerning orders has become very active in recent years. In particular, several methods are being de-veloped for learning functions used to sort objects repre-sented by attribute vectors from example orders. We call this task Supervised Ordering [14] and emphasize its use-[4, 9, 11, 20, 23] , and recommendation [8].

Several methods have been developed for the supervised ordering task. However, when the attribute vectors that de-scribe objects are in very high dimensional space, these su-pervised ordering methods are degraded in prediction per-formance. The main reason for this is that the number of model parameters to be learned grows in accordance with the increase of dimensionality; thus, the acquired functions might not perform well when sorting unseen objects due to over-fitting.
Dimension reduction techniques are one obvious solu-tion to the problems caused by high dimensionality. Dimen-sion reduction is the task of mapping points originally in high dimensional space to a lower dimensional sub-space, while limiting the amount of lost information. Principal component analysis (PCA) is one of the typical techniques for dimension reduction. PCA is designed so that varia-tions in original data are preserved as much as possible. It has been successfully used for other learning tasks but is less appropriate for a supervised ordering task. Since PCA is designed so as to preserve information regarding the ob-jects themselves, useful information in terms of the target ordering might be lost by this approach. Therefore, in this paper, we propose Rank Correlation Dimension Reduc-tion (RCDR) for dimension reduction in conjunction with supervised ordering. RCDR is designed to preserve infor-mation that is useful for mapping to the target ordering.
We show a formalization of the supervised ordering and known facts regarding orders in Section 2. We propose our RCDR methods in Section 3. Experimental results are shown in Section 4. We discuss and summarize the results in Section 5.
To describe the known properties of orders and the su-pervised ordering task, some basic notations must first be designed. An object, entity, or substance to be sorted is de-noted by x j . The universal object set, X  X  , consists of all possible objects. Each object x j is represented by the at-tribute value vector x j = [ x j 1 , x j 2 , . . . , x jK is the number of dimensions of attribute space. The order is denoted by O = x a  X  X  X  X  X  x j  X  X  X  X  X  x b . Note that sub-script j of x doesn X  X  mean  X  X he j -th object in this order, X  but that  X  X he object is uniquely indexed by j in X  X  . X  The order x 1  X  x 2 represents  X  x 1 precedes x 2 . X  An object set X ( O i ) or simply X i is composed of all objects in the or-der O i . The length of O i , i.e., | X i | , is denoted by complete order; otherwise, the order is incomplete. Rank, r ( O i , x j ) or simply r ij , is the cardinal number that indi-cates the position of the object x j in the order O i . For ex-ample, for O i = x 1  X  x 3  X  x 2 , r ( O i , x 2 ) or r i 2 orders, O 1 and O 2 , consider an object pair x a and x b dant w.r.t. x a and x b if the two objects are placed in the same order, i.e., ( r 1 a  X  r 1 b )( r 2 a  X  r 2 b )  X  0; they are discordant. Further, O 1 and O 2 are concordant if O 1 and O 2 are concordant w.r.t. all object pairs such that x , x b  X  X 1  X  X 2 , x a 6 = x b .
 We then describe the distance between two orders, O 1 and O 2 , composed of the same sets of objects, i.e., X ( O 1 ) = X ( O 2 )  X  X . Various kinds of distance for orders have been proposed [18]. Spearman distance d ( O 1 , O 2 ) is widely used. It is defined as the sum of the squared differences between ranks: By normalizing the range to be [  X  1 , 1] , Spearman X  X  rank correlation  X  is derived. where L = | X | . This exactly equals the correlation co-efficient between ranks of objects. The Kendall distance d
K ( O 1 , O 2 ) is another widely used distance. Consider a set of object pairs, { ( x a , x b )  X  X  X  X } , a 6 = b, x a pairs is M = ( L  X  1) L/ 2 . The Kendall distance is defined as the number of discordant pairs between O 1 and O 2 w.r.t. x a and x b . Formally, where sgn( x ) is a sign function that takes 1 if x&gt; 0 x =0 , and  X  1 otherwise. By normalizing the range to be [  X  1 , 1] , Kendall X  X  rank correlation  X  is derived. The computational costs for deriving  X  and  X  are O ( L log L ) and O ( L 2 ) , respectively. The values of  X  are highly correlated, because the difference between two criteria is bounded by Daniels X  inequality [16]:
A Supervised Ordering task (Figure 1) can be con-sidered as a regression or a fitting task whose target vari-ables are orders. The input data is a set of sample orders, S = { O 1 , . . . , O N } , where N is the number of samples. These samples give information about which objects should be ranked higher, and consist of objects represented by at-tribute vectors. No other side information, such as prefer-ence scores, is provided. The regression curve corresponds to a regression order. Analogous to a standard regression, a regression order is estimated so as to be concordant not only with given sample orders in S , but also with orders that will be generated. A regression order is modeled by an ordering function: Given an unordered object set, the ordering function outputs the estimated order, such that it is composed of the given unordered set and is concordant with the regression order. Supervised ordering also differs from classification because orders can be structured using symmetric groups, while classes cannot.

A supervised ordering task is closely related to a notion of a central order [18]: Given sample orders S , the cen-tral order  X  O ( S ) is defined as the order that minimizes the sum of the distances P above regression order in that concordance only with given samples is considered, and objects are represented not by at-tributes, but by unique identifiers. In a supervised ordering case, there may be objects not observed in given samples. (e.g., x 4 in Figure 1) Such objects should be ranked under the assumption that the neighboring objects in the attribute space would be close in rank.

Supervised ordering is also related to ordinal regres-sion [19], which is a regression in which response variables are ordered categorical. Similar to categorical variables, or-predefined values, and these values are ordered addition-ally; for example, the domain of a variable may be {  X  X ood X ,  X  X air X ,  X  X oor X  } . Ordered categories and orders differ in two points: First, while orders provide purely relative informa-tion, ordered categorical values additionally include abso-lute information. For example, while the category  X  X ood X  means absolutely good, x 1  X  x 2 means that x 1 is rela-tively better than x 2 . Second, the number of grades that can be represented by ordered categorical variables is lim-ited. Consider a set of four objects. Because at least two objects must be categorized into one of the three categories, {  X  X ood X ,  X  X air X ,  X  X oor X  } , the grades of these two objects are indistinguishable. However, orders can represent differ-ences of grades between any two objects. As pointed out in [3], supervised ordering is a more general problem than ordinal regression; thus, supervised ordering methods can be applied to ordinal regression tasks. Generally speaking, one should not try to solve a more general problem than is required. For example, an ordinal regression task can be solved by using an SVM designed for supervised ordering as in [10]. However, an SVM specialized for ordinal regres-sion is more efficient [21]. Therefore, the two tasks, super-vised ordering and ordinal regression, have to be carefully distinguished.
Several methods have been developed for supervised or-dering. In [14], these methods are surveyed and their pros and cons are shown. Below, we briefly show these meth-ods. Cohen et al. [4] proposed a method adopting the paired comparison approach. Training examples are first decom-posed into ordered pairs, and the algorithm learns probabil-ity functions in which one object precedes the other. Then, unordered objects are sorted so as to maximize the objec-tive function that is a sum of these probability functions. RankBoost [8] tries to find a score function that is a linear combination of weak hypotheses. Weak hypotheses provide some partial information about the target order to learn. By using a boosting technique, weights and weak hypotheses are chosen so that scores are concordant with given sample orders. Unordered objects can be sorted according to the learned scores. Order SVM [15] learns near-parallel hyper-planes in the attribute vector space; the hyperplanes sepa-rate higher-ranked objects from lower-ranked ones. In the sorting stage, objects are ordered along the direction per-pendicular to the hyperplanes. Support Vector Ordinal Re-gression (SVOR) [9] was proposed by Herbrich et al. In the learning stage, SVOR finds an optimal direction such that along this direction the minimum margin between a pair of objects is large. This method is independently proposed as the Ranking SVM by Joachims [11]. An active learning ex-tension of this method is proposed by Yu [23].

We turn to our Expected Rank Regression (ERR) method. After expected ranks of objects are derived, the function to estimate these expected ranks is learned using a standard regression technique. To derive expected ranks, assume that orders O i  X  S are generated as follows: First, an unseen complete order O  X  jects are then selected uniformly at random, and these are eliminated from O  X  to [1], under this assumption, the conditional expectation of ranks of the object x j  X  X i in the unseen complete order given O i is These expected ranks are calculated for all objects in each O i  X  S . Next, weights of regression function f ( x j ) are es-timated by applying a standard regression method. Samples for regression consist of the attribute vectors of objects, and their corresponding expected ranks, r ( O i , x j ) / ( L thus, the number of samples is P of f ( x j ) are learned, the order  X  O u can be estimated by sort-ing the objects x j  X  X u according to the values of f ( x
Next, we show some examples of applications. In [11, 20], a supervised ordering method is used to exploit relevance feedback data in a document retrieval task. The authors proposed an elegant technique to implicitly obtain users X  feedback information about preference in retrieved documents. Assume that retrieved documents are listed by sorting the degree of relevance to the given query. If the user selected the third document x c , this action implies that the user prefers this document to the first, x a , or the second, because he/she checks the sorted documents sequentially from the top of the list. So, relevance feedback data, x x a and x c  X  x b , can be implicitly obtained. Further, doc-uments are represented by features, such as similarity mea-sures to the query words, types of documents, or the ranks in lists generated without exploiting feedback information. From these feedback data and features of documents, a su-pervised ordering method makes it possible to learn func-tions for sorting documents according to the degree of user X  X  preference as well as the documents X  relevance to the user X  X  query.

In [17, 2], supervised ordering methods are used for sen-sory tests to examine which product features affect the value of the products. Metasearch engines are constructed in [4, 8]. Supervised ordering can be used to make content-based recommendation. Users X  relative preference data are first obtained. Based on these preference orders and fea-tures of items, items can be sorted according to the de-gree of users X  preference by applying a supervised ordering method. Finally, highly ranked items are recommended to users.
In the previous section, we defined a supervised learn-ing task. Here, we show a dimension reduction technique specially designed for these supervised ordering methods.
To obtain satisfactory results when using data mining or machine learning algorithms, it is important to apply pre-processing methods, such as feature selection, dealing with missing values, or dimension reduction. Appropriate pre-processing of data can improve prediction performance, and can occasionally reduce computational and/or memory costs. Some pre-processing techniques for mining or learn-ing methods dealing with orders have been proposed. Ba-hamonde et al. [2] applied wrapper-type feature selection to a supervised ordering task. Slotta et al. [22] performed feature selection for classification of orders. In [6, 5], rank statistics were used for selecting informative genes from mi-croarray data. To measure the similarities between orders, Kamishima and Akaho proposed a method to fill in missing objects in orders [13]. To our knowledge, however, dimen-sion reduction techniques specially designed for a super-vised ordering task have not yet been developed.

Similar to other types of learning tasks, such as classi-fication or regression, dimension reduction techniques will be beneficial for supervised ordering tasks, in particular, if the number of attributes, K , is very large. With reduced dimensions, the generalization ability can be improved. Be-cause the number of model parameters to be learned grows in accordance with K , the acquired functions might not per-form well when sorting unseen objects due to over-fitting. In particular, if there are many non-informative attributes or if complex models are used, the problem of over-fitting will be alleviated by reducing dimensions.

To reduce the number of dimensions before performing supervised ordering, one might assume that reduction tech-niques used for other learning tasks can be used. However, this is not the case. Principal component analysis (PCA) is one of typical techniques for dimension reduction. PCA is designed so that information about data in original at-tribute vector space is preserved as much as possible. This approach is less appropriate for a supervised ordering task. Specifically, because a supervised ordering task must find a mapping from attribute vectors to the target ordering, it is not sufficient to preserve information only in source vec-tors. On the other hand, Diaconis X  spectral analysis [7] for orders is another possibility. This is a technique to decom-pose distributions of orders into sub-components. For ex-ample, first-order components represent the frequency that the object x j is l -th ranked, while second-order components represent the frequency that objects x j and x k are l -th and m -th ranked, respectively. However, our goal is not to find decomposition in an ordinal space, but to find a sub-space in an attribute vector space.

From the above discussion, it should be clear that we had to develop reduction techniques that preserve information about mappings from attribute vectors to the target ordering. This is analogous to Fisher X  X  discriminant analysis, which is a dimension reduction technique to preserve information about a mapping from an attribute vector to target classes.
Additionally, the computational cost for reducing dimen-sions should not be much higher than that for supervised
Table 1. Computational complexities of su-pervised ordering algorithms ordering methods. Computational complexities of super-vised ordering methods in the learning stage are summa-rized in Table 1. We assume that the number of ordered pairs and objects in S are approximated by N  X  L 2 and N respectively (  X  L is the mean length of the sample orders). The SVM X  X  learning time is assumed to be quadratic in the number of training samples. The learning time of Cohen X  X  method or the RankBoost is linear in terms of N  X  L 2 , if the number of iterations is constant. However, in practical use, the number of iterations should be increased adaptively. In the experiment in [8], the number of iterations was linearly increased in accordance with the number of ordered pairs, N  X 
L 2 . Therefore, their time complexities approach N 2  X  L 4 k When dimension reduction methods require much higher computational costs than those in Table 1, the reduction of dimensions greatly lessens scalability.

Taking into account what is mentioned above, our di-mension reduction methods should satisfy two require-ments. 1. It must be designed so as to preserve information about 2. The computational complexity for dimension reduc-To fill these requirements, we propose Rank Correlation Dimension Reduction (RCDR) . Given a basis that consists of l vectors, the next l +1 vector is selected so as to preserve as much information about target ordering as possible. By repeating this procedure, we obtain the final sub-space.
First, we outline our RCDR method. Let w ( l ) be the l -th vector of a basis. The sub-space spanned by the basis, x l -th subspace W ( l ) Let W ( l )  X  be the complementary space of the W ( l ) , that is spanned by ( K  X  l ) vectors which are orthogonal to all orders S and attribute vectors, { x j } , and the basis of the l -th sub-space. This condition is depicted in Figure 2. The objects in the original K -dimensional spaces (marked by  X   X   X  in Figure 2) are projected to the complementary space W ( l )  X  of the l -th sub-space. The projected objects (marked by  X   X   X  in Figure 2) are denoted by x ( l ) this projection, we can eliminate information about the tar-get ordering contained in the sub-space W ( l ) . For each 1 , . . . , K , objects are sorted in descending order of the attribute values of the objects projected to W ( l )  X  . In Fig-ure 2, examples of those orders are x 1  X  x 3  X  x 2 in the first attribute and x 1  X  x 2  X  x 3 in the second attribute. The rank correlations between each of these orders and each sample order are calculated. Then, the sum of these rank corre-lations are denoted by R ( l ) later). This R ( l ) target ordering and the k -th attribute values of the objects projected on the l -th complementary space. A new vector, w ( l +1) , is chosen so that each element of this vector, is as proportional to the corresponding concordance, R ( l ) as possible.

Now, we formally describe our RCDR. Let w ( l ) = [ w 1 , w These vectors are orthonormal to each other, i.e., final sub-space is denoted by K 0 . We are given a set of sample orders S = { O 1 , . . . , O N } , the basis of the sub-space, W ( l ) , and the objects { x j | x j  X  X S } , X  X 
O i  X  S X i . From these, we derive the ( l +1) -th vector, w ( l +1) , as follows. First, we define R ( l ) 1 , . . . , R concordances between sample orders and the attribute val-ues of the objects projected on the complementary space, W ( l )  X  . Let us focus on the sample order O i and the attribute values of objects. Because the goal of a super-vised ordering task is to estimate the orders of objects, the relative ordering of attribute values is more important than the attribute values themselves. We therefore sort the th attribute values x ( l ) scending order, where x ( l ) of the object, x ( l ) space. Note that the projected objects are represented as [ x j 1 , . . . , x The resultant order is denoted by O ( X i , x ( l ) this O ( X i , x ( l ) set of objects, the concordance between these two orders can be measured by Kendall X  X   X  . Such rank correlations are calculated between the k -th attribute values and each of sample orders in S , and these correlations are summed up: We use this sum as a measure of the concordance between the k -th attribute values of objects and the target ordering. Next, to fill the first requirement of the RCDR, the ( l +1) vector is chosen so that the above concordance is preserved as much as possible. Let us consider the vector, Because the elements of this vector are the concordances between attribute values and the target ordering, this vec-tor would point in the direction that preserves information about the target ordering in the attribute space. Therefore, we choose the vector w ( l +1) so that it maximizes the co-sine between w ( l +1) and R ( l ) in the complementary space,
Figure 3. Kendall rank correlation dimension reduction W ( l )  X  . Further, the vector R ( l ) is constant, and 1 ; thus, the maximization of this cosine is equivalent to the maximization between the dot product between R ( l ) w ( l +1) . This optimization problem is formalized as fol-lows: Note that one might think that w ( l ) becomes a zero vec-tor, if l  X  2 , but this is not the case. When the perform-ing standard regression and Pearson X  X  correlation is maxi-mized, w ( l ) would be a zero vector for l  X  2 . This is be-cause zero Pearson X  X  correlation implies such orthogonality in the attribute space. However, because rank correlation doesn X  X  imply orthogonality, w ( l ) is generally a non-zero vector even if l  X  2 .

Next, we solve Equation (7). The derivation of w ( l +1) can be easily shown by the following procedure: Calcu-late the vector of the correlations sums, R ( l ) , project this vector to the l -th complementary space, and normalize the projected vector. Once a new vector is derived, objects in the l -th complementary space, x ( l ) , are mapped to the new complementary space, and iteratively the next vector can be computed. This algorithm is shown in Figure 3. R ( l ) computed in line 3, projected to the current complementary space in line 4, and normalized in line 5 so that its norm is one. In lines 6 and 7, the objects in the current complemen-tary space are projected to the new complementary space. Because the concordance is measured by Kendall X  X   X  , we call this method Kendall RCDR . The computational com-plexities of lines, 3, 4, 5, and 6-7 are O ( N  X  L 2 K ) Table 2. Vectors of a Basis derived by our RC-
DRs and the PCA O ( K ) and O ( N  X  LK ) , respectively; thus, the complexity per one iteration is O ( N  X  L 2 K ) (generally N  X  L 2  X  K 0 the total complexity is O ( N  X  L 2 KK 0 ) . As noted before, be-cause the complexity of Cohen X  X  method and RankBoost practically approaches O ( N 2  X  L 4 K ) , our Kendall RCDR is faster than supervised ordering methods except for ERR (see Table 1). To further save time complexity, we re-place Kendall X  X   X  in line 3 of the algorithm by Spearman X  X   X  , because  X  and  X  are highly correlated. We call this method Spearman RCDR . Because its time complexity is O ( NKK 0  X  L log  X  L ) , this method becomes faster than the ERR method if K 0 log  X  L &lt; K . Therefore, our RCDR meth-ods satisfy the second requirement. Note that the Kendall RCDR is faster than the Spearman RCDR in the special case: L i = 2 , O i  X  S . Joachims et al. proposed a method to implicitly collect sample orders whose lengths are two [11]. The Kendall RCDR is useful in such cases.
After showing a simple example of our RCDR methods, we describe the experimental results for real data sets.
To show what is produced by our two RCDR methods, we present a simple example using artificial data. We give dimensions of the original space as K = 5 and the number of objects as | X  X  | = 1000 . For each object x j  X  X  X  first to the fourth attribute values are randomly generated according to the normal distribution, N (0 , 1) , while the fifth value is equal to the fourth. We generated 300 sample orders as follows: Five objects were selected uniformly at random from X  X  ; then these objects were sorted in descending order of w  X  &gt; x j . We applied Kendall RCDR, Spearman RCDR, and PCA to this data set. The first and second vectors are shown in the upper and lower parts of Table 2, respectively. In each row, we show vectors derived by Kendall RCDR, Spearman RCDR, and PCA. The first to the fifth columns show the elements of vectors. In the last column, the norm lengths of the sum vector of rank correlations per sample order, k R ( l ) k /N , are shown for the RCDR cases, and the contribution ratios are shown for the PCA cases.

Let X  X  look at the first vector. The vectors derived by the two RCDR methods show resemblance. This indicates that one can use the faster RCDR method; concretely, Spearman RCDR is better except for the case L i = 2 . Because the fourth and the fifth elements of the w  X  are zero, no infor-mation useful for the target ordering is represented in these axes. In our RCDR cases, the fourth and the fifth weights of vectors are almost zero; thus, these useless axes can be ig-nored. In the PCA case, the fourth weight is far from zero, because no information about the target ordering is taken into account. The PCA merely ignores axes that are corre-lated in attribute space, such as in the fifth element. Further, because variances in all dimensions are equal, the contri-decided by a linear function.

We turn to the second component. In the RCDR cases, the correlation vector size k R (2) k /N is much smaller than k R (1) k /N ; this means that the second vector is far less in-formative than the first, because the target ordering is gen-erated by a linear function in this example. In the PCA case, the contribution ratio indicates that useful information still remains in this vector. Note that it is not guaranteed that the k R ( l ) k /N decreases in accordance with the increase of and vectors with bigger k R ( l ) k /N don X  X  always contribute to predicting the target ordering. However, we empirically observed that if k R ( l ) k /N is very small, the corresponding vector is not informative. We believe that k R ( l ) k /N used as an index for the importance of vectors.
We applied the methods described in Section 3 to real data from questionnaire surveys [14]. The first data set was a survey of preferences in sushi (Japanese food), and is de-noted by SUSHI . In this data set, N = 500 , L i = 10 , and | X  X  | = 100 . Objects are represented by 12 binary and 4 numerical attributes. The second data set was a question-naire survey of news article titles sorted according to their significance, and is denoted by NEWS . These news articles were obtained from  X  X D-Mainichi-Newspapers 2003. X  In this data set, N = 4000 , L i = 7 , and | X  X  | = 11872 . Titles were represented by 0 -1 vectors indicating whether a spec-ified keyword appears in the title. Among 18381 keywords, we selected 595 keywords that were observed 30 or more times. Additionally, we used 8 binary attributes to represent article categories; thus, the number of attributes was 603 in total.

To evaluate the usefulness of our dimension reduction methods, we applied the ERR supervised ordering method [14] to these two data sets. As a family of fitting functions, a linear model was adopted. Sample order sets were parti-tioned into testing and training sets. The ordering function was learned from training a sample order set with original attributes or reduced attributes. After learning, prediction performance was measured by the mean of  X  between an order in a testing set, O t , and the corresponding estimated order,  X  O t . The larger  X  was, the better the prediction perfor-mance was. The number of folds in cross-validation was ten for SUSHI and five for NEWS . In the left and right parts of the Figure 4, we show the variation of mean  X  in accor-dance with the dimensions of the reduced space, K 0 SUSHI and NEWS , respectively. For both data sets, N or L i was varied by eliminating sample orders or objects; the results for these sets are shown in each sub-figure. N and/or L i increased from the sub-figure (a) to (c); thus, orders be-came the most difficult to estimate in the sub-figure (a) case. The curves labeled by  X  X RCDR X ,  X  X RCDR X , and  X  X CA X  show the mean  X  derived by ERR after applying Kendall RCDR, Spearman RCDR, and PCA, respectively. The label  X  X RIG X  indicates that no reduction method was used, and original attribute vectors were adopted.

From these figure, the following conclusions can be drawn. First, the two RCDR methods show resemblance; thus, the faster method can be used for dimension reduc-tion. Second, both RCDRs performed better in predic-tion than PCA. The difference was particularly clear when the number of dimensions K 0 was small. This means that RCDR successfully preserved information useful for esti-mating target orders. Therefore, we can say that RCDR is more effective than PCA when carrying out a super-vised ordering task. Third, our RCDR technique could improve the prediction performance. The curves labeled  X  X RCDR X / X  X RCDR X  were compared with those labeled  X  X RIG. X  Surprisingly, the reduced vectors could lead to bet-ter prediction than the original vectors, even through some information might be lost by dimension reduction. We think that this is because the models used for ordering were sim-plified while useful information was preserved. This can be confirmed by the fact that the improvements were promi-nent when N and/or L i were small. The simpler model could produce better generalization ability for a limited number of samples. Therefore, our reduction technique is useful for improving prediction performance.

We then compared the results in Figure 4 with the experi- X   X   X  mental results in [14]. The results in [14] were copied to Ta-ble 3. Rather different attributes were used, as described in the note of the table. Note that when we applied the SVOR method together with the large attribute sets used in Fig-ure 4, the prediction accuracy was degraded. In this experi-ment, second-order polynomials were used for fitting func-tions for the ERR case; thus, the results differ from those in Figure 4. When observing these two results, for all NEWS and SUSHI -100:5 data sets, a linear ERR with RCDR could make a better prediction than all supervised ordering meth-ods in Table 3, which adopted non-linear models for order-ing. Further, our proposed method was the second best for the SUSHI -100:2 set, and the third best for the SUSHI -500:10 set. Therefore, we can say that our RCDR methods could successfully represent information about non-linear relations between attribute values and target ordering in or-thonormal subspace.

Finally, we can exploit the components of vectors for qualitative analysis. We obtained the first vector, derived from the SUSHI -500:10 data set by applying our Kendall RCDR method. The components of the vectors, w 1 , . . . , w absolute values, | w (1) lows: w 13 = 0 . 5951 w 15 = 0 . 4278 w 1 = 0 . 4237 w 14 = 0 . 2822 w 12 =  X  0 . 2317 From these components, we can say that  X  X sers primary pre-fer sushi that they frequently eat and that supplied in many sushi restaurants. X 
In this paper, we proposed a dimension reduction tech-nique specialized for a supervised ordering task. The method was designed so as to preserve information about a relation from object attribute vectors to the target ordering. For this purpose, we developed Kendall RCDR and Spear-man RCDR. We then applied these methods to real data sets. From the experimental results, we arrived at the fol-lowing conclusions. First, the RCDR methods outperform PCA when carrying out a supervised ordering task. Second, by using the RCDR technique, performance in prediction can be improved, especially when training samples are not adequate. Finally, our two RCDR methods are compara-ble in prediction performance. Therefore, the faster method should be used; concretely, Spearman RCDR is better ex-cept for the condition where L i = 2 .

Intuitively speaking, in the l -th iteration of the RCDR, the algorithm finds the vector that is most relevant to target ordering. After that, by mapping attribute vectors to the new sub-space, components in attributes related to this vector are subtracted. At this time, it might be effective to subtract the explained component in the target ordering from sample orders. We will try such improvement by using a technique like Diaconis X  spectral analysis [7].
 Acknowledgments: This work is supported by the grants-in-aid 14658106 and 16700157 of the Japan society for the promotion of science. Thanks are due to the Mainichi Newspapers for permission to use the articles.

