 Lexicalized phrase structure parsing techniques were first introduced by Charniak (2000) and Collins (2003) as generative probabilistic models. Nowadays most statistical models used in natu-ral language processing are discriminative: dis-criminative models provide more flexibility for modelling a large number of variables and conve-niently expressing their interactions. This trend is particularly striking if we consider the literature in dependency parsing. Most state of the art multi-lingual parsers are actually weighted by discrimi-native models (Nivre and Scholz, 2004; McDon-ald et al., 2005; Fern  X  andez-Gonz  X  alez and Martins, 2015).

With respect to multilingual phrase structure parsing, the situation is quite different. Most parsers focus on fixed word order languages like English or Chinese as exemplified by Zhu et al. (2013). Despite a few exceptions (Collins et al., 1999), multilingual state of the art results are gen-erally derived from the generative model of Petrov et al. (2006). Although more recently Hall et al. (2014) introduced a conditional random field parser that clearly improved the state of the art in the multilingual setting.

Both Petrov et al. (2006) and Hall et al. (2014) frame their parsing model to model in priority regular surfacic patterns and word order: Petrov et al. (2006) crucially infers category refinements (called category  X  X plits X ) in order to specialize the grammar on recurrent informative patterns ob-served on input spans. Hall et al. (2014) re-lies on a similar intuition : the model essentially aims to capture regularities on the spans of con-stituents and their immediate neighbourhood, fol-lowing earlier intuitions of Klein and Manning (2004). This modelling strategy has two main mo-tivations. First it reduces the burden of feature en-gineering, making it easier to generalize to multi-ple languages. Second it avoids modeling explic-itly bilexical dependencies for which parameters are notoriously hard to estimate from small data sets such as existing treebanks.

On the other hand this strategy becomes less in-tuitive when it comes to modeling free word or-der languages where word order and constituency should in principle be less informative. As such, the good results reported by Hall et al. (2014) are surprising. It suggests that word order and constituency might be more relevant than often thought for modelling free word order languages.
Nevertheless, free word order languages also tend to be morphologically rich languages. This paper shows that a parsing model that can effec-tively take morphology into account is key for parsing these languages. More specifically, we show that an efficient lexicalized phrase structure parser -modelling both dependencies and mor-phology -already significantly improves parsing accuracy. But we also show that an additional modelling of spans and constituency provides ad-ditional robustness that contributes to yield state of the art results on almost all languages consid-ered, while remaining quite efficient. Moreover, given the availability of existing multi-view tree-banks (Bhatt et al., 2009; Seddah et al., 2013; Qiu et al., 2014), our proposed solution only requires a lightweight infrastructure to achieve multilin-gual parsing without requiring costly language-dependent modifications such as feature engineer-ing.

The paper is organized as follows. We first re-view the properties of multiview treebanks (Sec-tion 2). As these treebanks typically do not pro-vide directly head annotation, an information re-quired for lexicalized parsing, we provide an au-tomated multilingual head annotation procedure (Section 3). We then describe in section 4 a vari-ant of lexicalized shift reduce parsing that we use for the multilingual setting. It provides a way to integrate morphology in the model. Section 5 fi-nally describes a set of experiments designed to test our main hypothesis and to point out the im-provements over state of the art in multilingual parsing. Multi-view treebanks are treebanks annotated both for constituents and dependencies that have the property to be token-wise aligned (Bhatt et al., 2009; Seddah et al., 2013; Qiu et al., 2014) . These double annotations are typically obtained by con-verting a constituency or dependency annotation into the other annotation type. This method was used for the construction of the dataset for the SPMRL 2013 shared task (Seddah et al., 2013), which contains multi-view treebanks for a num-ber of morphologically rich languages, for which either constituency or dependency treebanks were available. The same kind of process was applied to the Penn TreeBank using the Stanford conver-sion system to produce dependency annotations (de Marneffe et al., 2006). In this paper, we use both of these datasets.

Although in multi-view treebanks each sentence is annotated both for constituency and depen-dency, they are not normalized for categories nor lexical features accross languages such as depen-dencies in the Google Universal Treebank (Mc-Donald et al., 2013). What is more, the depen-dency and constituency structures may sometimes strongly differ. For some languages, like Hungar-ian, the conversion has involved some manual re-annotation (Vincze et al., 2010). Lexicalized phrase structure parsers traditionally use hand-crafted heuristics for head annotation (Collins, 2003). Although these heuristics are available for some languages, for others they are non existent or non explicit and typically hidden in conversion procedures. In order to leverage the burden of managing language specific heuristics, we first automate head annotation by taking ad-vantage of the multi-view annotation.

We begin by introducing some notation. As-suming a sentence W = w 1 ...w n , the depen-dency annotation of this sentence is assumed to be a dependency forest (Kuhlmann and Nivre, 2006). A dependency graph G =  X  V,E  X  where V = { 1 ...n } is the set of word indexes or vertices and E  X  V  X  V is a set of dependency links. By convention, a dependency ( i,j ) means that i gov-erns j . A dependency forest is a dependency graph such that a node has at most a single incoming edge and where there is no cycle. A node with no incoming edge is a root of the dependency forest and a dependency tree is a dependency forest with a single root. For some languages, such as Ger-man or Basque, the dependency structures found in the data set are actually dependency forests.
Lexicalized parsing relies on head annotation, in other words each node in a constituency tree is associated with the word index of its head. More formally, let A be the set of nodes in the c-tree, head annotation can be represented as a function h : A 7 X  { 1 ...n } which maps each node a  X  A to the index of its head in the input sentence. h is obtained by leveraging head-related information associated with each rule in the grammar. More precisely, each rule  X   X   X  , with  X  = a 1 ...a k , is associated with a head index i ( 1  X  i  X  k ) that states that the head h (  X  ) of any node labeled  X  in a constituency tree that is built using this rule is the same as the head of the right-hand side symbol a i .
A Naive h function is straightforwardly defined as the annotation of each local rule part of the tree
When  X  a  X  A such that h ( a ) =  X  we say that the annotation has failed.

However the naive procedure fails in a large number of cases. Failures fall into the four pat-terns that are illustrated in Figure 1. For each of the patterns we have highlighted in bold the sym-bols for which the Naive procedure currently fails. Local Restructuration I is where the c-structure is flatter than the d-structure. Here the Naive pro-cedure fails because ( a,c ) 6 X  E . Local Restruc-turation II is where the d-structure is flatter than the c-structure. The procedure fails because nei-ther ( a,b )  X  E nor ( b,a )  X  E . Forest Effect is where the d-structure is a dependency forest (here E =  X  ). And finally Non Projectivity is where the d-tree is non projective.
 We can easily correct the naive procedure for Local Restructuration I by taking advantage of E + , the non reflexive transitive closure of E , thus yielding the following Corrected procedure:
The three other cases are more problematic, since their correction would somehow require altering the structure of either the c-tree or the d-tree. Refraining from altering the constituency data set we instead use a catch-all procedure that essen-tially creates the problematic head annotation by analogy with the rest of the data, yielding a fully Robust procedure that is guaranteed to succeed in any case: where KNN (  X   X   X  ) is a function returning a guess for the position of the head in  X  , the right hand side of the rule, based on similarity to suc-cessfully head annotated rules.

The details are as follows. KNN (  X   X   X  ) sup-poses a dataset D = ( R i , H i ) N head annotated rules. In this dataset, each rule R i =  X   X   X  is associated with H i the posi-tion of the head in  X  . We define the similarity between two rules R 1 =  X  (1)  X  a (1) and R 2 =  X  (2)  X  a (2) the function returns the most frequent H among the 5 most similar rules in the data set.

The full head annotated data set D is built by reading off the rules from the trees successfully annotated in the treebank by the Corrected proce-dure in a first pass. A second pass yields the final annotation by running the Robust procedure. Analysis of the conversion We report in Table 1 an overall quantification of the conversion proce-dure: % Success ( Corrected ) reports the number of trees succesfully annotated by the Corrected procedure and Silver UAS reports an UAS score obtained by comparing the reference dependency trees to the conversion of those obtained from the Robust conversion of the head-annotated phrase structure trees back to dependency structures. The conversion works well apart from four languages (Arabic, Basque, German and Hungarian) which cause more difficulties.

In order to better understand the problems faced by the conversion procedure, we manually in-spected the errors returned by the Corrected pro-cedure. For each language, we sampled 20 ex-amples of failures encountered and we manually categorized the errors using the four patterns illus-trated in Figure 1. Across languages, 49.9% of the errors come from the pattern Local Restructura-tion II and 50% from the pattern Forest effect and more suprisingly, we found only one example in our sample from the pattern Non projectivity in the Hungarian treebank. This overall average hides however an important variation across treebanks. The Forest effect is indeed massively found in the marginally in the Hungarian data set. Most of the time, these are cases of short word sequences (2 to 5 tokens) where all nodes are annoted as roots of the dependency trees. The Local restructuration II is mostly found in the Arabic, Hebrew and Polish treebanks and less frequently in Hungarian. Ara-bic and Hebrew tend to follow a binary annotation scheme partially inspired by X-Bar, hence creat-ing additional constituent structures that are not di-rectly inferrable from the dependency annotation. Polish uses this restructuration in patterns involv-ing coordination. More surprisingly, non projec-tive patterns, which we expected to be a signifi-cant feature of these languages, remain marginal in comparison to annotation related idiosyncrasic problems. This section provides an overview of the design of the constituent parsing system. There are three re-cent proposals for beam-based discriminative shift reduce parsing for phrase structure grammar with a structured perceptron and beam search (Zhu et al., 2013; Crabb  X  e, 2014; Mi and Huang, 2015). All three proposals point out that for weighted phrase structure parsing, the shift reduce algorithm re-quires a special treatment of unary rules in order to compare derivations of the same length. They all provide different management schemes for these unaries.

The work described here draws on the LR algo-rithm introduced by Crabb  X  e (2014), but provides a simpler algorithm, it precisely describes the man-agement of unary rules and clarifies how spans and morphological information is represented (see sec-tion 5 ).
For each language, the grammar is induced from a treebank using the following preprocessing steps. The corpus is first head-annotated with the Robust head annotation procedure. Second, the treebank is head-markovized (order 0) and unary into unique symbols. Once this has been done we assume that tokens to be parsed are a list of couples (tag, wordform). The preprocessing steps ensure the binarized treebank implicitly encodes a binary lexicalized grammar whose rules are ei-ther in Chomsky Normal Form ( CNF ) like in (a) or are also of the form (b) X [ h ]  X  A [ h ] t , X [ t ]  X  A [ h ] t , X [ h ]  X  tB [ h ] , X [ t ]  X  tB [ h ] where A,B,X are delexicalized non-terminals, h,x,t are tokens (terminals) and A [ h ] ,A [ x ] ...X [ t ] are lexicalized non-terminals. Given a grammar in CNF , we can prove that for a sentence of length n , the number of derivation steps for a shift reduce parser is 3 n  X  1 . However our tagset-preserving transformation also introduces rules of the form (b), which explains why the number of derivation steps may vary from 2 n  X  1 to 3 n  X  1 .

To ensure that a derivation is of length 3 n  X  1 , the parser forces each shift to be followed by either a unary reduction or an alternative dummy Ghost Reduction (GR). Given the pre-processed treebank we infer the set A of actions used by the parser. Let  X  be the set of non-terminal symbols (includ-ing temporary symbols) read off from the binary treebank. The set of actions contains one Shift (S), one Ghost Reduction (GR) a set of |  X  | unary re-ductions (RU-X), one for each symbol, a set of |  X  | binary left reductions (RL-X) and a set of |  X  | bi-nary right reductions (RR-X) (see also Sagae and Lavie (2006) and Figure 3 for details).

The parser itself is organized around two data structures: a stack of symbols, S = ... | s 2 | s 1 | s 0 whose topmost element is s 0 . Symbols are lexi-calized non terminals or tokens of the form A [ x ] . The second structure is a queue statically filled with tokens T = t 1 ...t n . Parsing is performed by sequentially generating configurations C of the form  X  j, S ,  X  X  where S is a stack and j is the index of the first element of the queue. Given an ini-tial configuration C 0 =  X  1 ,,  X  X  , a derivation step C applying an action a t  X  1  X  A as defined in Fig-ure 3. The derivation is complete and successful I NIT  X  1 ,,  X  X  : 0 G OAL  X  n + 1 , X ,  X  X  : w once the action C 3 n  X  1 is generated. A derivation sequence C 0  X   X  is a sequence of derivation steps C Weighted prediction The choice of the action a  X  A at each derivation step is naturally non-deterministic. Determinism is provided by a weighting function based on a linear model of the form: where w  X  R d is a weight vector and  X  ( a i ,C i )  X  { 0 , 1 } d is a feature vector. The best parse is then the successful derivation with the maximum score: In practice, we use a beam of size K at each time step and lossy feature hashing, which makes the inference approximative.

For the purpose of computing weights, we ex-tend the representation of the stack and queue el-ements such that the feature functions have ac-cess to a richer context than just simple lexical-ized symbols of the form A [ x ] . As described in Figure 2 (left), features can also access the imme-diate left and right children of s 0 and s 1 as well as their left and right corner tokens. This allows us to encode the span models described in Section 5. We also use tuple-structured tokens encoding not only the word-form and the tag but also additional custom lexical features such as those enumerated in Figure 2 (right). This allows us to express the morphological models described in Section 5.
Finally, the parameters w are estimated with a parallel averaged structured perceptron designed to cope with inexact inference (beam search): we specifically rely on max-violation updates of Huang et al. (2012) and on minibatches to acceler-ate and parallelize training (Shalev-Shwartz et al., 2007; Zhao and Huang, 2013). The experiments aim to compare the contribution of span based features approximating some intu-itions of Hall et al. (2014) for shift reduce parsing and morphological features for parsing free word order languages. We start by describing the evalu-ation protocol and by defining the models used.
We use the standard S PMRL data set (Seddah et al., 2013). Part of speech tags are generated with Marmot (M  X  uller et al., 2013), a CRF tagger specif-ically designed to provide tuple-structured tags. The training and development sets are tagged by 10-fold jackknifing. Head annotation is supplied by the Robust procedure described in Section 3. The parser is systematically trained for 25 epochs with a max violation update perceptron, a beam of size 8 and a minibatch size of 24.
To enable a comparison with other published re-sults, the evaluation is performed with a version of evalb provided by the SPMRL organizers (Sed-dah et al., 2013) which takes punctuation into ac-count.
 Baseline model (B) The baseline model uses a set of templates identical to those of Zhu et al. (2013) for parsing English and Chinese except that we have no specific templates for unary reduc-tions.
 Span-based model (B+S) This model extends the B model by modeling spans. The span model approximates an intuition underlying Hall et al. (2014): constituent boundaries contain very infor-mative tokens (typically function words). These tokens together with the pattern of their neighbor-hood provide key clues for detecting and (sub-)typing constituents. Moreover, parameter esti-mation for frequent functional words should suf-fer less from data sparseness issues than the esti-mation of bilexical dependencies on lexical head words. The model includes conjunctions of non-terminal symbols on the stack with their left and right corners (words or tags) and also their imme-diately adjacent tokens across constituents. Using the notation given in Figure 2 we specifically in-cluded the following matrix templates : from which we derived additional backoff tem-plates where only a single corner condition is ex-pressed and/or words are replaced by tags.
 Morphological model (B+M) This model ex-tends the B model by adding morphological fea-tures. This model aims to approximate the intu-ition that morphological features such as case are key for identifying the structure of free word order languages. As feature engineering may become in principle quite complex once it comes to morphol-ogy, we targeted fairly crude models with the goal of providing a proof of concept. Therefore the morphologically informed models use as input a rich set of morphological features specified in Fig-ure 2 (right) predicted by the C RF tagger (M  X  uller et al., 2013) with the same jackkniffing as before. The content of Figure 2 provides an explicit indi-cation of the actual features defined in the original treebanks (see Seddah et al. (2013) and references therein for details), while the columns are indica-tive normalized names. For Basque most of the additional morphological features further encode case and verbal subcategorization. For French the mwe field abbreviates IOB predicted tags derived from multi-word expression annotations found in the original dataset.

Now let M be the set of values enumerated for a language in Figure 2 (right), we systematically added the following templates to model B : Essentially the model expresses interactions be-tween morphological features from the constituent heads on the top of the stack and the morphologi-cal features from the tokens at the beginning of the queue.
 Mixed model (B+S+M) Our last model is the union of the span model ( B+S ) and the morpho-logical model ( B+M ).
 Results (development) We measured the im-pact of the model variations on the development set for c-parsing on the SPMRL data sets (Table 2). We immediately observe that modelling spans tends to improve the results, in particular for lan-guages where the head annotation is more prob-ian and also Swedish however. So the span-based model seems to improve the parser X  X  robustness in cases when dependencies lack precision. For this model, the average behaviour is similar to that of Hall et al. (2014) although the variance is high.
On the other hand, the morphological model tends to be most important for languages where head annotation is easier: French, Korean, Polish and Swedish. It is key for very richly inflected lan-guages such as Basque and Hungarian even though comparison with Hall et al. (2014) also reveals that for Basque, Hungarian and Swedish, taking into account morphological information largely explains our improved results.
 Results (test) We observe in Table 3 that our joint B+S+M model yields a state of the art c-quite clear that both our span and morphology en-hanced models could be dramatically improved, but it shows that with reasonable feature engi-neering, these two sub-models are largely suffi-cient to improve the state of the art in c-parsing for these languages over strong baselines. Al-though in principle the Berkeley parsers (Petrov et al., 2006; Hall et al., 2014) are designed to be language-generic with an underlying design that is surprisingly accurate for free word order lan-guages end up suffering from a lack of sensitiv-ity to morphological information. Finally we also observe that our phrase structure parser clearly outperforms the TurboParser setup described by Fern  X  andez-Gonz  X  alez and Martins (2015) in which an elaborate output conversion procedure gener-ates c-parses from d-parses.
 Comparison with related work We conclude with a few comparisons with related work. This will enable us to show that our approach is not only accurate but also efficient. A comparison with dependency parsers will also allow us to bet-ter identify the properties of our proposal.
In order to test efficiency, we compared our parser to c-parsers trained on Penn Treebank (PTB) for which we have running times reported by Fern  X  andez-Gonz  X  alez and Martins (2015). This required first assigning heads, for which we used the Stanford tool for converting PTB to Basic De-pendencies, and then used our Robust conversion method. We performed a simple test using the PTB standard split with the same experimental set-ting as before, except that we use the standard evalb scorer (Table 5). Although the time com-parison remains indicative, it is clear that the pars-ing framework described in this paper is not only reasonably accurate on a fixed word order lan-guage such as English but it is also quite efficient. Parsing accuracies might be different with other head annotation schemes (See e.g. Elming et al. (2013) for illustrations). In our case, we compare the (B+S) model with automated head annotation to the Collins head annotation as implemented in the Standord CORE NLP library (Manning et al., 2014), where we can see that the Collins hand-crafted head annotation yields better results than
The question is now to which extend c-trees en-code meaningful dependencies? As lexicalized c-trees encode unlabeled dependency trees, our parser also directly outputs unlabeled d-trees by simply reading them off from the lexicalized c-structure. We report in Table 4 the UAS evalua-tion of those dependencies and we compare them to the best results obtained by dependency parsers in both SPMRL13 and SPMRL14 shared tasks. For each language, the comparison is made with English we compare against Standard TurboParser -which seems to be the most similar to our system-when parsing to Basic Stanford dependen-cies. The comparison with semi-supervised and ensemble parsers still provides a reasonable upper-line (Bj  X  orkelund et al., 2013).

As can be seen in Table 4, our results partly generalize the observation summarized by Cer et al. (2010) and Kong and Smith (2014) that phrase structure parsers tend to provide better dependen-cies than genuine dependency parsers for parsing to Stanford Dependencies. For English, our UAS is similar to that of TurboParser, but in a broader multilingual framework, the left side of the table shows that the unlabeled dependencies are clearly better than those of genuine dependency parsers. On the right side of the table are languages for which our dependencies are actually worse. This is not a surprise, since these are also the languages for which head annotation was more problematic in the first place. This last observation suggests that a lexicalized c-parser can also provide very accurate dependencies. A way to further gen-eralize this observation to problematic languages would be either to design a less immediate post-processing conversion scheme or to further nor-malize the data set to obtain the correct heads from the outset. Lexicalized phrase structure parsing of morpho-logically rich languages used to be difficult since existing implementations targeting essentially En-glish or Chinese do not allow a straightforward integration of morphology. Given multi-view treebanks, we achieve multilingual parsing with a language-agnostic head annotation procedure. Once this procedure has created the required data representation for lexicalized parsing, only mod-est and weakly language dependent feature engi-neering is required to achieve state-of-the-art ac-curacies on all languages considered: a minimal interface with morphology already contributes to improving accuracy, and this is specifically the case when heads are accurately identified. When heads are only approximatively identified, span-based configurational modelling tends to correct the approximation.

Leaving aside details concerning conversion and data normalization, we generally found that the unlabeled dependencies modelled by the lex-icalized c-parser also tend to be highly accu-rate. For languages where c-annotations and d-annotations are less compatible, additional lan-guage renormalizations would help to get better comparisons.
As suggested in this paper, future work for pars-ing morphologically rich languages will require to focus both on feature selection and on the interface between syntax and morphology, which means in our case the interface between the segmenter, the tagger and the parser.
 The author wishes to thank Djam  X  e Seddah for in-sigthful discussions regarding the work reported in this paper as well as R. Bawden, M. Coavoux and B. Sagot for their careful proofreading.

