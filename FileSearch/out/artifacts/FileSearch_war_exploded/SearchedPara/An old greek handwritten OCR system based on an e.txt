 ORIGINAL PAPER K. Ntzios  X  B. Gatos  X  I. Pratikakis  X  T. Konidaris  X  S. J. Perantonis Abstract Recognition of Old Greek Early Christian manuscripts is essential for efficient content exploita-tion of the valuable Old Greek Early Christian histor-ical collections. In this paper, we focus on the problem of recognizing Old Greek manuscripts and propose a novel recognition technique that has been tested in a large number of important historical manuscript collec-tions which are written in lowercase letters and originate from St. Catherine X  X  Mount Sinai Monastery. Based on an open and closed cavity character representation, we propose a novel, segmentation-free, fast and efficient technique for the detection and recognition of charac-ters and character ligatures. First, we detect open and closed cavities that exist in the skeletonized character body. Then, the classification of a specific character or character ligature is based on the protrusible segments that appear in the topological description of the charac-ter skeletons. Experimental results prove the efficiency of the proposed approach.
 Keywords Historical document recognition  X  Handwriting character recognition  X  Segmentation-free OCR 1 Introduction Recognition of old Greek manuscripts is essential for quick and efficient content exploitation of the valuable old Greek historical collections. In this paper, we focus on the recognition of Early Christian Greek manuscripts written in lower case letters (see Fig. 1a). Old Greek manuscripts can be found at writings from the Jewish Bible that became part of the Christian Old Testament, at copies of early extra-canonical writings such as the Gospel of Thomas or the Shepherd of Hermas, and at fragments of other, unknown writings, as well as litur-gical and theological texts. Any manuscript of Christian provenance can provide valuable historical information about early Christianity. Particularly, the Sinaitic Codex Number Three, which contains the Book of Job, is one of the best Greek manuscripts and one of the major masterpieces of world literature. Written in Hebrew ini-tially, the Book was translated into Greek approximately in the third century BC for the sake of the Hellenized Hebrews of Alexandria.

The work described in this paper has been devel-oped within the framework of the Greek Ministry of Research funded R&amp;D project, D-SCRIBE, which aims to develop an integrated system for digitization and pro-cessing of Old Greek manuscripts. It is expected that by the end of the project, 150,000 individual pages will be processed. D-SCRIBE strives toward the creation of a comprehensive software product, which can assist the content holders in turning an archive of manuscripts into a digital collection using automated methods. An immediate objective of the project is the digital preser-vation of a large number of important historical manu-scripts of the early Christian and Byzantine era from St. Catherine X  X  monastery, an outpost of the Hellenic world. Beyond this immediate goal, the product target includes an extensive number of organizations and com-panies related with the management of valuable manu-scripts like monasteries, institutions, libraries, private collections etc., in Greece and other countries. There-fore, the D-SCRIBE software is expected to play a key role in the digital preservation, processing and study of old Greek manuscripts, thus contributing to the preser-vation and advancement of cultural heritage.

In the field of handwriting recognition a great progress has occurred during the past years [35]. Many methods were developed for a variety of applications like automatic reading of postal addresses [2,19], fax forms [14] and bank checks [11,39], form processing, etc. In methodology, two general approaches can be identi-fied: the segmentation approach [7,16] and the global or segmentation-free approach [12,13,34]. The segmen-tation approach requires that each word has to be seg-mented into characters while the global approach entails the recognition of the whole word.

In the segmentation approach, the crucial step is to split a scanned bitmap image of a document into indi-vidual characters. Many segmentation algorithms have been proposed for handwritten words and digits. Lu and Shridhar gave an overview of the various techniques for the segmentation of handwritten characters [20]. Xiao and Leedman proposed a segmentation method based on certain knowledge of the handwriting [37], while Plamondon and Privitera introduced a segmenta-tion method that partly simulates the cognitive-behav-ioral process used by human beings in order to recover the temporal sequence of the strokes that composed the original pen movement [30]. Chi et al. proposed a con-tour curvature-based algorithm to segment single and double-touching handwritten digit strings [3]. Shuyan et al. proposed a two-stage approach to segment uncon-strained handwritten Chinese characters [32]. In their algorithm a character string is first coarsely segmented on the basis of the background skeleton, a vertical pro-jection and a set of geometric features. All possible segmentations paths are evaluated by using the fuzzy decision rules learned from examples discarding unsuit-able segmentation paths.

Global approaches avoid character segmentation, looking at words as entities using statistical methods to classify word samples [8]. Holistic strategies employ top-down approaches for recognizing the whole word, thus eliminating the segmentation problem [21,22,33]. In these strategies, global features extracted from the entirewordimageareusedfor therecognitionof limited-size lexicon. As the size of the lexicon becomes larger, the complexity of algorithms increases linearly due to the need for a larger search space and a more complex pattern representation. Although the global approaches are referred in the literature as  X  X egmentation-free X  approaches, they involve a word detection task.
Some approaches that do not involve any segmen-tation task are based on concepts and techniques that have been used in object recognition with occlusions [4,5]. According to these approaches, significant geo-metric features, such as short line segments, enclosed regions and corners, are extracted from a fully unseg-mented raw document bitmap by methods like template matching [1,6], peephole method [24], n -tuple feature [15,35] and hit-or-miss operator [12].
 In the case of historical documents, Manmatha and Croft [23] presented a method for word spotting wherein matching was based on the comparison of entire words rather than individual characters. In this method, an off-line grouping of words in a historical document and the manual characterization of each group by the ASCII equivalence of the corresponding words are required. The volume of the processed material was limited to a few pages. This process can become very tedious for large collections of documents. Futhermore in [10] is presented a novel segmentation-free approach for key-word search in historical typewritten documents com-bining image preprocessing, synthetic data creation, word spotting and user X  X  feedback technologies. It aims to search for keywords typed by the user in a large col-lection of digitized typewritten historical documents.
Traditional techniques for handwriting recognition cannot be applied to Old Greek manuscripts written in lower case letters, since continuity in writing of the same or consecutive words does not permit character or word segmentation. Furthermore, the discussed manuscripts entail several unique characteristics that are described in the following:  X  Consistent script writing. Although we refer to hand- X  Frequent appearance of character ligatures.  X  Frequent appearance of open and closed cavities in The continuity in writing for characters of the same or consecutive words as well as the unique characteristics of the lower case script in Early Greek Manuscripts guided us to develop a segmentation-free recognition technique as a fundamental assistance to Old Greek handwritten Manuscript OCR. Based on the existence of open and closed cavities in the majority of characters and character ligatures, we propose a technique for the detection and recognition of characters that contain open and closed cavities. The originality of the proposed method relies on two aspects. First, a set of discriminant features are used which are based on the protrusions that appear in the topological description of character skele-tons. Second, we strive toward the detection of open and closed cavities that sets the base for a robust classifier in combination with the aforementioned discriminant features.

In the proposed method, the document image is bina-rized, enhanced and skeletonized. Next, we detect the open and closed cavities of the skeletonized characters where we apply a feature extraction that sets the base for the recognition process. Finally, the individual cavities are recognized on the basis of their features. In Fig. 2, an overview of this handwritten recognition system is shown. 2 Preprocessing 2.1 Image binarization and enhancement Binarization is the starting step of most document image analysis systems and refers to the conversion of the gray-scale image to a binary image. Since historical document collections are most of the times of very low quality, an image enhancement stage is also essential. In the liter-ature, binarization is usually reported to be performed either globally or locally. The global methods (global thresholding) use a single threshold value to classify image pixels into objects or background classes [26], whereas the local schemes (adaptive thresholding) can use multiple values selected according to the local area information [17]. Most of the proposed algorithms for optimum image binarization rely on statistical methods, without taking into account the special nature of docu-ment images [25]. Global thresholding methods are not sufficient for document image binarization since docu-ment images usually have poor quality, shadows, no uni-form illumination, low contrast, large signal-dependent noise, smear and strains. Instead, techniques which are adaptive to local information have been developed for document binarization [31]. The proposed scheme for image binarization and enhancement is fully described in [9] and consists of five distinct steps: a preprocessing procedure using a low-pass Wiener filter, a rough esti-mation of foreground regions using Niblack X  X  approach [25], a background surface calculation by interpolating neighboring background intensities, a thresholding by combining the calculated background surface with the original image and finally a postprocessing step that improves the quality of text regions and preserve stroke connectivity. An example of the image binarization and enhancement result is demonstrated in Fig. 3. 2.2 Skeletonization For the skeletonization process, we use an iterative method presented in [18]. This method is simply an extension of the method of Zhang and Suen [40]. The skeleton obtained is not truly 8-connected, since some non-junction pixels have more than two neighbors, mak-ing the skeleton useless for algorithms that require this constraint. Therefore, some pixels have to be removed. The skeleton is inspected, and each pixel is tested using a lookup table. The result is a true 8-connected skeleton where only junction pixels have more than two 8-neigh-bors (see Fig. 4). 3 Open and closed cavities detection In this step, open and closed cavities are detected in the skeletonized image. For the closed cavity, several detection algorithms exist that are mainly based on con-tour following techniques that distinguish the external from internal contours [29,38]. We suggest a novel fast algorithm for closed cavity detection based on process-ing the white runs of the b/w image. In the following, a step-by-step description of the proposed algorithm is given. step 1 All horizontal and vertical image white runs that step 2 All horizontal and vertical white runs of unflag-step 3 Repeat Step 2 until no pixel remains to be step 4 All remaining white runs of unflagged pixels Some of the detected characters are ignored and are not considered for future processing. We consider only the cavities having width greater than a threshold T which is chosen to be the one-third of the mean width of all cavities. Additionally, an open cavity is ignored when it shares a common boundary with a closed cavity and the following condition holds: mean ( y u i )&lt; where mean ( y u i ) is the mean value of all y -coordinates of the pixels that compose the open cavity and mean ( y o j ) the mean value of all y -coordinates of the pixels that compose the neighbor closed cavity. In Fig. 6b an ignored open cavity is shown as a shaded area. 4 Feature estimation 4.1 Character detection Feature extraction is applied to characters that con-tain one or more open or closed cavities. The proposed method creates a bounding box W with the following top-left ( x TL , y TL ) and bottom right corner coordinates ( x terized as open or closed cavity. Let x i  X  X ,where X denotes the set of pixel coordinates of the cavity in the x direction and y i  X  Y ,where Y denotes the set of pixel coordinates of the cavity in the y direction. The bound-ing box is computed as follows: ( x ( x where, min(.), max(.), mean(.) denote the minimum value, the maximum value and the average value, of the set X or Y , respectively. Figure 6a, b shows the skeletonized components with the corresponding bounding box W around each open and closed cavity. 4.2 Feature extraction The feature extraction stage identifies all segments that belong to a protrusion of an isolated character X  X  cavity. It is applied in two consecutive modes: a vertical and a horizontal mode. The vertical mode is used to describe the protrusible segments that exist either at the top or at the bottom of the character X  X  cavity while the horizon-tal mode is used to describe the protrusible segments that exist either at the right or at the left side of the character. The feature set is composed of 15 features F ={ f the length of protrusible segments that appear on the protrusible segments that appear on the bottom of the ments that appear on the left and the right side of the character and f 11 denotes the upper slope of the pro-trusible segments. The remaining features ( f 12  X  f 15 ) used only for the characters with open cavities. Feature f 12 denotes the lower slope of the protrusible segment, features f 13 and f 14 denote the length of segments that appear in the block R 13 and R 14 (see Fig. 7a, b), while f 15 denotes the opening angle of the open cavity. This angle is constructed as in the following: we first deter-mine points A, B which denote the intersection points at the cavity and the horizontal line at a height n 1 D from the lower part of the cavity where D denotes the total height of the cavity, while n is chosen equal to 2. Then, we determine point P , which is the projection of the middle point in line AB to the lower part of the cavity. Finally the opening angle is the A  X  PB angle (see Fig. 8).
Feature estimation is employed in the following two steps.  X  step 1: Bounding box division into blocks  X  step 2: Block-based feature computation Let, H R i = x be the set of pixel coordinates depicted in block R i and meanwhile they do not comprise pixel of the cavity. For each pixel j of H Ri we determine its local orientation s takingnominal values from theset { W, SW, S, SE, E, NE, N, NW } in terms of the previous pixel during the tracing. Once the directions are evaluated the proposed feature f for closed cavities and f j for open cavities are defined as follows: f = 1 f = where g i (  X  ) is a function depending on the orientation of the pixel and the block considered and m i is the total number of pixels of the skeleton in block R i . The term D denotes the mean of the character X  X  cavity height and it is used as a normalization factor allowing the feature to be invariant with respect to character scaling. The g i (  X  ) explicitly defined in Table 1, denoting the contribution of certain orientation to the corresponding protrusible segment at the region R i . 4.3 Protrusible artifacts For the feature estimation of the characters an upper or lower protrusible segment cannot be considered as a protrusion of more than one character, although a protrusible segment can be found for more than one character X  X  bounding box. Therefore, a methodology is required to assign a protrusible segment to only one character. To accomplish this, we consider a methodol-ogy strictly following the next steps: step 1: During the closed cavity feature estimation step 2: The bounding boxes Wi of the corresponding 4.4 Cavity merging In this stage two or more cavities are merged when they share a common boundary. The merged closed char-acters can be characterized as (i) a character with two, three or four horizontal closed cavities, (ii) as a character with two vertical closed cavities and (iii) as a character with horizontal and vertical closed cavities (see Table 1). Therefore, when two closed cavities i and j , have com-mon pixels, the merged character is characterized as horizontal if Eq. 5 is true, otherwise it is characterized as vertical. min max ( y i )  X  max ( y j ) , min ( y i )  X  min ( y j ) &lt; min max ( y where max ( y i ) and min ( y i ) is the maximum and min-imum y -coordinate of i closed cavity and max ( y j ) and min ( y j ) is the maximum and minimum y -coordinate of j closed cavity.

Moreover, in this stage two or more open cavities that have a common boundary and they do not have upper and lower protrusible segments are merged, and the resulting cavity is characterized as a cavity with two or three open cavities (see Table 2). After merging the features of the resulting cavity are estimated exactly as being single. 5 Character recognition The character recognition process consists of two ba-sic stages. In the first stage each character is classified into a pattern by their spatial configuration as shown in Tables 2 and 3. For example, the characters that have one closed cavity are classified to the pattern with ID 1 and the characters with one open cavity are classi-fied to the pattern with ID 7. In the second stage for each pattern except the patterns with ID 4 X 6,8 that cor-respond to a unique character, there is a classification binary decision tree. Decision is taken at each node after the examination of specific feature valuation. All con-ditions, upon which a tree traversal is progressing, can be shown in Figs. 11,12,13,14 and 15. The corresponding threshold values T i , that support the required condition-ing is computed in the following: T where C is the set of the training set cavities (2,497 char-acters).
 6 Experimental results The purpose of the experiments was to test the classifi-cation performance of the proposed handwritten manu-scripts recognition procedure. The overall experimental samples originate from different manuscripts for train-ing and testing of the Book of Job collection, manually labeled with the ground truth. We have built a dictio-nary of open and closed cavity patterns that contains a total of 12,332 characters and character ligatures where 2,497 characters are used for the training set and 9,835 for the testing set. The annotation was done manually in the character or character ligature level.
For the training and testing set detailed distribution of the underlying patterns along with their spatial con-figuration is shown in Tables 2 and 3. Table 4 shows the results obtained by applying the algorithm, indicating the recall and the precision rates for each one of the characters. Recall ( R ) is the correct number of open and closed cavities classified divided by the total number open and closed cavities. Precision ( P ) is the number of correct open and closed cavities classified divided by the total number of open and closed cavities clas-sified. We further compute an overall Figure of Merit (FOM) which takes into account the average precision and recall, denoted as F = 2 P Our system recognizes basic characters with an average recall of 89, 49% and overall FOM of 89, 27%. Some errors come from the bad quality of the document which cannot be overcome from the preprocessing step. In bad quality documents broken characters are recognized as open cavities whereas they belong to the closed cavity patterns. Table 5 shows the confusion matrix for the rec-ognized characters. It can be noticed that we get certain cases of misclassifications. In particular it can be noticed that characters  X   X   X  and  X   X   X  are mutually misclassified 69 and 26 times, respectively. Character  X   X   X  is misclassi-fied as  X  X  X  11 times, while character  X   X   X  misclassified as  X   X   X 32times.

In Tables 6 and 7 we can see for each closed (Table 6) and open (Table 7) cavity the mean value of their fea-tures and in the brackets the standard deviation of them. The features that are significant for the character clas-sification according to the respective classification tree are marked in bold.

In Table 8, we can see some instances of  X   X   X  and  X  X  X  characters, along with their corresponding feature set. In this table the features that are marked as black are the nonsignificant features whose values do not play any discriminant role to the character classification. The green marked features are the features whose values are very small and the red marked features are the features that describe a protrusible segment. 7 Conclusions In this paper, we present a novel methodology for rec-ognition of Early Christian Greek manuscripts written in lower case letters. Using a robust character represen-tation based on open and closed cavities, we propose a segmentation-free, quick and efficient recognition tech-nique for the detection and recognition of characters and character ligatures. Experimental results show that the proposed method gives highly accurate results and offers a great assistance to Old Greek handwritten interpreta-tion. We strongly believe that this system in combination with an efficient postprocessing lexicon technique on Early Christian Greek manuscripts will further increase the accuracy of the results.
 References
