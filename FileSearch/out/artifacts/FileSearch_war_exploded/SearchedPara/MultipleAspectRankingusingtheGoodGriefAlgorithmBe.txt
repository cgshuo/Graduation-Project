 Pre vious work on sentiment cate gorization mak es an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turne y, 2002; Yu and Hatzi vassiloglou, 2003). Ho we ver, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant revie w may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would lik e to capture each aspect of the writer' s opinion separately , thereby pro viding a more ne-grained vie w of opinions in the revie w.
To this end, we aim to predict a set of numeric ranks that reects the user' s satisf action for each as-pect. In the example abo ve, we would assign a nu-meric rank from 1-5 for each of: food quality , ser -vice, and ambience.

A straightforw ard approach to this task would be to rank 1 the text independently for each aspect, us-ing standard ranking techniques such as regression or classication. Ho we ver, this approach fails to ex-ploit meaningful dependencies between users' judg-ments across dif ferent aspects. Kno wledge of these dependencies can be crucial in predicting accurate ranks, as a user' s opinions on one aspect can inu-ence his or her opinions on others.

The algorithm presented in this paper models the dependencies between dif ferent labels via the agreement relation . The agreement relation captures whether the user equally lik es all aspects of the item or whether he or she expresses dif ferent degrees of satisf action. Since this relation can often be deter -mined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to impro ve rank prediction.

The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are equal. The Good Grief decoding algorithm pre-dicts a set of ranks  X  one for each aspect  X  which maximally satisfy the preferences of the indi vidual rank ers and the agreement model. For example, if the agreement model predicts consensus but the in-dividual rank ers select ranks h 5 , 5 , 4 i , then the de-coder decides whether to trust the the third rank er, or alter its prediction and output h 5 , 5 , 5 i to be con-sistent with the agreement prediction. To obtain a model well-suited for this decoding, we also develop a joint training method that conjoins the training of multiple aspect models.

We demonstrate that the agreement-based joint model is more expressi ve than indi vidual ranking models. That is, every training set that can be per -fectly rank ed by indi vidual ranking models for each aspect can also be perfectly rank ed with our joint model. In addition, we give a simple example of a training set which cannot be perfectly rank ed with-out agreement-based joint inference. Our experi-mental results further conrm the strength of the Good Grief model. Our model signicantly outper -forms indi vidual ranking models as well as a state-of-the-art joint ranking model. Sentiment Classication Traditionally , cate goriza-tion of opinion texts has been cast as a binary classi-cation task (Pang et al., 2002; Turne y, 2002; Yu and Hatzi vassiloglou, 2003; Da ve et al., 2003). More recent work (Pang and Lee, 2005; Goldber g and Zhu, 2006) has expanded this analysis to the rank-ing frame work where the goal is to assess revie w polarity on a multi-point scale. While this approach pro vides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the prob-lem of analyzing multiple opinions  X  or multiple as-pects of an opinion. Since multiple opinions in a sin-gle text are related, it is insuf cient to treat them as separate single-aspect ranking tasks. This moti vates our exploration of a new method for joint multiple aspect ranking.

Ranking The ranking, or ordinal regression, problem has been extensi vly studied in the Machine Learning and Information Retrie val communities. In this section we focus on two online ranking methods which form the basis of our approach. The rst is a model proposed by Crammer and Singer (2001). The task is to predict a rank y 2 f 1 ,...,k g for ev-ery input x 2 R n . Their model stores a weight vector w 2 R n and a vector of increasing bound-aries b 0 = 1 b 1 ... b k  X  1 b k = 1 which divide the real line into k segments, one for each possible rank. The model rst scores each input with the weight vector: score ( x ) = w x . Finally , the model locates score ( x ) on the real line and re-turns the appropriate rank as indicated by the bound-aries. Formally , the model returns the rank r such that b r  X  1 score ( x ) &lt; b r . The model is trained with the Perceptron Ranking algorithm (or  X PRank algorithm X ), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors. The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database.
An extension of this model is pro vided by Basil-ico and Hofmann (2004) in the conte xt of collabora-tive ltering. Instead of training a separate model for each user , Basilico and Hofmann train a joint rank-ing model which shares a set of boundaries across all users. In addition to these shared boundaries, user -specic weight vectors are stored. To compute the score for input x and user i , the weight vectors for all users are emplo yed: score i ( x ) = w [ i ] x + where 0 sim ( i,j ) 1 is the cosine similarity be-tween users i and j , computed on the entire training set. Once the score has been computed, the predic-tion rule follo ws that of the PRanking model. The model is trained using the PRank algorithm, with the exception of the new denition for the scoring func-tion. 2 While this model shares information between the dif ferent ranking problems, it fails to explicitly model relations between the rank predictions. In contrast, our algorithm uses an agreement model to learn such relations and inform joint predictions. The goal of our algorithm is to nd a rank assign-ment that is consistent with predictions of indi vid-ual rank ers and the agreement model. To this end, we develop the Good Grief decoding procedure that minimizes the dissatisf action ( grief ) of indi vidual components with a joint prediction. In this section, we formally dene the grief of each component, and a mechanism for its minimization. We then describe our method for joint training of indi vidual rank ers that tak es into account the Good Grief decoding pro-cedure. 3.1 Pr oblem Formulation In an m-aspect ranking problem , we are given a training sequence of instance-label pairs ( x 1 , y 1 ) ,..., ( x t , y t ) ,... . Each instance x t is a feature vector in R n and the label y t is a vector of m ranks in Y m , where Y = f 1 ,..,k g is the set of possible ranks. The i th component of y t is the rank for the i th aspect, and will be denoted by y [ i ] t . The goal is to learn a mapping from instances to rank sets, H : X ! Y m , which minimizes the distance between predicted ranks and true ranks. 3.2 The Model Our m-aspect ranking model contains m + 1 compo-nents: ( h w [1] , b [1] i ,..., h w [ m ] , b [ m ] i , a ) m components are indi vidual ranking models, one for each aspect, and the nal component is the agree-ment model. For each aspect i 2 1 ...m , w [ i ] 2 R n is a vector of weights on the input features, and b [ i ] 2 R k  X  1 is a vector of boundaries which divide the real line into k interv als, corresponding to the k possible ranks. The def ault prediction of the as-pect ranking model simply uses the ranking rule of the PRank algorithm. This rule predicts the rank r score i ( x ) can be dened simply as the dot product w [ i ] x , or it can tak e into account the weight vectors for other aspects weighted by a measure of inter -aspect similarity . We adopt the denition given in equation 1, replacing the user -specic weight vec-tors with our aspect-specic weight vectors. The agreement model is a vector of weights a 2 R n . A value of a x &gt; 0 predicts that the ranks of all m aspects are equal, and a value of a x 0 indicates disagreement. The absolute value j a x j indicates the condence in the agreement prediction.
The goal of the decoding procedure is to predict a joint rank for the m aspects which satises the in-dividual ranking models as well as the agreement model. For a given input x , the indi vidual model for aspect i predicts a def ault rank  X  y [ i ] based on its feature weight and boundary vectors h w [ i ] , b [ i ] i addition, the agreement model mak es a prediction regarding rank consensus based on a x . Ho we ver, the def ault aspect predictions  X  y [1] ...  X  y [ m ] may not accord with the agreement model. For example, if a x &gt; 0 , but  X  y [ i ] 6 =  X  y [ j ] for some i,j 2 1 ...m the agreement model predicts complete consensus, whereas the indi vidual aspect models do not.
We therefore adopt a joint prediction criterion which simultaneously tak es into account all model components  X  indi vidual aspect models as well as the agreement model. For each possible predic-tion r = ( r [1] ,...,r [ m ]) this criterion assesses the level of grief associated with the i th -aspect ranking model, g i ( x ,r [ i ]) . Similarly , we compute the grief of the agreement model with the joint prediction, g ( x , r ) (both g i and g a are dened formally belo w). The decoder then predicts the m ranks which mini-mize the overall grief:
H ( x ) = arg min If the def ault rank predictions for the aspect models,  X  y = ( X  y [1] ,...,  X  y [ m ]) , are in accord with the agree-ment model (both indicating consensus or both in-dicating contrast), then the grief of all model com-ponents will be zero, and we simply output  X  y . On the other hand, if  X  y indicates disagreement but the agreement model predicts consensus, then we have the option of predicting  X  y and bearing the grief of the agreement model. Alternati vely , we can predict some consensus y 0 (i.e. with y 0 [ i ] = y 0 [ j ] , 8 i,j bear the grief of the component ranking models. The decoder H chooses the option with lowest overall grief. 4
No w we formally dene the measures of grief used in this criterion.
 Aspect Model Grief We dene the grief of the i th -aspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the input' s score into the r th segment of the real line: Agr eement Model Grief Similarly , we dene the grief of the agreement model with respect to a joint rank r = ( r [1] ,...,r [ m ]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the indi vidual ranks r [1] ,...,r [ m ] : 3.3 Training Ranking models Pseudo-code for Good Grief train-ing is sho wn in Figure 1. This training algorithm is based on PRanking (Crammer and Singer , 2001), an online perceptron algorithm. The training is per -formed by iterati vely ranking each training input x and updating the model. If the predicted rank  X  y is equal to the true rank y , the weight and boundaries vectors remain unchanged. On the other hand, if  X  y 6 = y , then the weights and boundaries are updated to impro ve the prediction for x (step 4.c in Figure 1). See (Crammer and Singer , 2001) for explanation and analysis of this update rule.

Our algorithm departs from PRanking by con-joining the updates for the m ranking models. We achie ve this by using Good Grief decoding at each step throughout training. Our decoder H ( x ) (from equation 2) uses all the aspect component models as well as the (pre viously trained) agreement model to determine the predicted rank for each aspect. In concrete terms, for every training instance x , we pre-dict the ranks of all aspects simultaneously (step 2 in Figure 1). Then, for each aspect we mak e a separate update based on this joint prediction (step 4 in Fig-ure 1), instead of using the indi vidual models' pre-dictions.

Agr eement model The agreement model a is as-sumed to have been pre viously trained on the same training data. An instance is labeled with a positi ve label if all the ranks associated with this instance are equal. The rest of the instances are labeled as nega-tive. This model can use any standard training algo-rithm for binary classication such as Perceptron or SVM optimization. 3.4 Featur e Repr esentation Ranking Models Follo wing pre vious work on senti-ment classication (Pang et al., 2002), we represent each revie w as a vector of lexical features. More specically , we extract all unigrams and bigrams, discarding those that appear fewer than three times. This process yields about 30,000 features.

Agr eement Model The agreement model also op-erates over lexicalized features. The effecti veness of these features for recognition of discourse rela-tions has been pre viously sho wn by Marcu and Echi-habi (2002). In addition to unigrams and bigrams, we also introduce a feature that measures the maxi-mum contrasti ve distance between pairs of words in a revie w. For example, the presence of  X delicious X  and  X dirty X  indicate high contrast, whereas the pair  X expensive X  and  X slow X  indicate low contrast. The contrasti ve distance for a pair of words is computed by considering the dif ference in relati ve weight as-signed to the words in indi vidually trained PRanking models. In this section, we pro ve that our model is able to perfectly rank a strict superset of the training cor -pora perfectly rankable by m ranking models indi-vidually . We rst sho w that if the independent rank-ing models can indi vidually rank a training set per -fectly , then our model can do so as well. Ne xt, we sho w that our model is more expressi ve by pro viding Input : ( x 1 , y 1 ) ,..., ( x T , y T ), Agreement model Initialize : Set w [ i ] 1 = 0 , b [ i ] 1 = 1 , 8 i 2 1 ...m .
 Loop : For t = 1 , 2 ,...,T : 1. Get a new instance x t 2 R n . 2. Predict  X  y t = H ( x ; w t , b t , a ) (Equation 2). 3. Get a new label y t . 4. For aspect i = 1 ,...,m : w [ i ] then y [ i ] t . r ) y [ i ] t r 0 then  X  [ i ] t r = y [ i ] t r . Output : H ( x ; w T +1 , b T +1 , a ) .
 dif fers in the joint computation of all aspect predictions  X  y t based on the Good Grief Criterion (step 2) and a simple illustrati ve example of a training set which can only be perfectly rank ed with the inclusion of an agreement model.

First we introduce some notation. For each train-ing instance ( x t , y t ) , each aspect i 2 1 ...m , and each rank r 2 1 ...k , dene an auxiliary variable y [ i ] t r with y [ i ] t r = 1 if y [ i ] t r and y [ i ] if y [ i ] t &gt; r . In words, y [ i ] t r indicates whether the true rank y [ i ] t is to the right or left of a potential rank r .

No w suppose that a training set ( x 1 , y 1 ) ,..., ( x T , y T ) is perfectly rankable for each aspect independently . That is, for each aspect i 2 1 ...m , there exists some ideal model v [ i ]  X  = ( w [ i ]  X  ,b [ i ]  X  ) such that the signed dis-tance from the prediction to the r th boundary: w [ i ]  X  x t b [ i ]  X  r has the same sign as the auxil-iary variable y [ i ] t mar gin over all training instances and ranks,  X  = min r,t f ( w [ i ]  X  x t b [ i ]  X  r ) y [ i ] t r g zero.

No w for the t th training instance, dene an agree-ment auxiliary variable a t , where a t = 1 when all aspects agree in rank and a t = 1 when at least two aspects disagree in rank. First consider the case where the agreement model a perfectly classies all training instances: ( a x t ) a t &gt; 0 , 8 t . It is clear that Good Grief decoding with the ideal joint model ( h w [1]  X  , b [1]  X  i ,..., h w [ m ]  X  , b [ m ]  X  i , a ) the same output as the component ranking models run separately (since the grief will always be zero for the def ault rank predictions). No w consider the case where the training data is not linearly separable with regard to agreement classication. Dene the mar -gin of the worst case error to be  X  = max t fj ( a x t ) j : ( a x t ) a t &lt; 0 g . If  X  &lt;  X  , then again Good Grief de-coding will always produce the def ault results (since the grief of the agreement model will be at most  X  in cases of error , whereas the grief of the ranking mod-els for any deviation from their def ault predictions will be at least  X  ). On the other hand, if  X   X  , then the agreement model errors could potentially disrupt the perfect ranking. Ho we ver, we need only rescale w the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error . Thus whene ver independent rank-ing models can perfectly rank a training set, a joint ranking model with Good Grief decoding can do so as well.

No w we give a simple example of a training set which can only be perfectly rank ed with the addi-tion of an agreement model. Consider a training set of four instances with two rank aspects: h x 1 , y 1 i = h (1 , 0 , 1) , (2 , 1) i h x 2 , y 2 i = h (1 , 0 , 0) , (2 , 2) i h x 3 , y 3 i = h (0 , 1 , 1) , (1 , 2) i h x 4 , y 4 i = h (0 , 1 , 0) , (1 , 1) i We can interpret these inputs as feature vectors cor -responding to the presence of  X good X ,  X bad X , and  X but not X  in the follo wing four sentences: The food was good , but not the ambience.
 The food was good , and so was the ambience. The food was bad , but not the ambience.
 The food was bad , and so was the ambience.
 We can further interpret the rst rank aspect as the quality of food, and the second as the quality of the ambience, both on a scale of 1-2.

A simple ranking model which only considers the words  X good X  and  X bad X  perfectly ranks the food as-pect. Ho we ver, it is easy to see that no single model perfectly ranks the ambience aspect. Consider any model h w , b = ( b ) i . Note that w x 1 &lt; b and w x 2 b together imply that w 3 &lt; 0 , whereas w x 3 b and w x 4 &lt; b together imply that w 3 &gt; 0 . Thus independent ranking models cannot perfectly rank this corpus.

The addition of an agreement model, howe ver, can easily yield a perfect ranking. With a = (0 , 0 , 5) (which predicts contrast with the presence of the words  X but not X ) and a ranking model for the ambience aspect such as w = (1 , 1 , 0) , b = (0) , the Good Grief decoder will produce a perfect rank. We evaluate our multi-aspect ranking algorithm on a corpus 5 of restaurant revie ws available on the web-site http://www.we8there.com . Re vie ws from this website have been pre viously used in other sentiment analysis tasks (Hig ashinaka et al., 2006). Each revie w is accompanied by a set of ve ranks, each on a scale of 1-5, covering food, ambience, ser -vice, value, and overall experience. These ranks are pro vided by consumers who wrote original revie ws. Our corpus does not contain incomplete data points since all the revie ws available on this website con-tain both a revie w text and the values for all the ve aspects.

Training and Testing Division Our corpus con-tains 4,488 revie ws, averaging 115 words. We ran-domly select 3,488 revie ws for training, 500 for de-velopment and 500 for testing.

Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models. Also, given an initial uncalibrated agreement model a 0 , we dene our agreement model to be a =  X  a 0 for an appropriate scaling factor  X  . We tune the value of  X  on the development set.

Cor pus Statistics Our training corpus contains 528 among 5 5 = 3025 possible rank sets. The most frequent rank set h 5 , 5 , 5 , 5 , 5 i accounts for 30.5% of the training set. Ho we ver, no other rank set com-prises more than 5% of the data. To cover 90% of occurrences in the training set, 227 rank sets are re-quired. Therefore, treating a rank tuple as a single label is not a viable option for this task. We also nd that revie ws with full agreement across rank as-pects are quite common in our corpus, accounting for 38% of the training data. Thus an agreement-based approach is natural and rele vant.

A rank of 5 is the most common rank for all as-pects and thus a prediction of all 5's gives a MAJOR -ITY baseline and a natural indication of task dif -culty .

Ev aluation Measur es We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer , 2001; Basilico and Hofmann, 2004). Rank-ing loss measures the average distance between the true rank and the predicted rank. Formally , given N test instances ( x 1 , y 1 ) ,..., ( x N , y N ) of an m -aspect ranking problem and the corresponding predictions  X  y 1 ,...,  X  y N , ranking loss is dened as P respond to a better performance of the algorithm. Comparison with Baselines Table 1 sho ws the per -formance of the Good Grief training algorithm GG TRAIN + DECODE along with various baselines, in-cluding the simple MAJORITY baseline mentioned in section 5. The rst competiti ve baseline, PRANK , learns a separate rank er for each aspect using the PRank algorithm. The second competiti ve baseline, SIM , shares the weight vectors across aspects using a similarity measure (Basilico and Hofmann, 2004). Figure 2: Rank loss for our algorithm and baselines as a function of training round.
 Both of these methods are described in detail in Sec-tion 2. In addition, we consider two variants of our algorithm: GG DECODE emplo ys the PRank train-ing algorithm to independently train all component ranking models and only applies Good Grief decod-ing at test time. GG O RACLE uses Good Grief train-ing and decoding but in both cases is given perfect kno wledge of whether or not the true ranks all agree (instead of using the trained agreement model).
Our model achie ves a rank error of 0.632, com-pared to 0.675 for PRANK and 0.663 for SIM . Both of these dif ferences are statistically signicant at p &lt; 0 . 002 by a Fisher Sign Test. The gain in perfor -mance is observ ed across all ve aspects. Our model also yields signicant impro vement ( p &lt; 0 . 05 ) over the decoding-only variant GG DECODE , conrm-ing the importance of joint training. As sho wn in Figure 2, our model demonstrates consistent im-pro vement over the baselines across all the training rounds.
 Model Analysis We separately analyze our per -PRANK 0.414 0.864 GG ORACLE 0.281 0.830 Table 2: Ranking loss for our model and PRANK computed separately on cases of actual consensus and actual disagreement. formance on the 210 test instances where all the tar get ranks agree and the remaining 290 instances where there is some contrast. As Table 2 sho ws, we outperform the PRANK baseline in both cases. Ho w-ever on the consensus instances we achie ve a relati ve reduction in error of 21.8% compared to only a 1.1% reduction for the other set. In cases of consensus, the agreement model can guide the ranking models by reducing the decision space to ve rank sets. In cases of disagreement, howe ver, our model does not pro vide suf cient constraints as the vast majority of ranking sets remain viable. This explains the perfor -mance of GG ORACLE , the variant of our algorithm with perfect kno wledge of agreement/disagreement facts. As sho wn in Table 1, GG ORACLE yields sub-stantial impro vement over our algorithm, but most of this gain comes from consensus instances (see Ta-ble 2).

We also examine the impact of the agreement model accurac y on our algorithm. The agreement model, when considered on its own, achie ves clas-sication accurac y of 67% on the test set, compared to a majority baseline of 58%. Ho we ver, those in-stances with high condence j a x j exhibit substan-tially higher classication accurac y. Figure 3 sho ws the performance of the agreement model as a func-tion of the condence value. The 10% of the data with highest condence values can be classied by Figure 3: Accurac y of the agreement model on sub-sets of test instances with highest condence j a x j . the agreement model with 90% accurac y, and the third of the data with highest condence can be clas-sied at 80% accurac y.

This property explains wh y the agreement model helps in joint ranking even though its overall accu-rac y may seem low. Under the Good Grief criterion, the agreement model' s prediction will only be en-forced when its grief outweighs that of the ranking models. Thus in cases where the prediction con-dence ( j a x j ) is relati vely low, 6 the agreement model will essentially be ignored. We considered the problem of analyzing multiple re-lated aspects of user revie ws. The algorithm pre-sented jointly learns ranking models for indi vidual aspects by modeling the dependencies between as-signed ranks. The strength of our algorithm lies in its ability to guide the prediction of indi vidual rank ers using rhetorical relations between aspects such as agreement and contrast. Our method yields signicant empirical impro vements over indi vidual rank ers as well as a state-of-the-art joint ranking model.

Our current model emplo ys a single rhetorical re-lation  X  agreement vs. contrast  X  to model depen-dencies between dif ferent opinions. As our analy-sis sho ws, this relation does not pro vide suf cient constraints for non-consensus instances. An avenue for future research is to consider the impact of addi-tional rhetorical relations between aspects. We also plan to theoretically analyze the con vergence prop-erties of this and other joint perceptron algorithms.
