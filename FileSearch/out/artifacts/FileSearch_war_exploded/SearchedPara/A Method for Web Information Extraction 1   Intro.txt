 During the past decade, information extr action has been extensively studied with many research results as well as systems developed. Since the late 1980 X  X , through the message understanding conference (MUC) sponsored by defense advances research project agency, many information extraction systems have been successfully developed and quantitatively evaluated [1]. 
The information source can be classified into three main types, including free text, structured text and semi-structured text. Originally, the extraction system focuses on free text extraction. Natural Language Processing (NLP) techniques are developed to extract this type of unrestricted, unregulated information, which employs the syntactic and semantic characteristics of the language to generate the extraction rules. The structured information usually comes from databases, which provide rigid or well defined formats of information, therefore, it is easy to extract through some query language such as Structured Query Language (SQL). The other type is the semi-structured information, which falls between free text and structured information. Web pages are a typical example of semi-structured information. In this paper, we will focus on extracting text information from web pages. 
According to the statistical results by Miniwatts Marking Group [URL1], the growth of web users during this decade is over 200% and there ar e more than 1 billion Internet users from over 233 countries and world regions. At the same time, public information and virtual places are increasin g accordingly, which almost covers any kind of information needs. Thus this attracts much attention on how to extract the useful information from the Web. 
Currently, the targeted web documents can easily be obtained by inputting some keywords with a web search engine. But the drawback is that the system may not necessarily provide relevant data rich pages and it is not easy for the computer to automatically extract or fully understand the information contained. The reason is due to the fact that web pages are designed for human browsing, rather than machine interpretation. Most of the pages are in Hypertext Markup Language (HTML) format, which is a semi-structured language, and the data are not given in a particular format and change frequently [1]. 
There are several challenges in extracting information from a semi-structured web page such as the lack of a schema, ill formatting, high update frequency and semantic heterogeneity of the information. In order to overcome these challenges, our system design transforms the page into a format called Extensible Hypertext Mark-up Language (XHTML) [URL2]. Then, we make use of the DOM tree hierarchy of a web page and regular expressions are extracted out using the Extensible Style sheet Language (XSL) [URL3, URL4] technique, with a human training process. The relevant information is extracted and transformed into another structured format X  Extensible Mark-up Language (XML) [URL5]. 
The remainder of the paper is organized as follows: some related works are illustrated in section 2, which involve a brief overview of the current web information extraction systems; then detail techniques in our approach are addressed in section 3; experimental results are explained in section 4; finally, the conclusion and future work are mentioned in the last section. From time to time, many extraction systems have been developed. In the very beginning, a wrapper is constructed to manually extract a particular format of information. However, the wrapper is not adaptive to change, it should be reconstructed accordingly to different types of information. In addition, it is complicated and knowledge intensive to construct the extraction rules used in a wrapper for a specific domain. Therefore only experts may have knowledge to do that. No doubt, the inflexibility and the development cost for construction are the main disadvantages of using wrappers. 
Due to the extensive work in manually constructing a wrapper, many wrapper generation techniques have been developed. Those techniques could be classified into several classes, including language development based, HTML tree processing based, natural language processing based, wra pper induction based, modeling based and ontology based [2]. 
In order to assist the user to accomplish the extraction task, a new language was developed for a language development based system. The famous systems for this type include TSIMMIS [3] and Web-OQL [4]. One of the drawbacks of such a model is that not all users are familiar with the new query language, so the performance of the system may not be as expected. Then, as most of the web pages are in HTML format, another type of extraction system, HTML tree processing based system, was proposed. By parsing the tree structure of a web page, a system is able to locate useful pieces of information. XWRAP [5] and Road Runner [6] are examples in this respect. In this solution, web pages need to be transformed into XHMTL or XML format due to limitations of the HTML format. For some pages which are mainly composed of grammatical text or paragraphs, Natural Language Processing (NLP) systems can be used. NLP is popularly used to extract free text information, and makes use of filtering, part-of-speech tagging and lexical semantic tagging technology to build up the extraction rules. SRV [7], WHISH [8] and KnowItAll [9] are examples of this technique. However, for some pages which are composed of the tabular or list format, NLP based tools may not be effective since the internal structure of the page can not be fully exploited. 
The wrapper induction based systems can induce the contextual rules for delimiting the information based on a set of training samples. SoftMealy [10] and STALKER [11] are typical examples. In modeling based systems, according to a set of modeling primitives, for example tables or lists, the data are conformed to the pre-given structure. Then the system tries to locate the information against given structures. NoDoSe [12] is an example of this type of systems. The last type is ontology based systems. Ontology techniques can be used to decompose a domain rely on the structures of web pages or the grammars of texts but instead an object is constructed for a specific type of data. WebDax [13] is a typical example in this respect. 
Besides classifying by the main techniques used, the wrapper can also be grouped into semi-automatic wrapper or fully-automatic wrapper. For the semi-automatic wrapper, human involvements are necessary. Most of the systems belong to this type, such as TSIMMIS [3] and XWRAP [5]. For the fully-automatic wrapper, no human intervention is needed, examples include Omini [14] and STAVIES [15], which make use of tree structures, or the visual structures of pages to perform the extraction task. 
Our proposed system belongs to the type of semi-automatic wrappers. Through the training process, our system learns rules for extraction. We suppose that with training, the system can be more adaptive to different type of pages if the training samples are broad enough. In addition, for different types of information, we make use of different techniques for extraction. For most existing systems, usually, only one main methodology is applied for extraction. The benefit of a multi extraction methodology is that the extraction can produce higher performance. The system works in two phases, pre-processing phase and the extraction phase as shown in figure 1. In the Pre-processing phase, in order to overcome the ill representations of HTML documents, all the pages are transformed into XHTML format. Then, the training process is performed, which gathers patterns or rules for the extraction phase. In the extraction phase, based on the human training results, the system chooses suitable extraction methods for different information fields. 3.1 Patterns and Rules of the Information All of the web pages are transformed from HTML into a W3C [3] recommended XHTML format in the preprocessing phase. Though the current web browsers can present correctly the ill formulated HTML documents, it is difficult to identify the hierarchical structures of web pages. For example, the tag &lt;P&gt; or &lt;BR&gt; may be used alone for HTML elements, without the corresponding closing tags associated. However, such usage is not allowed in XHTML format. Therefore the closing tag should be added accordi ngly during the preprocessing phase. 
An open source library called Tidy [URL6], provided by W3C organization, is HTML. After pages are transformed into XHTML format, the training process is performed. Users need to highlight the ex tracted word in sample pages. Figure 2 shows the interface of the training process in our system. field of information. Let PSet ={ p 1 , p 2 , ... , p m } be a set of sample training web pages and we suppose that target records can be ex tracted at least partly from each of those f : l n ), where l i is the location of the field f i in page p . 
The objective of the training process is to mine out (extract) patterns and rules for target information. For each field f i , a pattern set, annotated as PS i , is constructed from which occur just before the instance of f i in page p j , and EW i is the formulation rule of f  X  X  value, which is described using regular expression in our work and given by the we know, a web page is represented with a DOM tree. And all the fields for the same record are embedded in different tag nodes of the tree. We further use directed graph to describe the organizational constraint of the fields in the web page p j . Let E sibling elements in the DOM tree. Then, rl j =( PS j , CG j ) is called an extraction rule for process, multiple rules can be derived from those potential training web pages. We RS ={ rl j }. 
The number of rules in the original RS can be as many as the number of training pages in PSet . However, many rules may show redundant information or contradictory information. To reduce the size of rules, some reduction algorithm is performed on the original set of RS . To do so, we suppose that patterns of fields are orthogonal to constraints of fields. In fact, the former give the local context of fields, and latter describe their occurrence relationships in web pages. With such assumptions, we merge the patterns and the constraints independently. For the is a pre-word extracted from sample pages with respect to field f i , tf ( t ij ) is the number another extreme, all the pre-words are different from each other, then tf ( t ij )=1. Therefore, the weight of t ij indicates its significance degree as a pre-word for field f i . PS 1 : EW 1 , s-PS 2 : EW 2 ,..., s-PS n : EW n ). 
As we know, each constraint of the fields gives the organizational structure of with fields as nodes, and embedded relationship as the directed edges. We concatenate all those constraint graphs into one graph, with nodes as those fields, and CG . Then, ( SP , CG ) is called the extraction rule for schema r ( f 1 , f 2 , ... , f n ). 3.2 Information Extraction Processing extraction field into static field ( SF ) or non-static field ( NSF ) based on the EW i in SP . one extraction field contains rather simila r special characters, then this field is classified as STF . For example, all the e-mail addresses contain  X  X  X  and  X . X , and the structure is stable, which can be easily represented by a regular expression. 
In the following section, the extraction method for SF will be discussed first, followed by the NSF . 3.2.1 Methodology for Static Structure Information As the extracted information is in a static structure, the system makes use of this feature to generate an extraction rule using a regular expression. Before going into detail, a brief introduction to regular expressions will be provided first. Then the rule generation process will be explained in detail. Introduction to Regular Expression Regular expression is the value formulating pattern, which is defined by using regular expression syntax, to represent some information requirements. The regular expression syntax which has been used in our system is shown in table 1. 
For different types of languages used, the regular expression syntax is slightly different. In our system, all the extraction patterns are represented with an XSL template to generate a XML format output result. The regular expression syntax used in XSL is specified in XML Schema [URL5], which is based on the established conventions of languages such as Perl. Some specific feature for regular expression used in XSL, such as the curly braces doubled up in order to distinguish the attribute value representation used in XSL, must be cautioned. Here we do not explain it in more detail, since the syntax used in our extraction rule is the commonly used one. Rule Extraction After the training process, rule extraction for the target information has been obtained. As shown in the following equations, a particular processing extraction for a field f i is shown in equation (1), which is composed of several rule patterns rl ij with  X  X r X  regular expression of the extracted word EW i , as shown in equation (2), where  X  the confusion when more than one f i have the similar structure, such as phone number and fax number, therefore, the s-PW ij is added in the rule pattern. 
For the  X  function, the main syntaxes we have used are shown in table 1. The main idea is to transform all the space charact er into  X  X s* X  and all the text or number patterns into [\w]*. For simplicity, figure 3 shows the step of generating the extraction rule for email. Assume that the PW ij is  X  X -mail: X , and the EW ij is  X  X rofa@umac.mo X . As shown in the following steps, the process of  X  function is to replace all the word character into [\w]* and then replace the space character into \s*. Target Information Extraction represented with a XSL template in order to generate the XML output result. Figure 4 shows the XSL template used in our system for single information extraction. In order to take the advantages of the regular expression, the newest version of XSL, i.e. v.2.0, can provide some new instructions for this purpose. After the regular expression is passed into the XSL template, the xsl:anayze-string instruction will start to test if the regular expression  X  X egexp X  matches the content in the input string, here the selected input string is  X . X , that is the whole page. After that, each of the matched parts will be processed by xsl:matching-substring child instruction, here we only use the most simplest instruction xsl:value-of to get out the value of the matched part. 
XSL [URL4] can help us to find the element nodes in XML document. If the XSL file is set probably, it is able to capture out the most meaningful data in the transformed web page. In our system, XSL 2.0 is used, which is not popularly used, The XSL processor used in our system is Saxon-B 8.7, which is a limited but free version of XSL processor [URL7]. 3.2.2 Methodology for Non-static Information Extraction For the information without static structure, it is not suitable to use the regular expression rule. Therefore, our system trie s to exploit the special features of web page, the organizational structure of the fields in the DOM tree architecture. 
As mentioned before, all the pages are transformed into XHTML format before the extraction process can be performed. So the DOM tree should be valid, well formatted and correct. These requirements are essential and a pre-requisite for our system, because the whole extraction algo rithm relies on the HTML tags. 
For each KTagSe t ktag
Let p be a web page, and PTagSet be the set of tags in p , that is PTagSet = { tag 1 , tag 2 , tag 3 , ... , tag n }, and each of the tag is either open or close tag. From the result training process, keywords exist in the schema information. After analyzing, a set of keywords, KWSet i ={ KW 1 , KW 2 , ... , KW l } for extraction field f j is formed. Then the first most nearest open tag against KW i the keyword KTagSet will be put into a set. After all KW i in KWSet i are applied, KTagSet = { KTage 1 , KTage 2 , ... , KTage n } will be formed. part will be added to the output result set. 
The above methodology will be illustrated in figures 6 and 7. As an example, if keyword W i appears between the first pair of the &lt;LI&gt; as shown in figure 6, then the KTag will be &lt;UL&gt;, since it is the first open tag for the keyword W i . Then the whole page is split by the tag &lt;UL&gt;. After that, for each separated part, if the W i exists, the part will add to the result set. 
In example 7, a more complicated example is presented. As shown in figure 7, keyword W i appears more than one time in the page. In this example, the key tag list further split by &lt;LI&gt; and the process continues. 
The methodology mentioned above is only the basic one. In order to enhance the extraction result, instead of collecting the keyword KW from adifferent training sample pages, some equivalent and synonymous keywords will be added. In our system, the additional keywords added to the KWSet are selected from a lexical dictionary called WordNet [17], which provides semantic relations among words. For example, the  X  X each X  in WordNet 2.1 has the synonymous words  X  X nstruct X  and  X  X earn X . The performance of the basic methodology and the extended one will be discussed later in the evaluation section. and precision measurements are used. Recall measures the amount of the relevant information that the system correctly extracts, and the precision measures the reliability of the information extracted [18]. 
There are totally 450 pages for the training process and 2100 pages for testing. All of them come from the staff information page of different universities, including Boston University, Columbia University, Cornell University, University of Maryland, New York University, Perdue University, M acau University and Yale University. In addition, some other pages are chosen by using Google with the keyword  X  X rofessor X . All these pages are selected randomly in this experiment, in order to evaluate the normal performance. 
After the training process, the telephone , fax and email have a rather static interest , DOM tree analyzing methodology is used. Table 2 shows the results of our experiments on average. 
Since all the web pages are selected from different sources, the structures and content of them are comparatively divergent. By using only the regular expression, it is fair that the recall rate for the telephone and fax are around 0.7, while the precision rate can reach more than 0.9. To extract the information on teaching course and research interest , by comparing the two different methodologies, basic and the advanced one, it is obvious that the performances have some improvement. However, the major advantages of the advanced approach is that after enhancing the key word set, most of the information is captured out at the same time, since those of the added words come from a lexical dictionary, only the synonymous words with the same word type will be considered to be added. As a result, they will not cause much  X  X oise X  to the key word set. In addition, by using the advanced approach, it is possible words can be found from the dictionary. By using those words, it is enough to extract some information. Recall 0.721622 0.696703 0.77593 0.875255 0.82098 0.878205
Precision 0.9821 0.952381 0.831667 0.559 836 0.807409 0.562824 In this paper, we have firstly discussed the history and current developments in web information extraction. A further detailed analysis of our approach shows that it relies on the human training process. In contrast to most existing extraction systems, our system uses different methodologies to extract the information, either through the regular expressions or through analysis of the DOM tree. Furthermore, we have also proposed an advanced approach for DOM tree analysis by enhancing the keyword set. After the extraction process, the result is output to an XML file format. However, some limitations also exist. In the current system, the extraction task is only individual page based. It means that all the fields for the same record are supposed to be contained in the same pages. However, in many other situations, the fields may be located in different relevant pages, such as several linked web pages. In future work, we are going to extend our system to handle multi-page extractions. Acknowledgement. This Work was supported in part by the University Research Committee under Grant No. RG069/05-06S/07R/GZG/FST and by the Science and Technology Development Found of Macao Government under Grant No. 044/2006/A. 
