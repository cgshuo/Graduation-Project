 The pap er presen ts a metho d for pruning frequen t itemsets based on bac kground kno wledge represen ted by a Bayesian net work. The interestingness of an itemset is de ned as the absolute di erence between its supp ort estimated from data and from the Bayesian net work. Ecien t algorithms are pre-sen ted for nding interestingness of a collection of frequen t itemsets, and for nding all attribute sets with a given mini-mum interestingness. Practical usefulness of the algorithms and their eciency have been veri ed exp erimen tally . H.2.8 [ Database Managemen t ]: Database Applications| data mining Algorithms asso ciation rule, frequen t itemset, bac kground kno wledge, interestingness, Bayesian net work
Finding frequen t itemsets and asso ciation rules in data-base tables has been an activ e researc h area in recen t years. Unfortunately , the practical usefulness of the approac h is limited by huge num ber of patterns usually disco vered. For larger databases man y thousands of asso ciation rules may be pro duced when minim um supp ort is low. This creates a secondary data mining problem: after mining the data, we are now comp elled to mine the disco vered patterns. The problem has been addressed in literature mainly in the con-text of asso ciation rules, where the two main approac hes are sorting rules based on some interestingness measure, and pruning aiming at remo ving redundan t rules.

Full review of suc h metho ds is beyond the scop e of this pap er. Overviews of interestingness measures can be found for example in [3, 13, 11, 32], some of the pap ers on rule pruning are [30, 31, 7, 14, 28, 16, 17, 33].

Man y interestingness measures are based on the div er-gence between true probabilit y distributions and distribu-tions obtained under the indep endence assumption. Prun-ing metho ds are usually based on comparing the con dence of a rule to the con dence of rules related to it.
The main dra wbac k of those metho ds is that they tend to generate rules that are either obvious or have already been kno wn by the user. This is to be exp ected, since the most striking patterns whic h those metho ds select can also easily be disco vered using traditional metho ds or are kno wn directly from exp erience.

We believ e that the prop er way to address the problem is to include users bac kground kno wledge in the pro cess. The patterns whic h div erge the most from that bac kground kno wledge are deemed most interesting. Disco vered patterns can later be applied to impro ve the bac kground kno wledge itself.

Man y approac hes to using bac kground kno wledge in ma-chine learning are focused on using bac kground kno wledge to speed up the hypothesis disco very pro cess and not on dis-covering interesting patterns. Those metho ds often assume strict logical relationships, not probabilistic ones. Examples are kno wledge based neural net works (KBANNs) and uses of bac kground kno wledge in Inductiv e Logic Programming. See Chapter 12 in [20] for an overview of those metho ds and a list of further references.

Tuzhilin et. al. [23, 22, 29] work ed on applying bac kground kno wledge to nding interesting rules. In [29, 22] inter-estingness measures are presen ted, whic h tak e into accoun t prior beliefs; in another pap er [23], the authors presen t an algorithm for selecting a minim um set of interesting rules given bac kground kno wledge. The metho ds used in those pap ers are local, that is, they don't use a full join t probabil-ity of the data. Instead, interestingness of a rule is evaluated using rules in the bac kground kno wledge with the same con-sequen t. If no suc h kno wledge is presen t for a given rule, the rule is considered unin teresting. This mak es it imp ossible to tak e into accoun t transitivit y. Indeed, in the presence of the bac kground kno wledge represen ted by the rules A ) B and B ) C , the rule A ) C is unin teresting. However, this can-not be disco vered locally . See [25] for a detailed discussion of adv antages of global versus local metho ds. Some more comparisons can be found in [18].

In this pap er we presen t a metho d of nding interest-ing patterns using bac kground kno wledge represen ted by a Bayesian net work. The main adv antage of Bayesian net-works is that they concisely represen t full join t probabilit y distributions, and allo w for practically feasible probabilistic inference from those distributions [25, 15]. Other adv antages include the abilit y to represen t causal relationships, easy to understand graphical structure, as well as wide availabilit y of mo delling tools. Bayesian net works are also easy to mo d-ify by adding or deleting edges.

We opt to compute interestingness of frequen t itemsets instead of asso ciation rules, agreeing with [7] that directions of dep endence should be decided by the user based on her exp erience and not suggested by interestingness measures. Our approac h works by estimating supp orts of itemsets from Bayesian net works and comparing thus estimated supp orts with the data. Itemsets with strongly div erging supp orts are considered interesting.

Further de nitions of interestingness exploiting Bayesian net work's structure are presen ted, as well as ecien t meth-ods for computing interestingness of large num bers of item-sets and for nding all attribute sets with given minim um interestingness.

There are some analogies between mining emerging pat-terns [6] and our approac h, the main di erences being that in our case a Bayesian net work is used instead of a second dataset, and that we use a di eren t measure for comparing supp orts. Due to those di erences our problem requires a di eren t approac h and a di eren t set of algorithms.
Database attributes will be denoted with upp ercase let-ters A; B; C; : : : , domain of an attribute A will be denoted by Dom( A ). In this pap er we are only concerned with cat-egorical attributes, that is attributes with nite domains. Sets of attributes will be denoted with upp ercase letters I; J; : : : . We often use database notation for represen ting sets of attributes, i.e. I = A 1 A 2 : : : A k instead of the set the-oretical notation f A 1 ; A 2 ; : : : ; A k g . Domain of an attribute set I = A 1 A 2 : : : A k is de ned as Values from domains of attributes and attribute sets are de-noted with corresp onding lowercase boldface letters, e.g. i 2 Dom( I ).

Let P I denote a join t probabilit y distribution of the at-tribute set I . Similarly let P I j J be a distribution of I con-ditioned on J . When used in arithmetic operations suc h distributions will be treated as functions of attributes in I and I [ J resp ectiv ely, with values in the interv al [0 ; 1]. For example P I ( i ) denotes the probabilit y that I = i .
Let P I be a probabilit y distribution, and let J I . De-note by P # J I the marginalization of P I onto J , that is where the summation is over the domains of all variables from I n J .

Probabilit y distributions estimated from data will be de-noted by adding a hat sym bol, e.g. ^ P I .

An itemset is a pair ( I; i ), where I is an attribute set and i 2 Dom( I ). The supp ort of an itemset ( I; i ) is de ned as where the probabilit y is estimated from some dataset. An itemset is called frequent if its supp ort is greater than or equal to some user de ned threshold minsupp . Finding all frequen t itemsets in a given database table is a well kno wn datamining problem [1].
 A Bayesian network BN over a set of attributes H = A 1 : : : A n is a directed acyclic graph BN = ( V; E ) with the set of vertices V = f V A 1 ; : : : ; V A n g corresp onding to attributes of H , and a set of edges E V V , where eac h vertex V A i has asso ciated a conditional probabilit y distribu-tion P A of attributes corresp onding to paren ts of V A i in G . See [25, 15] for a detailed discussion of Bayesian net works.
A Bayesian net work BN over H uniquely de nes a join t probabilit y distribution of H . For I H the distribution over I marginalized from P H will be denoted by P BN I Let us rst de ne the supp ort of an itemset ( I; i ) in a Bayesian network BN as
Let BN be a Bayesian net work over an attribute set H , and let ( I; i ) be an itemset suc h that I H . The inter est-ingness of the itemset ( I; i ) with resp ect to BN is de ned as that is, the absolute di erence between the supp ort of the itemset estimated from data, and the estimate of this sup-port made from the Bayesian net work BN . In the remaining part of the pap er we assume that interestingness is alw ays computed with resp ect to a Bayesian net work BN and the subscript is omitted.

An itemset is -inter esting if its interestingness is greater than or equal to some user speci ed threshold .

A frequen t interesting itemset represen ts a frequen tly oc-curring (due to minim um supp ort requiremen t) pattern in the database whose probabilit y is signi can tly di eren t from what it is believ ed to be based on the Bayesian net work mo del.

An alternativ e would be to use supp ( I; i ) = supp BN ( I; i ) as the measure of interestingness [6]. We decided to use absolute di erence instead of a quotien t since we found it to be more robust, esp ecially when both supp orts are small.
One could think of applying our approac h to asso ciation rules with the di erence in con dences as a measure of inter-estingness but, as men tioned in the Introduction, we think that patterns whic h do not suggest a direction of in uence are more appropriate.

Since in Bayesian net works dep endencies are mo delled us-ing attributes not itemsets, it will often be easier to talk about interesting attribute sets, esp ecially when the disco v-ered interesting patterns are to be used to update the bac k-ground kno wledge.

Definition 3.1. Let I be an attribute set. The interest-ingness of I is de ne d as analo gously, I is -interesting if I ( I ) .

An alternativ e approac h would be to use generalizations of Bayesian net works allo wing dep endencies to vary for dif-feren t values of attributes, see [27], and deal with itemset interestingness directly .
Even though applying the above de nition and sorting attribute sets on their interestingness works well in practice, there migh t still be a large num ber of patterns retained, esp ecially if the bac kground kno wledge is not well dev elop ed and large num ber of attribute sets have high interestingness values. This motiv ates the follo wing two de nitions.
Definition 3.2. An attribute set I is hierarc hically -interesting if it is -inter esting and none of its proper subsets is -inter esting.

The idea is to prev ent large attribute sets from becoming interesting when the true cause of them being interesting lies in their subsets.

There is also another problem with De nition 3.1. Con-sider a Bayesian net work where nodes A and B have resp ectiv e probabilit y distribu-tions P A and P B j A attac hed. Supp ose also that A is -interesting. In this case even if P B j A is the same as ^ attribute sets B and AB may be considered -interesting. Belo w we presen t a de nition of interestingness aiming at prev enting suc h situations.
 A vertex V is an anc estor of a vertex W in a directed graph G if there is a directed path from V to W in G . The set of ancestors of a vertex V in a graph G is denoted by anc ( V ). Moreo ver, let us denote by anc ( I ) the set of all ancestor attributes in BN of an attribute set I . More formally: anc ( I ) = f A i = 2 I : V A i 2 anc ( V A j ) in BN, for some A
Definition 3.3. An attribute set I is top ologically -interesting if it is -inter esting, and ther e is no attribute set J such that 1. J anc ( I ) [ I , and 2. I 6 J , and 3. J is -inter esting.

The inten tion here is to prev ent interesting attribute sets from causing all their successors in the Bayesian net work (and the sup ersets of their successors) to become interesting in a cascading fashion.

To see why condition 2 is necessary consider a Bayesian net work Supp ose that there is a dep endency between A and B in data whic h mak es AB -interesting. Now however AB X may also become interesting, (ev en if P A j X and P B j X are correct in the net work) and cause AB to be pruned. Condition 2 prev ents AB from being pruned and AB X from becoming interesting.

Notice that top ological interestingness is stricter than hi-erarc hical interestingness. Indeed if J I is -interesting, then it satis es all the above conditions, and mak es I not top ologically -interesting.
In this section we presen t algorithms using the de nition of interestingness introduced in the previous section to select interesting itemsets or attribute sets. We begin by describ-ing a pro cedure for computing marginal distributions for a large collection of attribute sets from a Bayesian net work.
Computing the interestingness of a large num ber of fre-quen t itemsets requires the computation of a large num ber of marginal distributions from a Bayesian net work. The prob-lem has been addressed in literature mainly in the con text of nding marginals for every attribute [25, 15], while here we have to nd marginals for multiple, overlapping sets of attributes. The approac h tak en in this pap er is outlined below.
 The problem of computing marginal distributions from a Bayesian net work is kno wn to be NP-hard, nev ertheless in most cases the net work structure can be exploited to speed up the computations.
 Here we use exact metho ds for computing the marginals. Appro ximate metho ds like Gibbs sampling are an interesting topic for future work.

Best kno wn approac hes to exact marginalizations are join trees [12] and buc ket elimination [5]. We chose buc ket elim-ination metho d whic h is easier to implemen t and according to [5] as ecien t as join tree based metho ds. Also, join trees are mainly useful for computing marginals for single attributes, and not for sets of attributes.

The buc ket elimination metho d, whic h is based on the dis-tributiv e law, pro ceeds by rst choosing a variable ordering and then applying distributiv e law rep eatedly to simplify the summation. For example supp ose that a join t distribution of a Bayesian net work over H = AB C is expressed as and we want to nd P BN A . We need to compute the sum whic h can be rewritten as Assuming that domains of all attributes have size 3, com-puting the rst sum directly requires 12 additions and 18 multiplications, while the second sum requires only 4 addi-tions and 6 multiplications.

The expression is interpreted as a tree of buckets , eac h buc ket is either a single probabilit y distribution, or a sum over a single attribute tak en over a pro duct of its child buc k-ets in the tree. In the example above a special root buc ket without summation could be introduced for completeness.
In most cases the metho d signi can tly reduces the time complexit y of the problem. An imp ortan t problem is choos-ing the righ t variable ordering. Unfortunately that problem is itself NP-hard. We thus adopt a heuristic whic h orders variables according to the decreasing num ber of factors in the pro duct dep ending on eac h variable. A detailed discus-sion of the metho d can be found in [5].

Although buc ket elimination can be used to obtain sup-ports of itemsets directly (i.e. P I ( i )), we use it to obtain complete marginal distributions. This way we can directly apply marginalization to obtain distributions for subsets of I (see below).

Since buc ket elimination is performed rep eatedly we use memoization to speed it up, as suggested in [21]. We re-mem ber eac h partial sum and reuse it if possible. In the computed P BN A would have been remem bered.

Another metho d of obtaining a marginal distribution P I is marginalizing it from P J where I J using Equation (1), pro vided that P J is already kno wn. If j J n I j is small, this pro cedure is almost alw ays more ecien t than buc ket elim-ination, so whenev er some P I is computed by buc ket elimi-nation, distributions of all subsets of I are computed using Equation (1).
 Definition 4.1. Let C be a collection of attribute sets. The positiv e border of C [19], denote d by Bd + ( C ) , is the collection of those sets from C which have no proper superset in C : Bd + ( C ) = f I 2C : ther e is no J 2C such that I J g : It is clear from the discussion above that we only need to use buc ket elimination to compute distributions of itemsets in the positiv e border. We are going to go further than this; we will use buc ket elimination to obtain sup ersets of sets in the positiv e border, and then use Equation (1) to obtain marginals even for sets in the positiv e border. Exp erimen ts sho w that this approac h can give substan tial savings, esp e-cially when man y overlapping attribute sets from the posi-tive border can be covered by a single set only sligh tly larger then the covered ones.

The algorithm for selecting the marginal distribution to compute is motiv ated by the algorithm from [9] for com-puting views that should be materialized for OLAP query pro cessing. Buc ket elimination corresp onds to creating a materialized view, and marginalizing thus obtained distri-bution to answ ering OLAP queries.

We rst need to de ne costs of marginalization and buc ket elimination. In our case the cost is de ned as the total num ber of additions and multiplications used to compute the marginal distribution.

The cost of marginalizing P J from P I , J I using Equa-tion (1) is It follo ws from the fact that eac h value of P # J I requires adding j Dom( I n J ) j values from P I .

The cost of buc ket elimination can be computed cheaply without actually executing the pro cedure. Eac h buc ket is either an explicitly given probabilit y distribution, or com-putes a sum over a single variable of a pro duct of functions (computed in buc kets con tained in it) explicitly represen ted as multidimensional tables, see [5] for details. If the buc ket is an explicitly given probabilit y distribution, the cost is of course 0.

Consider now a buc ket b con taining child buc kets b 1 ; : : : ; b yielding functions f 1 ; : : : ; f n resp ectiv ely. Let Var( f of attributes on whic h f i dep ends.

Let f = f 1 f 2 f n denote the pro duct of all factors in b . We have Var( f ) = [ n i =1 Var( f i ), and since eac h value of f requires n 1 multiplications, computing f requires j Dom(V ar( f )) j ( n 1) multiplications. Let A b be the at-tribute over whic h summation in b tak es place. Computing the sum will require j Dom(V ar( f ) nf A b g ) j ( j Dom( A additions.

So the total cost of computing the function in buc ket b (including costs of computing its children) is thus cost ( b ) = The cost of computing P BN I through buc ket elimination, denoted cost BE ( P BN I ), is the cost of the root buc ket of the summation used to compute P BN I .

Let C be a collection of attribute sets. The gain of using buc ket elimination to nd P BN I for some I while computing interestingness of attribute sets from C can be expressed as: gain ( I ) = cost BE ( P BN I ) + An attribute set to whic h buc ket elimination will be applied is found using a greedy pro cedure by adding in eac h itera-tion the attribute giving the highest increase of gain . The complete algorithm is presen ted in Figure 1.
First we presen t an algorithm for computing interesting-ness of all itemsets in a given collection. Its a simple appli-cation of the algorithm in Figure 1. It is useful if we already Input: collection of attribute sets C , Bayesian net work BN Output: distributions P BN I for all I 2C 1. S Bd + ( C ) 2. while S 6 = ; : 3. I an attribute set from S . 4. for A in H n I : 5. compute gain ( I [f A g ) 6. pick A ? for whic h the gain in Step 5 was maximal 7. if gain ( I [f A ? g ) &gt; gain ( I ): 8. I I [f A ? g 9. goto 4 10. compute P BN I from BN using buc ket elimination 11. compute P BN I # J for all J 2S ; J I using Equa-12. remo ve from S all attribute sets included in I 13. compute P BN J for all J 2 C n Bd + ( C ) using Equa-Figure 1: Algorithm for computing a large num ber of marginal distributions from a Bayesian net work. have a collection of itemsets (e.g. all frequen t itemsets found in a database table) and want to select those whic h are the most interesting. The algorithm is given below Input: collection of itemsets K , supp orts of all itemsets in K , Bayesian net work BN Output: interestingness of all itemsets in K . 1. C f I : ( I; i ) 2K for some i 2 Dom( I ) g 2. compute P BN I for all I 2C using algorithm in Figure 1 3. compute interestingness of all itemsets in K using dis-
In this section we will presen t an algorithm for nding all attribute sets with interestingness greater than or equal to a speci ed threshold given a dataset and a Bayesian net work BN .
 Let us rst mak e an observ ation:
Obser vation 4.2. If an itemset ( I; i ) has inter estingness greater than or equal to with respect to a Bayesian network BN then its supp ort must be greater than or equal to in either the data or in BN . Mor eover if an attribute set is -inter esting, by de nition 3.1, at least one of its itemsets must be -inter esting.

It follo ws that if an attribute set is -interesting, then one of its itemsets must be frequen t, with minim um supp ort , either in the data or in the Bayesian net work.
 Input: Bayesian net work BN , minim um supp ort minsupp . Output: itemsets whose supp ort in BN is minsupp 1. k 1 2. Cand f ( I; i ) : j I j = 1 g 3. compute supp BN ( I; i ) for all ( I; i ) 2 Cand using the 4. F req k f ( I; i ) 2 Cand : supp BN ( I; i ) minsupp g 5. Cand generate new candidates from F req k 6. remo ve itemsets with infrequen t subsets from Cand 7. k k + 1; goto 3
The algorithm works in two stages. First all frequen t item-sets with minim um supp ort are found in the dataset and their interestingness is computed. The rst stage migh t have missed itemsets whic h are -interesting but don't have suf-cien t supp ort in the data.

In the second stage all itemsets frequen t in the Bayesian net work are found, and their supp orts in the data are com-puted using an extra database scan.

To nd all itemsets frequen t in the Bayesian net work we use the Apriori algorithm [1] with a mo di ed supp ort coun t-ing part, whic h we call AprioriBN . The sketch of the algo-rithm is sho wn in Figure 2, except for step 3 it is iden tical to the original algorithm.

We now have all the elemen ts needed to presen t the al-gorithm for nding all -interesting attribute sets, whic h is given in Figure 3.

Step 4 of the algorithm can reuse marginal distributions found in step 3 to speed up the computations.

Notice that it is alw ays possible to compute interesting-ness of every itemset in step 6 since both supp orts of eac h itemset will be computed either in steps 1 and 3, or in steps 4 and 5.

The authors implemen ted hierarc hical and top ological in-terestingness as a postpro cessing step. They could however be used to prune the attribute sets whic h are not interest-ing without evaluating their distributions, thus pro viding a poten tially large speedup in the computations. We plan to investigate that in the future.
In this section we presen t exp erimen tal evaluation of the metho d. One problem we were faced with was the lack of publicly available datasets with non trivial bac kground kno wledge that could be represen ted as a Bayesian net-work. The UCI Mac hine Learning rep ository con tains a few datasets with bac kground kno wledge (Japanese credit, molecular biology), but they are aimed primarily at Induc-tive Logic Programming: the relationships are logical rather than probabilistic, only relationships involving the class at-tribute are included. These examples are of little value for our approac h.

We have thus used net works constructed using our own Input: Bayesian net work BN , dataset, interestingness threshold .
 Output: all attribute sets with interestingness at least , and some of the attribute sets with lower interestingness. 1. K f ( I; i ) : supp ( I; i ) g (using Apriori algorithm) 2. C f I : ( I; i ) 2K for some i 2 Dom( I ) g 3. compute P BN I for all I 2C using algorithm in Figure 1 4. K 0 f ( I; i ) : supp BN ( I; i ) g (using AprioriBN 5. compute supp ort in data for all itemsets in K 0 nK by 6. compute interestingness of all itemsets in K[K 0 7. C 0 f I : ( I; i ) 2K 0 for some i 2 Dom( I ) g 8. compute interestingness of all attribute sets I in C 0 Figure 3: Algorithm for nding all -interesting at-tribute sets. common-sense kno wledge as well as net works learned from data.
We rst presen t a simple example demonstrating the use-fulness of the metho d. We use the KSL dataset of Danish 70 year olds, distributed with the DEAL Bayesian net work pac kage [4]. There are nine attributes, describ ed in Table 1, related to the person's general health and lifest yle. All con-tinuous attributes have been discretized into 3 levels using the equal weigh t metho d.

We began by designing a net work structure based on au-thors' (non-exp ert) kno wledge. The net work structure is given in Figure 4a. Since we were not sure about the rela-tion of cholesterol to other attributes, we left it unconnected. Conditional probabilities were estimated directly from the KSL dataset. Note that this is a valid approac h since even when the conditional probabilities matc h the data perfectly interesting patterns can still be found because the net work structure usually is not capable of represen ting the full join t distribution of the data. The interesting patterns can then be used to update the net work's structure. Of course if both the structure and the conditional probabilities are given by Figure 4: Net work structures for the KSL dataset constructed by the authors the exp ert, then the disco vered patterns can be used to up-date both the net work's structure and conditional probabil-ities.

We applied the algorithm for nding all interesting at-tribute sets to the KSL dataset and the net work, using the threshold of 0 : 01. The attribute sets returned were sorted by interestingness, and top 10 results were kept.

The two most interesting attribute sets were f F EV; Sex g with interestingness 0 : 0812 and f Alc; Y ear g with interest-ingness 0 : 0810.

Indeed, it is kno wn (see [8]) that women's lungs are on av-erage 20% 25% smaller than men's lungs, so sex in uences the forced ejection volume ( FEV ) much more than smoking does (whic h we though t was the primary in uence). This fact, although not new in general, was overlo oked by the authors, and we susp ect that, due to large amoun t of lit-erature on harmful e ects of smoking, it migh t have been overlo oked by man y domain exp erts. This pro ves the high value of our approac h for veri cation of Bayesian net work mo dels.

The data itself implied a gro wth in alcohol consumption between 1967 and 1984, whic h we considered to be a plau-sible nding.

We then decided to mo dify the net work structure based on our ndings by adding edges Sex ! F EV and Y ear ! Alc . One could of course consider other metho ds of mo difying net work structure, like deleting edges or rev ersing their di-rection. A brief overview of more adv anced metho ds of Bayesian net work mo di cation can be found in [15, Chap. 3, Sect. 3.5]. Instead of adapting the net work structure one could keep the structure unc hanged, and tune conditional probabilities in the net work instead, see [15, Chap. 3, Sect. 4] for details.
As a metho d of scoring net work structures we used the natural logarithm of the probabilit y of the structure condi-tioned on the data, see [10, 26] for details on computing the score.

The mo di ed net work structure had the score of 7162 : 71 whic h is better than that of the original net work: 7356 : 68.
With the mo di ed structure, the most interesting attribute set was f Kol; Sex; Y ear g with interestingness 0 : 0665. We found in the data that cholesterol levels decreased between the two years in whic h the study was made, and that choles-terol level dep ends on sex. We found similar trends in the U.S. population based on data from American Heart Asso-ciation [2]. Adding edges Y ear ! Kol and Sex ! Kol impro ved the net work score to 7095 : 25. f F EV; Alc; Y ear g became the most interesting attribute set with the interestingness of 0 : 0286. Its interestingness is however much lower than that of previous most interesting attribute sets. Also, we were not able to get any impro ve-men t in net work score after adding edges related to that attribute set.

Since we were unable to obtain a better net work in this case, we used top ological pruning, exp ecting that some other attribute sets migh t be the true cause of the observ ed dis-crepancies. Only four attribute sets, given below, were top o-logically 0 : 01-in teresting.
We found all those patters intuitiv ely valid, but were un-able to obtain an impro vemen t in the net work's score by adding related edges. Moreo ver, the interestingness values were quite small. We thus nished the interactiv e net work structure impro vemen t pro cess with the nal result given in Figure 4b.

The algorithm was implemen ted in Python and used on a 1.7GHz Pentium 4 mac hine. The computation of inter-estingness for this example took only a few seconds so an interactiv e use of the program was possible. Further perfor-mance evaluation is given below.
We now presen t the performance evaluation of the algo-rithm for nding all attribute sets with given minim um in-terestingness. We used the UCI datasets and Bayesian net-works learned from data using B-Course [26]. The results are given in Table 2.

The max. size column gives the maxim um size of frequen t attribute sets considered. The #mar ginals column gives the total num ber of marginal distributions computed from the Bayesian net work. The attribute sets whose marginal dis-tributions have been cac hed between the two stages of the algorithm are not coun ted twice.

The time does not include the initial run of the Apriori al-gorithm used to nd frequen t itemsets in the data (the time of the AprioriBN algorithm is included though). The times for larger net works can be substan tial; however the prop osed metho d has still a huge adv antage over man ually evaluating thousands of frequen t patterns, and there are sev eral possi-bilities to speed up the algorithm not yet implemen ted by the authors, discussed in the follo wing section. Figure 5: Time of computation dep ending on the num ber of marginal distributions computed for the lymphography database Figure 6: Time of computation dep ending on the num ber of attributes for datasets from Table 2
The maximum inter estingness column gives the interest-ingness of the most interesting attribute set found for a given dataset. It can be seen that there are still highly interesting patterns to be found after using classical Bayesian net work learning metho ds. This pro ves that frequen t pattern and as-sociation rule mining has the capabilit y to disco ver patterns whic h traditional metho ds migh t miss.

To give a better understanding of how the algorithm scales as the problem size increases we presen t two additional g-ures. Figure 5 sho ws how the computation time increases with the num ber of marginal distributions that must be com-puted from the Bayesian net work. It was obtained by vary-ing the maxim um size of attribute sets between 1 and 5. The value of = 0 : 067 was used (equiv alen t to one row in the database). It can be seen that the computation time gro ws sligh tly slower than the num ber of marginal distri-butions. The reason for that is that the more marginal distributions we need to compute, the more opp ortunities we have to avoid using buc ket elimination by using direct marginalization from a sup erset instead.

Determining how the computation time dep ends on the size of the net work is dicult, because the time dep ends also on the net work structure and the num ber of marginal distributions computed (whic h in turn dep ends on the max-imum size of attribute sets considered).

We nev ertheless sho w in Figure 6 the num bers of attributes and computation times plotted against eac h other for some of the datasets from Table 2. Data corresp onding to maxi-mum attribute set sizes equal to 3 and 4 are plotted sepa-rately .

It can be seen that the algorithm remains practically us-able for fairly large net works of up to 60 variables, even though the computation time gro ws exp onen tially . For larger net works appro ximate inference metho ds migh t be neces-sary , but this is beyond the scop e of this pap er.
A metho d of computing interestingness of itemsets and at-tribute sets with resp ect to bac kground kno wledge enco ded as a Bayesian net work was presen ted. We built ecien t al-gorithms for computing interestingness of frequen t itemsets and nding all attribute sets with given minim um interest-ingness. Exp erimen tal evaluation pro ved the e ectiv eness and practical usefulness of the algorithms for nding inter-esting, unexp ected patterns.

An obvious direction for future researc h is increasing ef-ciency of the algorithms. Partial solution would be to rewrite the code in C, or to use some o -the-shelf highly op-timized Bayesian net work library like Intel's PNL. Another approac h would be to use appro ximate inference metho ds like Gibbs sampling.

Adding or remo ving edges in a Bayesian net work does not alw ays in uence all of its marginal distributions. Interactiv-ity of net work building could be imp orv ed by making use of this prop erty.

Usefulness of metho ds dev elop ed for mining emerging pat-terns [6], esp ecially using borders to represen t collections of itemsets, could also be investigated.

Another interesting direction (suggested by a review er) could be to iterativ ely apply interesting patterns to mo dify the net work structure until no further impro vemen t in the net work score can be achiev ed. A similar pro cedure has been used in [24] for bac kground kno wledge represen ted by rules.
It should be noted however that it migh t be better to just inform the user about interesting patterns and let him/her use their exp erience to update the net work. Man ually up-dated net work migh t better re ect causal relationships be-tween attributes.

Another researc h area could be evaluating other proba-bilistic mo dels suc h as log-linear mo dels and chain graphs instead of Bayesian net works. [1] R. Agra wal, T. Imielinski, and A. Swami. Mining [2] American Heart Asso ciation. Risk factors: High blo od [3] R. J. Bayardo and R. Agra wal. Mining the most [4] Susanne G. Bttc her and Claus Dethlefsen. Deal: A [5] Rina Dec hter. Buc ket elimination: A unifying [6] Guozh u Dong and Jinyan Li. Ecien t mining of [7] William DuMouc hel and Daryl Pregib on. Empirical [8] H. Gra y. Gray's Anatomy . Grammercy Books, New [9] Venky Harinara yan, Anand Rajaraman, and Je rey D. [10] David Hec kerman. A tutorial on learning with [11] R. Hilderman and H. Hamilton. Kno wledge disco very [12] C. Huang and A. Darwic he. Inference in belief [13] S. Jaroszewicz and D. A. Simo vici. A general measure [14] S. Jaroszewicz and D. A. Simo vici. Pruning redundan t [15] Finn V. Jensen. Bayesian Networks and Decision [16] Bing Liu, Wynne Hsu, and Shu Chen. Using general [17] Bing Liu, Wynne Jsu, Yiming Ma, and Shu Chen. [18] Heikki Mannila. Local and global metho ds in data [19] Heikki Mannila and Hann u Toivonen. Lev elwise searc h [20] T.M. Mitc hell. Machine Learning . McGra w-Hill, 1997. [21] Kevin Murph y. A brief introduction to graphical [22] B. Padmanabhan and A. Tuzhilin. Belief-driv en [23] B. Padmanabhan and A. Tuzhilin. Small is beautiful: [24] B. Padmanabhan and A. Tuzhilin. Metho ds for [25] Judea Pearl. Probabilistic Reasoning in Intel ligent [26] P.Myllym X  aki, T.Silander, H.Tirri, and P.Uronen. [27] D. Poole and N. L. Zhang. Exploiting con textual [28] D. Shah, L. V. S. Lakshmanan, K. Ramamritham, and [29] Abraham Silb ersc hatz and Alexander Tuzhilin. On [30] E. Suzuki. Autonomous disco very of reliable exception [31] E. Suzuki and Y. Kodrato . Disco very of surprising [32] P.-N. Tan, V. Kumar, and J. Sriv asta va. Selecting the [33] M. J. Zaki. Generating non-redundan t asso ciation
