 Web content analysis often has two sequential and separate steps: Web Classification to identify the target Web pages, and Web Information Extraction to extract the metadata contained in the target Web pages. This decoupled strategy is highly ineffective since the errors in Web classification will be propagated to Web information extraction and eventu-ally accumulate to a high level. In this paper we study the mutual dependencies between these two steps and propose to combine them by using a model of Conditional Random Fields (CRFs). This model can be used to simultaneously recognize the target Web pages and extract the correspond-ing metadata. Systematic experiments in our project Of-Course for online course search show that this model signif-icantly improves the F1 value for both of the two steps. We believe that our model can be easily generalized to many Web applications.
 I.5.1 [ Pattern Recognition ]: Models -Statistical Algorithms, Experimentation Classification, Information extraction, Graphical model The explosive growth and popularity of the World Wide Web has resulted in a huge amount of information on the Internet. Currently access to this huge collection of informa-tion has been limited to searching and browsing at the level of Web pages. However, sophisticated Web applications, such as vertical search and entity search, call for flexible in-formation extraction techniques that transform information on the Web pages into structured (relational) data [7]. In Copyright 2009 ACM 978-1-60558-495-9/09/06 ... $ 5.00. recent years, progress in this area has moved us closer to this goal. Some research and industry groups have built systems that support more precise query on certain content verticals, such as online products [2], academic publications [8] etc.
Usually, to build such systems we need a process that is a cascade of two main steps, i.e. Web classification and Web information extraction , after the relevant Web pages are downloaded by focused crawling [10]. Since focused crawling inevitably includes some irrelevant pages we need to conduct a finer level of Web-page classification to separate out the relevant pages from the irrelevant ones. Then, after the Web pages are further categorized Web information extraction is performed on the real relevant pages to extract target enti-ties with their metadata. As stated by McCallum [7], it is highly ineffective to use this decoupled strategy -attempting to do Web classification and Web information extraction in two separate phases. Specifically, since these two steps are separate, the errors in Web classification will be propagated to Web information extraction and eventually accumulate to a high level. Therefore, the overall performance is upper-bounded by that of Web classification.

In this paper, we propose a method to combine Web clas-sification and information extraction and achieve mutual enhancement between these two operations. Rather than conducting these two steps separately and sequentially, our method utilizes the probabilistic graphical model to simulta-neously detect the target Web pages and extract the meta-data in them, through which we aim to improve both the recall and precision of Web classification and Web informa-tion extraction.
We begin by illustrating the problem with an example, drawn from a project OfCourse at HP Labs China. Of-Course 1 is a vertical search engine for online courses. The goal of this project is to accurately identify the online course pages and extract key information from them. The key in-formation include course title, course time, course ID and so on, and we call them course metadata. The traditional approach to reach this goal is to first identify the course homepages by Web page classification, then extract meta-data from the homepages. After studying this problem, we find that these two steps are closely related in that informa-tion obtained from one step can greatly help the other step. On one hand, if a Web page is a course homepage it usually contains a course title. On the other hand, the existence of some course metadata, in turn, indicates that the current
Available at http://fusion.hpl.hp.com/OfCourse/ Web page is a course homepage. This means that there is a forward dependency from Web classification to informa-tion extraction, and also a backward dependency from Web information extraction.

To understand the room for improvement by considering the backward dependency at an intuitive level, we give two typical course homepages, as shown in Figure 1. Before we detail these two examples we first give a brief introduction on how to identify the course homepages by text classifi-cation. The online course homepages usually contain some terms about course and teaching, e.g.,  X  instructor  X ,  X  teach-ing assistant  X ,  X  course scheduling  X ,  X  syllabus  X  etc. These terms, which frequently appear in course homepages, can be used as features to distinguish them from non course homepages. However, the occurrence of these terms is not a sufficient condition for course homepages. The Web page in Figure 1(a) describes how to enroll a class, and also con-tains many terms about course and teaching. Thus, it is likely that a classifier makes a wrong decision by predicting it to be a course homepage. As to the course homepage in Figure 1(b), it contains a conspicuous course title  X  Quantum Mechanics  X  with the course ID  X  physics 113B  X . However, it lacks the course terms to indicate it is a course homepage ( X  Recommended Books  X  is the only course term it contains). Thus, it is likely that a classifier predicts the page in Fig-ure 1(b) to be not a course homepage. To show the effect of the dependency from information extraction to classifica-tion, we assume for the moment that there exists an oracle to tell whether a Web page contains a course title or not, and a Web page is a course homepage if and only if it con-tains a course title. Then, under this assumption we can reach the right conclusion that the Web page in Figure 1(a) is not a course homepage since it does not contain a course title, and the one in Figure 1(b) is a course homepage since it contains a course title. These two examples show that we can leverage the existence of certain entity metadata to improve not only the precision (shown by the example in Figure 1(a)) and but also recall (shown by the example in Figure 1(b)) in classification and extraction. However, how can we obtain the oracle for course metadata? This is the problem we address in this paper. As described above, we find that Web classification and Web information extraction are actually two coupled steps, and they should not be separated. Therefore, in this paper we propose a method to jointly and simultaneously opti-mize these two steps by the probabilistic graphical model CRFs [5].

Given a Web page x and a set of DOM (Document Ob-ject Model) tree leaf nodes x 1 ,  X  X  X  , x k in x , we formulate the problem of Web classification and Web information extrac-tion as the task of assigning labels to x, x 1 ,  X  X  X  , x k on x indicates the class of this Web page, while the labels on x 1 ,  X  X  X  , x k indicates the types of the metadata for each leaf node. This way these two tasks can be integrated into one probabilistic model. Additionally, we explicitly consider the constraints between Web classification and Web infor-mation extraction (actually the constraints among the labels of x, x 1 ,  X  X  X  , x k ), and inject these constraints in both model learning and prediction. This way we greatly reduce the label space and achieve exact inference efficiently.
To the best of our knowledge, our approach is the first to simultaneously conduct Web classification and Web in-formation extraction by a unified graphical model. Specif-ically, we make the following contributions: 1) A mutually beneficial integration of Web classification and Web infor-mation extraction. This method explicitly consider both the forward and backward dependencies between Web classifica-tion and Web information extraction. It shows significant improvement on the F1 value of these two steps. 2) An empirical study of this combination method on the task of online course search. Two baseline methods are proposed and compared with our uniform graphical model. 3) The online course search engine OfCourse . It is built based on the method proposed in this paper and it is fully functional for public access now. It currently contains over 60, 000 courses from the top 50 schools in the US.
In the following we use our work in online course search as an example to formulate the problem of combining Web classification and Web information extraction.
In this work Web classification is to identify the course homepages in a given group of Web pages and Web infor-mation extraction is to extract the course metadata from the course homepages. Here, the course homepages, whose contents are provided by the teachers of the courses, include all the information about the course. Figure 2 gives two example segments of course homepages. In this paper, we consider three types of course metadata:  X  Course Title: the name of the course.  X  Course ID: the identifier of the course, often provided by the university or the department.  X  Course Time: the semester or time of the year for the course (it is not the time of the week when the lectures are offered, like Tuesday and Thursday).

These three kinds of course metadata in Figure 2(a) are  X  X ata Structures X ,  X  X CS 110 X , and  X  X pring 1997 X  respec-tively, while those in Figure 2(b) are  X  X ntroductory Microe-conomics X ,  X  X conomics 304K X , and  X  X pring 2000 X  respec-tively.

As shown by these two course homepages, course ID &amp; time usually conform to certain patterns, which can be ex-pressed by regular expressions (the patterns for course ID &amp; time, and their extracting process will be detailed later in Section 4.3). However, since online course pages are writ-ten in different formats by different authors and they cover a variety of disciplines, it is impossible to manually com-pose rules in terms of functions of dozens of features to ex-tract course titles. Although a course title is usually shown prominently in terms of both position and format the de-gree of prominence for course titles can not be accurately defined by simple rules. For example, one might think that a course title is the field with the biggest font size in the course homepage. The example in Figure 2(b) actually invalidates this assumption. In this course homepage the field with the biggest font size is the course ID  X  X conomics 304K X , while the course title  X  X ntroductory Microeconomics X  is below it in a smaller font. Therefore, in this work we adopt the statistic models proposed in this paper to extract course titles, and use pattern matching to extract course ID &amp; time. The rea-sons why we do not incorporate the extraction of course ID &amp; time into a unified graphical model will also be detailed in Section 4.3. Usually, the input HTML Web page is represented in a DOM tree, and a leaf node in the DOM tree contains text and format information (in the following we call a leaf node in the DOM tree a DOM node for short). Here, we assume that a DOM node may correspond to only one kind of meta-data. Although this assumption is true in most cases there are some exceptions as shown in Figure 2, which requires some preprocessing. The text  X  X CS -Data Structures -Spring 1997 (Sections A &amp; B) X  inside the red rectangle in Figure 2(a) is a DOM node, which contains all of the three kinds of course metadata: ID ( X  X CS 110 X ), title ( X  X ata Structures X ) and time ( X  X pring 1997 X ). In this case, we have to split such a DOM node into multiple units. We con-duct node splitting by turning all the candidates of course ID &amp; time, which match the course ID &amp; time patterns, into a new DOM node.

Therefore, a Web page can be represented as x, x 1 ,  X  X  X  , x where x is the input (observed) variable corresponding to the whole page and x 1 ,  X  X  X  , x k are the k DOM nodes after preprocessing in this page. Based on the representation of a Web page, we can now formally define the concepts of Web Classification and Web Information Extraction as follows. Definition 1. ( Web Classification ) Given a Web page, Web classification is the task of assigning a label to this page from a pre-defined set of labels.
 Definition 2. ( Web Information Extraction ) For all the DOM nodes after preprocessing, information extraction is the process of assigning metadata labels to these nodes.
The problem of joint optimization for both Web classifi-cation and Web information extraction is defined as follows.
Definition 3. ( Joint Optimization of Web classification and Web Information Extraction ) Given a Web page repre-sented as ~x = x, x 1 ,  X  X  X  , x k , where x is the input variable corresponding to the whole page and x 1 ,  X  X  X  , x k are the in-put variable of DOM nodes after preprocessing. Let ~y = y, y 1 ,  X  X  X  , y k be one possible label assignment for x, x By the principle of Maximum A Posteriori (MAP) the goal of Web classification and Web information extraction is to identify the label assignment y  X  such that Note that the label sets for classification and information extraction are different. Specifically, the label set for y con-tains all the categories of Web pages, while the label set for y 1 ,  X  X  X  , y k contains all types of metadata. Based on the above definitions, the main difficulty is the calculation of the conditional probability P ( ~y | ~x ). In the next section we intro-duce a model of CRFs with constrained output to compute this probability. Figure 3: The graphical model for combing Web classification and Web information extraction In this section, we first introduce some basic concepts of CRFs and its simplest version for the traditional classifi-cation problems with a single output variable. Then, we describe our proposed model for integrating Web classifica-tion and Web information extraction and give the method to learn the parameters and perform prediction.
CRFs [5] is an undirected graphical model that is globally conditioned on observations. Let g = ( ~v,~e ) be an undirected graph, where the node set can be partitioned into two groups ~x and ~y . ~x are input variables over the observations, and ~y are output variables over the corresponding labels. The con-ditional distribution of the labels ~y given the observations ~x has the form, where C is the set of all the cliques in G , ~y c and ~x c the components of ~y and ~x respectively,  X  c is a potential function with non-negative real values, and Z ( ~x ) is the nor-malization factor with the form, The potential function can be expressed in terms of the fea-ture functions q i ( ~x c , ~y c ) and their weights  X  i as Let ~x = x and ~y = y ( ~x and ~y are both single-variable vectors), then P ( y | x ) is simplified to When y is a discrete random variable, Equation 5, which is equivalent to Logistic Regression [3], can be used for classi-fication.
In the graphical model of Figure 3 (in this figure an open circle indicates that the variable is not generated by the model, and the edges among y 1 ,  X  X  X  , y k are omitted), x and y are the observed and the output variables for a Web page respectively. The label space of y contains all the possible class labels for Web classification, and in our application, y =  X  1, indicating whether it is a course homepage or not. x ( j = 1 ,  X  X  X  , k ) is the observed variable corresponding to a DOM node in the Web page x , and y j ( j = 1 ,  X  X  X  , k ) is the output variable of x j . The label space of y j contains all the possible labels for information extraction, and in our application, y j =  X  1, indicating whether it is a course title or not. Then, the conditional probability can be expressed as where and f i is the feature function applied to x and y for Web classification, g i is the feature function applied to x j on the j -th DOM node for Web information extraction ( j = 1 ,  X  X  X  , k ), and h i is the feature function on all the output variables y, y 1 ,  X  X  X  , y k . The h functions explicitly considers the dependency between y and the set of y 1 ,  X  X  X  , y k . This dependency is denoted by the lines between y and the set of y 1 ,  X  X  X  , y k in Figure 3. This way, the h functions further bridge the labels of the Web page and all the DOM nodes, and Equations (6) and (7) become a general model for both Web classification and information extraction.
Given a set of labeled data { ( ~x l , ~y l ) } n l =1 , the model pa-rameters ~ X  , ~  X  and ~ X  can be estimated under the principle of Maximum A-Posteriori . Specifically, with the Gaussian prior the model learning problem is to find the models that maximize: This concave function can be globally optimized by the non-linear optimization methods, such as L-BFGS [6] (we adopt this method in this paper). The gradient vectors on ~ X  , ~ and ~ X  are given as where E is defined in Equation (7), k l in Equation (10) is the number of the DOM nodes in the l -th Web pages.
Given the model, we are interested in finding the most probable configuration of ~y in the label space for a given instance ~x , as shown in Equation (1). This is the process of model inference.

The key problem in both model learning and inference is the computation of the normalization factor since the label space of ~y is usually exponential to the length of ~y . However, using the relation between the page type and the metadata we can significantly reduce the output label space. In our application of course homepage identification and course metadata extraction we assume that 1) a course homepage contains one and only one course title; 2) a non course homepage does not contain a course title.

These two assumptions are reasonable due to the following reasons. 1) Here, a course homepage is defined as the entry page of an online course, including all the information about the course. Thus, the course title is the indispensable ele-ment for a course homepage 2 . 2) A course homepage may contain the title string more than once, but we only con-sider the most prominent one in terms of format and place as the course title. 3) A non course homepage may talk about a course by mentioning the course name. Since the mentioned course name is not the key information in a non course homepage, it does not serve the purpose of a title by highlighting the format and position. So we do not consider this appearance of a course name as the metadata we want to extract. 4) A course listing page may list all the informa-tion of several different courses, including their titles, IDs and time. A course listing page is not considered a course homepage in this paper.

Thus, under these assumptions the label space of y, y 1 ,  X  X  X  , y has only ( k + 1) elements: With this reduced label space the denominator is efficiently tractable. Therefore, exact inference can be achieved in this model.

In our project we only define two h functions: h ( y, y 1 ,  X  X  X  , y k ) = h ( y, y 1 ,  X  X  X  , y k ) = These two h functions are actually defined to express the correlations between course homepage and course title.
Different networked CRFs models, such as 2D CRFs [13], and hierarchical CRFs [14], have been proposed according to the different structures in ~y and ~x . In some of these models the computation of the normalization factor of Equation 3 is often intractable, thus, approximate inference is often re-quired in their inference process. However, a recent work
There do exist some rare course homepages, which do not contain a course title on them. We ignore these course home-pages in this work. by A. Kulesza and F. Pereira [4] shows that the learning of structured models can fail even with an approximate infer-ence method with rigorous approximation guarantees. The proposed model of CRFs injects the constraints among out-put variables deep into it. This way, we greatly reduce the label space of ~y , and thus enable exact inference to achieve optimal result.

The way to constrain the output label space is application specific, but we believe that this idea is generally applicable to a wide range of applications. For example, in another project, we are trying to classify Web pages as news, blog, product, recipe, and so on, and also extract metadata from the pages. The metadata for different type of pages are dif-ferent. For example, we want to extract the title, author, and date from news pages but product name, picture, and price from product pages. If we use the model in this pa-per to conduct classification and extraction jointly, we can use the relation and constraints between the page type and metadata to significantly reduce the output label space to make the problem tractable.
Since a Web page and a DOM node are different entities with different granularity we use two groups of features to characterize them separately.
The course homepages usually contain some course terms, e.g.,  X  instructor  X ,  X  teaching assistant  X ,  X  course introduc-tion  X ,  X  course scheduling  X ,  X  syllabus  X  etc. These terms are the key features to distinguish course homepages from non course homepages. Additionally, we find that these terms often appear at a conspicuous place with some HTML for-matting in a course homepage. To filter out noisy terms, the HTML format and position of a text node are consid-ered. Specifically, we use the following heuristics to extract the candidates of course terms: 1) the course term candi-date cannot be too long in terms of the number of English words in it; 2) the course term candidate cannot be located in the right block, top block and bottom block of a Web page; 3) the course term candidate is usually conspicuous in the web page in terms of its format (e.g., header decoration, bold and all capital etc.); 4) a term ending with a colon is a course term candidate.

Furthermore, we manually collected a collection of course terms and partitioned them into multiple groups. The course terms in the same group have similar semantics. Table 1 shows some groups of similar course terms. Each group of course terms corresponds to a feature for Web classification, and this feature is set to true when a course term candidate, extracted by the above heuristics from the Web page, ap-pears in the corresponding term group. Some preliminary experiments show that the proposed features for Web classi-fication is much better than treating the Web page as a bag of words.
To extract course title we use as many effective features as possible. All these features can be divided into three groups: format features , position features and content features . The format features, which are about font size, font color, dec-oration type and so on, are used to describe the character-istics that course title is usually the most prominent field in the whole Web page or it is conspicuous locally with a different format from its neighboring texts. The position features are used to capture the heuristic that course title is often located in a noticeable position, such as the top half region of the Web page. The format features and position features are application-independent , however, the content features are application-dependent . Some typical character-istics addressed by the content features include: 1) course title cannot be too short or too long in terms of the number of English words in it; 2) course title cannot be the follow-ing expressions: Email, URL, Telephone, Time, combina-tions of letters and numbers; 3) Course title usually has a course ID or course time around it. Altogether we induce 34 features for course title extraction, including 29 application-independent features and 5 application-dependent features. The details of these features are omitted due to the space limitation.
In general, a course ID is a combination of letters and numbers. Usually, its starting letter string is 1) the first letter of each keyword in the course title, 2) the name of the course discipline, 3) the abbreviation of the course disci-pline, 4) combination of 2) or 3) for multi-discipline courses. Following this starting letter string is the course code, com-prising of numbers, letters, or delimiters. A course time usually contains information about year, semester, season or month. Therefore, Course ID &amp; time can be extracted by matching these patterns. However, not all fields that match the specified patterns are course ID or course time (for ex-ample, other time information, such as publication time and exam time, may appear in the course homepage). That is, matching a pattern is a necessary condition but not suffi-cient to extract course ID &amp; time. Nevertheless, we notice that course ID, course time and course title usually appear closely in a local area of the course homepage. Thus, we can leverage this heuristic to identify the true course ID &amp; time among all the course ID &amp; time candidates.

Based on the above analysis, we can extract course meta-data as follows: 1) Find all the candidates of course ID &amp; time, which match the corresponding patterns. 2) Extract course title by the proposed graphical model. 3) If Step 2 recognizes the given page as a course homepage and ob-tains the corresponding course title, identify the course ID &amp; time candidates which are closest to the extracted course title, and output them as the extracted course ID &amp; time. Note that we do not incorporate the extraction of course ID &amp; time into the graphical model. This is because: 1) As mentioned before, the candidates of course ID &amp; time can be easily extracted by pattern matching. 2) Excluding course ID &amp; time in the graphical model further simplify the label space of y 1 ,  X  X  X  , y k , thus help to achieve exact inference in model training and prediction. 3) We notice that there exist some correlations among course title, time and ID. Specif-ically, course ID, time and title usually appear closely in a local area of the course homepage. To take this point into account we introduce a feature for course title extraction to check whether a DOM node has a neighbor that is a course ID or time candidate. We believe that this method has the same effect as adding course ID &amp; time to the label set of course metadata and defining the feature functions on the labels of neighboring DOM nodes. Therefore, this technique reduces the complexity of the graphical model without sac-rificing performance.

This approach of using pattern-based method to extract some metadata and using machine learning to extract the rest is generally applicable in many applications. For exam-ple, if we want to build a vertical search engine for product search, the product metadata usually include product name, model number, price, and availability. Here, the model num-ber, price, and availability can be characterized by patterns. Specifically, model number is a short sequence of characters, numbers, and special characters like  X - X ; price usually fol-lows the $ sign and has two digits after the decimal point, and availability usually contains descriptions like  X  X n stock X ,  X  X ut of stock X ,  X  X vailable X ,  X  X hips in 1-2 business days X . On the other hand, the product name cannot be character-ized by patterns and is better extracted by machine learning methods.
We compare our approach with two baseline methods. For each baseline method, we detail its training and prediction processes. For training, we are given a set D of labeled Web pages, including a set D c of course homepages and a set D of non course homepages. For each course homepage in D c its course title is also labeled. In these baseline methods the classification model in Equation (5), which is equivalent to Logistic Regression, is adopted.
In this method we train two classifiers, C c and C e , for Web classification and Web information extraction respectively. C c can use the features for Web classification (detailed in Section 4.1) to give the probability that a Web page is a course homepage, while C e can use the features for Web information extraction (detailed in Section 4.2) to give the probability that a DOM node is a course title. Specifically, we use all the labeled Web pages in D to train C c , and we use the labeled DOM nodes from the course homepages in D c to train C e .

In the prediction process we first use C c to identify all the course homepages. Then, for each identified course home-page we use C e to output the probability of each DOM node in this page that it is a course title. Finally, we select the DOM node with the largest probability in a course home-page as the node for the course title.

Since each course homepage contains only one course ti-tle (positive instance) while all of the rest are negative in-stances, the training data set for C e is highly unbalanced. However, in Section 6.1 we will show that this property of unbalanced data does not affect the performance of course title extraction from the course homepages. In this method we train the same two classifiers, C c and C , as those in Section 5.1. In the prediction process, given a Web page x, x 1 ,  X  X  X  , x k , we use a joint inference similar to the one in Section 3.3 to assign their labels. Specifically, we use the conditional probability in Equation (6), in which the function E ( ~y, ~x ) is replaced by where ~ X  and ~  X  are actually the parameters in C c and C  X  &gt; 0 is the weight to balance the tradeoffs between the two models. The larger the value of  X  is, the more influence the information extraction model C e has on the final result. The smaller the value of  X  is, the more influence the classification model C c has on the final result. In this formulation we ignore the h functions in Equation (7).

In summary, the method in Section 5.1 performs local training and separate inference, while the method in Sec-tion 5.2 conducts local training and joint inference. Com-pared with these baseline methods our approach conducts joint learning in the training process and joint inference in the prediction process.
In this section, we report empirical results by applying our joint model to perform classification and information extraction simultaneously. We compare our approach with the two baseline methods, detailed in Section 5. For the two classifiers C c and C e used in the two baseline methods, we carefully tune their parameters used in the training process and compare their best performances with those of the pro-posed model. The results show that our model significantly improves both Web classification and Web information ex-traction.

To evaluate all these methods, we collected the labeled data set D , including the set D c of 530 course homepages and the set D n of 1200 non course pages, and manually an-notated the course title, ID and time for each course home-page.

For both Web classification and Web information extrac-tion, we use the standard accuracy , precision , recall and F1 measures to evaluate these methods. For course homepage identification these metrics are calculated using all the data in our data set D . For course title extraction the evaluation data sets are different, depending on the extraction method and the situation. Specifically, when evaluating the perfor-mance of C e in course title extraction, we assume that the inputs of C e are all course homepages. Thus, the correspond-ing evaluation metrics are calculated within all the course homepages. However, when evaluating the joint model for course title extraction, the inputs to this model include both course homepages and non course pages, so the evaluation is conducted with both types of pages. Since our goal is to find all the course titles accurately from a mixture of course homepages and non course pages, the metrics cal-culated within this mixed data set are the most important Table 2: The experimental results of C c for course homepage identification Figure 4: The accuracy of C e for course title extrac-tion from course homepages measures we care about. Since we aim to show the effec-tiveness of the proposed graphical model we omit the ex-perimental results of course ID &amp; time extraction in this section. Note also that all the experimental results in this section are obtained from 10-fold cross validation.
The evaluation results of the classifier C c for course home-page identification are listed in Table 2, where  X  is the pa-rameter on the penalty term to compensate for the over-fitting of complex models. These results show that 1) when  X  = 0 it achieves the best result of F1 value; 2) Along with the increase of  X  values, the precision increases while the recall decreases. We have conducted more theoretical anal-ysis on the relationship between  X  and the performances of precision and recall, however, it is out of the scope of this paper.

We evaluate the performance of C e for course title extrac-tion under the assumption that C e performs only on course homepages. Thus, we use only the 530 course main pages to conduct the 10-fold cross validation. Note again that the DOM tree for each page contains only one node that is the course title (the positive instance) but many more (several hundred) negative instances, so the data set is very unbalanced. To alleviate this problem, we under-sample the negative instances by a certain ratio p in each round of cross validation, but keep all the positive instances. The exper-iments in this part aim to answer the following questions: 1) What is the relationship between the sampling propor-tion p and the extraction accuracy? 2) How much does the extraction accuracy of course title improve with the 5 application-dependent features?
To answer these questions, we measure the performance of C e with two groups of features (all of the 34 features and the 29 application-independent features only) and ten dif-ferent under-sampling ratios: 0 . 1 , 0 . 2 ,  X  X  X  , 1. The results are shown in Figure 4. we give the experimental findings and the corresponding analysis as follows: 1) The application-dependent features significantly improve the extraction ac-curacy of course title. This improvement is independent of the value of the under-sampling ratio. On average, the per-formance of C e increases 8.72% by adding the application-dependent features. 2) C e yields almost the same perfor-mance under different under-sampling ratio. The reason for this is that different degrees of data imbalance may change the absolute value of probability that a node is a course title, however, it will not change the ranking of the nodes in the decrease order of this probability. Therefore, the node with the biggest probability value in a course Web page remains the same under different imbalance settings.

Note that in the above experiments the parameter  X  used in training C e is set to 0, and we found that different values of this parameter give similar results.
In this subsection we evaluate the performance of the joint optimization model and the two baseline methods for course homepage identification and course title extraction given a mixture of course homepages and non course pages. Thus, we use all the Web pages in D to conduct the 10-fold cross validation.

As shown in Sections 6.1, C c and C e achieve their best performances respectively when the parameters  X  are both set to 0 in their training. Thus, we adopt these parameter settings in training C c and C e for the two baseline methods. Furthermore, for the second baseline method we evaluate its performance under different values of the tradeoff parame-ter  X  in Equation 12. The results show that setting  X  to 1 achieves the best F1 value for both course homepage iden-tification and course title extraction. Thus, we only report this best performance for the second baseline method.
The performances of these two baseline methods are shown in Table 3. It shows that 1) the first method is slightly bet-ter than the second one in term of the F1 value for course title extraction; 2) the second method increases the recall for course title extraction at the cost of precision.
The performances of our joint optimization method with different settings of  X  1 ,  X  2 ,  X  3 in Equation 8 are recorded in Table 4. We also conduct the t -test on the results from the 10-fold cross validation to check whether the proposed model significantly outperforms the baseline methods with a 95% confidence interval. The results show that 1) under all these parameter settings the proposed method is significantly bet-ter than the two baseline methods in terms of the F1 values for both course homepage identification and course title ex-traction; 2) under all the parameter settings this joint model significantly outperforms the first baseline method in terms of all the evaluation metrics, including precision, recall and F1 value; 3) the parameter setting (1,1,0.001) achieves the best performance of course title extraction in terms of F1 value. In this parameter setting we intentionally set  X  3 parameter on the h functions) to a very small value in order to increase the effect of the h functions in this model. It shows that this setting slightly improves the performance of this model.
We have applied this combination model to the OfCourse portal for online course search. This portal currently con-tains over 60, 000 courses from the top 50 schools in the US. The current portal supports basic keyword search, ad-vanced search, and browsing by subject areas. In the ad-vanced search, the search can be conducted within the scope of the user-specified university and year. For each course in the search result, the extracted course title is displayed, to-gether with the school, the extracted year, and an excerpt from the course page containing the search term. The course title links to the course homepage on the Web. These func-tions are all supported by the combination model proposed in this paper. Some user studies show that these results of classification and information extraction are quite satisfac-tory. Additionally, the totally automatic process of course homepages identification and course metadata extraction al-lows us to easily make our portal an open system, in which any user can add new courses into the portal by only sub-mitting the URLs. The model behind can identify if the submitted URL is a course homepage and simultaneously extract its course metadata if it is. Interested users can ex-perience the effectiveness of our joint model by submitting any URL at the OfCourse 3 portal.
The works most related to course title extraction are title extraction from Web pages [12]. The assumption used in [12] is that a general title is usually the most conspicuous field in any Web page. However, the likelihood that this assumption is true is much lower for the course titles of online courses. To achieve high precision in course metadata extraction we propose some application-dependent features, which implic-itly consider the relationships among the features and labels of the neighboring DOM nodes. http://fusion.hpl.hp.com/OfCourse/addNewCourse.jsp
All of these previous works perform under the assump-tion that we have a perfect classifier, which can precisely and completely identify the target Web pages (always con-taining the metadata we want to extract) from a mixture of Web pages. However, this assumption is not always true. The motivation examples in Section 1.1 intuitively show the room of improvement in Web classification by considering the backward dependency from Web information extraction to Web classification. Thus, we seamlessly combine these two tasks into one graphical model to mutually boost their performances.

The methodology to combine multiple mutually-dependent tasks into one statistical graphical model appears in some previous works for different applications. Roth et al. focus on the topic of learning and inference over structured and constrained output with the applications of natural language processing [11, 9]. The work in [11] combines the tasks of entity and relation recognition, in which separate classifiers are first trained for entities and relations, and then these lo-cal classifiers are used to make global inferences for the most probable assignment for all entities and relations of interest. It actually motivates the baseline method in Section 5.2. Another related work is the integration of data record de-tection and attribute labeling for online product extracting from product Web pages by Hierarchical CRFs [14]. Both the learning and inference in this work are jointly optimized. The work in this paper further emphasizes the difference be-tween local and joint training, and the difference between local and joint inference. A recent paper [1] from Bhat-tacharya et al. deals with the combination of structured en-tity identification and document categorization. The com-bined two tasks in that paper are different from those in this paper. Additionally, they use the generative model in training and prediction, however, we use the discriminative model CRFs to achieve the combination.
This paper discussed the problem of combining Web clas-sification and Web information extraction, which are actu-ally two coupled steps for Web content analysis. Previous works in this area treat them as two separate and sequen-tial steps, which consider the forward dependency from Web classification to Web information extraction, but ignore the backward dependency from Web information extraction to Web classification. To mutually boost these two steps, we propose to combine them by using a model of CRFs. Specif-ically, this graphical model contains two kinds of observed random variables with different granularity: the variable of the whole Web page for Web classification, and the variables of the DOM nodes inside the Web page for Web informa-tion extraction. This way these two steps are formulated as a joint task about assigning labels to these variables. Ad-ditionally, the constraints among the two kinds of random variables can be utilized to simplify the learning and infer-ence process of this model. The experiments in our project OfCourse for online course search show that our joint model significantly outperforms the two baseline methods in terms of F1 value. Even though this approach is designed to solve the specific problem of course homepage identification and course metadata extraction, the methodology of combin-ing Web classification and Web information extraction is problem-independent and can be applied to many other ap-plications.
The authors thank Baoyao Zhou for providing part of the test data and a Web page segmentation tool. We are also grateful to Shicong Feng and Liwei Zheng for helpful collab-orations in the OfCourse project. The authors Fen Lin and Zhongzhi Shi are partially supported by the National Science Foundation of China (60775035), 863 National High-Tech Program (No.2007AA01Z132), and National Basic Research Priorities Programme (No. 2003CB317004, 2007CB311004). [1] I. Bhattacharya, S. Godbole, and S. Joshi. Structured [2] M. Castellanos, Q. Chen, U. Dayal, M. Hsu, [3] D. Hosmer and S. Lemeshow. Applied Logistic [4] A. Kulesza and F. Pereira. Structured learning with [5] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [6] D. C. Liu and J. Nocedal. On the limited memory bfgs [7] A. McCallum. Information extraction: Distilling [8] Z. Nie, J. Wen, and W. Ma. Object-level vertical [9] V. Punyakanok, D. Roth, W. Yih, and D. Zimak. [10] J. Rennie and A. McCallum. Using reinforcement [11] D. Roth and W. Yih. Probabilistic reasoning for entity [12] Y. Xue, Y. Hu, G. Xin, R. Song, S. Shi, Y. Cao, C.-Y. [13] J. Zhu, Z. Nie, J.-R. Wen, B. Zhang, and W.-Y. Ma. [14] J. Zhu, Z. Nie, J.-R. Wen, B. Zhang, and W.-Y. Ma.
