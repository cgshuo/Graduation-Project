 We are interested in organizing a continuous stream of sparse and noisy texts, known as  X  X weets X , in real time into an on-tology of hundreds of topics with measurable and stringently high precision. This inference is performed over a full-scale stream of Twitter data, whose statistical distribution evolves rapidly over time. The implementation in an industrial set-ting with the potential of affecting and being visible to real users made it necessary to overcome a host of practical chal-lenges. We present a spectrum of topic modeling techniques that contribute to a deployed system. These include non-topical tweet detection, automatic labeled data acquisition, evaluation with human computation, diagnostic and correc-tive learning and, most importantly, high-precision topic in-ference. The latter represents a novel two-stage training algorithm for tweet text classification and a close-loop infer-ence mechanism for combining texts with additional sources of information. The resulting system achieves 93% precision at substantial overall coverage.
 Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning
Twitter [1] is a global, public, distributed and real-time social and information network in which users post short messages called  X  tweets  X . Users on Twitter follow other users to form a network such that a user receives all the tweets posted by the users he follows. Tweets are restricted to contain no more than 140 characters of text, including any links. This constraint fosters immense creativity leading to many diverse types of styles and information carried in the tweets. As of early 2014, Twitter has more than 240 million monthly active users all over the world. These users send more than 500 million tweets every day, which corresponds to an average of 5700 tweets per second, with spikes at up to 25 times of that velocity during special events [2].
Tweets generated from all over the world are expected to be about a variety of topics. Figuring out exactly which tweet is about which topic(s) is interesting to us at Twit-ter in our goal to serve our users better as it enables per-sonalization, discovery, targeted recommendations, organi-zation of content, as well as aiding in studies and analyses to gain better insights into our platform as a whole. Such broad usefulness, and the potential of affecting and being visible to real users, however, propose demanding precision requirement. In this paper, we consider the problem of high-precision topic modeling of tweets in real-time as they are flowing through the network. This task raises a set of unique challenges given Twitter X  X  scale and the short, noisy and am-biguous nature of tweets. We present how we address these challenges via a unique collection of techniques that we have employed in order to build a real-time high-precision tweet topic modeling system that is currently deployed in produc-tion inside Twitter. Our system achieves 93% precision on  X  300 topics and 37% overall coverage on English tweets.
Given a tweet, and all the information associated with it (e.g., author, entities, URLs, engagements, context), we are interested in figuring out what topic(s) this tweet is about.
Topic modeling tasks have been commonly approached with unsupervised clustering algorithms (e.g., k -means, pLSI and LDA [5]), information filtering approaches (e.g., lan-guage models [19]), or weakly supervised models (e.g., su-pervised LDA [5], labeled LDA [22]). These approaches are effective in grouping documents into a predefined number of coarse clusters based on inter-document similarity or the co-occurrence patterns of terms (with help from very mild supervision). They are also cheap to train as no or very few labeled data is required. Nevertheless, they are not suit-able for our use because it is very difficult to align the topic clusters produced by these approaches to a predefined on-tology and perform low-latency topic inference with measur-able and decent precision. In fact, none of these approaches could attain the precision close to what we demand.
Instead, we primarily consider supervised approaches that classifying tweets into our taxonomy with controllable high-precision. Figure 1 illustrates the overview of our system. On the training path, our  X  X raining data collector X  constantly listens to the tweet stream to automatically acquire labeled training data, which, once accumulated to a certain amount, are fed into the  X  X rainer X  module to produce classification models; These models are then validated and calibrated to be used in our service. On the operation path, the classifier Figure 1: Overview of tweet topic inference system. models are replicated and served in parallel, together with knowledge about users and entities, in a  X  X ntegrative infer-ence X  module to provide high-precision topic annotation of tweets in real time. The inference results are used to gener-ate and refine the user-and-entity knowledge, making this a close-loop inference. The results are also scribed and used to evaluate and monitor the quality performance with help from crowd-sourced human annotation. Once the service is exposed to the user, we also employ a  X  X eedback collector X  and a  X  X iagnosis module X  to gather high-precision user cu-rated data, which are fed back into the trainer module to fine-tune the trained models.
 Related Works &amp; Contributions. Topic modeling of tweets has been examined extensively in the literature. Most existing research has been based on un-or weekly supervised techniques such as variations of LDA [17, 27]. The inference in these approaches usually takes considerable latency, and the results are not directly controllable when precision is concerned. Recent studies employ information filtering ap-proaches to detect tweets on specific topics of interests[19, 9]. These approaches usually have low latency and can perform inference in real-time; and when configured properly, the top-ranked results obtained by them can usually achieve a reasonable precision target. Nonetheless, because they iden-tify only a small fraction of tweets that are apparently rel-evant to a given topic while disregarding the majority rest, the recall and coverage of these approaches are very poor, yielding heavily biased inference results that are less useful in many of our use cases. Information filtering approaches are also only good at specific topics, usually topics of rela-tively focused meanings (e.g.,  X  X an Francisco X  and  X  X BA X ), and not directly applicable to a broad spectrum of topics such as an ontology, as what we study here. To the best of our knowledge, this is the first published documentation of a deployed topic modeling system that infers topics of short noisy texts at high precision in real-time.
We create a taxonomy represented as a fixed (and poten-tially deep) ontology to encode the structure and knowledge about the topics of our interest. The goal is to find a topic structure that is as complete as possible to cover as many semantic concepts (and their relationships) as we can, while focusing on topics that are frequently discussed on Twitter Figure 2: A subtree of our taxonomy. The deepest node is 6 hops way from the root node  X  X op X . and are less likely to be transient. Starting from existing tax-onomies such as ODP and Freebase, we undertook a num-ber of exploratory approaches and manual curations. For example, by building topic filters based on ontology corpora (e.g., ODP, Wikipedia), we can estimate coverage of top-ics on Twitter and trim or merge topics that are too niche; unsupervised models such as LDA were also used to map clusters to well defined ontology (e.g., ODP, Freebase) in order to estimate coverage. This process has been iterated through multiple refinements and over different time periods to avoid bias. The current taxonomy consists of  X  300 topics with a maximum of 6 levels, a fragment of which is shown in Figure 2.
A tweet can be associated with multiple modalities of in-formation such as text, named entities, users (the author and people who engaged with the tweet), etc. In this section, we consider topic classification of tweets based solely on its tex-tual features, which will be combined with other sources of information for integrative topic inference in Section 5.
High-precision tweet classification faces unique challenges because unlike ordinary documents, tweets are much sparser and noisier and the scale is daunting. The demanding preci-sion criteria make this even more challenging. A tweet may belong to multiple topical categories at the same time, or be non-topical, which makes this a multi-label classification problem. To meet quality criteria, the participating classi-fier needs to be abstaining, i.e., providing topic labels only if the expected precision or confidence is sufficiently high.
One key challenge for high-precision tweet classification is that a substantial proportion of tweets are non-topical [3], or at least their topics are not clear in the context of a single tweet. We use the term chatter to denote tweets that often primarily carry emotions, feelings or are otherwise related to personal status updates. While they are clearly an important part of user expression on Twitter, it is necessary to reject classifying these tweets (e.g., to save system latency and to control quality) and restrict the domain of interest to tweets that are not chatter. To that end, we build content filters to eliminate chatter tweets, as described in previous work [3].
Training high-quality tweet classification requires high-quality labeled data. Use of human annotation is prohibitive for a number of reasons. First, it is too expensive: for the scale we consider (i.e., 300 topics, millions of features) and the sparseness (i.e., only  X  10s feature presence per tweet), any reliable estimation would require hundreds of millions of labeled tweets and cost millions of dollars even with the cheapest crowd-sourced service in the market. Moreover, even human-assigned labels could be too noisy. A big tax-onomy like ours presents considerable cognitive overload to human annotators, for example, when asked to assign rele-vant topic labels (out of the 300 candidates) to each tweet, human raters turn to identify only a fraction of the true pos-itive labels. While it is possible to obtain samples of data within each category, such samples are likely to be biased and to the extent that some of these instances also belong to other categories, most of these memberships will remain unknown. Human labeled data are also subject to expertise bias because a lot of niche topics (e.g.,  X  X ata mining X ) ex-ceed the literacy of crowd-source workers, forcing them to respond with random-guess annotations. Because of these concerns, in our system, human labeled data (with enhanced quality assurance) are only used in quality evaluation (Sec-tion 4.7). For model training, we devise scalable algorithms to collect labeled data automatically from unlabeled tweets.
The labeled data acquisition pipeline is illustrated as Fig-ure 3. First, tweets are streamed through a set of topic priors to prefilter tweets that are weakly relevant to each topic. These topic priors are white-list rules that include: User-level priors: users who tweet predominantly about Entity-level priors: unambiguous named entities and hash-URL-level priors: for tweets containing URLs, the URL We also leverage the social annotation functionality to ex-tract topic priors. Many Twitter users represent authorities on a small set of topics by virtue of their profession, experi-ence, and the fact that they tend to publish on those topics on/off Twittter. This is recognized by other Twitter users who sometimes group users  X  X nown for X  a particular topic using Twitter X  X  List feature.

The data coming through the topic prior filters are only weakly relevant to each topic and thus very noisy. To obtain high-quality positive data for each topic, we employ a co-training [6] based data cleaning algorithm. In particular, we consider only these tweets containing URLs, and iteratively apply the following for each topic c : 1. train a webpage classifier to remove tweets whose asso-2. train a tweet classifier to remove URLs whose associ-The key assumption here is that when a URL is embedded in a tweet, the webpage that the URL links to is highly likely to be on the same topics as the tweet. To make this proce-dure more scalable and less coupled with the final classifier models, we use Naive Bayes classifiers in this step.
Once we have positive data, for most topics, negative ex-amples are very easy to collect, e.g., by using large-amount of randomly sampled tweets or using  X  one-vs-all  X  splitting of the positive data. However, for a considerable number of rel-atively broad topics that receive high velocity of coverage on Twitter (e.g., X  X ports X ,  X  X echnology X  and  X  X ews X ), randomly sampled negatives are very noisy. One-vs-all doesn X  X  work well for topics that are intercorrelated (e.g.,  X  X ports X  and  X  X asketball X ), because semantically equivalent documents can be presented to a learner as both positive and nega-tive examples. To this end, we employ algorithms devel-oped for learning from positive and unlabeled data (or PU-learning [12]) to acquire high-precision negative examples for each topic. In particular, we select tweets / webpages as negative instances for a particular topic only when the similarity scores with the centroid of that class are below (1-3 )-percentile, i.e., via a Rocchio classifier [20].
The effectiveness and efficiency in feature extraction are vitally important to the success of our system given our scale and quality requirement. Conventional unigram text fea-tures cannot meet our needs in both regards. First, tweets are short (bounded with 140 characters), and amount to merely  X  7 unigram terms (including common stop-words) on average  X  very difficult for any machine learning model to infer topics with high confidence. Second, traditional imple-mentation of unigram extraction can take considerable com-putational resources and dominate our system latency espe-cially for webpages which could exceed 10K terms, which is problematic as we require real-time scoring. The latter fac-tor is actually more critical and essentially prohibits us from using potentially more effective features such as n -grams, word2vec [21], or other time-consuming feature processing such as term pre-selection and stemming . Instead, in our system, we use the following two types of feature extrac-tions for tweets and webpages respectively: Binary hashed byte 4gram We use a circular extractor
Tweet texts are preprocessed at the time of scoring such that less meaningful strings such as URLs and @mentions are stripped off and all strings are converted to lower case. Hashed unigram frequency Term unigrams are extracted We found that these feature extractors achieve the best bal-ance of effectiveness and efficiency in our system.
Both tweet and webpage topic inference can be casted naturally as multi-class multi-label classification problems, making regularized logistic regression a perfect fit. Given a set of n training instances x i  X  R d together with their corresponding labels y i  X  { 1 , 2 ,...,C } , where C is the total number of topics, we consider two types of LR models: Multinomial logistic regression (MLR) also known as One-vs-all logistic regression (LR) models  X  X hether x Let l ( w,b | x,y ) be the log-likelihood of parameter ( w,b ) given example ( x,y ). To estimate the parameters ( w,b ), we mini-mize the negative log likelihood plus a penalty term: where the parameter  X  controls the strength of regulariza-tion. Here we consider the ElasticNet regularization which is a hybrid of ` 1 and ` 2 regularization types and includes both as special cases (when  X  takes 1 or 0). The non-differentiable nature of this regularization, when  X  &gt; 0, enforces sparsity of the model w , which is valuable to us because (1) a com-pacter model consumes less memory, loads faster and has better latency in real-time scoring; and (2) less useful (e.g., redundant or noisy) features will be automatically ruled out in training, which is important to us as we do not do at-tribute prepruning or stemming.

In our machine learning library, we provide a number of versatile model trainers on Hadoop, including distributed streaming training via SGD [18], and batch-mode training based on L-BFGS , conjugate gradient (CG) or coordinate descent (CD) algorithms. We also provide full regularization path sweeping for ` 1 and ElasticNet regularization.
The major difference between the two LR models lies in the Softmax function MLR employs to normalize the poste-rior scores p ( y | x ). There are both pros and cons for doing so. On the one hand, MLR provides comparable topic scores which enables us to apply max-a-posterior inference to in-crease topic coverage. As topics are competing against one Table 1: Streaming training (e.g., Pegasos) gen-erates models with lower AUCs than batch-mode training. Increasing the number of iterations (i.e., number of scans over the data) slowly improves the AUCs but makes the training time much longer. Table 2: Comparison of MLR and LR 1-vs-all classi-fiers, in terms of average AUC across topics and the % of topics with improved AUCs (%topic win).
 another, the inference results of MLR are less ambiguous (e.g., if topic c fires for a tweet x , topic c 0 6 = c is less likely to fire for x due to the  X  explaining-away  X  effect). On the other hand, however, MLR requires the label space to be exhaustive (i.e., a tweet must belong to at least one of the topics), and it discourages usage of examples with missing label, which is not perfectly compatible to our setting. Also, because topics are coupled with one another in MLR, it is hard to train models for multiple topics in parallel, or retrain model for a subset of topics without affecting others. Experiment Results. We use AUC (i.e., area under the ROC curve ) for model validation for its independence of the choice of decision threshold  X  evaluation of calibrated models at their desired thresholds are reported in Section 4.9. We test models on a held-out subset of the  X  X raining data ac-quisition X  output data, and estimate AUC with confidence intervals using standard Bootstrap . Because the standard errors are very small, we report here only the AUC scores.
Historically, our classifiers are trained in streaming-mode on Hadoop [18] via stochastic optimization, such as Pega-sos [26]. Streaming training consumes a constant amount of memory regardless of the scale of the training data, and it is very fast when the data is scanned through only once. However, we noticed that the model generated by stream-ing training are significantly worse than batch-mode trained ones (e.g., via L-BFGS, CD or CG). For example, on tweet classification (with Byte4Gram feature), Pegasos-trained mod-els, with 1 scan over the data, are 1.9% worse on average than batch-mode trained model, as seen in Table 1. Increas-ing the number of iterations helps to make the gap smaller, but at the same time, also makes the training time much longer (e.g., overhead in data I/O and communication). As we note in Section 4.10, we were able to scale up batch-mode training on massive data with distributed optimiza-tion. Hence, in the following experiments, the models are all trained in batch-mode unless noted explicitly.

Besides the advantages we mentioned previously, we found that 1-vs-all LR also performs better than MLR on both tweet and webpage classification. The results are summa-Figure 4: Effect of text length on LR classifier pos-terior score and precision of webpage classification. Left: longer texts receive higher posterior scores re-gardless of relevance; Right: precision drops as texts get longer when no feature normalization is used. rized in Table 2, where all the models use Byte4Gram fea-tures. On tweets, using LR improves the AUC on 86% of the topics and by 3.9% on average across all topics. Interest-ingly, the improvements on webpage is relatively small, i.e., only 1.7%. This is due to LR X  X  sensitivity to the variations in document lengths. Recall that here we use Byte4Gram features without normalization. When this is used in MLR, the softmax function automatically normalizes the posterior scores. However, in LR, if a text is getting longer, it gets more feature occurrences and in turn receives higher pos-terior scores regardless of the relevance of the documents. This effect is negligible on tweets as the lengths are bounded, but it is very dramatic on webpage and significantly dete-riorates the prediction quality (e.g., precision) on lengthier webpages, as shown in Figure 4. Indeed, when we added feature normalization to webpage classifier, we observed up to 1.5% more improvement on AUC, bringing the overall improvement to 3.2%. The comparative results on feature extractors 2 are summarized in Table 3. Because of their su-perior performance, we use HashedByte4Gram for tweets and Unigram-logTF-norm for webpages as default features.
Finally, using ElasticNet with regularization path sweep-ing (i.e., optimized  X  ), further improves average AUC by 0.75 X 0.98%, as shown in Table 4. Due to the sparse na-ture of tweets, we found that relatively dense models have slightly better AUCs  X  models that are too sparse tend to generalize poorly on Byte4Grams that are not seen in train-ing data. In our experiments, we found  X   X  0 . 05 provides the best trade-off between test set AUC and training speed (smaller  X  turns to slow down the regularization sweeping).
The ontological relations among topics, as defined by the hierarchical tree structure of our taxonomy, can be used to make our models smarter in learning classifiers for concep-tually related topics. We consider three approaches here: Label expansion This approach regularizes the learner by
For webpages, Byte4Gram consumes far more extraction time than Unigram and could slow down our system in real-time classification especially for very long documents. Table 3: Comparison of feature extractors, where *-norm denotes instance-level ` 1 -normalization.
 Table 4: Comparison of regularization type. Elastic-Net, with automatic regularization strength tuning, improves overall model AUC.
 Table 5: Using relational regularization significantly improve model quality.
 Cost-sensitive learning The structure of the taxonomy Hierarchical regularization [28, 15] We can encode the These approaches, while tackling relational regularization in three different aspects (i.e., data sharing, relational objec-tive, parameter sharing), usually achieve equivalent effects. Note that  X  X ost-sensitive optimization X  is more versatile as it can also handle label dependencies that are discovered from data (e.g., topic correlations) rather than prior knowledge. Experiment Results. In Table 5, we compared LR with-out relational regularization (denoted  X  X lat X ) vs the three approaches we described above. Relational regularization significantly improve model quality, e.g., over 2% boosts of average AUC. The differences among these three methods are nevertheless very small. For simplicity of implementa-tion, hereafter, we primarily use label expansion.
LR models return soft probabilistic scores p ( y | x ) in the range of [0 , 1]. In order to apply them to label the top-ics of tweets/webpages, we need to calibrate the models to-wards specific quality criteria in need, i.e., a precision target in our case. There are two key questions here: (1)  X  X ver what distribution should precision be measured? X  and (2)  X  X ow to find the optimal decision threshold? X . For precision measurement, stratified sampling in the range of posterior scores produced by a classifier has been advocated as a way to reduce the variance of precision estimates [4]. While this method is very effective once the decision threshold are set, it is not straightforward to use when one seeks to determine the optimum thresholds.

Given a topic c , a small seed set of positive examples P an unlimited stream of unlabeled data U c and a labeling oracle O , we use the following method to tune threshold  X  Initial threshold Use P c to estimate a rough lower bound Stratified sampling Apply the model to the output distri-Threshold estimation Let  X  j represent the posterior score Note that, other than the above one-side calibration ap-proach, a two-side approach is also possible, i.e., by ini-tially narrowing down the range with both upper and lower bounds and applying golden section search to find optimal  X  by progressively dividing bins (rather than fixed-size bins) and sub-stratified sampling. Although this two-side method can produce threshold estimations with arbitrary resolu-tions, we found in our experiments that one-side approach performs well enough and is much simpler to implement.
As we strive to achieve 90% or higher precision, high-quality gold standard data and accurate evaluation is critical to assess whether we achieve that goal. We use crowdsource human labeled data for quality evaluation. As we previously discussed in Section 4.2, human annotation is less accurate in a multi-label setting due to the cognitive overload caused
The boundaries of the bins can be defined based on the percentiles of the posterior scores of P c or U c . by any nontrivial taxonomy. We instead ask for confirma-tion labeling . That is, rather than presenting the whole tax-onomy to crowdsource workers and asking them to select whatever labels are relevant to a given tweet, we present a (tweet, topic) pair and ask them to provide binary answers to  X  X hether or not the supplied topic is correct for the given tweet X . As our primary goal is to estimate precision, binary judges on a sample output of our system are sufficient for our purposes. In principle, this protocol would require a lot more questions (i.e., labeling tasks) and in turn incur more costs if we were to estimate recall, e.g., for a given tweet and a 300-topic taxonomy, we need 300 labeling tasks in order to know the ground-truth topic of the tweet, compared to one task in the multi-label setting. Nevertheless, for precision assessment, this protocol has better quality assurance and it is more economical  X  because binary tasks are much eas-ier, we found crowdsource workers are more willing to take the tasks at a much lower price, and more importantly, they are less likely to provide mistaking judgments.

To effectively control cost, we assign each (tweet, topic)-pair sequentially to multiple workers; once the standard er-ror of the responses is below a certain threshold, the label is considered final and the task will be frozen without being assigned further. The quality of the response varies worker by worker. To ensure quality, when assigning tasks, we also incorporate a small set of randomly-sampled probe tasks for which we already know the true answers with high confi-dence. Those workers whose response accuracy consistently falls below our requirement will not be admitted for future tasks, and their response in the current batch are ignored.
Recall estimation faces challenges due to the nature of our labeling protocol, the needs for unbiased corpus and the fact that the distribution of our data evolves over time, which is the subject of another paper [7]. Instead, we use primarily precision and coverage for quality evaluation.
Once the system is deployed in production to label the topics of tweets, it is important to be able to identify cases where the model fails, and provide diagnostic insights as well as corrective actions to fix it on the flight. To this end, we instrumented a mechanism that associate topic labels as tags to tweets and expose them to the users. When users scroll on a topic tag, two buttons will show up to allow users to provide  X  X ight or wrong X  feedback about the topic tag. Clicking on a topic tag will also take you to a channel consisting of tweets all about that topic, which is useful for diagnosis of a topic model as tweets are flowing through. The UI of this feature is shown by the top chart of Figure 5. This feedback collection module makes it easy to exploit the wisdom of crowd in an ad hoc way to receive instantaneous feedback on the performance of our topic models and pro-vide opportunities to identify patterns for correction. Once failing cases or trouble areas are identified, another tool is used to visualize which parts of the tweet contributed to the offending decision the strongest, as shown in the bottom charts in Figure 5. This is useful to allow the modeler to manually zoom into the model and identify potential ove-fitting patterns. With all the diagnostic patterns, we then employ corrective learning [25, 23] to correct the mistakes of an existing topic classifier either manually or automati-cally on the flight. Corrective learning is used because it is desirable to adjust models gradually using a relatively small Figure 5: The diagnosis, corrective learning and feedback collection modules.Top: the feedback col-lection UI; Bottom: the corrective learning UI (Left), and visualization of an example tweet, which was misclassified as  X  X owing X  before (Mid) but cor-rected after (Right) corrective learning. number of new examples, and in such a way that the correc-tive action is limited to the area of the input space where the current models are actually misbehaving, while leaving the remainder unaffected. In Figure 5 (bottom), we show the UI for manual corrective learning, and one particular exam-ples. The tweet was misclassified into the topic  X  X owing X  because of overfitting to the Byte4Grams extracted from the substring  X -rowing X , as shown in the bottom left chart (pos-terior score p ( c | x ) = 0 . 97). Fortunately, once we identify this pattern and adjust the model with corrective learning, this mistake can be corrected, as shown in the bottom right chart ( p ( c | x ) = 0 . 41).
Although we strive to turn down the noise in our labeled data by employing various techniques such as topic prior based pre-filtering, co-training and PU-learning, the data coming out of our training data acquisition pipeline still con-tains substantial noises. As a result, very few of our topic classifiers, if trained on these data, can achieve 90% preci-sion. On the other hand, the labeled data collected from  X  X eedback collection X  and  X  X uality evaluation X  processes, al-beit expensive and in relatively small amount, has very high precision as it is produced by human with controllable qual-ity assurance. Can we use this small set of high-precision data together with the the large set of relatively noisy data to train better models? Naively training models on the com-bined data set is ineffective as only negligible AUC improve-ments were observed in our experiments (i.e., high-precision data is diluted/polluted by noisy data which is dominant in amount). We propose here a two stage training process: Pretraining Train LR model on the larger set of noisy data Fine tuning Tune the model on the small high-quality data To fine tune the model, we use the pretrained model, de-noted w 0 , as prior and shrink w towards w 0 while minimiz-Table 6: Comparative evaluation of models on crowdsource confirmation-labeled data.
 Table 7: Quality metrics of the calibrated classifiers. ing the regularized loss on the high precision data: where  X   X  [0 , 1] controls how much the model w is shrunk towards its prior w 0 vs w  X  (i.e., the maxima of the regu-larized likelihood on high-precision data), we sample  X  from Beta(  X  | n 0 ,n ), where n is the number of examples seen in the training set and n 0 is a positive number quantifying how strong we trust the prior 4 . This way, the shrinkage adap-tively finds a balance between the prior and the likelihood such that the more high-precision data we supply, the less the model will be shrunk towards the prior and vice versa. Experimental Results. In Table 6, we report the quality of the models produced by the two-stage training process. As a reference, the very first baseline (i.e., the third row of Table 1) and the stage-1 model (i.e., the third row of Ta-ble 5) are used for comparison. For more accurate assess-ment, we use crowdsource confirmation-labeled data (Sec-tion 4.7) in this evaluation and report the exact AUC scores. The fine-tuning ( stage-2 ) consistently and significantly im-proves the quality of the model on both tweet and webpage, e.g., average AUC is improved by over 5% on tweets.
In Table 7, we also report the quality metrics of the cali-brated models as seen in our service. The fine-tuned tweet model achieves  X  90% precision with 33% tweet converge and over 80% topic coverage, a huge improvement from our first baseline, which, for example, with  X  70% precision only achieves 8% tweet coverage and 21% topic coverage. Large-scale data. We discuss here two approaches, en-abled by the prior shrinkage framework 5 , which scale up our learning capability to massive data set in terabytes: The best choices of n 0 and n can be decided by applying Laplacian approximation to the likelihood. We use cross-validation in our experiments for simplicity.
Note that hierarchical regularization is also enabled by prior shrinkage. Active learning. One question one may ask is: since high-precision human labeled data is so effective in improving the model quality yet it is so expensive to acquire, would it be more efficient to apply active learning, i.e. , to train the model from scratch with a minimum amount of iteratively acquired training data? Unfortunately, the answer is  X  X o X . First, active learning requires integration of our machine learning capability into the API of the crowdsource agent, which is not supported by any crowdsouring services on the market. More importantly, the feature vector for each tweet is extremely sparse (e.g., only  X  70 binary occurrence in a 1M space). At such sparsity, even if we were to see each feature to occur once (for each of the 300 topics), it would require millions of tweets for querying and incur a cost that easily exceeds our budget. In contrast, we find corrective learning (Section 4.8) is more suitable to our case  X  it provides useful diagnosis and corrective feedback to an (underperforming) existing model, and allows us to quickly fix it on the flight; moreover, it also takes advantages of the large user base of Twitter to acquire high-quality labeled data at virtually no cost by using the wisdom of the crowd.
Up to now, the topic models we present infer topics solely based on the text of tweets. However, a tweet is an envelope of different things, for example, besides (1) the text, a tweet can also contain (2) author, (3) embedded URL, (4) entities (e.g., #hashtag , @mention and named entities), (5) engagers who have interacted with the tweet (e.g., favorite, retweet, reply), (6) multi-media (e.g., image, video) and (7) other context information such as geographic, temporal and social contexts. The question is, how to harness all these sources of signals to achieve highly accurate topic inference when each single signal might fall short?
One approach is to extract feature from each source of sig-nals and train LR models in the augmented feature space. There are some drawbacks with this approach. First, not all the features are available for every tweet (they may available at different stages or not available at all, e.g., engagements), leaving a lot of missing values. Second, the significant in-crease in feature dimensionality raises a lot of complications, e.g., it requires more labeled data for training and incurs significant overhead in memory consumption and runtime latency. Experiments also suggest that this approach is less effective in our system. An alternative approach is to use a hierarchical model similar to a feed-forward neural network , e.g., train a set of LR models on each input signal alone, and another layer of LR model on top of the output of these mod-els. The structure doesn X  X  have to be three layers. If needed, more hidden layers can be used to capture higher-order cor-relations among different features to further improve gen-eralization performance as suggested by recent advances in neural network research [16]. Unfortunately, this model re-quires more complicated machine learning capabilities which are not currently supported by our infrastructure.

Instead, we present here an integrative inference approach based on decision aggregation (DA), where each of the sig-nals is used as a weak decision maker to provide noisy topic labels for tweets, and the final topic labels are decided by ag-gregating these noisy predictions, e.g., via weighted majority voting. There is no restriction on what types of predictors to be used by each weak predictor as long as they all produce predicted labels for a given tweet. Similar methodologies have been applied successfully in the literature to handle tasks that involve multiple noisy experts or highly hetero-geneous data [11, 24]. In this section, we first describe how we derive topic decisions from users and entities, we then present the integrative inference algorithm in details.
A user is usually interested in only a few topics out of the 300+ topic space. When associated with a tweet via authorship or engagement, the user X  X  interest topics provide a valuable signal for inferring the topic of the tweet. User interest modeling is itself an important topic. In fact, one primary use case of tweet topic modeling is to understand the interest intent of Twitter users so as to enable topic-based personalization.

Based on what a user has produced and consumed on Twit-ter, we derive two types of interest topics. Particularly, if a user u prominently tweets about a topic c , we call u is  X  Known-for  X  (KF) c . Likewise, if a user u consistently shows interest to tweets about topic c as indicated by his engagements (e.g. retweet, favorite, reply, clicks), we say u is  X  X nterested-in X  (II) c . Note that KF of u is a subset of u  X  X  II. In our system, the distribution of interest topics for each user (i.e., KF and II) are derived in real-time by running a tfidf -style topic-ranking algorithm on a sliding widow. The foreground model is a smoothed likelihood of user u producing / consuming tweets about topic c , whereas the background model is a population-level mean of the for-mer. Let r t ( c | u ) denote the tfidf score of topic c to user u at time t , the topic distributions are rolled-over with a time-decay weight via p t ( c | u )  X  1 K ( t ) R t 0 k ( t  X  s )  X  ( r ( k (1) p t  X  1 ( c | u ) + k 2 (0)  X  ( r t ( c | u ))) / ( k (1) + k then normalized (over all the topics) and memcached for real-time access. Here k (  X  ) is a time-decay kernel (e.g., the Exponential PDF k ( t ) =  X e  X   X t ), K ( t ) = R t 0 k ( s ) ds is the CDF of k ( t ),  X  is a feature transformation function (e.g., Table 8: User interest models are evaluated in terms of precision at n (P@ n ) on user survey data.
 when  X  ( x ) = exp( x ) is used, the distribution p ( c | u ) will be a weighted softmax transformation of r t ( c | u )).
Another source of information for deriving user interest topics is to leverage the social annotation mechanism en-abled by Twitter List. In particular, if a user u is consis-tently labeled by other users with lists that can be mapped to topic c , we call u is known-for c . II topics are derived from KF by using the follow graph, i.e., if a user u directly follows a user v who is unambiguously known-for topic c , we say u is interested-in c .

Interest topics derived from authorship and engagements are dynamic, and can adapt over time as users X  interests evolve. In contrast, interests derived from social annota-tion are static. Nevertheless, we found that, especially for celebrities who don X  X  tweet proactively, social annotation is more accurate. In our system, these two approaches are combined linearly with weight tuned on validation data. Experimental Results. To evaluate the derived user in-terest topics, we conducted an online survey to collect ex-plicit feedback from users. For each participating user, we select a set of  X  10 topics, consisting of a subset of topics pre-dicted by our model and a few randomly-sampled probe top-ics. We present these topics to users in a random order and ask them to rate how much they are interested in tweets on each topic. Users are allowed to provide their answers on 7 scale from  X  X trongly interested (7) X  to  X  X trongly uninterested (1) X , and one additional option of  X  X ot sure X . Irresponsible or noisy answers which apparently conflict with what users explicitly stated in their profiles or other strong behavioral signals were filtered to reduce the noise. Based on  X  5000 survey responses, we assess our model in terms of precision at the top-n topics (or P@ n ). The results are reported in Table 8. The average score of the top-5 predicted interest topics is around 6 (i.e., interested), significantly higher than the average score of the probe topics, which is around 3 (i.e., somewhat uninterested), indicating that the predictions by our model are consistent with what users perceive about their true interests. Overall, our model achieve up to 80% precision, and on 34% tweets, the top-5 predicted interest topics overlap with the ground-truth interests of a user.
We present here an algorithm to infer topics from #hash-tags , although the algorithm is applicable to other types of entities (e.g., named entities, @mentions ) as well. Hashtag is one of the key features that define Twitter. In its essence,  X  hashtag = topic  X , i.e., hashtags are user-defined topics to better organize conversations. Hashtags are also way more predictive than ordinary text strings. Most hashtags span a rather narrow bandwidth out of a large spectrum of topics, e.g., #ObamaCare , #TheVoice and #NBA .

A straightforward approach is to use hashtag as features and train a LR model to classify tweet solely based on hash-tags. This approach is, however, inherently flawed due to the huge volume (e.g., the number of hashtags are in O (100 Ms )), Table 9: Performance of hashtag topic model: pre-cision at n (P@ n ), tweet coverage, and the ratio of output overlap with tweet classifier.
 the transient nature (i.e., hashtags are continuously evolving over time with new hashtags emerging every second) and the extreme sparseness (i.e., the majority of tweets usually con-tain no more than one hashtag). Indeed, our experiments indicate that, when hashtags are used as special unigram fea-tures together with regular text strings, only a very marginal (on some topics even negative) improvements were observed.
Instead, we infers hashtag topics via a retrieval algorithm, i.e., take a hashtag as a query, we aim to find the most repre-sentative topics for it. Unfortunately, because the hashtag-to-topic retrieval task is very different from conventional query-to-document retrieval, standard retrieval models such as pointwise mutual information (PMI), tfidf , BM25 and  X  -test did not perform well in our experiments. To this end, we present a learnable retrieval model supervised by the output of our high-precision tweet text classifiers. Feature For hashtag h and topic c we use the following Model We train a linear model using the logarithms of the Note that we can turn the direction of the retrieval model around, i.e., to find the most predictive hashtags for each topic, which is useful for a number of features such as hashtag-based topic summarization, topic-aware trends, as well as topic priors for training data acquisition (Section 4.2). Experimental Results. In evaluation, we randomly sam-pled  X  500 hashtags with 10 example tweets for each hash-tag, and asked crowdsource workers whether the ground-truth topic is within the top-n topics retrieved from hash-tag. The results are reported in Table 9. Overall, the model achieves 64% precision at position 5 (compared to 34% of user II topics). Moreover, hashtag topics overlap with tweet classification results on only 53% of the tweets, suggesing that they are also potentially useful for improving recall.
We now have five topic predictors for a tweet, i.e, Table 10: Quality metrics of integrative inference. Table 11: Importance ranks and scores of the 5 in-gredient predictors in integrative inference. We derive the final topics for each tweet by aggregating topic labels provided by each of these predictors via decision ag-gregation . In settings where there are multiple experts pro-viding noisy (possibly conflicting) decisions yet none of them is good enough alone, decision aggregation has been shown to beat or perform comparably to the best expert in hind-sight [11, 24]. As we are primarily concerned with precision, we aggregate the noisy topic labels providing by each pre-dictor using weighted majority voting : we assign a weight for each predictor and take all the topic labels to form a com-mittee to vote, where each topic takes the sum of weights from the predictors who activate it. These topics together with their weights are then propagated according to the tax-onomy tree to arrive at the final topic predictions. In our experiments, the weights are trained using the AdaBoost al-gorithm [14] on human confirmation-labeled data.

The integrative algorithm brings a close-loop inference mechanism and enables our system to dynamically adapt itself to cope with data drift, an importantly ability that a static LR classifier lacks. This is due to the fact that, by do-ing integrative inference, our system is running in real-time a close-loop iteration between (1) inferring topics about enti-ties and users from tweet topics and (2) refining tweet topics using entitiy and user topics based on emerging signals. Experimental Results While we can configure the inte-grative inference algorithm to improve precision and cover-age (or recall) at the same time, here we exemplify with a rather conservative configuration to optimize precision (i.e., eliminate maximum false positives). The results are de-picted in Table 10. At 12% higher coverage (compared to the tweet classifiers), the integrative model achieves 93% preci-sion. Table 11 also shows the rank and importance score (normalized against the maximum score) of the 5 ingredient signals, as learned by the AdaBoost algorithm.
We have presented a deployed large-scale topic modeling system that infers topics of tweets over an ontology of hun-dreds of topics in real-time and at stringently high precision. We have proposed a unique collection of topic modeling tech-niques that effectively helped us to address the challenges in implementing the system and satisfying the quality require-ment.
 Acknowledgements. We would like to thank our interns Ramnath Balasubramanyan, Mitja Trampus, Naftali Har-ris and Will Landecker, and the other members on the user modeling team especially Matthew Bilotti, Jake Manix, Praveen Bommannavar, Dimitar Simeonov, Dong Wang and David Burkett for their contributions to the system.
