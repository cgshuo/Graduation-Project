 We present DSA  X  Derivative time series Segment Approx-imation , a novel representation model for time series de-signed for effective and efficient similarity search. DSA sub-stantially exploits derivative estimation, segmentation and dimensionality reduction to meet at least the requirements of high sensitivity to main features (trends) of time series and robustness to outliers. Experiments show that DSA is drastically faster and still as good or better than the promi-nent state-of-the-art similarity methods.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications  X  Data mining; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval  X  Clustering General Terms: Algorithms, Experimentation Keywords: Time series Representation Model, Segmenta-tion, Dynamic Time Warping, Dimensionality Reduction
A time series T is a sequence of (real) numeric values upon which a total order based on timestamps is defined. The traditional form T = [( x 1 , t 1 ) , . . . , ( x n , t n ten as T = [ x 1 , . . . , x n ] when, as usual, a fixed sampling period is assumed. Significant amounts of time series data are naturally available on several sources of different do-mains, such as speech recognition, biomedical measurement, financial and market data analysis, telecommunication and telemetry, sensor networking, motion tracking.

Knowledge discovery and management in time series data represents a fruitful area of research, in which most efforts have been made to address the common problem of similar-ity search. The basic approach is dynamic time warping [1], and all outstanding methods are its extensions, possibly in-cluding techniques borrowed from string matching based on edit distance [6, 4, 5].

In this paper we present DSA  X  Derivative time series Seg-ment Approximation , a concise, feature-rich representation of time series designed for effective and efficient similarity search. DSA is able to synthesize the significant variations in the time series profile, as it intuitively segments the deriv-ative version of a time series before of approximating it into a higher level representation.
 In order to assess the ability of DSA in supporting effective and efficient similarity detection, we conducted an exten-sive experimental evaluation, mostly oriented to clustering tasks. We compared DSA as representation and similarity scheme to the prominent state-of-the-art methods, includ-ing DTW [2], DDTW [3], ERP [4], EDR [5], LCSS [6], and FTW [7]. Our results obtained on standard datasets show that DSA behaves as good or far better than major compet-ing methods, and dominates them in time performances.
Using the DSA model a time series is transformed into a new, smaller sequence by the following main steps: i) computation of the first derivatives of the original series to capture its significant trends, ii) identification of segments consisting of tight derivative points, iii) segment approxima-tion to finally obtain a lower dimensional still fine-grained representation of the original series.
 Derivation. Given a time series T = [ x tion step yields a sequence  X  T = [  X  x 1 , . . . ,  X  x  X  x are first derivative estimates. We use an estimation model that is sufficiently general (i.e. independent of the under-lying data distribution model) and still enough robust to outliers [3].
 Segmentation. The derivative time series  X  T = [  X  x is partitioned into p contiguous subsequences of points (seg-order to determine the segment delimiters, i.e. the p -1 points (derivatives), a series is divided according to the first deriv-ative such that the absolute difference between it and the mean of the previous derivatives is above a certain thresh-old  X   X  X his point becomes the anchor for the next segment to be identified in the rest of the series. Intuitively, this con-dition allows for aggregating subsequent points having close derivatives. In such a way, the growth segment represents a subsequence of points with a specific trend. The threshold  X  can be estimated globally with respect to a given collection of time series, by considering an index of dispersion of the derivatives within the same sequence around the respective mean value.
 Segment approximation. Individual segments of a deriv-ative time series S  X  T = [ s 1 , . . . , s p ] are mapped to angular values to obtain a new sequence  X  = [  X  1 , . . . ,  X  p ]. Such an-gular values are able to represent synthetic information on the average slopes within the segments. This is mathemat-ically expressed by the notion of arctangent applied to the mean of the derivatives in each segment.
Experimental evaluation was conceived to demonstrate the ability of DSA to achieve fast and accurate clustering
Using DTW is still a natural solution to compare two time series transformed by DSA. The choice falls to the warp-ing approach rather than string alignment based approaches (i.e. EDR, ERP, LCSS) mainly because basic DTW is para-meter-free; on the contrary, EDR and LCSS require a match-ing threshold to handle noise, as well as ERP employs a constant gap to enable a metric.

We used five well-known datasets in the time series do-main: GunX (200 series with 150 time steps, 2 classes), Trace (200 series with 275 time steps, 4 classes), CBF (300 series with 128 time steps, 3 classes), ControlChart (600 series with 60 time steps, 6 classes), and Twopat (800 se-ries with 128 time steps, 4 classes). All datasets but CBF are available at http://www.cis.temple.edu/  X latecki/ Test-Data/TS Koegh/. Dataset CBF was synthetically gener-ated according to the known formula (see for example http:// www.cs.ucr.edu/  X eamonn/TSDMA/cluster.html).

Finding the best strategy of time series clustering is not the main focus of this work, so we implemented a very sim-ple and popular clustering tool, namely the k -Means algo-rithm [8]. The problem of clustering time series is then formulated as follows: given a dataset of m time series D = { T 1 , . . . , T m } , partition D into k homogeneous, well-separated groups, or clusters. Note that the k -Means re-quirement of k desired clusters is not a shortcoming in our setting, since a reference partitioning (i.e. an ideal classifi-cation) is available for each test dataset. This also implies that evaluating the effectiveness of a time series clustering task can be accomplished by adopting an external validity criterion. The objective is to assess how well a clustering fits a predefined scheme of known classes. F-measure [9] is the most commonly used external criterion (ranging within [0 .. 1]), defined in terms of classic Information Retrieval no-tions X  precision and recall.

We devised the effectiveness evaluation in two stages. In the first stage we mainly aimed at checking the ability of DSA in producing high-quality clusterings. To accomplish this, we explored how clustering results can be influenced by choosing different alternatives for data preprocessing and setting the parameters therein involved. Indeed, raw time series are usually preprocessed in order to dampen the effect due to noise in data. This is accomplished by using some smoothing models, such as centered q -point moving average and exponential models.

Once favorite preprocessing setups for DSA were fixed, we compared DSA with other similarity methods on sev-eral clustering tests involving the chosen datasets. Table 1 summarizes main results and allows us to state the follow-ing: DSA dominates all competing measures in three out of five datasets, otherwise it fares as good or better than the other methods. Among the competitive similarity measures, DDTW turns out to be the best performing one.

We also carried out experiments to assess the time perfor-1 Details can be found at http://www.deis.unical.it/tagarelli/dsa/.
 Table 1: Clustering quality results in terms of F-measure values averaged over 100 runs. EDR, LCSS, ERP and FTW use default parameter setting mances of DSA based clustering, and to provide a compar-ative evaluation with respect to DTW, DDTW, and FTW based clustering. Such measures were chosen as they are those ones which inspired DSA. Moreover, comparing DSA to FTW is particularly significant since the claimed effi-ciency of FTW. We sampled each dataset in portions of size 10%, 25%, 50%, and 100%; for each of these portions and for each similarity method, we carried out 100 runs. Exper-imental evidence showed that DSA drastically outperforms DTW, DDTW, and even FTW on each dataset. In particu-lar, DSA is on average 6.6 times and up to 12.7 times faster
The surprisingly good accuracy and time performance ex-hibited by DSA suggests that it employs a sound underly-ing representation model. As future research directions, we shall investigate the capability of DSA as a technique for dimensionality reduction, to be used in its own right as well as integrated in other similarity or approximation methods. Moreover, we would like to extend DSA for multidimensional time series (i.e. moving object trajectories). [1] L. Rabiner and B.-H. Juang. Fundamentals of Speech [2] E. Keogh and M. Pazzani. Scaling up Dynamic Time [3] E. Keogh and M. Pazzani. Dynamic Time Warping [4] L. Chen and R. Ng. On The Marriage of Lp-norms and [5] L. Chen, M. T.  X  Ozsu, and V. Oria. Robust and Fast [6] M. Vlachos, D. Gunopulos, and G. Kollios. Discovering [7] Y. Sakurai, M. Yoshikawa, and C. Faloutsos. FTW: [8] A. K. Jain and R. C. Dubes. Algorithms for Clustering [9] C. J. van Rijsbergen. Information Retrieval . IV 3GHz with 2GB memory and running on Microsoft Win-dows XP Pro.
