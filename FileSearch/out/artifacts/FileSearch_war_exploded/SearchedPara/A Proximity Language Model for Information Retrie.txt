 The proximity of query terms in a document is a very im-portant information to enable ranking models go beyond the  X  X ag of word X  assumption in information retrieval. This pa-per studies the integration of term proximity information into the unigram language modeling. A new proximity lan-guage model (PLM) is proposed which views query terms X  proximity centrality as the Dirichlet hyper-parameter that weights the parameters of the unigram document language model. Several forms of proximity measure are developed to be used in PLM which could compute a query term X  X  proxi-mate centrality in a specific document. In experiments, the proximity language model is compared with the basic lan-guage model and previous works that combine the proximity information with language model using linear score combina-tion. The experiment results show that the proposed model performs better in both top precision and average precision. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms,Experimentation,Theory Information Retrieval,Proximity Model, Proximity Language Model
A key task in information retrieval is to rank a collec-tion of documents according to their respective relevance to a user query. Probabilistic models have been successfully applied in document ranking, such as the traditional proba-bilistic model [23, 13, 24] and stochastic language model [21, 15, 29] etc. By assuming full independence between terms, those models represent query and documents as bag of words and mainly exploit statistics such as term frequency, inverse document frequency and document length in ranking. One shortcoming of those traditional ranking models lies in that they don X  X  consider the proximity of the query terms within adocument.

Proximity represents the closeness or compactness of the query terms appearing in a document. The underlying in-tuition is that the more compact the terms, the more likely that they are topically related, and thus the more possible the document is relevant to the concept represented in the user query. Proximity can be seen as a kind of indirect mea-sure of term dependence. [2] shows that term proximity has a strong influence on the dependence between terms.
Several works have been done towards incorporating prox-imity factor into existing ranking functions [22, 3, 27, 1]. Those works show that a properly devised proximity mea-sure can improve the effectiveness of probabilistic ranking functions. However, there are two main shortcomings in previous works. One is that the underlying proximity struc-ture of the individual query terms is not exploited. As in-vestigated in [27], two kinds of proximity mechanisms could be distinguished in those approaches, span-based and pair-based. The span-based method rewards a document accord-ing to the text span of the overall query terms and doesn X  X  consider the internal structure of those terms. The pair-based method represents a document X  X  proximity score di-rectly via pair-wise term distance and also doesn X  X  take into account the individual term proximity structure. Another weakness of previous works lies in that the proximity fac-tor is combined with probabilistic models in a very intuitive way. Most works just linearly combine a document X  X  overall proximity score externally with relevance score computed by traditional ranking functions at the document level.
Language modeling has become a very promising direc-tion for information retrieval because of its solid theoretical background as well as its empirical good performance. In this paper, we try to integrate term proximity into the un-igram language modeling approach. We model the individ-ual query term X  X  proximate centrality as Dirichlet hyper-parameter that weights the corresponding term emission parameter of the multinomial document language model. Thus, we attain an integral ranking formula that effectively boosts the score contributio n from terms when the term is proximate to other query terms. This term level incorpora-tion of proximity is mathematically grounded. Further, it performs empirically better tha npreviousintuitivecombi-nation of proximity at document level.
The rest of this paper is organized as follows. We intro-duce the most related works in Section 2. In Section 3, we present the proximity integrated language model. Then,in Section 4, we study several different ways of measuring a query term X  X  proximate centrality. We report the experi-mental result in Section 5 and finally in Section 6, we con-clude our work.
Many efforts have been made to directly incorporate term dependency into probabilistic modeling against the unintu-itive  X  X ag of word X  assumption. In early attempts, [6, 28] try to incorporate term dependence over Binary Independence Model (BIM). However, those models don X  X  achieve stable improvement over BIM as expected and are rarely used in practice. The reason may lie in that independence assump-tion in BIM could actually be replaced by a weaker linked dependence assumption [5].

Recently, along with the booming of language modeling in information retrieval, several works are done to integrate term dependence into the language model. [25, 17] pro-pose general language models that combine bigram language model with unigram language model and thus can consider ordered adjacent dependence. [26] introduces biterm lan-guage model that also takes into account dependency be-tween unordered adjacent word pairs. [10, 19] put forward dependency language models that consider the relation be-tween terms existed in a dependency tree. [16] advances an exponential language model that allows the consideration of dependency in every subset of query terms. All those works report improvement over the unigram independence language model. However, the main problem of those de-pendency models lies in that the parameter space becomes very large by direct dependency consideration. This will make the parameter estimati on much more difficult to com-pute and sensitive to data sparse and noise. Those risks may counteract the relatively small benefits one could ob-tain from direct dependency modeling.
Another stream of works tries to incorporate bigger unit than word such as phrase or loose phrase in text represen-tation. In such a way, dependence between words can be captured indirectly. [8, 7] test the method of incorporat-ing syntactic and statistical phrases in text indexing. It is shown that statistic phrases perform better than syntac-tic phrases. However, the improvement of using phrases is not consistent. For several collections, significant improve-ments in effectiveness are achieved while for other collec-tions, marginal or negative improvements are attained. [18] re-examines the use of phrases for indexing and retrieval. In extracting statistical phrases, all pairs of non-function words that occur contiguously in a number of documents are regarded as phrases. And those phrases are viewed as indexing unit just like simple words. It concludes that the use of phrases do not have a major effect on precision at high ranks if a good basic ranking scheme is used.
As pointed out by [10], the reason for the ineffectiveness of phrase indexing may lie in the following two aspects. First, phrases are different in nature from words, so it may not be appropriate to apply the same weighting schemes for both of them. Second, phrases are likely to be systematically over-scored in the independent model.
Proximity modeling can be thought of as another indirect way to capture term dependence. In some early works, [11, 14, 4] propose similar proximity models that measure a doc-ument X  X  proximity score by the span and density of query terms. The shorter the text span that contains all the query terms, the more relevant the document. Also, the more span instances of query terms in a document, the more relevant the document. However, the above works don X  X  incorporate the proximity measure with an effective probabilistic rank-ing model.

Recently, several works try to incorporate proximity fac-tor into probabilistic ranking functions. [22, 3] extend BM25 ranking formula [24] with a term-proximity scoring part. The proximity part is a span-based measure that scores ev-ery query term pair occurring in a text segment covering all the query terms in analogous form as simple words. The ranking score of the document is the linear combination of the BM25 ranking score with the proximity score. The result shows improvement for top precision, with marginal impact on the average precision. [27] investigates the span-based measures and also propose several pair-based proximity measures that model proximity in terms of the pair-wise local distance between terms. [27] tries to combine the proximity factor with both BM25 rank-ing model and the KL divergence language model [15]. The integration of proximity factor is also in an external score combination form as: In which, KL ( q, d ) is ranking score attained by KL, and  X  ( q, d ) is a proximity distance measure of the document d with respect to query q . Although in such a simple way, [27] shows that by combining a properly proximity distance mea-sure with the KL language model, comparative result could be attained compared to the more complicated dependence language models such as [16].

Overall, compared to direct dependency modeling and phrase indexing, proximity modeling seems to be an eco-nomic while effective way to go beyond the  X  X ag of word X  assumption in retrieval. Our work tries to improve the pre-vious works in proximity modeling by the following aspects. We integrate the proximity factor into the unigram language modeling approach in a more systematic and internal way that is more effective than external linear score combination.
First, we introduce the traditional multinomial language model for information retrieval. Consider a query q and adocument d l in a collection C .Let V = { w 1 ,w 2 , ...,w denote the vocabulary set, then under the  X  X ag of words X  as-sumption, both the query and the document are represented as vectors of term counts: where q i and d l,i is the frequency of the i th term in query q and document d l respectively. Now, let  X  l =(  X  l, 1 ,  X   X 
V | ) be the parameters of a multinomial generation model for d l ,where  X  l,i means the probability of emission of term w i in the vocabulary. Then, the probability of generating a particular query q or a document d l could be given as: where n q = count in q and d l .

For each do cument d l , the parameters of  X  l are estimated by viewing the document as the observed data. The rele-vance of d l to a query q is then measured by the probability of generating q by the language model estimated from d l . So, the key element is the estimation of the multinomial pa-rameters in  X  l from a given document d l . The simplest way is the maximum likelihood estimation. To account for rare words and alleviate the data sparseness problem, various smoothing methods are usually applied to the estimation, which commonly interpolate the docu-ment estimated language model with collection estimated language models [29].
Now, we discuss how to integrate the proximity informa-tion of query terms in the document into the unigram lan-guage model. In forming a query, a user wants to use several terms to jointly express a specific concept. Term proximity information expresses the clos eness of query terms appeared in a document. The more adjacent terms appear in a doc-ument, the more possible that those terms are topically re-lated and functions together to express the concept under-lying the user query. So, given two document d a and d b supposing all others being equal while the query terms of q appears more proximate in d a than in d b , we believe that d a should be more releva nt to the query than d b .Inother words, if  X   X  a and  X   X  b represent the language model estimated from d a and d b respectively, we believe that the probability that q is generated from  X   X  a should be higher than  X   X  b
To achieve the above expectation, we view the proxim-ity centrality score of a query term w i as the weight on the term X  X  emission probability  X  l,i . That is, the estimates for different term X  X  emission probability should be related and proportional to each other according to each term X  X  proxim-ity centrality score. Here, a query term X  X  proximity central-ity means a term X  X  proximity relative to all other query terms in a document which is defined in Section 4. By Bayesian analysis. we could express our knowledge or belief on the uncertainty of the parameters by some prior distribution on it. The conjugate of the distribution where the parameters come from is usually exploited to express the prior belief. The natural conjugate of multinomial distribution is Dirich-let distribution.

Specifically, supposing that B is a proximity centrality computing model and Prox B ( w i ) be the computed proxim-ity centrality of term w i , we use a Dirichlet prior [9] on  X  with hyper-level parameters u =( u 1 ,u 2 , ..., u | V | ) where u i =  X P rox B ( w i ), and Z u =  X ( X  depend on the parameter  X  l . Dirichlet is the distribution for probabilities. By using the Dirichlet prior, we express our belief that the estimated probability of the matching words in a document should be correlated in a way that reflects the proximity structure of the terms.

Then, given the document d l and the proximity computing model, we could get the posterior estimation of  X  l as: By the property of natural conjugate distributions, the above equation is also a Dirichlet distribution as Dir (  X  l | u + d )in bution reflects our prior beliefs about the weight of  X  l the posterior distribution of  X  l reflects the updated beliefs about  X  l posterior to observing the frequency information of the data d l . In other words, the posterior distribution is centered at a point that represents a compromise between the prior belief and the data.

Given the posterior distribution, the estimation of the word emission probability can then be noted as:  X  In empirical Bayesian analysis, the hyper-parameters u in Dirichlet prior can still be estimated from the data although it conflicts with the intuitive meaning of  X  X rior X . Specifi-cally, for each document d l , a corresponding proximity cen-trality model B l will be computed from d l with respect to the given query according to specified measures introduced in Section 4. Such a B l is used in equation (7) to compute the word emission probability. In this way, we can see that the proximity information in a document is integrated into the estimated unigram document language model.

In the above estimated document model, the proximity information could be seen as transformed to word count in-formation which is the primary object that unigram lan-guage model has the ability to model. From another point of view, we could consider that the  X  X ag of word X  represen-tation of document d l is transformed to a pseudo  X  X ag of word X  document representation d B l given the document X  X  proximity model B l with respect to a query q .In d B l the matching term X  X  frequency is transformed from d l,i to d l,i +  X P rox B ( w i ), and the total document length is changed from n l to n l + ranking d l for a query q is changed to ranking d B l .On such ground, any  X  X ag of word X  based language model, e.g. the query likelihood model or the KL divergence model [15], could work on d B l and thus integrate the proximity infor-mation in the document in an internal way.

Now, we give the proximity integrated ranking function in the KL divergence language modeling framework. We further smooth the proximity integrated language model by a collection language model p (  X | C ) to account for unseen words in the document as in [29]. Specifically, the collection-based Dirichlet prior (  X p ( w 1 | C ),  X p ( w 1 | C ), ...,  X p ( w applied for smoothing. We can get the smoothed proximity integrated estimation: Then, the ranking function could be stated as:
By further deduction following standard smoothing scheme, it can be shown the above scoring formula is essentially: Rank ( q, d l )= where p s (  X | d l ,u ) is the seen word probability in the docu-ment d l with respect to the proximity model B l .  X  d p ( w is the probability assigned to the unseen words in d l .
In the above formation, note that the proximity factor mainly functions to adjust the parameters for seen matching terms with respect to a query in a document. It is conceptu-ally very different from the collection based priors used for smoothing which is motivated to weight the unseen words in the document.
A key notion in the above proximity language model is a term X  X  proximate centrality Prox B ( w i ). It represents a term X  X  importance in forming t he overall proximity struc-ture in a specific document relative to a given query. For non-query terms, we assume they have a constant score of zero. For a query term, its proximate centrality should be computed according to a proximity measure that reflects the term X  X  closeness to other query term X  X  in the same doc-ument. However,most previous woks in proximity modeling compute a document X  X  overall proximity score for a query. There is no well-established proximity measure for comput-ing the proximate centrality or proximity score for a specific individual term. In this section, we develop several measures that could give such term specific centrality score.
An intuitive idea to represent a term X  X  proximity is by measuring its distance to other query terms in the document. A short distance to other terms means that the term is in a high proximate area and should be assigned a high proximity score. There are two key points to implement such an idea. The first is how to define a term X  X  distance to other terms in a document. The second is how to devise a proper non-linear function to map such a distance to the proximity score of the term.

First, we define the distance between any pair of terms in a document. It could be measured through their occurring positions in the document. The main difficulty lies in that both terms may have many occurrences in the document. Let Q = { Q 1 ,Q 2 , ..., Q m } be the set of different query terms currence positions of the word Q i in document D .Weuse Dis ( x, y ; D ) to denote the pairwise distance between any two terms or two term occurrences in D . Following [27], the pairwise term distance is represented as the distance be-tween the closest occurrring positions of the two terms in the document.

Dis ( Q i ,Q j ; D )= in which, | D | is the length of document D and | O Q i the number of occurrences of query term Q i in D .Note that the pair distance measure is symmetric which means Dis ( Q i ,Q j ; D )= Dis ( Q j ,Q i ; D ).

Having the pair-wise distance measure in hand, now we define the function needed to transform the distance into pair-wise term proximity. The transformation function plays a very important role in setting the scale of proportional ratio between proximity score of different matching terms. The following exponential form is used and tested, of which data is the input distance score and para is the parameter to control the scale of the transformed score. Then, the pair-wise term proximity could be noted as:
Based on the above devices, we are ready to define the term proximity centrality measures we need. We propose and test on the following term proximity centrality mea-sures.
 In this measure, the proximity score of a query term is ob-tained by applying the proximity transformation function to the term X  X  minimum pair distance with other terms. Such a measure is noted as P MinDist : where f is the nonlinear monotonic function defined in equa-tion (15).
 Instead of relying on the minimum distance, this measure uses the term X  X  average pair distance with other terms to model the term X  X  proximate status. It is noted as P AveDist : P AveDist ( Q i )= f ( 1 n  X  1 of which, n is the number of unique query terms appeared in D .
 This measure models a term X  X  proximity as the summation over all the pair-wise proximity the term involved. It is
Figure 1: An Example of Term Distance Graph noted as P SumProx : P SumProx first transforms each pair distance to the pair proximity score by non-linear function and then sum over all the pair-wise proximity scores.

We illustrate the features of the above term proximity measures using a simple example. Supposing there are 4 matching terms A, B, C and D, the pair-wise distances be-tween them are illustrated in Fig.1. Now supposing that the proximity transformation function used is f =1 . 5  X  dist ble 1 illustrates the proximity centrality score computed by the three different term proximity measures.
We evaluate our model described in previous sections us-ing the following TREC data sets: the popular ad hoc collec-tion AP88 (Associated Press News, 1988), WSJ90-92(Wall Street Journal 1990-92) and WSJ87-92(Wall Street Journal, 1987-92) plus OHSUMED (MedLine Database Abstract, 1987-1991) collection [12] used in TREC-9 filtering track. The statistics of the collections are illustrated in Table 2. While the three ad hoc TREC collections and queries contain poly-morphic documents and topics from different domains, the OHSUMED collection is more monomorphic which could be seen as documents from a speific technical domain (medicine). To evaluate the impact of different queries on similar collec-tions, two different TREC topic set, TOPIC 251-300 and TOPIC 151-200 are used for the two overlapping WSJ doc-ument collections of different size. For TREC ad hoc topics, only the title field of the topic is used. For OHSUMED top-ics, only the description field is used. The OHSUMED query is more verbose than other TREC topics in nature. The rel-evance assessments for the OHSUMED collection were made by medical librarians and physicians based on the results of interactive searches which are graded as definitely or possi-bly relevant. We make no distinction between definitely and possibly relevant documents in our test.
 Table 1: Term proximity score computed by differ-ent measures.
 P MinDist 1 1 0.07 0.13 P AveDist 0.11 0.07 0.04 0.06 P SumProx 1.17 1.10 0.11 0.23 Collection #Doc. Len. Queries #qrel.
 WSJ90-92 74520 514 TOPIC251-300 1064 WSJ87-92 173252 473 TOPIC151-200 3913 OHSUMED 348566 129 Ohsumed topic 3875
All the experiments in this paper are done using Lemur toolkit [20]. All the documents and queries in the above col-lections are tokenized with a naive tokenizer which views all symbols other than English characters as delimiters. Stem-ming is applied by using Porter X  X  algorithm. No stop word removing is done at the index time. Instead, a very small stop word list 1 is used for eliminating the most frequent stop words at query time.

Both top precision and average precision are used to eval-uate the experiment result. Specifically, we use evaluation metric Pr@5, Pr@10 and MAP which are precision at top 5 documents, precision at top 10 documents and mean average precision respectively.
We compare the performance of the proximity integrated language model (noted as PLM) with the basic KL diver-gence langauge model (noted as LM) and the work for in-corporating proximity with langauge model in [27] (noted as LLM). [27] combines the proximity score of the document with relevance score of the language model by external lin-ear combination at the document level as shown in equation (1) in Section 2. In our implementation, the minimum pair distance measure is used for the document X  X  proximity dis-tance  X  ( Q, D ) which achieves the best performance as shown in [27]. However, such a simple combination doesn X  X  consider the scale and the weight of the proximity score and relevant score, and thus is sensitive to the surface form of the for-mula for the language model. For example, comparing the following two deduction forms for the KL langauge model: Rank ( q, d )= Rank ( q, d )= Although they are equivalent for ranking, it will be very different when combines directly with a proximity score be-cause of the different scale of the score. We strictly follow-ing [27] by using equation (21) as the KL ranking formula in spite of that equation (20) is the more common one.
There are several parameters need to be set in those mod-els. In LM, the most important parameter is the prior col-lection sample size  X  . It is set to 2000 across all the experi-ments [29]. Such a parameter is also used in LLM and PLM without any further optimization.

In LLM, there is a parameter  X  involved in the proximity model as shown in equation (1) in Section 2. We set it by optimizing the performance on the respective test collections http://www.ranks.nl/resources/stopwords.html through searching the parameter space
In PLM, the most two important parameters are  X  and para . Parameter  X  is the proximity argument for the prox-imity information as in equation (7) of Section 3. It controls the proportional weight of prior proximity factor relative to the observed word count information in a document. The other parameter para is the exponential weight of proximity transition function in equation (15) of Section 4. It con-trols the numeric scale of different terms X  proximity score. In other words, it controls the proportional ratio of prox-imity score between different query terms in a document. Currently, we set those two pa rameters by maximizing the empirical performance on the collections through exhaustive searches in the following parameter space. We are investigating a more efficient approach such as leave-one-out and will include this in our future work.
In Figure 2, we report the parameter sensitivity of PLM on the test collections when using P MinDist as the measure of term proximity centrality (Note, the optimal parameter setting for using P AveDist and P SumProx are a little different from P MinDist ). We can see that except for the case of WSJ90-92, the best performance is achieved when the proximity argument  X  is set to about 6. Also, The best  X  value is relatively insensitive to different para value in the term proximity measure. However, the exponential weight para which controls the proportional ratio between different terms does have some influence on the ranking performance. For the three ad hoc collections, a relatively bigger value of para of about 1 . 7 is preferred. While for the very different OHSUMED collection, a smaller value is better.
Table 3 shows the best overall performance achievable by different ranking models. In the table, notation like PLM (P MinDist) means PLM model which uses P MinDist as the term proximity measure.

First, we compare the performance of PLM when apply-ing different term proximity centrality measures. (*) marks the Wilcoxon significance at 0 . 05 compared with the basic LM. Overall, P SumProx and P MinDist performs much better than P AveDist .Also, P SumProx performs a little better than P MinDist , but they are comparable. Please note that the performance reported in Table 3 is when stop words are eliminated from the query as explained in Section 5.1. Things will be a little different when no stop word re-moving is done. We report the performance in such a case in Section 5.5.

Next, we compare our proximity integrated model PLM with LM and the model that uses external linear score com-bination (LLM) as described in Section 5.2. LLM improves the basic language model in the three ad hoc collections but failed for OHSUMED collection which has more verbose topics. It actually does some harm to LM. PLM performs much better than the basic language model as well as LLM in terms of both top precision and average precision. PLM performs very well on verbose topics of OHSUMED.
Comparing with the baseline models, the main power of the proximity integrated language model comes from the following aspect. In external linear score combination ap-proach, the combination of proximity score with the relevant score is done in a post-processing manner while in PLM the Table 4: Performance Comparison Considering Stop Words in the Query proximity factor is integrated in a pre-processing manner. For external score combination, the relevant score and prox-imity score of a document are computed separately for a document. The nature of the two resulted score is totally different and thus could only be combined in a very intuitive way. Most importantly, by using the proximity score at an external document level, the proximity information provided by different terms are mixed and the overall proximity infor-mation contained in a document is decreased. On the con-trary, in PLM, the proximity information is combined with other document statistics such as term frequency in a dis-tributed way. The proximity information provided by each query term makes its own effort to the matching weight. As we all know, stop word removing does much help in IR. However, in many practical cases, it is desirable not to remove stop words at all. A good ranking function should also perform well when stop words are considered in the user query. Stop words may have a big influence for proximity modeling. This is because a stop word usually has many occurrences in a document, resulting in a great chance to be proximate with other words in the document. It may make the proximity mechanism at risk to loose its effect. So, for proximity modeling, it is very important to test whether the models could resist to the influence of stop words.
We extract all the queries from TOPIC251-300 that con-tain at least one word in the used stop word list. This results in totally 23 queries. Then, we test different ranking models on AP88 and WSJ90-92 collection for those queries. Note, it is meant that we don X  X  use any stop word list in either indexing or retrieval phase in this test.

Table 4 shows the performance of various ranking models on the stop word containing queries. From it, we can see that LLM approach fails when stop words are considered in the query. It does harm to the performance of the basic language model on both collections. In such a case, PLM can still improve on the basic language model in some degree.
Comparing different term proximity measures in PLM, the improvement ratio of P MinDist on LM drops in some degree. P SumProx now performs much more better than it. The problem for P MinDist lies in that if there is a stopword like term in the query, say  X  X f X , this word may occur very proximate with each content word in the query due to its high occurring frequency. Thus, it will make some query term have a false big proximity score in some cases. Overall, P SumProx is a good choice for using in PLM, which performs well on improving the basic language model irrelevant of whether stop word removing is used.
In summary, we have developed a novel way to integrate proximity factor into the unigram language modeling for in-formation retrieval, which view s query terms X  proximate cen-trality as Dirichlet hyper-parameters to weight the param-eters of the multinomial document language models. This integration method has solid mathematical foundation and shows empirical better performance than previous works. Most importantly, besides simple keyword query, this model also performs well in verbose query and stop word containing query.
 However, there is still much work to be done in the future. First, as we mentioned in Section 5.3, we should develop a more efficient way to set the parameters of the PLM model instead of using exhaustive search. Second, in developing PLM, we don X  X  make any effort to normalize different query terms X  proximity centrality score. It will be very helpful to study how to normalize it to a given scale or even to prob-ability, and see the effect on ranking result as well as pa-rameter tuning. Finally, it will be very interesting to study how to combine the proximity information with other docu-ment information such as prior document strength to further improve the effectiveness of language modeling.
We would like to thank the anonymous reviewers for their valuable comments and helpful suggestions. [1] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and [2] D. Beeferman, A. Berger, and J. Lafferty. A model of [3] S. Buttcher and C. Clarke. Efficiency vs. Effectiveness [4] C. Clarke, G. Cormack, and E. Tudhope. Relevance [5] W. Cooper. Some Inconsistencies and Misidentified [6] W. Croft. Boolean queries and term dependencies in [7] W. Croft, H. Turtle, and D. Lewis. The use of phrases [8] J. Fagan. Experiments in Automatic Phrase Indexing [9] T. Ferguson. A Bayesian analysis of some [10] J. Gao, J. Nie, G. Wu, and G. Cao. Dependence [11] D. Hawking and P. Thistlewaite. Proximity [12] W. Hersh, C. Buckley, T. Leone, and D. Hickam. [13] K. Jones, S. Walker, and S. Robertson. AProbabilistic [14] C. LA Clark and G. Cormack. Shortest-Substring [15] J. Lafferty and C. Zhai. Document language models, [16] D. Metzler and W. Croft. A Markov random field [17] D. Miller, T. Leek, and R. Schwartz. A hidden Markov [18] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An [19] R. Nallapati and J. Allan. Capturing term [20] P. Ogilvie and J. Callan. Experiments Using the [21] J. Ponte and W. Croft. A language modeling approach [22] Y. Rasolofo and J. Savoy. Term Proximity Scoring for [23] S. Robertson, S. Jones, et al. Relevance Weighting of [24] S. Robertson, S. Walker, and M. Beaulieu. Okapi at [25] F. Song and W. Croft. A general language model for [26] M. Srikanth and R. Srihari. Biterm language models [27] T. Tao and C. Zhai. An exploration of proximity [28] C.Yu,C.Buckley,K.Lam,andG.Salton.A [29] C. Zhai and J. Lafferty. A study of smoothing
