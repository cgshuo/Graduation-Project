 GEORGIOS PALTOGLOU and MIKE THELWALL, University of Wolverhampton The proliferation of Facebook, MySpace, Twitter, blogs, forums, and other online means of communication has created a digital landscape where people are able to so-cialize and express themselves through a variety of means and applications. Indicative of this new trend of social interaction is the fact that popular social Web sites are chal-lenging the popularity of established search engines [Harvey 2010; Thelwall 2008] as the most visited Web sites on the Web.

Sentiment analysis , the process of automatically detecting if a text segment contains emotional or opinionated content and determining its polarity (e.g.,  X  X humbs up X  or  X  X humbs down X ), is a field of research that has received significant attention in recent years, both in academia and in industry. One of the main reasons for this phenomenon is the aforementioned increase of user-generated content on the Web which has resulted in a wealth of information that is potentially of vital importance to insti-tutions and companies [Wright 2009]. As a result, most research has centered around product reviews [Dave et al. 2003; Pang and Lee 2008; Turney 2002], aiming to predict whether a reviewer recommends a product or not, based on the textual content of the review.

The focus of this article is different: the far more widespread informal, social in-teractions on the Web. In this context, sentiment analysis aims to detect whether a textual communication contains expressions of private states [Quirk 1985] and subse-quently whether it expresses a positive (e.g., excitement, enthusiasm) or negative (e.g., argument, irony, disagreement) emotion. The unprecedented popularity of social plat-forms such as Facebook, Twitter, MySpace, and others has resulted in an unparalleled increase of public textual exchanges that remain relatively unexplored, especially in terms of their emotional content.

One of the main reasons for this discrepancy is the fact that no clear  X  X olden stan-dard X  exists in the domain of informal communication, in comparison to the more de facto domain of movie or general product reviews. In the latter case, it is generally easy to detect whether a review, extracted from a Web site is positively or negatively oriented towards a specific product, by simply extracting the specific metadata that accompanies the review, such as the  X  X umber of stars X  or the  X  X humbs up/down. X  As a result, there is an abundance of product-related sentiment analysis datasets [Baccianella et al. 2010a; Blitzer et al. 2007; Pang et al. 2002], but only a few for the more general field of informal communication on the Web [Mishne 2005; Pak and Paroubek 2010].

The differences between the two domains are numerous. First, reviews tend to be longer and more verbose than typical online social interactions, which may only be a few words long [Thelwall and Wilkinson 2010]. Second, social exchanges on the Web tend to be much more diverse in terms of their topics with issues ranging from politics and recent news to religion. In contrast, product reviews by definition have a specific subject, that is, the product under discussion. Last, informal communication often contains numerous nonstandard spellings [Thelwall and Wilkinson 2010], resulting in a very heterogeneous content. For example, Thelwall [2009] reports that 95% of the exchanged comments in MySpace contain at least one abbreviation (such as  X  X 8 X  for  X  X ate X ) of standard English.

These reasons (i.e., abundance of training documents, easily extractable classes, long documents) have channeled the research in the field towards review-related ap-plications, creating a suitable environment for machine learning approaches, while undermining the utilization of unsupervised approaches, which, although perhaps not particularly effective for product-related datasets, may demonstrate significant advan-tages over machine learning approaches in socially driven environments, where train-ing data (i.e., documents with preassigned classes) is difficult to come by and often requires extensive human labor.

In this article, we propose an unsupervised, lexicon-based classifier that esti-mates the level of emotional valence in text in order to make a prediction, explic-itly designed to address the issue of sentiment analysis in such environments. We have added an extensive list of linguistically driven functionalities to the classifier, such as: negation/capitalization detection, intensifier/diminisher detection and emoti-con/exclamation detection, all of which contribute to the final prediction. The proposed algorithm is applicable in two different but complementary settings: opinion detection (i.e., detecting whether the text contains an expression of opinion or is objective) and polarity detection (i.e., predicting whether a subjective text is negatively or positively oriented), overall constituting a solution that can be applied without any modifica-tion or training to a wide set of environments and settings. Extensive experiments presented in section 5, using real-world datasets from social Web sites and annotated by human assessors, demonstrate that the lexicon-based approach performs surprising well in the vast majority of cases, persistently outperforming state-of-the-art machine learning solutions, even in environments where there is a significant number of train-ing instances for the latter.

Although extensive research has been directed towards automatically creating sentiment lexicons [Baccianella et al. 2010b; Hassan and Radev 2010; Takamura et al. 2005], these attempts typically test the produced dictionaries against a  X  X old-standard X  lexicon (e.g., General Inquirer [Stone et al. 1966]), stopping short of exam-ining their effectiveness in classification scenarios. An important contribution of the present work is to demonstrate how such sentiment lexicons (created automatically or not) accompanied by intuitive syntactic and prose rules can be effectively utilized for this task. Although previous work has also studied the effect of negation or valence shifters detection [Kennedy and Inkpen 2006], this is the first time that an extensive list of stylistic and linguistic modifiers is utilized. Additionally, although the proposed method may be characterized as self-evident, there is a lack of similar approaches in the field to the best of our knowledge. Lastly, it must be noted that this is the first time that the effectiveness of a extensive lexicon-based solution is compared against stan-dard state-of-the-art machine-learning classifiers for the tasks that are being exam-ined here, in multiple test collections, and is found to outperform supervised methods, demonstrating that in certain environments simple and intuitive methods are able to offer a robust and adequate solution.

The rest of the article is structured as follows. The next section provides a brief overview of relevant work in sentiment analysis. Section 3 presents the lexicon-based classifier, and Section 4 describes the datasets that are utilized and details the exper-imental setup while Section 5 presents and analyzes the results. Finally, we conclude and present some potential future directions of research in Section 6. Sentiment analysis has been a popular research topic in recent years. Most research has focused on analyzing the content of either movie or general product reviews (e.g., [Blitzer et al. 2007; Dave et al. 2003; Pang et al. 2002]), but attempts to expand the application of sentiment analysis to other domains, such as political debates [Lin et al. 2006; Thomas et al. 2006] and news [Devitt and Ahmad 2007] have also been devel-oped. Pang and Lee [2008] present a thorough analysis of the work in the field. In this section we will focus on the more relevant to our work approaches. Pang et al. [2002] were among the first to explore the sentiment analysis of reviews focusing on machine-learning approaches. They experimented with three different algorithms: Support Vector Machines (SVMs), Naive Bayes, and Maximum Entropy classifiers, using a variety of features, such as unigrams and bigrams, part-of-speech tags and binary and term frequency feature weights. Their best accuracy attained in a dataset consisting of movie reviews used a SVM classifier with binary features, although all three classifiers gave very similar performance.

Later, the same authors presented an approach based on detecting and removing the objective parts of documents [Pang and Lee 2004]. The results showed an improvement over the baseline of using the whole text using a Naive Bayes classifier but only a slight increase compared to using a SVM classifier on the entire document.

Most other approaches in the field have focused on extending the feature set with semantically or linguistically driven features in order to improve classification accuracy. For example, Mullen and Collier [2004] used SVMs and enhanced the fea-ture set with information from a variety of diverse sources, such as Osgood X  X  Theory of Semantic Differentiation [Osgood et al. 1967] and Turney X  X  semantic orientation [Turney 2002] resulting in an improvement over the baseline of using only unigrams. Similarly, Whitelaw et al. [2005] used fine-grained semantic distinctions in the fea-ture set to improve classification. Their approach was based on a semi-automatically created dictionary of adjectives with their respective appraisal attribute values, which resulted in 1329 adjectives and modifiers in several taxonomies of appraisal attributes. Conjunctions of the produced lexical lemma with different appraisal groups and bag-of-word approaches were used as features to a Support Vector Machine classifier.
Wilson et al. [2009] studied the effect of analyzing the context of words with a known prior polarity. They hypothesized that it may have an impact on how the words are used in the text, for instance, a positive word may have its semantic orientation negated, and concluded that for improved effectiveness it is important to distinguish when a polar term is used in a neutral context. Zaidan et al. [2007] utilized addi-tional human annotation in order to improve classification accuracy, introducing the notion of  X  X nnotator rationales, X  that is, manually extracted words or phrases that strongly indicate the polarity of a document. Very recently, Yessenalina et al. [2010] explored methods of automatically detect such phrases, using OpinionFinder [Wilson et al. 2005b] and a polarity lexicon [Wilson et al. 2005a]. Dictionary/lexicon-based sentiment analysis is typically based on lists of words with predetermined emotional weight. Examples of such dictionaries include the Gen-eral Inquirer (GI) [Wilson et al. 2005b] and the  X  X inguistic Inquiry and Word Count X  (LIWC) software [Pennebaker et al. 2001], which is also used in the present study. Both lexicons are built with the aid of  X  X xperts X  that classify tokens in terms of their affective content (e.g., positive or negative). The  X  X ffective Norms for English Words X  (ANEW) lexicon [Bradley and Lang 1999] contains ratings of terms on a nine-point scale in three individual dimensions: valence, arousal and dominance. The ratings were produced manually by psychology students. Ways to produce such  X  X motional X  dictionaries in an automatic or semiautomatic fashion have also been introduced in research [Baccianella et al. 2010b; Brooke et al. 2009; Hassan and Radev 2010; Turney and Littman 2002]. Dictionary-based approaches have been utilized in psychol-ogy or sociology oriented research [Chung and Pennebaker 2007; Slatcher et al. 2007].
Turney X  X  PMI-IR algorithm [Turney 2002] is one of the few unsupervised approaches that have been presented in the field of sentiment analysis. It is based on automati-cally estimating the semantic orientation of phrases extracted from documents. The orientation of the extracted phrases is estimated based on their collocation with cer-tain preselected reference words, using a search engine as a reference corpus . 1
Qiu et al. [2009] presented a hybrid solution that combines both approaches. The idea is based on an iterative process that uses a dictionary in order to initially classify documents and then train a machine-learning classifier. The trained classifier is then used in order to revise the results of the first stage classification and then it is retrained on the refined classifications. The process is continued until the classification results over two consecutive iterations are the same.
 Our approach shares some common features with the Opinion Observer system of Ding et al. [2008], in that it also uses an opinion lexicon to derive the polarity of a set of terms and it incorporates a negation detection module that detects when the se-mantic orientation of a opinion word is reversed. Nonetheless, the approach presented here determines the overall polarity of a document based on the intensity of its emo-tional content (e.g., a document may be  X  X ery positive X  and at the same time  X  X ildly negative, X  such as  X  X  love you, I miss you!! X ). Additionally, Opinion Observer attempts to extract the semantic orientation of ambiguous words based on their collocation in other reviews of the same product, a process that is inapplicable in the setting of social interactions that we are examining. In addition, our approach incorporates a much wider set of linguistic and stylistic detectors, in order to better capture the intensity of expressed emotion, such as capitalization, emoticons, etc.

Lastly, some similarities are also present between our approach and the Affect Anal-ysis Model (AAM) by Neviarouskaya et al. [2007] and the SentiStrength algorithm by Thelwall et al. [2010], in that they also use emotional lexicons and incorporate modifiers, such as  X  X ery, X  X  X ardly, X   X  X ot X  etc., although those are utilized differently in our approach. However, in contrast to the present study their aim is to predict the emotional intensity and strength of textual utterances, the former in several different categories, for instance, anger, disgust etc. and the latter in terms of positivity and negativity. Additionally, AAM utilizes a syntactic parser, whose effectiveness is doubt-ful in the domain of informal, social communication where, as seen before, syntactic and orthographic errors are the norm and its effectiveness was tested on a limited number of manually selected 160 sentences providing no comparison between the pro-posed system and machine-learning approaches. In contrast, in the present work we report experiments on three different and diverse real-world datasets, some of which contain several thousand posts and compare the effectiveness of our solution against state-of-the-art supervised algorithms.

It is important here to note that most of the previous approaches focus only on one of two problems: subjectivity or polarity detection. In comparison, our proposed algorithm is applicable in both contexts and can therefore offer a much more robust solution to the general problem of detecting emotional content on the Web. The proposed classifier 2 is a typical example of an unsupervised approach, because it can function without any reference corpus and does not require any training (i.e., can be applied  X  X ff-the-shelf X ). The classifier is based on estimating the intensity of negative and positive emotion in text in order to make a ternary prediction for subjec-tivity and polarity, that is, the output of the classifier is one of { 0 , +1 ,  X  1 } . The notion that both negative and positive emotion is present in a text may seem somewhat pe-culiar, but is in accordance with a number or psychological studies [Cornelius 1996; Fox 2008; Schimmack 2001] and is therefore adopted as the underlying premise of our approach. The level of valence in each scale is measured in two independent ratings {
C tive ( C neg = { X  1 ,...,  X  5 } ), where higher absolute values indicate stronger emotion and values { 1 ,  X  1 } indicate lack of (i.e., objective text).

For example, a score like { +3 ,  X  1 } would indicate the presence of only positive emotion, { +1 ,  X  4 } would indicate the presence of (quite strong) negative emotion and { +4 ,  X  5 } would indicate the presence of both negative and positive emotion. In order to make a ternary prediction, the most prevalent emotion, that is, the one with the high-est absolute value, is returned as the final judgement, for instance, positive in the first example given and negative in the other two. For example, the sentence  X  X  hate the fact that I missed the bus, but at least I am glad I made it on time:-) X  expresses both negative and positive emotion, where the latter is considered dominant. We solve con-flicts of equality (e.g., { +3 ,  X  3 } ) by taking into consideration the number of positive and negative tokens and giving preference to the class with the largest number of tokens. ratings are only used as an intermediate step in making the final prediction.
The algorithm is based on the emotional dictionary from the  X  X inguistic Inquiry and Word Count X  (LIWC) software 3 [Pennebaker et al. 2001] which was derived from a number of psychological studies and maintains an extensive dictionary list along with human assigned emotional categories for each lemma. We use the weights as-signed to the LIWC lexicon by Thelwall et al. [2010]. The justification of utilizing the particular word-list is twofold: a) it has been extensively used in psychology-related research and its development is solely driven by psychological studies, in comparison to other review-oriented [Turney and Littman 2002], or linguistic-based [Baccianella et al. 2010b], dictionaries, and b) it is better suited for the type of informal communi-cation [Thelwall et al. 2010] usually found in social media. All dictionary lemmas (as well as processed text) are stemmed using the porter stemmer.

Given a document d , the algorithm detects all words that belong to the emotional dictionary and extracts their polarity and intensity. We modify the initial term scores with additional, prose-driven functionalities such as: negation detection (e.g.,  X  X ood X  versus  X  X ot good X ), capitalization detection (e.g.,  X  X ad X  versus  X  X AD X ), exclamation and emoticon detection (e.g.,  X  X appy!! X  and  X :-) X ) intensifiers (e.g.,  X  X iked X  versus  X  X iked very much X ) and diminishers (e.g.,  X  X xcellent X  versus  X  X ather excellent X ), to produce the fi-nal document scores. The list of modifiers and their respective weights was adapted from Neviarouskaya et al. [2007] and Thelwall et al. [2010] and manually checked for duplicates and conflicts.

The modules function in the following way: the neighborhood of every word that is present in the text and belongs to the LIWC lexicon is scanned for  X  X pecial X  terms, such as negators (e.g.,  X  X ot X ) intensifiers (e.g.,  X  X ery X ) or diminishers (e.g.,  X  X ittle X ). In order to capture long-distance phenomena, for instance,  X  X  don X  X  think this is a good movie..., X  neighborhood is defined as the area 5 words before and after the emotional term or the end or beginning of the sentence (defined as the encounter of a full stop, comma or question mark), whichever comes first. Preliminary experiments, not presented here due to space constraints, showed that the algorithm is quite robust to different neighborhood radii, and the specific threshold was chosen as it is consistent with some prior work [Santos et al. 2009].

At this point, it is important to explain why this simple approach was chosen for modifier detection, rather than a more syntactically correct approach of analyzing the text through a parser Jiang and Liu [2010] in order to extract the syntactic dependen-cies of the text. The reason is that, as mentioned before, the vast majority of informal textual communication contains significant spelling errors, making any such attempt very difficult and additionally seriously limiting the domain of applicability of the pro-posed solution. Some examples of such informal communication are provided later, in Table II, where the datasets that are used in this study are presented.

If an intensifier or diminisher word is found, then the original emotional value of the word is modified by the respective modifier score which is either added (in case of an intensifier) or subtracted (in case of a diminisher) to the absolute value of the term. This approach was adopted, instead of a more uniform  X  1 modification, because some modifiers are more intense than others, for instance, compare  X  X airly good X  with  X  X xtremely good X . For example, if  X  X ad X  has an initial value of  X  3 then  X  X ery bad X  would be modified to -4. Similarly,  X  X omewhat good X  would be judged as +2, taking into consideration that  X  X ood X  has an original value of +3.

If a negation term is found then the absolute value of the emotional term is de-creased by 1 and its polarity is reversed. For example  X  X ot bad X  would be +2. The intuition behind the reduction by one (instead of a simpler reversal of signs) is that although the polarity of a term is reversed with the usage of negation, the full original emotional weight of a term (such as  X  X ad X  in the given example) is not fully transferred to the other class and thus the reduction by one. Simply put, one does not typically use  X  X ot bad X  if one means  X  X ood. X 
Last, for the capitalization detection module, if a word, larger than two characters (in order to avoid false positives caused by normal article capitalization after a full stop), that is written fully in capital letters is detected within the neighborhood of an emotional word, including the actual emotional word, then the weight of the word is modified in the same manner as if an intensifier with a weight of 1 was detected. The exclamation detection module functions in the same manner. In contrast, emoticons are considered as explicit indicators of emotion [Derks et al. 2008] rather than modifiers and are assigned specific weights, that is, +3 for positive emoticons and -3 for negative.
The score of a document on the C pos and C neg scales is the maximum positive and negative number produced, respectively. As previously stated, for binary posi-tive/negative prediction the class with the highest absolute value is considered dom-inant. A document is classified as objective if its scores are { +1 ,  X  1 } . Algorithm 1 presents the full details of the classifier in pseudocode. We used three different datasets extracted from real-world social Web sites, in order to test the effectiveness of the proposed lexicon-based classifier.

The first dataset was extracted from the social networking and microblogging Web site Twitter. 4 The dataset is comprised of two subsets. The first one (henceforth re-ferred to as Train ) was collected through the Twitter API, based on the existence of particular emoticons, which were used to provide an indication of the polarity of the text: positive for tweets that contain  X  :)  X  X r X  :-)  X  and negative for tweets that contain  X  :(  X  X r X  :-(  X . Although this subset is at least two orders of magnitude larger than previ-ous datasets, it is expected to be more  X  X oisy X  than human-annotated data, containing misclassified documents. It is expected nonetheless, and in fact the reason for the cre-ation of datasets in such an automatic-fashion [Go et al. 2009; Read 2005] is that the abundance of training documents will override such errors and overall produce ade-quate supervised classifiers. The second subset (henceforth referred to as Test )was humanly annotated for objective, positive, and negative emotion. More information on the dataset is provided by Pak and Paroubek [2010].

The second dataset is from the social news Web site Digg, 5 one of the most popu-lar sites on the Web where people share and discuss news and ideas. The site is very loosely administered and therefore any kind of language (including profanity) is al-lowed. The original dataset spans the months February-April 2009 and contains about 1.6 million individual comments. The dataset is described in detail by Paltoglou et al. [2010] and is freely available.

The last dataset is from the social Web site MySpace, and it comprises a sample of comments exchanged between friends in each other X  X  public profile. The sample con-tains 40,000 profiles of members who joined on July 2007. All samples are filtered Algorithm 1 Lexicon-Based Classifier based on the country of origin of the user, and only those that were based on UK or USA were kept. Thelwall and Wilkinson [2010] provide more information about the dataset.

A random subset of 1,000 comments was sampled from the last two datasets and given to 3 human assessors with the task of manually annotating their emotional content on two 5-point scales for positive and negative sentiment: [no positive emotion or energy] +1,+2,...,+5 [very strong positive emotion] and [no negative emotion]  X  1,  X  2,...,  X  5 [very strong negative emotion] . The rea-sons for the specific annotation scheme as well as more information about the annota-tion process and the inter-annotator agreement is provided by Paltoglou et al. [2010]. Note that the process was applied only to the last two datasets and that the Twitter corpus already contains human ternary annotations for positive, negative, and neutral content.

We mapped the original two-dimensional 5-point scale human annotation to a binary scheme (objective vs. subjective and positive vs. negative) in the following manner.  X  All the posts that have been rated by the majority of annotators (at least 2 out of 3) with scores  X  1 and +1 are considered  X  X bjective. X   X  All posts that have been rated by the majority of annotators (at least 2 out of 3) with a positive score equal or higher than +3 and a negative score of  X  1or  X  2 are considered  X  X ositive. X   X  All posts that have been rated by the majority of annotators (at least 2 out of 3) with a negative score equal or lower than -3 and a positive score of +1 or +2 are considered  X  X egative. X  We use the union of positive and negative posts as  X  X ubjective. X  Although this process results in a smaller subset of the original 1,000 posts per dataset (see Table I), it also guarantees a high interannotator agreement on a ternary scale and the remaining posts are much more definitive of their emotional content as some of the ambiguities of the original annotations are removed. 6 In the experiments that are presented we use this subset as the  X  X old standard X  to train/test the machine-learning classifiers and evaluate the effectiveness of the lexicon-based classifier.

We used these three datasets because they provide typical examples of three inher-ently different types of online communication, that is, an open discussion environment, a social networking site, and a microblogging service, all of which are characteristic of the type of informal communication that takes place on the Web. Additionally, al-though smaller in size compared to the typical datasets used in review-oriented senti-ment analysis, [Pang et al. 2002], the aforementioned datasets are the largest, to our knowledge, that contain emotional annotations by human assessors of social media communication, therefore providing a  X  X olden standard X  for classification.
Table I presents some statistics about all three datasets, and Table II presents some characteristic comments from each.

We used three machine-learning approaches in order to compare the effectiveness of the proposed lexicon-based classifier: Naive Bayes (NB) [McCallum and Nigam 1998], Maximum Entropy (MaxEnt) [Nigam et al. 1999] and Support Vector Machines (SVM) [Chang and Lin 2001; Joachims 1999], using unigrams as features. All three have been used extensively in research and are considered state-of-the-art. Previous re-search in sentiment analysis [Pang et al. 2002] on review datasets has shown that binary features, which only capture the presence or absence of a term, often outper-form term frequency features, where the weight of a feature is equal to the number of times that feature occurs in the document, but since no comparison has been con-ducted on the specific environment that we are examining here, we present results using both weighting schemes for the SVM and Naive Bayes classifiers, 7 represented as SV M pr , NB pr and SV M tf , NB tf respectively. All machine-learning algorithms were implemented using the Weka Toolkit [Witten and Frank 1999]. 8
We present results using 10-fold cross validation for the machine-learning ap-proaches for the Digg, MySpace and Twitter-Test datasets for two classification tasks: objective vs. subjective and positive vs. negative. In addition, we use the Twitter-Train subset to train the classifiers on polarity classification and test them, in a hold-out evaluation, on the Twitter-Test subset. The lexicon classifier is tested on the complete datasets, since it does not require any reference corpus. Because of the fact that the datasets are unbalanced we also present results using two baselines: majority and random. In the former case, the class with the highest number of documents is always predicted, and in the latter case, a class is selected at random.

As is typical with unbalanced datasets [Li et al. 2005] we present results based on the average value of the F 1-score for both categories [Manning et al. 2008] to quan-tify classification quality. Category-based precision ( Pr . ), recall ( R . ) and F1 are also reported for completeness reasons. The F 1 for class i is defined as: where Pr i and R i are the precision and recall that the classifier attains for class i respectively, defined as: where tp is the number of documents correctly classified as belonging to class i ( X  X rue positive X ), fp is the number of documents falsely classified as belonging to class i ( X  X alse positive X ) and fn is the number of documents falsely classified as not belonging to class i ( X  X alse negative X ). The final average F 1 measure is calculated as F 1= 1 | i | i F 1 i . The results for polarity classification are shown in Tables III to V and for subjectivity detection are shown in Tables VI to VIII. The results from the polarity classification task on the Digg dataset are presented in Table III. The majority and random approaches provide a measure of baseline results. As it can be seen their average F1 value ranges from 40 . 3% for the former to 48 . 4% for the latter. The machine-learning approaches perform, as expected, much better with the two SVM approaches overall outperforming the Naive Bayes and the Max-imum Entropy classifier and also the latter outperforming the former. Additionally, presence-based features outperform frequency-based features, a result which is in ac-cordance with previous research [Pang et al. 2002] providing an potential indication for the existence of some  X  X niversal X  concepts for sentiment analysis, whether it is applied in product reviews or social communication. The lexicon-based classifier note-worthily outperforms the best of the supervised approaches (76 . 2% vs. 72 . 7% average F1), providing a first indication of its potential. Taking into consideration the diver-sity of topics discussed on Digg and the liberal usage of language at the Web site, the results can be considered very encouraging.
 Results from sentiment analysis on the MySpace dataset are provided in Table IV. In this setting, the Naive Bayes classifier using binary features performs unexpectedly well, even though still lower than the SV M pr classifier. Closer examination of the class-based results of the NB classifier nonetheless shows that the result is somewhat  X  X rtificial X  and does not reflect true effectiveness. Indeed, the result is mainly caused by the fact that the dataset is heavily unbalanced towards positive documents 9 and the well-documented preference of the Naive Bayes classifier for the more popular class. The lexicon-based classifier again manages to outperform every other approach, both in average F1 (80 . 6% vs. 73 . 2%) and even in class-based F1 values, 92 . 9% vs. 90 . 4% for the positive class and 68 . 3% vs. 56 . 3% for the negative (comparisons are always with the best performing supervised classifier at the particular metric). The results demonstrate one of the key advantages of using an unsupervised classifier in social-networking environments where gathering training data is not only very time-consuming and requires human labor (as in this case) but can often result in heavily unbalanced data, which, produce models that may have undesired biases. In comparison, the lexicon-based classifier can be applied  X  X ff-the-shelf X  without any modification or training and still produce good results.

Table V presents results from the Twitter dataset and are of particular interest, because of the existence of the significantly larger Train subset. In the 10-fold cross validation setting, the results are similar to the previous two settings, with the newly proposed solution managing to outperform all machine-learning approaches, by a considerable margin in almost every metric, for instance, 86 . 5% vs. 75 . 3% for average F1.

We focus on the performance of the lexicon-based classifier against the machine-learning approaches that have been trained on the more voluminous Train subset. As expected therefore, most of the algorithms, with the exception of the SVM classifier using term frequency features, perform better, taking advantage of the abundance of training documents. It must be noted that the performance of the machine-learning approaches in this setting is not greatly improved over the results produced in the previous setting, despite the existence of an order of magnitude more training data, demonstrating that applying sentiment analysis in informal textual communication is a particularly challenging problem. Nonetheless, the proposed unsupervised classifier is still able to outperform all approaches obtaining an average F1 value of 86 . 5% vs. the best performing supervised solution of 80 . 7% ( MaxEnt ). The specific results are of particular importance because they demonstrate that even in environments where massive training data can be collected through an automatic or semiautomatic fash-ion, the lexicon-based classifier is still able to outperform the produced supervised models.

Overall, the results from the polarity classification task show that the new classifier offers a very robust performance and is able to outperform machine-learning classi-fiers in most cases, even in settings where there are a significant number of training documents. This result is very important as it clearly demonstrates the robustness of the approach to different settings and social environments. The significant ad-vantage of the method is that it requires no training and can be applied without any modification, therefore it could easily and readily be applied to other similar environments which are not tested here, such as Facebook, blogs, and forums, offering overall a very robust solution. We change our focus from polarity classification to subjectivity classification, that is, the detection of whether segments of text contain opinions or are objective. As dis-cussed in Section 4, we have used the union of positive and negative documents as subjective (see Table I).

Table VI presents the first results of this task on the Digg dataset. Similarly to the polarity classification task, the lexicon-based classifier is able to outperform machine-learning approaches: 77 . 0% vs. 71 . 9% for the best performing supervised approach ( SV M pr ). Taking into consideration that the lexicon-based classifier also demonstrated very good performance in the previous task in the same dataset, it can be concluded that the algorithm provides a very good solution to the general problem of initially detecting and subsequently classifying opinions in such open discussion systems.
Results from the subjectivity detection task on the MySpace dataset are provided in Table VII. Again, the lexicon-based classifier is able to outperform, although only slightly, supervised approaches: 79 . 7% against 79 . 2% for the best performing super-vised approach ( SV M tf ). The results indicate that the proposed solution is not only able to effectively detect subjectivity in open discussion Web sites, such as Digg, but is also effective in social networking sites, such as MySpace and therefore potentially Facebook, Flickr, etc.

Last, we present results from the Twitter dataset for the task of subjectivity detec-tion at Table VIII. This is the only setting that the machine-learning approaches out-perform the lexicon-based classifier, which attains an average F1 of 70 . 9%. The SV M pr classifier performs best in this setting (75 . 3%), followed by the SV M tf (72 . 1%) and then by the Naive Bayes classifiers using binary (71 . 7%) features. Peculiarly enough, the lexicon classifier still outperforms the Maximum Entropy (63 . 9%), which was the best performing machine-learning classifier in the polarity classification task in the Twitter dataset (Table V). Careful analysis of the class-based metrics in this environ-ment reveal that the lexicon-based approach attains the best recall for the objective class and is therefore able to correctly identify most of the objective documents in this dataset while at the same time also attaining the best precision for subjective tweets, overall demonstrating that even when the classifier does not offer the best effectiveness over machine-learning approaches, it still manages to perform very suc-cessfully for some specific subtasks (i.e., a high-precision detection task for subjective documents).

Overall, the results from the subjectivity detection task show that the lexicon-based classifier proposed in this article is able to perform very adequately in the majority of environments and overall offers a very reliable solution. The fact that the classifier was tested in three inherently different types of online environments, that is, an open discussion environment, a social networking site and a microblogging service, without any modifications or training, provides a clear demonstration of its potential. In this article, we addressed the problem of sentiment analysis in social networking media, such as MySpace, Twitter, Digg, forums, blogs, etc. We argued that this area of application provides unique challenges, not addressed in typical review-focused senti-ment analysis environments.

We proposed an intuitive, unsupervised, lexicon-based algorithm which estimates the level of emotional strength in text in order to make a final prediction. Our proposed solution is applicable to two complimentary tasks: subjectivity detection and polarity classification, overall providing a comprehensive solution to the problem of sentiment analysis of informal communication on the Web. The advantages of the approach is that it requires no training and thus can be readily applied into a wide selection of environments.

We used three different real-world, humanly annotated datasets to compare the effectiveness of the classifier against state-of-the-art machine learning approaches. All datasets were extracted from popular Web sites, are publicly available and are indicative of the diverse set of environments that are available to users today: an open news-sharing and discussion forum, a social networking Web site and a microblogging service. Although, naturally, we have not exhausted the list of potential environments, we believe that the specific datasets provide an indicative list of relevant settings.
In the polarity classification task, the newly proposed classifier was able to outper-form machine learning approaches in the majority of experiments, even in settings where significant training data was available. Similarly, in the subjectivity detection task the lexicon-based approach was able to perform very well in the majority of en-vironments, outperforming other solutions in most cases. Overall, the experimental results demonstrated that the proposed solution, although simple in its conception offers a very robust and reliable solution.

In the future, we plan to incorporate the ANEW list of words [Bradley and Lang 1999], which readily provides emotional weights for tokens on a 1 X 9 scale, to the list of utilized lexicons used by the algorithm. Additionally, we plan to expand the emo-tional prediction capabilities of the algorithm to all three dimensions that the ANEW provides, that is, arousal and dominance in addition to valence, thus providing a more emotionally comprehensive analysis of textual communication.

In order to further improve the performance of the lexicon-based approach we plan to incorporate machine learning techniques to optimize the emotional weights of to-kens and modifiers. The aim of this approach would to make the algorithm more adaptable and easily configurable to different and novel environments.

Despite the fact that in this work we utilized a static emotional lexicon that can be easily manually expanded if new affective words or expressions become popular in social media, we intend to explore methods of automatically expanding it without human intervention. Such a process could function in a bootstrapping fashion, using the already existing lexicon to classify documents and extracting new words that have a significant discriminative power, based on some feature selection criteria, such as information gain, for addition to the emotional lexicon.

Finally, we also aim to experiment with non-English text, by using different, language-dependent emotional lexicons and translating the already available ones. Our goal is to extend the applicability of the proposed solution to other languages, for which training data is especially difficult to come by.

