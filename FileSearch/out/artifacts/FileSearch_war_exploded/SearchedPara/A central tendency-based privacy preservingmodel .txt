
Department of Computer Science, School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China
Institute of Automation, Chinese Academy of Sciences, Beijing, China College of Computer Sciences and Information Technology, King Faisal University, Saudi Arabia 1. Introduction
Privacy Preservation in Data Mining (PPDM) is an important issue in knowledge discovery especially from joint databases. Clifton et al. [5] have presented a comprehensive review of PPDM techniques to minimize the risk of disclosure of mined knowledge from external parties using association rules. PPDM techniques for association rule can generally be classified into data perturbation, support/confidence and hybrid approaches. Data perturbation approaches refer to transformation of original database by delet-ing sensitive items to hide sensitive association rules. The released or modified database is obtained after transforming original database by using strong rules [20], maximum prior weights [38], index ta-bles [41], filed rotation and binning averages [21], maximum conflict degree transactions [30], sensitive items in the correlated rules [14], application of sanitization algorithm on recurrent itemset [17] or mod-rules. The discovered association rules are interesting if they satisfy the minimum support and mini-mum confidence thresholds. These measures have been focused by researchers to improve privacy of association rules by manipulating support, confidence or both. Support and confidence framework ap-proaches can be found in literature such as Increase Support on LHS (ISL) and Decrease Support on RHS (DSR) [37], interval based support (minSup, maxSup) and confidence (minConf, maxConf) with Safety Margin (SM) [32], Decreasing Support Rule (DRS) algorithm [18], increasing and decreasing support of antecedent and consequent of an association rule [18] and inverted file [8,27]. A failure in hiding sensitive association rules directed the attention of academia towards hybrid approaches. Hy-brid approaches are composed of either aforementioned two techniques, or with the use of existing algorithms by perturbing data source to come up with a better solution. Hybrid techniques for privacy preserving in association rules are based on central tendency as weighing mechanism [26], Naive Bayes with Secure Multiparty Computation (SMC) for collaboration over vertically partitioned data to present a comprehensive model [16,35], transformation of quantitative database into Boolean format and with membership function [22] and Least Modified Algorithm (LMT) by modifying transactions for ran-domly selected sensitive association rules [42]. However, an optimal transformation is still an NP-hard problem, although some approximate solutions do exist such as [3]. In recent decade, it has been realized to standardize the data format with the rapid increase in the internet usage for e-commerce. eXtensible Markup Language (XML) has been developed by World Wide Web Consortium (W3C) for transforma-tion and storage of information with more flexibility and simplicity [10,15]. A number of XML mining techniques have been developed to discover knowledge through association rules such as incremen-tal data mining algorithm [40], querying XML document [6], query recommendation technique with an architectural framework [4], Standardized Data Source Template (SDST) [36], highly adaptive data mining techniques [24] and Hierarchical Layered Structured Pairset (HiLoP) [33]. Privacy preserving in association rule mining has not only been addressed in XML domain [19] using Bayesian networks but also, in finding spatial association rules in image databases [23] and discovery of frequent itemset in dis-tributed environment [43]. The main limitation of securing association rules in XML mining techniques is generation of side effects in the form of new and lost rules. These uncontrollable side effects are sig-nificant for the reason of constructing a hiding strategy based on assumed sensitive association rules, transformation of database and sensitive items. A subset of association rules is declared to be sensitive if it discloses valuable information to the external world. Therefore, sensitive association rules must be secured in the process of data mining. The process of data mining must ensure preservation of sensitive association rules with significant belief rather than assumptions. Bayesian networks (also known as be-lief networks) have been widely used in different domains [13,25,28,31,44] including privacy preserving in XML association rule mining [19] with an objective to accomplish consistent accuracy. Bayesian net-works represent a probabilistic graphical structure to discover knowledge about an uncertain domain. A graphical model consists of nodes / items and edges represent probabilistic dependencies for the corre-sponding nodes / items. Nodes or items represent random variable in this work. Bayesian networks can be developed using K2 algorithm [7]. K2 algorithm discovers relationship among items / nodes in an in-is the probability of XML database ( X D ) given the parents of i th node / item are  X  i . node / item in  X  i are instantiated with the j th instantiation in  X  i .

In this paper, a central tendency-based privacy preserving model for sensitive XML association rules using Bayesian networks is presented. The proposed PPDM model is composed of large frequent item-sets, K2 algorithm and apriori algorithm [7,34]. K2 and apriori algorithms are used to find Bayesian networks and large frequent XML itemset respectively. K2 algorithm is used to discover relationships among XML items/nodes in an incremental fashion with a predefined items X  order. The entire set of items/nodes is recorded. This set consists of the most influential nodes/items that are closely related to each other according to their occurrence in XML database. Central tendency measures are computed to find the most sensitive node/item to perturb the largest sized transaction of the original XML database. Apriori algorithm [34] is used to find XML large frequent itemset and discover XML association rules before and after perturbation XML database with the use of same minimum support threshold. The previously mined XML rules containing sensitive node/item either in antecedent or consequent are re-ferred to as sensitive XML association rules. The comparative results of central tendency measures via Bayesian networks over sample datasets and large UCI machine learning dataset are proven to be bet-ter than the existing privacy preserving for association rules [19,27,29,37]. The significance of proposed central tendency measures based privacy preserving with the use of Bayesian networks is the elimination of undesirable side effects in the form of new and lost rules.

The main contributions in this paper are as follows:  X  A sensitive item (node) has been determined automatically to choose sensitive XML association  X  The original XML based horizontal transactional item set is perturbed using sensitive item for the  X  Extensive experiments have been carried out over small (literature based case studies) and large  X  The privacy preserving model presented in this work has no side effects if original dataset is per-
Rest of the paper is organized as follows. Problem statement is explained in Section 2. Section 3 dis-cusses related work and the proposed privacy preserving model for XML association rules is presented in Section 4. Section 5 shows an experimental comparison of the proposed model with the existing approaches. Finally, in Section 6, we conclude the paper and present future work. 2. Problem statement Association rules mining was introduced in [2] and privacy preserving problem was explained in [5]. For mining association rules, Agrawal et al. [2] considered a set of transactions ( T ) consisting of a variable number of items ( I ) in a customer X  X  basket as a database. XML database ( X D ) , in this work, is also composed of a set of transactions ( T ) each having a unique identifier with items ( I ) . XML database (
X D ) can be represented in set theory notations. Consider a set of items I = { A, B, C, D, E, F, G } with transactions T = { ABCD,ABC,ABD,ACD,ABC,BD } in XML database X D = { ( tid =1 ,T 1 = ABCD ) , ( tid =2 ,T 2 = ABC ) , ( tid =3 ,T 3 = ABD ) , ( tid =4 ,T 4 = ACD ) , ( tid =5 ,T 5 = ABC ) , ( tid =6 ,T 6 = BD ) } . Association rules mined from a given X D are A  X  B (66.67%, 80%), A  X  C (66.67%, 80%), A  X  D (50%, 60%), B  X  C (50%, 60%), B  X  D (50%, 60%), C  X  D (33.33%, 50%), A, B  X  C (50%, 75%), A, B  X  D (33.33%, 50%), A, C  X  D (33.33%, 50%). All these association rules satisfy the minimum support and minimum confidence threshold criteria. The support and confidence of a rule can be computed by Eqs (2) and (3) [2].

The association rules S R = { A, B  X  D (33.33%, 50%), A, C  X  D (33.33%, 50%)} are declared to be sensitive in [27]. Three transactions S T = { T 1 ,T 3 ,T 4 } are considered as sensitive with their respective degrees { 2 , 1 , 1 } for a particular sensitive rules. The maximum degree transaction tid = 1 ,T 1 = ABCD is transformed because any superset of tid =3 ,T 3 = ABD for sensitive association rules S R = { A, B  X  D (33.33%, 50%), A, C  X  D (33.33%, 50%)} should not be allowed to be but the results are not very promising. Therefore, we focused on identifying sensitive association rules in XML domain and hiding them with the use of central tendency measures using Bayesian networks. Sensitive items are recognized by perturbing the original database automatically. Formally, the problem addressed in this work can defined as follows. Consider R = { A  X  B ; A  X  C ; A  X  D ; B  X  C ; B  X  D ; C  X  D ; A, B  X  C ; A, B  X  D ; A, C  X  D } be the entire set of XML association rules mined from X D and S R = { A, B  X  D ; A, C  X  D } be the set of sensitive XML association rules, that is S R  X  R . The objective is to automatically spot and hide the entire set of sensitive XML association rules based on sensitive items. To identify sensitive items ( S ) , our strategy is based on computing central tendency measures using Bayesian networks. Bayesian networks are used to measure the relationship among the items and these items are recorded according to ascending order. Central tendency measures are used to identify the sensitive item on the recorded related occurrence of items in Bayesian networks. The original XML dataset is perturbed by deleting sensitive items in the maximum degree transaction as in [27]. The transformed XML dataset is passed to apriori algorithm [34] to mine XML association rules without any change in the support and confidence measures. 3. Related works
The approaches for hiding sensitive association rules can be broadly divided into five categories namely; data perturbation approaches, Support/Confidence based approaches, Hybrid methods on sen-sitive association rule mining, XML association rule mining and Bayesian networks with reliability analysis and estimation for privacy preserving in the subsequent sub-sections. 3.1. Data perturbation approaches
The transformation of the original data source is referred to as data perturbation. The objective of per-turbing data is to hide sensitive association rules. Jinturkar et al. [20] presented an approach to measure formed based on strong rules. A rule ( A  X  C ) is said to be strong if it satisfies MST (Minimum Support Threshold) and MCT (Minimum Confidence Threshold) criteria. This criteria leads to the sensitivity analysis. The proposed framework converted the original data source ( D ) into transaction table and used it for efficient updating of template table. However, the updated template table resulted in unchanged, false and lost rules. Weng et al. [38] proposed FHSFI (Fast Hiding Sensitive Frequent Itemset) algorithm to generate a released database to hide SFI (Sensitive Frequent Itemsets) with minimum side effects. Af-ter scanning entire database, the transaction X  X  degree of correlation with SFI and a prior weight ( w i )of each transaction is computed heuristically. Thus, a large number of transactions with maximum prior weights is transformed to hide limited number of known SFI. In addition to hide sensitive rules, Wu et al. [41] proposed a framework which used an index table to transform the frequent itemset with an objective of fast retrieval of transactions and rules. Moreover, four lemmas were included in the sani-tizing algorithm to delete, insert and search the transactions with an increase or decrease in support of x or x  X  y . Consequently, greedy approximated approach is applied to the selected transactions with an aim to have minimum lost and artifactual rules. However, side effects are not reflected in experimental results. Kadampur et al. [21] presented novel methodology to perturb the numerical database to preserve sensitive information by field rotation and binning averages. The originality of data is compromised without identification of sensitive values or attributes for perturbation. Likewise, the variable support selection of sensitive data is chosen for each rule to define approximate support nearest to the actual support. Rajalaxmi et al. [30] proposed a novel CUIS (Conflict Based Utility Sanitization) approach to transform the original data source by reducing the sensitive itemset utility. The conflict degree for each known sensitive item carrying transaction is worked out. The maximum conflict degree transactions are selected and transformed after computing utility difference. An item with the maximum utility is deleted in the sanitization process. Duraiswamy et al. [14] proposed a new algorithm to secure sensitive rules. This algorithm assumes consequent based sensitive items with two items in association rule only. Sen-sitive rules are clustered based on consequent of rules with higher confidence values to declare them as sensitive. Transactions in the data source are perturbed after identifying the sensitive items in correlated rules without controlling side effects in terms of lost rules over a small dataset. Therefore, the proposed algorithm can yield more side effects over large dataset. An innovative reconstruction-based framework is proposed by Guo et al. [17]. This framework is based on FP-tree to mine frequent set conversely in three phases. First, all frequent itemsets are generated from the original database with their support values. Sanitizing algorithm is executed over recurrent itemset in order to outline hiding strategy and to recognize sensitive itemset. In the next step, released database is produced from the non-sensitive recurrent itemset by adopting FP-tree based frequent set mining algorithm. The framework considers the assumed sensitive rules to tailor the hiding strategy with drastic changes in the original data source. Pathak et al. [29] proposed a new sanitizing algorithm for hiding sensitive rules in three phases: itemset lattice generation, knowledge sanitization and data reconstruction. The first step is the replication of traditional fact as in [21] by mining lattice from the power set of items. The second step considers the itemset lattice and sensitive rules as input to modify the lattice. The third step generates a transformed database from the modified itemset lattice by mining frequent set conversely. The presented sanitizing algorithm finds satisfactory results in comparison over a small dataset. On the contrary, the framework does not consider new or lost rules after producing the dataset from the modified itemset lattice. The framework motivates to address selection of identification of sensitive rules or items based on explicit criteria. 3.2. Support/confidence based approaches
An Association rule can be called interesting if it satisfies the minimum support and confidence cri-teria. Researchers have focused on support and confidence measures to minimize the disclosure risk in association rule mining. Wang et al. [37] proposed Increase Support on LHS (ISL) and Decrease Support on RHS (DSR) algorithm to hide sensitive rules without prior knowledge. ISL increases the antecedent support and DSR decreases support of rule X  X  consequent. The traditional way of support and confidence manipulation could not minimize side effects. ISL algorithm resulted in lost rules excluding DSR. Say-gin et al. [32] presented new metrics for preserving privacy of association rules to find a generalized framework. Therefore, new metric represented by  X ? X  is introduced to alter the original setting. Addi-tionally, interval based support ( minSup , maxSup ) and confidence ( minConf , maxConf ) metrics are used to transform definition of support and confidence. Th ese ranges can be overlapped for the databases in which this metric does not exist. Sensitive association rules are assumed to prove minimum effect to non-sensitive rules and data quality. Safety Margin ( SM ) is introduced to make sensitive rules secure after reduction in minimum support and confidence. Despite SM, the new metrics framework could not control the side effects in privacy preservation of association rule mining. Gupta et al. [18] presented DRS (Decrease Rule Support) algorithm after incorporating fuzzy sets concepts and apriori algorithm. In this formation, a quantitative dataset (Wisconsin Breast Cancer) attributes were transformed to fuzzy sets with the help of membership function. DRS was used to mine fuzzy association rules with single items in antecedent and consequent. So, DRS algorithm maintains a higher quality of released database for known sensitive rules in advance. Dasseni et al. [9] investigated confidentiality issues of association rule mining by increasing and decreasing the antecedent and consequent support respectively. The pro-on several assumptions rather than on substantial foundation. Strong or interesting association rules are considered as sensitive rules to hide. However, the procedure of selecting strong rules as sensitive is not described in detail. Similar to [32], Clifton et al. [5] discussed the repercussions of privacy preserving by providing metrics and framework as a foundation for data mining. The main objective was to outfit implications for defining and measuring the privacy concerns over large scale. The privacy policies can be prepared for individual X  X  personal data [11] on legal grounds. Data Obscurity and Perfect Privacy ap-proaches are the essential necessity to preserve the privacy of sensitive data that can be disclosed in any form. In general, the corporate data can also be protected by building universal models as well as indi-vidual identifiable data. To unify the sensitivity issue, Oliveira et al. [27] presented a Unified Framework for Protecting sensitive Association Rules (UFPAR). UFPAR is based on Sensitive Association Rules (SAR) and Sensitive Transactions (ST) [27]. The sensitive knowledge is protected with the use of SAR and ST. The framework prepares an inverted file [8,27] to protect SAR. This inverted file is composed of sensitive rules and transactions IDs. The sensitive rules are composition of list of transactions. The in-verted file is passed on to the sanitization algorithms with an access to transactional database to produce sanitized database. A set of metrics is introduced to quantify the disclosed sensitive knowledge as well as to measure effectiveness of the proposed algorithms. The purpose of such metrics is to prevent the information loss and the side effect of the transformation.These two classified metrics are listed below. 1. Data sharing based metrics; 2. Pattern sharing based metrics.

The optimal sanitization is an NP-hard problem [3,27]. Heuristics are used to find a close to optimal solution [9,27] to reduce sanitization complexity. The following sanitization heuristics [27] are used to hide sensitive association rules in the transactional database.
 1. Degree of sensitivity; 2. Size of sensitive transaction; 3. Blocked inference channel.

Finally, Item Grouping Algorithm (IGA), Sliding Window Algorithm (SWA) and Downright Sanitiz-ing Algorithm (DSA) are presented based on the above listed sanitization heuristics respectively. 3.3. Hybrid methods on sensitive association rule mining
In PPDM, the processes for mining rules, hiding failures, artifactual rules and Misses Cost (MC) are the most important challenges. Naeem et al. [26] proposed a novel architecture to address some of these challenges. Central tendency is used in their proposed approach as a weighing mechanism. The weighing mechanism itself raises questions like  X  how variables in computations correlate to each other for certainty about weighing?  X  Every interesting rule is considered to be sensitive, which is not true, because S R  X  R . The architecture demands a review for consistent sensitivity in rule mining with in-tegrity. Vaidya et al. [35] presented a two-party algorithm over vertically partitioned data. The purpose of two-party algorithm is to secure sensitive information from external parties in a distributed data mining setting. Na X ve Bayes algorithm is used to secure sensitive information. Secure Multiparty Computation (SMC) [16] is used in designing an efficient algorithm. This algorithm computes probabilities of shared entities and, finds mean and variance without disclosing it to any class attribute and nominal attribute. The encrypted vector along with the encrypted key is computed based on nominal attribute to forward it to class attribute. Variances are computed and a random value is subtracted from its shared distances from forwarded data. Homomorphic encryption is used to ensure security of information without de-cryption. The components of such semi-honest supported model can be grown to a nasty model as a tempting research problem. Krishna et al. [22] presented a novel method to mine association rules from quantitative data by transforming it into Boolean format. These rules are called BAR s (Booleanized Association Rules). The quantitative association rules are referred to as SAR s (Statistical Association Rules) and FAR s (Fuzzy Association Rules). SAR s are represented with mean and standard deviation use of membership function like m fx ( x ): D ( x )  X  [0 , 1] . This transformation gives fuzzy sets like High (H), Medium (M) and Low (L). The commodity export dataset (99 commodities as columns and 220 countries as instances) is used for evaluation. Nevertheless, this novel approach requires further op-timization and improved memory consumption. Besides this, the sensitivity of association rule mining is not taken into account. Xue et al. [42] proposed Least Modified Transaction (LMT) algorithm to hide sensitive rules. This algorithm considers correlation between association rules and transforms transac-tions to hide randomly selected sensitive rules. In the first step, the algorithm reduces the likelihood of hiding failure by increasing support or confidence of few sensitive rules and deletes the modified rules after comparison. In the second step, LMT transforms the entire set of transactions to hide maximum number of sensitive association rules. The key features of LMT algorithm are efficiency, reduced hid-ing failure, reduced side effects and minimum modified transactions. LMT algorithm still produces side effects in privacy preservation process of association rules. For LMT, a question can be raised;  X  X ow to specify sensitive rules for hiding amongst the entire set of interesting association rules? X  All the techniques discussed so far do not provide a robust solution for preserving privacy of association rules. Atallah et al. [3] focused on set of association rules R mined from a source database D . The problem focused is how to transform a database D into a released database D  X . A set of sensitive rules ( S R )can be preserved through transformation of a data source by reducing the support of association rules. This transformation is referred to as sanitization of D by which the informative knowledge is preserved from disclosure to competitors. However, optimal transformation is still an NP-hard problem, but approximate solutions do exist [3] for mutually exclusive itemsets. 3.4. XML association rule mining
In recent decade, the internet usage has increased in e-commerce. Therefore, it has been realized to standardize the data format. The main purpose of this standardization was to facilitate the exchange of data over web for better interoperability among different technologies and tools. W3C (World Wide Web Consortium) introduced XML (eXtensible Markup Language) for transport and storage of data with simple and more flexible text format [10,15]. Gongxing proposed incremental data mining algorithm; namely FreqtTree and DFreqtTree based on DOM (Document Object Model) tree to mine frequent pat-terns in a distributed environment [40]. An XML file contains elements/objects in a tree format, it can be represented in a tree called DOM tree. Frequent patterns can be discovered from DOM tree using the FreqtTree algorithm with an additional support of ExpandTree algorithm. FreqtTree algorithm traverses DOM tree for k  X  i temsets to calculate candidate sets with their support counts. DFreqtTree used DOM tree to find association rules from an XML database (local and global sites). Global frequent patterns are mined from XML database. Efficient discovery of frequent patterns and association rules is the key contribution with the minimum communication cost. Abazeed et al. [1] presented a modified version of FLEX algorithm (MFLEX) that uses java based parsers (DOM and SAX) for XML mining. The parser class reads the transaction through XML node reader and stores the transaction information into Linked-HashedMap structure. This structure is a combination of hash table and linked list. The objective of this structure is to maintain iteration order and efficient information retrieval. XML file is mapped to Linked-HashedMap for subsequent levels after parsing. The main steps of implemented MFLEX algorithm are not presented to show its efficiency while compared to DOM and SAX. Combi et al. [6] proposed a query based approach to mine an XML document. The query can either be structural or content-based to extract information from an XML document. The main aim of query is to extract association rules, categorized into structural rules and value rules. The sub-graphs are extracted from XML graph to evaluate structural and Element S ( Value S )  X  Element D ( Value D ) . These rules are evaluated using Value Satisfiability De-gree ( VSD ). Structural rules are represented as Element S  X  Elements D ( Value D )= BookTitle(HTML) with the textual values. Consequently, this way of querying XML document provides a profound anal-ysis without any structure and content dependency. The disadvantage of this approach is that structural satisfiability degree tends to become zero, which is negligible especially in worst case for large num-ber of edges. To query XML documents, Bei et al. [4] suggested a query recommendation technique that accessed XML using association rules with an architectural framework. The framework architecture consisted of five components namely; Rule Miner, Result Recommender, Query Miner, Query Recom-mender and Querier . The entire set of these components recommends a functional query for the user after modeling the query pattern tree using goodness measure via an interface. Thus, user is supported by the functional query to find the desired information efficiently. However, the framework architecture does not present the effectiveness of the user query (issued query) and frequent query based on goodness measure.
 Wang et al. [36] focused on SDST (Standard Data Source Template) for XML mining. In SDST, XSL (eXtensible Stylesheet Language) and XSLT (eXtensible Stylesheet Language Transformation) are used for standardization after transformation of XML documents. Thus, SDST templates are built on item , itemset and transaction representing the dissimilar elements, set of elements and whole document respectively. Based on such information, an XML document is prepared as input to apriori algorithm [34, 36] to generate association rules. In this way, XSL and XSLT are used in XML mining to minimize the complexity of XML documents as well as enhanced adaptability after preprocessing. However, the XSLT must be tested and compared with XQuery over large number of XML documents for performance assessment and universal flexibility.

Li et al. [24] proposed a high adaptive data mining techniques for XML rule mining without user involvement. An index table is built from XML documents and jot down the entire mining process of XML association rules in the following steps:  X  Extract XML transactions into index table;  X  Extract items in transactions into index table;  X  Generate a relational table;  X  Use apriori algorithm [34] to mine XML association rules.

The key advantage of this approach is the removal of duplicate transactions and items. However, the approach is contingent on the traditional relational format of information. Shin et al. [33] presented HiLoP (Hierarchical Layered Structure of Pairset) which permitted the arrangement of multifaceted mining task with more integrity. In this arrangement, pairset is a significant component. Therefore, XML association rules can be mined with this component in three phases. First, a tree structured data of XML document is transformed into hierarchical structure pairset. Second, pairsets are manipulated with the lowest amount of support consideration. Third, XML association rules are mined from pairsets with the consideration of support and confidence. Consequently, pairsets are represented as a data structure in XML association rule mining. Thus, the number of iterations for candidate-tree-item-pruning is reduced. Despite this reduction, tree depth accessibility is the most important challenge for large XML documents. 3.5. Privacy preservation, reliability analysis and estimation with Bayesian network
Doguc et al. [12] proposed a BN model for System Operational Effectiveness (SOE) evaluation, sys-tem examination for irregularity discovery as well as analysis. SOE is associated with C (Cost), Sensi-tivity Effectiveness (SE) and Customer Satisfaction (CS). The Bayes rule is used to evaluate SOE with an independence assumption between nodes while computing CPT. In addition, CPT and nodes (C, SE, CS) are used to build BN with the assistance of MSBNX tool. Besides this, a findSource function in SLC is executed to discover problematic system component. As a result, BN is evaluated for its precision and suitability. Wright et al. [39] established a heuristic approach after transforming scoring function of K2 algorithm [7] without disturbing its results significantly. The purpose of this heuristic was to protect the distributed heterogeneous data from external parties while generating a Bayesian network structure. The revised form of K2 scoring function (as given in equation (1)) can be written as in Eq. (4). where;  X  ij 0 , X  ij 1 :(  X  parameters), for all possible values of i and j .

Equation (4) is obtained by applying ln (natural log) to K2 scoring function and use of stirling ap-proximation for any l 1 and l != l rule mining. These algorithms are divided into Heuristic-based , Data Reconstruction-based Association Rules and Cryptographic-based techniques. From this study, it can be concluded that there is no BN based approach for protected association rules.
 Iqbal et al. [19] presented a PPDM model for minimizing disclosure risk of sensitive XARs (XML Association Rules) with the use of BN. In this model, a BN based sensitive node is identified through K2 algorithm. A node is acknowledged as sensitive depending upon its frequent occurrence or maxi-mum probability computation throughout the building network structure. Based on sensitive node(s), the assumption is considered because of minimal effect to the original dataset. Therefore, partial sensitive XARs were hidden without any side effects.

In addition to preserving privacy, Samet et al. [31] proposed a Privacy Preserving Bayesian Network (PPBN) protocol for homogeneously partitioned data. In PPBN protocol, secure building blocks are used in the main protocol. The existing building blocks are enhanced in their confrontation against colluding attacks after alteration. K2 algorithm [7] scoring function is modified for computation as well as to achieve security in public channels. Doguc et al. [13] estimated the reliability of the system by constructing BN with the use of K2 algorithm. This algorithm does not compute CPT clearly. However, the transitional steps of K2 algorithm allowed modification for quantification of components association. Consequently, the whole system achievement is accomplished with the Bayes rule. 4. Central tendency PPDM model 4.1. An overview of the model The proposed model in Fig. 1 reads an XML dataset. This dataset is pruned to prepare XML file. The dataset in XML file is preprocessed into horizontally partitioned itemset and vertically partitioned dataset. Vertically partitioned dataset is input to K2 algorithm to get the related items/nodes according to their occurrence in the developmental process of Bayesian Network. The central tendency measures are computed from the related items/nodes obtained through Bayesian networks. Central tendency mea-sures include average, median, geometric mean and harmonic mean and are different from measures used in [19]. A horizontally partitioned itemset is transformed with the help of central tendency mea-sures using Bayesian networks. The transformed horizontally partitioned itemsets are input to apriori algorithm [34] to mine non-sensitive XML association rules. 4.2. Essential components of PPDM model
Our proposed privacy preserving model for XML association rules has the following components with a brief explanation for each. 4.2.1. XML document
XML document contains information of exemplary and large UCI machine learning dataset. XML has been widely used to store information for communication with the help of user defined tags. 4.2.2. Bayesian network using K2 algorithm
K2 algorithm [7] can be used to develop Bayesian networks in an incremental fashion. Bayesian networks are represented in a Directed Acyclic Graph (DAG) as given by Eq. (1). The conditionally dependent nodes can be represented by Eq. (5).
 Where, m represent the total number of conditionally dependent nodes/items in a relationship according to specified order while constructing Bayesian networks. 4.2.3. Apriori algorithm
Apriori algorithm [34] has been used widely to find association rules with the use of minimum support and confidence threshold as given in section II. This algorithm does not protect sensitive association rules disclosed by association rules. 4.2.4. Central tendency measures
A Central tendency measure is a statistical measure that can be used to identify the central position of a large dataset. Measures of central tendency are sometimes referred to as measures of central location. The average (arithmetic mean), median, geometric mean and harmonic mean are valid central tendency measures. Mean is a popular and well known central tendency measure that can be used with both discrete and continuous data. The mean can be computed from the nodes/items ( m ) appeared in DAG using Eq. (6). Where, m : total number of nodes and X t : set of nodes/items Median is the middle node or the average of the two middle nodes after arranging the nodes/items in DAG for odd or even number of items respectively. Geometric mean is calculated by taking the n th root of the entire set of ( m ) nodes/items ( X t ) . Geometric mean can be computed by Eq. (7).  X  represents product of the entire set of nodes. Harmonic mean is the reciprocal of the mean of the reciprocals of a specified set of nodes and can be computed by Eq. (8).
 4.2.5. Sensitive item/node
A sensitive node is found by taking ceiling value for each of the central tendency measures. The computed value by the central tendency measures for sensitive Node is rounded off to the nearest whole number for exact matching. 4.2.6. Maximum degree transaction Maximum degree transaction [27] represents a transaction which has the maximum number of items. A transaction with maximum number of items is more suitable for perturbation because of its degree. The maximum degree transaction is more effective in hiding sensitive association rules. 4.2.7. Side effects of PPDM model
The identification of sensitive node/item for perturbing maximum degree transaction limits the effects to original XML horizontally partitioned dataset. These precise computations ensure to hide sensitive XML association rules without sides effects like new (ghost) and lost XML association rules. 4.3. PPDM algorithm and implementation
The implemented PPDM Model uses central tendency measures through Bayesian networks to iden-tify sensitive node/item in an XML database. A sensitive node/item is used to perturb the original XML database. XML database consists of items and transactions. The PPDM model only perturbs the max-imum degree transaction by deleting the sensitive item/node. The objective of PPDM model is to hide maximum sensitive XML association rules with minimum perturbation to original data. The algorithm takes XML data with a set of items and transactions. The XML document is prepared from small sample datasets as used in literature [27,29,37] and large UCI machine learning datasets to validate the accuracy of the proposed model in hiding process. The detailed steps of the algorithm are presented in Fig. 2.
The algorithm performs several steps to ensure privacy of rules such as computation of central ten-dency measures, data perturbation and transformed XML horizontally partitioned data. A step wise working of the PPDM algorithm is explained below. 4.3.1. Reading XML document
In the first step, the algorithm reads an XML document and total number of transactions in the docu-ment. Variable XML_Data stores number of repeated items according to the transaction identifier. 4.3.2. Pruning transaction items
XML_Data may have repeating items in a number of scanned transactions in XML document. These item is assigned a unique symbol after ordering them in ascending order.
 4.3.3. Preprocessing data In preprocessing phase, horizontally partitioned itemset and vertically partitioned dataset are prepared. Steps from 8 X 23 prepare these two sets. Vertical ly partitioned dataset (Binary Table) is input to K2 algorithm to construct Bayesian networks using Eq. (1) to get conditionally dependent nodes or items as implemented in line 24. The conditionally dependent items represent highly demanding items in competitive businesses. 4.3.4. Perturbing data and transforming dataset
Central tendency measures are computed for identification of sensitive node/item. These sensitive nodes are used to perturb maximum degree transaction (maximum number of items) of the horizontally partitioned itemset. The resulting dataset is transformed into horizontally partitioned dataset. Steps 25 X  41 are used to perform this perturbation and transformation. Finally, large itemset are generated using apriori algorithm [34] as shown in the last line of the algorithm. 5. Experiments
We used three dataset based on literature in [29] and four UCI datasets as shown in Table 1 to validate the proposed model. These datasets are used from UCI Machine Learning Repository 1 andusedfor benchmarking data mining algorithm. We selected those datasets having a reasonable number of Boolean variables. We ignored the other variables which cannot be converted into Boolean format. The total numbers of ordinal variables used in our experiments are summarized in Table 1 with additional details of each dataset.

Furthermore, our proposed model is tested over three case studies used in literature. XML is prepared document based on these three case studies datasets for comparison of our approach with the published results of current methodologies. Subsequent sections present the examples for validation, evaluation and comparison. In last subsection of this section, central tendency measures based PPDM model is further evaluated over large datasets of Table 1. 5.1. Case study 1
The proposed algorithm is tested with the support of apriori and K2 algorithms to hide sensitive XML association rules. A sample case study dataset in [27,37] is used to prepare an XML document as shown in Fig. 3.

The results of our proposed algorithm are compared with the existing methodologies [27,29,37] as shown in Fig. 5. The case studies based sample dataset is used to generate Bayesian networks with the help of K2 algorithm, as shown in Fig. 4, to record conditionally dependent nodes/items.
 The recorded conditionally dependent nodes/items are used to compute central tendency measures. After computing central tendency measures, a sensitive XML node/item is declared to be sensitive to identify sensitive XML association rules as shown in Table 2 before perturbing original dataset. In this case study, a sensitive XML node/item is identified (that is,  X  X  X , the 3 rd node/item). Thus, XML as-sociation rules having  X  X  X  node are declared as sensitive to hide from disclosure with the use of 20% minimum support. After identifying and declaring sensitive item, the only third transactional itemset (WYZ) of original dataset is perturbed based on  X  X  X . Central tendency measures are computed using Bayesian networks in order to hide the sensitive XML association rules with the minimum support of 20%. The non-sensitive XML association rules are presented in Table 3. As a result, all sensitive XML association rules are hidden.

We also compared the results based on the sample datasets presented in [27,37]. Our proposed model was able to hide entire sensitive XML association rules while other techniques such as ISL DSR and data reconstruction based approaches were unable to do so for the same datasets. The central tendency mea-sures via Bayesian network hide from view the entire sensitive XML association rules when compared with ISL, DSR and Data Reconstruction based approach. 5.2. Case study 2
An excerpt of an XML document [37] is given in Fig. 6. The objective of using small XML document is confirmation of accuracy and validation of identifying sensitive XML node/item and hide sensitive XML association rules.

A sensitive XML item/node is identified to declare sensitive XML association rules.Bayesian network for the document is shown in Fig. 7.

The identified sensitive XML association rules are given in Table 4 after perturbing dataset of first, third and last transaction based sensitive item/node  X  X  X  with 20% minimum support. Minimum support is a threshold value defined by the user to find the entire set of XML association rules. The non-sensitive XML association rules are generated after original transactional itemset perturbation. These rules are presented in Table 5.
 A comparison of results of the proposed approach with those of well-known approaches is given in Fig. 8. A single XML association rule ( Y  X  Z ) is identified as sensitive. The proposed approach per-formed better than the existing approaches such as ISL, DSR and Data Reconstruction based approach. Also, our approach generated no side effects in the form of lost rules or ghost rules. 5.3. Case study 3
From previous examples, a question can be raised that  X  Are the identified sensitive XML association rules actually sensitive?  X  Therefore, a sample XML document is prepared on the same number of items and transactions as used in [27] and presented in Fig. 9.

The motivation of using this sample dataset is to justify the similarity in identifying the similar sen-sitive XML rules. The identified sensitive rules [27] are also declared as sensitive XML rules by our proposed model. Bayesian Network is shown in Fig. 10.

The minimum support (20%) is used for discovering XML association rules before and after perturba-tion of horizontally partitioned dataset. A sensitive item/node  X  X  X  is identified to declare sensitive XML association rules. Hence, sensitive XML association are hidden (as shown in Table 6) after perturbing the only largest transaction with ID =  X 1 X .

Sensitive Rules in Oliveira et al. [27] are A, B  X  D and A, C  X  D which used inverted file to hide the prior rule and hide later rule after transformation of the original database transaction T1, T3 with T1, T4. Our proposed approach performed better than [27] with only one transaction (T1) perturbation with the sensitive item (D). The non-sensitive XML association rules are displayed in Table 7 with the minimum side effects.

The proposed PPDM model is also compared with the existing mode and maximum probability node based methods in [19]. From Fig. 11, the proposed approach performed competitive, in preserving sen-sitive XML association rules, to approaches based on Maximum Probability and Mode. A sensitive node is declared, by maximum probability method, that has the largest probability according to its occurrence in the learning process of Bayesian networks. A mode based sensitive item represented the most frequent node found in the recorded data using Bayesian netwo rks. In contrary to maximum probability and simi-lar to mode, central tendency measures are computed from the Bayesian network based recorded data to declare a sensitive node/item for perturbation. Hence, our proposed PPDM model is validated and pro-duces similar results with accuracy without any side effects in comparison with the existing approaches over small datasets. 5.4. Comparative summary of case studies
The aforementioned case studies are used to test and validate our proposed PPDM model with the item ( Y ) is identified by using central tendency measures based on Bayesian networks. The objective of using first two case studies is to validate and confirm the accuracy of proposed PPDM model in order to hide sensitive XML association rules. For this purpose, a set of XML association rules containing sensitive item, either in antecedent (LHS of an Association Rule) or consequent (RHS of an Association Rule) of the rule without increase or decrease in support [37], are considered as sensitive XML associ-ation rules. Therefore, the original sample XML transactional itemsets (as shown in Figs 3 and 6) are perturbed by transforming the highest degree transaction [27] containing sensitive item. Similarly, pro-posed PPDM model is validated for the sensitive item identification [27] to provide a valid justification of transformation of highest degree (largest sized transaction). The proposed PPDM model also proves 100% accuracy for sensitive item identification ( D ) without prior knowledge of frequent items. In con-trary to our PPDM model, Oliveira et al. [27] assumed sensitive association rules rather than to come up with a dynamic solution. However, multiple sensitive items and over detection of sensitive XML associ-ation rules are two convincing questions can be raised on our proposed PPDM model. Each one of these legitimate questions are beyond the scope of this wo rk and can be a future research direction to deal with this NP-hard problem [3]. The comparison of aforementioned case studies on privacy preserving of sen-sitive XML association rules is presented in Table 8 that can be used as a basis for further improvements. Besides this, A, B  X  D,A,C  X  D are assumed sensitive association rules obtained from transaction T ,T 3 and T 1 ,T 4 containing the most frequent item ( D ) . After this, an inverted file is composed of sen-sitive rules and transaction IDs. On the contrary, our PPDM model automatically identify sensitive item (
YorD ) using central tendency measures based Bayesian networks and its presence in XML association rules to find sensitive XML association rules. Our PPDM model is similar to [27] while perturbing the highest degree transaction in original transactional itemset. The selection of the highest degree transac-tion(s) by perturbing sensitive item is to keep minimum transformation in original transactional itemset. The objective of keeping minimum perturbation yielded no side effects in the above considered case studies. 5.5. Comparative study over large datasets
Additionally, the proposed approach is validated and compared with work presented in [19] over four UCI machine learning datasets as already presented in Table 1. Four central tendency measures are computed which showed a relation among these measures like AM = Median &gt; GM &gt; HM on three datasets; Zoo, Vote and Hepatitis. However, the relationship among three measures remains same except harmonic mean. Therefore, the relationship in central tendency measures ( AM = Median = GM &gt; HM ) has taken dissimilar route trivially. The entire comparative summary of privacy preserving in XML association rules is depicted in Fig. 12.

From Fig. 12, a more comprehensive and clear picture of minimizing disclosure risk of XML associ-ation rules can be observed. 5.6. Effectiveness central tendency based PPDM model
The comparative study was carried out using UCI machine learning datasets to measure effectiveness for side effects. The effectiveness of the proposed PPDM algorithm can be measured in terms of adverse effect to sensitive XML association rules as well as non-sensitive XML rules hidden after perturbation of the horizontally partitioned dataset. Experiments were conducted on a PC, having an Intel Core 2 Duo (1.66 GHz) processor with 2.0 GB RAM on a Windows Xp Operating System. Our proposed PPDM model automatically restricted the mining of sensitive XML association rules than assumed selected set Therefore, the effectiveness of our PPDM model is measured with the results reported in [28] and shown in Fig. 13. The lower value of a method for side effects is considered as the best approach.
Our reported side effects (for example, new or ghost and lost XML association rules) results are stan-dardized according to the results reported in [28] for balanced comparison. Consequently, our PPDM model yielded better results than Na X ve, Downright Sanitizing Algorithm (DSA), Item Grouping Algo-rithm (IGA), Sliding Window Algorithm (SWA) and Algo2a [28]. 6. Conclusions and future work
In this work, Bayesian networks-based central tendency measures are used to present a PPDM model that hides sensitive XML association rules without any side effects. The goal was to identify a single dis-closure item/node that can reveal healthy running business secrets to external competitors in the mining process of XML association rules. Extensive tests were performed over literature based case studies and large machine learning datasets. Case studies were used to validate the proposed model accuracy over small datasets for better understanding by the academia. The proposed PPDM model performed better in all the considered cases without any side effects in comparison. Also, experiments were performed over large datasets to hide sensitive XML association rules with no side effects after comparison with the ex-isting approaches. Consequently, our PPDM model showed efficacy in the form of hiding sensitive XML association rules over large and small (case studies) datasets with the use of same support and minimum perturbation to original transactional itemsets. However, our PPDM model partially hide sensitive rules without having a constraint while selecting sensitive XML association rules based on a single sensitive item. Side effects are interpreted as new rules and lost rules generated by the proposed model after per-turbing the original transactional itemsets. For better results, a constraint based selection of sensitive XML association rules can be a dynamic future goal to avoid over detection of sensitive XML rules for generalization by including multiple sensitive items identification with minimum perturbation. Acknowledgement
The research was supported by National Natural Science Foundation of China (61105018, 61175020). References
