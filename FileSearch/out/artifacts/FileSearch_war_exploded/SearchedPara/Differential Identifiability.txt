 A key challenge in privacy-preserving data mining is ensur-ing that a data mining result does not inherently violate pri-vacy.  X  -Differential Privacy appears to provide a solution to this problem. However, there are no clear guidelines on how to set  X  to satisfy a privacy policy. We give an alternate for-mulation, Differential Identifiability , parameterized by the probability of individual identification. This provides the strong privacy guarantees of differential privacy, while let-ting policy makers set parameters based on the established privacy concept of individual identifiability.
 K.4.1 [ Public Policy Issues ]: Privacy; H.2.8 [ Database Applications ]: Data mining Security Differential Privacy; Identifiability
Privacy-preserving data mining has seen many advances, and today we can construct many data mining models with-out disclosing the input data [1, 21]. One key challenge re-mains: does the produced model inherently violate privacy? Differential Privacy [7] provides a means to address this challenge. The basic idea is to add enough noise to the out-come (e.g., the model resulting from training) to hide the contribution of any single individual to that outcome.  X  Support for this work was provided by MURI award FA9550-08-1-0265 from the Air Force Office of Scientific Re-search.

This appears to address the challenge perfectly. From a data mining perspective, the first privacy issue is reveal-ing information about an individual in the training data. Differential privacy essentially hides an individual by en-suring that the resulting model is nearly indistinguishable from the one without that individual  X  for any individual . Formally, given the query function f ,  X  S  X  Range ( M ) an  X  -differentially private mechanism M satisfies: where D 1 and D 2 are two databases differing by at most one element. 1 In data mining terms, M f is the learning algo-rithm, f is the resulting model, and D 1 and D 2 are nearly identical training databases. Since the (randomized) learn-ing algorithm guarantees that the results are indistinguish-able for any two training sets that differ by one individual, we can claim that the resulting model does not disclose in-formation about any single individual in the training set.
Unfortunately, indistinguishability only holds for  X  = 0, which makes the resulting model useless. The larger the value of  X  , the less noise needed in the model, and the more a single individual can impact that model (thus risk-ing disclosure of information about that individual). The problem is that  X  limits how much one individual can affect the resulting model, not how much information is revealed about an individual. This does not match legal definitions of privacy, which require protection of individually identifiable data [8,11].

We instead propose a parameterization based on the prob-ability that an individual contributes to the model. This corresponds to legal definitions, and we can use regulations such as the U.S. HIPAA safe harbor rule [11] to determine a probabilistic intent of the regulation. The HIPAA safe harbor rule requires removal of names, addresses, and iden-tifying numbers. However, it does allow geographic units as small as 20,000 people, age in years (if less than 90), gender, and ethnicity. Given that less than 1.7% of the U.S. pop-ulation is male and 85 or older 2 , knowing the age, gender, and address of someone over 85 would allow us to limit them to one of 68 people (on average) in safe-harbor de-identified data. In practice, the privacy provided could be much worse; e.g., in a college town there may be few older people. How-
There are several formulations of the definition of differ-ential privacy, such as a difference bound rather than the ratio given in Equation 1; we present the one from [7]. The differences between definitions is not critical to this paper.
U.S. Census, 2010 data ever, from this we can deduce that the goal of the privacy policy is met if we limit the estimate of the probability that an individual is in the data to approximately 1.5%.
The  X  in  X  -differential privacy does not correspond to such a probability; it has been shown that for a given value of  X  , the probability of identification can vary depending on data values, or even on values of individuals not in the data set [13]. Nor does  X  correspond to ability to infer private data values for an individual [4]. We give a definition  X  -differential identifiability that provides the same guarantees as differential privacy (proof against an arbitrarily strong adversary), but the parameter  X  bounds the probability es-timate that an individual contributed to the resulting model.
Differential Identifiability is a subtle but important vari-ation of differential privacy, and this paper shows that the general Laplacian noise addition mechanism for differential privacy can be adapted to provide differential identifiabil-ity. However, the mathematical formulation shows signifi-cant differences; there is no direct translation from  X  to  X  ; the relationship depends on additional information outside the scope of setting policy. The result is a method corre-sponding to real-world privacy policy (e.g.,  X  = . 015 provides privacy equivalent to what appears intended by the HIPAA safe harbor rules), with the strong formal guarantees of the adversary model used in differential privacy.
While many privacy definitions have been proposed, the common goal is to allow learning information about a group of individuals while protecting information of each individ-ual in the group. Samarati and Sweeney proposed k -anonymity [19, 20]. k -anonymity ensures that the identifying informa-tion for at least k tuples is identical, ensuring that individ-uals cannot be uniquely re-identified.

While k -anonymity prevents linking a record to an indi-vidual, it may still disclose sensitive information.  X  -diversity [14] shows that k -anonymous tables are vulnerable to homo-geneity. Their definitions require that each equivalence class with identical identifying information have at least  X  distinct values for each sensitive attribute.

While these and related approaches prevent the adver-sary from uniquely identifying an individual X  X  record, they assume little prior knowledge available to the adversary.  X  -presence [18] protects against determining if an individual is in a dataset even if the adversary has full knowledge of val-ues of individuals. Generalization/suppression bounds the adversary X  X  probability of inferring that an individual is in the database to the range  X  = (  X  min ,  X  max ).

A big advantage of the above methods is that the out-comes are guaranteed true, even though specificity may be lost through generalization/suppression. However, they as-sume the adversary X  X  knowledge is limited. For example, in k -anonymity and  X  -diversity, protection targets are not met if the adversary knows sensitive values of other individuals.
Differential privacy [7] avoids modeling prior knowledge of the adversary. To do this, it gives up the correctness guaran-tees of the above methods, instead providing a noisy result that hides the impact of any single individual on the result. The idea is that what is learned from a dataset with a par-ticular individual can also be learned from a dataset without that individual. Therefore, it hides presence or absence of an individual in the database by making the response gen-erated by two datasets (one with the individual and the
D  X  = D  X  i  X  Subset of D missing one individual other without the individual) indistinguishable. As stated previously, we follow in this model, but provide a parame-terization based on the risk of identifying an individual.
We now give background on differential privacy, the prob-lem of re-identification, and introduce a possible worlds ad-versary and sensitivity model. We first introduce notation.
A database D can be modeled as a (multi)set. Each ele-ment x i takes a fixed value from the universe U . Each entry in U corresponds to an individual whose privacy must be protected; I ( i ) denotes the identity of the individual cor-responding to database entry i . The set of individuals who contributed their data to D is denoted by I D = { I ( i ) | i  X  D } . Let D  X   X  D denote a database having one less element than D (i.e., | D  X  | = | D |  X  1). As with differential privacy, a query function f can be any function that extracts infor-mation from a database. Examples from differential privacy include aggregate queries (count, mean, sum, ...) used by a data mining algorithm [5,15,22], a data anonymizer [17], or a learning algorithm such as ID3 [10] or logistic regression model [3]. These notations are summarized in Table 1.
Another concept borrowed from differential privacy is sen-sitivity, which measures the maximum impact a single indi-vidual can have on the query result:
Definition 1 (Sensitivity). The sensitivity of a query function f for bounded differential privacy [6] is defined as where x  X  can be obtained by replacing one element in x with another.

To attempt to dispel confusion, we note that there are different ways of defining sensitivity, based on the defini-tion of neighboring databases D and D  X  differing by removal of an individual ( unbounded ) or replacement of an individ-ual ( bounded ). Unless we explicitly state otherwise, we use bounded differential privacy in this paper. Further discus-sion of these differences can be found in [12].
The goal of privacy-preserving data mining is to release the mining outcome without revealing identities of individ-uals in the database. Precisely, given  X   X  [0 , 1], the ad-versary X  X  expectation that any individual corresponding to i  X  U not previously known to be in D is in D is at most  X  . This can be thought of as a game between a privacy mech-anism and an adversary. The privacy mechanism M builds the model f ( D ) and adds noise to produce the perturbed re-sult ( response ) R = M f ( D ). The adversary tries to identify individuals whose data is in D from the given response R . If R enables the adversary to state any new individual belongs to D with confidence exceeding  X  , privacy is breached.
This paper assumes the same strong adversary as differ-ential privacy. The adversary X  X  prior knowledge is repre-sented as a triplet L = h U, D  X  , I D  X  i . The adversary has complete knowledge on the universe; every value in U is known. The adversary knows every tuple in D except one. In other words, the adversary has D  X  . The adversary also has I D  X  , the identities of the individuals corresponding to those tuples. The only piece of information the adversary does not have is who the n th individual of I D is. The ad-versary also knows the privacy mechanism M { , i.e., how the mechanism works and the noise distribution. This type of adversary is called an informed adversary [6]. 3
In our model, the goal of adversary is to determine the membership of the unknown individual in D with high con-fidence. To find out the identity of the missing individual, the adversary interacts with a randomized mechanism M ; issues a query and receives a noisy response R = M f ( D ). The adversary uses the response and prior knowledge to re-duce uncertainty about the missing individual.

To measure the adversary X  X  confidence in making an in-ference, we use a possible worlds model. The adversary considers the set of all possible databases. Given the prior knowledge L and R , the adversary creates a set of all pos-sible databases, called possible worlds , that may be the D that generated the perturbed answer R . Since the adversary only needs to determine the membership of one missing in-dividual, each possible world is the union of D  X  and one data entry from U . Given the adversary X  X  prior knowledge L , the set of all possible worlds, denoted by  X , is Notice that exactly one of the worlds in  X  is the true database that produced R . When the adversary knows k rows of D , the set of possible worlds that contains and doesn X  X  contain a data entry i , respectively.

Before seeing the response R , we assume every possible world is equally likely to be D (we discuss relaxing this as-sumption at the end of Section 4.1.) Once the adversary receives R , for each possible world  X   X   X , the adversary computes the probability that  X  is the original database D that generated the perturbed response R : These probabilities give the adversary an updated belief on each possible world. Among all possible worlds, the one with the highest probability will be the adversary X  X   X  X est guess X . If the mechanism allows the adversary to make a correct guess on the missing data entry i with high confi-dence, the privacy of the individual corresponding to i is at risk. Therefore, the goal of our privacy mechanism is to bound the probability that the adversary identifies an indi-vidual X  X  presence in the database.
As with differential privacy, we assume the adversary does not know U  X  D , the individuals not in the dataset, as know-ing U  X  D and D  X  reveals D and there is no privacy left to protect. Assuming a large universe U , knowledge of a few in-dividuals not in D has little impact, so the assumption that the adversary knows D  X  is a stronger practical assumption.
To determine the noise needed to hide the impact of any individual, differential identifiability (and differential pri-vacy) use the sensitivity of the result to the contribution of any single individual. The greater the variation from the contribution of a single individual, the more noise must be added. Given the query function f , the contribution of an individual can be stated as the change in the range of f due to having that individual in its domain. The formal definition of individual contribution is: Definition 2 (Contribution of an individual).
 The contribution of an individual corresponding to i to a query function f , C f ( i ) , is defined as:
To capture the largest contribution that can be made by any single individual in the universe, the sensitive range is defined as the range of a query function over the domain  X . This is the maximum distance over the range of f between two possible worlds, for any set of possible worlds (since we don X  X  know which individual the adversary doesn X  X  know.) The maximum contribution is the largest contribution that can be made by a single individual:
Definition 3 (Sensitive Range S ( f ) ). The sensitive range of a query function f is the range of f .
 where  X  is the set of possible worlds under the prior knowl-edge L .
 Note that S ( f ) = max i C f ( i ).
Our goal is to hide identities of data contributors; we mea-sure the adversary X  X  computed probability that any given individual is in the database. This is called identifiability risk . In an interactive privacy mechanism, the information to be disclosed is the answer to an aggregate query that does not relate to a specific individual. However, even though published statistics are presented in aggregated form, it is still possible they leak some information about an individ-ual. This is especially true when the database contains an individual whose contribution to the query answer is signif-icantly larger (or smaller) than that of others.

To illustrate, we give an example where the mean M re-leased by a differentially private mechanism enables the ad-versary to guess the missing element with high probability. sensitivity of the query function f = mean is 9 4 . Assume the adversary already knows { 1 , 2 , 3 } X  D . The possible worlds are  X  1 = { 1 , 2 , 3 , 4 } ,  X  2 = { 1 , 2 , 3 , 5 } , and  X  Assume that  X  = 2 and the response R = 5 . 041. The adver-sary computes the probability P r [ M f (  X  i )= R ] , 1  X  i  X  3 that R came from each distribution, and compares their ratio. P r [ M f (  X  3 )=5 . 041] = 0 . 1762 is much larger than the other two, P r [ M f (  X  1 )=5 . 041] = 0 . 0464 and P r [ M f (  X  0 . 0580. The adversary concludes that the missing element is 0 . 6278. (For the same R and a smaller value of  X  the confi-dence would be lower; see [13].)
Given the above, we can now provide a formal definition that satisfies the problem statement given in Section 3.2.
Definition 4 (  X  -differential identifiability). Given a query function f , a randomized mechanism M is said to satisfy  X  -differential identifiability if for all databases D ,  X  D  X  = D  X  i  X  , and  X  i  X  U  X  D  X  : A randomized mechanism M satisfying the above definition ensures that the identifiability risk of any individual in the universe is less than or equal to  X  . Basically, every possible world becomes indistinguishable within a factor of  X  . The parameter  X  in our work can be interpreted as the degree of indistinguishability between possible worlds, where the possible worlds differ by (any) one individual  X  providing an upper bound on the confidence that the individual is the difference between the worlds (and thus identifiable.) This differs from the  X  in differential privacy, which measures the difference in the query result given different possible worlds, not the difference in the likelihood of those worlds.
We now show how to calibrate noise to achieve  X  -differential identifiability, given the sensitive range of a query func-tion. As with the mechanism for differential privacy in-troduced in [6], noise Y is added to every query response, R = f ( D ) + Y , where Y is an i.i.d. random variable drawn from a Laplace distribution.
 Let  X ( i ) be the identifiability risk for the individual I ( i ).  X ( i ) represents the degree to which an adversary believes I ( i ) is in D given M f ( D ) = R . An upper bound on  X ( i ) can be obtained as follows:
 X ( i ) = P r [ I ( i )  X  X  D |M f ( D )= R, D  X  ] (2) cation of triangle inequality yields where m = |  X  | = | U | X  X  D  X  | .

Since ( m  X  1)  X  exp(  X  S ( f )  X  )  X  ( m  X  1), it is trivial to see that the lower bound of Equation (9) is This implies that it is impossible to protect the privacy of individuals in the database with the probability less than an adversary X  X  probability of a correct random guess.
To find  X  that ensures  X  i,  X ( i )  X   X  , it is sufficient to satisfy the following inequality Since  X   X  1, taking the natural log of both sides yields When ( m  X  1)  X  1  X   X   X  1, i.e.,  X   X  1 m , (14) can never be satisfied. For  X   X  1 m , The above leads to (and serves as a proof of):
Theorem 1. For an arbitrary adversary A , if m = |  X  | .

The construction given assumes that the prior probability of an individual being in D is the same for all individuals. In practice, some individuals may have a higher prior. This may make providing  X  -differential identifiability impossible: If the prior probability P r [ D = D  X   X  { i } ] &gt;  X  , then the privacy goal is inherently violated and no privacy mechanism can restore it. For less severe cases, the value for m in Equation (15) can be replaced with 1 / max  X  ( P r [ D = D { i } ]). This essentially says that the adversary X  X  best guess is no better than the average calculated in Equation (5).
While this seems to require additional knowledge of the capabilities of an adversary, it actually shows that differ-ential privacy does not provide guarantees on individual identifiability. Even if an adversary already has sufficient prior knowledge to identify an individual, differential privacy blindly produces the same (noisy) result as if the adversary had no such prior knowledge. The protection of differential privacy measures only the impact of an individual on the output, not the ability to identify an individual [13], or even the ability to infer data values for an individual [4].
Under the assumption that every possible world is equally likely and the number of possible worlds is known, there is a relationship between our definition and differential privacy. Any  X  -differentially private mechanism satisfies 1 1+( m  X  1) e  X   X  differential identifiability. This is easy to see once we apply the differential privacy guarantee ,  X  j, P r [ M f (  X  j e  X   X  P r [ M f ( D ) = R ], to Equation (6). Alternatively,  X   X  ln  X  , not an optimal value. Note that even setting this upper bound for  X  requires knowing the number of possible worlds m , making it difficult to develop a policy for a value of  X  to prevent re-identification.

Since the upper bound on the identifiability risk is com-puted by approximating the distances between the original database and other possible worlds, | f ( D )  X  f (  X  k ) | , with S ( f ), the calibrated noise could be greater than what is ac-tually required. For example, assume U = { 1 , 2 ,  X  X  X  , 10 } , D = { 1 , 2 , 3 } , and the subset of D known to the adversary is D  X  = { 1 , 3 } . Since the adversary already knows 1 and 3 belong to D and only needs to determine the membership of one missing element, the possible worlds generated by the adversary would be Table 2. Given the query function f mean ( X ) returning the mean of elements in X , the sensitive range is: Assuming that the privacy goal is to ensure that the prob-ability of identifying any individual in the database is no greater than 1 3 (i.e.,  X  = 1 3 ), Assume the response to the adversary R = 2. This is the worst case, as it maximizes the distance between the ex-pectation that  X  1 = D and other possible worlds. Given  X  = 8 The actual identifiability risk is less than the threshold  X  , thus it is possible to choose noise from a tighter distribu-tion while still satisfying the privacy constraint. Figure 1(a) shows the change of identifiability risk by varying  X  . However, the bound is tight for some function f . Consider the case where the query function is median. Notice that the median of every possible world is 3 except  X  1 = D . In this case, the sensitive range is S ( f median ) = | 3  X  2 | = 1 and | f ( D )  X  f (  X  k ) | = S ( f ) for 2  X  k  X  8. Therefore, Equa-tion (15) holds with equality. In other words, the amount of noise computed by the upper bound is the amount actually required. Figure 1(b) shows the tightness of the bound. Theorem 2. The upper bound on the identifiability risk,  X ( i ) , is tight, that is there exists a case where the bound holds with equality.

Proof. The conditions under which Equation (8) and (9) hold with equality are 1. R  X  f ( D )  X  f (  X  i )  X  R  X  f ( D )  X  f (  X  i ) and 2.  X   X ,  X  6 = D, | f ( D )  X  f (  X  ) | = S ( f ) respectively. Consider the case where the size of database to publish is one less than that of universe (i.e., | D | = | U | X  1). In this case, there exists only two possible worlds, namely  X  1 and  X  2 . Without loss of generality, assume f (  X  1 ) &lt;f (  X  Clearly there always exists an R that satisfies the first condi-tion. According to the definition of sensitive range, S ( f ) = | f (  X  1 )  X  f (  X  2 ) | . Hence, the second condition is always sat-isfied. Therefore, for any function f , there exists a case that satisfies both conditions at the same time.
We now look at practical applicability of differential iden-tifiability. Due to space constraints, we only show the case where f is a simple aggregate query. This mechanism can be applied to more complex queries (such as a data mining model) in the same manner as the Laplace noise mechanism for differential privacy.

We use the Adult Database from the UCI Machine Learn-ing Repository [9], comprised of 48,842 individuals from the 1994 U.S. Census, as our example database. This database contains 9 categorical and 5 numerical attributes. Only the numerical attributes (shown in Table 3) are used in this ex-ample. Since the example is census data, we assume the universe is all US residents. Assume we wish to release the  X  -differentially identifiable mean of hours-per-week. Let DB ( X  is relational projection), and D  X  be the database ob-tained by removing an element from D . We assume that the maximum and minimum hours-per-week in the universe are 99 and 1, respectively. This is a reasonable assumption, since Census data typically uses top and bottom-coding to prevent rare values from being identifiable (clearly, [0, 168] could also be used as bounds.) To determine the noise distribution, we must calculate S ( f ). Assume the adversary knows hours-per-week of ev-ery individual except one (i.e., the adversary has D  X  ). The possible worlds the adversary would generate are  X  1 = D  X  { 1 } ,  X  2 = D  X   X  X  2 } ,  X  X  X  ,  X  99 = D  X   X  X  99 } . Thus, the sensi-tive range S ( f ) = | f (  X  99 )  X  f (  X  1 ) | = 98 48842  X  = 1 10 , this gives  X  = S ( f )
Table 4 shows the parameters and three responses for both mean and median of each of the three attributes for  X  = 0 . 1 and 0 . 001. (This is assuming the adversary sees only a single one of these responses, returning all would require sharing the  X  X rivacy budget X  as with differential privacy.) Notice that when  X  = 0 . 001, the amount of noise required to enforce  X ( i )  X   X  for the attributes age, education-num and hours-per-week becomes infinite; the released statistics are essentially random noise and do not give any information about individuals in the database. This is because a random guess that D contains an individual with (say) 12 years of education is almost certainly correct.

To demonstrate the reliability of results from differential identifiability, Figure 2 shows the results of 1000 queries for mean as  X  is varied. This illustrates how often a querier would be seriously misled by the differentially identifiable result. The vertical axis gives the noise ratio: where R is a response and U range (= U max  X  U min ) is the range of the domain. Note that the scale is  X  10  X  3 , so ex-cept for the rightmost trial on each plot, all the responses are close to the true answer. The band and cross near the middle represent the median and mean of responses, respec-tively. The boxplot is omitted when the magnitude of noise added to the response is typically greater than any possible value in the domain (i.e., NR &gt; 1). From the figure, it is clear that the smaller the value of  X  (i.e., the higher the de-sired privacy), the more noise is required; responses become less useful. When the value of  X  is close to the probability of a random guess being correct, the mean and median of re-sponses are not in range even with 1000 trials. Note that for everything except years of education, answers for  X  = 1 . 5% are highly likely to be close to the true answer.

Figure 3 shows the same information for differential pri-vacy as the value of  X  varies. (Only Capital-gain and Capital-loss are shown due to space constraints; the other figures are similar.) Note the similarity of the figures; it is clear that  X  -differential privacy and  X  -differential identifiability are com-parable in terms of privacy. However, setting  X  to achieve this privacy is not an easy problem. While the plots appear similar, the scale of  X  is very different across the two plots. Interpretation of the semantics of epsilon with respect to re-identification is very difficult.

The effect of database size on the noise needed is shown in Figure 4. For this experiment, two databases of different sizes (1000 and 10,000) are constructed by randomly sam-pling from the original dataset. The red (left) column of each pair shows the noise ratio for the database of size 1000; the right (blue) is for the database of size 10,000. (The  X  values for each pair are the same.) We see that, for f = mean , the noise needed shrinks with the size of the dataset. However, for f = sum , the noise is independent of dataset size. This is because the change in a sum based on one individual is the same regardless of the number of individuals, whereas for mean the impact shrinks with the number of individuals. Thus for mean it becomes harder to identify an individual from the released statistics as the database grows.
Differential identifiability (as with differential privacy) can often be satisfied with little impact on the resulting model. Data mining models should not be too dependent on a sin-gle individual; this would suggest that the model would not generalize well to unseen data. In addition to the gen-eral Laplace noise addition method of [7], several specialized techniques have already been developed for differentially pri-vate data mining [2,10,16,23]. We have shown that the gen-eral Laplace noise addition approach can be used to satisfy differential identifiability; we expect that analogous methods to other differentially private mechanisms can be developed to support differential identifiability as well. [1] C. C. Aggarwal and P. S. Yu, Eds., Privacy-Preserving [2] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta, [3] K. Chaudhuri and C. Monteleoni,  X  X rivacy-preserving Mean Std.
 Dev.
 [4] G. Cormode,  X  X ersonal privacy vs population privacy: [5] B. Ding, M. Winslett, J. Han, and Z. Li,  X  X ifferentially [6] C. Dwork, F. McSherry, K. Nissim, and A. Smith, [7] C. Dwork,  X  X ifferential privacy, X  in 33rd International [8]  X  X irective 95/46/EC of the European Parliament and [9] A. Frank and A. Asuncion,  X  X CI machine learning [10] A. Friedman and A. Schuster,  X  X ata mining with [11]  X  X tandard for privacy of individually identifiable [12] D. Kifer and A. Machanavajjhala,  X  X o free lunch in [13] J. Lee and C. Clifton,  X  X ow much is enough? choosing [14] A. Machanavajjhala, D. Kifer, J. Gehrke, and [15] F. McSherry,  X  X rivacy integrated queries: an [16] F. McSherry and I. Mironov,  X  X ifferentially-private [17] N. Mohammed, R. Chen, B. C. Fung, and P. S. Yu, [18] M. E. Nergiz, M. Atzori, and C. Clifton,  X  X iding the [19] P. Samarati,  X  X rotecting respondents X  identities in [20] L. Sweeney,  X  X -anonymity: a model for protecting [21] J. Vaidya, C. Clifton, and M. Zhu, Privacy Preserving [22] X. Xiao, G. Wang, and J. Gehrke,  X  X ifferential privacy [23] N. Zhang, M. Li, and W. Lou,  X  X istributed data
