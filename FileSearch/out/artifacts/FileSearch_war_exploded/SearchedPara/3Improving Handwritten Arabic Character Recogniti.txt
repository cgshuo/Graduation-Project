 Although the latest improvements in Arabic character recognition methods and sys-tems are very promising, the automatic recognition of Arabic handwritten characters remains a challenging task. This is mainly because of the lack of large databases of Ara-bic handwritten characters and words compared to English and other Latin languages. Previous papers in handwritten Arabic character recognition either used specific small datasets of their own, such as Al-Badr and Mahmoud [1995], Amin [1998], and Khedher et al. [2005], or they talked about large databases that are not available to the public, such as Kharma et al. [1999] and Al-Ohali et al. [2003].

One possible solution to overcome this shortage of data is to synthesize artificial data to augment small databases. This can be done by introducing arbitrary distortions such as image noise and randomized affine transformations. However, such distortions do not capture human handwriting distortions and therefore will be less helpful. In this work, we propose the use of the congealing technique to model real distortions in handwriting and then use these distortion models to synthesize handwriting examples that are more realistic. We show that the use of this approach leads to significant improvements in handwritten character recognition.

The central technique that this article is based on is congealing. Congealing factors a set of images into a set of similar latent images and a set of transformation functions [Miller 2002]. This set of transformation functions can then be applied to other sets of images to synthesize artificial examples. We carefully study and examine the congeal-ing techniques on Arabic handwritten characters for the first time and identify some anomalies and limitations of the approach. We propose a simple yet highly effective extension to the original congealing approach, which extracts intermediate transfor-mation operators in addition to the final operators. As a result, our proposed extension generates more examples with more continuity over the data space than the origi-nal congealing. Experimental results show that the proposed extension significantly outperforms the original congealing approach in handwritten character recognition. We also show that our extended congealing approach outperforms the best systematic distortions. The effect of the generated examples was evaluated using two statistical classification methods, the k -nearest neighbor ( k -NN) and the support vector machines (SVM). To summarize, the contributions of this article are:  X  X roposing the use of congealing to model human handwriting distortions  X  X tudying and identifying anomalies and limitations of the original congealing ap-proach when applied to handwritten Arabic characters  X  X roposing the collection of intermediate as well as final congealing operators  X  X xperimental analysis showing significant improvement in classification perfor-mance due to our proposed approach
This article is organized as follows. The next section presents a mathematical back-ground for affine transformations and congealing. Our proposed extension to congealing is presented in Section 3. The synthesis of virtual images of Arabic characters is pre-sented in Section 4. Section 5 presents the experimental results and discussion on the effect of using virtual data on the performance of different classification methods. Sec-tion 6 summarizes the related work in handwritten character recognition. Concluding remarks and directions to future work are provided in Section 7. This section provides a background about affine transformations and congealing. An affine transformation refers to one or more linear transformations (rotation, x-scaling, y-scaling, x-shearing, and y-shearing) and/or a translation. Rotation is a cir-cular movement of an object around its center. Scaling enlarges or shrinks an object by a scale factor. Directional scaling, or stretching, is a nonuniform scaling obtained when the scaling factor in one axis is different from that in the other axis. Shearing displaces image points in a fixed direction by an amount proportional to its signed dis-tance from a line that is parallel to that direction. Translation moves every point in the image a constant distance in a certain direction. Global affine transformations (GATs) refer to affine transformations that are applied to the whole image. A combination of these transformations in homogeneous coordinates can be formed by multiplying the constituent matrices as follows [Miller 2002]: U = F ( t x , t y , X , S x , S y , h x , h y ) Congealing is an image factoring and alignment approach introduced by Miller [2002]. Congealing is a method of factoring a set of images into a set of latent images and a set of transformation functions using a joint gradient descent (a group of gradient descents, one for each image, occurring simultaneously). The sum of the pixelwise entropies is considered as the measure of similarity. Given a set of images for a particular class, congealing transforms each image through an iterative process to minimize the summed pixelwise entropies of the resulting images. The result is a set of estimated latent images and a set of transforms. The goal of congealing is to make a set of images more similar to each other by independently and simultaneously transforming each one of them [Learned-Miller 2006].
 Consider a set of N observed binary images of a particular class, each image having P pixels. Let the value of the i th pixel in the j th image be denoted by X j i . A pixel stack is a collection of pixels drawn from the same location in each of the set of N images as shown in Figure 1. The entropy of the pixel stack can be calculated as follows: where N 0 and N 1 are the number of occurrences of 0 (black) and 1 (white) in the binary-valued pixel stack, respectively. As the stack entropy reduces, the pixels in the stack become more similar to each other. Congealing minimizes the sum of all of the pixel stacks entropies: by making small adjustments in the affine transforms applied to each image [Learned-Miller 2006]. The following section studies and evaluates the congealing technique on Arabic characters. We applied and examined the congealing technique on Arabic characters. A set of images of the same character were congealed in order to align them as much as possible and to factorize each of them into a latent image and a vector of affine transformations. Figure 2 shows a set of handwritten examples of the letter Meem before congealing (a), after partial congealing (b), and after final congealing (c). It is clearly shown how the examples before congealing vary significantly in their characteristics such as size, position, and rotation, while they become more similar to each other after congealing.
One of the interesting points to notice is that when congealing is applied to some sets of character images, the process succeeds in minimizing the total pixel stack entropy but the produced latent image is irrelevant to the same character class. For example, when a set of images of the letter Kha was congealed, the result was a set of similar images, but they are irrelevant to the letter Kha, because they were almost straight lines as presented in Figure 3. It is also noticed that one of the letter Kha examples disappeared through congealing as shown in the totally blank (gray) image. This example was originally bigger in size than other examples. Instead of reducing its size, congealing tended to increase the size of this example until it disappeared out of the rectangular range. Another interesting point is that congealing may lead to shrinking the whole image until it disappears, keeping only a point or a black background with zero total entropy.
 Like other gradient descent techniques, congealing can be trapped in local minima. Figure 4(a) shows an image of the letter Kaf, which is a little bit smaller in size than the rest of images, such as the one in Figure 4(b). Instead of increasing the size of this image, congealing tended to reduce its size in order to align it with the curved part of other images and got stuck in this local minimum, as shown in Figure 4(c). It is this observation that inspired our modification or extension of congealing. Instead of collecting transformation operators after the convergence of the congealing process, we collect intermediate operators during the congealing iterations. In this way, when we apply the extracted congealing operators, we generate examples with more continuity over the data space. It should be noted that our proposed extension does not avoid being stuck in local minima. Instead, our approach minimizes the effect of bad local minima by keeping track of intermediate operators instead of only the final ones. Algorithm 1 illustrates the intermediate congealing approach.
 We evaluated two directions for synthesizing virtual examples of handwritten Ara-bic characters: systematic global affine transformations and congealing. Section 4.1 presents the synthetic examples through GATs, while the synthesis of Arabic char-acter examples using the transformations derived from congealing is presented in Section 4.2.
 Three different systematic GATs are used in different combinations. These transforma-tions are rotation within  X  7 . 5  X  , x-scaling and y-scaling within  X  25%, and x-shearing and y-shearing within  X  0 . 2. We applied each one of these transformations individually in addition to combining them with the Arabic characters in order to synthesize a vari-ety of shapes of character images. The virtual examples of the digit nine generated by applying a combination of rotation within  X  7 . 5  X  , scaling within  X  25%, and shearing within  X  0 . 2 are shown in Figure 5(a). As stated earlier in Section 2.2, congealing a set of observed images that belongs to a certain class produces a single latent image of that class as well as a set of transformation vectors. If we apply the inverse of these transformation vectors to the latent image, we should get the original observed images.

In the same manner, if we apply the inverse of the transformation vectors, which were factored from congealing, to a chosen observed image of a certain class, instead of the latent image, we can get a number of different virtual examples of images of the same class that equals the number of available transformation vectors. Similarly, if we have a few available observed images of a certain class and apply the inverse of the congealing transformation vectors to these observed images, we can get a number of dif-ferent virtual examples of images of the same class that equals the number of available transformation vectors times the number of the available observed images. Therefore, congealing provides an automated method for generating GAT operators that mimic human handwriting distortions. Figure 6 illustrates this process of synthesizing virtual digits. Figure 5(b) shows the synthesized images from applying congealing transforma-tions to a single image of the digit six. Figure 5(b) shows how the generated examples are clearly different from one another, despite being generated from the same single image. This section evaluates three approaches in synthesizing handwritten Arabic character examples from the few real available examples. The first approach is the systematic, hand-coded GAT, which includes rotation, scaling, and shearing. The second approach is the affine transformations factorized from applying congealing to one or more character classes. The third approach is our proposed intermediate congealing process.
The dataset used in these experiments contains 48 examples of each Arabic hand-written character class (28 letters and 10 digits) obtained from 48 different writers created by Khedher et al. [2005]. Several classification experiments were applied to the Arabic digits using two classification methods; k -Nearest Neighbor ( k -NN) as an example of instance-based methods, and Support Vector Machines (SVMs) as an exam-ple of parametric methods. The 48 examples of each character were divided into three folds for cross-validation. One fold is kept for testing, and the other two folds were used for training. The learning instances were represented by their pixel values. Initially, the two classifiers were trained and tested with real examples only. Then, different sets of affine transformations were applied to the real training data in order to synthesize virtual training examples. After that, the classifiers were trained with these virtual examples combined with the real examples.

For the systematic, hand-coded, GATs, we applied different combinations of affine transformations. These transformations are rotation within  X  7 . 5  X  , x-scaling and y-scaling within  X  25%, and x-shearing and y-shearing within  X  0 . 2. The congealing vir-tual examples are synthesized according to the method explained in Section 4.2. In intermediate congealing, we extracted intermediate operators in five locations (when iteration = 1, 2, 5, 10, and the final iteration). We concentrated on the first iterations because higher entropy difference takes place within them. It is worth saying that the same number of real training and test examples was used to examine GAT, congealing, and intermediate congealing. However, each perturbation method synthesized a differ-ent number of virtual training examples. The recognition rate of Arabic (Indian) digits is presented in Table I.

In the case of k -NN, we used three values of k (1, 3, and 5) as the number of neighbors, and the Euclidian distance as the measure of similarity between data points. In the case of SVM, two kernel functions were used: the polynomial function with three exponent values d = 1, 2, and 3, and the radial basis function with  X  = 0.01 and 0.07. Table I clearly shows that both classification methods (except SVM with d = 1) are significantly improved by the different types of virtual examples. Table I shows that GAT provides an overall better recognition rate than congealing. This can be inferred from the fewer number of virtual examples that are synthesized by congealing when compared to GAT. However, our proposed intermediate congealing approach significantly and consistently outperforms both the original congealing approach and the systematic distortions.
Another interesting observation is the effect of the learning parameters on the gen-erated examples. The recognition rate of SVM with the first-degree polynomial ( d = 1) is negatively affected by the virtual examples. One possible explanation is that the virtual examples of each class are closer to the examples of other classes, which makes the decision boundaries between classes nonlinear, and therefore, this linear function fails to construct this nonlinear decision boundary. The kernel function allows SVM to locate the hyperplane in high-dimensional space that effectively separates training data points [Zaki et al. 2004]. The RBF kernel maps samples into a higher-dimensional space, and therefore, unlike the linear kernel, it can handle the case when the relation between class labels and attributes is nonlinear [Zaki et al. 2011]. Notice also the ef-fect of increasing k in k -NN. The accuracy of k -NN using only real examples decreases, whereas the accuracy of intermediate congealing increases. One possible explanation is that with so many virtual examples, few of them might be bad, and so using more neighbors reduces the effect of noise. However, with only real examples, which are very few (only 32), increasing the number of neighbors may reduce accuracy.

We applied the same set of transformations to generate artificial images of Arabic letters. The effect of these affine transformations was tested using the nearest-neighbor classifier. Results are summarized in Table II. It is also shown that intermediate con-gealing significantly outperforms both the original congealing approach and the sys-tematic distortions. It is also noticeable that recognition rate of letters is less than that of digits due to the fact that there are 28 letter classes instead of 10, and therefore, letter recognition is harder than digit recognition.

For a better perspective, Table III shows the reported accuracy for the same datasets in previous work [Khedher et al. 2005]. Unlike our approach, which is unsupervised and does not use any sophisticated feature extraction (used the raw images), the results in Table III were obtained using highly sophisticated feature extraction techniques such as detecting closed loops and detecting the existence of a secondary object (e.g., dots). Our approach resulted in superior performance in recognizing digits and comparable performance in recognizing letters. It should be noted that our baseline is not GAT, but the state-of-the-art results in Table III. It is important to note that our approach can be integrated with feature extraction to achieve even better performance. There have been several research efforts done in handwritten Arabic character recogni-tion [Shatnawi 2015]. Graves and Schmidhuber [2009] introduced an offline handwrit-ing recognizer based on the multidimensional long short-term memory (LSTM) neural networks, which is trained by the raw pixel values of the input images. Mahmoud and Awaida [2009] described a technique for automatic recognition of offline writer-independent handwritten Arabic (Indian) numerals using SVM and Hidden Markov Models (HMMs). Hamdani et al. [2009] presented an offline handwriting recognition system based on the combination of multiple HMM classifiers. The system was imple-mented using the HMM Toolkit (HTK) 1 on the IFN/ENIT database [Pechwitz et al. 2002]. Al-Hajj Mohamad et al. [2009] proposed an Arabic handwritten city name recognition approach based on combining three HMM classifiers that include a set of baseline-dependent and baseline-independent features. The three classifiers are combined at the decision level using three combination schemes: the sum rule, the majority vote rule, and an original neural-network-based combination whose decision function is learned through candidate word scores. Mahmoud and Abu-Amara [2010] described a technique for the recognition of offline handwritten Arabic (Indian) nu-merals using Radon and Fourier Transforms. Parvez and Mahmoud [2013] proposed a structural-based character recognition method. An Arabic text line is segmented into words/subwords and dots are extracted. An adaptive slant correction algorithm that is able to correct the different slant angles of the different components of a text line is presented. Habash and Roth [2011] and Tomeh et al. [2013] incorporated linguisti-cally and semantically related features to Arabic character recognition systems. They presented an error detection system that uses deep lexical and morphological feature models to locate words or phrases that are likely incorrectly recognized. They used BBN s Byblos HMM-based offline handwriting recognition system [Saleem et al. 2009] to generate an N -best list of hypotheses for each segment of Arabic handwriting. Sahloul and Suen [2014] proposed a feed-forward back-propagation neural network approach. The method consists of five stages: binarization, normalization, noise removal, feature extraction, and classification. Three types of features were extracted: structural, statis-tical, and topological features. Structural features are the upper and lower profiles that capture the outlining shape of a connected part of the character as well as horizontal and vertical projection profiles. Statistical features include the four neighboring pixels for each pixel. Topological features include endpoints, pixel ration, and height-to-width ratio.

The use of synthetic data in building character recognition systems has been dis-cussed and examined by several researchers. Kanungo [1996] presented a framework to automatically generate synthetic documents together with ground truth (GT) data in a noise-free version with different types of degradation. Margner and Pechwitz [2001] introduced a system for automatic generation of synthetic printed data for Arabic OCR systems as follows. First, the Arabic text is typeset. Then, a noise-free bitmap of the document and the corresponding GT is automatically generated. And finally, an image distortion is superimposed on the character or word image to simulate the expected real-world noise of the intended application. Some previous work used affine trans-formations to normalize input images against each reference pattern [Ha and Bunke 1997; Wakahara and Odaka 1998; Wakahara et al. 2001]. In contrast, our approach applies the transformations to the training examples in order to synthesize virtual examples.

Miyao and Maruyama [2006] proposed a method to improve the SVM performance for offline handwritten Japanese Hiragana character recognition using virtual training examples synthesized from online characters by applying affine transformation to each stroke of real characters, and then applying affine transformation to each stroke of artificial characters, which were synthesized on the basis of the Principal Component Analysis (PCA). This approach applies the transformations to the training examples, which is similar to our approach. However, our approach differs in that the affine transformations we used are extracted from applying spatial congealing to the real training examples to model real distortions in addition to applying the systematic GATs.

Learned-Miller [2006] has successfully used spatial congealing in handwritten Ara-bic (Western) digit recognition. He built a digit classifier from one example of each class using the nearest-neighbor (1-NN) classifier. We extend his proposed approach with intermediate congealing and conduct an extensive set of experiments. Elarian et al. [2011] presented an approach to synthesize Arabic handwriting text. First, real-word images are segmented into labeled characters, which are then concatenated in an arbitrary way to synthesize artificial word images. The nearest Euclidean-distance neighbor is used for matching characters that can be concatenated to produce natural-looking words. This synthesized text is used to train and test an OCR system. This proposed approach is still in its infancy and was tested on only two writers from the IFN/ENIT dataset [Pechwitz et al. 2002]. Dinges et al. [2013] presented a method for Arabic handwriting synthesis using Active Shape Models (ASMs) computed based on 28,046 online samples of multiple writers. ASMs were used to generate unique letter representations for each synthesis. Subsequently, these representations were modified by affine transformations smoothed by B-Spline interpolation and composed to text. In this work, we model real distortions in handwriting Arabic characters from real examples and then use these distortion models to synthesize handwriting examples that are more realistic. We show that using this model of distortions to synthesize examples leads to significant improvements across different classification algorithms. This work extended the spatial congealing technique and applied and evaluated it on Arabic characters. The extended congealing approach along with different types of systematic affine transformations was used to synthesize a large number of virtual examples of isolated Arabic characters. The effect of these virtual examples was evalu-ated on two classification methods: nearest neighbor and support vector machines. The classification rate was improved when trained by synthetic examples in general, but our proposed extension to congealing was the winner with both classification methods. This approach can be integrated with a word segmentation technique to build a full Arabic character recognizer.

One of the interesting future directions is the use of virtual data in building a character recognizer of entire words that belong to a specific domain such as postal address. Another interesting direction is to segment each character image into parts, and then to use congealing for aligning each part independently. For example, the images of digit nine can be segmented into two parts, the circle and the vertical line. This added flexibility can improve the quality of extracted operators, and consequently the quality of generated examples.

