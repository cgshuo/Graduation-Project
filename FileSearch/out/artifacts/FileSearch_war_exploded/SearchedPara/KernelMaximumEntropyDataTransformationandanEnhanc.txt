 interest to this paper is kernel PCA [3], a member of the kernel-based methods [4]. has a huge impact on the performance of the algorithm.
 clustering is discussed in section 5. Finally , we conclude the paper in section 6. ( via the mapping x correlation matrix is reduced to an eigen value problem in terms of the kernel matrix K element ( i; j ) of K K a matrix with the eigen vectors as columns. Let to the PCA projection of the data points ( x l lar gest kernel space principal axes. Then, the l lar gest eigen values, and the ( N l ) matrix E induced featur e space .
 projection in the kernel space. The Ren yi quadratic entrop y is given by H associated with the random variable X . A d -dimensional data set x actual pdf by its Parzen windo w estimator , given by [10 ] For notational simplicity , we denote W p kernel matrix K kernel matrix as ^ H matrix K a data mapping results in a V ( y ) = 1 close as possible to V ( x ) = 1 similar to K dimensionality of interest).
 The kernel matrix can be decomposed as K inner -product matrix in the Mercer kernel induced feature space. Let since then K axes in the Mercer kernel feature space, hence dening a N -dimensional data set. follo ws [8] where e that the products to using the k lar gest and eigen vectors of K E thus refer to the mapping kernel feature space, which is dened as corresponding to the l lar gest eigen values of K space correlation matrix, we project ( x set is given by the sum of the lar gest eigen values).
 kernel space output quantity is the transformed data set quantity is the kernel matrix K matrix K kernel matrix K 3.1 Inter pretation in Terms of Cost Function Minimization Kernel MaxEnt produces a new kernel matrix K K and eigen values in order to minimize the cost function 1 T ( K kno wn that the kernel PCA matrix K the Frobenius norm of ( K squaring of matrix A .) 3.2 Kernel MaxEnt Eigen vectors Re veal Cluster Structur e (point clusters). Assume that subset one consists of N data points. Hence, N = N where 1 M M ( 0 M M ) is the ( M M ) all ones (zero) matrix and D = diag ( N point x will be represented by x concentrated eigen values of the kernel matrix.
 will mak e the sum of the elements in K Some illustrations of this property follo w in the next section. 3.3 Parzen Windo w Size Selection simplest, given by ^ = using this rule. values are not signicantly dif ferent. The bars in (b) sho ws the entrop y terms their blockwise appearance. The kernel matrix K wise matrix. It is not. In (g), the kernel MaxEnt approximation K (g) sho ws the corresponding K having a blockwise appearance. Both the transformed data utilized for further data analysis. In the follo wing, we focus on measure, which is closely connected to the Ren yi entrop y. Let ^ f be expressed as [6] where m the two clusters. Note that ^ D ( f ^ f between a data point and the mean vector m the kernel matrix K MaxEnt. 4) Initialize mean vectors. 5) For all data points: x initialization etc.) we refer to [7].
 comple x than clustering based on only the C lar gest eigen vectors. MaxEnt based on indenite kernels will be studied in future work. Ackno wledgements RJ is supported by NFR grant 171125/V30 and MG is supported by EPSRC grant EP/C010620/1.
