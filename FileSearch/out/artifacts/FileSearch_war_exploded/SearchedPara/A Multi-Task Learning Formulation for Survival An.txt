 Predicting the occurrence of a particular event of interest at future time points is the primary goal of survival analysis. The presence of incomplete observations due to time limi-tations or loss of data traces is known as censoring which brings unique challenges in this domain and differentiates survival analysis from other standard regression methods. The popularly used survival analysis methods such as Cox proportional hazard model and parametric survival regres-sion suffer from some strict assumptions and hypotheses that are not realistic in most of the real-world applications. To overcome the weaknesses of these two types of methods, we reformulate the survival analysis problem as a multi-task learning problem and propose a new multi-task learning based formulation to predict the survival time by estimating the survival status at each time interval during the study du-ration. We propose an indicator matrix to enable the multi-task learning algorithm to handle censored instances and in-corporate some of the important characteristics of survival problems such as non-negative non-increasing list structure into our model through max-heap projection. We employ the l 2 , 1 -norm penalty to learn a shared representation across related tasks and hence select important features and alle-viate over-fitting in high-dimensional feature spaces; thus, reducing the prediction error of each task. To efficiently handle the two non-smooth constraints, in this paper, we propose an optimization method which employs Alternat-ing Direction Method of Multipliers (ADMM) algorithm to solve the proposed multi-task learning problem. We demon-strate the performance of the proposed method using real-world microarray gene expression datasets and show that our methods outperform state-of-the-art methods.
  X  Computing methodologies  X  Multi-task learning;  X  Information systems  X  Data mining;  X  Mathematics of computing  X  Survival analysis; Survival analysis, Multi-task learning, regularization, high-dimensional data.
Survival analysis aims at modeling time-to-event data, which is typically collected in longitudinal studies that start from a particular time and last until a certain event of inter-est has occurred [11, 21]. However, the event of interest may not always be observed during the study period due to time limitations or losing data traces. This phenomenon is more commonly known as censoring and makes survival analysis different from (and more challenging than) the standard re-gression methods. For the cases where the event of interest has been observed, the time to the event of interest is known as the survival time ; while for the other instances, the last observed time is known as the censored time (we call these instances as censored instances). The most common form of censoring that occurs in real-world scenarios is right censor-ing 1 , where the survival time of a censored instance is longer than or equal to the censored time, but its precise value is unknown.

In survival analysis, the Cox proportional hazards model and parametric censored regression models are important fundamental techniques for survival time prediction. These two methods (and their extensions) have been extensively studied in the fields of statistical learning and data mining. We will now describe the primary weaknesses of these two methods.

The Cox model and its extensions are built based on the proportional hazards hypothesis , i.e., it assumes that the haz-ard ratio between two instances is constant in time. This hypothesis indicates that the survival curves of all instances share a similar shape which is not realistic in many real-world applications. Also, the Cox model does not predict the survival time directly but rather models the hazard ra-tio. To predict the survival time, a baseline hazard function has to be estimated separately and this estimation will in-duce more prediction errors. When tied observations (sur-vival times of multiple instances is exactly the same) occur during the study, Cox model has to use some approximation methods which suffer from either inducing bias (Breslow X  X  approximation and Efron X  X  approximation [6]) or bad scala-bility (Discrete method [25]).
In this paper, we refer to the right censored data as cen-sored data.
The parametric censored regression models suffer from even more critical weaknesses. The prediction performance of parametric censored regression is highly dependent on the choice of the distribution [12]. However, in real-world ap-plications there are too many complex interactions and sce-narios that can affect the event of interest in various ways; thus, in practice, choosing an appropriate theoretical distri-bution to approximate survival data is very difficult, if not impossible.

To overcome these weaknesses of both types of methods, in this paper, we propose the  X  MTLSA  X  model, which stands for  X  M ulti-T ask L earning model for S urvival A nalysis X . We formulate the original survival time prediction problem into a multi-task learning problem. The primary motivation of using multi-task learning is because of its ability to learn a shared representation across related tasks and reduce the prediction error of each task. Thus, the model can provide a more accurate estimation of whether an event occurs or not at the beginning of each time interval which will thus provide an accurate estimation of the survival time for each instance. Another advantage of using multi-task learning for survival time estimation is because it translates the re-gression problem into a series of related binary classifica-tion problems, and at each time interval the corresponding classifier only focuses on modeling the local problem and hence provides a more accurate estimation than the regres-sion models which aim at modeling the entire problem at once. Our model is built without any additional hypothesis except linear hypothesis, i.e., the feature and target exhibit a linear relationship, unlike the Cox proportional hazards model and parametric censored regression models.

As the survival status of a censored instance is unknown after the corresponding censored time, the target labeling matrix is not complete; therefore, the standard multi-task learning methods fail to handle the censored instances. To overcome this problem, we propose to use an additional in-dicator matrix which allows the model to simultaneously learn from both uncensored and censored instances (details can be found in Section 3.1) and hence the proposed model can simultaneously take advantage of both uncensored and censored instances. We notice that in survival analysis with non-recurring events, the survival status of instances natu-rally follows the non-negative non-increasing list structure, i.e., once the event occurs it will not occur again. Since the l 2 , 1 -norm encourages multiple predictors to share simi-lar sparsity patterns, it will not only select important fea-tures and alleviate over-fitting in high-dimensional feature spaces but will also learn a shared representation across all tasks at different time intervals. In MTLSA, we incorporate the non-negative non-increasing constraint and the l 2 , 1 with the loss function and propose an Alternating Direc-tion Method of Multipliers (ADMM) algorithm [3] for co-efficient estimation. In the proposed ADMM method, the non-negative non-increasing list constraint optimization is transformed into an Euclidean projection problem that can be learned efficiently.

In our empirical evaluation using various real-world gene expression cancer survival benchmark datasets, our model attains very competitive prediction performance and out-performs state-of-the-art methods in survival analysis. Ad-ditionally, we also demonstrate that our model outperforms most of the competing methods for the task of classifying whether or not a subject is alive at the beginning of each time interval in the observed study period.

The rest of the paper is organized as follows. In Sec-tion 2, related data mining approaches for survival analysis are discussed. In section 3, our proposed approach includ-ing the details of optimization procedure is explained. Sec-tion 4 demonstrates our experimental results on several real-world high-dimensional datasets while Section 5 concludes our work.
The prominent prediction methods in survival analysis can be categorized into three types: Cox-based, parametric cen-sored regression, and linear models. In this section, we will briefly describe the most important works under each cate-gory and highlight the differences and relationships between our proposed model and existing works.

The Cox proportional hazards model [5] is one of the ear-liest and most widely used survival analysis methods which has garnered significant interest from researchers in both statistics and data mining communities. The Cox is a semi-parametric model which does not make any assumption about the distribution of the survival outcomes and is usually learned by optimizing a partial likelihood function. To deal with high-dimensional data, some regularization methods have been proposed in the literature. These methods include LASSO-COX [26] which introduces the L 1 norm penalty in the log-partial likelihood loss function, Elastic-Net Cox (EN-COX) [23] which uses the elastic net penalty term and the kernel elastic net penalized Cox regression [31, 30] which modifies the elastic net penalty using a kernel matrix.
Parametric censored regression provides an important al-ternative to the Cox-based models. Parametric censored regression methods assume that the survival times (Case 1) or the logarithm of the survival times (Case 2) of all in-stances in the data follow a particular distribution [11]. The latter case, namely Case 2, is also termed as Accelerated failure time (AFT) models [34] because they assume that the covariate will  X  X ccelerate X  or  X  X ecelerate X  the time to the event of interest. Weibull distribution, logistic distribution, log-normal distribution, and log-logistic distribution [11] are the most commonly used distributions in parametric cen-sored regression and the last two are considered to be the AFT models. In [14], the elastic net penalty has been em-ployed to enable the parametric censored regression models to handle high-dimensional censored data.

Apart from the above two types of survival prediction methods, linear regression is another important branch of survival analysis. Strictly speaking, linear regression is a specific parametric censored regression; we consider linear censored regression models separately because linear regres-sion is a fundamental method in data analysis. Because of censoring, the least-squares estimator cannot be directly used in survival analysis. The Tobit model [27] is the earli-est attempt to extend the linear regression for data analysis with censored observations. Later, Buckley-James (BJ) es-timator [4] was proposed to solve survival prediction with the combination of the Kaplan-Meier (K-M) estimator [10] (which is a non-parametric model). Recently, the elastic net penalty has been used within the BJ regression (EN-BJ) [32] and a weighted linear model [13] for efficiently handling the high-dimensional survival analysis problems.
In this paper, different from all the above mentioned meth-ods, we propose a new approach which transforms the orig-inal survival analysis problem into a series of related binary classification problems, and then develop a multi-task learn-ing method which explicitly models two important struc-tural constraints of this problem, namely the non-negative and non-increasing list . In addition, the l 2 , 1 -norm penalty is employed to enable the model learn a shared represen-tation across related tasks and hence select important fea-tures in high-dimensional feature spaces. In [15], the authors proposed a multi-task logistic regression to predict the sur-vival times of each instance by using a likelihood function that combines multiple local logistic regression models and incorporate the dependency between these models. This procedure is very hard to learn and fails to handle high-dimensional cases.
In this section, we will first transform the original sur-vival analysis problem into a multi-task learning problem by decomposing the regression component into related clas-sification tasks. Then we will propose a new objective func-tion that can solve the transformed problem and develop an ADMM based algorithm to optimize the objective function. We also provide a detailed algorithmic analysis in terms of convergence and complexity and then discuss a variant of the algorithm by relaxing certain constraints.
In survival analysis, for each data instance, we observe either a survival time ( O i ) or a censored time ( C not both. The dataset is right-censored if and only if T min( O i ,C i ) can be observed during the study. An instance in survival data is usually represented by a triplet ( X i where X i is a 1  X  q feature vector;  X  i is the censoring indica-tor, i.e.,  X  i = 1 for an uncensored instance, and  X  i = 0 for a censored instance; and T i denotes the observed time and is equal to the survival time O i for uncensored instances and C i otherwise, i.e., For censored instances, O i is a latent value, and the goal of survival analysis is to model the relationship between X uncensored instances.

In practice, time is considered as countable time intervals rather than a real number (a number with a fraction). We translate the original label into a k -column target matrix Y , where k = max( T i ) ,  X  i = 1 , 2 ,  X  X  X  ,n, is the maximum fol-lowup time of all the instances. Each element in the target matrix indicates whether the event occurred ( X 0 X ) or not ( X 1 X ), and the original survival prediction problem can thus be transformed into a multi-task learning problem. The pri-mary motivation of transforming the survival analysis into a multi-task learning problem is that the dependency between the outcomes at various timepoints are accurately captured through a shared representation across related tasks in this multi-task transformation which will reduce the prediction error on each task. Note that for censored instances we know that the event did not occur until the corresponding observed times, but we do not know whether the event oc-curs or not afterwards.

For now, we represent those unknown cells using  X  X uestion marks X  and later we will discuss the details of converting them into a more viable form. Figure 1 shows an example of generating a target matrix Y from the original labels. For the four uncensored instances (ID  X  1 , 4 , 5 , 6), the cells of the corresponding rows in the target matrix Y are labeled as  X 1 X  until the observed time ( T 1 = 3 ,T 4 = 7 ,T 5 = 5 ,T 6 = 6) and as  X 0 X  for the remaining cells; for the censored instances (ID  X  2 , 3), the cells of the corresponding rows in the target matrix Y are labeled as  X 1 X  until the censored time ( T 2 6 ,T 3 = 2) and as  X ? X  for the remaining cells.
 Original(label Figure 1: Illustration of generating Y and W from the orig-inal label in a simple survival dataset.

In this paper, we only consider the non-recurring event scenario which means once the event occurs then it will not occur again. Hence, for a given row in the target matrix Y , once the label becomes  X 0 X  it will not change back to  X 1 X . Thus, we can see that each row of Y will have a non-negative non-increasing list structure:
P = { Y  X  0 ,Y ij  X  Y il | j  X  l,  X  j = 1 ,  X  X  X  ,k,  X  l = 1 ,  X  X  X  ,k } where i = 1 , 2 ,  X  X  X  ,n .

An intuitive approach for solve this multi-task learning based formulation is as follows: where B  X  R q  X  k is the estimated coefficient matrix, k  X  k denotes Frobenius norm, and R ( B ) denotes the regulariza-tion term that prevents over-fitting and incorporates the ad-ditional constraints imposed by this problem. Note that Y is not a complete n  X  k target matrix (from the previous dis-cussion). Now, we propose an indicator matrix W to handle the question marks in Y . W is an n  X  k binary matrix; we set W ij = 1 if the exact labeling information of Y ij is known, and W ij = 0 if Y ij =  X ? X . In Figure 1, we also show the corresponding W for the example survival data considered. The optimization problem in Eq.(3) is re-defined as: where
Let us now briefly discuss two unique characteristics of the proposed model and then design suitable regulariza-tion terms to incorporate other additional constraints. The proposed model in Eq.(4) has two unique properties, Non-negative non-increasing and Temporal smoothness , which are desired to match the nature of the non-recurring events survival analysis.  X  Non-negative and non-increasing: As discussed above, each row of the target matrix follows the non-negative non-increasing list structure. To preserve this characteristic, a corresponding structure constraint is added in Eq.(4) to ensure that the estimated output XB also follows the non-negative non-increasing list structure.  X  Temporal smoothness: Since many works in the survival data deal with non-recurring events, i.e., for a certain row in the target matrix Y , once the label becomes  X 0 X  it cannot change back to  X 1 X  (or any other value), and this label change will occur at most once. Hence, in most cases, the adjacent labels of each instance are the same; thus, for all the N instances, the label vectors of adjacent tasks are similar. This is the temporal smoothness characteristic which will be modeled in the proposed multi-task learning formulation.

In this paper, the non-negative max-heap projection [18] is employed to ensure that XB follows the non-negative non-increasing list structure. This projection approximates each element of every selected set of target values by their corre-sponding mean values (refer to the Appendix for more de-tails), and hence all elements in each selected set (a sublist in our case) share a same estimated target value; therefore, the non-negative max-heap projection also induces the tem-poral smoothness of XB . Apart from the two characteristics discussed above, our model should also alleviate over-fitting and induce sparsity in the estimated coefficients.  X  Sparsity: The goal here is to learn a shared representation across all the tasks; thus, the model can select important common hidden features and reduce the prediction error of each task. The l 2 , 1 -norm is chosen to be an additional reg-ularization term for our model because it encourages mul-tiple predictors to share similar sparsity patterns. Thus, the l 2 , 1 -norm regularized regression model is able to select some common features across all the tasks [1]. In addi-tion, such a sparsity inducing penalty will be able to help our model effectively handle high-dimensional datasets.  X  Overfitting: A Frobenius norm regularization on the coef-ficient matrix B is introduced to alleviate the over-fitting problem for high-dimensional data.

Incorporating all of the above additional constraints in the form of regularizers into the proposed multi-task learn-ing model, MTLSA , the following minimization problem is formulated: minimize where k X k 2 , 1 denotes the l 2 , 1 -norm, and  X  1  X  0 and  X  are two regularization parameters.
The solution of the optimization problem in Eq.(5) is not trivial since it contains non-negative and non-increasing con-straints along with the fact that the l 2 , 1 -norm is a non-smooth penalty. We propose an ADMM based algorithm to solve the optimization problem proposed in Eq.(5). By introducing a new matrix M = XB , Eq.(5) can be rewritten in ADMM form as minimize subject to M = XB Using the scaled dual variable  X  and penalty parameter  X  &gt; 0, the resulting augmented Largrangian of Eq.(6) is Thus, the scaled form of ADMM algorithm can be written as: M t +1 := arg min B t +1 := arg min
Now the main task of our model is to solve the optimiza-tion problems proposed in Eq.(8) and Eq.(9). Next we will present the algorithm for solving Eq.(8) and Eq.(9) in detail. Step 1: Update M t +1 given B t and  X  t (solve Eq.(8))
The updating of M t +1 is a constrained smooth convex op-timization problem which can be expressed as a generalized form: where g ( M ) = 1 2 k  X  W ( Y  X  M ) k 2 F +  X  2 k M  X  XB t a smooth function and P is the non-negative non-increasing list structure defined in Eq.(2). Let S =  X  t  X  XB t objective function can be reformulated as: g ( M ) = 1 2 = 1 2 = 1 2 = 1 2 = 1 2 pend on M ij . Therefor, the optimization problem in Eq.(11) equals to an Euclidean projection ensures M i 1  X  M i 2  X   X  X  X   X  M ik  X  0. The Euclidean pro-jection in Eq.(12) is a special case of the non-negative max-heap projection which can be efficiently solved [18, 17], and some details of the non-negative max-heap projection can be found in Appendix.
 Step 2: Update B t +1 given M t +1 and  X  t (solve Eq.(9))
The updating of B t +1 in Eq.(9) can be considered as a standard l 2 , 1 -norm regularization problem: arg min where L is the coresponding label matrix. To solve Eq.(9), we just need to set L = M t +1 +  X  t ,  X  L 1 =  X  2  X  , and  X  Then Eq.(9) can be solved via alternating minimization algo-rithm proposed in [1] or Nesterov X  X  method with efficient Eu-clidean projection [16]. In this paper, we choose the method proposed in [16] as the l 2 , 1 solver because it only requires
Combining all of the above components, the proposed method can be summarized as shown in Algorithm 1. We initalize the M 0 to be the target matrix, and then updat-ing M t and B t based on the two steps we discussed above, accordingly.
 Algorithm 1: Proposed MTLSA Algorithm 1 Initialize : t = 0 ,M t = Y, X  t = 0 ,B t = 0 ; 2 repeat 3 Compute M t +1 by solving Eq.(12); 5 Compute B t +1 via solving Eq.(13) by standard 7 t = t + 1; 8 until Convergence ; 9  X  B = B t ;
In this section, we will provide the details about the con-vergence and the time complexity of the proposed MTLSA algorithm.
The problem in Eq.(6) follows the standard ADMM form: where f 1 ( M ) = 1 2 k  X  W ( Y  X  M ) k 2 F and f 2 ( B ) = +  X  2 k B k 2 , 1 are convex functions, P and R q  X  k are closed convex sets. Due to space constraints, we do not provide the proof of convergence and the readers are referred to [3, 8] which provide the details of the convergence of the standard ADMM form given in Eq.(14). Based on the analysis in [8], the Algorithm 1 requires O ( 1 ) iterations to achieve an accuracy of .
We first analyze the time complexity of Step 1. In Eq.(12)  X  M ij can be calculated with a time complexity of O ( q ) be-B ( ,j ) (the j th column of B t ) all have q elements. For one instance (row), the Euclidean projection in Eq.(12) can be efficiently calculated with a worst-case complexity of O ( k ) [18], so for all n instances the Euclidean projection can be calculated in O ( nk ). Therefore, the updating of M t +1 in total O ( nqk ) calculations, where n , q , and k denote the training sample size, feature dimensionality, and the number of tasks, respectively.

From [16], we know that in Step 2, the standard l 2 , 1 -norm regularized multi-task least squares problem in Eq.(13) can be solved with a time complexity of O ( 1  X  ( nqk + qk )) = O ( 1  X  nqk ) to achieve an accuracy of . Moreover, based on the convergence rate of Algorithm 1, we can conclude that the total time complexity of the proposed method is O ( 1  X  nqk ) for achieving an -level optimal solution.
We also develop a variant of the MTLSA model in order to be able to effectively handle large number of tasks. The MTLSA model proposed earlier strictly enforces that the estimated output follows the non-negative non-increasing structure which is the inherent nature of the target matrix Y . However, when the problem has a large number of tasks, this constraint may become too strict that may lead to model overfitting. Thus, the new variant model, MTLSA.V2 , will not enforce the non-negative non-increasing list struc-ture constraint, in the training phase. Mathematically, the proposed MTLSA.V2 model is defined as: arg min This problem can be efficiently solved using the fast iterative shrinkage thresholding algorithm (FISTA) algorithm with l 2 , 1 projection [16] by incorporating the  X  W (  X  ) when calcu-lating the objective function and its gradient. Note that in the testing phase of both MTLSA and MTLSA.V2 , the non-negative non-increasing list structure regularization will be used on X  X  B to enforce the estimated output of test in-stances follow the property of non-recurring event survival analysis.
In this section, we will first describe the datasets used in our evaluation and then provide the performance results along with the implementation details. We also demonstrate the scalability of the proposed method.
For model evaluation, we used several publicly available high-dimensional gene expression cancer survival benchmark datasets 2 . The datasets used in our experiments are as follows:  X  The Norway/Stanford breast cancer data (NSBCD) [24] contains gene expression measurements of 115 women with breast cancer. These women are observed for 188 months to monitor the death time.  X  Van de Vijver X  X  Microarray Breast Cancer data (VDV) [29] contains gene expression profile information which can be used for predicting the clinical outcome of breast can-cer. It contains 4,707 gene expression values on 78 patients with survival information for 13 years.  X  Adult myeloid leukemia (AML) data contains gene expres-sion profiles of 116 AML patients with a maximum follow-up time of 1,625 days. In our experiments, we transform the observation time from daily basis to monthly basis and hence the observation lasts for 54 months.  X  Gene-expression profiles of lung adenocarcinoma (Lung) [2] is a dataset containing observations of 86 early-stage lung adenocarcinoma patients for a period of 110 months.  X  Mantle Cell Lymphoma (MCL) 3 [22] is the data collected from 92 MCL patients with survival information for 14 years.  X  The Dutch Breast Cancer Data (DBCD) from van Houwelin-gen et al. [28] contains information on 4,919 gene expres-sion levels for 295 women with breast cancer. The maxi-mum follow-up time of these patients was 18 years.  X  Diffuse Large B-Cell Lymphoma (DLBCL) is a dataset that contains Lymphochip DNA microarrays from 240 biopsy samples of DLBCL tumors for studying the sur-vival status of the corresponding patients and the obser-vation lasts 21 years.
 Table 1 provides the details of the datasets that are used in our experiments. In this table, the column titled  X # Cen-sored X  corresponds to the number of censored instances in each dataset. In cancer survival prediction, the event of interest is patient death; therefore, an uncensored instance corresponds to the death of the patient during the study, while a censored instance corresponds to the patient being still alive at the last observed time (censored time). Based on the study duration of each dataset, we translate the sur-vival prediction problem to a corresponding multi-task prob-lem as described in Section 3.1. The number of tasks for each data is given in the column titled  X # Tasks X  in the ta-ble. For model evaluation, we used 5-fold cross validation when the number of instances is greater than 150 and 3-fold cross validation otherwise.
 Dataset # Instances # Features # Censored # Tasks NSBCD 115 549 77 188 VDV 78 4705 44 13 AML 116 6283 49 54 Lung 86 7129 62 110 MCL 92 8810 28 14 DBCD 295 4919 216 18 DLBCL 240 7399 102 21 http://user.it.uu.se/  X liuya610/download.html http://llmpp.nih.gov/MCL/
We comprehensively compare our proposed methods with several popular state-of-the-art related methods. We now summarize the comparison methods into five categories, and we will briefly describe the basic idea and also provide the implementation details.  X  Cox based models : The Cox proportional hazards model [5] is the most commonly used semi-parametric model in survival analysis. The hazard function has the form baseline hazard function for all instances and  X  is the co-efficient vector which can be estimated by minimizing the negative log-partial likelihood function. The Cox model can be trained by using the coxph function in the survival package [25]. The l 1 -norm penalized Cox model  X  X ASSO-
COX X  and elastic net penalized Cox model  X  X N-COX X  can be learned using the cocktail function in the fastcox package [35].  X  Parametric censored regression models : In para-metric models, the joint probability of the uncensored instances can be formulated as a product of death den-sity functions and the joint probability of the censored instances can be formulated as a product of survival func-tions . Thus, a standard likelihood function can be built by combining these two components and the correspond-ing model parameters are estimated using the maximum-likelihood estimation (MLE) procedure [11]. In our exper-iments, the parametric censored regression methods are trained using the survreg function in the survival pack-age with Weibull, Logistic, Loglogistic, and Loggaussian distributions.  X  Linear models : Tobit model [27] is an extension of the linear regression y j = X j  X  +  X  j , X  j  X  N (0 , X  2 ), but the pa-rameters are estimated by the maximum likelihood method rather than using the least squares error. It can be trained using the survreg function with Gaussian distributions. The elastic net penalized Buckley-James regression  X  X N-
BJ X  is implemented using the bujar package [33]. We also compared our proposed methods with the ordinary least squares (OLS) linear regression since the loss function in our model has a similar form to the OLS. Note that, the
OLS is not a censored regression method and hence it is learned using only the uncensored instances rather than the entire set of training instances.  X  Pairwise ranking based models : Boosting concordance index (BoostCI) [19] is an approach where the concor-dance index metric is modified into an equivalent smoothed criterion using the sigmoid function and the resulting op-timization problem is solved using a gradient boosting al-gorithm. The implementation of BoostCI (using R code) can be found in the supporting file of [19] 4 .  X  Multi-task learning models : We compared our pro-posed methods with the standard multi-task learning mod-els, multi-task Lasso (Multi-LASSO) and l 2 , 1 -norm based multi-task feature learning method (Multi-l 2 , 1 ). In MAL-
SAR package, these two models are learned via  X  X est L21 X  and  X  X est Lasso X  functions, respectively [36]. Note that, the these two methods cannot handle censored instances, so the model is learned using only the uncensored in-stances rather than the entire set of training instances, files.figshare.com/1339232/Text S1.pdf with their standard deviations).
 COX based Parametric models Linear models Multi-task based and the labeling matrix is generated based on the scheme presented in Section 3.1. Our proposed methods MTLSA and MTLSA.V2 are implemented via Matlab and the source code can be download at the following website 5 .
Due to the presence of censoring in the data, the standard evaluation metrics for regression such as root of mean square error and R 2 are not suitable for measuring the performance in survival analysis [9]. Instead, the concordance index (C-index), or the concordance probability , is used to measure the performance of prediction models in survival analysis [7]. Let us consider a pair of bivariate observations ( y the predicted one. The concordance probability is defined as: By definition, the C-index has the same scale as the classical area under the ROC (AUC) in binary classification, and if y is binary, then the C-index is same as the AUC. In the Cox based models, the instances with a low hazard rate should https://github.com/MLSurvival/MTLSA survive longer, and the C-index will be calculated as follows: where num denotes the number of comparable pairs and I [  X  ] is the indicator function. The C-index in other methods which aim at directly learning the survival time should be calculated as: c = 1 learning models cannot directly predict the survival time but they can determine whether an instance is alive or not at each time interval (or task); thus, based on this information, we can predict the survival time. In Table 2, we provide the performance results of C-index values of different algorithms on various real-world high-dimensional micro-array cancer survival datasets. The best results are highlighted in bold. The results show that our proposed models outperform the other state-of-the-art models 6 .
The method described in [15] was not able run on our high-dimensional datasets and hence we were not able to obtain any results for that method. AUC (along with their standard deviations).
 COX based Parametric models Linear models Multi-task based
The C-index measures the model performance in regres-sion problems. In addition to it, we also evaluate the model performance in classification problems which corresponds to whether a patient can survive at each time interval or not. Since censoring occurs, the number of patients, who have a known survival status label ( X 1 X  or  X 0 X  in target matrix Y ), will reduce during the observation period, (as shown in Figure 1, all 6 instances are labeled in Day1 and Day2, and only 5 instances are labeled in Day3). Thus, in Table 3, we present the comparison of weighted average AUC values of different tasks (time intervals). The weighted average AUC is defined as where AUC ( i ) is the AUC value of the i th task, and n ( i ) the number of instances which have a known survival status label in the i th time interval. The results in Table 3 show that the proposed models obtain higher AUC avg in most of the datasets. This demonstrates that our proposed meth-ods have a better time-dependent prediction capability than other related methods.

From Tables 2 and 3, we note that our proposed methods significantly outperform the standard multi-task learning models. This reflects that the model, which is able to han-dle and utilize both censored and uncensored instances, can provide significantly better results compared to the model which will only use fully observed (uncensored) instances. Our proposed indicator matrix W can appropriately handle the censored instances. We can also observe that for some datasets, especially which have a large number of tasks like NSBCD and Lung, the MTLSA.V2 performs better than MTLSA . This is because the non-negative non-increasing structure is a strict constraint and may affect the flexibility of the MTLSA model as mentioned in Section 3.5. We can also observe the standard deviation values of the results ob-tained from both the models are significantly lower across all of the datasets compared to the other methods. This shows the robustness of our method with respect to obtaining bet-ter results across different folds. We empirically evaluate the scalability of the proposed MTLSA method with respect to the sample size ( n ), the number of features ( q ) and the number of tasks ( k ). The synthetic datasets are generated using the the function  X  X im-ple.surv.sim X  in survsim package [20] with different sample sizes, feature dimensionality, and maximum follow-up times (which corresponds to the tasks). All the features are gen-erated based on a uniform distribution, and each of them have a different randomly set interval. The coefficient vec-tor is also randomly generated and remains within [  X  1 , 1]. The observed time is assumed to follow a Log-logistic distri-bution and the time to censorship follows a Weibull distri-bution (which is a standarded practice in survival analysis). Figure 2(a) shows the runtimes for fixed q -k combination and varying n , Figure 2(b) shows the runtimes for fixed n -k combination and varying q , and Figure 2(c) shows the run-times for fixed n -q combination and varying k . These three plots clearly demonstrate that the runtime of MTLSA is close to being linear with respect to n , q and k .
In this paper, we formulated the survival analysis problem as a multi-task learning problem and proposed a new multi-task learning algorithm, MTLSA , which is able to han-dle censored instances in time-to-event data. The MTLSA algorithm explicitly models the critical properties of sin-gle event survival analysis by imposing the non-negative and non-increasing list structural constraint. In addition, the l 2 , 1 -norm penalty is used to enable the model learn a shared representation across related tasks and hence select important features thus alleviating over-fitting in the high-dimensional feature space. We also develop an adaptive vari-ant, MTLSA.V2 , which relaxes the structural constraints and produces better results when the number of tasks is large. We extensively compared the performance of the pro-posed algorithm with state-of-the-art survival analysis meth-ods using several publicly available high-dimensional mi-croarray gene expression datasets using both regression and classification based standard evaluation metrics. We also demonstrated the linear scalability of the proposed model with respect to the sample size, feature dimensionality, and the number of tasks (number of time intervals).
 This work was supported in part by the US National Science Foundation grants IIS-1231742, IIS-1527827, III-1539991, and III-1539722. [1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [2] D. G. Beer, S. L. Kardia, C.-C. Huang, T. J.
 [3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [4] J. Buckley and I. James. Linear regression with [5] D. R. Cox. Regression models and life-tables. Journal [6] B. Efron. The efficiency of cox X  X  likelihood function for [7] F. E. Harrell, R. M. Califf, D. B. Pryor, K. L. Lee, [8] B. He and X. Yuan. On the o(1/n) convergence rate of [9] P. J. Heagerty and Y. Zheng. Survival model [10] E. L. Kaplan and P. Meier. Nonparametric estimation [11] E. T. Lee and J. Wang. Statistical methods for survival [12] Y. Li, V. Rakesh, and C. K. Reddy. Project success [13] Y. Li, B. Vinzamuri, and C. K. Reddy. Regularized [14] Y. Li, K. S. Xu, and C. K. Reddy. Regularized [15] H.-c. Lin, V. Baracos, R. Greiner, and J. Y.
 [16] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [17] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [18] J. Liu, L. Sun, and J. Ye. Projection onto a [19] A. Mayr and M. Schmid. Boosting the concordance [20] D. Morina and A. Navarro. The R package survsim for [21] C. K. Reddy and Y. Li. A review of clinical prediction [22] A. Rosenwald, G. Wright, A. Wiestner, W. C. Chan, [23] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. [24] T. S X rlie, R. Tibshirani, J. Parker, T. Hastie, [25] T. Therneau. A package for survival analysis in S. R [26] R. Tibshirani. The lasso method for variable selection [27] J. Tobin. Estimation of relationships for limited [28] H. C. van Houwelingen, T. Bruinsma, A. A. Hart, [29] L. J. van X  X  Veer, H. Dai, M. J. Van De Vijver, Y. D. [30] B. Vinzamuri, Y. Li, and C. K. Reddy. Active learning [31] B. Vinzamuri and C. K. Reddy. Cox regression with [32] S. Wang, B. Nan, J. Zhu, and D. G. Beer. Doubly [33] Z. Wang, C. Wang, et al. Buckley-james boosting for [34] L. Wei. The accelerated failure time model: a useful [35] Y. Yang and H. Zou. A cocktail algorithm for solving [36] J. Zhou, J. Chen, and J. Ye. MALSAR: Multi-tAsk
A non-negative max-heap is an ordered tree where the values of the nodes are all non-negative and the value of any parent node is no less than the value(s) of its child node(s). It can be mathematically defined as: where T t = ( V t ,E t ) is a target tree with V t = { x 1 containing all the nodes and E t denotes all the edges. The non-negative non-increasing list structure defined in Eq.(2) is a special case of non-negative max-heap where the T t is a sequential list.

In [18], a maximal root-tree based algorithm (Atda) was proposed to solve the non-negative max-heap projection Before describing the algorithm itself, we first introduce some definitions to help understand the maximal root-tree.
Definition 1. For a non-empty tree T = ( V,E ) , its root- X  V  X  V , (2)  X  E  X  E , and (3)  X  T shares the same root as T .
Definition 2. For a non-empty tree T = ( V,E ) , R ( T ) is defined as the root-tree set which contains all root-trees of T .

Definition 3. For a non-empty tree T = ( V,E ) , we de-of all the nodes in T if such mean is non-negative, and 0 otherwise.

Definition 4. For a non-empty tree T = ( V,E ) , we de-fine its maximal root-tree as: all the root-trees of T , and if some root-trees share the same maximal value then M ( T ) is the one with the largest tree size.

The key idea of Atda is that, in the i th call, we find T M ( T ), the maximal root-tree of T , set its corresponding nodes to m ( T i ), then remove T i from the tree T , and apply Atda to the resulting trees one by one in a recursive manner. The detailed discussion to justify the working of Atda along with feasible solution of Eq.(21) is given in [18], and for the non-negative non-increasing list structure Atda has a (worst-case) linear time complexity.
