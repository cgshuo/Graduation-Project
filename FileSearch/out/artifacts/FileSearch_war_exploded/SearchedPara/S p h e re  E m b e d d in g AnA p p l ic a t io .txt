 The embedding of objects in a low -dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10 , 13 ] . Most m ethods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3 ], embed objects of heterogeneous types, based on their co -occurrence statistics. In this paper we demonstrate that the latter can be successfully appli ed to unsupervised part -of -speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1 , 4 , 5 , 6 , 7 ] .
 The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features ). The induction task is fully unsupervised, i.e. , it uses no annotations. This task has bee n addressed in the past using a variety of methods. Some approaches, such as [ 1 ] , combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [ 7 , 2 , 5 ] . Using a latent -descriptor clustering approach, [15] obtain t he best results to date for distributional -only unsupervised POS tagging of the widely -used WSJ corpus . Using a heterogeneous -data embedding approach for this task, we define separate embedding f unctions for the objects "left word" and "right word" based on their co -occurrence statistics, i.e. , based on bigram frequencies. We are interested in modeling the statistical interactions between left words and right words, as relevant to POS tagging, rat her than their joint distribution. Indeed, modeling the joint distribution directly results in models that do not handle rare words well. W e use the CODE (Co -Occurrence Data Embedding) model of [ 3 ] , where statistical interaction is modeled as the negative exponential of the Euclidean distance between the embedded points. This embedding model incorporates the marginal probabilities, or unigram frequencies, in a way that results in appropriate handling of both frequent and rare words.
 The size of the dataset (nu mber of points to embed) and th e e mbedding dimensionalit y are several -fold larger than in the applications studied in [ 3 ] , making th e optimization methods used by these authors impractical . Instead, we use a simple and intuitive stochastic -gradient pro cedure. Importantly, in order to handle both the large dataset and the relatively high dimensionality of the embedding needed for this application , we constrain the embed ding to lie on the unit sphere. We therefore refer to this method as Spherical CODE , or S -CODE . The spherical constraint causes the regularization term  X  the partition function  X  to be nearly constant and also makes the stochastic gradient ascent smoother ; t his allows a several -fold computational improvement , and yields excellent performance. After convergence of the embedding model, we use a k -means algorithm to cluster all the words of the corpus , based on their embeddings. The induced POS labels are evaluated using the standard setting for this task, yielding state -of -the -art t agging performance. 2 . 1 M o d e l W e represent a bigram, i.e. , an ordered pair of adjacent words in the corpus, as joint random variables ( X , Y ), each taking values in W , the set of word types occurring in the corpus. Since X and Y , the first and second words in a bigram, play different role s, we build a heterogeneous model, i.e. , use two embedding functions, and . Both map W i nto S , the unit sphere in the r -dimensional Euclidean space.
 We use for the word -type frequencies : is the number of word tokens of type x divided by the total number of tokens in the corpus. We refer to as the empirical marginal distribution , or unigram frequency . W e use for the empirical joint distribution of X word types for POS tagging, we want the embedding to be insensitive to the marginals: two word types with similar context distributions should be mapped to neighboring points in S even if their unigram frequencies are very different. We therefore use the marginal -marginal model of [ 3 ], defined by: T he log -likelihood ,  X  , of the corpus of bigrams i s the expected value , under the empirical bigram distribution , of the log of the model bigram probability : Th e model is parameterized by 2  X  | W | points on the unit sphere S in r dimensions : and . These points are initialized random ly, i.e. , independently and uniformly on S .
 To maximize the likelihood, we use a gradient -ascent approach. The gradient of the log likelihood is as follows (o bserve that the last term in (4) does not depend on the model, hence does not contribute to the gradient ): For sufficiently large problems such as POS tagging of a large corpus, computing the partition function, Z , after each gradient step or even once every fixed number of steps can be impractical. Instead, it turns out (see Discussion) that, thanks to the sphere constraint, we can approximate this dynamic variable, Z , using a constant, , which arises from a coarse approximation in which all pairs of embedd ed variables are distributed uniformly and independ ently on the sphere. Thus, we set with and i.i.d. uniformly on S , and get our estimate a s the expected value of the resulting random variable, : Numerical evaluation of (7) yields for the 25 -dimensional sphere . An even coarser approximation can be obtained by noting that , for large r , the random variable with r degrees of freedom , compressed by a factor of ). This yields the estimate For th e present application , we f i nd that performance does not suffer from using a constant rather than recomputing Z often during gradient -ascent. It is also fairly robust to the choice of . We observe only minor change s in performance for rang ing over [ 0.1 , 0.5 ] . W e use sampling to compute a stochastic approximation of the gradient. To implement the first sum in (5) and (6 )  X  represent ing an attraction force between the embedd ings of the words in a bigram  X  w e sample bigram s from the empirical joint . Given a sample emerge from these two sums are: where is the step size. In order to speed up the convergence process, we use a learning rate that decreases as word types are repeatedly observed. If is the number of times word type w has been previously encountered , we use: The model is very robust to the choice of the function (C), as long as it decreases smoothly. This modified learning rate also reduce s the variability of the tagging accuracy , while slightly increasing its mean.
 T he second sum in (5) and in (6)  X  represent ing a repulsion force  X  involve s not the empirical joint but the product of the empirical marginals. T hus, the complete updat e is : where is sampled from the joint , and x 2 and y 2 are sampled from the marginal updated vectors are projected back onto the sphere S .
 After convergence, for any word w , we have two embedded vectors, and . These vectors are concatenated to form a single geometric description of word type w . The collection of all these vectors is then clustered using a weighted k -means clustering algorithm : in e ach iteration , a cluster X  X  centroid is updated as the weighted mean of its currently assigned const ituent vectors, with the weight of the vector for word w equal to the PTB45 or the PTB17 tagset (see below, Section 2.2). 1 2 . 2 E v a l u a t i o n a n d d a t a The resulting assignment of cluster labels to word types is used to label the corpus. The standard practice for evaluating the performance of the induced labels is to either map them to the gold -standard tags, or to use an information -theoretic measure. W e use the three evaluation criteria that are most common in the recent literature . The first criteri on maps each cluster to the POS tag that it best matches according to the hand -annotated labels. The match is determined by finding the tag that is most frequently assigned to any token of any POS tag, this evaluation technique is called many -to -one mapping , or M TO . Once the map is constructed, the accuracy score is obtained as the fraction of all tokens whose inferred tag under the map matches the hand -annotated tag.
 from assigning multiple clusters to a single tag; hence it is called one -to -one mapping , or 1 -to -1. Most authors construct the 1 -to -1 mapping greedily, assigning maximal -score label -to -tag matches first ; some authors, e.g. [15], use the optimal map . Once the map is constructed, the accuracy is computed just as in MTO . The third criterion , variation of information , or VI , is a map -free information -theoretic metric [ 9 , 2] .
 We note that we and other authors found th e most reliable criterion for comparing unsupervised POS taggers to be MTO. However, we include all three criteria for completeness.
 We use the Wall Street Journal part of the Penn Treebank [8] (1,173,766 tokens). We ignore c apitalization, leaving 43, 766 word types , to compare performance with other models consistently . Evaluation i s done against the full tag set (PTB45), and against a coarse tag set (PTB17) [12] . For PTB45 evaluation, we use e ither 45 or 50 clusters, in order for our results to be comparable to all recent works . For PTB17 evaluation, we use 17 clusters, as do all other authors. Figure 1 shows the model performance when evaluated with several measures. MTO17 and MTO50 refer to the number of tokens tagged correctly under the many -to -1 mapping for the PTB45 and PTB17 tagsets respectively. The type -accuracy curves use the same mapping 
Source code is available at the author X  X  website: faculty.biu.ac.il/~marony . and tagsets, but record the fraction of word types whose inferred tag matches thei r "modal" annotated tag, i.e. , the annotated tag co -occurring most frequently with this word type. We also show the scaled log likelihood, to illustrate its convergence. T hese results we re produced us ing a constant , pre -computed , . Using this constant value allows the model to run in a matter of minutes rather than the hours or days required by HMMs and MRFs . Figure 2 shows the model performance for different dimensionalities r . As r increases, so does the performance. Unlike previous applications of CODE [3] (which often emphasize 
Figure 2: Comparison of models with different dimensionalities: r = 2, 5, 10 , 25. MTO17 is the Many -to -1 score based on 17 induced labels mapped to PTB17 tags. 
Figure 1: Scores against number of iterations (bigram updates). Scores are averaged over 10 sessions, and shown wi th 1 -std error bars. MTO17 is the Many -to -1 tagging accuracy score based on 17 induced labels mapped to 17 tags. MTO50 is the Many -to -1 score based on 50 induced labels mapped to 45 tags. Type Accuracy 17 (50) is the average accuracy per word type, where t he gold -standard tag of a word type is the modal annotated tag of that type (see text). All runs used = 0.154, r =25. visualization of data and thus require a low dimension), this unsupervised POS -tagging application benefits from high values of r . Larger values of r cause both the tagging accuracy to improve and the variability during convergence to decrease.
 Table 1 compare s our model, S -CODE, to previous state -of -the -art approaches. Under the Many -to -1 criterion, which we find to be the most appropriate of the three for the evaluation of unsupervised POS taggers, S -CODE is superior to HMM results, and scores comparabl y to [15] , the highest -performing model to date on this task .
 We find that the model is very robust to the choice of within the range 0.1 to 0.5. This robustness lends promise for the usefulness of this method for other applications in which the partition function is impractical to compute. This point is discussed further in the next section. The problem of embedding heterogeneous categorical data ( X , Y ) based on their co -occurrence statistics may be formulated as the task of finding a pair of maps and such that , for any pair ( x , y ) , the distance between the images of x and y reflects the statistical interaction between them. Such embeddings have been used mostly for the purpose of visualization and exploratory data analysis. Here we demonstrate that emb edding can be art performance . 4 . 1 S -C O D E v. C O D E The approach proposed here, S -CODE, is a variant of the CODE model of [3 ] . In the task at hand, the sets X and Y to be embedded are large (43K), making most conventional 
Model PTB17 PTB45
S -CODE ( Z =0.1456)
S -CODE ( Z =0.3)
LDC 75.1 Brown 67.8 70.5 50.1 51.3 3.47 3.45 HMM -EM 64 . 7 62 . 1 43 . 1 40 . 5 3.86 4.48 HMM -VB 63 . 7 60 . 5 51 . 4 46 . 1 3.44 4.28 HMM -GS 67 . 4 66 . 0 44 . 6 49 . 9 3.46 4.04 HMM -
Sparse(32)
VEM
Table 1: Comparison to other models, under three different evaluation measures. S -CODE uses r = 25 dimensions. It was run 10 times, each with 12  X  10 6 update steps . LDC is from [15]; Brown shows the best results from [14] and website mentioned therein ; HMM -EM, 
HMM -VB and HMM -GS show the best results from [2] ; HMM -Sparse(32) and VEM show the best results from [5] . The numbers in parentheses are standard deviations. For the VI criterion, lower values are better . PTB45 -45 maps 45 induced labels to 45 tags, while 
PTB45 -50 maps 50 induced labels to 45 t ags . embedding approaches , including CODE (as implemented in [ 3 ]) , impractical . As explaine d below, S -CODE overcomes the large -dataset challenge by constraining the maps to lie on the unit sphere. It uses stochastic gradient ascent to maximize the likelihood of the model. The gradient of the log -likelihood w.r.t. a given includes two components, each with a simple intuitive meaning. The first component embodies an attraction force, pulling toward in proportion to the empirical joint . The second component, the gradient of the regularization term , , embodies a repulsion force; it keeps the solution away from the trivial state where all x 's and y 's are mapped to the same point, and more generally attempts to keep Z small. The repulsion force pushes away from in proportion to the product of the empirical marginals and , and is scaled by . The computational complexity of Z , the partition function, is .
 In the application studied here, the use of the spherical constraint of S -CODE has two important consequences. First, it makes the computation of Z unnecessary. Indeed, when using the spherical constraint, we observed that Z , when actually computed and upd ated every 10 6 steps, does not deviate much from its initial value . For example, for r = 25, Z ris es smoothly from 0.1 45 to 0.182. Note that the absolute minimum of Z  X  obtained for a that maps all of W to a single point on S and a that maps all of W to the opposite point  X  is 1. We also observed that replacing Z , in the update algorithm, by any constant in the range [.1 .5 ] does not dramatically alter the behavior of the model . We nevertheless note that larger values of tend to yield a slightly higher performance of the POS tagger built from the model. Note that the only effect of changing in the stochastic gradient algorithm is to change the relative strength of the attraction and repulsion terms.
 We compared the performance of S -CODE with CODE. The original CODE implementation [3] could n o t support the size of our data set. To overcome this limitation, we used the stochastic -gradient meth od described above , but without projecting to the sphere. This required us to compute the partition function, which is highly computationally intensive . W e therefore computed the partition function only once every q update steps ( where one update step is the sampling of one bigram). We found that for q = 10 5 the partition function and likelihood changed smoothly enough and converged , and the embeddings yield ed tagging performances that did not differ significantly from those obtained with S -CODE . The second important consequence of imposing the spherical constraint is that it makes the stochastic gradient -ascent procedure markedly smoother. As a result, a relatively large step size can be used, achieving convergence and excellent tagging performance in about 10 minutes of computation time on a desktop machine. CODE requires a smaller step size as well as th e recomputation of the partition function , and, as a result, computation time in this application was 6 times longer than with S -CODE .
 When gauging the applicability of S -CODE to different large -scale embedding problems, one should try to gain some underst anding of why the spherical constraint stabilizes the partition function, and whether Z will stabilize around the same value for other problems. The answer to the first question appears to be that the regularization term is not so strong as to prevent clus ters from forming  X  this is demonstrated by the excellent performance of the model when used for POS tagging  X  yet it is strong enough to enforce a fairly uniform distribution of these clusters on the sphere  X  resulting in a fairly stable value of Z . One may reasonably conjecture that this behavior will generalize to other problems . To answer the second question, we note that the order of magnitude of Z is essentially set by the coarsest of the two estimates derived in Section 2, namely 0.135 , and that this estimate is problem -independent . As a result, S -CODE is, in principle, applicable to datasets of much larger size than the present problem. The computational complexity of the algorithm is O ( N r ), and the memory requirement is O (| W | r ) where N is the number of word tokens, and | W | is the number of word types. In contrast, and as mentioned above, CODE, even in our stochastic -gradient version , is considerably more computationally intensive ; it would clearly be completely impractical for much larger datasets. 4 . 2 C o m p a r i s o n t o o t h e r P O S i n d u c t i o n m o d e l s Even though embedding models have been studied extensively, they are not widely used for POS tagging (see however [18] ) . For the unsupervised POS tagging task , HMMs have until recently dominated the field. Here we show that an embedding model substantially outperforms HMMs , and achieve s the same level of performance as the best distributional -only model to date [15]. Models that use features, e.g. morphological, achieve higher tagging precision [11 , 14]. Incorporating features into S -CODE can easily be done, either directly or in a two -step approach as in [14] ; t his is left for future work.
 One of the widely -acknowledged challenges in applying HMMs to the unsupervised POS tagging problem is that these models do not afford a convenient vehicle to modeling an important sparseness property of natural languages, namely the fact that any given word type contrast, the approach presented here maps each word type to a single point in . H ence , it assigns a single tag to each word type, like a number of other recent approaches [15 , 16 , 17] . These approaches are incapable of disambiguating, i.e. , of assigning different tags to the same word depending on context, as in "I long to see a long movie." HMMs are, in principle, capable of doing so , but at the cost of over -parameterization. In view of the superior performance of S -CODE and of other type -level approaches , it appears that under -parameterization might be the better choice for this task.
 Another difference between our model and HMMs previously applied to this problem is that our model is symmetric, thereby modeling right and left context distribution s . In contrast, HMMs are asymmetric in that they typically model a left -to -right transition and would find a different solution if a righ t -to -left transition were modeled . We argue that using both distribution s in a symmetric way better captures the important linguistic information. In the past, left and right distribution s w ere extracted by factoring the bigram matrix and using the left an d right eigenvectors. Such a linear method does not handle rare words well. Instead, we choose to learn the ratio . This approach allows words with similar contexts but different unigram frequencies to be embedded near each other.
 Like HMMs, CODE provides a model of the distribution of the data at hand. S -CODE departs slightly from this framework . S ince it does not use the exact partition function in the stochastic gradient ascent procedure  X  and was actually found to per form best when replacing Z , in the update rule, by a constant that is substantially larger than the true value of Z  X  it only approximately converge s to a local maximum of a likelihood function. In future work, and a s a more radical deviation from the CODE model, one may then give up altogether modeling the distribution of X and Y , i nstead relying on a heuristically motivated objective function of sphere -constrained embeddings an d , to be maximized . P relimina ry studies using a number of alternative functional forms for the regularization term yielded promising results.
 Although S -CODE and LDC [15] achieve essentially the same level of performance on taggings that induce 17, 45, or 50 labels (Table 1), S -CODE proves superior for the induction of very fine -grained taggings. Thus, we compared the performances of S -CODE and LDC on the task of inducing 300 labels. Under the MTO criterion, LDC achieved 80.9% (PTB45 ) and 87.9% (PTB17). S -CODE significantly outperformed it, with 83.5% (PTB45) and 89.8% (PTB17).
 The appeal of S -CODE lies not only in its strong performance on the unsupervised POS tagging problem, but also in its simplicity, its robustness, and its math ematical grounding . The mathematics underlying CODE, as developed in [3], are intuitive and relatively simple . M odeling the joint probability of word type co -occurrence through distances between Euclidean embeddings , without relying on discrete categories or states , is a novel and promising approach for POS tagging . The spherical constraint introduced here permits the a pproximation of the partition function by a constant, which is the key to the efficiency of the algorithm for large datasets . T he s tochastic -gradient procedure produces two competing forces with intuitive meaning , familia r from the literature on learning in generative models. 
While the accuracy and computational efficiency of S -CODE is matched by the recent LDC algorithm [15], S -CODE is more robust , showing very little change in performance over a wide range of implementation choices. We expect that this improved robustness will allow n on -linguistic.
 R e f e r e n c e s
