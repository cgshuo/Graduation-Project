 Community Question Answering (CQA) service provides a platform for increasing number of users to ask and answer for their own needs but unanswered questions still exist within a fixed period. To address this, the paper aims to route questions to the right answerers who have a top rank in accordance of their previous answering performance. In or-der to rank the answerers, we propose a framework called Question Routing (QR) which consists of four phases: (1) performance profiling, (2) expertise estimation, (3) avail-ability estimation, and (4) answerer ranking. Applying the framework, we conduct experiments with Yahoo! Answers 1 dataset and the results demonstrate that on average each of 1,713 testing questions obtains at least one answer if it is routed to the top 20 ranked answerers.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  information filtering, selection process Algorithms, Experimentation community question answering, question routing
Community Question Answering (CQA) service is a spe-cial kind of Question Answering (QA) service which allows registered users to answer the questions asked by other peo-ple. CQA portals such as Yahoo! Answers and Baidu Zhi-dao 2 have attracted increasing number of users over the last http://answers.yahoo.com/ http://zhidao.baidu.com/ few years. According to Yahoo! Answers blog 3 , it has 200 million users worldwide and around 15 million visits daily.
Since CQA portals are so popular, one interesting and important question is whether this service can solve askers X  questions efficiently. In order to investigate the answer to this question, we randomly track 3,000 newly posted ques-tions in Yahoo! Answers and Baidu Zhidao respectively to observe these questions X  states after two days. We find that in Yahoo! Answers only 17.6% of questions receive satisfied answers within 48 hours. For those unresolved questions, nearly 1/5 of them receive no response. For Baidu Zhidao, 22.7% of questions are well resolved. However, 42.8% of unresolved questions receive no response at all. The obser-vations show that above two popular CAQ portals cannot solve users X  questions efficiently.

In order to address this problem, we propose the frame-work of Question Routing (QR) in CQA. This framework routes new posted questions to users who are most likely to give answers in a short period. The concept of QR contains two meanings: finding (1) the  X  X ight X  users who can provide high quality answers; (2) the users who receive the routed question must be able to give response quickly, i.e., they are available to answer this question in time.

Liu et al. X  X  work [2] which  X  X dentify the group of  X  X xperts X  who are likely to provide answers to given questions X  and Qu et al. X  X  question recommendation [4] are similar to our work. However, answer quality and user availability were not considered in these papers. Recently, Horowitz et al. [1] developed a social search engine called  X  X ardvark X  which  X  X outes the question to the person in the user X  X  extended so-cial network most likely to be able to answer that question, X  where the extended social network includes popular social network websites such as Facebook 4 and LinkedIn 5 .Com-pared with  X  X ardvark X , our work focus on the community in CQA services rather than the social network of the asker.
The paper is organized as follows. Section 2 details the framework of QR and several models within the framework. In Section 3, we describe the experimental setup and the analyses of results. Conclusions are given in Section 4.
The whole process of QR is illustrated in Fig. 1. For a question to be routed, we first extract all answerers in the portal and build their answering performance profiles. http://yanswersblog.com/ http://www.facebook.com/ http://www.linkedin.com/ This step is called answer performance profiling. Then we estimate each candidate X  X  expertise on the routed question based on his/her performance profile. Finally we rank all the answerers based on their expertise.

In the phase of performance profiling, we establish each answerer X  X  performance profile from his/her answering his-tory. For the user who has answered at least one question in CQA services, we use all questions he/she once answered and the corresponding answers providing by the user to build the his/her performance profile.
In this section we present three approaches to estimate each answerer X  X  expertise on q r . In the following, we use E ( u i ,q r )todenoteuser u i  X  X  expertise on the new question q , which ranges from 0 to 1. The higher value E ( u i ,q r the higher expertise the user u i has for the question q r
We adopt the query likelihood language (QLL) model as our first model. Formally, Let q u i denote all previously an-swered questions by user u i . For a new question q r , u expertise on q r is defined as how likely q r can be generated from q u i : With Jelinek-Mercer smoothing [8], where C is the collection of all questions and  X  is a weight-ing coefficient to adjust the weight of smoothing, tf (  X ,q means the term frequency of the term  X  in q u i and tf (  X ,C ) is the term frequency of the term  X  in C .Weset  X  =0 . 8in our experiments according to the empirical value [8].
The above model assumes that the user has high expertise on new question q r if he/she has answered many similar questions before. However, it does not consider the quality of previous answers. A user may answers a great number of questions which are similar to q r , but we cannot reach the conclusion that the user must be an expert for question q if most previous answers are of low qualities. In order to obtain a more accurate prediction, we utilize user X  X  answer quality in expertise estimation. Thus, where Q ( u i ,q r ) reflects user u i  X  X  answer quality for question q and  X  is a weighting coefficient.

We propose two models to estimate Q ( u i ,q r ) from pre-vious answers X  qualities of user u i .The Basic Model is straightforward: it assumes the user X  X  answer quality on new question q r is the weighted average answers qualities of sim-ilar questions he/she answered previously. It is defined as: where q j  X  u i denotes the questions u i has answered and sim ( q j ,q r ) means the cosine similarity between question q and q r .

We use vector space model [5] to represent each question and each term is weighted by its tf-idf value.

However, this model may suffers from data sparsity espe-cially when some users only answered one question. In order to better utilize the known information, we borrow the idea of similarity fusion in collaborative filtering [7] which lever-ages other similar users X  answer qualities on similar ques-tions to smooth the Basic Model .Inthe Smoothed Model , and The cosine similarities between two users is calculated based on the following features for each user: number of total points the user owns, number of answers the user has pro-vided, number of best answers the user has provided, number of questions the user has asked and number of stars the user received.

We use logistic regression to model the user X  X  answer qual-ity on previously answered questions. Given an answer a ( u whichispostedby u i for question q j ,weuse a ij to denote the feature vector of a ( u i ,q j ). Let the probability of a ( u being a good answer be P ( a ij ), then: while  X  is the coefficient vector of the regression model.
The following features for each answer are extracted in training:
We apply feature conversion on the non-monotonic fea-tures using Kernel Density Estimation (KDE) [3].
Assuming one user is available to provide answers for the routed questions when he log on Yahoo! Answers, we aim to estimate whether the user logs on in several days after the routed question is posted. We model this problem as a typ-ical trend analysis problem in time-series data mining and use an autoregressive model to make the forecasting. For-mally, let A ( u i ,t ) denote the probability that u i is available at time t to answer routed question, usually t represents one specific day. In practice, we set A ( u i ,t )=1when u i posted at least one answer on the day t ,otherwise A ( u i ,t )=0. The autoregressive model can be represented as follows:
A ( u i ,t )=  X  1 A ( u i ,t  X  1) + ... +  X  p A ( u i ,t  X  where the term  X  is the source of randomness and is called white noise. Given a group of training data { A ( u i ,t ) ,A ( u can estimate the value of  X  1 , X  2 , ...,  X  p . Then we can apply the above model to predict the value of A ( u i ,t )whengiven A ( u i ,t  X  1) , ..., A ( u i ,t  X  p ).

Thus, each answerer X  X  availability for a period of time T = { t 1 , ..., t s } is calculate as:
We treat user expertise on q r and user availability in a range of time T as independent and use their linear combi-nation as the final QR score for each answerer: Then, all answerers are ranked according to their QR scores.
We want to investigate the answers to those research ques-tions through experiments: 1. What X  X  the influence of answer quality to the perfor-2. Does the Smoothed Model give better answer qual-3. Is it useful to estimate users X  answer availabilities in
Our data set is a snapshot of resolved questions from April 6, 2010 to May 14, 2010 under the Computers &amp; Internet category of Yahoo! Answers. The stopwords in question sub-jects and answer content are removed. In our experiments, the questions posted after May 6 are treated as new ques-tions to be routed (Set A, testing data) and the left ones are treated as archive data (Set B). The ground truth for each question in Set A are the answerers who actually answer it. After splitting as stated above (we also remove the questions in Set A whose answerers all do not appear in Set B), Set A includes 1,713 questions, 5,403 answers and 2,891 answerers. Set B includes 17,182 questions, 48,663 answers and 16,298 answerers.

Recall that we use a logistic regression model to estimate each answerer X  X  expertise on the questions which the user has answered. We adopt the community and the askers X  choices to avoid manually labeling. Answers are labeled as  X  X ood X  and  X  X ad X  as follows. For each question in Set B, the answer is labeled as a  X  X ood X  answer only the following two conditions are met: (1) It is selected as the best answer; (2) It obtains more than 50% of rate-ups for all answers of the question. Meanwhile, one answer is labeled as a  X  X ad X  answer if it receives more than 50% of rate-downs for all answers of the question. As such, 2,153  X  X ood X  X nstances and 2,593  X  X ad X  X nstances served as training data to estimate the parameters of the logistic regression model.

We set T = 3 according to our observation (In Set B, the max duration time for a question to receive an answer is 2.16 days). Moreover, we set p =3whichmeansthefirst three days X  answering records are used to estimate the user X  X  availability in the fourth day.

We compare the QR performance of the following meth-ods. We adopt the Mean Reciprocal Rank (MRR) [6] as the eval-uation metric to evaluate the performance of above methods. Table 1 reports each method X  X  performance measured by MRR. Here we set  X  =0 . 6,  X  =0 . 8and  X  =0 . 9.
From Table 1 we observe that utilizing users X  answer qual-ities can improve the performance of QR significantly. The MRR values of Basic Q and Smoothed Q are 26.99% and 33.68% higher than that of QLL respectively. Similarly, the MRR values of Basic Q+AE and Smoothed Q +AE are 26.17% and 33.58% higher than that of QLL+AE .

In order to explore the impact of  X   X  X  value for QR per-formance, we fixed  X  =0 . 8 and tested different settings of  X  and the result is reported in Fig. 2 (a). First we observe that when  X  =0 . 6, both Basic Q and Smoothed Q get the highest MRR. Furthermore, when  X &gt; 0 . 3, the performance of them are always better than QLL. With this findings, we believe that users X  answer qualities on perviously answered questions indeed provide great help for us to finding experts on the routed question.
Smoothed Q outperforms Basic Q since the MRR of former one is about 5% higher than that of the latter. We think it is due to the help of utilizing similar users X  expertise on similar questions to smooth the user X  X  answer quality, es-pecially for the users who answered few questions. Figure 2 (b) give the performance of Smoothed Q with different settings of  X  (  X  =0 . 6), from which we find that the value of  X  affects the QR quality of this method to a great extent. When  X  = 0 which means we just rely on similar users X  X  expertise on similar questions to estimate the user X  X  exper-tise on routed question, the MRR is much lower than no smoothing (i.e.,  X  = 1). The best performance of MRR is gotten when  X  =0 . 8.
Figure 2 (c) present each method X  X  performance when con-sidering users X  answer availability. First, routing questions based on users X  answering availabilies only is very inaccurate: only about 1 out of 1,000 QRs will be successful when  X  =0. Second, QLL+AE , Basic Q+AE and Smoothed Q+AE perform best when  X  is setting around 0.9. When  X  =0 . 9, the MRR of these methods are 4.11%, 3.44% and 4.04% higher than corresponding methods without availability es-timation. Thus, the performance of QR can be improved by users X  availability estimation.
In this paper, we introduce the concept of Question Rout-ing in CQA services and propose a QR framework which considers both users X  expertise and users X  availabilities for providing answers in a range of time. We conduct exper-iments on Yahoo! Answers dataset and the results demon-strates that leveraging answer quality can greatly improve the performance of QR. In addition, utilizing similar users X  answer qualities on similar questions provides a more accu-rate expertise estimation and thus give better QR perfor-mance. Furthermore, users X  answer availability estimation can also boost the performance of QR. The best MRR value in our experiments is 0.0541, which means on average each tested question will get at least one answer if we route it to the top 20 ranked users. Considering totally there are 16,298 answerers for ranking, the result demonstrates that our QR framework has the ability to route new questions to those users who will provide answers in a short period of time. This work is supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK4128/08E and CUHK4154/09E).
