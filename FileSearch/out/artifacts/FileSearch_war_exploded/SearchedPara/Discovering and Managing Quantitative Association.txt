 Although association rule mining has been studied in the literature for quite a while and numerical attributes are prevalent, perhaps surprisingly, the state-of -the-art quantitative association rule mi n-ing is rather inefficient and ineffective in discovering all useful rules. In this paper, we propose a novel divide and conquer two-phase algorithm, which is guaranteed to find all good rules eff i-ciently . We further devise an optimization technique for perfo r-mance. Moreover, we discuss a few issues with managing and using the discovered quantitative association rules. We perform a comprehensive experiment al study which shows that our algo-rithm is one to two orders of magnitude faster than the state-of -the-art one . In addition, we discover significantly more rules that are useful for prediction. H.2.8 [ Database Management ]: Database Applications Association rules ; Numerical attributes ; Association rule mining. 
Mining association rules, as a way to discover statistical co n-nections among data items, has been studied in the literature for a long time [2, 3]. An association rule is of the form , where and are two sets of items, and the rule indicates that if o c-curs in a transaction, then is also very likely to appear in the same transaction as in a market basket analysis [13]. A simple example is { } { } , which indicates that if a cu s-tomer buys diapers, he or she tends to buy beer as well. An asso-ciation rule needs to satisfy two conditions. First, its support , informally defined as the fraction of tuples (or transactions) in the data where both and are true, has to be above a threshold. Second, its confidence , informally defined as the ratio between its support and the fraction of tuples in the data where is true, has to be above a threshold. Intuitively, the rule confidence is the empirical conditional probability that is true given that is true. Although the original association rule mining is only on categorical attributes, it is subsequently found to be very useful for numerical attributes also, due to their prevalence in practice. Here is a simple example. Example 1. In the widely used Iris dataset [8] , a snippet of which is shown in Table 1, there are four numerical attributes: SL (sepal length), SW (sepal width), PL (pedal length) and PW (pedal width), and only one categorical attribute. Association rules i n-volving numerical attributes will be of interest. Such a rule may consist solely of numerical attributes, or a mixture of numerical and categorical attributes. For instance, one rule can be [ ] [ ] [ ] , and another rule can be: [ ] . 
Hence, there have been some attempts to include numerical a t-tributes in association rules, which are called quantitative (or numerical) association rules . On the other hand, missing and uncertain data attributes are very common in many modern data man agement applications [1 ]. For example, some attributes may be hard or expensive to measure [14]. Such quantitative associ a-tion rules can be used to predict the unknown attributes based on the known attributes of the same tuple. Therefore, it is necessary that the quantitative association rules have a good coverage of the domain, so that suitable rules can be found for a particular predi c-tion task  X  in the event that multiple rules are found, they can be ranked by rule confidence [13], for example. 
Association rule mining was first introduced by Agrawal et al. [2, 3]. Since then, it has been extensively studied. We focus on the previous work that is related to numerical attributes. Most of the work we survey here is also discussed in [12] in detail. The first line of work uses a natural approach that discretizes (a.k.a. bins) each numerical attribute, which converts a numerical attribute into categorical and can then mines association rules. One problem with this approach is that it discretizes an attribute without con-sidering other attributes; thus it may not be able to obtain a rule with a good support and/or confidence. Furthermore, to make the rules suitable for prediction, one rule may cover a wide interval of an attribute while another rule may cover a narrow interval of the same attribute where and may have an overlap. This line of work cannot systematically discover such rules. 
Aumann et al. [4] use a statistical approac h that gives associ a-tion rules whose right hand side may contain some statistical i n-formation of a numerical attribute, e.g., the mean or variance. Webb [16] extends it to other statistical information, e.g., the mi n-imum and count. However, a major issue is that the rules are of restricted forms, e.g., only categorical attributes or a single discr e-tized numerical attribute may be on the left hand side. The final line of work for numerical attributes uses an optimization based approach. Fukuda et al. [9] study the tradeoff between support and confidence, and propose an optimization criterion based on a function of the two. The main limitation of their work is that a rule can contain at most two numerical attributes, since it uses a technique inspired from image segmentation . Rastogi et al. [11 ] and Brin et al. [6] have follow-up work to handle disjunctions, but the rules are still restricted to one or two numerical attributes. 
Mata et al. [10 ] use a genetic algorithm to optimize the support of itemsets defined on the intervals of numeric al attributes. How-ever, the confidence of a rule is not optimized. In addition, only numerical attributes are considered and their intervals are static. The state-of -the-art work on quantitative association rules is the one by Salleb-Aouissi et al. [12], called QuantMiner. Like [10], it also uses a genetic algorithm. But unlike [10], it discovers good intervals of numerical attributes in association rules by optimizing both the support and the confidence . For a rule template, it finds the best intervals of numerical attributes that optimize a heuristic  X  X itness X  function, which tends to favor high confidence, high support, and small intervals. 
Although QuantMiner [12] can dynamically determine good i n-tervals and handle both numerical and categorical attributes, it suffers from some serious drawbacks. The method is based on rule templates. For each given rule template, which specifies the attributes on the left and right hand sides of a rule (e.g., ), it only finds a  X  X est X  rule that optimizes a heuristic  X  X itness X  function. However, good and useful rules, especially for predi c-tion, do not have to have the maximum value of that heuristic fitness function. Therefore, QuantMiner may miss many useful rules. Consider the example rule template above. Suppose QuantMiner finds rule : [ ] [ ] many other rules, such as : [ ] [ ] even though they are suboptimal on the fitness function (which favors small intervals) and are discarded by QuantMiner. In fact, each interval on the left hand side of is wider than the corr e-sponding one in , which means may be more likely to be used for prediction. In addition,  X  X  right hand side is narrower than that of , implying that the prediction using gives more speci f-ic information about (less uncertainty), which is better. 
After formulating the problem ( Section 2 ), we propose an alg o-rithm, NAR-DISCOVERY, which proceeds in two phases using a divide-and-conquer approach ( Section 3 ). We further devise an optimization technique that improves the speed by orders of ma g-nitude ( Section 4 ). Additionally, we discuss issues with managing and using the discovered rules ( Sect ion 5 ). Finally, we perform an experimental evaluation using four widely used real-world d a-tasets ( Section 6 ). Our algorithm is one to two orders of magn i-tude faster than QuantMiner, and discovers significantly more rules useful for prediction. 
Consider a relational table that has numerical attributes { } and tuples. A numerical (or quantitative) association rule is of the form , where and are sets of range predicates [ ] . No two attributes in are the same. We call the left hand side (LHS) of the rule and the right hand side (RHS). Each attribute consists of (fine) buckets, which determines the satisfactory precision of the attri b-ute in association rules (based on the application); the range pre d-icate bounds ( and above) may only be the bucket boundary values. The support of a predicate set , , is: where the symbol denotes the number of elements of a set. That is, the support of is the fraction of tuples in that satisfy all the predicates in . The support of an association rule , , is , i.e., the fraction of tuples that satisfy all predicates in and . The confidence of the rule , , is . Definition 1 (Numerical Association Rule Discovery ). The nu-merical association rule discovery problem is to discover all n u-merical association rules : that satisfy the following co n-ditions: (1) , (2) , (3) , and (4) . 
Here, , , and are the minimum su p-port, minimum confidence, and maximum size (i.e., total number of predicates in ) parameters of . We require that the RHS ( ) has only one predicate, as a conjunction of multiple predicates on RHS can always be transformed to several rules whose RHS is of size 1. Conditions (3) and (4) together ensure that the total size of a rule is at most . can be selected based on application needs. It is straightforward to extend our algorithms to include both numerical and categorical attributes in a rule; thus we omit categorical attributes from the problem statement. 
First of all, as a preprocessing step, through a single parse of the table , we can easily obtain the numbers of tuples that are in each of the fine buckets of ; we denote this histogram distribution of as ( ). The main algorithm has two phases. In Phase 1, we get the coarse rules by partitioning each attribute into a small number of large buckets and mapping adjacent buckets to an  X  X tem X , so that we can use the classical frequent itemset mining algorithm. One key point is that any final fine-grained rule must be covered by a coarse rule, since we only check the threshold for frequent itemsets. In Phase 2, we recursively part i-tion only the outermost buckets of each (coarse) rule, and apply some bounding and filtering for termination. This is sufficient to  X  X oom in X  and find the fine rules.
 Algorithm NAR -DISCOVERY Input : : a relation 
Output : numerical association rules (NAR) in // Phase 1: Divide and conquer: get coarse rules 1: for each { } do // iterate through each attribute 2: partition into buckets (each has fine buckets) 3: map any number ( to ) of adjacent buckets to an item 4: map each tuple of to a transaction 5: find frequent itemsets with size in [ ] and no two 6: get rules of the form (where ) from the above // Phase 2: Recursively partition only outermost buckets 7: while is not empty do 8: pop a rule from 9: if then continue 10: if then continue 11: if then output 12: partition outermost buckets of into (smaller) buckets 13: for each rule derived from after the partition do 14: if then push onto 
Figure 1. (a) Illustration of intervals and cores in an association rule . (b) For , dashed curves indicate the interval and core before the recursive partition. .

Phase 1 is to first get some coarse association rules quickly. We evenly partition the range of each numerical attribute into a small number (e.g., ) of buckets, such that each bucket has about fine buckets (line 2). Then we use the classical  X  X a r-ket basket X  model to get frequent itemsets and association rules. To do that, we define what  X  items  X  and  X  transactions  X  are. Each possible range predicate using any number of adjacent buckets of attribute is an item (line 3). Thus, there are maps to a transaction (line 4 ). Naturally, a transaction contains an item associated with if the value in that tuple falls in the range of the predicate. Using any classical frequent itemset mi n-ing algorithms [13], we find itemsets with support at least no two items can be from the same attribute (line 5). Then in line 6, each frequent itemset of size produces association rules (by placing each item on the RHS resp.), which are pushed onto a stack . Note that this does not check the threshold. 
In the second phase, we pop out and process each rule in (lines 7-14). Lines 9 and 10 incorporate two termination cond i-tions which, if satisfied, stop us from examining rule and all its potential descendants. These will be discussed in detail in Sec. 3.2. Here denotes a variation of where the outermost buckets are removed from each predicate in ( is similar). We refer to the original predicate range as the interval , and the reduced predicate range with the outermost buckets removed as the core . Fig. 1(a) illustrates the intervals and cores of a rule another one on , shown as the  X  intervals  X , while is shown as the  X  core s X  in red, with the outermost buckets removed. Note that buckets are of varied sizes. This is because lines 12-14 part i-tion the outermost buckets only and push the derived rules onto . Line 11 outputs the current rule if it satisfies . 
Specifically, line 12 (like line 2) equally partitions an outermost bucket. Fig. 1(b) shows an example where we partition the two outermost buckets of into three smaller buckets (i.e., ). The dashed curves indicate the interval and core before the part i-tion, and the solid ones after. Line 13 of the algorithm iterates through all possible new intervals after the partition, each of which contains the original core, plus at least one (and at most ) smaller bucket in an original outermost bucket. In Fig. 1(b), the new interval (solid left curve) spans the original core (dashed right curve), one smaller bucket in the upper outermost (old) bucket, and two smaller buckets in the lower outermost (old) bucket. Each of the new rules produced this way is pushed onto and treated recursive ly, if it satisfies (line 14 ). Note that the recursive partition also finishes at line 13 when the partition reaches the size of fine buckets (which determines the precision). 
Finally, we remark that it is easy to extend NAR-DISCOVERY to also include categorical attributes. The only change is in Phase 1, where we map each value of a categorical attribute as an item as well. Thus, a rule will also include categorical attributes. We do not partition the values of categorical attributes in Phase 2. 
We study the termination conditions in NAR-DISCOVERY, and prove the correctness of the algorithm. First observe that a rule is popped from and not further partitioned if (1) line 9, or (2) line 10  X  X  condition is true, or (3) its outermost buckets are as small as fine buckets (hence no new rule is added to ). Let us look at (1) and (2), and begin with the following lemmas. Lemma 1. Consider any rule popped from stack in line 8 of NAR-DISCOVERY, and all its descendant rules through recu r-sive partitioning. Then for any rule , the interval (core, respectively) in any predicate in must be a subset (superset, respectively) of the corresponding one in . Proof. First consider the case where is an immediate descen d-ant rule of (i.e., in line 13 of the algorithm). Since an interval in is the result of partitioning the outermost buckets and selecting a subset of them, it follows that this interval in is a subset of the original one in . In addition, since a core is just an interval with outermost buckets removed, and since an outermost bucket in is no bigger than the corresponding one in , it must be true that a core in is a superset of the corresponding one in . Finally, due to the transitivity property of subsets and supersets, Lemma 1 holds for any rule .  X  Lemma 2. The expression in lines 9 and 10 is an upper bound of and , where is any descendant rule recu r-sively derived from in lines 7-14 throughout the algorithm. Proof. Rule and all its descendant rules form a set . Consider any rule of the form . By definition, . Firstly, because, from Lemma 1, the intervals of all predicates in are subsets of the corresponding ones in . Se c-on dly, , where the first inequality is because an interval is always a superset of its core, and the second inequality is again due to Lemma 1 (  X  X  core is a superset of  X  X  core). Combining the inequalities above, we have This immediately validates the termination condition in line 9. Corollary 1. The termination condition in line 9 of the algorithm is correct. That is, if , then rule and all its descendants based on recursive partitioning can be safely pruned. The termination condition in line 10 is based on an observation that we call rule domination . It can also be explained by the inte r-estingness of a rule as studied in previous work [13 ]. Definition 2 (Rule Domination). We say that a numerical ass o-ciation rule dominates another one if and only if (1) each attribute range on the LHS of is a subset of the corresponding one on the LHS of , (2) the attribute range on the RHS of is a subset of that on the RHS of , and (3) . 
The intuition of rule domination is that, informally, dom i-nates if is more applicable (i.e., can be used for inference in more situations), gives more specific information about the RHS, and has higher confidence, based on the three conditions in Def i-nition 2. Note that if an attribute is absent from the LHS of a rule, Figure 2 . Illustration of the execution of the NAR -DISCOVERY algorithm as multiple depth first search trees. .
 Figure 3. A pictorial illustration of the cores and intervals of a rule [ ] [ ] [ ] . it implies the whole range of the attribute. For example, if do m-inates , it is possible that an attribute is absent from the LHS of , but is present as on the LHS of . Lemma 3. If a numerical association rule dominates another numerical association rule , then . Proof. Let be and be . Then by definition, due to conditions (3) and (1) of Definition 2, respectively.  X  Lemma 3 indicates that even the support of the dominating rule is higher, further showing its superiority. Consider a trivial rule is possible to appear if and which holds when the range i n is wide enough. Lemma 4. If in NAR-DISCOVERY, then rule and all its descendant rules must be dominated by a trivial rule. Proof. Consider a trivial rule . Then and all its descendants. We show that dominates . First off , of Definition 2 is true. Moreover, each attribute range on  X  X  LHS is a subset of the corresponding one on  X  X  LHS, satisfying cond i-tribute range on  X  X  RHS ( ) is also the core of  X  X  RHS, which is a subset of the core of  X  X  RHS (Lemma 1). In turn, the core of  X  X  RHS is a subset of the interval of  X  s RHS. Thus,  X  X  RHS range is a subset of  X  X , and condition (2) is true also.  X 
When is barely , rule is still uninteresting; is more interesting when the former is significantly greater. Thus, in line 10, we have an adjustable parameter that controls how selective we are on this. We now prove the correctness of the whole algorithm. We say that rule minimally covers rule if and have the same set of attributes on both LHS and RHS, and for the same attribute, the range in is the minimal superset of that in , i.e., removing any one outermost bucket from (based on the current partition) would violate this superset requirement. Theorem 1. The algorithm NAR-DISCOVERY is correct: all true numerical association rules are discovered, and all output rules are numerical association rules satisfying Definition 1 . Proof. We first show that all output rules satisfy Definition 1. The only place that outputs a rule is at line 11. The rule is popped from a stack (line 8), while only lines 6 and 14 push a rule onto . In both places, the support of a rule pushed to must be at least (lines 5 and 14). Thus, condition (1) in Definition 1 is true. Moreover, condition (2) is true due to line 11, and cond i-tions (3) and (4) hold due to lines 5 and 6. 
For the other part of the proof, consider a true numerical a sso-ciation rule . We show that the algorithm must be able to find it. First of all, Phase 1 must be able to locate a rule that minimally covers . This is because the selection is based on rule support, and (since  X  X  ranges are sup ersets). In Phase 2, each recursive step also produces a rule that minimally covers . The outermost bucket size in the sequence exponentially decreases, until the sequence reaches a rule . From Corollary 1 and Lemma 4, the sequence must survive the pruning in lines 9-10 as a descendant is the true rule , which will eventually be output in line 11.  X 
We propose an optimization technique that improves the pe r-formance by one to two orders of magnitude. A bottleneck of NAR-DISCOVERY is in Phase 2 where it iteratively checks the support and confidence of derived rules after recursive part i-tioning. When the table is large , this is clearly very expensive. 
The NAR-DISCOVERY algorithm corresponds to a tree search where each node is a rule and its child nodes are the rules derived from it through recursive partitioning. This is illustrated in Fig. 2, where we show a forest. Each root node is a rule found in Phase 1 (line 6). In line 13, each rule corresponds to a child node in Fig. 2, while its parent node corresponds to the rule from which is derived. As such, NAR-DISCOVERY is a depth-first-search (DFS) of each tree in Fig. 2. Definition 3 (Core/Interval of a Rule). For a table and its numerical association rule , the core (interval, resp.) of the rule , denoted as ( , resp.), is the set of tuples in that fall in the core (interval, resp.) of each attribute in . Moreover, the core (interval, resp.) of w.r.t. its LHS , denoted as fall in the core (interval, resp.) of each attribute on the LHS of . Example 2. Consider an initial coarse rule in Table 1, [ ] [ ] [ ] , produced at Phase 1. The cores of each attribute are the ranges shown in the red ovals in Fig. 3 (similar to Fig. 1). The core of the rule , denoted as , however, refers to the set of tuples { } that fall in the core ranges of each attribute of (Definition 3). Similarly, 
We create a temporary table for each node, which is read by all its child nodes. Let node correspond to rule and a temporary table . is a projection and a selection of the original table. It only contains the attributes in . As illustrated by the sizes of the nodes in Fig. 2, we will show that the size of the temporary tables diminishes quickly as we traverse down the tree. The following is the extra information we maintain during the algorithm: (1) Associate a temporary table with each rule . Specifi cal-(2) Maintain a core count for each rule , which is the 
For a root node (rule ) in Fig. 2, we ma y consider its parent as the original table projected on  X  s attributes, with . We have the following result. Theorem 2. During the DFS of NAR-DISCOVERY, as we tra v-erse down the tree in Fig. 2, the core count must increase or stay the same, while the temporary table size must decrease or stay the same. More precisely, the temporary table at a node must be a subset of that at its parent node. Proof. By the definition of a temporary table in step (1),  X  (where  X  denotes the projection on  X  X  attributes ), while  X  X  core count from step (2). From Lemma 1, for any rule derived from rule , it holds that and . It follows that and .  X 
Based on Theorem 2, we have the following strategy: (1) During the DFS, when we first visit a node , we create its (2) and are subsequently used to derive its children X  X  (3) Finally, and are removed when the DFS finishes Example 3. Table 2 illustrates the temporary table created for the rule in Example 2 and Fig. 3. First, it is a projection of the ori g-inal table with only three attributes that are in the rule . Second, it consists of tuples that are outside , but inside and the core count . Any further partition of the boundary buckets of each attribute does not increase the size of the temporary table, and does not decrease the core count. 
Note that the temporary tables need not be created together. At any point in time, only the temporary tables on a root-to -leaf path co-exist, based on the steps (1) to (3) above. This is illustrated in Fig. 2, where only the temporary tables at the red solid nodes are current. Clearly, the depth of a tree is at most , since the recursion stops when the outermost buckets of all attri b-utes in a rule reach the size of a fine bucket (Sec. 3.1), if not earlier. The recursion may stop earlier due to the termination co n-ditions in lines 9, 10, and 14 of NAR-DISCOVERY. In this section, we discuss some issues related to managing and using the discovered quantitative association rules. 
When the relation is subject to frequent updates, it is also i m-portant to ensure that the association rules are up to date. First, for each rule , we record its support and confidence and . Whenever there is a tuple being inserted or deleted (the SQL UPDATE operation is a deletion plus an insertion ), we first find the set of rules that contain on its LHS . This is fine even if has unknown attributes. Then for each rule , we check if is consistent with (i.e., falls in the range of  X  X  RHS) or not. Based on this, we can easily update and . If 
Note that for a rule , is not affected, while slightly changes due to the increment or decrement (by 1) of , the number of tuples in (recall that is a fraction). However, the change is very small for large . We can simply update when significantly changes over time. Also note that our update procedure above may merely remove association rules. However, as data changes, new rules may arise, which can only be learned through a re-execution of NAR-DISCOVERY. 
Managing uncertain data is much needed in recent applications [1], where two types of uncertainty, tuple uncertainty and attribute uncertainty are commonly studied. Our approach can be easily generalized for uncertain data. For example, when a tuple has a probability (of existence in the table), consider an association rule for which satisfies the LHS predicates in and/or the RHS predicate in . Then the contribution of to the counts in the calculation of , or is a fraction (instead of an integer ). Similarly, if has an uncertain attribute, during NAR-DISCOVERY, we calculate the support using the probability that the uncertain attributes fall in the interval/core of a rule. 
Furthermore, we may incorporate such association rules as an integral part of an uncertain data management system. Suppose some attributes of a tuple are easy to measure, while other attri b-utes are unknown. We can first find the applicable rule(s), an d infer/learn the probability distributions of the unknown attributes. If the known attributes are uncertain, we can first obtain a prob a-bility of satisfying the LHS of a rule , which has a confidence . Treating as a conditional probability, we can infer that the RHS of (for an unknown attribute) is true with probability . We leave the in-depth study of such a probabilistic rule aware uncertain data management as an interesting future direction.
We use the following four real-world datasets, which are from the widely-used (in data mining and machine learning) UCI M a-chine Learning Repository [8]: (1) the Iris dataset, (2) the Auto MPG dataset, (3) the Pima Indians Diabetes dataset, and (4) the Abalone dataset. As the focus of our experimental study, we use the four datasets for comparison with QuantMiner [12], since QuantMiner can handle them in a reasonable amount of time. The rules that we discover and manage may contain both numerical and categorical attributes. 
Note that both QuantMiner and NAR-DISCOVERY can be ea s-ily parallelized (i.e., scaled out to multicores or multiple nodes) to improve their performance. In particular, the rules found in Phase 1 of NAR-DISCOVERY can be independently explored in para l-lel for Phase 2. In addition, sampling has been studied in associ a-tion rule mining (e.g., [15, 7]) for very large data. These are b e-yond the scope of this paper. For fair comparisons with QuantMiner [12], we also implement our algorithms in Java. All experiments are performed on a machine with an Intel Core i7 2.50 GHz processor and 8 GB memory. 
We first evaluate the effectiveness of the temporary table opt i-mization. Phase 1 of NAR-DISCOVERY reduces to calling any standard frequent itemset mining; we invoke the Apriori algorithm implemented by Borgelt [5]. Our temporary table optimization reduces the cost of repeatedly scanning the base table in Phase 2. Figure 4 shows the execution times (in seconds) of NAR-DISCOVERY with and without the optimization under the Auto MPG dataset, for various thresholds . In the sequel, unless otherwise specified, we use the following default values: attribute is 30. Without the temporary table optimization, the alg o-rithm reads the base table for processing needed information, while the rest of NAR-DISCOVERY stays the same. 
From Figure 4, we can see that the performance difference b e-tween the runs with and without the temporary table optimization is related to the number of rules that NAR-DISCOVERY has to  X  X oom in X  in Phase 2. When is smaller, this number is greater, which will significantly penalize the execution without temporary tables. The temporary table optimization improves the performance significantly, by one to two orders of magnitude. When we run the same experiment on the other larger datasets, the performance difference between the two versions is even more drastic; we omit the figures in the interest of space. 
Let us now focus on the comparisons with the previous best method QuantMiner [12]. We download their source code and add code to measure the execution time. We vary and co m-pare NAR-DISCOVERY with QuantMiner using the Pima Ind i-ans Diabetes dataset, as shown in Fig. 5. NAR-DISCOVERY is between one and two orders of magnitude faster than QuantMiner. An other interesting observation is that, as increases, NAR-DISCOVERY gets faster (similar to Fig. 4, as expected), but QuantMiner actually stays about the same or even becomes slightly slower. This is because , for each rule template, QuantMiner searches for one rule with the highest  X  X itness X  fun c-tion value, whatever the value may be. Since QuantMiner works with a small subset of the rules to begin with, increasing QuantMiner has to examine. Similarly, in Fig. 6, we show the result using the Abalone dataset. Our method is still one to two orders of magnitude faster. In this dataset, QuantMiner already takes rathe r a long time per execution  X  over 20 minutes. 
Furthermore, we examine how many more rules our method discovers than QuantMiner. Fig. 7 shows the result for four d a-tasets, where the bars indicate the number of rules discovered by our method divided by the number of rules output by QuantMiner. We first verify that all the rules found by QuantMiner are also found by NAR-DISCOVERY. NAR-DISCOVERY finds 10 to 60 times more rules. As these rules cover different ranges of the a t-tribute values, they can be useful at different times. Moreover, QuantMiner must first be given a rule template, and, for each template, it outputs the  X  X est X  rule that maximizes the heuristic fitness function value. However, this may miss many useful rules. By contrast, we freely discover all qualified rules. Note that, since the rules we discover are a superset of those of QuantMiner, we can also easily retrieve the ones that optimize a function if needed. 
In this paper, we tackle a challenging problem of mining quan-titative association rules. We propose an efficient divide-and-conquer two-phase algorithm. We also propose an optimization technique that drastically improves the speed, and discuss how to maintain the rules. Finally, we perform an experimental study with a focus of comparing with QuantMiner. Our algorithm is one to two orders of magnitude faster, and discovers significantly more rules which are useful for various prediction tasks. Acknowledgments. This work was supported in part by the NSF, under the grants IIS-1149417 and IIS-1239176. 
