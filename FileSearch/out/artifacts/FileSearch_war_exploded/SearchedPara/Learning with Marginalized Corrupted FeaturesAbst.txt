 Laurens van der Maaten lvdmaaten@gmail.com Minmin Chen mc15@cec.wustl.edu Stephen Tyree swtyree@wustl.edu Kilian Q. Weinberger kilian@wustl.edu Washington University, St. Louis, MO 63130, USA In the hypothetical scenario with infinite data drawn from the data distribution P , even a simple classi-fier such as nearest neighbor (Cover &amp; Hart, 1967) becomes close to optimal ( viz. its error is twice the Bayes error). In the more realistic scenario with a finite training set, some variations in the data distri-bution will not be captured and the learned classifier performs worse at test time than during training. In this paper, we propose an algorithm to train a classifier from infinite training data, by corrupting the existing finite training examples with a fixed noise distribution. So instead of approximating the exact statistics of P with finite data, we approximate a slightly modified data distribution P 0 with infinite data.
 Our augmented data distribution P 0 follows a simple stochastic rule: pick one of the finite training examples uniformly at random and transform it with some pre-defined corrupting distribution. Many corrupting dis-tributions are possible, but we focus on: i) Poisson cor-ruption and ii) blankout / dropout corruption (random deletion of features). The Poisson corruption model is of interest when the data comprises count vectors, e.g. , in document classification. It is particularly appealing as it introduces no additional hyper-parameters and, in our results, improves the test accuracy on almost all data sets and loss functions. Robustness to blankout corruption is of interest in settings with heavy-tailed feature distributions, and in the  X  X ightmare at test time X  scenario (Globerson &amp; Roweis, 2006) in which some of the features are blanked out during testing ( e.g. , due to sensors failing or because the feature com-putation exceeds a time budget).
 Previous work (Burges &amp; Sch  X olkopf, 1997) explicitly augments the training set with additional examples that are corrupted through similar transformations. Although the simplicity of such an approach is ap-pealing, it lacks elegance and the computational cost of processing the additional corrupted training exam-ples is prohibitive for most real-world problems. We show that it is efficient to train predictors on an infi-nite amount of corrupted copies of the training data by marginalizing out the corrupting distribution. In par-ticular, we focus on empirical risk minimization and derive analytical solutions for a large family of cor-rupting distributions and a variety of loss functions. In summary, we make the following contributions: i) we introduce learning with marginalized corrupted fea-tures (MCF), a framework that regularizes classifiers by marginalizing out feature corruptions; ii) we derive analytical solutions for quadratic, exponential, and lo-gistic loss functions for a range of corrupting distribu-tions; and iii) on several real-world data sets, we show that training with MCF may lead to better classifiers than training with common l 1 or l 2 -norm regularizers. Next to work that explicitly corrupts training data (Burges &amp; Sch  X olkopf, 1997), several prior studies consider implicit approaches to classifying objects that are subject to corruptions. Most of these studies mini-mize the loss under an adversarial worst-case scenario. In particular, Globerson &amp; Roweis (2006) and Dekel &amp; Shamir (2008) propose minimax formulations in which the maximum loss of an example over all D K possi-ble corrupted examples that blank out K features is minimized . Others (Bhattacharyya et al., 2004; Shiv-aswamy et al., 2006; Trafalis &amp; Gilbert, 2007; Xu et al., 2009) also use minimax approaches that minimize the loss under a worst-case scenario, but corrupt the data by adding a constant that is uniformly drawn from [  X  u,u ] to each of the features.
 Chechik et al. (2008) propose an algorithm that max-imizes the margin in the subspace of the observed fea-tures for each training instance to deal with randomly deleted features. Teo et al. (2008) generalize the worst-case scenario to obtain invariances to transformations such as image rotations or translations. Their frame-work incorporates several prior formulations on learn-ing with invariants as special cases; e.g. , Herbrich &amp; Graepel (2004), who generalized SVMs to be invariant under polynomial input transformations. Br  X uckner et al. (2012) study a worst-case scenario in which an adversary changes the data distribution to minimize a function that may be antagonistic to the loss. Prior work differs from MCF in that the corruption is not computed analytically in expectation. In partic-ular, existing approaches have two disadvantages: i) they are complex and computationally expensive and ii) they minimize the loss of a worst-case scenario that is unlikely to be encountered in practice. By contrast, MCF scales linearly in the number of training exam-ples and considers an average-case instead of a worst-case scenario. Moreover, MCF can readily be used with a variety of loss functions and corruption models. Bishop (1995) and Webb (1994) proposed approaches that can be viewed as approximations to MCF for ad-ditive noise with small variance. By contrast, MCF is exact and can be used with noise distributions with po-tentially very high variance as well. MCF was inspired by recent successes of denoising autoencoders (Glorot et al., 2011; Vincent et al., 2008). Since autoencoders are non-linear, the marginalization over the corrupting distribution cannot be performed analytically in such models. Linear denoising autoencoders (Chen et al., 2012) can be viewed as a special case of MCF that aim to minimize the expected value of the reconstruction error under blankout corruption. To derive the marginalized corrupted features (MCF) framework, we start by defining a corrupting distri-bution that specifies how training observations x are transformed into corrupted versions  X  x . We assume that the corrupting distribution factorizes over dimen-sions 1 and that each individual distribution is a mem-ber of the natural exponential family: where  X  d indicates (user-defined) parameters of the corrupting distribution on dimension d . We also as-sume that the corrupting distribution is unbiased , i.e. cause biases in the corrupting distribution may lead to undesired scale changes in the parameters  X  d . Most corrupting distributions of interest are unbiased or can easily be made so; examples for P E are the unbi-ased  X  X lankout X  noise (Vincent et al., 2008), Gaussian noise, Laplace noise, and Poisson noise.
 Explicit corruption. A simple approach to improv-ing the generalization of a classifier using a corrupt-ing distribution is to follow the spirit of Burges &amp; Sch  X olkopf (1997) by selecting each element of the train-ing set D = { ( x n ,y n ) } N n =1 and corrupting it M times, following (1). For each x n , this results in correspond-ing corrupted observations  X  x nm (with m = 1 ,...,M ). This leads to the construction of a new data set  X  D of size |  X  D| = MN . This extended data set can be used for training by minimizing: with  X  x nm  X  p (  X  x nm | x n ),  X  the set of model parame-ters, and L ( x ,y ;  X ) the loss function of the model. Implicit corruption. Although such an approach is effective, it lacks elegance and comes with high com-putational costs, as the minimization of L (  X  D ;  X ) scales linearly in the number of corrupted observations. It is, however, of interest to consider the limiting case in which M  X  X  X  . In this case, we can apply the weak law of large numbers and rewrite 1 M P M m =1 L (  X  x m ,y as its expectation (Duda et al., 2001,  X  2.10.2): Minimizing the expected value of the loss under the corruption model leads to a new approach for training predictors that we refer to as learning with marginal-ized corrupted features (MCF). 3.1. Specific loss functions The tractability of (2) depends on the choice of loss function and corrupting distribution P E . In this sec-tion, we show that for linear predictors that employ a quadratic or exponential loss function, the required expectations under p (  X  x | x ) in (2) can be computed an-alytically for all corrupting distributions in the natural exponential family. For linear predictors with logistic loss, we derive a practical upper bound on the expected loss under p (  X  x | x ), which serves as surrogate loss. Quadratic loss. Assuming 2 a label variable y  X  X  X  1 , +1 } , the expected value of the quadratic loss under corrupting distribution p (  X  x | x ) is given by: L ( D ; w ) = where V [ x ] is a diagonal D  X  D matrix storing the vari-ances of x , and all expectations are under p (  X  x Eq. (3) is convex irrespective of what corruption model is used; the optimal solution w  X  is given by: w To minimize the expected quadratic loss under the cor-ruption model, we only need to compute the variance of the corrupting distribution, which is practical for all exponential-family distributions. The mean is always x nd because our corrupting distributions are unbiased. Table 1 gives an overview of the variances of corrupt-ing distributions of interest. (In blankout corruption, we scale the value of  X  X reserved X  features by 1 1  X  q ensure that the corrupting distribution is unbiased.) An interesting setting of MCF with quadratic loss oc-curs when the corrupting distribution p (  X  x | x ) is an isotropic Gaussian distribution with mean x and vari-ance  X  2 I . For such a Gaussian corruption model, we obtain as special case (Chapelle et al., 2000): which is the standard l 2 -regularized quadratic loss with regularization parameter  X  2 N . Interestingly, us-ing MCF with Laplace noise also leads to ridge regres-sion (with regularization parameter 2  X  2 N ). Exponential loss. The expected value of the expo-nential loss under corruption model p (  X  x | x ) is:
Distribution E [exp(  X  y n w d  X  x nd )] p (  X  x
Blankout noise q d + (1  X  q d ) exp(  X  y n w d 1 1  X  q Gaussian noise exp(  X  y n w d x nd + 1 2  X  2 y 2 n w 2 d )
Laplace noise (1  X   X  2 y 2 n w 2 d )  X  1 exp(  X  y n w d x Poisson noise exp( x nd (exp(  X  y n w d )  X  1)) where we used the assumption that the corruption is independent across features. The above equation can be recognized as a product of moment-generating func-tions E [exp( t nd  X  x nd )] with t nd =  X  y n w d . The moment-generating function (MGF) can be computed for all corrupting distributions in the natural exponential family. An overview of these MGFs is given in Ta-ble 2. Because the expected exponential loss is a con-vex combination of convex functions, it is itself convex irrespective of what corruption model is used. The derivation above can readily be extended to multi-class exponential loss (Zhu et al., 2006) (with K classes) by replacing the weight vector w by a D  X  K weight matrix W , and by replacing the labels y by label vectors y = { 1 ,  X  1 K  X  1 } K with P K k =1 y k = 0. Logistic loss. In the case of the logistic loss, the solution to (2) cannot be computed in closed form. Instead, we derive an upper bound, which can be min-imized as a surrogate loss: Herein, we have made use of Jensen X  X  inequality to upper-bound E [log z ]. In the upper bound, we again recognize a product of MGFs, which can be computed in closed-form for corrupting distributions in the natu-ral exponential family (see Table 2). The upper bound on the expected logistic loss is convex whenever the moment-generating function is log-linear in w d , e.g. , for blankout corruption.
 Again, the above derivation can readily be extended to multi-class logistic loss (by redefining the labels y to be label vectors y  X  X  0 , 1 } K with P K k =1 y k = 1; and by defining the loss as the logarithm of the softmax-probability of making the correct prediction). Case study. As an illustrative example, we show the upper bound of the logistic loss (5), where inputs are corrupted with the Poisson distribution: L ( D ; w )  X  Remarkably, this regularized loss does not have any additional hyper-parameters. Further, the sum over all features can still be computed efficiently for sparse data by summing only over non-zero entries in x n . Computational complexity. The use of MCF does not impact the computational complexity of training: the complexity of the training algorithms remains lin-ear in N . The additional training time for minimiz-ing quadratic loss with MCF is negligible, because the computation time is dominated by the inversion of a D  X  D -matrix. For exponential and logistic loss, we empirically found that computing the gradient of the MCF loss was 2  X  and 10  X  slower than computing the gradient of the  X  X ormal X  loss, respectively. We perform experiments comparing MCF predictors with standard predictors on three tasks: i) docu-ment classification based on word-count features us-ing MCF with blankout and Poisson corruption; ii) image classification based on bag-of-visual-word fea-tures using MCF with blankout and Poisson corrup-tion; and iii) classification of objects in the  X  X ight-mare at test time X  scenario using MCF with blank-out corruption. The three experiments are described separately below. Code to reproduce the results of our experiments is available from http://homepage. tudelft.nl/19j49/mcf . 4.1. Document classification We first test MCF predictors with blankout and Poisson corruption on document classification tasks. Specifically, we focus on three data sets: the Dmoz data set, the Reuters data set, and the Amazon re-view benchmark set (Blitzer et al., 2007).
 Data sets. The Dmoz open directory ( http://www. dmoz.org ) contains a large collection of webpages ar-ranged into a tree hierarchy. The subset we use con-tains N = 8 , 980 webpages from the K = 16 cate-gories in the top level of the hierarchy. Each webpage is represented by a bag-of-words representation with D = 16 , 498 words. The Reuters data set is a collection of documents that appeared on the Reuters newswire in 1987. Documents with multiple labels were removed from the data, resulting in a set of N = 8 , 293 doc-uments from K = 65 categories. The bag-of-words representation contains D = 18 , 933 words for each document. The four Amazon data sets consist of ap-proximately N = 6 , 000 reviews of four types of prod-ucts: books, DVDs, electronics, and kitchen appli-ances. Each review is represented by a bag-of-words representation of the D = 20 , 000 most common words. On the Dmoz and Reuters data sets, the task is to classify the documents into one of the predefined cate-gories. On the Amazon data set, the task is to decide whether a review is positive or negative.
 Setup. On the Dmoz and Reuters data sets, we use a fixed training set of 75% of the data, and evaluate the performance of our predictors on a fixed test set of 25% of the data. On the Amazon data set, we follow the experimental setup of Blitzer et al. (2007) by using a fixed division of the data into approximately 2 , 000 training examples and about 4 , 000 test examples (the exact numbers vary slightly between tasks). All ex-periments are performed using linear classifiers that are trained with l 2 -regularization; the amount of l regularization is determined via cross-validation. We consider quadratic, exponential, and logistic loss func-tions (both with and without MCF). The minimiza-tion of the (expected) exponential and logistic losses is performed by running Mark Schmidt X  X  minFunc -implementation of L-BFGS until convergence or until a predefined maximum number of iterations is reached. All predictors included a bias term that is neither reg-ularized nor corrupted. In our experiments with MCF using blankout corruption, we use the same noise level for each feature, i.e. we assume that  X  d : q d = q . On all data sets, we first investigate the performance of MCF as a function of the corruption level q (but we still cross-validate over the l 2 -regularizer). In a second set of experiments, we cross-validate over the blankout corruption parameter q and study to what extent the performance (improvements) of MCF depend on the amount of available training data. (MCF with Pois-son corruption has no additional hyper-parameters, as a result of which it requires no extra cross-validations.) Results. Figure 1 shows the test error of our MCF predictors on all data sets as a function of the blank-out corruption level q . Herein, corruption level q = 0 corresponds to the baseline predictors, i.e. to pre-dictors that do not employ MCF. The results show: i) that MCF improves over standard predictors for both blankout corruption (for all corruption levels q ) and Poisson corruption on five out of six tasks; ii) that MCF with Poisson corruption leads to signifi-cant performance improvements over standard clas-sifiers whilst introducing no additional hyperparam-eters; and iii) that the best performance tends to be achieved by MCF with blankout corruption with high corruption levels, i.e. when q is in the order of 0 . 8. The best-performing MCF classifiers reduce the test errors by up to 22% on the Amazon data if q is properly set. In many of the experiments with MCF-trained losses (in particular, when blankout corruption is used), we also observe that the optimal level of l 2 -regularization is 0. This shows that MCF has a regularizing effect in itself, rendering additional regularization superfluous. Figure 2 presents the results of a second set of experi-ments on Dmoz and Reuters in which we study how the performance of MCF depends on the amount of train-ing data. For each training set size, we repeat the ex-periment five times with randomly sub-sampled train-ing sets; the figure reports the mean test errors and the corresponding standard deviations. The results show that classifiers trained with MCF (solid curves) signif-icantly outperform their counterparts without MCF (dashed curves). The performance improvement is consistent irrespective of the training set size, viz. up to 25% on the Dmoz data set.
 Explicit vs. implicit feature corruption. Fig-ure 3 shows the classification error on Amazon (books) when a classifier without MCF is trained on the data set with additional explicitly corrupted samples, as for-mulated in (3). Specifically, we use the blankout cor-ruption model with q set by cross-validation for each setting, and we train the classifiers with quadratic loss and l 2 -regularization. The graph shows a clear trend that the error decreases when the training set contains more corrupted versions of the original training data, i.e. with higher M in eq. (3). The graph illustrates that the best performance is obtained as M approaches infinity, which is equivalent to MCF with blankout cor-ruption (marker in the bottom right; q = 0 . 9). 4.2. Image classification We performed image-classification experiments on the CIFAR-10 data set (Krizhevsky, 2009), which is a sub-set of the 80 million tiny images (Torralba et al., 2008). The data set contains RGB images with 10 classes of size 32  X  32, and consists of a fixed training and test set of 50 , 000 and 10 , 000 images, respectively. Setup. We follow the experimental setup of Coates et al. (2011): we whiten the training images and ex-tract a set of 7  X  7 image patches on which we ap-ply k -means clustering (with k = 2048) to construct a codebook. Next, we slide a 7  X  7 pixel window over each image and identify the nearest prototype in the codebook for each window location. We construct an image descriptor 3 by subdividing the image into four equally sized quadrants and counting the number of times each prototype occurs in each quadrant. This leads to a descriptor of dimensionality D = 4  X  2048. We do not normalize the descriptors, because all im-ages have the same size. We train MCF predictors with blankout and Poisson corruption on the full set of training images, cross-validating over a range of l 2 -regularization parameters. The generalization error is evaluated on the test set.
 Results. The results are reported in Table 3. The baseline classifiers (without MCF) are comparable to the 68 . 8% accuracy reported by Coates et al. (2011) with exactly the same experimental setup (except for exponential loss). The results illustrate the potential of MCF classifiers to improve the prediction perfor-mance on bag-of-visual-words features, in particular, when using quadratic or logistic loss in combination with a Poisson corruption model. Although our focus in this section is to merely illustrate the potential of MCF on image classification tasks, it is worth noting that the best results in Table 3 match those of a highly non-linear mean-covariance RBMs trained on the same data (Ranzato &amp; Hinton, 2010), despite our use of very simple visual features and of linear classifiers. 4.3. Nightmare at test time To test the performance of our MCF predictors with blankout corruption under the  X  X ightmare at test No MCF 32.6% 39.7% 32.5% Poisson MCF 29.1% 39.5% 30.0% Blankout MCF 32.3% 37.9% 29.4% time X  scenario, we perform experiments on the MNIST handwritten digits data set. The MNIST data set con-tains N = 60 , 000 training and 10 , 000 test images of size D = 28  X  28 = 784 pixels with K = 10 classes. Setup. We train our predictors on the full training set, and evaluate their performance on versions of the test set in which a certain percentage of the pixels are randomly blanked out, i.e. set to zero. We compare the performance of our MCF-predictors (using blank-out corruption) with that of standard predictors that use l 1 or l 2 -regularized quadratic, exponential, logistic, and hinge loss. As before, we use cross-validation to determine the optimal value of the regularization pa-rameter. For MCF predictors, we also cross-validate over the blankout corruption level q (again, we use the same noise level for each feature, i.e.  X  d : q d = q ). In addition to the comparisons with standard predic-tors, we also compare the performance of MCF with that of FDROP (Globerson &amp; Roweis, 2006), which is a state-of-the-art algorithm for the  X  X ightmare at test time X  setting that minimizes the hinge loss under an adversarial worst-case scenario.
 The performances are reported as a function of the feature-deletion percentage in the test set, i.e. as a function of the probability with which a pixel in the test set is switched off. Following the experimental setting of Globerson &amp; Roweis (2006), we perform the cross-validation for each deletion percentage indepen-dently, i.e. we create a small validation set with the same feature-deletion level and use it to determine the best regularization parameters and blankout corrup-tion level q for that percentage of feature deletions. Results. Figure 4 shows the performance of our pre-dictors as a function of the percentage of deletions in the test images. The figure shows the performance for all three loss functions with MCF (solid lines) and without MCF (dashed lines). The performance of a standard predictor using hinge loss is shown as a red dashed line; the performance of FDROP is shown as a black dashed line. The results presented in Figure 4 clearly illustrate the ability of MCF with blankout corruption to produce predictors that are robust to the  X  X ightmare at test time X  scenario: MCF improves the performance substantially for all three loss func-tions considered. For instance, in the case in which 50% of the pixels in the test images is deleted, the performance improvements obtained using MCF for quadratic, exponential, and logistic loss are 40%, 47%, and 60%, respectively. Further, the results also indi-cate that MCF-losses may outperform FDROP: our MCF logistic loss outperforms FDROP X  X  worst-case hinge loss across the board 4 . This is particularly im-pressive as the standard hinge loss performs surpris-ingly better than standard logistic loss on this data set (the improvement of FDROP over the generic hinge-loss is relatively modest). This result suggests that it is better to consider an average-case than a worst-case scenario in the  X  X ightmare at test time X  setting. We presented an approach to learn classifiers by marginalizing corrupted features (MCF). Specifically, MCF trains predictors by introducing corruption on the training examples, which is marginalized out in the expectation of the loss function. We minimize the expected loss with respect to the model parameters. Our experimental results show that MCF predictors with blankout and Poisson corruption perform very well in the context of bag-of-words features. MCF with Poisson corruption is particularly interesting for such count features, as it improves classification per-formances without introducing any additional hyper-parameters. As a disclaimer, care must be taken when applying MCF with Poisson corruption on data sets with outliers. Poisson corruption may emphasize out-liers in the expected loss because the variance of a Poisson distribution is equal to its mean, and because our loss functions are not robust to outliers. A solu-tion to this problem may be to redefine the corruption distribution to p (  X  x d | x d ) = Pois (  X  x nd | min { x some cutoff parameter u  X  0.
 In most of our experiments with MCF, the l 2 regularizer parameter (which was set by cross-validation) ended up very close to zero. This implies that MCF in itself has a regularizing effect. At the same time, MCF with blankout corruption also ap-pears to prevent weight undertraining (Sutton et al., 2005): it encourages the weight on each feature to be non-zero, in case this particular feature survives the corruption. Our experiments also reveal that MCF with blankout corruption produces predictors that are more robust to the  X  X ightmare at test time X  scenario, making it useful in learning settings in which features in the test data may be missing. Learning with MCF is quite different from previous approaches for this setting (Dekel &amp; Shamir, 2008; Globerson &amp; Roweis, 2006): it does not learn under a worst-case scenario, but the (arguably) more common average-case sce-nario by considering all possible corrupted observa-tions. This has the advantage that it is computation-ally much cheaper and that it allows for incorporating prior knowledge. For instance, if the data is generated by a collection of unreliable sensors, knowledge on the sensor reliability may be used to set the q d -parameters. In future work, we intend to explore extensions of MCF to regression and structured prediction, as well as to investigate if MCF can be employed for kernel ma-chines. We also plan to explore in more detail what corruption models p (  X  x | x ) are useful for what types of data, and how these corruption models regularize clas-sifiers (Ng, 2004). Further, MCF could be used in the training of neural networks with a single layer of hid-den units: blankout noise on the hidden nodes can im-prove the performance of the networks (LeCun et al., 1990; Sietsma &amp; Dow, 1991; Hinton et al., 2012) and can be marginalized out analytically. A final interest-ing direction is to investigate the effect of marginaliz-ing corrupted labels (Lawrence &amp; Sch  X olkopf, 2001).
