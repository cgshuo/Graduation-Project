 Nearest-neighbor queries (NN), also known as similarity queries [19], are an important class of queries in data mining applications. The basic problem is: given a data table of records, find the nearest (most similar) records to a given query record. Specifically, given a data record represented as a d-dimensional vector of values q, and a data set D, determine a record x E D such that Dist(z,q) = (z -q)TS(x -q) is minimized. S is a positive semi-definite matrix. If the distance measure is the Euclidean distance or 2-norm, then S is the identity matrix. This definition can be generalized to retrieving the k nearest records. 
The many applications of NN queries include: predic-tive modeling, product catalog navigation (find similar products based on a long list of specifications), fraud de-tection, customer support, problem diagnosis, and man-agement of knowledge bases. NN queries also provide a more flexible means for querying databases by support-ing a  X  X ind similar X  capability useful in text or image databases, e.g., finding similar documents based on long large database of images based on a query image. Even if comparisons are not done on a per-pixel basis, typi-cally feature extraction over an image or document will result in many features and a dimensionality in the tens NN queries are also useful for flexible querying as well as data cleaning applications (e.g. matching records for merge-purge, or retrieving partial matches). 
Defining efficient methods for indexing very large databases to support NN queries remains an unsolved problem despite a large literature on the topic. The obvious approach to finding the NN is to scan the data and compute the distance between every record in the database and the query record -an unacceptable solution in massive databases. Indexing schemes developed in the database literature focused primarily 14, 181. There is increasing interests in avoiding this  X  X urse of dimensionality X  by performing approximate NN queries [13]. In data mining applications, the 
NN distance metric is a mathematical approximation of a user-defined similarity criterion which is often vague. Thus, improving performance by providing good 
Figure 1: Overview of derivation of index structure approximate NN estimates is an appropriate tradeoff. 
In this work we introduce a probability density based indexing (DBIN) method for calculating probabilistic or p-approximate NN queries (p-NN). A point s is a p-NN of q if with probability greater than 1 -p, s is a NN of q. Note that p-NN differs from the prior c-approximate NN (e-NN) [13]. A point s is the E-NN of q if for all z E D, dist(s,q) 5 (1 + 
E)diSt(z, q). We assume the data table is modeled as a probability density function (pdf) consisting of a mixture of components (e.g. Gaussians). The offline component of DBIN estimates this pdf and uses it to construct a clustered index of the data. At query time this pdf is used to prioritize and terminate the search for the NN. 
In the offline phase described in Section 3 and illustrated in Figure 1, the estimated pdf is calculated using scalable clustering algorithms [8, 71. Each cluster corresponds to a Gaussian. Using the pdf, we produce a global ordering on the data records that maximizes proximity of records that are  X  X ear X  each other according to a family of distance measures. The resulting data clusters are used to decide the ordering of the records in the table. Essentially the records need to be sorted by their cluster ID and the table materialized. 
This table will be scanned via a clustered index for the purposes of finding NN. 
Section 4 describes the query process. The query point is mapped to relevant values of the index column. 
The estimated pdf is used to prioritize search by producing an ordering by which the data pages should be scanned. As data is scanned, the statistical model is used to calculate the probability that the current 
NN or Ic-NN estimate is correct. When this estimated probability attains some threshold, the scan is stopped. 
It is important to note that at query time we require one additional query lookup stage to determine the cluster to be scanned next. The lookup consists of using the model to determine which cluster to scan next for the given query item. The clustered index is then used to scan the members of that cluster. In a file system implementation we simply write all members of a cluster in a separate file. This provides a simple means of scanning the file containing all the data items that belong to the cluster under consideration. ters exist in real data and that there is a strong corre-spondence between cluster membership and proximity according to the distance measure. In this paper we develop a theory and empirical evidence to justify this intuition. Much of the existing database literature has analyzed the NN problem under assumptions that data is identical and independently distributed (iid) in each dimension. Under these assumptions Beyer et al. [6] showed that NN queries in high dimensions are asymp-totically meaningless-the ratio of the nearest-neighbor and the farthest-neighbor distance approaches one as the dimensionality increases. So, for example, if data is drawn from a single Gaussian in high dimensions, then all the points are roughly the same distance apart. 
Herein we extend their results to show that for clus-tered data generated from a mixture model of well-separated Gaussians, NN is meaningful. In addition, efficient p-NN indexing methods exist based on clus-tering the data and modeling the contents statistically. 
We derive stability conditions under which DBIN (our indexing method) will perform optimally. Our com-putational results in Section 6, show that when these stability conditions are violated the performance of the method degrades slowly and is still more efficient than a full scan. The statistical model we utilize also allows us to detect when our indexing structure is unlikely to be useful. Hence an optimizer may use this informa-tion to decide the tradeoff between a sequential scan and using the index structure. The result is an efficient technique for indexing databases for multi-dimensional p-NN queries for a general family of distance functions that includes the most commonly used ones. 
Much of the existing work on L-NN queries use tree-based indexing. Some strategy is used to group the data by proximity. Each group is characterized by some bounding object and the bounding objects are organized in a tree structure to support efficient querying. For example Vornoi-based indexing and X-trees use bounding boxes [5, 41, SS-Trees use bounding hyperspheres [20]. Consequently, points or regions of the data space can be eliminated as possible candidate 
NN. Most methods work well in low dimensions but exhibit poor behavior in high-dimensions [13]. A common strategy is to map the data into a lower-dimensional space and perform the NN search in that space [lo]. Some hope has been expressed for avoiding the  X  X urse of dimensionality X  by using approximate NN queries [13]. Locality-Sensitive Hashing [12] provides a general scheme for mapping data into a lower-dimensional space and finding NN in the reduced space that are approximate c-NN in the original space. DBIN also finds approximate NN but in a probabilistic sense. 
Why are approximations needed? Bounding objects do not effectively constrain search in high-dimensional spaces. For exact NN queries, if the estimated query ball intersects the bounded regions, that region must be investigated. Unfortunately, as the dimensionality grows, the intersected area grows dramatically. Con-sider this simple example. Suppose a cluster of data is in a d-dimensional sphere of radius r inscribed in a minimum bounding box. The ratio of the volume of the inscribed sphere to the volume of the minimum bound-ing box converges rapidly to 0 as d goes to infinity X . 
When d is large, most of the box volume is in the cor-ners (i.e. outside the ball) which contain no data. A 
NN query will intersect most data pages, causing them to be scanned needlessly. Increase in overlap with di-mensionality is a manifestation of this phenomena [5]. 
In contrast our probabilistic approximate NN identifies data partitions most likely to contain relevant data and avoids regions unlikely to contain relevant points. 
Step 1 (Estimating Density): The first step in the offline preprocessing phase is estimating the density of the data. By density we mean a joint probability density function f governing the distribution of data values. We chose to model the data using a mixture of Gaussians. Hence the pdf has the form: f(z) = x2, piG(z]pi, Ci) where G(z]pi, Xi) is a Gaussian pdf parameterized with a mean vector pi and a covariance matrix C 2. There are several desirable properties of the 
Gaussian mixture model for this problem including: 1. Expressive power: any distribution can be repre-sented as a mixture of Gaussians [17]. 2. Computational efficiency: using our existing scal-able approaches to estimate the density in this form [8]. 3. Suitability to distance measure. We can exploit this form to help construct index structures for a large family of distance functions. 4. Ability to analyze the model, study its properties, and convenience of its use to compute probabilities of regions and events of interest. 
There are many ways to find a mixture-of-Gaussians pdf that fits data. We use data clustering (a.k.a. segmentation) algorithms as an efficient means of obtaining the statistical model [8]. In clustering, the input is a number of clusters, an initial guess at where the clusters are centered, and what their initial parameters (means and covariances) are. The initial guess is typically some random setting unless one has prior knowledge of the data. We use a scalable EM algorithm [8] that was developed to cluster large databases using a mixture-of-Gauss&amp;s pdf model. The actual algorithm is discussed in detail in [8] and can build models over large databases in a single scan. In this paper we assume that the clustering model is given as an input. We focus on its use to derive a multidimensional index. Step 2 (Determine Clusters): The model consist-ing of a mixture of K Gaussians determined by scalable EM [8] is used to precompute the index structure based on cluster membership. Membership of a data point be the cluster with highest probability: where G(zJpi, Ci) is the Gaussian representing cluster Ci. The optimal Bayes decision rule for a mixture-of-Gaussians pdf [9, Chapter 21 is as follows: a data point z is assigned to cluster Cl if i = I maximizes the quantity si = -;(s-Wmz-pi)-;loglci(+logpi. (1) This decision rule maximizes the probability that z is generated by the .th 2 Gaussian distribution (cluster Ci) and in that sense it is an optimal decision. This high-dimensional Euclidean space where Ri contains the set of points assigned to cluster Ci. We define R(s) to be the region-identification function; that is, each data point z belongs to region R(z) where R(z) is one of the K regions RI,. . . , RK. An in-memory directory structure consisting of the means, variances and weights of the K Gaussians is required to determine R(z). Step 3 (Materialize Sorted Data Table): The next step is to materialize the table on disk sorted by cluster membership. If this new table is to be used within a database, then we assume a clustered index will be created on the new column. Alternatively, data from each cluster can be written in a separate file if the scheme is to be used with a file system (no DBMS). The primary distinguishing characteristic of DBIN is its use of a model of the density to reorganize the data. Most other indexing schemes, due to efficiency considerations, define regions by examining one dimension at a time. 
Projections to a single dimension can cause confusion when the data is high-dimensional. In contrast, clustering works on all dimensions simultaneously. 
The second component of DBIN is activated at query time. It finds with high probability a NN of a query point q, denoted by n(q). The probability model is used to prioritize the order of clusters to be scanned and to terminate search once the NN has been estimated with high probability. For its description we use the notation 
B(q,r) to denote the query ball, a sphere centered on q and having radius r. We also denote by E the set of regions scanned so far, and by e the knowledge learned by scanning the regions in E. A NN of q is then found as follows. NearestNeighbor (q ; RI,. . . , RK, f) in Rj and its distance T from q; 
While P(B(q,r) is Empty ] e) &lt; tolerance 
The quantity P(B(q,r) is Empty ] e) is the proba-bility that B(q, r) is empty, given the evidence e col-lected so far. The evidence consists simply of the list of points included in the regions scanned so far. Before we show how to compute this quantity and how to compute 
Pr(B(q, T-) fl Ri is Empty ] e), we explain the algorithm using the simple example depicted in Figure 2. where Cj is the cluster assigned to zi using the optimal Bayes decision rule. The same reasoning as above and the fact P(z~ E B(q, r) n Rjlxi E Rj) = P(z~ E erated three regions RI, RS and RB whose boundaries Rjs are shown, A given query point q is found to reside in region RI. The algorithm scans RI, a current NN estimate is found, a current minimum distance r is determined, and a query ball B(q,r) is formed. The query ball is shown in the figure. If our algorithm was deterministic it would be forced to scan Rs and 
R3 since they intersect the query ball. Instead the algorithm determines the probability that the ball is empty given the fact that region RI has been scanned. 
Suppose this probability does not exceed the toler-ance. The algorithm must now choose between scan-ning Rz and scanning Rs. A choice should be made according to the region that maximizes the probabil-ity to find a NN once that region is scanned, namely, the algorithm should scan the region that minimizes 
P(B(q,r) n Ri is Empty j e). This quantity is hard to compute and so the algorithm approximates this quan-tity (using Equation 4 which we develop below). In this example, region Rz is selected to be scanned. The al-gorithm now halts because P(B(q,r) is Empty I e) is sufficiently large once RI and Rz have been scanned. 
We compute P(B(q,r) is Empty I e) based on the assumption that the points are independent and identically distributed (iid) samples from the estimated mixture-of-Gaussians pdf f. In other words, we take f to be the true model that generated the database. The sensitivity of the algorithm to this assumption must be tested using real data sets. By the iid assumption, the probability the ball is empty is the product of the probabilities that each point does not fall in the ball. Consequently, we have where R(zi) is the region of zi and n is the number of of data points. If R(Q) has been scanned then PC% E B(q,r) I zi E R(zi)) is not computable. Fortunately, Theorem 4.1 below shows that we can use the approximation The remaining task of computing has been dealt with in the statistical literature in a more general setting. This probability can be calculated numerically using a variety of approaches [15]. In particular, numerical approaches have been devised to calculate probabilities of the form P[(x -q)TD(x -q) 5 r2] where x is a data point assumed to be generated by a multivariate Gaussian distribution G(s]p, C) and D is a positive semi-definite matrix. When Euclidean distance is used to measure distances between data points, as we do herein, D is simply the identity matrix. Since numerical methods apply to distance functions of the form d(z, q) = (x -q)TD(x -q) where D is a positive semi-definite matrix, DBIN can be readily applied to any distance metric of this form. 
The pdf of the random variable (x -q)TD(x -q) is a x2 distribution when D = I and q = h. It is a noncentralized x2 when D = I and q # CL. It is a sum of non-centralized x2 pdfs in the general case of a positive semi-definite quadratic form. The general method uses a linear transformation to calculate the cumulative distribution of a linear combination of chi-squares. We use the method of Sheil and O X  X urcheartaigh [15]. 
Note that the needed probability is computable for the general case, but we illustrate the idea of the computation assuming that C is diagonal and that the 
Euclidean distance metric is used (D is the identity matrix). We start with where qj is the j-th component of q, and transform zj to a standard normal random variable zj. where Sj = ( X  X kPj) and pj is the j-th component of p. 
NOW (Z -Sj)2 has a noncentral chi-square distribution with 1 degree of freedom and noncentrality parameter 6;. We denote this as x2 (1, Sj X ) . The final result is 
P[(x -q)*D(x -q) 5 r2] = P 
This cumulative distribution function (cdf) can be expanded into an infinite series. In our implementation we used, AS 204 [ll], the terms of the series are calculated until an acceptable bound on the truncation error is achieved. Other techniques for calculating or estimating quadratic normal forms have been proposed [15] and may also be suitable. 
The next theorem shows that what we compute with these known techniques is not very far from what is desired. Let B be the event  X  X (q,r) is Empty, X  Cl the event  X  X  is generated by Ci , X  and Ri be the event  X  X  E Ri X . 
Theorem 4.1 Let the events B, Rx, and Ci be as defined above. Then, IP(BIR1) -P(BICl)I is bounded by p(B X  X , X  X l). 
Proof. Using the definition of conditional probability and the triangle inequality, we have 
The bound gets arbitrarily tight as the sample size increases because the radius of NN ball converges to zero as the sample size increases. Also the smaller the probability the ball intersects the region the better the bound. The optimal Bayes decision rule minimizes the differences between P(Ci) and P(Ri) which means that this bound for a typical pair Ri, Ci is on average as tight as possible when the optimal Bayes decision rule is used to cluster Under what conditions should DBIN perform well? 
Assuming a mixture of Gaussians, we define stability conditions that ensure that NN queries are meaningful and that the NN of a point is in the same cluster as the point itself. If the NN is always in the same cluster, then DBIN need only scan one cluster. Note that these results are asymptotic in the sense that they hold as the attribute dimensionality increases to infinity. 
Our results extend those of Beyer et al [6]. They proved the disturbing result that NN neighbor queries are meaningless in high dimensions under commonly used assumptions pertaining to data distributions. The concern raised in their work is that the ratio of nearest and farthest neighbor distance converges in probability to 1 as the dimensionality increases. We show that under reasonable assumptions this negative result is not applicable. We first present the notation and results in 
Definition 5.1 (Notation) d is dimensionality of a Euclidean space. n is.number of samples taken. 
F1,i, F2,i,. . ., i = 1,. . . , n are sequences of data distributions. &amp;1,&amp;z,... is a sequence of query distributions. points per d such that xd,i is generated from Fd,i. qd is a query point generated from Qd. 
We use the squared Euclidean distance ]]&amp;j,i -Qd]12, however, our results generalize to other distance mea-sures as well. DMINd = mini{llXd,i -qd112 1 1 5 i 5 n}. 
Theorem 5.1 (Meaningless NN Queries [S]) ,%X&amp;i and Qd be random variables with pdf Fd,i and Qd, respec-tively. If then for every E &gt; 0, limd+, P[DMA&amp; 5 (1 + c)DMINd] = 1. 
In a mixture of Gaussian distributions, each data point or query is generated by a single Gaussian distribution. We can think of a random set of points generated by such a mixture model as being clustered by the Gaussian distribution that generated them. 
Theorem 5.1 applies in particular to data generated by a single Gaussian distribution and so it shows that the distance between arbitrary two points in the same cluster approach the mean within cluster distance as the dimensionality d increases. We say that the within cluster distance is unstable because roughly every point in a cluster is the same distance apart. Specifically, 
Similarly, the distance between any two points from two distinct clusters approaches the mean between cluster distance. 
Corollary 5.2 ( Between Cluster Dist. Converge) Lc zd,i, xd,j, and qd be random variables with pdf Fd,i, Fdj, and Qd, respectively. If and then .w ---So A. dominates the within cluster distance, we say the clusters are stable with respect to each other. In 
Figure 3, Clusters 1 and 2 are stable and Clusters 2 and 3 are not stable. 
Definition 5.2 (Pairwise Stability) Let x&amp;i, xd,j, and qd be random variables with pdf F&amp;i, Fd,j, pairwise stable with parameter A. Theorem 5.2 (Stable Cluster Distances) If 
Clusters i and j are pairwise stable with parameter A, then for any E &gt; 0, know vd -+P 1 and wd +P A. Thus m = 
Corollary 5.1 ( Within Cluster Dist. Converge) If lim&amp;.+ 0, 
If every cluster is stable with respect to at least one other cluster then NN is well defined in the sense that the nearest and farthest neighbor distances are bounded apart. With probability 1 as d grows, the ratio of the farthest and NN is bigger than some constant greater than 1. For example in Figure 3, Cluster 1 is stable with respect to both Clusters 2 and 3 so NN is well defined. Theorem 5.3 (NN Well-Defined) If Cluster i and 
Cluster j are pairwise stable with parameter A, then for any 6 &gt; 0, limd+, P[DMA&amp; 2 (A -E)DMINd] = 1 
Proof. By conditions of the theorem &amp;$f&amp;$&amp; +p 1 and -+pA. 
By definition of minimum and m:ximum that for any c &gt; 0, lim,,, P[/~~;~I~$ 2 A -E] = 1. 
From the last two statements, 
If every cluster is stable with respect to every other cluster then if a point belongs to one cluster, its NN also belongs to that cluster. Therefore if we partition our data by cluster membership then with probability 1, as d grows, our index will only need to visit one cluster to find the NN. With probability one, other clusters can be skipped with no false drops of points. ter i is pairwise stable with every Cluster j, j = 1 . . 7 n, j # i, with parameter Aij &gt; A &gt; 1 respec-tzkely, then for any point zd,j from cluster j # i, and any E &gt; 0, Proof. By Theorem 5.2 for every zd,j j # i and every 
E &gt; 0, Since Ai,j &gt; A, the result follows. 0 
These results show that if we have a stable mixture of Gaussians where the between cluster distance domi-nates the within cluster distance, then if we partition by a cluster membership function that assigns all data gen-erated by the same Gaussian to the same partition, the index would work perfectly for NN queries generated by the same distribution. The higher the dimensionality, the better it would work. There is no  X  X urse of dimen-sionality X  in this case. For example in [2], it was shown that clusters generated by a mixture of spherical Gaus-sian with identical covariance matrices C such that the Mahalanobis distance between the cluster centers grows at least as fast as &amp; then the clusters are stable. 
Note that we actually do not know which Gaussian generates a data point. But, as discussed in Section 4, we can use the optimal Bayes decision rule [9] to estimate the Gaussian that generated a point with minimum error. Figure 2 illustrates stable clusters and the corresponding partitioning of the data space into regions. Our initial experimental goals were to confirm the validity of the theoretical results, to determine the accuracy of the probability estimates, and to establish that DBIN would be practical on real-world data with unknown distribution. We assumed that if a cluster is visited that the entire cluster is scanned. We did not address paging of data within a cluster. We experimented with both synthetic and real-world databases. The purpose of synthetic databases is to study the behavior of DBIN in well-understood situations. The experiments on the real data sets verify that our assumptions are not too restrictive and apply in natural situations. First we verified the stability theory introduced in Section 5 by applying DBIN to both stable and unstable mixtures of Gaussian data. We used synthetic data sets drawn from a mixture of ten Gaussians. First, we used stable clusters with a known generating model, and finally, stable clusters with an unknown generating model, i.e. a situation in which we had to estimate the density (and do the clustering). Stable Case, Known Probability Density: We generated a mixture of ten Gaussians in d dimensions with distance rd between the means of the distribution. Each Gaussian had covariance matrix u2 I where I is the identity matrix and (T 2 = .Ol. As discussed in Section 5 fix the distance between the means to be rd apart, we set the ith mean pi = mei where ei is a vector of zeros with a one in the ith component. The size of the database was fixed and we generated 500000/d points for d &lt; 100 dimensions and 1000000/d for d 2 100. DBIN was used to find the 2 NN for 250 query points randomly selected from the database. To remove any variation due to inaccuracies in clustering, we first used the true generating density as input to DBIN. Hence, this represents an extreme for a best-case scenario. Figure 4: Fraction scanned: unstable case rd = 21s. 
The stable case, with rd = ad was evaluated on 10, 20, 30, 40, 50, 60, 70, 100, 200, and 500 dimensions. 
In every case the 2 NN were found by examining only one cluster. In addition DBIN correctly determined that no additional clusters needed to be searched. 
Since each cluster contained ten percent of the data, each query required a scan of ten percent of the data. Similar experiments for 10 NN produced the same conclusion. In the best of worlds when the clusters are stable and the model is known, DBIN works perfectly. As the theory in Section 5 predicted, the curse of dimensionality vanishes when the distributions are stable. 
Unstable Case, Known Probability Density For unstable data, we fixed 7d = 2g. With this distribution, any problem with dimensionality over 4 is unstable. 
The amount of overlap of the Gaussians is growing exponentially with d. This is a worst-case scenario as there is no separation between the clusters and the asymptotic stability theory does not apply. 
To evaluate how well DBIN estimated when to stop scanning, we compared it with the Ideal approach that scans the clusters in the prioritized order predicted by DBIN and stops scanning additional clusters once the 
NN is found. DBIN stops scanning when the NN is found with high confidence. Since this case is very unstable, we would expect any algorithm to require a scan of much of the database. The percentages of data scanned by a full scan, DBIN, and the Ideal approach are given in Figure 4. Table 1 provides the percentage accuracy of DBIN on this unstable data. 
An estimate is considered correct if the estimated k-nearest neighbor distance does not exceed the actual k-NN distance. The percentage of the data scanned increased gradually with dimensionality. The Ideal algorithm scanned less data. This difference between the Ideal algorithm and DBIN indicates that the DBIN probability estimate is conservative. In many cases DBIN slightly overestimates the probability of a NN residing in a cluster so there may be an opportunity for tightening the probability estimate used. 
Table 1: Percentage accuracy of DBIN on unstable data Random Stable Clusters, Unknown Probability 
Density, Varying K: In the next set of experiments we applied the full DBIN approach to generated data. 
We utilized some data and corresponding clusters generated originally for [8]. The data were generated from 10 Gaussians with means independently and identically distributed in each dimension from a uniform distribution on [-5,5]. Each diagonal element of covariance matrices was generated from a uniform distribution on [0.7,1.5]. Hence this data is well-separated and should be stable, but is not at an extreme of stability. We used clusters generated by the 
EM algorithm without using knowledge of the known distribution except the number of clusters. Results on problems with 10 to 100 dimensions, with K (number of nearest neighbors to find) varying from 2 to 50, are given in Figure 5. Note that DBIN made no errors at all in finding the nearest neighbors for all values of K. Hence there is no point in plotting those results. 
This is no surprise since in high dimensions the distance between the uniformly generated means approaches the expected value which is proportional to &amp;. The 20-dimensional result is an artifact of the fact that our EM clustering algorithm happened to find a non-optimal solution. hence more than 1 cluster is scanned on average. Note that because we limited ourselves to 10 clusters, 0.1 is the minimum possible scan fraction here. hence we are near the optimal. Larger number of clusters will result in less data scanning. 6.2 Real Database Results 
We experimented on real-world datasets. Overall the results are promising, but we found some additional Accuracy 99.0% 100% 100% 97.1% 
Table 2: Accuracy and Fraction Scanned Results for 11-dimensional Census Data Accuracy ( 94.7% 1 90.0% 1 85.0% 1 78.6% Disc. Accuracy 1 97.3% 1 96.4% 1 95.6% 1 95.2% 
Table 3: Accuracy and Fraction Scanned Results for 29-dimensional Astronomy Data issues to resolve. In the current implementation of 
DBIN, we did not address categorical data fields. Some numerical stability problems occur with sparse datasets that result in zero variance clusters. The algorithm still performed as we expected. We need to extend the theory to the case where data is not truly continuous. 
Since our primary goal in this paper is to introduce the basic idea, and demonstrate that it is possible to index higher dimensional data, the results presented here are not optimized (i.e. we did not build the best possible clustering model). Nor did we optimize the algorithm to gracefully deal with small variances or discrete data masquerading as continuous values. The goal is to demonstrate that in principle we can derive useful indexing information from statistical structure. 
All results are averaged over 1000 query points drawn randomly from the data. U.S. Census Data: publicly available U.S. Census 
Bureau data set consisting of 300K records, and we selected the 11 dimensions that appeared to be numeric. These were fields such as ages, incomes, taxes, etc. 
Table 2 shows the results for varying K. The model we constructed had 100 clusters, so in principle the minimum possible scan fraction is around 0.01. 
Astronomy Data: This data set consists of 650K records of measurements on sky objects. For each object there are 29 measurements and we clustered the data into 10 clusters. Again, this was by no means an optimized clustering, and a larger number of clusters would imply a lower possible minimum fraction of data to scan. Results are shown in Table 3. Here we introduce the notion of Discounted Accuracy. 
Our accuracy measure counts a full error if for K=50, we found 49 out of 50 true nearest neighbors. Discounted accuracy gives an error of &amp; in this case and gives more insight into what is happening. Investor Data Sets: The last data set we evaluated is derived from a financial database consisting on historical stock prices, market valuations and so forth. In all, we used 34 dimensions. The data consisted of 834K records. We tried two experiments, one with 20 clusters and one with 47 clusters. Again these clusters were not optimized. The results for K=2 gave an error of 0.01 with 45% of the data scanned in case of 20 clusters. With 47 clusters the accuracy was 100% with 42% of the data scanned. As we increased K, performance deteriorated quickly. This, we believe is due to problems in fitting a model to data. We need to optimize the model. Also, numerical instability and the problem of zero variances showed up in spades in this data set. However, the results at least show than in principle the structure is useful (i.e. we did not have to scan 100% of the data). 7 Conclusions and Future Work In the discussion so far, we have not addressed the issue of determining the number of clusters required. From the perspective of indexing, the more clusters we have, the less data we are likely to have to scan. However, recall that determining which cluster to scan next requires a lookup into a table of clusters. If there are too many clusters, this lookup becomes too expensive. Consider the extreme case where each point in the database is its own cluster. In this case, each cluster identifies the result directly but the lookup into the cluster table is as expensive as scanning the entire database. 
Generally, we use a small number of clusters (5 to 100). The cost of computing probabilities from the model in this case is fairly negligible. Note that with 5 clusters, assuming well-separated clusters, we can expect an 80% savings in scan cost. So not many clusters are needed. There are also clustering algorithms that choose K as part of the clustering session. In this case, we let the algorithm choose the most appropriate K to fit the data, so long as K does not get too large. The trade-off between cluster lookup cost and data scan cost can be optimized on a per application basis. 
We presented DBIN, a scheme that exploits structure in data to derive a multi-dimensional index for NN queries. We also developed a probabilistic theory to support the method. We analyzed the NN query problem under the assumption that data is modeled by a mixture of Gaussians. We defined the notion of cluster stability which gives us the means to assess if a data set is amenable to our method. We derived two key results. The first is that nearest neighbor queries can be meaningful in high dimensions. The second is that it is possible to exploit the statistical structure of the data to construct a multi-dimensional indexing scheme. 
We derived a criterion for estimating the likelihood of payoff of scanning further, enabling a stopping criterion. 
The result is a new indexing algorithm based on density estimation. It provides a confidence level in the answer found so far. Because it is based on modeling the data content, DBIN provides sufficient information regarding the suitability of its indexing scheme to a given database. This can enable an optimizer to decide whether to invoke DBIN X  X  indexing structure or opt for the sequential scan when appropriate. 
We showed empirically that the proposed algorithm works when the data is stable. We used synthetic data to verify this and to study behavior when data is dra-matically unstable. We also demonstrated for several real-world databases that real data is not uniformly dis-tributed or concentrated in a single cluster. Our method showed significant improvement over a sequential scan. 
DBIN has many properties that are desirable from a data mining perspective. It is an anytime algotithm: it can provide the  X  X est answer so far X  whenever queried. 
The user can then stop the scan if the answer is satisfactory. It also provides a confidence level on the answer found so far. It can provide accurate estimates of how much work is left, allowing the user application to perform a cost-benefit analysis. It can utilize much of the existing SQL backend assuming clustered indices are supported. It leverages new work on scalable clustering techniques. Finally, we have found empirically that a full data scan is almost always avoided. 
The results indicate that visiting data in order of nearest clusters first is sound. In fact, we have found that in many of the instances when our stopping criterion does not cause us to stop after the first cluster, the actual NN turned out to be in the first scanned cluster after all. This suggests that it is possible to derive improved bounds for stopping earlier. 
Additional steps are needed to make-DBIN into a practical system. A full comparison with existing indexing methods is needed to assess the overall advantages of DBIN and whether performance increases are due to improved clustering or approximate NN processing. One challenge associated with DBIN is that it must sometimes calculate probabilities that are very close to zero without underflowing. In these cases, we run into numerical stability issues. Numbers get small due to high dimensionality and to large numbers of records. While high-dimensionality is handled by the theory we develop, in practice we are left with computing vanishingly small probabilities. A more robust scheme for computing these is desirable. This paper validates the basic framework and theory rather than provide complete details for implementation. We would like to generalize the theory to models other than 
Gaussians. Also, an evaluation of an implementation tied to a SQL backend that utilizes native indexing structures would be useful to demonstrate practical feasibility. This work was done while Kristin Bennett and Dan Geiger were visiting researchers at Microsoft Research. [31 141 151 [15] A. Mathai and S. Provost. Quadratic Forms in Random 
