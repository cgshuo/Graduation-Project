
Faculty of Organizational Sciences, University of Belgrade, Belgrade, Serbia Information Science and Technology Center, Temple University, Philadelphia, PA, USA 1. Introduction
Many decision tree algorithms have been developed, but there is no evidence that any algorithm outperforms all others in every situation. Strong support for this claim is given by No Free Lunch (NFL) theories [27] where authors prove that there is no classi fi cation algorithm that outperforms others on every dataset, but one can always fi nd an algorithm that is optimal for a dataset. Therefore, it can be important to broaden the space of available algorithms. Component-based algorithms, derived by combining components from known algorithms or partial algorithm improvements, support this goal.
One problem in using machine learning algorithms is that most users are limited to several available algorithms either incorporated in popularsoftware or frequently used in the research community. Another problem is that algorithms are typically designed in a black-box manner with limited adaptability to datasets through a set of parameters. According to [23] the black-box approach slows down development of data mining algorithms, because new algorithms are developed incrementally and become more complex, therefore reimplementation takes a lot of time. This fact caused a large time gap between the development of algorithms and their application in practice.
The need for standardized algorithm components that can be interchanged between algorithms is reported [23]. The same article also supports the development of open source frameworks that will serve the machine learning and data mining community for fast algorithm development and fair performance comparison between algorithms and their parts. It is also emphasized that such an approach would speed up the development and application of new data mining algorithms because it would enable:  X  Combining advantages of various algorithms,  X  Reproducing scienti fi c results,  X  Comparing algorithms in more details,  X  Building on existing resources with less re-implementation,  X  Faster adaption in other disciplines and industry,  X  Collaborative emergence of standards.

The question of whether the combination of components could improve algorithm performance was asked previously [23]. A positive answer to this question is suggested in our study. Our research supports this problem in three ways: 1. We proposed a framework for storing reusable decision tree algorithm components as well as a 2. We implemented the components, the GDT algorithm structure, and also a testing framework 3. We provided statistical evidence that component-based algorithms can outperform, on speci fi c 2. Related work
A lot of work is being done in developing platforms for machine learning and on software engineering based on reusable components. A brief review of prior work related to our study is contained in this section. 2.1. Machine learning software tools and algorithms Among the most famous open-source machine learning platforms are Weka [26], R [20] and Rapid Miner[16]. These platform s supportvarious data mining tasksandhavecapab ilities in datapreprocessing, model generating, model evaluation, and model exporting. The machine learning algorithms are usually implemented as black boxes, although some effort can be noticed in generalizing algorithms. For example, the authors of Rapid Miner implemented a decision tree which can use different split evaluation criteria (ratio gain from C4.5 [19], information gain from ID3 [18], the Gini impurity measure from CART [4] etc.).

In [29] authors propose a DMTL (data mining template library) which consists of generic containers and algorithms for frequent pattern mining. They show that  X  X he use of generic algorithms is competitive with special purpose algorithms X .

Some comparison of decision tree design can be found in [17,21]. Key topics important for decision tree construction are discussed in these papers, although no effort is being made towards identifying generic structures and reusable components. Ther e are also many hybrid algorith ms in the literature that combine various machine learning algorithms [13,30]. A popular hybrid approach consists of combining two or more black-box algorithms into one. Frameworks for combining components are, however, rarely found in the literature. One su ch framework is proposed in [8].

In [11] a framework for fast decision tree construction of large datasets is proposed. The authors analyzed well-known algorithms and improved their performance, but the main goal of their proposed generic decision tree was to improve the scalability of these algorithms on large datasets. 2.2. Reusable components in software engineering
There is no precise way to identify reusable com ponents. Still, reusable design is not new, as it is widely used in software engineering. In software engineering reusable components are de fi ned as triplets consisting of concept, content and context [24]. The concept is the description of what a component does; it describes the interface and the semantics represented by pre and post conditions. The content describes how the component is realized, which is encapsulated and hidden from the end user. The context explains the application domain of the component, which helps to fi nd the right component for a speci fi c problem.

Reusable components, as described at [24] allo w, not just the decomposition of an algorithm into smaller units, but also better description of what the component does and when it should be used. This is done better than in classical algorithms that are implemented as a whole, i.e. black boxes, where it is more dif fi cult to describe when the algorithm should be u sed. Additionally, the user of a white-box algorithm has the ability to adapt and modify an algorithm to speci fi c data or constraints. Here, we believe, further research can be done towards automatic data-driven composition of reusable components into algorithms.

The theory of reusable design, more popularly known as the pattern theory, is grounded in architec-ture [1], software engineering [10], organizational design [5] and many other areas. The main purpose of these researches is sharing of good ideas and easier maintenance of built systems.

An open list of features that every reusable component should have is proposed in [25], but this list can be used merely as a guideline, and not as a formal tool for components identi fi cation. Because there are still no unique agreements of what a reusable c omponent is, we can not imply that the components we identi fi ed are the last word in decision tree design. 3. Generic decision trees based on reusable components
In our study, the main principle of component identi fi cation was to discriminate algorithm design structure from speci fi c algorithm solutions. The design structure was saved in a generic algorithm shell while the speci fi c solutions were identi fi ed as reusable components.

Reusable components (RCs) were identi fi ed in well-known algorithms as well as in partial algorithm improvements. We analyzed several decision tree induction algorithms, namely ID3 [18], C4.5 [19], CART [4], and CHAID [12]. The choice of these algorithms was guided by how much they are available within popular software, together with a survey that points out more popular algorithms [28]. Further, we analyzed partial algorithm improvements in [14] and [15]. We identi fi ed reusable components classi fi ed according to frequently occurring sub-problems in decision tree design. RCs are solutions for sub-problems within the process of inducing the decision tree. For each of these sub-problems, we isolated several solutions as RCs from the original algorithms. The sub-problems play an important role in the decision tree design because for each sub-problem there are many possible solutions, i.e. RCs, and the designer of the algorithms can choose which RC should be used for solving a speci fi c sub-problem.
Table 1 shows basic RCs for tree growth from four algorithms analyzed in this paper which have the same generic structure, but differ in speci fi c solutions for certain sub-problems. The components for a sub-problem, as RCs have the same input and output speci fi cations.

From Table 1 we can notice that algorithms can be easily upgraded to handle sub-problems they couldn X  X  originally. For example, the CHAID algorithm cannot handle numerical (continuous) data, but in a component-based design it can easily adapt RCs for creating possible numerical splits from algorithms that include this feature.

Bene fi tfromthe component-based approachincludes identifying RCs in algorithms and providing these for use in other algorithms. For example, CHAID includes a RC that groups attribute categories and splits tree nodes on grouped categories. In decision tree growth, instead of trying to branch a categorical attribute on all categories (as in ID3 or C4.5), or make binary groupings (as in CART), CHAID tries to estimate the optimal grouping of attribute categories using the chi-square test for estimating category differences. For an attribute with 5 values, CHAID can decide to group the values into 2, 3, 4 or 5 separate value groups, based on the difference in class distribution.

This behavior can be used independently within any decision tree induction algorithm, as a RC. It could solve the problem of an overly detailed number of categories of an attribute that are not informative for a decision. Nevertheless, we are not aware of any algorithm, beside CHAID, that uses this reasoning to calculate near-optimal category grouping. This approach apparently has been forgotten by the machine learning community.

RCs are combined following a GDT structure presented in Fig. 1. This structure, we believe, suits the analyzed algorithms soundly. It is important to notice that the units in Fig. 1 should not be regarded as algorithmic steps, but as sub-problems that de fi ne an algorithm structure.

In Table 2 we show RCs identi fi ed in algorithms and partial algorithm improvements we X  X e analyzed and grouped according sub-problems they belong to. The last column in Table 2 shows whether RCs are currently implemented in the open-source platform we propose, and that will be explained later. The sub-problem  X  X emove insigni fi cant attributes X  (RIA) was inspired by attribute selection in [14]. Although in their paper only one attrib ute is selected fo r splitting, we modi fi ed this idea to opt out attributes that should not be candidates for tree nodes, thus improving the speed of an algorithm. The attributes in every tree node separately.

For  X  X reate split (numerical) X  (CSN) we identi fi ed one RC, namely  X  X inary X , that is used to divide numerical attribute values into two distinct subsets as proposed at [9]. This RC can be found in C4.5 and CART.

For  X  X reate split (categorical) X  (CSC) three RCs were identi fi ed.  X  X inary X  is used in CART for generating all possible binary splits, and  X  X ultiway X  in ID3 and C4.5 for generating splits on as many branches as there are categories in the attribute.  X  X igni fi cant X  was proposed in CHAID and it is used to fi nd near-optimal groupings of categories.
For  X  X valuate split X  (ES) we identi fi ed fi ve RCs.  X  X hi square X  was used in CHAID,  X  X nformation gain X  in ID3,  X  X ain ratio X  in C4.5, and  X  X ini X  in CART. We also identi fi ed the component  X  X istance measure X  in [15] which represents a partial algorithm improvement.
 For  X  X top criteria X  we used typical RCs identi fi ed in most decision tree algorithms.
The  X  X rune tree X  (PT) components were found in CART and C4.5 algorithm.  X  X educed error pruning X ,  X  X essimistic error pruning X  and  X  X rror-based pruning X  prune algorithms are employed in C4.5 while  X  X ost-complexity pruning X  is used in CART.

The proposed GDT structure allows the reproduction of analyzed algorithms, although it is not the main goal of the generic algorithms design. A structure which doesn X  X  allow replication of the original algorithms, but allows idea sharing could still be quite useful. Figure 2 illustrates how C4.5 can be reproduced using RCs.

Inputs and outputs for every sub-problem are de fi ned at Table 3. These de fi nitions allow generic tree their inputs, outputs, and parameters.

The GDT algorithm is shown in Fig. 3. The proposed GDT can be extended with more RCs or with additional sub-problems that could fi t the GDT structure.
 4. WhiBo: an open-source framework
We implemented the proposed component-based framework, WhiBo, as a plug-in for Rapid Miner that enables the creation of generic decision tree algorithms for classi fi cation. In this platform every constructed algorithm represents one Rapid Miner operator that can be used in the environment together with other operators. That means that algorithms generated from WhiBo are fully integrated with other Rapid Miner operators like IO, data preprocessing, performance evaluation, visualization, learners etc.
The WhiBo generic tree user interface (shown at Fig. 4) contains four panels:  X  The left panel contains an array of buttons. Every button represents a concrete sub-problem in  X  The central panel allows users to choose an RC for solving a selected sub-problem. Additionally,  X  The right panel shows the designed GDT structure (selected RCs and their parameters).  X  The top panel contains options for creating new, saving current or opening existing component-based
For illustration purposes, we will construct two co mponent-based algorithms that differ in a single component. The algorithms will then be applied on the  X  X ar X  dataset from UCI repository [3]. Figure 5 shows the de fi nitions of these two algorithms: the one shown on the left panel is the classical CART algorithm and on the right is CART with  X  X ultiway X  instead of  X  X inary X  split for  X  X reate split (categorical) X .
 The question is will this  X  X light X  difference in algorithm design have effect on the resulting tree model? Figure 6 shows tree models generated with algorithms de fi ned in Fig. 5.
The complexity of the tree models is obviously affected by this change. This complexity differences can be attributed to change in a single RC. In section fi ve we give evidence that changes in a single component can result in statistically different classi fi cation accuracy.

To illustrate the robustness of WhiBo environment, in the next subsection we provide an example of extending a repository by a new RC and sub-problem.

WhiBo can be found at the following web page http ://whibo.fon.bg.ac.rs. Data mining and machine learning researchers are invited to join our efforts to exchange components of decision trees and other machine learning algorithms in an open way based on the proposed WhiBo platform, as to establish a standard for interchange of components among decision tree based classi fi cation algorithms, as well as other machine learning algorithms.
 5. Experiments Using WhiBo we designed 80 component-based algorithms. The algorithms were created varying RCs from four sub-problems (RIA, CSC, ES, and PT) while the RC used for CSN was constant in all algorithms ( X  X in X ). Algorithms were created by combining RCs shown in Table 4.
 Two parameters for splittin g and merging in the  X  X hs X  component were set to 5% in all algorithms. Sixteen of these eighty algorithms include the RC  X  X ll X . It is a RC that uses, in each induction step, several methods for sp litting categories (in our experiment:  X  X in X  +  X  X ul X  +  X  X ig X ). In our experiments we wanted to test if broadening the space of candi date splits improves the accu racy of the decision tree algorithms.

We performed three types of experiments in which we tested: 1. Statistical signi fi cant differences among 80 component-based algorithms on 15 datasets with a total 2. Accuracy, time, tree complexity (weighted average tree depth, number of nodes, etc.) of 80 algo-3. The  X  X hs/anf X  RC X  X  trade-off between time and accuracy.

We conductedthese experiments on benchmarkdatasets chosen from the UCI repository [3]. Acknowl-it doesn X  X  try to fi nd a superior algorithm, but merely shows differences in accuracy among components on different datasets. The goal of our research is f undamentally different from what X  X  criticized, since needs. The chosen datasets are shown in Table 5 (column labeled as  X  X igni fi cant differences X  will be explained in Section 5.1), while Table 6 lists some basic properties of the datasets. 5.1. Statistical signi fi cance of component interchange in accuracy among component-based algorithms on a selected dataset. If differences were signi fi cant, this would provide evidence that component exchangebetween algorithms can help signi fi cantly improve accuracy. So, this would suggest that for a speci fi c dataset we should search for the optimal component interplay instead of looking for the optimal among prede fi ned algorithms.

For this experiment we used combined 5 iterations 2-fold cross-validation F-test [2] because it was shown to have considerable statistical power, while keeping the type I error low [7] compared to other popular signi fi cance tests.

We compared pair-wise accuracy of 80 algorithms on 15 datasets. In other words, for each dataset we made 3160 pair-wise comparisons searching for a statistically signi fi cant difference in prediction accuracy. Signi fi cant differences in accuracy were noticed on 13 datasets, while differences above 5% were noticed on six datasets. Datasets in Table 5 are sorted according to the fraction of signi fi cant differences found on classi fi cationaccuracies between algorithms. For example, in car 58% of algorithms signi fi cant differences in algorithm accuracy were noticed. In 3160  X  15 = 47,400 tests a total of 15% signi fi cant differences between algorithm accuracy were found. This signi fi cance was measured with 95% of con fi dence causing at most 5% false positives.

We were searching for the signi fi cantly most accurate algorithm on each dataset, i.e. the winner algorithm on a dataset. We compared algorithms in pairs and calculated a summary score for each algorithm. Algorithms, compared in pairs, received 1 point if there were no signi fi cant differences in algorithms accuracy ( X  X  draw X ). If there were signi fi cant differences the more accurate algorithm got 2 points ( X  X  victory X ), and the beaten algorithm 0 points ( X  X  loss X ). This way we made for each dataset a scoring for all algorithms. Algorithms had an average score of 79 with 6.4 standard deviation and 83.03 median. The distribution of the algorithms accuracy scores is shown in Fig. 7. Two groups of algorithms scores are clearly visible where differences within groups are no more than 5 points (i.e. the group on the left achieved from 82 to 87 points, and the group on the right from 69 to 74 points), whereas scores between groups differ at least 8 points.

We grouped algorithms in two classes: 1. Best algorithms, scored in range [82,87] points, and 2. Worst algorithms, scored in range [69,74] points.

We labeled each of the 80 algorithms with the class the algorithm belongs to, according to signi fi cance scoring, and performed a decision tree algorithm to fi nd rules by which algorithms were assigned to a class. In Table 7 we show extracted rules that can soundly describe the scoring classes with 100% accuracy.

From Table 7 we see which components were parts of  X  X est X  and  X  X orst X  class of algorithms. We can conclude that  X  Remove insigni fi cant attributes X  and  X  Prune tree X  have no in fl uence on algorithms being classi fi ed as  X  X est X  or  X  X orst X . Algorithms containing  X  X ultiway X  are always part of the  X  X orst X  algorithms assembly, and also algorithms that combine  X  X ll X  with  X  X hi-square X ,  X  X ini X  or  X  X nformation gain X  .

Popular algorithms reconstructed with RCs are classi fi ed according rules in Table 7: 1. C4.5 (!-M-GR-P, !-M-GR-!): worst 2. CART (!-B-G-!): best ,and 3. CHAID (!-S-C-!): best .

This picture, though, can be hugely changed on a speci fi c dataset. An algorithm (!-M-C-!) including
Table 8 illustrates ten interesting examples of signi fi cant differences found in algorithms that differ in only one RC. In row 4 we can see that an algorithm signi fi cantly looses accuracy when it uses the  X  X hi-square/anova f test X  RC. On the other hand, in rows 7 through 10 algorithms improve accuracy when using this RC.

In our experiments, the  X  X ar X ,  X  X ur X , and  X  X ic X  datasets showed the largest fraction of signi fi cant differences in pair-wise accuracies among 80 algorithms. What X  X  more interesting is that these differences occur between the same algorithms. The statistically signi fi cant differences found in  X  X ar X  are 78% similar to those found in the  X  X ur X  dataset, and 70% to those found in  X  X ic X . This suggests that algorithms behave similarly on these datasets, which could be due to some intrinsic dataset properties.
Signi fi cant differences observed when replacing RCs motivate analysis of classi fi cation algorithms on the level of components.
 5.2. Performance analysis
In the previous experiment we showed that changing RCs in algorithms can result in statistically signi fi cant differences in algorithm accuracy. Our second experiment aims to explore these differences. Here, besides accuracy, we have also r ecorded additional properties tha t describe the complexity of the resulting tree model (weighted average tree depth, number of nodes, run time, etc.).

These experiments follow a similar scheme to the previous ones, only this time we explore the overall accuracy of component-based algorithms, rather than score from signi fi cance of the pair-wise comparisons.

The 80 algorithms X  accuracies are distributed as on Fig. 8. The accuracies vary in average between 83.22% and 79.37%. The results are reported for 10-fold cross-validation test with strati fi ed sampling. The 80 algorithms X  average achieved accuracy on all datasets was 81.45, with standard deviation 1.17. We classi fi ed algorithms X  accuracies into three classes using average accuracy and standard deviation, as algorithms were in average similarly accurate and there weren X  X  well-separated groups of algorithms accuracy: 1. The best algorithms (accuracy &gt; 82.62 (average + stand. deviation)), 2. Average algorithms (accuracy [80.28, 82.62]), and 3. The worst algorithms (accuracy &lt; 80.28 (average  X  stand. deviation)).

We then used a decision tree algorithm on the dataset (inputs 80 algorithms components, output accuracy class) and discovered 8 rules for the three classes of algorithms, shown in Table 9. The tree showed an accuracy of 97.25%, with two average algorithms being misclassi fi ed as the best.
From Table 9 we can also notice that the PT component had no in fl uence on classi fi cation accuracy of algorithms. We see that the RC  X  X hi-square/anova f test X  ( chs/anf ) improves algorithms accuracy in general. Also we notice that  X  X ultiway X  ( mul ) algorithms perform badly on average, as do  X  X ll X  for RIA are  X  X hs/anf X , for CSC  X  X inary X  (bin) and  X  X igni fi cant X  (sig), or for CSC  X  X ll X  with ES RCs  X  X istance measure X  ( dis ) and  X  X ain ratio X  ( gai ). This indicates that some RCs are more preferable than others on average.

Using this rule we classi fi ed the popular algorithms reconstructed with RCs as: 1. C4.5 (!-M-GR-P, !-M-GR-!): worst 2. CART (!-B-G-!): average ,and 3. CHAID (!-S-C-!): average .

We notice that the three famous algorithms are not part of  X  X he best X  algorithms class in general and that C4.5 performed on the selected 15 datasets in average very bad. Although the  X  X hs/anf X  RC raises accuracy in general, according to the results presented in Section 5.1, this raise of accuracy is not statistically signi fi cant. We must also keep in mind that average accuracy is not representative for all datasets.

The distribution of  X  X ar X  dataset accuracies is shown on Fig. 9. Algorithms achieved average accuracy 94.48% with standard deviation 3.63%. We classi fi ed algorithms by accuracy into three classes: [98,09 X  96.76], [91.84 X 91.49], and [89 X 88.71] where accuracies inside groups differ no more than 1.33, and accuracies between neighboring groups differ at least 2.49.

We present rules from a decision tree model (100% accurate) in Table 10. Using this rules we classi fi ed the popular algorithms reconstructed with RCs as: 1. C4.5 (!-M-GR-P, !-M-GR-!): worst 2. CART (!-B-G-!): best ,and 3. CHAID (!-S-C-!): best .

On the  X  X ar X  dataset the component  X  X hs/anf X , which was in average part of the best accuracy group, hadnoin fl uence on an algorithm being classi fi ed in the best accuracy group.

Accuracy of algorithms varies between datasets. A RC performing well on one dataset can perform poor on another dataset. However, we noticed that 80 algorithms had similar accuracy patterns on  X  X ur X ,  X  X ar X , and  X  X ic X  dataset. We show accuracy patterns for algorithms using  X  X hs/anf X  (Fig. 10) and for algorithms not using this RC (Fig. 11).

On these three datasets, and possibly some other datasets, algorithms behavior could follow the same pattern. This opens the question whether these datasets are somehow similar. If they were similar in some measurable way, it would be possible to predict algorithms X  performance, based on the performance on similar datasets. This could potentially be used to aid the algorithm selection process.
Algorithms using  X  X ul X  RC on these three datasets results in accuracy loss. Similar accuracy loss was observed for  X  X hs X  and  X  X in X  when combined with  X  X ll X . We can also notice that  X  X hs/anf X  reduces accuracy when used with  X  X ul X , and  X  X ll X  on  X  X ic X , but improves accuracy on  X  X ar X .

We showed that effectiveness of using a RC in an algorithm is dataset dependent, and that there are datasets where algorithms perform similarly. This indicates that RC performance should be related to dataset properties.
One further point in our research was that component-based design enables easier analysis of partial algorithm improvements. We only give some indications for this. Figures 12 (a) and (b) show average accuracy and average run time of algorithms using CSC RCs. Run time was measured on the level of algorithms.

From Figs 12 (a) and (b) we notice that algorithms including  X  X in X  had the best accuracy in average, but second worst run time. We expected bene fi t in average accuracy using  X  X ll X  because it generates the widest space of candidate splits. However, there wasn X  X . Moreover,  X  X ll X  only outperformed  X  X ul X , and in addition had the l ongest processing tim e. Figure 12 (a) show that  X  X ig  X  has comparable accuracy with  X  X in X , but computes faster as shown at Fig. 12 (b).

Although algorithms using  X  X ll X  showed on average not as the best alternative there are, however, datasets where  X  X ll X  is part of the top ranking algorithms. On the  X  X ar X  dataset  X  X ll X  belongs to the group of most accurate algorithms when combined with  X  X istance X , while when combined with  X  X hi X ,  X  X nf X , or  X  X in X , was performing bad, or average when combined with  X  X hs/anf X , on this dataset (Table 10). This indicates biases present in evaluation measures towards  X  X eep X  or  X  X hallow X  trees.

The  X  X ig X  RC seems as an excellent choice in average, because it performs similar to  X  X in X  but requires less computational time.

We explored interactions between  X  X reate splits (categorical) X  RCs and  X  X valuate split X  RCs. These are shown on Figs 13 X 14. We used  X  X ll X  to test if there are biases between ES RCs and CSC RCs. As Fig. 13 shows,  X  X ll X , combined with  X  X hs X ,  X  X in X  or  X  X nf X , performs similarly to  X  X ul X , while  X  X ll X  combined with  X  X is X  or  X  X ai X  performs similarly to  X  X in X  and  X  X ig X .

This indicates that  X  X hs X ,  X  X in X , and  X  X nf X  are bi ased towards choosing  X  X ul X  splits, while  X  X ai X  is biased towards bin. This is also indicated by the results summarized in Fig. 14, where a similar pattern of behavior can be noticed on the average number of tree nodes.

We measured tree complexity, also, with weighted average tree depth (WATD) which can be calculated as the average product of leaf X  X  depth and their corresponding number of cases ( instances ) where d i is a leaf X  X  depth, c i is the number of cases in leaf i ,and l is the total number of leafs. This measure indicates the average length of the tree path needed to classify an example. The results are shown in Fig. 15. We see that  X  X ai X  is consistently part of the  X  X eepest X  trees while  X  X nf X  produces the  X  X hallowest X  trees.

In Fig. 16 we show that algorithms that evaluate split based on  X  X ai X  consistently required most processing time, but they were also accurate. On the other hand algorithms with split evaluation based on  X  X is X  were fast and accurate (Fig. 13).

Finally, we show algorithms X  average accuracies on fi fteen datasets. The datasets in Table 11 are sorted according to the number of signi fi cant differences found in the previous section. The column  X  X ax-Min X  shows the difference between the best performing algorithm and worst performing algorithm on a dataset. These differences must be used with caution, because they tell as not much about signi fi cant differences between algorithm accuracy found in data (Table 5). 5.3. Analysis of tradeoff between accuracy and speed when removing insigni fi cant attributes
Algorithms including the  X  X hs/anf X  RC performed better in average by accuracy. We wanted to test how this RC in fl uences accuracy and computational speed. This time we didn X  X  remove attributes that induction step, and removed a de fi ned percentage of the least signi fi cant attributes. We tested the 80 algorithms with four experiments: 1. Experiment 1: In each node we removed 40% of the least signi fi cant attributes. They weren X  X  used 2. Experiment 2: In each node we removed 60% of the least signi fi cant attributes. 3. Experiment 3: In each node we removed 80% of the least signi fi cant attributes. 4. Experiment 4: In each node we found only the most signi fi cant attribute like in [13] and used it for
In Tables 12 and 13 we show the averaged results of our experiments. The best average values for each dataset, i.e. row are bolded, while the worst values are underlined. On one hand, we can make the conclusion that reducing the number of attributes in each induction step reduces on most datasets computational speed. It would be expected that Experiment 4 needs the least computational time, while Experiment 1 should need the most time. This, however, doesn X  X  happen on all datasets because the time needed to calculate the signi fi cance of attributes was not compensated by reduced calculations performed after usage of this RC. A more detailed analysis, which is out of the scope of this paper, would be needed to analyze these fi ndings more thoroughly.

On the other hand, RIA reduces average accuracy of decision tree classi fi ers (Table 11). The difference between the best and poorest accurate average accuracy achieved on experiments is shown in the rightmost column in Table 13. These differences are small compared to the results shown in Table 11.
In contrary to what would be expected,there are some datasets where accuracy improves when choosing only the most signi fi cant attribute (e.g.  X  X ic X ,  X  X mc X , and  X  X dv X ). So, using this RC is recommended on most datasets, because it can reduc e computational time, but still not decreasing accuracy too much.
Obviously, one can always fi nd a dataset where this doesn X  X  hold, so it is important to fi nd for each dataset an appropriate RC interplay. 6. Conclusion and future research
Reusable component design is a relatively new research topic in data mining. Although classical research possibilities within these algorithms when looki ng at the components level.

We proposed a white-box decision tree design approach that is aimed to help the cost-effective design of classi fi cation algorithms that could perform better in speci fi c situations. We showed that there is signi fi cant statistical evidence that such a white-box approach can produce more useful algorithms.
Three experiments reported in this article provide evidence that a component-based approach can outperform existing decision tree algorithms on speci fi c datasets.

We conclude that: 1. Component-based algorithms are useful for testing of performance in fl uences of each algorithm 2. Algorithm (and component) performance is in fl uenced by the interplay between the components 3.  X  X emove insigni fi cant attributes X  RC (when used with 5% threshold), that is used in each induction 4. It can be bene fi cial to use  X  X ll X  RC (as a union of  X  X inary X ,  X  X ultiway X , and  X  X igni fi cant X ) in 5. RCs  X  X ig X  and  X  X is X  seem as the best alternative for algorithms design if the trade-off between 6.  X  X emove insigni fi cant attributes X  RC (when used for removing a de fi ned percentage of least sig-7. One can always fi nd a dataset where the previous conclusions do not hold.

The big question  X  X hy X  a particular RC performs better on speci fi c datasets remains an issue for further research. Solving this could help better describe and improve parts of algorithms. Further experiments are needed to explore the relationship between dataset properties and RC performance. There is also the possibility of analyzing how RCs are interacting to improve performance.
 and other families of machine learning algorithms. For example, a generic algorithm for partitioning clustering is already proposed in [6].

Another research direction for the future is to solve the problem of fi nding the most appropriate algorithm for a problem when the number of possible algorithms is huge. By expanding the number of RCs and sub-problems of a generic algorithm, fi nding the most appropriate algorithm will a challenging task. We believe that meta-heuristics, like genetic algorithms or variable neighborhood search, could be used in further research.

Our research supports the call for standardization of components and algorithms [23]. Standardization would enable easier and faster interchange of algorithm ideas and implementations. We offer WhiBo as a framework towards this standardization.
 Acknowledgements This research was partially funded by a grant from the Serbian Ministry of Science and Technological Development, project ID TR12013, 2008 X 2009.
 References
