 The need for evaluating large amounts of topics (queries) makes IR evaluation an uneasy task. In this paper, we study a topic se-lection problem for IR evaluation. The selection criterion is based on the overall difficulty of the chosen set, as well as the uncertainty of the final IR metric applied to the systems. Our preliminary ex-periments demonstrate that our approach helps to identify a set of topics that provides confident estimates of systems X  performance while keeping the requirement of the query difficulty.
 H.3 [ Information Storage and Retrieval ]: H.3.3Information Search and Retrieval
Experimentation, Measurement, Performance
Given the tremendous amount of data on the Web and the huge volumes of queries that users issue to search engines, it is a big challenge to evaluate how well a search system can perform well to satisfy users X  information needs. In order to tackle the challenge, we propose to select a set of queries from a large query pool so that a system X  X  performance on the selected set can serve as an accurate indicator of the system X  X  performance on the whole pool of queries.
The Cranfield evaluation methodology, which has been adopted in evaluation initiatives such as the Text REtrieval Conference (TREC), uses standard test collections consisting of documents, topics, and relevance judgments. An IR system produces a ranked list of doc-uments for each topic, a metric such as average precision (AP) is calculated on the ranked results and indicates how well this sys-tem did on this topic. The mean of such a metric over a number of topics (e.g. mean average precision or MAP), is generally used to measure the system X  X  overall performance.

The robustness and reliability of this methodology has been well studied (e.g. [1, 6, 5]). The sources of error include limited pool depths, incomplete relevance information, topic set size, averag-ing across topics with different numbers of relevant documents etc. Given such shortcomings, when we use a set of topics and rele-vance judgements to evaluate a system, there will be some uncer-tainty in our estimate. Motivated by the Modern Portfolio Theory of finance [2], we attempt to account for, quantify and then mini-mize the uncertainty involved in running such an evaluation.
In this context, we consider topic difficulty, which potentially convolutes many factors: (i) individual systems find particular top-ics difficult (ii) topics are likely to be difficult for all systems if they do not have corresponding conten t in the collection (iii) ambiguous or wide topics can be difficult even if the collection contains rele-vant content. An estimate of a topic X  X  intrinsic difficulty (i.e., (ii) and (iii) above) can be obtained by l ooking across systems X  perfor-mance on that topic.

We first propose our approach in Section 2, present experimental results in Section 3, and conclude in Section 4.
Given a set consisting of N topics, the aim is to select a set of topics { t k }, k  X  [1 ,n ] for system testing. We have the AP (average precision) values of M systems on the N topics, which forms an M  X  N matrix, where AP i,j denotes the AP of the i -th system on the j -th topic. The mean value of the i -th row is the MAP for the th system, MAP i , and the mean of the j -th column is the expected AP or AAP j (Average Average Precision) [3] for the j -th topic.
Therefore, for the selected n topics, the expected MAP ( EMAP i.e., the mean of the expected AP values for the n topics, and the uncertainty associated with the expected MAP (for detailed deriva-tion of Eq. 1 and 2, we refer to [2]) are:
In Equation 2,  X  2 t k and  X  2 t l are variances/uncertainties due to in-dividual topics t k and t l . This quantity can be used to capture fac-tors like uncertainty in the number of relevant documents for the topic, incomplete relevanc e judgments, etc. The quantity the correlation between topic t k and t l . It can be seen that if we want to pick the topics with the lowest Unc n , then one way to do that would be to pick topics with the lowest  X  2 t k .Thatistosay, as part of the evaluation set, we pick the topics about which we know most. In this paper, we fix all  X  2 t k =0 . 01 (or the standard deviation  X  t k =0 . 1 ), leaving more accurate estimations for future work. Assuming that some level of uncertainty regarding topics is inevitable, attempting to compile a set of n topics with minimum Unc n boils down to picking topics with low (preferably negative) correlations  X  t k ,t l .

We estimate  X  t k ,t l as the Pearson correlation coefficient [4] be-tween the t k -th and t l -th columns in the AP matrix.  X  lies between -1 and 1) measures the relationship between two top-ics. The higher the value of  X  t k ,t l , the more closely related the two topics are. Note that a high value for the correlation between two topics does not necessarily mean that the pair are related in terms of content, it only means that systems which do well (or badly) on one topic tend to do well (correspondingly, badly) on the other.
To reduce the uncertainty in EMAP n , i.e., Unc n in Equation 2, we need to have small correlation coefficients (preferably negative) between topics. To understand why this is so, consider having AP values for all systems for a given topic t j . If we add another topic t which is highly positively correlated with t j , knowing the AP values for the systems on t k is unlikely to provide any new infor-mation regarding the relative goodness of the systems. From an evaluation perspective, it is sufficient to have one of t topic set if there is a strong positive correlation between them. If we aim to find the minimal set of topics that provide informative and certain estimates of system effectiveness, diversifying the set of topics will lead to each individual being more informative and the uncertainty associated with EMAP n will be reduced.

The main goal of topic selection now is to minimize the uncer-tainty Unc n when we set a preference for the overall difficulty of selected queries as indicated by EMAP n . We propose a greedy approach for topic selection, which may not produce the globally optimal solution but allows for the trade-off between computational cost and correctness. We start with all the N topics, and remove one topic at each step, until n topics remain. In each step, we remove the topic that can result in the largest reduction in the overall uncer-tainty Unc n and at the same time the EMAP n of the remaining topics matches the criterion of our topic difficulty preference.
We applied our approach to 249 topics in the TREC 2004 Ro-bust Track collection consisting of topics of different levels of dif-ficulty. There are 110 runs submitted by 14 groups. The range of the MAP is (0 . 0756  X  0 . 3586) , and range of the expected AP,  X  AP ,is (0 . 0077  X  0 . 7717) . The EMAP and overall uncertainty of the 249 topics are 0.2599 and 0.0026, respectively. Two strongly correlated topics are topic 392  X  X obotics X  and 431  X  X obotic Tech-nology X  with Pearson correlation coefficient as 0.8320.

As indicated before, when going from a topic set of size n n  X  1 , we have to define a criterion that indicates our preference for the topic to be dropped. We experimented with three policies for removing topics: (1) minimization of uncertainty only, indicating no preference on topic difficulty, (2) minimization of uncertainty while maintaining EMAP above 0 . 2599 (=the total average), indi-cating preference for retaining easy topics, and (3) minimization of uncertainty while EMAP decreases monotonically when removing topics, indicating preference for retaining difficult topics.
For each set of n topics, we calculate EMAP n as given in Equa-tion 1. The uncertainty or variance of this value is given by as in Equation 2, we monitor the square-root of Unc n (the stan-dard deviation) as a function of the size of the topic set. Fig. 1 (a) and (b) plot the EMAP and standard deviation versus n n  X  [1 , 249] . For comparison, we also show a baseline policy that randomly drops topics.

All the four curves share the right-most point, when the topic set consists of all the 249 topics. The experiment proceeds by trac-ing one of the curves (a specified policy for dropping topics) while moving towards the left. The first observation from Fig. 1 (a) is that our three approaches select topics according to the topic difficulty preferences. For the first preference, the EMAP decreases until around 50 topics so that the standard deviation is optimally mini-mized. For the second preference, EMAP is always above 0.2599 to keep the topics easy overall. For the third preference, EMAP decreases the most quickly in order to find more difficult topics.
From Fig. 1 (b) we see that all our three preference based ap-proaches outperform the baseline in terms of reducing the uncer-tainty. The overall standard deviation decreases for all three prefer-ences until the number of selected topics is around 50. The standard deviation decreases the most quickly when there are no restrictions on the EMAP. When the number of topics decreases under a small value, the standard deviation starts increasing for all three prefer-ences as well as the baseline, since topics are becoming more in-Figure 1: Comparison of three topic selection policies against a random topic selection baseline. We plot the number of topics vs. (a) EMAP and (b) Standard Deviation. All methods start at the same point on the right end and move leftwards while eliminating different topics according to the specified policy. dependent of each other and there are few negatively correlated topics. This also helps explain that the number of topics for IR evaluation should be above a certain number, such as 50. It is in-teresting to note that amongst our three policies, preferring difficult topics leads to maximum standard deviation (uncertainty), since systems all perform relatively badly on such topics so that they do not provide us with reliable indicators of system effectiveness.
This paper considered the standard IR evaluation task of calcu-lating the relative effectiveness of a group of systems based on a set of topics. The outcome of such a measurement has some in-herent uncertainty due to properties of individual queries/topics in the set, as well as interactions between them. Motivated by the Modern Portfolio Theory, we propose a novel mean-variance based topic selection approach for minimizing the uncertainty in the mea-sured effectiveness. Given a restriction on the size of the subset, our approach identifies those topics that reduce the uncertainty of sys-tems X  effectiveness, under the condition that the overall difficulty of the set matches predefined preferences.
