 The quantum probabilistic framework has recently been ap-plied to Information Retrieval (IR). A representative is the Quantum Language Model (QLM), which is developed for the ad-hoc retrieval with single queries and has achieved sig-nificant improvements over traditional language models. In QLM, a density matrix, defined on the quantum probabilis-tic space, is estimated as a representation of user X  X  search intention with respect to a specific query. However, QLM is unable to capture the dynamics of user X  X  information need in query history. This limitation restricts its further appli-cation on the dynamic search tasks, e.g., session search. In this paper, we propose a Session-based Quantum Language Model (SQLM) that deals with multi-query session search task. In SQLM, a transformation model of density matrices is proposed to model the evolution of user X  X  information need in response to the user X  X  interaction with search engine, by incorporating features extracted from both positive feedback (clicked documents) and negative feedback (skipped docu-ments). Extensive experiments conducted on TREC 2013 and 2014 session track data demonstrate the effectiveness of SQLM in comparison with the classic QLM.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation, Relevance feedback, Retrieval Models Quantum Language Model, Session Search, Density Matrix Transformation
Recently, various quantum theory (QT) based IR models are developed under the inspiration of the pioneering work of van Rijsbergen [8], which draws a clear connection be-tween the QT and IR. Piwowarski et al. [5] proposed that queries and documents can be modeled as density operators and subspaces respectively, but the tensor space based rep-resentation method has not led to good performance. The advent of Quantum Language Model (QLM) [7] , a represen-tative QT-based IR model, successfully solved this issue. In QLM, both single terms and compound term dependencies are represented as projectors in a vector space, while queries and documents are represented as density matrices defining a quantum probability distribution in the space. An EM-based training method for the estimation of density matrix is then devised [7]. The advantages of QLM over traditional language models have been demonstrated from both theo-retical and experimental perspectives.

Despite its success in the ad-hoc retrieval, QLM (referred to as classical QLM in the rest of the paper) is solely targeted on single ad-hoc queries. It is insufficient to capture the dynamics of users X  information need in response to the user X  X  interaction with the search engine. As a result, it is difficult for the classical QLM to be applied in more complex search tasks, such as multi-query session search.

To address this challenge, we propose to integrate user X  short-term interaction information into the estimation of QLM for the current query, and correspondingly a novel Session-based QLM (SQLM) is proposed. The evolution of the user X  X  information need within a search session is mod-eled by the density matrix transformation, i.e., transform-ing the original density matrices (for single queries) by some principled rules based on user interactions (e.g., the click and dwell time). We also put forward the concepts of positive projectors and negative projectors extracted from the pos-itive feedback documents (clicked documents) and negative feedback documents (skipped documents), respectively, to enhance the representation ability of the QLM. Specially, a novel training algorithm for QLM with different projectors is devised. Although there exists a body of related work [3][9] for integrating users X  interaction information in IR models, they did not model term dependencies in queries and docu-ments, compared with the SQLM proposed in this paper.
In the field of IR, the quantum probability is defined on a real finite space R n [7] for simplicity (originally, de-fined on the infinite Hilbert space). In this paper, we use the Dirac X  X  notation to represent a unit column vector ~u  X  R n as | u  X  and its transpose ~u T as  X  u | , respectively. An elementary quantum event can be uniquely represented by a projector onto a 1-dimensional subspace of R n . For a unit vector | u  X  , the corresponding elementary quantum event, or the projector, is denoted as | u  X  X  u | . Suppose | e 1  X  , | e forms an orthonormal basis for R n , then each unit vector | v  X  can be uniquely expressed as the superposition of | e | v  X  = P i v i | e i  X  , where P i v 2 i =1.

A measure  X  is introduced to define the quantum proba-bility on R n . It satisfies two conditions: (I) for every pro-basis {| u i  X  X  for R n , we have P n i =1  X  ( | u i  X  X  u Gleason X  X  Theorem [2] can prove the existence of a mapping density matrix  X   X  S n ( S n is the density matrix space con-taining all n -by-n positive semi-definite matrices with trace 1, i.e., tr (  X  ) = 1). Formally, any density matrix  X  assigns a quantum probability for each quantum event in vector space R n , thereby uniquely determining a quantum probability distribution over the vector space.
The classical Quantum Language Model(QLM) aims at modeling term dependencies in the principled quantum the-ory formulation. Different from traditional language models, QLM extracts term dependencies in each document as pro-jectors in the quantum probabilistic space. The single words correspond to projectors | e i  X  X  e i | , and the compound terms (with two or more words for each term) correspond to pro-jectors | v  X  X  v | (refer to Section 2.1). The projectors are used to estimate density matrices  X  q and  X  d for a query and each document by maximizing a likelihood function with the EM-based iterative approach, i.e., R X R algorithm [7]. Then, the top retrieved documents in the initial search results returned by the traditional language model are re-ranked according to the negative VN-Divergence between  X  q and  X  d . For details of the classical QLM, please refer to [7].
In QLM, a single query can be represented by a density matrix  X  over a vector space for a certain vocabulary. The positive definite matrix with unitary trace can be decom-posed as follows: where | u i  X  is a eigenvector, and  X  i is the eigenvalue. Corre-spondingly,  X  i = | u i  X  X  u i | can be interpreted as an elemen-tary quantum event or projector, and  X  i is the corresponding quantum probability for the elementary event ( P n i =1  X  By obtaining a density matrix, we actually obtain a set of mutually orthogonal quantum elementary events along with the corresponding discrete probability distribution, and vice versa. In a real search scenario, a user often interacts with the search engine many times before achieving his/her actual information need. We propose a density matrix transforma-tion framework to model the interaction process, which is mathematically formulated as a mapping function T in the density matrix space S n .

In session search, we assume that there exists an  X  X deal X  transformation T for density matrices which can model dy-namic query intention in the historical interactions. Specif-ically, T is a transformation that for any two consecutive queries q i  X  1 and q i , the estimated density matrix  X   X  represents the user X  X  information need for q i , where  X  i  X  1 a representation of q i  X  1 . This implies we further make a 1 order Markov assumption that a query is dependent sole-ly on its last previous query. This assumption is reasonable because the dependency can continuously back-propagate in the session.

Theoretically, from Eq.(1), we can easily find that T can be divided into two separate transformation process: T = T
T 2 . T 1 is the transformation operator for quantum events | u  X  . Since | u i  X  forms an orthogonal basis, T 1 can be any standard transition matrix. T 2 changes the original proba-bility distribution for the events (namely, change the values of  X  i ), and it is be a diagonal matrix. In this sense, the transformation of density matrix is basically a transforma-tion of main quantum events, and a reallocation of quantum probability for each event.

In practice, however, this consideration seems infeasible due to its high degree of freedom. Suppose a vocabulary V with |V| distinct words, T 1 will have a freedom of O( |V| and T 2 will have a freedom of O( |V| ). Thus the model is prone to be overfiting and computationally expensive. More-over, it is hard to draw a clear and reasonable connection from the training of T 1 and T 2 to the extraction of projec-tors. Therefore, we propose an iterative training approach to represent the transformation process, inspired by the up-dating method of the classical QLM. Specially in this paper, we use the density matrix  X  i  X  1 for query q i  X  1 as the initial density matrix to train the density matrix  X  i for query q
To facilitate subsequent discussions, we define notations for the session search. In a search session, we have a set of historical interaction units { Q i ,R i ,C i } N  X  1 i =1 and C i represent the query, returned documents and clicked documents for the i th interaction unit respectively. We need to use the historical interaction information to retrieve docu-ments for the current query Q N . To this end, we first obtain the top N retrieved documents returned by the traditional language model (LM), denoted as R N . {  X  i } N  X  1 i =1 set of |V| -order density matrices representing user X  X  infor-mation need for each historical query, where |V| is the size of the vocabulary containing all distinct terms in the histor-ical queries and the current query.
For a historical interaction of a search session, the first clicked document of the query is not always the first one in the search results list. In other words, users often skip some irrelevant results before clicking the first assumingly relevant document. Therefore, we assume that the  X  X kip X  behavior is a strong negative feedback signal of users, since the user would have otherwise clicked them. In our assumption, some extreme cases are neglected. For example, the user may gain the right information only by reading the snippets without detecting any click behaviors. Based on this point, we form a positive documents set R pos ( i ) with all clicked documents as well as a negative set R neg ( i ) with all skipped documents in R i for each query q i . Note that, R pos ( i ) is equivalent to C , and R neg ( i ) is null for the queries whose first returned document is clicked.

From R pos ( i ) and R neg ( i ), positive projectors P pos and negative projectors P neg = {  X  j } M neg j =1 are extracted us-ing the method discussed in Section 2.2, where M pos M neg is the number of positive and negative projectors re-spectively. Note that some details when extracting projec-tors: i) only single words, bi-grams and tri-grams are con-sidered as possible compound dependencies, since otherwise the computational complexity will be exponential to the vo-cabulary size; ii) we use TFIDF to assign the superposition weight v i rather than the IDF or UNIFORM weights intro-duced in the original paper [7], since TFIDF is a documen-t specific measure and has better distinguishability across documents. In order to maximize the probability that all positive events happen while all negative events not happen with respect to the quantum probability distribution (i.e., the density matrix  X  ), the Maximum Likelihood Estimation (MLE) problem can be formulated as  X   X  = argmax ( where  X  i and  X  j denote a positive projector and a negative projector. Since where c  X  j = I  X   X  j |V| X  1 is also a legal density matrix and |V| X  1 is a constant. Then Eq.(3) can be rewritten as Eq.5 is similar to the objective function in classical QLM. Thus we can apply the similar updating method used in [7] to update the density matrix  X  . Since the R X R algorithm in [7] dose not guarantee convergence, we revise it by utilizing the updating method in [4]:  X   X  It can be strictly proved in [4] that for a sufficiently small value of  X  , Eq.(6) guarantees global convergence. Although this updating method guarantees the global convergence the-oretically, it requires a sufficiently small value of parameter  X  , resulting in a slow training speed. Therefore, in this pa-per we do not target on training the density matrix to its convergence, but control an appropriate iterative steps (will be discussed in Section 3.3).

In SQLM, we also model the dwelling time and click se-quence for each clicked document. The assumption is that a longer dwelling time and an earlier click mean that the doc-ument is more likely to be relevant. Specifically, the weight for a clicked document d is calculated as where t d is the dwelling time for the document d , t the lasting time of the whole interaction, Seq d denotes the rank of d in the returned document list, and c is a decaying parameter in [0,1], which we will further discuss in Section 4.2. The objective function (3) can therefore be updated as where D ( i ) is the document containing the projector  X  The new objective function is similar to Eq.5, and the only difference is that the new one multiplies each projector in clicked documents by a weight W D ( i ) . Thus the updating methods discussed for Eq.6 can still be applied to the new objective function in Eq.8.
In this paper, we do not train the quantum events trans-formation operator T 1 and the quantum probability change operator T 2 for density matrix transformation operator T , because of the high freedom. Instead, we propose an itera-tive training algorithm to approximate the process of density matrix transformation between two subsequent queries: Algorithm 1 : Density Matrix Transformation. 1:  X  0  X  diag ( LM ); // Initiate the density matrix  X  0 with 2: for k = 1; k  X  N  X  1; k += 1 do 3: Extract projectors from R k pos ,R k neg (Section 3.2); 4: Estimate  X  k with initial density matrix  X  k  X  1 with 5: end for 6: Return the desired density matrix for interactions  X  N  X  1
The training steps are different for different queries, s-ince we believe nearer queries to the current query will have stronger influence on the estimation of current query. The initial training steps S and the discount factor  X  are free parameters which need to be further discussed. The more steps the density matrix is trained, the closer it moves to-wards the current query density matrix and away from the initial matrix. Thus, gaining an appropriate training steps can achieve a balance between the current query information and historical interaction information.
We use the top K (we set K = 50 in this paper) retrieved documents (pseudo feedback documents) returned by tradi-tional LM to train a pseudo feedback QLM  X  p N for current query. The representation of user X  X  search intention can be formulated as the linear combination of  X  N  X  1 and  X  p N where  X  controls the extent to which the history influence on the query representation. After obtaining  X   X  , it can be used to re-rank the retrieved documents following the same method in [7].
Empirical evaluations are conducted on the TREC 2013 and 2014 session track data shown in Table 1. The corpus Table 1: Statistics For TREC 2013 and 2014 Datasets (TREC 2014 X  X  official ground truth only contains the first 100 sessions).
 Table 2: Performance on TREC 2013 and 2014.
 used in our experiments is the ClueWeb12 full corpus 1 which consists of 733,019,372 English webpages collected from the Internet. We index the corpus with Indri search engine In the indexing process, we filtered out all documents with Waterloo X  X  spam scores [1] less than 70, removed the stop words and stemmed all words with Porter stemmer [6].
To verify the effectiveness of the proposed model, we com-pared the following models: (i) QLM , the classic quantum language model which is regarded as the baseline model; (ii) SQLM , the proposed session-based quantum language mod-el; (iii) SQLM+LM , the combination model of SQLM and traditional language model (LM), which takes the feature of LM into consideration (the linear combination parameter is  X  ). We employ the official evaluation metrics MAP and NDCG@10 to evaluate the models.

A number of parameters are involved in the proposed models, and they are summarized as follows: c in Eq.7, S and  X  in Algorithm 1,  X  in Eq.9, and  X  in model SQLM + LM . For the global setup, we select  X  = 0 . 01 in Eq.6. The selec-tion of best parameters will be discussed in next section.
Table 2 reports the experimental results for TREC 2013 and 2014 datasets respectively. In the tables,  X  X hg% X  means the improvement percentage over the baseline, i.e., QLM.
Since the modeling process of SQLM only involves ma-trix addition and multiplication, the computing complexity is low, allowing us to conduct a grid search to find the best parameter configuration. For TREC 2013, the best param-eter configuration is { c = 0 . 95, S = 10,  X  = 1 . 05,  X  = 0 . 7 } for SQLM; and { c = 0 . 95, S = 30,  X  = 1 . 05,  X  = 0 . 6 ,  X  = 0 . 9 } for SQLM+LM. For TREC 2014, the best param-eter configuration is { c = 0 . 95, S = 30,  X  = 1 . 0,  X  = 0 . 7 } for SQLM, and { c = 0 . 95, S = 30,  X  = 1 . 15,  X  = 0 . 9,  X  = 0 . 9 } for SQLM+LM.

The results indicate that the proposed SQLM achieves improvements over the classical QLM, on both TREC 2013 (11.01% improvement for NDCG and 5.33% for MAP), and TREC 2014 Session data (4.51% relative improvements for NDCG and 3.66% for MAP). Moreover, a linear combination http://www.lemurproject.org/clueweb12/index.php http://www.lemurproject.org of SQLM and LM can further enhance the performance of SQLM, suggesting that SQLM is adaptive to other features such as the traditional LM. It also indicates that SQLM has a large potential for further improvements.
In this paper, we present a novel quantum theory based probabilistic framework for multi-query retrieval task, i.e., session search. By extending the classical Quantum Lan-guage Model (QLM), our proposed Session-based Quantum Language Model (SQLM) incorporates the sound mechanis-m of the density matrix transformation to approximate the dynamics of information need entailed in historical inter-actions, for re-ranking the initial results generated by the search engine. At the operational level, we utilise the in-formation from both clicked documents and top unclicked documents, and devise a new training algorithm. Extensive experiments on both TREC 2013 and 2014 Session track datasets demonstrate that SQLM does perform better than classical QLM for multi-query retrieval systems, and also show its potential of being further improved for session search.
Therefore, it is safe and reasonable to conclude that the proposed Session-based Quantum Language Model(SQLM) is a feasible expansion of classical Quantum Language Mod-el(QLM) on the multi-query session search tasks. As for future work, we believe that a better retrieval result could be achieved if one can find a better realization of density matrix transformation based on the quantum inference, and incorporate more features into the framework. We will also apply the model to data closer to real-time retrieval systems.
This work is supported in part by them Chinese Nation-al Program on Key Basic Research Project (973 Program, grant No.2013CB329304, 2014CB744604), the Chinese 863 Program (grant No. 2015AA015403), the Natural Science Foundation of China (grant No. 61402324, 61272265), and the Research Fund for the Doctoral Program of Higher Edu-cation of China (grant no. 20130032120044). Any comments from anonymous reviewers are appreciated.
