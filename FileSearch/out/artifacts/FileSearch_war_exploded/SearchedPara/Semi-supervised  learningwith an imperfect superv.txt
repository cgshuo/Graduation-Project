 REGULAR PAPER Massih R. Amini  X  Patrick Gallinari Abstract Real-life applications may involve huge data sets with misclassified or partially classified training data. Semi-supervised learning and learning in the presence of label noise have recently emerged as new paradigms in the machine learning community to cope with this kind of problems. This paper describes a new discriminant algorithm for semi-supervised learning. This algorithm opti-mizes the classification maximum likelihood (CML) of a set of labeled X  X nlabeled data, using a discriminant extension of the Classification Expectation Maximiza-tion algorithm. We further propose to extend this algorithm by modeling imperfec-tions in the estimated class labels for unlabeled data. The parameters of this label-error model are learned together with the semi-supervised classifier parameters. We demonstrate the effectiveness of the approach using extensive experiments on different datasets.
 Keywords Semi-supervised learnin  X  Imperfect supervision  X  Classification Expectation Maximisation  X  Classification Maximum Likelihood 1 Introduction Most statistical classifiers rely on a supervised learning paradigm where a deci-in which each example is described by a pattern x i and by the response of a super-visor y i . Under this paradigm, data are supposed to be drawn independently from a joint distribution p ( x , y ) and the learned decision rule is supposed to capture the relation between these two variables.
 sources and is often unrealistic. For example, for many Information Retrieval problems, labeling data is a time consuming and a difficult task. For other prob-lems such as medical imaging , labeling data may require very expensive tests so that only a small set of labeled data may be available. Sometimes noise is inherent to the labeling process which further complicates the learning problem. In such cases, modeling the label noise can lead to performance increase.
 lems and proposed different methods for learning with partially labeled and mis-classified labeled data. More recently, both problems have attracted the interest of the machine learning community.
 algorithms. Most studies on the semi-supervised paradigm rely on a generative approach. Using a mixture density model where mixture components are identified to classes, they attempt to maximize the joint likelihood of labeled and unlabeled data using the EM algorithm [ 22 ]. Recent papers have proposed new semi-super-vised models based on the discriminative approach. This is a natural way for clas-sification, since discriminant models usually perform better than generative ones. contributions are algorithmic and experimental. We first propose a new discrimi-nant semi-supervised algorithm (Sect. 5 ). We then introduce a label-error model which aims at improving the labelings of unlabeled data computed by semi-supervised algorithms. This error model will be learned together with the classifier parameters and may be used in both generative and discriminative semi-supervised settings (Sect. 6 ). We also provide an extensive set of experiments for evaluating empirically the contribution of these different ideas and comparing the proposed algorithms with baseline classifiers and state-of-the-art semi-supervised methods. ting of classification maximum likelihood ( CML ) and classification EM ( CEM )ap-proaches [ 15 ]. The main benefit gained by introducing this formalism is that CML , CEM and their extensions described here do provide a natural framework for ex-pressing both generative and discriminative approaches for the semi-supervised learning problem. This is not the case for the usual maximum likelihood approach or for any other method.
 rithms using generative models. CML has been extended to semi-supervised learn-ing for generative models by McLachlan [ 32 ]. We introduce here a discriminative form of the CML criterion and a CEM algorithm for optimizing this discriminative CML over labeled and unlabeled data. This CEM algorithm will serve as a basis for the proposed discriminant semi-supervised algorithm. Considering only unlabeled data, this would amount to perform clustering with discriminant classifiers by es-timating cluster posteriors instead of conditional densities as it is usually done in most clustering approaches (including plain EM ). When considering only labeled data, CML reduces to cross-entropy between targets and estimated class posteriors. Maximizing the discriminative CML criterion in the presence of partially labeled data will lead to optimize simultaneously the cross-entropy for labeled data and the CML for unlabeled data.
 ing is evident, the motivation for using a label-error model requires an explanation. Both the generative and discriminative versions of semi-supervised CEM are itera-tive algorithms which compute at each iteration tentative labels for unlabeled data. Together with labels of labeled data, these tentative labels are considered as target labels when updating the system parameters. Let us now consider an ideal learning problem, where the true labels of unlabeled data would be known. With respect to this ideal classification problem, the semi-supervised learning algorithm will compute, at each iteration, erroneous class labels for part of the unlabeled data set. Such labeling errors are inherent to any semi-supervised algorithm. We will make the hypothesis that these label errors do correspond to a stochastic process and propose to learn an error model on labels predicted by the algorithm, simul-taneously with the system parameters. As will be seen in Sect. 7, the combination of discriminant learning and label-error modeling will prove very efficient on all the data sets which have been used here.
 semi-supervised learning and learning in the presence of label noise in Sect. 2 , we present the baseline generative CEM algorithm for semi-supervised learning (Sect. 4 ). In Sect. 5 , we introduce our discriminant semi-supervised algorithm using the CEM framework. In Sect. 6 , we describe the label-error model for semi-supervised learning and its use for generative and discriminative models. We fi-nally show that discriminant semi-supervised learning performs well on different real size data sets and that modeling the label-error process is a valuable addi-tion to the semi-supervised models for both generative and discriminative cases (Sect. 7 ). In the appendix, we detail instances of this model first for the generative semi-supervised CEM by considering discrete and real-valued data and then for the discriminant semi-supervised CEM algorithm. 2 Related work In the following, we will review the main approaches for respectively semi-supervised learning and learning in the presence of label noise. 2.1 Learning with partially classified training data A discussion of the respective merits of discriminative and generative approaches to semi-supervised learning and an excellent review of work prior to 2000 in the machine learning community is given by [ 41 ]. We give later a synthetic presen-tation of the work performed by the statistics and the machine learning commu-nities on this subject by distinguishing between generative and discriminative ap-proaches. 2.1.1 Generative approaches Most generative approaches to semi-supervised learning rely on mixture models. In this case, unlabeled data are supposed to be generated from a mixture den-sity, while the mixture components of labeled data are known. Training criterion is usually the data log-likelihood of all (labeled + unlabeled) observations. Once the model parameters are learned, unknown data are classified using the mixture components associated to each class. 2.1.1.1. Algorithmic studies A review of work prior to 1992 in the statistics com-munity may be found in [ 32 ]. Most of these approaches are based on an iterative EM -like algorithm working under the assumption of multivariate normal compo-nents with a common covariance matrix [ 31 ]. Some authors have suggested up-dating procedures for no-normal group conditional densities using for example kernel estimators for the mixture components [ 35 ]. Most papers in machine learn-ing developed the same type of ideas. Miller et al. [ 33 ] considered a mixture model where each class is described by several component densities. Nigam et al. pro-posed a semi-supervised EM algorithm for text classification making use of a naive Bayes estimator for modeling the different densities [ 37 ]. Basu et al. presented a semi-supervised algorithm for clustering using available classified data to first generate seed clusters and then to guide the clustering process [ 9 ]. Chapelle and Zien proposed a semi-supervised method that uses distributional hypotheses on data in order to find decision boundaries laying down in low-density regions [ 16 ]. 2.1.1.2. Theoretical issues There are still very few theoretical contributions for understanding semi-supervised learning and most crucial questions are still open. Some authors attempted to characterize the role and the importance of unlabeled samples for learning. For a mixture of Gaussians in a 2-class classification task, O X  X eill et al. proved that if the Mahalanobis distance between the class cen-troids of two populations is over a fixed threshold, unlabeled data may be helpful for learning [ 38 ]. Using the same distributional assumptions, Castelli and Cover proved that with known observation densities and unknown mixing parameters, the labeled and unlabeled data play a very similar role in reducing the probabil-ity of classification error [ 14 ]. They further showed that for unknown observa-tion densities and known mixing parameters the probability of error converges exponentially to zero in the number of labeled samples. Another analysis of the value of unlabeled samples by learning from a mixture of densities was carried out by [ 39 ] within the PAC framework. They reached the same conclusion as [ 14 ], showing that the probability of error decreases exponentially fast in the number of labeled data, while it decreases only as an inverse polynomial in the number of unlabeled observations. Using also the PAC model, Cozman et al. [ 20 ]were more controversial in their conclusions about the role of unlabeled data for semi-supervised learning. They showed via an example that for incorrect models with parameters , for which the data distribution p ( x , y ) does not belong to the family p ( x , y | ) , performance may degrade with unlabeled training data. 2.1.2 Discriminative approaches More recently, some authors have proposed discriminant algorithms for semi-supervised learning. In the following, we distinguish between co-training like methods that consider multi-modal data representations and others that use classi-cal vector representations.
 2.1.2.1. Multi-modal view The co-training paradigm has been proposed by Blum et al. [ 13 ] for training classifiers when examples may be described by two modal-ities assumed conditionally independent given the class variable. Two classifiers are used, one on each modality, operating alternatively as teacher and learner, i.e. tentative labels estimated by the output of a classifier for unlabeled data are used to train the other classifier. Collins and Singer presented an interesting extension of the boosting algorithm which incorporates co-training to perform named entity classification [ 19 ]. The work of [ 21 ] also bears similarities with this technique. Muslea et al. proposed to combine active and semi-supervised learning using the multi-modal view framework [ 36 ]. 2.1.2.2. Uni-modal view To the best of our knowledge, the earliest work for dis-criminant semi-supervised learning is the one proposed by Anderson who used together labeled and unlabeled observations for training a logistic regression clas-sifier [ 6 ]. As learning criterion, he proposed to maximize the joint likelihood of labeled and unlabeled data. Recently, Joachims proposed a transductive version of SVMs [ 24 ], where each new unlabeled example is used to modify the param-eters of an existing classifier. Bennett and Demiriz found small improvements on UCI data sets with this type of transduction [ 11 ]. Roth and Steinhage proposed a framework for semi-supervised learning which extends classical linear discrim-inant analysis (LDA) to kernel discriminant analysis (KDA) [ 40 ]. Szummer and Jaakkola presented a kernel expansion algorithm which augments the represen-tation of examples using a Fisher score vector estimated over both labeled and unlabeled data [ 43 ]. Using these new feature vectors, they derived Bayes optimal decision boundary from the maximum entropy and maximum likelihood frame-works. The runtime of their algorithm is proportional to the product of the number of labeled observations and the total number of examples. Jaakkola et al. proposed a general framework for classification based on maximum entropy discrimina-tion which extends the semi-supervised paradigm [ 23 ]. Recently, different authors have studied the geometric structure of the data sets for partially labeled classifi-cation. For example, Zhu et al. have presented a Gaussian random field model for semi-supervised learning [ 48 ] and Belkin and Niyogi have proposed approaches based on the geometry of manifolds [ 10 ]. 2.2 Learning with misclassified training data There are different cases where the actual labels of training observations may be subject to error. Practical applications, like remote-sensing, have motivated in the early 1970s an intensive research in the pattern recognition community on the problem of learning in the presence of label noise. These studies distinguished be-tween random and no-random imperfect supervisions, for the latter the probability of misclassification of an observation does depend on its feature vector, while it does not for the former. 2.2.1 Random imperfect supervision Random imperfect supervision may arise in the case where the labeling of the training data is made automatically on the basis of a machine output, for example in blood test results [ 1 ]. In such cases, the error on the class label does not de-pend on the input x . McLachlan studied conditional error rates using their asymp-totic expansions for the case where one group does not get mislabeled sample [ 30 ]. Chittineni obtained error bounds on the performance of Bayes and nearest neighbor classifiers trained with imperfect labeled observations [ 18 ]. Chhikara and McKoen proved that training classifiers by ignoring mislabeling in the train-ing set can degrade classification performance [ 17 ]. Using the maximum likeli-hood principle, [ 25 ] derived the likelihood estimation of parameters for two group multivariate normal mixtures with a common covariance matrix in a 2-classes classification problem. Under this framework, Krishnan studied the efficiency of an imperfect supervision scheme compared to a perfect supervision case [ 26 ]. This efficiency, called the Asymptotic Relative Efficiency , measures the relative sample sizes required in both the perfect and imperfect supervision cases in or-der to achieve the same classification performance. More recently, Lawrence and Scholkopf proposed an algorithm for constructing a kernel Fisher discriminant from training examples in the presence of label noise [ 28 ]. 2.2.2 No-random imperfect supervision In the same context of medical diagnosis, no-random imperfect supervision would correspond to the case where the classification of patient diseases is carried out by human experts using disease symptoms. Lachenbrunch studied conditional error rates of no-random misclassification models using Monte Carlo methods [ 29 ]. He expressed the probability of mislabeling as a function of the distance between each sample and its group mean. In [ 46 ], Titterington worked out an EM algorithm for estimating the parameters of a logistic-normal distribution [ 2 ]. More recently, Ambroise and Govaert proposed an EM algorithm [ 3 ] to estimate the posterior distribution of the true label class with respect to the incomplete data. sume here that label errors do come from the data acquisition or from a manual labeling process. The noise over labels does come here from the classification algorithm itself and the label-error model will tend to correct these mislabelings. work by iteratively predicting labels for unlabeled data. At each iteration these predicted labels are considered as targets and classifier parameters are learned to predict them. Thus, at each labeling step, the semi-supervised learning system acts as an imperfect supervisor for unlabeled data. As the misclassification of an example does not depend on its feature vector, we propose to model this label-error process using the Random imperfect supervision framework. 3 Notations We suppose that each example belongs to one and only one class and that there are available a set of n labeled examples D l ={ ( x i , y i ) | i = 1 ,..., n } and a set of m unlabeled examples D u ={ x i | i = n + 1 ,..., n + m } . A classifier is to be trained on the basis of these n + m , d -dimensional feature vectors x  X  R d .For and the indicator vector class associated to x i . During training, unlabeled samples will be given tentative labels. Let y and t de-observation x estimated with a learning system.
 denotes the vector of all unknown parameters for a generative model,  X  k being the parameters of the k th class.
 drawn from a mixture of c groups in proportions  X  1 ,..., X  c , respectively, where G tion,  X  k the specific classifier parameters corresponding to class k and B the set of all classifier parameters.
 set D u into c classes. C will denote such a partition. C ( j ) is then a partition of D u found at iteration j and C ( j ) k the k th class of C ( j ) . 4 Background: CEM and generative semi-supervised learning CEM is a general clustering algorithm which relies on a mixture density model of the data. Both CEM and its variants have always been used in this generative setting. We first describe later the baseline CEM algorithm and then its extension for semi-supervised learning. 4.1 Classification maximum likelihood estimation and classification expectation-maximization algorithm Symons [ 44 ] distinguishes two main approaches to clustering: maximum likeli-hood (ML) and classification maximum likelihood ( CML ). The former optimizes the data likelihood by modeling the component densities, clustering is then per-formed using the estimated densities and Bayes rule. The latter directly optimizes the classification of data into different clusters. For both approaches, samples are supposed to be generated via a mixture density: For CML , each example belongs to exactly one mixture component and the CML criterion is the complete data log-likelihood: Here, the class indicator vectors  X  t of unlabeled samples are model parameters and have to be estimated together with the parameters . CEM is a general frame-work which encompasses most CML clustering algorithms. It is a general algo-rithm which aims to find the mixing weights of classes  X  k , the parameters  X  k of density functions modeling the data and the clusters C k under the CML approach [ 15 ]. This algorithm can be seen as a classification version of the EM algorithm: it contains an additional C-step (Algorithm 1), where each unlabeled example x i is assigned to one and only one component of the mixture (between the Expecta-tion and the Maximization steps of the EM algorithm).
 Algorithm 1 (Generative unsupervised and semi-supervised CEM ) random and the f k (. ,  X  ( 0 ) k ) are estimated on the corresponding classes. For semi-the labeled data D l , and C ( 0 ) is defined accordingly. j th iteration, j  X  0 :  X  E-step : Estimate the posterior class probability that each unlabeled exam- X  C-step : Assign each x i  X  D u to the cluster C ( j + 1 ) k with maximal posterior  X  M-step : Estimate the new parameters ( j + 1 ) which maximize 4.2 Generative semi-supervised CEM algorithm McLachlan has extended CML and CEM for generative algorithms to the case where both labeled and unlabeled data are used for learning [ 32 , p. 39]. In this context, the indicator vector class for labeled data are known whereas they are estimated for unlabeled data. The complete-data log-likelihood criterion ( 4 ) becomes:
L In this expression, the first summation is over the labeled examples, and the second one over the unlabeled samples.
 maximizing ( 5 ) instead of ( 4 ). Algorithm 1 describes both the unsupervised and semi-supervised versions of the CEM algorithm. For the former the initial parti-The three steps are then iterated until convergence. For the latter, initial density unlabeled data are then estimated as in the classical CEM ( C-step ) while they are kept fixed, in all iterations, to their known value for labeled data. For unlabeled data, the class conditional probabilities are estimated ( E-step ) and a classifica-tion decision is then made according to Bayes rule ( C-step ). In both cases, the algorithm will converge to a local maximum of L CML ( C ( j + 1 ) , ( j ) ) for unsuper-vised learning and L c ( C ( j + 1 ) , ( j ) ) for semi-supervised learning. L The first term corresponds to labeled data (the same as for CML ), the second one is the likelihood of unlabeled data instead of the classification likelihood for CML . stead of the usual generative one. This formulation will allow to handle the semi-supervised learning problem with a whole set of discriminant techniques [ 47 ], and will lead to a new family of discriminant semi-supervised algorithms.
 supervised techniques, both generative and discriminative. This is the reason why we have introduced this framework here. Usually, generative methods are devel-oped under the ML framework and discriminative methods using a risk minimiza-tion setting. 5 Discriminant semi-supervised CEM algorithm The generative approach to semi-supervised learning indirectly computes poste-riors p ( y = k | x ,) via conditional density estimation. This is known to lead to poor estimates for high dimensions or when only few data are labeled which is exactly the interesting case for semi-supervised learning.
 directly estimate posterior probabilities. The discriminant semi-supervised algo-rithm described later makes use of a discriminant classifier. It explicitly makes the hypothesis that the outputs of this classifier estimate the class posteriors. of posterior probabilities: As no assumption is made on the distributional nature of data, maximizing L c is equivalent to the maximization of a the following criterion L c [ 32 , p. 261]: L ( C , B ) = Algorithm 2 (Semi-supervised discriminant CEM ) corresponding initial discriminant function estimates. j th iteration, j  X  0 :  X  C-step : Assign each x i  X  D u to the cluster C ( j + 1 ) k with maximal posterior  X  M-step : Find new parameters B ( j + 1 ) which maximize Algorithm 2 describes the semi-supervised discriminant CEM algorithm for opti-mizing ( 8 ). A discriminant classifier is first trained on the labeled data set D l .The outputs of the classifier G k (. ,  X  k ) are then used to estimate the posteriors for un-labeled data. Each unlabeled example x i is assigned to the class with maximum posterior. Indicator variables t ki are defined accordingly ( C step). Using this set of labels the classifier is trained to optimize L c ( M step). These new estimators are then used in the next iteration so as to provide new posterior estimates and therefore new labels for the unlabeled data. Note that in this algorithm, labels for labeled data are always kept fixed to their true value since they are known. The E-step is trivial here since the posterior estimates are given by the classifier out-puts, it does not explicitly appear in Algorithm 2. This algorithm iterates the two steps C and M until it converges to a local maximum of L c ( C , B ) ( 8 ). the true class and the class posterior estimates which is a classical training crite-rion for supervised learning. The second term in ( 8 )isthe CML criterion discussed previously. For both labeled and unlabeled data, Algorithm 2 maximizes simulta-neously the cross-entropy for labeled data and the CML for unlabeled data. When cross-entropy (initialization step only). When there is only unlabeled data, this is a clustering algorithm where clusters are directly attributed via discriminant func-tions instead of density estimation as this is usually the case.
 tion (see Sect. 6.3 ). It can be used with any discriminant classifier provided its outputs can be interpreted as posterior class probabilities and it can be trained to optimize criterion ( 8 ). Many different classifier families fulfill these two require-ments.
 label-error model. The latter will help improve the predictive labeling of unlabeled samples during the iterations of the semi-supervised CEM algorithms. 6 Learning a label-error model on tentative labels for semi-supervised classification For both generative and discriminant semi-supervised CEM , tentative labels for beled data and iteratively improve tentative labels for unlabeled data by optimizing the classification likelihood. With respect to a classical supervised classification problem where all the labels were known, the algorithm solves at each iteration a classification problem for which some of the labels X  X mong those computed for unlabeled data X  X re wrong. Such errors on tentative labels are inherent to semi-supervised learning algorithms. We will make here the hypothesis that the errors of the classifier on these tentative labels come from a stochastic process. If we knew this process, we could try to reduce the classifier errors at each iteration, which should improve the classification performance.
 tive labels during CEM iterations simultaneously with the classifier parameters [ 4 ]. This error model will apply only to tentative labels computed for unlabeled data. In our setting, labels of labeled data are known and considered as correct, so that they will not be changed throughout the algorithm iterations. After introducing some notations, we show in Sects. 6.1 and 6.2 how this model could be estimated for re-spectively generative and discriminant semi-supervised learning. This will lead to two new semi-supervised algorithms, which are respectively enhanced versions of Algorithm 1 (generative semi-supervised learning) and Algorithm 2 (discriminant semi-supervised learning). From now on,  X  y and  X  t will denote the computed class label and the indicator class vector of an unlabeled observation estimated with the label-error model, while y and t denote as previously, the estimations made by the classifier before applying the label-error model.
 Which are subject to the constraint: Consider now the joint probability of an example and its corrected label: Assume further that: Using ( 12 )and( 9 ), ( 11 ) can be rewritten: p ( x 6.1 Updating the parameters of a generative model using a label-error model for unlabeled data The label-error model will attempt to correct imperfect labels t estimated with for unlabeled data. The complete-data log-likelihood is then computed with respect to the set of labeled data D l and to the set of unlabeled data D u with their corrected  X  t , i  X  X  n + 1 ,..., n + m } . For this specification, the complete-data log-likelihood writes:
L c ( C ,,) = When introducing the probability density functions f k and the label-error model ( 9 ) into the training CML criterion ( 14 ), from ( 13 ) the latter writes: rameters for the misclassification model and for the generative model estimated at iteration j of the algorithm. The learning criterion ( 15 ) is a function of C , and . As in Sect. 4.2 , we adopt an iterative approach for its maximization. Parame-ters are first initialized on the labeled data set D l . The three steps (E, C, M), in Algorithm 3, are then iterated until the convergence to a local maximum of L c .At each iteration in the E and the C steps, the error model modifies the assignment Algorithm 3 (Generative semi-supervised CEM with label-error modeling) ing to labeled data D l ,  X  ( 0 ) kh are initialized at random between 0 and 1 j th iteration, j  X  0 :  X  E-step : Estimate the joint class probability of each x i  X  D u and its cor- X  C-step : Assign each x i  X  D u to the cluster C ( j + 1 ) k with maximal joint prob- X  M-step : Estimate the new parameters ( ( j + 1 ) , ( j + 1 ) ) which maximize L c of unlabeled examples. All model parameters, including those of the label-error model, are modified in the M-step . The new value of the  X  will depend on the old values and on the current estimated conditional densities. We will provide a proof of convergence of the algorithm in Sect. 6.3 .
 of this algorithm for two particular instances (a discrete naive Bayes classifier and a Gaussian model) which have been used in our experiments. 6.2 Updating the parameters of a discriminant model using a label-error model for unlabeled data We show now how the error model is incorporated in the discriminant CEM algorithm (Algorithm 2). Following Sect. 5 , the modified complete-data log-likelihood in the discriminative case using the corrected labels  X  t for the unlabeled data writes: Using Bayes rule and ( 13 ), we get: From ( 19 ), ( 18 ) becomes: The learning criterion ( 20 ) is a function of , C and B . In this case, parameters B are first initialized by training the classifier on the labeled data set D l .Twosteps are then iterated until the convergence of L c (Algorithm 4). In the first step, the classifier is considered as an imperfect supervisor for unlabeled data. To make a decision over the class of an unlabeled observation x , the outputs of the classifier, G the imperfect labels obtained in the previous step as well as the labeled data. At fier parameters B and the error model will be reached. As for Algorithm 2, the E-step directly follows from the estimates G k (. ,  X  k ) and does not appear explicitly in Algorithm 4.
 later. Reestimation formulae for the special case of a logistic classifier, used for the experiments, are provided in the appendix.
 Algorithm 4 (Discriminant semi-supervised learning with label-error modeling) random between 0 and 1. Let C ( 0 ) be the initial partition obtained from this model on D u j th iteration, j  X  0 :  X  C-step : Apply L R B ( j ) on D u , estimate the imperfect class posterior prob- X  M-step : 6.3 Convergence Semi-supervised Algorithms 1 X 4, converge to a local optimum of their objective function. We show later a proof for Algorithm 4. The same proof applies for the other algorithms.
 Lemma 6.1 The modified CML criterion, L c , increases for every sequence (
C ( j ) , B ( j ) , ( j ) ) of the Algorithm 4 and the sequence L verges to a stationary point.
 Proof We first show that L c is increasing.
 Finally, as there is a finite number of partitions of the example into c -classes, the converges to a stationary value. This is a local optimum of the objective function L . 7 Experimental results We will now describe and analyze results obtained with the semi-supervised al-gorithms on data sets with different characteristics. We first describe these data sets and the evaluation measures we have been using. After that, we present and discuss a series of experiments. 7.1 Data sets In our experiments we used the Emailspan and Mushroom collections from the UCI repository 1 [ 12 ], the 7sectors data set from the CMU Web-kB project 2 as well as the Computation and Language (Cmp lg) collection of TIP-STER SUMMAC 3 for text summarization. All collections but 7sectors corre-spond to 2-classes classification problems. Table 1 summarizes the characteristics of these data sets.
 hierarchical order. We labeled each document in this collection with its ini-tial parent class label, namely basic, energy, financial, health, transportation, technology and utilities . In order to test the al-gorithms on many different classification problems, we considered n ( n  X  1 )/ 2 binary classification problems where each class is classified against any of the other classes. Documents are tokenized by removing html tags as well as words on a stop list. Low document frequency words (occurring in less than three docu-ments) are also removed. Stemming is then performed using the Porter algorithm. For each training set, log-odds ratio feature selection [ 34 ] is used to prune the vo-cabulary to 3,000 words. Documents are represented in the vector space by their term frequency. tion is used for text summarization in the Summac competition [ 42 ]. The aim is to propose a summary for each article in the collection. For this, we adopt the text-span extraction paradigm which amounts at selecting a subset of represen-tative document sentences. This is a classification problem where sentences are to be classified as relevant or non-relevant for the extracted summary. There are 28,985 sentences in the collection. We represent each sentence using a continu-ous version of the features proposed by [ 27 ]. This characterization has given good results in previous work [ 5 ]. Each sentence i , with length l ( i ) is characterized by x = ( X  i normalized number of acronyms (such as  X  X SA X ,  X  X ASA X ,  X  X BM X , etc.) in i ,  X  i 4 is the position indicator feature of i in the document it belongs to (beginning, mid-dle or end of the document) and  X  i 5 is the normalized number of common terms between a generic query q and the sentence i . In our experiments, query q is gen-erated using the title and the most frequent words of each document. This is the usual setting for generic summarization.
 the UCI collection. The Emailspan database consists in 4,601 e-mails gath-ered manually from personal e-mails. The class label is spam or no-spam and there are 57 quantitative predictors.
 to 23 species of mushrooms. Each observation is identified as edible or poisonous and characterized with 22 qualitative attributes. We removed 2,480 examples with missing attributes from this database. 7.2 Evaluation measures For the two UCI data sets there is approximately the same proportion of examples for each class (Table 1 ). We used as performance criterion the percentage of good classification (PGC) defined as: For text summarization, we followed the SUMMAC evaluation by using a 10% compression ratio with respect to the original document. Hence, for each docu-ment in the test set, we have formed its summary by selecting the top 10% sen-evaluation, we compared these sentences with the desired summary of each doc-ument. The desired summaries were generated from the abstract of each article using the text-span alignment method described in [ 8 ]. They too consist of 10% sentences from the original document so that desired and computed summaries have the same number of sentences. Since the collection is not well balanced be-tween positive and negative examples, PGC is meaningless. For the evaluation, we used the average precision (AP) measure defined later.

AP = Note that in this particular case, precision and recall are equal.
 even point measure. 4 7.3 Experiments For the experiments, we have proceeded as follows. For the generative algorithms we used a naive Bayes and a Gaussian mixture classifier for respectively discrete and continuous data. For the discriminative approach, we used a logistic classifier. We also performed tests with more complex non-linear discriminant classifiers. Since the observed results and behavior were similar to the simple logistic system, the latter was adopted for the experiments here. The different algorithms (base-lines, generative and discriminant with and without error model) are compared on different data sets. We also used the three classifiers (naive Bayes, Gaussian, logis-tic) as baseline fully supervised models by training them on the same proportion of labeled data as for the semi-supervised methods. We also provide a comparison between CEM and a classical generative EM semi-supervised algorithm [ 37 ], and between our discriminant algorithms and the transductive SVM algorithm used for semi-supervised learning in [ 24 ]. 7.4 Results 7.4.1 Generative classifiers Bayes as baseline and three semi-supervised methods: the classical generative EM semi-supervised [ 37 ], the generative semi-supervised CEM (Algorithm 1) and the generative semi-supervised CEM with label-error modeling (Algorithm 3). We compare the performance of these three algorithms on 5 cross-validation runs on the Mushroom and 7sectors data sets, using a fixed pro-portion of labeled X  X nlabeled data on the training set for learning. For each data set and each cross-validation run, 25% of the examples are held aside as a test set. Results for the Mushroom and 7sectors data sets appear respectively in Tables 2 and 3.
 training set. In the results described here, this proportion is 5 X 95% for Mushroom and 1 X 99% for 7sectors , but similar conclusions also hold for any other pro-portion. In both cases, the same behavior is observed. Using unlabeled data consid-erably increases the performance ( + 36.4 for Mushrooms and + 8.9 for 7sectors) and the error-label model allows for an additional increase compared to generative semi-supervised CEM ( + 4.6%) for Mushroom and ( + 3.6%) in mean for 7sectors . Note that for 7sectors we used only a very small number of la-beled examples. The performance increase tends to be lower for smaller data sets such as energy or utilities .Forthe Mushroom database, the performance increase with the error model is more than 40% compared to the baseline naive Bayes classifier.
 tion of labeled X  X nlabeled data in the training set is varied. Figures 1 and 2 respec-tively show performance on the test sets for Emailspam and Cmp lg collections as a function of the proportion of labeled data in the training set. On the x -axis, 5% means that 5% of labeled data in the training set were used for training, the 95% re-maining being used as unlabeled training data. Each experiment was carried out on 20 paired trials of randomly selected training-test splits. On the y -axis, each point thus represents the mean performance for the 20 runs and the error bars correspond to the standard deviation for the estimated performance [ 45 ]. For both datasets, at-tribute values are on a continuous scale. For the generative methods, data were then assumed to be drawn from two normal populations. Performance curves (Fig. 1 (top) for Emailspam and Fig. 2 for Cmp lg ) of the generative semi-supervised learning algorithms (generative semi-supervised CEM and gen-erative semi-supervised CEM with label-error modeling) confirm the conclusions obtained on the previous data sets for all labeled X  X nlabeled data proportions. One also may observe that semi-supervised CEM and semi-supervised EM behave similarly and that a significant performance increase is reached for all labeled X  X nlabeled proportions when using the error model.
 7.4.2 Discriminant classifiers The same tests have been performed with three discriminant algorithms: a baseline supervised logistic classifier and two semi-supervised algorithm ( CEM -discriminant (Algorithm 2) and discriminant semi-supervised CEM with label-error modeling (Algorithm 4)). Results are shown respectively in Fig. 1 (bottom) and Fig. 2 for Emailspan and Cmp lg . case, semi-supervised training allows for a considerable performance increase for all labeled X  X nlabeled training data proportions. The label-error model also offers for both data sets and all proportions a significant increase. For example, if we consider the text summarization task (Fig. 2 ), using only 5% of the labeled sen-tences, the discriminant semi-supervised algorithm CEM with error model allows to increase performance by 12% compared to a fully supervised logistic classi-fier trained on 5% labeled data. 40% labeled data are needed to reach the same performance level with the baseline logistic method. At 5% labeled data the error model increases the performance of the discriminant semi-supervised algorithm by about 10%. This increase is lower when the proportion of labeled data is in-creased but remains consequent (about 5% on the average precision for 10% la-beleddatainFig. 2 ). The discriminant semi-supervised algorithm with label-error modeling thus provides an important performance increase compared to both the discriminant baseline and CEM -discriminant especially when there are only few labeled data available for training which is the most interesting situation in semi-supervised learning. 7.4.3 Discriminant versus generative A second observation is that discriminant training clearly outperforms genera-tive training on both data sets for all proportions. In Fig. 1 (bottom) and Fig. 2 , the curve for the best generative model (generative semi-supervised CEM with label error modeling) is below that of the semi-supervised discriminant classi-fiers with and without label-error model. It can be seen that maximal perfor-mance, obtained with 100% labeled training data for supervised learning, can be reached much sooner when using the discriminant semi-supervised CEM with label-error modeling (about 20% labeled data for Emailspan and 50% for Cmp lg ). 7.4.4 EM versus CEM EM and CEM do have a similar behavior in the context of generative semi-supervised learning. Experiments performed with a generative semi-supervised EM algorithm [ 37 ] and the generative semi-supervised CEM led to similar results. Table 2 gives the performance of EM and CEM for the mushroom dataset and they are indeed similar. The major interest and advantage of CEM here is that, besides the usual generative approach it also provides a natural framework for in-troducing discriminant semi-supervised algorithms which is not the case for EM . CML and CEM thus allow to express a whole family of generative and discrimi-nant semi-supervised methods. As seen in Sect. 5 , semi-supervised discriminant CEM (Algorithm 2) optimizes simultaneously cross-entropy for labeled data and classification likelihood for unlabeled data, thus providing a link between classifi-cation and clustering. 7.4.5 Why a simple logistic classifier for semi-supervised learning? All the algorithms introduced here can be instantiated with different density estimator functions or discriminant classifiers. We also performed tests with different discriminant classifiers instead of the logistic classifier used here. Linear regression gave slightly lower results. More complex non-linear methods did not improved performance with respect to logistic regression. It seems natural for semi-supervised learning to choose low variance classifiers like those used in the experiments reported here. There are many arguments for that. When using small amounts of labeled data, it is likely that complex methods will not be able to cap-ture the intrinsic non-linearity of data by lack of information. Complex classifiers could then produce high variance error terms. Also for most real problems, the decision frontier for small amounts of labeled data is likely to be nearly linear and using unlabeled data will not bring any evidence for learning non-linear semi-supervised classification is still to be investigated. We have also compared the CEM algorithms introduced here with with the transductive SVM method [ 24 ]. Average performance for this transductive SVM is similar to the semi-supervised discriminant logistic with no error-model introduced in this paper (Fig. 1 (bottom) and Fig. 2 ). However, in our experiments, the variance for the different runs was much higher for this transductive SVM than for the semi-supervised logistic classifier especially when only few labeled data are used which is the interesting case. There is no fundamental reason for that, but this has been observed in all our experiments and for different parameter settings of the SVM algorithm. In all the experiments, transductive SVM is below the discriminant and label-error model algorithm.
 and discriminant methods, semi-supervised training allowed for an important performance increase compared to supervised training on the same amount of labeled data. Discriminant training with the logistic classifier clearly outper-forms generative training (either naive Bayes or Gaussian mixture). Training with a label-error model provides a significant additional performance increase in all cases for the aforementioned experiments. The combination of dis-criminant CEM and label-error training appears as a powerful semi-supervised method.
 of data sets and experimental conditions. Our results are algorithmic and exper-imental. There remains to develop a theoretical framework for semi-supervised learning which could help explaining the observed behavior. Up to now, the few theoretical results we know of have been obtained under very strong hypothesis and cannot be used as a basis for explaining the observed phenomena or analyzing the value of unlabeled data on practical datasets. 8Conclusion We have proposed a new family of discriminant algorithms for semi-supervised learning. These methods have been introduced using the CML  X  CEM formal-ism. This extension to the classical generative setting of CML  X  CEM shows that CML  X  CEM can be used as a general framework for describing both genera-tive and discriminant semi-supervised methods. We have also proposed to learn a label-error model for the tentative labels iteratively computed for unlabeled data.
 different datasets. The combination of discriminant training and error modeling proved particularly efficient. In all cases, empirical evidence clearly supports the validity of the proposed ideas. It is remarkable that the same algorithm behavior has been observed for all data sets and experimental conditions. In particular, the respective ranking of the different methods is similar in all cases. This provides additional support for the conclusions which can be drawn from this set of ex-periments. Additionally, the proposed models are simple and easy to implement. Of course, there remain many open problems for assessing when and how semi-supervised learning should be used and for predicting or measuring the importance of unlabeled samples for learning.
 Appendix A Reestimation of the label-error parameters B Reestimation for the generative CEM model with a label-error model (Algorithm 3) B.1 Discrete-valued data: naive Bayes model B.2 Real-valued data: Normal case C Logistic reestimation formulae (Algorithms 2 and 4) References
