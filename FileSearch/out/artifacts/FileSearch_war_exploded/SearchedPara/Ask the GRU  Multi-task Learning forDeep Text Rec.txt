 In a variety of application domains the content to be recom-mended to users is associated with text. This includes re-search papers, movies with associated plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can be extended naturally to leverage text by employing an explicit mapping from text to factors. This enables recommendations for new, unseen content, and may generalize better, since the factors for all items are pro-duced by a compactly-parametrized model. Previous work has used topic models or averages of word embeddings for this mapping. In this paper we present a method lever-aging deep recurrent neural networks to encode the text sequence into a latent vector, specifically gated recurrent units (GRUs) trained end-to-end on the collaborative filter-ing task. For the task of scientific paper recommendation, this yields models with significantly higher accuracy. In cold-start scenarios, we beat the previous state-of-the-art, all of which ignore word order. Performance is further improved by multi-task learning, where the text encoder network is trained for a combination of content recommendation and item metadata prediction. This regularizes the collabora-tive filtering model, ameliorating the problem of sparsity of the observed rating matrix.
 Recommender Systems; Deep Learning; Neural Networks; Cold Start; Multi-task Learning
Text recommendation is an important problem that has the potential to drive significant profits for e-businesses thr-ough increased user engagement. Examples of text recom-mendations include recommending blogs, social media posts [1 ], news articles [2 , 3], movies (based on plot summaries), products (based on reviews) [4] and research papers [ 5].
Methods for recommending text items can be broadly clas-sified into collaborative filtering (CF), content-based, and hybrid methods. Collaborative filtering [6] methods use the user-item rating matrix to construct user and item profiles from past ratings. Classical examples of this include ma-trix factorization methods [6, 7] which completely ignore text information and rely solely on the rating matrix. Such methods suffer from the cold-start problem  X  how to rank unseen or unrated items  X  which is ubiquitous in most do-mains. Content-based methods [8 , 9], on the other hand, use the item text or attributes, and make recommendations based on similarity between such attributes, ignoring data from other users. Such methods can make recommenda-tions for new items but are limited in their performance since they cannot employ similarity between user preferences [5, 10 , 11 ]. Hybrid recommendation systems seek the best of both worlds, by leveraging both item content and user-item ratings [5 , 10 , 12 , 13 , 14 ]. Hybrid recommendation methods that consume item text for recommendation often ignore word order [ 5, 13 , 14 , 15], and either use bags-of-words as features for a linear model [14 , 16 ] or define an unsupervised learning objective on the text such as a topic model [5 , 15 ]. Such methods are unable to fully leverage the text content, being limited to bag-of-words sufficient statistics [17], and furthermore unsupervised learning is unlikely to focus on the aspects of text relevant for content recommendation.
In this paper we present a method leveraging recurrent neural networks (RNNs) [ 18] to represent text items for col-laborative filtering. In recent years, RNNs have provided substantial performance gains in a variety of natural lan-guage processing applications such as language modeling [19] and machine translation [20]. RNNs have a number of note-worthy characteristics: (1) they are sensitive to word order, (2) they do not require hand-engineered features, (3) it is easy to leverage large unlabeled datasets, by pretraining the RNN parameters with unsupervised language modeling ob-jectives [ 47 ], (4) RNN computation can be parallelized on a GPU, and (5) the RNN applies naturally in the cold-start scenario, as a feature extractor, whenever we have text as-sociated with new items.

Due to the extreme data sparsity of content recommenda-tion datasets [ 21 ], regularization is also an important con-sideration. This is particularly important for deep models such as RNNs, since these high-capacity models are prone to overfitting. Existing hybrid methods have used unsuper-vised learning objectives on text content to regularize the parameters of the recommendation model [4, 22 , 23 ]. How-ever, since we consume the text directly as an input for prediction, we can not use this approach. Instead, we pro-vide regularization by performing multi-task learning com-bining collaborative filtering with a simple side task: pre-dicting item meta-data such as genres or item tags. Here, the network producing vector representations for items directly from their text content is shared for both tag prediction and recommendation tasks. This allows us to make predictions in cold-start conditions, while providing regularization for the recommendation model.

We evaluate our recurrent neural network approach on the task of scientific paper recommendation using two publicly available datasets, where items are associated with text ab-stracts [ 5, 13 ]. We find that the RNN-based models yield up to 34% relative-improvement in Recall@50 for cold-start recommendation over collaborative topic regression (CTR) approach of Wang and Blei [5 ] and a word-embedding based model model [24 ], while giving competitive performance for warm-start recommendation. We also note that a simple linear model that represents documents using an average of word embeddings trained in a completely supervised fash-ion [ 24 ], obtains competitive results to CTR. Finally, we find that multi-task learning improves the performance of all of the models significantly, including the baselines.
This paper focuses on the task of recommending items as-sociated with text content. The j -th text item is a sequence of n j word tokens, X j = ( w 1 ,w 2 ,...,w n j ) where each token is one of V words from a vocabulary. Additionally, the text items may be associated with multiple tags (user or author provided). If item j  X  [ N d ] has tag l  X  [ N t ] then we denote it by t jl = 1 and 0 otherwise.

There are N u users who have liked/rated/saved some of the text items. The rating provided by user i on item j is denoted by r ij . We consider the implicit feedback [25, 26 ] setting, where we only observe whether a person has viewed or liked an item and do not observe explicit ratings. r ij if user i liked item j and 0 otherwise. Denote the user-item matrix of likes by R . Let R + i denote the set of all items liked by user i and R  X  i denote the remaining items.

The recommendation problem is to find for each user i a personalized ranking of all unrated items, j  X  R  X  i , given the text of the items { X j } , the matrix of users X  previous likes { R i } and the tagging information of the items { t il } .
The methods we consider will often represent users, items, tags and words by K -dimensional vectors  X  u i ,  X  v j  X 
R K , respectively. We will refer to such vectors as embed-dings . All vectors are treated as column vectors.  X  ( . ) will denote the sigmoid function,  X  ( x ) = 1 1+ e  X  x . Latent factor models [ 6] for content recommendation learn K dimensional vector embeddings of items and users: b ,b j are user and item specific biases, and  X  u i is the vector embedding for user i and  X  v j is the embedding of item j .
A simple method for learning the model parameters,  X   X   X  = { b ,b j ,  X  u,  X  v } , is to specify a cost function and perform stochas-tic gradient descent. For implicit feedback, an unobserved rating might indicate that either the user does not like the item or the user has never seen the item. In such cases, it is common to use a weighted regularized squared loss [5 , 25 ]: Often, one uses c ui = a for observed items and c ui = b for unobserved items, with b a [5, 13], signifying the uncer-tainity in the unobserved ratings.  X (  X   X   X  ) is a regularization on the parameters, for example in PMF [7 ] the embeddings are assigned Gaussian priors, which leads to a ` 2 regulariza-tion. Some implicit feedback recommendation systems use a ranking-based loss instead [24, 26 ]. The methods we pro-pose can be trained with any differentiable cost function. We will use a weighted squared loss in our experiments to be consistent with the baselines [5].
In many applications, the factorization (1 ) is unusable, since it suffers from the cold-start problem [11 , 14 ]: new or unseen items can not be recommended to users because we do not have an associated embedding. This has lead to in-creased interest in hybrid CF methods which can leverage additional information, such as item content, to make cold-start recommendations. In some cases, we may also face a cold-start problem for new users. Though we do not con-sider this case, the techniques of this paper can be extended naturally to accommodate it whenever we have text content associated with users. We consider: Where f (  X  ) is a vector-valued function of the item X  X  text. For differentiable f (  X  ), (3 ) can also be trained using (2 ). Throughout the paper, we will refer to f (  X  ) as an encoder . Existing hybrid CF methods [11, 16 , 27 , 28 ] which use item metadata take this form. In such cases, f ( . ) is a linear function of manually extracted item features. For exam-ple, Agarwal and Chen [16 ], Gantner et al. [29 ] incorporate side information through a linear regression based formu-lation on metadata like category, user X  X  age, location, etc. Rendle [28 ] proposed a more general framework for incorpo-rating higher order interactions among features in a factor model. Refer to Shi et al. [27], and the references therein, for a recent review on such hybrid CF methods.

Our experiments compare to collaborative topic regression (CTR) [ 5], a state-of-the-art technique that simultaneously factorizes the item-word count matrix (through probabilistic topic modeling) and the user-item rating matrix (through a latent factor model). By learning low-dimensional (topical) representations of items, CTR is able to provide recommen-dations to unseen items.
Typical CF datasets are highly sparse, and thus it is im-portant to leverage all available training signals [21]. In many applications, it is useful to perform multi-task learn-ing [30 ] that combines CF and auxiliary tasks, where a shared feature representation for items (or users) is used for all tasks. Collective matrix factorization [31 ] jointly fac-torizes multiple observation matrices with shared entities for relational learning. Ma et al. [32 ] seek to predict side information associated with users. Finally, McAuley and Leskovec [ 4] used topic models and Almahairi et al. [23] used language models on review text.
In many applications, text items are associated with tags, including research papers with keywords, news articles with user or editor provided labels, social media posts with hash-tags, movies with genres, etc. These can be used as features X j in (3) [ 16, 28 ]. However, there are considerable draw-backs to this approach. First, tags are often assigned by users, which may lead to a cold-start problem [33], since new items have no annotation. Moreover, tags can be noisy, especially if they are user-assigned, or too general [3].
While tag annotation may be unreliable and incomplete as input features, encouraging items X  representations to be pre-dictive of these tags can yield useful regularization for the CF problem. Besides providing regularization, this multi-task learning approach is especially useful in cold-start sce-narios, since the tags are only used at train time and hence need not be available at test time. In Section 3.3 we employ this approach.
In our work, we represent the item-to-embedding mapping f (  X  ) using a deep neural network. See [ 34 ] for a comprehen-sive overview of deep learning methods. We provide here a brief review of deep learning for recommendation systems.
Neural networks have received limited attention from the recommendation systems community. [35] used restricted Boltzmann machines as one of the component models to tackle the Netflix challenge. Recently, [36 , 37 ] proposed de-noising auto-encoder based models for collaborative filter-ing which are trained to denoise corrupted versions of entire sparse vectors of user-item likes or item-user likes (i.e. rows or columns of the R matrix). However, these models are un-able to handle the cold-start problem. Wang et al. [13 ] ad-dresses this by incorporating a bag-of-words autoencoder in the model within a Bayesian framework. Elkahky et al. [38] proposed to use neural networks on manually extracted user and item feature representations for content based multi-domain recommendation. Dziugaite and Roy [39] proposed to use a neural network to learn the similarity function be-tween user and item latent factors. Van den Oord et al. [40 ], Wang and Wang [ 41 ] developed music recommender systems which use features extracted from the music audio using convolutional neural networks (CNN) or deep belief networks. However, these methods process the user-item rating matrix in isolation from the content information and thus are unable to exploit the direct interaction between item content and ratings [13 ]. Weston et al. [42 ] proposed a CNN based model to predict hashtags on social media posts and found the learned representations to also be useful for document recommendation. Recently, He and McAuley [43 ] used image-features from a separately trained CNN to im-prove product recommendation and tackle cold-start. Alma-hairi et al. [ 23] used neural network based language models [19 , 44 ] on review text to regularize the latent factors for product recommendation, as opposed to using topic mod-els, as in McAuley and Leskovec [4 ]. They found that RNN based language models perform poorly as regularizers and word embedding models Mikolov et al. [44] perform better.
This section presents neural network-based encoders for explicitly mapping an item X  X  text content to a vector of la-tent factors. This allows us to perform cold-start prediction on new items. In addition, since the vector representations for items are tied together by a shared parametric model, we may be able to generalize better from limited data.
As is standard in deep learning approaches to NLP, our encoders first map input text X j = ( w 1 ,w 2 ,...,w n j quence of K w -dimensional embeddings [44], ( e 1 ,e 2 ,...,e using a lookup table with one vector for every word in our vocabulary. Then, we define a transformation that collapses the sequence of embeddings to a single vector, g ( X j ).
In all of our models, we maintain a separate item-specific embedding  X  v j , which helps capture user behavior that can-not be modeled by content alone [5 ]. Thus, we set the final document representation as: For cold-start prediction, there is no training data to esti-mate the item-specific embedding and we set  X  v j = 0 [ 5, 13].
A simple order-insensitive encoder of the document text can be obtained by averaging word embeddings: This corresponds exactly to a linear model on a bag-of-words representation for the document. However, using the repre-sentation ( 5) is useful because the word embeddings can be pre-trained, in an unsupervised manner, on a large corpus [45]. Note that ( 5) is similar to the embedding-based model used in Weston et al. [42 ] for hastag prediction.

Note that CTR [ 5], described in 2.3 , also operates on bag-of-words sufficient statistics. Here, it does not have an explicit parametric encoder g (  X  ) from text to a vector, but instead defines an implicit mapping via the process of doing posterior inference in the probabilistic topic model.
Bag-of-words models are limited in their capacity, as they cannot distinguish between sentences that have similar un-igram statistics but completely different meanings [17]. As a toy example, consider the research paper abstracts:  X  X his paper is about deep learning, not LDA X  and  X  X his paper is about LDA, not deep learning X . They have the same un-igram statistics but would be of interest to different sets of users. A more powerful model that can exploit the addi-tional information inherent in word order would be expected to recognize this and thus perform better recommendation.
In response, we parametrize g (  X  ) as a recurrent neural net-work (RNN). It reads the text one word at a time and pro-duces a single vector representation. RNNs can provide im-pressive compression of the salient properties of text. For example, accurate translation of an English sentence can be performed by conditioning on a single vector encoding [46].
The extracted item representation is combined with a user embedding, as in ( 3), to get the predicted rating for a user-item pair. The model can then be trained for recommen-dation in a completely supervised manner, using a differen-tiable cost function such as (2). Note that a key difference between this approach and the existing approaches which use item content [5 , 13 ], apart from sensitivity to word or-der, is that we do not define an unsupervised objective (like likelihood of observing bag-of-words under a topic model)
This is paper about deep learning for extracting a text representation. However, our model can benefit from unsupervised data through pre-training of word embeddings [ 44 ] or pre-training of RNN parameters us-ing language models [ 47 ] (our experiments use embeddings).
Traditional RNN architectures suffer from the problem of vanishing and exploding gradients [48 ], rendering optimiza-tion difficult and prohibiting them from learning long-term dependencies. There have been several modifications to the RNN proposed to remedy this problem, of which the most popular are long short-term memory units (LSTMs) [ 49 ] and the more recent gated recurrent units (GRUs) [ 20]. We use GRUs, which are simpler than LSTM, have fewer parame-ters, and give competitive performance to LSTMs [50, 51].
The GRU hidden vector output at step t , h t , for the input sequence X j = ( w 1 ,...,w t ,...,w n j ) is given by: dimension of input word embeddings and K h the number of hidden units in the RNN. denotes element-wise prod-uct. Intuitively, f t (6) acts as a  X  X orget X  (or  X  X eset X ) gate that decides what parts of the previous hidden state to consider or ignore at the current step, c t (7) computes a candidate state for the current time step using the parts of the pre-vious hidden state as dictated by f t , and o t (6) acts as the output (or update) gate which decides what parts of the pre-vious memory to change to the new candidate memory (8 ). All forget and update operations are differentiable to allow learning through backpropagation.

The final architecture, shown in Figure 1, consists of two stacked layers of RNN with GRU hidden units. We use a bi-directional RNN [52 ] at the first layer and feed the con-catenation of the forward and backward hidden states as the input to the second layer. The output of the hidden states of the second layer is pooled to obtain the item content rep-resentation g ( X j ). In our experiments, mean pooling per-forms best. Models that use the final RNN state take much longer to optimize. Following (4), the final item representa-tion is obtained by combining the RNN representation with an item-specific embedding v j .
The encoder f (  X  ) can be used as a generic feature extractor for items. Therefore, we can employ the multi-task learning approach of Section 2.4 . The tags associated with papers can be considered as a (coarse) summary or topics of the items and thus forcing the encoder to be predictive of the tags will provide a useful inductive bias. Consider again the toy example of Figure 1. Observing the tag  X  X NN X  but not  X  X DA X  on the paper, even though the term LDA is present in the text, will force the network to pay attention to the sequence of words  X  X ot LDA X  in order to explain the tags. We define the probability of observing tag l on item j as: P ( t jl = 1) = p jl =  X  ( f ( X j ) T  X  t l ), where  X  for tag l . The cost for predicting the tags is taken as the sum of the weighted binary log likelihood of each tag:
C T (  X   X   X  ) = 1 | T | X where c 0 jl down-weights the cost for predicting the unob-served tags. The final cost is C (  X   X   X  ) =  X C R (  X   X   X  ) + (1  X   X  ) C with C R defined in ( 2), and  X  is a hyperparameter.
It is worth noting the differences between our approach and Almahairi et al. [ 23], who use language modeling on the text as an unsupervised multi-task objective with the item latent factors as the shared parameters. Almahairi et al. [23] found that the increased flexibility offered by the RNN makes it too strong a regularizer leading to worse perfor-mance than simpler bag-of-words models. In contrast, our RNN is trained fully supervised, which forces the item rep-resentations to be discriminative for recommendation and tag prediction. Furthermore, by using the text as an input to g (  X  ) at test time, rather than just for train-time regular-ization, we can alleviate the cold-start problem.
Datasets: We use two datasets made available by Wang et al. [13 ] from CiteULike 1 . CiteULike is an online platform which allows registered users to create personal libraries by saving papers which are of interest to them. The datasets consist of the papers in the users X  libraries (which are treated as  X  X ikes X ), user provided tags on the papers, and the title and abstract of the papers. Similar to Wang and Blei [5], we remove users with less than 5 ratings (since they cannot be evaluated properly) and removed tags that occur on less than 10 articles. Citeulike-a [5 ] consists of 5551 users, 16980 papers and 3629 tags with a total of 204,987 user-item likes. Citeulike-t [5] consists of 5219 users, 25975 papers and 4222 tags with a total of 134,860 user-item likes. Note Citeulike-t is much more sparse (99.90%) than Citeulike-a (99.78%).
Evaluation Methodology: Following, Wang and Blei [5 ], we test the models on held-out user-article likes under both warm-start and cold-start scenarios.

Warm-Start: This is the case of in-matrix prediction, where every test item had at least one like in the training data. For each user we do a 5-fold split of papers from their like history. Papers with less than 5 likes are always kept in the training data, since they cannot be evaluated properly. After learning, we predict ratings across all active test set items and for each user filter out the items in their training set from the ranked list.

Cold-Start: This is the task of predicting user interest in a new paper with no existing likes, based on the text content of the paper. The set of all papers is split into 5 folds. Again, papers with less than 5 likes are always kept in training set. For each fold, we remove all likes on the papers in that fold forming the test-set and keep the other folds as training-set. We fit the models on the training set items for each fold and form predictive per-user ranking of items in the test set.
Evaluation Metric: Accuracy of recommendation from im-plicit feedback is often measured by recall. Precision is not reasonable since the zero ratings may mean that a user ei-ther does not like the article or does not know of it. Thus, we use Recall@M [ 5] and average the per-user metric:
We compare the proposed methods with CTR , which mod-els item content using topic modeling. The approach put forth by CTR [5 ] cannot perform tag-prediction and thus, for a fair comparison, we modify CTR to do tag prediction. This can be viewed as a probabilistic version of collective ma-trix factorization [ 31 ]. Deriving an alternating least squares inference algorithm along the line of [5] is not possible for a sigmoid loss. Thus, for CTR, we formulate tag prediction using a weighted squared loss instead. Learning this model is a straightforward extension of CTR: rather than performing alternating updates on two blocks of parameters, we rotate among three. We call this CTR-MTL . The word embedding-based model with order-insensitive document encoder (sec-tion 3.1 ) is Embed , and the RNN-based model (section 3.2 ) is GRU . The corresponding models trained with multi-task learning are Embed-MTL and GRU-MTL . http://www.citeulike.org/
For CTR, we follow Wang and Blei [5] for setting hyper-parameters. We use latent factor dimension K = 200, regu-larization parameters  X  u = 0 . 01 ,  X  v = 100 and cost weights a = 1 , b = 0 . 01. The same parameters gave good results for CTR-MTL. CTR and CTR-MTL are trained using the EM algorithm, which updates the latent factors using alternating least squares on full data [5 , 25 ]. CTR is sensitive to good pre-processing of the text, which is common in topic mod-eling [5 ]. We use the provided pre-processed text for CTR, which was obtained by removing stop-words and choosing top words based on tf-idf. We initialized CTR with the out-put of a topic model trained only on the text. We used the CTR code provided by the authors.

For the Embed and GRU models, we used word embed-dings of dimension K w = K = 200, in order to be consistent with CTR. For GRU models, the first layer of the RNN has hidden state dimension K h 1 = 400 and the second layer (the output layer) has hidden state dimension K h 2 = 200. We pre-trained the word embeddings using CBOW [44] on a corpus of 440,756 ACM abstracts (including the Citeulike abstracts). Dropout is used at every layer of the network. The probabilities of dropping a dimension are 0.1, 0.5 and 0.3 at the embedding layer, the output of the first layer and the output of the second layer, respectively. We also reg-ularize the user embeddings with weight 0 . 01. We do very mild preprocessing of the text. We replace numbers with a &lt; NUM &gt; token and all words which have a total frequency of less than 5 by &lt; UNK &gt; . Note that we don X  X  remove stop words or frequent words. This leaves a vocabulary of 21,129 words for Citeulike-a and 24,697 words for Citeulike-t .
The models are optimized via stochastic gradient descent, where mini-batch randomly samples a subset of B users and for each user we sample one positive and one negative exam-ple. We set the weights c ij in ( 2) to c ij = 1 +  X  log(1 + where | R i | is the number of items liked by user i , with  X  = 10 , = 1 e  X  8. Unlike Wang and Blei [5 ] we do not weight the cost function differently for positive and nega-tive samples. Since the total number of negative examples is much larger than the positive examples for each user, stochastically sampling only one negative per positive ex-ample implicitly down-weights the negatives. We used a mini-batch size of B = 512 users and used Adam [54 ] for optimization. We run the models for a maximum of 20k mini-batch updates and use early-stopping based on recall on a validation set from the training examples.
Table 1 summarizes Recall@50 for all the models, on the two CiteULike datasets, for both warm-start and cold-start. Figure 3, further shows the variation of Recall@M for differ-ent values of M for the multi-task learning models.
Cold-Start : Recall that for cold-start recommendation, the item-specific embeddings  X  v j in ( 4) are identically equal to zero, and thus the items X  representations depend solely on their text content. We first note the performance of the models without multi-task learning. The GRU model is better than the best score of either the CTR model or the Embed model by 18.36% (relative improvement) on CiteUlike-a and by 32.74% on CiteULike-t. This signifi-cant gain demonstrates that the GRU model is much better at representing the content of the items. Improvements are higher on the CitULike-t dataset because it is much more sparse, and so models which can utilize content appropri-ately give better recommendations. CTR and Embed mod-els perform competitively with each other.

Next, observe that multi-task learning uniformly improves performance for all models. The GRU model X  X  recall im-proves by 7.8% on Citulike-a and by 7.6% on Citeulike-t. This leads to an overall improvement of 19.30% on Citeulike-a and 34.22% on Citeulike-t, over best of the base-lines. Comparatively, improvement for CTR is smaller. This is expected since the Bayesian topic model provides strong regularization for the model parameters. Contrary to this, Embed models also benefits a lot by MTL (up to 8.2%). This is expected since unlike CTR, all the K w  X  V param-eters in the Embed model are free parameters which are trained directly for recommendation, and thus MTL pro-vides necessary regularization.

Warm-Start : Collaborative filtering methods based on matrix factorization [6 ] often perform as well as hybrid meth-ods in the warm-start scenario, due to the flexibility of the item-specific embeddings  X  v j in (4 ) [5 , 41]. Consider again the models trained without MTL. GRU model performs bet-ter than either the CTR or the Embed model, with rel-ative improvement of 8.5% on CiteULike-a and 5.3% on CiteULike-t, over the best of the two models. Multi-task learning again improves performance for all the models. Im-provements are particularly significant for CTR-MTL over CTR (up to 15.8%). Since the tags associated with test items were observed during training, they provide a strong inductive bias leading to improved performance. Interest-ingly, the GRU-MTL model performs slightly better than the CTR-MTL model on one dataset and slightly worse on the other. The first and third plots in Figure 3 demon-strate that the GRU-MTL performs slightly better than the CTR-MTL for smaller M , i.e. more relevant articles are ranked toward the top. To quantify this, we evaluate aver-age reciprocal Hit-Rank@10 [3]. Given a list of M ranked articles for user i , let c 1 ,c 2 ,...,c h denote the ranks of h articles in [ M ] which the user actually liked. HR is then de-correct. GRU-MTL gives HR@10 of 0.098 and CTR-MTL gives HR@10 to be 0.077, which confirms that the top of the list for GRU-MTL contains more relevant recommendations.
Tag Prediction: Although the focus of the models is rec-ommendation, we evaluate the performance of the multi-task models on tag prediction. We again use Recall@50 (defined per article) and evaluate in the cold-start scenario, where there are no tags present for the test article. The GRU and Embed models perform similarly. CTR-MTL is significantly worse, which could be due to our use of the squared loss for training or because hyperparameters were selected for rec-ommendation performance, not tag prediction. We employ a simple, easy-to-implement tool for analyzing RNN predictions, based on Denil et al. [ 55] and Li et al. [ 56]. We produce a heatmap where every input word is associated with its leverage on the output prediction. Suppose that we recommended item j to user i . In other words, suppose that  X  r ij is large. Let E j = ( e j, 1 ,e j, 2 ,...,e j,n j ) be the sequence of word embeddings for item j . Since f (  X  ) is encoded as a neu-produce the heatmap X  X  value for word t , we convert d  X  r a scalar. This is not possible by backpropagation, as d  X  r is not well-defined, since x j,t is a discrete index. Instead we
We employ deep recurrent neural networks to provide vec-tor representations for the text content associated with items in collaborative filtering. This generic text-to-vector map-ping is useful because it can be trained directly with gradient descent and provides opportunities to perform multi-task learning. For scientific paper recommendation, the RNN and multi-task learning both provide complementary perfor-mance improvements. We encourage further use of the tech-nique in a variety of application domains. In future work, we would like to apply deep architectures to users X  data and to explore additional objectives for multi-task learning that employ multiple modalities of inputs, such as movies X  images and text descriptions.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by The Allen Insti-tute for Artificial Intelligence, in part by NSF grant #CNS-0958392, in part by the National Science Foundation (NSF) grant number DMR-1534431, and in part by DARPA un-der agreement number FA8750-13-2-0020. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. [1] Ido Guy, Naama Zwerdling, Inbal Ronen, David [2] Owen Phelan, Kevin McCarthy, and Barry Smyth. Us-[3] Trapit Bansal, Mrinal Das, and Chiranjib Bhat-[4] Julian McAuley and Jure Leskovec. Hidden factors and [5] Chong Wang and David M Blei. Collaborative topic [6] Yehuda Koren, Robert Bell, and Chris Volinsky. Ma-[7] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic [8] Marko Balabanovi  X c and Yoav Shoham. Fab: content-[9] Raymond J Mooney and Loriene Roy. Content-based [10] Chumki Basu, Haym Hirsh, William Cohen, et al. [11] Andrew I Schein, Alexandrin Popescul, Lyle H Ungar, [12] Justin Basilico and Thomas Hofmann. Unifying collab-[13] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Col-[14] Prem Melville, Raymond J Mooney, and Ramadass Na-[15] Prem K Gopalan, Laurent Charlin, and David Blei. [16] Deepak Agarwal and Bee-Chung Chen. Regression-[17] Hanna M Wallach. Topic modeling: beyond bag-of-[18] Paul J Werbos. Backpropagation through time: what [19] Tomas Mikolov, Martin Karafi  X at, Lukas Burget, Jan [20] Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-[21] Robert M Bell and Yehuda Koren. Lessons from the [22] Guang Ling, Michael R Lyu, and Irwin King. Ratings [23] Amjad Almahairi, Kyle Kastner, Kyunghyun Cho, and [24] Jason Weston, Samy Bengio, and Nicolas Usunier. Ws-[25] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collabo-[26] Steffen Rendle, Christoph Freudenthaler, Zeno Gant-[27] Yue Shi, Martha Larson, and Alan Hanjalic. Collabora-[28] Steffen Rendle. Factorization machines. In ICDM , 2010. [29] Zeno Gantner, Lucas Drumond, Christoph Freuden-[30] Rich Caruana. Multitask learning. Machine learning , [31] Ajit P Singh and Geoffrey J Gordon. Relational learn-[32] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin [33] Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl. [34] Yoshua Bengio Ian Goodfellow and Aaron Courville. [35] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hin-[36] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, [37] Yao Wu, Christopher DuBois, Alice X. Zheng, and Mar-[38] Ali Mamdouh Elkahky, Yang Song, and Xiaodong He. [39] Gintare Karolina Dziugaite and Daniel M Roy. Neu-[40] Aaron Van den Oord, Sander Dieleman, and Benjamin [41] Xinxi Wang and Ye Wang. Improving content-based [42] Jason Weston, Sumit Chopra, and Keith Adams. # [43] R. He and J. McAuley. VBPR: visual bayesian person-[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-[45] Ronan Collobert, Jason Weston, L  X eon Bottou, Michael [46] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence [47] Andrew M Dai and Quoc V Le. Semi-supervised se-[48] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. [49] Sepp Hochreiter and J  X  urgen Schmidhuber. Long short-[50] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, [51] Rafal Jozefowicz, Wojciech Zaremba, and Ilya [52] Mike Schuster and Kuldip K Paliwal. Bidirectional [53] Arthur P Dempster, Nan M Laird, and Donald B Ru-[54] Diederik Kingma and Jimmy Ba. Adam: A [55] Misha Denil, Alban Demiraj, and Nando de Freitas. Ex-[56] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Juraf-
