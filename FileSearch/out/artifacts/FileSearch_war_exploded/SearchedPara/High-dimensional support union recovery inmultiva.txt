 norms. Such structured norms are well motivated in various settings, among them kernel learn-structured norm also enforces blockwise sparsity , in the sense that parameters within blocks are more likely to be zero (or non-zero) simultaneously.
 allow the model dimension p (as well as other structural parameters) to grow along with the sample showing for instance that the Lasso can recover the support of a sparse signal even when p  X  n . Some more recent work has studied consistency issues for block-regularization schemes, including understand the following question: under what conditions does block regularization lead to a quan-n required to recover the support union; we wish to know how this scales as a function of prob-lem parameters. Our main contribution is to provide a function quantifying the benefits of block under suitable structural conditions on the data, the block-norm regularization we consider never complexity.
 coefficients as a p  X  K matrix B  X  , the regression model takes the form if S k = parameter coefficients, an intractable non-convex function. More generally, block-norm regulariza-specifically, let  X   X  i denote the i th row of B  X  , and define, for q  X  1 , assumption that we are not willing to make.
 the following disguised second-order cone program: where ||| M ||| F : = ( under high-dimensional scaling, meaning that the number of observations n , the ambient dimen-parameter  X  ` all sequences ( n, p, s, B  X  ) such that the control parameter  X  ` threshold  X  crit &lt; +  X  . Note that  X  ` is, the sample size required for exact recovery as a function of the problem parameters. Whereas tasks (columns of B  X  ).
 of our analysis and how quickly the asymptotic regime arises. 1.1 Notations For a (possibly random) matrix M  X  R p  X  K , and for parameters 1  X  a  X  b  X   X  , we distinguish the ` a /` b block norms from the ( a, b ) -operator norms, defined respectively as spectral norm ||| M ||| 2 , 2 as ||| M ||| 2 , and the `  X  -operator norm ||| M |||  X  ,  X  = max i we assume that the measurement or design matrices X have rows drawn in an i.i.d. manner from a zero-mean Gaussian N (0 ,  X ) , where  X   X  0 is a p  X  p covariance matrix.
 matrix, the design matrix and its covariance matrix: In addition, we make the following assumptions about the covariance  X  of the design matrix: ( A 2 ) Mutual incoherence: There exists  X   X  (0 , 1] such that ( A 3 ) Self incoherence: There exists a constant D max such that Assumption A 1 is a standard condition required to prevent excess dependence among elements of the design matrix associated with the support S . The mutual incoherence assumption A 2 is also well known from previous work on model selection with the Lasso [10, 14]. These assumptions are More generally, it can be shown that various matrix classes satisfy these conditions [14, 11]. 2.1 Statement of main result using the following procedure. Solve the block-regularized problem (2) with regularization param-of the support union as b S ( b B ) : = is unique, and such that P [ b S = S ]  X  1 .
 parameter, which we define here. For any vector  X  i 6 = 0 , define  X  (  X  i ) : =  X  i k  X  and the sample complexity parameter  X  `  X  ( B ) : = this notation, we have the following result: Theorem 1. Consider a random design matrix X drawn with i.i.d. N (0 ,  X ) row vectors, an obser-the block-regularized program (2) with regularization parameter  X  n =  X  For any sequence ( n, p, B  X  ) such that the ` 1 /` 2 control parameter  X  `  X  crit ( X ) = 1 . (ii) A technical condition that we require on the regularization parameter is which is satisfied by the choice given in the statement. 2.2 Some consequences of Theorem 1 s )) , which matches the scaling established in previous work on the Lasso [11]. interval [ s KC We then have [  X  ( B  X  )] ij = sign (  X   X  i ) / sample complexity relative to the naive strategy of solving separate Lasso problems and construct-model (1), we essentially have Kn observations of the coefficient vector  X   X  with the same design gence in high-dimensional results such as Theorem 1 are not determined by the noise variance, but rather by the number of interfering variables ( p  X  s ).
 length. Under this condition, we have once the sample complexity parameter n/ (2 s K log( p  X  s )) is larger than 1. For the standard Gaussian ensemble, it is known [11] that the Lasso fails with probability one for provides a K -fold reduction in the number of samples required for exact support recovery. regression problems are all disjoint. The sample complexity parameter for each of the individual ` /` 2 -regularization can have higher sample complexity than separate Lassos. and  X  S = X S ( b  X  SS )  X  1 X T S denotes the orthogonal projection onto the range of X S . High-level proof outline: At a high level, our proof is based on the notion of what we refer to as a primal-dual witness : we first formulate the problem (2) as a second-order cone program (SOCP), with the same primal variable B as in (2) and a dual variable Z whose rows coincide at optimality matrix b Z such that, under the conditions of Theorem 1, with probability converging to 1: (a) The pair ( b B, b Z ) satisfies the Karush-Kuhn-Tucker (KKT) conditions of the SOCP. (b) In spite of the fact that for general high-dimensional problems (with p  X  n ), the SOCP need guarantees that b B is the unique optimal solution of (2). (c) The support union  X  S of b B is identical to the support union S of B  X  . Lemma 1. Suppose that there exists a primal-dual pair ( b B, b Z ) that satisfy the conditions: construction.
 3.1 Construction of primal-dual witness  X  k b invertible, we may solve as follows For any row i  X  S , we have k b  X  i k 2  X  k  X   X  i k 2  X  k U S k ` following event occurs with high probability to show that no row of b B S is identically zero. We establish this result later in this section. tion (6c), we obtain a ( p  X  s )  X  K random matrix V S c , whose row j  X  S c is given by converges to one as n tends to infinity.
 Correct inclusion of supporting covariates: We begin by analyzing the probability of E ( U S ) . Lemma 2. Under assumption A 3 and conditions (5) of Theorem 1, with probability 1  X  exp(  X   X (log s )) , we have some  X  n  X  0 , we have following arguments, we drop the index S c and write V for V S c . In order to show that k V k `  X  n with probability converging to one, we make use of the decomposition T 2 : = Therefore, to show that 1  X   X  with high probability. Lemma 4. Conditionally on W and X S , we have where  X  j  X  N ( ~ 0 K , I K ) and where the K  X  K matrix M n = M n ( X S , W ) is given by But the covariance matrix M n is itself concentrated. Indeed, Lemma 5. Under the conditions (5) of Theorem 1, for any  X  &gt; 0 , the following event T (  X  ) has probability converging to 1: P [ T (  X  ) c ]  X  0 , so that it suffices to deal with the first term.
 P [ T 0 3  X   X  |T (  X  )]  X  P Finally using the union bound and a large deviation bound for  X  2 variates we get the following condition which is equivalent to the condition of Theorem 1:  X  ` Lemma 6. P In this section, we illustrate the sharpness of Theorem 1 and furthermore ascertain how quickly with entries  X   X  ij in { X  1 / sampled from the standard Gaussian ensemble. Since |  X   X  ij | = 1 /  X   X   X   X   X 
 X   X  ( B  X  ) T  X  ( B  X  )  X  X nterference X  of irrelevant covariates, and not by the variance of a noise component.  X  matrix B  X  =  X  ( B  X  ) from copies of vectors of length 4. Denoting by  X  the usual matrix tensor product, we consider: of the ambient dimension and the two types of sparsity described above. Note how the curves all undergo a threshold phenomenon, with the location consistent with the predictions of Theorem 1. We studied support union recovery under high-dimensional scaling with the ` 1 /` 2 regularization, Gaussian designs, the regularization seems  X  X daptive X  in sense that it doesn X  X  perform worse than situations, which need to be characterized in future work, it could do worse than the Lasso.
