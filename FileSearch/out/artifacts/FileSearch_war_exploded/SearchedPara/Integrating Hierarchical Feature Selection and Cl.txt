 It is well accepted that using high-dimensional multi-modal visual features for image content representation and classi-fier training may achieve more sufficient characterization of the diverse visual properties of the images and further result in higher discrimination power of the classifiers. However, training the classifiers in a high-dimensional multi-modal feature space requires a large number of labeled training images, which will further result in the problem of curse of dimensionality . To tackle this problem, a hierarchical fea-ture subset selection algorithm is proposed to enable more accurate image classification, where the processes for feature selection and classifier training are seamlessly integrated in a single framework. First, a feature hierarchy (i.e., con-cept tree for automatic feature space partition and organi-zation) is used to automatically partition high-dimensional heterogeneous multi-modal visual features into multiple low-dimensional homogeneous single-modal feature subsets ac-cording to their certain physical meanings and each of them is used to characterize one certain type of the diverse visu-al properties of the images. Second, principal component analysis (PCA) is performed on each homogeneous single-modal feature subset to select the most representative fea-ture dimensions and a weak classifier is learned simultane-ously. After the weak classifiers and their representative feature dimensions are available for all these homogeneous single-modal feature subsets, they are combined to generate an ensemble image classifier and achieve hierarchical feature subset selection. Our experiments on a specific domain of natural images have also obtained very positive results. Categories and Subject Descriptors I.4.8 [ Image Processing and Computer Vision ]: Scene Analysis-object recognition , H.2.8 [ Database Managemen-t ]: Database Applications -image databases .
 General Terms Algorithms, Measurement, Experimentation Keywords: Feature hierarchy, SVM image classifier, hier-archical feature selection, boosting.
When large-scale image collections come into view, auto-matic detection of object classes and image concepts may provide more effective solutions for image indexing and re-trieval [23, 5, 26, 12]. Image classification is one of the potential solutions for automatic detection of object class-es and image concepts from large-scale image collections. Thus achieving more accurate training of the image classi-fiers plays an important role in supporting multi-label image annotation and semantic image retrieval via keywords [8, 14, 30, 11, 17, 35, 34, 1, 29, 32, 7, 3, 16]. However, the performance of the image classifiers largely depends on two inter-related issues: (1) high-dimensional multi-modal visu-al features for characterizing the diverse visual properties of the images more sufficiently; (2) effective algorithms for achieving more accurate training of the image classifiers and selecting more representative feature subsets automatically.
Many image classification algorithms have been proposed in the literature [8, 14, 30, 11, 17, 35, 34, 1, 29]. Ideally, using more visual features for image content representation and classifier training has more capacity to characterize vari-ous visual properties of the images effectively and efficiently. This may further enhance the classifier X  X  ability on detect-ing different image concepts and object classes and result in higher classification accuracy. However, learning the im-age classifiers in a high-dimensional multi-modal feature s-pace requires a large number of labeled training images and generally increases exponentially as the feature dimension increases (i.e., curse of dimensionality ). When only a lim-ited number of labeled training images are available, there is an urgent need to develop new techniques that are able to achieve accurate training of the image classifiers by using a small number of labeled training images. One promising approach is to select the most representative feature subsets and train the image classifiers simultaneously.

To select the optimal feature subsets for classifier training, many feature selection algorithms have been proposed and they can generally be classified into two categories: lter and wrapper [33, 18, 13, 28, 20, 24]. A lter algorithm separates the procedures for feature selection and classifier training by merely calculating the ranking information for each fea-ture dimension based on its correlation score with the given image concept or object class. On the other hand, a wrap-per algorithm wraps the procedure for feature selection with the procedure for classifier training. However, both the fil-ter and wrapper approaches ignore the feature hierarchy and the heterogeneity of high-dimensional multi-modal visu-al features. They perform feature selection directly in the high-dimensional multi-modal feature space and thus they require a large number of labeled training samples to achieve reliable feature selection. Recently, Viola et al. have devel-oped a new feature selection approach by using AdaBoost to train a cascade of linear classifiers [29], which can be treat-ed as a new wrapper-based approach. Each weak classifier depends on only one-dimensional single-modal Harr feature and thus the correlation among the multi-modal visual fea-tures has been ignored completely.

By incorporating the feature hierarchy for automatic fea-ture space partition and organization, our solution is to automatically partition the high-dimensional heterogeneous multi-modal feature space into multiple low-dimensional ho-mogeneous single-modal feature subsets according to their certain physical meanings and perform classifier training in each homogeneous single-modal feature subset simultane-ously. The advantages of our proposed framework include: (a) Partitioning the high-dimensional heterogeneous multi-modal feature space into multiple low-dimensional homoge-neous single-modal feature subsets according to their certain physical meanings can scale up SVM image classifier train-ing significantly because the number of the labeled training images (that are required for classifier training in a low-dimensional homogeneous single-modal feature subset) is re-duced dramatically; (b) Different homogeneous single-modal feature subsets are used to characterize different principal visual properties of the images, thus the corresponding weak classifiers are diverse, complementary and can be combined to generate an ensemble classifier with higher prediction accuracy; (c) It can support more effective kernel function selection because the geometric property for the distribution of the images in each homogeneous single-modal feature subset can effectively be approximated by using RBF kernel functions; (d) It can significantly reduce human efforts on labeling large-scale training images by incorporating unlabeled im-ages and feature hierarchy for SVM classifier training, and it is able to boost the image classifiers significantly and re-sult in higher classification accuracy; (e) It is able to select the most representative feature sub-sets for different object classes and image concepts and speed up the process for image classification significantly. It also has good scalability with the sizes of the image concepts (object classes) and the feature dimensions.

The major differences between our new approach and the technique proposed in [29] are: (1) feature correlation is exploited to enable more accurate classifier training; (2) ensemble classifier is learned by boosting both the labeled training images and the multi-modal visual features.
This paper is organized as follows: Section 2 introduces our feature extraction framework; Section 3 presents our new framework for joint classifier training and feature selection; Section 4 gives our extensive experimental results on multi-label image annotation; We conclude this paper in Section 5.
There are three widely accepted image patterns for im-age content representation and feature extraction [23]: (a) region-based approach by using homogeneous image regions for feature extraction; (b) scene-based approach by treating whole image as single visual pattern for feature extraction; (c) object-based approach by using image objects for feature extraction.

The major problem for the region-based approach is that there is no accurate correspondence between the image se-mantics and the homogeneous image regions, and thus the region-based visual features may not be able to detect var-ious image concepts accurately. On the other hand, one single image may consist of multiple object classes, thus the image-based global visual features may not be able to detect the object classes accurately. Using the semantic objects for feature extraction is able to improve the quality of the visual features significantly and result in the image classifiers with higher discrimination power. The major problem for the object-based approach is that automatic object extraction is generally difficult because homogeneous image regions in color or texture do not correspond to the semantic objects directly.

Based on these observations, we believe that it is very important to develop a middle-level framework for achiev-ing semantic-sensitive image content representation, which is able to enhance the quality of the low-level visual features and the discrimination power of the image classifiers. In ad-dition, this middle-level framework should be able to reduce the cost for object extraction and feature extraction. Based on this understanding, we use image Blobs for image con-tent representation and feature extraction. The image blobs are defined as the groups of the neighboring image regions which may have similar color or texture [5].

To detect the image blobs automatically, we use an au-tomatic image segmentation technique that is developed by Deng and Majunanth [6]. The neighboring homogeneous image regions with similar colors or textures are then merged as semantic-sensitive image blobs for image content repre-sentation and feature extraction.

After the image blobs are formed, we extract 83-dimensional multi-modal visual features to characterize their multi-modal visual properties of the images. These 83-dimensional het-erogeneous multi-modal visual features include: 7-dimensional R,G,B average colors and their variances, 7-dimensional L,U,V average colors and their variances, 62-dimensional texture feature from Gabor filter banks and 7-dimensional Tamu-ra texture. It is worth noting that all these 83-dimensional heterogeneous multi-modal visual features can be organized more effectively by using a feature hierarchy , i.e., mul-tiple homogeneous single-modal feature subsets with cer-tain physical meanings and each feature sunset consists of multiple feature dimensions. To reduce the number of la-beled training images that are required for achieving accu-rate SVM classifier training, all these 83-dimensional hetero-geneous multi-modal visual features are automatically par-titioned into multiple low-dimensional homogeneous single-modal feature subsets according to their certain physical meanings. The feature dimensions in the same homoge-neous single-modal feature subset share the same physical meaning, and each homogeneous single-modal feature sub-set is used to characterize one certain type of the diverse visual properties of the images. Obviously, different image concepts and object classes may have different representa-tive feature subsets. Thus it is very important to develop new techniques that are able to select the most representa-tive feature subsets for different image concepts and object classes.

In our experiments, all these 83-dimensional heteroge-neous multi-modal feature space are automatically parti-tioned into 9 homogeneous single-modal feature subsets ac-cording to their certain physical meanings: 3-dimensional R,G,B average color; 4-dimensional R,G,B color variances; 3-dimensional L,U,V average color; 4-dimensional L,U,V col-or variances; 2-dimensional average &amp; standard deviation of Gabor filter bank channel energy; 30-dimensional Gabor av-erage channel energy; 30-dimensional Gabor channel energy deviation; 2-dimensional Tamura texture features (coarse &amp; contrast), and 5-dimensional angel histogram derived from Tamura texture. Thus the heterogeneous multi-modal visual features are organized effectively by using two-level feature hierarchy : (a) multiple homogeneous single-modal feature subsets with certain physical meanings; (b) multiple visual features for each homogeneous single-modal feature subset that share the same physical meaning.
In order to reduce the number of labeled training images that are required for achieving accurate classifier training, we propose a new framework to enable hierarchical feature subset selection , where a weak classifier is learned for each homogeneous single-modal feature subset. Because the di-mensions for each homogeneous single-modal feature subset are relatively low, we can use a smaller number of labeled training images to learn the weak classifier accurately. The weak classifiers, which are learned from different homoge-neous single-modal feature subsets, are then combined to generate an ensemble classifier for achieving more accurate image classification.

By incorporating the feature hierarchy for automatic fea-ture space partition and organization, our lab developed a novel hierarchical algorithm to seamlessly integrate classifier training and hierarchical feature selection in a single frame-work [11]: (a) The weak classifiers for all these 9 homoge-neous single-modal feature subsets are learned simultaneous-ly and principal component analysis (PCA) is used to exploit the feature correlation and select the most representative feature components for each homogeneous single-modal fea-ture subset (i.e., in-set feature selection ); (b) The weak classifiers are combined to boost an ensemble classifier, and feature subset selection is achieved by selecting the most ef-fective weak classifiers and the corresponding homogeneous single-modal feature subsets (i.e., inter-set feature selec-tion ). Different from [11], which focuses on Grid representa-tion of the feature set, we select features using its self-proved robustness.

Support vector machine (SVM) is used to learn weak clas-sifiers [27, 21, 15, 2], while Adaboost is used to learn the ensemble classifier from combination of weak classifiers [10, 9]. Bagging proposed by Breiman [4] is also a candidate for ensemble classifier learning. Some experiments have shown that AdaBoost sometimes outperforms Bagging. Thus, we use AdaBoost for weak classifier combination.
For a given image concept or object class C j , we use one-against-all rule to label the positive training images Table 1: The optimal parameter pairs (  X  C;  X  ) of the SVM classi ers for some object classes. object classes grass purple flower red flower object classes rock sand field sky object classes snow water sunset  X  labeled training image is represented by ( X l ; Y l ), where X stands for 83-dimensional blob-based multi-modal visual fea-tures , and Y l , the semantic label. The unlabeled images are denoted as Omega c j .

The weak classifier for each homogeneous single-modal feature subset is first learned by using only  X  c j . For each pairs in  X  c j , a pair of transformation parameters W and b exists that satisfies f ( X l ) = Y l ( W  X   X ( X l ) + b ) where  X ( X l ) maps X l into higher-dimensional space. The kernel function is defined as ( X i ; X j ) =  X ( X i ) T  X ( X The radial basis function (RBF) is selected, ( X i ; X j ) = exp (  X  || X i  X  X j || 2 ) ; &gt; 0. These two supporting planes have a margin of 2 = || W || 2 . The weak SVM classifier is then designed to maximize the margin with the constraints f ( X l ) = Y l ( W  X   X ( X l ) + b )  X  +1 for the positive images ( Y l = +1) and negative ones ( Y l =  X  1).

The margin maximization procedure can be done by the following optimization task with the set of  X  c j : subject to: where l  X  0 is the training error rate, C &gt; 0, the penalty parameter and 1 2  X  W  X  2 , the regularization term.
In order to determine the optimal model parameters (  X  C;  X  ) for the weak SVM classifiers, an algorithm was designed as follows: (a) Images in  X  c j are partitioned into groups in equal size,  X  1 groups for classifier training, the residual one for validation. (b) The visual features for each homogeneous single-modal feature subset are firstly normalized to avoid numerical problem. (c) The numeric ranges for C and are coarsely partitioned into small pieces with M pairs.  X  1 groups are used to train the classifier for each pair. When the M classifier models are available, the underlying optimal parameter pair ( C; ) is determined via cross-validation. (d) When ( C; ) at the coarse level is available, a fine partition of the search space around it is used to determine more accurate parameter pair (  X  C;  X  ) in a hierarchical way. (e) With the optimal parameter pair (  X  C;  X  ), the final model for the weak SVM classifier is trained again by using the whole set of training images. Table 1 and 2 shows the optimal parameter pairs (  X  C;  X  ) for some object classes and semantic concepts.

To select the most representative feature components for weak classifier training, for a given homogeneous single-modal Table 2: The optimal parameter pairs (  X  C;  X  ) of the SVM classi ers for some image concepts. semantic concepts mountain view beach garden semantic concepts sailing skiing desert feature subset S j , PCA is used to exploit the feature corre-lation and enable in-set feature selection as follows: (1) PCA is used to determine its feature components, which are ranked based on the values of their Eigenvalues. (2) The unrepresentative feature components with small Eigenvalues are sequentially removed from S j , and the residual ones are used for learning a new weak classifier, which is then tested on the validation image set, and the relevant loss function L j ( X n ; Y n ) = ness for S j is defined as: (4) The above procedure is performed repeatedly until the goodness of S j is below a pre-defined threshold. (5) The re-sults will be used as the inputs for ensemble classifier train-ing.
In order to achieve accurate image classifiers, it is some-times too expensive to manually label large amounts of train-ing images. When this is concerned, semi-supervised classi-fier training becomes a promising solution to take advantage of unlabeled images [15, 2, 19, 25, 31].

To incorporate the unlabeled images  X  c j for semi-supervised training of the weak SVM classifiers, an incremental frame-work was developed to predict labels for images in  X  c j . A hyperplane &lt; W; b &gt; is then estimated by the framework, which separates  X  c j and  X  c j with maximum margin [27, 21]. Thus the semi-supervised training of SVM classifier can be formulated as: subject to : { When the number of unlabeled images is small, the training work can be solved simply by trying all possible label as-signments for images in  X  c j . However, this will become too expensive and the outlying unlabeled images may mislead the classifiers when the number gets larger.

In order to solve this, an incremental framework was developed for semi-supervised training of the SVM classi-fiers, which takes the following major steps: (1) For a given image concept or object class, a weak SVM classifier is first learned from  X  c j as introduced in Section 3.1. (2) the clas-sifier is then used to predict the labels for images in  X  while the confidence score for the label of the unlabeled Figure 1: The SVM boundaries that are obtained by a image X  X  j is calculated by applying an additional sigmoid function [21]: where f ( X  X  j ) is the output of the weak SVM classifier for X j , and can be determined by minimizing the negative log-likelihood (NLL) function on the validation image set. The value of P ( X  X  j ) is used to determine whether or not the unlabeled image X  X  j will be removed from the set through a pre-defined threshold. (3) A new SVM classifier can then be learned incrementally with all the remaining high-confident unlabeled images . (4) Considering the small shift of the hy-perplane from previous step, the new SVM classifier is used to predict new labels for these high-confident unlabeled im-ages. Images with inconsistent predicted labels are restored as the unlabeled images. (5) By integrating unlabeled im-ages with consistent predicted labels for incremental classi-fier training, our incremental algorithm is performed repeat-edly until it converges.

A two-dimensional synthetic data set is used to visual-ize and show the convergence of our incremental framework for semi-supervised SVM classifier training. As discussed in [22], this synthetic data set has two classes with two attributes, each class has a bimodal distribution which is formed by equal mixture of two normal distributions, and two normal distributions share the same covariance matrix. By treating this synthetic data set as an example, we have experimentally obtained the convergence of our incremen-tal framework for semi-supervised SVM classifier training as shown in Fig. 1 and Fig. 2. From our experimental results, one can observe that our incremental framework for semi-supervised SVM classifier training finally converges to the same decision boundaries that are obtained by the batch-based training approach. The theoretical proof of the convergence for incremental SVM classifier training can be found at [27].
AdaBoost is used for weak classifier combination too en-able both a two level feature selection and ensemble classifier training. Feature selection is performed at two different lev-els simultaneously and accurate image classifiers are learned by using a small number of labeled images. At the first level, Figure 2: The SVM boundaries that are obtained by each visual feature dimension can be treated as a selection unit. PCA is used to select the most representative feature components. At the second level, each homogeneous single-modal feature subset is treated as an individual selection unit, whose goodness is measured by estimating the perfor-mance of the relevant weak classifier on the cross-validation image set. Thus, the most effective weak classifiers and their corresponding homogeneous single-modal feature sub-sets form the inter-set feature selection at the second level.

The process for inter-set feature selection at the second level can be treated as a process for ensemble classifier train-ing, where AdaBoost can be used to boost the relevant weak classifiers: where f j t ( X ) is the weak classifier for the j th homogeneous single-modal feature subset S j at the t th iteration, and T is the total number of iterations. Higher prediction accuracy can be expected since the ensemble classifier combines the predictions of the weak classifiers and boosts the results. The homogeneous single-modal feature subsets corresponds to the most effective weak classifiers are then selected for image classification.

Compared with previous works, the advantages of our proposed work include: (a) Incorporating the feature hierar-chy and feature subset selection for SVM classifier training can speed up SVM classifier training significantly because the number of the labeled training images is reduced dramat-ically for each low-dimensional homogeneous single-modal feature subset; (b) Partitioning the heterogeneous multi-modal feature space into multiple homogeneous single-modal feature subsets automatically can support more effective ker-nel function selection because the geometric property of the distribution of the images in each homogeneous single-modal feature subset can be effectively approximated by using RBF functions; (c) Incorporating the unlabeled images for semi-supervised SVM classifier training can significantly reduce human efforts on labeling large-scale training images while achieving higher classification accuracy; (d) Our boosting algorithm is able to simultaneously boost both the training images and the feature subsets, thus higher classification ac-curacy can be obtained; (e) Our proposed classifier training framework is able to select both the most suitable feature subsets for characterizing various visual properties of differ-ent image concepts and object classes accurately; (f) Our proposed classifier training framework is scalable with the image concepts (object classes) and the feature dimensions effectively.
Given a certain test image, its image blobs and the rele-vant 83-dimensional multi-modal visual features are detect-ed automatically, which are then classified into the most rel-evant object classes. The neighboring image blobs belongs to the same class are merged into a single region. Based on these object classes, the whole image is then classified into the most relevant image concept and multi-label image annotation can be achieved accurately.

The task of this paper is to detect 19 object classes and 15 image concepts . After an unlabeled test image is clas-sified, the text keywords for interpreting the object classes and image concepts provide the annotations of the images at the content and concept level respectively. As shown through Fig. 3 to Fig. 6, our multi-label image annotation framework can support more expressive interpretations of the image semantics, and is very attractive to enable multi-modal image retrieval via keywords such that the naive users will have more flexibility to specify their query concepts via various keywords at different semantic levels.
We evaluate our algorithm on two databases: image database from the Google image search engine with about 30,000 pic-tures, and Corel image database with more than 3,800 pic-tures consisting of different image concepts and object class-es. For each object class and image concept, only 50 images are labeled for classifier training.
 Figure 3: Multi-label image annotation results for the
With the same number of training images, four sets of comparison experiments are performed to evaluate the effec-t of our proposed framework for joint feature selection and classifier training using different conditions: (a) Learning the optimal ensemble classifier by using all the weak clas-sifiers and the corresponding homogeneous feature subsets. (b) Performing only the second level of feature selection. (c) Figure 4: Multi-label image annotation results for the Figure 5: Multi-label image annotation results for the Figure 6: Multi-label image annotation results for the Performing both the first level (PCA) and the second level of feature selection. (d) Comparing the performance difference between our approach, AdaBoost, and FeatureBoost.
There are 9 homogeneous single-modal feature subsets and the total number of iterations for AdaBoost is T = 50. Thus we have obtained 9  X  50 = 450 weak SVM classifiers. All Figure 7: The relationship between the ensemble clas-Figure 8: The relationship between the goodness of the these weak SVM classifiers can be used to boost the ensem-ble classifier. Ideally, integrating more weak SVM classifiers for ensemble classifier training may improve the classifier X  X  performance sequentially. For a given object class  X  X ea wa-ter X , the relationship is obtained between the ensemble clas-sifier X  X  performance and the number of weak SVM classifiers used for ensemble classifier training. As shown in Fig. 7, although adding more weak SVM classifiers may improve accuracy, it is not able to achieve significant improvemen-t after some iterations. This also proves that selecting the most effective weak classifiers and the corresponding homo-geneous single-modal feature subsets for image classification can achieve acceptable accuracy. For the same object class  X  X ea water X , we have also obtained the optimal number of homogeneous single-modal feature subsets for ensemble clas-sifier training as shown in Fig. 8. It X  X  obvious that only the top 3 homogeneous single-modal feature subsets may boost the classifier X  X  performance significantly.

The performance difference of our ensemble classifier is also evaluated by performing only the second-level feature selection against both levels. As shown in Fig. 9 and Fig. 10, the latter is able to exploit the feature correlation effec-tively and improve the accuracy of the ensemble classifier significantly. Table 3: The performance of our classi ers (i.e., pre-cision/recall) for some object classes. object classes grass purple flower red flower object classes rock sand field sky object classes snow water sunset Figure 9: The performance differences of our ensemble
The performance differences of multiple approaches for ensemble classifier training is shown in Fig. 11 and Fig. 12. With the advantages of both AdaBoost and FeatureBoost, higher classification accuracy can be expected. Results show that our approach can outperform AdaBoost and Feature-Boost. The average performance of our classifiers for some object classes and image concepts are given in Table 3 and Table 4.

The performance of our classifiers is also evaluated with different sizes of unlabeled images for classifier training. Re-sults are shown through Fig. 13 to Fig. 16. It can be observed that if the training set is small, the unlabeled im-ages can improve the classifier X  X  performance significantly. The reasons are: (a) The certain unlabeled images , orig-inated from the existing image context classes for concept interpretation, are able to improve the underlying SVM de-cision boundaries. (b) The informative unlabeled images , originated from the unknown image context classes, have the capability to provide additional context knowledge for updating the underlying SVM decision boundaries, which Figure 10: The performance differences of our ensemble Figure 11: The performance comparison (i.e., 100 ) for Figure 12: The performance comparison (i.e., 100 ) for Figure 13: The classification accuracy (i.e., precision ) Figure 14: The classification accuracy (i.e., precision ) Table 4: The comparison of our classi ers (i.e., pre-cision/recall) for some image concepts. concepts mountain view beach garden concepts sailing skiing desert concepts ocean view waterway prairie Figure 15: The classification accuracy (i.e., precision ) will in turn influence the prediction accuracy of the SVM classifiers learned incrementally. (c) The outlying unlabeled images , originated from outliers, can be predicted accurate-ly, while misleading effects on classifier training can be elim-inated by determining an optimal value of the penalty term C automatically.

When a limited number of labeled images are available and more unlabeled images are involved for semi-supervised classifier training, the classifier X  X  performance will decrease, which is also foreseeable, since large-scale outliers may mis-lead it.

By using the same number of training samples for classifier training, we have also compared the performance differences between out approach and simply training the classifier di-rectly from the 83-dimensional heterogeneous multi-modal feature space, shown in Fig. 17. It also shows that our approach can obtain higher classification accuracy when la-beled training samples are limited. By incorporating the feature hierarchy for ensemble im-Figure 16: The classification accuracy (i.e., precision ) Figure 17: The performance comparison results be-age classifier training, we have proposed a novel approach to enabling hierarchical feature subset selection. By select-ing the most effective weak classifiers and the corresponding homogeneous single-modal feature subsets to boost the en-semble image classifier, our proposed framework is able to achieve higher prediction accuracy for image classification and object detection. In addition, our proposed framework has also supported a novel solution for multi-label image an-notation and image retrieval via keywords. Our experiments on a specific domain of natural images have also obtained very positive results. Obviously, our proposed algorithm for joint classifier training and feature subset selection can also be applied to other data domains.

The main problem for image classification is the large range of possible variations within the same image concept or object class because of various viewing and illumination conditions. Thus it is also very important to develop new techniques that are able to handle the changes of viewing and illumination conditions effectively. By treating various viewing conditions or illumination conditions as the addi-tional selection units, we will label the training images to learn the relevant classifiers under various view or illumina-tion conditions, and our proposed image classifier training technique can be used to combine these classifiers effectively for final prediction and thus it is able to generalize across different viewing and illumination conditions.
The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by 973 Program (2010CB327906), Shanghai Leading Academ-ic Discipline Project (B114), Doctoral Fund of Ministry of Education of China (20100071120033), and Shanghai Mu-nicipal R&amp;D Foundation (08dz1500109). [1] K. Barnard and D. Forsyth. Learning the semantics of [2] K. Bennet and A. Demiriz. Semi-supervised support [3] D. Blei and M. Jordan. Modeling annotated data. [4] L. Breiman. Bagging predictors. Machine Learning , [5] C. Carson, S. Belongie, H. Greenspan, and J. Malik. [6] Y. Deng and M. Manjunath. Unsupervised [7] P. Duygulu, K. Barnard, J. Freitas, and D. Forsyth. [8] J. Fan, Y. Gao, and H. Luo. Multi-level annotation of [9] W. Fan, S. Stolfo, J. Zhang, and P. Chan. Adacost: [10] Y. Freund and R. Schapire. Experiments with a new [11] Y. Gao, J. Fan, H. Luo, X. Xue, and R. Jain. [12] K. Ghahremani, C. Shahabi, S. Yao, and [13] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic [14] Y. Jin, L. Khan, L. Wang, and M. Awad. Image [15] T. Joachims. Transductive inference for text [16] V. Lavrenko, R. Manmatha, and J. Jeon. A model for [17] A. Makadia, V. Pavlovic, and S. Kumar. A new [18] D. Modha and W. S. Spangler. Feature weighting in [19] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. [20] J. O X  X ullivan, J. Langford, R. Caruana, and A. Blum. [21] J. Platt. Probabilistic outputs for support vector [22] B. Ripley. Neural network and related methods for [23] A. Smeulders, M. Worring, S. Santini, A. Gupta, and [24] C. Sutton, M. Sindelar, and A. McCallum. Feature [25] M. Szummer and T. Jaakkola. Information [26] A. Torralba and A. Oliva. Semantic organization of [27] V. Vapnik. Statistical learning theory. 1998. [28] N. Vasconcelos and M. Vasconcelos. Scalable [29] P. Viola and M. Jones. Robust real-time face [30] X.-J. Wang, W.-Y. Ma, L. Zhang, and X. Li.
 [31] M. Weber, M. Welling, and P. Perona. Unsupervised [32] C. Yang, M. Dong, and J. Hua. Region-based image [33] L. Yu and H. Liu. Feature selection for [34] R. Zhang, Z. Zhang, M. Li, W.-Y. Ma, and H. Zhang. [35] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and
