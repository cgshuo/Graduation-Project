 Given a user-specified minimum correlation threshold  X  and a market basket database with N items and T transactions, an all-strong-pairs correlation query finds all item pairs with correlations above the threshold  X  . However, when the num-ber of items and transactions are large, the computation cost of this query can be very high. In this paper, we identify an upper bound of Pearson X  X  correlation coefficient for bi-nary variables. This upper bound is not only much cheaper to compute than Pearson X  X  correlation coefficient but also exhibits a special monotone property which allows prun-ing of many item pairs even without computing their upper bounds. A T wo-step A ll-strong-P airs corr E lation que R y (TAPER) algorithm is proposed to exploit these proper-ties in a filter-and-refine manner. Furthermore, we provide an algebraic cost model which shows that the computation savings from pruning is independent or improves when the number of items is increased in data sets with common Zipf or linear rank-support distributions. Experimental results from synthetic and real data sets exhibit similar trends and show that the TAPER algorithm can be an order of magni-tude faster than brute-force alternatives.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Pearson X  X  Correlation Coefficient, Statistical Computing
With the wide spread use of statistical techniques for data analysis, it is expected that many such techniques will be made available in a database environment where users can apply the techniques more flexibly, efficiently, easily, and Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. with minimal mathematical a ssumptions. Our research is directed towards developing such techniques.

More specifically, this paper examines the problem of com-puting correlations efficiently from large databases. Correla-tion analysis plays an important role in many application do-mains such as market-basket analysis, climate studies, and public health. Our focus, however, is on computing an all-strong-pairs correlation query that returns pairs of high posi-tively correlated items (or binary attributes). This problem can be formalized as follows: Given a user-specified mini-mum correlation threshold  X  and a market basket database with N items and T transactions, an all-strong-pairs corre-lation query finds all item pairs with correlations above the minimum correla tion threshold,  X  .

However, as the number of items and transactions in-creases, the computation cost for an all-strong-pairs corre-lation query becomes prohibitively expensive. For example, consider a database of 10 6 items, which may represent the collection of books available at an e-commerce Web site. Answering the all-strong-pai rs correlation query from such a massive database requires computing the correlations of ` computationally feasible to apply a brute-force approach to compute correlations for all half trillion item pairs, particu-larly when the number of transactions is also large.
Note that the all-strong-pairs correlation query problem is different from the standard association-rule mining problem [1, 3, 5, 9, 14]. Given a set of transactions, the objective of association rule mining is to extract all subsets of items that satisfy a minimum support threshold. Support measures the fraction of transactions that contain a particular subset of items. The notions of support and correlation may not nec-essarily agree with each other. This is because item pairs with high support may be poorly correlated while those that are highly correlated may have very low support. For in-stance, suppose we have an item pair { A, B } ,where supp ( A ) = supp ( B )=0 . 8and supp ( A, B )=0 . 64. Both items are uncorrelated because supp ( A, B )= supp ( A ) supp ( B ). In contrast, an item pair { A, B } with supp ( A )= supp ( B )= supp ( A, B )=0 . 001 is perfectly correlated despite its low support. Patterns with low support but high correlation are useful for capturing interesting associations among rare anomalous events or rare but expensive items such as gold necklaces and earrings.

In this paper, we focus on the efficient computation of statistical correlation for all pairs of items with high pos-itive correlation. More specifically, we provide an upper bound of Pearson X  X  correlation coefficient for binary vari-ables. The computation of this upper bound is much cheaper than the computation of the exact correlation, since this up-per bound can be computed as a function of the support of individual items. Furthermore, we show that this upper bound has a special monotone property which allows elim-ination of many item pairs even without computing their upper bounds, as shown in Figure 1. The x-axis in the fig-ure represents the set of items having a lower level of support than the support for item x i . These items are sorted from left to right in decreasing order of their individual support values. The y-axis indicates the correlation between each item x and item x i . Upperbound ( x i ,x ) represents the upper bound of correlation ( x i ,x ) and has a monotone decreas-ing behavior. This behavior guarantees that an item pair ( x i ,x k ) can be pruned if there exists an item x j such that upperbound ( x i ,x j ) &lt; X  and supp ( x k ) &lt;supp ( x j ). Figure 1: Illustration of the Filtering Techniques. (The curves are only used for illustration purposes.)
A T wo-step A ll-strong-P airs corr E lation que R y (TAPER) algorithm is proposed to exploit these properties in a filter-and-refine manner which consists of two steps: filtering and refinement. In the filtering step, many item pairs are fil-tered out using the easy-to-compute upperbound ( x i ,x )and its monotone property. In the refinement step, the exact correlation is computed for remaining pairs to determine the final query results.

In addition, we have proved the completeness and cor-rectness of the TAPER algorithm and provided an alge-braic cost model to quantify its computational savings. As demonstrated by our experiments on both real and synthetic data sets, TAPER can be an order of magnitude faster than brute-force alternatives and the computational savings by TAPER is independent or improves when the number of items is increased in data sets with common Zipf [18] or linear rank-support distributions. Related literature can be grouped into two categories. One category has focused on statistical correlation measures.
Jermaine [10] investigated the implication of incorporat-ing chi-square (  X  2 ) [15] based queries to data cube com-putations. He showed that finding the subcubes that sat-isfy statistical tests such as  X  2 are inherently NP-hard, but can be made more tractable using approximation schemes. Also, Jermaine presented an iterative procedure for high-dimensional correlation analysis by shaving off part of the database via feedback from human experts [11]. Finally, Brin [3] proposed a  X  2 -based correlation rule mining strat-egy. However,  X  2 does not possess a desired upward closure property for exploiting efficient computation [7].
In this paper, we focus on the efficient computation of statistical correlation for all pairs of items with high pos-itive correlation. Given n items, a traditional brute force approach computes Pearson X  X  correlation coefficient for all `  X  using matrix algebra in statistical software package as the  X  X orrelation matrix X  [12] function, which computes Pear-son X  X  correlation coefficient for all pairs of columns. This approach is applicable to but not efficient for the case of Boolean matrices, which can model market-basket-type data sets. The approach proposed in this paper does not need to compute all data sets with a Zipf-like rank-support distribution, we show that only a small portion of the item pairs needs to be ex-amined. In the real world, Zipf-like distributions have been observed in a variety of application domains, such as retail data and Web click-streams.

Another category of related work is from the association-rule mining framework [1], namely constraint-based associa-tion pattern mining [2, 4, 6, 8, 13]. Instead of using statisti-cal correlation measures as the constraints, these approaches use some other measures (constraints), such as support, lift, and the Jaccard measure, for efficiently pruning the pattern search space and identifying interesting patterns.
The remainder of this paper is organized as follows. Sec-tion 2 presents basic concepts. In section 3, we introduce the upper bound of Pearson X  X  correlation coefficient for bi-nary variables. Section 4 proposes the TAPER algorithm. In section 5, we analyze the TAPER algorithm in the areas of completeness, correctness, and computation gain. Section 6 presents the experimental results. Finally, in section 7, we draw conclusions and suggest future work.

Thescopeoftheall-strong-p airs correlation query prob-lem proposed in this paper is restricted to market basket databases with binary variables, and the correlation compu-tational form is Pearson X  X  correlation coefficient for binary variables, which is also called the  X  correlation coefficient. Furthermore, we assume that the support of items is be-tween 0 and 1 but not equal to either 0 or 1. These boundary cases can be handled separately.
In statistics, a measure of association is a numerical in-dex which describes the strength or magnitude of a relation-ship among variables. Although literally dozens of measures exist, they can be categorized into two broad groups: or-dinal and nominal. Relationships among ordinal variables can be analyzed with ordinal measures of association such as Kendall X  X  Tau and Spearman X  X  Rank Correlation Coef-ficient. In contrast, relationships among nominal variables can be analyzed with nominal measures of association such as Pearson X  X  Correlation Coefficient, the Odds Ratio, and measures based on Chi Square [15].

The  X  correlation coefficient [15] is the computation form of Pearson X  X  Correlation Coefficient for binary variables. In this section, we describe the  X  correlation coefficient and show how it can be computed using the support measure of association-rule mining [1].
In a 2  X  2 two-way table shown in Figure 2, the calculation of the  X  correlation coefficient reduces to where P ( ij ) , for i = 0, 1 and j = 0, 1, denote the number of samples which are classified in the i th row and j th column of the table. Furthermore, we let P ( i +) denote the total number of samples classified in the i th row, and we let P (+ j ) the total number of samples classified in the j th column. Figure 2: A two-way table of item A and item B.
 In the two-way table, N is the total number of samples. Furthermore, we can transform Equation 1 as follows.
Hence, when adopting the support measure of association rule mining [1], for two items A and B in a market basket database, we have supp ( A )= P (1+) /N , supp ( B )= P (+1) and supp ( A, B )= P (11) /N . With support notations and the above new derivations of Equation 1, we can derive the support form of the  X  correlation coefficient as shown below in Equation 2.
In this section, we present some properties of the  X  corre-lation coefficient. These properties are useful for the efficient computation of all-strong -pairs correlation query.
In this subsection, we reveal that the support measure is closely related with the  X  correlation coefficient. Specifically, we prove that an upper bound of the  X  correlation coefficient for a given pair { A, B } exists and is determined only by the support value of item A and the support value of item B, as shown below in Lemma 1.

Lemma 1. Given an item pair { A, B } , the support value supp(A) for item A, and the support value supp(B) for item B, without loss of generality, let supp ( A )  X  supp ( B ) .The upper bound upper(  X  { A,B } )ofthe  X  correlation coefficient for an item pair { A, B } can be obtained when supp(A, B) = supp(B) and Proof: According to Equation 2, for an item pair { A, B }
When the support values supp(A) and supp(B) are fixed,  X  { A,B } is monotone increasing with the increase of the sup-port value supp(A, B). By the given condition supp(A)  X  supp(B) and the anti-monotone property of the support measure, we get the maximum possible value of supp(A, B) is supp(B). As a result, the upper bound upper(  X  { A,B } of the  X  correlation coefficient for an item pair { A, B } be obtained when supp(A, B) = supp(B). Hence,
As can be seen in Equation 3, the upper bound of the  X  correlation coefficient for an item pair { A, B } relies only on the support value of item A and the support value of item B. In other words, there is no requirement to get the support value supp(A, B) of an item pair { A, B } for the calculation of this upper bound. As already noted, when the number of items N becomes very large, it is difficult to store the support of every item pair in the memory, since N ( N  X  1) / 2 is a huge number. However, it is possible to store the support of individual items in the main memory. As a result, this upper bound can serve as a coarse filter to filter out item pairs which are of no interest, thus saving I/O cost by reducing the computation of the support values of those pruned pairs.
In this subsection, we present a conditional monotone property of the upper bound of the  X  correlation coefficient as shown below in Lemma 2
Lemma 2. For a pair of items { A, B } ,ifwelet supp ( A ) &gt; supp ( B ) and fix the item A, the upper (  X  { A,B } ) of pair B } is monotone decreasing with the decrease of the support value of item B.
 Proof: By Lemma 1, we get: supp ( B 2 ), we need to prove upper (  X  { A,B 1 } ) &gt; upper (  X  This claim can be proved as follows: The above follows the given condition that supp ( B 1 ) &gt; supp ( B 2 )and(1  X  supp ( B 1 )) &lt; (1  X  supp ( B 2 )).
Lemma 2 allows us to push the upper bound of the  X  cor-relation coefficient into the search algorithm, thus efficiently pruning the search space.

Corollary 1. When searching for all pairs of items with correlations above a user-specified threshold  X  ,ifanitemlist { i 1 ,i 2 ,...,i m } is sorted by item supports in non-increasing order, an item pair { i a ,i c } with supp ( i a ) &gt;supp ( i c ) pruned if upper (  X  { i a ,i b } ) &lt; X  and supp ( i c ) = upper (  X  ( i a ,i b )) &lt; X  according to Equation 3 and the given condition upper (  X  { i a ,i b } ) &lt; X  , then we can prune the item pair { i a ,i c } . Next, we consider supp ( i c ) &lt;supp ( i { i a ,i c } is pruned.
In this section, we present the T wo-step A ll-strong-P airs corr E lation que R y (TAPER) algorithm. The TAPER algo-rithm is a two-step filter-and-refine query processing strat-egy which consists of two steps: filtering and refinement.
The Filtering Step: In this step, the TAPER algorithm applies two pruning techniques. The first technique uses the upper bound of the  X  correlation coefficient as a coarse filter. In other words, if the upper bound of the  X  correlation coefficient for an item pair is less than the user-specified correlation threshold, we can prune this item pair right way. The second pruning technique prunes item pairs based on the conditional monotone property of the upper bound of the  X  correlation coefficient. The correctness of this pruning is guaranteed by Corollary 1 and the process of this pruning is illustrated in Figure 1 as previously noted in introduction. In summary, the purpose of the filtering step is to reduce false positive item pairs and further processing cost. The Refinement Step: In the refinement step, the TA-PER algorithm computes the exact correlation for each sur-viving pair from the filtering step and retrieves the pairs with correlations above the use r-specified minimum correla-tion threshold as the query results.

Figure 3 shows the pseudocode of the TAPER algorithm, including the CoarseFilter and Refine procedures.
Procedure CoarseFilter works as follows. Line 1 initial-ize the variables and creates an empty query result set P . Lines 2 -10 use Rymon X  X  generic set-enumeration tree search framework [16] to enumerate candidate pairs and filter out item pairs whose correlations are obviously less than the user-specified correlation threshold  X  . Line 2 starts an outer loop. Each outer loop corresponds to a search tree branch. Line 3 specifies the reference item A, and line 4 starts a search within each branch. Line 5 specifies the target item B, and line 6 computes the upper bound of the  X  correla-tion coefficient for item pair { A, B } . In line 7, if this upper bound is less than the user-specified correlation threshold  X  , the search within this branch can stop by exiting from the inner loop, as shown in line 8. The reason is as follows. First, the reference item A is fixed in each branch and it has the maximum support value due to the way we construct the branch. Also, items within each branch are sorted based on their support in non-increasing order. Then, by Lemma 2, the upper bound of the  X  correlation coefficient for the item pair { A, B } is monotone decreasing with the decrease of the support of item B. Hence, if we find the first target item B which results in an upper bound upper (  X  { A,B } )thatis less than the user-specified correlation threshold  X  ,wecan stop the search in this branch. Line 10 calls the procedure Refine to compute the exact correlation for each surviving candidate pair and continues to check the next target item until no target item is left in the current search branch.
Procedure Refine works as follows. Line 11 gets the sup-port for the item pair { A, B } . Note that the I/O cost can 1. L = size( S ), P =  X  2. for i from 0 to L-1 3. A= S [ i ] 4. for j from i+1 to L 5. B= S [ j ] 6. upper (  X  )= 7. if ( upper (  X  ) &lt; X  ) then 8. break from inner loop 9. else 10. P=P  X  Refine(A, B,  X  ) 11. Get the support supp(A, B) of item set { A, B } 13. if  X &lt; X  then 14. return  X  //return NULL 15. else 16. return {{ A, B } , X  } be very expensive for line 11 when the number of items is large since we cannot store the support of all item pairs in the memory. Line 12 calculates the exact correlation coef-ficient of this item pair. If the correlation is greater than the user-specified minimum correlation threshold, this item pair is returned as a query result in line 16. Otherwise, the procedure returns NULL in line 14.

Example 1. To illustrate the TAPER algorithm, consider a database shown in Figure 4. To simplify the discussion, we use an item list { 1, 2, 3, 4, 5, 6 } which is sorted by item sup-port in non-increasing order. For a given correlation thresh-old 0.36, we can use Rymon X  X  generic set-enumeration tree search framework [16] to demonstrate how two-step filter-and-refine query processing works. For instance, for the branch starting from item 1, we identify that the upper bound of the  X  correlation coefficient for the item pair { 1, 3 0.333, which is less than the given correlation threshold 0.36. Hence, we can prune this item pair immediately. Also, since the item list { 1, 2, 3, 4, 5, 6 } is sorted by item supports in non-increasing order, we can prune pairs { 1, 4 } , { 1, 5 { 1, 6 } by Lemma 2 without any further computation cost. In contrast, for the traditional filter-and-refine paradigm, the coarse filter can only prune the item pair { 1, 3 } .Thereis no technique to prune item pairs { 1, 4 } , { 1, 5 } ,and Finally, in the refinement step, only seven item pairs are re-quired to compute the exact correlation coefficients, as shown in Figure 4 (c). More than half of the item pairs are pruned in the filter step even though the correlation threshold is as low as 0.36. Figure 4: Illustration of the filter-and-refine strat-egy. NC means there is no computation required.
In this section, we analyze TAPER in the areas of com-pleteness, correctness, and the computation savings.
Lemma 3. The TAPER algorithm is complete. In other words, this algorithm finds all pairs which have correlations above a user-specified minimum correlation threshold. Proof: This lemma proof as well as some following lemma proofs are presented in our Technical Report [17].
Lemma 4. The TAPER algorithm is correct. In other words, every pair this algorithm finds has a correlation above a user-specified minimum correlation threshold.
This section presents analytical results for the amount of computational savings obtained by TAPER. First, we illus-trate the relationship between the choices of the minimum correlation threshold and the size of the reduced search space (after performing the filtering step). Knowing the relation-ship gives us an idea of the amount of pruning achieved using the upper-bound function of correlation.

Figure 5 illustrates a 2-dimensional plot for every possible combination of support pairs, supp ( x )and supp ( y ). If we impose the constraint that supp ( x )  X  supp ( y ), then all item pairs must be projected to the upper left triangle since the diagonal line represents the condition supp ( x )= supp ( y ).
To determine the size of the reduced search space, let us start from the upper bound function of correlation.
The above inequality provides a lower bound on supp ( y ) such that any item pair involving x and y can be pruned us-ing the conditional monotone property of the upper bound function. In other words, any surviving item pair that un-dergoes the refinement step must violate the condition given in Equation 4. These item pairs are indicated by the shaded region shown in Figure 5. During the refinement step, TA-PER has to compute the exact correlation for all item pairs that fall in the shaded region between the diagonal and the polyline drawn by Equation 5.
As can be seen from Figure 5, the size of the reduced search space depends on the choice of minimum correlation threshold. If we increase the threshold from 0.5 to 0.8, the search space for the refinement step is reduced substantially. When the correlation threshold is 1.0, the polyline from Equation 5 overlaps with the diagonal line. In this limit, the search space for the refinement step becomes zero. Figure 5: An illustration of the reduced search space for the refinement step of the TAPER algorithm.
 Only item pairs within the shaded region must be computed for their correlation.

The above analysis shows only the size of the reduced search space that must be explored during the refinement step of the TAPER algorithm. The actual amount of prun-ing achieved by TAPER depends on the support distribu-tion of items in the database. To facilitate our discussion, we first introduce the definitions of several concepts used in the remainder of this section.

Definition 1. The pruning ratio of the TAPER algo-rithm is defined by the following equation. where  X  is the minimum correlation threshold, S (  X  ) is the number of item pairs which are pruned before computing their exact correlations at the correlation threshold  X  ,and T is the total number of item pairs in the database. For a given database, T is a fixed number and is equal to =
Definition 2. For a sorted item list, the rank-support function f ( k ) is a discrete function which present the support in terms of the rank k .
For a given database, let I = { A 1 ,A 2 ,...,A n } be an item list sorted by item supports in non-increasing order. Then item A 1 has the maximum support and the rank-support function f ( k )= supp ( A k ),  X  1  X  k  X  n ,which is monotone decreasing with the increase of the rank k . (1  X  j&lt;n )atthethreshold  X  , we need to find only the first item A l ( j&lt;l  X  n ) such that upper (  X  { A j ,A By Lemma 2, if upper (  X  { A j ,A l } ) &lt; X  , we can guarantee that upper (  X  { A j ,A i } ), where l  X  i  X  n , is less than the correla-tion threshold  X  . In other words, all these n  X  l +1 pairs can be pruned without a further computation requirement. AccordingtoLemma1,weget
Since the rank-support function f(k) is monotone decreasing with the increase of the rank k, we get
To make the computation simple, we let l = f  X  1 (  X  2 f ( j ))+ 1. Therefore, for a given item A j (1 &lt;j  X  n ), the compu-tation cost for ( n  X  f  X  1 (  X  2 f ( j ))) item pairs can be saved. As a result, the total computation savings of the TAPER algorithm is shown below in Equation 7. Note that the com-putation savings shown in Equation 7 is an underestimated value of the real computation savings which can be achieved by the TAPER algorithm.

Finally, we conduct computation savings analysis on the data sets with some special rank-support distributions. Specif-ically, we consider three special rank-support distributions: a uniform distribution, a linear distribution, and a general-ized Zipf distribution [18], as shown in the following three cases.
 In this case, the rank-support function f ( k )= C ,where C is a constant. According to Equation 3, the upper bound of the  X  correlation coefficient for any item pair is 1, which is the maximum possible value for the correlation. Hence, for any given item A j , we cannot find an item A l ( j&lt;l  X  such that upper (  X  { A j ,A l } ) &lt; X  ,where  X   X  1. As a result, the total computation savings S (  X  )iszero.
 In this case, the rank-support function has a linear distribu-tion and f ( k )= a  X  mk ,where m is the absolute value of the slope and a is the intercept
Lemma 5. When a database has a linear rank-support distribution f ( k ) and f ( k )= a  X  mk ( a&gt; 0 , m&gt; 0 ), for a user-specified minimum correlation threshold  X  , the pruning ratio of the TAPER algorithm increases with the decrease of the ratio a/m , the increase of the correlation threshold  X  , and the increase of the number of items, where 0 &lt; X   X  1 . In this case, the rank-support function has a generalized Zipf distribution and f ( k )= c k p , where c and p are constants and p  X  1. When p is equal to 1, the rank-support function has a Zipf distribution.

Lemma 6. When a database has a generalized Zipf rank-support distribution f ( k ) and f ( k )= c k p , for a user-specified minimum correlation threshold  X  , the pruning ratio of the TAPER algorithm increases with the increase of p and the correlation threshold  X  ,where 0 &lt; X   X  1 .Furthermore,the pruning ratio is independent when the number of items is increased.
 Proof: Since the rank-support function f ( k )= c k p ,the Applying Equation 7, we get: Thus, we can derive three rules as follows:
Therefore, the claim that the pruning ratio of the TAPER algorithm increases with the increase of p and the correlation threshold  X  holds. Also, rule 3 indicates that the pruning ratio is independent when the number of items is increased in data sets with Zipf distributions.
In this section, we present the results of extensive experi-ments to evaluate the performance of the TAPER algorithm. Specifically, we demonstrate: (1) a performance comparison between the TAPER algorithm and a brute-force approach, (2) the effectiveness of the proposed algebraic cost model, and (3) the scalability of the TAPER algorithm.

Experimental Data Sets: Our experiments were per-formed on both real and synthetic data sets. Synthetic data sets were generated such that the rank-support distributions follow Zipf X  X  law, as shown in Figure 6. Note that, in log-log scales, the rank-support plot of a Zipf distribution will be a straight line with a slope equal to the exponent P in the  X  ( c ) Retail  X  ( c ) Retail Figure 6: The plot of the Zipf rank-support distri-butions of synthetic data sets in log-log scale.
 Zipf distribution. A summary of the parameter settings used to generate the synthetic data sets is presented in Table 1, where T is the number of transactions, N is the number of items, C is the constant of a generalized Zipf distribution, and P is the exponent of a generalized Zipf distribution.
The real data sets were obtained from several different application domains. Table 2 shows some characteristics of these data sets. The first five data sets in the table, i.e., pumsb , pumsb  X  , chess , mushroom ,and connect are of-ten used as benchmark for evaluating the performance of association rule algorithms on dense data sets. The pumsb and pumsb  X  data sets correspond to binarized versions of a census data set from IBM 1 . The difference between them is that pumsb  X  does not contain items with support greater than 80%. The chess , mushroom ,and connect data sets are benchmark data sets from UCI machine learning repos-itory 2 .The LA1 data set is part of the TREC-5 collection (http://trec.nist.gov) and contains news articles from the Los Angeles Times. Finally, retail is a masked data set obtained from a large mail-order company.
 Experimental Platform: We implemented TAPER using C++ and all experiments were performed on a Sun Ultra 10 workstation with a 440 MHz CPU and 128 Mbytes of memory running the SunOS 5.7 operating system.
In this subsection, we present a performance comparison between the TAPER algorithm and a brute-force approach using several benchmark data sets from IBM, a UCI machine learning repository, and some other sources, such as retail stores. The implementation of the brute-force approach is
These data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html.
These data sets and data content descriptions are available at http://www.ics.uci.edu/  X  mlearn/MLRepository.html similar to that of the TAPER algorithm except that the filtering mechanism implemented in the TAPER algorithm is not included in the brute-force approach.

Figure 7 shows the relative computation performance of the TAPER algorithm and the brute-force approach on the pumsb , pumsb  X  ,and retail data sets. As can be seen, the performance of the brute-force approach does not change much for any of the three data sets. However, the execution time of the TAPER algorithm can be an order of magnitude faster than the brute-force approach even if the minimum correlation threshold is low. For instance, as shown in Figure 7 (a), the execution time of TAPER on the pumsb data set is one order of magnitude less than that of the brute-force approach at the correlation threshold 0.4. Also, when the minimum correlation threshold increases, the execution time of TAPER dramatically decreases on the pumsb data set. Similar computation effects can also be observed on the pumsb  X  and retail data sets although the computation savings on the retail data set is not as significant as it is on the other two data sets.

To better understand the above computation effects, we also present the pruning ratio of the TAPER algorithm on these data sets in Figure 8. As can be seen, the pruning ra-tio of TAPER on the retail data set is much smaller than that on the pumsb and pumsb  X  data sets. This smaller prun-ing ratio explains why the computation savings on retail is less than that on the other two data sets. Also, Fig-ure 9 shows the pruning ratio of TAPER on UCI connect , mushroom ,and chess data sets. The pruning ratio achieved on these data sets are comparable with the pruning ratio we obtained on the pumsb data set. This indicates that TA-PER also achieves much better computation performance than the brute-force approach on UCI benchmark data sets.
In this subsection, we present the effect of correlation thresholds on the computation savings of the TAPER al-gorithm. Recall that our algebraic cost model shows that the pruning ratio of the TAPER algorithm increases with in-creases of the correlation thresholds for data sets with linear and Zipf-like distributions. Figure 8 shows such an increas-ing trend of the pruning ratio on the pumsb , pumsb  X  ,and retail data sets as correlation thresholds increase. Also, Figure 9 shows a similar increasing trend of the pruning ra-tio on the UCI benchmark datasets including mushroom , chess ,and connect .

One common feature of all the above data sets is the skewed nature of their rank-support distributions. As a re-sult, these experimental results still exhibit a similar trend as the proposed algebraic cost model although the rank-support distributions of these datasets do not follow Zipf X  X  law exactly.
 Table 3: Groups of items for the Retail data set Figure 10: The plot of the rank-support distribu-tions of the retail data set and its three item groups with a linear regression fitting line (trendline).
Recall that the algebraic cost model for data sets with a linear rank-support distribution provides rules which in-dicate that the pruning ratio of the TAPER algorithm in-creases with the decrease of the ratio a/m and the pruning ratio increases with the increas e of the correlation threshold. In this subsection, we empirically evaluate the effect of the Figure 11: Pruning ratios with the decrease of a/m for data sets with linear rank-support distribution. ratio a/m on the performance of the TAPER algorithm for data sets with a linear rank-support distribution.
First, we generated three groups of data from the re-tail data set by sorting all the items in the data set in non-decreasing order and then partitioning them into four groups. Each of the first three groups contains 4700 items and the last group contains 362 items. The first three groups are the group data sets shown in Table 3. Figure 10 (a) shows the plot of the rank-support distribution of the retail data set and Figure 10 (b), (c), and (d) shows the plots of the rank-support distributions of three groups of data generated from the retail data set. As can be seen, the rank-support distributions of the three groups approximately follow a lin-ear distribution. Table 3 lists some of the characteristics of these data set groups. Each group has the same number of items and transactions but a different a/m ratio. Group I has the highest a/m ratio and Group III has the lowest a/m ratio. Since the major difference among these three data set groups is the ratio a/m , we can apply these data sets to show the impact of the a/m on the performance of the TAPER algorithm. Figure 11 shows the pruning ratio of the TAPER algorithm on the data set with linear rank-support distributions. As can be seen, the pruning ratio increases as the a/m ratio decreases at different correlation thresholds. The pruning ratio also increas es as correlation thresholds are increased. These experimental results confirm the trend exhibited by the cost model. Figure 12: The increase of pruning ratios with the increase of p for data sets with Zipf-like distribution. In this subsection, we examine the effect of the exponent P on the performance of the TAPER algorithm for data Figure 13: The plot of the rank-support distribution of the LA1 data set in log-log scale. sets with a generalized Zipf rank-support distribution. We used the synthetic data sets presented in Table 1 for this ex-periment. All the synthetic data sets in the table have the same number of transactions and items. The rank-support distributions of these data sets follow Zipf X  X  law but with different exponent P . Figure 12 displays the pruning ratio of the TAPER algorithm on data sets with different expo-nent P . Again, the pruning ratios of the TAPER algorithm increase with the increase of the exponent P at different cor-relation thresholds. Also, we can observe that the pruning ratios of the TAPER algorithm increase with the increase of the correlation thresholds. Recall that the proposed al-gebraic cost model for data sets with a generalized Zipf dis-tributions provides two rules which confirm the above two observations. Figure 14: The effect of database dimensions on the pruning ratio for data sets with Zipf-like rank-support distributions.
In this subsection, we show the scalability of the TAPER algorithm with respect to database dimensions. Figure 13 shows the plot of the rank-support distribution of the LA1 data set in log-log scale. Although this plot does not follow Zipf X  X  law exactly, it does show Zipf-like behavior. In other words, the LA1 data set has an approximate Zipf-like dis-tribution with the exponent P =1 . 406. In this experiment, we generated three data sets, with 12000, 18000, and 24000 items respectively, from the LA1 data set by random sam-pling on the item set. Due to the random sampling, the three data sets can have almost the same rank-support distribu-tions as the LA1 data set. As a result, we used these three Figure 15: The effect of database dimensions on the execution time for data sets with Zipf-like rank-support distributions. generated data sets and the LA1 data set for our scale-up experiments.
 For data sets with Zipf-like rank-support distributions, Figure 14 shows the effect of database dimensions on the performance of the TAPER algorithm. As can be seen, the pruning ratios of the TAPER algorithm show almost no change or slightly increase at different correlation thresh-olds. This indicates that the pruning ratios of the TAPER algorithm can be maintained when the number of items is increased. Recall that the proposed algebraic cost model for data sets with a generalized Zipf distribution exhibits a similar trend as the result of this experiment.

Finally, in Figure 15, we show that the execution time for our scale-up experiments increases linearly with the in-crease of the number of items at several different minimum correlation thresholds.
In this paper, we proposed using an upper bound of the  X  correlation coefficient, which shows a conditional mono-tonic property. Based on this upper bound, we designed an efficient two-step filter-and-refine algorithm, called TAPER, to search all the item pairs with correlations above a user-specified minimum correlation threshold. In addition, we provided an algebraic cost model to quantify the computa-tion savings of TAPER. As demonstrated by our experimen-tal results on both real and synthetic data sets, the pruning ratio of TAPER can be maintained or even increases with the increase of database dimensions, and the performance of TAPER confirms the proposed algebraic cost model. There are several potential directions for future research. First, we plan to generalize the TAPER algorithm as a stan-dard algorithm for efficient computation of other measures of association. In particular, we will examine the potential upper bound functions of other measures for their monotone property. Second, we propose to extend our methodology to answer correlation-like queries beyond pairs of items. Fi-nally, we will extend the TAPER algorithm to find all pairs of high negatively correlated items. This work was partially supported by NASA grant # NCC 2 1231, DOE/LLNL W-7045-ENG-48, and by Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAD19-01-2-0014. The content of this work does not necessarily reflect the position or policy of the government and no official en-dorsement should be inferred. Access to computing facilities was provided by the AHPCRC and the Minnesota Super-computing Institute. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] R. Bayardo, R. Agrawal, and D. Gunopulos.
 [3] S. Brin, R. Motwani, and C. Silverstein. Beyond [4] C.Bucila,J.Gehrke,D.Kifer,andW.M.White.
 [5] D. Burdick, M. Calimlim, and J. Gehrke. Mafia: A [6] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, [7] W. DuMouchel and D. Pregibon. Empirical bayes [8] G.Grahne,L.V.Lakshmanan,andX.Wang.Efficient [9] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [10] C. Jermaine. The computational complexity of [11] C. Jermaine. Playing hide-and-seek with correlations. [12] S. K. Kachigan. Multivariate Statistical Analysis: A [13] R. Ng, L. Lakshmanan, J. Han, and A. Pang.
 [14] R. Rastogi and K. Shim. Mining optimized association [15] H. T. Reynolds. The Analysis of Cross-classifications . [16] R. Rymon. Search through systematic set [17] H. Xiong, , S. Shekhar, P. Tan, and V. Kumar. Taper: [18] G. Zipf. Human Behavior and Principle of Least
