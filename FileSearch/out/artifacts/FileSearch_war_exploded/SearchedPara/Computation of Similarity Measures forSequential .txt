 The ability to operate on sequential data is a vital prerequi site for application of machine learning techniques in many challenging domains. Examples of such ap plications are natural language pro-cessing (text documents), bioinformatics (DNA and protein sequences) and computer security (byte streams or system call traces). A key instrument for handlin g such data is the efficient computation of pairwise similarity between sequences. Similarity meas ures can be seen as an abstraction between particular structure of data and learning theory.
 original ideas of Watkins [4] and Haussler [5] and extending to application-specific kernels such as the ones for text and natural language processing [e.g. 6 X  8], bioinformatics [e.g. 9 X 14], spam filtering [15] and computer security [e.g. 16; 17].
 Although kernel-based learning has gained a major focus in m achine learning research, a kernel function is obviously only one of various possibilities for measuring similarity between objects. The choice of a similarity measure is essentially determine d by (a) understanding of a problem and (b) properties of the learning algorithm to be applied. Some algorithms operate in vector spaces, others in inner product, metric or even non-metric feature s paces. Investigation of techniques for learning in spaces other than RKHS is currently one of the act ive research fields in machine learning [e.g. 18 X 21].
 The focus of this contribution lies on general similarity me asures for sequential data, especially on efficient algorithms for their computation. A large number o f such similarity measures can be ex-pressed in a generic form so that a simple linear-time algori thm can be applied for computation of a wide class of similarity measures . This algorithm enables the investigation of alternative r epresen-tations of problem domain knowledge other than kernel funct ions. As an example, two applications are presented for which replacement of a kernel  X  or equivale ntly, the Euclidean distance  X  with a different similarity measure yields a significant improvem ent of accuracy in an unsupervised learn-ing scenario.
 The rest of the paper is organized as follows. Section 2 provi des a brief review of common simi-larity measures for sequential data and introduces a generi c form in which a large variety of them can be cast. The generalized suffix tree and a corresponding a lgorithm for linear-time computation of similarity measures are presented in Section 3. Finally, the experiments in Section 4 demon-strate efficiency and utility of the proposed algorithm on re al-world applications: network intrusion detection, DNA sequence analysis and text processing. 2.1 Embedding of sequences A common way to define similarity measures for sequential dat a is via explicit embedding into a high-dimensional feature space. A sequence x is defined as concatenation of symbols from a finite alphabet  X  . To model the content of a sequence, we consider a language L  X   X   X  comprising subse-quences w  X  L . We refer to these subsequences as words , even though they may not correspond to a natural language. Typical examples for L are a  X  X ag of words X  [e.g. 22], the set of all sequences of fixed length ( k -grams or k -mers) [e.g. 10; 23] or the set of all contained subsequences [e.g. 8; 24]. Given a language L , a sequence x can be mapped into an | L | -dimensional feature space by calcu-lating an embedding function  X  follows where occ( w, x ) is the number of occurrences of w in x ,  X  a numerical transformation, e.g. a conversion to frequencies, and W a weighting assigned to individual words, e.g. length-depe ndent or position-dependent weights [cf. 3; 24]. By employing the feature space induced through L and  X  , one can adapt many vectorial similarity measures to operat e on sequences.
 The feature space defined via explicit embedding is sparse, s ince the number of non-zero dimensions for each feature vector is bounded by the sequence length. Th us the essential parameter for measur-ing complexity of computation is the sequence length, denot ed hereinafter as n . Furthermore, the length of a word | w | or in case of a set of words the maximum length is denoted by k . 2.2 Vectorial similarity measures Several vectorial kernel and distance functions can be appl ied to the proposed embedding of sequen-tial data. A list of common functions in terms of L and  X  is given in Table 1.
 application to sequential data [25]. The coefficients are co nstructed using three summation variables a,b and c , which in the case of binary vectors correspond to the number of matching component pairs (1-1), left mismatching pairs (0-1) and right mismatc hing pairs (1-0) [cf. 26; 27] Common similarity coefficients are given in Table 2. For applicatio n to non-binary data these summation variables can be extended as proposed in [25]: 2.3 A generic representation One can easily see that the presented similarity measures ca n be cast in a generic form that consists of an outer function  X  and an inner function m : Given this definition, the kernel and distance functions pre sented in Table 1 can be re-formulated in terms of  X  and m . Adaptation of similarity coefficients to the generic form ( 2) involves a re-formulation of the summation variables a , b and c . The particular definitions of outer and inner functions for the presented similarity measures are given i n Table 3. The polynomial and RBF ker-nels are not shown since they can be expressed in terms of a lin ear kernel or a distance respectively. The key to efficient comparison of two sequences lies in consi dering only the minimum of words necessary for computation of the generic form (2) of similar ity measures. In the case of kernels only the intersection of words in both sequences needs to be considered, while the union of words is needed for calculating distances and non-metric similar ity coefficients. A simple and well-known approach for such comparison is representing the words of ea ch sequence in a sorted list. For words of maximum length k such a list can be constructed in O ( kn log n ) using general sorting or O ( kn ) using radix-sort. If the length of words k is unbounded, sorted lists are no longer an option as the sorting time becomes quadratic.
 Thus, special data structures are needed for efficient compa rison of sequences. Two data structures previously used for computation of kernels are tries [28; 29 ] and suffix trees [30]. Both have been using matching statistics [24]. In this contribution we wil l argue that a generalized suffix tree is suitable for computation of all similarity measures of the form (2) in O ( n ) run-time. simplest way to construct a generalized suffix tree is to exte nd each string x i with a delimiter $ to apply a suffix tree construction algorithm [e.g. 32] to the concatenation of strings x 1 $ In the remaining part we will restrict ourselves to the case o f two strings x and y delimited by # and $ , computation of an entire similarity matrix using a single G ST for a set of strings being a straightforward extension. An example of a generalized su ffix tree for the strings  X  X ab# X  and  X  X abab$ X  is shown in Fig. 1(a). Once a generalized suffix tree is constructed, it remains to d etermine the number of occurences occ( w, x ) and occ( w, y ) of each word w present in the sequences x and y . Unlike the case for kernels for which only nodes corresponding to both sequence s need to be considered [24], the con-tributions must be correctly computed for all nodes in the generalized suffix tree. The following simple recursive algorithm computes a generic similarity m easure between the sequence x and y in one depth-first traversal of the generalized suffix tree (cf. Algorithm 1).
 all suffixes of x and y , every word w in x and y is represented by at least one leaf. Whether a leaf contributes to x or y can be determined by considering the edge at the leaf. Due to t he uniqueness of the delimiter # , no branching nodes can occur below an edge containing # , thus a leaf node at an edge starting before the index of # must contain a suffix of x ; otherwise it contains a suffix of y . The contributions of all leaves are aggregated in two varia bles x and y during a post-order traversal. At each node the inner function m of (2) is calculated using  X  ( x ) and  X  ( y ) according to the embedding  X  in (1). A snapshot of the traversal procedure is illustrated in Fig. 1(b). To account implicit nodes along the edges of the GST and to sup port weighted embeddings  X  , the weighting function W EIGHT introduced in [24] is employed. At a node v the function takes the arguments to determine how much the node and edge contribute to the similarity measure, e.g. for k -gram models only nodes up to a path depth of k need to be considered.
 Algorithm 1 Suffix tree comparison Similarly to the extension of string kernels proposed in [33 ], the GST traversal can be performed on an enhanced suffix array [34] for further run-time and space r eduction.
 To prove correctness of our algorithm, a different approach must be taken than the one in [24]. We cannot claim that the computed similarity value is equivale nt to the one returned by the matching statistic algorithm, since the latter is restricted to kern el functions. Instead we show that at each recursive call to the M ATCH function correct numbers of occurences are maintained.
 Theorem 1. A word w occurs occ( w, x ) and occ( w, y ) times in x and y if and only if M ATCH (  X  w ) returns x = occ( w, x ) and y = occ( w, y ) , where  X  w is the node at the end of a path from the root reassembling w in the generalized suffix tree of x and y .
 Proof. If w occurs m times in x , there exist exactly m suffixes of x with w as prefix. Since w corresponds to a path from the root of the GST to a node  X  w all m suffixes must pass  X  w . Due to the unique delimiters # each suffix of x corresponds to one leaf node in the GST whose incoming edge contains # . Hence m equals occ( w, x ) and is exactly the aggregated quantity x returned by M
ATCH (  X  w ). Likewise, occ( w, y ) is the number of suffixes beginning after # and having a prefix w , which is computed by y . 4.1 Run-time experiments In order to illustrate the efficiency of the proposed algorit hm, we conducted run-time experiments on three benchmark data sets for sequential data: network conn ection payloads from the DARPA 1999 IDS evaluation [35], news articles from the Reuters-21578 d ata set [36] and DNA sequences from the human genome [14]. Table 4 gives an overview of the data se ts and their specific properties. We compared the run-time of the generalized suffix tree algor ithm with a recent trie-based method supporting computation of distances. Tries yield better or equal run-time complexity for computa-tion of similarity measures over k -grams than algorithms using indexed arrays and hash tables . A detailed description of the trie-based approach is given in [25]. Note that in all of the following experiments tries were generated in a pre-processing step a nd the reported run-time corresponds to the comparison procedure only.
 For each of the three data sets, we implemented the following experimental protocol: the Manhattan distances were calculated for 1000 pairs of randomly select ed sequences using k -grams as an em-bedding language. The procedure was repeated 10 times for va rious values of k , and the run-time was averaged over all runs. Fig. 2 compares the run-time of se quence comparison algorithms using the generalized suffix trees and tries. On all three data sets the trie-based comparison has a low run-time for small values of n but grows linearly with k . The algorithm using a generalized suffix tree is independent from complexity of the embedding langua ge, although this comes at a price of higher constants due to a more complex data structure. It is o bvious that a generalized suffix tree is the algorithm of choice for higher values of k . 4.2 Applications As a second part of our evaluation, we show that the ability of our approach to compute diverse sim-ilarity measures pays off when it comes to real applications , especially in an unsupervised learning scenario. The experiments were performed for (a) intrusion detection in real network traffic and (b) transcription start site (TSS) recognition in DNA sequence s.
 For the first application, network data was generated by memb ers of our laboratory using virtual network servers. Recent attacks were injected by a penetrat ion-testing expert. The distance-based anomaly detection method Zeta [17] was applied to 5-grams ex tracted from byte sequences of TCP connections using different similarity measures: the line ar kernel, the Manhattan distance and the Kulczynski coefficient. The results on network data from the HTTP protocol are shown in Fig. 3(a). Application of the Kulczynski coefficient yields the highes t detection accuracy. Over 78% of all attacks are identified with no false-positives in an unsuper vised setup. In comparison, the linear kernel yields roughly 30% lower detection rates.
 The second application focused on TSS recognition in DNA seq uences. The data set comprises fixed length DNA sequences that either cover the TSS of protein cod ing genes or have been extracted ran-domly from the interior of genes [14]. We evaluated three met hods on this data: an unsupervised k -nearest neighbor (kNN) classifier, a supervised and bagged kNN classifier and a Support Vec-tor Machine (SVM). Each method was trained and tested using a linear kernel and the Manhattan distance as a similarity measure over 4-grams. Fig. 3(b) sho ws the performance achieved by the the Manhattan distance yield similar accuracy in a supervis ed setup, their performance differs sig-nificantly in unsupervised application. In the absence of pr ior knowledge of labels the Manhattan distance expresses better discriminative properties for T SS recognition than the linear kernel. For the supervised application the classication performance is bo unded for both similarity measures, since only some discriminative features for TSS recognition are e ncapsulated in n -gram models [14]. Kernel functions for sequences have recently gained strong attention in many applications of ma-chine learning, especially in bioinformatics and natural l anguage processing. In this contribution we have shown that other similarity measures such as metric d istances or non-metric similarity co-efficients can be computed with the same run-time complexity as kernel functions. The proposed algorithm is based on a post-order traversal of a generalize d suffix tree of two or more sequences. During the traversal, the counts of matching and mismatchin g words from an embedding language are computed in time linear in sequence length  X  regardless o f the particular kind of chosen lan-guage: words, k -grams or even all consecutive subsequences. By using a gene ric representation of the considered similarity measures based on an outer and inn er function, the same algorithm can be applied for various kernel, distance and similarity functi ons on sequential data.
 Our experiments demonstrate that the use of general similar ity measures can bring significant im-provement to learning accuracy  X  in our case observed for uns upervised learning  X  and emphasize importance of further investigation of distance-and simil arity-based learning algorithms. Acknowledgments The authors gratefully acknowledge the funding from Bundesministerium f  X  ur Bildung und Forschung under the project MIND (FKZ 01-SC40A) and would like to thank Klaus-Robert M  X uller and Mikio Braun for fruitful discussions and support.

