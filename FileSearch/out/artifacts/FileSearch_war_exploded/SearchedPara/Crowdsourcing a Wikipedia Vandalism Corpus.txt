 We report on the construction of the PAN Wikipedia vandalism cor-pus, PAN-WVC-10, using Amazon X  X  Mechanical Turk. The corpu s compiles 32 452 edits on 28 468 Wikipedia articles, among whi ch 2 391 vandalism edits have been identified. 753 human annotat ors cast a total of 193 022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved leve l of agreement was analyzed in order to label an edit as  X  X egular X  or  X  X andalism. X  The corpus is available free of charge. 1 Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval]: Systems and Software X  Performance Evaluation General Terms : Experimentation Keywords : Wikipedia, Vandalism Detection, Evaluation, Corpus Wikipedia is an encyclopedia written by the crowd. The key to Wikipedia X  X  success is a collaborative writing process, wh ere ev-erybody can edit every article. Ideally, the reader of an art icle also revises it to the best of her abilities, e.g. by correcting er rors, by improving the writing style, by adding missing information , or by removing redundancy. In this way Wikipedia X  X  articles get c ontinu-ously improved and updated. This  X  X reedom of editing X  gave t he lie to those who suggested that the resulting articles would be c harac-terized by poor quality and instability. Wikipedia thrives . There is no free lunch, however, and Wikipedia faces problems that li mit its growth, such as vandalism, edit wars, and lobbyism. Our conc ern is the automatic detection of vandalism in Wikipedia, i.e., th e detec-tion of edits that were made with bad intentions. We contribu te to this research field by developing a large corpus of human-ann otated edits, which is a prerequisite for the meaningful evaluatio n of van-dalism detection algorithms. In particular, we report on ou r efforts to use Amazon X  X  Mechanical Turk as a possibility to drive the cor-pus size to the necessary order of magnitude without comprom ising the corpus quality.
 Related Work. Although vandalism has been observed in Wikipe-dia right from the start, and, although vandalism is often de emed one of Wikipedia X  X  biggest problems, research has addresse d auto-matic vandalism detection only recently X  X or the first time i n [3, 5, 7]. Vandalized articles often get restored rather quickl y by other editors, but still, the authors of [6] find that the number of t imes vandalized articles get viewed amounts up to hundreds of mil lions, and that the probability of encountering vandalism grew exp onen-tially between 2003 and 2006. In reaction to this developmen t, the Wikipedia community has developed a number of rule-based ro bots that are capable of restoring the most obvious cases of vanda lism, or that aid editors to do so [2]. However, the performance of t he robots is surpassed, for instance, by an approach based on ma chine learning [5]. Other reactions include the temporary suspen sion of the freedom of editing for articles that are often vandalize d, which threatens the very idea of Wikipedia.

The first vandalism corpus was the Webis-WVC-07, which con-sists of 940 human-annotated edits of which 301 are vandalis m [4]. The PAN-WVC-10 is two orders of magnitude larger and has been annotated by many different people; it thus forms a more repr e-sentative sample of vandalism and allows for better estimat es of whether a vandalism retrieval model will actually work in pr actice. In this respect, the Mechanical Turk provides an exciting ne w way to scale up corpus construction, which has also been applied suc-cessfully, e.g., to recreate TREC assessments [1]. Corpus Layout. An edit marks the transition from one article re-vision to another. On Wikipedia, each revision of every arti cle is accessible by means of a permanent identifier, so that an edit is de-scribed uniquely by a pair of revision IDs referencing the ol d article revision and the new revision. 2 Basically, our corpus is a list of re-vision ID pairs along with labels whether or not the respecti ve edit is vandalism. Moreover, for each edit meta information is gi ven as well as the plain texts of both the old and the new article revi sion. Corpus Acquisition. Our sample of edits is drawn from the revision histories of Wikipedia articles by means of probability pro portional to size sampling, where in our case, the  X  X ize X  of an article i s the average number of times it gets edited in a given time frame. W e hypothesize that the average edit ratio of an article correl ates with the number of times it gets viewed. In that case, our edit samp le resembles well the distribution of article importance at th e time of sampling, which presumably also influences the articles cho sen by vandals. By contrast, the edits of the Webis-WVC-07 were cho sen in search for vandalism from articles whose topics, per se, h ave a high conflict potential, which reveals a sample bias of that c orpus. Corpus Annotation. Amazon X  X  Mechanical Turk is a platform for paid crowdsourcing. It acts as an intermediary between work ers and so-called requesters who offer tasks and a reward for eac h task being solved. Typically, task assignment and result submis sion is handled double-blind. This sense of anonymity and the fact t hat real money can be earned tempts some workers to fake results i n order to get paid without working. Requesters therefore may ap-prove or reject results while workers are paid only if their r esults are approved X  X hich in turn of course tempts requesters to rejec t ac-ceptable results to save the money. From this it becomes clea r that requesters need to analyze the results obtained via the Mech anical Turk to sort out bad workers, while the type, design, and rewa rd of a task may influence their amount significantly. Hence, a ta sk should be designed so as to make faithful work more worthwhil e than deception. In our case we first presented workers with a l ist of links to edits, and along each link, a form to select whethe r the linked edit is regular or vandalism. This simple and straigh tfor-ward design led 80 % of the workers to quickly select options a t random without clicking on the associated link. We therefor e re-designed our task as a dialog that shows the worker one edit at a time, along the aforementioned form. This lowers the bar for faith-ful work since no additional interactions are necessary, an d at the same, faking results requires the same amount of interactio n.
Before compiling our own corpus, we have first evaluated the quality of the annotations obtained via the Mechanical Turk by re-annotating the Webis-WVC-07 edits. This allows to deter mine whether and how scaling up vandalism annotation works. Addi -tionally, we surveyed how often the workers use Wikipedia. Corpus Annotation Accuracy. Table 1 shows the results of two rounds of re-annotating the Webis-WVC-07, each with a diffe rent number of annotators per edit. When considering three annot ators per edit, two things can happen: all annotators agree with ea ch other, or it is two against one. Moreover, in each of these cas es the annotators either agree or disagree with the gold standard. In 56 % of the cases the annotators achieve perfect agreement with t he gold standard, while in 2 % of the cases 3 annotators disagree com-pletely. We have analyzed the latter edits and found that in h alf of these cases the annotations of the Webis-WVC-07 are wrong . In the other half of the disagreement cases we found that, with a ratio of about 3:1, more vandalism edits were considered regular t han the other way around. We have conducted the same analysis for a ro und of 16 annotators per edit, only this time we consider more tha n 2/3 agreement among annotators as sufficient, while less agr eement is considered a tie. Again, the majority of annotators eithe r agree or disagree with the gold standard. We find that 93 % of the edit s are annotated in accordance with the Webis-WVC-07. The rema in-der of the edits either correspond to the erroneous cases men tioned above or they are truly tough calls, even for an expert. Altog ether, when considering only edits on which more than 2/3 of the anno ta-tors agree, the classification accuracies are 96 % and 99 %, wh ich increase significantly over the baseline.
 Worker Survey. The results of the survey are summarized in Ta-ble 2: while the majority of workers read Wikipedia daily to w eekly a much smaller proportion also edit articles. 2 % of the worke rs admittedly vandalized Wikipedia. Interestingly, most of t he work-Table 2: Wikipedia usage of 753 Mechanical Turk workers.
 ers do not often notice vandalism, however, when considerin g only workers who edit daily to monthly, the picture is turned upsi de down: Wikipedia X  X  editors often have to restore vandalism. In any case, these numbers have to be taken with a grain of salt: they are not representative of all Wikipedia users, and there is no wa y of knowing whether the workers answered truthfully. In an atte mpt to minimize false answers, filling out the questionnaire was op tional.
In sum, we pursued the following strategy to generate our cor -pus: 33 000 edits were sampled from Wikipedia and annotated b y 3 annotators each. All edits on which no more than 2/3 of its an no-tators agreed were re-annotated by another 3 annotators, an d again, until ties were resolved or their number was small enough to b e re-viewed manually. We observe that the number of tie edits decr eases exponentially with each iteration:
In order to check up on the worker X  X  success in annotating edi ts, every 5th edit to be classified was in fact a vandalism edit cho sen at random from the Webis-WVC-07. From iteration 3 onwards, how -ever, the check edits were chosen from the vandalism edits al ready identified. This way, these edits received more votes than ne ces-sary, but in the long run, false positives may have been retra cted. The 70 edits that were still tied after the 8th iteration have been re-viewed by two experts who made a decision about them to the bes t of their knowledge. A handful of edits turned out to be undeci d-able, and those were given the benefit of the doubt. Finally, s ome of the edits became inaccessible along the way due to errors o r ad-ministrative removal on the side of Wikipedia. A total of 32 4 52 edits were successfully annotated of which 2 391 are vandali sm.
