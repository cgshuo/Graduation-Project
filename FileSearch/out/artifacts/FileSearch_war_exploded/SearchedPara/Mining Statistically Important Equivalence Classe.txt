 The support-con dence framework is the most common mea-sure used in itemset mining algorithms, for its antimono-tonicity that e ectively simpli es the search lattice. This computational convenience brings both quality and statisti-cal aws to the results as observed by many previous studies. In this paper, we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square, risk ratio, odds ratio, etc. Our algorithm is based on the concept of equivalence classes . An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions. Therefore, itemsets within an equivalence class all share the same level of statistical signi cance regardless of the variety of test statistics. As an equivalence class can be uniquely deter-mined and concisely represented by a closed pattern and a set of generators, we just mine closed patterns and genera-tors, taking a simultaneous depth-rst search scheme. This parallel approach has not been exploited by any prior work. We evaluate our algorithm on two aspects. In general, we compare to LCM and FPclose which are the best algo-rithms tailored for mining only closed patterns. In particu-lar, we compare to epMiner which is the most recent algo-rithm for mining a type of relative risk patterns, known as minimal emerging patterns. Experimental results show that our algorithm is faster than all of them, sometimes even mul-tiple orders of magnitude faster. These statistically ranked patterns and the eciency have a high potential for real-life applications, especially in biomedical and nancial elds where classical test statistics are of dominant interest. H.2.8 [ Database Management ]: Database applications| Data mining
Jinyan was full-time working at I 2 R till June 30, 2007 where this work was done.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms, Performance Equivalence classes, itemsets with ranked statistical merit
Given a database consisting of classes of transactions, it is important to nd frequent itemsets that are capable of making statistically ne distinctions between the classes, e.g. in the elds of evidence-based medicine, sequence motif detection, nancial stocks portfolio construction, risk man-agement, etc. The statistical signi cance of these frequent itemsets can be measured by various test statistics such as the widely known chi-square, relative risk, odds ratio, and so on [28, 30]. However, there have been a lack of ef-cient algorithms that can nd and rank statistically im-portant patterns from large datasets that meet a certain signi cance threshold. Many itemsets mining algorithms using support and con dence as thresholds cannot accom-plish this|sometimes, they even cause statistical aws to the results [1, 6, 10]. As an innovative work on statistical data mining, Li et al [15] proposed a so-called \plateau-redundance" theorem which proves that many \plateaus" of itemsets can be located at the same level of signi cance under relative risk and odds ratio. However, the work [15] did not examine ecient mining and implementation de-tails; also whether the method can handle other test statis-tics remains unknown. Tan et al [25] conducted an excellent comparative study to select the right interestingness mea-sure for association rules, but their random-and-disjoint al-gorithm is heavily dependent on subjective domain expertise and it is also computationally expensive. As a type of rela-tive risk patterns, minimal emerging patterns [9, 17, 24, 20, 11] have long been proposed to capture statistically impor-tant di erence between two classes, or trends from temporal databases. However, all known algorithms for mining emerg-ing patterns cannot handle multi-class data eciently and have not noticed the redundance problem in the minimal patterns. Therefore, in this paper, we investigate ( i ) how to discover a concise representation of statistically important itemsets to get rid of the redundance, ( ii ) how to easily unify the method for a wide range of test statistics, and ( iii ) how to easily extend to handling multi-class data.

The key idea of our method is to mine a concise represen-tation of equivalence classes [4] of frequent itemsets [2] from a transactional database D .

An equiv alence class EC is a set of itemsets that alw ays occur together in some transactions of D . That is, for all X and Y 2 EC , f D ( X ) = f D ( Y ), where f D ( Z ) = f T 2 D j Z T g . Supp ose D consists of two classes of data D and D 2 , let X be an itemset of D , and assume f D ( X ) consists of n 1 and n 2 transactions from D 1 and D 2 resp ectiv ely. Then n 1 and n 2 are sucien t information to calculate various test statistics of X . For instance, X 's relative risk in D i D and D j ( j 6 = i ) is By de nition of equiv alence classes, it is not dicult to see that for any other itemset in the same equiv alence class of X , that itemset is at the same level of signi cance as X under a given test statistics.

Equiv alence classes have a nice prop erty that: An equiv-alence class EC can be uniquely and concisely represen ted by a close d pattern [21] C and a set G of gener ators [21, 4], in the form of EC = [ G ; C ] ; where [ G ; C ] = f X j there is G 2 G ; G X C g . Therefore, just closed patterns and generators of D are sucien t to guaran tee completeness| all and exactly those frequen t itemsets meeting the speci-ed statistical signi cance threshold can be deterministically iden ti ed. One could use the best algorithms for mining closed patterns [26, 12, 27, 32] and generators [7, 16] sep-arately , and then com bine their outputs to solv e the prob-lem. However, this brute-force approac h has to access to the database at least twice, and incurs a large overhead to prop-erly pair up the closed patterns with their corresp onding generators. To achiev e high eciency , we mine closed pat-terns and generators of D sim ultaneously under one depth-rst searc h framew ork. This novel algorithm is even faster than state-of-the-art closed pattern mining algorithms suc h as LCM [26] (version 3) and FPclose [12] that do not output generators at the same time.

As a more challenging problem in this work, we investi-gate how to pinp oint a special subt ype of equiv alence classes called non-redundan t -discriminative equiv alence classes. This concept is an extension to jumping emerging patterns [9, 17] (recen tly also kno wn as minimal emerging patterns [20, 11]). Using this concept, we not only can iden tify pat-terns with strong relativ e risk, but also can group the mini-mal emerging patterns into equiv alence classes, reducing the redundance to a minim um degree. As those -discriminativ e equiv alence classes are rarely located at the bottom, while very often at the middle, of the searc h lattice, breadth-rst searc h algorithms would cost huge amoun t of memory and examine man y unnecessary candidates. We re ne our main algorithm to skip all low-rank ed equiv alence classes, so that our algorithm can pro duce the results with high eciency . We compare it to the state-of-the-art epMiner algorithms [20, 11]; exp erimen tal results sho w that our al-gorithm can be multiple orders of magnitude faster for the same mining tasks.

Contribution by this paper : (1) We focus on the min-ing of a concise represen tation of statistically imp ortan t equiv alence classes with rank ed merit from large datasets. Our work is con trast to the pioneer CBA metho d by Liu et al. [18] or its varian ts whic h mine individual classi ca-tion rules measured by only supp ort and con dence. By our approac h, redundan t patterns can be greatly reduced, and sophisticated test statistics can be applied. (2) We mine closed patterns and generators sim ultaneously under one depth-rst searc h scheme. The eciency is even higher than the best algorithms that are tailored for mining only closed patterns. We are the rst to prop ose suc h a sim ulta-neous mining. (3) Our algorithm can be easily uni ed for a wide range of test statistics, and can disco ver and rank sta-tistically imp ortan t patterns in one go. This is very helpful to nd those top-rank ed patterns according to various com-binations of statistical merits. (4) Our algorithm and results will be helpful for those mac hine learning applications con-taining multiple classes of data. This is because traditional learning metho ds [3, 14, 31, 22] have to organize the mul-tiple classes of data through exhaustiv e pairwise coupling approac hes (one-vs-one or all-vs-all), leading to very hea vy computational redundancy . However, our mining algorithm does not require any pairwise coupling of data.

Paper organization : Section 2 formally de nes the re-searc h problems. Section 3 presen ts our computational steps to solv e the problems. Section 4 describ es details of the min-ing algorithm. Section 5 rep orts a performance evaluation of our algorithms in comparison with other metho ds. Section 6 discusses related works. Section 7 concludes the pap er.
We consider n items i 1 , i 2 , ..., i n . A dataset is a set of transactions . A transaction is a non-empt y set of items. An itemset is a set of items. The supp ort of an itemset P in a data set D , denoted sup ( P; D ), is the percen tage of transactions in D that con tain P . The negate d supp ort of P in D is sup ( P; D ) = 1 sup ( P; D ).
 Problem 1 : Giv en k non-empt y classes (datasets D 1 , D , ..., D k ) of transactions, we investigate how to
For a pair of datasets D p and D n , we assume that a test statistics on an itemset X , denoted st ( X; D ) where D = D D n , is a function of the supp orts of X in D p , D n , and/or in D . That is, st ( X; D ) = h ( sup ( X; D p ) ; sup ( X; D for some function h . This assumption is not very restrictiv e, as it is sucien tly general to cover a broad range of widely used test statistics. For example, under the odds ratio (OR) test: A chi-square test with 1 degree of freedom is: where x = sup ( X; D p ) jD p j , y = sup ( X; D n ) jD n (1 sup ( X; D p )) jD p j , and y = (1 sup ( X; D n )) jD longer list of statistical tests allo wable under our assump-tion includes relativ e risk test, Studen t's t , Fisher sign test, likeliho od ratio, Yule's Q , and Yule's Y [28, 30, 25].
Only frequen t itemsets are in our consideration, because infrequen t itemsets are likely to be random noise patterns, and the inclusion of infrequen t itemsets would much enlarge the size of mining results whic h is deemed not unfa vorable to the end users. We also note that the ranking of itemsets under di eren t statistical tests can be quite di eren t. Their intersections certainly con tain some itemsets that possess man y statistical merits. Di eren t statistics may also be bet-ter suited for di eren t applications|e.g., OR is applicable to case-con trol studies, while relativ e risk is applicable to cohort studies [30]. Thus we are interested in unifying our mining metho d to cover various test statistics.

Problem 1 can be reduced to the mining of closed pat-terns [21] and generators [21, 4] of frequen t equiv alence classes [4] from D .

Definition 1. Let X be an itemset of a dataset D . The equiv alence class of X in D , denote d [ X ] D , is the set f A j f D ( A ) = f D ( X ) g , wher e f D ( Z ) = f T 2 D j Z T g . The maximal itemset and the minimal itemsets of [ X ] D are called the close d pattern and the gener ators of this equiva-lenc e class respectively. The close d patterns and gener ators of D are all the close d patterns and gener ators of their equiv-alenc e classes.

Pr oper ty 1. Let C be the close d pattern of an equiva-lenc e class EC and G be a gener ator of EC . Then all item-set X satisfying G X C are also in this equivalenc e class.

This convexity prop erty says that the entire equiv alence class can be concisely bounded as EC = [ G ; C ], where G is the set of generators of EC , C is the closed pattern, and [ G ; C ] = f X j there is G 2 G , G X C g .
 It is easy to see that equiv alence classes do not overlap.
Pr oper ty 2. Let D be a dataset. Let EC 1 and EC 2 be distinct equivalenc e classes of D . Then EC 1 \ EC 2 = fg .
Cor ollar y 1. Let D be a data set and ms be a minimum supp ort threshold. Then the set F ms D of frequent itemsets of D can be exactly partitione d into frequent equivalenc e classes without overlapping. That is, wher e C 1 ; C 2 , ..., C m are m frequent close d patterns of D , G is the set of gener ators of [ C i ] D .
 For j [ C i ] D j = 1|i.e., only one itemset in this equiv alence class|w e simplify [ f C i g ; C i ] as C i . Therefore, S is a true concise and lossless represen tation of F ms D .
As we assume that the statistical signi cance of an item-set X is a function h of X 's global and local supp ort in-formation, the class lab el information of the transactions that con tain X are imp ortan t. We refer to this informa-tion as the class label distribution of X , denoted n i ( X ) = j f
D ( X ) \ D i j ; i = 1 ; 2 ; ; k . As all itemsets within an equiv-alence class have the same class lab el distribution, it is easy to see the follo wing prop erty:
Pr oper ty 3. All itemsets within an equivalenc e class shar e the same level of statistic al signi c anc e.

Therefore, Problem 1 can be solv ed by mining the genera-tors and closed patterns of frequen t equiv alence classes, and their asso ciated class lab el distribution information.
Example 1. Supp ose we are given 4 transactions cate-gorize d into two classes (class 1 and class 2). Class 1 has two transactions f a; b; c g and f a; b; c; d g ; Class 2 has trans-actions f u; v; w g and f u; v; w; x g . Let the minimum supp ort threshold be 30% . Then ther e are two frequent close d pat-terns: abc and uvw . Their corresponding equivalenc e classes concisely represent the total 14 frequent itemsets (not in-cluding the empty set). The class label distribution of abc is: n 1 ( abc ) = 2 and n 2 ( abc ) = 0 ; while the class label dis-tribution of uvw is: n 1 ( uvw ) = 0 and n 2 ( uvw ) = 2 . Thus, the statistic al imp ortanc e of EC 1 and EC 2 can be calculate d straightforwar d.

Problem 2 : As a special and most useful case of Prob-lem 1, we also investigate how to directly output a subt ype of equiv alence classes that are -discriminative and non-redundant .

Definition 2. Let D = S k i =1 D i . Let EC be a frequent equivalenc e class of D , and C be the close d pattern in EC . Let n i ( C ) ; i = 1 ; 2 ; ; k , be the class label distribution in-formation of C . Then EC is a -discriminative equivalenc e class if ther e is i 2 f 1 ; 2 ; ; k g such that P j 6 = i wher e is usual ly a smal l inte ger numb er 1 or 2.
In other words, every itemset in a -discriminativ e equiv-alence class occurs in only one of the k classes with almost no occurrence in any other classes. The follo wing table il-lustrates an example, where every itemset in EC sho ws an outstanding occurrence in D 1 , while few occurrence in any of the other classes, exhibiting a sharp di erence between D 1 versus all other classes.
The de nition of -discriminativ e equiv alence classes con-ceptually di ers from jumping emerging patterns (JEPs) [17, 9, 20, 11]. A JEP is an itemset whic h occurs in one class, but nev er occurs in the other class. Thus, our notion looks at the itemsets in groups, but the notion of JEPs treats them individually; We highligh t the redundancy within an equiv-alence class, while JEPs did not notice the hidden redun-dancy problem. Also, itemsets in a -discriminativ e equiv-alence class are required to be frequen t, but JEPs may be infrequen t.

Itemsets in a -discriminativ e equiv alence class often have a statistical value of in nit y under the odds ratio test or the relativ e risk test. We use EC 1 D to denote all -discriminativ e equiv alence classes of D .

Definition 3. Let D be a dataset. Let EC 1 ; EC 2 2 EC 1 D be two equivalenc e classes of D satisfying C 1 C 2 , wher e C 1 and C 2 are the close d patterns of EC 1 and EC 2 respec-tively. Then EC 2 is said to be a redundant -discriminative equivalenc e class with respect to EC 1 .

This notion of redundancy mak es sense, because C 1 C 2 and thus f D ( C 2 ) f D ( C 1 ) by de nition. In other words, C 's transaction set is entirely subsumed by C 1 's transaction set. This is true for every itemset in EC 2 . Therefore, EC can be considered as a redundan t equiv alence class.
It follo ws that only the most general (minimal) equiv a-lence classes of EC 1 D are non-redundan t. Here equiv alence classes are ordered so that an equiv alence class EC 1 is said to be more general than another equiv alence class EC 2 if EC 1 's closed pattern is more general than that of EC 2 .
Example 2. Following Example 1, let the minimum sup-port threshold be 10% . Then ther e are 4 frequent equiv-EC 2 = [ f u; v; w g ; uvw ] , and EC 0 2 = [ f x g ; uvwx ] . Observe that all the 4 equivalenc e classes are -discriminative, be-cause each of them has a zero occurr ence in either Class 1 or Class2. However, only EC 1 and EC 2 are non-r edundant: EC 0 1 is redundant to EC 0 1 because abcd abc , and EC 0 redundant to EC 2 because uvwx uvw .

Non-redundan t -discriminativ e equiv alence classes are of-ten located in the middle of the searc h lattice. Thus, breadth-rst searc h algorithms would cost huge amoun t of memory and examine man y unnecessary candidates. We re ne our main algorithm (solving Problem 1) to solv e Problem 2.
Giv en k non-empt y classes of transactions| D 1 , D 2 , ..., and D k |and a supp ort threshold ms , our metho d to dis-cover and rank statistically imp ortan t equiv alence classes for the k classes of data (Problem 1) consists of the follo wing 5 steps: 1. Let D = S k i =1 D i . 2. Mine frequen t closed patterns and generators to con-3. For every frequen t closed pattern X , determine the 4. For a test statistics h and a pair of datasets D p and 5. Rank all X as per st h ( X; D p [ D n ), and output sig-
The main computational part is at Step 2 for mining closed patterns and generators from D . Step 3 can be actu-ally integrated into Step 2. Steps 4{5 are rep eatable when di eren t test statistics and di eren t class lab el pairs are speci ed. This rep etition does not require access to D . Therefore, our metho d can be easily extended to unify a broad range of test statistics without much extra computa-tional cost. The completeness of the algorithm is guaran teed at Step 2 by Corollary 1. All patterns at the same level of statistical signi cance are concisely represen ted as sho wn in Step 5. Thus, our metho d is indeed a uni ed style, accessing the data only once, and yet pro viding sucien t information for determining the concise represen tation of the statisti-cally imp ortan t patterns, and for calculating the multiple test statistics in one run. Our metho d also handles multi-class data.

To solv e Problem 2, we re ne Step 2. The implemen tation is easy as we adopt a depth-rst searc h strategy on a set-enumeration tree [23]|w e just stop the depth-rst searc h along a branc h whenev er we reac h a -discriminativ e equiv-alence class. This re nemen t can save much cost as low-rank ed redundan t equiv alence classes are not touc hed in the entire mining pro cess. Details are describ ed in Section 4.2.
This section presen ts our novel ideas for the sim ultaneous mining of closed patterns and generators on top of a mo di-ed data structure of the seminal FP-tree [13]. The FP-tree structure, originally prop osed for ecien tly mining frequen t itemsets [13], has also been used before to mine frequen t closed patterns by FPclose [12]. FPclose uses conditional FP-trees to generates candidate closed patterns, then these candidates are inserted into a separate pattern tree to chec k whether they are true closed patterns or not. The critical dra wbac k of FPclose is the separate pattern tree structure whic h consumes a large amoun t of memory and whic h is also a bottlenec k for speeding up the eciency . However, we mo dify FP-tree in suc h a way that we can directly out-put closed patterns one-b y-one, without an additional data structure to chec k whether the patterns are true closed pat-terns or not. Our novel mo di cations also include a smaller head table, a smaller pro jected database, and a new sub-tree structure storing all full-supp ort items. With these sub-tle and e ectiv e mo di cations, our algorithm is capable of a sim ultaneous mining of both closed patterns and genera-tors in a high eciency . The mining speed is even faster than existing best algorithms for mining closed patterns alone. Our algorithm is termed as DPMiner ( d iscriminativ e p attern m iner).
A brute-force metho d to mine equiv alence classes is to use one of the best closed pattern mining algorithms [32, 27, 19, 12, 26] and the best generator mining algorithms [7, 16], and then asso ciate those generators with their closed patterns. However, this approac h explores the searc h space twice, and has a considerable overhead to matc h up the closed patterns and generators, as men tioned in Introduction.

DPMiner speeds up the eciency by integrating the min-ing of closed patterns and generators into one depth-rst framew ork. It constructs a conditional database for eac h frequen t generator; A tail sub-tree is main tained together with these conditional databases to generate the closure of eac h generator. The anti-monotonicit y [21] of generators (two versions), review ed below, are frequen tly exploited in the mining:
Pr oper ty 4. Let D be a dataset and F G be the set of frequent gener ators of D . (1) If X 2 F G , then Y 2 F G for all Y X . (2) If X 62 F G , then Y 62 F G for all Y X .
Pr oper ty 5. Let X be a frequent itemset of D . X is a gener ator if and only if sup ( X; D ) &lt; sup ( Y; D ) for every imme diate proper subset Y of X .
In a normal FP-tree [13], all frequen t items are sorted into a descending frequency order in the head table, our mo di cation is that frequen t items with a full supp ort |i.e., those items that app ear in every transaction of the original database or conditional pro jected databases|m ust be re-moved from the head table of FP-trees. The reason is that Table 1: The projected database for F = f d : 5 ; b : 4 ; a : 4 ; c : 3 ; h : 3 ; i : 2 g .
 they and those itemsets containing them are not generators, due to the anti-monotone property of generators. This mod-i cation often leads to very small tree structures.
An example of our modi ed FP-tree is shown in Figure 1, which is constructed from a database in Table 1 containing 6 transactions. The minimum support threshold ms is set to 30%. The frequent item e appears in every transaction, it is thus not allowed to be in the head table. Please refer to [13] for detailed steps for constructing projected transactions, node links, parent links, and conditional FP-trees. After our modi ed FP-tree is constructed, all subsequent mining are operated on top of the tree.
We make use of Property 5 for further pruning of non-generators. The idea is that an itemset X is a generator if and only if sup ( X; D ) &lt; sup ( Y; D ) for every immediate proper subset Y of X . Therefore, X can be identi ed as a generator, or ltered out, by just comparing the support of X with that of X 's (immediate) subsets. To be able to do this checking during the mining process, the subsets of an itemset must be discovered prior to that itemset. To ensure this ordering, we built conditional databases according to the descending frequency order of items in the head table.
We present details as follows. When mining a generator l 's conditional database D l , DPMiner rst traverses D l nd frequent items in D l , denoted as F l = f a 1 , a 2 , , a and removes each item a i from F l if l [f a i g is not a gen-erator, and then constructs a new FP-tree which stores the conditional databases of the remaining items in F l . More speci cally, it checks whether sup ( l [f a i g ; D )= sup ( l; D ) is true for every item a i in F l . If sup ( l [f a i g ; D )= sup ( l; D ) is true for some item a i in F l , it means that l [f a i not a generator, and item a i should be removed from F l . This checking is performed immediately after all the fre-quent items in D l are discovered and it incurs little over-head. DPMiner then checks whether there exists an itemset l such that l 0 ( l [f a i g ) and sup ( l 0 ; D )= sup ( l [f a for each remaining item a i in F l . It is not necessary to compare l [f a i g with all of its subsets. It is adequate to compare l [f a i g with its immediate subsets based on the anti-monotone property of generators.

DPMiner maintains the set of frequent generators that have been discovered so far in a hash table to facilitate the subset checking. The hash function hashes an itemset to an integer by: H ( l ) = P i 2 l h ( i ) mod L table , and h ( i ) = 2 mod 32 + i + 2 order ( i ) mod 32 + order ( i ) + 1, where l is a generator, order ( i ) is the position of item i if the frequent items in the original database are sorted into descending frequency order, and L table is the size of the hash table and it is a prime number. In the above hash function, both the id of an item and the position of an item in descending frequency order are used. It is to reduce the possibility that two di erent items are mapped into the same value. The reason being that the position of an item in descending frequency order depends on the frequency of the item and is independent of the id of the item. Our experiments show that this hash function is very e ective in avoiding con icts.
The FP-tree structure provides an additional pruning op-portunity. If an itemset l 's conditional database D l contains only one branch, then there is no need to construct a new FP-tree from D l even if there are more than one frequent items in D l that can be appended to l to form frequent generators. The reason being that if D l contains only one branch, then for any two items a i and a j in D l , itemset l [f a i ; a j g cannot be a generator because sup ( l [f a = min f sup ( l [f a i g ; D ), sup ( l [f a j g ; D ) g .
Generators in the same equivalence class have the same closure and DPMiner associates generators in the same class together via their common closure. DPMiner maintains the set of closed itemsets in a hash table during the mining pro-cess, and the hash function of the hash table is the same as the one used in the previous subsection. When a genera-tor is identi ed, DPMiner generates its closure, and checks whether the closed itemset is already in the hash table. If it is true, then the generator is added to the equivalence class of the closed itemset. Otherwise, the closed itemset is inserted into the hash table to create a new equivalence class. Next we describe how to generate the closure of each generator.

The closure of a generator l , denoted as C l , is the inter-section of all the transactions containing l . That is, C T t 2D^ l t t . Every item i in ( C l l ) satis es sup ( l [f i g ; D ) = sup ( l; D ), due to convexity of equivalence classes, so we call the items in ( C l l ) cover items with respect to l . Let l length-( j l j -1) pre x of frequent generator l , D l 0 be the con-ditional database of l 0 , T l 0 be the FP-tree constructed from D l 0 , and a i be the last item of l . Note that l = l 0 [f a FP-tree T l 0 stores all conditional databases of the frequent items in D l 0 . In FP-tree T l 0 , any path containing a resents a set of transactions containing l . Correspondingly, any transaction containing l is stored in some branch of T and the branch must contain the item a i . If an item appears in all the branches containing a i and has the same support as a i , then the item should be included in the closure of l . However, not all such items are in D l as D l contains only the upper portion of the branches containing a i , that is, the portion between the nodes containing a i and the root. To obtain all the cover items with respect to l , we need to tra-verse the lower portion of these branches as well, that is, the subtrees rooted at the nodes containing item a i . We call these subtrees tail subtrees of l .

We use the example FP-tree shown in Figure 1 to illus-trate closure generation. There are three transactions con-taining itemset c in the example database shown in Table 1. Correspondingly, there are three branches in the FP-tree shown in Figure 1 containing item c : dbach : 1, dbchi : 1, and dac : 1. To obtain all the cover items with respect to c , we need to access all the nodes in these three branches. The FP-tree nodes containing item c split each of the three branches into two parts. The upper portions of the three branches| dbac , dbc , and dac |form itemset c 's conditional database D c . Every node in D c can be visited by follow-ing the node-links of item c as in the FP-growth algorithm. The lower portions of the three branches| h : 1, hi : 1, and |are the tail subtrees of itemset c . We use the depth-rst strategy to traverse each tail subtree, and move to the next tail subtree following the node-links of item c .

If an item appears in l 's closure, then the item must ap-pear in l 's supersets' closure. If an item a j does not appear in l 's closure but l [f a j g is frequent, it is still possible that a j appears in l 's supersets' closure. An item a j can either appear in D l or in the tail subtrees of l . Therefore, when constructing the FP-tree storing all the frequent items in D , we need to include the frequent items in the tail sub-trees of l as well. The frequent items in the tail subtrees of l are not candidate extensions of l as a generator, so they are not put into the header table. In the FP-tree constructed from D l , these items are always put after the items in the header table so that they can appear in the tail subtrees of l 's supersets but they never appear in l 's superset's condi-tional databases. We use an example to illustrate this. In Figure 1, there are two frequent items a and b in c 's condi-tional database D c . We need to construct an FP-tree from D . Item h does not appear in D c but it is frequent in the tail subtrees of c , so we need to include h in the FP-tree constructed from D c , and item h is put after item a and b in this FP-tree as shown in Figure 2. When we traverse the tail subtrees of cb , we nd that item h is in the closure of cb .
 Figure 2: The FP-tree of itemset c with frequent tail items
To generate the closure of a generator l , DPMiner needs to traverse all the tail subtrees of l besides traversing D an FP-tree node is always in the tail subtrees of all of its an-cestors, the number of times that an FP-node is additionally visited is equal to the number of ancestors of the FP-tree node. As we also need to include frequent tail items into FP-trees for closure generation, our FP-trees can be sometimes large.

To solve this problem, DPMiner identi es and prunes tail subtrees that do not contain any potential cover items as soon as possible. Let B be a branch in generator l 's condi-tional database, l B be the set of items contained in branch B , and T B be the tail subtree rooted at the last node of branch B . If an item i in T B is a cover item of l 1 , where l l 1 l [ l B , then the support of item i in T B must be the same as that of branch B , because otherwise there ex-ists at least one transaction that contains l 1 but does not contain item i . The support of an FP-tree node is always no larger than that of its ancestors. Therefore, if the support sum of the children node of an FP-node in T B is less than the support of the FP-node, then there is no need to access the descendants of the FP-node. After T B is traversed, we check the support of the items in T B and discard those items whose support in T B is less than the support of the branch B . After all the tail subtrees are visited, the support of the items are accumulated over all the tail subtrees, and fre-quent items are included into the FP-tree constructed from D . We use an example to illustrate this pruning. In Figure 1, item a 's conditional database D a contains two branches: abd : 2 and ad : 2. We rst visit the rst tail subtree of a and get the support of items c and h . The second tail subtree of a is a single branch and the support of the rst node of this single branch is 1, which is less than the support of branch ad : 2. Therefore, we discard that single branch. After the two tail subtrees are visited, we accumulate the support of items c and h . The support of item c is less than 2, so we discard item c from further consideration. If the above pruning technique is not used, then item c would not be pruned, as its accumulated support in the two tail sub-trees is 2. Therefore, the above pruning technique not only avoids traversing unnecessary tail subtrees but also prunes those items that are not cover-items in a very early stage.
To rank equivalence classes, the class label distribution information of the itemsets are required as shown in Sec-tion 3. We accomplish this by replacing the support count of an FP-node with the class label distribution of the FP-node. For example, if transactions 1, 3, and 5 in Table 1 are in class 0 and the other three transactions are in class 1, then the FP-tree is modi ed as shown in Figure 3. The class distribution of an item in the FP-tree is computed by accumulating the class distribution of the FP-nodes contain-ing that item. The global support of an item is the sum of the support of the item in all the classes. Generators and closed itemsets are identi ed and generated as described in the previous subsection.
 Figure 3: Class distribution information are added into FP-tree nodes for ranking equivalence classes.
Algorithm 1 shows the pseudo-codes of DPMiner to solve our Problem 1 , which can be also re ned to solve our Problem 2 . As shown, DPMiner maintains the set of Algorithm 1 The DPMiner Algorithm frequen t generators F G in a hash table. DPMiner chec ks whether itemset l [ f a i g is a generator by searc hing the im-mediate subsets of l [ f a i g in the hash table (Line 10). If l [ f a i g is not a generator, then it is remo ved from F l (Line 11); otherwise it is inserted into the hash table (Line 13). DP-Miner also main tains the set of closed itemsets F C in a hash table, and eac h closed itemset represen ts an equiv a-lence class. Generators in the same equiv alence class are as-sociated together by their common closed itemset. If the clo-sure of a generator is already in the hash table, then the gen-erator is added to the equiv alence class of the closed itemset (Line 8). Otherwise, the closed itemset is inserted into the hash table to create a new equiv alence class (Lines 6{7). To generate closed itemsets, DPMiner main tains the set of cover items C l on the curren t path during the mining pro-cess to propagate the cover items with resp ect to l to l 's sup ersets' closure. Besides the cover items inherited from l 's pre x, the other cover items of l can either from l 's con-ditional database (Lines 2{3) or from the tail subtrees of l (Lines 4{5).

To solv e Problem 2 (mining -discriminativ e equiv alence classes (ECs) that are also non-redundan t), we mak e use of the follo wing observ ation. If an EC is -discriminativ e and non-redundan t, then none of its subset ECs can be -discriminativ e and none of its sup erset ECs can be non-redundan t. Therefore, mining non-redundan t -discriminativ e ECs is just to nd a border in the set-en umeration tree [23] suc h that the patterns above the border (close to the top root node empt y set) are not -discriminativ e and the pat-terns below the border are redundan t. So, we re ne line 10 of Algorithm 1|W e just chec k whether l [ f a i g is a -discriminativ e patterns. If it is, then we remo ve a i from F .
DPMiner main tains two hash tables during the mining pro cess. One hash table stores generators for chec king the minimalit y of generators. The other one stores closed item-sets for asso ciating generators with their closed itemsets. When the num ber of generators or closed itemsets is large, the space occupied by these two hash tables may exceed the size of the main memory . To solv e this problem, we can asso ciate generators with their closed itemsets in a post-pro cessing step to avoid keeping the second hash table in the memory . It is possible that the hash table storing gen-erators is larger than the main memory . In this case, we can use the CFP-tree structure [19] to store frequen t generators. The in-core querying time of CFP-tree may be a little longer than the hash table, but CFP-tree requires much less space than the hash table and it also supp orts ecien t retriev al of itemsets on disk.

DPMiner constructs a conditional database for eac h gen-erator and the closure of eac h generator is generated during the mining pro cess. An equiv alence class may have multiple generators. Therefore, a closed itemset may be generated multiple times in the algorithm. An alternativ e approac h is to construct a conditional database for eac h closed itemset to ensure every closed itemset is generated only once, and then generate the generators of eac h closed itemset. The reason for us to adopt the rst approac h is that items that cannot app ear in the closure of a generator are iden ti ed and pruned very early in DPMiner. Therefore, generating the closure of generators incurs little overhead in the al-gorithm. However, it is much more dicult and costly to generate generators from closed itemsets.

The metho d used by DPMiner for generating the closure of generators can be used alone to mine only frequen t closed itemsets. An itemset is a closed itemset if and only if the closure of the itemset is the same as the itemset itself. A tric k due to this prop erty is that: If an itemset l is not a closed itemset and the closure of l con tains an item i suc h that item i does not app ear in l 's conditional database, then there is no need to pro cess l 's conditional database. The reason being that any closed itemset con taining l must also con tain item i (because the closure of l con tains item i ), and all of the itemsets disco vered from l 's conditional database con tain l , but none of them con tain item i . Therefore, none of the itemsets disco vered from l 's conditional database can be closed, and we can skip mining them. Therefore, if an itemset is not a closed itemset, then there is no need to pro cess the conditional database of the itemset. By using this metho d, we do not need to main tain the set of frequen t closed itemsets in the memory , but most of existing frequen t closed itemset mining algorithms suc h as CLOSET+ [27], and FPclose [12] require this. We used the follo wing 8 benc hmark datasets to evaluate DPMiner: where `MaxTL' and `AvgTL' indicate the maximal and aver-age transaction length. The rst four datasets are obtained from the UCI mac hine learning rep ository . The BMS-POS and pumsb datasets have been also widely used to evaluate the performance of frequen t itemset mining algorithms, and they are available at http://fimi.cs.helsinki.fi/d ata/ . These two datasets do not con tain class information. The ALL-AML and LungCancer datasets (available at http: //research.i2r.a-star.edu.s g/rp/ ) con tain gene expres-sion levels of leuk emia or lung cancer patien ts. They are challenging, as discussed in [20], because every transaction con tains hundreds (865 in ALL-AML) or even thousands (2172 in LungCancer) of items. The exp erimen ts were con-ducted on a 3.60Ghz Pentium IV with 2GB memory running Fedora core. All codes were compiled using g++.
To evaluate the eciency of DPMiner for mining equiv-alence classes, we compare DPMiner with LCM [26] and FPclose [12]. We obtained the source codes of LCM and FPclose from their corresp ondence authors. The LCM al-gorithm represen ts the state-of-the-art algorithm for mining frequen t closed itemsets. It sho ws the best performance in a comparativ e study of frequen t closed itemset mining algo-rithms; see http://fimi.cs.helsinki.fi/fi mi04/ . Here we use its new est version|LCM ver. 3 [26]. The main tech-nique of LCM is a paren t-child relationship de ned on closed patterns whic h constructs tree-shap ed transv ersal routes com-posed of only closed patterns. FPclose is the winning algo-rithm in FIMI03; see http://fimi.cs.helsinki.fi/fi mi03/ . FPclose uses conditional FP-trees to generate candidate closed patterns, then these candidates are inserted into a separate pattern tree to chec k whether they are true or not closed patterns. DPMiner di ers from all of them as we store the data using a mo di ed version of the FP-tree, and we output both closed patterns and generators in parallel directly from those FP-trees.

As said, the main purp ose of DPMiner is to output both closed patterns and generators in parallel. To directly com-pare with FPclose and LCM, we pro vide an optional imple-men tation of DPMiner that mines and outputs only closed itemsets by using the closure generation technique describ ed in Section 4.1.3 and Section 4.3. We term this option of DP-Miner as \DPMiner-close". We found that our algorithm is not only faster, but also we use less amoun t of memory as we do not have to store all the closed patterns in the memory .
Figure 4 sho ws the running time of the algorithms on the rst six datasets. DPMiner performs better than LCM3 on four datasets. The DPMiner-close algorithm is signi can tly faster than LCM3 and FPclose on all the datasets except BMS-POS. This indicates that our closure generation tech-nique is very ecien t. The reason why DPMiner-close is slower than LCM3 and FPclose on BMS-POS is that BMS-POS is a very sparse dataset, the cost for building FP-trees from sparse datasets is relativ ely high.
Loekito and Bailey [20] prop osed an excellen t algorithm, called epMiner, for mining minimal emerging patterns based on zero-suppressed binary decision diagrams. The epMiner algorithm is claimed to signi can tly outp erform another state-of-the-art algorithm prop osed by Fan and Kotagiri [11]. Giv en two classes of data D pos and D neg , the epMiner algorithm mines those minimal patterns occurring frequen tly ( &gt; %) in the positiv e class and infrequen tly ( &lt; %) in the negativ e class. Suc h patterns are kno wn as minimal emerging pat-terns [9]|patterns with good relativ e risk and odds ratio test values.
 For a fair comparison with the epMiner algorithm [20], DPMiner tak es the option of mining non-redundan t -discriminativ e equiv alence classes under the setting and Then, all the generators of these equiv alence classes are ex-actly the minimal %-% emerging patterns that the ep-Miner algorithm outputs. As DPMiner also outputs the closed patterns of the equiv alence classes, strictly speaking, we can actually ignore the mining of closed patterns to save more cost for a fairer comparison.
 Figure 5 sho ws the running time of DPMiner and ep-Miner. From this gure, we can see that DPMiner is con-stan tly faster than epMiner with multiple orders of magni-tude on the two high-dimensional gene expression datasets when varying and . (Our running time included the time for mining the closed patterns. The exp erimen ts on the ALL-AML dataset was terminated when the time reac hed 50000 seconds; on the LungCancer dataset was terminated when reac hed 10000 seconds.)
The high eciency of DPMiner is mainly attributed to the pre x trees (the revised FP-tree structure) to store the data, and the use of closed patterns and generators to con-cisely represen t the equiv alence classes. We also note that for handling multiple-class transactions, DPMiner can gain even more eciency , as the epMiner algorithm [20] has to be run rep eatedly on multiple pairwise com binations of the datasets.
Cong et al. [8] studied the problem of mining top-k cover-ing rule groups from binary classes of gene expression data. Unlik e our work dealing with statistically signi can t equiv-alence classes, the notion of top-k rule groups is limited to only asso ciation rules whic h are rank ed by the rules' con dence and supp ort level. A rule group 1 is rank ed higher than rule group 2 if 1 :conf &gt; 2 :conf or 1 :supp &gt; :supp when 1 :conf = 2 :conf . By this ranking criteria, top-k rule groups can con tain much redundancy . For ex-ample, in situations where 1 :conf = 2 :conf , 1 's sup erset closed patterns could be all in the top-k patterns. Suc h a top-k rule group con tains a very high redundancy because k 1 of them are totally subsumed to the rst one in terms of their transaction set. Second, the mining of generators by Cong et al. is a post-mining pro cess after mining closed patterns, and it tak es a breadth-rst searc h strategy . How-ever, DPMiner tak es a parallel depth-rst approac h to min-ing both closed patterns and generators. Our algorithm is an order of magnitude faster than Cong et al on man y benc h-mark datasets. (Detailed comparison results are not sho wn here due to space constrain t.) Our previous work by [15] is also related to this work. The previous one was focused on a theoretical study on a concise represen tation of odds ratio patterns and relativ e risk patterns from binary classes of data, by using the con-cept of \plateaus". However, the curren t work does not use the concept of \plateaus". Instead, we presen t new data structures, searc h strategies, and algorithm details to mine statistically imp ortan t equiv alence classes in general, and mine non-redundan t -discriminativ e equiv alence classes in particular. This work also pro vides new ideas to unify the metho d suitable for broad range of test statistics and mul-tiple classes of transactions.

Recen t work on mining contr ast sets by STUCCO [5, 29] is relev ant to our work. Unlik e emerging patterns signifying the supp ort ratio of an itemset between two classes of trans-actions, a con trast set is an itemset emphasizing the absolute supp ort di erence. They are a kind of statistical itemsets under absolute risk reduction test [28, 30]. Therefore, con-trast sets can be considered as a special case of this work. Also, STUCCO uses a breadth-rst searc h framew ork. Al-though it can nicely handle datasets con taining two classes of transactions, it may become incon venien t when the data con tain multiple classes of transactions as its pruning and ltering ideas become not that straigh tforw ard.

Our algorithm and results will be useful for the tradi-tional mac hine learning eld, esp ecially when the applica-tion involves multiple classes of data. This is because the traditional learning metho ds [14, 3, 31, 22] have to orga-nize the multiple classes of data through two typical pair-wise coupling approac hes. One is the one-vs-al l approac h| (a) ALL-AML (varying ) (c) Lung Cancer (varying ) Figure 5: Running time comparison between DP-Miner and epMiner. constructing all possible data pairwise com binations of one class against all the rest. The other is the all-vs-al l approac h| constructing all possible pairs of single classes. Then, the base learning algorithms are rep eated man y times to nd patterns characterizing every class| k times in the one-vs-all metho d, and k ( k 1) = 2 times in the all-vs-all metho d, where k is the num ber of classes. However, our mining algorithm can totally avoid suc h a hea vy computational redundancy no matter how man y classes are involved.
In this pap er, we have investigated how to ecien tly mine a concise represen tation of statistically imp ortan t equiv a-lence classes. The key idea is to mine the generators, closed patterns, and the class lab el distribution information of these itemsets. We prop osed DPMiner that tak es a depth-rst searc h strategy for mining both closed patterns and genera-tors sim ultaneously under a revised FP-tree data structures. DPMiner-close (and DPMiner itself ) is much faster than the best closed pattern mining algorithms LCM3 and FPclose. We also re ned DPMiner to ecien tly pro duce a subt ype of equiv alence classes called non-redundan t -discriminativ e equiv alence classes. Exp erimen t results sho w that this re-ned algorithm is magni cen tly faster than epMiner, the most recen t algorithm for the same mining tasks. As a fu-ture work, we will tak e adv antage of the high eciency for sup ervised classi cation problems where the classical statis-tical tests are of strong interests.
 We thank to James Bailey for the source code of epMiner, and to Jian Pei and Guozh u Dong for their commen ts. [1] C. C. Aggarw al and P. S. Yu. A new framew ork for [2] R. Agra wal, T. Imielinski, and A. Swami. Mining [3] E. L. Allw ein, R. E. Schapire, and Y. Singer. [4] Y. Bastide, R. Taouil, N. Pasquier, G. Stumme, and [5] S. D. Bay and M. J. Pazzani. Detecting group [6] T. Brijs, K. Vanho of, and G. W. G. De ning [7] T. Calders and B. Goethals. Depth-rst non-deriv able [8] G. Cong, K.-L. Tan, A. K. H. Tung, and X. Xu.
 [9] G. Dong and J. Li. Ecien t mining of emerging [10] W. DuMouc hel and D. Pregib on. Empirical bayes [11] H. Fan and K. Ramamohanarao. Fast disco very and [12] G. Grahne and J. Zhu. Fast algorithms for frequen t [13] J. Han, J. Pei, and Y. Yin. Mining frequen t patterns [14] T. Hastie and R. Tibshirani. Classi cation by pairwise [15] H. Li, J. Li, L. Wong, M. Feng, and Y.-P . Tan. [16] J. Li, H. Li, L. Wong, J. Pei, and G. Dong. Minim um [17] J. Li, K. Ramamohanarao, and G. Dong. The space of [18] B. Liu, W. Hsu, and Y. Ma. Integrating classi cation [19] G. Liu, H. Lu, W. Lou, and J. X. Yu. On computing, [20] E. Loekito and J. Bailey . Fast mining of high [21] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [22] R. M. Rifkin and A. Klautau. In defense of one-vs-all [23] R. Rymon. Searc h through systematic set [24] A. Soulet, B. Cremilleux, and F. Rioult. Condensed [25] P.-N. Tan, V. Kumar, and J. Sriv asta va. Selecting the [26] T. Uno, M. Kiy omi, and H. Arim ura. Lcm ver. 3: [27] J. Wang, J. Han, and J. Pei. CLOSET+ : Searc hing [28] S. Wassertheil-Smoller. Biostatistics and [29] G. I. Webb, S. Butler, and D. Newlands. On detecting [30] K. M. Weiss. Genetic Variation and Human Dise ase: [31] T.-F. Wu, C.-J. Lin, and R. C. Weng. Probabilit y [32] M. J. Zaki and C.-J. Hsiao. CHARM: An ecien t
