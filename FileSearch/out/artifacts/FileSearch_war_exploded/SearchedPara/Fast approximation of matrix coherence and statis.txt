 Petros Drineas drinep@cs.rpi.edu Malik Magdon-Ismail magdon@cs.rpi.edu Michael W. Mahoney mmahoney@cs.stanford.edu David P. Woodruff dpwoodru@us.ibm.com The concept of statistical leverage measures the ex-tent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data anal-ysis and in the analysis of randomized matrix al-gorithms ( Mahoney &amp; Drineas , 2009 ; Drineas et al. , 2008 ). A related notion is that of matrix coherence , which has been of interest in recently popular problems such as matrix completion and Nystr  X om-based low-rank matrix approximation ( Candes &amp; Recht , 2009 ; Talwalkar &amp; Rostamizadeh , 2010 ). Statistical lever-age scores have a long history in statistical data anal-ysis, where they have been used for outlier detection in regression diagnostics ( Hoaglin &amp; Welsch , 1978 ; Chatterjee &amp; Hadi , 1986 ). Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical im-plementation and that are useful to domain scien-tists ( Drineas et al. , 2008 ; Mahoney &amp; Drineas , 2009 ; Sarl  X os , 2006 ; Drineas et al. , 2010 ); see ( Mahoney , 2011 ) for a detailed discussion. The na  X  X ve and best previously existing algorithm to compute these scores would compute an orthogonal basis for the dominant part of the spectrum of A , e.g. , the basis provided by the Singular Value Decomposition (SVD) or a basis provided by a QR decomposition, and then use that basis to compute the leverage scores, by taking the norms of the rows.
 We present a randomized algorithm to compute relative-error approximations to every statistical lever-age score in time qualitatively faster than the time re-quired to compute an orthogonal basis. For the case of an arbitrary n  X  d matrix A , with n  X  d , our main algorithm runs in O ( nd log n/ X  2 ) time (under assump-tions on the precise values of n and d , see Theorem 1 for an exact statement). This is the first algorithm to break the O ( nd 2 ) barrier required by the na  X  X ve algo-rithm, and provide accuracy to within relative error. As a corollary, our algorithm provides a relative-error approximation to the coherence of an arbitrary ma-trix in the same time. In addition, we discuss sev-eral practically-important extensions of the basic idea underlying our main algorithm: computing so-called cross leverage scores; computing leverage scores for  X  X at X  matrices with n  X  d with respect to a low-rank parameter k ; and, computing leverage scores in data streaming environments. 1.1. Overview and definitions We start with the following definition.
 Definition 1. Given an arbitrary n  X  d matrix A , with n &gt; d , let U denote the n  X  d matrix consist-ing of the d left singular vectors of A , and let U ( i ) denote the i -th row of the matrix U as a row vector. Then, the statistical leverage scores of the rows of A are given by  X  i = U ( i ) 2 herence  X  of the rows of A is  X  = max i  X  X  1 ,...,n }  X  i it is the largest statistical leverage score of A ; and the ( i, j )-cross-leverage scores c ij are c ij = i.e., they are the dot products between the i th row and the j th row of U .
 Although we have defined these quantities in terms of a particular basis, they clearly do not depend on that particular basis, but only on the space spanned by that basis. To see this, let P A denote the projec-tion matrix onto the span of the columns of A . Then,  X  tistical leverage scores of a matrix A are equal to the diagonal elements of the projection matrix onto the span of its columns. Similarly, the ( i, j )-cross-leverage scores are equal to the off-diagonal elements of this projection matrix, i.e. , c ij = ( P A ) ij = Clearly, O ( nd 2 ) time suffices to compute all the statis-tical leverage scores exactly: simply perform the SVD or compute a QR decomposition of A in order to ob-tain any orthogonal basis for the range of A and then compute the Euclidean norm of the rows of the result-ing matrix. Thus, in this paper, we are interested in algorithms that run in o ( nd 2 ) time. 1.2. Our main result Our main result is a randomized algorithm for com-puting relative-error approximations to every statisti-cal leverage score, as well as an additive-error approx-imation to all of the large cross-leverage scores, of an arbitrary n  X  d matrix, with n  X  d , in time qualita-tively faster than the time required to compute an or-thogonal basis for the range of that matrix. Our main algorithm for computing approximations to the statis-tical leverage scores (see Algorithm 1 in Section 3 ) will amount to constructing a  X  X andomized sketch X  of the input matrix and then computing the Euclidean norms of the rows of that sketch. This sketch can also be used to compute approximations to the large cross-leverage scores (see Algorithm 2 of Section 3 ).
 The following is our main result for Algorithm 1 . Theorem 1. Let A be a full-rank n  X  d ma-trix, with n  X  d ; let  X   X  (0 , 1 / 2] be an error parameter; and recall the de nition of the statis-tical leverage scores  X  i from De nition 1 . Then, there exists a randomized algorithm (Algorithm 1 of Section 3 below) that returns values  X   X  i , for all i  X  d  X  n  X  e d , the running time of the algorithm is O ( Algorithm 1 provides a relative-error approximation to all of the statistical leverage scores  X  i of A and, assuming d ln d = o as a constant, its running time is o ( nd 2 ), as desired. As a corollary, the largest leverage score (and thus the coherence) is approximated to relative-error in the o ( nd 2 ) time.
 The following is our main result for Algorithm 2 . Theorem 2. Let A be a full-rank n  X  d matrix, with n  X  d ; let  X   X  (0 , 1 / 2] be an error parameter; let  X  be a parameter; and recall the de nition of the cross-leverage scores c ij from De nition 1 . Then, there ex-ists a randomized algorithm (Algorithm 2 of Section 3 below) that returns the pairs { ( i, j ) } together with esti-returned, then c 2 ij  X  that are returned,  X  c 2 ij  X  30  X  X  i  X  j  X  c 2 ij  X   X  c Note that by setting  X  = n ln n , we can compute all the  X  X arge X  cross-leverage scores, i.e. , those satisfying c time (treating  X  as a constant). If ln 3 n = o ( d ) the overall running time is o ( nd 2 ), as desired. Due to space limitations, numerous details of our anal-ysis are omitted; a full presentation, including full de-tails of the proofs, can be found in the technical report version of this paper ( Drineas et al. , 2011 ). 1.3. Significance and related work Significance in theoretical computer science. The statistical leverage scores define the key struc-tural nonuniformity that must be dealt with ( i.e. , either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast ran-domized algorithms for matrix problems such as least-squares regression ( Sarl  X os , 2006 ; Drineas et al. , 2010 ) and low-rank matrix approximation ( Sarl  X os , 2006 ; Drineas et al. , 2008 ; Mahoney &amp; Drineas , 2009 ; Boutsidis et al. , 2009 ). Roughly, the best random sampling algorithms use these scores (or the general-ized leverage scores relative to the best rank-k approx-imation to A ) as an importance sampling distribution to sample with respect to. On the other hand, the best random projection algorithms rotate to a basis where these scores are approximately uniform and thus in which uniform sampling is appropriate. See ( Mahoney , 2011 ) for a detailed discussion.
 Applications in statistics. The statistical leverage scores equal the diagonal elements of the so-called  X  X at 1986 ). As such, they have a natural statistical inter-pretation in terms of the  X  X everage X  or  X  X nfluence X  as-sociated with each of the data points. Historically, these quantities have been widely-used for outlier iden-tification in diagnostic regression analysis. Applications in machine learning. Matrix coher-ence arises in many other applications. For example, ( Candes &amp; Recht , 2009 ) is interested in the problem of matrix completion; ( Talwalkar &amp; Rostamizadeh , 2010 ) is interested in Nystr  X om-based low-rank matrix approximation; ( Mohri &amp; Talwalkar , 2011 ) explicitly ask the question of whether matrix coherence be effi-ciently and accurately estimated X  X nd thus our main result provides a positive answer to their question. (In other applications, the largest cross-leverage score is called the coherence ( Tropp , 2004 ; Mougeot et al. ); thus our results also provide a bound for this quantity.) 2.1. Basic linear algebra and notation Let [ n ] denote the set of integers { 1 , 2 , . . . , n } of A as a row vector, and let A ( j ) , j  X  [ d ] denote the j -th column of A as a column vector. Let  X  A  X  2 F =  X  norm of A , and let  X  A  X  2 = sup  X  x  X  note the spectral norm of A . Relatedly, for any vector x  X  R n , its Euclidean norm (or  X  root of the sum of the squares of its elements. The dot product between two vectors x, y  X  R n will be denoted  X  x, y  X  , or alternatively as x T y . Finally, let e i  X  R all i  X  [ n ], denote the standard basis vectors for R n and let I n denote the n  X  n identity matrix. Let the rank of A be  X   X  min { n, d } , in which case the SVD of A is denoted by A = U  X  V T , where U  X  R n  X   X  ,  X   X  R  X   X   X  , and V  X  R d  X   X  . (For a general matrix X , we will write X = U X  X  X V T X .) Let  X  i ( A ) , i  X  denote the i -th singular value of A , and let  X  max ( A ) and  X  min ( A ) denote the maximum and minimum sin-gular values of A , respectively. The Moore-Penrose pseudoinverse of A is the d  X  n matrix defined by A 2.2. The Fast JL Transform (FJLT) Given  X  &gt; 0 and a set of points x 1 , . . . , x n with x JLT), denoted  X   X  R r  X  d , is a projection of the points into R r such that To construct an  X  -JLT with high probability, sim-ply choose every entry of  X  independently, equal to  X   X  (with probability 2 / 3) ( Achlioptas , 2003 ). Let  X  JLT be a matrix drawn from such a distribution over r  X  d matrices. Then, the following lemma holds.
 Lemma 1 (Theorem 1.1 of ( Achlioptas , 2003 )) . Let x , . . . , x n be an arbitrary (but xed) set of points, where x i  X  R d and let 0 &lt;  X   X  1 / 2 be an accuracy parameter. If r  X  1  X  2 ability at least 1  X   X  ,  X  JLT  X  R r  X  d is an  X  -JLT . For our main results, we will also need a stronger requirement than the simple  X  -JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in ( Ailon &amp; Chazelle , 2009 ). Consider an orthogonal matrix U  X  R n  X  d , viewed as d vectors in R n . A FJLT projects the vectors from R n to R r , while preserv-ing the orthogonality of U ; moreover, it does so very quickly. Specifically, given  X  &gt; 0,  X   X  R r  X  n is an  X  -FJLT for U if: I d  X  U T  X  T  X  U X  X  R n  X  d , the matrix product  X  X can be computed in O ( nd ln r ) time. The next lemma follows from the definition of an  X  -FJLT and its proof can be found in ( Drineas et al. , 2006 ; 2010 ).
 Lemma 2. Let A by any matrix in R n  X  d with n  X  d and rank ( A ) = d . Let the SVD of A be A = U  X  V T , let  X  be an  X  -FJLT for U (with 0 &lt;  X   X  1 / 2 ) and let  X  =  X  U = U  X   X   X  V T  X  . Then, all the following hold: 2.3. The Subsampled Randomized Hadamard One can use a Randomized Hadamard Transform (RHT) to construct, with high probability, an  X  -FJLT. Our main algorithm will use this efficient construction in a crucial way. Recall that the (unnormalized) n  X  n matrix of the Hadamard transform  X  H n is defined re-cursively by  X  H 2 n = n  X  n normalized matrix of the Hadamard transform is equal to H n =  X  H n / ity and without loss of generality, we assume that n is a power of 2 and we will suppress n and just write H . Let D  X  R n  X  n be a random diagonal matrix with independent diagonal entries D ii = +1 with proba-bility 1 / 2 and D ii =  X  1 with probability 1 / 2. The product HD is a RHT and it has three useful proper-ties. First, when applied to a vector, it  X  X preads out X  its energy. Second, computing the product HDx for any vector x  X  R n takes O ( n log 2 n ) time. Third, if we only need to access r elements in the transformed vector, then those r elements can be computed in O ( n log 2 r ) time ( Ailon &amp; Liberty , 2008 ). The Sub-sampled Randomized Hadamard Transform (SRHT) randomly samples (according to the uniform distribu-tion) a set of r rows of a RHT.
 Using the sampling matrix formalism described previ-ously ( Drineas et al. , 2006 ; 2008 ; 2010 ), we will rep-resent the operation of randomly sampling r rows of an n  X  d matrix A using an r  X  n linear sampling operator S T . Let the matrix  X  F JLT = S T HD be generated using the SRHT. The most important prop-erty about the distribution  X  F JLT is that if r is large enough, then, with high probability,  X  F JLT generates an  X  -FJLT. We summarize this discussion in the fol-lowing lemma (which is essentially a combination of Lemmas 3 and 4 from ( Drineas et al. , 2010 ), restated to fit our notation).
 Lemma 3. Let  X  F JLT  X  R r  X  n be generated using the SRHT as described above and let U  X  R n  X  d ( n  X  d ) be an (arbitrary but xed) orthogonal matrix. If r  X  least 0 . 9 ,  X  F JLT is an  X  -FJLT for U . 3.1. Outline of our basic approach Recall that our first goal is to approximate, for all i  X  [ n ], the quantities where e i is a standard basis vector. The hard part of computing the scores  X  i according to Eqn. ( 5 ) is computing an orthogonal matrix U spanning the range of A , which takes O ( nd 2 ) time. Since U U T = AA  X  , it follows that where the first equality follows from the orthogonality of (the columns of) U . The hard part of computing the scores  X  i according to Eqn. ( 6 ) is two-fold: first, computing the pseudoinverse; and second, performing the matrix-matrix multiplication of A and A  X  . Both of these procedures take O ( nd 2 ) time. As we will see, we can get around both of these bottlenecks by the ju-dicious application of random projections to Eqn. ( 6 ). To get around the bottleneck of O ( nd 2 ) time due to computing A  X  in Eqn. ( 6 ), we will compute the pseu-doinverse of a  X  X maller X  matrix that approximates A . A necessary condition for such a smaller matrix is that it preserves rank. So, na  X  X ve ideas such as uniformly sampling r 1  X  n rows from A and computing the pseu-doinverse of this sampled matrix will not work well for an arbitrary A . For example, this idea will fail (with high probability) to return a meaningful approxima-tion for matrices consisting of n  X  1 identical rows and a single row with a nonzero component in the direction perpendicular to that the identical rows; finding that  X  X utlying X  row is crucial to obtaining a relative-error approximation. This is where the SRHT enters, since it preserves important structures of A , in particular its rank, by first rotating A to a random basis and then uniformly sampling rows from the rotated ma-trix (see ( Drineas et al. , 2010 ) for more details). More formally, recall that the SVD of A is U  X  V T and let  X  the SRHT of Lemma 3 with the appropriate choice for r ). One could approximate the  X  i  X  X  of Eqn. ( 6 ) by where we approximated the n  X  d matrix A by the r 1  X  d matrix  X  1 A . Computing A ( X  1 A )  X  in this way takes O ( ndr 1 ) time, which is not efficient because r 1 &gt; d (from Lemma 3 ).
 To get around this bottleneck, recall that we only need the Euclidean norms of the rows of the matrix Algorithm 1 Approximating the (diagonal) statisti-cal leverage scores  X  i .
 Input: A  X  R n  X  d (with SVD A = U  X  V T ), error parameter  X   X  (0 , 1 / 2].
 Output:  X   X  i , i  X  [ n ]. 1. Construct  X  1  X  R r 1  X  n to be an  X  -FJLT for U , 2. Compute  X  1 A  X  R r 1  X  d and its SVD,  X  1 A = 3. View the normalized rows of AR  X  1  X  R n  X  d as 4. Construct the matrix product  X  = AR  X  1  X  2 . 5.  X  i  X  [ n ] compute and return  X   X  i =  X  ( i ) 2 dimensionality of this matrix by using an  X  -JLT to re-duce the dimension r 1 =  X ( d ) to r 2 = O (ln n ). Specif-the matrix  X  = A ( X  1 A )  X   X  2 . This n  X  r 2 matrix  X  may be viewed as our  X  X andomized sketch X  of the rows of AA  X  . Then, we can compute and return for each i  X  [ n ], which is essentially what Algorithm 1 does. Not surprisingly, the sketch A ( X  1 A )  X   X  2 can be used in other ways: for example, by considering the dot product between two different rows of this randomized sketching matrix Algorithm 2 approximates the large cross-leverage scores of A . 3.2. Approximating all the leverage scores Our first main result is Algorithm 1 , which takes as input an n  X  d matrix A and an error parameter  X   X  (0 , 1 / 2], and returns as output numbers  X   X  Although the basic idea to approximate ( AA  X  ) ( i ) 2 was described in the previous section, we can im-prove the efficiency of our approach by avoiding the full sketch of the pseudoinverse. In particular, let  X  A =  X  1 A and let its SVD be  X  A = U  X  A  X   X  A V T  X  R thogonalizer for  X  A since U  X  A =  X  AR  X  1 is an orthogonal matrix. In addition, note that AR  X  1 is approximately orthogonal. Thus, we can compute AR  X  1 and use it as an approximate orthogonal basis for A and then compute  X   X  i as the squared row-norms of AR  X  1 . The next lemma states that this is exactly what our main algorithm does.
 Lemma 4. Let R  X  1 be such that Q =  X  1 AR  X  1 is an orthogonal matrix with rank ( Q ) = rank ( X  1 A ) . Then, 3.3. Approximating large cross-leverage scores By combining Lemmas 6 and 7 (in Section 4.1 below) with the triangle inequality, one immediately obtains the following lemma.
 Lemma 5. Let  X  be either the sketching matrix con-structed by Algorithm 1 , i.e. ,  X  = AR  X  1  X  2 , or  X  = A ( X  1 A )  X   X  2 as described in Section 3.1 . Then, the pairwise dot-products of the rows of  X  are additive-error approximations to the leverage scores and cross-leverage scores:  X  U ( i ) , U ( j )  X  X  X  X   X  ( i ) ,  X  That is, if one were interested in obtaining an approx-imation to all the cross-leverage scores to within ad-ditive error (and thus the diagonal statistical lever-age scores to relative-error), then the algorithm which first computes  X  followed by all the pairwise inner products achieves this in time T ( X )+ O T ( X ) is the time to compute  X  from Section 3.2 and r 2 = O (  X  computational complexity and this can be done if one is interested only in the large cross-leverage scores. Our second main result is provided by Algorithms 2 and 3 . Algorithm 2 takes as input an n  X  d matrix A , a parameter  X  &gt; 1, and an error parameter  X   X  estimates  X  c ij satisfying Theorem 2 . The first step of the algorithm is to compute the matrix  X  = AR  X  1  X  2 constructed by Algorithm 1 . Then, the algorithm calls Algorithm 3 as a subroutine to compute  X  X eavy hitter X  pairs of rows from a matrix. 4.1. Proof of Theorem 1 We condition all our analysis on the events that  X  1  X  R JLT for n 2 points in R r 1 . Define  X  u i = e T i A ( X  1  X  u = e T i A ( X  1 A )  X   X  2 . Then,  X   X  i =  X   X  u i  X  2 2 and The proof will follow from the following two lemmas. Algorithm 2 Approximating the large (off-diagonal) cross-leverage scores c ij .
 Input: A  X  R n  X  d and parameters  X  &gt; 1,  X   X  (0 , 1 / 2]. Output: The set H consisting of pairs ( i, j ) together with estimates  X  c ij satisfying Theorem 2 . 1. Compute the n  X  r 2 matrix  X  = AR  X  1  X  2 from 2. Use Algorithm 3 with inputs  X  and  X   X  =  X  (1 + 3. Return the pairs in H as the  X  -heavy pairs of A . Lemma 6. For i, j  X  [ n ] , Lemma 7. For i, j  X  [ n ] , Lemma 6 states that  X   X  u i ,  X  u j  X  is an additive error ap-proximation to all the cross-leverage scores ( i  X  = j ) and a relative error approximation for the diagonals ( i = j ). Similarly, Lemma 7 shows that these cross-leverage scores are preserved by  X  2 . Indeed, with i = j , from Lemma 6 we have |  X   X  i  X   X  i |  X   X  1  X   X   X  i triangle inequality and  X   X  1 / 2:  X  The theorem follows after rescaling  X  .
 Running Times. By Lemma 4 , we can use V estimates. Since  X  1 is an  X  -FJLT, the product  X  1 A can be computed in O ( nd ln r 1 ) while its SVD takes an additional O ( r 1 d 2 ) time to return V  X  1 A  X   X  1  X  an additional O ( r 2 d 2 ) time. Finally, premultiplying by A takes O ( ndr 2 ) time, and computing and returning the squared row-norms of  X  = AV  X  1 A  X   X  1  X  takes O ( nr 2 ) time. So, the total running time is the sum of all these operations, which is O ( nd ln r 1 + ndr 2 + r 1 d 2 + r 2 d 2 ) . For our imple-mentations of the  X  -JLTs and  X  -FJLTs (  X  = 0 . 1), r 1 = O ( Algorithm 3 Computing heavy pairs of a matrix. Input: X  X  R n  X  r with rows x 1 , . . . , x n and a param-eter  X  &gt; 1.
 Output: H = { ( i, j ) ,  X  c ij } containing all heavy (un-ordered) pairs. The pair ( i, j ) ,  X  c ij  X  H if and only if  X  c 1: Compute the norms  X  x i  X  2 and sort the rows ac-2: H X  X } ; z 1  X  n ; z 2  X  1. 3: while z 2  X  z 1 do 5: z 2  X  z 2 + 1. 6: if z 2 &gt; z 1 then 7: return H . 8: end if 9: end while 10: for each pair ( i, j ) where i = z 1 and j  X  12: if  X  c 2 ij  X  X T X 2 F then 13: add ( i, j ) and  X  c ij to H . 14: end if 15: z 1  X  z 1  X  1. 16: end for 17: end while 18: return H .
 It follows that the asymptotic running time is O ( To simplify, suppose that d  X  n  X  e d and treat  X  as a constant. Then, the asymptotic running time is O ( 4.2. Proof of Theorem 2 We first construct an algorithm to estimate the large inner products among the rows of an arbitrary matrix X  X  R n  X  r with n &gt; r . This general algorithm will be applied to the matrix  X  = AV  X  1 A  X   X  1  X  x , . . . , x n denote the rows of X ; for a given  X  &gt; 1, the pair ( i, j ) is heavy if  X  x i , x j  X  2  X  1  X  X T X 2 Cauchy-Schwarz inequality, this implies that so it suffices to find all the pairs ( i, j ) for which Eqn. ( 11 ) holds. We will call such pairs norm-heavy . Let s be the number of norm-heavy pairs satisfying Eqn. ( 11 ). We first bound the number of such pairs. Lemma 8. Using the above notation, s  X   X r .
 Algorithm 3 starts by computing the norms  X  x i  X  2 2 for all i  X  [ n ] and sorting them (in O ( nr + n ln n ) time) so that we can assume that  X  x 1  X  2  X   X  X  X   X   X  x n  X  2 . Then, we initialize the set of norm-heavy pairs to H = {} and we also initialize two pointers z 1 = n and z 2 = 1. The basic loop in the algorithm checks if z 2 &gt; z 1 and stops if that is the case. Otherwise, we increment z 2 to the first pair ( z 1 , z 2 ) that is norm-heavy. If none of pairs are norm heavy ( i.e., z 2 &gt; z 1 occurs), then we stop and output H ; otherwise, we add ( z 1 , z 2 ) , ( z ( z 1 , i ) with i decrease z 1 by one and if z 1 &lt; z 2 we stop and output H ; otherwise, we repeat the basic loop. Note that in the basic loop z 2 is always incremented . This occurs whenever the pair ( z 1 , z 2 ) is not norm-heavy. Since z 2 can be incremented at most n times, the number of times we check whether a pair is norm-heavy and fail is at most n . Every successful check results in the addition of at least one norm-heavy pair into H and thus the number of times we check if a pair is norm heavy (a constant-time operation) is at most n + s . The number of pair additions into H is exactly s and thus the total running time is O ( nr + n ln n + s ). Finally, we must check each norm-heavy pair to verify whether or not it is actually heavy by computing s inner products vectors in R r ; this can be done in O ( sr ) time. Using s  X   X r we get the following lemma.
 Lemma 9. Algorithm 3 returns H including all the heavy pairs of X in O ( nr +  X r 2 + n ln n ) time. To complete the proof, we apply Algorithm 3 with A = U  X  V T . Let u 1 , . . . , u n denote the rows of U ; then, from Lemma 5 , the pair of vectors u i and u j where  X  =  X   X  u i  X  2  X  u j  X  2 , and where the last equality follows from U T U 2 after squaring and using  X  &lt; 0 . 5,  X  u where  X  =  X   X  u i  X  2  X  u j  X  2 . Thus,  X   X  u i ,  X  u j summing Eqn. ( 13 ) over all i, j we get  X  T  X  2 d + 30  X d 2 , or, equivalently, d  X  that  X  u i , u j  X  2  X  d  X  + 12  X   X  u i  X  2  X  u j  X  2 implies By construction, Algorithm 3 is invoked with  X   X  = for which  X  u i , u j  X  2  X  d  X  + 12  X   X  u i  X  2  X  u j  X  2 since every pair returned satisfies  X   X  u i ,  X  u j  X  2 Eqn. ( 13 ), c ij  X  d/ X   X  30  X  X  i  X  j . This proves the first claim of the Theorem; the second claim follows analo-gously from Eqn. ( 13 ).
 Using Lemma 9 , the running time of our approach is O ( by Eqn. ( 14 ),  X   X  =  X   X  T  X  2 overall running time is O Our main result can be extended to the computation of the statistical leverage scores for general  X  X at X  ma-trices, i.e. , matrices A  X  R n  X  d , where both n and d are large, e.g. , d = n or d =  X ( n ), when a rank parameter k  X  min { n, d } is specified. In this case, we wish to ob-tain the statistical leverage scores  X  i = ( U k ) ( i ) 2 A k = U k  X  k V T k , the best rank-k approximation to A . As stated, this is an ill-posed problem; and thus the main technical challenge is to deal with this posedness issue. To do so, we note that the leverage scores are used to approximate the matrix in some way; and thus we only care that the estimated leverage scores are a good approximation to the leverage scores of some good low-rank approximation to A . Thus, we can de-fine the set of matrices that are good approximations, e.g. , with respect to the spectral norm or Frobenius norm, to the best rank k approximation to A , and we can prove that we can efficiently approximate the leverage scores for some matrix in this set.
 Our main result can also be extended to estimate the leverage scores and related statistics in data stream en-vironments. In this context, one is interested in com-puting statistics of the data stream while making one pass over the data from external storage and using only a small amount of additional space. For an n  X  d matrix A , with n  X  d , small additional space means that the space complexity only depends logarithmically on the high dimension n and polynomially on the low dimen-sion d . The general strategy behind our algorithms is as follows. First, as the data streams by, compute T A , for an appropriate problem-dependent linear sketching matrix T , and also compute  X  A , for a random projec-tion matrix  X . Second, after the first pass over the data, compute the matrix R  X  1 , as described in Algo-rithm 1 , corresponding to  X  A (or compute the pseu-doinverse of  X  A or the R matrix from any other QR decomposition of A ). Third, compute T AR  X  1  X  2 , for a random projection matrix  X  2 , such as the one used by Algorithm 1 .
 With the procedure outlined above, the matrix T is effectively applied to the rows of AR  X  1  X  2 , i.e. , to the sketch of A that has rows with Euclidean norms ap-proximately equal to the row norms of U , and pair-wise inner products approximately equal to those in U . Thus statistics related to U can be extracted. For example, in one pass we can: find the indices of all rows of A for with  X  X arge X  leverage scores and compute a (1 +  X  )-approximation to the leverage scores of these large rows; approximately compute statistics such as the entropy of the distribution of leverage scores of A ; and obtain samples of rows of A with proability proportional to their leverage score distribution. More details can be found in the technical report ver-sion of this paper ( Drineas et al. , 2011 ). Achlioptas, D. Database-friendly random projections:
Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences , 66(4):671 X 687, 2003.
 Ailon, N. and Chazelle, B. The fast Johnson-
Lindenstrauss transform and approximate nearest neighbors. SIAM Journal on Computing , 39(1):302 X  322, 2009.
 Ailon, N. and Liberty, E. Fast dimension reduction using Rademacher series on dual BCH codes. In
Proceedings of the 19th Annual ACM-SIAM Sympo-sium on Discrete Algorithms , pp. 1 X 9, 2008.
 Boutsidis, C., Mahoney, M.W., and Drineas, P. An improved approximation algorithm for the column subset selection problem. In Proceedings of the 20th
Annual ACM-SIAM Symposium on Discrete Algo-rithms , pp. 968 X 977, 2009.
 Candes, E.J. and Recht, B. Exact matrix completion via convex optimization. Foundations of Computa-tional Mathematics , 9(6):717 X 772, 2009.
 Chatterjee, S. and Hadi, A.S. Influential observations, high leverage points, and outliers in linear regres-sion. Statistical Science , 1(3):379 X 393, 1986. Drineas, P., Mahoney, M.W., and Muthukrishnan, S.
Sampling algorithms for  X  2 regression and applica-tions. In Proceedings of the 17th Annual ACM-SIAM
Symposium on Discrete Algorithms , pp. 1127 X 1136, 2006.
 Drineas, P., Mahoney, M.W., and Muthukrishnan, S. Relative-error CUR matrix decompositions. SIAM
Journal on Matrix Analysis and Applications , 30: 844 X 881, 2008.
 Drineas, P., Mahoney, M.W., Muthukrishnan, S., and
Sarl  X os, T. Faster least squares approximation. Nu-merische Mathematik , 117(2):219 X 249, 2010.
 Drineas, P., Magdon-Ismail, M., Mahoney, M. W., and
Woodruff, D. P. Fast approximation of matrix co-herence and statistical leverage. Technical report, 2011. Preprint: arXiv:1109.3843 (2011).
 Hoaglin, D.C. and Welsch, R.E. The hat matrix in regression and ANOVA. The American Statistician , 32(1):17 X 22, 1978.
 Mahoney, M. W. Randomized algorithms for matri-ces and data . Foundations and Trends in Machine
Learning. NOW Publishers, Boston, 2011. Also available at: arXiv:1104.5557.
 Mahoney, M.W. and Drineas, P. CUR matrix decom-positions for improved data analysis. Proc. Natl. Acad. Sci. USA , 106:697 X 702, 2009.
 Mohri, M. and Talwalkar, A. Can matrix coherence be efficiently and accurately estimated? In Proceedings of the 14th International Workshop on Arti cial In-telligence and Statistics , 2011.
 Mougeot, M., Picard, D., and Tribouley, K. Learn-ing out of leaders. Technical report. Preprint: arXiv:arXiv:1001.1919 (2010).
 Sarl  X os, T. Improved approximation algorithms for large matrices via random projections. In Proceed-ings of the 47th Annual IEEE Symposium on Foun-dations of Computer Science , pp. 143 X 152, 2006. Talwalkar, A. and Rostamizadeh, A. Matrix coherence and the Nystr  X om method. In Proceedings of the 26th
Conference in Uncertainty in Arti cial Intelligence , 2010.
 Tropp, J.A. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Infor-
