 1. Introduction
Every day people encounter more information than they can possibly use. Friends, colleagues, books, news-papers, television, and Web sites are just a few of the resources and media contributing to the flow of infor-mation. But all information is not necessarily of equal value. In many cases, certain information appears to be better, or more trustworthy, than other information. The challenge that most people then face is to judge which information is more credible. The concept of credibility has received considerable attention since the late 1990s when the Internet began providing a new information interaction environment that allowed users to seek for information and communicate with others in ways never before possible. As a consequence, researchers and practitioners in diverse fields including information science, marketing, management informa-tion systems, communication studies, human X  X omputer interaction (HCI), and psychology have examined credibility assessment from a variety of different perspectives ( Rieh &amp; Danielson, 2007 ).

Many studies of credibility tend, however, to investigate credibility by relying on definitions, approaches, and presuppositions that are field-specific ( Flanagin &amp; Metzger, 2007 ). In information science, credibility has been understood as one of the criteria of relevance judgment used when making the decision to accept or reject retrieved information ( Rieh &amp; Danielson, 2007 ). Communication researchers have, on the other hand, been examining credibility as a research agenda distinguishing message credibility, source credibility, and media credibility ( Metzger, Flanagin, Eyal, Lemus, &amp; McCann, 2003 ). Management information systems (MIS) researchers have examined credibility issues in expert systems and decision support systems by querying people as to whether they believe the advice these systems provide. Consumer behavior researchers have addressed how consumers distinguish subjective and objective claims in e-commerce Web sites.

Consequently, previous studies of credibility have provided insights in relation to particular media such as the Web (e.g., Fogg et al., 2003; Huerta, 2003; Rieh, 2002 ), particular types of information such as political information ( Johnson &amp; Kaye, 1998, 2000 ), news information ( Sundar, 1999 ), and scholarly information ( Liu, 2004 ) as well as particular settings such as schools ( Fidel et al., 1999 ). This approach is potentially problematic in that people may carry over strategies and criteria used for one type of information (e.g., political informa-tion) to another type of information (e.g., scholarly information) and vice versa. People also may rely on mul-tiple types of media or resources in the course of a single information seeking episode. What is called for, then, is a framework that identifies common aspects of credibility assessment regardless of media, type of informa-tion, and environment of information use.

This paper proposes a unifying framework of credibility assessment in which credibility is characterized across various types of information resources and diverse information tasks including work-and school-related problems and personal interests. Importantly, this paper differentiates the terms credibility assessment and credibility judgment. Credibility assessment is herein seen as an iterative process involving one or more credibility judgments. As an information seeker encounters information, a series of judgments are made about the credibility of that information. These judgments are based on various factors to be discussed in greater depth in the following sections. Taken together, those judgments comprise the credibility assessment which feeds into the decision to accept or reject information.

This article begins with definitions of credibility and existing theoretical frameworks of credibility assess-ment which help to motivate the need for new research to develop a credibility assessment framework with respect to people X  X  information seeking behaviors in which multiple information resources are used. The meth-ods of data collection and analysis used in the present study are then presented. Results are discussed in the form of an emerging unifying framework that is grounded in empirical findings. Finally, implications and con-tributions of the framework are discussed with respect to the previous literature on credibility. 2. Review of related literature 2.1. Definition of credibility
Credibility has been defined as believability, trust, reliability, accuracy, fairness, objectivity, and dozens of other concepts and combination thereof ( Self, 1996 ). It also has been defined in terms of characteristics of per-suasive sources, characteristics of the message structure and content, and perceptions of media ( Metzger et al., 2003 ). Some studies focus on the characteristics that make sources or information worthy of being believed, while others examine the characteristics that make sources or information likely to be believed ( Flanagin &amp; Metzger, 2007 ).

Despite the fact that communication researchers have been interested in source credibility since the 1950s, there is as yet no clear definition of credibility. The overarching view across definitions is believability. Cred-ible people are believable people, and credible information is believable information ( Tseng &amp; Fogg, 1999 ).
Most credibility researchers agree that there are at least two key dimensions of credibility: trustworthiness and expertise ( Hovland, Janis, &amp; Kelley, 1953 ), both of which contribute to the concept of credibility. Trust-worthiness is a key factor in credibility assessment. A person is trustworthy for being honest, careful in choice of words, and disinclined to deceive ( Wilson, 1983 ). Information is trustworthy when it appears to be reliable, unbiased, and fair. Expertise is  X  X  X he perceived knowledge, skill, and experience of the source X  X  ( Fogg, 2003a, p. 124 ). Expertise is also an important factor because it is closely related to user perceptions of the ability of a source to provide information both accurate and valid. When people find that sources have expertise, they are likely to judge that information to be trustworthy. Assessment of sources X  expertise come in multiple ways: people might have prior first-hand experience with a source; they might have heard about a source from other people; they might know that a source has a good reputation; and they might recognize expertise when a source has credentials, among others ( Rieh, 2002 ). People X  X  credibility judgments are subjective and implicit because they need to recognize expertise to conclude that the information is trustworthy.

Credibility differs from cognitive authority and information quality, while being closely related to these two concepts. Cognitive authorities are those people who actually exert influence on other people X  X  thoughts by being recognized as proper ( Wilson, 1983 ). According to Wilson, an authority X  X  influence is thought proper because  X  X  X e is thought credible, worthy of belief X  X  (p. 15). Cognitive authorities are clearly among those who are considered to be credible sources. A person may be recognized as credible in an area even if not exert-ing influence on other people X  X  thoughts. In Wilson X  X  words,  X  X  X hose we think credible constitute the potential pool of cognitive authorities on which we might draw X  X  (Wilson, p. 16). Wilson claims that people do not attri-bute cognitive authority exclusively to individuals. Cognitive authority can be found in books, instruments, organizations, and institutions.

Information quality refers to people X  X  subjective judgment of goodness and usefulness of information in cer-tain information use settings with respect to their own expectations of information or in regard to other infor-mation available. Information quality is composed of five facets: usefulness, goodness, accuracy, currency, and importance ( Rieh, 2002 ). These facets of information quality are not necessarily always consistent. Informa-tion may be accurate but not useful, useful but not important, important but no longer current, current but inaccurate, and so forth. In such cases, one of the questions people ask themselves would be whether they can believe what the information says or, if not, whether they can at least take it seriously ( Wilson, 1983 ). Thus, credibility is a chief aspect of information quality . 2.2. Existing theoretical frameworks of credibility assessment
Several existing theoretical frameworks inform the understanding of credibility assessment. Fogg X  X  (2003b) Prominence-Interpretation Theory grew out of a series of research projects on Web credibility con-ducted for four years at the Stanford Web Credibility Research Lab. This theory posits that two things need to happen for people to make a credibility assessment: the user notices something (prominence), and the user makes a judgment about what she/he notices (interpretation). Prominence is  X  X  X he likelihood that a
Web site element will be noticed or perceived X  X  (p. 722). If the element is not noticed, it will have no impact on how the user assesses the credibility of the site. Fogg claims that at least five factors affect prominence: involvement (motivation and ability to scrutinize Web site content), topic of the Web site, task of the user, experience of the user, and individual differences of users. Interpretation, the second component of the the-ory, is  X  X  X  person X  X  judgment about an element under examination X  X  (p. 723), or the evaluation of a Web site element in terms of being good or bad. According to Fogg X  X  theory, various factors affect interpretation: user assumptions (culture, past experiences, heuristics, etc.), user skill/knowledge (level of competency in the site X  X  subject matter), and context (user environment, user expectations, situational norms, etc.). This process of prominence and interpretation takes place more than once when a person evaluates a Web site because new aspects of the site are continually noticed and interpreted in the process of making overall assessments of credibility.

Rieh X  X  (2002) model also looks at credibility assessment as an iterative process. While Fogg X  X  (2003b) theory describes the credibility assessment process from the point when a user notices something in a Web site to the point when the user makes a judgment, Rieh X  X  model begins earlier at the point at which a user makes a pre-dictive judgment about which Web site will contain credible information and follows through to include eval-uative judgments by which the user expresses preferences for information encountered. Rieh states that the kinds of factors influencing predictive judgments of information quality and cognitive authority differ from those that influence evaluative judgments. Her empirical study findings indicate that users tend to rely on their previous knowledge in terms of systems (system functions and structures) or topic area for making predictive judgments while their evaluative judgments are based on the characteristics of information objects (content, type of information object, and presentation). The characteristics of sources are consistently important criteria for both predictive judgments and evaluative judgments, according to Rieh X  X  study.

Wathen and Burkell (2002) present a model of credibility assessment in which a staged process is laid out in the context of health information searching on the Web. They propose that a user X  X  first task is to rate the credibility of the medium based on surface characteristics such as appearance/presentation, usability/ interface design, and organization of information. The second task for the user is rating the source and the message. The evaluation of a source is often made in terms of its expertise/competence, trustworthiness, credentials, and so forth. The message is evaluated in terms of content, relevance, currency, accuracy, and tailoring. The third aspect of process involves the interaction of presentation and content with the user X  X  cognitive state. Wathen and Burkell X  X  model is also iterative given that the user makes judgments of the initial surface of Web sites and source cues via interactions with Web sites. Their model, however, has not been tested empirically.

Sundar (2007) presents four of what he calls  X  X  X ffordances X  X  in digital media capable of cueing cognitive heu-ristics pertinent to credibility assessments: Modality (M), Agency (A), Interactivity (I), and Navigability (N).
His MAIN model argues that these four technical affordances help to explain the perceived credibility of dig-ital media and their offerings beyond what is explained by content characteristics. According to Sundar, the affordances have the power to amplify or diminish content effects on credibility because they can play the role of a moderator in a variety of psychological ways. Sundar further suggests that a deeper understanding of the role of heuristics in the credibility assessment process will clear up apparent contradictions in the credibility literature. While heuristics are not infallible and do not guarantee success, they likely appeal to many individ-uals who might cope with the deluge of information. Further discussion of the importance of heuristics in con-ceptualizing credibility assessment is found in Petty and Cacioppo X  X  Elaboration Likelihood Model (ELM) to be discussed below. 2.3. Elaboration Likelihood Model (ELM)
The Elaboration Likelihood Model (ELM) provides a fairly general framework for understanding the basic process underlying persuasive communication, and as such it has often been adopted by credibility researchers to characterize credibility assessment processes and elements (e.g., Eastin, Yang, &amp; Nathanson, 2006; Sundar, 2007; Wathen &amp; Burkell, 2002 ). The ELM explains attitudinal changes in individuals as they encounter mes-sages and the sources of those messages ( Petty &amp; Cacioppo, 1981, 1986 ). The two key constructs capable of affecting the amount and direction of attitude change are argument/message quality and peripheral cues. In the ELM, arguments are viewed  X  X  X s bits of information contained in a communication that are relevant to a person X  X  subjective determination of the true merits of an advocated position X  X  (p. 133). Thus, one way to influence attitude in a persuasive message is to strengthen the quality of the arguments. Another way to influ-ence attitude is to use a simple cue even in the absence of arguments. Peripheral cues can pertain to either source or message. An example of a peripheral source cue is the reputation of the source (highly respected author, etc.), while an example of a peripheral message cue would be the length of the message or the number of points made in the argument. The distinction Petty and Cacioppo make between content and peripheral cues may have important implications for credibility research given that  X  X  X eripheral route X  X  and  X  X  X entral route X  X  characterize where people tend to put their effort and attention in evaluating information credibility when using digital media ( Sundar, 2007 ).

Petty and Cacioppo (1981, 1986) also identify two types of message processing in which individuals engage when encountering messages: systematic and heuristic. Systematic processing is effortful and involves analyzing a message based on content cues. In many instances, however, individuals lack either the motivation or the ability to engage in systematic processing. At those times they resort to heuristic pro-cessing, basing their judgments of a message not on its content but on peripheral cues from the message and/or the source. Heuristics are practical rules or guidelines that aid in problem solving, decision making, and discovery ( Newell, Shaw, &amp; Simon, 1960 ), and as such tend to reduce mental effort ( Petty &amp; Cacioppo, 1986; Sundar, 2007 ). 3. Methodology
The goal of this study was to better understand how people make credibility assessments in a wide variety of everyday life information seeking contexts. To accomplish this goal, undergraduate students were selected as potential participants under the assumption that their lives would involve a wide variety of information seeking activities across work, school, and personal pursuits. 3.1. Participants
A purposive sampling approach to recruit participants was used, ensuring that a variety of undergraduate students representing different sizes of colleges, academic majors, and genders were included. Recruiting was continued up to the point of theoretical saturation (i.e., when participants began repeating the same themes and the authors ceased learning anything new). Twenty-six undergraduate students were recruited from three different institutions in the US Midwestern state. The three institutions included a large research university, a medium-sized state university, and a community college. Two participants dropped out of the study before they could be interviewed, and their data are not reported here. So, in total, 24 students participated in the study. Initially, only first year students were recruited on the belief that they would likely be encountering a variety of novel information tasks in their new learning and living environments. At the research university, the sampling was limited to first year students; this limiting, however, proved extremely difficult at the other two institutions as recruitment for the study occurred primarily during the summer months when fewer first year students were on campus. Therefore, recruitment was extended to include undergraduate students at all levels.

Although the average age of the participants was 21.8 years, 11 participants were 18-year old. Half of the participants were in the first year of their undergraduate education. There were only three non-traditional stu-dents, including a 29-year old, a 31-year old, and a 43-year old. Fourteen were female, and ten were male. The participants X  majors included engineering, dance, film, psychology, business, nursing, education, and pre-med-icine, among others.

A pre-interview questionnaire was administered to gather background information on the participants. On average, participants said they used the Internet 3.6 hours each day and their institution X  X  online library cat-alog system about 0.75 hour each week. Twenty-one participants (87.5%) reported they used email multiple times daily and went on the Internet for personal interests four or more times each week. Twenty participants (83.3%) also responded that they used the Internet for school research approximately four to seven days per week. 3.2. Data collection
Diaries have proven useful to researchers given that the method enables the collection of a wealth of detailed, actual information behaviors in specific contexts ( Toms &amp; Duff, 2002 ). Furthermore, diaries make it possible to capture various kinds of information activities such as topics in task, resources used, time spent on task, and outcome of search activity while they are still fresh in the mind of the participant and are useful for triggering memories of those details during a subsequent in-depth interview ( Rieh, 2004 ). Thus, partici-pants are not left to rely entirely on their memories to recall their information needs and activities, making diaries a useful means of capturing the salient aspects of real information seeking activities for discussion dur-ing later interviews ( Cool &amp; Belkin, 2002 ).

Participants were asked to record one information seeking activity per day for 10 days. The general instruc-tions given to the participants were to:  X  X  X hink about situations today in which you needed to find informa-tion. Choose the one that was most important to you; then answer the following questions, providing as much detail as you can. X  X  They were encouraged to include activities that involved the range of information sources (human, Web site, book, newspaper, etc.), because the intent was not to limit the investigation to any one system, source or medium. The diaries were kept online via a password-protected Web page. Participants answered a series of 11 specific questions about the activity to ensure important details were captured. The questions posed in the diary form included the topic of information seeking activity, goals of information seeking, familiarity with the topic, resources used, information seeking process, information use plans, and their next steps. No question directly related to credibility was included in the diaries. This is because ques-tions about credibility in the diary may have changed the participants X  behaviors and attitudes by having them focus on credibility issues during the process of information seeking.

Once the activities from the entire 10 days had been recorded, an in-depth interview was conducted. The researcher prepared two hard copies of the participant X  X  diary and gave one to the participant so that both the researcher and the participant could read the entries in the diary during the interview. The interview was initiated by asking the participant to recount each activity, providing additional details about the topic, the resources used, and the step-by-step information seeking process. The participant was then queried about issues of credibility, including whether or not credibility was a concern and how it was judged. The participant was also asked to provide reasons for the judgments of credibility that they made. Additional questions in the interview included the participant X  X  perceived levels of difficulty and confidence along with comments on the general seeking experience. The interviews were not, however, strictly limited to the 10 activities contained in the diaries: when appropriate, participants were asked to discuss comparable or contrasting activities with respect to their judgments of credibility generally. 3.3. Data analysis
The interviews ranged in length from 1 to 1.5 hours. The audiotapes of these interviews were transcribed and then analyzed using the grounded theory approach ( Glaser &amp; Strauss, 1967 ). The two authors conducted an initial content analysis separately on the same transcripts. The encodings of three transcripts resulted in the first draft of the coding scheme. Through discussions and explanations of how and why each code was applied, the coding scheme was revised. The three transcripts were finally recoded using the revised coding scheme, and the major themes emerged from the content analysis.
 Two Library and Information Services masters students in the School of Information at the University of
Michigan served as coders. They and the authors held weekly meetings from June to August 2006. The authors presented the coders with the coding scheme along with operational definitions and examples. Each coder was responsible for analyzing the transcripts of the twelve interviews, and in the first three weeks they coded two transcripts each week as well as brought questions and  X  X  X ncodable X  X  quotes to the meeting for the purposes of revising the coding scheme. Through the iteration of coding scheme revisions over several weeks, the coding scheme was finalized and the themes emerged more clearly. The coders entered their encodings using NVivo 2.0, qualitative data analysis software. The meetings of the group of four were continued until all the enco-dings were completed in order to resolve any differences in encodings between the two coders. The common themes and categories emerged in the grounded theory analyses were then broken down into seven major cat-egories and 25 sub-categories as shown in Table 1 .

This article focuses on the development of a unifying framework of credibility assessment. Thus, the three levels X  X onstruct, heuristics, and interaction X  X nd the context of credibility judgments that emerged from the data analysis will be discussed in detail in the following chapters of this article. References can be made to other works by the authors ( Rieh &amp; Hilligoss, 2007; Rieh, Hilligoss, &amp; Yang, 2007 ) for other themes such as goals, tasks, media/resources, credibility judgment process, and information seeking strategies. 4. A unifying framework of credibility assessment
A unifying framework of credibility assessment was developed as a result of data analysis. Three distinct levels of credibility judgments emerged: construct, heuristics, and interaction. The construct level pertains to how a person constructs, conceptualizes, or defines credibility. It is the most abstract level and as such involves broad notions of credibility that influence the person X  X  judgments. The heuristics level involves gen-eral rules of thumb used to make judgments of credibility. This level is fairly general, being broad enough to apply to a variety of situations rather than specific to any particular situation. The interaction level refers to credibility judgments based on specific source or content cues. A summary of the three levels is presented in Table 2 .

The three levels of credibility judgments above do not necessarily operate independently. In fact, as pre-sented in Fig. 1 , any or all of the levels interlink. This framework implies that each level affects the other levels in both directions from abstract to specific levels and vice versa. For instance, if a person constructs credibility in terms of reliability, that construct may influence the kind of heuristics that can help in identifying a resource likely to be reliable (e.g., official Web site). Heuristics may influence the ways in which a person assesses cred-ibility by drawing attention to certain characteristics when interacting with the information resource. Some-times a person makes a credibility judgment based on certain cues from a source of information and finds later that the judgment may contradict the original heuristic. In such cases, the current heuristic can be extended by adding a new general rule of thumb. On the other hand, once a person repeatedly relies on the same kind of cues relative to information objects within a single information seeking episode or across multiple information seeking episodes, the use of such cues can be established as personal heuristics. If a person X  X  heuristics prove consistent over time, the heuristics may become the person X  X  construct of credibility.

As Fig. 1 shows, context also emerged as an important factor influencing all three levels by playing a central role in the process of making a credibility judgment. The context is the social, relational, and dynamic frame of reference surrounding the person X  X  information seeking process. In general, it creates boundaries around the information seeking activity or the credibility judgment itself. The context of credibility judgments can either guide the selection of resources or limit the applicability of such judgments.
 4.1. Construct level
The construct level is the highest in the framework because it concerns itself with how a person conceptu-alizes and defines credibility, providing a particular point of view for judging credibility in fundamental ways.
The data analysis reveals that participants in this study conceptualize information credibility with respect to five different aspects: truthfulness, believability, trustworthiness, objectivity, and reliability. The distinct cate-gories of a credibility construct indicate how a person defines credibility differently from other people at any given time.

There were a few cases in which judgments of credibility were made based on participants X  estimations of the truthfulness of the information they encountered.  X  X  X  guess what makes it credible would be: what type of information do they produce? Like, as long as they X  X e producing the truth, and they X  X e not altering it or pro-viding false information, I would think of a source as credible X  X  (P22 A11). ticipants used included accuracy, validity, correctness, and legitimacy.

Believability is another construct of credibility employed by several participants in this study. As one par-ticipant put it:  X  X  X redible is, like, believable X  X  (P02 A11). In some cases participants applied this definition of ible, then how can you believe in it? X  X  (P13 A13).

Trustworthiness is the definition of credibility that participants mentioned most frequently. P16 said that  X  X  X t comes down to being able to trust the source and trust the information X  X  (P16 A12). As this quote dem-onstrates, the conceptualization of credibility as trustworthiness is one that can be applied to both information and sources. In another example, when asked to talk about the credibility of an article on Wikipedia, one par-ticipant talked about the issue of trust by saying  X  X  X ut I do have a reason not to trust Wikipedia X  X  (P19 A3). He went on to explain that  X  X  X ecause ... anybody can edit any information on there ... it X  X  this thing where any-body can do anything to the information that X  X  there. So, I just can X  X  trust it because it could be modified X  X  (P19 A3).

Another conceptualization of credibility that emerged during the interviews is objectivity. One participant defined credibility as  X  X  X resenting [a] balance of facts and knowledge, showing different sides of the issues, not being biased, not being involved directly in what X  X  going on is important X  X  (P10 A2). This quote describes objective information ( X  X  X howing different sides of the issues X  X ) and objective sources ( X  X  X ot being involved directly in what X  X  going on X  X ).

Finally, several participants defined credibility in terms of reliability, a conceptualization of credibility clo-sely linked with verifiability. One participant explained that  X  X  X eing able to verify it with another source X  X  makes for credible information (P18 A11). One interesting point that becomes apparent with this conceptu-alization of credibility is that it suggests certain strategies or behaviors for judging credibility. Participants who considered credibility as being about reliability felt they needed to seek out other sources of information or actually use the information in order to confirm its credibility.

Thus, it is evident that different people conceptualize credibility in different ways. This is not, however, to suggest that individuals have only one definition of credibility. In fact, the participants in this study often held multiple concepts of credibility. They applied certain constructs of credibility depending on the situation or type of information encountered, as shown in the following quote:
In this example the participant was concerned with whether or not the information told the truth (truth-fulness) when discussing information related to an amusement park. When talking about health information, the participant constructed credibility in terms of trustworthiness. 4.2. Heuristics level The heuristics level is comprised of general rules of thumb used to make decisions regarding credibility.
These rules of thumb are fairly general, broad enough to apply to a variety of situations rather than being specific to any particular situation. Participants often explained their information seeking behaviors as doing what was  X  X  X onvenient X  X  and  X  X  X uick. X  X  The use of heuristics supports their objective of finding information quickly and conveniently. In many cases, heuristics allow individuals to almost instantly jump to a judgment of credibility without much substantial engagement with the information or source itself. As noted by Petty and Cacioppo (1981, 1986) , systematic processing of the content of a message is time consuming and cogni-tively demanding, and in some cases individuals lack the motivation or ability to systematically process (or evaluate) the content of a message (or information object). In these cases individuals may engage in heuristic processing because they are either unwilling or unable to spend the time or effort required for systematic pro-cessing. The data herein revealed four categories of heuristics: media-related, source-related, endorsement-based, and aesthetics-based. 4.2.1. Media-related heuristics
Many of the heuristics that individuals hold pertain to specific media. Here, the term media is used broadly to refer to any media, format, or channel through which information is conveyed. In some instances, heuristics about different media are used to compare and contrast the relative credibility of those media. These general rules of thumb can exert various kinds of influence on the credibility assessment process, affecting such deci-sions as to where to look for information as well as the scrutiny or skepticism with which one can approach information found in a given media format.
 Participants in the study expressed general heuristics about books, peer-reviewed journal articles, the
Web, blogs, and libraries. Books and scholarly journal articles were consistently perceived as credible media, particularly in comparison with the Internet as shown in the comment that  X  X  X eople can just make up sites and stuff online, but with a book you have to go through publishing companies and getting that whole book [publishing] process. It just goes through so many procedures to make sure that it X  X  credible, but the Inter-net, it X  X  so easy [to post there] X  X  (P02 A10). Knowledge of the peer-review process also lies behind the posi-tive heuristics of scholarly journal articles. One student said,  X  X  X  know it X  X  really hard to become published, so, I X  X  assuming that it X  X  credible X  X  (P06 A8). Similarly, another participant explained that  X  X  X ith a peer reviewed article, I know that the person saying it is supposed to know something about the topic ... . I know that other people who are supposed to know something of the topic agreed with them or at least consider what they X  X e saying a reliable possibility X  X  (P01 A12). Thus, a general, if not detailed, understanding of the peer-review publishing process lay behind the confidence participants expressed in the credibility of books and scholarly journals.

Many participants are well aware of the notion that  X  X  ... anybody can write anything on the Web X  X  (P20 A7) and so tend to evaluate more carefully the information on the Web. Teaching in schools seems to play a part in the participants X  heuristics about the Web as indicated in the statement that  X  X  ... in high school I pretty much learned that everything you read [on the Web] is not true, and you can X  X  trust everything that you read X  X  (P22
A6). The concerns about the Web-based information did not, however, prevent the participants from using the information found on the Web. Rather, the heuristics about specific media will at times increase or decrease concerns about credibility, which can influence the scrutiny an individual uses in evaluation.

Some participants differentiated between types of Web sites, comparing and contrasting blogs to other Web sites. Participants who spoke of blogs perceived them as opinion-based and thus lacking credibility. As one participant put it,  X  X  X  would say a blog would have a lot less credibility than a professional Web site; a lot of times they don X  X  have first hand information X  X  (P09 A5). That same participant elaborated to the effect that  X  X  X On] a blog, someone could say anything they want and just keep going. There X  X  no journalistic integrity required or anything, because there X  X  no repercussions X  X  (P09 A5). Other participants noted that blogs can be useful as  X  X  X pring boards, X  X  providing new ideas and leads for information seeking. One participant explained  X  X  X o, I might use the information I found in a forum or a blog to point me in a direction to find out if something is true, but I wouldn X  X  take that information as credible X  X  (P10 A5). Thus, while people may perceive certain media or specific media formats to be non-credible, they still may have good reason for using those media. 4.2.2. Source-related heuristics
Analysis of the data revealed the participants employ heuristics about sources of information in two ways: familiar vs. unfamiliar sources and primary vs. secondary sources.

In terms of familiarity, the most common heuristics mentioned by the participants is that familiar, known sources are more credible than unfamiliar ones. One participant reported that  X  X  X f it X  X  from a trusted source ...
I would probably trust [it] more than, you know, when you ask someone random for directions; you don X  X  really know if what they are saying is right X  X  (P05 A11). Use of the word  X  X  X andom X  X  is notable. Another par-ticipant spoke about random Web sites (P01 A11), while yet another participant referred to random organi-zations or random companies (P15 A2). In these cases, the heuristic is that unknown or random sources of information are less likely to be credible than known ones.

The second source-related heuristics is related to the dichotomy of primary vs. secondary sources. In gen-eral, participants perceived primary sources as being credible, with the  X  X  X fficial site X  X  being perceived as most credible. One participant stated that  X  X  X  would say information is more credible, more trustworthy ... if the source is sort of the official source ... regarding what you X  X e looking for X  X  (P08 A12). Another participant X  X  characterization of the heuristics of secondary sources held that  X  X  ... if it X  X  like a second[ary] source, like, a paper someone has written online or a blog or an article even, then I kind of second guess whether it X  X  a true
Web site, whether that information is valid. So it depends on the site I X  X  on X  X  (P03 A11). 4.2.3. Endorsement-based heuristics
A third kind of source-related heuristics involves endorsements. Here, participants perceived a particular information object or source to be credible because it has been endorsed, recommended, or otherwise upheld by knowledgeable and trusted individuals. Those individuals might be known (friends, family, colleagues, etc.), or not known (e.g., experts in a particular field). Endorsements may also come in the form of citations, one participant explaining that  X  X  X f [the book] had been cited in other history papers, then I know that people find it reliable X  X  (P07 A8).

Organizations are also seen as givers of endorsements. Some participants felt that if the Web site of a trusted organization provided a link to another Web site, then that trusted organization was effectively endors-ing the other Web site. One participant explained that  X  X  ... if it X  X  something that [the University] will send a link to, or a Web site I have used sends a link to, I trust that more. It X  X  more credible to me because if they X  X e asso-ciated with it, then they X  X e done their checks X  X  (P12 A12).

The findings demonstrate that some participants perceive popularity as a form of endorsement, following the general rule of thumb that information sources and objects widely used are are more likely to be credible.
One participant talked about his trust in several Web sites that review information technologies, calling them  X  X  X eputable X  X  and  X  X  X espected in the industry. X  X  When asked to explain how he knew these Web sites were well respected, he replied that  X  X  ... the fact that they X  X e so popular, it X  X  not like a perfect indicator, but it usually indicates they have some reputability X  X  (P09 A5). The perception of popularity as a measure of credibility is not, however, restricted to Web sites. Another participant explaining why he might trust a moving company more if it were popular said  X  X  X  think my general conclusion is that [if] everyone X  X  using them ... , then they must be a good company and a trusted company X  X  (P15 A2). Another participant talked about the theory of global warming and how his own opinion about that theory had changed over time as he encountered  X  X  X ore and more and more X  X  scientific articles supporting the theory. He said he now believed the theory because of  X  X  X he overwhelming difference in proportion and amount [of scientists on the two sides]. It was like one hun-dred to one, really X  X  (P16 A11). Nonetheless, he acknowledged that  X  X  X he majority doesn X  X  always mean it X  X  correct X  X  (P16 A11). 4.2.4. Aesthetics-based heuristics
A number of participants used heuristics that connected credibility and aesthetic design in Web sites. As one participant expressed it,  X  X  X  also judge Web sites based on the quality of their layout. It X  X  kind of snobbish, but if something looks like it X  X  been put together by a five year old on his first GeoCities page, like, lots of blinky things and whatever, then I usually think the Web site X  X  probably crap, because they haven X  X  put a whole lot of effort into it X  X  (P18 A8). One participant offered that  X  X  X  guess a professional design of a Web site, like a nice and clean layout kind of X  X enerally, if someone puts that much time into designing the Web site, they put more time into what they X  X e saying. It X  X  not always true, but it X  X  kind of a good rule of thumb X  X  (P09
A10). 4.3. Interaction level
The third and final level of influence is the interaction level, in which specific attributes associated with par-ticular information objects and sources enter into an individual X  X  judgment of credibility. In contrast to heu-ristics, which are broad and widely applicable, interaction level judgments are unique to a specific information object or source that participants encounter. The data analysis reveals that the interaction level involves three types of interactions: those with content cues, those with source peripheral cues, and those with information object peripheral cues. 4.3.1. Interactions with content cues
Content refers to the message itself, that is, the substantive information being conveyed. Interactions with content are arguably the most cognitively demanding of the three kinds of interactions ( Petty &amp; Cacioppo, 1986 ). This study finds that the use of personal knowledge to evaluate information is the primary method by which people interact with content from a credibility assessment perspective. P20, researching a particular Islamic scholar for a school assignment, conducted a Google search and found an article about him on
Wikipedia. Since she already knew something about the scholar, she was able to recognize several errors in the content provided on the Wikipedia site. The following quote explains her interactions with the content with respect to her prior knowledge:
When the participants lacked the knowledge required for judging the credibility of information, they took other strategies for making credibility judgments based on content cues. For instance, participants looked for multiple sources with concurring information in order to verify content. One participant, researching the effec-tiveness of evidence-based substance abuse prevention programs, explored numerous scholarly articles. He explained why he felt that a particular article he found was credible:  X  X  X ll of the information follows suit with what others studies have said. It kind of falls in line with that X  X  (P16 A7). Similarly, another participant was looking for the date for a particular upcoming community event. She conducted a search on Google, clicking on several of the links in the results set even though the first one she tried provided the date. She explained her reason for doing this as  X  X  X ust reassurance X  X ecause I don X  X  know which one is like X  X ome sites on the Internet are not credible. So I wanted to make sure that it was the right information, and by giving the same date a couple of times, I believed it X  X  (P22 A6). 4.3.2. Interactions with peripheral source cues
Any cue pertaining to a source that was used in a judgment of the credibility of information was deemed a peripheral source cue. In this study, individuals, groups, organizations, and any other social aggregate were considered sources. Source peripheral cues included affiliation, reputation, author X  X  educational background, type of institution, and so forth.

Many participants explained their confidence in the credibility of information they received from friends, family, and other acquaintances. For example, one participant trusted her roommate for help on a sociology assignment because she knew the roommate was also taking the class and  X  X  X oing pretty well X  X  (P05 A1).
Another participant trusted his friend for advice on ice skating because he knew  X  X  X he has been skating many years X 10 years X  X  (P07 A2). Still another participant accepted input from his track coach about how best to treat an injury because he knew the coach  X  X  X as been running marathon for nearly thirty years X  X  (P15 A3).
A different participant knew she could ask her boyfriend for information on just about any topic and have confidence that he would be knowledgeable because  X  X  X e X  X  always doing research on everything X  X  (P06 A4).
These are just a few examples of how knowledge about the habits, behaviors and talents of another person can influence a person X  X  credibility judgment.
 Past experience with sources sometimes quite significantly influenced a participant X  X  subsequent judgments. Several participants explained their selection of Web sites by pointing to positive prior experiences with those Web sites. One participant was looking for information about an upcoming performance by a musical band.
He checked two Web sites, both of which he had used previously, and found the information he needed. When asked whether he felt the information might have been not credible, he replied,  X  X  X ot really. Both the sites I X  X e been to many times before and completely trust ... so I didn X  X  have any issues like that X  X  (P04 A2). Previous experiences can also boost a person X  X  confidence in an e-commerce Web site. One participant explained why she used Amazon.com saying,  X  X  X  X  X e used them in the past. I X  X e ordered things from them. I haven X  X  had any bad experiences with ordering things or pre-ordering items X  X  (P12 A8). First-hand knowledge can also make people doubt the credibility of information sources. One participant asked his friend for information about an upcoming football game but then decided to double check the information with another, more knowledgeable friend. In explaining why he was hesitant to trust the first friend he said  X  X  ... he has a history of just sort of saying whatever when I ask him these trivial questions, because it X  X  not a big issue. He doesn X  X  really consider it a big issue, so he X  X l just say whatever X  X  (P08 A10).

In addition to their personal experiences, people may also be influenced by the experiences of others when making credibility judgments. One participant trusted a book she called  X  X  X he Gardener X  X  Bible, X  X  explaining  X  X  X  do remember hearing from other people a long time back that they X  X  used it and found it helpful. And so I thought that X  X  probably a good bet X  X  (P20 A3). A different participant talked about an activity in which she needed more information about a female musical artist. She explained why she turned to and believed the information she found on a particular Web site by citing her stepfather X  X  experience with that Web site:
As these examples illustrate, knowledge of a source, whether first or second-hand, influenced credibility judgments as participants interacted with sources. 4.3.3. Interactions with peripheral information object cues
The third kind of interaction involves peripheral cues derived from the information object. Most commonly these cues pertain to the appearance or presentation of the information. Considerations of presentation tend to involve the external, aesthetic aspects of the information object, or the emotional effect of interaction with the object. As noted above, many participants used aesthetics-based heuristics to judge information credibility.
When applying such heuristics to a specific information object, such as examining how well a particular Web site is designed, an individual uses information object peripheral cues to judge credibility on the interaction level. One participant X  X  effort to contrast Google and Yahoo! is illustrative:
In addition to cues pertaining to the appearance or presentation of information, participants also consid-ered cues derived from the language used. Referring to articles he had read on zdnet.com, cnet.com, and slash-dot.com, one participant said,  X  X  X  think they X  X e well written in general, so I guess that leads it to be more reputable X  X  (P9 A4). In another example, P10 looked online for a listing of Martin Luther X  X  Ninety-five Theses.
He explained why he trusted the listing he found.  X  X  X ell, I mean it was the old English. The wording was kind of complex unnecessarily. It sounded like something from that period X  X  (P10 A9).

Information object peripheral cues can also evoke affective responses that may influence credibility judg-ments. In talking about why he judged a particular Web site about Albert Einstein to be credible, one partic-ipant said there was a  X  X  X cientific mood to the Web site. It X  X  hard to describe X  X  (P8 A5). Others talked about information  X  X  X eeming X  X  or  X  X  X eeling X  X  correct or credible but were often unable to elaborate. Such judgments, based on interactions with information objects, are shaped by peripheral cues pertaining to those objects. 4.4. Context of credibility judgments
The data analysis reveals that contextual factors can intervene and influence credibility judgments by con-straining selection of resources for a particular information seeking activity. For instance, P08 talked about a paper he was writing for a class assignment. While he was comfortable using the Web and felt that the Web provided many resources of high quality and credibility, he noted that he was reluctant to use many Web resources out of fear that they would be perceived by his professor as non-credible simply because they were
Web-based. He elaborated by stating that  X  X  X f it X  X  something that I could find an actual book on, I like that too, because just using the Internet X  X  X  X  pretty nervous about just using the Internet X  X ust because professors usually don X  X  like that, and it X  X  good to include some actual books X  X  (P08 A3). In this example, the participant followed a strategy for selecting resources that was based not entirely on his own judgment of what constituted credible information but partly as well on the school context to accommodate a professor X  X  judgment criteria.
Participants sometimes accepted certain resources as credible primarily by relying on the context in which they encountered the resources. For example, one participant explained her confidence in the textbook she was using for one of her classes with  X  X  ... the same guy who wrote the textbook wrote my test. So, I would trust it X  X  (P05 A8). In this example one particular resource, the textbook, was intricately intertwined with the context out of which her information need had arisen. A similar example was found with a participant who turned to the professor to clarify the class assignment. She said  X  X  ... I knew I could trust my professor to give me the correct information because it was his problem I was working on X  X  (P01 A12). Again, selection of resources and judgments of credibility were guided by the context in which the information need emerged.

Another finding from the data analysis is that contextual factors also enter into judgments of credibility by bounding or otherwise limiting the applicability of those judgments. Several instances were found in which contextual factors both led the participants to make certain credibility judgments and prevented them from confidently extending those judgments to other contexts. These cases were most frequently noted when par-ticipants made credibility judgments in the context of their classes. Several participants talked about the cred-ibility of the information they received from their class instructors, acknowledging that while the information might not be credible in the world outside the class, it was credible within. As one student expressed it,  X  X  X t X  X  reliable in the context of the class. It may or may not be reliable in the real world X  X  (P01 A12). Another said,  X  X  X  would assume it X  X  credible, but I also know it X  X  completely credible within the class because the class is designed around it. So, even if it X  X  false, it X  X  true within the bounds of the classroom X  X  (P09 A3).
Context not only bounds credibility judgments of information received from instructors but also bounds judgments of other sources as well. For instance, one participant commented on the information found in her course textbook.  X  X  X ell, it isn X  X  necessarily that the book would be right. It X  X  that everything was taken from the book that would be on the exam. So, if the book wasn X  X  right, like, technically, then, it was irrelevant.
It X  X  be irrelevant to my goal [of passing the test] X  X  (P06 A5). Comments like these indicate that credibility is not always viewed as absolute but rather may be seen as relative given its relationship to certain contexts of infor-mation seeking and use. 4.5. Interplay among construct, heuristics, interaction, and context
As mentioned briefly at the beginning of Section 4 , the multiple levels and contextual factors are interlinked in credibility assessment. This section provides examples from the data which illustrate how multiple levels and contextual factors can shape and influence credibility judgments.

One of the common credibility judgment strategies that the participants took was to apply their own heu-ristics of credibility assessment to a certain information seeking activity. In many cases, it proved to be useful.
In one example, a participant who was planning to move from the United States to Australia wanted to find out whether or not she would be able to work legally in Australia. When beginning her search for this infor-mation, she thought of the Australian embassy and looked for its Web site. She explained the reasoning behind her actions this way:  X  X  ... it X  X  sort of the official source, I guess, of information for people who are immi-grating to Australia. I turned to it as a credible source, I guess, because it X  X  the official line X  X  (P24 A5). Embed-ded in her explanation is the very common heuristic that official sources are credible sources. Ultimately, her heuristic proved useful because she found the information she needed on the embassy X  X  Web site. In this exam-ple, the participant used heuristics in the process of information seeking to help her identify a resource that was likely to be credible. As she interacted with that resource, nothing about it contradicted that prediction of credibility derived from the heuristic. Along this same line, another participant explained why he trusted information he found in the textbook for his Spanish class:  X  X  X  didn X  X  have any reason not to trust it X  X  (P10
A1). Thus, as long as the participants do not encounter any cues on the interaction level that contradict or challenge their heuristics, they appeared to be fairly confident in their credibility judgments based solely on heuristics.

Heuristics are sometimes formed around the roles that a source plays in the world. Librarian is one such role which several participants perceived as a credible source of information. One participant X  X  search for fic-tion at her college library illustrates how this heuristic played out in relation to the interaction level. Not knowing whether or not the library collection included popular fiction or how to find it if it did, the participant decided to seek help from the librarian. The participant explained:  X  X  X ecause she [librarian] works here so I figured she knew how the system worked and where to find it. I didn X  X  think she would direct me in the wrong area to find something X  X  (P12 A1). This example indicates that nothing about the interaction with the librarian contradicted the heuristic, so the participant was able to keep her confidence in the librarian X  X  credibility.
While heuristics are useful, they have their limits. When factors from other levels intervened, participants deviated from heuristics. For example, one participant expressed heuristics that educational and non-profit
Web sites are more credible than commercial ones. As he put it, .coms  X  X  X ight not be as reliable as the other types, like .edu and .orgs ...  X  X  (P11 A4). While this general rule of thumb might have been useful in guiding him in previous information seeking activities, he made an exception in the case of searching for information about bi-polar disorder for a paper he was writing. He turned to WebMD, a commercial health information Web site saying,  X  X  ... when you X  X e doing a research paper, try to stay away from the.coms because [they] might not be as valid, but I just also assumed that WebMD would be pretty reliable X  X  (P11 A4). In this example, interactions with peripheral source cues on a commercial Web site influenced the participant to go against the heuristic and to judge the WebMD site credible.

Another example from this study indicated how context could prompt participants to make exceptions to the heuristics. As mentioned earlier, many participants ascribed credibility to textbooks assigned by profes-sors. This heuristic was applied directly in two different activities of one participant as she discussed the text-books for her anthropology and astronomy classes. However, the context surrounding her art history class, made her less trustful of the textbook:
This participant did not fully distrust the textbook; however she did not accept it without question as she did for her other courses. She said that she needed to attend the lectures to know what information in the text-book she should believe and what she should not. Furthermore, her caution stemmed not from any cues on the interaction level. That is, nothing about content, source, or book itself made her concerned about the credi-bility of information. Instead, the context in which the textbook was used influenced her credibility judgment. 5. Discussion
The purpose of this research is to develop a theoretical framework of credibility assessment in which peo-ple X  X  credibility judgments are extended across multiple media for a variety of information seeking goals and tasks. Data from 24 in-depth interviews with college students seeking information through a variety of media and channels for work, school, and personal interests were collected and analyzed in order to develop a frame-work of credibility assessment. This framework involves the three levels of credibility judgments: construct, heuristics, and interactions, any or all of which may be involved in an attempt to assess the credibility of infor-mation encountered. Context emerged as an important factor that influences the three levels. This framework also indicates that these multiple levels interplay with each other in shaping a person X  X  credibility judgments.
Previous credibility studies have tended to use the measures of credibility concept by presenting multiple terms with which to study participants when examining credibility issues. To evaluate the credibility of news-papers, TV news, and online news, researchers have used measures such as fairness, accuracy, factuality, truth-fulness, believability, completeness, precision, objectivity, lack of bias, trustworthiness, objectivity, depth, informativeness ( Bucy, 2003; Johnson &amp; Kaye, 2000; Meyer, 1988; Newhagen &amp; Nass, 1989; Salwen, 1987;
Sundar, 1999 ). It is, however, unclear whether these measures are predictors of perceived credibility or are underlying dimensions of the credibility concept itself ( Newhagen &amp; Nass, 1989 ). Multiple constructs of cred-ibility that characterize conceptualizations or definitions of credibility were identified: truthfulness, believabil-ity, trustworthiness, objectivity, and reliability. At least two interesting findings were noted. First, each person seeming to possess his/her strong sense of what credibility means. Second, a person may have multiple con-structs of credibility, with the construct being closely related to the type of information to be evaluated.
For instance, when the person evaluates news information, objectivity could be a more important construct than others. When evaluating information from primary sources, reliability may play an important role for credibility constructs. With health information, for instance, the question of whether people can trust the source of information would be more important than any other constructs. The framework herein indicates that linking the constructs of credibility with types of information will be a useful direction for future research in credibility.

A few researchers (e.g., Flanagin &amp; Metzger, 2000; Sundar, 2007 ), have paid attention to the heuristics of credibility assessment lying behind general credibility perceptions. The present study contributes to the liter-ature of credibility by identifying the role that heuristics plays in credibility judgments. As general rules of thumb or principles for evaluating information, heuristics help people to judge information credibility some-what intuitively while retaining some consistencies across different information seeking and use situations. For example, as people develop heuristics about certain digital media, they may not need to examine various char-acteristics of information objects or sources at the interaction level in order to evaluate the information they encounter. Similarly, if people hold heuristics about the aesthetic aspects of Web sites, they tend to apply these heuristics across various Web sites.
 Most previous credibility research has focused on cues on the interaction level (e.g., Fogg et al., 2001, 2003;
Hong, 2006; Rieh, 2002 ). The results of this study show that credibility judgments taking place when interact-ing with specific cues are affected by credibility judgments made on the other two levels. For instance, a person may have heuristics that the information issuing from commercial Web sites is not credible compared to that issuing from government or educational Web sites. However, when the content is read carefully, the person may trust the information from commercial sites as sounding credible. Once the person gains this experience, it would become a heuristic that may be used in future credibility assessments. The findings also imply that people X  X  perceptions of context influence credibility judgments on the interaction level in fundamental ways that go beyond the appearance, source, and content of information.
 This study X  X  findings reveal that credibility is not viewed as an absolute attribute of information or sources.
Rather, credibility is seen as relative to the social context in which information seeking is pursued and cred-ibility judgments are made ( Rieh &amp; Hilligoss, 2007 ). Awareness of the boundaries of credibility assessment helps participants to determine their selection of resources by filtering out stimuli in the information seeking process. The contextual factors also influence credibility assessment by  X  X  X ounding X  X  or otherwise limiting the information use environments. 6. Conclusion
Given that this framework was developed from a qualitative study of 24 participants, the findings need to be discussed with caution. Nonetheless, the unifying framework provides a fruitful initial base with which to understand better the multiple-layered and complex nature of credibility assessment for future research on credibility. This article contributes to the understanding of credibility in three distinct ways.
First, credibility judgments can be understood by taking into consideration multiple levels from construct to heuristics and interaction that go beyond focusing on cues for assessment. The cues that people rely on to make credibility judgments have been the focus of much credibility research because these cues are easily observed by researchers and discussed by study participants. However, the framework herein suggests that credibility assessment needs to be understood beyond the level of interaction, by incorporating much broader perspectives given that people also make credibility judgments in their process of developing constructs, rely-ing on heuristics, and considering contexts.

Secondly, this research has implications for methodology when investigating credibility. Credibility assess-ments are often made internally, and it might prove difficult for many people to articulate their cognitively processed judgments. The methodology used in this research took a naturalistic approach in which partici-pants were asked to keep track of their information seeking activities in diaries. In the interviews, questions were primarily asked about information seeking resources selected and strategies taken. That is because cred-ibility judgments are embedded in the information seeking process as people tend to judge the value of cred-ibility implicitly when deciding where to find information and what to select.

Third, the framework herein demonstrates the importance of understanding credibility across multiple media and information resources. The findings indicate that participants relied on multiple types of media and resources within one information seeking episode. More importantly, they often preferred to make cred-ibility judgments by comparing the information retrieved from different resources. Thus, rather than focusing on credibility issues on the Web, in online news, or in electronic scholarly journals, for example, credibility should be investigated by taking into consideration multiple kinds of information media and resources that people are likely to use for their information seeking processes.
 Acknowledgements
The authors are grateful to Jiyeon Yang and Aiko Takazawa for providing research assistance in coding the interview transcripts using NVivo 2.0. Thanks also to Professor Joan Durrance and the members of the Infor-mation Behavior Theory Research Group at the School of Information for their insightful suggestions and helpful discussions on an early draft. The authors also thank the anonymous reviewers for constructive comments.
 References
