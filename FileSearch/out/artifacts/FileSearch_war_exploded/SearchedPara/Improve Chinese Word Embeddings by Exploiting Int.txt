 Distributed representations of knowledge has re-ceived wide attention in recent years. Researchers have proposed various models to learn it at different granularity levels. Distributed word representation-s, also known as word embeddings, were learned in (Rumelhart et al., 1988; Bengio et al., 2006; Mni-h and Hinton, 2009; Mikolov et al., 2013a). Larger granularity levels than words have also been inves-tigated, including phrase level (Socher et al., 2010; Zhang et al., 2014; Yu and Dredze, 2015), sentence level (Le and Mikolov, 2014; Socher et al., 2013; Kalchbrenner et al., 2014; Kiros et al., 2015), and document level (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Srivastava et al., 2013).
For language like Chinese, some smaller units than word also provide rich semantic information. For example, Chinese characters in word, Chinese radicals in character. These internal structures have been proved to be useful for Chinese word and char-acter embeddings (Chen et al., 2015; Li et al., 2015). Chen et al. (2015) took Chinese characters in a word into account when modeling the semantic meaning of the word. They proposed a character-enhanced word embeddings model (CWE) by adding the em-bedding of component characters in a word with the same weight to the word embedding. However, the internal characters in a Chinese word have dif-ferent semantic contributions to its meaning. Take Chinese word  X   X   X  (frog) as an example. The character  X   X   X  (blue or green) is to decorate char-acter  X   X  (frog). It is obvious that the latter char-acter contributes more than the former one to the word meaning. In Li et al. (2015), they proposed a component-enhanced Chinese character embed-dings model based on the feature that most Chinese characters are phono-semantic compounds. They considered characters and bi-characters as the ba-sic embedding units. However, some bi-characters are meaningless, and may not form a Chinese word. These bi-characters may undermine embeddings of others.

This paper, motivated by Chen et al. (2015), ex-ploits the internal structures of Chinese word, name-ly the Chinese characters. We propose a method to calculate the semantic contribution of character-s to a word in a cross-lingual manner. The basic idea is that the semantic contribution of Chinese characters in most Chinese words can be learned from their translations in other languages. Such as the word  X   X   X  we mentioned above. The word embeddings of other languages are used to calculate semantic contribution of characters to the word they compose. Moreover, Chinese character-s are more ambiguous than words. To tackle this problem, multiple-prototype character embeddings is proposed. Different meanings of characters will be represented by different embeddings. Our contri-butions can be summarized as follows: 1. We provide a method to calculate the semantic contribution of Chinese characters to the word they compose with English translation. Compared with English, there are fewer human-made resources to supervise the learning process of Chinese word and character embeddings. While translation resources are always easy to be accessed on the Internet. 2. We propose a novel way to disambiguate Chi-nese characters with translating resources. There are some limitations in existing cluster-based algo-rithms (Huang et al., 2012; Neelakantan et al., 2015; Chen et al., 2015). They either fixed the number of clusters or proposed a nonparametric way to learn it for each word. However, the number of clusters for words varies a lot. For nonparametric method, dif-ferent hyperparameters have to be tune to control the number of clusters for different datasets. 3. We provide a method to distinguish whether a Chinese word is semantically compositional auto-matically. Not all Chinese words exhibit semantic compositions from their component characters. For example, entity names, transliterated words like  X   X  u  X  (sofa), single-morpheme multi-character words like  X   X  }  X  (wander). In Chen et al. (2015), they performed part-of-speech tagging to identify entity names. The transliterated words are tagged manual-ly, which requires human work and need to be up-dated when new words are created.

The evaluations on word similarity, text classifica-tion, Chinese characters disambiguation, and quali-tative analysis of word embeddings demonstrate the effectiveness of our method. 2.1 Word2vec Word2vec (Mikolov et al., 2013a) is an algorithm to learn distributed word representations using a neu-ral language model. Word2vec has two models, the continuous bag-of-words model (CBOW) and the skip-gram model. In this paper, we propose a new model based on the CBOW, hence we focus atten-tion on it. CBOW aims at predicting the target word given context words in a slide window. Given a word sequence D = { x 1 ,x 2 ,...,x T } , the objective of CBOW is to maximize the average log probability where v x representations of word x i . Since the size of En-glish vocabulary W may be up to 10 6 scale, hier-archical softmax and negative sampling (Mikolov et al., 2013b) are applied during training to learn the model efficiently. However, using CBOW to learn Chinese word embeddings directly may have some limitations. It fails to capture the internal struc-ture of words. In (Botha and Blunsom, 2014; Lu-ong et al., 2013; Trask et al., 2015; Chen et al., 2015), they demonstrated the usefulness to exploit the internal structure of words, and proposed some morphological-based methods. For example, Chen et al. (2015) exploit the internal structure in Chinese words. 2.2 The CWE model The basic idea of CWE is that both external context words and internal component characters in words provide rich information in modeling the semantic meaning of the target word. In CWE, they learned word embeddings with its component characters embeddings. Let C denotes the Chinese characters set, and the word x t in context x i + j several characters in C , let x t = { c 1 ,c 2 ,...,c N c k denotes the k -th character in x t , where b v x notes the number of Chinese characters in x t . To address the issue of ambiguity in Chinese charac-ters, they proposed several approaches for multiple-prototype character embeddings: position-based, cluster-based, nonparametric methods, and position-cluster-based character embeddings. These meth-ods are denoted as CWE+P, CWE+L, CWE+N, CWE+LP respectively. However, this model has some limitations. The internal characters are of the same contribution to the semantic meaning of the word in CWE, which is not the case for most Chi-nese words. Our method can be described as three stages:  X  Obtain translations of Chinese words and  X  Perform Chinese character sense disam- X  Learn word and character embeddings with 3.1 Obtain translations of Chinese words and We use segmentation tools to segment words in Chi-nese training corpus, and perform part-of-speech tagging to recognize all the entity names. Since entity name words do not exhibit semantic com-positions, they are identified as non-compositional words. We count the times of characters appearing in different words. Words with Chinese character-s rarely combined with other characters are classi-fied as single-morpheme multi-character words and identified as non-compositional.

Then programming interface of online translation tool is used to translate Chinese words and char-acters into English. For non-compositional Chi-nese words, they are not included in the translation list. Table 1 shows the English meanings of Chinese word  X   X  W  X ,  X   X  u  X  and their component charac-ters  X   X   X  and  X  W  X ,  X   X   X  and  X  u  X . 3.2 Perform Chinese character sense We train an English corpus with CBOW to get En-glish word embeddings. Then, the meanings of char-acters with small difference are merged.

In Table 1, we observe that the difference between some meanings of character  X  W  X  is very small, some of them differ only in their part-of-speech. In Chinese, the same characters and words are used in different part-of-speech but express the same se-mantic meaning. Hence these meanings are merged as one semantic meaning. Let Sim(  X  ) denotes the function to calculate the similarity between mean-ings of Chinese words and characters, we use cosine distance as the distance metric. The i -th and j -th meanings of Chinese character c are c i and c j . Their similarity is defined as: where Trans( c i ) denotes the English translation words set of c i , stop words(en) denotes the stop words in English, x m and x n are not in these stop words. For example, the Chinese word  X   X  W  X  in Table 1, c 2 denotes the second character  X 
W  X  in the word. Trans( c 3 2 ) is the third trans-lation English words set of character  X  W  X , which is { pleasure,enjoyment } . Therefore x m can be pleasure or enjoyment here.

If the Sim( c i ,c j ) is above a threshold  X  , then they are merged as one semantic meaning. For simplici-ty, we use the union of English translation words set. One character may be translated into several English words. We may average all the translation word em-beddings and then compute the similarity, or selec-t the maximum value of the similarity between all English word pairs. In our experiments, maximum method works better.

Finally, we perform Chinese character sense dis-ambiguation. In Chinese, characters may have mul-tiple meanings, but for a certain word, their mean-ings are determined. For exmaple, the word  X   X  W  X , the English translation is music. For character  X  W  X , the first translation  X  X usic X  matches the meaning of the word. For character  X   X   X , the best match is the first translation  X  X ound X . For transliterated word like  X   X  u  X , the English translations are sofa and settee, neither sofa nor settee have high similarity with En-glish translation words of character  X   X   X  and charac-ter  X  u  X . Formally, if max(Sim( x t ,c k )) &gt;  X ,c k  X  x , then x t is identified as compositional word, and belongs to the compositional set COMP . For com-positional words, we build a set where
For example, the word  X   X  W  X  is defined as ( X   X  W  X , { Sim ( X   X  W  X ,  X   X   X ), Sim ( X   X  W  X ,  X  W  X ) } , { 1,1 } ) in F . 3.3 Learn word and character vectors with The internal characters in a word make differen-t contributions to its semantic meaning. However, in Chen et al. (2015), the contribution of compo-nent characters to the semantic meaning of word are treated equally. They add character embeddings to the word embeddings with the same weight, which may undermine the quality of word embeddings. Based on this point, we propose a similarity-based character-enhanced word embedding model, which takes the contribution of characters into account. We name it SCWE for ease of reference in the later part. The architecture of CWE and SCWE are shown in Fig. 1.
 Similarity-Based Character-Enhanced word Embedding In the character sense disambiguation stage, we build a set F , which contains composi-tional words, the similarity between words and its component characters, and the meaning order num-ber of characters in the word. Suppose x t in W is a compositional word, in SCWE,
To deal with ambiguity problem of Chinese char-acters, we propose multiple-prototype character em-beddings and denote it as SCWE+M model. Since the meaning of a character is determined in a given word, we utilize the information provided by the last element in set F , and use different character embed-dings for different meanings of characters. Then, in SCWE+M,
Complexity analysis We analyze the complexi-ties of CBOW, CWE, SCWE and SCWE+M. Let S denotes the size of corpus, | W | denotes the size of vocabulary, | C | denotes the number of Chinese char-acters in corpus. And d is the dimensions of Chi-nese word and character embeddings, k is the con-text window size, f is the time spend in computing hierarchical softmax or negative sampling, n is the average number of characters in a Chinese word, m is the average meaning number of Chinese charac-ters. The results are shown in Table 2.

In Chinese, most of words are composed by t-wo Chinese characters, and the meaning number of commonly used characters are usually less than five. Moreover, according to CJK Unified Ideographs 1 , the total number of Chinese characters is 20913, the commonly used characters are less than 10000. Therefore, our model is competitive to other meth-ods in model parameters and computational com-plexity.
 4.1 Experiments Settings We select English Wikipedia Dump 2 to train English word embeddings with CBOW, and set dimensions to 200. For Chinese word embeddings, we selec-t Chinese Wikipedia Dump 3 to train character and word embeddings. Before training, pure digits and non-Chinese characters are removed. We use an open-source Chinese segment tool called ANSJ 4 to segment words in corpus. ANSJ is a java implemen-tation of ICTCLAS (Institute of Computing Tech-nology, Chinese Lexical Analysis System). It can process about one million words in a second, and get up to 96 percent accuracy in segmentation task. The part-of-speech tagging and name entity recog-nition tasks are also done in this process. We select provides us with an application programming inter-face. CBOW and CWE are used as baseline meth-ods. Context window size is set as 5 and both Chi-nese word and character embeddings are set as 100 dimension. After some cross validation steps, our threshold  X  and  X  are set as 0.5 and 0.4 in character disambiguation process. The influence of  X  and  X  is report in the later part.
 4.2 Word Similarity Word similarity is a task to compute semantic relat-edness between given word pairs. The relatedness between word pairs have been scored by human in advance. The correlation between model results and human judgement can be used to evaluate the per-formance of models. In this paper, wordsim-240 and wordsim-296 (Jin and Wu, 2012) are used as e-valuation datasets. The Spearman X  X  rank correlation (Myers et al., 2010) is applied to compute the corre-lation. The experimental results are summarized in Table 3.
 We observe that on wordsim-240, SCWE and SCWE+M outperform the baseline methods, which indicates the effectiveness of exploiting the inter-nal structure. On dataset wordsim-296, we can see that CBOW, CWE, SCWE perform similarly. This may be explained by some highly ambiguous Chinese characters in this dataset. In SCWE and CWE, representing these ambiguous characters with the same embeddings may undermine word embed-dings. Therefore, SCWE+M achieves a better per-formance by applying multiple-prototype character embeddings. 4.3 Text Classification In this experiment, we use Fudan Corpus 6 as datasets, which contains 20 categories of docu-ments, including economy, politics, sports and etc.. The number of documents in each category ranges from 27 to 1061. To avoid imbalance, we select 10 categories and organize them into 2 groups. One group is named Fudan-large and each category in this group contains more than 1000 documents. The other is named Fudan-small and each category con-tains less than 100 documents. In each category, 80 percent of documents are used as training set, the rest are used as testing set to evaluate the perfor-mance. The detailed information for two datasets are reported in Table 4.

Similar to the way we deal with Chinese training corpus, pure digits and non-Chinese characters are removed and ANSJ is used to do word segmentation on these datasets. The publish information of each document is removed. We represent each documen-t by averaging word embeddings in the document. The classifiers are trained using LIBLINEAR pack-age(Fan et al., 2008) with the embeddings obtained from different methods. The performance of each method is evaluated by predicting accuracy on test-ing set. Experiment results are given in Table 5.
It is observed that our methods outperform the baseline methods on both datasets. This can be ex-plained that the semantic relatedness of a word with the component characters which have more contri-bution to its semantic meaning is strengthen in our methods. Such as, in sports documents, the word  X   X   X  (ball) is used frequently. For Chinese word-s like  X  ;  X   X  (basketball) and  X   X   X  (tennis), the character  X   X   X  contributes more to their semantic meaning than other characters. Therefore, they lie closer to character  X   X   X  in embedding space ob-tained by our model than CBOW and CWE, and tend to form a cluster in embedding space. 4.4 Multiple Prototype of Chinese Charaters To tackle the ambiguity of Chinese characters, we propose multiple-prototype character embeddings. To evaluate the effectiveness of our method, we use PCA to conduct dimensionality reduction on word and character embeddings. The results are illustrat-ed in Fig 2. We take 3 different meanings of Chinese characters  X   X  and  X  1  X , and 2 of their top-related words as examples. The character followed by a dig-it i denotes the i -th meanings of it.

We can observe that characters and words, which have similar meanings are gathered together. For ex-ample,  X  1 3 X ,  X  1 r  X  and  X  fl 1  X   X  are all related to the light. Thus, they get closer in the embedding space.

We also develop a dataset to compare our method with the disambiguation methods in Chen et al. (2015). We select some ambiguous Chinese char-acters, and then use online Xinhua Dictionary 7 as our standard to disambiguate the words that contain these ambiguous characters. Each word is assigned a number according to their explanation in the dic-tionary. We use KNN as classifier to evaluate all the methods. The results are shown in Table 7. It is observed that our method outperforms the methods proposed in Chen et al. (2015).
 4.5 Qualitative analysis of word embeddings In this part, we take two Chinese words as examples, and list their nearest words to examine the quality of word embeddings obtained by CWE and SCWE. The results are shown in Table 8. We can observe the most similar words return by CWE and SCWE both tend to share common characters with the giv-en word. In CWE, characters with little semantic contribution to the word may undermine the quali-ty of word embeddings. For example, the charac-ter  X   X   X  in word  X   X   X . The semantic relatedness of words with character  X   X   X  to the given word are overestimated in CWE. In our model, by calculat-ing the semantic contribution of internal characters to the word, we alleviate this misjudgement greatly, which demonstrates the effectiveness of our model. 4.6 Parameter Analysis In this part, the influence of parameters on our model is investigated. The parameters include the compo-sitional word similarity threshold  X  , character dis-ambiguation threshold  X  .

Compositional word similarity To investigate how  X  influence the process of non-compositional word detection, we build a word list of transliterated words manually, which consists of 161 words. Then 161 of most frequent semantic compositional words with more than one Chinese characters are added to the list in the corpus. In Table 9, the performance of our method in classifying transliterated words when  X  ranges from 0.25 to 0.55 are reported. From Ta-ble 9, we can observe as  X  increases, more composi-tional words will be classified as non-compositional words, while transliterated words are more likely to be classified correctly. Our method achieves best F-Score when  X  = 0 . 4 .

Character disambiguation threshold In Table 10, we show the performance of our model in disam-biguating Chinese characters. We adopted the same datasets in Section 4.4 with different  X  . From Ta-ble 1, we can observe some meanings of a character are very close, therefore, a high  X  are adopted in our model. When  X  = 0 . 5 , our model gets the best result in our dataset.
 In this paper, we exploit the internal structure in Chinese words by learning the semantic contribu-tion of internal characters to the word. We pro-pose a method to improve Chinese word and char-acter embeddings with a similarity-based character-enhanced word embeddings model. Ambiguity problem of Chinese characters can also be tack-led in our method. Moreover, we build a way to classify whether a Chinese word is compositional automatically, which requires to be labelled man-ually in CWE. We argue that our method may be used to improve word embeddings of other language whose internal structure is similar to Chinese. The code and datasets we use is available at: https: //github.com/JianXu123/SCWE .
 This work is supported by NSFC grants 91546116 and 61511130083.

