 Offline handwriting recognition is generally observed to be harder than online handwriting recogni-tion [14]. In the online case, features can be extracted from both the pen trajectory and the resulting image, whereas in the offline case only the image is available. Nonetheless, the standard recognition process is essentially the same: a sequence of features are extracted from the data, then matched to a sequence of labels (usually characters or sub-character strokes) using either a hidden Markov model (HMM) [9] or an HMM-neural network hybrid [10].
 The main drawback of this approach is that the input features must meet the stringent independence assumptions imposed by HMMs (these assumptions are somewhat relaxed in the case of hybrid systems, but long-range input dependencies are still problematic). In practice this means the features must be redesigned for every alphabet, and, to a lesser extent, for every language. For example it would be impossible to use the same system to recognise both English and Arabic.
 Following our recent success in transcribing raw online handwriting data with recurrent net-works [6], we wanted to build an offline recognition system that would work on raw pixels. As well as being alphabet-independent, such a system would have the advantage of being globally trainable, with the image features optimised along with the classifier.
 The online case was relatively straightforward, since the input data formed a 1D sequence that could be fed directly to a recurrent network. The long short-term memory (LSTM) network architec-ture [8, 3] was chosen for its ability to access long-range context, and the connectionist temporal classification [5] output layer allowed the network to transcribe the data with no prior segmentation. The offline case, however, is more challenging, since the input is no longer one-dimensional. A naive approach would be to present the images to the network one vertical line at a time, thereby transforming them into 1D sequences. However such a system would be unable to handle distor-Figure 1: Two dimensional MDRNN . The thick lines show connections to the current point ( i,j ) . The connections within the hidden layer plane are recurrent. The dashed lines show the scanning strips along which previous points were visited, starting at the top left corner. tions along the vertical axis; for example the same image shifted up by one pixel would appear completely different. A more flexible solution is offered by multidimensional recurrent neural net-works (MDRNNs) [7]. MDRNNs, which are a special case of directed acyclic graph networks [1], generalise standard RNNs by providing recurrent connections along all spatio-temporal dimensions present in the data. These connections make MDRNNs robust to local distortions along any com-bination of input dimensions (e.g. image rotations and shears, which mix vertical and horizontal displacements) and allow them to model multidimensional context in a flexible way. We use multi-dimensional LSTM because it is able to access long-range context.
 The problem remains, though, of how to transform two-dimensional images into one-dimensional label sequences. Our solution is to pass the data through a hierarchy of MDRNN layers, with blocks of activations gathered together after each level. The heights of the blocks are chosen to incrementally collapse the 2D images onto 1D sequences, which can then be labelled by the output layer. Such hierarchical structures are common in computer vision [15], because they allow complex features to be built up in stages. In particular our multilayered structure is similar to that used by convolution networks [11], although it should be noted that because convolution networks are not recurrent, they cannot be used for cursive handwriting recognition without presegmented inputs. The method is described in detail in Section 2, experimental results are given in Section 3, and conclusions and directions for future work are given in Section 4. The three components of our recognition system are: (1) multidimensional recurrent neural net-works, and multidimensional LSTM in particular; (2) the connectionist temporal classification out-put layer; and (3) the hierarchical structure. In what follows we describe each component in turn, then show how they fit together to form a complete system. For a more detailed description of (1) and (2) we refer the reader to [4] 2.1 Multidimensional Recurrent Neural Networks The basic idea of multidimensional recurrent neural networks (MDRNNs) [7] is to replace the single recurrent connection found in standard recurrent networks with as many connections as there are spatio-temporal dimensions in the data. These connections allow the network to create a flexible internal representation of surrounding context, which is robust to localised distortions. An MDRNN hidden layer scans through the input in 1D strips, storing its activations in a buffer. The strips are ordered in such a way that at every point the layer has already visited the points one step back along every dimension. The hidden activations at these previous points are fed to the current point through recurrent connections, along with the input. The 2D case is illustrated in Fig. 1. One such layer is sufficient to give the network access to all context against the direction of scan-ning from the current point (e.g. to the top and left of ( i,j ) in Fig. 1). However we usually want surrounding context in all directions. The same problem exists in 1D networks, where it is often useful to have information about the future as well as the past. The canonical 1D solution is bidi-rectional recurrent networks [16], where two separate hidden layers scan through the input forwards and backwards. The generalisation of bidirectional networks to n dimensions requires 2 n hidden layers, starting in every corner of the n dimensional hypercube and scanning in opposite directions. For example, a 2D network has four layers, one starting in the top left and scanning down and right, one starting in the bottom left and scanning up and right, etc. All the hidden layers are connected to a single output layer, which therefore receives information about all surrounding context. The error gradient of an MDRNN can be calculated with an n-dimensional extension of backprop-agation through time. As in the 1D case, the data is processed in the reverse order of the forward pass, with each hidden layer receiving both the output derivatives and its own n  X  X uture X  derivatives at every timestep.
 dimensional input sequence x with dimensions ( D 1 ,...,D n ) . Let p  X  d = ( p 1 ,...,p d  X  1 ,...,p n ) connection from unit i to unit j and the recurrent connection from i to j along dimension d . Let  X  h be the activation function of hidden unit h , and for some unit j and some differentiable objective function O let  X  p j =  X  X   X  X  p with I input units, K output units, and H hidden summation units are as follows: 2.1.1 Multidimensional LSTM Long Short-Term Memory (LSTM) [8, 3] is an RNN architecture designed for data with long-range interdependencies. An LSTM layer consists of recurrently connected  X  X emory cells X , whose activa-tions are controlled by three multiplicative gate units: the input gate, forget gate and output gate. The gates allows the cells to store and retrieve information over time, giving them access to long-range context.
 The standard formulation of LSTM is explicitly one-dimensional, since each cell contains a single recurrent connection, whose activation is controlled by a single forget gate. However we can extend this to n dimensions by using instead n recurrent connections (one for each of the cell X  X  previous states along every dimension) with n forget gates.
 Consider an MDLSTM memory cell in a hidden layer of H cells, connected to I input units and K output units. The subscripts c ,  X  ,  X  and  X  refer to the cell, input gate, forget gate and output gate cell input and output activation functions. The suffix  X ,d denotes the forget gate corresponding to recurrent connection d . The input gate  X  is connected to previous cell c along all dimensions with for each dimension d . Then the forward and backward equations are as follows: 2.2 Connectionist Temporal Classification Connectionist temporal classification (CTC) [5] is an output layer designed for sequence labelling with RNNs. Unlike other neural network output layers it does not require pre-segmented training data, or postprocessing to transform its outputs into transcriptions. Instead, it trains the network to directly estimate the conditional probabilities of the possible labellings given the input sequences. A CTC output layer contains one more unit than there are elements in the alphabet L of labels for the task. The output activations are normalised at each timestep with the softmax activation function [2]. The first | L | outputs estimate the probabilities of observing the corresponding labels at that time, and the extra output estimates the probability of observing a  X  X lank X , or no label. The combined output sequence estimates the joint probability of all possible alignments of the input sequence with all sequences of labels and blanks. The probability of a particular labelling can then be estimated by summing over the probabilities of all the alignments that correspond to it.
 More precisely, for a length T input sequence x , the CTC outputs define a probability distribution over the set L 0 T of length T sequences over the alphabet L 0 = L  X  X  blank } . To distinguish them from labellings, we refer to the elements of L 0 T as paths . Since the probabilities of the labels at each timestep are conditionally independent given x , the conditional probability of a path  X   X  L 0 T is given by p (  X  | x ) = Q T t =1 y t  X  Paths are mapped onto labellings l  X  L  X  T by an operator B that removes first the repeated labels, then the blanks. So for example, both B ( a,  X  ,a,b,  X  ) and B (  X  ,a,a,  X  ,  X  ,a,b,b ) yield the labelling ( a,a,b ) . Since the paths are mutually exclusive, the conditional probability of some labelling l  X  L  X  T is the sum of the probabilities of all paths corresponding to it: p ( l | x ) = P Although a naive calculation of this sum is unfeasible, it can be efficiently evaluated with a dynamic programming algorithm, similar to the forward-backward algorithm for HMMs.
 To allow for blanks in the output paths, for each labelling l  X  L  X  T consider a modified labelling l  X  L 0  X  T , with blanks added to the beginning and the end and inserted between every pair of labels. The length | l 0 | of l 0 is therefore 2 | l | + 1 .
 For a labelling l , define the forward variable  X  t ( s ) as the summed probability of all path beginnings reaching index s of l 0 at time t , and the backward variables  X  t ( s ) as the summed probability of all path endings that would complete the labelling l if the path beginning had reached s at time t . Both the forward and backward variables are calculated recursively [5]. The label sequence probability is given by the sum of the products of the forward and backward variables at any timestep, i.e. p ( l | x ) = P | l 0 | s =1  X  t ( s )  X  t ( s ) .
 Then the objective function O for CTC is the negative log probability of the network correctly first differentiating O with respect to the outputs, then using backpropagation through time to find the derivatives with respect to the weights.
 Note that the same label (or blank) may be repeated several times for a single labelling l . We define the set of positions where label k occurs as lab ( l ,k ) = { s : l 0 s = k } , which may be empty. Setting l = z and differentiating O with respect to the network outputs, we obtain: Once the network is trained, we can label some unknown input sequence x by choosing the labelling l with the highest conditional probability, i.e. l  X  = arg max l p ( l | x ) . In cases where a dictionary is used, the labelling can be constrained to yield only sequences of complete words by using the CTC token passing algorithm [6]. For the experiments in this paper, the labellings were further constrained to give single word sequences only, and the ten most probable words were recorded. 2.3 Network Hierarchy Many computer vision systems use a hierarchical approach to feature extraction, with the features at each level used as input to the next level [15]. This allows complex visual properties to be built up in stages. Typically, such systems use subsampling, with the feature resolution decreased at each stage. They also generally have more features at the higher levels. The basic idea is to progress from a small number of simple local features to a large number of complex global features.
 We created a hierarchical structure by repeatedly composing MDLSTM layers with feedforward layers. The basic procedure is as follows: (1) the image is divided into small pixel blocks, each of which is presented as a single input to the first set of MDLSTM layers (e.g. a 4x3 block is reduced to a length 12 vector). If the image does not divide exactly into blocks, it is padded with zeros. (2) the four MDLSTM layers scan through the pixel blocks in all directions. (3) the activations of the MDLSTM layers are collected into blocks. (4) these blocks are given as input to a feedforward layer. Note that all the layers have a 2D array of activations: e.g. a 10 unit feedforward layer with input from a 5x5 array of MDLSTM blocks has a total of 250 activations.
 The above process is repeated as many times as required, with the activations of the feedforward layer taking the place of the original image. The purpose of the blocks is twofold: to collect local contextual information, and to reduce the area of the activation arrays. In particular, we want to reduce the vertical dimension, since the CTC output layer requires a 1D sequence as input. Note that the blocks themselves do not reduce the overall amount of data; that is done by the layers that process them, which are therefore analogous to the subsampling steps in other approaches (although with trainable weights rather than a fixed subsampling function).
 For most tasks we find that a hierarchy of three MDLSTM/feedforward stages gives the best results. We use the standard  X  X nverted pyramid X  structure, with small layers at the bottom and large layers at the top. As well as allowing for more features at higher levels, this leads to efficient networks, since most of the weights are concentrated in the upper layers, which have a smaller input area. In general we cannot assume that the input images are of fixed size. Therefore it is difficult to choose block heights that ensure that the final activation array will always be one-dimensional, as required by CTC. A simple solution is to collapse the final array by summing over all the inputs in each uncollapsed input to unit k at point ( x,y ) in the final array. Figure 2: The complete recognition system . First the input image is collected into boxes 3 pixels wide and 4 pixels high which are then scanned by four MDLSTM layers. The activations of the cells in each layer are displayed separately, and the arrows in the corners indicates the scanning direction. Next the MDLSTM activations are gathered into 4 x 3 boxes and fed to a feedforward layer of tanh summation units. This process is repeated two more times, until the final MDLSTM activations are collapsed to a 1D sequence and transcribed by the CTC layer. In this case all characters are correctly labelled except the second last one, and the correct town name is chosen from the dictionary. To see how our method compared to the state of the art, we applied it to data from the ICDAR 2007 Arabic handwriting recognition competition [12]. Although we were too late to enter the competition itself, the organisers kindly agreed to evaluate our system according to the competition criteria. We did not receive the test data at any point, and all evaluations were carried out by them. The goal of the competition was to identify the postcodes of Tunisian town and village names. The names are presented individually, so it is an isolated word recognition task. However we would like to point out that our system is equally applicable to unconstrained handwriting, and has been successfully applied to complete lines of English text. 3.1 Data The competition was based on the IFN/ENIT database of handwritten Arabic words [13]. The publically available data consists of 32,492 images of handwritten Tunisian town names, of which we used 30,000 for training, and 2,492 for validation. The images were extracted from artificial Table 1: Results on the ICDAR 2007 Arabic handwriting recognition contest . All scores are percentages of correctly identified postcodes. The systems are ordered by the  X  X op 1 X  results on test set  X  X  X . The best score in each column is shown in bold.
 forms filled in by over 400 Tunisian people. The forms were designed to simulate writing on a letter, and contained no lines or boxes to constrain the writing style.
 Each image was supplied with a ground truth transcription for the individual characters 1 . There were 120 distinct characters in total. A list of 937 town names and postcodes was provided. Many of the town names had transcription variants, giving a total of 1,518 entries in the complete dictionary. The test data (which is not published) was divided into sets  X  X  X  and  X  X  X . The main competition results were based on set  X  X  X . Set  X  X  X  contains data collected in the United Arab Emirates using the same forms; its purpose was to test the robustness of the recognisers to regional writing variations. The systems were allowed to choose up to 10 postcodes for each image, in order of preference. The test set performance using the top 1, top 5, and top 10 answers was recorded by the organisers. 3.2 Network Parameters The structure shown in Figure 2 was used, with each layer fully connected to the next layer in the hierarchy, all MDLSTM layers connected to themselves, and all units connected to a bias weight. There were 159,369 weights in total. This may sound like a lot, but as mentioned in Section 2.3, the  X  X nverted pyramid X  structure greatly reduces the actual number of weight operations. In effect the higher up networks (where the vast majority of the weights are concentrated) are processing much smaller images than those lower down. The squashing function for the gates was the logistic sigmoid f ( x ) = 1 / (1 + e  X  x ) , while tanh was used for f 2 and f 3 . Each pass through the training set took about an hour on a desktop computer, and the network converged after 85 passes.
 The complete system was trained with online gradient descent, using a learning rate of 10  X  4 and a momentum of 0 . 9 . The character error rate was evaluated on the validation set after every pass through the training set, and training was stopped after 50 evaluations with no improvement. The weights giving the lowest error rate on the validation set were passed to the competition organisers for assessment on the test sets. 3.3 Results Table 1 clearly shows that our system outperformed all entries in the 2007 ICDAR Arabic recogni-tion contest. The other systems, most of which are based on hidden Markov models, are identified by the names of the groups that submitted them (see [12] for more information). We have combined multidimensional LSTM with connectionist temporal classification and a hierar-chical layer structure to create a powerful offline handwriting recogniser. The system is very general, and has been successfully applied to English as well as Arabic. Indeed, since the dimensionality of the networks can be changed to match that of the data, it could in principle be used for almost any supervised sequence labelling task.
 Acknowledgements We would like to thank Haikal El Abed for giving us access to the ICDAR competition data, and for persisting in the face of technical despair to install and evaluate our software. This work was supported by the excellence cluster  X  X ognition for Technical Systems X  (CoTeSys) from the German Research Foundation (DFG).

