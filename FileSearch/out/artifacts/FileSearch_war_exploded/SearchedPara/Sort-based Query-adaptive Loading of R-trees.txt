 Bulk-loading of R-trees has been an important problem in academia and industry for more than twenty years. Current algorithms create R-trees without any information about the expected query profile. However, query profiles are ex-tremely useful for the design of efficient indexes. In this paper, we address this deficiency and present query-adaptive algorithms for building R-trees optimally designed for a given query profile. Since optimal R-tree loading is NP-hard (even without tuning the structure to a query profile), we provide efficient, easy to implement heuristics. Our sort-based algo-rithms for query-adaptive loading consist of two steps: First, sorting orders are identified resulting in better R-trees than those obtained from standard space-filling curves. Second, for a given sorting order, we propose a dynamic program-ming algorithm for generating R-trees in linear runtime. Our experimental results confirm that our algorithms generally create significantly better R-trees than the ones obtained from standard sort-based loading algorithms, even when the query profile is unknown.
 H.2.2 [ Physical Design ]: Access methods R-tree, Bulk-loading, Dynamic Programming, Z-Curve
Index bulk-loading of very large data sets has been an important problem in database research. Loading is neces-sary when an index has to be built up for the first time. Moreover, clustered indexes also require periodical reload to reestablish the clustering of records. It is well known that loading indexes by inserting tuples one by one is less effi-cient than specially designed bulk-algorithms that run with the same complexity as external sorting. Bulk-loading is therefore an interesting option for supporting updates on indexes by buffering updates and reloading the index after the buffer is sufficiently filled up.

While there is a standard approach for loading of B-trees, many different techniques [8, 7, 5, 11, 16, 10] were proposed for multidimensional indexes like R-trees. For loading R-trees, there is always a tradeoff between loading efficiency and index quality, i.e., how efficient an R-tree can support queries (and updates). Despite the large number of loading strategies, none appear to be ideal for the following reasons:
First, current loading strategies for R-trees do not (similar to B-trees) consider the query profile. Ignoring the query profile has only a minor impact on the quality of B-trees, but might result in poorly loaded R-trees. Consider the example of two extreme query profiles for a two-dimensional data set: One profile only contains partial exact match queries ( pemq ) with an exact match in the first dimension, while the other contains pemq with exact match in the second dimension. Note that the ideal R-tree would actually be a B+-tree on the first and second dimension, respectively. All loading algorithms for R-trees ignore the query profile and would build the same R-tree although neither is designed for these extreme profiles.

Second, many of the sophisticated loading algorithms seem to be quite complicated to implement. While these algo-rithms create R-trees with excellent worst-case query per-formance (see [4]), integration into a system turns out to be quite difficult. It is therefore not surprising that less com-plex loading algorithms for R-trees are used in commercial systems like STR [11] and popular sort-based loading strate-gies [10]. However, the resulting R-trees are not optimized for specific queries. So far, a theoretical foundation of these methods does not exist.
 In this paper, we revise the problem of loading R-trees. We aim to design algorithms for query-adaptive loading R-trees optimized in respect to a given query profile. Here, we focus on sort-based techniques because of their conceptual simplicity. This makes these techniques very appealing for commercial systems where other approaches are difficult to integrate due to their implementation complexity.
In this paper, we design a novel (sort-based) algorithm that takes the underlying query profile into account, we sketch NP-hardness of the optimal R-tree loading and pro-pose algorithms based on dynamic programming (DP) for generating optimal R-trees given a specific sort order (one of them requires linear runtime and space). Our experimen-tal results show that R-trees built with our methods provide a better quality than competitors, even in the case of un-k nown query profile.
In this paper, we address the problem of R-tree loading for a d -dimensional set of N rectangles { r 1 , . . . , r consist of pages with maximum capacity B and minimum occupation b  X   X  B/ 2  X  . We consider the case d = 2, the generalized case for d &gt; 2 is only discussed when necessary.
For query-adaptive loading, we assume that a query pro-file QP for range queries is given. QP provides a (statisti-cal) model that is derived from a collection of representative queries. For the sake of simplicity, we consider the query profile for range queries that is given by the average size of the range in each dimension. For d = 2, QP = ( sx, sy ), where sx and sy is the average size of the range query in the first and second dimension, respectively. We assume that queries, more precisely their centers, are uniformly dis-tributed in the underlying domain. In a real application, multidimensional histograms can be used to overcome the uniform assumption by maintaining these parameters for each histogram cell independently [3, 15]. This approach has already been used successfully for the analysis of R-trees [17].
For a given query profile QP = ( sx, sy ), our goal is to generate R-trees with minimum average query performance. In order to measure query performance, we use the number of leaf accesses as our performance indicators. This is jus-tified for sufficiently large range queries. Moreover, upper levels of the trees are often located in memory, while leaf pages are generally not.

Our goal is to create optimal R-trees level by level, bottom-up. However, as we show in Section 3, the problem of gen-erating the optimal leaf level of R-trees is already NP-hard. Therefore, sort-based heuristics are examined traversing the following five steps: 1. Determination of Sort Order: For a given QP determine a sort order that minimizes the cost. 2. Sorting: Sort the rectangles with respect to the de-termined order. 3. Partitioning: Partition the sorted sequence into sub-sequences of size between b and B and store each of them in a page. 4. Generation of Index Entries: For each page, com-pute the bounding box of its partitions and create the cor-responding index entry. 5. Recursion: If the total number of index entries is less than B , store them in a newly allocated root. Other-wise, start the algorithm with the generated index entries (bounding boxes) from Step 4.

Step 2 and Step 4 are very similar to the traditional sort-based loading of R-trees [16]. The crucial optimization oc-curs in the first and third step. Step 1 computes a sort order from the query profile. We exploit the fact that a space-filling curve (SFC) does not require a symmetric treatment of dimension, but allows more flexibility [13]. As an exam-ple, consider partial exact match queries orthogonal to the x -axis. Then the sort order should be only influenced by the x -value. This corresponds to a SFC where all bits of the x -axis should precede the bits of the y -axis. For lack of space,we do not present the details in this paper, but refer the interested reader to [2]. In step 4, the rectangles are then assigned to pages such that the capacity constraints of the R-tree are met. Filling up pages to the maximum (or as generally suggested to a constant degree) does not lead to R-trees optimized in respect to the given query profile. High storage utilization is only useful for fairly large queries, while the performance of smaller queries suffer. In Section 3, we present a heuristic partitioning algorithm that is optimized according to the underlying query profile. Step 3 as well as Step 4 make use of a cost model that is derived from our query profile. Our work is based on cost models proposed by [10, 14]. As given in [14], range queries are classified according to as-pect ratio, location and size. If the aspect ratio is set to 1:1, there are two possibilities for each location and size property. For the location property, query rectangles follow either the distribution of the underlying data or an uniform distribu-tion. The size of queries can be defined either by area or by the number of objects. Combining these two properties re-sults in 4 different range query models ( W QM 1  X  4 as defined in [14]). The simplest W QM 1 models an uniform distribu-tion of query rectangles with fixed area and aspect ratio 1:1. In the following, we illustrate the case for d = 2.
Assume that the domain corresponds to the two-dimensional unit square [0 , 1) 2 . A rectangle r i = ( cx i , cy i , dx resented by its center ( cx i , cy i ) and its extension ( dx For a window query W Q q,s given by its center q = ( qx, qy ) and its extension s = ( sx, sy ), the probability of a rectangle r intersecting the window is ( dx i + sx )  X  ( dy i + sy ). The average number of rectangles intersecting the query window is then given by: Note that for point queries with s = (0 , 0), the equation computes the sum of areas. We obtain the expected num-ber of leaf accesses by applying the formula to the set of bounding boxes of the leaves.
In this section, we show that the problem of partitioning a set of rectangles is NP-hard, given that each bucket p i from partition P b,B := p 1 , . . . , p n has b  X  | p i |  X  B rectangles, so that for a given weight function w : p i  X  R + , the sum of weights is minimized. Let MBB ( p ) be a minimal bounding box of bucket p . According to the cost model, the weight function is defined as w ( p ) = area ( MBB ( p )). Based on these results, we develop a heuristic approach that optimally solves the partitioning problem for a given sorting order and area ( MBB ( p )). The justification for a heuristic approach lies in the NP-hardness of the problem.

Theorem 1. The problem of partitioning P b,B for N given rectangles that minimizes To prove the theorem, it suffices to consider the special case of B = 3 , b = 2 ( as b =  X  B/ 2  X  ) and a 2-dimensional space. For lack of space, we only present the key idea of the proof. The proof uses a polynomial time reduction from the version of planar 3SAT [12, 18, 2] in which for each variable, also an edge can be embedded in the plain. The edge is suited between the positive and the negative literal of a variable.
In this section we consider the problem of query-adaptive partitioning a sorted sequence r 1 , . . . , r N of rectangles such that each bucket of the partition corresponds to a page of the R-tree. This approach is a heuristic that is based on the specific sorting order, since the computation of an optimal partition is NP-hard for area ( MBB ( p )). Every bucket cor-responds to a contiguous subsequence p i,j = r i , . . . , r that b  X  j  X  i + 1  X  B is satisfied. A valid partition P consists of the subsequences p i,j such that each rectangle belongs to exactly one of them. Let S N denote the set of all valid partitions and let S N,m be the partitions that consist of exactly m buckets. While the standard sort-based load-ing strategy stores a fixed number of rectangles per page, we do not require equal numbers of objects per pages in our approach. This gives us flexibility to optimize the partition according to a given query profile QP again. Based on the cost model (see Equation 1) we consider the following opti-mization problems: 1. Storage-bounded Loading: Compute a partition 2. Query-optimal Loading: Compute a partition Note that query-optimal loading results in better partition, but the worst-case, storage utilization of the resulting R-trees can be as low as b/B . Storage-optimal loading allows to choose the desired storage utilization ( N/ ( m  X  B )) in advance by setting m .
 Let QP = ( sx, sy ) be a given query profile and C QP ( S ) = P p  X  S area + ( MBB ( p ) , QP ) be the sum of areas extended with average side length from query profile QP . More for-mally, area + ( r, QP ) = ( dx + sx )  X  ( dy + sy ) for a rectangle r = ( cx, cy, dx, dy ). C QP ( S ) denotes denotes the cost of a partition S  X  S N for a given query profile QP . This func-tion has a nice property that allows the design of an efficient algorithm to compute the optimum. Consider a split of a partition S into two arbitrary partitions S l and S r . Then, the following equality holds for our cost function: In particular, this equality is satisfied for the optimal parti-tion S opt . Note that, S l and S r must also be optimal parti-tions of their associated rectangles. In fact, this observation allows us to use the paradigm of dynamic programming in a similar way as for computing optimal histograms [9, 18]. For the first i rectangles and k contiguous sequences, computa-tion of the minimum cost opt  X  ( i, k ) is given by the following recursion: opt  X  ( i, k ) = min In order to compute opt  X  ( N, m ), we apply the recursive for-mula for all 1  X  i  X  N and 1  X  k  X  m , in increasing order of k , and for any fixed k in increasing order of i . We store all computed values of opt  X  ( i, k ) in a table. Thus, when a new opt  X  ( i  X  , k  X  ) is calculated using Equation 2, any opt may be needed can be read from the table. After computing the optimal cost, we can read out the contiguous sequences of the input rectangles from the dynamic programming ta-ble. Thus, the following theorem holds:
Theorem 2. The optimal partition S N,m of N rectangles into m buckets, each of them containing between b and B contiguous rectangles, can be computed in O ( N 2  X  B ) time and O ( N  X  m ) space.
 Next, we consider query-optimal loading, the problem of computing the optimal partition without user-defined stor-age utilization. At first glance, the problem appears to be harder because the solution space is larger. However, the opposite is true because the parameter m has no effect on the optimal solution anymore. This results in the following simplified recursion: In order to compute gopt  X  ( N ), we compute the recursive formula for all 1  X  i  X  N in increasing order of i . We store all computed values of gopt  X  ( i ) in a table. Thus, when a new gopt  X  (  X  i ) is calculated using Equation 3, any opt  X  ( i ) that may be needed can be read from the table. As in the case for opt  X  , we obtain the result sequences from the table. Thus, the following theorem holds:
Theorem 3. The optimal partition S N of N rectangles into buckets, each of them containing between b and B con-tiguous rectangles, can be computed in O ( N  X  B ) time and O(N) space.
 Theorem 3 shows that optimal loading is possible in as lit-tle as linear time. The required CPU-time is much lower compared to the optimal solution of space-bounded loading. Note that storage utilization of R-trees generated by query-optimal loading largely depends on the underlying query profile. If the query size is large, the optimal partitioning also causes high storage utilization.
In the following, we discuss useful guidelines for processing a large set of rectangles. Because computing opt  X  requires quadratic space, it is unlikely that the whole intermediate data sets can be processed in memory. Thus, we cut the data in sufficiently small equi-sized chunks and apply opt on each of them independently. In our experiments, we ob-served that B 2 (where B is the page capacity) is sufficient to obtain near-optimal results. In this case opt  X  needs ( B 2 ) 2 space for chunk processing. For the computation of gopt  X  the same strategy can be applied. However, since only the last B entires are required by gopt  X  , a buffer of B entries is sufficient.

After the first level has been constructed, the index entries of the next level could be re-sorted again. However, we noticed that for a given query profile, the produced sequence of MBRs already preserves the order of the input rectangles so that we skip the extra sorting step to reduce the total build-up time.
In this section, we compare different sort-based loading al-gorithms in a set of experiments and show the improvements of our query-adaptive technique. We first briefly describe data files and query sets. Then, we present improvements achieved by our algorithms. Detailed results can be found in [2].
In our experiments, we used an adaptation of the test framework developed for the evaluation of the RR*-tree [6]. The framework consists of 28 different data sets, either points or rectangles. They belong to eight groups abs, bit, dia, par, ped, pha, uni, rea . Each of the first seven groups contain three artificially generated data sets with 2,3, and 9-dimensional data following the same distribution in ev-ery dimension. Each of the artificial data sets contains at least 1 million objects from [0 , 1) d . For example, the group uni consists of 3 files of 1  X  000  X  000 two-, three-and nine-dimensional uniformly distributed points. The eighth group consist of seven real data sets with 2,3,5,9,16,22, and 26 di-mensions. For example, the 2-dimensional rea data set con-sists of 1  X  888  X  012 bounding boxes of streets in California. A full description of data generation and sources is given in [6].
In the original test framework, three range-query sets qr 1, qr 2 and qr 3 were considered for each data set. Except for the group ped , the query sets were generated as follows: The queries of qr 1, qr 2 and qr 3 refer to square-shaped windows and deliver 1, 100 and 1000 results on average, respectively. All queries followed the underlying data distributions. Ac-cording to the query taxonomy [14], these query sets are of type W QM 4 (queries follow the underlying data distri-bution and query size is expressed in the number of an-swers). For group ped , queries were generated in a more traditional way. The square-shaped range of qr 1, qr 2 and qr 3 cover k/ 1  X  000  X  000 of the entire data space, k = 1, 100, 1000. In addition, ped queries were uniformly distributed (type W QM 1 ).
Table 1 provides a summary of all methods used in our experiments. As a reference method, we used the traditional sort-based loading termed Z-loading and H-loading using Z-ordering and H-ordering, respectively. Both of the loading techniques are parameterized with storage utilization set to 80%. Note that in our experiments, higher storage utiliza-tion did not improve the query performance. Z-GO stands for globally optimized partitioning technique applied to Z-ordered input, whereas H-GO is based on H-ordering. H-SO uses our storage-bounded partitioning with a storage utiliza-tion of 80%.

We also examined STR [11] and TGS [8] because of their popularity. Storage utilization was again set to 80%. In ad-dition, we also present an improved version of STR, termed STR-GO, which combines STR with our globally optimized partitioning method. STR-GO performs as STR for the first d  X  1 dimensions, but uses our partitioning technique for the last dimension. This is directly applicable because the data objects are distributed among the leaf pages regarding the d-th dimension.

All algorithms are implemented in Java. Experiments were conducted on a 64 bit Intel Core2Duo (2 x 3.33 Ghz) machine with 8 Gb memory running Windows 7. We used 4KB pages with a capacity B = 128 and minimum occupa-Shortcut Sorting Order Partitioning H Hilbet-Order naive Z-GO symmetric Z-Order gopt  X  ( i ) H-GO Hilbert-Order gopt  X  ( i ) H-SO Hilbert-Order opt  X  ( i, k ) STR not applicable naive STR-GO not applicable gopt  X  ( i ) TGS not applicable n/a Figure 1: Avg. number of leaf accesses per query for d=2. tion b = 42 for d=2. In our experiments we used chunks of size B 2 = 16384.
 Algorithm efficiency is measured by I/O and CPU time. We consider the number of leaves touched during query traversal as a default I/O metric. Note that we do not count repeated accesses to the same leaf. As confirmed in our ex-periments, this is indeed good performance indicator for the total runtime of a query.
This section discusses the improvements achieved by our partitioning strategies. In addition to the methods based on space-filling curves, we also report the results of TGS, STR and STR-GO.

Figure 1 depicts the I/O performance for eight 2-dimen-sional data sets and query files qr 1, qr 2 and qr 3. Note that all loading methods that use our partitioning strategies are superior to H-loading. Moreover, STR-GO performs better than its original counterpart. For TGS we observed simi-lar effects as reported in [8]. TGS performs well for point queries qr 1, but its performance deteriorates with an in-creasing query region. It is noteworthy that there is no sig-nificant difference between H-GO and Z-GO except for dia , where Z-GO is clearly superior.

The most significant improvements over H-loading are achie-ved for point queries on the 2-dimensional data set ped . This Table 2: Avg. query performance of Z-GO and H-G O-loaded R-trees over square-shaped queries for different dimensionalities in leaf accesses (results are normalized to H-loaded R-trees). data set is the only for which the queries are uniformly dis-tributed. Note that this is in full agreement with the goal function used in our optimization. This also explains the large difference in performance between STR and STR-GO. We observed that the impact of the query size is marginal for storage bounded algorithms H-SO and Z-SO in compari-son to the H-GO and Z-GO counterparts. Thus, minimizing the area (which is only optimal for point queries) achieves already good results for all query profiles qr 1, qr 2 and qr 3. The query size influences the relative R-tree performance. This is not surprising, as for larger regions, the storage uti-lization will have greater impact (than the clustering capa-bility of the loading techniques). This is also in agreement with the analytical results obtained from the cost model. For example, R-trees generated from H-GO-loading perform small queries on the California data set ( rea ) with only 60% of the disk accesses compared to H-loaded R-trees. For queries qr 3 with 1000 results the performance difference is only 20%. We achieved similar results for the 3-dimensional data sets. H-loading is superior to STR-GO for only some of the data files, but inferior to Z-GO and H-GO in all cases. The average normalized results for two, three and nine di-mensions are reported in Table 2 (performance is expressed as the ratio of average number of leaf accesses for the spe-cific and the H-loaded R-tree). The results indicate slight improvements for higher dimensions.
 As expected, the number of leaf pages occupied by our R-trees generated from Z-GO and H-GO in relation to the number of leaves of H-loaded R-trees is higher for small queries. For larger queries, it is typically below 100%, i.e., the storage utilization is higher than 80% for the R-trees generated by Z-GO and H-GO.
In this paper, we reconsidered the problem of sort-based bulk-loading of R-trees. We designed new loading algo-rithms based on two innovative techniques. The first gen-erates an optimal partitioning for a given sequence of rect-angles, while the second consists of a new sorting technique based on non-symmetric Z-order. Both techniques are opti-mized according to a commonly used cost model for range queries. Their conceptual simplicity allows easy integration into modern database systems. We have already imple-mented our loading algorithms in the open-source NoSQL database system CouchDB [1]. Our algorithm enhance the spatial functionality of GeoCouch, the geo-plugin of CouchDB.
We would like to thank Anne Sophie Kn  X  oller for insightful feedback and reviewing this piece of work. [1] Couchdb. http://couchdb.apache.org. [2] D. Achakeev, B. Seeger, and P. Widmayer. Sort-based [3] S. Acharya, V. Poosala, and S. Ramaswamy.
 [4] L. Arge, M. D. Berg, H. Haverkort, and K. Yi. The [5] L. Arge, K. Hinrichs, J. Vahrenhold, and J. S. Vitter. [6] N. Beckmann and B. Seeger. A revised r*-tree in [7] J. V. d. Bercken, B. Seeger, and P. Widmayer. A [8] Y. J. Garc  X  X a R, M. A. L  X opez, and S. T. Leutenegger. A [9] H. V. Jagadish, N. Koudas, S. Muthukrishnan, [10] I. Kamel and C. Faloutsos. On packing r-trees. In [11] S. Leutenegger, M. A. Lopez, and J. Edgington. Str: [12] D. Lichtenstein. Planar formulae and their uses. SIAM [13] J. A. Orenstein and T. H. Merrett. A class of data [14] B.-U. Pagel, H.-W. Six, H. Toben, and P. Widmayer. [15] V. Poosala and Y. E. Ioannidis. Selectivity estimation [16] N. Roussopoulos and D. Leifker. Direct spatial search [17] Y. Theodoridis and T. Sellis. A model for the [18] K. Yi, X. Lian, F. Li, and L. Chen. The world in a
