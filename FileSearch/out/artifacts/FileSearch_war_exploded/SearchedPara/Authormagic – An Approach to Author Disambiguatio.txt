 A collaboration of leading research centers in the field of High Energy Physics (HEP) has built INSPIRE, a novel information infrastructure, which comprises the entire cor-pus of about one million documents produced within the discipline, including a rich set of metadata, citation infor-mation and half a million full-text documents, and offers a unique opportunity for author disambiguation strategies. The presented approach features extended metadata com-parison metrics and a three-step unsupervised graph cluster-ing technique. The algorithm aided in identifying 200 X 000 individuals from 6 X 500 X 000 author signatures. Preliminary tests based on knowledge of external experts and a pilot of a crowd-sourcing system show a success rate of more than 96% within the selected test cases. The obtained author clusters serve as a recommendation for INSPIRE users to further clean the publication list in a crowd-sourced approach. H.3.3 [ Information Search and Retrieval ]: Clustering; I.2.6 [ Learning ]: Knowledge acquisition Algorithms, Human Factors Author disambiguation, entity resolution, digital libraries, knowledge management
For decades, knowledge about the attribution of scholarly artifacts to individuals, in the frequent cases of name am-biguity, has been with the authors X  peers or knowledgeable  X  Supported by the Wolfgang-Gentner-Programme of the Bundesministerium f  X  ur Bildung und Forschung (BMBF) experts in libraries and has since been a known and sys-tematic problem for repositories of all kinds and sizes. The digitization of scholarly communication, public interest in science, the international expansion of the scientific commu-nity and other factors constitute to a substantial growth of the information environment in both quantity and complex-ity. In turn, these advances make unambiguous attribution of scholarly artifacts an even harder task, while making it all the more valuable for the discovery and retrieval of informa-tion in the environment of scholarly communication and the increased market for evaluation of scientific productivity.
A collaboration of leading research centers in the field of High-Energy Physics (HEP) has developed INSPIRE 1 ,a complete digital library of all HEP publications. INSPIRE follows in the steps of the HEP tradition of community-based scientific information infrastructures [8] and offers unique opportunities for author disambiguation. The data set com-prises the entire corpus of documents produced within the discipline including a comprehensive citation database.
Data quality is becoming ever more important and auto-mated systems aid human experts in increasing the overall quality of metadata in digital libraries. Novel and dynamic approaches to identify metadata similarities are required to make better decisions in the attribution of scholarly arti-facts. Author disambiguation is a long-standing challenge with various approaches in different disciplines [5, 6, 7, 9, 10, 11]. The algorithm described in this document has been designed and implemented to identify which individual re-searcher wrote which particular documents when, where, with whom and about what. The algorithm determines metadata similarities of an author X  X  name, affiliation, coau-thors and citations between all author signatures (author names along with describing metadata on documents). Un-supervised clustering of this related-authors-graph results in potential author clusters.

The final clustering result is used as a recommendation in a crowd-sourcing approach to author disambiguation, which empowers users of INSPIRE to participate in the task of keeping publication and citation lists as up-to-date as pos-sible. Precise knowledge of authorship yields the design of unexampled services in information discovery and knowledge extraction (e.g. extended author-based analyses of citation or co-authorship graphs). It also enables meaningful author-centric scientometric information and removes uncertainties from attempts to use bibliographic and citation data to eval-uate scientific quality of an individual. http://inspirehep.net
Recent years have shown advances in automated and col-laborative approaches to author disambiguation. Some so-lutions involve manual assignments of author data by li-brarians and collaborative approaches, e.g.  X  X CM Author Pages X  2 ,  X  X he Names Project X  3 or, as one of the most promis-ing collaborative approaches involving research institutes, universities and publishers alike, ORCID 4 .

Name ambiguity is a special instance of identity uncer-tainty and several approaches aim for an automated solution to the challenge [11]. Computer-aided approaches employ unsupervised clustering strategies [3, 9, 10] and supervised techniques [4, 7] or a hybrid of those [6].

Supervised strategies to author disambiguation require training data to allow for a compilation of the set of docu-ments an author produced [4, 11] or to predict the author-ship of a document [7]. Training sets are tedious to maintain and mostly do not cover all the cases needed to perform au-thor disambiguation [3].

Different similarity measures have been proposed in vari-ous author clustering scenarios to compare metadata of doc-uments [3, 10, 11, 9]. The selection of metadata similarity measures is mostly limited to the comparison of either name strings, coauthors or citations mentioned on a document, while disregarding the usefulness of combining various pieces of information.

The ideas closest to this work are probably [10, 3] due to their approach to solve name ambiguity though graph manipulations. The presented approach is different in such that the graph clustering strategy uses another approach and that the overall comparison strategy is focused on a broader selection of metadata.
As a preparation, potentially ambiguous name data is uni-fied for better comparability. This is accomplished by split-ting each name in several parts: last name(s) , initial(s) and first name(s) . The identification of the last name in a name string is a challenge in itself [2]. The following rules are applied to identify the last name in a name string: 1) The last name is the first part of a comma-separation or 2) the last name is spelled in all caps or 3) if no other rule applies, the last substring of a name string is used to denote the last name.

In a first step, author signatures are partitioned into groups of similar last names to reduce the total number of com-parisons. A regular expression aids in finding commonly misspelled names as well as compound names with different compound characters, e.g. t X  X ooft vs. thooft vs.  X  X  Hooft or Ruiz-Perez vs. Ruiz Perez .

The second step comprises of pair-wise comparisons of all author signatures within a last name group. These compar-isons allow the identification of potential authors through transitive graph clustering as explained in section 3.1.
The third and last step performs comparisons of individual author signatures that have not been connected in step 2, with the aggregated information of a potential author entity as explained in section 3.2. http://plone.acm.org/membership/author pages http://names.mimas.ac.uk http://orcid.org Algorithm 1 Authormagic algorithm pseudo code Ensure: All names are unified. { cf. section 3 } for all lastnames from the dataset as lname do end for
To identify a potential author PA , it is necessary to com-pute the similarity between all author signatures AS within their respective last name groups. Similarity metrics are ap-plied on the following metadata attached to each AS :com-mon co-authors, affiliations, names, publication dates and keywords. It is also assessed if two AS appear on one doc-ument, which yields absolute inequality. These metrics are then combined to a total result p pair ( AS 1 ,AS 2 ) following formula 1. n p is the count of metrics of total inequality f , while n s is the count of metrics f s with the metrics X  respective weight w i . n + describes the number of metrics that resulted in a probability &gt; 0 (This is done to address possible metadata deficiencies). All weights of the similarity functions have been determined by librarians according to the importance of each function in the manual disambigua-tion process.
The pair-wise comparisons will form a fully-connected graph between all AS s. The edges describe the similarity between the two AS s. with the compatibility between each node as a weighted edge. To identify a PA , the transitive sub-tree around each node is formed. A minimum similarity &gt;. 78 is required to be part of the transitive sub-tree. The threshold has been chosen to be a constant slightly above the average distance of all author signatures from a hand-labeled test set to ensure a high cluster quality.
Upon completion of the previous step (cf. section 3.1), single AS nodes may remain, which are then compared with all the PA s.
 An average of the similarity values of the single node AS and all AS sina PA cluster is computed first using the cached results of the pair-wise comparisons. The sec-ond computation compares the metadata information of the AS to the aggregated metadata information of all AS nodes within the PA cluster. Metrics for aggregated information include comparisons of frequent co-authors and most fre-quent citations of all AS in a PA .

Formula 2 describes the computation.  X  will set the en-Figure 1: A clustered graph in matrix representa-tion. The dark and light gray parts form the two transitive clusters { 0 , 1 , 6 , 7 } and { 2 , 3 , 5 } , while node 4 is not connected to a cluster. The thresholds d =0 . 8 and  X  =0 . 5 were used for this simplified example. tire equation to zero if absolute inequalities are detected in the pair-wise comparisons. n a indicates the total number of metrics for aggregated information computations f a i along with the respective weights w i and n + , the count of metric results that returned a similarity of &gt; 0. Formula 2 results in p c in the range [0 .. 1].

A second threshold,  X  =0 . 59 is introduced to regulate the connection of any AS to a PA cluster.  X  has been chosen slightly beneath the result of a comparison that only features the name comparison without support from other metrics to prevent under-clustering of initial-only names. A connection happens iff p c  X   X  and only when all comparisons of all single nodes with all PA s have finished. Nodes that fit either multiple or none of the clusters are left disconnected to form their own PA .
 Figure 1 shows a clustering based on simplified thresholds. gether in the described two steps. Node { 4 } remains discon-nected and will form a potential author by itself.  X   X  1
This section presents results that were obtained from us-ing the algorithm on a real-world set of scientific documents in HEP. The accuracy of the algorithm and the impact of additional metrics on the computation shall be shown on the example of a hand-labeled subset.
The total data set comprises of 6 X 380 X 821 author signa-tures on 892 X 318 documents with a total of 407 X 626 differ-ent name strings. Each author signature holds information about an author X  X  name, affiliation, co-authors, outgoing ci-tations, the date of publication and the ID of the document the author is listed on.

A subset of these documents has been hand-labeled by librarians of the INSPIRE collaboration. This subset of 15 X 744 documents will be referred to as control set C .It holds hand-checked data of 127 distinct authors with 611 name variations across 103 last names with 24 authors shar-ing a last name.
 Table 1 shows the selected test groups alongside their number of hand-tagged documents and the number of known authors in the control set C . NOTE that the number of au-thors only represents the hand-labeled authors to perform the evaluation on; each name group includes far more than three authors to be disambiguated by the algorithm to form the reseult set R . The F1 metric featuring precision and recall measures is used for the cluster evaluation. Due to the nature of the al-gorithm in using aggregated information of clusters, it is not feasible to use a strictly pair-wise F1 measure. To counter-act this limitation, the cluster with the highest number of true positives is chosen as a reference for the computation of precision and recall as it is defined in [1]. The F1 measure is calculated for each cluster in R in accordance to C using An experimental cluster evaluation method is proposed as an external criteria for the evaluation of cluster quality. The proposed C-Measure is a contextualized metric for the area of document attribution tasks, where the result set might be split in distributed clusters.

Let each AS on each paper be identified by a unique in-teger number n ;each AS can thus be represented by it X  X  own unique string pn . If the set of all available signatures in
R is called S : S = {  X  pn  X  | X  signature n } the definition of the control set C follows straightforward as a set of sets, each containing elements from S ; each labeled as  X  C x  X  X ith x  X  ( a | b | ... | z )[ a, b  X  z ]  X  . Formally, C = {K as C with C x C z =  X  for each x = z . In a similar way, the results set R can be defined as R = {K as R k |K X  S } . Empty intersections between all the subsets can be requested but is not a strict requirement.

The idea is to define the correctness of a clustering R with respect to a control set C in terms of average distance between the two. The total distance is computed averaging the distances between each set C k and all the respective sets R , for it may happen that elements of C k are spread among more than one set in R .
 Summarizing: for each C k find all R s in such that C k  X  R s =  X  ;foreach R s compute a distance E ( C k ,R s )and average in E ( C k ), which will then be used to compute the average total distance.

The average error is defined as:  X  E C = C k  X  X  E ( C k ) / in which E ( C k ) is defined as the average error for that clus-ter. In particular: E ( C k )= R s  X  X  C where K C k is the set of overlapping clusters. Formally de-fined: K C k = { R s | R s  X  C k =  X  X 
Thedistancemetric E ( C k ,R s ) is defined as the euclidean distance between two vectors. For each couple ( C k ,R s from A ks = C k  X  R s a n -dimensional space P n with n = is defined, thus each element of A ks may be located on a different dimension of P n . It is now possible to define the vectors v,w  X  X  n such that v i =1iff i  X  C k and w i =1iff i  X  R s ,for i  X  A ks . The distance E ( C k ,R s is then simply defined as the standard euclidean distance: E ( C k ,R s )= v  X  w 2
To measure the impact of the comparison methods that compare single author signatures with the aggregated in-formation in a potential author cluster, the results are di-vided into two scenarios; executions of the algorithm with and without the additional metrics. The evaluation metrics described in section 4.2 are used to measure the respective accuracy of each run. The algorithm has been executed 10 times with a randomly shuffle d processing queue for each scenario to determine a bias towards the order of computa-tion. The results of all runs were invariant.

Table 2 shows the results of the test runs. With Q = 1  X   X  E C , it can be defined that F  X  and Q  X  denote the re-sults from runs without the metrics for aggregated informa-tion. F  X  and Q  X  denote the runs that did use the metrics for aggregated information. The results obtained from the cluster quality measures F and Q in table 2 show a slight in-crease in clustering accuracy while employing the additional metrics and suggest further exploration in the direction of mining aggregated information for the purpose of author disambiguation. Preliminary results from a pilot project, further described in section 5, indicate that more than 96% of all documents the users claimed ownership of were rec-ommended correctly.
Since each information system, digital library or digital repository features a different set of complex requirements, curation strategies and data quality management ideas, there is no universal response to the challenge of name ambiguity. The increasing amount of metadata about digital objects offers new possibilities in disambiguation strategies.
The 200 X 000 author clusters, obtained from the corpus of documents in HEP using the discussed approach, are cur-rently used to pre-populate author profiles within INSPIRE. The user and owner of the profile is then presented with a set of tools to actively participate in the author data cura-tion process by claiming or repealing authorship of a given document. Actions that users have taken in attributing doc-uments to themselves or other researchers will have an effect on the algorithm: The information about authorship attri-bution supplied by the user feeds back into the algorithm and serves as an unquestionable reference point for future author disambiguation tasks making it a deterministic, yet evolu-tionary approach in the long-term. This data also allows for more comprehensive evaluations of the matching accuracy of the Authormagic algorithm to further tweak the metrics towards an improved matching quality.

Although a purely algorithmic approach to solve uncer-tainties in authorship attribution may never reach 100% ac-curacy, it is a great step towards a high-quality data set that enables novel user-and author-centric services. With a little help of the users to clean the last nuggets of precious infor-mation, extended scientometric and meaningful bibliometric evaluations in scholarly communication will emerge. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] W. Cohen, P. Ravikumar, and S. Fienberg. A [3] R. G. Cota, A. A. Ferreira, C. Nascimento, M. A. [4] A. Culotta, P. Kanani, R. Hall, M. Wick, and [5] A. Dai and A. Storkey. Author disambiguation: a [6] A. A. Ferreira, A. Veloso, M. A. Gon  X  calves, and A. H. [7] H. Han, L. Giles, H. Zha, C. Li, and [8] R.-D. Heuer, A. Holtkamp, and S. Mele. Innovation in [9] I. Kang, S. Na, S. Lee, H. Jung, P. Kim, W. Sung, and [10] B.-W. On, E. Elmacioglu, D. Lee, J. Kang, and J. Pei. [11] V. Torvik and N. Smalheiser. Author name
