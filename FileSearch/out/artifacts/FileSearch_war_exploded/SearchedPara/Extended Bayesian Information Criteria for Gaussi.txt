 This paper is concerned with the problem of model selection (or structure learning) in Gaussian graphical modelling. A Gaussian graphical model for a random vector X = ( X 1 ,...,X p ) is de-termined by a graph G on p nodes. The model comprises all multivariate normal distributions For background on these models, including a discussion of the conditional independence interpreta-tion of the graph, we refer the reader to [1].
 In many applications, in particular in the analysis of gene expression data, inference of the graph G is of significant interest. Information criteria provide an important tool for this problem. They provide the objective to be minimized in (heuristic) searches over the space of graphs and are sometimes used to select tuning parameters in other methods such as the graphical lasso of [2]. In this work we study an extended Bayesian information criterion (BIC) for Gaussian graphical models. Given a sample of n independent and identically distributed observations, this criterion takes the form where E is the edge set of a candidate graph and l n (  X   X ( E )) denotes the maximized log-likelihood function of the associated model. (In this context an edge set comprises unordered pairs { j,k } of well known to lead to (asymptotically) consistent model selection in the setting of fixed number of variables p and growing sample size n . Consistency is understood to mean selection of the smallest true graph whose edge set we denote E 0 . Positive  X  leads to stronger penalization of large graphs and our main result states that the (asymptotic) consistency of an exhaustive search over a restricted model space may then also hold in a scenario where p grows moderately with n (see the Main Theorem in Section 2). Our numerical work demonstrates that positive values of  X  indeed lead to improved graph inference when p and n are of comparable size (Section 3).
 The choice of the criterion in (1) is in analogy to a similar criterion for regression models that was first proposed in [5] and theoretically studied in [3, 6]. Our theoretical study employs ideas from these latter two papers as well as distribution theory available for decomposable graphical models. As mentioned above, we treat an exhaustive search over a restricted model space that contains all decomposable models given by an edge set of cardinality | E | X  q . One difference to the regression true model. This is necessary for connected graphs to be covered by our work.
 In practice, an exhaustive search is infeasible even for moderate values of p and q . Therefore, we must choose some method for preselecting a smaller set of models, each of which is then scored by applying the extended BIC (EBIC). Our simulations show that the combination of EBIC and graphical lasso gives good results well beyond the realm of the assumptions made in our theoretical analysis. This combination is consistent in settings where both the lasso and the exhaustive search are consistent but in light of the good theoretical properties of lasso procedures (see [7]), studying this particular combination in itself would be an interesting topic for future work. 2.1 Notation and definitions In the sequel we make no distinction between the edge set E of a graph on p nodes and the asso-ciated Gaussian graphical model. Without loss of generality we assume a zero mean vector for all for the diagonal entries of the matrix.
 Let  X  0 be a positive definite matrix supported on  X   X  E 0 . In other words, the non-zero entries of  X  0 are precisely the diagonal entries as well as the off-diagonal positions indexed by E 0 ; note that a single edge in E 0 corresponds to two positions in the matrix due to symmetry. Suppose the random vectors X 1 ,...,X n are independent and distributed identically according to N (0 ,  X   X  1 0 ) . Let S = 1 n P i X i X T i be the sample covariance matrix. The Gaussian log-likelihood function simplifies to We introduce some further notation. First, we define the maximum variance of the individual nodes: Next, we define  X  0 = min e  X  E 0 | ( X  0 ) e | , the minimum signal over the edges present in the graph. (For edge e = { j,k } , let ( X  0 ) e = ( X  0 ) jk = ( X  0 ) kj .) Finally, we write  X  max for the maximum eigenvalue of  X  0 . Observe that the product  X  2 max  X  max is no larger than the condition number of  X  0 2.2 Main result Suppose that n tends to infinity with the following asymptotic assumptions on data and model: Here C, X  &gt; 0 and  X  are fixed reals, while the integers p,q , the edge set E 0 , the matrix  X  0 , and dependence on n in the notation. The  X  X ig oh X  O (  X  ) and the  X  X mall oh X  o (  X  ) are the Landau symbols. Main Theorem. Suppose that conditions (3) hold. Let E be the set of all decomposable models E with | E | X  q . Then with probability tending to 1 as n  X  X  X  , That is, the extended BIC with parameter  X  selects the smallest true model E 0 when applied to any subset of E containing E 0 .
 In order to prove this theorem we use two techniques for comparing likelihoods of different mod-els. Firstly, in Chen and Chen X  X  work on the GLM case [6], the Taylor approximation to the log-likelihood function is used and we will proceed similarly when comparing the smallest true model E 0 to models E which do not contain E 0 . The technique produces a lower bound on the decrease in likelihood when the true model is replaced by a false model.
 Theorem 1. Suppose that conditions (3) hold. Let E 1 be the set of models E with E 6 X  E 0 and | E | X  q . Then with probability tending to 1 as n  X  X  X  , Secondly, Porteous [8] shows that in the case of two nested models which are both decomposable, the likelihood ratio (at the maximum likelihood estimates) follows a distribution that can be ex-pressed exactly as a log product of Beta distributions. We will use this to address the comparison between the model E 0 and decomposable models E containing E 0 and obtain an upper bound on the improvement in likelihood when the true model is expanded to a larger decomposable model. Theorem 2. Suppose that conditions (3) hold. Let E 0 be the set of decomposable models E with E  X  E 0 and | E | X  q . Then with probability tending to 1 as n  X  X  X  , Proof of the Main Theorem. With probability tending to 1 as n  X   X  , both of the conclusions of Theorems 1 and 2 hold. We will show that both conclusions holding simultaneously implies the desired result.
 Observe that E  X  X  0  X  X  1 . Choose any E  X  X \{ E 0 } . If E  X  X  0 , then (by Theorem 2): If instead E  X  X  1 , then (by Theorem 1, since | E 0 | X  q ): Therefore, for any E  X  X \{ E 0 } , BIC  X  ( E ) &gt; BIC  X  ( E 0 ) , which yields the desired result. Some details on the proofs of Theorems 1 and 2 are given in the Appendix in Section 5. In this section, we demonstrate that the EBIC with positive  X  indeed leads to better model selection properties in practically relevant settings. We let n grow, set p  X  n  X  for various values of  X  , and mentioned in the introduction, we first use the graphical lasso of [2] (as implemented in the  X  X lasso X  package for R) to define a small set of models to consider (details given below). From the selected set we choose the model with the lowest EBIC. This is repeated for 100 trials for each combination of values of n,p, X  in each scaling scenario. For each case, the average positive selection rate (PSR) and false discovery rate (FDR) are computed.
 We recall that the graphical lasso places an ` 1 penalty on the inverse covariance matrix. Given a penalty  X   X  0 , we obtain the estimate (Here we may define k  X  k 1 as the sum of absolute values of all entries, or only of off-diagonal en-tries; both variants are common). The ` 1 penalty promotes zeros in the estimated inverse covariance of models recovered over the full range of penalties  X   X  [0 ,  X  ) , gives a small set of models which, roughly, include the  X  X est X  models at various levels of sparsity. We may therefore apply the EBIC to this manageably small set of models (without further restriction to decomposable models). Consis-tency results on the graphical lasso require the penalty  X  to satisfy bounds that involve measures of regularity in the unknown matrix  X  0 ; see [7]. Minimizing the EBIC can be viewed as a data-driven method of tuning  X  , one that does not require creation of test data.
 While cross-validation does not generally have consistency properties for model selection (see [9]), it is nevertheless interesting to compare our method to cross-validation. For the considered simulated data, we start with the set of models from the  X  X lasso path X , as before, and then perform 100-fold cross-validation. For each model and each choice of training set and test set, we fit the model to the training set and then evaluate its performance on each sample in the test set, by measuring error in predicting each individual node conditional on the other nodes and then taking the sum of the squared errors. We note that this method is computationally much more intensive than the BIC or EBIC, because models need to be fitted many more times. 3.1 Design In our simulations, we examine the EBIC as applied to the case where the graph is a chain with node j being connected to nodes j  X  1 ,j +1 , and to the  X  X ouble chain X , where node j is connected to nodes j  X  2 ,j  X  1 ,j + 1 ,j + 2 . Figure 1 shows examples of the two types of graphs, which have on the order of p and 2 p edges, respectively. For both the chain and the double chain, we investigate four we test n = 100 , 200 , 400 , 800 , and define p  X  n  X  with the constant of proportionality chosen such that p = 10 when n = 100 for better comparability.
 In the case of a chain, the true inverse covariance matrix  X  0 is tridiagonal with all diagonal entries ( X  ( X  uniformly in the matrix size p .
 For each data set generated from N (0 ,  X   X  1 0 ) , we use the  X  X lasso X  package [2] in R to compute the  X  X lasso path X . We choose 100 penalty values  X  which are logarithmically evenly spaced between  X  max (the smallest value which will result in a no-edge model) and  X  max / 100 . At each penalty value  X  , we compute  X   X   X  from (4) and define the model E  X  based on this estimate X  X  support. The R routine also allows us to compute the unpenalized maximum likelihood estimate  X   X ( E  X  ) . We may then readily compute the EBIC from (1). There is no guarantee that this procedure will find the model with the lowest EBIC along the full  X  X lasso path X , let alone among the space of all possible models of size  X  q . Nonetheless, it serves as a fast way to select a model without any manual tuning. 3.2 Results Chain graph: The results for the chain graph are displayed in Figure 2. The figure shows the positive selection rate (PSR) and false discovery rate (FDR) in the four scaling scenarios. We observe that, three values of  X  ; however, the FDR rate is noticeably better for the positive values of  X  , especially for higher scaling exponents  X  . Therefore, for moderately large n , the EBIC with  X  = 0 . 5 or  X  = 1 performs very well, while the ordinary BIC 0 produces a non-trivial amount of false positives. For 100-fold cross-validation, while the PSR is initially slightly higher, the growing FDR demonstrates the extreme inconsistency of this method in the given setting.
 Double chain graph: The results for the double chain graph are displayed in Figure 3. In each of the four scaling scenarios for this case, we see a noticeable decline in the PSR as  X  increases. Nonetheless, for each value of  X  , the PSR increases as n and p grow. Furthermore, the FDR for the ordinary BIC 0 is again noticeably higher than for the positive values of  X  , and in the scaling scenar-ios  X   X  0 . 9 , the FDR for BIC 0 is actually increasing as n and p grow, suggesting that asymptotic consistency may not hold in these cases, as is supported by our theoretical results. 100-fold cross-validation shows significantly better PSR than the BIC and EBIC methods, but the FDR is again extremely high and increases quickly as the model grows, which shows the unreliability of cross-validation in this setting. Similarly to what Chen and Chen [3] conclude for the regression case, it appears that the EBIC with parameter  X  = 0 . 5 performs well. Although the PSR is necessarily lower than with  X  = 0 , the FDR is quite low and decreasing as n and p grow, as desired. (non-asymptotic) setting. For low values of  X  , we are more likely to obtain a good (high) positive selection rate. For higher values of  X  , we are more likely to obtain a good (low) false discovery rate. (In the Appendix, this corresponds to assumptions (5) and (6)). However, asymptotically, the conditions (3) guarantee consistency, meaning that the trade-off becomes irrelevant for large n and p . In the finite case,  X  = 0 . 5 seems to be a good compromise in simulations, but the question of determining the best value of  X  in general settings is an open question. Nonetheless, this method offers guaranteed asymptotic consistency for (known) values of  X  depending only on n and p . We have proposed the use of an extended Bayesian information criterion for multivariate data gener-ated by sparse graphical models. Our main result gives a specific scaling for the number of variables p , the sample size n , the bound on the number of edges q , and other technical quantities relating to the true model, which will ensure asymptotic consistency. Our simulation study demonstrates the the practical potential of the extended BIC, particularly as a way to tune the graphical lasso. The results show that the extended BIC with positive  X  gives strong improvement in false discovery rate over the classical BIC, and even more so over cross-validation, while showing comparable positive increasing, positive selection rate for the double chain with a large number of weaker signals. We now sketch proofs of non-asymptotic versions of Theorems 1 and 2, which are formulated as Theorems 3 and 4. We also give a non-asymptotic formulation of the Main Theorem; see Theorem 5. In the non-asymptotic approach, we treat all quantities as fixed (e.g. n,p,q , etc.) and state precise assumptions on those quantities, and then give an explicit lower bound on the probability of the extended BIC recovering the model E 0 exactly. We do this to give an intuition for the magnitude of the sample size n necessary for a good chance of exact recovery in a given setting but due to the proof techniques, the resulting implications about sample size are extremely conservative. 5.1 Preliminaries We begin by stating two lemmas that are used in the proof of the main result, but are also more generally interesting as tools for precise bounds on Gaussian and chi-square distributions. First, Cai [10, Lemma 4] proves the following chi-square bound. For any n  X  1 , X  &gt; 0 , We can give an analagous left-tail upper bound. The proof is similar to Cai X  X  proof and omitted here. We will refer to these two bounds together as (CSB). Lemma 1. For any  X  &gt; 0 , for n such that n  X  4  X   X  2 + 1 , Second, we give a distributional result about the sample correlation when sampling from a bivariate normal distribution.
 Lemma 2. Suppose ( X 1 ,Y 1 ) ,..., ( X n ,Y n ) are independent draws from a bivariate normal distri-bution with zero mean, variances equal to one and covariance  X  . Then the following distributional equivalence holds, where A and B are independent  X  2 n variables: Proof. Let A 1 ,B 1 ,A 2 ,B 2 ,...,A n ,B n be independent standard normal random variables. Define: Then the variables X 1 ,Y 1 ,X 2 ,Y 2 ,...,X n ,Y n have the desired joint distribution, and A,B are in-dependent  X  2 n variables. The claim follows from writing P i X i Y i in terms of A and B . 5.2 Non-asymptotic versions of the theorems We assume the following two conditions, where 0 , 1 &gt; 0 , C  X   X  2 max  X  max ,  X  = log n p , and  X  0 =  X   X  (1  X  1 4  X  ) : Theorem 3. Suppose assumption (5) holds. Then with probability at least 1  X  1  X  E 6 X  E 0 with | E | X  q , centered at the true  X  0 to approximate the likelihood at  X   X ( E ) . The score and the negative Hessian of the log-likelihood function in (2) are Here, the symbol  X  denotes the Kronecker product of matrices. Note that, while we require  X  to be symmetric positive definite, this is not reflected in the derivatives above. We adopt this convention for the notational convenience in the sequel. bound |  X   X ( E )  X   X  0 | F  X   X  0 in terms of the Frobenius norm. By concavity of the log-likelihood function, it suffices to show that the desired inequality holds for all  X  with support on  X   X  E 0  X  E with |  X   X   X  0 | F =  X  0 . By Taylor expansion, for some  X   X  on the path from  X  0 to  X  , we have: Next, by (CSB) and Lemma 2, with probability at least 1  X  1  X   X  log p e  X  1 log p , the following bound holds for all edges e in the complete graph (we omit the details): Now assume that this bound holds for all edges. Fix some E as above, and fix  X  with support on  X   X  E 0  X  E , with |  X   X   X  0 | =  X  0 . Note that the support has at most ( p + 2 q ) entries. Therefore, Furthermore, the eigenvalues of  X  are bounded by  X  max +  X  0  X  2  X  max , and so by properties of Combining this bound with our assumptions above, we obtain the desired result.
 Theorem 4. Suppose additionally that assumption (6) holds (in particular, this implies that  X  &gt; that E ) E 0 and | E | X  q , l variables and the constants c 1 ,...,c m are bounded by 1 less than the maximal clique size of the graph given by model E , implying c i  X   X  log( B i )  X  1 Finally, combining the assumptions on n,p,q and the (CSB) inequalities, we obtain: Next, note that the number of models | E | with E  X  E 0 and | E | X  X  E 0 | = m is bounded by p 2 m . Taking the union bound over all choices of m and all choices of E with that given m , we obtain that the desired result holds with the desired probability.
 We are now ready to give a non-asymptotic version of the Main Theorem. For its proof apply the union bound to the statements in Theorems 3 and 4, as in the asymptotic proof given in section 2. Theorem 5. Suppose assumptions (5) and (6) hold. Let E be the set of subsets E of edges be-tween the p nodes, satisfying | E |  X  q and representing a decomposable model. Then it holds with That is, the extended BIC with parameter  X  selects the smallest true model.
 Finally, we note that translating the above to the asymptotic version of the result is simple. If the conditions (3) hold, then for sufficiently large n (and thus sufficiently large p ), assumptions (5) and (6) hold. Furthermore, although we may not have the exact equality  X  = log n p , we will have n . The proofs then follow from the non-asymptotic results. References [1] Steffen L. Lauritzen. Graphical models , volume 17 of Oxford Statistical Science Series . The [2] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation [3] Jiahua Chen and Zehua Chen. Extended Bayesian information criterion for model selection [4] Gideon Schwarz. Estimating the dimension of a model. Ann. Statist. , 6(2):461 X 464, 1978. [5] Malgorzata Bogdan, Jayanta K. Ghosh, and R. W. Doerge. Modifying the Schwarz Bayesian [6] Jiahua Chen and Zehua Chen. Extended BIC for small-n -large-p sparse GLM. Preprint. [7] Pradeep Ravikumar, Martin J. Wainwright, Garvesh Raskutti, and Bin Yu. High-[9] Jun Shao. Linear model selection by cross-validation. J. Amer. Statist. Assoc. , 88(422):486 X  [10] T. Tony Cai. On block thresholding in wavelet regression: adaptivity, block size, and threshold [11] P. Svante Eriksen. Tests in covariance selection models. Scand. J. Statist. , 23(3):275 X 284,
