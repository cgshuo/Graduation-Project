 Thomas M  X  uller 1 Lemmatization is important for many NLP tasks, including parsing (Bj  X  orkelund et al., 2010; Seddah et al., 2010) and machine translation (Fraser et al., 2012). Lemmata are required whenever we want to map words to lexical resources and establish the relation between inflected forms, particularly crit-ical for morphologically rich languages to address the sparsity of unlemmatized forms. This strongly motivates work on language-independent token-based lemmatization, but until now there has been little work (Chrupa X a et al., 2008).

Many regular transformations can be described by simple replacement rules, but lemmatization of unknown words requires more than this. For instance the Spanish paradigms for verbs end-ing in ir and er share the same 3rd person plu-ral ending en ; this makes it hard to decide which of problems requires global features on the lemma. Global features of this kind were not supported by previous work (Dreyer et al., 2008; Chrupa X a, 2006; Toutanova and Cherry, 2009; Cotterell et al., 2014).

There is a strong mutual dependency between (i) lemmatization of a form in context and (ii) disambiguating its part-of-speech (POS) and mor-phological attributes. Attributes often disam-biguate the lemma of a form, which explains why many NLP systems (Manning et al., 2014; Padr  X  o and Stanilovsky, 2012) apply a pipeline approach of tagging followed by lemmatization. Conversely, knowing the lemma of a form is of-ten beneficial for tagging, for instance in the pres-ence of syncretism; e.g., since German plural noun phrases do not mark gender, it is important to know the lemma (singular form) to correctly tag gender on the noun.

We make the following contributions. (i) We present the first joint log-linear model of mor-phological analysis and lemmatization that oper-ates at the token level and is also able to lem-matize unknown forms; and release it as open-It is trainable on corpora annotated with gold stan-dard tags and lemmata. Unlike other work (e.g., (Smith et al., 2005)) it does not rely on morpho-logical dictionaries or analyzers. (ii) We describe a log-linear model for lemmatization that can eas-ily be incorporated into other models and supports arbitrary global features on the lemma. (iii) We set the new state of the art in token-based sta-tistical lemmatization on six languages (English, German, Czech, Hungarian, Latin and Spanish). (iv) We experimentally show that jointly model-ing morphological tags and lemmata is mutually beneficial and yields significant improvements in joint (tag+lemma) accuracy for four out of six lan-guages; e.g., Czech lemma errors are reduced by &gt; 37% and tag+lemma errors by &gt; 6%. Chrupa X a (2006) formalizes lemmatization as a classification task through the deterministic pre-extraction of edit operations transforming forms into lemmata. Our lemmatization model is in this vein, but allows the addition of external lexical in-formation, e.g., whether the candidate lemma is in a dictionary. Formally, lemmatization is a string-to-string transduction task. Given an alphabet  X  , it maps an inflected form w  X   X   X  to its lemma l  X   X   X  given its morphological attributes m . We model this process by a log-linear model: where f represents hand-crafted feature functions,  X  is a weight vector, and h w :  X   X   X  { 0 , 1 } deter-mines the support of the distribution, i.e., the set of candidates with non-zero probability.

Candidate selection. A proper choice of the support function h (  X  ) is crucial to the success of the model  X  too permissive a function and the computational cost will build up, too restrictive and the correct lemma may receive no probability mass. Following Chrupa X a (2008), we define h (  X  ) through a deterministic pre-extraction of edit trees . To extract an edit tree e for a pair form-lemma  X  w,l  X  , we first find the longest common substring (LCS) (Gusfield, 1997) between them and then re-cursively model the prefix and suffix pairs of the LCS. When no LCS can be found the string pair is represented as a substitution operation transform-ing the first string to the second. The resulting edit tree does not encode the LCSs but only the length of their prefixes and suffixes and the substitution nodes (cf. Figure 1); e.g., the same tree transforms worked into work and touched into touch .

As a preprocessing step, we extract all edit trees that can be used for more than one pair  X  w,l  X  . To generate the candidates of a word-form, we apply all edit trees and also add all lemmata this form was seen with in the training set (note that only a small subset of the edit trees is applicable for any given form because most require incompatible
Features. Our novel formalization lets us com-bine a wide variety of features that have been used in different previous models. All features are extracted given a form-lemma pair  X  w,l  X  created with an edit tree e .
 We use the following three edit tree features of Chrupa X a (2008). (i) The edit tree e . (ii) The pair  X  e,w  X  . This feature is crucial for the model to memorize irregular forms, e.g., the lemma of was is be . (iii) For each form affix (of maximum length 10): its conjunction with e . These features are use-ful in learning orthographic and phonological reg-ularities, e.g., the lemma of signalling is signal , not signall .
 We define the following alignment features. Similar to Toutanova and Cherry (2009) (TC), we define an alignment between w and l . Our align-ments can be read from an edit tree by aligning the characters in LCS nodes character by character and characters in substitution nodes block-wise. Thus the alignment of umgeschaut -umschauen is: u-u, m-m, ge-, s-s, c-c, h-h, a-a, u-u, t-en. Each alignment pair constitutes a feature in our model. These features allow the model to learn that the substitution t / en is likely in German. We also concatenate each alignment pair with its form and lemma character context (of up to length 6) to learn, e.g., that ge is often deleted after um .
We define two simple lemma features . (i) We use the lemma itself as a feature, allowing us to learn which lemmata are common in the language. (ii) Prefixes and suffixes of the lemma (of maxi-mum length 10). This feature allows us to learn that the typical endings of Spanish verbs are ir , er , ar .

We also use two dictionary features (on lem-mata): Whether l occurs &gt; 5 times in Wikipedia and whether it occurs in the dictionary A SPELL . 3 We use a similar feature for different capitaliza-tion variants of the lemma (lowercase, first letter uppercase, all uppercase, mixed). This differenti-ation is important for German, where nouns are capitalized and en is both a noun plural marker and a frequent verb ending. Ignoring capitaliza-tion would thus lead to confusion.

POS &amp; morphological attributes. For each fea-ture listed previously, we create a conjunction with We model the sequence of morphological tags us-ing M AR M O T (M  X  uller et al., 2013), a pruned higher-order CRF. This model avoids the exponen-tial runtime of higher-order models by employing a pruning strategy. Its feature set consists of stan-dard tagging features: the current word, its affixes and shape (capitalization, digits, hyphens) and the immediate lexical context. We combine lemmati-zation and higher-order CRF components in a tree-structured CRF. Given a sequence of forms w with lemmata l and morphological+POS tags m , we define a globally normalized model: p ( l , m | w )  X  where f and g are the features associated with lemma and tag cliques respectively and  X  and  X  are weight vectors. The graphical model is shown in Figure 2. We perform inference with belief propagation (Pearl, 1988) and estimate the pa-rameters with SGD (Tsuruoka et al., 2009). We greatly improved the results of the joint model by initializing it with the parameters of a pretrained tagging model. In functionality, our system resembles M ORFETTE (Chrupa X a et al., 2008), which generates lemma candidates by extracting edit operation sequences between lemmata and surface forms (Chrupa X a, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for mor-phological tagging and lemmatization, which are queried using a beam search decoder.
 M
ORFETTE . This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupa X a, 2008). Models similar to M ORFETTE include those of Bj  X  orkelund et al. (2010) and Ges-mundo and Samardzic (2012) and have also been Wicentowski (2002) similarly treats lemmatiza-tion as classification over a deterministically cho-sen candidate set, but uses distributional informa-tion extracted from large corpora as a key source of information.

Toutanova and Cherry (2009) X  X  joint morpho-logical analyzer predicts the set of possible lem-mata and coarse-grained POS for a word type . This is different from our problem of lemmatiza-tion and fine-grained morphological tagging of to-kens in context . Despite the superficial similarity of the two problems, direct comparison is not pos-sible . TC X  X  model is best thought of as inducing a tagging dictionary for OOV types, mapping them to a set of tag and lemma pairs, whereas L EM -MING is a token-level, context-based morphologi-cal tagger.

We do, however, use TC X  X  model of lemmati-zation, a string-to-string transduction model based on Jiampojamarn et al. (2008) (JCK), as a stand-alone baseline. Our tagging-in-context model is faced with higher complexity of learning and in-ference since it addresses a more difficult task; thus, while we could in principle use JCK as a re-placement for our candidate selection, the edit tree approach  X  which has high coverage at a low aver-age number of lemma candidates (cf. Section 5)  X  allows us to train and apply L EMMING efficiently.
Smith et al. (2005) proposed a log-linear model for the context-based disambiguation of a morpho-logical dictionary. This has the effect of joint tag-ging, morphological segmentation and lemmatiza-tion, but, critically, is limited to the entries in the morphological dictionary (without which the ap-proach cannot be used), causing problems of re-call. In contrast, L EMMING can analyze any word, including OOVs, and only requires the same train-ing corpus as a generic tagger (containing tags and lemmata), a resource that is available for many languages. Datasets. We present experiments on the joint task of lemmatization and tagging in six diverse languages: English, German, Czech, Hungarian, Latin and Spanish. We use the same data sets as in M  X  uller and Sch  X  utze (2015), but do not use the out-of-domain test sets. The English data is from the Penn Treebank (Marcus et al., 1993), Latin from PROIEL (Haug and J X hndal, 2008), Ger-man and Hungarian from SPMRL 2013 (Seddah et al., 2013), and Spanish and Czech from CoNLL 2009 (Haji  X  c et al., 2009). For German, Hungar-ian, Spanish and Czech we use the splits from the shared tasks; for English the split from SANCL (Petrov and McDonald, 2012); and for Latin a 8/1/1 split into train/dev/test . For all languages we limit our training data to the first 100,000 tokens. Dataset statistics can be found in Table A4 of the appendix. The lemma of Spanish se is set to be consistent.

Baselines. We compare our model to three baselines. (i) M ORFETTE (see Section 4). (ii) SIMPLE , a system that for each form-POS pair, re-turns the most frequent lemma in the training data or the form if the pair is unknown. (iii) JCK, our reimplementation of Jiampojamarn et al. (2008). Recall that JCK is TC X  X  lemmatization model and that the full TC model is a type-based model that cannot be applied to our task.

As JCK struggles to memorize irregulars, we only use it for unknown form-POS pairs and use SIMPLE otherwise. For aligning the training data we use the edit-tree-based alignment described in the feature section. We only use output alpha-bet symbols that are used for  X  5 form-lemma pairs and also add a special output symbol that indicates that the aligned input should simply be copied. We train the model using a structured av-eraged perceptron and stop after 10 training itera-tions. In preliminary experiments we found type-based training to outperform token-based training. This is understandable as we only apply our model to unseen form-POS pairs. The feature set is an exact reimplementation of (Jiampojamarn et al., 2008), it consists of input-output pairs and their character context in a window of 6.

Results. Our candidate selection strategy re-sults in an average number of lemma candidates between 7 (Hungarian) and 91 (Czech) and a cov-erage of the correct lemma on dev of &gt; 99.4 (ex-lines to L EMMING -P, a pipeline based on Sec-tion 2, that lemmatizes a word given a predicted tag and is trained using L-BFGS (Liu and No-cedal, 1989). We use the implementation of M AL -LET (McCallum, 2002). For these experiments we train all models on gold attributes and test on at-tributes predicted by M ORFETTE . M ORFETTE  X  X  lemmatizer can only be used with its own tags. We thus use M ORFETTE tags to have a uniform setup, which isolates the effects of the different taggers. Numbers for M AR M O T tags are in the appendix (Table A1). For the initial experiments, we only use POS and ignore additional morphological at-tributes. We use different feature sets to illustrate the utility of our templates.

The first model uses the edit tree features (edit-tree). Table 1 shows that this version of L EM -MING outperforms the baselines on half of the lan-guages. 7 In a second experiment we add the align-ment (+align) and lemma features (+lemma) and show that this consistently outperforms all base-lines and edittree. We then add the dictionary fea-ture (+dict). The resulting model outperforms all previous models and is significantly better than the ments show that LEMMING -P yields state-of-the-art results and that all our features are needed to obtain optimal performance. The improvements over the baselines are &gt; 1 for Czech and Latin and  X  .5 for German and Hungarian.

The last experiment also uses the additional morphological attributes predicted by M ORFETTE (+mrph). This leads to a drop in lemmatization performance in all languages except Spanish (En-glish has no additional attributes). However, pre-liminary experiments showed that correct mor-phological attributes would substantially improve lemmatization as they help in cases of ambigu-ity. As an example, number helps to lemmatize the singular German noun Raps  X  X anola X , which looks like the plural of Rap  X  X ap X . Numbers can be found in Table A3 of the appendix. This motivates the necessity of joint tagging and lemmatization .
For the final experiments, we run pipeline mod-els on tags predicted by M AR M O T (M  X  uller et al., 2013) and compare them to L EMMING -J, the joint model described in Section 3. All L EMMING versions use exactly the same features. Table 2 shows that L EMMING -J outperforms L EMMING -P in three measures (see bold tag, lemma &amp; joint (tag+lemma) accuracies) except for English, where we observe a tie in lemma accuracy and a small drop in tag and tag+lemma accuracy. Coupling morphological attributes and lemmatiza-tion (lines 8 X 10 vs 11 X 13) improves tag+lemma prediction for five languages. Improvements in lemma accuracy of the joint over the best pipeline systems range from .1 (Spanish), over &gt; .3 (Ger-man, Hungarian) to  X  .96 (Czech, Latin).

Lemma accuracy improvements for our best models (lines 4 X 13) over the best baseline (lines 2 X 3) are &gt; 1 (German, Spanish, Hungarian), &gt; 2 (Czech, Latin) and even more pronounced on un-known forms: &gt; 1 (English), &gt; 5 (German, Span-ish, Hungarian) and &gt; 12 (Czech, Latin). L
EMMING is a modular lemmatization model that supports arbitrary global lemma features and joint modeling of lemmata and morphological tags. It is trainable on corpora annotated with gold standard tags and lemmata, and does not rely on morpho-logical dictionaries or analyzers. We have shown that modeling lemmatization and tagging jointly benefits both tasks, and we set the new state of the art in token-based lemmatization on six languages. L
EMMING is available under an open-source li-We would like to thank the anonymous reviewers for their comments. The first author is a recipient of the Google Europe Fellowship in Natural Lan-guage Processing, and this research is supported by this Google fellowship. The second author is supported by a Fulbright fellowship awarded by the German-American Fulbright Commission and the National Science Foundation under Grant No. 1423276. This project has received funding from the European Union X  X  Horizon 2020 research and innovation programme under grant agreement No 644402 (HimL) and the DFG grant Models of Morphosyntax for Statistical Machine Transla-tion . The fourth author was partially supported by Deutsche Forschungsgemeinschaft (grant SCHU 2246/10-1).
