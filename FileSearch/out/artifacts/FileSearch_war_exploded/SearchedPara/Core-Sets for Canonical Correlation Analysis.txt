 Canonical Correlation Analysis (CCA) is a technique that finds how  X  X imilar X  are the subspaces that are spanned by the columns of two different matrices A  X  R m  X  n and B  X  R m  X  ` . CCA measures similarity by means of the cosines of the so-called principal angles between the two subspaces. Those values are also known as canonical correlations of the matrix pair ( A , B ). In this work, we consider the over-constrained case where the number of rows is greater than the number of columns ( m &gt; max { n,` } ). We study the problem of con-structing  X  X ore-sets X  for CCA. A core-set is a subset of rows from A and the corresponding subset of rows from B -de-noted by  X  A and  X  B , respectively. A  X  X ood X  core-set is a subset of rows such that the canonical correlations of the core-set (  X 
A ,  X  B ) are  X  X lose X  to the canonical correlations of the original matrix pair ( A , B ). There is a natural tradeoff between the core-set size and the approximation accuracy of a core-set. We present two algorithms namely, single-set spectral spar-sification and leverage-score sampling, which find core-sets with additive-error guarantees to canonical correlations. G.3 [ Probability and Statistics ]: Correlation and regres-sion analysis; G.1.0 [ Numerical Analysis ]: Numerical Al-gorithms Algorithms, Theory, Experimentation Canonical Correlation Analysis; Sampling; Dimension Re-duction
Canonical Correlation Analysis (CCA) [1] is an impor-tant technique in data mining, scientific computing, statis-tics, and machine learning. CCA has been used in many c  X  Notation. A , B ,... denote matrices and  X  , b ,... denote column vectors; e i is the standard basis, whose dimension-ality will be clear from the context; I m is the m  X  m identity matrix, and 0 m  X  n is the m  X  n all-zeros matrix. R ( . ) de-notes the column space of its argument matrix. We make use of the Singular Value Decomposition (SVD) of a matrix A  X  R m  X  n of rank  X   X  min { m,n } =  X  and we distinguish between the  X  X hin X  and the  X  X ull X  SVD of A . The thin SVD of A is A = U X V T , where U  X  R m  X   X  is an orthogonal matrix containing the left singular vectors of A ,  X   X  R  X   X   X  is a diagonal matrix containing only the positive singular values  X  1  X   X  2  X  ... X   X  &gt; 0, and V  X  R n  X   X  is an orthogo-nal matrix containing the right singular vectors of A . The full SVD of A is A = U X V T , where U  X  R m  X   X  is an or-thogonal matrix containing the left singular vectors of A ,  X   X  R  X   X   X  is a diagonal matrix containing the singular val-ues  X  1  X   X  2  X  ... X   X   X  0, and V  X  R n  X   X  is an orthogonal matrix containing the right singular vectors of A .

We use i : j to denote the set i,...,j, and [ m ] = 1 : m. We denote by [ A ; B ] the matrix obtained by concatenating the columns of B next to the columns of A . Given a subset of indices T  X  [ m ] , the corresponding  X  X ampling X  matrix S is the | T | X  m matrix obtained by discarding from I m the rows whose index is not in T. Note that SA is the matrix obtained by keeping only the rows in A whose index appears in T and DSA is the matrix obtained by keeping only the rows in A whose index appears in T and then rescaling those rows by the corresponding elements of a square diagonal matrix D . We call D a  X  X escaling X  matrix.
 Contributions. We present a deterministic and a random-ized algorithm to construct core-sets of sizes r = O ( p X   X  2 ) specified accuracy parameter with 1 / 2 &gt;  X  &gt; 0 and  X  is a failure probability. We use single-set spectral sparsifica-tion and leverage-score sampling as the deterministic and randomized algorithms respectively for core-set construc-tion. We prove that the canonical correlations and canonical weights of the core-set are an additive  X  approximation far from the canonical correlations and canonical weights of the input matrix pair ( A , B ).
 Related work. The work more closely related to ours is that of Avron et al., [3], who also provided a core-set con-struction (randomized) algorithm for CCA. Unfortunately, the approximation bound of their algorithm is weak and it is only useful for matrices with low  X  X oherence X . Their method used linear combination of data-points, whereas we select pure data-points for approximate core-set construction.
Definition 2. Given a matrix A with m rows and rank p , the coherence of A is defined as  X  ( A ) = max where e i is the i -th standard basis (column) vector of R m , and U A  X  R m  X  p contains the left singular vectors of A from the thin SVD of A .
 The coherence of a matrix is a measure of how close a basis is to sharing a vector with a canonical basis. Note that the coherence of A is a property of the column space of A , and does not depend on particular choice of basis (e.g. the basis described by the columns of A ). Therefore, if R ( A ) = R ( B ) then  X  ( A ) =  X  ( B ). Furthermore, it is easy to verify that if R ( A )  X  R ( B ) , then,  X  ( A )  X   X  ( B ). We also mention that for every matrix A with m rows: rank( A ) /m  X   X  ( A )  X  1 . 4. end for 5. Multiply all the weights in D by 6. Return S and D .

Lemma 1. (BSS Sampling [4]) Given U  X  R m  X  d with m &gt; d satisfying U T U = I d and sampling parameter r &gt; d , there exists a deterministic algorithm to construct sampling and rescaling matrices S  X  R r  X  m and D  X  R r  X  r such that for all i = 1 : d and  X  = 3 p d/r , it is:  X  1 +  X . The running time of the algorithm is O ( md 2 r ) . We will also use a generalization of Lemma 1.The proof of this lemma (Lemma 2) can be found in page 49 of [6]. A similar lemma can be shown for leverage-score sampling.
Lemma 2. [6] Given A  X  R m  X  d of rank p with m &gt; d and sampling parameter r &gt; p , there exists a deterministic algorithm to construct sampling and rescaling matrices S  X  R r  X  m and D  X  R r  X  r such that for  X  = 3 p p/r and R = DS , it is: A T R T RA  X  A T A 2  X   X  k A k 2 2 . The running time of the algorithm is O ( md 2 r ) . The algorithm in this lemma applies the algorithm of the previous lemma on the matrix U
A  X  R m  X  p from the thin SVD of A .
Let U be the top-d left singular vectors of the data ma-trix X  X  R m  X  d . A carefully chosen probability distribution of the form p i = k U i k 2 2 d for i = 1 ,..m , i.e. proportional to the squared Euclidean norms of the rows of the left-singular vectors is constructed. Select r rows of U in i.i.d trials and re-scale the rows with 1 / inated by the time to compute the SVD of X .

Lemma 3. (Leverage-score Sampling [5]) Let U  X  R m  X  d have orthonormal columns with m &gt; k and U T U = I k . Let 0  X   X   X  1 , 0 &lt;  X  &lt; 1 , and 4 kln (2 k X  ) / X   X  r  X  n. Let S  X  R r  X  m and D  X  R r  X  r be the sampling and re-scaling matrices. Then for all i = 1 ,  X  X  X  ,k, w.p. at least (1  X   X  ) , for
Consider the following algorithm to construct a core-set for the CCA of a matrix pair using BSS. A similar algorithm can be shown by using leverage-score sampling. Inputs are A  X  R m  X  n of rank p , B  X  R m  X  ` of rank q  X  p, and accuracy parameter 0 &lt;  X  &lt; 1 / 2. We further assume that m  X  p + q . 2. Let U C  X  R m  X   X  be the left singular vectors of C from 4. Let U  X  R m  X   X  be the left singular vectors of U T from We also use the following  X  X tructural X  result for dimension-ality reduction in Canonical Correlation Analysis.
 Theorem 4. (CCA dimension reduction [3]) Let A  X  R m  X  n ( m  X  n ) have rank p and B  X  R m  X  ` ( m  X  ` ) have rank q  X  p . Let 0 &lt;  X  &lt; 1 / 2 be an accuracy parameter. Let A = U A  X  A V T A , with U A  X  R m  X  p ,  X  A  X  R p  X  p , and V A  X  R p  X  n be the thin SVD of A . Let B = U B  X  B V T B , with U SVD of C with  X  = rank ( C )  X  rank ( B )  X  rank ( A ) . As-sume the following relations hold for some matrix R  X  R r  X  m ( r  X  max { n,` } ) : 1. For every r  X  [ p ] : p 1  X   X / 3  X   X  r ( RU A )  X  p 1 +  X / 3 . 2. For every k  X  [ q ] : p 1  X   X / 3  X   X  k ( RU B )  X  p 1 +  X / 3 . 3. For every h  X  [  X  ] : p 1  X   X / 3  X   X  h ( RU C )  X  p 1 +  X / 3 . Denote  X  A = RA  X  R r  X  n and  X  B = RB  X  R r  X  ` . For i = and  X  w i =  X  x i /  X  A  X  x i Also,  X  x i  X  R n and  X  y i  X  R ` are some vectors chosen as statements (1) , (2) , (3) of Theorem 2 hold simultaneously. The proof of Theorem 3 follows by using the three conditions of Lemma 5 into Theorem 4.
We performed experiments on synthetic data on an intel-i7 desktop with 8GB RAM in MATLAB environment. We compared the canonical correlation coefficients of the full-data with leverage-score sampling, single-set spectral spar-sification and uniform sampling. Since, leverage-score sam-pling (uniform sampling) is a randomized algorithm, we re-peat the experiments ten times to get around the random-ness and choose the result from the best run.

We generate a pair of tall-and-thin matrices A and B ,
Figure 1: Plots of Leverage-scores of Synthetic Data. (of size m-by-n, where m=3000 and n=10) with specific leverage-scores using [7]. By construction, the first 10 lever-age scores of A are generated by b 1 + ( a 1  X  b 1)  X  rand (10 , 1) , where a 1 = 10 ,b 1 = 9 and the next 2990 leverage-scores are generated by b 2 + ( a 2  X  b 2)  X  randn (2990 , 1) , where a 2 = 3 ,b 2 = 1 , and rand(m,n) and randn(m,n) are the Mat-lab functions generating mn i.i.d entries from the standard
