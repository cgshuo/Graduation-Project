 In the last decade, search engines have b een essential tools for information re-trieval. People rely heavily on these tools to find information on the Web, and as a result, most Web sites get a considerable number of visitors via search engines. Since the increase in visitors usually means the increase in financial profit, and approximately 50% of search engine users look at no more than the first 5 results in the list [1], obtaining a high ranking in the search results became crucial for the success of sites.

Web spamming is the behavior that manipulates Web page features to get a higher ranking than the page deserves. Web spamming techniques can be cate-gorized into term spamming and link spamming. Term spamming manipulates textual contents of pages by repeating specific keywords that are not related with page contents and by adding irrelevant meta-keywords or anchor text. Search engines which employ textual relevance to rank pages will return these manip-ulated pages at the top of the result list. Link spamming manipulates the link structure of the Web to mislead link-based ranking algorithms such as PageRank [4]. Link-based ranking algorithms consider a link as an endorsement for pages. Therefore, spammers crea te numerous false links and construct an artificially interlinked link structure, so called a spam farm, where all pages link to the target spam page in order to centralize its link-based importance.

Links from external normal pages to spam pages are needed in order to attract the attention of search engines and feed ranking scores to spam farms. These links that are created without any agreements of page owners are called hijacked link . To hijack link, spammers post comments including URLs to spam pages on public bulletin boards, buy expired domains and sponsor pages. Hijacked links affect link-based ranking algorithms significantly, when they are pointing to spam farms containing a large amount of spam pages.

In this paper, we propose a novel method for detecting Web sites hijacked by spammers. Most of previous researc hhasfocusedondem oting or detecting spam, and as far as we know, there has been no study on detecting link hijacking that is important in the following situations:  X  Hijacked sites are prone to be attacked continuously by various spammers  X  Once we detect hijacked sites, we can modify link-based ranking algorithms  X  Crawling spam sites is a sheer waste o f time and resources. Most crawlers
In order to identify hijacked sites, we consider characteristics of the link struc-ture around hijacked sites. As Figure 1 indicates, hijacked sites are supposed to have a certain number of links to both normal and spam sites, and exist at the boundary of them. To detect this boundary, we take account of trustworthiness and spamicity of whole sites. Normal sites would have high trustworthiness and low spamicity, and in contrast, spam sites would have low trustworthiness and high spamicity. These relations will be reversed at the link between normal sites and spam sites, or where link hijacking occurs. Based on this idea, we detect the point where trustworthiness and spa micity are reversed in order to extract hijacked sites.

In addition, we focus on the fact that hijacked sites have links pointing to both normal and spam sites. Out-neighbors of normal sites will show much more trust-worthiness than spamicity, and vice versa. Thus, it would be assumed that overall trustworthiness and spamicity in out-neighbors of hijacked sites are equivalent compared to those of normal or spam.

Trustworthiness and spamicity of a site can be evaluated by some link-based ranking algorithms such as modified versions of PageRank. For each site, we calculate white and spam scores using two different modified PageRanks. In-tuitively, these scores represent the deg ree of trustworthiness and spamicity of sites.

The rest of this paper proceeds as follows. In Section 2, we review back-ground knowledge for PageRank and link spamming. Section 3 introduces modi-fied PageRank algorithms and several approaches to detect or demote link spam-ming. Section 4 presents our method for de tecting hijacked sites. In Section 5, we report the experimental result of our algorithm. Finally, we conclude and summarize the result of our approach. 2.1 Web Graph The entire Web can be considered as a di rected graph. We can denote the Web as G =( V, E ), where V is the set of nodes and E is a set of directed edges &lt;p,q&gt; .Node v can be a page, host or site.
 Each node has some incoming links( inlinks ) and outgoing links( outlinks ). In ( p ) represents the set of nodes pointing to p (the in-neighbors of p )and Out ( p ) is the set of nodes pointed to by p (the out-neighbors of p ). We will use n to de-scribe V , the number of total nodes on the Web. 2.2 PageRank PageRank [4] is one of the most well-known link-based ranking algorithms. The basic idea of PageRank is that a Web page is important if it is linked by many other important pages. This recursive definition can be showed as following ma-trix equation: where p is PageRank score vector, T is transition matrix. T ( p, q )is1 / Out ( q ) if there is a link from node q to node p , and 0 otherwise. The decay factor  X &lt; 1 (usually 0.85) is necessary to guarantee convergence and to limit an effect of rank sink. d is a uniformly distributed random vector. Instead of following links to next pages, we can jump from a page to a random one chosen according to distribution d . 2.3 Link Spamming After the success of Google which adop ted PageRank as the main ranking al-gorithm, PageRank became a primary target of link spammers. Z. Gy  X  ongyi et al. studied about link spam in [6] and introduced an optimal link structure to maximize PageRank score, a spam f arm . The spam farm consists of a target page and boosting pages. All boosting pages link to the target page in order to increase the rank score of it. Then, the target page distributes its boosted PageRank score back to supporter pages. By this, members of a spam farm can boost their PageRank scores. Due to the low costs of domain registration and Web hosting, spammers can create spam farms easily, and actually there exist spam farms with thousands of different domain names [10].

In addition to constructing the internal link structure, spammers make ex-ternal links from outside of spam farms to attract search engines and provide PageRank scores to the target page. To make links from non-spam pages to spam pages, various hijacking techniques are exploited. Spammers send track-backs that lead to spam sites, or post comments including links pointing to spam pages. Expired domains can be bought by spammers, and then changed to spam sites. Spammers can also sponsor Web sites to insert advertisements of spam sites on their pages.

Examples of link hijacking are shown in Figure 2 and 3. Figure 2 illustrates spam comments on blogs. Spammers post comment containing a large amount of links to spam sites. By this, they can create massive links to spam sites easily in a short period. Figure 3 shows a page containing many links to spam pages. Although it is the page about Java and Linux, it contains links pointing to a lot of sport sites which are densely conn ected together. Note that major search engines and blog services employ counter-measures like rel="nofollow" tags, which is attached to hyperlinks that should be ignored by link-based ranking algorithms [15]. However, there still exist a number of Web services that do not support such means, and hijacking techniques like buying expired domains cannot be penalized by "nofollow" tag. 3.1 TrustRank and Anti-trustRank To improve the PageRank algorithm, Gy  X  ongyi et al. presented the TrustRank algorithm [8]. The basic intuition of TrustRank is that good pages seldom link to spam pages. People trust good pages, and can trust pages pointed to by good pages. Like this, trust can be propagated through the link structure of the Web. Therefore, in TrustRank, a list of highly trustworthy pages is created as a seed set and each of these pages is assigned a non-zero initial trust score, while all the other pages on the Web have initial values of 0. After computation, good pages will get a decent trust score, and spam pages get a lower trust scores.
The matrix notation of TrustRank is following: where t is TrustRank score vector,  X  is decay factor(0.85), and d  X  is a random jump distribution vector where Krishnan et al. proposed Anti-TrustRank to find out spam pages [11]. Instead of selecting good pages as a seed set, Anti -TrustRank starts score propagation from spam seed pages. Each spam page is assigned Anti-Trust score and this score is propagated along incoming links. 3.2 Core-Based PageRank Core-based PageRank was suggested by Gy  X  ongyi et al. [10]. When we have a seed set S , we describe a core-based PageRank score of a page p as PR ( p ). A core-based PageRank score vector p is : where a random jump distribution d  X  is : Core-based PageRank is different from TrustRank by random jump vector. Core-based PageRank adopts a random jump distribution 1 /n , which is normal-ized by the number of whole Web site, instead of 1 / S .
In this paper, we use two types of core-based PageRank scores.  X  X R + = a core-based PageRank score with a trust seed set S + .  X  X R  X  = a core-based PageRank score with a spam seed set S  X  .

Z. Gy  X  ongyi et al. mentioned a core-based PageRank with a spam seed set in [10]. They refer to blending PR + and PR  X  (e.g. compute weighted average) in order to detect spam pages. However, this view is different from ours. We think PR + and PR  X  separately and focus on the change in scores through links to discover hijacked links. 3.3 Other Approaches Several approaches have been also suggested for the purpose of detecting and demoting link spam.

To demote spam pages and make PageRank resilient to link spamming, Wu et al. complemented TrustRank with topicality in [9]. They computed TrustRank score for each topic to solve a bias problem of TrustRank.

To detect link spam, Benczur et al. introduced SpamRank [12]. SpamRank checks PageRank score distributions of all in-neighbors of a target page. If this distribution is abnormal, SpamRank regards a target page as spam and penalizes it. Gy  X  ongyi et al. suggested Mass Estimation in [10]. They evaluated spam mass , a measure of how many PageRank scores a page gets through links from spam pages. Saito et al. employed a graph algorithm to detect Web spam [13]. They extracted spam pages by the strongly co nnected component decomposition and used them as a seed set to separate spam pages from non-spam pages.
Du. et al. discussed an effect of hijacked links on the spam farm in [7]. They introduced an extended optimal spam farm by dropping the assumption of [6] that leakage by link hijacking is constant. Although Du. et al. considered link hijacking, they did not studied features of hijacking and its detection, which is different from our work.

As we reviewed, although there are various approaches to link spam, link hijacking has never been explored closely. In this paper, we propose a new ap-proach to discover hijacked links and sites. With our approach, we expect to contribute to new spam detection techni ques and improve th e performance of link-based ranking algorithms. Based on characteristics of the change in trustworthiness and spamicity around hijacked sites, we compute a hijacked score of sites.
 To begin with, we assign white score and spam score to whole Web sites. We employ the notion of White ( p )and Spam ( p ), which represent the degree of trustworthiness of site p and the degree of spamicity of site p , respectively. For example, TrustRank score and core-based PageRank score with a white seed set can be used as white score. Anti-TrustRank score and core-based PageRank score with spam seeds are available for spam scores.

With these scores, we define the score reversal relation. Hijacked site p would have a higher white score than its spam score, and spam site would have lower white score than its spam score. The se features can be described with relative trust , RT .

We used log value since PageRank score obeys power law distribution. If the log difference between whit e score and spam score of site p is bigger than threshold  X  , p is more likely to be normal. In contrast, RT ( p ) lower than 0 implies p might be spam. When we use a higher  X  value, we consider the white score of hijacked sites is much higher than its spam score. Therefore, our algorithm will choose a site nearer to normal side as a hijacked. When we use a lower  X  value, we regard the white score of a hijacked sit e is lower, so select sites close to spam side as hijacked.

AsweseeinSection1,if p is hijacked, there must be spam sites in out-neighbors of p by definition. In addition to this, we take account of only sites with a lower white score and higher spam score than those of p .Withthis condition, we can check sites that are more likely to be spam than p is. We will call this relation score reversal .
 With the score reversal relation, we define the hijacked score. First, we create R ( p ), a set of sites which have the score reversal relation with site p .
Then, we define a set H of hijacked candidates. A hijacked site h will have higher trustworthiness than its spamicity, and will have at least one out-neighbor node that is in the score reversal relation with it.
 Finally, we compute the hijacked score of h . Two different hijacked detection methods are designed.

As a naive approach, we considered only the total trustworthiness difference between a hijacked candidate and sites in the score reversal relation with it. This can be described as following:
We found out that when both trustworthiness and spamicity of around a hijacked site are considered, a higher det ection precision can be achieved. Overall out-neighbor trustworthiness of a hijacked site will be obtained by the average RT of total normal outnodes. In the same manner, spamicity of out-neighbors will be computed by the average  X  RT of all spam outnodes. To obtain out-neighbor trustworthiness and spamicity of a hijacked site, we divide Out ( h )into a set of normal sites nOut ( h ) and a set of spam sites sOut ( h ). The following is the improved hijacked score H all ( h ).
 While  X  is a smoothing factor which alleviates an effect of RT when the number of normal or spam out-neighbors is very small.

H all ( h ) increases as both trustworthiness and spamicity grow. When either trustworthiness or spamicity is getting lower, H all ( h ) decreases since site h seems to be a spam or normal site. To test our method, we prepared a large Web data set. White seed set and spam seed set were also generated to co mpute white and spam scores. As for computations of white and spam scores, we used a core-based PageRank score PR + and PR  X  pair. With white and spam scores, we obtained two kinds of hijacked scores and evaluated the precisions of results. 5.1 Data Set and Seed Set To evaluate our algorithm, we performed ex periments on a large-scale snapshot of our Japanese Web archive built by a cra wling conducted in May 2004. Basically, our crawler is based on breadth-first c rawling [14], except that it focuses on pages written in Japanese. Pages outside the .jp domain are collected if they were written in Japanese. We used a site as a unit when filtering non-Japanese pages. The crawler stopped collecting pages from a site, if it could not find any Japanese pages on the site within the first few pages. Hence, our dataset contains fairly amount of pages in English or other languages. The percentage of Japanese pages is estimated to be 60%. This snapshot is composed of 96 million pages and 4.5 billion links.

We use an unweighted site level graph of the Web, in which nodes are Web sites and edges represent the existence of links between pages in different sites. In the site graph, we can easily find dense connections among spam sites that cannot be found in the page level graph. To build the site graph, we first chose the representative page of each site that has 3 or more inlinks from other sites, and whose URL is within 3 tiers (e.g. http://A/B/C/). Then, pages below each representative page are contracted to o ne site. Finally, edges between two sites are created when there exist links betw een pages in these sites. The site graph built from our snapshot includes 5.8 million sites and 283 million links. We call this dataset Web graph in our experiments.

To compute white and spam scores, we constructed trust seed set and spam seed set. We used manual and automated selecting methods for both seed sets.
In order to generate the white seed set, we computed PageRank score of whole sites and performed a manual selection on top 1,000 sites with a high PageRank score. Well-known sites (e.g. Google, Yahoo!, and MSN), authoritative university sites and well-supervised company sites are selected as white seed sites. After a manual check, 389 sites are labeled as trustworthy sites. To make up for the small size of a seed set, we extracted sit es with specific URL including .gov (US governmental sites) and .go.jp (Japanese governmental sites). In the end, we have 40,396 sites as trust sites.

For the spam seed set, we chose sites with high PageRank score and checked manually. Sites including many unrelated keywords and links, redirecting to spam sites, containing invisible terms and different domains for each menu are judged spam sites. We have 1,182 sites after a manual check. In addition, we used automatically extracted spam sites obtained by [13]. Saito et al. obtained this large spam seed set by following steps. Fi rst, they extracted strongly connected components (SCC) from the Web graph. Since spam sites tend to construct a densely connected link structure, it c ould be assumed that spam sites form SCC. In addition to detecting spam located at the fringe of the Web, Saito et al. counted up maximal cliques in the largest SCC, or a core. Cliques whose sizes are less than 40 were extracted from the core and about 8,000 spam sites were obtained. Finally, they used these spam sites as a reliable spam seed set and expanded them by a minimum cut technique that separates links between spam and non-spam sites. Since this meth od showed a high extra ction precision, we used their spam sites as seed sites. Finally, Total 580,325 sites are used as a spam seed set. 5.2 Types of Hijacking In order to understand a layout of sites at the boundary of normal and spam, we randomly chose 1,392 sites near to spam seeds. These samples are checked by hand and classified into 4 categories; hijacked, normal, spam and unknown. Unknown sites were written in unrecognizable languages such as Chinese, Dutch, German and so on. Table 1 shows the result of classification. The 33% of total sites was identified as hijacked, and these 465 sites are divided into 8 types as follows.  X  Blog sites with spam comments or trackbacks and public bulletin boards  X  Expired sites bought by spammers. Spammers can buy expired domains and  X  Hosting sites that include spam sites of some customers.  X  Normal sites that point to expired hijacking sites. Expired hijacking sites are  X  Free link registration sites that allow spammers to register links on them.  X  Normal sites that create links to spam sites by mistakes. Authors of some  X  Normal sites that contain advertising links pointing to spam sites. Spammers  X  Sites with public access statistics t hat show links to referrers. Spammers
Table 2 shows the number of sites in each hijacking type. We can see that the most frequently used technique is blog and bbs hijacking. Expired hijacking is a quite popular technique among spammers, too. Particularly, domains for official sites of movies and singers are prone to be hijacked because they are used for a while, not permanently. 5.3 Evaluation Using the white and spam seed sets, we computed core-based PageRank scores for white and spam scores. Hijacked scores were obtained as well, with different  X  values. (See Section 4.) Precision with a naive approach. For  X  from  X  2 to +2, we chose top 200 sites with high H rev scores and checked them by hand. Detected samples are categorized into hijacked, normal, spam, and unknown. The detail is shown in Table 3. The best precision 42.5% was obtained when  X  was 1.
 Precision with a suggested approach. With different  X  values from  X  5 . 0 to 1 . 0, we computed H all score and evaluated top 200 sites. In order to deter-mine smoothing factor  X  ,wecalculated H all score of sample sites mentioned in Section 5.2 and examined the result. Sin ce the best precision for top 200 sites was obtained when  X  = 40, we used the same value for the whole sites evalu-ation. The result is described in Table 4. We detected hijacked sites with the best precision of 67.5% when  X  is  X  2.

As both Table 3, 4 indicate , normal sites increase as  X  increases. This is because with a higher  X  , hijacked sites should have a higher white score. Likewise, as  X  decreases, the proportio n of spam sites increases. This means our algorithms become tolerant and consider sites with a r elatively high spam score as hijacked. As for  X  , we found out that as  X  increases, H all of spam sites decreases. However, if the value of  X  exceeds 40, the number of spam sites in the top result hardly changes. The ratio of normal sites with high H all remain stable regardless of  X  .
We computed H rev score with a TrustRank and Anti-TrustRank score pair and investigated the performance. Howe ver, the precision was far worse than that with a core-based PageRank pair. In this paper, we proposed a new method for link hijacking detection. Link hijacking is one of the essential methods for link spamming and massive hijacked links are now being generated by spammers. Since link hijacking has a significant impact on link-based ranking algorithms, de tecting hijacked sites and penalizing hijacked links are serious problems to be solved.

In order to identify hijacked sites, we focused on characteristics of the link structure around hijacked sites. Based on the observation that white and spam score reversal occurs between hijacked sites and hijacking sites, we computed hijacked scores.
Experimental result showed that ou r approach is quite effective. Our best result for finding hijacked sites outperformed about 25% compared to a naive approach.

