 Collaborative filtering is a widely used technique for rat-ing prediction in recommender systems. Memory based col-laborative filtering algorithms assign weights to the users to capture similarities between them. The weighted aver-age of similar users X  ratings for the test item is output as prediction. We propose a memory based algorithm that is markedly different from the existing approaches. We use preference relations instead of absolute ratings for simil ar-ity calculations, as preference relations between items ar e generally more consistent than ratings across like-minded users. Each user X  X  ratings are viewed as a preference graph. Similarity weights are learned using an iterative method mo -tivated by online learning. These weights are used to create an aggregate preference graph. Ratings are inferred to max-imally agree with this aggregate graph. The use of prefer-ence relations allows the rating of an item to be influenced by other items, which is not the case in the weighted-average approaches of the existing techniques. This is very effectiv e when the data is sparse, specially for the items rated by few users. Our experiments show that the our method outper-forms other methods in the sparse regions. However, for dense regions, sometimes our results are comparable to the competing approaches, and sometimes worse.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering, selection process; H.3.4 [ Systems and Software ]: User profiles and alert services Algorithms, Experimentation, Performance Information Retrieval, Recommender Systems, Collabora-tive Filtering, Online Learning
Users in recommender systems often express their opin-ions about different items by rating them on a fixed rating scale. The rating scale can have two categories -like/disli ke, or multiple categories where higher categories denote high er level of user satisfaction. The main task of such recom-mender systems is to predict the rating that a user would give to an item he has not rated yet. Based on the as-sumption that users with similar tastes would rate items similarly, collaborative recommendation systems first find a group of users having similar interests. Opinions of the users from that group are used to predict the unknown rat-ing. The problem has received increased interest after the Netflix challenge was announced in 2006.

Collaborative filtering techniques can be divided into memory-based and model-based approaches. Memory-based approaches are further classified into user-based and item-based methods. User-based methods look at the rating data-base to compute similarity between each user pair. While predicting the rating of a test item for a target user , the weighted average of the ratings assigned to the test item by other users is output as the estimate. The weight is a non-decreasing function of the similarity values between users . Item-based methods work analogously; the final prediction is a weighted average of the ratings assigned to the different items by the target user. Model-based approaches create a user and/or item model from the rating data. Based on their rating patterns, users are clustered into a small number of groups. While predicting, first the target user is classified into one of the predefined user clusters. Rating of that group on the test item is output as the prediction. Model based algorithms using Bayesian network [4], [17], aspect models [7], [9], random walk models [19] are discussed in literatur e. Methods that combine both memory-based and model-based approaches for rating prediction are also suggested [16], [ 15].
User similarities may be calculated by Pearson Correlation or Vector similarity measures [2]. Several other variants h ave also been suggested. Opinions on certain items may give more insight about similarities between two users. In [8] a set of importance factors are learned for the items so that the distribution of similar users form compact clusters in t he item space. The method described in [1] learns the weights so as to minimize the prediction error on the training set.
For user-based approaches, the weight of a user is a mea-sure of confidence on that user X  X  opinion. The confidence can be biased if the number of available ratings is small. Moreover, if not many users have rated the test item, then the estimate can be poor as it is the weighted average of a small number of ratings. The same problem is shared by item-based approaches as well. For model-based approaches , probabilistic inferences may go wrong if the data is sparse. Data sparsity is a known challenge for any collaborative rec -ommendation system.

It is understood that collaborative systems looking at pure ratings only have some drawbacks. Even similar users tend to rate the same items differently. This is because rating is subjective, and users have different levels of leniency whil e rating items. Moreover, it might be difficult to pick a partic-ular rating for an item, but given an item pair it is probably easier to say which one is better (or both are equally good). Keeping this in view, we propose in this paper a memory-based collaborative filtering approach that uses preferenc e relations between items instead of absolute ratings.
Though preference relation based techniques exist in liter -ature, they are much fewer when compared with the rating-based techniques. However, to the best of our knowledge, there is no rating prediction method that completely ignore s actual ratings given by the similar users, and looks at rela-tive ratings only. In [9], the authors have suggested a pref-erence model where the orderings of items preferred by the user are modeled. [5], [12], [10], [3] use preference relati ons for determining user similarities, but the similarities ar e used to recommend a ranking on the unrated items. [11] assigns similarity weights based on preference relation over items . This measure of similarity, called Somers coefficient , is used to aggregate absolute ratings of the users. Experiments us-ing  X  X  dataset that is neither too sparse nor too dense X  [11] showed the efficacy of the algorithm for moderately sparse data. A random walk recommender (RWR) is proposed in [19]. Their approach uses transitive information for captu r-ing item similarities. If no user has rated both the items a and b , most of the item similarity measures will predict a similarity score of 0 for a and b . However, these items may be considered similar if there is an item c which is similar to both of them. The authors suggested a finite length random walk scheme to use such transitive relations between items for determining item similarities.

In any real dataset, there are many users who rate few items, and there are many items which are rated by very few users. We concentrate on the items that have less ratings. Generally, existing methods perform poorly while predicti ng ratings for such items. We focus on the task of improving the prediction accuracy for items which have fewer ratings. Such items form a significant part of any real dataset. There are two reasons why our method performs well on sparse dataset: (a) we use preference relations instead of actual ratings. It is a known fact that users have similar tastes of items may not have similar rating patterns due to differ-ent rating biases or habits. This becomes more prominent when there are fewer ratings available for the items. How-ever, preference relations over the items are expected to be similar for similar users, and (b) our formulation allows th e confidence associated with the prediction of a less rated ite m to be boosted by other items that are heavily rated.
The rest of this paper is organized as follows. Section 2 outlines our methodology for rating prediction. We discuss the preference aggregation algorithm in Section 3. Method for recovering personalized rating from the aggregate pref -erence graph is explained in Section 4. Section 5 gives the experimental methodology and the results. We conclude and discuss future work in Section 6.
The assumption that users having similar interests would rate items similarly does not often hold in reality. This has been observed by many researchers in the past. For the  X  X e-nient X  users, ratings of items tend to be higher than others even though they might have similar tastes for items. The rating pattern of a user is determined by his/her interests as well as the rating strategy/habit [9]. We believe that use of preference relations between items can reduce the bias due to the users X  rating habit and may give a clearer under-standing about the qualities of the items. In our opinion, the issues with absolute ratings as discussed in Section 1 ca n be addressed using preference relations between items.
For an item pair ( i, j ), if a user u gives higher rating to i than j , it can be assumed that u has higher preference for i . Using this observation, we can represent the ratings of a given user by a preference graph.
 Preference graph for a user: Preference graph G v = Figure 1: Example preference graph created from user ratings
For predicting the ratings, we pick one user at a time. For that user, ratings for the test items are predicted. Then we move on to predict for the next test user. For the similar users, we are interested in the preference graphs induced by the items that u has rated and the ones for which ratings are to be predicted. The task of rating prediction for a test user is split into two phases.

Aggregation phase: In the first phase, preference graphs from similar users are combined to produce a weighted pref-erence graph biased towards the interest of the target user u . Weights are assigned to users depending on similarities between their preference graphs. We propose a learning al-gorithm to determine the aggregation weights.

Rating recovery phase: The next phase recovers un-known ratings from the aggregate preference graph using the rating profile of the target user. Ratings are assigned to the test items so as to minimize the number of violations to the preference relations as obtained from the aggregate prefer -ence graph. A directed edge e ij in the preference graph is considered as a back edge if the rating of i is greater than the rating of j . We try to minimize the weighted number of back edges while paying attention to the rating profile of the user. Details of these two phases are described in sections 3 and 4 . Initially, let us assume that all users rate all the items. We define the difference  X ( G x , G y ) between the preference graphs G x and G y as the sum total of the differences in edge weights between the graphs. Mathematically, we can write: In reality, all users do not rate all items. We address this situation by setting | w ijx  X  w ijy | to some constant  X   X  [0 , 1] if e ij is not present in exactly one of G x and G y . Different possible values for | w ijx  X  w ijy | are shown in Table 1. We term this table as the loss matrix . In the table,  X  denotes that weight for the edge is not available (either or both of the items are not rated).
We tried three different approaches for weight assignment: (a) The user having minimum difference with the target user in terms of their preference graphs is given weight of 1, all others are given weight 0, (b) Defining weight as a non-increasing function of  X ; we selected w = 1  X   X  as the weight function, and (c) an iterative weight update method. We found out experimentally that the third method was the best among these three approaches. Here we present the third approach -an iterative weight update mechanism. Algorithm 1 Finding aggregation weights 1:  X  : Weight update parameter. Value is between 0 and 1. 2: W : Expert-weight vector. 3: L : Loss matrix. 4: N : Number of users. 5: Set W k = 1 N ,  X  k = 1 , . . . , N 6: for all new edge e ij do 7: Set W k = W k 8: for all users v = 1 to N do 9: l v = L ( w iju , w ijv ) 10: W v = W v  X  l v 11: end for 12: end for
Let u be the target user for whom we want to predict the ratings. Initially we shortlist a set of users based on the number of items that the user and u have rated in common. Users in this set are called similar users. Weights are as-signed to these similar users. The top-K users among them having the highest weights are considered as experts .
The aggregate preference graph will be a weighted com-bination of the preference graphs obtained from the similar users. We want this weighted aggregate graph to be as close as possible to the preference graph of the target user. So, we will need the weights of the similar users to depict the similarities between their preference graphs and that of th e target user. To learn this set of weights, we suggest an it-erative weight update algorithm that is motivated by the Hedge algorithm [6] proposed in the online learning litera-ture. Basic steps of our weight update algorithm are: 1. Repeat the following steps for each edge in the target 2. Note the direction of the edge in the target user X  X  3. Note the directions of this edge in the graphs obtained 4. If this direction is different from the direction in target Here we try to explain the rationale behind using this weight update algorithm. The weight of an expert at any instant can be viewed as the confidence on him/her till that instant. Suppose we look at the preference graphs from the experts, and attempt to create a preference graph P that is an ap-proximation of G u , by adding edges to P one by one. We use the confidence values to pick a similar user x at random, take that edge from G x , and put it in P . It can be shown that P is expected to be a good approximation of G u as where  X  is the parameter of the weight update algorithm and best is the user whose preference graph is maximally similar with the preference graph of the target user (i.e. best = arg max v  X ( G v , G u )). Equation 2 directly follows from the analysis given in [6]. It gives a theoretical up-per bound on  X ( P, G u ) at all points of the weight update method. As the weight of an expert v is reduced when-ever his/her relative preference for an item pair differs fro m the preference given by the target user, we may expect that weights of the experts having high similarity with the targe t user to be high. So, the weight of an expert is a measure of the experts X  X  similarity with the target user. Experimen -tal results show that the prediction accuracy increases as we increase the number of experts K . Detailed steps of the weight update algorithm are given in Algorithm 1. Weight update: an illustrative example: Let us consider two users u 1 and u 2; their ratings and corre-sponding preference graphs are given in Figure 2. We want to compute the similarity weights assigned to u 1 and u 2 when u is the target user. The ratings and preference graphs of u are given in Figure 1. The following table illustrates the different steps of the weight update algorithm.
Once the expert weights are determined, we can create the aggregate preference graph that will be used for predicting the ratings. The aggregate graph should contain the items that u has rated as well as the items for which ratings are to be predicted. Weight of edge e ij (where at least one of i and j is a test item) in G v denotes v  X  X  vote in favor of the preference relation j is better than i . For adding a directed Algorithm 2 Constructing the aggregate preference graph 1: A : Aggregate graph. Initialize it with G u . 2: c V u : Test items for user u . 3: N : Number of experts. 4: W : The expert-weight vector. 5: for all item i  X  c V u do 6: Add a node to A for item i . 7: for all item j in the aggregate graph A do 8: Add edges ( i, j ) and ( j, i ). 9: Initialize: pos = 0 , neg = 0. 10: for all v = 1 to N do 11: if w ijv 6 =  X  then 12: neg = neg + (1  X  w ijv ) W v 13: pos = pos + w ijv W v 14: end if 15: w ij = neg /* weight of e ij in P */ 16: w ji = pos /* weight of e ji in P */ 17: end for 18: end for 19: end for 20: Return A edge e ij in the aggregate graph, we label the edge with the weighted sum of the votes for the relation j is preferred over i . The process is outlined in Algorithm 2. Steps 10 to 17 compute the weighted votes. If weight of an edge is 0 . 5 in G , then the vote of v is split equally between both the directions.

One can view the total weighted votes for an edge as the confidence of the algorithm on the preference relation induced by that edge. This final aggregated graph repre-sents the confidence on different possible preference relati ons based on the expert opinions. The expert weights are indica-tive of the similarities of the experts with the target user. In other words, more importance is given to the better experts during the construction of the aggregate graph. Hence, we can expect the aggregate graph to be biased towards the preference of the target user. Time complexity of the aggre-gation phase is O ( K ( n tu + n pu ) 2 ). n tu and n pu denote the number of available and test items for the target user u , and K denotes the number of experts considered for this phase.
There are items in the aggregate graph for which we know the ratings. We want to predict the ratings for the remaining items. While assigning the ratings, we want to be consistent with the preference relations as much as possible, and also b e close the target user X  X  rating profile. We view the problem as a back-edge minimization problem . If items i and j have ratings k and l respectively where k &gt; l , and e ij  X  E , then we say that e ij is a back-edge . The cost of this back-edge is w ij . Our goal is to assign ratings so as to minimize the total cost of back-edges. It should be noted that we are concerned about the edges which connect a test item to another test item or an item already rated by the target user. Edges between two items rated by the target user do not play any role in this rating recovery phase.

Suppose e ij  X  E and i has a rating k . Then we can assign to j any rating l  X  k without changing the weighted number of back edges . i.e., the problem has multiple solutions. To address this scenario, we add a penalty term in the objective function. This penalty term aims to keep the ratings closer to the average rating assigned by the user. Hence, the goal is to minimize the following objective function: Where, M is the set of test items for this target user. | M | = m . T is the set of items already rated by the target user.
R = { 1 , . . . , R} is the set of possible ratings, where R is the maximum possible rating. x ik = { 0 , 1 } . Value is 1 only if i  X  X  predicted rating is k . Otherwise x ik is 0.

I (  X  ) is an indicator function that evaluates to true only if the predicate  X  is true. Otherwise the value is 0. w ij is weighted vote in favor of the edge e ij . w ii = 0,  X  i .  X  is the average rating of the target user.

C  X  0 is a penalty parameter. Higher value of C allows smaller deviation from the average rating assigned by the user. r a is user-provided rating of a .

The first term of the RHS of Equation 3 involves the edges that are between two items unrated by the target user. The second term considers the edges between a rated and an unrated item. The third term is used to give importance to the rating profile of the target user. The solution to the above equation gives the predicted ratings for all the test items together.

An important observation from the above formulation is that for users who have given same rating (say r ) to all the items, the solution will predict rating r for all the remaining items. This is because the value of the objective function becomes minimum for this assignment. A prediction where all the items are given same rating may not be useful for the target user as it provides almost no information to him. One way to address this problem is to replace the boolean function I ( k &lt; l ) by a three-state function  X  kl that assumes three different values based on the three possible ordering relations between k and l . We now provide a framework for rating assignment from weighted preference graphs with a view to minimizing the weighted number of back-edges . The mathematical formulation is given below. In Equation 7, value of c is between 0 and 1. Definitions of remaining symbols are already given. Equation 8 re-laxes the constraint x ik  X  X  0 , 1 } . We can treat the values { x i 1 , . . . x i R } as probability values and select a rating k for i based on these probabilities. The optimization problem involves R n pu free variables and n pu constraints, where n is the number of items for which ratings are to be predicted for the target user u .
We used the Movielens data set (downloaded from http:// www.grouplens.org/node/73#attachments) for our experi-ments. The dataset contains ratings of 943 users for 1682 movies (items). Each user has rated at least 20 movies. The rating scale is from 1 to 5.
 We used Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as the evaluation metrics for our ex-periments. Formula for computing MAE and RMSE are given below.

MAE = In the expressions above, r ui is the actual rating that u has given to i , and  X  r ui is the predicted rating. N is the total number of test ratings. Both MAE and RMSE attempt to measure the prediction error. RMSE is considered as a stronger measure than MAE as larger prediction errors are penalized more in it. For both the metrics, smaller value indicates better performance. Figure 3: Distribution of number of ratings for items in Movielens dataset The dataset has many items that are rated by fewer users. We concentrate on predicting the ratings for such iems. Fig-ure 3 shows that number of items that are rated by many users is small, whereas most of the items have moderate or fewer number of ratings.
We compared the performance of our algorithm with four existing approaches: User based Pearson Correlation (UPCC ), Item based Pearson Correlation (IPCC), Random Walk Rec-ommender (RWR) [19] and Somers Coefficient based algo-rithm [11].
 Pearson Correlation based Methods (UPCC and IPCC): User based Pearson Correlation (UPCC) algorithm assigns weights to the training users based on similarities of their ratings with that of the target user. For calculating the similarity between target user u and another user v , first the set of items ( I uv ) that both u and v have rated is selected. UPCC then calculates the similarity weights usin g the following formula where  X  r u and  X  r v denote the average ratings for users u and v respectively. All the summations are over the set of common items ( I uv ) rated by u and v . Assigning weights following Equation 9 may give high weights to users v who have very few items in I uv and those ratings match with the ones given by u . Recent studies suggest giving importance to the size of I uv while assigning the weights [13, 14]. Following that idea, for a target user u , we did not compute weights for every other user v . We computed w P C uv only if v is one of the top 20% users in terms of the size of I uv .
After computing similarity weights, top-K users with max-imum weights are selected as experts. Suppose u is a test user and i is a corresponding test item. Let  X  u be the set of experts for u who have rated i . The predicted rating  X  r computed as:
If  X  u is empty, i.e. no expert has rated the test item i , then average rating of the user is output as the prediction.
IPCC works in analogous way. It finds out items similar to the test item, and assigns similarity weights to them. We used top 200 experts for rating prediction using IPCC.
Somers coefficient based approach (Somers): Pear-son correlation assigns similarity weights based on simila ri-ties between absolute ratings given by users. Somers coeffi-cient considers relative rating assigned to items by differe nt users. Suppose users u and v both have rated an item i . The item i is called concordant if r ui  X   X  r u and r vi  X   X  r positive or both negative, and discordant if one is positive and the other is negative. If either of the quantities is zero , or if at least one of u and v has not rated i , a tie is said to have occurred. If N is the number of items, C , D and T are the numbers of concordant, discordant and tied ratings, then Somers coefficient [11] between u and v is computed as: w uv = C  X  D N  X  T . These weights are then used in Equation 10 for predicting the ratings.

Random Walk Recommender (RWR): RWR uses a finite length random walk based method for rating predic-tion [19]. The items are represented as nodes of a graph; edge labels denote similarity between items. It allows sim-ilarity information to propagate in transitive manner. It first determines the rank scores for the test items, and then scales the scores in the range of the rating scale. RWR has two parameters, length of the random walk (  X  ), and teleport probability (  X  ). Values of these parameters are determined empirically. As the rank scores are scaled to produce the final ratings, performance of the algorithm is heavily depen -dent on the value of these parameters. We performed some limited experiments, and found  X  = 0 . 9 and  X  = 0 . 4 to give best results for those experiments. We then fixed these values for the experiments for comparative evaluation. Our preference relation based approach (PrefAggr): We use the techniques described in Sections 3 and 4 for rat-ing prediction. In Phase 1, we set the value of weight update parameter  X  as 0 . 3. For Phase 2, value of the penalty pa-rameter C was fixed at 0 . 1. If user X  X  average rating is a , and average rating for the test item is b and a 6 = b , then we set x ia and x ib as 0 . 6 and 0 . 4 respectively in the initial solution. Otherwise x ia was set to 0 . 8. Also, instead of waiting for the QP solution to converge, we can stop the solver after a fixed small number (say 50 or 100) of iterations.
For the first set of experiments, we randomly selected 500 users from the Movielens collection as training users. Re-maining users were included in the test set. Each test user X  X  ratings were randomly split into two equal sized sets -ob-served items and held-out items. Ratings for the held-out items were to be predicted. In order to investigate the ef-fect of data sparsity on the performance of our suggested method, we randomly removed some ratings from the initial observed and held-out sets. Similar approaches are used in [15, 18] for experimental evaluation. We define the sparsity of the trimmed dataset using two parameters: user sparsity and item sparsity . User sparsity denotes the maximum num-ber of items rated by test users in the observed set (e.g. less than 10, 20, 30, or 40 per item). Item sparsity denotes the number of users who have rated test items in the held-out set. For example, item sparsity of 20 means that no item from the held out set was rated by more than 20 users in the training or observed set.
Table 4 compares the MAE and RMSE values for item sparsity = 40. Fixing the item sparsity at 40, we vary the user sparsity to 10, 20, 30 and 40. Table 4 indicates that the predictions obtained using our algorithm is superior to that obtained using the competing methods. Similar obser-vation can be made from Table 5, which shows the results corresponding to item sparsity = 70. However, as the data becomes more dense, other methods start doing better than PrefAggr . This behavior is depicted in Figure 4(a) and Figure 4(b), which compare the percentage improvements in MAE and RMSE values respectively over the best base-line for that configuration. For most of the cases, UPCC turned out to be the best alternative. On the other hand, these figures also show that PrefAggr does very well in the sparse region.
In the second set of experiments, we tried to predict the ratings for the items which have very few observed rat-ings. The training set is used to learn the similarity weight s. These weights are then used to predict the ratings for the test items which are rated by at most a fixed number of users (values used in our experiments: 30, 40, 50, 60 and 70). Such items cover a significant portion of the test data. For example, more than 30% of the original test set is cov-ered if we set item sparsity to 70.
Experimental results for this task are displayed graphi-cally in Figure 5. Due to lack of space, we report the RMSE values only. If we go towards left along x-axis of Figure 5, error rates for the competing methods methods increase rapidly. This rate of increase of error is much less for our preference relation based approach. The results clearly in -dicate the superiority of our algorithm over the rating base d Figure 5: RMSE comparison for different item spar-sities schemes when the number of available ratings for the items are small.
As the number of experts is increased, we expect the pre-dictions to be better. However, after sometime, the predic-tion accuracy will almost saturate, or may start decreasing . This happens because an attempt to expand the neighbor-hood may result in adding some users as experts who are not actually similar to the target user. Input from such users do not give additional information to the prediction algorith m, and in some cases, they might influence the prediction task adversely. As it can be seen from Figure 6, the same be-havior is depicted by our algorithm. Value of C was chosen to be 0 . 5. For our experiments, MAE and RMSE values go on decreasing as neighborhood size increases from 10 to 60. After that, the curves become almost horizontal.
In the rating based approaches, rating for a particular item is influenced by ratings for that item only (we do not consider item attributes in this work). In our preference re -lation based approach, rating for an item can be influenced by other items also. For example, suppose item b is rated by few users, whereas a has many ratings. Also suppose there are few experts who liked b more than a . Then, we can pre-Figure 6: Change in RMSE with increase in neigh-borhood size dict a rating for a confidently, and some of that confidence can be transferred to the rating that we predict for b via the preference edges from a to b . However, as the number of available ratings for an item increases, other methods star t doing better. This indicates that it may not be wise to ig-nore the available ratings altogether, specially when high er number of ratings are available. One may design a hybrid algorithm for rating prediction that uses preference relat ion in the sparse zone and absolute ratings in dense zone .
In this work, we have suggested a novel preference rela-tion based scheme for collaborative recommendation. The approach assigns similarity weights based on similarities in preference relations, instead of concentrating on actual r at-ings. Weights are assigned to similar users using a super-vised learning framework. These weights are used to gener-ate an aggregate preference graph for the target user. Rat-ings are recovered by minimizing the total cost of back edges . If rating for a heavily-rated item can be predicted confi-dently, then that confidence can be passed through the pref-erence edges to other items which are not rated by many users. Experimental comparisons seem to support this claim . An extension of this work can be to incorporate item at-tributes for weight assignment and rating prediction. An-other direction can be to design hybrid algorithms that uses preference relation based approach for items having fewer ratings and rating based approach for items having more number of ratings.
 The first author is supported by a PhD Fellowship from Microsoft Research. [1] R. Bell, Y. Koren, and C. Volinsky. Modeling [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] Y.-L. Chen and L.-C. Cheng. A novel collaborative [4] L. M. de Campos, J. M. Fern  X andez-Luna, and J. F. [5] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. [6] Y. Freund and R. E. Schapire. A decision-theoretic [7] T. Hofmann and J. Puzicha. Latent class models for [8] R. Jin, J. Y. Chai, and L. Si. An automatic weighting [9] R. Jin, L. Si, and C. Zhai. Preference-based graphic [10] T. Kamishima. Nantonac collaborative filtering: [11] N. Lathia, S. Hailes, and L. Capra. Private distributed [12] N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented [13] H. Ma, I. King, and M. R. Lyu. Effective missing data [14] M. R. McLaughlin and J. L. Herlocker. A collaborative [15] Y. Moshfeghi, D. Agarwal, B. Piwowarski, and J. M. [16] D. Pennock, E. Horvitz, S. Lawrence, and C. L. Giles. [17] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox: [18] J. Wang, A. P. de Vries, and M. J. T. Reinders. [19] H. Yildirim and M. S. Krishnamoorthy. A random
