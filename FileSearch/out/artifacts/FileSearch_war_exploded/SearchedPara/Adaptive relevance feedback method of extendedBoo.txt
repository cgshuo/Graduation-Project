 1. Introduction
Many relevance feedback methods have been studied ( Ide, 1971; Rocchio, 1971; Salton &amp; Buckley, 1990 ). The main idea of relevance feedback is to improve the retrieval performance by reformulating the queries based on users  X  judgments of relevance of the retrieved documents. They are mostly based on the vector model for information retrieval. As an alternative to the conventional Boolean model, the vector model is developed to represent documents and queries as vectors without using Boolean operators. How-ever, since the structure provided by the Boolean operators is absent, the retrieval methodology in the vec-tor model is not compatible with the conventional Boolean method. To apply the advantages of the vector model to the Boolean model, Salton, Fox, and Wu (1983) proposed the P-Norm extended Boolean model. Many other researchers also have tried to extend the Boolean model and have proposed various Extended Boolean models such as Fuzzy Set, Waller-Kraft, Paice, and Infinite-One ( Bookstein, 1980; Paice, 1984; tended Boolean models using mathematical properties. He found that some models do not satisfy these properties but P-Norm model satisfies all the mathematical properties.

In the extended Boolean model, the relevance feedback implies not only that new query terms must be identified and re-weighted, but also that the terms must be connected with Boolean And/Or operators prop-erly. A number of attempts have been made in the past to develop adequate methods for Boolean query terms for query reformulation. The second part is to generate a Boolean query using these terms. For exam-ple, Salton et al. proposed a relevance feedback method for the extended Boolean model, called the DNF (disjunctive normal form) method ( Salton et al., 1984, 1985 ). Although this method is able to overcome some problems of the earlier method ( Dillon et al., 1983 ) using a parameter to show how many documents users or the system do not correctly compute the number of retrieved documents, this method can not be this method may miss important Boolean clauses or add too many unimportant Boolean clauses in order to just meet the parameter value. Furthermore, these methods mainly focused on generating Boolean queries but did not concern about re-weighting query terms.

In this paper, we propose an adaptive relevance feedback method which can cope with the above prob-lems of the earlier methods of the extended Boolean model. The intuitive idea behind the proposed method information need. We can then split them into different groups using clustering methods. Each group may 1971 ), all groups are different from each other but documents in a group have common features. If we sup-pose that common features denote a group and these are common terms in a group, it seems reasonable to get Boolean queries with the conjunction of terms for each group. In Boolean information retrieval models, we can reformulate the Boolean query. In addition, we illustrate that the query can be further improved by adjusting the weight values with the same users  X  relevance feedbacks. For this purpose, we propose a neural network learning method based on the P-Norm based extended Boolean model. Using this method, we de-velop a way to transform a P-Norm based extended Boolean expression into a neural network.
The rest of this paper is organized as follows. In Section 2, we briefly describe the P-Norm based ex-tended Boolean model and relevance feedback methods of the Boolean models. We present how to refor-mulate the Boolean queries using the hierarchical clustering techniques in Section 3. In Section 4, we propose a neural network model in which the weighting values used in the extended Boolean model can be adjusted. In Section 5, we show experiment results conducted on DOE collection in TREC 1 and the
Web TREC 10 collection. 1 2. Related works
The Boolean model was the first operational Information Retrieval model. In this model, documents are indexed with a set of terms, and queries are terms combined with the conjunctive (And), disjunctive (Or), or negation (Not) operators. The Boolean system is designed to retrieve all documents which exactly match the user  X  s query formula. Although the Boolean system has been popular in operational situations, the sys-tem has been criticized for several reasons ( Salton et al., 1983 ): (1) the construction of a query formulation is too difficult for the user, (2) it does not support ranked output in any order of presumed importance to the user, (3) the size of the retrieved documents is difficult to control, (4) there is no provision for term weighting either the documents or the queries.

In order to solve these problems, many researchers have tried to extend the Boolean model and in turn have proposed various Extended Boolean models such as Fuzzy Set, Waller-Kraft, Paice, P-Norm, and Infi-after some researchers analyzed these models, they found some problems. Lee (1994) analyzed the behavioral aspects of these extended Boolean models using mathematical properties. He found that some models do not satisfy these properties but the P-Norm model satisfies all the mathematical properties. In contrast to other models, the P-Norm and the Infinite-One support query weighting schemes. Thus, Lee stated that the P-Norm model is more suitable for achieving high retrieval performance than any other extended Bool-ean models. It is for this reason that we use the P-Norm based extended Boolean model in our research.
In this section, we first describe the P-Norm based extended Boolean model. We then briefly explain the relevance feedback methods of the Boolean models, especially the DNF method that Salton et al. (1984, 1985) proposed for the extended Boolean model and investigate the problems of this method. 2.1. P-Norm based extended Boolean model Salton et al. proposed an extended Boolean model in order to integrate advantages in the conventional in the document space. The query-document similarity is measured by using the L documents and these points according to the type of the query. For AND-query, the similarity value is mea-sured as the complement of the distances between the (1,1, ... ,1) point and documents. On the other hand, the similarity value for OR-query is computed as the distances between the (0,0, ... ,0) point and docu-ments. In addition, this model supports the weighted-query terms reflecting the importance of the individ-ual terms in the query. In what follows, we show the general P-norm based extended Boolean model.
Consider a set of terms t 1 , t 2 , ... , t n and let a i ( a , a 2 , ... , a n ), where 1 6 i 6 n and 0 6 a i 6 1. Suppose that an OR-query is given as Q
OR p ( q pose that an AND-query is given as Q AND( p ) = AND p ( q
In this extended Boolean model, when p = 1, we can obtain
In this case, the similarity between queries and documents can be computed by the inner product between 1983 ). When p = 1 and all query term weights are equal to 1, we can obtain
Similarly,
By varying the value of p between 1 and 1 , it is possible to obtain an intermediate system between the pure vector model ( p = 1) and the conventional Boolean retrieval system ( p = 1 ). 2.2. Relevance feedback methods in extended Boolean model
A number of attempts have been made in the past to develop adequate methods for an extended Boolean Boolean query using these terms.

Dillon et al. (1983) proposed a method of improving Boolean queries using relevance feedback. First, for retrieval is computed based on the occurrences of the term in the relevant, or nonrelevant previously re-trieved documents. These terms are ordered according to their prevalence weights and in turn segmented into a series of floors as follows:
Second, terms with a very high term prevalence measure(prev &gt; f(1)) are OR-ed together. Terms in the second group (prev &lt; f(1) and prev &gt; f(2)) are AND-ed in pairs. These pairs are then OR-ed together.
Terms with a very negative measure are NOT-ed, and other terms are ignored. However, this method had the following problems: (1) it does not use terms in initial queries, (2) there are the problems inherent in negative relevance feedback, (3) it is hard to control the number of Boolean clauses for a reformulated query, (4) it is hard to get effective floor values.

In order to solve these problems, Salton et al. provided a relevance feedback method for the P-Norm based extended Boolean model, called the DNF (disjunctive normal form) method. The DNF method con-single term or a conjunction of several terms connected by And operator (  X  ). To decide the good term clauses, two kinds of measures are used: (1) the relevance weight of clause, which expresses the degree to which the clause is likely to be useful in retrieving relevant documents from the collection; and (2) the expected posting frequency, which represents the approximate number of documents that the clause may be expected to retrieve. The relevance weight of clause c is computed as follows: where R is the number of relevant retrieved documents, qcount is the parameter which adjusts the occur-rence characteristics of the query terms (in practice a qcount of 2 is good), r collection, we can compute expected posting frequencies for t, st, and rst as follows: of terms r , s , t , respectively and N is the total number of documents in the collection.
The second process is to generate an extended Boolean query in a disjunctive normal form (DNF) with the properly chosen clauses as follows. Suppose that the reformulated query is expected to retrieve approx-imately T documents, where T is a parameter specified by the user or the system. Among the constructed number of documents to be expected by the chosen clauses reaches near to T . By  X  X  X ring X  X  all chosen clauses, we can reformulate the query.

Although this method overcomes some problems of the earlier method ( Dillon et al., 1983 ) using a parameter ( T ) to show how many documents are desired, there were still some problems. First of all, it of retrieved documents, this method can not be applied to generating the reformulated queries. Even if users can correctly estimate the parameter value, this method may miss important Boolean clauses or add too many unimportant Boolean clauses in order to just meet the parameter value. If the expected num-ber of documents ( T ) is small, it might occur that most of posting frequencies of terms are greater than T and then important clauses are hardly chosen. Second, if T is large, it might occur that unimportant clauses can be selected since most of posting frequencies of terms are less than T . In our experimental tests de-scribed in Section 5, we will show these phenomena.
 3. Hierarchical clustering relevance feedback method
We propose a new relevance feedback method of the P-Norm based extended Boolean model using hier-junctive form of conjunctions of terms. If we suppose that the conjunctions denote different concepts, it seems reasonable to get the concepts from the retrieved relevant documents and to reformulate an extended Boolean query with them as following ideas.

If we assume that a query q is a conjunction of terms and two documents D lists of terms, we think of several relations between the query q and two documents D state that the query q in (a) is a right query which makes D
By clustering retrieved relevant documents, we detect groups that reduce the inter-group similarity and that represents user  X  s information need in the group using the above idea. By  X  X  X ring X  X  such conjunctions, we can generate a Boolean query in a form of DNF.

From above ideas, our method consists of two processes. The first process is to cluster the retrieved rel-evant documents by hierarchical clustering. This process constructs a tree structure (a dendrogram) of doc-ument clusters, called a cluster tree. The second process is to generate an extended Boolean query in a form of DNF using the generated cluster tree. 3.1. Generating cluster tree
In order to generate a cluster tree, we performed a divisive hierarchical clustering which starts by placing all relevance feedback documents into a single group and then dividing this group into two subgroups iter-atively. In general, a hierarchical clustering method needs to determine when the clustering process stops.
We use the depth of a cluster tree and the minimum number of documents in a node for this decision. If the depth of a cluster tree is larger, we can get more clusters. On the other hand, if the minimum number of assume that the depth of a cluster tree is m . The maximum number of clusters is 2 increases, the dividing process might stop before reaching the given depth m and the number of clusters might decrease in the end. In addition, a divisive hierarchical clustering method needs a way to divide a group into two subgroups. Generally a divisive hierarchical clustering method takes two objects that are farthest away in a group and uses them as seed points for two subgroups and assigns all objects in the group into the new subgroup that has the closest seed. However, based on our idea, we select a dominant term which can represent the group well and split the group into two subgroups using the dominant term.
One group consists of documents, including the dominant term. The other group consists of documents, excluding the term. In short, this process is summarized in Fig. 2 .

For example, given a set of retrieved relevant documents, we begin with a cluster tree with a root node the above algorithm. Each term t i in nodes is a dominant term which represents its node. To select a dominant term, we take the current node into account and use the well known term selection methods in relevance feedback: Salton  X  smethod( Salton et al., 1985 ), F4MODIFIED method ( Efthimis, 1996; Robertson Salton  X  s method is to select terms using Eq. (3) to compute relevant weight in DNF method. The F4MODIFIED method ( Robertson, 1986 ) is a modification to the F4point-5 formula ( Robertson &amp; Sparck-Jones, 1976 ). This method takes into consideration the addition of new terms to the original query.
The formula is vant documents (from the sample R ) assigned to term t .
Porter and Galpin proposed a method with the following scheme to rank terms for query expansion ( Porter &amp; Galpin, 1988 ): where r , R , n , N are defined the same way as they are defined in the F4MODIFIED method. 3.2. Generating extended Boolean query in DNF
In order to generate an extended Boolean query from a cluster tree, we first need to define a group (clus-ter) in a cluster tree. For example, given a cluster tree in Fig. 3 , we can represent all terminal nodes
T , T 2 , ... , T 8 in conjunction of common terms from left to right order as the following. concept of terminal node T 2 includes the concept of terminal node T
T in a conjunction with dominant terms that are on the path from the root to the parent node. Accordingly, we obtain four groups from the cluster tree of Fig. 3 . These groups can be represented as the following:
Then an extended Boolean query in DNF can be obtained as below:
The procedure of generating extended Boolean query is summarized in Fig. 4 . 4. Neural network model for re-weighting terms
In Section 3, we focused on generating DNF Boolean queries based on users  X  relevance feedback. How-ever, in the P-Norm based extended Boolean model, the terms in a clause in the DNF formula are not only weighted but also the clause itself is weighted. In this section, we present a method to re-weight terms and clauses in a P-Norm based extended Boolean query using a neural network learning method ( Choi, Kim, &amp; Raghavan, 2001 ).

The greatest reason for using neural networks in information retrieval systems is due to its learning capa-bility. Many researchers have proposed adaptive IR systems which might continuously update relations be-tween documents and index terms, documents and documents, or index terms and index terms using neural networks ( Belew, 1984; Kwok, 1989; Schutze, David, &amp; Jan, 1995; Wilkinson &amp; Hingston, 1991; Wong, classification to a traditional relevance feedback method (Rocchio relevance feedback) for document rout-ing problems. It showed that the neural network performed better than a query expansion. Wong et al. (1993) proposed a method for computing term associations based on a three-layer feed-forward neural net-work. Nodes in an input layer correspond into document terms while nodes in the hidden layer correspond to query terms. These two layers are fully connected and used to compute term-association matrix by using the perceptron learning algorithm. However, the previous methods are different from our method in the following aspects. First, a majority of previous methods were mainly concerned with the vector model and the probabilistic model while our approach is concerned with the extended Boolean model. Second, a majority of previous methods built networks for all documents and all index terms. The cost of maintain-ing neural networks increases exponentially if the number of documents or index terms increases. As our method constructs the network for just a given DNF Boolean query, such problems never occur. Third, in most of the previous approaches, they introduced their own network model in order to map the retrieval problems into the neural network domain. In our approach, we use an already proven neural network model in terms of its performance.

The following two subsections describe how to construct the multi-layered perceptron corresponding to a P-Norm based extended Boolean expression and suggest its activation functions and learning rule. 4.1. Neural network structure for extended Boolean expressions
Let us consider a P-Norm based extended Boolean query q , for example, q = OR(( t ( c , q 7 ),( c 2 , q 8 )), where c 1 = AND(( t 3 , q 3 ),( t to term t i for i = 1 to 6, and q 7 and q 8 are the corresponding weights to the conjunction c tively. For this expression, we can consider an AND/OR tree
In Fig. 5 , we assume that the input term weights a 1 , ... , a t between document d and query q using Eqs. (1) and (2) . We can rewrite the equations as follows:
In the above equations, w i  X  q p i
Now, we try to transform the AND/OR tree for the query q into a multi-layered perceptron. If we con-sider the network with the same topology of the AND/OR tree, the network will not be a multi-layered perceptron since every net input value in an OR node and an AND node has to be computed as the fol-lowing Eqs. (8) and (9) :
Generally, a net input value of a neuron in the neural network is computed as summing the input signals, ceptron with the same topology of the AND/OR tree uses the p powered functions to compute net input values. In order to resolve this problem, we introduce an I-OR node which is able to calculate the powered input signal before computing the net input value of an OR node and introduce an I-AND node which is able to calculate the powered input signal before computing the net input value of an AND node. For example, we can transform the AND/OR tree in Fig. 5 into the network like Fig. 6 .

In Fig. 6 , the corresponding weight w i is computed as in Eq. (10) . For the node i , net input value h defined in Eq. (11) . where w ij is the weight from the node j to the node i and, a above, we can get the activation functions for OR and AND nodes from Eqs. (6) and (7) as follows:
We can also define the activation functions for I-OR and I-AND nodes from Eqs. (8) and (9) as follows: 4.2. Activation function and learning rule
From a mathematical point of view, the functions in Eqs. (12) and (13) , which have 1/ p in the exponent, are exactly the correct choices to be used as the activation functions for the proposed neural network. How-the learning process results in convergence. Therefore, we use an approximation of the 1/ p powered func-hyperbolic tangent sigmoid defined as follows:
We use above b F  X  x  X  , instead of x 1/ p , to approximate the activation functions for OR nodes and AND nodes, respectively, as follows:
For the learning process, we use the back-propagation learning rule ( Lippmann, 1987 ). Its derivation is fairly straightforward. The error measure E is defined as the total quadratic function at the output nodes:
D w where c is the learning rate. We can write
From Eq. (11) , we see that the second factor is
When we define the error signal we can write D w ij : given by to which it directly connects and the weights of those connections. Therefore, the error signal is given by where k is over all nodes in the layers above node i . 5. Experiments and results
For evaluating our proposed method, we conducted two experiments. In the first experiment, we show that Salton  X  s DNF method has some problems. In the second experiment, we evaluate the effectiveness of the adaptive relevance feedback method in the extended Boolean model. For these experiments, we used two data sets: the DOE (Department of Energy) collection in the TREC 1 and the Web TREC 10 collec-tion. The DOE collection is a relatively small data set and contains about 220,000 documents. Ten topics selecting these topics was that these topics had more relevant documents than other topics. The Web TREC 10 collection is a relatively large data set and contains over 1,600,000 documents. For this collection, we used fifty topics: topics 501 X 550.

To evaluate the retrieval performance, it is common to use the average recall-precision measure to com-1.0 in steps of 0.1. Recall is measured as the ratio of the relevant documents retrieved to the number of relevant documents in the collection. Precision is measured as the ratio of the relevant documents retrieved to the number of documents retrieved. Although the average recall-precision measure is widely used in information literature, this measure encounters certain problems in relevance feedback methods ( Chang, evant retrieved documents used to reformulate the queries to the top of the document ranking. This ranking effect artificially improves the average recall-precision but does not show how much the relevance feedback improves the retrieval of unseen relevant documents.

In our experiments, we used the average recall-precision to evaluate overall retrieval performance and used the residual ranking measure which had been originally suggested by Ide and investigated by Chang et al. (1971) to evaluate the feedback effect. In this method, all relevant and nonrelevant documents which are seen for the relevance feedback are removed from the collection before evaluation. After the relevance fect of feedback on the unseen relevant documents. 5.1. Experiment 1
The Salton  X  s DNF method generates the Boolean queries using a parameter ( T ) to show how many doc-uments are desired. Salton and his colleagues set the number of desired documents ( T ) to 100 because both users and the DNF method do not know the correct number of retrieved documents. This potentially shows that the DNF method is greatly influenced by an estimated number of desired documents. In order to show the problems of the DNF method, we applied various estimated number of retrieved documents to the
DNF method: 100, 500, 1000, 5000, 10000. And furthermore, we assumed that the correct number of de-sired documents of a topic is the total number of relevant documents of the topic in the given collection.
The average numbers of relevant documents on the DOE collection and the Web TREC 10 collection were 120 and 1,408, respectively. We used p = 2.0 as a standard for the P-Norm extended Boolean system. We used the top 100 retrieved documents for relevance and qcount = 2 for the DNF method.

Graph 1 shows the average number of terms and clauses and Graph 2 shows the average recall-precision on the two document collections as various Ts were employed. The correct number of the desired docu-ments of a topic is designated as CorrectNum .
From Graphs 1 and 2 , we found the following facts: (a) As T grows larger, the average numbers of terms and clauses also grow. (b) The retrieval performance becomes worse due to the generation fault clauses when T is large enough. (c) In T :100 and T :500, the DNF method improve on the DOE collection but gets worse on the Web (d) In CorrectNum , the DNF method has some improvement on the DOE collection but gets worse on
Furthermore, we found that when the number of the desired documents ( T ) is small enough, an ex-panded query and an initial query become the same. As a consequence, we can state that the DNF method does not properly reformulate queries because of the estimated the number of the retrieved documents ( T ). 5.2. Experiment 2
In this experiment, we evaluated our adaptive relevance feedback method and compared with the Sal-ton  X  s DNF method. First of all, we conducted the experiments about the depth and the minimum number of documents in a node. Second, we examined the effect of the number of iteration on our neural network model. Lastly, we compared our proposed method with the Salton  X  s DNF method using parameter values acquired from the experiments. We used following parameter values for this experiment.

We used p = 2.0 for the P-Norm extended Boolean system such as experiment 1. We used qcount =2 and the estimated number of retrieved documents ( T ) = 100 for the DNF method. We used various depths (3 X 6) and minimum numbers (3 X 6) for the experiment. We also used the Salton  X  s term selection method as described in Section 3.1. And we used a = 1.6 for the approximation of the 1/ p powered func-tions and the learning rate c = 0.0001 for the back-propagation learning rule. We used the top 100 re-trieved documents for relevance. We got the results of the experiments using those parameter values as shown in Graphs 3 X 5 .
Graph 3 shows the results of the experiment on various depths. On the DOE collection, average recall-precision and residual ranking on depth 6 is higher than others. On the Web TREC 10 collection, average recall-precision on depth 6 is higher than others and residual ranking on depth 3 is higher than others. The Graph 1 shows that overall result is good on the depth 6.

Graph 4 shows the results of the experiment on various minimum numbers of documents in a node. On the DOE collection, average recall-precision on minimum number 4 is higher than others and residual rank-ing on minimum number 5 is higher than others. On the Web TREC 10 collection, average recall-precision on minimum number 3 is higher than others and residual ranking on minimum number 4 is higher than others. The Graph 2 shows that overall result is good on the minimum number 4.
Graph 5 shows the results of the experiment on the various numbers of iteration on our neural network model. On the DOE collection, average recall-precision and residual ranking on the number of iteration 100 is higher than others. On the Web TREC 10 collection, average recall-precision on the number of iteration 50 is higher than others and residual ranking on the number of iteration 100 is higher than others. Graph 3 shows that overall result is good on the number of iteration 100.

We found that the depth of cluster tree 6, the minimum number of documents 4, and the number of iter-ation 100 were generally good from the above experiments. We got the results of the experiments as shown in Graph 6 using those parameter values. In this graph, DNF is the result of the Salton  X  s DNF method.
HCR1, HCR2, and HCR3 are the results of our proposed method for the query reformulation with three term selection methods: the Salton  X  s method, the Porter  X  s method, and the F4MODIFIED method, respec-tively. DNF_NN, HCR1_NN, HCR2_NN, and HCR3_NN are the results from applying our re-weighting query terms method to DNF, HCR1, HCR2, and HCR3, respectively. Graph 4 shows the results as follows: (a) Others except HCR3 outperform DNF on both DOE collection and Wet TREC 10 collection. (b) Applying our neural network model to the result of DNF, all except the average recall-precision on (c) Applying our neural network model to the result of hierarchical clustering method, HCR2_NN is not
From what has been shown above, we can summarize as follows. As overall results, our proposed meth-od outperforms the DNF method regardless of the size of the collection. Especially, when we see that the result of residual ranking measure is better than result of average recall-precision, our method has a great performance enhancement of relevance feedback. Furthermore, the depth and the minimum number of the cluster tree do not have big effect on the performance of our method. However, term selection methods heavily affect the performance. And it is hard to get correct and fixed number of iteration for the neural network model. 6. Conclusions
In this study, we develop a new relevance feedback method to select query terms and reformulate the expanded query using hierarchical clustering techniques in the P-Norm based extended Boolean model.
Furthermore, we develop a formal way to map a P-Norm based extended Boolean expression into a multi-layered perceptron in order to re-weight the weight values of query terms with users  X  relevance feed-back information. From the experiments, we reach the following conclusions. (1) We find that our proposed methods are better than the DNF method which is well known for a rel-(2) Multi-layered perceptron and back-propagation rule can be used for adjusting the term weights in (3) The proposed neural network learning method obtains improved retrieval performance when this (4) Whenever initial queries are not well defined, they are usually better reformulated by our proposed
A more remaining problem is to develop more effective methods that facilitate the term selection and the reformulation of the query in the extended Boolean model. And it is another problem to get valuable num-ber of iteration for the neural network model.
 Acknowledgement This work is performed as a part of National Research Laboratory supported by Ministry of Science and Technology in Korea (M10302000087-03J0000-044000).
 References
