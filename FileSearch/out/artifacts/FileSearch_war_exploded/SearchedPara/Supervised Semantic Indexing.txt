 In this article we propose Supervised Semantic Indexing (SSI), an algorithm that is trained on (query, document) pairs of text documents to predict the quality of their match. Like Latent Semantic Indexing (LSI), our models take ac-count of correlations between words (synonymy, polysemy). However, unlike LSI our models are trained with a super-vised signal directly on the ranking task of interest, which we argue is the reason for our superior results. As the query and target texts are modeled separately, our approach is easily generalized to different retrieval tasks, such as online advertising placement. Dealing with models on all pairs of words features is computationally challenging. We propose several improvements to our basic model for addressing this issue, including low rank (but diagonal preserving) represen-tations, and correlated feature hashing (CFH). We provide an empirical study of all these methods on retrieval tasks based on Wikipedia documents as well as an Internet adver-tisement task. We obtain state-of-the-art performance while providing realistically scalable methods.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods; H.3.3 [ Information Search and Retrieval ]: Re-trieval models Algorithms semantic indexing, learning to rank, content matching
In this article we study the task of learning to rank doc-uments, given a query, by modeling their semantic content. Although natural language can express the same concepts in many different ways using different words, classical ranking algorithms do not attempt to model the semantics of lan-guage, and simply measure the word overlap between texts. For example, a classical vector space model, see e.g. [1], uses weighted word counts (e.g. via tf-idf) as a feature represen-tation of a text, and the cosine similarity for comparing to other texts. If two words in query and document texts mean the same thing but are different unique strings, there is no contribution to the matching score derived from this seman-tic similarity. Indeed, if the texts do not share any words at all, no match is inferred.

There exist several unsupervised learning methods to try to model semantics, in particular Latent Semantic Index-ing [11], and related methods such as pLSA and LDA [19, 3]. These methods choose a low dimensional feature rep-resentation of  X  X atent concepts X  that is constructed via a linear mapping from the (bag of words) content of the text. This mapping is learnt with a reconstruction objective, ei-ther based on mean squared error (LSI) or likelihood (pLSA, LDA). As these models are unsupervised, they may not learn a matching score that works well for the task of interest. Su-pervised LDA (sLDA) [2] has been proposed where a set of auxiliary labels are trained on jointly with the unsupervised task. However, the supervised task is not a task of learning to rank because the supervised signal is at the document level and is query independent.

In this article we propose Supervised Semantic Indexing (SSI) which defines a class of models that can be trained on a supervised signal (i.e., labeled data) to provide a rank-ing of a database of documents given a query. This signal is defined at the (query,documents) level and can either be point-wise  X  for instance the relevance of the document to the query  X  or pairwise  X  a given document is better than another for a given query. In this work, we focus on pair-wise preferences. For example, if one has click-through data yielding query-target relationships, one can use this to train these models to perform well on this task [22]. Or, if one is interested in finding documents related to a given query doc-ument , one can use known hyperlinks to learn a model that performs well on this task [16]. Moreover, our approach can model queries and documents separately, which can accom-modate for differing word distributions between documents and queries. This might be important in cases like matching advertisements to web pages where the two distributions are different, and a good match does not necessarily have over-lapping words.

Learning to rank as a supervised task is not a new subject, however most methods and models have typically relied on optimizing over only a few hand-constructed features, e.g. based on existing vector space models such as tf-idf, the ti-tle, URL, PageRank and other information, see e.g. [22, 6]. Our work is orthogonal to those works, as it presents a way of learning a model for query and target texts by consid-ering features generated by all pairs of words between the two texts. The difficulty here is that such feature spaces are very large and we present several models that deal with memory, speed and capacity control issues. In particular we propose constraints on our model that are diagonal pre-serving but otherwise low rank and a technique of hashing features (sharing weights) based on their correlation, called correlated feature hashing (CFH). In fact, both our proposed methods can be used in conjunction with other features and methods explored in previous work for further gains. We show experimentally on retrieval tasks developed from Wikipedia that our method strongly outperforms word-feature based models such as tf-idf vector space models, LSI and other baselines on document-document and query-document tasks. Finally, we give results on an Internet advertising task using proprietary data from an online advertising company.
The rest of this article is as follows. In Section 2 we de-scribe our method, Section 3 discusses prior work, Section 4 describes the experimental study of our method, and Section 5 concludes with a discussion.
Let us denote the set of documents in the corpus as { d t R
D and a query text as q  X  R D , where D is the dictionary size 1 , and the j th dimension of a vector indicates the fre-quency of occurrence of the j th word, e.g. using the tf-idf weighting and then normalizing to unit length [1].
The set of models we propose are all special cases of the following type of model: where f ( q,d ) is the score between a query q and a given document d , and W  X  R D X D is the weight matrix, which will be learned from a supervised signal. This model can capture synonymy and polysemy (hence the term  X  X emantic X  in the name of the algorithm) as it looks at all possible cross terms, and can be tuned directly for the task of interest. We do not use stemming since our model can already match words with common stems (if it is useful for the task). Note that negative correlations via negative values in the weight matrix W can also be encoded.

Expressed in another way, given the pair q,d we are con-structing the joint feature map: where  X  s (  X  ) is the s th dimension in our feature space, and choosing the set of models:
In fact in our resulting methods there is no need to restrict that both q and d have the same dimensionality D but we will make this assumption for simplicity of exposition. Note that a model taking pairs of words as features is es-sential here, a simple approach concatenating ( q,d ) into a single vector and using f ( q,d ) = w  X  [ q,d ] is not a viable option as it would result in the same document ordering for any query.

We could train any standard method such as a ranking perceptron or a ranking SVM using our choice of features. However, without further modifications, this basic approach has a number of problems in terms of speed, storage space and capacity as we will now discuss.

We analyze both memory and speed considerations. Firstly, this method so far assumes that W fits in memory (un-less sparsity is somehow enforced). If the dictionary size D = 30 , 000, then this requires 3.4Gb of RAM (assuming floats), and if the dictionary size is 2.5 Million (as it will be in our experiments in Section 4) this amounts to 14.5 Terabytes. The vectors q and d are sparse so the speed of computation of a single query-document pair involves mn computations q i W ij d j , where q and d have m and n non-zero terms, respectively. We have found this is reasonable for training, but may be an issue at test time 2 . Alternatively, one can compute v = q &gt; W once, and then compute vd for each document. This is the same speed as a classical vector space model where the query contains D terms, assuming W is dense. The capacity of this model is also obviously rather large. As every pair of words between query and target is modeled separately it means that any pair not seen during the training phase will not have its weight trained. Regular-izing the weights so that unseen pairs have W ij = 0 is thus essential, as discussed in Section 2.3. However, this is still not ideal and clearly a huge number of training examples will be necessary to train so many weights, most of which are not used for any given training pair ( q,d ).

Overall, a dense matrix W is challenging in terms of mem-ory footprint, computation time and controlling its capacity for good generalization. In the next section we describe ways of improving over this basic approach. An efficient scheme is to constrain W in the following way: Here, U and V are N  X  D matrices. This induces a N -dimensional  X  X atent concept X  space in a similar way to LSI. However, it differs in several ways:
Of course, any method can be sped up by applying it to only a subset of pre-filtered documents, filtering using some faster method.
However, the efficiency and memory footprint are as fa-vorable as LSI. Typically, one caches the N -dimensional rep-resentation for each document to use at query time.
We also highlight several other regularization variants, which are further possible ways of constraining W :
Another way to both lower the capacity of our model and decrease its storage requirements is to share weights among features.

In [29] the authors proposed a general technique called  X  X ash Kernels X  where they approximate the feature repre-sentation  X ( x ) with: where h : W  X  { 1 ,..., H} is a hash function that reduces an the feature space down to H dimensions, while maintain-ing sparsity, where W is the set of initial feature indices. The software Vowpal Wabbit 3 implements this idea (as a regression task) for joint feature spaces on pairs of objects, e.g. document/query pairs. In this case, the hash function used for a pair of words ( s,t ) is h ( s,t ) = mod( sP + t, H ) where P is a large prime. This yields where  X  s,t (  X  ) indexes the feature on the word pair ( s,t ), e.g.  X  sharing weights, i.e. constraining W st = W kl when h ( s,t ) = h ( k,l ). In this case, the sharing is done pseudo-randomly, and collisions in the hash table generally results in sharing weights between term pairs that share no common meaning. http://hunch.net/~vw/ Table 1: Correlated Feature Hashing: some exam-ples of 1-grams along with their top 5 matches (from the most frequent 30,000 words) by DICE coefficient generated from Wikipedia. Table 2: Correlated Feature Hashing: some exam-ples of 2-grams along with their top 5 matches (from the most frequent 30,000 words) by DICE coefficient generated from Wikipedia.
We thus suggest a technique to share weights (or equiva-lently hash features) so that collisions actually happens for terms with close meaning. For that purpose, we first sort the words in our dictionary in frequency order, so that i = 1 is the most frequent, and i = D is the least frequent. For each word i = 1 ,..., D , we calculate its DICE coefficient [30] with respect to each word j = 1 ,..., F among the top F most frequent words: where cooccur( i,j ) counts the number of co-occurences for i and j at the document or sentence level, and occur( i ) is the total number of occurences of word i . Note that these scores can be calculated from a large corpus of completely unla-beled documents. For each i , we sort the F scores (largest first) so that S p ( i )  X  { 1 ,..., F} correspond to the index of the p th largest DICE score DICE ( i,S p ( i )). We can then use the Hash Kernel approximation  X   X (  X  ) given in equation (5) relying on the  X  X ashing X  function: This strategy is equivalent to pre-processing our documents and replacing all the words indexed by i with S 1 ( i ). Note that we have reduced our feature space from D 2 features to H = F 2 features. This reduction can be important as shown in our experiments, see Section 4: e.g. for our Wikipedia experiments, we have F = 30 , 000 as opposed to D = 2 . 5 Million. Typical examples of the top k matches to a word using the DICE score are given in Table 1.

Moreover, we can also combine correlated feature hashing with the low rank W matrix constraint described in Section 2.2. In that case U and V are reduced from D X  N dimen-sional matrices to F  X  N matrices instead because the set of features is no longer the entire dictionary, but the first F words.

It is also suggested in [29] to hash a feature  X  i (  X  ) so that it contributes to multiple features  X   X  j (  X  ) in the reduced feature space. This strategy theoretically lessens the consequence of collisions. In our case, we can construct multiple hash functions from the values S p (  X  ) , p = 1 ,...,k , i.e. the top k correlated words according to their DICE scores: where
Equation (6) defines the reduced feature space as the mean of k feature maps which are built using hashing functions using the p = 1 ,...,k most correlated words. Equation (7) defines the hash function for a pair of words i and j using the p th most correlated words S p ( i ) and S p ( j ). That is, the new feature space consists of, for each word in the original document, the top k most correlated words from the set of size F of the most frequently occurring words. Hence as before there are never more than H = F 2 possible features. Overall, this is in fact equivalent to pre-processing our documents and replacing all the words indexed by i with S ( i ) ,...,S k ( i ), with appropriate weights.

One can also use these techniques to incorporate n -gram features into the model without requiring a huge feature representation that would have no way of fitting in memory. We simply use the DICE coefficient between an n -gram i and the first F words j = 1 ,..., F , and proceed as before. In fact, our feature space size does not increase at all, and we are free to use any value of n . Some examples of the top k matches for a 2-gram using the DICE score are given in Table 2.
We now discuss how to train the models we have described in the previous section.
Suppose we are given a set of tuples R (labeled data), where each tuple contains a query q , a relevant document d and an irrelevant (or lower ranked) document d  X  . We would like to choose W such that q &gt; Wd + &gt; q &gt; Wd  X  , expressing that d + should be ranked higher than d  X  .

For that purpose, we employ the margin ranking loss [18] which has already been used in several IR methods before [22, 6, 16], and minimize: This optimization problem is solved through stochastic gra-dient descent, (see, e.g. [6]): iteratively, one picks a random tuple and makes a gradient step for that tuple: W  X  W +  X  ( q ( d + ) &gt;  X  q ( d  X  ) &gt; ) , if 1  X  q Obviously, one should exploit the sparsity of q and d when calculating these updates. To train our model, we choose the (fixed) learning rate  X  which minimizes the training er-ror. We also suggest to initialize the training with W = I as this initializes the model to the same solution as a cosine similarity score. This introduces a prior expressing that the weight matrix should be close to I , considering term cor-relation only when it is necessary to increase the score of a relevant document, or conversely decreasing the score of a non-relevant document. Termination is then performed by viewing when the error is no longer improving, using a validation set.

Stochastic training is highly scalable and is easy to imple-ment for our model. Our method thus far is a margin rank-ing perceptron [9] with a particular choice of features (2). It thus involves a convex optimization problem and is hence related to a ranking SVM [18, 22], except we have a highly scalable optimizer. However, we note that such optimiza-tion cannot be easily applied to probabilistic methods such as pLSA because of their normalization constraints. Recent methods like LDA [3] also suffer from scalability issues.
Researchers have also explored optimizing various alter-native loss functions other than the ranking loss including optimizing normalized discounted cumulative gain (NDCG) and mean average precision (MAP) [6, 5, 7, 33]. In fact, one could use those optimization strategies to train our models instead of optimizing the ranking loss as well.
When the W matrix is constrained, e.g. W = U &gt; V + I , training is done in a similar way to before, but in this case by making a gradient step to optimize the parameters U and V :
U  X  U +  X V ( d +  X  d  X  ) q &gt; , if 1  X  f ( q,d + ) + f ( q,d
V  X  V +  X Uq ( d +  X  d  X  ) &gt; , if 1  X  f ( q,d + ) + f ( q,d Note this is no longer a convex optimization problem. In our experiments we initialized the matrices U and V randomly using a normal distribution with mean zero and standard deviation one.
Feature hashing simply provides a different choice of fea-ture map, dependent on the hashing technique chosen. There-fore, the training techniques described above can be applied in this case as well.
We consider two standard retrieval models: returning rel-evant documents given a keyword-based query, and finding related documents with respect to a given query document, which we call the query-document and document-document tasks.
 Our methods naturally can be trained to solve these tasks. We note here that so far our models have only included fea-tures based on the bag-of-words model, but there is nothing stopping us adding other kinds of features as well. Typical choices include: features based on the title, body, words in bold font, the popularity of a page, its PageRank, the URL, and so on, see e.g. [1]. However, for clarity and simplicity, this paper solely focuses on raw words.
Our models can also be applied to match other types of text pairs. In content matching, one is interested in pairing an online text such as a web page, an email or a chat log with a targeted advertisement. In this case, click-through data can provide supervision. Here, again for simplicity, we assume both text and advert are represented as words. In practice, however, other types of engineered features could be added for optimal performance.
A tf-idf vector space model and LSI [11] are two main baselines we will compare to. We already mentioned that pLSA [19] and LDA [3] both have scalability problems and are not reported to generally outperform LSA and TF-IDF [13]. Moreover in the introduction we discussed how sLDA[2] provides supervision at the document level (via a class label or regression value) and is not a task of learning to rank, whereas here we study supervision at the (query,documents) level. In this section, we now discuss other relevant methods.
In [16] the authors learned the weights of an orthogonal vector space model on Wikipedia links, improving over the OKAPI method. Joachims et al.[22] trained a SVM with hand-designed features based on the title, body, search en-gines rankings and the URL. Burges et al.[6] proposed a neural network method using a similar set of features (569 in total). In contrast we limited ourselves to body text (not using title, URL, etc.) and train on at most D 2 = 900 mil-lion features.

Query Expansion, often referred to as blind relevance feed-back, is another way to deal with synonyms, but requires manual tuning and does not always yield a consistent im-provement [34].

The authors of [17] used a related model to the ones we describe, but for the task of image retrieval, and [15] also used a related (regression-based) method for advert place-ment. They both use the idea of using the cross product space of features in the perceptron algorithm as in equation (2) which is implemented in related software to these two publications, PAMIR 4 and Vowpal Wabbit 5 . The task of document retrieval, and the use of low rank matrices, is not studied.

Several authors [28, 23] have proposed interesting non-linear versions of (unsupervised) LSI using neural networks and showed they outperform LSI or pLSA. However, in the case of [28] we note their method might require considerable computationally expense, and hence they only used a dictio-nary size of 2000. Finally, [31] proposes a  X  X upervised X  LSI for classification. This has a similar sounding title to ours, but is quite different because it is based on applying LSI to document classification rather than improving ranking via known preference relations. The authors of [12] proposed  X  X xplicit Semantic Analysis X  which represents the meaning of texts in a high-dimensional space of concepts by build-ing a feature space derived from the document categories of an encyclopedia, e.g. Wikipedia. In the new space, cosine http://www.idiap.ch/pamir/ http://hunch.net/~vw/ similarity is applied. SSI could be applied to such feature representations so that they are not agnostic to a particular supervised task as well.

Another related area of research is in distance metric learn-ing [32, 21, 14]. Methods like LMNN [32] also learn a model similar to the basic model (2.1) with the full matrix W (but not with our improvements to this model). They constrain during the optimization that W should be a positive semidef-inite matrix. Their method has considerable computational cost. For example, even after a carefull optimization of the algorithm, it still needs 3.5 hours to train on 60,000 exam-ples and 169 features from a handwritten digit classifiction problem. This would hence not be scalable for large scale text ranking experiments. Nevertheless, Chechik et al. com-pared LMNN [32], LEGO [21] and MCML [14] to a stochas-tic gradient method with a full matrix W (the basic model (2.1)) on a small image ranking task and report in fact that the stochastic method provides both improved results and efficiency 6 .
Learning a model of term correlations over a large vocabu-lary is a considerable challenge that requires a large amount of training data. Standard retrieval datasets like TREC 7 LETOR [24] contain only a few hundred training queries, and are hence too small for that purpose. Moreover, some datasets only provide few pre-processed features like page-rank, or BM25, and not the actual words. Click-through from web search engines could provide valuable supervision. However, such data is not publicly available, and hence ex-periments on such data are not reproducible.

We hence conducted most experiments on Wikipedia and used links within Wikipedia to build a large scale ranking task. Thanks to its abundant, high-quality labeling and structuring, Wikipedia has been exploited in a number of applications such as disambiguation [4, 10], text categoriza-tion [26, 20], relationship extraction [27, 8], and searching [25] etc. Specifically, Wikipedia link structures were also used in [26, 25, 27].

We considered several tasks: document-document retrieval described in Section 4.1, query-document retrieval described in Section 4.2. In Section 4.3 we also give results on an Inter-net advertising task using proprietary data from an online advertising company.

In these experiments we compare our approach, Super-vised Semantic Indexing (SSI), to the following methods: tf-idf with cosine similarity (TFIDF), Query Expansion (QE), LSI 8 ,  X  LSI + (1  X   X  ) TFIDF and a margin ranking per-ceptron using Hash Kernels. Moreover SSI with an  X  X n-constrained W X  is just a margin ranking perceptron with a particular choice of feature map, and SSI using hash ker-nels is the approach of [29] employing a ranking loss. For LSI we report the best value of  X  and embedding dimension (50, 100, 200, 500, 750 or 1000), optimized on the training set ranking loss. We then report the low rank version of SSI using the same choice of dimension. Query Expansion
Oral presentation at the (Snowbird) Machine Learn-ing Workshop, see http://snowbird.djvuzone.org/ abstracts/119.pdf http://trec.nist.gov/
We use the SVDLIBC software http://tedlab.mit.edu/ ~dr/svdlibc/ and the cosine distance in the latent concept space. involves applying TFIDF and then adding the mean vector  X  P E i =1 d r i of the top E retrieved documents multiplied by a weighting  X  to the query, and applying TFIDF again. We report the error rate where  X  and E are optimized using the training set ranking loss.

For each method, we measure the ranking loss (the per-centage of tuples in R that are incorrectly ordered), preci-sion P ( n ) at position n = 10 (P@10) and the mean average precision (MAP), as well as their standard errors. For com-putational reasons, MAP and P@10 were measured by av-eraging over a fixed set of 1000 test queries, where for each query the linked test set documents plus random subsets of 10,000 documents were used as the database, rather than the whole testing set. The ranking loss is measured using 100,000 testing tuples (i.e. 100,000 queries, and for each query one random positive and one random negative target document were selected).
We considered a set of 1,828,645 English Wikipedia doc-uments as a database, and split the 24,667,286 links 9 ran-domly into two portions, 70% for training and 30% for test-ing. We then considered the following task: given a query document q , rank the other documents such that if q links to d then d should be highly ranked.

In our first experiments, we used only the top 30,000 most frequent words. This allowed us to compare all methods with the proposed approach, Supervised Semantic Indexing (SSI), using a completely unconstrained W matrix as in equation (1). LSI is also feasible to compute in this setting. We com-pare several variants of our approach, as detailed in Section 2.2.

Results on the test set are given in Table 3. All the vari-ants of our method SSI strongly outperform the existing techniques TFIDF, LSI and QE. SSI with unconstrained W performs worse than the low rank counterparts  X  probably because it has too much capacity given the training set size. Non-symmetric low-rank SSI W = U &gt; V + I slightly outper-forms its symmetric counterpart W = U &gt; U + I . SSI with Diagonal SSI W = D is only a learned re-weighting of word weights, but still slightly outperforms TFIDF. In terms of our baselines, LSI is slightly better than TFIDF but QE in this case does not improve much over TFIDF, perhaps be-cause of the difficulty of this task, i.e. there may too often many irrelevant documents in the top E documents initially retrieved for QE to help.

In our second experiment we no longer constrained meth-ods to a fixed dictionary size, so all 2.5 million words are used. Due to being unable to compute LSI for the full dic-tionary size, we used the LSI computed in the previous ex-periment on 30,000 words and combined it with TFIDF us-ing the entire dictionary. In this setting we compared our baselines with the low rank SSI method W = ( U &gt; V ) n where n means that we constrained the rows of U and V for infrequent words (i.e. all words apart from the most fre-quent n ) to equal zero. The reason for this constraint is
We removed links to calendar years as they provide little information while being very frequent. that it can stop the method overfitting: if a word is used in one document only then its embedding can take on any value independent of its content. Infrequent words are still used in the diagonal of the matrix (via the + I term). The results, given in Table 4, show that using this constraint outperforms an unconstrained choice of n = 2 . 5 M . Figure 1 shows scatter plots where SSI outperforms the baselines TFIDF and LSI in terms of average precision.

Overall, compared to the baselines the same trends are observed as in the limited dictionary case, indicating that the restriction in the previous experiment did not bias the results in favor of any one algorithm. Note also that as a page has on average just over 3 test set links to other pages, the maximum P@10 one can achieve in this case is 0.31, while our best model reaches 0.263 for this measure.
On the full dictionary size experiments in Table 4 we also compare Hash Kernels [29] with our Correlated Fea-ture Hashing method described in Section 2.2.1. For Hash Kernels we tried several sizes of hash table H (1M, 3M and 6M), we also tried adding a diagonal to the matrix learned in a similar way as is done for LSI. We note that if the hash table is big enough this method is equivalent to SSI with an unconstrained W , however for the hash sizes we tried Hash Kernels did not perform as well. For correlated feature hash-ing, we simply used the SSI model W = ( U &gt; V ) 30 k + I from the 7 th row in the table to model the most frequent 30,000 words and trained a second model using equation (6) with k = 5 to model all other words, and combined the two mod-els with a mixing factor (which was also learned). The result  X  X SI: CFH (1-grams) X  is the best performing method we have found. Doing the same trick but with 2-grams instead also improved over Low Rank SSI, but not by as much. Com-bining both 1-grams and 2-grams, however did not improve over 1-grams alone.

In some cases, one might be worried that our experimental setup has split training and testing data only by partition-ing the links, but not the documents, hence performance of our model when new unseen documents are added to the database might be in question. We therefore also tested an experimental setup where the test set of documents is completely separate from the training set of documents, by completely removing all training set links between training and testing documents. In fact, this does not alter the per-formance significantly, as shown in Table 5. This outlines that our model can accommodate a growing corpus without frequent re-training.
 In the above experiments we simply chose the dimension N of the low rank matrices to be the same as the best la-tent concept dimension for LSI. However, we also tried some experiments varying N and found that the error rates are fairly invariant to this parameter. For example, using a lim-ited dictionary size of 30,000 words we achieve a ranking loss 0.39%, 0.30% or 0.31% for N =100, 200, 500 using a W = U &gt; V + I type model. words).
 on all words are included.
 of 30,000 words). The two setups yield similar results. Figure 1: Scatter plots of Average Precision for 500 The addition of the identity term in our model W = U &gt; V + I allows this model to automatically learn the trade-off between using the low dimensional space and a classi-cal vector space model. The diagonal elements count when there are exact matches (co-ocurrences) of words between the documents. The off-diagonal (approximated with a low rank representation) captures topics and synonyms. Using only W = I yields the inferior TFIDF model. Using only W = U &gt; V also does not work as well as W = U &gt; V + I . Indeed, we obtain a mean average precision of 0.42 with the former, and 0.51 with the latter. Similar results can be seen with the error rate of LSI with or without adding the (1  X   X  )TFIDF term, however for LSI this modification seems rather ad-hoc rather than being a natural constraint on the general form of W as in our method.

On the other hand, for some tasks it is not possible to use the identity matrix at all, e.g. if one wishes to per-form cross-language retrieval. Out of curiosity, we thus also tested our method SSI training a dense matrix W where the diagonal is constrained to be zero 10 , so only synonyms can be used. This obtained a test ranking loss of 0.69% (limited dictionary size case), compare to 0.41% with the diagonal.
Training our model over the 1.2M documents (where the number of triplets R is obviously much larger) takes on the order of a few days on standard machine (single CPU) with our current implementation. As triplets are sampled stochastically, not all possible triplets have been seen in this time, however the generalization error on validation data has reached a minimum by that time. We also tested our approach in a query-document setup. We used the same setup as before but we constructed queries by keeping only k random words from query documents in an attempt to mimic a  X  X eyword search X . First, using the same setup as in the previous section with a limited dictionary size of 30,000 words we present results for keyword queries of length k = 5 , 10 and 20 in Table 6. SSI yields similar im-provements as in the document-document retrieval case over the baselines. Here, we do not report full results for Query Expansion, however it did not give large improvements over TFIDF, e.g. for the k = 10 case we obtain 0.084 MAP and 0.0376 P@10 for QE at best. Results for k = 10 using an unconstrained dictionary are given in Table 7. Again, SSI yields similar improvements. Overall, non-symmetric SSI gives a slight but consistent improvement over symmetric SSI. Changing the embedding dimension N (capacity) did not appear to effect this, for example for k = 10 and N = 100 we obtain 3 . 11% / 0 . 215 / 0 . 097 for Rank Loss/MAP/P@10 using SSI W = U &gt; U + I and 2 . 93% / 0 . 235 / 0 . 102 using SSI W = U &gt; V + I (results in Table 6 are for N = 200). Fi-nally, correlated feature hashing again improved over models without hashing.
We present results on matching adverts to web pages, a problem closely related to document retrieval. We obtained proprietary data from an online advertising company of the form of pairs of web pages and adverts that were clicked while displayed on that page. We only considered clicks in position 1 and discarded the sessions in which none of the ads was clicked. This is a way to circumvent the well known position bias problem  X  the fact that links appearing in lower positions are less likely to be clicked even if they are relevant. Indeed, by construction, every negative example comes from a slate of adverts in which there was a click in a lower position; it is thus likely that the user examined that negative example but deemed it irrelevant (as opposed to the user not clicking because he did not even look at the advert).

We consider these (webpage,clicked-on-ad) pairs as posi-tive examples ( q,d + ), and any other randomly chosen ad is considered as a negative example d  X  for that query page. 1.9M pairs were used for training and 100,000 pairs for test-
Note that the model W = U &gt; V with the identity achieved a ranking loss of 0.56%, however this model can represent at least some of the diagonal. ing. The web pages contained 87 features (words) on aver-age, while the ads contained 19 features on average. The two classes (clicks and no-clicks) are roughly balanced. From the way we construct the dataset, this means than when a user clicks on an advert, he/she clicks about half of the time on the one in the first position.

We compared TFIDF, Hash Kernels and Low Rank SSI on this task. The results are given in Table 8. In this case TFIDF performs very poorly, often the positive (page, ad) pairs share very few, if any, words, and even if they do this does not appear to be very discriminative. Hash Kernels and Low Rank SSI appear to perform rather sim-ilarly, both strongly outperforming TFIDF. The rank loss on this dataset is two orders of magnitude higher than on the Wikipedia experiments described in the previous sec-tions. This is probably due to a combination of two factors: first, the positive and negative classes are balanced, whereas there was only a few positive documents in the Wikipedia experiments; and second, clicks data are much more noisy.
We might add, however, that at test time, Low Rank SSI has a considerable advantage over Hash Kernels in terms of speed. As the vectors Uq and V d can be cached for each page and ad, a matching operation only requires N multipli-cations (a dot product in the  X  X mbedding X  space). However, for hash kernels | q || d | hashing operations and multiplications have to be performed, where | X | means the number of non-zero elements. For values such as | q | = 100 , | d | = 100 and N = 100 that would mean Hash Kernels would be around 100 times slower than Low Rank SSI at test time, and this difference gets larger if more features are used.
 Table 8: Content Matching experiments on propri-etary data of web-page/advertisement pairs.

As stated earlier, learning a model from raw word features requires considerably more training data than for models based on hand-designed features. This can be a problem when little data is available for training. However, a model which can benefit from very training sets can also be an ad-vantage, especially in today X  X  web search environment where search engines collect a considerable amount of preference data from user behavior logs. Table 9 reports results ob-tained over different sized training sets. Indeed, one can no-tice that our model succeed in leveraging from large datasets: the more training data available, the lower the ranking loss.
We have described a versatile, powerful set of discrimina-tively trained models for document ranking. Many  X  X earning to rank X  papers have focused on the problem of selecting the objective to optimize (given a fixed class of functions) and typically use a relatively small number of hand-engineered and P@10 metrics. D = 2 . 5 million words).
 Table 9: Influence of Training Set Size over Wikipedia Data: training data varies from 1% to 30% of the links. In all cases, the test data cor-responds to 70% of the links. The model uses the most frequent 30000 words, and has 100 output di-mensions.
 features as input. This work is orthogonal to those works as it studies models with large feature sets generated by all pairs of words between the query and target texts. The chal-lenge here is that such feature spaces are very large, and we thus presented low rank models that deal with the memory, speed and capacity control issues. In fact, all of our proposed methods can be used in conjunction with other features and different objective functions explored in previous work for further gains.

Many generalizations of our work are possible: adding more features into our models as we just mentioned, gener-alizing to other kinds of nonlinear models, and exploring the use of the same models for other tasks such as question an-swering. In general, web search and other standard retrieval tasks currently often depend on entering query keywords which are likely to be contained in the target document, rather than the user directly describing what they want to find. Our models are capable of learning to rank using either the former or the latter. [1] R. Baeza-Yates, B. Ribeiro-Neto, et al. Modern [2] D. M. Blei and J. D. McAuliffe. Supervised topic [3] D. M. Blei, A. Ng, and M. I. Jordan. Latent dirichlet [4] R. Bunescu and M. Pasca. Using encyclopedic [5] C. Burges, R. Ragno, and Q.V. Le. Learning to Rank [6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [7] Z. Cao, T. Qin, T.Y. Liu, M.F. Tsai, and H. Li. [8] S. Chernov, T. Iofciu, W. Nejdl, and X. Zhou.
 [9] M. Collins and N. Duffy. New ranking algorithms for [10] S. Cucerzan. Large-scale named entity disambiguation [11] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [12] E. Gabrilovich and S. Markovitch. Computing [13] P. Gehler, A. Holub, and M. Welling. The rate [14] A. Globerson and S. Roweis. Visualizing pairwise [15] S. Goel, J. Langford, and A. Strehl. Predictive [16] D. Grangier and S. Bengio. Inferring document [17] D. Grangier and S. Bengio. A discriminative [18] R. Herbrich, T. Graepel, and K. Obermayer. Large [19] T. Hofmann. Probabilistic latent semantic indexing. In [20] J. Hu, L. Fang, Y. Cao, H. Zeng, H. Li, Q. Yang, and [21] P. Jain, B. Kulis, I. S. Dhillon, and K. Grauman. [22] T. Joachims. Optimizing search engines using [23] M. Keller and S. Bengio. A Neural Network for Text [24] T.Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [25] D. N. Milne, I. H. Witten, and D. M. Nichols. A [26] Z. Minier, Z. Bodo, and L. Csato. Wikipedia-based [27] M. Ruiz-casado, E. Alfonseca, and P. Castells. [28] R. Salakhutdinov and G. Hinton. Semantic Hashing. [29] Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, [30] F. Smadja, K. R. McKeown, and V. Hatzivassiloglou. [31] J. Sun, Z. Chen, H. Zeng, Y. Lu, C. Shi, and W. Ma. [32] K. Weinberger and L. Saul. Fast solvers and efficient [33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [34] L. Zighelnic and O. Kurland. Query-drift prevention
