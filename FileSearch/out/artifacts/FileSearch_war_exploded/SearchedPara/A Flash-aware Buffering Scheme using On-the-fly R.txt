 In this paper, we address how to reduce the amount of page updates in flash-based DBMS e quipped with SSD (solid state drive). We propose a novel buffering scheme that evicts a dirty page X without flushing it into SSD, and restores the right image of X when X is requested for later access. The restoration of X having previous flushing-less eviction is performed through our online redo actions on X . We call this page-restoring online redo the on-the-fly redo . Although our on-the-fly redo mechanism has some overhead of increasing the numbe r of page reads, this can be compensated by infrequent page updates. Additionally, since the proposed buffering scheme with the on-the-fly redo can easily support the no-steal policy in buffer management, we can enjoy the advantages of smaller loggi ng overhead and faster recovery. Through the TPC-C benchmarks us ing a Berkeley DB, we show that our scheme shortens the tran saction processing times by up to 53%. H.2.4 [ Database Management ]: Systems Flash storage; database recove ry; buffer scheme; checkpointing Recently, a lot of research efforts have been done to efficiently use flash memory as primary storage media alternative to HDD in DBMS. The popularity of flash memory is mainly due to its faster I/O speeds and lower power consumption, even though it is still more expensive than HDD. Howeve r, the update cost in flash memory is much higher than the read cost or the write cost (for writing data into an empty page) because of its disability of in-place updating [2][5]. Therefore, it is crucial to reduce the number of page updates for developi ng high-performance flash-based DBMS [2][3]. Since traditional buffering schemes were mainly targeted at HDD with slow random reads, the hit rate of cached pages acts as a key yardstick for evaluating their efficiency [1][5]. As a result, the LRU (least-recently used) algorithm or its variations are regarded as the most reasonable algorithms for buffer replacement. However, those algorithms may easily yield a lot of flushing of dirty pages to the storage (i.e., incurring page updates), for buffer space reclamation [5]. In addition, traditional buffering schemes often flush out dirty pages into HDD just for faster recovery [1][4]. Because of a relatively cheap update cost in HDD, disk-based DBMS does not require an y new buffering scheme whose main goal is to reduce the number of dirty pages flushed to storage media, even with sacrificing performance of other components. However, it is not th e case in flash-based DBMS [2-5]. In this light, we propose a novel flash-aware buffering scheme that significantly reduces the amount of page updates in SSD (solid state drive). To this end, our scheme evicts a dirty page from the buffer without flushing it to SSD. To identify the pages that lose up-to-date images because of the flushing-less eviction, we keep the information on the history of flushing-less evictions in the page/frame mapping table that is managed in a buffer pool. At the same time, we maintain an online log table containing a portion of redo log records in order to enable a redo action to restore the correct image of a pa ge experiencing its prior flushing-less eviction. We refer to this re do for online page restoration as the on-the-fly redo . Although the on-the-fly redo produces more I/Os for page reads and consumes more CPU time, such extra overheads are sufficiently compensated by the benefit from the considerable reduction of page updates in SSD. By flushing-less evictions of di rty pages, we can easily support the no-steal policy [1] in buffer management, thereby eliminating the need of the undo phase during the system recovery time. In addition, our recovery algorithm does not update data pages for redoing committed updates during the redo phase. Instead, before restarting DBMS, it just rebuilds the before-crash state of the online log table by retrieving the log file in SSD. After system restart, we provide database c onsistency by applying the on-the-fly redo mechanism in accordance with user X  X  page requests. Thanks to this simple and effici ent recovery strategy, we do not need to flush dirty pages consta ntly to SSD for fast recovery, which is typically required in disk-based DBMS though. The proposed approach significantly reduces the amount of page updates to SSD in normal processing. The advantages of the approach is quantitatively verified through extensive experiments with TPC-C benchmarks conducte d on the Berkeley DB that stores its database on SSD. As storage media for DBMS, flash-based SSD, which is composed of multiple flash dices and an embedded H/W controller, has drawn much atten tion owing to its very fast and uniform speeds of random reads and low power consumption [2-4]. The page write into empty space is also 9~10 times faster, compared with cutting-edge HDD. Recently, the speeds of sequential reads/writes have been remarkably improved due to the use of internal cache memory [2]. To utilize such desirable I/O characteristics, a diversity of research has been done for developing flash-based DBMS [2-5]. For instance, the authors in [4] employ SSD as a kind of cache storage deployed between a buffer pool and HDD, thereby obt aining cached pages from SSD rather than HDD. Because of th e limited usage of SSD as a cache storage, however, they could not drastically improve the DBMS performance. Unlike this, the TIPL scheme [3] uses SSD as primary storage media and employs the page-granularity recovery mechanism to reduce the number of page updates. In TIPL, an update on any data page is substituted with writi ng of log data indicating that an update happens. Then, at the time of loading of a page X , the TIPL buffer manager performs redo or undo actions on X , if X  X  X  log data exists. As a result, TI PL replaces a page update with writing of a smaller size of log data to a clean sector in SSD. In order to manage log data sectors within an SSD block, however, TIPL inevitably suffers from a significant degree of storage waste and I/O overhead for block merge operations. Furthermore, since it requires a specific type of SLC (single level cell) SSD, the TIPL scheme appears to be not acceptable in reality these days. To solve those problems, we design a new flash-aware buffering scheme that could run on off-the -shelf SSD without the use of reserved log areas in SSD. In this section, we present our buffer manageme nt scheme. We explain how to maintain an online log table for buffer pool management, how to handle trans action aborts, and how to handle transaction commits. Similarly to the TIPL scheme, we employ a recovery mechanism, which originally aims at restoring a failed system, for the reduction of page updates. Howe ver, our buffering scheme does not require extra storage reserved for logging and does not perform undo actions in rolling aborted updates back. To explain this, we use Figure 1 that shows the organization of a buffer pool. The buffer pool has three components: A set of buffer frames , a page/frame mapping table (PFMT) , and an online log table . In this figure, we have five buffer frames for storing users X  data, and the mapping information between page id  X  X  and its corresponding frame index is maintained in PFMT. The online log table keeps a portion of log records. In Figure 1(a), buffer frames are full with five cached pages, among which pages B , D and E (grayed) are dirty. Log records describing update histories of dirty pages are indexed in the in-memory online log table. Pages A and C have no log record in the online log table since they are not dirty ones now. Now, suppose that transaction T2 wants to read a new page F to access a record within page F . If page E is selected as a victim for buffer replacement, then the cached image of E should be flushed into storage and its corresponding PFMT entry is deleted when a traditional buffering scheme is applied. Unlike this, our new buffering scheme simply cl eans the frame of page E from the buffer pool without flushing it to storage. Correspondingly, the indicate the flushing-less eviction of E . If a page with flushing-less eviction (such as page E ) is requested later for its access, then our scheme reads the page fro m SSD and executes a redo action on it using the associated online (redo) log records. Note that only redo log records are required in our scheme since we never execute undo actions. Consequently, the proposed buffering scheme successfully avoids enfor ced flushing of a victim dirty page in buffer replacement, thereby considerably reducing the amount of page updates to SSD. We refer to such a real-time redo compensating a flushing-less eviction as the on-the-fly redo . Due to a big gap between the read cost and the update cost in SSD, the proposed on-the-fly redo mechanism is able to provide a significant performance gain. Besides low update frequency, the on-the-fly redo mechanism provides another advantage of easily supporting the no-steal policy in buffer management. If we try to provide the no-steal policy without employing the on-the-fly redo mechanism, the buffer replacement algorithm cannot select a dirty page as a victim when it is owned by any of uncommitted transactions. This restriction is extremely undesirable in victim selections, and thus inevitably aggravates the cache hit rate. In particular, some long-lived transactions may degrade the buffering efficiency seriously [1][5]. For this reason, the steal policy is generally accepted in traditional DBMS. However, our proposed buffering scheme has a way to evict a dirty page from the buffer pool without flushing it into storage, thereby making it possible to delay the update time of a dirty page until its ow ner transactions commit. Since the no-steal policy is guaranteed, we do not require the undo phase during recovery from sy stem failure. However, we still need to roll back the update s from the transactions aborted during normal processing . In the case of traditional DBMS, compensation log records are stored for persistent cancellation of aborted updates [1][3]. The use of compensation log records, however, requires a complicated recovery algorithm and enlarges I/O overhead to save those comp ensation log records. However, we can easily handle transaction aborts based on our on-the-fly redo mechanism. To show this, we use Figure 2, where transaction T2 has updated two pages of D and F , and is about to abort at the time of Figure 2(a). In the case of page D , there is no update other than T2  X  X  and the no-steal policy ensures that transaction T2 did not affect the in-SSD image of page D yet. Therefore, it is sufficient to clean page D from the buffer pool to abort T2 . Meanwhile, page F contains other two updates made by transactions T1 and T3 . To remove T2  X  X  update on E , we delete T2  X  X  log record from the online log table and set page F as required for the on-the-fly redo as in Figure 2(b). Consequently, page F is set to include the updates made by T1 and T3 only. While the actions for Figure 2(b) are underway, we neither create the compensation log records nor require any page updates to SSD. For transaction durability, we write commit log records before their owner transaction commits [1][3] as in traditional DBMS. However, while writing committe d log records for guaranteeing transaction commit, we do not delete their copies bookkept in the online log table. To explain the idea behind this, let us assume that the online log records owned by T3 are deleted when transaction T3 of Figure 2(b) commits. Then, our on-the-fly redo cannot reconstruct page B or F correctly, if any of them is evicted. For this reason, the deletion of committed log records needs to be carefully conducted by following the checkpointing algorithm given in Section 4. Instead, while flushing dirty pages into storage, we gradually remove online log records for committed transactions. Since traditional recovery schemes write log records in a chronological manner , uncommitted and committed log records are stored together in the same log page via a single I/O call. Since our scheme does not need undo log records, it is sufficient to store only committed log records. Owing to this idea, we save the committed log records only, and adopt the group commit protocol [1][3] for making log data written with a single I/O call as large as possible. According to this commit protocol, we defer the commit time of a group of transactions until we gather committed log records large enough to meet a predefined space utilization of the log space. With the group commit protocol, we can reduce a significant number of log writes as well as the waste of storage caused by empty space in the log file. Of course, we need to give a delay limit on the group commit. In this section, we present our schemes of checkpointing and restart processes for system recovery. For faster system recovery, the fuzzy checkpointing scheme is usually accepted. It creates a dirty page table to save the id s of dirty pages being snapshot and the log file offset (i.e., log sequence number, LSN in short), from which the redo phase is started for system recovery. Since the scheme does not enforce flushing of dirty pages, the star ting offset of the redo phase may go backwards too far in the presence of long-lived dirty pages. To prevent a too long redo phase, a disk-based buffering scheme usually takes a strategy to flush long-lived dirty pages at a proper rate during normal processing [1]. Unlike this, we do another job in checkpointing, i.e., compaction of the online log table. Through this log compaction, we delete online log records for committed transactions while flushing dirty pages into storage. Figure 3 show s our checkpointing algorithm. It is periodically initiated by a background-mode thread that works in collaboration with our buffer manager. To compact the log records of page P , the currently cached image of P is written into storage, if P  X  X  log records are all committed ones (lines 10-11); otherwise, a new image of P is generated by applying the committed log records to the in-SSD image of P . Then, page P is updated with the newly generated image (line 15), and P  X  X  PFMT entry is modified for demanding the on-the-fly redo on it. To reflect the above update (line 10 or 15), the committed log records of P are deleted (line 17). If log compaction is not possible with P, P is recorded as a dirty page (line 19). Note that page P having no committed log records is not included in the dirty page table according to our no-steal buffering policy. LSN of the first committed log record of P is saved (line 19). In the redo phase, the smallest LSN in all the dirty pages is used as a starting point of a log scan [1]. Finally, the master record in the recovery system is modified to point to the ne wly created checkpoint record. To restart the system after failure, we read the latest checkpoint record by referring to the system master record. Then, we start to restore the crashed buffer pool by following the algorithm of Figure 4. Due to space limitations, we give only a core part of the algorithm. Initially, the log file pointer is set to the starting offset for the redo phase. Recall that, since we store only the log records of committed transactions, the log records in the algorithm are all committed ones. We ignore the log records of P (lines 3-4), if their LSNs are earlier than the latest checkpointing time and P is not a dirty page. This is because those log records are already reflected on storage. A log record R of P is inserted into the online log table (lines 7-8), if LSN of R is greater than that of the record, then P becomes a page demanding the on-the-fly redo (line 10). During the recovery process, we reincarnate the online log table without page updates. Th en, the database consistency is preserved through the on-the-fly redos on outdated pages. Due to the update-free redo phase and the absence of the undo phase, we can shorten the halting time considerably after an abrupt system failure. For performance evaluation, we incorporated the proposed buffering and logging schemes into the open-sourced Berkeley DB. Then, we executed 100K tran sactions according to the TPC-C scenarios on both of the original Berkeley DB and that equipped with our buffering and logging schemes. Since our schemes are intended to work we ll for the off-the-shelf SSD, we have no special H/W assumptions. This is different from other work such as that presented in [3]. Table 1 shows the key H/W specifications for our TPC-C benchmark experiments. Figure 5 gives the comparisons of transaction processing times between Original (i.e., the original Berkeley DB) and OTF (i.e., that with our schemes). Both of group commit and ordinary immediate-commit protocols are applied to OTF . In our experiments, we used three differe nt sizes for the buffer pool, i.e., 20MB, 40MB, and 60MB. Note th at, for fair evaluation, we allocated some portion of memory for the online log table in OTFs from the total memory. As in the figure, we observe OTFs reduce the execution time by 20% to 53% in comparison with Original . Particularly, in the case of 60MB, the performance improvement rises up to 53% owing to the page updates greatly reduced. We also see that the group commit protocol reduces the processing time from 15% to 27%, compared with the immediate-commit protocol. It is mainly because the I/O cost for writing log data is decreased with more populated log pages. Figure 6. Comparisons on I/Os for transaction processing. For more detailed analysis on performance improvement, we counted the average numbers of page reads and page writes (including updates) that arise during executing the benchmarks. Figure 6 shows the I/O counting results. We see that OTFs have a larger number of page reads by almost 45% but a less number of page writes by up to 30%. The increasing number of page reads is due to re-caching of the pages for the on-the-fly redos and a relatively low cache hit rate caused by reduced memory space for buffer frames, which is due to our fair evaluation, mentioned before. In this paper, we have proposed a new flash-aware buffering scheme and a recovery mechanis m for it. Our buffering scheme employs the page-granularity on-the-fly redo, which nullifies enforced dirty-page flushing at buffer replacement. Owing to our on-the-fly redo, we can substantially reduce the amount of page updates. Additionally, our scheme helps reduce the time for system recovery. Our scheme, however, requires some more memory allocated for the online log table, which could impair the system performance to a degree because of threshing in the buffer pool. Through extensive experime nts with TPC-C benchmarks using a modified Berkeley DB running on SSD, we quantitatively verified the merits of our scheme in spite of less buffer space. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. NRF-2014R1A2A1A10054151). [1] Ramez Elmasri and Shamkant B. Navathe, Fundamentals of [2] Yinan Li et al.,  X  X ree Indexing on Solid State Drives, X  In [3] Sang-Won Lee and Bongki Moon,  X  X ransactional In-Page [4] Jaeyoung Do, Donghui Zhang, Jigne sh M. Patel, and David [5] Jian Hu, Hong Jiang, Lei Tian, and Lei Xu,  X  X UD-LRU: An 
