 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications  X  Data Mining General Terms: Algorithms.
 Keywords: Privacy Preservation, Independent Component Analysis, Cholesky X  X  Decomposition.
Protecting an individual X  X  confidential information in mul-tivariate numerical data has attracted a great deal of atten-tion in recent years. The most relevant works are those based on perturbation techniques, which can be classified into two categories. The first consist of random data per-turbation [1] [2] and random rotation perturbation [3], which disguise sensitive data by distorting the structure of the data. The second category generates a new synthetic data set that preserves certain statistical requirements.
Simulated-based approaches release a synthetic data set for public use. Synthetic data is sufficiently different to the original data, but it retains the statistical characteristics of the original data as much as possible. Liew et al. proposed a simulation-based scheme, called data distortion by proba-bility distribution, which generates a synthetic data set by randomly drawing from the underlying distribution of the original data set [17]. Because the distribution of the original data set is invalid, in the first step, the Komlogorov-Smirnov test [22] is used to identify the underlying density function of the original data set from some pre-determined set and estimate the parameters of the function. In this framework, the candidates for the density function of the original data set should be empirically selected by the user in advance. Because of the tremendous number of possible data distri-butions, in most cases, we can not find a suitable density function in the pre-determined set to fit the data. Further-more, the synthetic data generated by the Liew et al. pro-posed technique does not preserve the internal relationships among the attributes of a multivariate data set, because the process of identifying the underlying density function deals with each attribute separately. Consequently, synthetic data generated based on the probability distribution leads to high information loss in many cases. In contrast, Dandekar et al. [6] used the Latin Hypercube Sampling (LHS) technique to generate synthetic data for privacy preserving. It maintains Spearman X  X  rank correlation structure of the original data.
Mateo-Sanz proposed another simulated-based approach based on Cholesky X  X  decomposition of the covariance matrix of sensitive data. The major advantage of this approach is that Pearson X  X  covariance matrix is preserved exactly in the synthetic data set. However, as shown in Figure 1, the major problem with the Cholesky-based approach is that the quality of the synthetic data set depends on that of the generated seeds. Figure 1(a) is the scatter plot of some gen-erated seeds G , while Figure 1(b) shows the original data set X and the synthetic data generated by the Cholesky-based approach from G . In this example, the synthetic data set clearly misrepresents the probability distribution of the original data set X . Observably, the joint distribution of the synthetic data set is sensitive to the generated seeds G .Es-sentially, the spread shape of the synthetic data shall imitate the spread shape of the generated seeds. Since the distribu-tion of X is invalid, generating good seeds could be very difficult if the Cholesky-based approach is used for privacy preserving. How to develop good seeds for the Cholesky-based approach still is an open problem.

In summary, existing simulation-based approaches only (a) The scatter plot of a (2  X  500) generated seeds with identity covariance for the Cholesky-based approach Figure 1: An example of the Cholesky-based ap-proach. preserve some specific statistics of the original data set. Ex-plicitly, synthetic data generated by these simulated-based approaches can not provide analytical validity. In [5], the author asked: Why not directly publish the statistics one wants to preserve directly, rather than release a synthetic data set? To remedy the data utility problem in these ap-proaches, we propose two simulation-based frameworks us-ing independent component analysis (ICA).
In this section, we introduce the simulation-based pri-vacy preserving framework PRIMP for multivariate numer-ical data sets.

We assume that users must be given access to released data without restrictions [21]. The goal of privacy preser-vation is to provide released data that has analytical va-lidity, but does not disclose confidential information about individuals. The analytical validity represents the extent to which analyzing the synthetic data provides similar re-sults to those obtained by analyzing the original data [21]. Simulation-based privacy preservation generates a synthetic data set that preserves the statistical characteristics of the original data set for release to the public. If the synthetic data is sufficiently different from the original data, then the privacy of the original data is said to be preserved.
Let X be an original sensitive data set, with n records and d attributes. For ease of exposition, X can be viewed as a d  X  n matrix whose columns and rows represent, respectively, a record and the collection of samples for an attribute of X . For j =1 , 2 ,  X  X  X  ,n , we assume that the j th column of X (,i.e. X ( j )) is sampled from some unknown random vector X =( X 1 , X 2 ,  X  X  X  , X d ) T . In fact, we have no information about the probability distribution of X except for the data set X . We know that all the statistical characteristics of any data set are determined by its probability distribution. Therefore, the simulation-based approach to database pri-vacy preservation involves generating a synthetic data set  X  X with the same joint distributions as the original sensitive data set X . Explicitly, our purpose is to design an effective and efficient simulating schema based only on the observed data. A list of symbols used throughout this paper is given in Table 1.
 Figure 2: (a) The scatter plot of the observed data X . (b) The scatter plot of the independent source signals estimated by using the FastICA algorithm.
Here, we introduce some basic concepts of independent component analysis (ICA). A detailed study of ICA can be found in [14]. ICA defines a generative model for observed multivariate data. The model X  X  data variables are assumed to be linear or nonlinear mixtures of some unknown indepen-dent latent variables, and the mixing system is also unknown [14]. The latent variables are called independent components or source signals .Let x be a d -dimensional observation. Explicitly, ICA is a problem of recovering a latent random vector s from observations of unknown m -variate ( m  X  d ) functions of that vector. The latent vector s is assumed to be sampled from some unknown random vector, i.e., source signals, whose elements are mutually independent . Formally, we can write the relationship of the observed data x and the realization of the source signals s in a general form where g : R m  X  R d is an unknown real-value d -component vector function. Given n independent, identically distributed observations { X (1) ,X (2) ,  X  X  X  ,X ( n ) } , ICA tries to find an inverse mapping g  X  1 : R d  X  R m , which estimates of n re-alizations of the source signals by for any particular X ( t ), t =1 , 2 ,  X  X  X  ,n . The functions g and g 1 are called a mixing function and an unmixing function , respectively. Example 1: Consider the scenario of a 2-dimensional data set X with attributes X 1 and X 2 , as shown in Figure 2(a). Clearly, the attributes X 1 and X 2 of X are not independent; thus, it is difficult to predict the value of one attribute from the value of the other. It is also very difficult to generate a synthetic data set to preserve the joint distribution of X based on the sampled data set only [9].

We use FastICA 1 to estimate the source signals and the mixing matrix. The joint distribution of the independent source signals is shown in Figure 2(b). Clearly, the joint dis-tribution of the source signals is uniform on a square, so the joint probability density of the signals is simply the product of their marginal probability densities. Hence, we can use a re-sampling technique or LHS to generate a synthetic data set with the joint distribution as the source signals. Fur-thermore, we can use the mixing matrix and the synthetic data generated from the source signals to obtain a synthetic data set of X , which preserves the distribution of X .
The above results show that the  X  X ndependence X  prop-erty of the source signals estimated by ICA simplifies the problem of generating a synthetic data set based on sam-pled (or observed) data only. In addition, in terms of the re-identification disclosure risk 2 , it is better to distort or transform the data structure in the independent component space, rather than in the original data set.
In this section, we describe how ICA can be extended to a new application for privacy preservation. We assume that the normalized sensitive data set X satisfies the ICA model, as in Equation (1).

The outline of PRIMP is given in Table 2. Whitening is useful as a preprocessing step in ICA [14]. Because whiten-ing is essentially decorrelation followed by scaling, the tech-nique of principle component analysis is often used [14], but it is not advisable to use a covariance matrix to find the principle components when the attributes X  variances are very different [15]. So, normalization can reduce the complexity and improve the quality of ICA substantially [14]. There-fore, the first step of PRIMP normalizes the original data set.

As mentioned earlier, how to generate a multivariate syn-thetic data set with the same distribution of input is still an open question. Because of the tremendous number of inter-nal relationships among attributes, generating a synthetic data set that preserves all the statistical characteristics of a multivariate data set without distribution information is almost impossible [9]. However, if the attributes of the in-put are mutually independent, generating a synthetic data set based solely on the sampled data set becomes straight-forward.

The proposed approach views the sensitive data set gen-erated by an unknown function of some latent independent random process. Explicitly, we assume that the normalized data set  X  X belonging to the sensitive data set X satisfies the equation  X  X = g ( S ), where S is the latent independent ran-dom process. Under this assumption, we gain two important insights for designing a simulation-based privacy preserving technique. First, for privacy preservation, we can transform the source signals obtained by ICA into protected data by some distortion technique. Hence, we hide any clue about the distortion technique in the source signals to prevent a hacker accessing the sensitive data. (Note that most exist-ing distortion techniques disguise the sensitive attributes in original data. Consequently, clues about adapted distortion techniques can be obtained by comparing the difference be-tween the released data and the original data [2][13][16].) Second, the  X  X ndependence X  property of source signals sim-plifies the problem of generating a synthetic data for source signals. If the attributes of the original data set are mutually independent, we can apply a re-sampling technique, such as bootstrap [8], or LHS [19] to individual attributes to gen-erate the synthetic data. In theory, the inter-relationships and statistical characteristics among attributes should be well maintained by such re-sampling techniques, because we do not need to consider the inter-relationships of attributes. In other words, using the ICA technique to generate a syn-thetic data set for the original data set requires little effort. In PRIMP, we randomly shuffle the realizations 3 of the es-timated source signals to preserve privacy. Because of the  X  X ndependence X  X roperty, the shuffled source signals preserve the overall statistical characteristics of the original source signals.

We mix the the shuffled source signals by mixing func-tion, as shown in Equation (1), to obtain the (normalized) synthetic data set  X  X . Therefore, the statistical characteris-tics of the (normalized) synthetic data set can approximate the (normalized) original data set X . The quality of the synthetic data in terms of its data utility depends on the approximation of independence determined by ICA. Note that, the first step of PRIMP normalizes the original data set. Consequently, adjusting the synthetic data to preserve the mean and variance of each attribute of X is the final step of PRIMP.
 Example 2: Recall the data set X in Example 1. Figure 3(a) shows the scatter plot of the synthetic data generated by PRIMP. Because the assumption of multivariate normal distribution for the original data set is made by some pri-vacy preserving techniques [13] [20], we generate a multi-variate normal data set with the same mean and covariance structure as X , as shown in Figure 3(b). Figures 3(c) and 3(d) present the scatter plots of the synthetic data sets gen-erated by the LHS-based and Cholesky-based approaches, respectively. As the figures show, all the sets misrepresent the joint distribution of the original data X , except for the set generated by PRIMP. Figure 3: The synthetic data sets for the data set X in Example 1.
In this section, we develop a hybrid privacy preserving method that combines PRIMP and the Cholesky-based ap-proach [18]. The fundamental concept of the Cholesky-based privacy preserving technique is based on the following theo-rem.
 Theorem 1. Let X be any d -dimensional data set and G be any d  X  n matrix whose Pearson X  X  covariance matrix is equal to the identity matrix. Then, the covariance ma-trix of LG should be the same as that of X ,where L is a lower triangular matrix by the equation cov ( X )= LL T using Cholesky X  X  decomposition.
 From Theorem 1, we can generate a d  X  n random matrix G whose Pearson X  X  covariance matrix is equal to the identity matrix, where n and d are the size of the synthetic data set and the dimensions of the sensitive data set X , respectively. Therefore, the synthetic data set should be obtained by  X  LG ,where L is a lower triangular matrix derived by the equation C = LL T using Cholesky X  X  decomposition and C is the Pearson covariance matrix of X . By Theorem 1, the covariance matrix and correlation matrix of X are preserved exactly in the synthetic data set  X  X . We call the random matrix G the seeds of Cholesky-based privacy preservation. Table 3: The hybrid PRIMP and Cholesky-based approach for privacy preservation.

As mentioned above, in terms of data utility, the perfor-mance of the Cholesky X  X  approach depends on the generated seeds G . For example, as shown in Figures 1 and 3(d), the distributions of synthetic data sets are different significantly from those of the original data. Consequently, the synthetic data can not provide analytical valid. For example, in these cases, the data X  X  extreme statistics and the range of each attribute differ significantly from those of the original data. Unfortunately, how to generate good seeds for the Cholesky-based approach is still an open question.

The probability distribution of synthetic data generated by PRIMP can successfully approximate the probability dis-tribution of the original data. However, in terms of preserv-ing the covariance structure, the Cholesky-based method outperforms all existing simulation-based approaches. Fol-lowing the above observations, we use the Cholesky-based technique to modify the covariance structure of the synthetic data generated by PRIMP for covariance matrix preserva-tion. In the other words, because the output of PRIMP can satisfactorily preserve the distribution of the original data, we use PRIMP in the construction process to generate seeds for the Cholesky-based approach. As such, the hybrid PRIMP and Cholesky-based approach can preserve the dis-tribution and the covariance matrix of a sensitive data set. The stages of the hybrid approach are the same as those of PRIMP, except for the rescaling procedure, which uses the Cholesky-based approach to refine the covariance struc-ture of the synthetic data. The resulting, synthetic data preserves the covariance and correlation structures of the original data exactly like the Cholesky-based approach. It also solves the problem of how to generate good seeds for the Cholesky-based approach. The outline of the hybrid ap-proach is summarized in Table 3.
 Example 3: Again, we consider the data set X in Example 1. Figure 4 shows the scatter plots of the synthetic data gen-erated by the hybrid method. Because PRIMP can preserve the distribution, the outputs of the hybrid method approxi-mate the distribution of the original data sufficiently.
To assess the performance of PRIMP and the hybrid ap-proach in terms of data utility, we conducted a series of experiments on both artificial and real data sets. We com-pared the information loss in the synthetic data sets pro-duced by PRIMP with the synthetic data sets produced by Figure 4: The synthetic data set generated by the hybrid approach for the data set X in Example 1. the following privacy preserving approaches: the LHS-based approach, the Cholesky-based approach, the hybrid PRIMP and Cholesky-based approach, and sampling under the mul-tivariate normal assumption (MN).
 Table 4 summarizes the data sets used in our experiments. Data set 1 was derived from the American Housing Survey of the U. S. Census Bureau using the DataFerrett system 4 Data sets 2 and 3 were used in [20] and [21], respectively Data sets 3 to 10 were downloaded from the UCI Machine Learning Database [10]. Data set 11, downloaded from the web site of the Computational Aspects of Statistical Confi-dentiality (CASC) project 6 , was also used in [18], [7], and [11]. Note that, in this paper, our proposed method focuses on multivariate numerical data sets. Therefore, we removed the non-numerical attributes of the above data sets.
As noted in [14] (page 152), for linear ICA model, the independent source signals must have non-Gaussian distri-butions; at most, one of the independent components can have a Gaussian distribution. To evaluate the effect of the non-Gaussian assumption, we generate the following arti-ficial data set as Data set 12, X =( X 1 ,X 2 ,X 3 ) T .For i =1 , 2 , 3 ,  X  X  X  , 2000, where x i,j is the j -th element of X i , s 1 ,i and s 2 ,i domly sampled from a univariate Gaussian distribution with mean 0 and variance 1, and s 3 ,i is randomly sampled uni-formly from (0 , 1).

The generated source signals S j =( s j, 1 ,  X  X  X  ,s j, 2000 1 , 2 , 3, are mutually independent. Clearly, the model (3) vi-olates the non-Gaussian restriction of the linear ICA model.
Here, we use three well-known measures of bivariate re-lationships: Pearson X  X  correlation matrix, Spearman X  X  rank correlation matrix, and Kendall X  X  tau to evaluate the in-formation loss of privacy preserving algorithms. Let X i = ( x servations of the i th and j th attributes of data set X ,respec-tively. The ij th element of (Pearson X  X ) correlation matrix, (Spearman X  X ) rank correlation matrix, and Kendall X  X  Tau can be computed as follows ([4], chapter 3): Table 4: The data sets used in our experiments.
 where  X  x i = n k =1 x i,k /n ; R i,k is the rank of x i,k 1  X  X  X  ,n } in ascending order; and sgn (  X  ) is the sign function, defined as sgn ( x )=1if x  X  0or sgn ( x )=  X  1if x&lt; 0, for i =1 , 2 ,  X  X  X  ,d , k =1 , 2 ,  X  X  X  ,n .

Let C and  X  C ( P ) denote, respectively, the matrix of bi-variate relationship measurements for the original data set and the synthetic data set generated by algorithm P .The relative bias of C for algorithm P is defined as: where C ij and  X  C ij ( P ) denote, respectively, the elements of C and  X  C in the i th row and j th column. A smaller relative bias implies better preservation of the bivariate structure C . That is, a lower relative bias represents less information loss by the statistic C .

In our experiments, we use FastICA to evaluate the in-formation loss of PRIMP. The elements of the seeds of the Cholesky-based approach were generated from a uniform dis-tribution over (0 , 1). Then, the Gram-Schmidt process [12] was applied to transform the covariance matrix of the gener-ated seeds into an identity matrix. We also set the number of source signals to be the same as the dimensions of the input (,i.e. m = d ). Because the scales of the attributes vary, we normalize all of the data sets in the pre-processing stage to prevent the side effect encountered by attributes with differing variances significantly. The empirical results of 100 trials are summarized in Table 5.

As expected, the Cholesky-based approach preserves the correlation structure of the original data set exactly. How-ever, the probability distribution of the synthetic data gen-erated by the Cholesky-based approach depends on the seeds generated; thus, the output of the approach does not always preserve the distribution of the original data successfully, as shown in Figure 3(d). Compared to the relative biases of Spearman X  X  rank correlation matrix and Kendall X  X  Tau, the performance of PRIMP is better than those approaches in most cases. Even though data set 12 violates the non-Gaussian restriction of the linear ICA model, PRIMP out-performs the other approaches.

Although the results show that the hybrid approach is not always better than PRIMP for Spearman X  X  rank correlation Table 5: The approach which has the minimum av-erage relative bias among PRIMP, the multivariate normal assumption, the LHS-based approach, the Cholesky-based approach, and the hybrid method.
 Table 6: The average relative biases of correlation matrix based on the hybrid method and Cholesky-based approach over 100 trials.
 matrix and Kendall X  X  tau matrix, like Cholesky-based ap-proach, it can exactly preserve the covariance matrix of the original data as shown in Table 6. In the other words, the hybrid method significantly outperforms PRIMP in terms of preserving the correlation matrix. Also, it solves the prob-lem of generating seeds for the Cholesky-based approach. Furthermore, in theory, the leakage risk of the hybrid ap-proach is less than that of PRIMP. Therefore, it is also useful for privacy preserving.
We have proposed two new simulation-based privacy pre-serving frameworks for multivariate numerical data sets. The first is called PRIMP (PRivacy preserving by Independent coMPonents). It is shown empirically that the synthetic data set generated by PRIMP can preserve more statisti-cal characteristics of the original data set than previous ap-proaches. We have also proposed a hybrid method that com-bines PRIMP and the Cholesky-based approach for privacy preservation. The proposed method resolves the problem of generating good seeds for the Cholesky-based approach. Al-though the empirical results show that the hybrid approach is not always better than PRIMP in terms of Spearman X  X  rank correlation matrix and Kendall X  X  tau matrix for pre-serving privacy, like the Cholesky-based approach, it can preserve the covariance matrix of the original data exactly. Finally, in theory, the leakage risk of the hybrid approach is less than that of PRIMP. Overall, it is also useful for privacy preservation. The work was supported in part by the National Science Council of Taiwan, R.O.C., under Contracts NSC97-2221-E-002-172-MY3.
