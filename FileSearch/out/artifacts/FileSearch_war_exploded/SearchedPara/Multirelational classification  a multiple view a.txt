 Hongyu Guo  X  Herna L. Viktor Abstract Multirelational classification aims at discovering useful patterns across multiple inter-connected tables (relations) in a relational database. Many traditional learning tech-niques, however, assume a single table or a flat file as input (the so-called propositional algorithms). Existing multirelational classification approaches either  X  X pgrade X  mature prop-ositional learning methods to deal with relational presentation or extensively  X  X latten X  multi-ple tables into a single flat file, which is then solved by propositional algorithms. This article reports a multiple view strategy X  X here neither  X  X pgrading X  nor  X  X lattening X  is required X  for mining in relational databases. Our approach learns from multiple views (feature set) of a relational databases, and then integrates the information acquired by individual view learn-ers to construct a final model. Our empirical studies show that the method compares well in comparison with the classifiers induced by the majority of multirelational mining systems, in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for multirelational research and applications. In addition, the method has practical significance: it is appropriate for directly mining many real-world databases.
 Keywords Multirelational data mining  X  Classification  X  Relational database  X  Multi-view learning  X  Ensemble 1 Introduction Multirelational Data Mining (MRDM) aims to discover useful patterns across multiple rela-tions (tables) in a relational database. In general, a relational database consists of multiple relations, which are inter-connected by means of foreign key joins. Since their first release in 1970s, relational databases have been routinely used to collect and organize many real-world data X  X rom financial transactions, medical records, to health informatics observation. Thus, mining these data offers a unique opportunity for the data mining community. However, most traditional data mining approaches assume that the data are represented in a single table or a flat file (the so-called propositional methods). As a result, such propositional strategies, such as Decision Trees [ 59 ] and Support Vector Machines (SVMs) [ 10 , 36 ], can not exploit knowledge embedded across multiple inter-linked tables. Mining from such structured data, therefore, has become strategic important.

In order to learn from relational data, in the past decade, existing approaches either  X  X pgrade X  propositional algorithms to deal with relational presentations by  X  X e-inventing the wheels X  or  X  X latten X  multiple relations into a universal flat file, which is then used as the input of propositional learning methods. Unfortunately, existing  X  X pgrading X  approaches, especially those using Logic Programming techniques, often suffer not only from poor sca-lability when dealing with complex database schemas but also from unsatisfactory predictive performance while handling noisy or numeric values in real-world applications. Contrary to those  X  X pgrading X  algorithms,  X  X lattening X  methods aim to directly use propositional learn-ing algorithms by transforming multiple relations of a relational database into a universal flat file. However,  X  X lattening X  strategies tend to require considerable time and effort for the data transformation, result in losing the compact representations of the normalized dat-abases, and produce an extremely large table with huge number of additional attributes and numerous NULL values (missing values). As a result, these difficulties have prevented a wider application of multirelational mining, and post an urgent challenge to the data mining community.

To address the above mentioned problems, this article introduces a multiple view approach X  X hereneither X  X pgrading X  X or X  X lattening X  X srequired X  X obridgethegapbetween propositional learning algorithms and relational databases. On the one hand, our approach enables traditional data mining methods to utilize information across multiple relations in a relational database. Hence, many efficient and accurate propositional learning algorithms make a wider choice of mining methods available for multirelational data mining applica-tions. On the other hand, the strategy excludes the need to transform multiple inter-connected tables into a universal relation. Therefore, the above mentioned shortcomings resulted from the  X  X lattening X  process can be avoided.

Our approach was inspired by a promising new strategy, i.e. multi-view learning. Multi-view learning describes the problem of learning from multiple independent sets of features, i.e. views, of the presented data. This framework has been applied successfully to many real-world applications such as information extraction [ 8 ] and face recognition [ 19 ]. As in the example given by Blum and Mitchell [ 8 ], one can classify segments of televised broadcast based either on the video or on the audio information. In fact, a multi-view learning problem with n views can be seen as n strongly uncorrelated feature sets which are distributed in the multiple relations of a relational database. Often, a relational database is designed by domain experts using an Entity Relationship (ER) model, where multiple relations are connected via entity-relationship links [ 24 , 62 ]. Each relation usually has a naturally divided disjoint feature set, providing various attributes (information) contributing to the target concepts to be learned. As an example, consider the loan problem, as shown in Fig. 1 , from the PKDD 99 discovery challenge [ 3 ]. Here a banking database which consists of eight relations is depicted. Each relation describes the different characteristics of a client. For example, the Client relation contains a customer X  X  age, but the Account relation identifies a customer X  X  banking account information. In other words, each relation from this database provides dif-ferent types of information, or views, for the concept to be learned, i.e. whether a customer is a risk or not .
This problem is therefore a perfect candidate for learning from multiple views, as will be discussed in detail in Sect. 3 .

Using a relational database as input, our so-called Multi-view Relational Classification (MRC) strategy learns from multiple views (feature set) of a relational database, and then information acquired by view learners are integrated to construct a final classification model. Our empirical studies show that our method compares well in comparison with the classifiers induced by the majority of multirelational mining systems, in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for the multirela-tional research and applications. In addition, the MRC method has practical significance: it is appropriate for directly mining many real-world databases.

This paper is organized as follows. Section 2 introduces the preliminary concepts and related work. Next, in Sects. 3 , 4 ,and 5 , we present the MRC strategy. This is followed, in Section 6 , by our experimental results. Section 7 concludes the paper. 2 Related work 2.1 Preliminary concept Recall that, multirelational data mining aims to discover useful patterns across multiple rela-tions (tables) in a relational database. A schema for a relational database describes a set of tables = { T i } n 1 and a set of relationships between pairs of tables. A table T i consists of a set of tuples, a primary key (denoted by T . key ), a set of foreign keys (in this paper, we refer to primary key and foreign keys as key attributes ); the other attributes are descriptive attributes . Foreign key attributes 1 link to primary keys of other tables: this link specifies a join between two tables, and a foreign key attribute of table T 2 referencing table T 1 is denoted as T 2 . T 1 -key . Figure 1 gives a simple example of a relational database schema with eight tables. All relationships among tables are represented with bold lines. The bold lines show the foreign key links, which relate the key attributes of one table to the corresponding key attributes of another table. For example, the Account table has a foreign key attribute account-id , which is linked to the key attribute account-id in the Loan table.

Using a relational database as input, multirelational data mining approaches can be initiated to search for patterns. Classification is one of the most commonly addressed tasks within the data mining community [ 2 , 25 ]. In a multirelational classification setting, there is a database , which consists of a target table T target and a set of background relations {
T this table (target tuple) is associated with a class label which belongs to Y . Typically, the relational classification task is to find a function F ( x ) which maps each target tuple x to the category Y : Consider a two-class problem (e.g. positive and negative). The task of a multirelational clas-sification is to search relevant features across different relations (i.e. both from T target and {
T relation T target . 2.2 Previous work Two main categories of approaches for relational data mining, namely,  X  X pgrading X  and  X  X lattening X  strategies, have been actively investigated. 2.2.1 Upgrading approach  X  X pgrading X  methods refer to strategies which  X  X pgrade X  conventional learning algorithms to deal with relational presentations. A number of methods, which are considered as first-order upgrades of existing propositional learners, have been proposed in the Inductive Logic Programming (ILP) community [ 54 , 61 , 66 ]. For example, the popular first-order inductive learner (FOIL) method [ 60 ] upgrades the well-known CN2 algorithm [ 15 ] to deal with first-order representations. This approach employs a general-to-specific search to build rules to explain many positive examples and cover few negative ones. The top-down induction of logical decision trees (TILDE) method [ 6 ] extends the popular C4.5 propositional learner to tackle relational representations. The TILDE algorithm applies logical queries in nodes of the decision tree instead of testing attribute values. The divide-and-conquer nature embedded in the decision tree construction makes the TILDE method an efficient one. The well-known Progol method [ 51 ] is considered as an upgrade of the AQ algorithm [ 50 ]. Scalability, how-ever, is a big challenge to approaches using ILP [ 52 , 76 ]. The scalability problem is observed since the size of the hypothesis search space increases rapidly with the number of relations. Also, the construction of a single hypothesis may require repeated join operations to be per-formed in the database. Each join operation is very costly in terms of run time and storage space. To tackle this problem, Yin et al. [ 76 ] present the CrossMine algorithm, an extension of the FOIL approach. Combining with other learning techniques such as the look-one-ahead and sampling strategies, this approach demonstrates high scalability and accuracy.
The first-order upgrade method also attracts significant attention in the statistical rela-tional learning (SRL) community. These approaches usually are proposed to combine the logic theory with probability methodology. Popular ones include Probabilistic Relational Models (PRMs) [ 26 ], Relational Markov Networks (RMNs) [ 68 ], and Relational Probability Trees (RPTs) [ 55 ]. In short, the ILP strategy was historically the first approach to deal with multirelational data and has been extensively studied since then.

Unfortunately, existing  X  X pgrade X  approaches, especially those using Logic Program-ming techniques, however, usually are insufficient to learn from large real-world business databases. ILP techniques tend to be time consuming when building hypotheses. The data sets to be dealt with by the ILP methods are, therefore, typically small [ 7 ]. As a result, efficiency and scalability concerns have been consistently raising in ILP-based approaches. In addition, using ILP approaches usually requires some efforts to decide on the right representation for the information (e.g. the training examples and background knowledge) and to choose the right trade-off between expressive power and efficiency (such as the hypothesis language bias). Another major drawback of the ILP-based approaches is that, these methods usually show unsatisfactory performance when handling noisy or numeric-value business data. 2.2.2 Flattening approach The second important category of strategies used to handle relational domains is the so-called propositionalization method, in which multiple relations are flattened into a universal one. Subsequently, this flattened relation is presented to propositional learners. The LINUS algorithm was first presented in detail in 1990 [ 49 ], and is one of the pioneering systems of propositionalization strategies. The chief idea of the LINUS method is that the background knowledge can be used to introduce new attributes into the learning process. Following the same lines, propositionalization approaches such as DINUS [ 48 ] ( X  X eterminate LINUS X ) and SINUS [ 45 ] were designed. Well-known variants of propositionalization method such as the LINUS algorithm and its successors have shown their limitations when dealing with non-determinate relationships. For instance, it is difficult for these methods to handle a one-to-many relationship in a relational database. Recently, Knobbe and Siebes developed the RollUp algorithm to perform the  X  X lattening X  of multiple relations in a relational data-base [ 40 ]. The RollUp approach employs a depth-first search (DFS) in the relational database to aggregate information in deeper level relevant tables (i.e. tables far from the target rela-tion) to their parent tables(i.e. tables less far from the target relation), using the aggregate functions found in SQL. This summarization procedure stops if attributes from all relevant tables in the DFS search are aggregated onto the target table. However, the drawbacks of this approach are that, first, the  X  X oll up X  process may aggregate the same background table multiple times onto the target relation; second, the search depth of the DFS search has to be given. To overcomes the above-mentioned limitations, more recently, Krogel [ 44 ] introduced the so-called RELAGGS propositionalization method. This algorithm shows its superior pre-dictive performance over the RollUp and Dinus algorithms [ 44 ]. In the RELAGGS strategy, new features are constructed for the target relation through an aggregation process, where function dependencies in the database are considered.

Unfortunately,severedrawbacksofpropositionalizationapproachesarealsonoted.Firstly,  X  X lattening X  is a non-trivial task: here, extensive preprocessing efforts are needed. In addi-tion, the resulting flat file may contain large amounts of NULL values. More importantly, flattening relational data often results in a very big table with exponentially large numbers of additional attributes, which causes further scaling challenges and over-fitting problems for propositional algorithms [ 35 ]. In fact, the shortcomings from the resulting large flat file mainly attribute to the development of many  X  X pgrading X  propositional approaches [ 22 ]. 2.3 Distributed data mining Our work presented here also relates to learning from distributed databases. In such a learn-ing task, useful patterns are generalized from multiple distributed sources [ 11 , 13 , 14 , 38 , 56 , 64 , 65 , 74 , 75 , 77 ]. For example, Zhang et al. [ 77 ] presented the Multi-database Mining (MDM) framework to mine from multiple databases. The approach first classifies the multiple databases into different local groups. Next, local patterns are learned from each of these groups. Finally, a weighting process is initiated to synthesize the obtained local patterns. Also, Wu and Zhang [ 75 ] described a high performance model to synthesize high-frequency asso-ciation rules learned from distributed databases. In addition, some key issues concerning distributed data mining strategies such as record linkage [ 4 ] and duplicate detection [ 5 ] have also been investigated. Record linkage and duplicate detection tackle the problem of determining which database records refer to the same entity.

The next section presents the MRC approach. 3 The multiple view approach The MRC algorithm enables one to classify relational objects by applying conventional data mining methods, while there is no need to flatten multiple relations to a universal one. Our approach initially employs multiple view learners to separately capture essential informa-tion embedded in individual relation. Subsequently, the acquired knowledge is incorporated into a meta-learning mechanism to construct a final classification model. In detail, the MRC framework, as depicted in Fig. 2 , consists of five sequential stages: (1) Information Propagation Stage: The Information Propagation Stage, first of all, con-structs training data sets for use by a number of view learners, using a relational database as input. The Information Propagation Element propagates essential information from the target relation to the background relations, based on the foreign key links. In this way, each result-ing relation contains efficient and various information, which then enables a propositional learner to efficiently learn the target concept. (2) Aggregation Stage: After the Information Propagation ,the Aggregation Stage summa-rizes information embedded in multiple tuples and squeeze them into one row. This procedure is applied to each of the data sets constructed in the Information Propagation Stage .Inthis stage, aggregation functions are applied to each background relation (to which the essential information from the target relation were propagated). By applying the basic aggregation functions in SQL, new features are created to summarize information stored in multiple tuples. Each newly constructed background relation is then used as training data for a par-ticular view learner. (3) Multiple Views Construction Stage: In the third phase of the MRC algorithm, the Multiple Views Construction Stage constructs various hypotheses on the target concept, based on the multiple training data sets given by the Aggregation Stage . Conventional single-table data mining methods (view learners) are used in order to learn the target concept from each view of the database separately. In this stage, a number of view learners, which differ from one another, are trained. (4) View Validation Stage: All view learners constructed in the Multiple Views Construc-tion Stage is then evaluated in the the View Validation Stage . The trained view learners need to be validated before being used by the meta learner. This processing is needed to ensure that they are sufficiently able to learn the target concept on their respective training sets. In addition, strongly uncorrelated view learners are preferred. (5) View Combination Stage: In the last step of the MRC strategy, the resulting multiple view learners (from the View Validation Stage ) are incorporated into a meta learner to con-struct the final classification model. The meta learner is called upon to produce a function to control how the view learners work together, to achieve maximum classification accuracy. This function, along with the hypotheses constructed by each of the view learners, constitutes the final model.

The next two sections, i.e. Sects. 4 and 5 will discuss related issues of the above five stages. 4 Learning from individual views In a multi-view learning setting, each view learner is assigned a set of training data with different views (feature sets), from which each should be able to learn the target concept. To apply this setting to multirelational classification tasks, consider a target variable Y that propagate the target variable Y to all other background relations. This process is discussed in detail next. 4.1 Information propagation The MRC approach uses foreign key chains to implement this propagation process. The tuple IDs (identifiers for each tuple in the relation) from the target table, which will be used by the aggregation functions, must also be propagated to the background tables. This procedure is formally defined as follows.
 Definition 1 ( Directed Foreign Key Chain Propagation ) Given a database ={ T 1 ,..., T , T target } ,where T target is the target table with primary key T target . key and target con-cept Y . Consider a background table T i ,where i  X  X  1 , 2 ,..., n } , with an attribute set A (
T i ) , and a foreign key T i . T target -key that links to the target table T target .Ifatuplein table T i has a key value referencing a tuple in the target table through the foreign key T .

T target -key , we say that this tuple is joinable. All joinable tuples in table T i are selected A ( table T i . Example 1 ( Directed Foreign Key Chain Propagation ) Let us use the sample database in Fig. 1 to demonstrate this definition. The Loan target table has attributes loan -id , account -id , date , amount , duration , payment ,and status . Here, the tuple ID is the attri-bute loan -id and the target concept is from the attribute status . For the background Order table, training data from this view will be created. The background relation Order has a reference key linking to the target relation Loan , as shown in Fig. 3 . By Definition 1, we know that this relation follows the directed foreign key chain propagation scenario. As such, the training data for the view Order consists of all tuples which have an account -id linking to the account -id in the target table. Each of these tuples has a loan -id ,i.e.atupleIDand status from the target table, along with the attributes to -bank , to -account , account ,and type from the Order table.
 Definition 2 ( Undirected Foreign Key Chain Propagation ) Given a database = {
T concept Y . Consider a background table T i ,where i  X  X  1 , 2 ,..., n } , with a foreign key T .

T target -key referencing the target table T target . Next, consider a join path p = T a T b  X  X  X  , T m (where a , b ,..., m = i and a , b ,..., m  X  X  1 , 2 ,..., n } ) and another background that links to table T i through the join path p (note that p may consists of zero table). If a tuple in table T k has a key value linking to a tuple in table T i , and this tuple has a link to a tuple in the target table T target , we say that the tuple from T k is joinable to the target table T target . All joinable tuples in table T k are selected to form a new relation T k -ne w .Wedefine where A key ( T k ) are the key attributes of table T k .
 Example 2 ( Undirected Foreign Key Chain Propagation ) Consider now the situation pre-sented in Fig. 4 which is taken from Fig. 1 . For the background Client table, the training data from this view is created as follows. First we notice that the Client relation has no directed foreign key referencing to the target relation Loan . However, it does have a ref-erence key linked to relation Disposition , which in turn has a foreign key referencing the target relation Loan . By Definition 2, this relation follows the undirected foreign key chain propagation scenario. Following this definition, the tuples from table Client which have cli ent -id linkedtothe cli ent -id attribute in table Disposition and the tuples from table Disposition with account -id linked to the target table are selected to construct the training data. Since the Client table has attributes cli ent -id , bir t hday , gender ,and district -id , the final training data consists of four attributes, namely bir t hday and gender from relation Client and loan -id and status from the target relation Loan .

The MRC strategy builds views from relations which contain at least one descriptive attri-bute in the database. Except the view built directly from the target table, each background relations in a database will construct one view using the shortest directed or undirected for-eign key chain, i.e. the foreign key chain with less involved foreign key joins. The use of shortest foreign key chain is based on the following observations. First, shorter join paths can save storage and computational cost, compared to longer ones. In addition, join paths involv-ing many relations have more chance to decrease the number of entities related to the target tuples, potentially leading to provide less information about the target tuples. Finally, the semantic link with too many joins usually becomes very weak in a relational database [ 76 ]. Such weak links, therefore, have less chance to offer sufficient knowledge for learning the target concept. Basing on these observations, the MRC algorithm discourages long join paths.
Two special cases must be given more attention when using the shortest foreign key chains to implement the information propagation process. First, in the case where one background relation has two different foreign key chains that have the same length (chains with the same number of involved foreign joins), the chain with the highest number of related target tuples is chosen. In other words, the chain resulting in a larger number of related tuples in the target relation is selected in order to provide more information for learning. In addition, in some very rare cases, a relation may have more than one foreign key chain with the same lengths and the same number of related target tuples. In these cases, we have empirically determined that the choice of chain makes little difference on the final result.

Note that, Yin et al. presented the Tuple ID Propagation strategy in [ 76 ]. In their approach, the numbers of positive and negative class labels are propagated between different tables. In contrast, we here need to propagate the original class labels in order to enable learning from multiple views.

After the propagation, aggregation functions are then applied to the newly generated procedure is described next in Sect. 4.2 . 4.2 Aggregation-based feature generation Relational data mining approaches have to handle the difficulties inherent with one-to-many and many-to-many associations between relations [ 22 , 69 ]. The relations T i -ne w obtained in Sect. 4 consist of one-to-many relationships with respect to the primary key value T target . key (i.e. the tuple ID) as propagated from the target relation T target . This kind of indeterminate relationship has a significant impact on the predictive performance and has been studied by several researchers [ 41 , 46 , 57 ]. To address this problem, the MRC method applies aggrega-tion functions taken from the field of relational databases.
Aggregation has been widely used to summarize the properties of a set of related objects. In the MRC algorithm, the aggregation function squeezes related tuples into a single row using the primary key from the target relation. Each new table T i -ne w is aggregated to form a view attributes, the MRC method applies the aggregation function COUNT .For Numeric attri-butes, the SU M , AV G , MIN , MAX , ST DDEV ,and COUNT functions are employed.
 The aggregation-based feature generation procedure is formally defined as follows. Consider new attributes are generated to replace the attribute A i based on the following constraints. If A i is a Nominal Attribute, the aggregation-based feature generation procedure produces a new attribute  X ( A i ) , in which the total number of occurrences of values of attribute A i is stored. The value is calculated using the COUNT function in the database. If A i is a Numeric attributes, the values for SU M ( m ), AV G ( m ), MIN ( m ) , MAX ( m ), ST DDEV ( m ) ,and COUNT ( m ) are stored, respectively. 4.3 View learner construction After constructing the multi-view training data sets, the MRC algorithm calls upon the view learners to learn the target concept from each of these sets. Each learner constructs a different hypothesis based on the data it is given. Many traditional single-table learning algorithms, such as Decision Trees, SVMs, or Neural Networks, can be applied. In this way, all view learners make different observations on the target concept based on their perspective. The results from the learners will be validated and combined to construct the final classification model. This is discussed in the next section, namely Sect. 5 . 5 Multiple view combination 5.1 View validation The view learners trained in Sect. 4.3 need to be validated before being used by the meta learner. This step is needed to ensure that they are sufficiently able to learn the target concept on their respective training set. In addition, strongly uncorrelated view learners are preferred in order to maximize the predictive performance of the combined model. These two issues are discussed next. 5.1.1 View efficiency The view validation has to be able to differentiate the strong learners from the weak ones. In other words, it must identify those learners which are only able to learn concepts that are strictly more general or specific than the target one [ 18 ]. In the MRC method, we use the error rate in order to evaluate the predictive quality of a view learner. That is, learners with training error greater than 50% are discarded. In other words, only learners with predictive performance better than random guessing are used to construct the final model. 5.1.2 View correlation Besides high efficiency, view learners prefer to be as uncorrelated to each other as possible.
The theoretical foundations of multi-view learning are based on the assumptions that the views are independent [ 8 ]. However, research has shown that in real-world domains, the ideal assumption of multiple strictly independent views is not fully satisfied. As pointed out by Muslea et al. [ 54 ], in real-world problems, one seldom encounters problems with independent views.
 Disjoint views are preferred by multi-view learning. Dasupta et al. [ 18 ] give Probably Approximately Correct (PAC) bounds for the generalization error of co-training in terms of the agreement rate of hypothesis in two disjoint views. This also justifies Collins and Singer X  X  approach of directly optimizing the agreement rate of learners over different views [ 16 ]. Fol-lowing this line of thought, the MRC algorithm introduces a novel Correlation-based View Validation (CVV) algorithm.

The goal of the CVV method is to select a subset of views which are highly correlated with the target concept, but irrelevant to one another. Recall that this is an ideal assumption, since one rarely encounters real world problems with independent views. The CVV strategy uses a heuristic measure to evaluate the correlation between views. A similar heuristic principle has been applied in the test theory by Ghiselli [ 27 ] and feature selection approach by Hall [ 33 ]. The heuristic  X  X oodness X  of a subset of features is formalized in Equation 2 [ 33 ]: where C is the heuristic  X  X oodness X  of a selected feature subset. K is the number of features in the selected subset. R cf calculates the average feature-to-class correlation, and R ff stands for the average feature-to-feature dependence.

To measure the correlations between features and the class, and between features, we adopt the Symmetrical Uncertainty ( U ) [ 58 ] to calculate R cf and R ff . This measure is a modified information gain ( InfoGain ) measure [ 59 ]. It compensates for InfoGain  X  X  bias toward attri-butes with more values. The Symmetrical uncertainty is defined as follows: Given features X and Y , where and In order to apply the heuristic  X  X oodness X  C to select a subset of strongly uncorrected views, we need representatives of multiple views. In the CVV method, views are represented by View Features . View Features describe the knowledge, against the target concept, possessed by view learners.
 Definition 3 (View Feature) Let { V 1 ,..., V n }be n views to be validated. Given a view validation data set T v with m labels { y 1 ,..., y m }. For each instance t (with label L )in T k  X  X  1 ,..., m } ) for it. Here, f y k y , as predicted by view learner over view V i .Inthisway,a view validation example which along with the original class labels y k , is constructed.

That is, V 1 , for example, is described and represented by features { f y k V 1 ( t ) } V . By doing so, a set of view validation examples T v are created. Each instance t consists of a set of View Features to describe the corresponding view, along with a class label of the instance. In this way, a subset of view features can then be selected based on measure C .
After finishing constructing the view feature set, the CVV algorithm subsequently ranks view feature subsets according to the correlation-based heuristic evaluation measure C .It searches all possible view feature subsets, and constructs a ranking on them. The best rank-ing subset will be selected, i.e. the subset with the highest value of C .

To search the view feature space, various heuristic approaches such as hill climbing , beam search ,and best first are often employed [ 42 , 43 ]. The CVV method uses the best first search strategy [ 28 , 63 ], since this method has demonstrated its successes in many feature selection approaches [ 33 , 43 , 29 ]. The best search strategy initiates with an empty set of features, and keeps expanding with one more feature. In each round of the expansion, the best feature sub-set, namely the subset with the highest  X  X oodness X  value C will be chosen to keep expanding in the same way. Additionally, the best search traversal keeps a ranking of all its visited subsets. If the current expansion results in no improvement in terms of  X  X oodness X  value, the search can go back to the next best path and use it to expand its feature set. Often, a stopping criteria is usually imposed to a best first search. The CVV algorithm will terminate the search if a number of consecutive non-improvement expansions occur. Based on our experimental observations, we set the number to five heuristically.

In the CVV method, views are selected based on the final best subset of view features, which are considered highly correlate to the target concept to be learned. If a view has no view features to be considered strongly correlate to the class to be learned, it means that knowledge possessed by this view is not important for the learning. Thus, it makes sense to ignore this view. Therefore, the CVV algorithm selects a view if and only if any of its view features appears in the final best ranking subset of the view feature selection procedure. This subset of view features means only knowledge from views V 1 and V 4 really contributes to build the final model. Thus views V 1 and V 4 are selected by the Correlation-based View Validation method. Views other than V 1 and V 4 are ignored because they are considered weakly relevant to the target concept to be learned.

Algorithm 1 shows the CVV method in detail. As presented in Algorithm 1 ,the View Val-idation element first removes view learners with accuracy less than 50%. Next, it generates a view validation data set , where hypothesis knowledge possessed by multiple views are represented by view features . Thirdly, based on the heuristic correlation evaluation measure as described in Eq. ( 2 ), the best first search strategy is employed to select the best subset of view features. Finally, views are filtered out if none of its view features appear in the final view feature subset. Algorithm 1 Correlation-based View Validation 5.2 View combination The last step of the MRC approach is to construct the final classification model, using the trained view learners.

Strategies for combining models have been investigated by many researchers [ 1 , 9 , 12 , ing [ 73 ]. Voting approaches usually only make sense if the learners perform comparably well [ 72 ]. A meta learner method, such as the one used in stacking, is designed to learn which base classifiers are the reliable ones, using another learning algorithm. In the multi-view learning framework, multiple view learners may results in various performances, thus it is hard to guarantee the comparable performances of the view learners. Therefore, the meta-learning method is more suitable to the multi-view learning framework.
 Algorithm 2 MRC Algorithm
In meta-learning schemes, a learning algorithm is usually used to construct the function that combines the predictions of the individual learners. This combination process contains two steps. Firstly, a meta training data set is generated. Each instance of the meta training data consists of the class label predictions (denote the probabilities that the instance belongs to different classes), made by the individual learner on a specific training example, along with the original class label for that example. Secondly, a meta-learner is trained using the meta data constructed to achieve a strong predictive performance, as the final classification model.
In the last stage of the learning, the MRC approach returns a set of view learners, along with a meta learner which knows how to combine these multiple learners to achieve a strong predictive performance.

Algorithm 2 describes the entire steps of the MRC strategy. As shown in Algorithm 2 , the MRC method initially propagates and aggregates information to form multiple views from a relational databases. Subsequently, Algorithm 1 is called upon to identify a subset of uncorrelated views. After doing so, the MRC algorithm initiates the view learners to learn the target concept from each of these uncorrelated views. In this way, each view learner constructs a different hypothesis based on the data it is given. Finally, multiple view learners are then combined to form a final model.

The next section presents our empirical evaluations. 6 Experimental study This section provides the results obtained for the MRC algorithm on benchmark real-world databases. These results are presented in comparison, in terms of predictive performance achieved and running time needed, with four other well known multirelational data min-ing systems, namely the FOIL rule learner [ 60 ], CrossMine method [ 76 ], TILDE first-order logical trees [ 6 ], and RelAggs algorithm [ 44 ], along with a  X  X aseline X   X  X lattening X  approach (denoted as SimFlat).

Recall from Sect. 2 that, these three logic-based  X  X pgrading X  methods and two  X  X lattening X  algorithms employ different strategies to deal with structured data. The FOIL algorithm is the best known approach to deal with relational data, and is commonly used when benchmark-ing the performance of relational data mining methods. The CrossMine method is a recent addition. It extends the FOIL approach and has demonstrated its high scalability and accu-racy when mining relational databases [ 76 ]. In contrast to the general-to-specific searching approach as employed by FOIL and CrossMine algorithms, the divide-and-conquer search-ing technique is applied in the TILDE method [ 6 ] to build a logical decision tree. On the other hand, the RelAggs algorithm is a state-of-the-art  X  X lattening X  approach, where aggre-gate operators are used to transform multiple relations into a single table in order to be able to use propositional algorithms for the learning.

In order to provide a  X  X aseline X  approach, we devise the SimFlat strategy. In contrast to consideringadvancedaggregationtechniques(suchasfunctiondependencyamongattributes) in the RelAggs strategy, the SimFlat method employs the same aggregation calculations as that of the MRC algorithm to convert multiple relations into a universal single table. That is, the SimFlat strategy uses the same aggregate functions and follows the same join chains used by the MRC method to  X  X latten X  relations. The goal of the SimFlat strategy is to pro-vide a universal flat file which consists of all features used by individual views in the MRC strategy. As a consequence of such a special flattening, models built on the resulting flat file will provide us a  X  X aseline X  performance. In this way, the performance achieved by the MRC algorithm, which uses multiple sets of feature set (here, each feature set corresponds to an individual view), can be compared to performance obtained by the  X  X aseline X  approach, which uses all features available in a flat file. 6.1 Methodology Six learning tasks derived from four standard real-world databases were used to evaluate our algorithm. The four benchmark databases, namely the Financial, Mutagenesis, Thrombosis, and Warehouse databases, come from different application domains, have variant relational structures, consist of different numbers of tuples in the entire database and in the target rela-tion, and present varying degree of class distribution in the target relation. In addition, in order to test how different numbers of tuples in the target relation (with the same database schema) affect the performance of the MRC algorithm, we derived three learning tasks from the Financial database. Each of these three tasks has different number of target tuples but shares the same background relations.
 We implemented the MRC algorithm using Weka [ 72 ]. Also, our experiments applied C4.5 decision trees [ 59 ] and Naive Bayes probabilistic classifiers [ 37 ] as view learners and meta learners of the MRC algorithm. The C4.5 decision tree learner was used due to its de facto standard for empirical comparisons. In addition, Naive Bayes were chose because of their sensitivity to the changes of the input attributes [ 20 ]. The default settings of these two learning methods were used. Each of these experiments produces accuracy results using tenfold cross validation. The MRC algorithm, SimFlat strategy, TILDE approach, RelAggs algorithm, and CrossMine method were run on a 3 GHz Pentium 4 PC with 1 GByte of RAM running Windows XP and MySQL. The FOIL approach was run on a Sun4u machine with 4 CPUs. For each data set and system we report the average running time of each fold. In addition, the running time of the RelAggs approach included the time used to convert multiple relations into a flat file. Also, the  X  X lattening X  preprocessing of the Sim-Flat method, using SQL, takes considerable manual time and effort and assumes an expert level of database programming skills. We therefore do not provide the running time for this method. 6.2 The data sets used for experimental comparison 6.2.1 Mutagenesis database Our first experiment (denoted as MUT188) was conducted against the Mutagenesis data set [ 67 ]. This benchmark data set is composed of the structural descriptions of 188 Regres-sion Friendly molecules that are to be classified as mutagenic or not. (Of the 188 instances 125 tuples are positive and 63 are negative.) The background relations of this learning problem consist of descriptions about the atoms and bonds that make up the molecules, which include 4,893 atoms and 5,244 bonds. The Atom and Bond relations link to the tar-get relation Molecule through the Molecule-Atom relation which only contains key attri-butes. A summary of the characteristics for the learning data set is given in Table 1 .For this database, 3 views, namely molecule, bond ,and atom are constructed by the MRC algorithm. 6.2.2 Financial database Our second experiment was conducted against the Financial database, which was used in the PKDD 1999 discovery challenge [ 3 ]. The database was offered by a Czech bank and contains typical business data. The schema of the Financial database is shown in Fig. 1 . Recall that the original database is composed of eight tables. The target table, i.e. the Loan table consists of 682 records, including a class attribute status which indicates the status of the loan, i.e. A (finished and good), B (finished but bad), C (good but not finished), or D (bad and not finished). The background information for each loan is stored in the relations Account, Client, Order, Transaction, Credit Card, Disposition and Demographic . All background relations relate to the target table through directed or undirected foreign key chains, as shown in Fig. 1 . This database provides us with three different learning problems. Our first learning task is to learn if a loan is good or bad from the 234 finished tuples. The second learning problem attempts to classify if the loan is good or bad from the 682 instances, regardless of whether the loan is finished or not. Our third experimental task uses the Financial database as prepared in [ 76 ], which has only 400 examples in the target table. The authors sampled the Transaction relation, since it contains an extremely large number of instances and discarded some positive examples from the target relation to make the learning problem more balanced. A summary of the three learning data sets is also presented in Table 1 , where F234AC, F682AC and F400AC denote the first, second and third learning tasks respectively. For this database, eight views, namely, the views loan, account, client, order, transaction, credit card, disposition , and demographic were constructed for each task. Note that, for comparison purpose, all these three learning tasks use the same background relations as prepared in [ 76 ]. 6.2.3 Thrombosis database Our next experiment used a database derived from the Thrombosis database used for the PKDD 2001 Discovery Challenge [ 17 ]. This database is originally organized using seven relations. We used the Antibody-exam relation as the target table, and Thrombosis as the target concept. This concept describes the different degrees for Thrombosis, i.e. None, Most Severe, Severe, and Mild. The target table has 770 records describing the results of special laboratory examinations performed on patients. Our task here is to determine whether or not a patient is thrombosis free. For this task, we include four relations for our background knowledge, namely Patient-info, Diagnosis, Ana-pattern ,and Thrombosis . All four back-ground relations are linked to the target table by foreign keys. Therefore, for this data set, five different views are constructed by the MRC algorithm. A summary of this data set is shown as part of Table 1 (identified by Throm). 6.2.4 ECML98 database Our last experiment used the database for the ECML 1998 Sisyphus Workshop. This database was extracted from a customer data warehouse of a Swiss insurance company [ 39 ]. The task of this learning problem is to classify 7,329 households of class 1 or 2. Eight background relations are provided for the learning task. They are stored in tables Eadr, Hhold, Padr, Parrol, Part, Tfkomp, Tfrol and Vvert , respectively. In this experiment, we used the new start schemes prepared in [ 44 ]. A summary of the this learning data set is also presented in Table 1 (denoted as ECML98). 6.3 Experimental results In this section, we conducted three experiments to examine the performance of the MRC method, in terms of both the accuracy obtained and the running time needed. These three experiments used different propositional learning algorithms as the view learners and meta learners for the MRC algorithms in order to evaluate how different propositional algorithms impact the performance of the MRC algorithm. 6.3.1 Experiment #1: Using decision trees as propositional learners and view learners as In the first experiment, we examine the performance of the MRC algorithms in terms of accuracy obtained and running time needed. In this experiment, C4.5 decision trees were used by the RelAggs and SimFlat approaches as the propositional learners and by the MRC algorithm as the view learners and meta learners.

We present the predictive accuracy obtained for each of the six learning tasks in Table 2 , where MRC with VV and MRC without VV stand for the MRC approaches with view val-idation applied and without view validation, respectively. For each data set in Table 2 ,the highest results are highlighted in bold . In addition, in the parentheses of this table, we provide the accuracy gains (denoted by  X + X ) or lost (denoted by  X   X   X ) of each approach, compared to that of the MRC algorithm with view validation applied. To evaluate the performance of the MRC strategy in terms of run time, we also provide the running time needed (in seconds) for each learning tasks in Table 3 , where the best results for each data set are also highlighted in bold . 6.3.2 Discussion of experiment #1 The predictive performance results, as presented in Table 2 , show that the MRC algorithm appears to consistently reduce the error rate for almost all of the data sets, when com-pared to the logic-based FOIL, CrossMine, and TILDE methods. The only exception is that the MRC and the FOIL algorithm achieved the same predictive performance against the THROM database. In addition, our results, as shown in Table 2 , also indicate that, in many cases the error rate reduction achieved by the MRC approaches is large. For example, the MRC approaches reduced the error rates by at least 9% against (1) the F682AC, F400AC, F234AC, and ECML98 data sets, compared to the FOIL method; (2) the THROM data set, in comparison with the CrossMine algorithm; and (3) the ECML98 and THROM data sets when comparing with the TILDE approach.

When considering the comparison with the  X  X lattening X  based RelAggs and SimFlat strate-gies, the MRC algorithm conducted comparable predictive performance. As shown in Table 2 , the results indicate that the RelAggs method performed a bit better in three of the six data sets, but was less accurate in two of the six cases, when compared with the MRC approach. Also, the predictive performance of the MRC approach was similar with that of the Sim-Flat algorithm. However, the experimental results showed that the differences of accuracy achieved by these three algorithms are less than 2%, except for the THROM data set. In the THROM data set, the SimFlat approach yielded a 9.6% of accuracy loss, compared to that of the MRC algorithm. Our further analysis suggests that, this significant performance lost was due to the fact that, the flattening process of the SimFlat approach resulted in a large number of NULL values in the converted single table, and these NULL values then confused the propositional learners.
 In terms of running time needed, one can see from the experimental results (shown in Table 3 ) that the MRC methods achieved very promising outcomes, when compared to the other four well-known algorithms. The MRC algorithm meaningfully reduced the running time needed for most of the cases. For example, against four of the six data sets, namely the F682AC, F400AC, F234AC, and ECML98 data sets, the MRC methods were at least 25 times faster than the FOIL and TILDE algorithms. Comparing to the CrossMine strategy, the MRC approaches were at least twice as fast as the CrossMine method when learning against the F682AC, F400AC, and F234AC data sets. Not surprisingly, the RelAggs approaches were time consuming in many cases due to the flattening process, when comparing with the MRC strategy. For example, against the F234AC, F400AC, F682AC, ECML98, and MUT188 data sets, the MRC algorithm was 25, 21, 16, 4, and 3.9 times faster than the RelAggs strategy, respectively. In short, the results against the six data sets suggest that (1) the logic-based CrossMine method was efficient to learn from multiple relations, com-pared to the FOIL and TILDE strategies; (2) the MRC algorithm, in general, was much faster than the three well-know relational learning methods, namely the RelAggs, FOIL, and TILDE, and required very comparable running time when compared with the CrossMine algorithm.

When considering the impact of the view validation process in the MRC algorithm, the experimental results, as shown in Tables 2 and 3 imply that the MRC algorithms, with and without the view validation applied, resulted in little difference in terms of accuracy achieved and running time needed when C4.5 decision trees were used as the meta learners. 6.3.3 Experiment #2: Using Decision Trees as propositional learners and view learners as The second experiment aims to investigate the impact of the meta learning algorithms on the MRC approach. In this experiment, we used C4.5 decision trees as the propositional learners of the RelAggs and SimFlat approaches and as view learners of the MRC algorithm(as we did in experiment #1). However, we applied Naive Bayes as the meta learners of the MRC algorithm in this setting, instead of using C4.5 decision trees.

We present the predictive accuracy obtained for each of the six learning tasks in Table 4 and running time needed (in seconds) in Table 5 . Note that, in this experiment, the RelAggs, SimFlat, FOIL, TILDE, and CrossMine methods have the same performance as that in the experiment #1 since the change of the meta learners did not affect them. 6.3.4 Discussion of experiment #2 The experimental results, as presented in Tables 4 and 5 , show that the MRC algorithms had consistent performance in terms of predictive accuracy obtained and running time needed, regardless the meta learning algorithms used, namely no matter if the C4.5 decision trees or Naive Bayes were applied as the meta learners.

In addition, the experimental results, as shown in the last two column of Table 4 , implied that the view validation process improved the accuracy achieved by the MRC algorithms when Naive Bayes were used as meta learners. For example, in five of the six data sets, the view validation assisted MRC algorithm to improve or achieve equal accuracies. As shown in Table 4 , against the F234AC and F400AC data sets, the view validation helped the MRC algorithms to achieve accuracy gains by 6.4 and 2.6%, respectively. Only against the MUT188 data set, the MRC algorithm with view validation obtained slightly lower accuracy than that of the MRC algorithm without the view validation (lower by only 0.5%).

In terms of running time needed, as shown in Table 5 , the MRC algorithms with and without the view validation resulted in little difference.

In summary, these results indicate that when using Naive Bayes, which may be unstable in regard to changes in the features used, as meta learning algorithms, the view validation com-ponent was able to improve the predictive performance of the MRC algorithm. This outcome suggests that the view validation process was able to successfully remove the uncorrelated views. Theoretically, the basic assumption made by Naive Bayes is that of feature indepen-dence [ 34 ]. Research has shown that correlations between features degrade the predictive performance of the Naive Bayes methods [ 21 , 47 ]. Our experimental results here imply that, after successfully identifying and removing correlated views, the MRC framework was able to generate meta data which are less correlated, thus resulting in improving the predictive performance of the final model induced by the Naive Bayes methods. This implication also justifies the experimental results observed in Experiment#1, where C4.5 decision trees were used for the meta learners. That is, the use of decision trees as meta learners had little impact on the performance of the MRC methods, in terms of accuracy obtained. The C4.5 decision dependency between the features, compared to the Naive Bayes approaches. 6.3.5 Experiment #3: Using Naive Bayes as propositional learners and view learners as In the last experiment, we aim to evaluate the performance impact of the MRC algorithm when different propositional learning methods are applied as view learners. Contrary to the settings in the experiments #1 and 2, we here used Naive Bayes as the propositional learn-ers of the RelAggs and SimFlat approaches, and as the multiple view learners of the MRC algorithm (where the C4.5 decision trees were employed as meta learners).

We present the predictive accuracy obtained for each of the six learning tasks in Table 6 and running time needed (in seconds) in Table 7 . Note that, in this experiment, the FOIL, TILDE, and CrossMine have the same performance as that of the experiments #1 and 2 since they do not dependent on the use of the propositional and meta learners. 6.3.6 Discussion of experiment #3 Our results, as presented in Table 6 , surprisingly show that RelAggs and SimFlat approaches appears to performance very poorly in terms of accuracy obtained, compared to that of exper-iments #1 and 2, where C4.5 decision trees were used as propositional learners. For example, against the F682AC, F400AC, ECML98, and F234AC data sets, the accuracy lost of the RelAggs algorithm with Naive Bayes as propositional learners were 18.3, 15.7, 6.6, and 4.3%, respectively, compared to that of the RelAggs approach with C4.5 decision trees as propo-sitional learners (presented in experiments #1 and 2). In addition, the  X  X lattening X  approach SimFlat also experienced large accuracy lost against almost all the data sets, due to the use of Naive Bayes as propositional learners (instead of C4.5 decision trees). For example, the accuracy lost were at least 14% against four of the six learning data sets. Our further analysis of the experimental results implies that the significantly predictive performance lost of the  X  X lattening X  based approaches were due to the large numbers of newly created aggregation attributes in the converted flat files. In addition, a close look at the THROM data set show us that, there were a large number of NULL values in the resulting flat file, which further confused the Naive Bayes learning algorithms.
 Contrary to the large loss of predictive performance conducted by the RelAggs and Sim-Flat methods, the MRC algorithm resulted in very comparable predictive accuracy regardless of the propositional learners used. Against the THROM, F400AC, and F234AC data sets, the MRC algorithm obtained equal accuracies or improved the predictive performance, compared to the MRC methods which used decision trees as view learners. Only against the MUT188, F682AC, and ECML98 data sets, the Naive Bayes learners brought small performance lost to the MRC algorithms. Further analysis of the results suggests that such consistent performance benefits from combining multiple views.

Importantly, the results of this experiment (shown in Table 6 ) indicate that the MRC algo-rithm outperformed the two  X  X lattening X  based and three logic-based approaches, in terms of accuracy obtained, against almost all data sets. The results show that the MRC strat-egy achieved the highest accuracy in 28 out of the 30 cases. One exception is against the ECML98 data set, where slightly lower accuracy (compared to that of the CrossMine method) was obtained by the MRC methods (lower by only 2.2%); another exception is against the THROM data set, where the FOIL method achieved the same accuracy of 100% as that of the MRC algorithms.

Promisingly, the experimental results as presented in Table 6 show that the MRC algo-rithms significantly benefited from the framework of learning from multiple view. Recall that, the flat file conducted by the SimFlat approach consists of attributes from all views of the MRC strategy. That is to say, in the MRC algorithm, a set of feature set (each feature set corresponds to a view in the MRC strategy) were separately employed to learn the target concepts. On the other hand, the SimFlat approach combined all sets of feature set into a flat file, and then learned from all these features available. When comparing the accuracy resulted from the SimFlat and MRC algorithms, the results presented in Table 6 show that the MRC approach meaningfully outperformed the SimFlat method against almost all six data sets. For example, against the ECML98, F682, F400AC, F234AC, and THROM data sets, the MRC algorithm with view validation improved the accuracy over the SimFlat method by 32.6, 21.6, 19.2, 15.9, and 14.0%, respectively. These results imply that, in this experi-ment, the learning of using cooperation of multiple feature sets can significantly improve the predictive performance gains over that of using all features available.

In terms of running time needed, the experimental result show that the propositional learn-ing methods do not have a large impact on the RelAggs, SimFlat, and MRC algorithms. In addition, the MRC algorithms resulted in little difference regardless the use of different view learners and meta learners as well. 6.3.7 Summary of the three experiments In summary, the three experimental results (shown in Tables 2 , 3 , 4 , 5 , 6 ,and 7 ) indicate that the MRC approach achieves promising results in comparison with two flattening-based and three logic-based relational learning techniques, when evaluated in terms of overall accuracy obtained and run time needed. In addition, our results also suggest the following findings.
Firstly, our further analysis of these results implies the following reasons for the improve-ment of the MRC algorithm over other compared methods. When compared to logic-based methods, the MRC strategy first takes advantage of efficient, accurate, and state-of-the-art propositional learning algorithms and techniques. In addition, the application of aggregation provides more useful attributes for the MRC strategy. On the other hand, when comparing with flattening-based approaches, the MRC algorithm avoids putting all features together. In a data set with a large number of attributes, the learning method may ignore some second-best feature sets or get confused by the large number of features available. Another reason is that, the MRC algorithm does not create additional NULL values, which may also cause problems for propositionalization methods, as being observed in our experiments.

In addition to the promising predictive and scaling results achieved by the MRC algorithm, the results also suggest that the MRC framework yields robust models, in terms of predic-tive performance and run time required, regardless of the view learners and meta learners employed.

Another important finding of our experiments is that, the results imply that the cooperation of multiple feature sets can yield significantly predictive performance gains over the method which uses all features available. This observation suggest that the multiple view techniques can enable the use of rich features to benefit the learning in the multirelational domains. 7 Conclusions Vast amounts of real world data X  X rom financial transactions, medical records, to health informatics observation X  X re routinely collected into and organized in relational databases. These repositories offer unique opportunities to data miners, and thereby precious knowledge to decision makers, if knowledge discovery techniques can efficiently exploit the multiple interconnected relations in databases. Existing approaches either  X  X pgrade X  propositional learning methods to deal with multiple interlinked relations or  X  X lattening X  multiple tables into a single flat file. Unfortunately, the former strategies often result in problems such as unsatisfactory predictive and scaling performance against large business databases, where noisy and numeric value often presented; the latter approaches usually require extensive preprocessing of the data before the learning process can start and often result in a big table with large amounts of redundant attributes.

This article reports a multiple view approach X  X here neither  X  X pgrading X  nor  X  X latten-ing X  is required X  X o bridge the gap between propositional learning algorithms and relations databases. Our approach learns from multiple views of a relational databases, and informa-tion acquired by view learners is then integrated to construct a final model. Our empirical studies show that the method compares well in comparison with the classifiers induced by the majority of multirelational systems in terms of accuracy obtained and running time needed.
This paper here makes two chief contributions to the multirelational data mining com-munity. First, a novel approach, namely, the MRC strategy is devised for multirelational mining. Employing knowledge discovery methods which can be chosen from a wide range of existing propositional mining algorithms, learning directly from relational databases, and excluding the extensively  X  X lattening X  preprocessing make the MRC algorithm appropriate for mining useful patterns from many real-world databases. Another contribution is that, the paper here suggests the benefits of incorporating sets of features (views) when dealing with multirelational data. In other words, the adoption of multi-view learning framework may shed light on the issue of making good use of the rich feature space presented in structured domains, where attributes to describe an object are usually large and often highlight from different aspects.

Typically, a relational database is designed by database experts who use an entity rela-tionship diagram. In these diagrams, each entity thus intuitively corresponds to a different concept or view of the problem domain. However, in some cases, two attributes in different tables may be related. That is, the combination of these two seemingly unrelated attributes may provide us with new knowledge about the problem domain. A possible solution would be to join these two tables together as a database view, and treat this view as a background table. Currently, the MRC algorithm does not take this scenario into considerations. Investigating this issue will be part of our future work. In addition, we will also focus our attention on data streams, i.e. handling evolving relational databases. Our future work will also involve testing this method X  X  scalability against very large databases. Also, experimental evaluation on learn-ing tasks with more than two classes will be further investigated. It would also be interesting to examine the influence of feature selection strategies on the multi-view framework. References Author Biographies
