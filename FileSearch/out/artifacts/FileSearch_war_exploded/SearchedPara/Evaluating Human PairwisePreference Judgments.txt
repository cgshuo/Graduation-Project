 Centre for Language Technology Macquarie University Human evaluation plays an important role in NLP, often in the form of preference judgments.
Although there has been some use of classical non-parametric and bespoke approaches to evaluat-ing these sorts of judgments, there is an entire body of work on this in the context of sensory approach, Log-Linear Bradley-Terry models, and apply it to sample NLP data. 1. Introduction
Human evaluation is a key aspect of many NLP technologies. Automatic metrics that correlate with human judgments have been de veloped, especially in Machine Transla-tion, to relieve some of the burden. Nevert hess, Callison-Burch et al. (2007) note in their meta-evaluation that in MT they still  X  X onsider the human evaluation to be primary. X 
Whereas MT has traditionally used a Likert scale score for the criteria of adequacy and fluency, this meta-evaluation noted that these are  X  X eemingly difficult things for judges to agree on X ; consequently, asking judges to express a preference between alternative translations is increasingly used on the grounds of ease and intuitiveness. Further, where the major empirical results of a paper are from automatic metrics, it is still useful to supplement them: As two examples, Co llins, Koehn, and Kucerova (2005) and Lewis and Steedman (2013), in addition to a metric-based evaluation, present human judg-ments of preferences for their systems with respect to a baseline (Fig. 1). For results in published work, the reader is typically left to draw inferences from the numbers. For the data in Figure 1, is there a strong preference for the non-baseline system overall, or do null preferences count against that? Is anything about the results statistically significant? human judgment results. However, to our knowledge, the field has not taken advantage of a body of work dedicated to analyzing human preferences X  X redominantly in the context of sensory discrimination testing, and consequent consumer behavior X  X hich is supported by a great deal of statistical theory. It is linked to the mixed-effect models that are increasingly prominent in psycholin guistics and elsewhere, it has associated freely available R software, and it permits questions like the following to be asked: Can we say that the judges are expressing a preference at all, as opposed to no preference?
Is there an effect from judge disagreement or inconsistency? (Section 3) and discuss the issues that arise from this, and look at some of the approaches used in MT (Section 4). We then (Section 5) introduce ideas from human sensory pref-erence testing, where we review log-linear Bradley-Terry models of preferences, and apply this to our data, including discussion of ties, of subject effects, and of multiple pairwise comparisons. 2. Two Data Sets
Single Pairwise Comparison. Our basic single pairwise comparisons are those presented in Figure 1(a) and (b). Figure 1(c) contains the counts we will be using in later analysis:
We refer to counts in favor of the new system by n + , those in favor of the baseline by n and those reflecting no preference as n 0 ; the Lewis and Steedman results were over 87 pairwise judgments. We add some further arti ficial data to illustrate how the Log-Linear
Bradley-Terry (LLBT) models of Section 5 behave in accordance with intuition for data where the conclusion should be clear. These comprise a distribution with a moderate preference for + over  X  and not too many null preferences (ModPref), a distribution of equal preferences over all three categories (EqualPref), a distribution with mostly null preferences and equal n + and n  X  (NoPref), and a distribution with very few null preferences (StrongPref).

Multiple Pairwise Comparison. As noted in Section 1, there has been a trend to using human preference judgments, particularly in the workshops on statistical machine translation from Callison-Burch et al. (2007) onwards. Schemes have included asking humans to rank random samples of five translations, each from a different system.
Vilar et al. (2007) propose using binary rather than n -ary rankings, arguing that this these techniques can be extended to n -ary comparisons. In our example, there are four systems A , B , C , D and four judges J1, J2, J3, J4. The judges have pairwise ranked 240 translation pairs from systems x and y , indicating whether the translation of x is better than y ( x y ), worse than y ( x  X  y ), or similar in quality to y ( x overall impression, totalling all pairwise first preferences for each system (Section 4), gives a ranking of systems A X  X  X  X  X  X . It can also be seen that there is little in the way of undecidedness, and also that judge J3 differs from the general judge opinion in pairwise ranking of AD and BC. 310 xRy =  X  =  X  =  X  =  X  =  X  =  X  3. Classical Non-Parametric Methods
A classical approach to evaluating preference s is the non-parametric sign test (Sprent and Smeeton 2007). The fir st issue in applying this test here is ties, or expressions of no preference X  X hese are often ignored when the proportion of ties is small, but for our typical examples of Figure 1, this is no t true. Randles (2001) observes, regarding the approach most widely recommended by textbooks of just ignoring ties, that  X  X he constrained number of possible p values an d its  X  X limination of zeroes X  has caused concern and controversy through the years. X  Randles (2001) and Rayner and Best (2001, chapter 2), reviewing several approaches to handling ties, both advocate splitting ties in various ways depending on the problem setting, for (in Randles X  X  characterization)  X  X t is desirable that zeros have a conservative influence on declaring preference, but not to the same degree as negative responses. X  The key p oint is that modeling of ties explicitly can be important, although there is no consensus on how this should be done; no approach apart from ignoring ties appears to be in widespread use. The second issue with the sign test is that of multiple judges, where data points are related (e.g., the same items are given to all judges). The Friedman test (Sprent and Smeeton 2007, Section 7.3.1) can be viewed as an extension that can be applied to multiple subjects ranking multiple items (see Bi 2006, Section 5.1.3, for an example). However, Francis, Dittrich, and Hatzinger (2010) note that 4. Methods in Machine Translation
Human evaluation in NLP is a pervasive i ssue, but here we focus on MT and its shared tasks. The 2007 shared task (Callison-Bur ch et al. 2007) was the fir st to investigate a range of approaches that spec ifically included ranking of n translations, from best to worst, allowing ties (which were ignored); from this they defined an aggregate  X  X ank, X   X  X he average number of times that a system was judged to be better than any other system in the sentence ranking evaluation. X  They assessed inter-annotator agreement, and X  X ith a key goal of the meta-evaluation being to find the automatic evaluation metric that best matched human evaluations X  X alculated Spearman X  X  rank correlation coefficient between the two types of assessm ent. The 2008 shared task (Callison-Burch these is an open discussion, and certainly w arrants further thought, X  in particular because of ties  X  X urther complicating matters. X  Pado et al. (2009) modified the system-level predictions approach to become  X  X ie-aware, X  and noted that that this  X  X akes a considerable practical difference, improving correlation figures by 5 X 10 points. X  At around the same time Vilar et al. (2007) examined the use of pairwise comparisons in MT evaluation. They pose the problem as one where, given an order relationship is-better-than between pairs of systems, the goal is to find an ordering of all the systems:
They see this as the fundamental computer science problem of sorting. They define an aggregate evaluation score for compari ng systems, estimating expected value and standard error for hypothesis testing. Howe ver, in aggregating this way information about ties is lost.
 interannotator agreement. Lopez (2012) extends the analysis of Bojar et al. and casts the problem as  X  X inding the minimum feedback arc set in a tournament, a well-known NP-complete problem. X  He advocates using the pa irwise rankings themselves, rather than aggregate statistics like Vila r et al. (2007), and aims to minimi ze the number of violations among these. Koehn (2012) evaluates empirically the approaches of both Bojar et al. (2011) and Lopez (2012), with a focus on determining which systems are statistically distinguishable in terms of performance, de fining confidence bounds for this purpose. particular rankings could be trusted. They proposed a model based on Item Response
Theory (IRT), which underlies many standardized tests. They draw an analogy with judges assessing students on the basis of an underlying distribution of the student X  X  ability, with items authored by students having a quality drawn from the student X  X  ability distribution. They note in passing that a Gaussian parameterization of their IRT models resembles Thurstone and Bradley-Terry models; this leads us to the topic of Section 5.
 priate for preference judgments. Some of this involves moderately heavy-duty compu-tation for bootstrapping; this is suitable for large-scale WMT evaluations with dozens of competing systems, but perhaps less so for the scenarios we envisage in Section 1. More-over, examining what techniques other fields have developed could be useful, especially when they come with ready-made, easy-to-use tools for smaller-scale evaluation. 5. Preferences and Log-Linear Bradley-Terry Methods
The statistical analysis of human perception and preferences dates back at least to the psychophysics work of German physiolo gist E. H. Weber in the nineteenth century.
A progression from the way humans perceive differences between physical stimuli to more general analysis of human preferences has occurred particularly in the context of investigating consumer behavior X  X ea ling with questions like whether there is a definite preference for a food with a particular type of ingredient, for example X  X nd this is now a fully fledged area of resear ch. Sources like Lawl ess and Heymann (2010) cited models for pairwise comparisons are the Thurstone model (Thurstone 1927) and the closely related Bradley-Terry (BT) model (Bradley and Terry 1952); these have connections to the IRT models, widely used i n analyzing responses to questionnaires, which Hopkins and May (2013) drew on. Here we only look at BT models. 312 asetof J objects in a particular pairwise comparison jk is given by p ( O +  X  k for all j = k ,where  X  j and  X  k are non-negative  X  X orth X  parameters describing the location of the object on the scale of preferences for some attribute. For n objects, there will be n 2 pairwise comparisons.

Log-Linear Models. It is now standard to fit BT models as log-linear models (Agresti 2007, for example), which allows them to be treated in a uniform way with much of modern statistical analysis. Log-linear models are a variety of generalized linear models (GLM), as is, for example, the logistic regression used throughout NLP. GLMs consist of a random component that identifies the response variable Y and selects a probability distribution for it; a systematic component that specifies some linear combination of the explanatory variables x i ; and a link function g (  X   X  to this linear combination. They thus have the form g (  X  log-linear models, the response variables are counts that are assumed to follow a Pois-son distribution, and the link function is g (  X  ) = log( g (  X  ) = log  X  and the various x i might be gender, socioeconomic s tatus, and so forth. GLMs are a key tool for modern categorical data analysis, Agresti (2007, p. 65) noting that using models rather than the non-parametric approaches of Section 3 has several benefits: relative to another can be derived from the parameters. (Typically, software chooses a reference parameter and other parameter values are relative to that.) Statistical signif-icance scores and standard errors can be calculated for these parameters. In addition,
GLMs allow for testing of model fit. There are various model choices (e.g., should we include ties? should we include terms representing interactions?) and goodness-of-fit tests can assess the alternatives (see, e.g., Agresti 2007, Section 7.2.1). The model with a separate parameter for each cell in the associated contingency table is called the saturated model , and fits the data perfectly, making it a suitable comparator for alternatives. Deviance is a likelihood ratio s tatistic comparing a proposed model to the saturated one, allowing a test of the hypothesis that parameters not included in the model are zero, via goodness of fit tests; large test statistics and small p-values provide evidence of model lack of fit.

Models with Ties. To set out the representation of LLBT models, we follow the for-mulation of Dittrich and Hatzinger (2009). Let n ( jk ) between objects j and k ;andlet Y ( jk ) j be the number of preferences for object j with also be regarded as a J 2  X  J incomplete two-dimensional c ontingency table: There are 2 rows of pairwise comparisons, and J columns recording choices of the j th object.
As with log-linear models in general, the distribution of random variables Y
Y ( jk ) k is assumed to be Poisson. Conditional on fixed n follow a binomial (more generally, multinomi al) distribution. The expected number of preferences of object j with respect to object k is denoted m with p ( jk ) j the binomial probability. So far this is only for binary preferences; there are various ways to account for ties. We describe the approach of Davidson and Beaver (1977), which appears quite widely used, where there is a common null preference effect for all pairwise comparisons. Then where the  X   X  X  are  X  X uisance X  parameters that fix the n ( jk )  X  X  represent object parameters, m ( jk )0 is the expected number of null preferences for pair ( jk ), and  X  is the undecided effect. The object parameters are related to the worth parameters of the original definition by log  X  = 2  X  O : These represent the log-odds. parisons, a key benefit is the availability of packages in candidates allowing a variety of sophisticated models are by Turner and Firth (2012) and Hatzinger and Dittrich (2012); we use the latter as the current version of the former does not handle ties. We first apply the model described by Equations (1) to the single pairwise data with ties from Section 2 using R . We refer the reader to the associated data bundle 1 for the full output; we only excerpt it in the discussion below. Immediately following is a snippet of the R output for the ModPref data from Figure 1. variable  X  O j for the + category, o2 for the  X  category, negative value of the estimate for g1 combined with its statistical significance says that there is a strong tendency for an expression of preference. The positive value of the parameter and its significance indicate that the + group is strongly preferred: The odds in favor of this group with respect to the  X  group is exp(2 this to the description of the data in Section 2, then, there is a strong preference for trans-lations by the proposed system relative to the baseline, even taking into account null preferences. The LLBT model confirms that even small data sets like this can produce meaningful and statistically significant results. For the other artificial preference data of
Figure 1, the parameters behave as expected: for EqualPref, parameter estimates are all zero, signifying that they all have the same odds; for NoPref, the positive a strong tendency towards no preference; for StrongPref, the negative strong tendency towards some preference, but with + or  X  equally likely. Note that all of these are saturated models: there are three objects and three parameters, so the model fits perfectly (indicated also by zer o residual deviance). When we apply them 314 to the real count data of Figure 1 (c), the results indicate that for the Collins et al. data there is a weak to moderate tendency not to choose ( g1 estimate 0.303, p = 0 given that, there is a significant (0 . 0001) preference in favor of the reordered system.
For the Lewis and Steedman results, the model gives similar results, albeit with a much stronger disposition to null preferences. In the data bundle we also carry out the sign test ignoring ties for each data set for comparison; it gives the same results in each case for the relation of + than  X , but does not allow an evaluation of the effect of ties. of Table 1. In the R output,thefoursystemsA,B,C,Dcorrespondtoobjects and g1 again to null preferences. As per the overview of the MT data in Section 2, there is little undecidedness (large negative g1 ). The coefficients show that object
A) is most preferred, followed by o4 (D), then o2 (B) and case, the model is not saturated: There is a non-zero residual deviance. As mentioned, log-linear models can be compared in terms o f goodness of fit: Dittrich, Hatzinger, and Katzenbeisser (1998) and Dittrich and Hatzinger (2009) discuss this in some detail for
LLBT models. Chi-squared statistics can be used to assess goodness of fit based on the residual deviance; the degrees of freedom (d.f.) equal the number of cell counts minus the number of model parameters; both deviance and d.f. are given in the this data deviance is 30.646 on 8 d.f., whereas by contrast if the ties ( 221.22 on 9 d.f. A chi-squared test would establish the goodness of fit for each model; but even without consulting the test it can be seen that leaving out the one parameter related to ties (1 d.f.) gives a seven-fold increa se in deviance, so clearly inclusion of ties produces a much better model.

Introducing Subject Covariates. The model can also incorporate a range of other factors, a possibility not easily open to non-parame tric methods. The one we look at here is the notion of a categorical covariate, introduced into LLBT models in Dittrich, Hatzinger, and Katzenbeisser (1998): This allows the objects (items) to vary with characteristics of the subject (judge). Many types of subject covariates could be added, grouping subjects by native language of the speaker, source of j udges (e.g., Mechanical Turk, university), and so forth. Here we add just one, the identity of the subject. (Typically in a GLM this would be a random effect; we treat it as a covariate just for our simple illustration.)
We define our categorical covariate S to have levels l , l expected number of preferences for object j with respect to object k for subjects in covariate class l . The log-linear representation is then as follows: representing the ordering for that group; th e orderings for other groups are obtained by adding the  X  OS jl  X  X  specific to group l to the  X  O l are again  X  X uisance X  parameters, the latter r epresenting the main effect of the subject parameters describing the effect of the subj ect covariate on the preference for object j (similarly  X  OS kl and object k ). We apply the model described by Equations (2) to the multiple pairwise data, with the subject cov ariate SUBJ with four levels (one per judge
J i of Table 1). There are a few complexities in interpreting the output, beyond the scope of this article to discuss but covered in Dittrich, Hatzinger, and Katzenbeisser (1998).
The broad interpretations to draw from the output are that interactions o2:SUBJ3 are large and significant, and contribute to the model, unlike any others. These correspond to the different pairwise rankings given by judge J3 to system A (relative to D) and to B (relative to C): This is how subject effects are indicated in these LLBT models.
 the-art overview of such extensions across a range of approaches, with an emphasis on dependent data. We only note two extensions here that are incorporated into and relevant to NLP. With categorical object covariates , items can be grouped as well, sources. With non-pairwise rankings , judges can rank over more than two elements, as in the standard WMT evaluations, although th is needs a special treatment in the models. 6. Conclusions We have looked at the sort of (pairwise) preference data that is encountered often in
NLP. A particular characteristic of NLP da ta is that ties or undecided results may be frequent, and there is often a concern with inter-judge consistency. Reviewing classical non-parametric approaches, we note the opinion that it is important to model ties, and also note that approaches to looking at subje ct (judge) effects have several issues, such as a lack of quantitative interpretation of results. Among NLP approaches, especially within MT, new techniques are still being derived, which could benefit from views from outside the field. What we present are techniques from the field of sensory preference evaluation, where there has been a long history of development by statistics researchers.
Recently, log-linear models have attracte d attention. Applying them to sample data, we find that they provide the sort of information and uniform framework for analysis that
NLP researchers could find useful. Given both extensive theoretical underpinings and freely available statistical software, we recommend LLBT models as a potential tool. References 316
