 Linear and Quadratic Discrimi nant Analysis have been used widely in many areas of data mining, machine learning, and bioinformatics. Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis, called Regu-larized Discriminant Analysis (RDA), which has been shown to be more flexible in dealing with various class distribu-tions. RDA applies the regularization techniques by em-ploying two regularization parameters, which are chosen to jointly maximize the classification performance. The opti-mal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs. It is computation-ally prohibitive for high dimensional data, especially when the candidate set is large, which limits the applications of RDA to low dimensional data.

In this paper, a novel algorithm for RDA is presented for high dimensional data. It can estimate the optimal regular-ization parameters from a large set of parameter candidates efficiently. Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency, and also show that, for a properly chosen pair of regularization parame-ters, RDA performs favorably in classification, in compari-son with other existing classification methods.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Dimensionality reduction, Quadratic Discrim-inant Analysis, regularization, cross-validation
Statistical discriminant analysis is a frequently used and widely applicable tool in a variety of areas [6, 12, 26, 27]. The aim of discriminant analysis is to assign a data point to one of several classes (groups) on the basis of a number of feature variables. Numerous m ethods on discriminant anal-ysis have been proposed and applied in the past. The most Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. frequently used methods are parametric approaches, espe-cially Linear and Quadratic Discriminant Analysis. Linear Discriminant Analysis (LDA) is based on the assumption that the variables are multivariate normally distributed in each class with different mean vectors and a common co-variance matrix. It has been used in various applications [1, 6, 19, 24]. In Quadratic Discriminant Analysis (QDA), the variables are assumed to be multivariate normally dis-tributed in each class with different mean vectors and differ-ent covariance matrices [14]. QDA provides a less restrictive procedure by allowing different covariance matrices and may thus fit the data better than LDA. However, LDA involves a much smaller number of parameters to estimate than QDA, and is thus more robust and reliable than QDA in the pa-rameter estimation.
 Friedman [8] proposed a compromise between LDA and QDA, called Regularized Discriminant Analysis (or RDA in short), which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA. The regu-larized covariance matrix of the i -th class has the following form: pooled covariance matrix as used in LDA, is also known as the within-class scatter matrix [9], I d is the identity matrix of size d by d ,and d is the dimensionality of the data. Here  X   X  [0 , 1] and  X   X  [0 , 1] are two regularization parameters. The trace term in the original RDA formulation in [8] is absorbed into the  X  parameter for simplicity.

RDA provides a fairly rich class of regularization alterna-tives. The four corners defining the extremes of the (  X ,  X  ) plane represent well-known classification procedures. The upper right corner (  X  =1 , X  =1)representsQDA.The upper left corner (  X  =0 , X  = 1) represents LDA. The line connecting the lower left and lower right corner, i.e.,  X  =0 with 0  X   X   X  1, corresponds to the nearest-centroid clas-sifier well known in pattern recognition, where a test data point is assigned to the class with the closest (Euclidean dis-tance) centroid. Varying  X  with  X  fixed at 1 produces mod-els between QDA and LDA. For a given training dataset,  X  and  X  are commonly estimated via cross-validation. Se-lecting an optimal value for a parameter pair such as (  X ,  X  ) is called model selection [14]. The computational cost of model selection for RDA is high, especially when the data dimensionality, d , is large, since it requires expensive matrix computations for each candidate pair. This restricts RDA to low dimensional data.

In this paper, we make the first attempt in extending the applicability of RDA to high dimensional, low sample size (HDLSS) data, as HDLSS data are emerging from vari-ous fields. In high throughput gene expression experiments, technologies have been designed to measure the gene ex-pression levels of tens of thousands of genes in a single mi-croarray chip. However, the sample size in each dataset is typically small ranging from tens to low hundreds due to the cost of the experiments. In image-based object or face recognition applications, two or three dimensional images are usually converted to column representations, resulting in high dimensional data while the number of images is usu-ally small. In text document classification, the number of features equals to the number of distinct words in the docu-ments, which is typically in the thousands, while the number of documents in the study may be much smaller. A common characteristic of all these data sets is that the dimensionality, d , of the data vector is much larger than the sample size. This leads to various statistical issues, known as the high dimensional, low sample size problem [13]. In this paper, we propose an efficient algorithm for RDA on HDLSS data. The primary contributions of this work include:
The rest of the paper is organized as follows. An overview of QDA and RDA is given in Section 2. An efficient algo-rithm for RDA is presented in Section 3. Experimental re-sults are given in Section 4. Conclusions are presented in Section 5.
For convenience, we present in Table 1 the important no-tations that will be used in the rest of the paper.
Notation Description Table 1: Important notations used in the paper.

In this section, we briefly review the Quadratic Discrimi-nant Analysis (QDA), some issues related to the application of QDA on HDLSS data, and the Regularized Discriminant Analysis (RDA). Note that LDA is a special case of QDA when all classes share a common class covariance.
Given a training dataset of n data points { ( x i ,y i ) } n where x i  X  IR d is the feature vector of the i -th data point, d is the data dimensionality, y i = g ( x i )  X  X  1 , 2 ,  X  X  X  class label of x i ,and k is the number of classes. Let be the data matrix, which can be decomposed into k classes as A =[ A 1 ,A 2 ,  X  X  X  ,A k ], where A i contains all data points class. We have n = k i =1 n i .

Assuming the class densities follow the normal distribu-tion, we apply the following classification rule [8, 14]: a test point x  X  IR d is classified as class C ( x ) defined by C ( x ) = argmin i ( x  X   X  i ) T  X   X  1 i ( x  X   X  i )+ln | where the centroid  X  i of i -th class is defined as e )  X  IR n i is a vector of all ones, and the covariance matrix  X  i of i -th class is defined as Note that we have assumed an equal prior for all classes in Eq. (1) for simplicity.

The decision boundary using the above classification rule is quadratic and the algorithm is thus called Quadratic Dis-criminant Analysis (QDA). In a special case where all classes share a common covariance, that is,  X  i = X  j , for any class i and class j , QDA is reduced to the well-known Linear Dis-criminant Analysis (LDA) [5, 7, 9, 14].

The traditional QDA formulation in Eq. (1) requires all class covariance matrices to be nonsingular. However, for many applications involving HDLSS data, such as text doc-ument classification, face recognition, and microarray gene expression data analysis, all class covariance matrices may be singular, since the data dimensionality may be much dataset. Furthermore, the estimates of the class covariance matrices may be biased and unreliable. As pointed out in [8], this bias is more pronounced, when their eigenvalues tend toward equality, while it is correspondingly less severe when their eigenvalues are highly disparate. In both cases, this phenomenon becomes more pronounced as the sample size decreases. Thus, HDLSS data presents a major challenge for theapplicationofQDA.In[8],Friedmanproposedacom-promise between LDA and QDA, called Regularized Dis-criminant Analysis (RDA), which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA by employing regularization techniques. Regular-ization has been commonly used in the solution of ill-posed inverse problems [20], where the number of parameters ex-ceeds the sample size. In such cases, the parameter estimates can be highly unstable, giving rise to high variance. By em-ploying a method of regularization, one attempts to improve the estimates by regulating this bias variance trade-off.
Quadratic Discriminant Analysis is ill-posed if n k &lt;d for any class. One method of regularization is to replace the individual class covariance matrix  X  i by S i (  X  ) as follows: where S w is the weighted average of the class covariance matrices, called pooled covar iance matrix, or within-class scatter matrix [9], which is defined as The regularization parameter  X  takes on values between 0 and 1. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled esti-mate. The value  X  = 1 gives rise to QDA, whereas  X  =0 yields LDA.
 However, the regularization in Eq. (4) is still fairly limited. First, it might not provide enough regularization. If the total sample size, n , is less than the data dimensionality, d , then even LDA is ill-posed [17, 22]. Second, biasing the class covariance matrices toward commonality may not be the most effective way to shrink them. Recall that ridge regression regularizes ordinar y linear least squares regression by shrinking toward a multiple of the identity matrix [14, 15]. To this end, a further regularization is given by where I d is the identity matrix of d by d and  X  is an ad-ditional regularization parame ter, which controls shrinkage toward a multiple of the identity matrix.

In this paper, we apply a variant of the regularized class covariance matrix in Eq. (6), given by where total scatter matrix S t is defined as  X   X  [0 , 1], and  X   X  [0 , 1]. The minor difference here lies in the matrix S t used in Eq. (7), while S w is employed in Eq. (6). It is interesting to note that, when  X   X  1, the classification rule based on the regularized class covariance matrix in Eq. (6) may be numerically unstable, while the one based on the matrix in Eq. (7) is stable even for HDLSS data (see Section 3). The use of S t instead of S w has recently been explored in LDA for improving numerical stability [3, 22].

Atestpoint x in RDA is classified as class  X  C ( x )givenby
C ( x ) = argmin i ( x  X   X  i ) T  X   X   X  1 i ( x  X   X  i )+ln where  X   X  i is defined in Eq. (7). The performance of RDA may be critically dependent on the value of the parameters  X  and  X  . Cross-validation is commonly used to estimate the optimal  X  and  X  from a finite set,  X  = { (  X  i , X  j ) } ,where i =1 ,  X  X  X  ,r ,and j =1 ,  X  X  X  ,s . The number of candidate pairs (  X ,  X  )is |  X  | = rs . In practice, a large number, rs , of candidate pairs is often desirable to achieve good classi-fication performance. However, with a large number of pa-rameter pairs, the computational cost of model selection for RDA may be prohibitive for HDLSS data, since it requires expensive matrix computations for each candidate pair.
A direct implementation of RDA as the one used in [8] involves the formation of  X   X  i and the inversion of  X   X  i i . The computation of the inversion of all k class covariance matrices takes O ( kd 3 ) time and is prohibitive for HDLSS data, where the data dimensionality d is large. This limits the applications of RDA to low dimensional data.
In this section, we first establish a key property of RDA, whichshowsthattheclassificationruleinRDAcanbede-composed into two components. The first component in-volves matrices of low dimensionality, while the second com-ponent involves matrices of high dimensionality. More im-portantly, we show that for a given test data point, the second component in the classification rule is constant for all classes, which has no effect on the classification and can be simply removed. Thus, the computational cost of RDA can be significantly reduced. We call this the decomposition property of RDA.

We show below that the essence of the decomposition property of RDA is that the first component of the clas-sification rule lies in the orthogonal complement of the null space of S t , which has low dimensionality for HDLSS data, while the second component lies in the null space of S t of high dimensionality.

Define the between-class scatter matrix S b ,usedindis-criminant analysis [9], as follows: It follows from the definition that We have the following result concerning the relationship be-tween the null space of S t and the null space of  X  i and S Lemma 3.1. Let  X  i , S b , S t ,and S w be defined as above. The null space of S t denoted as N ( S t ) is a subset of the null space, N ( S b ) of S b and a subset of the null space, N ( X   X  , for all i . That is, N ( S t )  X  N ( S b ) and N ( S t ) for all i .

Proof. Consider any x  X  N ( S t ). That is, S t x =0and x S t x = 0. From Eqs. (5) and (11), we have It follows that Since  X  i , for all i ,and S b are positive semi-definite, we have x
 X  i x = 0, for all i ,and x T S b x = 0. It follows that  X  and S b x = 0. Therefore, x also lies in the null space of  X  and S b .Hence, N ( S t )  X  N ( S b )and N ( S t )  X  N ( X 
Let S t = UDU T be the Singular Value Decomposition (SVD) [10] of S t ,where U is orthogonal, D = D t 0 00 , D t  X  t  X  t is diagonal and t =rank( S t ). Note that t  X  n . Partition U into U =[ U 1 ,U 2 ], where U 1  X  IR d  X  t and U IR d  X  ( d  X  t ) .Then U 2 lies in the null space of S t , i.e., S We have the following result concerning the decomposition structure of  X   X  i :
Lemma 3.2. Let U =[ U 1 ,U 2 ] be defined as above and let  X   X  i be defined as in Eq. (7). Then  X   X  i can be expressed as where and  X   X  i = U T 1  X  i U 1 .

Proof. Recall from Eq. (7) that It follows that
U T  X   X  i U =  X   X U T  X  i U +(1  X   X  ) U T S t U +(1  X   X  ) I From Lemma 3.1,  X  i U 2 = 0, for all i . It follows that  X   X  i = U  X   X U T  X  i U +(1  X   X  ) U T S t U +(1  X   X  ) I d U T Lemma 3.2 implies that all k regularized class covariance matrices share a similar decomposition structure, which leads to the decomposition property of RDA as summarized in the following proposition: Proposition 3.1. Let U 1 , U 2 ,and M i be defined as above. Then the classification rule in Eq. (9) is equivalent to:  X 
C ( x )= argmin i ( x  X   X  i ) T U 1 M  X  1 i U T 1 ( x  X   X 
Proof. Denote F i =( x  X   X  i ) T  X   X   X  1 from Lemma 3.2 that
F i =( x  X   X  i ) T  X   X  The result follows directly from Lemma 3.2 and Eq. (16), as ln |  X 
 X  i | =ln | M i | +ln | (1  X   X  ) I d  X  t | =ln | M i | +(1 Proposition 3.1 implies that the classification rule in RDA can be decomposed into two components as in Eq. (15). The first component, i.e., ( x  X   X  i ) T U 1 M  X  1 i U T 1 ( x U , which lies in the orthogonal complement of the null space of S t , while the second component, i.e., (1  X   X  )  X  the null space of S t . Note that the null space of S t is of dimension d  X  t , which is much larger than the dimension, t , of the orthogonal complement of the null space of S t HDLSS data.

However, two issues need to be resolved before we apply the classification rule in Eq. (16). First, the computation may be numerically unstable as  X   X  1, due to the presence of (1  X   X  )  X  1 in the computation. Second, finding the best parameterpair(  X ,  X  ) from a set,  X , of candidate pairs may data. Interestingly, both issues can be addressed simultane-ously by simply removing the second term in Eq. (16), based on the lemma below: Lemma 3.3. Let U 2 ,  X  i and  X  be defined as above, then U Proof. Note that U 2 lies in the null space of S t .From Lemma 3.1, U 2 also lies in the null space of S b .Thatis, U 2 S b =0. S b inEq.(10)canbeexpressedas S b = H b H T b , where
H b = 1  X  It follows from U T 2 S b =0that U T 2 H b = 0, i.e., and U T 2 (  X  i  X   X  )=0. Hence, U T 2 ( x  X   X  i )= U T 2 ( x From Lemma 3.3, the classification rule in Eq. (15) can be further simplified by removing the second component as  X  C ( x ) = argmin i ( x  X   X  i ) T U 1 M  X  1 i U T 1 ( x where  X  x = U T 1 x and  X   X  i = U T 1  X  i .
The main computations in Eq. (18) are the inversion of M i and the determinant of M i , for all i ,whichtake O ( kt 3 O ( kn 3 )time,as M i  X  IR t  X  t and t  X  n . Recall from Sec-tion 2 that the direct implementation of RDA computes the inversion of  X   X  i for all i directly with the time complexity of O ( kd 3 ), which is significantly higher than O ( kn 3 )forHDLSS data. In the following, we present an efficient way of com-puting the inversion of M i and the determinant of M i , for all i , with a time complexity of O ( n 3 /k ), thus further reducing the complexity of the algorithm.
 Define the matrix H i  X  IR d  X  n i as follows: where A i  X  IR d  X  n i is the data matrix of the i -th class,  X  ones of length n i . Then the class covariance matrix,  X  the i -th class can be expressed as: It follows that where  X  H i = U T 1 H i . Denote D  X  X  as the diagonal matrix: From Eq. (14), We have M i =  X  X   X  H i  X  H T i + D  X  X  where It follows from the Sherman-Woodbury-Morrison formula [10] that Note that the matrix inversion in Eq. (24) is on However, the inversion M  X  1 i will not be formed explicitly, as the multiplication of (  X  x  X   X   X  i ) T M  X  1 i (  X  x time, for each x . Note from Eq. (24) that which takes O ( n 3 i ) for computing N  X  1 i and O ( nn i all other computations, for each x . The total complexity is thus O ( nn i + n 3 i )time,foreach x . Thus, the computation of (  X  x Assuming all classes are of approximately equal size, that is, n i  X  n/k , then the time complexity for the computation is O ( n 2 + n 3 /k 2 ).

One key observation here is that the computation of N  X  1 is independent of the test point x . Note that the total number of test points in v -fold cross-validation is n/v .In this case, the total computational cost for all test points is O ( n 3 + n 3 /k 2 ), instead of O ( n 3 + n 4 /k 2 ).
Next, we consider the computation of | M i | , which is inde-pendent of the test point x . From Eq. (23), where the last equality follows from the following lemma:
Lemma 3.4. Let X  X  IR t  X  n i be any matrix of size t by n . Then the following equality always holds: Proof. Let X = RDS T be the SVD of X ,where R and S are orthogonal and D =  X  m 0 00 is diagonal with  X  m = diag(  X  1 ,  X  X  X  , X  m )and m =rank( X ). It follows that Thus | XX T + I t | = | X T X + I n i | .

From Eq. (27), the time complexity of the computation of |
M i | , for all i ,is which is O ( n 3 /k ), assuming all classes are of approximately equal size.
Recall that the key matrix in the decomposition property of RDA in Proposition 3.1 is U 1 , which lies in the orthogo-nal complement of the null space of S t . Wehaveappliedthe SVD for computing U 1 as S t = UDU T ,where U =[ U 1 ,U 2 is a partition of U . When the data dimensionality d is large, the full SVD computation of S t  X  IR d  X  d is expensive. How-ever from Lemma 3.3, only the first component of the clas-sification rule, which involves U 1 , is effective in RDA, while U ,thenullspaceof S t can simply be omitted. Thus, U 1 can be computed efficiently witho ut the full SVD computation of S t as follows. Define matrix H t as: where  X  is the global centroid and e is the vector of all ones. It follows from the definition that S t = H t H T t that H t  X  IR d  X  n , which is much smaller than S t for HDLSS data. Let H t =  X  U  X   X   X  V T be the reduced SVD of H t ,where  X  U  X  IR d  X  t and  X  V  X  IR n  X  t have orthonormal columns and  X   X   X  IR t  X  t is diagonal with t =rank( H t ). It follows that S time complexity of the reduced SVD computation of H t is
Let  X  = { (  X  i , X  j ) } ,where i =1 ,  X  X  X  ,r and j =1 ,  X  X  X  be the candidate set for the regularization parameters. In model selection, v -fold cross-validation is applied, where the data is divided into v subsets of (approximately) equal size. All subsets are mutually exclusive, and in the i -th fold, the i -th subset is held out for test and all other subsets are used in training. For each (  X  i , X  j ), we compute the cross-validation accuracy, Accu( i, j ), defined as the mean of the accuracies for all folds. The best regularization pair (  X   X  i , X   X  j proposed algorithm is given below.
 Algorithm RDA
Input: data matrix A
Output: the optimal parameter pair (  X  i  X  , X  j  X  ) 1. For h =1: v /* v -fold cross validation */ 2. Construct A h and A  X  h ; 3. Construct H t using A h as in Eq. (28); 4. Compute the reduced SVD of H t : H t =  X  U  X   X   X  V T ; 5. t  X  rank( H t ); U 1  X   X  U ; D t  X   X   X  2 ; 7. Form {  X  H u } k u =1 based on A h L as in Eq. (19); 8. For i =1: r /*  X  1 , X  2 ,  X  X  X  , X  r */ 9. For j =1: s /*  X  1 , X  2 ,  X  X  X  , X  s */ 10. D  X  X   X  (1  X   X  i )  X  j D t +(1  X   X  j ) I t ; 11. For u =1: k 13. N  X  1 i  X  ( I + X T u X u )  X  1 as in Eq. (25), 14. Compute | M u | as in Eq. (27), 15. EndFor 16. temp  X  0; 17. For each  X  x  X  A  X  h L /*  X  x = U T 1 x and x  X  A  X  18. C ( x )  X  argmin u { ( x  X   X   X  u ) T M  X  1 u ( x  X   X   X  19. +ln | M u |} ; 20. If ( C ( x )== g ( x )) temp  X  temp +1; 21. EndFor 22. Accu( h, i, j )  X  temp/ | A  X  h L | ; 23. EndFor 24. EndFor 25. EndFor 26. Accu( i, j )  X  1 v v h =1 Accu( h, i, j ); /* Accu( i, j ) 27. ( i  X  ,j  X  )  X  arg max i,j Accu( i, j );
Line 4 takes O ( n 2 d ) time for the reduced SVD computa-tion [10]. Lines 5 and 6 take O ( dn 2 ) time for the matrix multiplications. The  X  X or X  loop from Line 11 to Line 15 takes O ( n 3 /k ) time. There are about n/v elements in A thus Line 18 to Line 20 within the  X  X or X  loop run about n/v times. Following the multiplication in Eq. (26), the computations from Line 18 to Line 20 take O ( n 2 )time,and the  X  X or X  loop from Line 17 to Line 21 take O ( n 3 /v )time. Thus, the double  X  X or X  loops from Line 8 to Line 26 take O n 3 rs (1 /k +1 /v ) time. The total running time of the algorithm is thus It follows that T ( r, s )
T (1 , 1) For HDLSS data, where the sample size n is much smaller than the data dimensionality d , i.e., n d , the overhead of estimating the optimal regularization pair among a large search space may be small.

Note that the first stage of RDA takes O ( vn 2 d )time, which is expensive for HDLSS data. However, it is inde-pendent of the parameters. In the second stage of RDA, the most expensive steps are the computations of M  X  1 i and |
M i | ,whichtake O vn dent of the data dimensionality d . Thisisthekeyreason why the proposed RDA algorithm is applicable for HDLSS data for a large candidate set of parameters.
We conclude this section by showing an interesting rela-tionship between RDA and Uncorrelated LDA (LDA) [23]. ULDA is an extension of the original formulation in [16] to high dimensional, small sample size data. It follows the basic framework of LDA [5, 7, 9, 14] that computes the optimal transformation (projection) by minimizing the ra-tio of the within-class distance to the between-class dis-tance, thus achieving maximum discrimination. One key property of ULDA is that the features in the transformed space are uncorrelated, thus ensuring minimum redundancy among the features in the reduced space. It has been ap-plied successfully in microarra y gene expression data anal-ysis [24]. It was shown in [22] that the optimal transfor-mation G of ULDA consists of the first q eigenvectors of S t S b ,where q =rank( S b ). With the computed G ,atest point x can be classified in ULDA as class h ,where h = arg min i || G T ( x  X   X  i ) || 2 . It has been shown in [22] that arg min Interestingly, we can show that the limit of RDA when  X   X  0and  X   X  1 is equivalent to ULDA as summarized in the following lemma:
Theorem 3.1. The classification rule in RDA approaches that of ULDA, when  X   X  0 and  X   X  1 . That is, if G is the transformation of ULDA. Then,
Proof. From Eq. (18), the classification rule in RDA is equivalent to  X 
C ( x ) = argmin i (  X  x  X   X   X  i ) T M  X  1 i (  X  x  X   X   X  From Eq. (14), we have lim  X   X  0 , X   X  1 M i = D t .Thus lim which is equivalent to the classification rule in ULDA.
 Theorem3.1showsthatULDAisaspecialcaseofRDA when  X  =0and  X  = 1. With a properly chosen param-eter pair (  X ,  X  ) through cross-validation, RDA is expected to outperform ULDA, which is confirmed by the empirical results presented in the next section.

Note that the limit of  X   X   X  1 i ,as  X   X  0and  X   X  1doesnot exist for HDLSS data, as the limit of is singular, when d&gt;n . However, Theorem 3.1 shows that the limit below exists: due to the decomposition property of RDA as in Eq. (18).
In this section, we experimentally evaluate the perfor-mance of the proposed RDA algorithm. v -fold cross vali-dation with v = 5 has been used in RDA for model selec-tion. All of our experiments have been performed on a P4 3.00GHz Windows XP machine with 2GB memory.
We have used three types of HDLSS data for the evalu-ation, including text documents, face images, and gene ex-pression data. The important statistics of these datasets are summarized below (see also Table 2): In this experiment, we test the efficiency of the proposed RDA algorithm. Table 3 shows the computational time (in seconds) of RDA on different numbers of parameter pairs r  X  s .Weset r = s for simplicity with r taking values from 1 to 32, thus the size of the candidate set, |  X  | ranges from 1 to 1024. It is clear from the table that the computational cost of RDA grows slowly as r  X  s is small. When r  X  s is large, the cost, T ( r, s ), of the proposed RDA algorithm is still sig-nificantly smaller than rsT (1 , 1), the computational cost of RDA without applying the optimizations proposed in this paper. 1 For example, we can observe that T (16 , 16) /T (1 , 1) on different datasets is less th an 7, which is significantly smaller than 16  X  16 = 256, while T (32 , 32) /T (1 , 1) is less than 25 in all cases, much smaller than 32  X  32 = 1024. Among all datasets, the document datasets have relatively larger increasing rates of running time than the others, while the gene expression datasets have the smallest increasing rates. Note that the ratio of the sample size to the data di-mensionality, i.e., n/d , is relatively large for both document datasets, while it is relatively small for both gene expression datasets. These results are co nsistent with the theoretical estimation of the efficiency in Section 3.
In this experiment, we evaluate RDA in classification and compare it with Uncorrelated LDA [23] and Support Vector Machines (SVM) [2, 4, 21]. For each dataset, we first set the percentage of the data for training to be either 1 / 2or1 / 3 (by a random partition). Then we apply the proposed RDA algorithm, as well as ULDA and SVM, on the training data to learn the model, which is further applied to the remaining
Note that the cost of RDA will be even higher if the decom-position property of RDA from this paper is not applied. Table 3: Computational time (in seconds) of RDA for different numbers of parameter pairs. test data to get the accuracy of classification. To give a better estimation of accuracy, the procedure is repeated 30 times and the resulting accuracies are averaged. Note that for RDA, we choose the optimal model from 900 parameter pairs with r =30and s = 30. Because of the improved efficiency of the proposed RDA algorithm, it is practical to select the optimal model from such a large search space.
The classification accuracies of the 30 different partitions for all six datasets are shown in Fig. 1 X 3. In Table 4, we report the mean accuracy and standard derivation of the 30 different partitions for each dataset, where ratio denotes the percentage of the data for training and is either 1 / 3or1 / 2in our experiments. For all the datasets, the performance using ratio =1 / 2 is better than that using ratio =1 / 3interms of classification accuracy. This conforms to our expectation that the classification performance may be improved with a larger number of training data. We can observe from the accuracy curves in Fig. 1 X 3 that RDA and SVM often follow similar trends.

For both document datasets re1 and re0, all three algo-rithms achieve comparable performance in re1 (all three ac-curacy curves in Fig. 1 are very close to each other), while RDA and SVM outperform ULDA in re0. For both face image datasets, RDA and SVM outperform ULDA by a large margin, while RDA achieves slightly higher accuracies than SVM. As for both gene expression datasets, the accu-racy curves are similar for all three algorithms, while RDA achieves a smaller overall variance than ULDA and SVM. Overall, RDA is very competitive with ULDA and SVM in classification. Recall from Theorem 3.1 that ULDA is a spe-cial case of RDA when  X  =0and  X  = 1. With a properly chosen parameters, RDA is expected to outperform ULDA, which is confirmed by our empirical results above.
We present in this paper a novel algorithm for RDA that is applicable for high dimensional, low sample size data. RDA is a compromise between LDA and QDA, regulated by two regularization parameters. A major advantage of the pro-posed RDA algorithm is its low computational cost in se-lecting the optimal parameters from a large candidate set, in comparison with the traditional RDA formulation. Thus it facilitates efficient model selection for RDA. The key to the proposed efficient model selection procedure lies in the decomposition property of RDA established in this paper. We evaluate the proposed algorithm using document, im-age, and gene expression datasets. RDA is compared with ULDA and SVM in classification. Results confirm the high efficiency of the proposed algorithm. Our experiments also demonstrate that with the proposed efficient model selection algorithm, RDA can be effectively applied to high dimen-sional, low sample size data.

The relative performance of RDA over ULDA varies a lot for different types of data. RDA outperforms ULDA for both image datasets by a large margin, while they are comparable for both gene expression datasets. One of the future work is to study the effect of the characteristics of the data on the performance of RDA. We also plan to apply RDA to other applications involving HDLSS data such as gene expression pattern images, protein expression data, etc.
 Table 4: Comparison of classification accuracy (in per-Research of JY is sponsored, in part, by the Center for Evo-lutionary Functional Genomics of the Biodesign Institute at the Arizona State University. [1] P.N. Belhumeour, J.P. Hespanha, and D.J. Kriegman. [2] C. J. C. Burges. A tutorial on support vector [3] L.F. Chen, H.Y.M. Liao, M.T. Ko, J.C. Lin, and G.J. [4] N. Cristianini and J.S. Taylor. Support Vector [5] R.O. Duda, P.E. Hart, and D. Stork. Pattern [6] S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison [7] R.A. Fisher. The use of multiple measurements in [8] J.H. Friedman. Regularized discriminant analysis. [9] K. Fukunaga. Introduction to Statistical Pattern [10] G. H. Golub and C. F. Van Loan. Matrix [11] T.R. Golub and et al. Molecular classification of [12] U. Grouven, F. Bergel, and A. Schultz.
 [13] P. Hall, J.S. Marron, and A. Neeman. Geometric [14] T. Hastie, R. Tibshirani, and J.H. Friedman. The [15] A. Hoerl and R. Kennard. Ridge regression: Biased [16] Z. Jin, J. Y. Yang, Z.S. Hu, and Z. Lou. Face [17] W.J. Krzanowski, P. Jonathan, W.V McCarthy, and [18] D.D. Lewis. Reuters-21578 text categorization test [19] D. L. Swets and J. Weng. Using discriminant [20] A. N. Tikhonov and V. Y. Arsenin. Solutions of [21] V.N. Vapnik. Statistical Learning Theory . Wiley, 1998. [22] J. Ye. Characterization of a family of algorithms for [23] J. Ye, R. Janardan, Q. Li, and H. Park. Feature [24] J. Ye, T. Li, T. Xiong, and R. Janardan. Using [25] E.J. Yeoh et al. Classification, subtype discovery, and [26] L. Zhang and L. Luo. Splice site prediction with [27] M. Zhang. Identification of protein coding regions in
