 The exact solution for the reinforcement learning (RL) and planning problems with large state space is difficult or impossible to obtain, so one usually has to aim for approximate solutions. Approximate Policy Iteration (API) and Approximate Value Iteration (AVI) are two classes of iterative algorithms to solve RL/Planning problems with large state spaces. They try to approximately find the fixed-point solution of the Bellman optimality operator.
 AVI starts from an initial value function V 0 (or Q 0 ), and iteratively applies an approximation of T , the Bellman optimality operator, (or T  X  for the policy evaluation problem) to the previous estimate, i.e., V k +1  X  T  X  V k . In general, V k +1 is not equal to T  X  V k because (1) we do not have direct access to the Bellman operator but only some samples from it, and (2) the function space in which V belongs is not representative enough. Thus there would be an approximation error  X  k = T  X  V k  X  V k +1 between the result of the exact VI and AVI.
 Some examples of AVI-based approaches are tree-based Fitted Q-Iteration of Ernst et al. [1], multi-layer perceptron-based Fitted Q-Iteration of Riedmiller [2], and regularized Fitted Q-Iteration of Farahmand et al. [3]. See the work of Munos and Szepesv  X  ari [4] for more information about AVI. API is another iterative algorithm to find an approximate solution to the fixed point of the Bellman it finds a Q 0 that satisfies T  X  0 Q 0  X  Q 0 . Afterwards, it performs a policy improvement step, which is to calculate the greedy policy with respect to (w.r.t.) the most recent action-value function, to get approximately evaluating the newly obtained policy  X  1 to get Q 1 and repeating the whole process again, generating a sequence of policies and their corresponding approximate action-value functions Q 0  X   X  1  X  Q 1  X   X  2  X   X  X  X  . Same as AVI, we may encounter a difference between the ap-proximate solution Q k ( T  X  k Q k  X  Q k ) and the true value of the policy Q  X  k , which is the solution by the Bellman residual of Q k (  X  k = Q k  X  T  X  k Q k ) or the policy evaluation approximation error (  X  API is a popular approach in RL literature. One well-known algorithm is LSPI of Lagoudakis and Parr [5] that combines Least-Squares Temporal Difference (LSTD) algorithm (Bradtke and Barto [6]) with a policy improvement step. Another API method is to use the Bellman Residual Mini-mization (BRM) and its variants for policy evaluation and iteratively apply the policy improvement mand et al. [9] introduced a nonparametric extension of LSPI and BRM and formulated them as an optimization problem in a reproducing kernel Hilbert space and analyzed its statistical behavior. Kolter and Ng [10] formulated an l 1 regularization extension of LSTD. See Xu et al. [11] and Jung and Polani [12] for other examples of kernel-based extension of LSTD/LSPI, and Taylor and Parr [13] for a unified framework. Also see the proto-value function-based approach of Mahadevan and Maggioni [14] and iLSTD of Geramifard et al. [15].
 A crucial question in the applicability of API/AVI, which is the main topic of this work, is to un-derstand how either the approximation error or the Bellman residual at each iteration of API or AVI affects the quality of the resulted policy. Suppose we run API/AVI for K iterations to obtain a policy  X 
K . Does the knowledge that all  X  k s are small (maybe because we have had a lot of samples and used powerful function approximators) imply that V  X  K is close to the optimal value function V  X  too? If so, how does the errors occurred at a certain iteration k propagate through iterations of API/AVI and affect the final performance loss? There have already been some results that partially address this question. As an example, Propo-Similarly for AVI, if the approximation errors are uniformly bounded ( k T  X  V k  X  V k +1 k  X   X   X  ), we expressed as the supremum norm of the approximation errors k V  X  k  X  V k k  X  or the Bellman error k Q k  X  T  X  k Q k k  X  . Compared to L p norms, the supremum norm is conservative. It is quite possible that the result of a learning algorithm has a small L p norm but a very large L  X  norm. Therefore, it is desirable to have a result expressed in L p norm of the approximation/Bellman residual  X  k . In the past couple of years, there have been attempts to extend L  X  norm results to L p ones [18, 17, 7]. As a typical example, we quote the following from Antos et al. [7]: Proposition 1 (Error Propagation for API  X  [7]) . Let p  X  1 be a real and K be a positive integer. Then, for any sequence of functions { Q ( k ) }  X  B ( X  X A ; Q max )(0  X  k &lt; K ) , the space of Q max -bounded measurable functions, and their corresponding Bellman residuals  X  k = Q k  X  T  X  Q k , the following inequalities hold: where R max is an upper bound on the magnitude of the expected reward function and This result indeed uses L p norm of the Bellman residuals and is an improvement over results like Bertsekas and Tsitsiklis [16, Proposition 6.2], but still is pessimistic in some ways and does not answer several important questions. For instance, this result implies that the uniform-over-all-may wonder if this condition is really necessary, and ask whether it is better to put more emphasis on earlier/later iterations? Or another question is whether the appearance of terms in the form of || performance loss of API/AVI algorithms. These bounds help one understand what factors contribute to the difficulty of a learning problem. We base our analysis on the work of Munos [17], Antos et al. [7], Munos [18] and provide upper bounds on the performance loss in the form of k V  X   X  V  X  k k 1 , X  (the expected loss weighted according to the evaluation probability distribution  X   X  this is defined in Section 2) for API (Section 3) and AVI (Section 4). This performance loss depends on a certain function of  X  -weighted L 2 norms of  X  k s, in which  X  is the data sampling distribution, and C  X , X  ( K ) that depends on the MDP, two probability distributions  X  and  X  , and the number of iterations K . In addition to relating the performance loss to L p norm of the Bellman residual/approximation er-ror, this work has three main contributions that to our knowledge have not been considered before: (1) We show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution, to be specified in Section 3, rather than its supremum. The dif-ference between this expectation and the supremum can be considerable. For instance, for a finite state space with N states, the ratio can be of order O ( N 1 / 2 ) . (2) The contribution of the Bell-man/approximation error to the performance loss is more prominent in later iterations of API/AVI. and the effect of an error term in early iterations decays exponentially fast. (3) There are certain structures in the definition of concentrability coefficients that have not been explored before. We thoroughly discuss these qualitative/structural improvements in Section 5. the theory of Markov Decision Processes (MDP) and reinforcement learning (RL) and a few other notations. For further information about MDPs and RL the reader is referred to [19, 16, 20, 21]. A finite-action discounted MDP is a 5-tuple ( X , A ,P, R , X  ) , where X is a measurable state space, A is the discount factor. The transition kernel P is a mapping with domain X  X  A evaluated at ( x,a )  X  X  X A that gives a distribution over X , which we shall denote by P (  X | x,a ) . Likewise, R is a mapping with domain X  X A that gives a distribution of immediate reward over R , which bounded by R max .
 A mapping  X  : X  X  A is called a deterministic Markov stationary policy, or just a policy in short. Following a policy  X  in an MDP means that at each time step A t =  X  ( X t ) . Upon taking action A t at X t , we receive reward R t  X  R (  X | x,a ) , and the Markov chain evolves according to X t +1  X  P (  X | X t ,A t ) . We denote the probability transition kernel of following a policy  X  by P  X  , i.e., P  X  ( dy | x ) = P ( dy | x, X  ( x )) .
 The value function V  X  for a policy  X  is defined as V  X  ( x ) , E h P  X  t =0  X  t R t X 0 = x i and the MDP, we define the optimal value and action-value functions by V  X  ( x ) = sup  X  V  X  ( x ) (  X  x  X  X ) and Q  X  ( x,a ) = sup  X  Q  X  ( x,a ) (  X  x  X  X ,  X  a  X  A ) . We say that a policy  X   X  is optimal holds for all x  X  X . Similarly, the policy  X  is greedy w.r.t. V , if for all x  X  X ,  X  ( x )  X  is chosen in an arbitrary deterministic manner). Greedy policies are important because a greedy pol-icy w.r.t. Q  X  (or V  X  ) is an optimal policy. Hence, knowing Q  X  is sufficient for behaving optimally (cf. Proposition 4.3 of [19]). r ( x,a ) +  X  R max a 0 Q ( x 0 ,a 0 ) P ( dx 0 | x,a ) .
 measures over  X  X . For a probability measure  X   X  M ( X ) and the transition kernel P  X  , we define distribution of states if the starting state distribution is  X  and we follow P  X  for m steps. In what follows we shall use k V k p, X  to denote the L p (  X  ) -norm of a measurable function V : X  X  R : Consider the API procedure and the sequence Q 0  X   X  1  X  Q 1  X   X  2  X   X  X  X   X  Q K  X  1  X   X  K , where  X  k is the greedy policy w.r.t. Q k  X  1 and Q k is the approximate action-value function for policy  X  (AE) at each iteration by formance loss k Q  X   X  Q  X  K k p, X  of the outcome policy  X  K .
 The choice of  X  and  X  is arbitrary, however, a natural choice for  X  is the sampling distribution of the data, which is used by the policy evaluation module. On the other hand, the probability distribution  X  reflects the importance of various regions of the state space and is selected by the practitioner. One common choice, though not necessarily the best, is the stationary distribution of the optimal policy. Because of the dynamical nature of MDP, the performance loss k Q  X   X  Q  X  K k p, X  depends on the difference between the sampling distribution  X  and the future-state distribution in the form of  X P  X  1 P  X  2  X  X  X  . The precise form of this dependence will be formalized in Theorems 3 and 4. Before stating the results, we require to define the following concentrability coefficients.
 Definition 2 (Expected Concentrability of the Future-State Distribution) . Given  X , X   X  M ( X ) ,  X   X  1 (  X  is the Lebesgue measure), m  X  0 , and an arbitrary sequence of stationary policies {  X  m } m  X  1 , let  X P  X  1 P  X  2 ...P  X  m  X  M ( X ) denote the future-state distribution obtained when the first state is distributed according to  X  and then we follow the sequence of policies {  X  k } m k =1 . Define the following concentrability coefficients that is used in API analysis: with the understanding that if the future-state distribution  X  ( P  X   X  ) m 1 ( P  X  ) m 2 (or c 1 , X , X  ( m 1 ,m 2 ;  X  ) =  X  (similar for others).
 Also define the following concentrability coefficient that is used in AVI analysis: tinuous w.r.t.  X  , then we take c VI , X , X  ( m 1 ,m 2 ;  X  ) =  X  .
 In order to compactly present our results, we define the following notation: Theorem 3 (Error Propagation for API) . Let p  X  1 be a real number, K be a positive integer, measurable functions defined on X  X A ) and the corresponding sequence {  X  k } K  X  1 k =0 defined in (1) or (2) , we have (a) If  X  k =  X  BR for all 0  X  k &lt; K , we have C (b) If  X  k =  X  AE for all 0  X  k &lt; K , we have C Consider the AVI procedure and the sequence V 0  X  V 1  X   X  X  X   X  V K  X  1 , in which V k +1 is the result of approximately applying the Bellman optimality operator on the previous estimate V k , i.e., V k +1  X  T  X  V k . Denote the approximation error caused at each iteration by The goal of this section is to analyze AVI procedure and to relate the approximation error sequence {  X  policy w.r.t. V K  X  1 . Theorem 4 (Error Propagation for AVI) . Let p  X  1 be a real number, K be a positive integer, and V {  X  where C In this section, we discuss significant improvements of Theorems 3 and 4 over previous results such as [16, 18, 17, 7]. 5.1 L p norm instead of L  X  norm As opposed to most error upper bounds, Theorems 3 and 4 relate k V  X   X  V  X  K k p, X  to the L p norm trasted with the traditional, and more conservative, results such as lim sup k  X  X  X  k V  X   X  V  X  k k  X   X  use of L p norm not only is a huge improvement over conservative supremum norm, but also allows us to benefit from the vast literature on supervised learning techniques, which usually provides error upper bounds in the form of L p norms, in the context of RL/Planning problems. This is especially interesting for the case of p = 1 as the performance loss k V  X   X  V  X  K k 1 , X  is the difference between the expected return of the optimal policy and the resulted policy  X  K when the initial state distribu-tion is  X  . Convenient enough, the errors appearing in the upper bound are in the form of k  X  k k 2 , X  which is very common in the supervised learning literature. This type of improvement, however, has been done in the past couple of years [18, 17, 7] -see Proposition 1 in Section 1. 5.2 Expected versus supremum concentrability of the future-state distribution formance loss k V  X   X  V  X  K k p, X  . Previously it was thought that the key contributing factor to the per-formance loss is the supremum of the Radon-Nikodym derivative of these two distributions. This is Nevertheless, it turns out that the key contributing factor that determines the performance loss is the expectation of the squared Radon-Nikodym derivative instead of its supremum. Intuitively this is very small, performance loss due to it is still small. This phenomenon has not been suggested by previous results.
 As an illustration of this difference, consider a Chain Walk with 1000 states with a single policy that drifts toward state 1 of the chain. We start with  X  ( x ) = 1 201 for x  X  [400 , 600] and zero everywhere is the uniform distribution. The result is shown in Figure 1a. One sees that the ratio is constant in the beginning, but increases when the distribution  X  ( P  X  ) m concentrates around state 1 , until it reaches steady-state. The growth and the final value of the expectation-based concentrability coefficient is much smaller than that of supremum-based. k Q  X   X  Q k k 1 for uniform and exponential data sampling schedule. The total number of samples is the same. [The Y -scale of both plots is logarithmic.] It is easy to show that if the Chain Walk has N states and the policy has the same concentrating m  X   X  . The ratio, therefore, would be of order  X ( this new analysis in a simple problem. One may anticipate that this sharper behavior happens in many other problems too.
 with N states and  X  is the uniform distribution, C  X   X  N but C L 2  X  other differences between our results and the previous ones, we get a performance upper bound shows a significant improvement. 5.3 Error decaying property Theorems 3 and 4 show that the dependence of performance loss k V  X   X  V  X  K k p, X  (or a very special structure in that the approximation errors at later iterations have more contribution to the final performance loss. This behavior is obscure in previous results such as [17, 7] that the depen-(see Proposition 1).
 This property has practical and algorithmic implications too. It says that it is better to put more effort on having a lower Bellman or approximation error at later iterations of API/AVI. This, for instance, can be done by gradually increasing the number of samples throughout iterations, or to use more powerful, and possibly computationally more expensive, function approximators for the later iterations of API/AVI.
 To illustrate this property, we compare two different sampling schedules on a simple MDP. The MDP is a 100 -state, 2 -action chain similar to Chain Walk problem in the work of Lagoudakis and Parr [5]. We use AVI with a lookup-table function representation. In the first sampling schedule, every 20 iterations we generate a fixed number of fresh samples by following a uniformly random walk on the chain (this means that we throw away old samples). This is the fixed strategy. In the exponential strategy, we again generate new samples every 20 iterations but the number of samples at the k th iteration is ck  X  . The constant c is tuned such that the total number of both sampling strategy is almost the same (we give a slight margin of about 0 . 1% of samples in favor of the fixed strategy). What we compare is k Q  X   X  Q k k 1 , X  when  X  is the uniform distribution. The result can be seen in Figure 1b. The improvement of the exponential sampling schedule is evident. Of course, one may think of more sophisticated sampling schedules but this simple illustration should be sufficient to attract the attention of practitioners to this phenomenon. 5.4 Restricted search over policy space One interesting feature of our results is that it puts more structure and restriction on the way policies may be selected. Comparing C PI , X , X  ( K ; r ) (Theorem 3) and C VI , X , X  ( K ; r ) (Theorem 4) with C  X , X  (Proposition 1) we see that: (1) Each concentrability coefficient in the definition of C PI , X , X  ( K ; r ) depends only on a single or m th term in C  X , X  has  X  1 ,..., X  m as degrees of freedom, and this number is growing as m  X  X  X  . (2) The operator sup in C PI , X , X  and C VI , X , X  appears outside the summation. Because of that, we only have K + 1 degrees of freedom  X  0 0 ,..., X  0 K to choose from in API and remarkably only a single degree of freedom in AVI. On the other other hand, sup appears inside the summation in the definition of C  X , X  . One may construct an MDP that this difference in the ordering of sup leads to an arbitrarily large ratio of two different ways of defining the concentrability coefficients. c coefficients). This special structure is hidden in the definition of C  X , X  in Proposition 1, and instead we have an extra m 1 degrees of flexibility.
 Remark 1. For general MDPs, the computation of concentrability coefficients in Definition 2 is difficult, as it is for similar coefficients defined in [18, 17, 7]. To analyze an API/AVI algorithm and to study its statistical properties such as consistency or con-vergence rate, we require to (1) analyze the statistical properties of the algorithm running at each iteration, and (2) study the way the policy approximation/Bellman errors propagate and influence the quality of the resulted policy.
 The analysis in the first step heavily uses tools from the Statistical Learning Theory (SLT) literature, e.g., Gy  X  orfi et al. [22]. In some cases, such as AVI, the problem can be cast as a standard regression with the twist that extra care should be taken to the temporal dependency of data in RL scenario. The situation is a bit more complicated for API methods that directly aim for the fixed-point solution (such as LSTD and its variants), but still the same kind of tools from SLT can be used too  X  see Antos et al. [7], Maillard et al. [8].
 The analysis for the second step is what this work has been about. In our Theorems 3 and 4, we have provided upper bounds that relate the errors at each iteration of API/AVI to the performance loss of the whole procedure. These bounds are qualitatively tighter than the previous results such as those reported by [18, 17, 7], and provide a better understanding of what factors contribute to the difficulty of the problem. In Section 5, we discussed the significance of these new results and the way they improve previous ones.
 Finally, we should note that there are still some unaddressed issues. Perhaps the most important one better understanding of this question alongside a good understanding of the way each term  X  k in E (  X  0 ,..., X  K  X  1 ; r ) behaves, help us gain more insight about the error convergence behavior of the RL/Planning algorithms.
 [1] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement [2] Martin Riedmiller. Neural fitted Q iteration  X  first experiences with a data efficient neural [3] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv  X  ari, and Shie Mannor. [4] R  X  emi Munos and Csaba Szepesv  X  ari. Finite-time bounds for fitted value iteration. Journal of [5] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine [6] Steven J. Bradtke and Andrew G. Barto. Linear least-squares algorithms for temporal differ-[8] Odalric Maillard, R  X  emi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh. Finite-[9] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv  X  ari, and Shie Mannor. [10] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares tempo-[11] Xin Xu, Dewen Hu, and Xicheng Lu. Kernel-based least squares policy iteration for reinforce-[12] Tobias Jung and Daniel Polani. Least squares SVM for least squares TD learning. In In Proc. [13] Gavin Taylor and Ronald Parr. Kernelized value function approximation for reinforcement [14] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A Laplacian framework [15] Alborz Geramifard, Michael Bowling, Michael Zinkevich, and Richard S. Sutton. iLSTD: El-[16] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and [17] R  X  emi Munos. Performance bounds in l p norm for approximate value iteration. SIAM Journal [18] R  X  emi Munos. Error bounds for approximate policy iteration. In ICML 2003: Proceedings of [19] Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete-Time [20] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive [21] Csaba Szepesv  X  ari. Algorithms for Reinforcement Learning . Morgan Claypool Publishers,
