 A prominent approach in collaborative filtering based rec-ommender systems is using dimensionality reduction (ma-trix factorization) techniques to map users and items into low-dimensional vectors. In such systems, a higher inner product between a user vector and an item vector indicates that the item better suits the user X  X  preference. Tradition-ally, retrieving the most suitable items is done by scoring and sorting all items. Real world online recommender systems must adhere to strict response-time constraints, so when the number of items is large, scoring all items is intractable.
We propose a novel order preserving transformation, map-ping the maximum inner product search problem to Eu-clidean space nearest neighbor search problem. Utilizing this transformation, we study the efficiency of several (approxi-mate) nearest neighbor data structures. Our final solution is based on a novel use of the PCA-Tree data structure in which results are augmented using paths one hamming dis-tance away from the query (neighborhood boosting). The end result is a system which allows approximate matches (items with relatively high inner product, but not necessar-ily the highest one). We evaluate our techniques on two large-scale recommendation datasets, Xbox Movies and Ya-hoo Music, and show that this technique allows trading off a slight degradation in the recommendation quality for a significant improvement in the retrieval time.
 H.5 [ Information systems ]: Information retrieval X  retrieval models and ranking, retrieval tasks and goals Recommender systems, matrix factorization, inner product search, fast retrieval
The massive growth in online services data gives rise to the need for better information filtering techniques. In the context of recommender systems the data consists of (1) the item catalog; (2) the users; and (3) the user feedback (ratings). The goal of a recommender system is to find for every user a limited set of items that have the highest chance to be consumed. Modern recommender systems have two major parts. In the first part, the learning phase, a model is learned (offline) based on user feedback 1 . In the second part, the retrieval phase, recommendations are issued per user (online). This paper studies the scalability of the retrieval phase (the second part) in massive recommender systems based on matrix factorization. Specifically, we introduce a new approach which offers a trade-off between running time and the quality of the results presented to a user.
Matrix Factorization (MF) is one of the most popular ap-proaches for collaborative filtering. This method has re-peatedly demonstrated better accuracy than other methods such as nearest neighbor models and restricted Boltzmann machines [2, 8]. In MF models, users and items are repre-sented by latent feature vectors. A Bayesian MF model is also at the heart of the Xbox recommendation system [16] which serves games, movies, and music recommendations to millions of users daily. In this system, users and items are represented by (low-dimensional) vectors in R 50 .Thequal-ity of the match between a user u represented by the vector x u and the item i represented by the vector y i is given by the inner product x u  X  y i between these two vectors. A higher inner product implies a higher chance of the user consuming the item.

The Retrieval Problem: Ideally, given a user u repre-sented by a vector x u , all the item vectors ( y 1 ,..., y examined. For each such item vector y i , its match quality with the user ( x u  X  y i ) is computed, and the items sorted according to their match quality. The items with the high-est match quality in the list are then selected to form the final list of recommendations. However, the catalog of items is often too large to allow an exhaustive computation of all the inner products within a limited allowed retrieval time.
The Xbox catalog consists of millions of items of various kinds. If a linear scan is used, millions of inner product com-putations are required for each single recommendation. The
This phase cannot be done entirely offline when a context is used to issue the recommended items. user vectors can take into account contextual information that is only available during user engagement. Hence, the complete user vector is computed online (at runtime). As a result, the retrieval of the recommended items list can only be performed online, and cannot be pre-computed offline. This task constitutes the single most computational inten-sive task imposed on the online servers. Thereby, having a fast alternative for this process is highly desirable.
Our Contribution: This paper shows how to signifi-cantly speed up the recommendation retrieval process. The optimal item-user match retri eval is relaxed to an approxi-mate search: retrieving items that have a high inner product with the user vector, but not necessarily the highest one. The approach combines several building blocks. First, we define a novel transformation from the inner product prob-lem to a Euclidean nearest neighbor problem (Section 3). As a pre-processing step, this transformation is applied to the item vectors. During item retrieval, another transformation is applied to the user vector. The item with the smallest Eu-clidean distance in the transformed space is then retrieved. To expedite the nearest neighbor search, the PCA-Tree [21] data structure is used together with a novel neighborhood boosting scheme (Section 4).

To demonstrate the effectiveness of the proposed approach, it is applied to an Xbox recommendations dataset and the publicly available Yahoo Music dataset [8]. Experiments show a trade-off curve of a slight degradation in the rec-ommendation quality for a significant improvement in the retrieval time (Section 5). In addition, the achievable time-accuracy trade-offs are compared with two baseline approaches, an implementation based on Locality Sensitive Hashing [1] and the current state of the art method for approximate rec-ommendation in matrix-factorization based CF systems [13]. We show that for a given required recommendation quality (accuracy in picking the optimal items), our approach allows achieving a much higher speedup than these alternatives.
Notation: We use lower-case fonts for scalars, bold lower-case fonts for vectors, and bold upper-case fonts for matrices. For example, x is a scalar, x is a vector, and X is a matrix. Given a vector x  X  R d ,let x i be the measure in dimension i ,with( x 1 ,x 2 ,...,x d ) T  X  R d . The norm is denoted by in Euclidean space x = d i =1 x 2 i .Wedenoteby x  X  y a dot product (inner product) between x and y . Finally, we use a, x T T to denote a concatenation of a scalar a with a vector x .
In this section we will explain the problem of finding best recommendations in MF models and review possible ap-proaches for efficient retrieval of recommendations.
In MF models, each user u is associated with a user-traits vector x u  X  R d , and each item i with an item-traits vector y i  X  R d . The predicted rating of a user u to an item i is denoted by  X  r ui and obtained using the rule:
The contextual information may include the time of day, recent search queries, etc. where  X  is the overall mean rating value and b i and b u repre-sent the item and user biases respectively. The above model is a simple baseline model similar to [14]. It can be readily extended to form the core of a variety of more complex MF models, and adapted to different kinds of user feedback.
While  X  and b i are important components of the model, they do not effect the ranking of items for any given user, and the rule  X  r ui = b i + x u  X  y i will produce the same set of recommendations as that of Equation 1. We can also concatenate the item bias b i to the user vector and reduce our prediction rule to a simple dot product:  X  r ui =  X  x  X  y ,where  X  x u (1 , x T u ) T ,and  X  y i ( b i , y T i ) T computing recommendations in MF models amounts to a simple search in an inner product space: given a user vector  X  x , we wish to find items with vectors  X  y i that will maximize the inner product  X  x u  X   X  y i . For the sake of readability, from this point onward we will drop the bar and refer to  X  x u  X  y u as x u and y i . We therefore focus on the problem of finding maximal inner product matches as described above. The problem of efficient retreival of recommendations in MF models is relatively new, but it has been discussed in the past [10, 11, 13]. In real-world large scale systems such as the Xbox Recommender, this is a concrete problem, and we identified it as the main bottleneck that drains our online resources.

Previous studies can be categorized into two basic ap-proaches. The first approach is to propose new recommen-dation algorithms in which the prediction rule is not based on inner-product matches. This was the approach taken by Khoshneshin et al. [10], who were first to raise the problem of efficient retrieval of recommendations in MF models. In [10] a new model is proposed in which users and items are em-bedded based on their Euclidean similarity rather than their inner-product. In a Euclidean space, the plethora of algo-rithms for nearest-neighbor search can be utilized for an effi-cient retrieval of recommendations. A similar approach was taken by [11] where an item-oriented model was designed to alleviate retrieval of recommendations by embedding items in a Euclidean space. While these methods show significant improvements in retrieval times, they deviate from the well familiar MF framework. These approaches which are based on new algorithms do not benefit the core of existing MF based recommender systems in which the retrieval of rec-ommendations is still based on inner-products.

The second approach to this problem is based on designing new algorithms to mitigate maximal inner-product search. These algorithms can be used in any existing MF based sys-tem and require only to implement a new data structure on top of the recommender to assist at the retrieval phase. For example, in [13] a new IP-Tree data structure was pro-posed that enables a branch-and-bound search scheme in inner-product spaces. In order to reach higher speedup val-ues, the IP-Tree was combined with spherical user clustering that allows to pre-compute and cache recommendations to similar users. However, this approach requires prior knowl-edge of all the user vectors which is not available in systems such as the Xbox recommender where ad-hoc contextual in-formation is used to update the user vectors. This work was later continued in [18] for the general problem of maximal inner-product search, but these extensions showed effective-ness in high-dimensional sparse datasets which is not the case for vectors generated by a MF process.

This paper builds upon a novel transformation that re-duces the maximal inner-product problem to simple nearest neighbor search in a Euclidean space. On one hand the pro-posed approach can be employed by any classical MF model, and on the other hand it enables using any of the existing algorithms for Euclidean spaces. Next, we review several alternatives for solving the problem in a Euclidean Space.
Locality Sensitive Hashing (LSH) was recently popular-ized as an effective approximate retrieval algorithm. LSH was introduced by Broder et al. to find documents with high Jaccard similarity[4]. It was later extended to other metrics including the Euclidean distance [9], cosine similarity [5], and earth mover distance [5].
 A different approach is based on space partitioning trees: KD-trees [3] is a data structure that partitions R d into hyper-rectangular (axis parallel) cells. In construction time, nodes are split along one coordinate. At query time, one can search of all points in a rectangular box and nearest neighbors effi-ciently. Several augmented splits are used to improve the query time. For example, (1) Principal component axes trees (PCA-Trees) transform the original coordinates to the principal components [21]; (2) Principal Axis Trees (PAC-Trees) [15] use a principal component axis at every node; (3) Random Projection Trees (RPT) use a random axis at each node [6]; and (4) Maximum Margin Trees (MMT) use a maximum margin axis at every node [20]. A theoretical and empirical comparison for some variants can be found [19].
Our approach makes use of PCA-trees and combines it with a novel neighborhood boosting scheme. In Section 5 we compare to alternatives such as LSH, KD-Trees, and PAC-Trees. We do not compare against MMT and RPT as we don X  X  see their advantage over the other methods for the particular problem at hand.
A key contribution of this work is focused on the concept of efficient reductions between search problems. In this sec-tion we formalize the concept of a search problem and show efficient reductions between known variants.
 We define a search problem as:
Definition 1. A search problem S ( I , Q ,s ) consists of an instance set of n items I = { i 1 ,i 2 ,...,i n } X  X  , a query q  X  X  ,andasearchfunction Function s retrieves the index of an item in I for a given query q . The goal is to pre-process the items with g : I X  I such that each query is answered efficiently. The pre-processing g can involve a transformation from one domain to another, so that a transformed search problem can oper-ate on a different domain. The following definition formal-izes the reduction concept between search problems:
Definition 2. A search problem S 1 ( I , Q ,s 1 ) is reducible to a search problem S 2 ( I , Q ,s 2 ) , denoted by S 1  X  there exist functions g : I X  X  and h : Q X  X  such that This reduction does not apply any constraints on the run-ning time of g and h .Notethat g runs only once as a pre-processing step, while h is applied at the query time. This yields a requirement that h has a O (1) running time. We formalize this with the following notation: Definition 3. We say that S 1  X  and the running time of g and h are O ( f ( n )) and O (1) re-spectively.

For a query vector in R d , we consider three search prob-lems in this paper: MIP , the maximum inner product from n vectors in R d (MIP n,d ); NN , the nearest neighbor from n vectors in R d (NN n,d ); MCS , the maximum cosine sim-ilarity from n vectors in R d (MCS n,d ). They are formally defined as follows: Instance: Amatrixof n vectors Y =[ y 1 , y 2 ,..., y n ]such Query: Avector x  X  R d ;hence Q = R d .
 Objective: Retrieve an index according to The following section shows how transformations between these three problems can be achieved with MCS n,d  X  O ( n
The triangle inequality does not hold between vectors x , y ,and y j when an inner product compares them, as is the case in MIP . Many efficient search data structures rely on the triangle inequality, and if MIP can be transformed to NN with its Euclidian distance, these data structures would immediately become applicable. Our first theorem states that MIP can be reduced to NN by having an Euclidian metric in one more dimension than the original problem. Theorem 1. MIP n,d  X  Proof: Let  X  max i y i and preprocess input with:  X  y i = g ( y i )=  X  2  X  y i 2 , y T i h ( x )= 0 , x T T .As we have  X  x  X   X  y i 2 =  X  x 2 +  X  y 2  X  2  X  x  X   X  y i = x 2 Finally, as  X  and x are independent of index i , Theorem 1 provides the main workhorse for our proposed approach (Section 4). In the remaining of this section, we present its properties as well the related transformations.
If it is known that the transformed  X  Y =[  X  y 1 ,  X  y is in a manifold, as given above, we might expect to recover Y by reducing back with NN n,d  X  O ( n ) MIP n,d  X  1 . However, in the general case the transformation is only possible by increasing the dimensionality by one again: Theorem 2. NN n,d  X  Proof: The preprocessing of the input:  X  y i = g ( y i )= y i 2 , y T i T . During query time:  X  x = h ( x )= 1 ,  X  We have  X  x  X   X  y i = y i 2  X  2 x  X  y i . Finally, MIP search can also be embedded in a MCS search by increasing the dimensionality by one: Theorem 3. MIP n,d  X  Proof: Preprocessing and query transformation are iden-tical to Theorem 1. The preprocessing of the input:  X  max i y i and let  X  y i = g ( y i )=  X  2  X  y i 2 , y T i ing query time:  X  x = h ( x )= 0 , x T T . Finally, j =argmax However, MCS is simply MIP searching over normalized vectors: Theorem 4. MCS n,d  X  Proof: The preprocessing of the input:  X  y i = g ( y )= y During query time:  X  x = h ( x )= x . Finally, Our final result states that a NN search can be transformed to a MCS search by increasing the dimensionality by one: Theorem 5. NN n,d  X  Proof: Same reduction as in Theorem 1. The prepro-cessing of the input:  X  max i y i and  X  y i = g ( y i )= 0 , x T T . Thus by Theorem 1, j =argmax
Next, we utilize Theorem 1 for speeding up retrieval of rec-ommendations in Xbox and other MF based recommender systems.
 Algorithm 1 TransformAndIndex( Y , d ) input: item vectors Y ,depth d  X  d +1 output: tree t compute  X  ,  X  , W
S =  X  for i =1: n do end for return T  X  PCA-Tree( S , d )
Our solution is based on two components, a reduction to a Euclidian search problem, and a PCA-Tree to address it. The reduction is very similar to that defined in Theorem 1, but composed with an additional shift and rotation, so that the MIP search problem is reduced to NN search, with all vectors aligned to their principal components.
We begin with defining the first reduction function follow-ing Theorem 1. Let  X  max i y i ,and which, when applied to Y , gives elements y i  X  R d +1 .This reduces MIP to NN .As NN is invariant to shifts and ro-tations in the input space, we can compose the transforma-tions with PCA rotation and still keep an equivalent search problem.

We mean-center and rotate the data: Let  X  = 1 n i y i be the mean after the first reduction, and M  X  R d +1  X  n matrix with  X  replicated along its columns. The SVD of the centered data matrix is where data items appear in the columns of Y .Matrix W is a ( d +1) by ( d + 1) matrix. Each of the columns of W =[ w 1 ,..., w d +1 ] defines an orthogonal unit-length eigenvector, so that each w j defines a hyperplane onto which each y i  X   X  is projected. Matrix W is a r otation matrix that aligns the vectors to their principal components. 3 We define the centered rotation as our second transformation, The composition still defines a reduction from MIP to NN .Using  X  y i = g ( y gives us a transformed set of input vectors  X  Y ,overwhichan Euclidian search can be performed. Moreover, after this transformation, the points are rotated so that their compo-nents are in decreasing order of variance. Next, we index the transformed item vectors in  X  Y using a PCA-Tree data structure. We summarize the above logic in Algorithm 1.
Notice that  X  is not included, as the Euclidian metric is invariant under rotations of the space, but not shears. Algorithm 2 PCA-Tree( S ,  X  ) input: item vectors set S ,depth  X  output: tree t if  X  =0 then end if j = d +1  X   X  // principal component at depth  X  m =median( {  X  y ij for all  X  y i  X  X } ) S  X  = {  X  y i  X  X  where  X  y ij  X  m }
S &gt; = {  X  y i  X  X  where  X  y ij &gt;m } t .leftChild = PCA-Tree( S  X  ,  X   X  1) t .rightChild = PCA-Tree( S &gt; ,  X   X  1) return t
Building the PCA-Tree follows from a the KD-Tree con-struction algorithm on  X  Y . Since the axes are aligned with the d + 1 principal components of Y ,wecanmakeuseofa KD-tree constriction process to get a PCA-Tree data struc-ture. The top d  X  d + 1 principal components are used, and each item vector is assigned to its representative leaf. Algorithm 2 defines this tree construction procedure.
At the retrieval time, the transformed user vector  X  x = h ( x ) is used to traverse the tree to the appropriate leaf. The leaf contains the item vectors in the neighborhood of  X  x , hence vectors that are on the same side of all the splitting hyperplanes (the top principal components). The items in this leaf form an initial candidates set from which the top items or nearest neighbors are selected using a direct ranking by distance.

The number of items in each leaf decays exponentially in the depth d of the tree. By increasing the depth we are left with less candidates hence trading better speedup values with lower accuracy. The process allows achieving differ-ent trade-offs between the quality of the recommendations and an allotted running time: with a larger d , a smaller proportion of candidates are examined, resulting in a larger speedup, but also a reduced accuracy. Our empirical analy-sis (Section 5) examines the trade-offs we can achieve using our PCA-trees, and contrasts this with trade-offs achievable using other methods.
While the initial candidates set includes many nearby items, it is possible that some of the optimal top K vec-tors are indexed in other leafs and most likely the adjacent leafs. In our approach we propose boosting the candidates set with the item vectors in leafs that are on the  X  X rong X  side in at most one of the median-shifted PCA hyperplane compared to  X  x . These vectors are likely to have a small Euclidean distance from the user vector.

Our PCA-Tree is a complete binary tree of height d , where each leaf corresponds to a binary vector of length d . We supplement the initial candidates set from the leaf of the user vector, with all the candidates of leafs with a Hamming distance of  X 1 X , and hence examine candidates from d of the 2 d leafs. In Section 5.1.1 we show that this approach is in-strumental in achieving the best balance between speedup and accuracy.
We use two large scale datasets to evaluate the speedup achieved by several methods: 1. Xbox Movies [12]  X  This dataset is a Microsoft pro-2. Yahoo! Music [8]  X  This is a publicly available ratings From both datasets we created a set of item vectors and user vectors of dimensionality d = 50. The following evaluations are based on these vectors.

We quantify the improvement of an algorithm A over an-other (naive) algorithm A 0 by the following term: In all of our evaluations we measure the speedup with re-spect to the same algorithm: a naive search algorithm that iterates over all items to find the best recommendations for every user (i.e. computes the inner product between the user vector and each of the item vectors, keeping track of the item with the highest inner product found so far). Thus denoting by T naive the time taken by the naive algorithm we have: T naive = X (#users  X  #items  X  d ).

The state of the art method for finding approximately optimal recommendations uses a combination of IP-Trees and user cones [13]. In the following evaluation we dubbed this method IP-Tree . The IP-Tree approach assumes all the user vectors (queries) are computed in advance and can be clustered into a structure of user cones. In many real-world systems like the Xbox recommender the user vectors are computed or updated online, so this approach cannot be used. In contrast, our method does not require having all the user vectors in advance, and is thus applicable in these settings.

The IP-Tree method relies on an adaptation of the branch-and-bound search in metric-trees [17] to handle nearest neigh-bor search in inner-product spaces. However, the construc-tion of the underlaying metric-tree data structure, which is a space partitioning tree, is not adapted to inner-product spaces (it partitions vectors according to Euclidean proxim-ity). By using the Euclidean transformation of Theorem 1, we can utilize the data structures and algorithms designed for Euclidean spaces in their original form, without adapta-tions that may curb their effectiveness. Next, we show that our approach achieves a superior computation speedup , despite having no access to any prior knowledge about the user vectors or their distribution. 4
We focus on online processing time, i.e. the time to choose an item to recommend for a target user. We ignore the computation time required by offline preprocessing steps.
Theorem 1 allows using various approximate nearest-neighbor algorithms for Euclidean spaces, whose performance depends on the specific dataset used. We propose using PCA-Trees as explained in Section 4.2, and show that they have an excellent performance for both the Xbox movies and Ya-hoo! music datasets, consisting of low dimensionality dense vectors obtained by matrix factorization. A different and arguably more popular approach for finding approximate-nearest-neighbors in Euclidean spaces is Locality-Sensitive Hashing (LSH) [1]. In the evaluations below we also in-clude a comparison against LSH. We emphasize that using both our PCA-Trees approach and LSH techniques is only enabled by our Euclidean transformation (Theorem 1).
Our approximate retrieval algorithms introduce a trade-off between accuracy and speedup. We use two measures to quantify the quality of the top K recommendations. The first measure Precision@K denotes how similar the approxi-mate recommendations are to the optimal top K recommen-dations (as retrieved by the naive approach): where L rec and L opt are the lists of the top K approximate and the top K optimal recommendations respectively. Our evaluation metrics only consider the items at the top of the approximate and optimal lists. 5
Ahighvaluefor Precision implies that the approximate recommendations are very similar to the optimal recommen-dations. In many practical applications (especially for large item catalogs), it is possible to have low Precision rates but still recommend very relevant items (with a high inner prod-uct between the user and item ve ctors). This motivates our second measure RMSE@K which examines the preference to the approximate items compared to the optimal items: where L rec ( k )and L opt ( k ) are the scores (predicted ratings) of the k  X  X h recommended item in the approximated list and the optimal list respectively. Namely, L rec ( k )and L opt are the values of inner products between the user vector and k  X  X h recommended item vector and optimal item vector respectively. Note that the amount L rec ( k )  X  L opt ( k ) is always positive as the items in each list are ranked by their scores.
Our initial evaluation considers three approximation algo-rithms: IP-Tree , LSH , and our approach (Section 4.2). Fig-ure 1(a) depicts Precision@10 for the Xbox Movies dataset (higher values indicate better performance). The Precision values are plotted against the average speedup values they enable. At very low speedup values the LSH algorithm shows the best trade-off between precision and speedup, but when higher speedup values are considered the LSH performance drops significantly and becomes worst. One possible rea-son for this is that our Euclidean transformation results in transformed vectors with one dimension being very large compared with the other dimensions, which is a difficult
Note that for this evaluation the recall is completely deter-mined by the precision .
 Table 1: A summary of the different tree ap-proaches. IP-Tree is the baseline from [13], which requires prior knowledge of the users vectors. All other approaches (as well as LSH) were not feasible before Theorem 1 was introduced in this paper. input distribution for LSH approaches. 6 In contrast, the tree-based approaches (IP-Tree and our approach) show a similar behavior of a slow and steady decrease in Precision values as the speedup increases. The speedup values of our approach offers a better precision-vs-speedup tradeoff than the IP-tree approach, though their precision is almost the same for high speedup values.

Figure 1(b) depicts the RMSE@10 (lower values indicate better performance) vs. speedup for the three approaches. The trend shows significantly superior results for our PCA-Tree approach, for all speedup values. Similarly to Fig-ure 1(a), we see a sharp degradation of the LSH approach as the speedup increases, while the tree-based approaches show a trend of a slow increase in RMSE values as the speedup in-creases. We note that even for high speed-up values, which yield low precision rates in Figure 1(a), the RMSE values remain very low, indicating that very high quality of rec-ommendations can be achieved at a fraction of the compu-tational costs of the naive algorithm. In other words, the recommended items are still very relevant to the user, al-though the list of recommended items is quite different from the optimal list of items.

Figure 2 depicts Precision@10 and RMSE@10 for the Ya-hoo! Music dataset. The general trends of all three algo-rithms seem to agree with those of Figure 1: LSH starts bet-ter but deteriorates quickly, and the tree-based approaches have similar trends. The scale of the RMSE errors in Fig-ure 1(b) is different (larger) because the predicted scores are in the range of 0-100, whereas in the Xbox Movies dataset the predictions are binary.

The empirical analysis on both the Xbox and Yahoo! datasets shows that it is possible to achieve excellent recommenda-tions for very low computational costs by employing our Eu-clidean transformation and using an approximate Euclidean nearest neighbor method. The results indicate that tree-based approaches are superior to an LSH based approach (except when the required speedup is very small). Further, the results indicate that our method yields higher quality recommendations than the IP-trees approach [13]. Note that we also compared Precision@K and RMSE@K for other K values. While the figures are not included in this paper, the trends are all similar to those presented above. A key building block in our approach is aligning the item vectors with their principal components (Equation 3) and using PCA-Trees rather than KD-Trees. Another essential
The larger dimension is the auxiliary dimension (  X  2  X  y i 2 )inEquation2. ingredient in our approach is the neighborhood boosting of Section 4.2.1. One may question the vitality of PCA-Trees or the neighborhood boosting to our overall solution. We there-fore present a detailed comparison of the different tree based approaches. For the sake of completeness, we also included acomparisonto PAC-Trees [15]. Table 1 summarizes the different data structures. Except the IP-Tree approach, all of these approaches were not feasible before Theorem 1 was introduced in this paper. Note that neighborhood boosting is possible only when the tree splits are all based on a single consistent axis system. It is therefore prohibited in IP-Tees and PAC-Trees where the splitting hyperplanes are ad-hoc on every node.

We compare the approach proposed in this paper with simple KD-Trees, PAC-Trees, and with PCA-Trees without neighborhood boosting (our approach without neighborhood boosting). Figure 3 depicts Precision@10 and RMSE@10 on the Yahoo! Music dataset. As the speedup levels increase, we notice an evident advantage in favor of PCA aligned trees over KD-Trees. When comparing PCA-Trees with-out neighborhood boosting to PAC-Trees we see a mixed picture: For low speedup values PCA-Trees perform better, but for higher speedup values we notice an eminent advan-tage in favor of PAC-Trees. To conclude, we note the overall advantage for the method proposed in this paper over any of the other tree based alternatives both in terms of Precision and RMSE .
We presented a novel transformation mapping a maximal inner product search to Euclidean nearest neighbor search, andshowedhowitcanbeusedtospeed-uptherecommenda-tion process in a matrix factorization based recommenders such as the Xbox recommender system.

We proposed a method for approximately solving the Eu-clidean nearest neighbor problem using PCA-Trees, and em-pirically evaluated it on the Xbox Movie recommendations and the Yahoo Music datasets. Our analysis shows that our approach allows achieving excellent quality recommen-dations at a fraction of the computational cost of a naive ap-proach, and that it achieves superior quality-speedup trade-offs compared with state-of-the-art methods. [1] Alexandr Andoni and Piotr Indyk. Near-optimal [2] Robert M. Bell and Yehuda Koren. Lessons from the [3] Jon Louis Bentley. Multidimensional binary search [4] Andrei Broder. On the resemblance and containment [5] Moses S. Charikar. Similarity estimation techniques [6] Sanjoy Dasgupta and Yoav Freund. Random [7] Gideon Dror, Noam Koenigstein, and Yehuda Koren. [8] Gideon Dror, Noam Koenigstein, Yehuda Koren, and [9] Piotr Indyk and Rajeev Motwani. Approximate [10] Mohammad Khoshneshin and W. Nick Street.
 [11] Noam Koenigstein and Yehuda Koren. Towards [12] Noam Koenigstein and Ulrich Paquet. Xbox movies [13] Noam Koenigstein, Parikshit Ram, and Yuval Shavitt. [14] Yehuda Koren, Robert M. Bell, and Chris Volinsky. [15] James McNames. A fast nearest-neighbor algorithm [16] Ulrich Paquet and Noam Koenigstein. One-class [17] Franco P. Preparata and Michael I. Shamos.
 [18] Parikshit Ram and Alexander Gray. Maximum [19] Parikshit Ram and Alexander G. Gray. Which space [20] Parikshit Ram, Dongryeol Lee, and Alexander G. [21] Robert F. Sproull. Refinements to nearest-neighbor
