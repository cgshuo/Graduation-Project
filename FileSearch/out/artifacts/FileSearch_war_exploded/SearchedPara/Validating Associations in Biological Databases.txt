 To cope with the large amount of biological sequences being produced, a significant number of genes and proteins have been annotated by automated tools. A protein annotation is an association between a protein and a term describing its role. These tools have produced a significant number of misannotations that are now present in biological databases. This paper proposes a new method for automatically scor-ing associations by comparing them to preexisting curated associations. An association is a pair that links two enti-ties. The score can be used to filter incorrect or uncommon associations.

We evaluated the method using the automated protein an-notations submitted to BioCreAtIvE, an international evalu-ation of state-of-the-art text-mining systems in Biology. The method scored each of these annotations and those scored below a certain threshold were discarded. The results have shown a small trade-off in recall for a large improvement in precision. For example, we were able to discard 44.6%, 66.8% and 81% of the misannotations, maintaining 96.9%, 84.2%, and 47.8% of the correct annotations, respectively. Moreover, we were able to outperform each individual sub-mission to BioCreAtIvE by proper adjustment of the thresh-old.
 H.2.8 [ Information Systems ]: Database Management Databases applications[Data Mining]; J.3 [ Life and Med-ical Sciences ]: Biology and genetics Algorithms, Experimentation Knowledge management, Biological databases, Filtering as-sociations Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
The large amount of data available nowadays has trans-formed the traditional way of conducting scientific work. However, the data generated is not always accurate and de-tecting errors and inconsistencies in the databases is an ex-pensive and arduous task. For example, traditional func-tional characterisation of genes and proteins cannot cope with the large amount of sequences being produced. There-fore, a significant number of genes and proteins have been functionally characterised by automated tools, which ex-trapolate functional annotations from similar sequences. How-ever, these tools have also produced a significant number of misannotations that are now present in the databases [16]. Some of these tools have been extrapolating new annotations from misannotations and are therefore spreading the errors. This happens because most databases do not distinguish between extrapolated and curated annotations. Functional characterisation is not normally linked to the experimental evidence that substantiates it, which makes it difficult to judge if it is correct.

This paper proposes a new approach to validate uncurated associations. An association is a pair that links two members of two entities. The proposed approach compares each mem-ber of the association with the members of known curated associations. The underlying intuition is that uncurated as-sociations having similar curated associations should also be correct. The intuition is motivated by the observation of the manual annotation technique adopted by biological curators, which consists in using preexisting curated information as a guide to evaluate uncurated biological data [8].

We applied the proposed approach to automatically filter protein misannotations by developing CAC (Correlate the Annotations X  Components), a novel heuristic method that scores uncurated annotations. A protein annotation is an association between a protein and a term describing its role. CAC requires minimal human intervention, since it takes advantage of publicly available domain knowledge, i.e. pre-viously curated annotations, to score each uncurated anno-tation. CAC avoids the complexities of creating rules and patterns covering all possible cases or creating training sets that are too specific to be extended to new domains [29]. Be-sides avoiding direct human intervention, automatically col-lected domain knowledge is usually much larger than manu-ally generated domain knowledge and does not become out-dated, since public databases can be tracked for updates as they evolve [10].

An example scenario where automated annotation sys-tems produced a significant number of misannotations was BioCreAtIvE (Critical Assessment of Information Extrac-tion systems in Biology) [22]. We applied CAC to all the annotations submitted to BioCreAtIvE, and CAC was able to obtain a good accuracy by discarding a significant num-ber of these misannotations. The results obtained by CAC in this task demonstrate the efficiency and feasibility of the proposed approach.

The remainder of this paper is organised as follows. Sec-tion 2 introduces the Gene Ontology. Section 3 describes BioCreAtIvE and discusses the results obtained by its par-ticipants. Section 4 describes CAC in detail. Section 5 presents the experimental evaluation of CAC using the an-notations submitted to BioCreAtIvE. Section 6 discusses the obtained results. Finally, Section 7 expresses our main con-clusions.
Biological databases annotate genes or proteins with state-ments that describe their biological role. Sometimes, these annotations are stored as ambiguous statements that are do-main specific and context dependent. To cope with this, the research community is developing and using BioOntologies to annotate genes and proteins [30]. Using a BioOntology to annotate genes or proteins avoids ambiguous statements that are domain specific and context dependent.

For example, the GO (Gene Ontology) is a well-established structured vocabulary that for example has been success-fully used for gene annotation of different species [18]. The GO project is one of the major efforts in Molecular Biology, for constructing a BioOntology of broad scope and wide ap-plicability. GO provides a structured controlled vocabulary of gene and protein biological roles, which can be applied to different species. GO comprised 20,069 distinct terms in De-cember 2005. Since the activity or function of a protein can be defined at different levels, GO has three different aspects: molecular function, biological process and cellular compo-nent. Each protein has elementary molecular functions that normally are independent of the environment, such as cat-alytic or binding activities. Sets of proteins interact and are involved in cellular processes, such as metabolism, signal transduction or RNA processing. Proteins can act in differ-ent cellular localisations, such as the nucleus or membrane. GO organises the concepts as a DAG (Directed Acyclic Graph), one for each aspect. Each node of the graph repre-sents a concept, and the edges represent the links between concepts (see example in Figure 1). Links can represent two relationship types: is-a and part-of. GO is a dynamic hier-archy: its content changes every month with the publication of a new release. Any user can request modifications to GO, which is maintained by a group of curators who add, re-move and change terms and their relationships in response to modification requests. This prevents GO from becoming outdated and from providing incorrect information.
GO started by adding generic terms and simple relation-ships to provide a complete coverage of the Molecular Biol-ogy domain. Thus, the main limitation of GO is the lack of specific terms that, for example, represent precise biochem-ical reactions. However, as different research communities understand the importance of adding their domain knowl-edge to GO, it will acquire more specific terms and relation-ships and therefore overcome this limitation.

Many databases are using GO terms to annotate their proteins. For example, the GOA (Gene Ontology Annota-tion) database provides GO annotations to supplement the UniProt (Universal Protein Resource) [8]. UniProt is a uni-versal repository of protein sequence and functional data [2]. GOA provides high-quality manual GO annotations, but manual curation is a time-consuming task that currently covers less than 5% of UniProt. The manual processing ca-pacity for gene and protein characterisation is overloaded by the increasingly larger amounts of literature to analyse. Thus, the GOA database mainly consists of uncurated anno-tations that have a lower quality than manual annotations.
A large amount of the information discovered in Molecu-lar Biology has been mainly published in BioLiterature (a shorter designation for the biological and biomedical scien-tific literature). Analysing and identifying information in a large collection of unstructured texts is a painful and hard task, even to an expert. To improve the access to the in-formation, most researchers also deposit their findings in a structured form in databases, such as UniProt, which collect and distribute biological information. However, the manage-ment of these databases also became a complex problem, and most of them contain a significant number of errors. More-over, most facts are only valid in a specific biological set-ting, and should not be directly extrapolated to other cases. Therefore, researchers cannot only rely in the facts available in these databases, they also need the evidence substantiat-ing the facts, which is normally present in the BioLiterature. The evidence text can be the description of the biological setting where the experiment was conducted or the subse-quent discussion of the results. In addition, different re-search communities have different needs and requirements at a given period in time. As these constraints evolve, its man-agement becomes harder to fulfil by databases, which have a static structure. Thus, researchers tend to use databases as an additional source to store and find facts, but the evi-dence substantiating them is still described as unstructured text, given its higher flexibility. As a consequence, a large amount of the knowledge acquired in Molecular Biology can only be found in the BioLiterature.

An approach to improve the access to the knowledge pub-lished in BioLiterature is to use Text Mining, which aims at automatically extracting knowledge from natural language text [20]. The application of text-mining tools to BioLitera-ture started just a few years ago [1]. Since then, the interest in the topic has been steadily increasing, motivated by the vast amount of documents that curators have to read to up-date biological databases, or simply to help researchers keep up with progress in a specific area [11]. Thus, bioinformat-ics tools are increasingly using Text Mining to collect more information about the concepts they analyse. Text-mining tools have mainly been used to identify: entities, such as genes, proteins and cellular components; relationships, such as protein localisation or protein interactions; events, such as experimental methods used to discover protein interac-tions.

An important application of text-mining tools is the au-tomatic annotation of genes and proteins. A gene or pro-tein annotation consists of a pair composed by the gene or protein and a description of its biological role. The biolog-ical role is often a concept from a BioOntology (e.g. GO). Using a BioOntology to annotate genes or proteins avoids ambiguous statements that are domain specific and context dependent. To understand the activity of a gene or protein, it is also important to know the biological entities that in-teract with it. Thus, the annotation of a gene or protein also involves identifying interacting chemical substances, drugs, genes and proteins.

Most of the manual annotation process done by the GOA team involves analysing the literature, which is a painful and hard task, even to an expert. Thus, the GOA team accepted to take part in BioCreAtIvE, to access the ability of text mining-systems for assisting curators in the annota-tion of UniProt proteins to GO terms. BioCreAtIvE was a challenging evaluation that compared the performance of different text-mining systems in solving common tasks using the same corpus. The tasks addressed meaningful challenges for text-mining systems and at the same time real problems of Biology. The biologically realistic scenarios posed addi-tional difficulties for the participants, which resulted in less successful performances than to the ones obtained in the Genomics TREC 2004, a similar challenging evaluation [21]. The subtask 2.2 of BioCreAtIvE aimed at predicting GO an-notations to human proteins from 200 new full-text articles from the Journal of Biological Chemistry. Table 1 shows the participants of this subtask and the approaches used. Each participant could submit three different sets of predictions to test the parameters of his system. Overall, there were 18 sets of submitted annotations that were individually evalu-ated.

For each scientific article, the participants had to sub-mit the list of annotations predicted by their system and evidence text for each annotation. Three curators of the GOA team manually evaluated each predicted annotation Algorithm 1 CAC Input: a predicted , an uncurated annotation predicted by an Output: conf idence  X  [0 , +  X  ], confidence score of the pre-1: conf idence ( a predicted )=0 7: propSim = P 8: conf idence ( a predicted )+= geneSim  X  propSim 9: end for and respective evidence [7]. They evaluated if the predicted GO term assignment was correct, or close to what a curator would choose. Sometimes, the GO term was in the correct lineage, but the curators considered it as incorrect because it was too generic or too specific. The GOA team consid-ered a submission correct when it contained both a correct annotation and a valid evidence text substantiating it.
The predictions submitted to this subtask achieved unac-ceptable levels of accuracy. The participant with the best accuracy identified 6% of all the correct annotations found by all the participants, and only 35% of his predictions were correct. The task addressed by BioCreAtIvE is representa-tive of the complexities that have to be faced in real bio-logical research environments. Without improvements, such automated systems are unhelpful to curators [26]. There-fore, techniques that could achieve good solutions to validate the automated annotations and improve their accuracy are much needed.
CAC assumes that an annotation is correct when there is at least a preexisting curated annotation composed by a similar gene (or protein) and a similar property. CAC considers an annotation as a pair ( g, p ), where g is a gene (or a protein) and p a biological property. For example, the annotations submitted to BioCreAtIvE were composed by a UniProt protein and a GO term that are instances of gene and property, respectively.

Algorithm 1 outlines CAC, which assigns a confidence score to a predicted , an annotation predicted by an automated system given as input. CAC also receives as input A curated a set of preexisting curated annotations collected from pub-lic databases, e.g. GOA.

CAC starts by assigning a zero confidence score to the predicted annotation (line 1). Next, CAC collects all the genes in the set of curated annotations (line 3). For each curated gene, CAC collects the properties annotated to it (line 5). Next, CAC calculates the similarity between the curated and the predicted genes (line 6), and calculates the similarity between the predicted property and each property annotated to the curated gene (line 7). CAC increments the confidence of the predicted annotation by the product of the gene similarity and the sum of all property similarities (line 8). Thus, the confidence only increases if both the gene similarity and at least one property similarity are larger than zero, i.e., if they are similar genes and have been annotated with at least one similar property.

However, the A curated set can contain groups of similar genes that are over-represented. In this case, the predicted annotations that contain genes with a large number of simi-lar curated genes will tend to have higher confidence scores. To overcome this problem, CAC calculates the number of curated genes similar to the predicted gene (line 10), and employs it as a damping factor (line 11). This factor re-duces the effect of the amount of similar curated genes in the confidence score calculation.
 CAC returns a confidence score of a predicted being correct. To filter the annotations predicted by an automated system, CAC scores each predicted annotation and discards those scored below a confidence threshold ( CT ). CAC is able to trade precision against recall by manipulating CT . Raising CT increases precision and decreases recall, lowering CT has the opposite effect.

CAC cannot score annotations without similar curated annotations. When the given predicted annotation has no similar curated genes ( SG = 0), CAC assigns a confidence score of +  X  to it. This means that the predicted annotation will never be filtered independently of the threshold used. Therefore, CAC does not discard new knowledge; instead, it gives the curators the opportunity to manually verify these potentially novel annotations.
The most popular way to calculate the similarity between two genes is by comparing their sequence [3]. However, se-quence similarity is not the only kind of structural similarity that can be computed between two proteins. Family sim-ilarity is also a structural similarity of a higher level than sequence similarity. Each family describes a set of related proteins, which can have identical molecular functions, are involved in the same process, or act in the same cellular location. Classifying proteins in families has been a com-mon technique to organise them according to their biologi-cal role. For example, the most successful large-scale effort for increasing the coverage of GO annotations within the UniProt database is based on the exploitation of family an-notations [8]. Unlike standard sequence similarity methods, family categorisation is normally based on experimental re-sults about protein domains, which represent some evolu-tionarily conserved structure and have implications on the protein X  X  biological role. geneSim was calculated from the number of shared Pfam families. Pfam is a database that provides a set of protein domains and families [4]. These families are constructed semi-automatically using hidden Markov models (HMMs). Each family describes a set of related proteins that can have identical molecular functions, are involved in the same pro-cess, or act in the same cellular location. This database contained 8183 families in December 2005. The UniProt database provides family assignments, where each protein is assigned to a set of Pfam families. This calculation can be improved by taking in account the sequence related to each Pfam family. For example, the length of the sequence and the percentage of similarity may constitute important factors to calculate the geneSim function. Apart from the sequence the geneSim could also use other type of informa-tion, e.g. gene expression profiles and evolutionary profiles.
CAC assumes that two properties are similar if one of them subsumes the other or if they have a common parent in the functional classification scheme, e.g. GO. To calculate the degree of similarity between properties, CAC can use a semantic similarity measure that combines the structure and content of a BioOntology with statistical information from corpus [27]. Recent projects investigated the use of semantic similarity measures over GO [14, 15, 24]. Their results demonstrated the feasibility of a semantic similarity measure in a biological setting. propSim was calculated using the measure proposed by Jiang&amp;Conrath, which is one of the most efficient semantic similarity measures [6, 23]. Jiang&amp;Conrath defined the se-mantic distance of two concepts in a corpus as the difference between their information content and the information con-tent of their most informative common ancestor. The infor-mation content of a concept is inversely proportional to its frequency in the corpus. Concepts that are frequent in the corpus have low information content. For example, the stop words (such as the ) that occur almost everywhere in the text normally provide little semantic information. The informa-tion content of a GO term was calculated as the number of proteins annotated with it. The ancestor of two GO terms having the largest information content was considered the most informative common ancestor of both terms. We implemented CAC as a Java/MySQL application [19]. The execution time of CAC is linearly proportional to the number of curated annotations used, which makes it scal-able. The performance of CAC is directly linked to the time spent on the calculation of both geneSim and propSim . geneSim can be implemented as a simple SQL query count-ing the number of shared families, and therefore it is not computational expensive. On the other hand, the calcula-tion of propSim is more complex but it is also not computa-tional expensive as it is demonstrated by FuSSiMeG (Func-tional Semantic Similarity Measure between Gene-Products), a web tool that measures the functional similarity between proteins based on the semantic similarity of the GO terms annotated to them [12]. FuSSiMeG is available on the Web 1 affording the similarity calculation on the fly.
In the subtask 2.2 of BioCreAtIvE, the participants an-notated the protein Lipid phosphate phosphohydrolase 1 to http://xldb.fc.ul.pt/rebil/tools/ssm/ the GO terms membrane and mRNA metabolism [5]. How-ever, only the assignment of membrane is correct. Below the results obtained by CAC for these two annotations are described.

The protein Lipid phosphate phosphohydrolase 1 belongs to the PF01569 family. For the annotation of this pro-tein to membrane, CAC found 91 curated proteins from the PF01569 family ( geneSim = 1) that were annotated to similar GO terms ( propSim &gt; 0) in GOA. From these 91 proteins, 21 were annotated to the same term. For exam-ple, the protein Lipid phosphate phosphohydrolase 2 belongs to the PF01569 family ( geneSim = 1) and is annotated to membrane and integral to membrane, which results in propSim =1 . 445297776. The confidence score resulted from these 91 proteins is 53 . 09, but since the PF01569 family con-tains 630 proteins ( SG = 629), CAC returned 53 . 09 639  X 
On the other hand, for the annotation of the protein Lipid phosphate phosphohydrolase 1 to mRNA metabolism, CAC only found one curated protein ( HH1165 ) from the PF01569 family ( geneSim = 1) that was annotated to a similar GO term ( metabolism )( propSim =0 . 1) in GOA. Thus, in this case CAC returned 0 . 1 639  X  0 . 0002.
We tested CAC to find how effectively it could discard the misannotations submitted to BioCreAtIvE independently of their evidence text. CAC scored each submitted annota-tion individually ( a predicted ), using the GOA annotations as the curated set of annotations ( A curated ). The annotations submitted to BioCreAtIvE 2 and the GOA 3 annotations are both publicly available on the Web. However, in the pub-licly available information there is no reference to the author of each annotation submitted to BioCreAtIvE. It is not even possible to know which annotations were submitted by the same system.

We decided not to increase the confidence of a predicted annotation based on curated annotations to the same pro-tein, i.e., the protein g predicted was discarded from G curated This way, CAC was restricted to score each predicted an-notation based only on curated annotations to similar but distinct proteins. This restriction ensures a fair evaluation of CAC by checking if CAC copes with proteins having no previously curated annotations.

The restriction increased the number of proteins for which it was not possible to obtain similar proteins, i.e., having SG = 0. However, only 455 out of the 3740 predicted anno-tations did not have a similar protein in the December 2004 release of GOA. These novel annotations have a precision of 7%, i.e., only 32 of them were correct. The assumption http://www.pdg.cnb.uam.es/BioLINK/workshop BioCreative 04/results/data/ ftp://ftp.ebi.ac.uk/pub/databases/GO/goa/UNIPROT/ gene association.goa uniprot.gz that supports CAC is not applicable to these novel anno-tations, thus scoring these annotations is out of CAC ob-jectives. CAC does not discard these annotations, since it assigns an infinite score to them. Therefore, in the first part of the evaluation these annotations were disregarded, but they were included in the end to show the overall impact of CAC on the curation process.
 The 3285 annotations having SG &gt; 0 assign 1239 distinct GO terms to 77 UniProt proteins. The 77 proteins were assigned to 87 distinct Pfam families with an average of 1.6 families per protein. These 87 families contained 64863 distinct proteins. Thus, each protein had 64863 87  X  1 . 6= 1192 . 9 similar curated proteins on average.

To compare the performance of CAC when applied to over-annotated or under-annotated proteins, the 3285 an-notations were divided in three different sets ( Set-1 , Set-2 and Set-3 ) according to the number of similar curated pro-teins ( SG ). Table 2 shows statistical information about each set.
Each distinct confidence score was used as a confidence threshold to obtain different subsets of the 3285 predicted annotations. For each confidence threshold, the resulting subset contains all the annotations with a confidence score not below the threshold. For a zero confidence threshold, the subset contains all the predicted annotations, since none of them is discarded. As the confidence threshold increases, the size of the subset decreases. For each subset, it was cal-culated: the precision, representing the fraction of correct annotations in the subset; the recall, representing the num-ber of correct annotations in the subset over the number of correct annotations in the original set; and the F-measure = cision and recall. Note that if we replace CAC by a random model to filter the annotations, the precision would remain constant. For instance, if we select at random 25% of the annotations in the original set, it is predictable that the selected annotations also contain 25% of the correct anno-tations in the original set.
 Only 227 out of the 3285 annotations submitted to BioCre-AtIvE were considered correct, a precision of 6.9%. The real recall is unknown, since the organisation of BioCreAtIvE did not measure it. Thus, we can assume a recall of 100% for the original set of annotations. Note that CAC cannot increase recall. As a filter, it does not generate new annotations.
Figure 2(a) shows the F-measure for different confidence thresholds. For confidence thresholds smaller than one, the chart shows that the use of CAC to discard annotations is beneficial by achieving a substantial improvement in F-measure. The F-measure achieves its maximum value when the confidence threshold is around 0.1. Figure 2(c) shows the precision and recall obtained for different confidence thresh-olds. With a few exceptions, we have a steadily increase in precision as we increase the confidence threshold.
Table 3 shows the accuracy of the predicted annotations when not using CAC ( CT = 0), and the accuracy of the sub-sets of annotations retained by different confidence thresh-olds. Besides the precision, recall and F-measure, the Table shows the number of correct and incorrect annotations that were not discarded by CAC, and the percentage of misanno-The All Proteins lines represent all the 3285 annotations. The all the other annotations not present in Set 1 and Set 3.
The without CAC baselines represent the original constant precision for any filter rate.
 tations discarded by CAC from the original set. For exam-ple, by using CT =0 . 001 CAC discarded 50.8% ( 3058  X  1506 of the misannotations, maintaining 96.5% ( 219 227 ) of the cor-rect annotations.

The confidence threshold has no biological meaning to cu-rators. They simply would like to discard a given amount of annotations to speedup the curation process without loosing a significant part of valuable information. This can be done by increasing CT until a defined filter rate is reached. The filter rate means the percentage of annotations that are dis-carded by CAC from the original set. For example, a filter rate of 90% means that only 10% of the original annotations were retained. Figure 2(b) shows the F-measure obtained by CAC for different filter rates. The chart shows that the use of CAC to discard annotations is beneficial by achieving a steady improvement in F-measure as we increase the filter rate, except for filter rates larger than 99% ( CT &gt; 1). Ta-ble 4 shows the precision and the recall of the different sets of annotations over different filter rates, together with the selected CT in each set. The standard deviation of both recall and precision is always smaller than 5% for the same filter rate, even with a standard deviation of 0.8% in preci-sion in the original sets. The selected CT is almost the same in all sets, except in the Set-3 where in some cases CT is about 1 / 3 smaller. The increase in precision is already a positive result to GOA curators, since they primarily require high precision in an automated annotation system. In this experiment, All Proteins CAC increased precision at the cost of a low decrease in re-call. The trade-off between precision and recall is worth it, as it is shown by the increase in the F-measure. This is al-ways true except for filter rates larger than 99% ( CT &gt; 1), because recall decreases and precision is not improved. For such high confidence thresholds, there are still some mis-annotations not discarded. For example, CAC assigned a high confidence score to the annotation that assigns the GO term kinase activity to the protein Sulfate transporter 1.2, but this annotation is not in GOA. However, the GO term protein kinase activity is annotated to the same protein in GOA. Since the term kinase activity is a generalisation of protein kinase activity, the predicted annotation is correct but still not of interest to curators.

From 3058 misannotations, four remain with a confidence threshold of one. These four annotations are not defined in GOA because all of them assign generic GO terms to proteins. This does not mean that they represent incor-rect assignments, they are only too generic to be of interest to curators. Since in reality these generic annotations are correct, CAC does not discard these annotations even with large confidence thresholds. This explains the sharp drop in precision when recall is close to zero in Figure 2(c), be-cause these generic annotations were considered incorrect in our assessment. Thus, by considering generic annotations as correct, the performance of CAC would increase, but this would not reflect the curators X  interest for precise and spe-cific annotations. Nevertheless, it is undesirable to discard these generic annotations, since the evidence substantiating them may be of interest to curators.

The participant of BioCreAtIvE who achieved the largest precision predicted 41 annotations, 14 of which were cor-rect. Using a confidence threshold of 1, CAC selected 43 annotations, 39 of which were correct. On the other hand, the participant who achieved the largest recall predicted 661 annotations, 78 of which were correct. Using a confidence threshold of 0 . 1, CAC selected 327 annotations, 92 of which were correct. Therefore, by proper adjustment of the con-fidence threshold we can use CAC to outperform each indi-vidual submission to BioCreAtIvE.

For a small decrease in recall, CAC was able to obtain a large improvement in precision, since annotations that clearly do not satisfy the correlation between structure and function are normally incorrect. Unfortunately, there are exceptions. Using a confidence threshold of 0.001, CAC dis-carded 8 out of 227 correct annotations. For these eight an-notations, CAC could not find similar annotations mainly because of the restriction that discarded curated annota-tions to similar but distinct proteins. When CAC was tested without this restriction, 47% of the misannotations were dis-carded maintaining all the correct annotations, i.e., a two-fold increase in precision maintaining 100% recall. This re-striction was applied to ensure a fair evaluation of CAC. However, in a real application setting, this restriction would not be applied and therefore obtain a higher performance. It is expected that, as the scientific community produces better classification schemes, CAC will also improve its per-formance.

The results of the three different sets of annotations show that CAC is not biased toward proteins with a large num-ber of similar curated proteins. In Figure 2, the results of these sets were uniform over all the confidence thresholds. The small differences are due to different precision values of each original set. The Set 1 of under-annotated proteins has the highest precision (7.5%) and the Set 3 of over-annotated proteins has the lowest precision (5.8%). The Set 1 achieves a precision of 100% for a recall larger than 20%, because any correct annotation to under-annotated proteins is of in-terest to curators, i.e., the problem of generic annotations described above is not applicable to these proteins.
The results show that the performance obtained by a given filter rate is preserved when applied to different sets of an-notations. Therefore, curators can expect to obtain similar performances in different sets of annotations by using simi-lar filter rates. Using different sets of curated and uncurated annotations may imply different CT for obtaining the same filter rate. For example, the uncurated annotations in Set-3 have more similar curated annotations, thus it is also ex-pected to have larger confidence scores. However, curators can easily adjust CT to obtain a required filter rate.
CAC does not discard new knowledge, but it does not dis-card the misannotations to under-annotated proteins either. To measure the real impact of using CAC on the curation process it should take into account the 455 novel annota-tions. CAC never discards these annotations, leaving the decision to the curator by assigning an infinite confidence score to them. Table 5 shows that including these novel annotations has a small effect on the performance of CAC. For example, by using a filter rate of 41.7% ( CT =0 . 001) the curator only has to verify 58.3% (100%-41.7%) of the original annotations only loosing 3.1% (100%-96.9%) of the correct annotations. However, the precision for large filter rates is constrained by the precision of the novel annota-tions. Since CAC does not discard any of the 455 novel an-notations, the precision converges to 7% (32 out of 455 anno-tations are correct) as CT increases. Nevertheless, CAC can overcome this limitation and contribute toward adding new knowledge. Nowadays, there are automated systems that predict generic annotations with high precision. If these generic annotations were considered, CAC would use them to score specific annotations, which is what curators really want. CAC can also be used to crosscheck annotations pre-dicted by different automated systems. For example, CAC can score annotations predicted by a text-mining system based on annotations predicted by sequence similarity.
This paper proposed the use of curated associations as domain knowledge for scoring uncurated associations. To demonstrate its feasibility and efficiency, we developed and evaluated CAC, which scores uncurated annotations based on similar curated annotations. The results obtained in a realistic scenario show that CAC can effectively be used to speed up the curation process by discarding a large amount of misannotations without loosing a significant amount of correct annotations. Thus, CAC can be used by any auto-mated annotation system to improve the accuracy and to reduce the effort of curators. Its main advantage is that it requires minimal human intervention, since CAC uses exten-sive domain knowledge automatically collected from public databases.

The precision/recall trade-off is tunable by a method X  X  confidence threshold, which can be adjusted to obtain dif-ferent filter rates according to the curator X  X  requirements. The results obtained by similar filter rates were consistent for different subsets of the annotations, so the effectiveness of CAC is predictable as we change a single tuning param-eter.

One may argue that the proposed approach is only effec-tive when there is a substantial amount of curated informa-tion available. However, the amount of curated information will tend to increase as more genes are characterised, espe-cially for model organisms (e.g. human) whose characterisa-tion has a great fundamental economical and social impact. On the other hand, the percentage of curated genes will tend to decrease, since the manual characterisation efforts are powerless to overcome the huge amount of data being generated by high-throughput analysis tools. Therefore, au-tomatic methods, such as CAC, are much required to help the characterisation efforts.

CAC can be easily adapted to score associations between other objects than genes and biological properties. All it requires is a similarity measure for each kind of object used and a set of curated associations. [1] M. Andrade and P. Bork. Automated extraction of [2] R. Apweiler, A. Bairoch, C. Wu, W. Barker, [3] T. Attwood and D. Parry-Smith. Introduction to [4] A. Bateman, L. Coin, R. Durbin, R. Finn, V. Hollich, [5] C. Blaschke, E. Leon, M. Krallinger, and A. Valencia. [6] A. Budanitsky and G. Hirst. Semantic distance in [7] E. Camon, D. Barrell, E. Dimmer, V. Lee, [8] E. Camon, M. Magrane, D. Barrell, V. Lee, [9] J. Chiang and H. Yu. Extracting functional [10] F. Couto, B. Martins, and M. Silva. Classifying [11] F. Couto and M. Silva. Advanced Data Mining [12] F. Couto, M. Silva, and P. Coutinho. Implementation [13] F. Couto, M. Silva, and P. Coutinho. Finding genomic [14] F. Couto, M. Silva, and P. Coutinho. Semantic [15] F. Couto, M. Silva, and P. Coutinho. Measuring [16] D. Devos and A. Valencia. Intrinsic errors in genome [17] F. Ehrler, A. Jimeno, and P. Ruch. Data-poor [18] GO-Consortium. The Gene Ontology (GO) database [19] M. Grand. Java Language Reference . O X  X eilly, 1997. [20] M. Hearst. Untangling text data mining. In Proc. of [21] W. Hersh, R. Bhuptiraju, L. Ross, P. Johnson, [22] L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia. [23] J. Jiang and D. Conrath. Semantic similarity based on [24] P. Lord, R. Stevens, A. Brass, and C. Goble. Semantic [25] S. Ray and M. Craven. Learning statistical models for [26] D. Rebholz-Schuhmann, H. Kirsch, and F. Couto. [27] P. Resnik. Using information content to evaluate [28] S. Rice, G. Nenadic, and B. Stapley. Mining protein [29] H. Shatkay and R. Feldman. Mining the biomedical [30] R. Stevens, C. Wroe, P. Lord, and C. Goble. Handbook [31] K. Verspoor, J. Cohn, C. Joslyn, S. Mniszewski,
