 Learning to rank is a cross field of machine learning and information retrieval. It aims to learn the rank function with the relevance judgments from training set, while the rank function is used to sort by the document relevance. The core issue of the learning to rank is how to construct a model or function to predict the relevance of the docu-ments. Ranking task is defined as follows. The training data (referred as D), which consists of a set of records of the form &lt; q, d, r &gt;, where q is a query, d is a document (represented as a list of features {f 1 , f 2 , ..., f m }), and r is the relevance judgment of d to q. The relevance draws its values from a discrete set of possibilities (e.g., 0, 1). The test set (referred as T) consists of records &lt; q, d ,? &gt; in which only the query q and the document d are known, while the relevance judgment of d to q is unknown. The train-ing data is used to construct a model which relates features of the documents to their corresponding relevance in order to predict the relevance of the document of test data for getting ranking list. 
Although many algorithms and models based on labeled data set were introduced to learning to rank, the research on unlabeled data is still insufficient. In view of this, this paper deals with the data set composed of labeled and unlabeled data by SVD, introducing the information of unlabeled data into the training set, in order to improve the performance of the RankBoost and the ranking result. 
The paper is divided as follows: Section 2 introduces the related work about learn-ing to rank. Section 3 proposes the idea that to use SVD introduce the unlabeled data information. Then section 4 presents the experimental results. Finally, we conclude this work and point out some directions for future research. Many models and theories have been introduced to the field of learning to rank for obtaining the ranking function. Freund et al. use AdaBoost to the web meta-search task, and propose the RankBoost algorithm [1] for learning to rank. Herbrich et al. propose a learning algorithm for ranking on the basis of Support Vector Machines, called RankSVM [2]. Zhe Cao et al. propose the Listnet [3] based on neural network and gradient descent. Guiver et al. get satisfactory ranking result for introduce Gaus-sian processes [4] to the task. Adriano Veloso et al. extract the interval association rules from features to improve the ranking performance [5]. Although they are rewarded by a pretty success, none of them utilize the unlabeled data to improve the results. 
Information retrieval and machine learning algorithms used to ranking task achieve a great success, most of the experts and scholars on the basis of using labeled data sets for researching. It appears inadequate on the study of unlabeled data. However, some scholars try to improve the performance of ranking on unlabeled data set. They pro-pose some methods to introduce unlabeled corpus to training model. Massih-Reza Amini et al, [6] assume that an unlabeled instance that is similar to a labeled instance should have similar label, and introduce the instance to the training set. The experi-mental results show it can improve the ranking result. Kevin Duh et al. apply kernel PCA (kernel-based principal component analysis) [7] on the test set mode principal component pattern extraction, and use this pattern to extract new features from the training set. And thus it is adding the information of test set to training set implicitly, and gets good ranking result. This method is only adding test set of information to the training set, and it does not take introducing the training set information to the test set to improve the performance of ranking model prediction. In addition, it does not take the impact of the feature normalization in to account for principal component pattern extraction. These two issues are the main research contents of this paper. 
In this paper, drawing on previous research methods, we try to make more efficient apply unlabeled corpus to improve the predicting model for the relevance using a new idea originated from transfer learning. SVD is more suitable for mining information and relationships in the document features. In this paper, there are two phases to rank-ing task. In phase 1, we use SVD to look for the new features as a preprocessing step. The training set is combined with test set; the combined collection is processed by singular value decomposition. In this way not only it can introduce the unlabeled information but also the two feature sets can be projected to a same dimension space for extracting new set of feature vectors. Further more we deal with the feature vector by normalizing at query level, then select the appropriate feature subset to SVD processing; finally extract new feature vectors from decomposed feature set. In phase 2, we use the RankBoot to learn a new ranking model from the feature set added new features, so as to improve the ranking performance. Feature-representation-transfer is one of transfer learning methods. Its aim is to look for  X  X ood X  features in order to reduce the difference of source domain and target domain and lower error rate of model predicting. These features are generated by the pivot features that are contained in both source domain and target domain. In this paper we propose an approach similar to the SCL (Structural Correspondence Learn-ing) [8] that is often used to structure new features for transfer learning, we use SVD to do that to introduce the unlabeled data information. The training set and test set are seemed as source domain and target domain; all the features of them are used as pivot features. it is different that we simply use SVD to process the set(has been normal-ized) merged by training set and test set in order to construct association relation with the labeled data(training set) and unlabeled data(test set),and introduce the informa-tion to each other at the same time. Unlabeled data information can be used in Rank-Boost model training process, while it is useful for predicting the relevance of the document in the test set. Meanwhile, SVD is a method for extracting principal com-ponent features; this method can implicitly obtain the more effective feature vectors. By adding such identical distribution feature vectors to the training set and test set it is meaningful to the iterative training and th e relevance predicting. After SVD, the ei-genvectors with the largest eigenvalues can form a projection matrix. RankBoost is based on training the features to obtain the ranking model for predicting the document relevance, so it is helpful to expend the feature set, especially by adding the features which contains unlabeled data information for improving the performance of ranking model. Therefore, this paper apply RankBoost for training ranking model with new feature set to study whether it is helpful to introduce the SVD to rank. 3.1 SVD for Extracting Features Singular value decomposition (SVD) is a feature extraction methods based on matrix transformation. After SVD, we can get tree matrixes: one is the document-potential feature matrix U, the second is the eigenvalue matrix S, and the third is the potential feature-original feature matrix P. The eigenvalue shows how much information of original matrix, which its corresponding eigenvector contains. The bigger the eigen-value is, the more information the eigenvector contains. So we choose the SVD-feature subset F based on the eigenvalues. The method is as table 1 showing. 
Algorithm 1 shows the pseudo code for this SVD-feature extracting. After that, we obtain the new training set which is introduced the test set information, and the new test set also contains the information of training set. Naturally, we project the new features to the same dimensional space for next process of RankBoost training. Step 2 is used to normalize the feature value into scaling of (0, 1) at query level. The docu-ments associated with the same query may have the same value with respect to one feature, after Step 2 we can see that all of the values for that feature become zero. It will not be used for SVD, so we used Step 3 to filter the features like that. In the next part of our experiment we can use the U to choose the most appropriate new feature subset to learn the rank model, taking the result and time-cost into account. 3.2 RankBoost Algorithm RankBoost is a kind of pairwise approach which can reduce ranking to classification on document pairs with respect to the same query, no longer assume absolute rele-vance. Its primary task is to make document pairs based on relevance judgments with document, x 1 is relevance document. Meanwhile, RankBoost is one of the improved AdaBoost algorithms for ranking task. A high weight assigned to a pair of documents indicates a great importance that the weak learner orders that pair correctly. We also assign to the weak learner to show its performance of predicting the document rele-vance. Once iterative it generates a weak learning. Reserving weak learning of each iterative and accumulating them multiplied by their weights; we can obtain the final ranking model. RankBoost Algorithm [1] we use is as table 2 showing. 
Weak rankings have the form h t (x). We think of these as providing ranking infor-mation in the same manner as ranking features and the final ranking. The weak learn-ers we used in our experiments are based on the original given ranking features and new features extracted by SVD. We focus in this section and in our experiments on {0, 1}-valued weak rankings that use the ordering information provided by the rank-ing features, but ignore specific scoring information. In particular, we will use weak ranking derived from a ranking feature f i by comparing the score of f i on a given in-stance to a threshold  X  .  X  t is the weight of the weak learning which can be generated at each iteration and computed by RankBoost. It shows the importance of the corre-sponding weak learning, with respect to predicting the relevance of document. Z t is a normalization factor (chosen so that D t+1 will be a distribution).Let T equal to D  X  , and we can get the ranking model by introducing the new features extracted by SVD. 4.1 Data Description We used the Letor2.0 [9] data set released by Microsoft Research Asia. This data set contains three data sets: the OHSUMED data set, the TREC2003 data set (TD2003) and the TREC2004 data set (TD2004). The OHSUMED data set derived from medi-cine retrieval task, while TD2003 and TD2004 come from TREC task. 
Letor2.0 is based on many query-document features. For each query-document pair of OHSUMED, there is a 25-dimensional feature vector that contains the most fre-quently used features in information retrieval, for example tf-idf, BM25 [10] score etc., while that in TD2003 and TD2004 is represented by a 44-dimensional feature vector, the feature in the vector such as HITS[11]  X  Page Rank[12] and LMIR[13] etc . In this paper, we deal the matrix constructed by 44 features in TREC data set with SVD, to choose 10 new features based on its quantity of information with respect to the original matrix [14], and for the OHSUMED data set we choose 5 new features adding to training set. 4.2 Experimental Results In order to evaluate the performance of the proposed approach, we adopt MAP [15] as evaluation method. The average precision of a query is the average of the precision scores after each relevant document retrieved. Average precision (AP) takes the posi-tions of relevance documents on ranking list into account to give scores of the list with respect to one query. Finally, MAP is obtained by the mean of the average preci-sion over a set of queries. There are three data subsets in the Letor2.0, and 5 groups of training set and test set in each subset. Our Experiments give the result of SVD-RankBoost (using SVD features and original features) compared with a baseline released by letor2.0 (using original features only).There are tree tables: table 3, table 4 and table 5 used for showing the results of OHSUMED, TD2003 and TD2004. 
Table 3 shows the results of baseline and adding five SVD features with the top 5 largest eigenvalues, according to the ranking model used to OHSUMED. 
Table 4 shows the results of baseline and adding ten SVD features with the top 10 largest eigenvalues, according to the ranking model used to TD2003. 
Table 5 shows the results of baseline and adding ten SVD features with the top 10 largest eigenvalues, according to the ranking model used to TD2004. 
We compare the performance of other boosting method for ranking with our ap-proach using SVD. For comparisons of the different models, we report the perform-ance measured by both the MAP and NDCG at position 1, 5 and 10.We average these performance measures over 5 folds for each data set. The results are shown in Table 6. 4.3 Experimental Analysis The experiment results show that it is helpful to improve the documents ranking list according to the relevance to the query in most of test set. We use the both the SVD and original features for ranking model training and relevance predicting. These SVD features reduced the difference of the training set and test set, so they can improve the ranking model trained by training set and used to predict the relevance of the docu-some results of test sets that aren X  X  improved, it is may be caused by the features that aren X  X  chosen furthermore, and the experiment is based on the average the number of new features extracted by SVD. Especially for some test set the average number are number of new features added to training set. 
In order to get most appropriate features and the number of them, our work is as the information to each other, for the training set it is introduced the unlabeled infor-mation for RankBoost to training ranking model to improve its relevance prediction the new features of the training set and test on the same dimensional vector space, which make the ranking model more meaningful for using the new features. Sec-ondly, on the detail of experiment, we normalize every feature to (0, 1) scaling. After that we can see the values of a few features are equal to zero with respect to the same query. We think that these features are no use to training ranking model by Rank-Boost, so in the process of the SVD, we introduce the original features without them, and the table 4 show the differences of results between the features after normalizing and filtering and not ,that used for SVD. Finally, we choose 10 SVD features accord-ing to top 10 eigenvalues for TD2003 and TD2004, 5 features according to top 5 eigenvalues for OHSUMED, after taking the performance of final ranking model and the cost of training time into account. 
For RankBoost, there are two factors that sh ould be considered to implement algo-rithm: 1) the number of iterations; 2) the initial weight of document pairs. We use cross validation to select the number of iterations. In this paper, the initial weight distribution of document pairs is set to uniform distribution as D 1 (x 0 , x 1 )=1/|D|. Table 7 shows the results of all the methods we used to rank. The approach SVD-1 denotes we only use the original features to SVD process compared with SVD-2 that the fea-tures are processed with normalizing, while we use SVD-3 to show the results after normalizing and filtering. Finally we list the result of RankSVM and the result of the PCA approach [7] as compare. We can see that SVD-1, SVD-2, SVD-3 are all helpful to improve the performance of ranking, and for the different test set, they have differ-ent effects. However, in our work SVD-3 get the best result. It is our further work to study how to merge the different ranking model to improve the result of ranking. In this paper, we propose an approach whose idea is derived from transfer learning to use the unlabeled data for ranking. We apply SVD to extract new features from training set and test set, and add these features to each other. The difference between training set and test set can be decreased, so as to improve the ranking model. The experiment shows that it is meaningful for the learning to rank to introduce the SVD features and unlabeled data information to training sets, but it can X  X  improve the re-sults of all the test set. It may be caused by the detail of the experiments. For all that, this method improves the ranking performance in most of test sets. We will continue proposed in this paper. In addition, the other methods will be also our research con-tents. Possible extensions and future work include: using ties [16] (the documents pairs with the same relevance judgment) to learning to rank, which can expand the sample space, whose key is how to combine this ranking model with the model trained by document pairs with the different relevance judgment. Furthermore, it is an important research direction to apply the ot her model and algorithm used in the field of information retrieval and machine learning. This work is supported by grant from the Natural Science Foundation of China (No.60373095 and 60673039) and the National High Tech Research and Develop-ment Plan of China (2006AA01Z151).
