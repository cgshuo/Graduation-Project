 components analysis, linear discriminant analysis, etc X  X odel each input object as a single sample drawn from a multivariate Gaussian. For example, the k -means algorithm assumes that each input is a single sample drawn from one of k (unknown) isotropic Gaussians. The goal of k -means can be viewed as the discovery of the mean of each Gaussian and recovery of the generating distribution of each input object.
 However, in many real-life settings, each input object is naturally represented by multiple samples users may rate the same movie. Multiple samples are also ubiquitous in time-series data such as sensor networks, where each sensor device continually monitors its environmental conditions (e.g. applications. For example, clustering sensor network devices has been used for optimizing routing of the network and also for discovering trends between sensor nodes. If the k -means algorithm is employed, then only the means of the distributions will be clustered, ignoring all second order covariance information. Clearly, a better solution is needed.
 by a multivariate Gaussian distribution. The  X  X istance X  between two Gaussians can be quantified the differential relative entropy between two multivariate Gaussians can be expressed as the con-vex combination of two Bregman divergences X  X  Mahalanobis distance between mean vectors and algorithm and show that the optimal cluster parameters can be cheaply determined via a simple, closed-form solution. Our algorithm is a Bregman-like clustering method that clusters both means and covariances of the distributions in a unified framework.
 We evaluate our method across several domains. First, we present results from synthetic data exper-iments, and show that incorporating second order information can dramatically increase clustering accuracy. Next, we apply our algorithm to a real-world sensor network dataset comprised of 52 known software bugs. We first present some essential background material. The multivariate Gaussian distribution is the of a d -dimensional multivariate Gaussian is parameterized by mean vector  X  and positive definite covariance matrix  X  : where |  X  | is the determinant of  X  .
 The Bregman divergence [2] with respect to  X  is defined as: Bregman divergence is the standard squared Euclidean distance. Similarly, if  X  ( x )= x T A T Ax , M entropy. Bregman divergences generalize many properties of squared loss and relative entropy. Bregman divergences can be naturally extended to matrices, as follows: where X and Y are matrices,  X  is a real-valued, strictly convex function defined over matrices, and tr ( A ) denotes the trace of A . Consider the function  X  ( X )= X 2 F . Then the corresponding Bregman matrix divergence is the squared Frobenius norm, X  X  Y 2 F . The Burg matrix diver- X  (
X )=  X  divergence is: well as of the eigenvectors of X and Y .
 The differential entropy of a continuous random variable x with probability density function f is defined as Shannon entropy approximately equal to h ( f )+ n . The continuous analog of the discrete relative differential relative entropy is defined as S  X  , ...,  X  k . Each cluster j can be represented by a multivariate Gaussian parameterized by mean  X  j and covariance  X  j . Using differential relative entropy as the distance measure between Gaussians, the problem of clustering may be posed as the minimization (over all clusterings) of 3.1 Differential Relative Entropy and Multivariate Gaussians We first show that the differential entropy between two multivariate Gaussians can be expressed as a convex combination of a Mahalanobis distance between means and the Burg matrix divergence between covariance matrices.
 Consider two multivariate Gaussians, parameterized by mean vectors m and  X  , and covariances S p ( x | m , S ) , which can be shown [3] to be: We now consider the second term: p ( x | m , S ) log p ( x |  X  ,  X  )= p ( x | m , S )  X  follows from the definition of  X  = E [( x  X  m )( x  X  m ) T ] and also from the fact that E [( x  X  m )( m  X   X  ) T ]= E [ x  X  m ]( m  X   X  ) T = 0 . Thus, we have
D ( p ( x | m , S ) || p ( x |  X  ,  X  )) =  X  rameterized by the covariance matrix  X  .
 We now consider the problem of finding the optimal representative Gaussian for a set of c Gaussians i  X  i =1 , the optimal representative minimizes the cumulative differential relative entropy: The second term can be viewed as minimizing the Bregman information with respect to some fixed (albeit unknown) Bregman divergence (i.e. the Mahalanobis distance parameterized by some co-variance matrix  X  ). Consequently, it has a unique minimizer [1] of the form setting the gradient of (7) with respect to  X   X  1 to 0: Setting this to zero yields Figure 1 illustrates optimal representatives of two 2-dimensional Gaussians with means marked by points A and B, and covariances outlined with solid lines. The optimal Gaussian representatives are (8), the optimal representative mean is the weighted average of the means of the constituent Gaus-plus rank one updates. These rank-one changes account for the deviations from the individual means to the representative mean. 3.2 Algorithm Algorithm 1 presents our clustering algorithm for the case where each Gaussian has equal weight  X  the mean and covariance parameters for the cluster representative distributions are optimally com-puted given the cluster assignments. These parameters are updated as shown in (8) and (9). Next, the cluster assignments  X  are updated for each input Gaussian. This is done by assigning the i th shown. Note that the problem is NP -hard, so convergence to a global optima cannot be guaranteed. We next consider the running time of Algorithm 1 when the input Gaussians are d -dimensional. Lines 6 and 9 compute the optimal means and covariances for each cluster, which requires O ( nd ) be computed in O ( d 2 ) time. The second term can similarly be computed once for each cluster for O (  X kd 2 ( n + d )) .
 Algorithm 1 Differential Entropic Clustering of Multivariate Gaussians 1: { m 1 , ..., m n } X  means of input Gaussians 2: { S 1 , ..., S n } X  covariance matrices of input Gaussians 3:  X   X  initial cluster assignments 4: while not converged do 5: for j =1 to k do { update cluster means } 7: end for 8: for j =1 to k do { update cluster covariances } 10: end for 11: for i =1 to n do { assign each Gaussian to the closest cluster representative Gaussian } 13: end for 14: end while We now present experimental results for our algorithm across three different domains: a synthetic dataset, sensor network data, and a statistical debugging application. 4.1 Synthetic Data Our synthetic datasets consist of a set of 200 objects, each of which consists of 30 samples drawn from one of k randomly generated d -dimensional multivariate Gaussians. The k Gaussians are generated by choosing a mean vector uniformly at random from the unit simplex and randomly selecting a covariance matrix from the set of matrices with eigenvalues 1 , 2 , ..., d . In Figure 2, we compare our algorithm to the k -means algorithm, which clusters each object solely on the mean of the samples. Accuracy is quantified in terms of normalized mutual information (NMI) between discovered clusters and the true clusters, a standard technique for determining the quality for five clusters across a varying number of dimensions. All results represent averaged NMI values across 50 experiments. As can be seen in Figure 2, our multivariate Gaussian clustering algorithm yields significantly higher NMI values than k -means for all experiments. 4.2 Sensor Networks Sensor networks are wireless networks composed of small, low-cost sensors that monitor their sur-rounding environment. An open question in sensor networks research is how to minimize communi-cation costs between the sensors and the base station: wireless communication requires a relatively large amount of power, a limited resource on current sensor devices (which are usually battery pow-ered).
 A recently proposed sensor network system, BBQ [4], reduces communication costs by modelling sensor network data at each sensor device using a time-varying multivariate Gaussian and trans-mitting only model parameters to the base station. We apply our multivariate Gaussian clustering between groups of sensor devices. The Intel sensor network consists of 52 working sensors, each of which monitors ambient temperature, humidity, light levels, and voltage every thirty seconds. Conditioned on time, the sensor readings can be fit quite well by a multivariate Gaussian. Figure 3 shows the results of our multivariate Gaussian clustering algorithm applied to this sensor network data. For each device, we compute the sample mean and covariance from sensor readings between noon and 2pm each day, for 36 total days. To account for varying scales of measurement, of the lab. Since the measurements were taken during lunchtime, we expect higher traffic in these one is characterized by high temperatures, which is not surprising, as there are several windows on primarily located in the center and the right of lab, away from outside windows.
 4.3 Statistical Debugging Leveraging program runtime statistics for the purpose of software debugging has received recent research attention [12]. Here we apply our algorithm to cluster functional behavior patterns over an incorrect number of columns in an array environment) with ambiguous or unclear error messages ample, the message  X  X aTeX Error: There X  X  no line here to end X  can be caused by numerous problems in the source document.
 Each function in the program X  X  source is measured by the frequency with which it is called across one dimension for each bug. The distributions are estimated from a set of samples; each sample execution, function counts are measured and recorded. More details can be found in [7]. in understanding error dependent program behavior. Figure 4 shows three covariance matrices from a sample clustering of eight clusters. To capture the dependencies between bugs, we normalize each Conversely, clusters (b) and (c) show that some functions are highly error dependent. Cluster (b) shows a high dependency between bugs 1 and 4, while cluster (c) exhibits high covariation between bugs 1 and 3, and between bugs 2 and 4.  X   X   X  In this work, we showed that the differential relative entropy between two multivariate Gaussian distributions can be expressed as a convex combination of the Mahalanobis distance between their some finite set. In [5], no parametric form is assumed, and the Kullback-Liebler divergence (i.e. connection to the Burg matrix divergence was made there.
 Although the closed-form updates used by our algorithm are similar to those employed by a Bregman clustering algorithm [1], we note that the computation of the optimal covariance matrix (equation (9)) involves the optimal mean vector.
 The resulting algorithm, however, is much more computationally expensive than ours; whereas in our method, the optimal means and covariance parameters can be computed via a simple closed form problem of finding the optimal Gaussian with respect to the first argument (note that equation (6) is minimized with respect to the second argument) is considered in [11] for the problem of speaker interpolation. Here, only one source is assumed, and thus clustering is not needed. existing clustering algorithms, our algorithm optimizes both first and second order information in the data. We have demonstrated the use of our method on sensor network data and a statistical debugging application.

