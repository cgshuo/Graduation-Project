 Term weighting is a fundamental problem in IR research and numerous weighting models have been proposed. Proper term weighting can greatly improve retrieval accuracies, which essentially involves two types of query understand-ing: interpreting the query and judging the relative contri-bution of the terms to the query. These two steps are often dealt with separately, and complicated yet not so effective weighting strategies are proposed. In this paper, we pro-pose to address query interpretation and term weighting in a unified framework built upon distributed representations of words from recent advances in neural network language modeling. Specifically, we represent term and query as vec-tors in the same latent space, construct features for terms using their word vectors and learn a model to map the fea-tures onto the defined target term weights. The proposed method is simple yet effective. Experiments using four col-lections and two retrieval models demonstrates significantly higher retrieval accuracies than baseline models. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation Query term weighting, distributed representations, word vectors
Performance of text search engines relies heavily on query understanding, of which one important problem is how to c weight the contribution of each individual term to the re-term recall weights [22]) are used, they can boost retrieval accuracies by up to 30% given relevance judgments. Prop-erly setting the query term weights requires accurately in-terpreting and properly representing the query first. This is no easy task as query intent understanding itself is a diffcult problem in IR research [7].

In this paper, we attempt to address query interpretation and term weighing from a different angle, with a unified framework built upon recent advances in neural network language modeling [13, 3]. Recent research in the appli-cation of neural network to text problems exploits the co-occurrence of words to represent words by multidimensional vectors. Distributed representations learned from neural network based models [13, 3] are designed and shown to be effective for measuring semantic similarity among words and identifying similar neighbors for a given word. The mapping from words to vectors gives the ability to not only measure word-word similarity but also ways to represent the query in the same vector space from word vectors from its terms (such as taking the average of the word vectors of all terms as the vector representation for the query).

As proper query term weights reflects the relative impor-tance of the term with respect to the query, specifically we propose to construct features from word vectors representa-tions of terms and the query and to learn the relationship between the feature vectors and target term weight (such as the term recall weight [22] estimated from relevance judg-ments). A regularized linear regression problem from the feature vectors onto term weights is formulated and the pre-dicted term weights are used for both bag-of-words queries and term dependency queries. We demonstrate the effec-tiveness of our method using two popular retrieval models, four standard test collections, word vectors developed from a variety of sources, and three baseline methods.
The contributions of our work are three fold. First, we join the work of distributed word vectors to the prediction of query term weights in IR, and propose a simple yet ef-fective framework to predict effective term weights. Second, we observe significant improvement over the baseline mod-one is the matching score a retrieval function assigns to a term and a document pair; the other is the relative impor-tance of the term to the query. The first setting is handled by the retrieval model while the query model deals with the second one; in this paper, we refer  X  X erm weight X  or  X  X erm reweighting X  to that in the second setting. els with two retrieval models over four standard collections whe n using predicted term recall as term weights. Third, the proposed method is much more efficient than previous work on query term weight prediction, i.e., term recall weight, which requires an initial retrieval and a local SVD for every new incoming query to obtain features for predicting term recall [22]. The proposed framework derives feature vectors directly from precomputed distributed word vectors; simple computations are sufficient for predicting term weights for new queries.

The remainder of this paper is organized as follows: Sec-tion 2 introduces prior research related to query term weight-ing and term recall weight. Section 3 discusses the pre-liminaries for term recall prediction and distributed word vectors. Section 4 formally presents our approach in term weight modeling and estimation. Section 5 describes the data sets and the experimental settings. Experimental re-sults as well as data analysis are presented in Section 6. At last, we conclude the paper and discuss some future work in Section 7.
Query term weighting has been extensively studied in IR literature and a retrieval model reflects its choice of query term weights used. Conceptually, any retrieval model can be abstracted as the following scoring function: where f ( t, D ) is the matching score of term t and document D (for example, term frequency), and w ( t ), the query term weight, which doesn X  X  specifically depend on D (for example, inverse term frequency), is the quantity we are interested in this paper. By formulating this way, existing retrieval models make different choices about term weight. The most commonly used query term weights in the literature is idf, for example, the vector space model, language model [21], BM25, etc.

Another well known term weight, the term recall weight, is closely related to idf and also attracts broad atten-tion. It is originally captured by the Binary Independence Model (BIM)[16] to emphasize the importance of query term weights, as shown below: wher e R q is the set of relevant documents,  X  R q is the set of non-relevant documents, d is a document to be ranked, q is the query, and t i is a query term. The probability P ( t | R provides one way to weight query terms, known as the RSJ term weight, to improve retrieval performance. However, due to involvement of the relevant set of documents of query q , it is hard to give a reliable estimation of P ( t | R evance information is unavailable. Indeed, researchers rec-ognized that the use of term recall weight could lead to huge retrieval gain as P ( t | R q ) is actually the only term about rel-evance in the ranking function [22], and proposed several ways to predict it.

Croft and Harper [5] modeled the query term weight as a tuned constant (the Croft/Harper Combination Match model). Greiff [6] tried to predict term weight and P ( t | with a linear function of idf. His experiments showed some improvement over the BIM model. More recently, Metzler [12] modeled term weight as a linear function of document frequency. The above modeling of term weight only used df or idf features. The predictions were inadequate as they did not reflect the insight that P ( t | R q ) is a query dependent quantity and that query dependent features are needed to estimate term weight. Recently, Zhao et al. [22] proposed a framework to construct features for query terms from pseudo relevance feedback [20] and use them to predict term weight.
Retrieval models that capture term dependencies have attracted research attention and also demonstrated their retrieval effectiveness compared to unigram query models. One widely used model is the sequential dependency model (SD) [10], which features three types of query concepts : terms, bigrams and proximity expression. An example of a sequential dependency query in the Indri query language for the bag-of-words query apple pie recipe is: #weight( 0.8 #combine( apple pie recipe ) ) #1(apple pie) matches when apple and pie form a bigram. #uw8(apple pie) matches when apple and pie occur in any order within a window of 8 words. And #combine is a prob-abilistic AND operator.

The sequential dependence model provides basic weight-ing for different types of query concepts. Broad empirical results have validated the effectiveness of SD over the un-weighted bag-of-words query model [1, 2, 10]. However, con-cepts of the same type share the same weight, which is not optimal. Recent research proposed to predict term weights for each concept using features computed from collection statistics, an adaptive model, and an optimization goal of maximizing an evaluation metric such as Mean Average Pre-cision (MAP) [1, 2]. Term weighting strategy emphasizing query aspects is also proposed [23].

Our approach differs from the above methods in that we represent terms and queries using distributed word vectors in a semantic vector space learnt from a global corpus and we construct novel feature vectors from the distributed rep-resentations to automatically learn term weights for efficient retrieval.
In this section, we briefly introduce recent work on dis-tributed representations learning from neural network lan-guage models and also defines the target term weights we use in our framework.
Neural network based language models aim to learn the word vector representations and a statistical language model for the underlying text. These models can be mainly at-tributed to two categories. Models from the first category try to learn the word vector representations and the lan-guage model jointly. One example is the neural network language model (NNLM) [3], where a linear projection layer and a non-linear hidden layer are adopted to form a feed-forward neural network. Models in the second category learn the word vector representations first and then train the lan-guage model with the word vectors. For example, Mikolov, et al. [14] used one neural network with a single hidden layer to learn word vector representations and then trained an N-gram language model on the word vectors. Typically the simple structure of the neural networks used by the second approach makes it less computationally expensive, thus we confine the discussion to this type of system.

Recently, Mikolov et al. [13] proposed two new models, the Continuous Bag-of-Words model (CBOW) and the Con-tinuous Skip-gram model, that have greater training effi-ciency. CBOW tries to maximize classification of a word by building a log-linear classifier with word vectors of sev-eral history words and future words around that location as input, while the Continuous Skip-gram model tries to pre-dict words within a range before and after the current word given the word vector of the current word. Both CBOW and the Continuous Skip-gram model have only one projec-tion layer between the input and output layer without any hidden layer, which significantly reduces the computational cost caused by the non-linear hidden layers in prior neural network based language models. Negative sampling is used to learn the CBOW and the Continuous Skip-gram models [15]. Word vector representations on a Google News corpus with 100 billion words for a vocabulary of 3 million words can be learned in less than one day using modest hardware.
Word vectors learned by both models have performed well in several semantic related task evaluations [13]. Mikolov et al. released the software for training CBOW and Continuous Skip-gram models and a set of pre-trained 300-dimensional word vector representations on the above mentioned Google words to  X  Chinese river  X  in terms of cosine similarity in the word vector representation space.
 Table 1: Example of closest n-gram terms to the phrase  X  Chinese river  X .

It can be seen that the neighbors given by word vectors are indeed semantically related to the input. Also, in this example, unlike Yangtze_River , the phrase Chinese_river does not belong to the vocabulary of the model; there is no word vector representation for Chinese_river . Instead, word vectors for both Chinese and river are fetched and averaged to represent Chinese_river in the search for the closest neighbors, which yields meaningful results. A recent analysis [8] justifies the above results by showing that the learning of distributed representations is essentially factoriz-ing a word-context matrix which ensures that words sharing similar context (thus similar meaning) will have similar vec-tor representations.

This word vector addition property is the key moti-vation to represent query and terms as word vectors and thus to derive term features from them to predict target term ht tps://code.google.com/p/word2vec/ weights . See [15] for more examples of word vector addition property. In this paper, we adopt the CBOW framework to learn word vectors and use them to build a term weight prediction framework. The proposed framework can also be applied to word vectors learned by the Continuous Skip-gram model, which we leave as future work.
As discussed in Section 1, proper query term weights re-flects the relative importance of a term to a query and should also help improve retrieval performance. We choose term re-call weight as our target term weight to predict in our frame-work for its simplicity to compute and also great potential to improve retrieval performance [22]. Given relevance judg-ments, it can be estimated as P ( t | R q ) = | R q,t | | R is the set of relevant documents to q and R q,t  X  R q is the subset of relevant documents that contain term t .
In this section, we present how true or estimated target term weights can be integrated into different retrieval mod-els.
One commonly used retrieval model in IR is the language model, often together with Dirichlet smoothing as shown in Eq. (3), where tf t is the term frequency of term t in document D , cf t is the collection frequency of t , | D | is the length of doc-ument D , | C | is the total length of the collection and  X  is the smoothing parameter.

True term recall weight or term recall weight estimates can used by a language model in similar fashion as [9], as shwon in Eq. (4).
 The term recall weighted language model shown above is equivalent to the relevance model under the assumption of binary term occurrences and uniform document length [22].
Another widely adopted retrieval model is BM25, as shown in Eq. (5) where df t is the document frequency of term t , avgdl is the average document length over the collection and k 1 and b are free parameters. BM25 is actually more directly connected to term recall weight, as the above original BM25 model is an extension of the BIM [16]. By inserting the recall weight estimates to BM25, we get the recall-enhanced BM25 as shown in Eq. (6).

In this section, we present our model for estimating term weights with distributed word vectors.
 Suppose we have a set of M que ries Q = { q 1 , q 2 , ..., q and each query q i has n i terms for i = 1 , 2 , ..., M . Let t ij represent the j th term of query q i for j = 1 , 2 , ..., n r ij denote the true term weight estimated from relevance judgments for t ij , hence r ij  X  [0 , 1] and N = the total number of query terms. (Refer to Table 2 for a summary of notations.)
Let w ij  X  R p denote the continuous distributed word vec-tor representation for term t ij , where p is the dimension of the word vector. In this paper, we propose to directly con-struct the feature vector x ij for term t ij from the distributed word vector representations of term t ij and other terms in the same query q i as wher e i s the mean of all word vectors of terms in q i . Hence, the feature vectors have the same dimension as the word vectors. An naive example of feature vector construction with p = 2 is illustrated in Figure 1.

Intuitively, proper term weights measures the relative im-portance of a term w.r.t. the whole query (hence w.r.t. the other terms in the same query). In other words, a term with a higher term weight means that it X  X  more important for the term to represent the meaning of the query. Meanwhile, the feature vector of a term defined above is the difference of the term to the center of distributed representations for all terms in the query, which also serves as the vector repre-sentation for the entire query (by applying the word vector addition properties). Hence, the feature vector measures the semantic difference of a term to the whole query. Believing that the above two measures are somewhat related, we pro-pose to construct features above and learn a model to map from the feature vectors to term weight labels.

As a side note, we can see that the above constructed feature vectors are hence query dependent, which is also necessary in that term weight, from its definition, is also query dependent. As we will see in the experiments, the above construction is simple and yet effective.

Having defined the features, we now turn to formulate the model to map from feature to term weight labels. As the Figure 1: Demonstration of transforming global 2-dimen sional word vectors into feature vectors local to the query. In the graph, blue solid circles rep-resent terms in one query and green diamonds rep-resent terms in another query. Note that the two queries share one common term located at (3 , 4) . The red solid circle represents the center for the cir-cles, and the red diamond represents the center for the diamonds. The dashed and solid arrows are then the transformed feature vectors for all the terms. adopted target term weight is a probability, we first map it from (0 , 1) onto the real line R with a logit function to avoid numerical issues, as shown below. W e then formulate the  X  X ransformed X  term weight y as a linear function of the term feature vectors defined above, that is y =  X   X  x , and employ  X  1 -norm regularization to learn the feature weights  X  (We also tried  X  2 -norm regularization and  X  1 -norm regularization works a little better). Formally, the optimization problem is where  X   X  0 is the regularization parameter controlling the balance between prediction error on training data and model complexity, and needs to be tuned by cross validation. In matrix form, the above optimization problem can be ex-pressed as whe re y  X  R N is is target term weight vector with term weights of all terms in the training queries stacked, and X  X  R N  X  p is It is easy to verify that optimization problem (11) is equiv-alent to (10) and is of standard form of LASSO regression [19].
Specifically, the subgradient of the objective function w.r.t  X  in problem (11) is where
Efficient gradient based methods can be applied to solve the above problem and after we get the optimum  X   X  , when a new query term t  X  with feature vector x  X  arrives, we predict its term weight as
W e refer to the above proposed method as DeepTR for it using distributed representations of words to model term weights.

DeepTR is a very efficient method of predicting term weights, which enables it to be used in online services where latency (the time required to respond to a query) must be kept low. The word vectors and the regression model are trained offline. Predicting P ( t ij | R ) for a new query involves loading the word vectors for the query terms, transforming them into feature vectors (averaging and subtraction), an in-ner product with the learned feature weights, and a sigmoid function. This is far more efficient than some prior methods that were effective but computationally complex (e.g., [22]).
After we gain estimates of the term weights, we construct the following query model variations in Indri query language (with the above apple pie recipe keyword query as an ex-ample): Note that in DeepTR-SD , we mainly focus on reweighting the unigrams, as unigram reweighting plays a much more significant role than bigrams and proximity expressions in improving retrieval accuracies, which is also confirmed by previous studies [22, 1, 2]. The proposed framework can be directly extended to bigrams, proximity expressions and other query concepts that can be represented by a standard inverted list containing positions, however the training data are more sparse for more complex concepts, thus we leave that for future study.

The two query model variations proposed above can be used with language modeling and BM25 retrieval models; in the next section, we present experiments with both types of retrieval model.
This section describes how we evaluated our work exper-imentally. We conduct experiments on 4 TREC test collec-tions consisting of one Robust track dataset and three Web Track datasets. The document collections differ in size, from a small and traditional TREC Robust Track collection to large Web Track collections of web documents. The dataset sizes and the queries used with each dataset are shown in Table 3. The first three datasets are standard datasets used without change. The ClueWeb09B dataset is the standard ClueWeb09 Category B dataset after spam documents are the filtering threshold was set to 70%.
 Table 3: Collections and topics. Spam documents were removed from the ClueWeb09 Category B dataset (Waterloo Fusion spam scores, 70% thresh-old).
 Collection # Docs # Words TREC Topics ROBUST04 528K 253M 351-450, 601-700 WT10g 1,692K 1,076M 451-550 GOV2 25,205K 24,007M 701-850 ClueWeb09B 29,038K 23,890M 1-200 We use the Indri search engine 4 to index the corpus. The Krovetz stemmer was used on queries and documents. A standard list of English stop words is maintained for query processing. Additionally, for thee web track datasets, anchor texts from in-links is treated as part of the document and hence is indexed. We use descriptions of the TREC topics as queries stopped by a list of standard stop words and several stop phrases such as  X  X ind information X .

We use DeepTR-BOW to denote the re-weighted keyword queries and DeepTR-SD to denote the re-weighted sequential dependency queries. All of the above query models can be expressed in the Indri query language. When constructing the sequential dependency model queries, we use weights 0.8, 0.1 and 0.1 for terms, bigrams and unordered windows, respectively as they have shown to be effective in prior re-search and practical applications [10, 11].

For retrieval, we use a language model with Dirichlet smoothing [21] and BM25 to test both types of weighted queries. Although sequential dependency model queries are not typically used with the BM25 retrieval model, they are not incompatible with BM25. It is straightforward to cal-culate the tf and df statistics that BM25 needs in order to calculate scores for bigrams ( #1(apple pie) ) and proximity expressions ( #uw8(apple pie) ) [2]. We show the results for these query concepts with BM25 to demonstrate the effec-tiveness of our method. A minor change is made to Indri to allow for the insertion of term weight estimates to the BM25 retrieval model, as shown in Eqn. (6). Retrieval parameters are all set to Indri default values, which is  X  = 2500 for language model and k 1 = 1 . 2 , b = 0 . 75 for BM25. ht tp://plg.uwaterloo.ca/~gvcormac/clueweb09spam http://lemurproject.org/indri/
For DeepTR , we use several sets of distributed word vec-tor representations. The first one is the pre-trained 300-dimensional vectors released by Google, which is trained on a set of word vectors was trained on a subset of the ClueWeb09 scribed above; the remaining documents were processed by Mikolov X  X  Continuous Bag-of-Words Model software [13, 15] was used to learn the word vectors with 100, 300, 500 di-mensions, repectively. We also trained word vectors on the ROBUST04 and WT10g collections with 300 dimensions to enable comparison of corpus-specific word vectors trained from small datasets with corpus-independent word vectors trained on larger web datasets.

We train DeepTR on the set of queries with 5-fold cross validation and report the averaged performance. That is, we divide the queries to 5 folds and on each fold, we train the model using three of them, validates the performance on one fold of them as development set to pick the regularization report the performance on the rest fold as testing data. After the model is trained on each fold, term weights for queries in the testing fold are predicted and retrieval performances for the reweighted queries are reported.

We use the trec_eval 8 tool provided by TREC to as-sess the retrieval results. We focus mainly on the Mean Average Precision (MAP) and Precision at 10 (P@10) met-rics. For web collections such as GOV2 and ClueWeb09B, where graded relavence judgments are available, we also re-port Normalized Discounted Cumulative Gain (NDCG) at 20 and Expected Reciprocal Rank (ERR)[4] at 20 values using the gdeval.pl 9 tool provided by TREC. Statistical significance of model differences in terms of retrieval perfor-mance is judged by a two-sided paired randomization test with  X  &lt; 0 . 05 rather than the Wilcoxon signed rank test that was used in prior research [1], which is prone to type I errors and considered not reliable for ad hoc IR [17, 18].
We compare the retrieval performance of DeepTR to three baseline query models. The first is the unstructured bag-of-words query model ( BOW ). The second is the original sequen-tial dependency model ( SD ) [10]. The third is the weighted sequential dependence model ( WSD ) [1], which is the state of the art model for query reweighting. For WSD , we im-plemented the model with coordinate ascent [11] to directly optimize MAP on the training queries for WSD . However, we got slightly worse results than reported in [1, 2], possibly due to that we didn X  X  have the MSN query log feature used in the original paper. As we use an overlapping set of collections and set of queries, and our BOW and SD baseline results are very close to the ones reported in the WSD paper, we decide to show and compare with the performance numbers of WSD reported in the original paper [1, 2]. We also implemented the method proposed by Zhao et al. [22] to predict term re-call, but the experimental results on our datasets were worse than our unweighted bag-of-words query baselines, thus we omit the comparison from this paper. ht tps://code.google.com/p/word2vec/ http://lemurproject.org/clueweb09/ https://code.google.com/p/boilerpipe/ http://trec.nist.gov/trec_eval/trec_eval_latest. tar.gz http://trec.nist.gov/data/web/10/gdeval.pl
This section presents the results of experiments that com-pare the DeepTR method of setting term weights in two re-trieval models (language models, BM25) to three baseline methods (unweighted queries, sequential dependency mod-els, weighted sequential dependency models); the effects of word vectors of varying dimensionality; and the effects of word vectors trained from different types of data.
The first experiment compared query term weights pro-vided by DeepTR in the language model retrieval model to three baseline methods (unweighted queries, sequential dependency models, weighted sequential dependency mod-els). Experimental results are shown in Table 4 for both DeepTR-BOW and DeepTR-SD . In this experiment, we fix the dimension of word vectors to be 300 and compare the per-formances of the proposed models with different sources of word vectors.

DeepTR-BOW term weights perform better than the un-weighted bag-of-words query model ( BOW ) over all collec-tions, significantly outperforming BOW on ROBUST04 and GOV2 in terms of MAP and on ROBUST 04 in terms of P@10. DeepTR-BOW even achieves higher MAP than the sequential dependency model ( SD ) on WT10g, how-ever the results are not statistically significant. DeepTR-BOW gains comparable MAPs to WSD in ROBUST04, WT10g and ClueWeb09B. This suggests that DeepTR-BOW which mod-els no bigrams and proximity expressions is able to achieve comparable retrieval performances with the sequential de-pendency models (both unweighted and weighted).

The DeepTR-SD query model performs better than the standard sequential dependency model over all collections with all sources of word vectors. Particularly, significant differences are observed on ROBUST04, WT10g and GOV2 with all sets of word vectors; DeepTR-SD with word vectors trained on the Google News corpus achieves significantly higher MAPs over BOW and SD on all four collections. The significant gains of DeepTR-SD with respect to SD range from 4.2% to 12.1% in MAP. Similar significant improvements for P@10 are also observed. While we cannot draw statistical significance conclusions as we have no performance on indi-vidual query of WSD , on ROBUST04 and WT10g, DeepTR-SD attains better MAPs than WSD . This validates the effective-ness of using distributed representations of words as feature sources to predict term weights in improving both overall retrieval performance and top-rank results over the baseline models.
We also show the retrieval results with BM25 in Table 5 for both DeepTR-BOW and DeepTR-SD to demonstrate the boost in retrieval performance with different retrieval models. Prior research on weighted sequential dependency models [1] did not publish results for BM25, so we are unable to compare against that baseline in this experiment.

DeepTR-BOW term weights perform better than the un-weighted bag-of-words query model ( BOW ) over all collec-tions, significantly outperforming BOW for MAP over all data sets and all word vector variations except for ClueWeb09B with the Google word vectors; although not statistically sig-nificant, DeepTR-BOW also performs better than SD with all word vectors settings on ROBUST04, WT10g and GOV2.
The DeepTR-SD query model performs better than un-weighted queries and the standard sequential dependency model, delivering significantly higher MAPs over all collec-tions and all sources of word vectors except for ClueWeb09B with the Google word vectors. The significant gains of DeepTR-SD over SD range from 1.9% to 7.3%. Similar trends over P@10 are also observed. DeepTR-SD with word vectors trained on ClueWeb09B achieves significantly higher P@10 than SD on ROBUST04, WT10g and GOV2.
Word vectors of different dimensionality provide different levels of granularity that may or may not be useful for set-ting term weights; they may also require different amounts of training data. Our third experiment investigates the ef-fects of word vectors containing 100, 300, and 500 dimen-sions on term weights produced for language models. The ClueWeb09B corpus is used for these experiments due to its size, availability, and strong performance in the experiments above. Experimental results for DeepTR-BOW and DeepTR-SD are shown d together in Table 6.

Word vectors of 100 dimensions work best for un-structured BOW queries on ROBUST04, WT10g and ClueWeb09B, while the best MAP for GOV2 is achieved with word vectors of 300 dimensions. Similar trends are ob-served for the DeepTR-SD query model. Notably, DeepTR-SD with word vectors of 100 dimensions attains a significant 7.9% MAP improvement over SD on ROBUST04, and a sig-nificant 16.8% MAP improvement over SD on WT10g. On GOV2, word vectors of 300 dimensions help DeepTR-SD sig-nificantly improve over SD by 7.5%; last on ClueWeb09B, word vectors of 500 dimensions achieve the best result over all three dimensions, by a 4.9% over SD . We also observe similar results on P@10 when varying the dimensionality of word vectors. We also evaluated the effects of word vec-tor dimensionality on term weights for BM25 retrieval and trends are similar to those for language model retrieval; the results are omitted due to space constraints.

Our results suggest that 100 dimensions is sufficient for estimating very effective term weights.
We turn back to Tables 4 and 5 to compare retrieval re-sults obtained with corpus-specific word vectors and word vectors trained from larger, more general, and external cor-pora (GOV2, ClueWeb09B, and Google News).

DeepTR-BOW performed about equally well with all three external corpora; the differences among them were too small and inconsistent to support any conclusion about which is best. However, although no external corpus was best for all datasets, the language model and BM25 retrieval mod-els agreed on which source of word vectors was best for a particular corpus. This agreement suggests that the learned term weights are independent of a particular retrieval model, which was the purpose of training with term weight values.
The corpus-specific word vectors were never best in these experiments, even for GOV2 and ClueWeb09, which are large and provided  X  X est X  performance for other datasets. However, given the wide range of training data sizes  X  vary-ing from 250 million words to 100 billion words  X  it is striking how little correlation there is between search accuracy and the amount of training data.

Similar trends are observed for the DeepTR-SD query mod-els.
Figure 2 presents the detailed number of queries improved or hurt by proposed method using word vectors trained from various sources compared to the LM baseline. It X  X  clear that for all methods using the predicted term weights, the num-bers of improved queries are significantly larger than that of hurt queries. Moreover, when compared to sequential de-pendency model ( SD ), all the methods using the estimated weights help more and hurt fewer queries than sequential dependency model ( SD ), especially in the [-20%, 0) range as shown in the figure. conducted in [2]) no results on ClueWeb09B were reported in [2])
We investigate how DeepTR-SD performs on queries with different lengths. We divide the queries into three groups: with no more than 3 terms ( Len -3 ), 4 to 5 terms ( Len 4-5 ) and 6 terms or more ( Len 6+ ). Figure 3 shows the relative gains of SD and DeepTR-SD (using vectors pre-trained from Google News) over BOW . It X  X  clear that DeepTR-SD works sig-nificantly better than SD on mid-range and long queries ( Len 4-5 and Len 6+ ) on all collections. It X  X  worth noting that SD performs worse than BOW for long queries on WT10g, while in contrast DeepTR-SD improves BOW by 14%. For short queries ( Len -3 ), DeepTR-SD outperforms SD over 3 out of 4 collec-tions, which also renders DeepTR-SD a better alternative for SD .
Our final experiment investigates retrieval quality using the graded relevance judgments that are available for web collections such as GOV2 and ClueWeb09B. Table 7 presents the experimental results using the NDCG@20 and ERR@20 metrics that have been widely used in web search scenar-ios. Similar to prior experiments, we see that DeepTR-SD produces more accurate rankings than BOW and SD on both GOV2 and ClueWeb09.
In this paper, we present a novel framework for learning term weights using distributed representations of words from the deep learning literature. We motivate the framework by adopting the word vectors to represent terms and further to represent the query due to the ability to represent things semantically of word vectors. Therefore we propose to con-struct features for terms directly from the word vectors and model the target term weight as a regression problem.
We conducted experiments with two retrieval models, the language model and BM25, over four text collections of vary-ing sizes to demonstrate the effectiveness of the proposed framework. Specifically, the proposed framework predicts term weight for query terms that can be used to weight un-structured bag-of-words queries and queries that are a slight variant of the sequential dependency model. We observed significant improvements at high precision and through-out the rankings over the unweighted bag-of-words queries and the original unweighted sequential dependency model queries.

The proposed method is time efficient. Once the word vectors are trained in an offline step (that is itself relatively efficient), online prediction of term weight for new query terms involves only a simple inner product with the learned feature weights.
 There are several promising directions for further research. One can extend the method from creating vectors for bi-grams and proximity terms by averaging the word vectors of their constituents to a direct modeling of bigrams and proximity terms; although one might expect sparse training data for these query terms to be a problem, our results with small corpora suggest that it may not be necessary to have massive amounts of information for each term. Word vec-tors may also be useful for identifying terms that should be the focus of query expansion or terms that would be good expansion terms.
 This work is supported by National Science Foundation un-der grant IIS-1018317. We thank the anonymous reviewers for their helpful comments. word vector source.
 Figure 3: Performance gains of SD an d DeepTR-SD over BOW w.r.t to query length. Y-axis shows the rela-tive gain in MAP over BOW and parentheses in X-axis present the percentage of queries in each group. All retrievals are performed using LM. (best viewed in color)
