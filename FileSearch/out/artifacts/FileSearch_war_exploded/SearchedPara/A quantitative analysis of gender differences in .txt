 Gender has been an important research topic in the social sciences, with studies conducted on the ef-fect of gender on various aspects of human percep-tion and expression (Benshoff and Griffin, 2011) as well as investigations of the societal (Behm-Morawitz and Mastro, 2008) and career implica-tions of gender and possible underlying biases. Previous studies report significant implications of gender on career progress in medicine (Sidhu et al., 2009), information technology (Cohoon and Aspray, 2006), politics (Niven, 2006) and show-business (Smith, 2010).

In this paper we investigate the depictions of the genders in feature films, through the analysis of their respective dialogues. The differences in depiction are a contentious subject, since aspects of these can be viewed as the result of stereotyp-ing or gender bias, with the relative presence of women being a well investigated subject (Bielby and Bielby, 1996; Lincoln and Allen, 2004). We are interested in the existing gender depictions, re-gardless of relative frequencies, as well as any fac-tors that may affect them. While popular tools such as the Bechdel test provide a test for detecting female presence in the movies, we hope to iden-tify more subtle forms of gender differences across character gender from the dialogues. Our aim is to devise a non-binary metric that can be used to compare or rank movies, characters and perhaps individual utterances.

To analyze the dialogues we propose using a metric of language gender ladenness, a num-ber representing a normative rating of the  X  X er-ceived feminine or masculine association X  (Paivio et al., 1968) of language. The metric, as origi-nally defined, is meant to provide an indication of gender-specificity of individual words, with ex-treme values assigned to highly stereotypical con-cepts. Generating this rating for male and fe-male character dialogues and comparing the char-acter gender with this rating of  X  X anguage gender X  should allow us to observe stereotypical behavior.
Word based ratings such as the gender laden-ness are referred to as linguistic norms (or psy-cholinguistic norms when corresponding to psy-chological constructs) and are popular in cognitive psychology (Clark and Paivio, 2004) and some computational disciplines, such as sentiment anal-ysis (Nielsen, 2011) and opinion mining. To uti-lize gender ladenness, we follow an approach sim-ilar to simple sentiment analysis, with word-level norms automatically generated based on a small starting set of manually annotated norms and sen-tence (and higher) level norms estimated through word-level norm statistics. The resulting algo-rithm allows us to estimate gender ladenness at any arbitrary granularity.

We use these ratings of dialogue language to quantify the depictions of male and female char-acters and attempt to relate the observed gender ladenness with objective factors.

In section 2 and 3 we describe the data corpus Table 1: Sample word norms(  X  [  X  1 , 1] );  X  1 : Most masculine and +1 : Most feminine. and the feature extraction process respectively. We detail the experimental procedure in section 4 and analysis in section 5 and conclude with future ex-tensions in section 6. Gender Ladenness, as defined in (Clark and Paivio, 2004) represents the degree of perceived  X  X eminine or masculine association X  on a numeri-cal scale ranging from very masculine to very fem-inine. It is important to note that there was no re-striction to what  X  X ssociation X  may mean: while it is reasonable to assume that associations of the form  X  X  is B X  or  X  X  has B X  would dominate an-notator perception, that does not preclude other forms of association. Because of that, referring to the norms as indicators of how masculine or femi-nine the words are is not entirely accurate, though it is a reasonable approximation. The original rat-ings were re-scaled to [  X  1 , 1] for our purposes, with lower values indicating a masculine associ-ation and high values indicating a feminine asso-ciation. Some sample words, utterances and their corresponding ratings are presented in Table 1 and Table 3. Figure 1 shows the average gender laden-ness across all utterances for the major characters of a few movies. The annotations as a whole are reflective of stereotypical views of gender roles, e.g., words related to war and violence have a strong masculine association, whereas words re-lated to family or positive emotions carry strong feminine associations.

The manual annotations from (Clark and Paivio, 2004) contain ratings for only 925 words, which are not enough to provide sufficient coverage. Figure 1: Average Gender Ladenness for a few sample movies, marker size proportional to num-ber of utterances. Filled markers: Female charac-ters, Hollow markers: Male characters; PF: Pulp Fiction, KB: Kill Bill, BC: Breakfast Club, AH: Annie Hall.
 Therefore we use a lexicon expansion method, in-spired by the work of (Malandrakis et al., 2013) to estimate the gender ladenness  X  g ( w i ) of word w i using the semantic similarities s () between w i and reference words or concepts c j , as where the terms  X  i are trained model parameters.
Given a manually annotated lexicon and a set of reference words, this equation can be used to cre-ate a linear system. Solving the system via Least Squares Estimation (LSE) gives us the parameters  X  and an equation that can be used to generate gen-der ladenness for any new set of words.

Gender ladenness for larger lexical units is gen-erated via simple statistics, as the average of word gender ladenness over all content words (adjec-tives, nouns, verbs and adverbs). Our primary data source is the Movie DiC corpus (Banchs, 2012) which includes 619 movie scripts parsed from The Internet Movie Script Database (IMSDb, 2015). The xml formatted scripts con-tain transcripts with speaker information as well as some structural information. Additional metadata for each movie were collected from the Internet Movie Database (IMDb, 2015).
Since our goal was to analyze gender depic-tions, we had to annotate each script utterance with a gender label. The process was complicated by inconsistencies between the information contained in the IMDb and Movie DiC corpora, like mis-matched names, particularly for minor characters. Initially the script character names were cleaned using simple heuristics, such as the removal of all instances of the possessive  X  X  X  X . The IMDb api (IMDbPY, 2015) was used to recover candi-date movies matching the script movie name and, in the case of multiple candidates, the best candi-date was selected based on the number of character names matching the script. Character names were compared using the Jaro-Winkler distance (Win-kler, 1990). Having achieved a one to one map-ping between IMDb and Movie DiC, we assigned a gender label to each matched character, using the gender predictor (NamSor Applied Onomas-tics, 2015). To make these predictions, we first use the name of the corresponding actor portraying that role; if there was no character match, we use the name of the character. Finally, we calculate a confidence score of our gender assignment per ut-terance for each movie, equal to the percentage of utterances with perfectly matched character name and a high confidence by the gender predictor. For the movies for which the confidence scores are not satisfactory, we manually match the script charac-ters with IMDb X  X  characters and annotate genders. In our experiments, we did this manual annotation with roughly 75 movies.

Having a mapping of scripts to IMDb en-tries, we collected more information about the movie such as the list of genres it belongs to and the members of the production team (producers, scriptwriters, directors), and followed a similar process as described above to assign genders to all persons. While movies may be created by multi-ple scriptwriters and directors, we retain only the first name, the primary credit, in each category. We removed infrequent genres and movies which belonged only to the removed genres. We also fil-tered out utterances with missing or incorrect char-acter information and the utterances correspond-ing to characters for which the gender predictor fails to make confident predictions.

Movies with missing fields were also removed, leaving us with a total of 568 movies after the aforementioned pre-processing steps. Table 2 lists some descriptive statistics of the processed movie Movie Utterances 107372 256274 363646 Assistant Directors 846 2739 3585 Screenplay Writers 130 1217 1347 Casting Directors 548 195 743 Table 3: Average gender ladenness for sample ut-terances from the dataset corpus. At least in terms of raw frequencies, the gender ratio is clearly skewed towards male, par-ticularly in the case of directors and with the ex-ception of casting directors.

The norm generating equation (1) requires a se-mantic similarity estimate s () , which for the pur-poses of this paper is the cosine of context vectors generated over a large corpus of raw web text. The corpus was created by posing a query to the Ya-hoo! search engine for every word in the English version of the aspell spell-checker and collecting the top 500 result previews. Each preview is com-posed of a title and a sample of the content, each being a single sentence. Overall the collected cor-pus contains approximately 117 million sentences. The descriptive feature in this method is gender ladenness, so we extracted an estimate for each utterance of every movie. Initially, all utterances were part-of-speech tagged and non-content words were removed. Then, word-level gender landen-ness norms were generated for every remaining word.

To generate word-level norms, we used equa-tion (1) with the intermediate seed words w i be-ing the top 10000 most frequent words in our cor-pus of web text with length longer that 3 char-acters. For each word in our corpus, we gener-ated a binary weighted context vector (of window size 1) of size  X  125000 . Then, for each word of interest we calculated a 10000 place similar-ity vector, containing the cosine similarity scores between the context vector of said word and the context vectors of the 10000 intermediate seeds. Using the training set we generated a K  X  10000 matrix of similarities to the seed words and ap-plied dimensionality reduction via Principal Com-ponent Analysis (PCA), keeping the first N = 300 components. These transformed similarities be-came the similarity terms s () of equation (1) and were used to train the model. For any word in the scripts, a 10000 place similarity vector is gen-erated and transformed using the pre-calculated PCA matrix, then equation (1) is used to create the gender ladenness estimate.

Ratings were generated at the utterance level, and collective ratings (per character, gender or movie) were calculated as utterance rating aver-ages. To evaluate the word norm generation algorithm, we performed a 10-fold cross-validation experi-ment on the 925 manually annotated norms in (Paivio et al., 1968). The generated norms were evaluated against the ground truth and the method achieved a 0.801 Pearson correlation to the ground truth. While there is no comparable result in lit-erature, the resulting performance appears suffi-ciently high.

We first investigated the overall gender laden-ness of movies, represented as the average of all utterance level scores, with respect to the genre(s) the movie belongs to. The independent variables for this analysis were nine binary indicator vari-ables, one for each of the most frequent genre la-bels in our movie corpus, with values of zero if the movie does not belong to the specific genre and one if it does. The particular representa-tion of genres as separate variables was chosen because each movie can belong to multiple gen-res. Interaction terms were included. Running n-way ANOVA with the aggregate gender ladenness across both character genders as the dependent variable revealed significant differences between genres, with Action movies leaning towards the masculine ( p = 0 . 013 ) compared to Non-Action movies, a not surprising result.

A few significant interactions between genres are shown in figure 2. Fig. 2a indicates that among non-drama movies, romantic movies tend to in-(a) Drama v/s Romance (b) Crime v/s Thriller (c) Action v/s Drama (d) Adventure v/s Mystery clude more feminine language compared to non-romantic movies. However, if a movie belongs to the genre drama, its mean gender ladenness scores remain fairly constant, irrespective of its other genres. Similar interpretations can be drawn from the other plots in figure 2.

To analyze the effect of character gender on the gender ladenness scores, we next ran ANOVA with the character gender and the movie writer X  X  gender as additional independent variables. The dependent variable in this case was the aggregate gender ladenness score across all utterances for male and female characters, so two scores per movie. The interaction of character gender and movie genre is shown in figure 3. The scores of male and female characters are correlated, which can be attributed to the underlying concepts in the utterances from these movies. The differ-ence between genders is significant ( p = 0 . 034 ), with male characters consistently using signifi-cantly more masculine language than their female counterparts, a finding that lends some credence to the metric used. Looking at the binary genre variables revealed that
Action movies contained significantly more masculine language than Non-Action movies ( p &lt; 10  X  5 ) and the same holds for Crime movies ( p &lt; 10  X  5 ). Conversely, Romantic movies leaned towards the more feminine language than non-Romantic movies ( p &lt; 10  X  5 ) and similarly for Comedy movies compared to non-Comedy movies ( p = 0 . 02 ). The male -female character gender Figure 3: Interaction of movie genre with charac-ter gender ladenness distance however is not affected in any significant way by the genre.

We include only the screenplay writer X  X  gen-der in our analysis, though both the directors and screenplay writers influence the dialog lines (utter-ances), since the writers are more likely to directly influence the actual language used. In addition, the very small number of female directors in the data, as seen in table 2, leads to a violation of ANOVA X  X  homoscedasticity assumption. Though the writer gender itself was not a significant factor, the inter-action of writer X  X  gender with the Action genre was significant ( p = 0 . 005 ). The plot illustrating this interaction is shown in figure 4. It appears that female script writers write more masculine utter-ances compared to their male colleagues, at least for Action movies. We also investigated interac-tions between the writer and character gender, but none proved significant. We used regression to extrapolate manually an-notated psycholinguistic normatives to movie ut-terances and investigated the use of these met-rics to describe gender depictions. The metric proved successful, showing significant differences between the genders and predictable patterns with respect to movie genres.

Future work will include the use of further met-rics, with those describing emotions being the first candidates. We also intend to collect more movie and character level metadata to be used in anal-ysis. Finally, it is worth remembering that lan-guage provides only a partial description of de-Figure 4: Interaction of screenplay writer X  X  gender with genre picted characters, so we should aim to augment with aural/visual information. The authors gratefully acknowledge support from NSF, Geena Davis Institute on Gender in Media and Google.org.

