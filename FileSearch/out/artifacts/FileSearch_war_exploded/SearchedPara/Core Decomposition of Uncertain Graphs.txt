 Core decomposition has proven to be a useful primitive for a wide range of graph analyses. One of its most appealing features is that, unlike other notions of dense subgraphs, it can be computed linearly in the size of the input graph.
In this paper we provide an analogous tool for uncertain graphs, i.e., graphs whose edges are assigned a probability of existence. The fact that core decomposition can be com-puted efficiently in deterministic graphs does not guarantee efficiency in uncertain graphs, where even the simplest graph operations may become computationally intensive. Here we show that core decomposition of uncertain graphs can be carried out efficiently as well.

We extensively evaluate our definitions and methods on a number of real-world datasets and applications, such as influence maximization and task-driven team formation . H.2.8 [ Database Management ]: [Database Applications-Data Mining]; G.2.2 [ Discrete Mathematics ]: [Graph Theory-Graph Algorithms] uncertain graphs; dense subgraph; core decomposition
Uncertain graphs , i.e., graphs whose edges are assigned a probability of existence (see an example in Figure 1), arise in several emerging applications [24, 14, 15]. For instance, in biological networks and protein-interaction networks ver-tices represent genes and/or proteins, while edges represent interactions among them. Since the interactions are derived through noisy and error-prone laboratory experiments, the existence of each edge is uncertain [4, 26, 24]. In social networks uncertainty arises for various reasons [1]. Edge probabilities may represent the outcome of a link-prediction task [20] or the influence of one person on another, like in Figure 1: An uncertain graph and its ( k ,  X  ) -core de-composition for  X  = 0 . 04 . Vertex 1 has core number 1, vertices 2 and 7 have core number 2, and vertices 3, 4, 5 and 6 have core number 3. viral marketing [11]. Uncertainty can also be intentionally injected for privacy purposes [7].

Finding dense subgraphs is a fundamental primitive in many graph-analysis tasks [21]. There exist many differ-ent definitions of what a dense subgraph is, e.g., cliques, n-cliques, n-clans, k-plexes, f-groups, n-clubs, lambda sets , most of which are NP -hard to compute or at least quadratic in the size of the input graph. In this respect, the notion of core decomposition is particularly appealing as ( i ) it can be computed in linear time [5], and ( ii ) it is related to many other definitions of a dense subgraph (as discussed later).
The k -core of a graph is defined as a maximal subgraph in which every vertex is connected to at least k other ver-tices within that subgraph. The set of all k -cores of a graph G forms the core decomposition of G [25]. The fact that core decomposition can be performed in linear time in de-terministic graphs does not guarantee efficiency in uncertain graphs. Indeed, in such graphs even the simplest tasks may become hard. As an example, consider the two-terminal-reachability problem, which asks whether two query vertices are connected. In a deterministic graph the solution to this problem requires a simple scan of the graph. Instead, in un-certain graphs, computing the probability that two vertices are connected is a # P -complete problem [28].

Thus, a major question we aim at answering in this pa-per is: can the core decomposition of an uncertain graph be computed efficiently? Related work and applications. Existing research on uncertain graphs has mainly focused on querying [15, 33, 24, 31] and mining, particularly on extracting frequent sub-graphs [34] or subgraphs that are connected with high prob-ability [14], and clustering [22, 18].

Core decomposition of deterministic graphs has been ex-ploited to analyse the nature of a network and discover dense substructures [2, 17]. It has been applied in many different d omains, such as bioinformatics [30], software engineering [32], and social networks [17]. Core decomposition has been also used to speed-up the computation of more complex def-initions of a dense subgraph. For instance, it serves to find maximal cliques more efficiently [10], and it is at the ba-sis of linear-time approximation algorithms for the densest-subgraph problem [19] and the densest at-least-k -subgraph problem [3]. It is also used to approximate betweenness cen-trality [13]. A core-decomposition tool for uncertain graphs would thus provide a natural extension of all these appli-cations to the context of uncertain graphs. Other direct applications of core decomposition of uncertain graphs in-clude influence maximization and task-driven team forma-tion , which we showcase in Section 6 and 7, respectively.
In influence maximization [16], the probability of an edge ( u, v ) represents the influence that u exerts on v , i.e., the likelihood that some action/information propagates from u to v . The greedy algorithm [12] traditionally used to find the users that maximize the information spread over the network requires a number of Monte Carlo simulations that largely limit its efficiency. In Section 6 we show how our probabilistic core-decomposition tool can be used to speed-up the influence-maximization process.

In task-driven team formation , the input is a collabora-tion graph G = ( V, E,  X  ), where vertices are individuals and edges exhibit a probabilistic topic model  X  representing the topic(s) of past collaborations. A query is a pair h T, Q i , where T is a set of terms describing a new task, and Q is a set of vertices. The goal is to find an answer set of vertices A , such that A  X  Q is a good team for the task described by T . The given query task T , along with the topic model  X  , induces a (single) probability value p T for each edge ( u, v )  X  E , such that p T ( u, v ) represents the like-lihood that u and v collaborate on T . This gives rise to an uncertain graph to which one can naturally apply core decomposition in order to find the desired team (Section 7). Challenges and contributions. In this paper we study the problem of core decomposition of uncertain graphs, which, to the best of our knowledge, has never been con-sidered so far. We introduce (Section 2) the notion of ( k,  X  ) -core as a maximal subgraph whose vertices have at least k neigbours in that subgraph with probability no less than  X  ; here  X   X  [0 , 1] is a threshold defining the desired level of certainty of the output cores.

Let the  X  -degree of a vertex v be the maximum degree such that the probability for v to have that degree is no less than  X  . We design an algorithm for finding a ( k,  X  )-core decomposition that iteratively removes the vertex hav-ing the smallest  X  -degree and prove its correctness (Section 3). The proposed algorithm resembles the traditional al-gorithm for computing the core decomposition of a deter-ministic graph [5]; however, as usual when the attention is shifted from the deterministic context to uncertain graphs, the adaptation of that algorithm is non-trivial. A major challenge is the capability of handling large graphs.
Two main critical steps affect our algorithm: computing initial  X  -degrees and updating  X  -degrees whenever a vertex is removed from the graph. While the corresponding steps in the deterministic case (i.e., computing and updating the degree of a vertex) are straightforward, performing them efficiently in uncertain graphs needs a great deal of atten-tion; approaching them na  X   X vely, indeed, may even lead to intractable (exponential) time complexity. We show how to overcome the exponential-time complexity by devising a novel yet efficient dynamic-programming method to com-pute  X  -degrees from scratch. We also exploit the same intu-ition underlying the dynamic-programming algorithm so as to efficiently update  X  -degrees after a vertex removal. As a result, we show that computing a ( k,  X  )-core decomposition takes O ( m  X ) time, where m is the number of edges in the input uncertain graph and  X  is the maximum  X  -degree.
As a further contribution, we devise a novel method to improve the efficiency of the proposed ( k,  X  )-core-decomposition algorithm (Section 4). The idea is to exploit a fast-to-compute lower bound on the  X  -degree that can be used as a placeholder during the first iterations while being replaced with the actual  X  -degree only when the vertex at hand is selected and the graph has become smaller.
Finally, we report experiments on efficiency and numeri-cal stability on real-world graphs (Section 5) and show our proposal at work in two real-life applications (Sections 6 X 7). Cores of deterministic graphs. Before focusing on un-certain graphs, we briefly recall the problem of computing cores of deterministic graphs. Let G = ( V, E ) be an undi-rected graph, where V is a set of n vertices and E  X  V  X  V is a set of m edges. For every vertex v  X  V , let deg ( v ) and deg H ( v ) denote the degree of v in G and in a sub-graph H of G , respectively. Also, given a set of vertices C  X  V , let E | C denote the subset of edges induced by C , i.e., E | C = { ( u, v )  X  E | u  X  C, v  X  C } .

Definition 1 ( k -core). The k -core (or core of order k ) of G is a maximal subgraph H = ( C, E | C ) such that  X  v  X  C : deg H ( v )  X  k . The core number (or core index ) of a vertex v , denoted c ( v ) , is the highest order of a core that contains v . The set of all k -cores of G , for all k , is the core decomposition of G .

The notion of k -core is strictly related to the notion of k -shell , that is the subgraph induced by the set of all vertices having core number equal to k . Note that neither k -cores nor k -shells are necessarily connected subgraphs. Also, while these two notions usually refer to subgraphs of the input graph, in the remainder we slightly abuse of notation and de-note by k -core (or k -shell) both the subgraph H = ( C, E | C ) itself and the vertex set C that induces H .

All k -shells of a graph G form a partition of the vertex set V , while all k -cores are nested into each other: G = C 0  X  C 1  X   X  X  X   X  C k  X  ( k  X  = max v  X  V c ( v )). As a result, the core decomposition of G is unique and fully determined by the core number c ( v ) of all vertices v in G : the k -core of G simply corresponds to (the subgraph induced by) the set of all vertices v having core number c ( v )  X  k .

Batagelj and Zaver X snik [5] show how to compute the core decomposition of a graph G in linear time (Algorithm 1). The algorithm iteratively removes the smallest-degree ver-tex and sets the core number of the removed vertex accord-ingly. Vertices are thus required to be ordered based on their degree. Defining the initial vertex ordering and keep-ing vertices ordered during the execution of the algorithm take O ( n ) and O (1) time, respectively. The idea is to employ an n -dimensional vector D whose single cells D [ i ] store all vertices having degree equal to i in the current graph. The overall time complexity of the algorithm is hence O ( n + m ). Algorithm 1 k -c ores 2: for all v  X  V do 3: d [ v ]  X  deg ( v ) 5: end for 6: for all k = 0 , 1 , . . . , n do 7: while D [ k ] 6 =  X  do 8: pick and remove a vertex v from D [ k ] 9: c [ v ]  X  k 13: end for 14: remove v from G 15: end while 16: end for Cores of uncertain graphs. L et G = ( V, E, p ) be an uncertain graph, where p : E  X  (0 , 1] is a function that assigns a probability of existence to each edge. 1 For the sake of brevity, we hereinafter denote the probabilities p ( e ) with p e . For every vertex v  X  V , let N v = { ( u, v )  X  E } denote the set of edges incident to v , and d v = | N v | its size.
To define our notion of core decomposition of an uncertain graph, we resort to the well-known possible-world semantics , which has been recognized as a sound principle to define queries on probabilistic data [9]. Broadly, such a princi-ple interprets the probabilistic data as a set of deterministic instantiations, called possible worlds , each of which associ-ated with its probability of being observed. In the context of uncertain graphs, the bulk of the literature assumes the probabilities of existence of the edges independent from one another [24, 14, 15]. Under this assumption, the possible-world semantics interprets an uncertain graph G with m edges as a set of 2 m possible deterministic graphs (worlds), each of which containing a subset of the edges in E . More precisely, an uncertain graph G = ( V, E, p ) yields a set of possible graphs { G = ( V, E G ) } E G  X  E , and the probability of observing a possible graph G = ( V, E G )  X  X  is: According to the possible-world semantics, answering a probabilistic query q means to derive a probability distri-bution over all possible deterministic answers a to the query q , where the probability of an answer a corresponds to the sum of the probabilities of all worlds where a is the an-swer to q . As this answer distribution is usually too large and sparse to be explicitly interpreted or computed/stored, the general turnaround adopted is to assign a score to each domain object based on its probability of being part of an answer to the probabilistic query q , and return the objects having highest scores as a final answer to q [9].

We cast such a general framework to our context by defin-ing the score of each vertex v to be part of a k -core H as the probability that v has degree no less than k in H , i.e., Pr[ deg H ( v )  X  k ]. Then, we employ a classic threshold-based approach to decide which vertices should actually form a k -core based on their scores. As a result, the notion of proba-bilistic ( k,  X  ) -core we come up with is the following:
Definition 2 (Probabilistic ( k ,  X  )-cores). Given an uncertain graph G = ( V, E, p ) , and a threshold  X   X  [0 , 1] , the probabilistic ( k,  X  )-core of G is a maximal subgraph H = ( C, E | C, p ) such that the probability that each vertex v  X  C has degree no less than k in H is greater than or equal to  X  , i.e.,  X  v  X  C : Pr[ deg H ( v )  X  k ]  X   X  . The notion of  X  -core number immediately follows from the definition of ( k ,  X  )-core and is defined as the highest order k of a ( k ,  X  )-core containing v .
 The problem we address in this work is the following. Problem 1 (ProbCores). Given an uncertain graph G and a probability threshold  X   X  [0 , 1] , find the ( k,  X  ) -core decomposition of G , that is the set of all ( k ,  X  )-cores of G .
Our definition of core decomposition of an uncertain g raph, has the desirable feature of being unique, as formally shown in the next theorem.

Theorem 1. Given an uncertain graph G and a probabil-ity threshold  X  , the ( k,  X  ) -core decomposition of G is unique.
Proof. We prove the theorem by showing that G cannot have more than one ( k,  X  )-core, for all k . Assume that G has two ( k,  X  )-cores and denote them by H 1 and H 2 , respectively. According to Definition 2, it holds that H 1 is a maximal subgraph of G such that  X  v  X  H 1 : Pr[ deg H 1 ( v )  X  k ]  X   X  , and the same happens for H 2 . Combining the ( k,  X  )-core conditions of H 1 and H 2 leads to the subgraph H 1  X  H 2 to satisfy the ( k,  X  )-core condition too, as  X  v  X  H 1 Pr[ deg H 1 ( v )  X  k ]  X   X   X  X  X  v  X  H 2 : Pr[ deg H 2 ( v )  X  k ]  X   X  clearly implies that  X  v  X  X  1  X  X  2 : Pr[ deg H 1  X  X  2 ( v )  X  k ]  X   X  . This means that neither H 1 nor H 2 are maximal, thus contradicting the hypothesis. The theorem follows.
An example of ( k ,  X  )-core decomposition of an uncertain graph is provided in Figure 1.
For a vertex v of the input uncertain graph G , the proba-bility Pr[ deg ( v )  X  k ] can be expressed as: where G  X  k v is the set of all possible graphs drawn from G where v has degree  X  k , i.e., G  X  k v = { G  X  X  | deg G ( v )  X  k } .
It is easy to see that such a probability value is mono-tonically non-increasing with k , i.e., Pr[ deg ( v )  X  0]  X  Pr[ deg ( v )  X  1]  X  . . .  X  Pr[ deg ( v )  X  d v ]. Then, given a threshold  X  , for every vertex v in the graph, there exists a value  X  k  X  [0 ..d v ] such that Pr[ deg ( v )  X  h ]  X   X  , for all h  X   X  -degree of vertex v.
 Definition 3 (  X  -degree). Given an uncertain graph G = ( V, E, p ) and a threshold  X   X  [0 , 1] , the  X  -degree  X  -deg ( v ) of a vertex v  X  V is defined as Let also  X  -deg H ( v ) be the  X  -degree of v in a subgraph H . Algorithm 2 ( k ,  X  ) -cores 3: for all v  X  V do 6: end for 7: for all k = 0 , 1 , . . . , n do 8: while D [ k ] 6 =  X  do 9: pick and remove a vertex v from D [ k ] 10: c [ v ]  X  k 12: recompute  X  -deg ( u ) 15: end for 16: remove v from G 17: end while 18: end for Intuitively, the notion of  X  -degree gives an idea of the de-gree of a vertex given a specific threshold  X  . We exploit the notion  X  -degree to adapt the k -cores algorithm used for de-terministic graphs to the context of uncertain graphs. The proposed algorithm, called ( k ,  X  )-cores (Algorithm 2), fol-lows the same scheme as in the deterministic case with the main difference of the use of the  X  -degree. The soundness of the proposed algorithm is shown in the following theorem.
Theorem 2. Given an uncertain graph G and a threshold  X  , Algorithm 2 provides the ( k ,  X  )-core decomposition of G .
Proof. For every v  X  V and every subgraph H = ( C, E C , p | C ) of G , it is easy to see that  X  -deg H  X  -deg ( v ), as the  X  -degree computation in G relies on more successful events than those encountered in H . This implies that  X  -deg H ( v ) is a monotonic vertex property function [5], where, for every v  X  V and C  X  V , a vertex property func-tion on G is a function  X  ( v, C ) : V  X  2 V  X  R , and the mono-tonicity property holds if  X  C 1 , C 2  X  V : C 1  X  C 2 implies that  X  v  X  V :  X  ( v, C 1 )  X   X  ( v, C 2 ). The proof is completed by the result by Batagelj and Zaver X snik [5], who show that, for a monotonic vertex property function  X  ( v, C ), the algo-rithm that repeatedly removes a vertex with the smallest  X  value gives the desired core decomposition. 2
Instead of computing/updating standard degrees, in the p robabilistic case one thus needs to ( i ) compute all  X  -degrees at the beginning of the algorithm (Line 1), and ( ii ) update the  X  -degree of a neighbour of the currently being processed vertex v (Line 12). While computing/updating degrees in the deterministic case is straightforward, for the  X  -degrees such steps are non-trivial, as shown next.
 To show how to derive  X  -degrees from scratch, we first fo-cus on the computation of Pr[ deg ( v )  X  k ] for a vertex v , and note that Pr[ deg ( v )  X  k ] is equal to the sum of the probabil-for all i  X  [0 ..k  X  1]:
Pr[ deg ( v )  X  k ] =
Furthermore, we observe that each individual Pr[ deg ( v ) = i ] can in turn be computed considering all subsets of edges N  X  N v of size i and summing over the probabilities that all and only the edges in these various N exist: The sum in the above formula is over all subsets N  X  N v , | N | = i ; thus, a na  X   X ve computation would lead to a time complexity exponential in the size of N v . We can however manage this by rearranging the formula as where P v = next recursive formula, which has originally been introduced in [8] for sampling from a finite population with unequal probabilities and without replacement: where T ( j, N v ) = Equation (5) allows for computing all individual Pr[deg( v ) = i ] values, for all i  X  [0 ..k  X  1] (which, according to Equa-tion (3), are needed to derive the desired Pr[deg( v )  X  k ]) in polynomial time, precisely in O ( kd v ) time.
 A dynamic-programming method. Although the above way of computing Pr[deg( v ) = i ] solves a seemingly exponential-time problem, it still has weaknesses due to the recursive formula in Equation (5). Firstly, as the formula in-volves both products and sums of  X  p e values that can be either very large (when p e  X  1) or very small (when p e  X  0), it may incur numerical-stability issues, which might make the computation of Pr[deg( v ) = i ] problematic when executed by a computer. Secondly, using such a formula, the  X  -degree of a vertex v when one of its incident edges is removed can-not be recomputed faster than a from-scratch computation.
For the above reasons, we propose here an alternative way of computing Pr[deg( v ) = i ]. Consider a vertex v and an edge e incident to v , and let G \{ e } denote the subgraph of G where e is not present. The method is based on the following key observation: the event  X  v has degree k in G  X  implies that either  X  e exists and v has degree k  X  1 in G \{ e }  X  e does not exist and v has degree k in G \{ e }  X . This way, the probability for v to have degree k in the original graph G can be computed as a linear combination of the probabilities that v has degree either k  X  1 or k in the subgraph G \{ e }
The above reasoning can be generalised to every subgraph of G and formally expressed in the next theorem (for which we omit a formal proof due to limited space).

Theorem 3. Given an uncertain graph G = ( V, E, p ) and incident to v ordered in some way. Also, given a subset N  X  N v , let deg ( v | N ) denote the degree of v in the subgraph  X 
Theorem 3 provides a principled way to efficiently com-p ute Pr[ deg ( v ) = i ] based on the dynamic-programming paradigm. Particularly, we take an arbitrary ordering of the edges incident to the vertex v being currently under consideration and define a proper recursive formula that allows for computing partial solutions relying only on the first i edges. The ultimate score (i.e., the actual value of Pr[ deg ( v ) = i ]) is available only when all the edges have been considered; this makes the overall computation inde-pendent from the specific ordering of the edges. Formally, j  X  [  X  1 ..i ]. We set the following base cases: while we exploit Equation (6) to compute the generic dynamic-programming recursive step as for all h  X  [1 ..d v ], j  X  [0 ..h ]. We need to compute all X (  X  ,  X  ) values so as to get to X ( d v , i ), which corresponds to the desired probability Pr[ deg ( v ) = i ]. This requires O ( id time.

Moreover, one can notice that the values of the entire set probability values { Pr[ deg ( v ) = j ] } i j =0 . Thus, employing the proposed dynamic-programming method and setting i = k  X  1], which are required for computing Pr[ deg ( v )  X  k ] according to Equation (3), can all be derived in O ( kd v
Thus, the dynamic-programming method just described has the same complexity as the method based on Equa-tion (4). But, at the same time, it ( i ) alleviates the numerical-stability shortcomings, as the numbers involved into Equation (6) are all probabilities  X  1 (unlike the num-bers  X  p e which range from [0 ,  X  )), and ( ii ) can easily be employed for efficiently updating  X  -degrees when an edge is removed from the graph, as described next.
 Time complexity. The  X  -degree of a vertex v can be com-puted incrementally. We start with k = 0 and Pr[ deg ( v )  X  0] = 1. Then, we increase k one by one and compute stop once Pr[ deg ( v )  X  k ] &lt;  X  , and we set  X  -deg ( v ) = k  X  1. This way, we need to compute probabilities Pr[ deg ( v ) = k ] only for k = 0 , ...,  X  -deg ( v ) + 1, which, according to the findings reported above, leads to a time complexity of O (  X  -deg ( v )  X  d v ). Clearly, in the worst case, such a com-plexity equals O ( d 2 v ), but we expect in practice  X  -deg ( v ) rea-sonably lower than d v , especially for those vertices having very large d v and/or large enough  X  values.
 Computing all  X  -degrees hence takes O ( d ). Denoting by  X  the maximum  X  -degree over all vertices in the graph, i.e.,  X  = max v  X  V  X  -deg ( v ), the complexity can be more compactly expressed as O ( We now consider the case where the  X  -degree of a vertex v needs to be updated because an edge incident to v has been removed. We recall that this is the other crucial step of our ( k ,  X  )-cores algorithm (Algorithm 2, Line 12).

As anticipated, we can exploit Theorem 3 to avoid from-scratch recomputations. The problem can be re-duced to (efficiently) updating the probabilities Pr[ deg ( v ) = able because of the computation of the earlier  X  -degree. Once all these new probabilities are computed, the new  X  -degree can be derived by the same incremental process described in the previous paragraph  X  X ime complexity X . Let e denote the edge to be removed and let hand for the new probabilities Pr[ deg ( v | N v \{ e } ) = i ] to be computed. Such Pr[ deg ( v | X  e ) = i ] values can be derived by rearranging Equation (6) as follows: This way, one can set Pr[ deg ( v | X  e ) = 0] = 1 1  X  p 0], and apply Equation (7) to compute the remaining ability Pr[ deg ( v | X  e ) = i ] takes constant time. Computing all the new probabilities, and, hence, updating the  X  -degree of v , globally takes O (  X  -deg ( v )) time, thus improving upon the O (  X  -deg ( v )  X  d v ) time of a from-scratch recomputation. Overall running time of ( k ,  X  )-cores We analyse now the overall time complexity of our ( k ,  X  )-cores algorithm. The initialisation phase (Lines 1 X 6) is dominated by the computation of the initial  X  -degree for all vertices, which takes O ( m  X ) time ( X  is the maximum  X  -degree over all vertices). In the main cycle (Lines 7 X  18), like the deterministic case, each vertex is visited only once and then removed from the graph. For each vis-ited vertex v , the  X  -degree of all its neighbours has to be updated. As reported above, for a single neighbour u , this takes O (  X  -deg ( u )). Thus, the main cycle globally takes O ( O ( m  X ). In conclusion, the running time of the ( k ,  X  )-cores algorithm is therefore O ( m  X ).
In this section we show how to further speed-up our ( k ,  X  )-cores algorithm. Our key observation is that the main bottleneck of ( k ,  X  )-cores is the computation of initial  X  -degrees (experimentally confirmed in Section 5): although this step is asymptotically as fast as updating  X  -degrees af-ter a vertex removal, the latter is in practice faster as it is performed on a graph that gets progressively smaller. In this regard, we derive a fast-to-compute lower bound on the  X  -degree and use it as a placeholder during the first itera-tions, while replacing it with the actual  X  -degree only when the vertex at hand is going to be processed. This way, the initial  X  -degrees can be computed only when actually needed and on a smaller graph, thus leading to the desired speed-up.
In the following we provide the details of our lower bound on the  X  -degree and show how to efficiently update this bound after vertex removals. Then, we describe how to in-corporate such findings into the enhanced algorithm. Lower bound on the  X  -degree. We define our lower bound on the  X  -degree in terms of the regularised beta func-tion . Given a real number z  X  [0 , 1] and two integers a and b , the regularized beta function I z ( a, b ) is defined as the ra-tio between the incomplete beta function B ( z ; a, b ) and the beta function B ( a, b ) [29]:
I ( a, b ) = B ( z ; a, b ) Given a vertex v in the input graph, let p min ( v ) denote the minimum probability on the edges incident to v , i.e., p min ( v ) = min e  X  N v p e . The next lemma shows how the probability for v to have degree no less than k can be lower-bounded by using the regularised beta function I .
Lemma 1. Given an uncertain graph G = ( V, E, p ) , for every vertex v  X  V and for all k  X  [0 ..d v ] it holds that
Proof. Consider a vertex v  X  having as many incident edges as v , and assume that each edge incident to v  X  has probability p min ( v ). It is easy to see that Pr[deg( v ) = i ]  X  Pr[deg( v  X  ) = i ], for all i . Exploiting Equation (4) we get: Combining such a result with Equation (3) we obtain: Pr[ deg ( v )  X  k ] =  X  The lemma follows.

The desired lower bound on  X  -degree can now immediately be derived by exploiting Lemma 1. We denote such a lower bound by  X  -lb and formally state it in the next theorem.
Theorem 4. Given an uncertain graph G = ( V, E, p ) , for every vertex v  X  V it holds that  X  -deg ( v )  X   X  -lb ( v )=max { k  X  [0 ..d v ] | I p min The computation of the above lower bound is very fast. For a fixed z , the values I z ( a, b ) of the regularised beta func-tion are monotonically non-increasing as a increases and/or b decreases. Therefore, the lower bounds on Pr[ deg ( v )  X  k ] are monotonically non-increasing as k increases and one can thus perform binary search to derive the maximum k such that I p min ( k, d v  X  k +1)  X   X  , which, according to Theorem 4, corresponds to the lower bound  X  -lb ( v ). The computation of  X  -lb ( v ) requires a logarithmic (in the number of edges of v ) number of evaluations of I z . Each evaluation of I can be computed in constant time using tables [23]. Thus, computing  X  -lb ( v ) for a vertex v takes O (log d v ) time.
A major feature of the lower bound  X  -lb is its fast from-scratch computation. Here we show that it can also be up-dated very efficiently (i.e., in constant time) when an edge is removed from the graph. To this end, we first need to report a couple of results. We start by showing that the  X  -degree of a vertex v can decrease at most by one when an edge incident to v is removed (Lemma 2).

Lemma 2. Given an uncertain graph G = ( V, E, p ) and a vertex v  X  V , let e  X  N v be an edge incident to v and let H = ( V, E \{ e } , p ) be the subgraph of G where e is missing. Also, let  X  -deg H ( v ) be the  X  -degree of v in H . It holds that  X  -deg H ( v ) &gt;  X  -deg ( v )  X  2 .
 Proof.
 Pr[ deg ( v )  X  k ] = =  X  Pr[ deg H ( v )  X  k  X  1] .
 By the definition of  X  -degree we know that Pr[ deg H ( v )  X   X  -deg H ( v ) + 1] &lt;  X  ; thus, setting k =  X  -deg H above inequality, we get Pr[ deg ( v )  X   X  -deg H ( v )+2]  X  Pr[ deg H ( v )  X   X  -deg Then,  X  -deg ( v ) &lt;  X  -deg H ( v ) + 2, or, equivalently,  X  -deg H ( v ) &gt;  X  -deg ( v )  X  2. The lemma follows.
Based on the above lemma, we can also prove that the l ower bound  X  -lb ( v ) of a vertex v can decrease at most by one when an edge incident to v is removed.

Theorem 5. Given an uncertain graph G = ( V, E, p ) and a vertex v  X  V , let e  X  N v be an edge incident to v and let H = ( V, E \{ e } , p ) be the subgraph of G where e is missing. Also, let  X  -lb H ( v ) be the lower bound on the  X  -degree of v in H . It holds that  X  -lb H ( v ) &gt;  X  -lb ( v )  X  2 .
Proof. Consider a vertex v  X  having as many incident edges as v , and assume that each edge incident to v  X  has probability p min ( v ). It is easy to see that the  X  -degree  X  -deg ( v  X  ) of v  X  equals the lower bound  X  -lb ( v ). Combin-ing this with Lemma 1, we get  X  -lb H ( v ) =  X  -deg H ( v  X  -deg ( v  X  )  X  2 =  X  -lb ( v )  X  2 . The theorem follows.
Theorem 5 can be exploited for safely updating  X  -l b in constant time. Let e denote again the edge incident to v to be removed and let H be the subgraph of G where e is missing. Thus,  X  -lb ( v ) denotes the earlier lower bound of v , while  X  -lb H ( v ) denotes the new lower bound to be computed after the e  X  X  removal. The idea is to compute (in constant time) just the value I p min ( v ) (  X  -lb ( v ) , ( d v I min ( v ) (  X  -lb ( v ) , d v  X   X  -lb ( v )). Lemma 1 ensures that
Pr[ deg H ( v )  X   X  -lb ( v )]  X  I p min ( v ) (  X  -lb ( v ) , d the lower bound has not changed, i.e.,  X  -lb H ( v ) =  X  -lb ( v ). Otherwise, it means that the lower bound has decreased. According to Theorem 5, this decreasing can be at most by one, hence we can safely set  X  -lb H ( v ) =  X  -lb ( v )  X  1.
A major shortcoming of updating  X  -l b as described above is that, for each vertex v , we need to load/keep-in-memory O ( d 2 v ) values of I z (i.e., all values within { I p min 1) | h  X  [0 ..d v ] , k  X  [0 ..h ] } ). This would penalize too much both time and space complexity of the algorithm. However, this can be overcome by still relying on Theorem 5. The time an edge e incident to v is removed, no matter whether I min ( v ) (  X  -lb ( v ) , d v  X   X  -lb ( v ))  X   X  or not. Indeed, Theo-rem 5 guarantees that  X  -lb ( v )  X  1 is still a lower-bound for  X  -deg ( v ), even though possibly less tight. This way our algo-rithm would require only O ( d v ) values of I z for each vertex v , i.e., just the values { I p min ( v ) ( k, d v  X  k + 1) | k  X  [0 ..d The E-( k ,  X  )-cores algorithm. We now provide the details of our enhanced -( k ,  X  )-cores (for short, E-( k ,  X  )-cores ) algorithm (pseudocode omitted for space reasons). The algo-rithm follows the scheme of the basic ( k ,  X  )-cores algorithm (Algorithm 2). The main difference is that, for each vertex v , the lower bound  X  -lb ( v ) is computed in the initialisation phase, rather than the exact  X  -degree. A set e V keeps trace of the vertices for which the exact  X  -degree has not been com-puted yet. Right after initialisation, e V corresponds to the whole vertex set V . In the main cycle, vertices are processed based on their (lower bound on)  X  -degree. When a vertex v is being processed, it is primarily checked whether its exact  X  -degree is already available. If not, the exact  X  -degree of v is computed and v is moved to the proper set of the vector D , so that it can be processed in the correct (possibly later) iteration. Otherwise, if the exact  X  -degree of v is available, the  X  -core number of v is set and the  X  -degrees (either the exact or the lower bounds) of all v  X  X  neighbours are updated.
The worst-case time complexity of E-( k ,  X  )-cores is the same as the basic ( k ,  X  )-cores algorithm, i.e., O ( m  X ). How-ever, smaller running times are expected in practice due to the lazy computation/updating of  X  -degrees in reduced ver-sions of the input graph.
In this section we report quantitative experiments on ef-ficiency and numerical stability of our ( k ,  X  )-cores and E-( k ,  X  )-cores algorithms (Sections 3 and 4). 3 For this task we use the following real-world uncertain graphs.
 Flickr ( www.flickr.com , | V | = 24 125, | E | = 300 836). We borrowed the dataset from [24], where the probability of an edge between two users is defined based on homophily , the principle that similar interests indicate social ties. Par-ticularly, [24] uses as a measure of homophily the Jaccard coefficient of the interest groups shared by the two users. DBLP ( www.informatik.uni-trier.de/~ley/db/ , | V | = 684 911, | E | = 2 284 991). The dataset was borrowed from [24, 15]. Two authors are connected if they co-authored at least once, and the probability on an edge expresses the fact that the collaboration has not happened by chance: the more the collaborations, the larger the probability. Pre-cisely, [24, 15] define the probability of each edge based on an exponential function to the number of collaborations. BioMine ( biomine.org , | V | = 1 008 200, | E | = 6 742 939). A snapshot of the database of the BioMine project [26] con-taining biological interactions. Edges inherently come with Table 1: Times (secs) of the proposed methods for com-probabilities. The probability of any edge provides evidenc e that the interaction actually exists.
 Efficiency. Table 1 reports on the running times exhibited by our ( k ,  X  )-cores (left) and E-( k ,  X  )-cores (right) algo-rithms on the selected datasets. Times are split by the main phases of computing initial  X  -degrees and running the main cycle. Both algorithms are very fast on Flickr and DBLP . They take on average around 20 and 60 seconds, respec-tively. On BioMine , which is much larger and denser, clearly the time increases. However, the time required by our al-gorithms on the latter dataset is in the order of one hour. This is reasonable for networks of such size and testifies the applicability of our methods to very large uncertain graphs.
As expected, E-( k ,  X  )-cores runs faster than the basic ( k ,  X  )-cores algorithm, allowing a reduction of the total time up to around 30% ( DBLP ,  X  = 0 . 1). The gain is more evident on the larger datasets (i.e., DBLP and BioMine ) and is generally increasing as  X  decreases. The latter finding is expected because the smaller  X  , the larger the  X  -degree of a vertex, and, thus, the better the chance for the lower-bound to be tighter and lead to better pruning. Larger  X  -degrees for smaller  X  is also the reason why times (for both phases and both algorithms) are increasing with smaller  X  . Numerical stability. As discussed in Section 3, proba-bilities may lead to numerical instability. To prevent this, one can exploit native solutions provided by modern pro-gramming languages to enlarge range and/or precision of the numerical representation. As a side effect, this would slow down the overall computation as larger precision im-plies slower arithmetic computations. Thus, the goal is to minimise the number of critical operations that may lead to numerical instability, to avoid using a too large preci-sion with the aim of achieving reasonable accuracy. As re-ported in Section 3, a major feature of the novel dynamic-programming method we employ in our algorithms to com-pute/update  X  -degrees is to alleviate such numerical issues. We next provide experimental evidence on this.

First, we report results by varying the precision used for representing numbers (we consider 32, 64, 128, and 256 bits Table 2: A ccuracy of ( k ,  X  )-core index for  X  = 0 w.r.t. de-as precision levels). 4 W e note that, for  X  = 0, the ( k ,  X  )-core decomposition of an uncertain graph G should ideally corre-spond to the core decomposition of the deterministic graph derived from G by ignoring probabilities. Thus, we measure accuracy by comparing, for each vertex, the 0-core number outputted by our algorithms with the core number returned by the standard k -core algorithm (Algorithm 1) on such a deterministic graph.

Tables 2 and 3 show accuracy results (in terms of per-vertex average absolute error and percentage of vertices with core number other than the exact one) and running times, respectively. We report times separately for ( k ,  X  )-cores and E-( k ,  X  )-cores , while accuracy is the same for both. As expected, larger precision leads to better accuracy and worse efficiency. Particularly, the results show a linear trend: doubling the precision, time doubles while errors get halved.
We also compare the results of our algorithms when equipped with the proposed dynamic-programming method to the results of our algorithms equipped with the method that computes/updates  X  -degrees using the formula in Equa-tion (5). We denote our proposed combination X  ( k ,  X  )-cores + dynamic-programming method X  simply as ( k ,  X  )-cores , while we refer to the  X  X aseline X  combination  X  ( k ,  X  )-cores + Equation (5)-based method X  as Eq 5. These results are summarised in Table 4 (precision 64 bits). Our method out-performs Eq 5 in terms of both average absolute error and percentage of vertices with non-zero error. Particularly, the average absolute error of the Eq 5 method is reduced by 9% ( Flickr ), 41% ( DBLP ), and 40% ( BioMine ).
The influence-maximization problem [16], has received a great deal of attention over the last decade. It requires to find a set of vertices S , with | S | = s , that maximizes the expected spread , i.e., the expected number of vertices that would be infected by a viral propagation started in S , under a certain probabilistic propagation model.

The independent cascade model [16] is a widely-used prop-agation model; under this model, the problem of finding a set S of s vertices that maximizes the expected spread  X  ( S ) is NP -hard. However, the submodularity of  X  ( S ) al-lows the Greedy algorithm that iteratively adds to S the vertex bringing the largest marginal gain to the objective function to achieve a (1  X  1 e ) approximation guarantee. Un-fortunately, finding the maximum-marginal-gain vertex re-quires to solve a #P -complete reliability problem. Hence, existing approaches usually apply sampling methods (e.g., Monte Carlo) to estimate the best seed vertex at each it-eration of the algorithm. This drastically affects the effi-Table 3: Times (secs) of the two proposed methods for Table 4: A ccuracy of the proposed method in terms of ciency of the algorithm, thus limiting its applicability onl y to moderately-sized networks (the time complexity of the al-gorithm is O ( sT nm ), where T is the number of Monte Carlo samples, with T  X  [1 000 , 10 000], usually). Optimizations of the basic algorithm have been defined which exploit the sub-modularity of  X  to avoid unneeded computations [12], but the improvement achieved is typically not enough to handle large graphs (in the experiment that we show below, on a moderately sized graph a state-of-the-art algorithm such as Celf++ [12] could not finish after several weeks).
Within this view, a useful application of our ( k,  X  )-core de-composition is to provide a way to speed-up the execution of the Greedy algorithm. The idea is simple: just reduce the input graph G by keeping only the inner-most  X  -shells and run the (optimized version of the) Greedy algorithm on such a reduced graph. The rationale here is that, as ex-perimentally observed in [17], the core decomposition of the deterministic version of G , is a direct indicator of the ex-pected spread of a vertex: the higher the core index is, the more likely the vertex is an influential spreader. The finding in [17] however exploits cores derived from a deterministic version of the input graph, thus completely ignoring its prob-abilistic nature. We conjecture that exploiting a notion of core decomposition defined ad-hoc for uncertain graphs can only positively affect the behaviour observed in [17]. We next empirically show the correctness of our conjecture. Experiments. We use a small directed graph from Twitter ( | V | = 21 882 , | E | = 372 005), and a set of propagations of URLs in the social graph, which we use as past evidence to learn the influence probabilities (we employ the traditional method described in [11] for this). Each edge ( u, v ) expresses the fact that v is a follower of u and the corresponding prob-ability provides evidence that an action performed by u will be performed by v as well.
 The objective here is to show that running the standard Greedy influence-maximization algorithm on a reduced ver-sion of the graph given by the inner-most ( k,  X  )-shells allows to achieve high-quality results while keeping the running time small. We test our method replacing the notion of de-Table 5: E xpected spread achieved by the proposed gree with out-degree (given that the graph is directed) and s etting  X  = 0 . 5. We obtain 8 cores and keep the three inner-most ( k,  X  )-shells. This gives a reduced graph with 2 064 vertices and 86 142 edges. We run the optimized version of the Greedy algorithm defined in [12], i.e., the Celf++ al-gorithm, on such a reduced graph and take the seed vertices S outputted as our result.

For accuracy evaluation, we compute the expected spread achieved by S on the whole graph (using Monte Carlo sam-pling with 10 000 samples). As criteria for comparison, we use the top-K vertices ranked according to the following baseline ranking functions: ( i ) maximum out-degree (ignor-ing probabilities, as suggested in the seminal work on influ-ence maximization [16]), ( ii ) maximum  X  -degree, ( iii ) maxi-mum expected degree (computed by summing the probabil-ities on the edges outgoing from a vertex), and ( iv ) vertices computed by running Celf++ on the graph reduced ac-cording to deterministic core decomposition (ignoring prob-abilities). Note that we could not use the results of the direct execution of Celf++ on the whole graph due to its excessive running time (it could not finish in several weeks).
The results reported in Table 5 (we vary | S | from 10 to 30) show how our ( k ,  X  )-cores -based method evidently out-performs all the baselines, allowing to increase the spread up to 590 ( out-degree ), 551 (  X  -degree ), 558 ( exp-degree ), and 436 ( k -cores ). As far as efficiency, we report runtimes in the order of 4 X 5 hours (with | S | = 30), which are times clearly affordable X  X ontrast to the unaffordable runtime of the direct execution of Celf++ on the whole graph.
In task-driven team formation we are given a collabora-tion graph G = ( V, E,  X  ), where vertices are individuals and edges are assigned a probabilistic topic model  X  , represent-ing (a distribution on) the topics exhibited by past collab-orations. The topic model can be produced by standard methods, such as the popular Latent Dirichlet Allocation (LDA) [6]. The input of LDA (or any other similar method) is ( i ) a number Z of topics, and ( ii ) for each edge ( u, v )  X  E , a document d ( u, v ) representing all the past collaborations between u and v . The document d ( u, v ) is a bag of terms coming from a finite vocabulary  X . The output is the topic model  X  , that is:  X  for each edge ( u, v )  X  E and each topic z  X  [1 ..Z ],  X  for each term t  X   X , a distribution over topics, i.e.,
A task-driven team-formation query is a pair h T, Q i , where T  X   X  is a set of terms describing a task, and Q  X  V is a set of vertices (possibly even a single vertex). The goal is to find an answer vertex set A , with Q  X  A , which is a good team to perform the task described by the terms in T . Being a good team means having a good affinity among the team members with respect to the given task. We report more formal details on this in the following.

The query task T , together with the topic model  X  , induce a single probability value p T ( u, v ) for each edge ( u, v )  X  E , such that p T ( u, v ) represents the likelihood that T has been generated by a collaboration between u and v :
Hence, given a task T , the input collaboration graph G yields an uncertain graph G T = ( V, E, p T ). This way, given G
T and a set of query vertices Q  X  V , the task of finding a good team for the query at hand directly translates into finding a subgraph of G T that represents a good community for Q . Formally, the goal is to find a connected subgraph H = ( V H , E H ) of G T that ( i ) contains all query vertices ( Q  X  V H ), and ( ii ) maximizes a notion of density. Particu-larly, as far as the density measure, the minimum degree has been widely recognized as a principled choice for this kind of problem. 5 We therefore rely on this notion of density and ask for the subgraph H to maximize the minimum  X  -degree of a vertex in H . The resulting problem statement is: Problem 2 (Task-Driven Team Formation).
 Given a collaboration graph G = ( V, E,  X  ) and a query h T, Q i , let G T be the uncertain graph derived from G and T as described in Equation (8). Given a threshold  X   X  [0 , 1] , we want to find a connected subgraph H = ( V H , E H ) of G induced by a set of vertices V H such that Exploiting ( k ,  X  ) -cores for team formation. We now show that Problem 2 can be optimally solved by resorting to our notion of ( k ,  X  )-core decomposition. This result is stated in the next theorem (we omit the proof for space reasons).
Theorem 6. Given an uncertain graph G T and a thresh-decomposition of G T (with C 0  X  C 1  X   X  X  X   X  C k  X  ), and, given a set of query vertices Q  X  V , let C  X  Q be the smallest-sized core in C such that every q  X  Q belongs to the same connected component of C  X  Q .

Then, the solution to Problem 2 is given by the connected component of C  X  Q that contains Q .
 Theorem 6 provides us with a principled way of solving Prob-l em 2. The solution can be summarized as follows: 1. Given a collaboration graph G = ( V, E,  X  ) and a task-2. Compute the ( k ,  X  )-core decomposition C of G T ; 3. Visit the cores in C s tarting from the smallest-sized 4. Return the connected component of C  X  Q containing Q Experiments. We consider task-driven team formation in the context of collaborations among computer-science re-searchers. We build a collaboration network from the DBLP database ( www.informatik.uni-trier.de/~ley/db/ ): ver-tices are authors and an edge connects two authors if they co-authored at least once. The resulting graph has | V | = 1 089 442 and | E | = 4 144 697. For each edge, we take the bag of words of the titles of all papers coauthored by the two authors (words are stemmed and stop-words are removed), and apply LDA to infer the topic model  X  (we set Z = 100).
In Table 6 we report the results of three task-driven team-formation queries. The first two queries share the query ver-tex H. V. Jagadish , but the first task is about gene expres-sion while the second one is about xml : as expected the two proposed teams are very different. The third query shares with the second one the vertex S. Muthukrishnan ; but, un-like the previous one that is about xml (a database topic), the third query is about auction models (an algorithm-theory topic): the different teams proposed correctly reflect the difference in the tasks. It is worth noticing that the extraction of these teams, following the process described above and exploiting our efficient ( k ,  X  )-core decomposition, takes approximately 2-3 seconds on a commodity laptop.
In this paper we extend the graph tool of core decom-position to the context of uncertain graphs. We define the ( k,  X  )-core concept, and we devise efficient algorithms for computing a ( k,  X  )-core decomposition. As a future work, we plan to investigate the relationship between ( k,  X  )-cores and other definitions of (probabilistic) dense subgraphs, so as to exploit the former as a speeding-up preprocessing.
