 1. Introduction the problem and the additional real-time constraints.

Data streams occur in many examples of practical interest, such as in sensor data generated by sensor networks, online messages that are placed in telecommunication networks, daily weather/traffic records, stock market data, or performance measurements in network traffic management and monitoring. The mining of flowing data offers new challenges, because data itemsets relate to groups of rainfall levels that are mostly occurring within a time period. Both cases will be used as domains of application as part of our experimentation.
 stream of transactions when a time horizon is given, specifically, when this horizon is smoothly defined by fuzzy sets or determined according to stream characteristics.
 new transactions are entering the frame of interest. Other approaches have been proposed in the literature. Although the literature contains some optimizations to Apriori, their logic is based on re-iterating the algorithm at each step. is to maintain a memory of itemset candidates that might become frequent and to update this list as new transactions are the algorithm to perform better than the other algorithms proposed in the literature. further enhancement to the performances, it expands the meaningfulness of the algorithm, enabling a higher expressivity. In improves the algorithm's scalability.

This paper is organized as follows: Section 2 introduces the definitions and the properties of frequent itemsets for the experimentation on datasets that have different characteristics, and Section 7 outlines the conclusions. 2. Preliminaries to as an itemset . More formally, let X ={ i 1 , i 2 , ... transactions over X . Each transaction (or record) r  X  D contains a subset of items, i.e., r provides its length . The transactions in D can entail different lengths. A subset of items I itemsets are defined with respect to the support threshold S ; as a result an itemset I is frequent if supp ( I )
Therefore,
In addition, it is useful to further classify an itemset X as being maximal frequent if X is frequent, but any Y
The problem of counting the number of distinct itemsets in a dataset, given an arbitrary support threshold, is NP-complete, To reduce the combinatorial search space, most of the algorithms exploit the following two properties: -Downward closure : all of the subsets of a frequent itemset are frequent -Anti -monotonocity : supersets of an infrequent itemset must be infrequent, too. 3. Related research 3.1. Mining of frequent itemsets
Traditionally, itemset mining is associated with the discovery of association rules [2,11] because they provide a subset of (i) seeking frequent itemsets; and (ii) discovering association rules among the found itemsets.
The first and most noticeable algorithm for mining frequent itemsets is known as Apriori , which was proposed independently by Agrawal and Srikant [3] and Mannila, Toivonen and Verkamo [12] and was later joined in [4]. Apriori is a levelwise, breadth-first, bottom-up algorithm, as outlined by Algorithm 1. and are placed in G k +1 . The generation of candidates is performed by joining L itemsets I 1 , I 2  X  L k , which share k -1 items in common. The resulting itemset I = I
G
Next, their support is computed by scanning the dataset and discarding those itemsets whose support is below the minimal the largest transaction. The result of each step is the list of frequent itemsets L
The algorithm stops when L k +1 is empty or when k = ml . The algorithm output is obtained by merging lists of frequent properties.
 several enhancements have been proposed to overcome this limitation. Among them, we note Partition , which was proposed by the set of all potentially frequent itemsets. In the second pass, the overall support of each itemset is computed. Other contributions in this area are found in [14  X  22]. 3.2. Frequent itemset mining in data streams
The main challenge in mining frequent itemsets in data streams is to enumerate them at a rate that is compatible with the speed at which the transactions are presented. This goal requires algorithms with in-memory data structures and a minimal (ii) damped -and (iii) sliding -window based mining.

In landmark algorithms, itemsets are counted on the transactions between a specific timestamp, the landmark, and the present. Thus, in landmark algorithms, records are ongoing in the frame of interest.
Li, Lee and Shan investigated the problem of mining all frequent itemsets [24] and maximal frequent itemsets [25] in a pruning those elements that result in infrequent itemsets. Thus, all frequent itemsets that are embedded by the transaction and a time-efficient solution based on one-pass support counting.
 are deleted. The pruning of infrequent sub-trees is optimized.

The mining of frequent itemsets is usually faced by looking for an exact answer to the question of what are the most often in data streaming.

Manku and Motwani [28] developed two single-pass algorithms, namely Sticky Sampling and Lossy Counting , for mining items (and itemsets) in a landmark window. Sticky Sampling provides an approximation of the resulting elements, linking the grouping elements into buckets of a given size and support. To apply Lossy Counting to the mining of frequent itemsets, the authors proposed TRIE, a lattice-based in-memory data structure, which is used to store itemsets and approximate supports.
In contrast to landmark models in which all of the itemsets are treated in the same way in spite of how recent they are, in itemset support computation.
 new itemset that has a high possibility of becoming frequent in the near future.

Lee, Lin and Chen [30] proposed a sliding-window filtering (SWF) algorithm. The sliding window is made of a sequence of the frequent itemsets, which is the main drawback of this approach.

The problem of considering efficient in-memory data structures have been considered by other authors. Chang and Lee [31] within a transaction-sensitive sliding window. These authors adopt a prefix-tree-based data structure, which is called the infrequent gateway nodes, unpromising gateway nodes, intermediate nodes and closed nodes. This information is used by
Moment to traverse the CET structure when a new transaction comes into the frame of interest but also when the oldest transaction leaves the frame. Therefore, traversing the CET can be time consuming.

More recently, Li and Lee [23] proposed two one-pass algorithms, MFI -TransSW and MFI -TimeSW ; the first algorithm is first phase, called window initialization , is composed of transforming each item of the new incoming transaction into its frequent itemsets of the previous step, in a fashion similar to Apriori. 4. Frequent itemsets within a limited window elements because of new records that are added; however, the oldest are not discarded.
De fi nition 1. Limited window A limited window has a finite lower bound.  X  in and out at each iteration. Moving one slot forward, a group of transactions enters the scope of the window while another leaves. For the sake of simplicity, we will assume that the number of elements considered within the window does not change over time. Thus, the number of records that flow into the window is equal to the number of records that flow out.
The number of elements considered within a window represents the memory capacity that is offered by the window. Limited windows have a limited capacity. Unlimited windows might entail a capacity that goes on ad infinitum. This case involves windows whereas each element is considered to have the same relevance as the others.
However, in practice, elements can be accounted for by a different degree of interest, as is accomplished by some of the algorithms illustrated earlier. This approach leads the window to fade at its edges, entailing a smooth window depicted in Fig. 2 .

The sum of the degree of interest  X  ( t i ) by which each time-slot t logic, provides the window memory capacity C , which is defined as
Obviously, windows can be continuous or discrete in time, and causal (if all of the elements refer back to past events) or anti -causal . For the intent of this paper, we will consider only time-discrete causal windows.
Convexity is an important characteristic of smooth windows. A smooth window is convex if the function that describes the interval. Formally, we provide the following definition:
De fi nition 2. Convexity
A smooth window W is convex iff
Smooth windows can be unbound but still have a limited capacity. This circumstance is possible when assuming that the Formally, we have the following definition:
De fi nition 3. Asymptotically limited window words, iff
In this paper, we are interested in limited windows. The memory capacity of a given window offers an upper bound to the minimum support threshold S that is used to select the frequent itemsets. Indeed, we need S necessary condition helps to test if an itemset I is possibly frequent or not.
Proposition 1. Given a window W and a minimum support threshold S , such that S must necessarily appear in a transaction taken among a subset of points W Proof. The proof is straightforward. Indeed, if I does not appear in W which contradicts the hypothesis that I is frequent.  X  C ( W ), although the itemset support count should be approximated by the limit.
 there is a support threshold S = 8, then we need a test window such that C ( W choose a window that is composed of consecutive elements within the interval [2
The efficiency of the test depends on the number of transactions to inspect in order to find a given itemset. This number becomes smaller by increasing the minimum support threshold S , because fewer transactions are required to overcome the quantity C ( W )  X  S . However, given the value of S the following criterion assures a minimal number of transactions. Proposition 2. Given the windows W and a support threshold S , the window subset W elements w.r.t. the degree of interest  X  has the minimal number of elements that can overcome the quantity C ( W )
Proof. Let ( X ) be a decreasing ordered permutation of the elements t  X  ( t
Let us assume that W S is not the subset that has the minimal number of transactions that can cover the quantity C ( W ) there is at least another W S that is composed of fewer elements, with no more than m -1 elements. However, because W made of the m  X  1 elements that have the largest degree, we get
Thus, we are not able to overcome the quantity C ( W )  X  S as required.
If W is convex, then the optimality criterion given by Proposition 2 suggests taking an element with the maximum degree of interest and moving to the left and right side of it, until the quantity C ( W ) windows, because the larger number of transactions should be accounted for to cover the quantity C ( W ) enlarges the number of candidates to consider. This approach is described by the following example. can select any W S such that C ( W S ) N 4. A first choice could be to use the window tail W smaller set of candidates. Indeed, if we choose W S  X  [1  X  not frequent.

The variant Apriori with Window Test (AWT), which is outlined by Algorithm 2, accounts for the window test, as suggested by Propositions 1 and 2. In this case, the mining of frequent itemsets is also driven by both conditions (i) online mining algorithm, as shown in the following section.

A smooth window offers additional possibilities compared with a sharp window. On the qualitative side, a smooth window leads to shaping a fuzzy set over the window, where the membership degree reflects the importance of the slot when counting occurrences. We call this property interpretability .

Besides this property, on the quantitative side, a smooth window allows us to reduce the width of the optimal test window, given the same time horizon. Indeed, we recall that the test window W threshold S , C ( W S ) is smaller. We call this property fitness .
 50  X  S , but a testing window that is much smaller.

Several methods can be employed to build a smooth window that fits characteristics of the data stream. This problem would leaving further research to provide more depth on this issue.

The most immediate approach is to assign a degree of relevance to each slot on a subjective basis. For example, it would be possible to consider what are the minimal and maximal horizons that we are interested in and to move from the first to the second linearly. This approach will lead to shape a trapezoidal window as shown in some of the examples above. 1. Choose a collection of itemsets to which we are interested; 2. Observe the stream for a period of time in such a way that we are able to characterize it statistically; 3. Define the window width that can assure events with a given probability; 5. Assign at each length l a degree of relevance decreasing by p be independent (memorylessness), and governed by Poisson distribution where N ( t +  X  )  X  N ( t )= k is the number of events in the time interval ( t , t + k denotes their number of occurrences.

With respect to step 5, the degree of relevance decreases by a probability because the remaining part becomes less and less choosing the maximum degree for each slot, in order to account for the characterization that is provided by each itemset.
The above description is only one example of a possible method to shape the window. Other methods are possible and are based on different criteria. For example, they could attempt to capture when the itemsets tend to appear and when they disappear, and to shape the window accordingly. 5. Online frequent itemsets update records flow in, some itemsets become more frequent while others disappear.

C which reaches, in some cases, the whole set of itemsets which is composed of 2 However, this issue is outside of the focus of this work, and will not be considered further here. For each itemset I , WIS makes use of an occurrence vector v (i.e., Boolean). To identify those itemsets that are quitting the testing region W last ( I ) which is the last position recorded for I within W first position; the result is that, when last ( I ) reaches the W
An example of how WIS works is provided in Fig. 8 . For the sake of simplicity, we consider only a regular flow of 1 record entering W and 1 record leaving W at each iteration. When a cluster of incoming records R flows into W , another group R records flows into W S , which makes new candidates appear while those leaving W test window W S while AC leaves W S . This event produces a right -shift of the occurrence vector v example, BC becomes a candidate in Fig. 8 . For those itemsets that are not yet included in C , the occurrence vector v by looking at the current records in W . For the other itemsets, the number of times I occurs in R is stored in v easily computed from v I as:
Those itemsets I  X  C such that supp ( I )  X  S are frequent and are recorded in the corresponding F . 6. Experimental results
To assess WIS performances and behavior, we performed a set of experiments that were aimed at testing WIS, under different transactions and other transaction-sensitive sliding window solutions, in particular MFI [23] and SWF [30].
The algorithms were all implemented in Java 1.5, and experiments were conducted on an Intel Core Duo 2.53 machine with 4 GB of RAM running on a Mac OS X 10.6.4 Snow Leopard.
 particular trapezoidal, triangular and sharp.

For each algorithm, it is possible to distinguish two phases: a window initialization phase , in which algorithms mine the frequent itemsets in the initial window, and a window sliding phase , in which the algorithms mine frequent itemsets while window phase because the initialization window phase does not have great interest in a stream of data. 6.1. Comparison to the Apriori approach
Specifically, for datasets that had a large number of items, we chose T20I6D100K from the PUMS (Public Use Microdata Sample) sample, which has 192 different items. In addition, we randomly generated a threshold (10%). We observe that exceeding the threshold of 50% would not make sense for smooth windows, because this value is the upper limit for a triangular window capacity, i.e., C ( W ) values: when the stream moves 1 slot per time, and when it moves ahead the 10% of the window size. Experimental results are reported in Table 1 .
 data structures used, because WIS keeps an occurrence vector v support threshold. Indeed, by reducing the support threshold, we enlarge the size of the test window W recomputing an exponentially larger set of frequent itemsets exceeds the cost that is paid by WIS in shifting v new candidate itemsets that will be at most 2 ns , where n it processes nf l  X  1 nf l  X  1 for experiments reported in Tables 3 and 5 . 6.2. Comparison to the sliding-window algorithms
Among the different options that are presented in Section 3 , we adopted an implementation of MFI and SWF, according to the pseudo-code provided, respectively, in [23] and [30] . Specifically, we considered MFI-TransSW because this code is the transaction-sensitive variant of MFI.
 In this case, we considered only WIS with a sharp horizon, to obtain the same output of MFI and SWF.
Specifically, we simulated two streams: the first stream has a single incoming transaction while the second stream has an incoming slide of 10% with respect to the initial window. 6.3. Comparison with real datasets
We tested WIS versus Apriori, AWT, MFI and SWF on the two real datasets mentioned as motivating examples in the introduction. The first stores the rainfall data that were collected monthly from January 1895 until December 2011. considered data on a 9-point grid that is centered near the Boston metropolitan area (Lat.:42.50  X  0.50; Lon.: of these windows, we still considered two support thresholds, namely 10% and 50%. For the size of the incoming slide, we of the algorithm in terms of the k -itemsets that were discovered.

The main advantage of WIS relies on retaining a memory of the candidates that flow across the window. Table 4 shows how number of itemset variations during the whole execution. The minimum value has always been equal to zero and, thus, is not possible to update the list of frequent item sets efficiently.
 For the second benchmark, we chose MSNBC.com Anonymous Web Data dataset, which is available at the UCI Machine The results are reported in Table 5 .

We also profiled the WIS execution time at each step compared to Apriori, AWT, MFI and SWF. In this case, we also recorded with a support threshold set at 10%. The execution profiles are reported in Table 6 .
From all of the examples provided above, we can observe that WIS performed faster than MFI and SWF but had a slightly test window. In addition, WIS does not require a pass through the dataset to compute the support. 7. Conclusions
In this paper, we considered the problem of mining frequent itemsets in a stream of transactions within a limited window. A variable degree of interest can be expressed on each slot of the window, which usually decreases over time. The traditional even if most of them stay frequent when the window moves ahead. We proposed the Window Itemset Shift (WIS) as an structures, which avoid re-exploring the entire itemset lattice.

References
