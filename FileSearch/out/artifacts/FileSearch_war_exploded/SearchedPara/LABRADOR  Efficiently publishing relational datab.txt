 1. Introduction
A vast amount of valuable information, produced and consumed by people and institutions, is currently ness operations. For many purposes, there is an ever increasing demand for having these databases published Levy, &amp; Mendelzon, 1998 ).

The most common solution for publishing Web databases is to develop a customized Web application that usually presents many search forms containing search text boxes, one for each attribute of the underlying complex and (2) the development of such a kind of application is time consuming and costly. On the other hand, Web users are mostly familiar with the simple keyword-based interfaces provided by Web search of common Web search interfaces.
 In this paper, we present LABRADOR, a system for efficiently publishing relational databases on the
Web by using a simple text box query interface. LABRADOR was designed to publish databases without requiring any additional programming effort or database schema modification (e.g., creation of additional tables or triggers). To achieve this, the system takes unstructured keyword-based queries posed by users and automatically derives equivalent SQL queries that fit the user information needs expressed by the ori-ginal queries.
 in the underlying database schema, thus producing a set of candidate SQL queries. As the number of potential culate a score value that expresses the likelihood of each candidate SQL query corresponding to the original unstructured query, thus generating a ranking. Therefore, only the highest scored SQL queries would be con-sidered to be processed. The system can automatically submit the structured query placed on the top position ranks the query results by using a second Bayesian network model that evaluates the likelihood of a query result satisfying the original unstructured query.

Ribeiro-Neto, 2002 ), we have used a similar technique to structure queries in order to be issued over Web interfaces without specific concerns on query processing issues. In the case of LABRADOR, we assume that the structured query processing is performed by a relational DBMS. Thus, we must cope with this scenario to
Bhalotia, Nakhe, Hulgeri, Chakrabarti, &amp; Sudarshan, 2002; Hristidis, Gravano, &amp; Papakonstantinou, way with an insignificant query processing overhead.

Experiments performed indicate that the proposed model is able to rank the correct structured queries in the first position of the ranking in more than 75% of the cases and that the correct structured query is ranked among the first four positions for all queries. Yet, the overhead produced by the LABRA-
DOR system when processing real queries is, on average, smaller than 6% of the whole query processing time.

This paper is organized as follows. In Section 2 , we present an overview of our approach. Section 3 presents our method for query structuring, including the generation of candidate structured queries and the ranking of these queries according to a Bayesian network model. In Section 4 , we describe our method for ranking the results of the submitted queries, which is also based on a Bayesian network model. In Section 5 , we discuss implementation details of the LABRADOR system. Experimental results are presented in Section 6 . Section 7 discusses the related work. Finally, conclusions are presented in
Section 8 . 2. Overview 2.1. Basic concepts and assumptions terms . Let W i be the set of all terms in the database that compose the values of an attribute A unstructured query , denoted by U ={ t 1 , t 2 , ... , t k form Q ={  X  A 1 : v 1  X  , ... ,  X  A m : v m  X  }, m P 1, where each A v in U to an attribute A j defined in the database schema. We notice that a same attribute is allowed to occur many times in a same structured query, but each term must appear just once.

An example of an unstructured query would be U = { X  X  Agatha  X  X , X  X  Christie  X  X , X  X  Murder  X  X  X . For a database containing attributes Author and Title , a possible structured query derived from U would be SQL query that corresponds to Q is the following:
S 1 : SELECT * FROM Books WHERE Author LIKE  X %agatha% X 
In Query S 1 the attributes Author and Title occur in the same table Books . If these attributes occurred in two distinct tables, say Authors and Books , the following SQL query would correspond to Q :
S 2 : SELECT * FROM Authors NATURAL JOIN Books
Moreover, instead of LIKE , we could have used a full-text search operator such as CONTAINS , which is available in most DBMS such as Oracle, SQL Server, PostgreSQL and MySQL.

When constructing structured queries, we must ensure that the structured query results are as general as of the query produced. For instance, common real-case Web database applications would use specific forms
S 3 : SELECT * FROM Books WHERE Author LIKE  X %agatha christie% X 
S 4 : SELECT * FROM Books WHERE Author =  X  X gatha christie X 
An important consequence of the semantics assumed for our structured queries is that the number of results 2.2. Query processing Processing unstructured keyword-based queries in LABRADOR involves four basic steps, as illustrated in candidate queries is submitted to be processed by a target DBMS (this query can be automatically selected by submitted query and ranks them according to their estimated relevance to the user.
To illustrate the LABRADOR system functionality, Fig. 2 presents an exemplary query interface built any other interface can be built upon the LABRADOR system to fulfill specific application requirement, as the developer could present them as hyperlinks to be selected by the user.

In Step 2, LABRADOR derives candidate structured queries from the unstructured user query. These que-ries are ranked according to the probability that they have of representing the best assignment between the terms provided by the user and the database attributes defined in the schema. This process is called query structuring . Formally, LABRADOR would generate all possible pairs  X  attribute , query term  X  for all terms in the unstructured query. For instance, consider an unstructured query U ={ t database, t 1 occurs as a term for the values of the attributes Abstract and Title , and t for values of the attribute Author . The possible attribute-term pairs for t tle : t 1  X  . Likewise, there is only one possible attribute-term pair for t structured queries, the system combines all attribute-term pairs observing that no term should appear more than once in a query. In this example, the following structured queries would be generated: Q stract : t 1  X  ,  X  Author : t 2  X  } and Q 2 ={  X  Title : t
In Section 3 , we discuss in detail how candidate structured queries are generated from a set of keywords forming an unstructured query. 2.3. Publishing databases through LABRADOR
To allow users to process their unstructured queries, LABRADOR requires a number of index structures to database portions to be available for querying. Since the only requirement for both publishing and querying a database is having read-only access to it, not only tables but also views can be published through LABRA-
DOR. Furthermore, several distinct databases can be published and made available for querying through a single interface with no programming effort. 3. Query structuring
In this section, we discuss the query structuring process, a crucial issue in the LABRADOR system. First, a user. Next, we present the Bayesian network model we use to rank structured queries according to the like-lihood of representing the unstructured query. 3.1. Structured query generation possible.

Let t i be a term in the unstructured query, and let A  X  t occurs. Formally, to guarantee the first condition, we would have to generate the Cartesian product over assignments during the generation of plausible structured queries.

For instance, consider the unstructured U ={ t 1 , t 2 }, where t
A , i.e., A  X  t 1  X  X f A 1 ; A 3 g ; and t 2 occurs for the values of the attributes A
Cartesian product A  X  t 1  X  A  X  t 2  X  is [{ A 1 , A 2 }, { A queries that satisfy the first condition are {  X  A 1 : t 1 {  X  A 3 : t 1  X  ,  X  A 4 : t 2  X  }. Still, consider that A 1 and A represented by {  X  A 3 : t 1  X  ,  X  A 2 : t 2  X  }, {  X  A 1 set, since they do not satisfy the second condition.

The structured query generation is accomplished by Algorithm SQGen , shown in Fig. 1 , which we explain next.
 attribute-term pairs (ATP) of the form  X  A j , t i  X  . These pairs associate each term t the database, if t i occurs as a term for values of A j . The set of attributes for which t function TermOccurrence in Line 7, thus guaranteeing that condition (1) is satisfied.
The generated ATPs are combined to produce the candidate queries, but not all combinations of ATPs pro-adding a newly generated ATP satisfies condition (2), before adding it to the set of structured queries.
We notice that a naive implementation for the function TermOccurrence would require running an SQL query using the  X  X  X IKE X  X  operator, or the  X  X  X UBSTRING X  X  function, over all attributes on the database, for such information internally stored in the LABRADOR system.

When Algorithm SQGen terminates, the set SQ contains a set of plausible structured queries derived from original unstructured query. This likelihood is estimated by means of a Bayesian network model and serves as 3.2. Ranking structured queries the SQGen algorithm in Fig. 1 , we want to associate to each Q representing the user intention when formulating U . The score of each query Q ability of all attribute-term assignments described in Q i bilities by examining the current state of the database. The computed scores are then used to rank the process is called structured query ranking .
In this process, we model structured queries, terms and database attributes as nodes in a Bayesian network schema: Books(Author,Title) .In Fig. 3 , we present an example of a Bayesian network model for ranking expanded for any other database schema or unstructured query. Although the discussion below refers to a would consider the node T as representing the natural join operation on these tables.
In a Bayesian network, each node represents a piece of information. In Fig. 3 , node A bute Author and node A 2 represents the attribute Title . Q component of Q i that involves the attribute A j . Each node a bute A i stored in the database and a i ! is a vector containing all terms found in the values of A resents the table Books .
 To illustrate these nodes and their roles, consider the following example. Given the structured query
Q ={  X  Author : X  X  Agatha  X  X   X  ,  X  Title : X  X  Murder  X  X   X  }, the node Q the attribute Author ,  X  Author : X  X  Agatha  X  X   X  , and Q 22  X 
Title : X  X  Murder  X  X   X  , where the terms  X  X  Agatha  X  X  and  X  X  Murder  X  X  are represented by the nodes a that the corresponding information will be considered for the ranking computation. In this case, we say that the information was observed .
 Analyzing the Bayesian network in Fig. 3 , we can derive the following general equation: where a is a normalizing constant.

The probability P  X  Q i j a 1 ! ; a 2 !  X  of observing the structured query Q expanded as follows:
In Eq. (2) , the term P ( Q i j Q i 1 , Q i 2 ) corresponds to the probability of observing Q nodes Q i 1 and Q i 2 . As we consider that the query Q i also active, this translates to the following equation:
Still in Eq. (2) , the terms P  X  Q i 1 j a 1 !  X  and P  X  Q
Q i 1 and Q i 2 given the state of the attributes A 1 and A match the terms in the query component nodes Q ij . This translates to the following equation: where g k  X  a i !  X  gives the value of the k th component of the vector a
The sum in Eq. (1) takes into account all sets of possible active terms in a active terms exactly match the query terms referring to attributes A we can simplify Eq. (1) to: where the active terms in a 1 ! and a 2 ! are exactly those present in Q
P  X  T j a 1 ! ; a 2 !  X  using the probabilities P ( T j A a ! and a where a 1 ! and a 2 ! are the states where only the query terms in Q are active.
 We can now use the properties of the structured queries to solve the conditional probabilities appearing in
Eq. (6) , namely P  X  A i j a i !  X  , P  X  a i !  X  and P ( T j A any particular set of terms, is defined as a constant: where n i is the total number of distinct terms that occur for values of the attribute A bilities are computed considering two alternative models. The first, which we refer to as TF X  X AF, adapts the detail each of these alternatives. 3.2.1. The TF X  X AF model
In TF X  X AF, the cosine measure is used to estimate the probability of observing the attribute A base, given the terms indicated by a i ! . The probability of observing A between a vector A i represents the values for attribute A i in the query, i.e., quency ( TF ) and inverse attribute frequency ( IAF ).
 where it occurs. This is computed by the following formula: where f ki is the number of occurrences of term k in the attribute A that occurs in the values of attribute A i .
IAF is an adaptation of the concept of inverse document frequency (IDF) found in the context of informa-values of the attributes according to the following formula: where n a is the total number of attributes in the database and a term k occurs. We use IAF as an estimation of the degree of ambiguity of the term with respect to the attri-ter is considered ambiguous, since it occurs for both attributes Author and Title . On the other hand, the term Halloween is typical to the Title attribute, and thus it is unambiguous.

The weights according to the TF X  X AF model are computed by the following formula: where f ki is the number of occurrences of the term k in the attribute A that occur in the values of the attribute A i , n a is the total number of attributes in the database and a number of attributes in whose values the term k occurs. 3.2.2. The AF model
For each attribute-term pair in the structured query, the AF model attempts to estimate how typical the bute-pair fitness . We say that a term t k is typical to an attribute A in the values of the attribute A i than other terms , and (2) the term t where f ki is the number of occurrences of term k in the values of the attribute A term that has more occurrences in the values of the attribute A the term k in database.

The probability of observing the attribute A i in the database, given the terms indicated by a by taking into account the fitness between the active terms in a
AF, to estimate the probabilities P  X  A i j a i !  X  . 3.2.3. Final ranking equation are observed. Thus, we define this probability as follows: where j T j is the number of tables involved in the query.

Once all conditional probabilities are defined, we can derive the final equation that represents the proba-bility of observing Q i given the current state of table T . As we experiment with two distincts models, TF X 
IAF and AF, to compute the conditional probability P  X  A i Eq. (6) . For the TF X  X AF model, we have that and for the AF model, we have that where a 1 ! and a 2 ! are the states where only the query terms referring to attributes A active, j T j is the number of tables involved in the query and g accounts for the constants a , P  X  a
Eqs. (15) or (16) are applied to each candidate structured query and return a probabilistic score for each one. To complete the ranking process, these queries are sorted by their score. 4. Ranking query results potentially high. To save users from having to browse the whole set of answers to find the tuples that match
To build such a ranking, we assign a score value to each tuple returned by the DBMS. In our work, we mitted to the database. This is computed using a second Bayesian network model, which we now describe.
For the discussion that follows, consider the Bayesian network model presented in Fig. 5 , which was built to our example database. In the network of Fig. 5 , node Q system. Nodes Q 21 and Q 22 represent the components of the structured query Q
A , respectively. Each node a ij represents a term in a i ! a tuple t j . Finally, each node t j represents a returned tuple.

A tuple t j is ranked according to the probability of satisfying the structured query Q observing t j , given that Q 2 was observed, P ( t j j Q 2 where a is a normalizing constant.

The probability P  X  Q 2 j a 1 ! ; a 2 !  X  of observing the structured query Q expanded as follows:
As in Section 3.2 , the terms P ( Q 2 j Q 21 , Q 22 ), P  X  Q where g k  X  a i !  X  indicates the value of the k th component of the vector a equation guarantees that only the state where the active terms are those of the query Q will be taken into consideration.

The sum in Eq. (17) takes into account all sets of possible active terms in a active terms exactly match the query terms referring to attributes A we can simplify Eq. (17) to:
For P  X  t j j a 1 ! ; a 2 !  X  , we want the probability of observing the tuple t attributes increases. Thus, we define: These definitions, and the fact that a 1 ! and a 2 ! are independent, allow us to rewrite Eq. (17) as
The probability of observing the value A i given that the terms indicated by a fined using the cosine measure. Thus, the probability of A where A ij where f ki is the number of occurrences of the term k in values of the attribute A set.

Again, since there is no preference for any particular set of terms, the probability P  X  a constant: where n i is the total number of distinct terms in a i ! .
 and the terms for the respective attributes in the structured query. This translates to the following final equation:
Ordering the computed scores, the tuples can be presented to the user as a ranked set of answers. 5. The LABRADOR system
In this section we present details on the implementation of the LABRADOR system. We begin by discuss-
LABRADOR engine on the top of a relational DBMS. In Section 5.2 we present the index structure that allows structured query generation and ranking to be performed efficiently. Finally, the generation of SQL statements to be submitted is shown in Section 5.3 . 5.1. Architecture
The LABRADOR system is composed of two main modules, as illustrated in Fig. 6 : the LABRADOR engine and the LABRADOR API. The engine implements the main system features, while the API provides an interface that connects any Web application to the engine. Socket communication is used between the engine and the API. The engine uses threads for query processing, so it can submit queries to multiple DBMS and simultaneously receive queries from multiple Web applications.

In the proposed architecture, Web application developers can build specific search interfaces by calling engine functions through the API. Once the Web application call the search function, the API submits the search interface using any ordinary Web browser.

The LABRADOR engine is a daemon that acts as a tier to allow keyword-based searching for the Web application. It implements the following tasks: query structuring, which includes the generation and ranking of the structured queries, DBMS interfacing, and query results ranking.

The main requirements for DBMS interfacing in LABRADOR are portability and efficiency. For the latter, using native connectivity facilities provided by the DBMS vendors would be the best choice. Unfortunately,
Connectivity), allowing the LABRADOR system to interface with any DBMS that provides an ODBC driver. 5.2. Index structure
To allow a feasible implementation of the query structuring procedure described in Section 3 , an indexing in the database is needed to compute the function TermOccurrence in the SQLgen algorithm in Fig. 1 and the
Inverted files maintain a vocabulary , which is composed of the set of all terms found in the collection of defines how specific this occurrence is. Our approach adopts an attribute granularity that indicates simply in which attributes the term occurs.

Fig. 7 illustrates the index used by the LABRADOR engine. For each term in the vocabulary, there is a list and frequency indicates how many times the term was found in the values of the attribute. The vocabulary is in main memory.
 attributes as being alphanumeric. Although this is not strictly true, using other data types would require applying specific similarity functions, which are out of the scope of our current work. Notice that building it is not necessary to modify any database instance or its schema.

The process of building the inverted index follows the following four main steps, for each table to be published: (1) Retrieve all tuples from the table, selecting only the attributes chosen to be published. (3) For each extracted term, verify whether or not the term exists in the vocabulary. If the term does not very efficiently. This is crucial to guarantee a minimal overhead when processing queries through the LABRA-
DOR engine. 5.3. Generating the SQL queries
Once a structured query has been selected to be processed, the system has to generate a corresponding SQL statement to the original query, before submitting it to the DMBS. For instance, given the structured query Q ={  X  Author : X  X  Agatha  X  X   X  ,  X  Title : X  X  Murder  X  X   X  }, the SQL query in Fig. 8 could be generated.
The SQL query in Fig. 8 uses the  X  X  X IKE X  X  operator to seek for terms within an attribute value. This may cause a serious performance problem when querying a large database, since the DBMS must examine each One alternative to avoid this problem is to use full-text indexing facilities provided by major commercial
DBMS, like Oracle, SQL Server, PostgreSQL and MySQL. These facilities allow the DBMS to seek a term within an attribute value by just looking in an index. In Fig. 9 we present an example of an SQL query that uses a full-text search function provided by Oracle. In LABRADOR, we use this alternative, taking advantage of the full-text index, whenever it is available for the target attribute. 6. Experiments
In this section, we present the results of experiments carried out with the LABRADOR system on real case databases. The experiments consist in submitting unstructured queries to the LABRADOR system backed up ries to the underlying DBMS. Second, the impact caused by the system on the time taken to see the query results. By mean of these experiments, we ultimately seek to corroborate our claims that the system is able to provide useful and meaningful answers for possibly vague keyword-based queries with a minimal process-ing time overhead. 6.1. Experimental setup The LABRADOR system was run over a server machine (Pentium IV, 3.0 GHz, 4 GB RAM) running
Linux and the Apache Web server. The two DBMS used, Oracle and MySQL, were run on a separated de-dicated machine (Pentium IV, 1.7 GHz, 2 GB RAM, non-RAID SCSI Hard Disks) running Linux.
For the experiments, we have used three real databases along with their query logs: Purchase , University and Library . The Purchase database contains records about purchase requests, including product description, sors and courses. The Library database contains records about books, including author names, titles and subjects.

The experimental databases were chosen in order to evaluate the system X  X  behavior when publishing dat-abases that present ambuiguity among the domains of their attributes. This is a very common feature in real cases and it is potentially problematic to our approach. We say that two or more attributes are ambiguous
Title and Subject from the Library database and the attribute Course from the University database. We domains in the database schema, or multiple meanings for a same term in the  X  X  X eal world X  X . Furthermore, we consider another kind of ambuigity that we call error-bound ambiguity which occurs when the data is imprecisely entered into the attribute values. For instance,  X  X  X gatha Christie X  X  and  X  X  X urder on the Orient bute Title might have been entered as  X  X  X urder on the Orient Express (Agatha Christie) X  X . In particular, all databases present natural ambiguity among their attributes Student Name , Professor
Name , Author and Requisitioner Name ; and the set of attributes Course , Title and Subject .In addition, the Library database presents error-bound ambiguity for several of its attributes. These features of the ranking, since undesirable terms might occur in the results of the queries targeted to databases with error-bound ambiguity.

In Table 1 we present the number of published tables in each of these databases, along with the correspond-iments and the DBMS used.
 The queries used for the experiments were randomly extracted from the actual query logs of each database.
For each extracted structured query Q i we have generated an unstructured query U  X  X  Murder  X  X  X .

By examining the query logs, we have found that the number of terms in the unstructured queries is 1.56 on average and the largest number of terms is 4. This confirms our expectation that typical user queries indeed contain very few terms. 6.2. Evaluation criteria
To evaluate the quality of the query structuring algorithm and the query result ranking, two criteria were structured query ranking to the correct structured query. We accomplished this by submitting each query U the system and analyzing whether the query structuring mechanism was able to place the correct structured available while the queries were submitted.

With respect to the Quality of Returned Tuples criterion, we evaluate if the returned tuples are relevant regarding the user query. This is accomplished using the well-known information retrieval measures precision be submitted. Assume that Q was generated from an unstructured query U which was derived from an SQL tics. Nevertheless, the experiments show that the results from both queries are quite similar. of results as we descend along the ranking. 6.3. Quality experiments We begin this section by presenting the results of the query structuring evaluation using the TF X  X AF and
Table 2 presents the results for the query correctness experiment. The line labeled  X  X  X op 1 X  X  shows the per-lines show the percentage of times the correct query was present in the  X  X  X op i  X  X  ranking positions. For instance,  X  X  X op 2 X  X  accounts for the correct query appearing in the first or second position, and so on.
It can be observed that the AF model was able to rank the correct structured query in the first position in more than 75% of the cases. This suggests that automatically submitting the top ranked structured query already provides very good results. Even if human intervention were required, the user would never have to look beyond the top four ranked queries, since the TF X  X AF and AF models were always able to place the correct structured queries among the top four positions. This shows the feasibility of having the query to be submitted chosen by the user.

To evaluate the ranking of the results from the structured queries submitted to the DBMS, two scenarios ically submitted the first ranked query. For this experiment, we have used the query ranking obtained by the ent average precision X  X ecall curves for the top ranked queries and for the user-chosen queries. may be indeed useful.
 that the LABRADOR system retrieves results as good as a customized Web application would retrieve, even for databases presenting error-bound ambiguity between some of their attributes.

Fig. 10 , yielding P -values below 0.01, 0.02 and 0.06 for the Library, Purchase and University databases, respectively.
 6.4. Performance experiments
In the performance experiments, we aim at verifying the processing time overhead due to our keyword-based search system over the overall query processing time. These experiments consist in measuring the time queries. 6.4.1. Time to process unstructured queries Table 3 presents the performance results for the query structuring task using the TF X  X AF and AF models.
This table also presents the distribution of the queries extracted from the database log according to the number of query terms. On average, the query structuring task using the AF model was processed in 0.81 ms, the highest time being 15.3 ms. We note that both models have presented very similar processing times for all queries. Despite the exponential growth of the processing time as the number of query terms ations, since user queries commonly contain only a few terms. 6.4.2. Time to process structured queries
We now present a second set of experiments performed to evaluate the time taken to process the results of a also summed up the time spent with several other miscellaneous tasks such as socket communication, database connection, etc., calling them others .
 In the quality experiments presented in Section 6.3 , all queries retrieved only a few hundreds of tuples.
Thus, to better analyze the behavior of the system when processing queries that return large result sets, we ment we merge the real and synthetic query sets resulting in a single set with 162 queries.
In Table 4 we present the processing times obtained for each component, considering several distinct query than the time spent to process the query by the DBMS, and it represents less than 15% of the overall time to
To further elaborate on this issue, Fig. 12 shows that the time for query processing inside the DBMS dom-total time taken to process the query. 7. Related work
In this section we review recent work related to the use of keyword-based searching for querying structured and semi-structured databases.

Keyword-based searching over relational databases is supported by systems such as BANKS ( Bhalotia et al., 2002 ), DISCOVER ( Hristidis &amp; Papakonstantinou, 2002 ), DBXplorer ( Agrawal et al., 2002 ) and
ObjectRank ( Balmin et al., 2004 ). In general, these works approach this problem by modeling the database are known as  X  X  X oin trees X  X  or  X  X  X oining networks X  X  and are ordered by a structure-based relevance score. BANKS and ObjectRank feature a ranking function based on Google X  X  PageRank ( Brin &amp; Page, 1998 ), while
DISCOVER and DBXplorer just consider the number of nodes in the answer tree. In ( Hristidis et al., 2003 ), the authors discuss how to leverage IR-related features available on commercial DBMS to compute a rele-vance ranking of  X  X  X oining networks X  X .

Our approach differs from the previous ones in three aspects. First, we adopt Bayesian networks to struc-
By doing so, we leverage IR techniques to measure the similarity between query terms and database values, which clearly presents better quality results than to leverage only structure-based evidences. Unlike the approach presented in ( Hristidis et al., 2003 ), the LABRADOR system is independent of specific DBMS fea-tures, such as Oracle Text and IBM DB2 Text Information Extender , to process unstructured queries. This allows searching over several databases managed by distinct DBMS with a single query. Second, we consider conditions regardless of any database physical issue. Third, our query processing does not require full-text indexes for the published attributes. Thus, the LABRADOR system can publish any database without changes in the schema or the data instances.

Unstructured query processing over XML databases has been recently addressed by many works in the lit-erature such as XSEarch ( Cohen, Mamou, Kanza, &amp; Sagiv, 2003 ), XKeyword ( Hristidis, Papakonstantinou, &amp; Balmin, 2003 ), XRANK ( Guo, Shao, Botev, &amp; Shanmugasundaram, 2003 ), and the approach proposed by
Xu et al. ( Xu &amp; Papakonstantinou, 2005 ). These works present keyword-based query processing mechanisms over XML documents, modeled as labeled trees, and consider that the desired answer is a meaningful part of the XML document, retrieving nodes of the XML graph that contain the query terms. In ( Hristidis et al., searching by allowing the user to specify labels and keyword-labels in the query. XRANK ( Guo et al., 2003 ) a ranking score for the answer trees. A unified search approach to heterogeneous XML and Web documents is proposed by SphereSearch ( Graupmann, Schenkel, &amp; Weikum, 2005 ). In this work, Web documents are auto-matically converted to XML documents and the unified search problem becomes a problem of keyword-based searching over XML documents. In addition, there are other approaches that aim at going beyond simple keyword-based searching. NaLIX ( Li, Yu, &amp; Jagadish, 2005 ) is a natural language system for querying
XML in which the submitted queries are adjusted by interaction with the user until the query can be parsed by the system. Then, the system generates an XQuery expression through the use of Schema-Free XQuery ( Li,
Yu, &amp; Jagadish, 2004 ), which is an extension of the well-known structured query language XQuery, that provides support to unstructured or partially unstructured queries.
 Bayesian network models were first used in IR problems by Turtle and Croft (1990) , and later by Ribeiro-
Neto and Muntz (1996) . This effective and flexible framework for modeling distinct sources of evidence in support of ranking was later reviewed in Cristo et al. (2003) . The Bayesian network framework for query structuring was introduced by Calado et al. (2002) and later extended, detailed and formalized in Calado
Our Bayesian network for query structuring differs from the original proposal in Calado et al. (2002) in the still keeping the good quality of the query results, as confirmed by our experiments.
Techniques to build rankings for SQL query results have been attracted some research interest recently larity between this problem and our query result ranking, the two problems differ in their core definition. ranking in our approach. 8. Conclusions
In this paper we have presented LABRADOR, a system for publishing and querying the contents of any relational database on the Web, by using a simple text box interface that allows keyword-based queries. The system operates by taking an unstructured keyword-based query, posed by a user, and automatically deriving then sent to the DBMS and their results are processed by LABRADOR to create a relevance-based ranking of them.
 sible queries by the likelihood that they correspond to the original query. Experiments with this strategy thermore, the best query was placed among the top four queries in the ranking in all cases we have tested.
Another novel strategy was also presented for ranking the results returned by the DMBS for the submitted set of results returned by the DBMS. We have presented recall X  X recision curves that demonstrate the accuracy of the ranking produced by this strategy.

The LABRADOR system was designed for operating in a non-intrusive way, in the sense that it requires no processing time is almost insignificant, as it was also demonstrated through experiments. Indeed, we have found that the overhead introduced in practical situations is less than 6% of the whole time spent when pro-cessing the queries.

Our experience in the development of the LABRADOR system has raised a number of suggestions for currently supporting only exact matches between terms appearing in the input unstructured query and those be necessary to incorporate similarity evaluation models distinct from the ones we are currently using.
Another interesting future work is to investigate the possibility of matching the keywords provided by the users in their queries with meta-data information (e.g., column or table names). This would give the oppor-path we did not explore in our current work. This could also be associated with some syntax convention tures would require some revision of the current Bayesian network model. Apart from that, the overall pro-cessing is not expected to change much.

Finally, we intend to investigate how our current framework could be expanded to support keyword-based queries over semi-structured databases, such as XML repositories, and compare the results with other works that deal with this problem. A potential advantage we foresee in such an investigation is the natural way by which Bayesian network models deal with intricate structures, as is the case of XML data. Acknowledgement
Special thanks to Dinorah Monteiro for developing a graphical interface to LABRADOR and to Wemer-son Souza for helping with the experiments.
 References
