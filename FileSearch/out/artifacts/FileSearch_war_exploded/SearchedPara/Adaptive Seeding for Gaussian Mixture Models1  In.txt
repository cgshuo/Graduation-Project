 ter analysis. A common approach is the method of maximum likelihood for which the Expectation-Maximization (EM) algorithm [ 13 ] can be applied. The EM algorithm iteratively tries to improve a given initial mixture model and converges to a stationary point of the likelihood function. Unfortunately, the likelihood function is generally non-convex, possessing many stationary points [ 23 ]. The initial model determines to which of these points the EM algorithm converges [ 5 ]. 1.1 Maximum Likelihood Estimation for Gaussian Mixtures A Gaussian mixture model ( K -GMM) over R D can be described by a parameter  X  = { ( w k , X  k , X  k ) } k =1 ,...,K , where w k  X  R is the mixing weight (  X   X  R D is the mean, and  X  th mixture component. Its probability density function is given by N (  X |  X ,  X  ). Given a data set X  X  R D , the Maximum Likelihood Estimation (MLE) problem is to find a K -GMM  X  that maximizes the likelihood algorithm, whose outcome heavily depends on the initial model, can be applied. 1.2 Related Work A common way to initialize the EM algorithm is to first draw means uniformly at random from the input set and then to approximate covariances and weights Often, few steps of the EM, Classification EM, or Stochastic EM algorithm are candidates is computationally expensive.
 reduced m  X  -dimensional space and applies HAC only on these modes. However, Moreover, in [ 22 ] it is outperformed by simple random methods. solution but also the number of components. It initializes the means by points which have a  X  X igh concentration X  of neighbors. To this end, the size m of the depends on the choice of m . Hence, we ignore this method in this paper. ture models with 1 through K components. Given a model  X  k it constructs several new candidates with k + 1 components. Each candidate is constructed by adding a new component to  X  k and executing the EM algorithm. Furthermore, there are some practical applications using the K -means++ algo-rithm for the initialization of GMMs (e.g., in [ 19 ] GMMs are used for speech 1.3 Our Contribution mainly two problems: Firstly, they are rather complex and time consuming. Sec-ondly, the choice of hyperparameters is crucial for the outcome. choosing sensitive hyperparameters. These methods can be seen as adaptions of the K -means++ algorithm [ 3 ] and the Gonzalez algorithm [ 17 ] and as an superiority of our methods compared to a large number of alternative methods. The most widely used initializations start by choosing K data points: Unif draws K points independently and uniformly at random from X . hierarchical clustering with average linkage cost on S .
 discrete radius K -clustering problem. Iteratively, it chooses the point with the largest Euclidean distance from the already chosen points. KM++ executes the K -Means++ algorithm [ 3 ], which has been designed for the K -means problem. In each round, KM++ samples a data point (i.e. the next mean) from the given data set X with probability proportional to its K -means cost (with respect to the points chosen so far). In expectation, the resulting the K -means algorithm is a special case of the EM algorithm [ 8 ]. Then, given K data points, Algorithm 1 is used to create a GMM, which is then the initial solution that is fed to the EM algorithm.
 Algorithm 1. Means2GMM ( X  X  R D , C X  R D , |C| = k ) A popular alternative is to apply the K -means algorithm with the chosen data To avoid this problem, one first runs a different algorithm that optimizes a We refer to the K -means algorithm as an intermediate algorithm and indicate its use by the postfix  X  km  X . Our new adaptive methods construct a sequence of models with k = 1 through k = K components adaptively. Given a ( k  X  1)-GMM  X  k  X  1 choose a point from the data set that is not described well by the given  X  which is hopefully a good representative of a component of an optimal k -GMM. Choosing a Point. The negative log-likelihood of a point x GMM  X  k  X  1 , measures how well x is described by  X  k  X  1 take negative values and does not scale with the data set and the GMM also applies to the minimum component-wise negative log-likelihood due to the first summand. Hence, we use the minimum Mahalanobis distance Our first method chooses the point x  X  X maximizing m ( x a uniform sample of X , which is chosen in advance (cf. Algorithm 2 ). also consider adding an  X  portion of uniform distribution, i.e. drawing x with probability Constructing a GMM. Then, given a point x  X  X and the means of  X  we construct a k -GMM. In our first experiments, we used Algorithm 1 to con-struct a k -GMM. However, it turned out that estimating only spherical covari-ance matrices (with variable variances) yields a better performance than esti-mating full covariance matrices. We assume that this is due to the fact that  X   X  1 is only a very coarse estimate of ( k  X  1)-components of an optimal k -GMM. Formally, we replace the covariance update in Line 3 of Algorithm 1 by  X  Means2SphGMM . Given the resulting k -GMM, our methods then again choose a new point from X as already described above.
 Intermediate Algorithm. Recall that some baselines use the K -means algorithm ances. We indicate its use by the postfix  X  cem  X .
 the hyperparameters  X  and s in our experiments.
 Comparison to Baselines. Our adaptive initializations can be seen as adap-tions of the Gonzalez and Kmeans++ algorithm. Simply speaking, these meth-ods assume that each component is represented by a Gaussian with the same another adaption of the Gonzalez algorithm is presented, which we denote by KwedlosGonzalez (KG) . Unlike our method, it chooses weights and covariance matrices randomly and independently of the means (and of each other).
Algorithm 2. SphericalGonzalez (SG) data sets. Our implementation as well as the complete results are available at Quality Measure. Recall that the goal of our paper (and the EM algorithm) is to find a maximum likelihood estimate (MLE). Thus, the likelihood is not only the common but also the appropriate way of evaluating our methods. Other measures need to be treated with caution: Some authors consider their methods only with respect to some specific tasks where fitting a GMM to some data is part of some framework. Hence, any observed effects might be due to of real world data sets do not necessarily correspond to the structure of an MLE. The same holds for data sets and classifications generated according to some GMM. Moreover, a cross-validation , that examines whether methods over-finding a model that does not fit too well to the given data set. Finally, one should not generate data sets according to some  X  X round truth X  GMM  X  compare GMMs with  X  gt because in many cases (e.g. small | expect  X  gt to be a good surrogate of the MLE.
 Setup. Recall that in Algorithms 2 and 3 hyperparameters  X  and s are used. We do not optimize them, but test reasonable values, i.e.  X  s 25 rounds and the EM algorithm for 50 rounds. If only the EM algorithm is applied, then we execute it for 75 rounds. 4.1 Artificial Data Sets Data Generation. We create 192 test sets, each containing 30 data sets that share certain characteristics [ 9 ]. For each data set, we first create a GMM random but control the following properties: First, the components of a GMM can either be spherical or elliptical. We describe the eccentricity of  X  have different sizes, in terms of the smallest eigenvalue of the corresponding covariance matrices. Third, components have different weights. or less. Following [ 12 ], we define the separation parameter c  X  = min l = k  X  In high dimension D 1, c  X  =2cor-responds to almost completely separated clusters (i.e. points generated by the same component), while c  X   X  X  0 . 5 , 1 } indi-cates a slight but still negligible overlap [ 11 ]. However, in small dimension, c  X   X  { 0 . 5 , 1 } corresponds to significant over-laps between clusters, while c  X  = 2 implies rather separated clusters (cf. Fig. 1 ).
 lows. Initially, we draw means uniformly at random from a cube with a fixed side length. For the weights, we fix some c { 2 sizes and the eccentricity, we fix the minimum and maximum eigenvalue and draw the remaining values uniformly at random from the interval. Then, we set  X  k = Q T diag(  X  2 k 1 ,..., X  2 kD ) Q for a random Q scaled as to fit the predefined separation parameter. Given the resulting GMM  X  , we first draw some points according to  X  . Then, we construct a bounding at random from the resized box.
 parameters: K = 20, | X | X  X  1000 , 5000 } , D  X  X  3 , 10 } { and without or with 10 % noise points.
 Evaluation Method. We consider the initial solutions produced by the initial-obtained by running the EM algorithm afterwards. For each data set, we compute these averages, we create rankings of the algorithms 4 . Then, we compute the average rank (and standard deviation of the rank) of each algorithm over all datasets matching certain properties.
 Results. In general, we observe that one should use an intermediate algorithm Data without Noise. For these rather simple data sets, there is no method that or the G km initialization that performs best.
 the performance is determined by the separation. Furthermore, a good initial solution does not imply a good final solution. Given overlap ( c their initial solutions have low average ranks compared to KM++ covariance matrices do not matter much if means are assigned properly in the first place. Indeed, the simple G km and KG do the trick. is prone to choose outliers. Overall, given data sets without noise, Ad performs best.
 Noisy Data. When introducing noise, our adaptive methods are still among the best methods, while the performance of some others degenerates significantly. Tables 4 and 5 show that SG cem and Ad cem still work well for c contrast to data without noise, also for separated instances ( c G km are now among the methods with the lowest average rank. This is not a one can draw the same conclusion, i.e. KG and G km can not handle noisy data. For noisy data, our Ad cem methods outperform the others. Low Dimensional or High Sample Size Data. We expect that, if the dimension is KM++ km and Unif km perform best. For data sets with D =3and However, if we are given noise and small separation, the simple Unif We also increased the sample size to | X | = 5000 and the dimension to D = 10, expecting that the higher sample size can make up for the higher dimension too small a separation, our Ad cem methods and the simple Unif 4.2 Real World Data Sets We use four publicly available data sets: Covertype ( | X valued features) [ 4 ]; two Aloi data sets ( | X | = 110 250, D [ 2 ] and the Amsterdam Library of Object Images [ 16 ]; Cities ( 1000 [ 1 ]; Spambase ( | X | = 4601, D = 10 real-valued features) [ 4 ]. SG cem ( s = 1) is considerably better than the other methods. For Cities and Spambase ( K = 10), SG ( s = 1) does better (without running the CEM). For 4.3 Time Measurement The run times of the compared methods match our expectation (cf. Table 8 ). the EM algorithm. Second, sampling and running methods on a random subset of the data should in general reduce the run time. methods: Given a data set with a large number of points per cluster or low dimension, the K -means++ initialization followed by the K -means algorithm and Means2GMM should do well. Otherwise, we recommend our new methods Ad and SG followed by the spherical CEM algorithm, especially if your data is presumably noisy. Last but not least, whatever you prefer, we suggest trying intermediate steps of the spherical CEM or K -means algorithm. For the K -means++ algorithm and the Gonzalez algorithm there are prov-retical analysis that will transfer these results to the MLE problem for GMMs.
