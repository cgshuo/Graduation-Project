 CrowdRec 2015 provides the recommender system commu-nity with a forum at which to discuss crowdsourcing and hu-man computation. Systems that explicitly collect informa-tion from human annotators to improve recommendations are becoming more widespread. At this year X  X  workshop, we highlight incentivization and the issue of avoiding bias. We take a special look at how recommender systems can influence collective behavior, and the contribution that the crowd can make to recommender system evaluation.
 H.3.3 [ Information Storage and Retrieval ]: Information Filtering human computation; human intelligence; crowdsourcing; rec-ommender systems
The success of recommender systems arises from their ability to make use of explicit or implicit information about user preferences for items. Under conventional approaches, a recommender system is passive, waiting for users to con-tribute information or collecting it from existing sources. A crowdsourcing approach to recommendation changes the as-sumption that passivity is a given. Instead, systems actively approach users to ask for information that is needed in order to improve recommendations. Alternatively, information can be elicited from crowdworkers on a commercial crowd-sourcing platform, or contributed by dedicated experts.
The ACM RecSys 2015 CrowdRec Workshop 1 is the third in a series of workshops on crowdsourcing and human com-putation for recommender systems. Since the series began in 2013, it has been clear that crowdsourcing is a permanent http://crowdrecworkshop.org crowdworkers X  ability to support themselves, and to their quality of life. The needs and wishes of crowdworkers are various, and difficult to understand without careful study. Work such as [10] is an important step towards taking the preferences of crowdworkers into account. Recommender systems themselves are an important tool for the personal-ization of crowdsourcing tasks to crowdmember needs.
In order to motivate users to contribute to recommender systems, special mechanisms must be deployed. There can be as simple as reminding users of the connection between their contributions and improvements in the recommenda-tions that they receive. Games and gamification are other ways in which users can be incentivized.

Human contributors must not only be motivated to con-tribute, but they must make contributions in good faith. Making use of information collected from crowdworkers or users may open a recommender system to a collusion attack. Under such an attack, human contributors work together to bias the outcome of recommendation technology. How-ever, if appropriate design of crowdsourcing mechanisms, this probability can be reduced. We also mention the pos-sibility that human contributions may have an important contribution to make, as the recommender system commu-nity becomes increasingly aware of the dangers of biased recommendations [8]. Human contributors have the ability to act as guards to ensure that recommendations remain at the level of helpful personalization, and do not disadvan-tage particular groups of users (e.g., low income, or living in certain geographic regions).
In addition to contributing information that addresses the knowledge acquisition bottleneck , as in [4], connecting the crowd to recommender systems opens up the possibility of new applications. Here, we mention two.
A recommender system helps users to make choices, but also has a net influence on their decisions. As such, rec-ommender systems can be used to steer the behavior of a community in a manner that benefits all its members. An example is Commutastic [6], which rewards users for avoid-ing traffic. It recommends alternate afterwork activities, which gives it the potential to guide collective behavior. It is critical that users remain aware and informed of the pur-pose of the application. By choosing to adopt its sugges-tions, they are also choosing to improve the conditions of the community at large.
Finally, we mention the potential of using feedback of the crowd to help evaluate recommender systems. A basic ex-ample is a recommender system that asks its users for feed-back on their perceptions of recommendations. The power of using the crowd for help in evaluation is that it is possible to collect fine-grained information about preferences, which would not have otherwise been available, i.e., as in [9].
Crowdsourcing platforms benefit from, and also provide benefit to, recommender systems. The information provided by the crowd can fill in important gaps that help to address
