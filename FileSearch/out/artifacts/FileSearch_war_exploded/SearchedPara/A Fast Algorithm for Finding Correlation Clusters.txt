 Clustering is a classical technique in computing and statistics. Noise deteriorates cluster quality significantly and prevents finding mean ingful clusters when the amount of noise is big. It is difficult to distinguish noise data objects from normal ones when we do not have prior knowledge about the data. However, clustering can serve as the first step to explore such a data set with noise, particularly when the prior knowledge about the data is unavailable.

Generalised projected clustering sheds light on solving this problem by finding cor-related clusters. When data objects are projected to a data subspace using Singular Value Decomposition (SVD) or PCA, the correlatio n clusters are condensed to a small area whereas noise data objects scatter across the projected space. Therefore, it is possible to separate correlated data objects from noise ones.

Most existing projected clustering methods use agglomeration methods to find corre-lation clusters. A big data set is randomly partitioned into a large number of micro clus-ters, and then an agglomeration approach is used to group correlation clusters. When correlated data are split into a number of micro clusters, they themselves become noise too. This process of randomly generated s eeds affects the quality of found clusters. When a process for noise elimination is employed, many data objects in the correlation clusters are removed before they are grouped into clusters. Some well known examples of generalized projected clustering are PROCLUS [2], ORCLUS [1], 4 C [4], CURLER algorithm [5] and HARP [7]. We do not consider axis-parallel projection methods, also called subspace clustering, such as CLIQUE [3], and EPCH [6].

Instead of agglomerating randomly generated micro clusters into final clusters, we partition a data set into clusters in a top-down manner. The key idea for such a divisive method is to find a suitable criterion for data partition. We capture the direction of the largest variance of data using the corresponding principle vector, thereby taking small risk of partition correlation clusters into s eparate clusters. We employ grouping tech-nique used in agglomeration methods to group correlation clusters after partitions. The proposed divisive projected clustering method preserves the essence of projected cluster-ing, overcoming the drawbacks of existing pro jected clustering me thods. In addition, the proposed algorithm is significantly more efficient than most agglomeration algorithms. Projected clustering searches for hidden s ubspaces together with a set of data objects such that data objects are closed with each other in the lower dimensional subspaces. The hidden data spaces are found by using S VD decomposition. Eigenvectors corre-sponding to eigenvalues with low spreads for ms a subspace. The intuitive explanation for this is as follows ( see more justifications in Section 4). When the covariance matrix of a set of correlated points is decomposed by SVD, some eigenvalues should be zero or close to zero. All the points are projected along a line in the subspace spanned by eigenvectors corresponding to these zero eigenvalues. In other words, the tightness of objects in the subspace defined by eigenvector s associated with the lowest eigenvalues is an alternative to measuring the correlation level of data objects.

Formally, let D be a dataset of m data objects (row vectors) being treated as d -dimensional feature (column) vectors. o i  X  D stands for the i -th object in D where o =( o i 1 ,o i 2 ,...,o id ) . Simply, we have D =[ o ij ] , 1  X  i  X  m ,and 1  X  j  X  d . Definition 1. Generalised projected clustering Given the user-specified l and k , a data set D is partitioned into k disjoint subsets D 1 , D 2 , ... , D k horizontally, such that, for all 1 eigenvalues, where U p S p V T p =cov( D p ) ( cov is the covariance matrix of D p , the SVD decomposition of which results in U p ,S p , and V T p ).
 Data points are clustered based on their clo seness in some projected subspaces instead of the original space. This clustering captures correlations among data points.
To measure the closeness of data points in a subspace, the projected distance is de-fined as following.
 Definition 2. Projected distance Let D p be a subset of a data D . U p S p V T p =cov( D p ) ( cov( D p ) is the d  X  d covariance matrix of D p , and U p ,S p , and V T p are results of SVD decomposition). Let E be the set of eigenvectors corresponding to l smallest eigenvalues. A data object p  X  D is distance of objects p and q , denoted by Pdist( p , q ,E ) ,is their Euclidean distance in projected space E .
 The projected distance between two data poi nts is the Euclidean distance between their projected images in a subspace. This dist ance varies in different subspaces.
To measure the projected distance varia tion over a group of data points in a subspace, the projected energy is defined as the following.
 Definition 3. Projected energy The projected energy of data set D p is defined as Energy( D p ,E )= i = N i =1 Pdist( o i , c , E ) /N ,where N is the number of objects in D p , c the centroid of all objects in D p , and E an associated subspace.
 The smaller the projected energy, the denser the data point in the subspace. In clustering, low projected energy is preferred.

In projected clustering, th e traditional distances between data objects are replaced by the projected distances in subspaces. However, there are no uniform and invariant distances in projected clustering since each tentative cluster has its own subspace. In other words, the distance between two objects varies in different subspaces.
Projected distances have been studied in statistics. The Mahalanobis distance [8] measures distance between two objects by usi ng a set of reference data. But the Maha-lanobis distance is the projected distance in the entire space. The g eneralised projected distance is defined in a subspace, and is a generalised Mahalanobis distance.
As mentioned before, the ORCLUS algorithm [1] presents a variant agglomerative method to find k projected clusters. First, D are randomly partitioned into k 0 initial data subsets D 1 , D 2 , ... , D k 0 ,where k 0 &gt;&gt; k . If each data object is considered as an initial micro cluster, then the computational cost will be too expensive. The smaller k , the faster the ORCLUS. However, the high quality of clusters is sacrificed if k 0 is small.

Second, ORCLUS performs the following two iterations: 1. Merge pairs of clusters with the smallest, combined project energy until the number 2. Redistribute all data objects to the k p clusters according to their respective, pro-
The above procedure terminates until k p = k with a parameter  X  to control the step size. If the step parameter is big, then a lot of merge occur in one iteration and the quality of final clusters is not guaranteed. If the step parameter is small, the execution time is increased.

A significant computational cost of the algorithm is from the decomposition of a data subset D i . The complexity of such a decomposition is determined by the the number of dimension (attributes). Specifically, it costs O ( d 3 ) . Moreover, the computation has to be done in each merger of two data subsets. The complexity of the ORCLUS algorithm is therefore as high as k 3 0 + k 0 Nd + k 2 0 d 2 0 .

A heuristic way of speeding up the ORCLUS algorithm is to make k 0 small and to conduct more merges in each step. However, the quality of clustering has been traded off. This problem is caused by the fact that ORCLUS is an agglomerative algorithm and too many merges are required to form a small number of clusters. In contrast, the divisive method needs much less steps to form clusters. Large computational costs of projected clustering lie in computing covariance matrixes and SVD (or PCA) decomposition. The computational costs of covariance matrixes and SVD (or PCA) decomposition is largely determined by the number of attributes, d , rather than the number of objects in a data set N i .

In most applications, we have k&lt;&lt;m where m is the number of objects in the data set, and k is the number of clusters. Therefore, a top-down method (divisive method) needs significantly less number of computations of covariance matrixes and SVD (or PCA) decomposition than a bottom-up one (agglomerative method).

A key question is how to partition the data. Given a dataset, the projected cluster problem can be regarded as the one of partition of the dataset into k clusters such that the sum of the projected distance of each da ta object to its cluster centroid is mini-mized. Compared to the clustering in full-di mensional space, the p rojected clustering makes use of the projected distance instead of the full-dimensional distance. Recall that we need to find a best subspace and their asso ciated subsets of dat a objects such that the sum of the projected distances of data objects to their centroids is minimized. The number of the projected clusters in our algorithm is given. So it is to determine only the directions of spanning vectors. Within one cluster the optimal direction of the vector to which its associated data objects are projected should reflect the minimal variance of these data. An eigenvalue is numerically rela ted to the variance it captures. The higher the value, the more variance it has captured. The principal vector defines a projection that encapsulates the maximum amount of variation in a dataset. This principal vector is in fact the eigenvector with the highest corresponding eigenvalue.

We make use of the principle vector of eigenvectors. All data objects D are projected to the principle vector as discussed in the previous section, and the centroid separates data into two groups: D 1 and D 2 . D 1 contains data objects whos e projected values are greater than or equal to the means, and D 2 contains the rest.
 The pseudo code of the algorithm is listed below.
 The DPCLUS algorithm partitions a data set into clusters in a top-down manner. The splitting point is the centroid of data objects projected to the principal vector. This saves a lot of computation for covariance matrixes and SVD decompositions as done in the ORCLUS algorithm. The sole dependence on the principal vector to separate data is rough and does not produce quality clusters. We design the Redistribution function to minimise the projected energy of each clusters after clusters are formed by partitions.

The number of final clusters can be greater than k because the number of leaves is not tested until all data sets stored in the newest tree layer are split and redistributed.
Outliers affect the quality of final clusters very much since they change the orien-tations of data objects greatly. Some data objects may not belong to any cluster and are considered outliers. To deal with this problem, we set an outlier threshold in Re-distribution step, say  X  . When the projected distance of a data object to any cluster is greater than  X  , then the data object is considered as an outlier and is excluded from the subsequent clustering.
 We discuss the complexity of the algorithm in the following.

It is assumed that k denotes the number of final classes, N the number of data objects, d the dimension of the data set, and l the dimension of subspace. The cost for partition is kd 3 / 2 . d 3 comes from computing covariance matrices and SVD decomposition for a cluster. Since the partition is conducted in a binary way, the number of total partitions is k . Each partition requires a SVD decomposition to determine the subspace.

After the partition, each cluster has to be decomposed again to determine whether or not it satisfies the projected clustering requirement. If no, it will participate in redistri-bution. The number of such decomposition is 2 k , and hence the costs for the decom-positions is 2 kd 3 . All clusters are projected to l dimensional subspaces, and each data object has to be checked against each cluster. Th erefore, the total costs for distribution is kNl .
 In sum, the computational complexity for the DPCLUS algorithm is O (3 kd 3 + kNd ) . Note that we could not do much for term d 3 since it is for computing a covariance matrix and a SVD decomposition. However, the proposed algorithm has reduced the number of such computations significantly.

Our DPCLUS algorithm is faster than most exiting generalised projected clustering algorithms. We compare the time complexities in the table below.

It should be noted that in order to speed up, some algorithms make use of techniques such as heuristics, small number of micro-clusters, and random samples. However, these techniques come with a price; that is, some of the clustering quality must be sacrificed. 4.1 Efficiency Comparison to ORCLUS We use synthetic data sets for this experime nt. More details about how these data sets were generated will be given in the following subsection.

For the test of scalability with the size of data sets, the data sets each contain 10 attributes and up to 300,000 objects. For the test of scalability with the number of at-tributes, the data sets each contain 100,000 objects and up to 50 attributes. The number of embedded clusters is fixed to 20 for the above two tests. l is set to 10 for both methods. k 0 is set to 15  X  k to make ORCLUS efficient.  X  for both methods is set as 0.01. min N varies for different data sets, but is set as the same for both methods. A value less than 0.0001 is consider as 0 in the experiments to test the satisfaction of Definition 1.

Figure 1 shows that DPCLUS is more efficient than ORCLUS in large data sets as well as in high dimensional data sets. Consider that most computational time for pro-jected clustering is spent on data decomposition, whose time complexity is cubic to the dimension and independent of the data set size. DPCLUST outperforms ORCLUS significantly in high dimensional data sets since it reduces the number of data decom-positions significantly. 4.2 Clustering Quality To demonstrate the clustering quality of DPCLUS, we compare it to three clustering methods on a synthetic data set. We embedded 20 clusters that are correlated in some subspaces over a set of random data objects. The data set contai ns 10,000 data objects, with each object having 20 attributes. Each embedded cluster contains 250 data ob-jects, which have 10% variations from the original pattern. Other 5,000 data objects are random data objects generated by the uniform distribution.
 We set the parameters of DPCLUS as l =10 , k =20 , min N =50 ,and  X  =0 . 01 . The results from DPCLUS are shown in Figure 2. DPCLUS is able to find all embedded clusters correctly. Although k is set as 20 in the experiment, the number of final clusters can be any integer number between 20 to 32, because the number of clusters is not tested until all data sets stored in the newest tree layer are split and redistributed. The number of the found clusters are greater than 20, since some clusters are split into two. For example, clusters at row 2: 1 and 2 are from the same cluster. DPCLUS has successfully identified cluster patterns from random data.

We set the parameters of ORCLUS as k =20 , l =10 , k 0 = 350 ,and  X  =0 . 01 .Fig-ure 2 shows a good result. ORCLUS identified fewer than a half of embedded clusters with high quality. Since initial micro-clust ers in ORCLUS is randomly chosen, the final clusters vary in different executions.

We further show that both k -means and hierarchical clustering methods failed to find quality clusters in such noise data in Figure 3. Data is sampled for hierarchical clustering method because of efficiency constraint. We have presented a divisive, projected clustering algorithm for detecting correlation clusters in highly noised data. The distinction of noise points from correlated data points in a projected space offers benefits for projected clustering algorithms to discover clus-ters in noise data. The proposed algorithm mainly explores this potential. Further, the proposed algorithm is faster than most existing general projected clustering algorithms, which are agglomerative clustering ones. Unlike those agglomerative algorithms, the produced clusters by the proposed algorithm do not rely on the choice of randomly generated initial seeds, and are completely determined by the data distribution. We ex-perimentally show that the proposed algorithm is faster and more scalable than than ORCLUS, a well-known agglomerative projected clustering, and that the proposed al-gorithm detects correlation clusters in noise data better than ORCLUS.

