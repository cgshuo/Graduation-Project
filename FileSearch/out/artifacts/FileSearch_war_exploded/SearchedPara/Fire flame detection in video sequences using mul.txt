 Tung Xuan Truong, Jong-Myon Kim n 1. Introduction security and commercial applications, and several conventional methods have been proposed to detect fire. However, most of these methods require a close proximity to the source of the fire and are based on particle sensors ( Jones, 2004 ). Therefore, they cannot detect fire in open or large spaces and cannot provide additional information regarding the burning process. To overcome these weaknesses, video fire detection is a suitable candidate. posed ( Chen et al., 2004 ; Toreyin and Centin, 2007 ; Celik and Demirel, 2009 ; Ko et al., 2009 ; Toreyin et al., 2006 ; Borges and
Izquierdo, 2010 ; Han and Lee, 2006 ; Celik et al., 2007 ). Most of these algorithms are based on color pixel recognition, motion detection, or both. In ( Chen et al., 2004 ), a dynamic analysis of flames using an RGB/HIS color model was used to determine the existence of fire. However, the decision rule of this method is not good at distinguishing real fire regions from regions of movement or noise since the flame difference is measured only between two consecutive frames. In ( Toreyin and Centin, 2007 ; Toreyin et al., 2006 ), the boundary of the flames was represented in the wavelet domain, and the high frequency natures of the boundaries of fire regions were used to spatially model flame flicker, yielding good results. In ( Celik and Demirel, 2009 ), a new method of flame detection that used flame pixel color properties was proposed.
This method used the YCbCr color space because the RGB color space has illumination dependence disadvantages. This means that if the illumination in the image changes, the fire pixel classification rules will not perform well. In ( Ko et al., 2009 ), fire detection based on vision sensors and support vector machines was proposed. In the study, a non-linear classification method using support vector machines and luminescence maps was proposed, showing robust results for flame detection. In ( Borges and Izquierdo, 2010 ), a probabilistic model for color-based fire detection was utilized to extract candidate fire regions. In addition, four parameters were extracted from the features of candidate fire regions, such as area size, surface coarseness, boundary roughness, and skewness. More-over, a Bayes classifier was used to distinguish between fire and non-fire. In ( Han and Lee, 2006 ), the authors proposed the fire and smoke detection system in the tunnel environment. This algorithm detects the fire by comparing image of normal state with input image using color information. In addition, it automatically detects smoke using motion direction, edge detection, and comparison of color information of input images. However, this algorithm uses many ad-hoc parameters, prohibiting for applying to dynamic fire situations. A real-time fire detection method using the statistical color model and foreground object information was proposed in ( Celik et al., 2007 ). The authors introduced the statistic color model for generic fire model. Some of the above algorithms were applied to real systems with considerable success. However, each of these methods still had limited application and lacked enough robustness. In order to enhance the performance of fire detection, we propose an efficient four-stage fire detection approach. In the first stage, movement-containing regions are detected using an adaptive Gaussian mixture model. In the second stage, fire and non-fire regions are segmented using fuzzy c-means (FCM) clus-tering. In the third stage, additional features are extracted from the tempo-spatial characteristics of fire regions. In the final stage, fire is classified using support vector machines (SVM). Experi-mental results indicate that the proposed approach outperforms existing fire detection algorithms in terms of accuracy of fire detection, providing a low false alarm rate and high reliability in both indoor and outdoor test videos.

The rest of this paper is organized as follows. Section 2 introduces the features of fire. Section 3 describes the proposed four-stage fire detection algorithm. Section 4 discusses experimental results of the proposed method and compares the performance of the proposed method with those of other fire detection algorithms, and Section 5 presents conclusions of the study. 2. Features of fire
The features of fire play an important role in the development of fire detection systems since these features are used to distin-guish between fire and non-fire. In this study, important charac-teristics of fire such as color, region, time, and space were analyzed. In practice, most fuels will burn under appropriate conditions, reacting with oxygen from the air, generating combustion pro-ducts, emitting light, and releasing heat. When fire is produced, it usually arises from a stable location, drifting upward in a diffuse manner. The color usually ranges from red to yellow and may turn white when the temperature is very high. The size, area, shape and number of fire regions in an image vary from frame to frame. The surfaces and contours of fire regions are usually rough. 3. The proposed fire detection algorithm
A flowchart of the proposed fire detection approach is depicted in Fig. 1 . The proposed algorithm is comprised of four stages: (1) moving region detection using an adaptive mixture Gaussian model, (2) fire color segmentation using FCM clustering, (3) para-meter extraction from the tempo-spatial characteristics of fire regions and (4) fire identification using support vector machines. The approach is also composed of the two phases of training and classification. In the training phase, the training data sets are extracted from the training videos and used to train the SVM. In the classification phase, the trained SVM is utilized to distinguish between fire and non-fire. In the following sections, the proposed fire detection algorithm is analyzed in detail. 3.1. Moving region detection
The detection of moving regions is a fundamental key in video fire detection, which is the first stage of the proposed method. Several methods have been proposed to detect moving regions, with background subtraction being the most typical. As the name suggests, background subtraction is the process of separating out foreground objects from the background in a sequence of video frames. Many different techniques have been proposed with different strengths and weaknesses in terms of performance and computational requirements ( Piccardi, 2004 ). Among these meth-ods, the Gaussian mixture model (GMM) that was first introduced by Stuaffer and Grimson in 1999 is the most widely used method for background subtraction due to its speed, simplicity and ease of implementation ( Stauffer and Grimson, 1999 ; Stauffer and Grimson, 2000 ). In this method, each pixel is modeled as a mixture of Gaussian distributions, and any pixel intensity value that does not fit into one of the modeled Gaussian distributions is marked as a foreground pixel. In order to achieve high performance, we utilize an adaptive Gaussian mixture model ( KaewTraKulPong and Bowden, 2002 ) because it can successfully manage illumination changes and reduces the effects of small repetitive motions such as moving vegetation and small camera displacement.

In GMM, each pixel location in RGB color space is modeled with a set of K Gaussian distributions. The probability of obser-ving the current pixel value X t at the time t is defined as follows:
P  X  X  X  X  where K is the number of Gaussian components per pixel, o the estimate of the weight parameter of the k th Gaussian component at the time t , m k , t and P k , t are the mean and the covariance matrix of the k th Gaussian component in the mixture function which is defined as follows:  X  X , m , S  X  X  1 where d  X  3 because X t is in the RGB color space, and P k , t
This assumes that the red, green and blue pixel values in RGB color space are independent and have the same variances.
The mixture of Gaussians actually models both foreground pixels and background pixels without distinction; that is, some mixture components model foreground pixels, while others model background pixels. In order to identify the foreground pixels, the background pixels are determined as follows: K distributions are sorted based on the fitness value o k / s then the first B Gaussian distributions are used as a model of the image background, where B is estimated as follows:
B  X  arg min where T is the threshold. The other distributions are considered to represent a foreground pixel. The values of the weight o k mean m k , and the covariance s k are updated using online Expecta-tion Maximization (EM) algorithms. This information is repre-sented in the detail in ( KaewTraKulPong and Bowden, 2002 ). model with and without morphological operations for detecting moving regions. Fig. 2 A shows the original fire image. In general, the boundary of flame tends to fluctuate continuously and the inner region of flame tends to static. Therefore, the foreground flame region can be detected easily in the initial stage of fire.
However, as times go on, the inner regions of flame are merged into backgrounds. Thus, the holes (or  X  X host X ) might exist in the foreground as shown in Fig. 2 B. In order to improve the quality of the movement regions, adaptive GMM method was utilized as shown in Fig. 2 C. In addition, morphological operations (dilation and erosion) were used to remove some inherent noise and smooth the result as shown in Fig. 2 D. However, the moving regions may consist of fire or non-fire objects, such as smoke, people or objects. FCM clustering is utilized to cluster only candidate fire regions from these moving regions. 3.2. Color segmentation of fire using the FCM algorithm addition to fire, such as people, vehicles, birds, clouds, and smoke.
However, the colors of these objects generally will differ from the color of fire. Thus, we use color segmentation of fire in this study.
The basic idea is composed of two steps: (1) the pixels in the moving regions are distributed into groups, and then (2) groups having colors similar to fire are selected. To accomplish this, the well-known FCM algorithm is employed ( Bezdek, 1981 ; Bezdek et al., 2005 ). In addition, we use the CIE LAB color space to construct a generic chrominance model for fire pixel segmentation instead of red-green-blue (RGB) color space. In contrast to RGB color space, CIE LAB color space is completely device-independent and makes it possible to separate luminance/illumination from chrominance information. The CIE LAB color components can be obtained by converting from RGB to CIE LAB ( Bhaskaran and
Konstantinides, 1997 ) and include the three components L , A and B , where L indicates the lightness of pixels, A and B indicate the colors of the pixels. The chrominance components A and B are inputs for the FCM algorithm, while the output is the clusters of pixels in the moving regions. The steps used in the FCM algorithm are as follows: 1. Compute the number of groups c and initialize the centroid 2. Compute the membership values u ij for each data element using the following equation: where m represents the degree of fuzziness. 3. Update the centroid value v i as follows: v i  X  4. Evaluate the terminating condition max (where : . : is the Euclidean norm). The iteration stops when this is satisfied; otherwise go to step 2. 5. Assign all pixels to a cluster according to the corresponding maximum membership values.

In order to improve the accuracy of clustering, it is necessary to compute the precise number of clusters and the initial values of the centroids in the clusters. To compute the initialization of the aforementioned parameters, we utilized the empirical method of ( Tan and Isa, 2010 ). The value of the centroid of each cluster is compared with the color of fire. The clusters with centroid values close to the color of fire are selected for processing in the next step. If there is no suitable centroid, it can be concluded that the objects in the moving regions are not fire.

Fig. 3 shows segmentation results based on fire color from the movement-containing regions using the FCM algorithm. The original frame from the video clip and the movement-containing regions are shown in Fig. 3 (A) and (B), respectively. The candidate regions of fire are shown in Fig. 3 (C), and (D) presents the non-fire regions. 3.3. Parameter extraction
The selected candidate regions of fire, shown in Fig. 3 (C), may still be non-fire because several moving objects have the same colors as that of fire, such as red vehicles, vehicle brake lights, or persons carrying red objects or wearing red clothes. By investi-gating several videos, we observed that the temporal and spatial features of fire regions vary during the manifestation of the fire. Thus, we extract the following features of the candidate regions to identify them as either fire or non-fire.

Let X  X  { x 1 , x 2 , x 3 , y , x n } represent the data set. The mean and variance values of the data set are defined as follows: M V  X  where M x is the mean value of X , and V x is the variance value of the data set X . 3.3.1. Area randomness
We know that the size of a fire region randomly changes from frame to frame. In contrast, non-fire regions do not show such random changes. Using this feature, we can extract parameters by calculating the difference in area between two consecutive frames and then summing these differences among the previous frames and the current frame. The changes in the sizes of the candidate regions from frame to frame are calculated using the following rule: D A n  X  9 A n A n 1 9 ,  X  8  X  where D A is the difference size between the areas of the candidate regions in the n -th and ( n 1)-th frames. Using all of the D As , the mean value ( M A ) and variance value ( V A ) are calculated using Eqs. (6) and (7). 3.3.2. Surface roughness
Unlike moving non-fire regions with the same color as fire, the intensity of the fire region pixels in a gray image is always non-uniform, and the surfaces of these fire regions are coarse. There-fore, at every frame in the video sequence, we can calculate feature parameters of the fire (e.g., mean ( M I ) and variance ( V intensity of all the pixels in the candidate regions) using Eqs. (6) and (7). 3.3.3. Contour roughness
Different from non-fire moving regions, the contour of moving fire regions is usually rough and coarse. In order to utilize this feature, we perform a simulation using different shape descriptor techniques and find that Hu invariant moments ( Hu 1962 ) are very effective for the description of the fire regions because they clearly distinguish between moving fire and non-fire regions. For every new frame, we calculate the Hu invariant moments of the candidate moving regions ( H 1 , H 2 , H 3 , H 4 , H 5 , H 3.3.4. Motion estimation using motion templates
Motion plays an important role in video fire detection, provid-ing both spatial and temporal characteristics of fire regions. Hence, some parameters are extracted from the motion features of fire. In order to accomplish this, we use motion templates to estimate the motion of candidate fire regions. Motion templates were intro-duced by Bobick and Davis ( Bobick and Davis, 1996 ; Davis and Bobick, 1997 ) and were further developed by Davis ( Davis and
Bradski, 2009 ). They are an effective way to track general move-ment and are especially applicable to the movement of fire. Using motion templates, two parameters can be extracted from every new frame: the magnitude ( M ) and the orientation ( O ) of the moving vector of the candidate regions. On the other hand, it is not sufficient to distinguish between moving fire regions and moving non-fire regions because there are some moving regions that have the same values. Therefore, the mean and variance of M v and O are calculated using Eqs. (6) and (7). As a result, six parameters are extracted from the features of the motion vector for further analysis: the magnitude ( M v ), orientation ( O v ), mean of M variance of M v ( V M ), mean of O v ( M O ), and variance of O 3.4. Fire identification using support vector machines
Support vector machines are supervised learning techniques and were introduced by Vapnik ( Vapnik, 1982 , 1995 ). SVM is widely applied to many fields of pattern recognition ( Burges, 1998 ). The SVM is not only capable of learning in high-dimen-sional spaces but also can provide high performance with limited training data sets. Therefore, many techniques have been pro-posed to improve the performance of SVM ( Platt, 1998 ; Cristianini and Shawe-Taylor, 2000 ). The basic idea of the linear SVM is to create a suitable hyper-plane to divide a given data set into two parts with maximum margin. After that, the SVM is utilized to classify unlabeled data sets. However, in practice, many data that are not fully linearly or nonlinearly separable. For that reason, no hyper-plane may exist that can split the data into two parts. In order to solve this problem, we used a non-linear SVM.
In the previous section, we described a set of 17 parameters extracted from the spatial and temporal characteristics of fire: sv  X  [ M A , V A , M I , V I , H 1 , H 2 , H 3 , H 4 , H 5 , H
These 17 parameters are used as an input feature vector to train the SVM classifier, which is then used to produce a fire alarm in the video frame. In order to accomplish this, both the training data set ( SV ) and the kernel function k ( sv i , sv j ) are required. The training clips of fire, non-fire, or moving objects in different scenes. The selection of the kernel function is very important for the training stage because it affects the accuracy of the SVM classification. If the kernel function is suitable for the data set, the classification result is going to be good, and vice versa. Through the analysis of several video clips, we selected the radial basis function kernel because it is the most useful for the features of fire. This kernal is defined as follows: k  X  sv , sv j  X  X  exp J sv i sv j J where k ( sv i , sv j ) is the kernel function, sv i and sv feature vectors, and d is a parameter set by the user that determines the width of the effective basis kernel function. If small d values are used, overtraining occurs with the basis function wrapped tightly around the data points. If large d values are used, the basis function forms an oval around the points without defining the shape or pattern. In this study, our simulations showed that the best performance was achieved when the value of d ranged from 5 to 10. Therefore, the default value for d was set at 7.5. The trained SVM was then used to distinguish between fire and non-fire. 4. Experimental results 4.1. Experimental environment compared with those of four state-of-the-art algorithms: fire detection in video sequences using a generic color model ( Algo-rithm 1 )( Celik and Demirel, 2009 ), fire detection based on vision sensors and support vector machines ( Algorithm 2 )( Ko et al., 2009 ), a computer vision-based method for real-time fire and flame detection ( Algorithm 3 )( Toreyin et al., 2006 ), and a probabilistic approach for vision-based fire detection in videos ( Algorithm 4 ) ( Borges and Izquierdo, 2010 ). The proposed and existing algo-rithms are implemented using MATLAB 7.5 and are tested on a
Pentium Quad-Core 2.8 GHz PC platform. In order to train the support vector machines, we use several training movies which include indoor fire, outdoor fire, non-fire, and moving objects that are the same color as fire and we utilized 25,000 fire-containing frames and 15,000 non-fire-containing frames, respectively. In addition, the test is performed using ten video sequences, each of which has a 320 240 image size, as shown in Fig. 4 . Movies 1, 6 and 8 are indoor fires, 2 X 5 and 7 are outdoor fires, 9 X 11 are indoor non-fires, and the others are outdoor non-fires. feature parameter values for the proposed approach: the number of Gaussian components per pixel K value in Eq. (1) is 3, the fuzziness coefficient m value in Eq. (4) and (5) are 2, the terminal condition e value for FCM is 0.001 and the scaling factor d value is 7.5 in Eq. (9). 4.2. Experimental evaluation
The comparison results are presented in Table 1 , where the true positive (TP) is the rate of correctly detecting a real fire as a fire and false positive (FP) is the rate of recognizing a real fire as a non-fire, respectively. For Movie 1 and Movie 8, the accuracies of all algorithms are the highest because it contains a clearly visible fire in an indoor space. For Movie 3 and Movie 4, since the fire is blurred at some frame in the movie, the accuracies of all algo-rithms are relatively low. For all fire-containing movies, the proposed approach outperforms the other algorithms in terms of TP and FP for both the indoor and outdoor test videos, showing an average TP of 94.78% versus TPs of 94.13%, 94.25%, 94.23%, and 94.41% for other methods.

Table 2 shows additional data on the accuracy of fire detection in four videos with no fire, where the true negative (TN) is the rate of correctly detecting a non-fire as a non-fire and false negative (FN) is the rate of recognizing a non-fire as a fire, respectively. For Movies 9 and 12, FN is zero because the color of the moving regions is different from the color of the fire regions. For Movie 10, the moving regions include cars with colors similar to that of fire. Thus, FN is high for all algorithms. For
Movie 11, the moving regions are a red color, but the regions are uniform, and the shape and the movement are different from those in the fire region. Thus, FN of the proposed algorithm is low. For all non-fire-containing movies, the proposed approach out-performs the four conventional algorithms in terms of reducing the false fire detection rate, showing an average FN of 1.32% versus FNs of 2.34%, 2.25%, 2.03%, and 1.85% for other methods.
In addition, we used a Wilcoxon signed-rank test that is a non-parametric statistical hypothesis test to evaluate the performance of the proposed technique. In this Wilcoxon signed-rank test, two related samples are compared to assess whether their population meansdiffer( Wilcoxon, 1945 ; Corder and Foreman, 2009 ). Tables 3 X 6 show the results of the Wilcoxon signed-rank test between our proposed technique ( X ) and conventional techniques ( Y 1, Y 2, Y 3, Y 4).
For example, Table 1 shows the result of the Wilcoxon signed-rank test between the proposed technique ( X ) and conventional algorithm 1 ( Y 1). The Wilcoxon signed-rank test includes 6 steps: 1. The sign of X -Y 1 is denoted in the Sign column by either (  X  )or ( ). If X and Y 1 are equal, the value is then thrown out. 2. The values of X -Y 1 are given in the next two columns. 3. The last two columns are the ranks. The Absolute Rank column has no signs, and the Signed Rank column gives the ranks along with their signs. 4. The data is ranked from the smallest value to the largest value. In the case of a tie, ranks are added together and divided by the 5. The test statistic, W  X  , is given by the sum of all of the positive 6. Lastly, this test statistic was analyzed using a table of critical between our proposed technique and other techniques. Overall, our proposed technique outperforms conventional techniques. conventional algorithms by consistently increasing the accuracy of fire detection and decreasing the error of false fire detection for each video. The proposed method was also tested with movies that were downloaded from two websites: http://www.ultimate chase.com/Fire_Video.htm and http://signal.ee.bilkent.edu.tr/Visi Fire/Demo/FireClips . The results were the same.
 our proposed algorithm and conventional algorithms. The proposed algorithm requires more computational time than conven-tional algorithms due to the four stages of the proposed algorithm is more complex than others. Since tod ay X  X  high performance processors including NVIDIA GPUs (Graphics Processing Unit) and TI DSPs (Digital Signal Processor) can support these algorithms in real time, the accuracy of fire detection is more important. In the future, we will work on reducing the computational time of our proposed algorithm while maintaining the accuracy. 5. Conclusions
In this paper, we proposed an effective novel approach for fire detection in video sequences using segmentation and classifica-tion algorithms. The proposed approach consists of four stages: (1) an adaptive mixture of Gaussian models for detecting moving regions, (2) the FCM algorithm for selecting candidate fire regions from other moving regions based on the color of fire, (3) para-meter extraction for extracting special parameters based on the tempo-spatial characteristics of fire regions, and (4) an SVM for distinguishing between fire and non-fire. Experimental results showed that the proposed approach outperforms other state-of-the-art fire detection algorithms in terms of fire detection accuracy, providing a low false alarm rate and high reliability in open and large spaces.
 Acknowledgment
This work was supported by 2012 Research Fund of University of Ulsan.
 References
