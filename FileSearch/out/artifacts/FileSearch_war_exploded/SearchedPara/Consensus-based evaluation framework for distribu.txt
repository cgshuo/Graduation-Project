 Jason J. Jung Abstract Multi-agent systems have been attacking the challenges of information retrieval tasks on distributed environment. In this paper, we propose a consensus choice selection method based framework to evaluate the performance of cooperative information retrieval tasks of the multiple agents. Thereby, two well-known measurements, precision and recall , are extended to handle consensual closeness (i.e., local and global consensus) between the results can be ranked with respect to the consensual score, and the ranking mechanism has been verified to be more reasonable.
 Keywords Information retrieval  X  Multi-agent systems  X  Evaluation 1 Introduction Information retrieval (IR) systems have been applied to distributed computing environment where several different hosts store only a part of information partitioned from a very large amount of information (particularly, textual documents). Basic operations of the IR system unioned to create the final result set.

In this paper, we investigate evaluation issue of the retrieval strategies between multiple agents, and propose an efficient evaluation framework measuring their performance. Espe-The most well-known evaluation measurements are precision and recall . They are based on the comparison of the resulting document sets with another desired output document set, effectively comparing which documents are retrieved and which are not. These criteria are well understood and widely accepted.

On the other hand, three different forms (e.g., the archetypal, probabilistic, and informa-tion retrieval-expert system (IR-ES) approach [ 11 ]) of dilemma taken from IR measurements have remained intractable even given the different ontological and methodological assump-nature of the subject matter of the field, (b) the nature of relevance judgement, and (c) the nature of cognition and knowledge. Especially, in distributed computing environment, these intractability might become more serious [ 7 , 31 ].

In this paper, with regard to the dilemma on IR measurement, we want to deal with the problem caused by the all-or-nothing manners of the measurements for a certain query q represented as a set of terms (this will be formulated and explained in detail, later). Even though a IR system may show users the results that are very close to the expected results, its evaluation measurements are quite lower than it deserves. The reason for this is that the agents take into account only the common results from the set of corresponding partial the measurement is zero. It is thus necessary to leverage the classic measurements with the proximity between the partial results.

We thereby exploit a consensus method, which is capable of building two kinds of consen-sus (i.e., local and global consensus) to overcome the problem presented above. In particular, the ratio of co-occurrence patterns among documents is playing an important role of obtaining consensus results d C . Basically, as a decision-making process for relevance , the consensus aims to be:  X  Inclusive: As many agents as possible should be involved in the consensus decision- X  Participatory: The consensus process should actively solicit the input and participation  X  Co-operative: Participant agents in an effective consensus process should strive to reach  X  Egalitarian: All agent members of a consensus decision-making body should be afforded,  X  Solution-oriented: An effective consensus decision-making body strives to emphasize
According to each agent X  X  IR strategies in multi-agent architecture, performance measure-ments are heterogeneous with each other. In the context of the consensus-based decision mak-ing, we believe that the heterogeneous measurements can be somehow integrated, by finding out the consensual measurement cooperatively. Consequently, the integrated measurements are comparable to find out the best combination of the strategies among multi-agents.
The outline of this paper is as follows. In Sect. 2 , we describe background knowledge about distributed IR and preliminary notations, and our motivation of this study. Section 3 shows the way to extend the classic measurement for better evaluation of IR performance on multi-experimental results will be given to prove the proposed evaluation measurements. Section 5 discuss some significant issues and mentions the existing studies related to consensus-based 2 Motivation and backgrounds Above all, we want to note the primitive architecture of agent-based IR system on distrib-uted environment. Based on the previous work, e.g., Teraphim [ 10 ], MIND [ 4 , 30 ], DILI-GENT [ 28 ], in a distributed IR system, a document collection D is split into N partitions {
D as a set of agents M ={ M 1 ,..., M S } , as shown in Fig. 1 . The formation and strategy of this MAS are entirely dependent on the user X  X  experiences and heuristics.

On this distributed environment, the IR process is mainly composed of two steps, as follows; 1. Query-based information collection, and 2. Integration of the retrieved collections.
 As the first step, the multiple agents deploy their own retrieval strategy M i . Definition 2.1 (Retrieval strategy) A retrieval strategy of an agent M x can be applied to a host with a query q . It is denoted as where D i is a target host that the agent has accessed to. As a result, a set of documents d h meeting the given query q are returned.
 Such strategies are including boolean, extended boolean, vector models, and so on. Further-more, the strategy can work in a hybrid manner.

They can access to the available hosts with the query given by the corresponding user, and D j and other hosts. It means that D corresponding host.

Next step is to integrate the results (e.g., document collections) retrieved by the multiple agentsofwhichstrategiesareheterogeneous.Essentially,theremightbeaspecificmechanism to merge the results. Regarding to the distributed information architecture shown in Fig. 1 , we may consider two possible cases, (a) integration of the sets of results retrieved from a functions and can be defined as; for a set of results retrieved from a same host, and then, for integrating the set of results of all hosts, Here, D i ( + ) can allow result sets retrieved by only agents which have accessed to the cor-responding hosts. As previously mentioned, even though the result set may be empty, the intersection operation in Eq. 4 is different from simple intersection.

Finally, the distributed IR strategies have to be evaluated. Main reason of this evaluation is formation among agents) seem to be closely related to the performance of this IR process. Established the factors, we have to compute measurements to judge which factors is better. More exactly, we have to consider combination of factors, because the factors are correlated (or trade-off) with each other. As the most well-known and simplest measurement, we can compute two measures precision and recall 1 from the retrieved document collection D + sent from the N hosts.
 Definition 2.2 (Precision, Recall) Given a set of reference (or desired) documents D Ref for a certain query q , the precision and recall of retrieval for the query are given by respectively.

Naturally, these two measures are correlated with each other. As the IR strategies are changed, the measurements are not providing significant knowledge any more. Example 2.1 (Recall-Precision Trade-off) As a simple example, Recall-Precision Trade-off  X  higher precision P and the lower recall R (and, vice versa).
 This trade-off between them means that they are difficult to determine ranking of the results. More seriously, applications are requiring their own goals, so that they need to more specific evaluation measurement. For patent managers, recall is the most important measurement, because they are asked to search for all of the relevant documents.

Thereby, there are some studies to propose an integration scheme, i.e., combining several measurements into a single number. These include the average precision at seen document, the R-precision, the E-measure, van Rijsbergen X  X  F and the average precision over all documents. However, all of these numbers are directly based on the concepts of precision and recall. the integration of the measures are trivial and impossible to capture meaningful knowledge about the performance of IR process. Moreover, with a number of measurement information sent from multiple agents, the integration mechanism has to be modified to play an important role of indicator of distributed IR tasks. 3 Consensus-based relaxation In this paper, the problem described previously will be dealt with consensus choice selection method. Basically, consensus is a way to represent multiple opinions as an unified one. The consensus decision-making is a decision-making process that not only seeks the agreement of most agreeable decision. Consensus is usually defined as meaning both general agreement, and the process of getting to such agreement. Consensus decision-making is thus concerned primarily with that process. 3.1 Problems However, these traditional IR criteria do not work properly to evaluate a certain cooperative retrieval strategy. Especially, for the performance measurements of distributed IR, each agent might provide heterogeneous performances about information hosts. This task can be difficult We mainly concern about the following two problems of the traditional measurements; 1. Multi-agent system-based IR system on distributed environment has been using the inte-2. It is difficult for the distributed IR system based on to rank a set of results retrieved Thus, the evaluation process should be changed. We propose a consensus-based perfor-mance measurement integrating the different results. We refer to this process as measurement integrating the set of results.
 Example 3.1 Suppose that a document collection D be split into two hosts H 1 and H 2 (i.e., N = 2), and MAS be organized by three agents M  X  , M  X  ,and M  X  with additional agent for integration. Their IR strategies are given by  X  Classic boolean model M  X  ( q , D 1 ) ={ d i | X  t k  X  q , Occur ( t k , d i ) } ,  X  Vector-space model M  X  ( q , D 1 ) ={ d i | where  X   X  and  X   X  are user-specified thresholds, and function f can extract a certain set of term features. Additionally, M  X  and M  X  can return the corresponding scores value scr . Five retrieved results are merged into D + by either or .
 patterns such as d 2 , meaning a type of score measuring the consensus among the hosts. and vector model might be more flexible, compared with the classical boolean model. In this case, the ranking information has been taken into account to maintain the result d 2 among the hosts. 3.2 Building consensus and relaxation In order to solve this problem, we want to exploit a consensus method to relax the evaluation measurements. Consensus methods is basically able to resolve the conflicts among the results by the difference between the retrieved results. For consensus choice functions, we extended some postulates defined in [ 21 ]. The distance function  X  is given by The consensus is made up when the summation of all possible pairs of agents is minimized. Two kinds of consensus are considered in this paper. Definition 3.1 (Local consensus) A local (or host) consensus within a host can be established by a set of document sets retrieved by a set of corresponding agents. Given a query q ,the local consensus C L ( i ) of H i is given by where n is a number of agents accessing to the host, and n C 2 is a combinatorial computation
For obtaining d C L , we can compute the co-occurrence ratio where  X  C L is a score threshold for local consensus. Function src is a normalized score computed from initial ranked results by (it simply express linear proportion, but it can also be applied to logarithm function.). For the unranked results (e.g., in boolean model  X  ), it should be where this score (1 / 2) is equivalent to that of lowest ranked result from Eq. 10 .Fromthe previous Example 3.1 , we can build the local consensus as by when  X  C L = 1 2 .
 Definition 3.2 (Global consensus) A global consensus can be established by a set of local consensus of corresponding hosts. Given a set of C L , the global consensus C G for query q is given by where N is the number of hosts, and  X  C G is a score threshold for global consensus. With respect to the global consensus score scr C G , the results can be ranked. More importantly, two different heuristics for measuring global consensus score of each document are given by which return the normalized maximum score and average score, respectively. We will em-pirically compare them in Sect. 4 .

Now, we can rewrite the conventional measurements (shown in Eq. 5 ). The relaxed pre-cision and recall are given by respectively. If D Ref is ranked, we can obtain more elaborated evaluation measurement. 4 Experimental results In order to prove the rationality of the proposed evaluation method, we conduct several experimentations. For building testing bed, we chose 20 newsgroups dataset 2 as testing dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. We randomly partitioned them into seven fragments for seven hosts. The specification of this testing bed is shown in Table 1 .

We constructed three kinds of multi-agent formations for cooperative IR from the testing bed, as shown in Table 2 . Especially, in F 3 , we considered two more variances by changing the portion of IR agent population.

According to the MAS formations, the consensus was built to measure consensus based precision P C and recall R C by comparing with five D Ref s of the corresponding queries. We  X 
Given 20 queries, we measured the conventional measurements; i.e., P = 0 . 68 and R = 0 . slightly improvement, while F 3 provides about 25.5% higher precision, as shown in Fig. 2 . Especially, as shown in Fig. 3 , except for F 1 MAS, F 2 and F 3 outperform the conventional evaluations (over approximately 32% higher). 5 Discussion and related work We want to discuss two issues;  X  relationship between the number of agents and the evaluation measures, and  X  relationship between the combination of IR methods and the evaluation measures While the consensus-based precisions P C are in the nearly same level, the consensus-based recalls R C show significantly different patterns between F 1 and the other two. We found out that boolean model-based IR strategies have the more dominant influence as the population of agents is getting less.
 these results are ranked to be compared with D Ref , as shown in Figs. 4 and 5 . We found out  X  from mismatched ranking between extended boolean model and vector-space model. On the We found out that it is caused by ranking strategy of the IR methods. As the global consensus threshold is higher, the irrelevant results have ranked better.
As related work, recently, Nguyen et al. proposed consensus-based IR system [ 22 ]. It applies the consensus method to obtain the final answers, by adjusting the queries given by concerned each partial results without considering merging results. Moreover, INQUERY [ 5 ] has proposed error-based evaluation method for evaluating the collection of ranked results. CQE (Collaborative Querying Environment) [ 12 ] has provided visualization facilities for user intuition.

As similar experimentation, in [ 25 ], they compared 14 metrics designed for IR evaluation with graded relevance, together with 10 traditional metrics based on binary relevance, in terms of stability, sensitivity and resemblance of system rankings using three different data sets from NTCIR CLIR track series [ 6 ].

Another interesting experimental study is on query-specific document clustering with various kinds of representation, duplicated documents, and results with disparate qualities, which frequently happen in the distributed IR environment [ 9 ]. The study was to test if the cluster hypothesis would hold in a DIR environment and if presenting retrieved documents in a hierarchical clustering would still be as effective as it is in classical IR. 6 Conclusions and future work The partial results retrieved by heterogeneous IR methodologies should be reasonably inte-grated for improving the understandability of end users. In this paper, we have been proposing a multi-agent evaluation method based on two kinds of consensus (i.e., local and global con-sensus) for distributed IR systems. Co-occurrence pattern-driven consensus information has been exploited to build the global consensus and minimize the distances between partial posed measurements have shown better performance in the following point of views  X  evaluation of queries generated by the users; some queries might be very vague  X  evaluation of retrieval strategies of agents; relaxing boolean type models, and  X  evaluation of integration strategies of agents.

As a future work, evaluation of socialized information systems such as grid computing is necessary [ 27 ]. We want to measure the relationship between information sources to build social information space. We believe that the consensus measure will be more enhanced with this evaluation method on ontology-based approach systems [ 17 ]. They can automatically communicate with each other by semantic alignment. Especially, this kind of IR evaluation platform is very important to interactive IR systems for improving the IR performance [ 2 ]. References Author Biography
