 common, with examples ranging from digitized collections o f books by companies such as Google recommendation data set. These data sets present major oppo rtunities for machine learning, such as the ability to explore much richer and more expressive mod els, as well as providing new and interesting domains for the application of learning algori thms.
 ularly in terms of computation time and memory requirements . For example, a text corpus with 1 million documents, each containing 1000 words on average, w ill require approximately 12 Gbytes of memory to store the 10 9 words, which is beyond the main memory capacity for most sing le pro-cessor machines. Similarly, if one were to assume that a simp le operation (such as computing a then a full pass through 10 9 words will take 1000 seconds. Thus, algorithms that make mul tiple have run times in days.
 to get around the memory problem by distributing 1 how to combine local processing on each of the P processors to arrive at a useful global solution. models are arguably among the most successful recent learni ng algorithms for analyzing count data would be particularly useful for this type of model.
 The novel contributions of this paper are as follows: Before introducing our distributed algorithms for LDA, we b riefly review the standard LDA model. LDA models each of D documents as a mixture over K latent topics, each being a multinomial distribution over a W word vocabulary. For document j , we first draw a mixing proportion  X  from a Dirichlet with parameter  X  . For the i th word in the document, a topic z k chosen with probability  X  w with probability  X  Thus, the generative process is given by Given the observed words x = { x distribution over the latent topic indices z = { z  X  out, and the latent variables z are sampled. Given the current state of all but one variable z conditional probability of z where the superscript  X  ij means the corresponding data-item is excluded in the count v alues, and where n out: n processors. We distribute the D documents over P processors, with D processor. We partition the data x (words from the D documents) into x = { x and the corresponding topic assignments into z = { z on processor p . Document-specific counts n maintains its own copy of word-topic and topic counts, n counts as n 3.1 Approximate Distributed Inference In our Approximate Distributed LDA model (AD-LDA), we simpl y implement LDA on each pro-cessor, and simultaneous Gibbs sampling is performed indep endently on each of the P processors, one variable z Note that n P number of words on processor p . After processor p has reassigned z p , we have modified counts n using a reduce-scatter operation, where n The counts n topic assignments z (i.e., n We can consider this algorithm to be an approximation to the s ingle-processor Gibbs sampler in However, as each processor starts sampling, the global coun t matrix is changing in a way that is with  X  X epairing X  reversibility of the sampler by adding a ph ase which re-traces the Gibbs moves steps ended up being rejected. 3.2 Hierarchical Distributed Inference A more principled way to model parallel processes is to build them directly into the probabilistic model. Imagine a parent collection of topics  X  the topic distributions on the various processors. We assum e  X  is sampled from  X  according to a Dirichlet distribution with topic-dependent strength par ameter  X  processor is simply an LDA model. Hence, the generative proc ess is given by, The graphical model corresponding to this Hierarchical Dis tributed LDA (HD-LDA) is shown on deeper hierarchical prior on  X  (instead of on  X  ) while the second deals with a document-specific computation.
 As is the case for LDA, inference for HD-LDA is most efficient i f we marginalize out  X  and  X  . We derive the following conditional probabilities necessary for the Gibbs sampler, In our experiments we learn MAP estimates for the global vari ables  X  ,  X  and  X  . Alternatively, one can derive Gibbs sampling equations using the auxiliary variable method explained in [6], but can be understood as expectation maximization on a collapse d space where the M-step corresponds to MAP-updates and the E-step corresponds to sampling. As su ch, the proposed Monte Carlo EM (MCEM) algorithm is guaranteed to converge in expectation ( e.g., [8]). The MAP learning rules are derived by using the bounds derived in [9]. They are given by where  X  is the digamma function. Careful selection of hyper-parame ters is critical to making HD-LDA work well, and we used our experience with AD-LDA to guide these choices. For AD-LDA P of matching the value of  X  used in our LDA and AD-LDA experiments.
 We can view HD-LDA as a mixture model with P LDA mixture components, where the data have ing: First we sample assignment variables z each processor. Assuming equal prior weights for each proce ssor we then compute responsibilities, the test document is then given by the responsibility-weigh ted average over the processors. counting topics in documents, n AD-LDA that the count arrays n every global update). For each run of each algorithm, a sampl e was taken after 500 iterations of were simulated in software (by separating data, running seq uentially through each processor, and simulating the global update step), except for the speedup e xperiments which were run on a 16-processor computer.
 Figure 2: (Left) L topics onto simplex, showing convergence to mode. (Right) S ame setup as center panel, but with P = 10 processors.
 It is not obvious a priori that the AD-LDA algorithm will in ge neral converge to a useful result. Later in this section we describe a set of systematic empiric al results with AD-LDA, but we first use an illustrative toy example to provide some insight as to how AD-LDA learns a model. The toy example has W = 3 words, K = 2 topics. The left panel of Figure 2 shows the L of Gibbs iterations, for both single-processor LDA and AD-L DA with P = 2 . LDA and AD-LDA in X ) correspond to somewhat random movement close to the ran domly initialized starting point. In the next phase ( X  X urn-in X ) both algorithms rapidly move in p arameter space towards the posterior mode. And finally at equilibrium, both are sampling around th e mode. The center panel of Figure 2 to each of the two individual processors, and those paramete rs after merging, are shown (for AD-qualitative behavior as in the center panel, but now for 10 pr ocessors. One might worry that the  X  X atches X  onto a consistent labeling that then rapidly move s it towards the posterior mode. It is useful to think of LDA as an approximation to stochastic descent in the space of assignment algorithm to move up the likelihood surface. With multiple p rocessors, each processor computes an recombines these directions by vector-addition, in the sam e way as one would compute a gradient or concave, but will break down at saddle-points. We conject ure AD-LDA works reliably because peaked for LDA models and high-dimensional count data sets.
 To evaluate AD-LDA and HD-LDA systematically, we measured p erformance using test set per-plexity, computed as Perp ( x test ) = exp(  X  1 mix  X  perplexity computation exactly follows that of LDA, since a single set of topic counts n when a sample is taken. In contrast, all P copies of n HD-LDA, as described in the previous section. Except where s tated, perplexities are computed for all algorithms using S = 10 samples from the posterior (from 10 different chains) using with the analogous expression being used for HD-LDA.
 We compared LDA (Gibbs sampling on a single processor) and ou r two distributed algorithms, AD-LDA and HD-LDA, using three data sets: KOS (from dailykos.co m), NIPS (from books.nips.cc) and for number of processors, P , ranging from 10 to 1000 for our distributed models. same whether we use single-processor LDA or either of the two algorithms with data distributed across multiple processors (either 10 or 100). The figure sho ws the test set perplexity for KOS (left) and NIPS (right), versus number of processors, P . The P = 1 perplexity is computed by LDA (circles), and we use our distributed models  X  AD-LDA (cr osses), and HD-LDA (squares)  X  to compute the P = 10 and P = 100 perplexities. Though not shown, perplexities for AD-LDA P=1 corresponds to LDA (circles), and AD-LDA (crosses), and HD-LDA (squares) are shown at P=10 and 100 . remained approximately constant as the number of processor s was further increased to P = 1000 for KOS and P = 500 for NIPS, demonstrating effective distributed learning wi th only 3 documents to be  X  X ard X  to learn, i.e., topics mutually exclusively dis tributed over processors. parallelized samplers are systematically converging more slowly than single processor sampling. If iments consistently showed (somewhat surprisingly) that t he convergence rate for the distributed algorithms is just as rapid as for the single processor case. As an example, Figure 4 (left) shows test perplexity versus iteration number of the Gibbs sample r (NIPS, K = 20 ). During burn-in, up LDA. Also note that 1 iteration of AD-LDA (or HD-LDA) on a para llel computer takes a fraction of the wall-clock time of 1 iteration of LDA.
 e.g., perhaps the distributed algorithms X  performance div erges when the number of topics becomes number of processors P = 10 (not shown here are the results for the KOS data set which were quite This lower perplexity may be due to: for AD-LDA, parameters c onstantly splitting and merging producing an internal averaging effect; and for HD-LDA, tes t perplexity being computed using P copies of saved parameters.
 ( perplexity of 1575 for AD-LDA and HD-LDA. This shows that sim ple averaging of results from separate processors does not perform nearly as well as the di stributed coordinated learning. Our distributed algorithms also perform well under other pe rformance metrics. We performed pre-cision/recall calculations using TREC X  X  AP and FR collecti ons and measured performance using the well-known mean average precision (MAP) metric used in IR re search. Figure 5 (left) again shows that AD-LDA and HD-LDA (both using P=10) perform similarly t o LDA. All three K = 200 LDA models have significantly higher precision than TF-IDF on th e AP and FR collections (significance was computed using a t-test at the 0.05 level).
 The per-processor per-iteration time and space complexity of LDA and AD-LDA are shown in Ta-ble 1. AD-LDA X  X  memory requirement scales well as collectio ns grow, because while N and D Similarly the time complexity scales well since the leading order term N K is divided by P . The C term accounts for the communication cost of the reduce-sum o peration on the count difference ( n that of AD-LDA, but HD-LDA has bigger constants.
 Using our large NYTIMES data set, we performed speedup exper iments on a 16-processor SMP shared memory computer using P = 1, 2, 4, 8 and 16 processors (since we did not have access to a distributed memory computer). The single processor LDA run with 1000 iterations for this efficiency, with a 8 . 5  X  speedup using P = 16 processors. This speedup reduces our NYTIMES 10-Note, however, that while the implementation on an SMP machi ne captures some distributed effects we do expect that for problems with large N general yield a proper MCMC sampler except in special cases [ 10]. Mimno and McCallum [11] re-cently proposed the DCM-LDA model, where processor-specifi c sets of topics are learned indepen-dently on each processor for local subsets of data, without a ny communication between processors, nor is it defined by a generative process.
 We proposed two different approaches to distributing MCMC s ampling across different processors for an LDA model. With AD-LDA we sample from an approximation to the posterior density by of the data. Despite having no formal convergence guarantee s, AD-LDA works very well empir-ically and is easy to implement. With HD-LDA we adapt the unde rlying LDA model to map to the distributed computational infrastructure. While this m odel is more complicated than AD-LDA, MCEM. Careful selection of hyper-parameters was critical t o making HD-LDA work well. In conclusion, both of our proposed algorithms learn models with predictive performance that is no different than single-processor LDA. On each processor the y burn-in and converge at the same rate as LDA, yielding significant speedups in practice. The space and time complexity of both models make them scalable to run on collections with billions to tri llions of words. This material is based upon work supported by the National Sc ience Foundation: DN and PS were supported by NSF grants SCI-0225642, CNS-0551510, and IIS-0083489, AA was supported by an NSF graduate fellowship, and MW was supported by grants IIS-0535278 and IIS-0447903.
