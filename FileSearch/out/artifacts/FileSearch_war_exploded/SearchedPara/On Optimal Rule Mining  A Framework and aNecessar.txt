 Rule discovery has attracted a lot of attention from data mining researchers probably because rules are an understandable representation of knowledge. In unsupervised paradigm, association rule mining is certainly the most popular method. Since its initial formulation by [1], the problem of association rule min-ing -and the underlying problem of the frequent set mining-has focused many works. Our paper focuses on association r ule mining, especially on classifica-tion rule mining as proposed by [2] i.e. with one predetermined target like in supervised learning.

An association rule is a rule A  X  B ,where A and B are two sets of items (also called itemsets) such that A =  X  , B =  X  and A  X  B =  X  , meaning that given a database D of transactions (where each transaction is a set of items) whenever a transaction T contains A ,then T probably contains B also [1]. The problem of mining association rules is then to generate all association rules that have support and confidence greater than a user-specified minimum support and minimum confidence respectively. Support is defined as the proportion of transactions containing A and B in D (noted P ( AB )or supp ( AB )), while confidence is the proportion of transactions containing A and B inside the set of transactions containing A in D (noted P ( B | A )or conf ( A  X  B )). Association rule mining is a two-step process: minimum support constraint is firstly applied to find all frequent itemsets in D and secondly, the frequent itemsets and the minimum confidence constraint are used to form rules.

Finding all frequent itemsets in D is computationally expensive since there are 2 k possible itemsets where k is the number of items in D . However using the downward-closure property of support (also called antimonotonicity), very efficient algorithms [3,4] can find all frequ ent itemsets. An interesting survey may be found in [5]. This step gives in any circumstances an important role to the support constraint and mining interesting rules without the support requirement has been identified as an important problem. Some researches try to avoid the use of support [6] or to avoid the task of threshold fixation [7].

Finding all and only all interesting rules is also a problem because the gener-ated rule sets are quite large, especially within the support-confidence framework while the percentage of interesting rules is often only a very small fraction of the all rules. A first strategy to reduce the number of mined rules consists of increas-ing the user-specified minimum support. Unfortunately with this strategy many interesting rules and especially nuggets will be missed. A second strategy con-sists of increasing the user-specified minimum confidence. This will favor rules with large consequent which may give many uninteresting rules.

An another popular strategy is to rank the rules in a post-analysis phase with additional objective measures of interest. A large number of interestingness measures were proposed. Interesting surveys and comparisons may be found in [8,9,10]. An another way of reducing the rule sets is to allow the user to specify which items have to or cannot be part of the left-hand-side or the right-hand-side of the rules. This is the case for associative classification which focuses on association rules whose right-hand-side are restricted to a class attribute [2,11]. Unfortunately these strategies are always subject to the dictatorship of support. A more efficient approach is to apply additional constraints [12] on item appear-ance or to use additional interestingness measures as soon as possible to reduce both the time to mine databases and the number of founded itemsets [13]. In particular we are here interested in measures that can reduce the search space and also be useful for the evaluation of the quality of mined patterns. We be-lieve that this approach is certainly the most promising one. To overcome the previous mentioned problems with the dictatorship of the support constraint different solutions were proposed at the algorithmic level and mainly for the confidence measure.

The rest of the paper is organized as follows. In Section 2, we present an overview of recent works that focus on the algorithmic properties of measures. In Section 3, we present a new formal framework which allows us to make the link between analytic and algorithmic properties of the measures. We apply in Section 4 this framework to optimal rules, and we demonstrate a necessary and sufficient condition of existence for applying a pruning strategy with a large set of measures. We conclude in Section 5. To get rid of support constraint, several authors proposed to focus on the algo-rithmic properties of confidence. The goal is to succeed in getting all the high confident rules, even the nuggets of knowledge. We here briefly review some of these works. Some of these works are mainly based on algorithmic properties of confidence, while others focus on different measures, by exploiting their intrinsic properties.

In [14] the authors introduce the h-confidence a new measure over itemsets mathematically identical to all-confidence proposed by [15]. Both of these mea-sures are antimonotone. They also introduce the concept of cross-support pat-terns i.e. -uninteresting -patterns involving items with substantially different support levels. The authors thus propose hyperclique miner , an efficient algo-rithm that utilizes both the cross-support and anti-monotone properties of the h-confidence measure. In [16] the authors introduce the Universal Existential Up-ward Closure property based on a certain monotonicity of the confidence. This property applies to classification rules and allows to examine only confident rules of larger size for generating confident rules of smaller size. The authors deduce from this property a top-down confidence-based pruning strategy.

In [17] the authors adapt a technique by [18] to propose a branch-and-bound algorithm for associative classification, based on an antimonotonic property of convexity of the  X  2 . In [19] the authors introduce a new type of antimonotony, called Loose Anti-Monotony, that can be applied for statistical constraints. This concept is then efficiently exploited in an Apriori-like algorithm. In [20] the author introduces the notion of optimal rule set for classification rules. A rule set is optimal if it contains all rules, except those with no greater interestingness than one of its more general rules. This concept defines a pruning strategy that applies to a large set of measures. The author gives an individual proof for 12 measures.

In the following, we introduce a new framework and use it to link together analytic properties of measures and the antimonotone property of [20]. 3.1 Adapted Functions of Measure Firstly, we will precise the concept of a ssociated measure introduced in [21]. In this article, measures are considered as functions of R 3  X  X  X  R . The authors only focus on the parametrization of interestingness measures in function of the number of examples, antecedents, and consequents. Sinc e the antimonotone property of optimal rule mining relies o n the number of counter-examples, we will study the behavior of measures according to this quantity. Similar approaches can be found in [22,9,23,10]. We here give a general framework to study analytic properties of measures.
 Definition 1. (adapted function of measure) Let call function of measure adapted to a given measure of interest m a couple ( D , X  m ) ,where D X  Q 3 and  X  (1)  X  association rule r ,  X  X  X  X  such that  X  m ( X )= m ( r ) ; (2)  X  X  X  X  ,  X  an association rule r such that m ( r )=  X  m ( X ) .
 The domain D is then called an adapted domain.
 We thus define a surjection from the space of association rules, over all the databases, in D . The specification of D allows to make the difference between all the parametrizations: support of examples, support of counterexamples, or confidence of the rule. The adapted function depends of the parametrization. The second condition assures of the usability of D by assuming that every point in D corresponds to a real situation.
 Example 1. (Recall Measure) Rec ( A  X  B )= y  X  x z where x = P ( A  X  B ), y = P ( A ) and z = P ( B ). If we study its variations according to z , we find that it increases if y  X  x&lt; 0 and decreases if y  X  x&gt; 0. But constraints of support let us assume that y&gt;x ,thus Rec decreases with z . 3.2 A Domain for Counterexamples We present now a domain according to cou nterexamples. Using the constraints define the following domain, and prove that it is adapted: Property 1. Let m be an interestingness measu re such that there exists  X  m with function of measure for m .
 Proof. The first condition comes immediately from definitions by taking X = D , we need to construct a database DB containing an association rule A  X  B , such that m ( A  X  B )equalsto  X  m ( x, y, z ). We define n as an integer such that ( x  X  n, y  X  n, z  X  n )  X  N 3 . Our database should verify the following equalities P ( A  X  B )= x, P ( A )= y, P ( B )= z . The constraints on the domain assure that x  X  y  X  x + z  X  1 holds. We can thus construct the database of table 1 satisfying the equalities.
 We now benefit of a rigorous formal framework for the study of interestingness measures. We will use it in the following to establish a necessary and sufficient condition (NSC) for existence of an antimonotone property. We thus focus on the parametrization with counterexamples. 4.1 A Condition of Optimonotony From now, we use the same notations as in [20]: P is an itemset, X is an item not in P and c a class item in a supervised context.
 Theorem 1. (Antimonotone property, [20]) If supp ( PX  X  c )= supp ( P  X  c ) then rule PX  X  c and all its more-specific rules will not occur in an optimal rule set defined by confidence, odds ratio, lift, gain, added-value, Klosgen, conviction, p-s, Laplace, cosine, certainty factor, or Jaccard.
 In addition Li et al. [24] prove that the Relative Risk verifies this property. We introduce the general notion of optimonotony , the antimonotone property for optimal rule mining, and formulate a NSC for it.
 Definition 2. (optimonotony) A measure of interest m is optimonotone if, given a rule P  X  c and a specification PX  X  c , we have ( supp ( PX  X  c )= supp ( P  X  c ) =  X  m ( PX  X  c )  X  m ( P  X  c )) .
 An optimonotone measure can be clearly added to the list of theorem 1. Theorem 2. (NSC of optimonotony) Let m be an interestingness measure, and (
D , X  m ) its adapted function of measure. m is optimonotone iff  X  m increases with the second variable (associated with supp ( P ) ).
 Proof. Sufficiency. Let  X  m increases with the second variable, P be an itemset, c a class attribute, X an item /  X  P ,and supp ( P  X  c )= supp ( PX  X  c ), we show m ( PX  X  c )  X  m ( P  X  c ). Since supp ( P )  X  supp ( PX ), we write: Necessity. We want to prove that, given two elements ( x, y, z )and( x, y ,z )of D such that y  X  y ,wehave  X  m ( x, y ,z )  X   X  m ( x, y, z ) . Let n be an integer such that n  X  x, n  X  y , n  X  y and n  X  z are integers. We construct the database DB of table 2, in which the supports are the followings: supp ( P  X  c )= x = supp ( PX  X  c ) ,supp ( c )= z, supp ( P )= y, supp ( PX )= y . Thus, we do have the equality supp ( PX  X  c )= supp ( P  X  c ), and deduce, following the hypothesis of im-plication, that m ( PX  X  c )  X  m ( P  X  c ). Since ( D,  X  m ) is a function of measure adapted to m ,  X  m ( x, y ,z )  X   X  m ( x, y, z ) . Therefore,  X  m does increase with the second variable. 4.2 Classification of the Measures We thus now have a NSC for the optimonotony. This condition is applied to 32 different measures. The results are pr esented in table 3. These results show that 26 measures are compatible with the optimonotony, making this pruning strategy very interesting.
 Example 2. In table 3 we can notice that, in contrary to the original theorem 1 [20], Klosgen measure K doesn X  X  verify the property. A 3-attributes-database terexample. We have P ( PX  X  c )= P ( P  X  c ) but K ( P  X  c )=  X  0 . 096 &lt; 0and K ( PX  X  c ) = 0 i.e. K ( PX  X  c ) K ( P  X  c ). Measures may possess many algorithmic properties, that have not been ex-ploited. We here propose a formal framework for the analytical study of measures and use it to study the optimonotony property we define. We then demonstrate a NSC for optimonotony. Finally we show that many measures are optimonotone and thus could be used with the underlying pruning strategy.

