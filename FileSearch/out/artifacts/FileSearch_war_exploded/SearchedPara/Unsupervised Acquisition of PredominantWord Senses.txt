 University of Sussex University of Sussex University of Sussex University of Sussex or automatically classified. 1. Introduction
In word sense disambiguation, the  X  X irst sense X  heuristic (choosing the first, or predom-inant sense of a word) is used by most state-of-the-art systems as a back-off method when information from the context is not sufficient to make a more informed choice. predominant senses for words from raw text (McCarthy et al. 2004a).
 observation that the more prevalent a sense of a word, the more neighbors will relate to that sense, and the higher their distributional similarity scores will be. The senses of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation and Edmonds 2004). The distributional strength of the neighbors is associated with the senses of a word using a measure of semantic similarity which relies on the relationships between word senses, such as hyponyms (available in an inventory such as WordNet) or overlap in the definitions of word senses (available in most dictionaries), or both. motivation behind the first sense heuristic, and a full description of our method. We extend previously reported work in a number of different directions: 554 dominant sense information in WSD systems and the need for acquiring this information automatically. In Section 3 we give an overview of related work in acquisition of prior sense distributions and domain-specific sense information. Section 4 work reported in this article. Section 6 describes four experiments. The first evaluates the first sense heuristic using predominant sense information acquired for all PoS on
SemCor; for nouns we compare two semantic similarity methods and three different method. The second experiment compares the performance of the automatic method to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking particularly at nouns which have a low frequency in SemCor. The third uses corpora in restricted domains and the subject field code gold standard of Magnini and Cavagli ` a (2000) to investigate the potential for domain-specific rankings for different PoS. The as the test data, and (2) where the training data is harvested automatically from another corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss directions for future work (Section 8). 2. Motivation
The problem of disambiguating the meanings of words in text has received much attention recently, particularly since the inception of the Senseval evaluation exercises (Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).
One of the standard Senseval tasks (the  X  X ll words X  task) is to tag each open class word with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum 1998). The most accurate word sense disambiguation ( machine learning approaches (Stevenson and Wilks 2001), trained on text which has and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The largest all words sense tagged corpus is SemCor, which is 220,000 words taken from 103 passages, each of about 2,000 words, from the Brown corpus (Francis and Ku  X  cera 1979) and the complete text of a 19th-century American novel, The Red Badge of Courage , which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and these have been linked to WordNet senses by human taggers using a software interface.
Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make use of hand-tagged data (in some form or other) performed substantially worse than the best 1 two supervised (S) and unsupervised (U) 2 systems for the English all words and English lexical sample for Senseval-2 3 and -3, along with the first sense baseline (FS) reported by the task organizers. 4 This is a simple application of the  X  X irst sense X  heuristic X  X hat is, using the most common sense of a word for every instance of it in the test corpus, regardless of context. Although contextual WSD baseline is a very powerful one and unsupervised systems find it surprisingly hard to beat (indeed, some of the systems that report themselves as unsupervised actually make some use of a manually obtained first-sense heuristic). Considering both precision and heuristic as derived from SemCor (61.5% 5 ), and then by only a few percentage points (the top system scoring 65% precision and recall) despite using hand-tagged training data available from SemCor and previous Senseval data sets, large sets of contextual features, and sophisticated machine learning algorithms.
 are useful for applications and where systems can be confident of high precision. In cases where systems are less confident, but word senses, rather than words, are needed, the first sense heuristic is a powerful back-off strategy. This strategy is dependent on information provided in dictionaries. Two dictionaries that have been used by English WSD systems are the Longman Dictionary of Contemporary English ( 556 Senseval-2 S 69.0 69.0 64.2 64.2 Senseval-2 S 63.6 63.6 63.8 63.8 Senseval-2 U 45.1 45.1 40.2 40.1 Senseval-2 U 36.0 36.0 58.1 31.9 FS baseline 57.0 57.0 47.6 47.6 Senseval-3 S 65.1 65.1 72.9 72.9 Senseval-3 S 65.1 64.2 72.6 72.6 Senseval-3 U 58.3 58.2 66.1 65.7 Senseval-3 U 55.7 54.6 56.3 56.3
FS baseline 61.5 61.5 55.2 55.2 1978) and WordNet (Fellbaum 1998). These both provide a ranking of senses accord-ing to their predominance. The sense ordering in LDOCE intuition, whereas in WordNet the senses are ordered according to their frequency in SemCor (Miller et al. 1993).
 disambiguating popular news stories then celebrity would be more likely. Domain, topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al. 2002) dictionary will provide only a single sense ranking, whether this is derived from sense-tagged data as in WordNet, lexicographer intuition as in LDOCE data as in the Oxford Advanced Learner X  X  Dictionary (Hornby 1989). A fixed order of senses may not reflect the data that an NLP system is dealing with.
 use of hand-tagged resources, such as SemCor. Such resources are relatively small due to the cost of manual tagging (Kilgarriff 1998). Many words will simply not be covered, or occur only a few times. For many words in WordNet the ordering of word senses is based on a very small number of occurrences in SemCor. For example, the first sense of tiger is an audacious person whereas most people would assume the carnivorous animal sense is more prevalent. This is because the two senses each occur exactly once in SemCor, and when there is no frequency information to break the tie the WordNet sense ordering is assigned arbitrarily. There are many fairly common words (such as percentage of words 6 in WordNet and the BNC which do not occur in SemCor. As one would expect from Zipf X  X  law, a substantial number of words do not occur in SemCor, even when we do not consider multiwords. Many of these words are extremely rare, but
PoS No. % No. % noun 43,781 81.9 360,535 97.5 verb 4,741 56.4 25,292 87.6 adjective 14,991 72.3 95,908 95.4 adverb 2,405 64.4 10,223 89.2
PoS No. % No. % No. % No. % No. % No. % noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7 verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4 adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5 adverb 1 0.9 1 0.9 2 1.8  X   X   X   X   X   X  in any given document it is likely that there will be at least some words without SemCor the percentage of polysemous word types with no frequency information in SemCor, the percentage with zero or one occurrences, and the percentage with up to five occurrences. (For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7% in Senseval-3, have five or fewer occurrences in SemCor.) Thus, although SemCor may cover many frequently occurring word types in a given document, there are likely to be a substantial proportion for which there is very little or no information available. within the six documents making up the Senseval-2 and -3 all-words test data. They document, and the extent to which this is the same as that given by SemCor. The two tables share a common format: columns 2 X 5 give percentages over all  X  X ocument/word type X  combinations. The second column shows the percentage of the  X  X ocument/word type X  combinations where the word is used in the document in only one of its senses.
The fourth column shows the same percentage but for  X  X ocument/word type X  combi-nations where the word is used in more than one sense in the document. The third and fifth columns give the percentage of the words in the preceding columns (second and
SemCor (FS = SC FS). For the third column, this is the only sense that this word appears in within the document. (Note that for any row, columns 2 and 4 account for all possibil-ities so will always add up to 100.) The sixth column gives the mean degree of polysemy, according to WordNet, for the set of words that these figures are calculated for. 558
PoS % FS = SC FS % % FS = SC FS % Mean polysemy noun 72.2 52.2 27.8 7.3 5.9 verb 45.6 25.1 54.4 16.9 12.7 adjective 62.9 40.5 37.1 10.3 4.8 adverb 64.7 50.0 35.3 17.6 4.7 document 7 is strongest for nouns, although adverbs and adjectives also tend towards one sense. Verbs are on average much more polysemous than the other parts of speech yet still 45.6% of polysemous verbs which occur more than once are used in only a single sense. However, because verbs are in general more polysemous, it makes it less likely SemCor.
 curring more than once), showing the accuracy of a SemCor-derived first-sense heuristic for words with a frequency below a specified threshold (column 1) in SemCor. The table shows that although having a first sense from SemCor is certainly useful, when looking senses other than the one indicated by SemCor. Furthermore, the lower the frequency in SemCor the more likely that the first sense indicated by SemCor is wrong. (However, frequency in SemCor and they are on average not very polysemous, so for them a first sense derived from a resource like SemCor X  X here one exists X  X s possibly sufficient.) words for which its coverage is inadequate. In addition, few languages have extensive hand-tagged resources or sense orderings produced by lexicographers. Moreover, gen-eral resources containing word sense information are not likely to be appropriate when processing language for a wider variety of domains, topics, and genres. What is needed is a means to find predominant senses automatically. 3. Related Work
Most research in WSD to date has concentrated on using contextual features, typically neighboring words, to help infer the correct sense of a target word. In contrast, our work is aimed at discovering the predominant sense of a word from raw text because
Frequency % FS = SC FS % % FS = SC FS % Mean polysemy  X  1 (54) 96.3 24.1 3.7 0.0 2.8  X  5 (118) 96.6 43.2 3.4 0.0 3.2  X  10 (191) 96.9 48.7 3.1 0.0 3.3 all (792) 88.8 51.6 11.2 2.5 5.5  X  1 (21) 100.0 33.3 0.0 0.0 2.4  X  5 (64) 98.4 35.9 1.6 1.6 3.2  X  10 (110) 98.2 38.2 1.8 1.8 3.5 all (671) 82.6 39.3 17.4 5.1 9.0  X  1 (31) 93.5 19.4 6.5 0.0 2.5  X  5 (83) 95.2 34.9 4.8 1.2 2.7  X  10 (120) 90.8 40.8 9.2 1.7 2.8 all (385) 82.6 46.2 17.4 3.6 5.1  X  1 (1) 0.0 0.0 100.0 0.0 2.0  X  5 (2) 50.0 50.0 50.0 0.0 2.0  X  10 (8) 87.5 62.5 12.5 0.0 2.3 all (111) 82.9 62.2 17.1 5.4 4.0 the first sense heuristic is so powerful, and because manually sense-tagged data is not always available.
 used syntactic evidence to find a prior distribution for Levin (1993) verb classes, and classes directly from subcategorization evidence in a parsed corpus, whereas we use word which reflect the different senses of the word and have associated distributional
We would, however, agree that subcategorization evidence should be very useful for disambiguating verbs, and would hope to combine such evidence with our ranking models for context-based WSD .
 senses for any desired domain and text type. Buitelaar and Sacaleanu (2001) explored taken from information retrieval. Buitelaar and Sacaleanu evaluated their method on identifying domain-specific concepts using human judgments on 100 items. We evaluate 560 and Carroll 2005). Magnini and Cavagli ` a (2000) associated WordNet word senses with particular domains, and this has proved useful for high precision al. 2001); indeed, we have used their domain labels (or subject field codes, and required a considerable amount of hand-labeling. Our approach requires only raw text from the given domain and because of this it can easily be applied to a new domain or sense inventory, as long as there is enough appropriate text.
 raw data. Gliozzo, Giuliano, and Strapparava (2005) induced domain models from raw data using unsupervised latent semantic models and then fed this into a supervised
WSD model and evaluated on Senseval-3 lexical sample data in four languages. Chan and Ng (2005) obtained probability distributions to feed into their supervised els. They used multilingual parallel corpus data to provide probability estimates for a subset of 22 nouns from the lexical sample task. They then fed this into a supervised model and verified that the estimates for prior distributions improved performance for supervised WSD models. Although unsupervised models seem to be beaten whenever there is training data to be had, we anticipate that unsupervised models with improved priors from the ranking might outperform supervised systems in situations where there for back-off in a WSD system, the method could be applied to finding a prior distribution duced by our prevalence ranking, either as predominant senses or prior distributions over word senses, could be combined with contextual information for from corpora which makes use of the category information in the Macquarie Thesaurus.
Evaluation is performed on an artificially constructed test set from unambiguous words in the same category as the 27 test words (nouns, verbs, and adjectives). The senses of the words are the categories of the thesaurus and the experiment uses only two senses of each word, the two most predominant ones. The predominance of the two senses is altered systematically. The results are encouraging because a much smaller amount of corpus data is needed compared to our approach. However, their method has only been has yet to be applied in a domain-specific setting, which is the chief motivation of our work.
 the word. Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluate the discovered classes with respect to WordNet as a gold standard. The CBC method
WordNet) and a recall of 51% (the percentage of senses discovered from the union of those discovered with different clustering algorithms that they tried). cedure, predominant senses have more of a chance of being found than other senses, are assigned to a cluster so that less frequent senses can be discovered. Our method, target in a predefined inventory (we use WordNet). We rank the senses using a measure which sums over the distributional similarity of neighbors weighted by the strength of the association between the neighbors and the sense. This is done on the assumption that more prevalent senses will have strong associations with more nearest neighbors thesaurus. Both the number and the distributional similarity of the neighbors are used in our prevalence ranking measure. Pantel and Lin process the possible clusters in order of their average distributional similarity and number of neighbors but do not take the of the cluster with the target word because their aim is one of sense discovery. Their measure is the similarity between the cluster and the target word and does not retain there is a low frequency sense of a target word with synonyms that form a nice cohesive group.
 ranking score, intuition suggests that a combination of the quantity and distributional predominance of senses. In Section 6 we test this hypothesis using a simplified version in order to evaluate acquisition of predominance information we have used publicly available gold-standard sense-tagged corpora, and these have WordNet senses. CBC will not always find WordNet senses. For example, using the on-line demonstration of
CBC, 9 several common senses from nouns from the Senseval-2 lexical sample are not discovered, including the upright object sense of post ,the block of something sense of bar ,the daytime sense of day and the meaning of the word sense of the word sense .
Automatic acquisition of sense inventories is an important endeavor, and we hope to look at ways of combining our method for detecting predominance with automatically induced inventories such as those produced by CBC. Evaluation of induced inventories should be done in the context of an application, because the senses will be keyed to the acquisition corpus and not to WordNet.
 present in a predefined inventory. Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns. 562 4. Method
In our method, the predominant sense for a target word is determined from a preva-lence ranking of the possible senses for that word. The senses come from a predefined derived using a distributional thesaurus automatically produced from a large corpus, and a semantic similarity measure defined over the sense inventory. The distributional thesaurus contains a set of words that are  X  X earest neighbors X  to the target word with is based on the hypothesis of Harris, 1968, that words which occur in similar contexts have related meanings.) The thesaurus assigns a distributional similarity score to each neighbor word, indicating its closeness to the target word. For example, the nearest neighbors of sandwich might be: and the nearest neighbors of the polysemous noun star 11 might be:
These neighbors reflect the various senses of the word, which for star might be:
We assume that the number and distributional similarity scores of neighbors pertaining to a given sense of a target word will reflect the prevalence of that sense in the corpus from which the thesaurus was derived. This is because the more prevalent senses of the word will appear more frequently and in more contexts than other, less prevalent senses.
The neighbors of the target word relate to its senses, but are themselves word forms rather than senses. The senses of the target word are predefined in a sense inventory and we use a semantic similarity score defined over the sense inventory to relate the neighbors to the various senses of the target word. The two semantic similarity scores that we use in this article are implemented in the WordNet similarity package. One uses the overlap in definitions of word senses, based on Lesk (1986), and the other uses a combination of corpus statistics and the WordNet hyponym hierarchy, based on Jiang and Conrath (1997). We describe these fully in Section 4.2. We now describe intuitively the measure for ranking the senses according to predominance, and then give a more formal definition.
 neighbors. This total is divided between the senses of the target word by apportioning the distributional similarity of each neighbor to the senses. The contribution of each neighbors count for more. The distributional similarity score of each neighbor is divided between the various senses rather than attributing the neighbor to only one sense. This is done because neighbors can relate to more than one sense due to relationships such
Section 4.1 acquired from the BNC , chicken has neighbors duck and goose which relate to both the meat and animal senses. We apportion the contribution of a neighbor to each of the word senses according to a weight which is the normalized semantic similarity score between the sense and the neighbor. We normalize the semantic similarity scores because some of the semantic similarity scores that we use, described in Section 4.2, can get disproportionately large. Because we normalize the semantic similarity scores, the sum of the ranking scores for a word equals the sum of the distributional similarity scores. To summarize, we rank the senses of the target word, such as star , by apportion-ing the distributional similarity scores of the top k neighbors between the senses. Each in turn and obtain a prevalence score. Let N w = { n 1 , n top scoring k neighbors of w from the distributional thesaurus with associated scores { 564 and n j . sss is the maximum WordNet similarity score ( sss ) between s n where
We describe dss and sss in Sections 4.1 and 4.2. Note that the dss for a given neighbor normalized sss . 4.1 The Distributional Similarity Score
Measures of distributional similarity take into account the shared contexts of the two words. Several measures of distributional similarity have been described in the litera-ture. In our experiments, dss is computed using Lin X  X  similarity measure (Lin 1998a).
We set the number of nearest neighbors to equal 50. 14 We use three different sources of data for our first two experiments, resulting in three distributional thesauruses. These are described in the next section. We use domain-specific data for our third and fourth experiments. The data sources for these are described in Sections 6.3 and 6.4. where each feature is a pair r , x consisting of a grammatical relation name and the words of the same PoS where each word X  X  total feature frequency was at least 10. A the-literature (see Weeds 2003 for a review) and comparing them is outside the scope of this work. However, the study of Weeds and Weir (2005) provides interesting insights into what makes a  X  X ood X  distributional similarity measure in the contexts of semantic simi-larity prediction and language modeling. In particular, weighting features by pointwise mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise mutual information ( I ( w , f )) between a word and a feature is calculated as
Intuitively, this means that the occurrence of a less-common feature is more important in describing a word than a more-common feature. For example, the verb eat is more selective and tells us more about the meaning of its arguments than the verb be . it is an unparameterized measure which uses pointwise mutual information to weight features and which has been shown (Weeds 2003) to be highly competitive in making predictions of semantic similarity. This measure is based on Lin X  X  information-theoretic similarity theorem (Lin 1997): similarity between two words, w and n ,is
However, due to this choice of dss and the openness of the domain, we restrict ourselves do show that distributional similarity can be computed for lower frequency words but this is using a highly specialized corpus of 400,000 words from the biomedical domain.
Further, it has been shown (Weeds et al. 2005; Weeds and Weir 2005) that performance of Lin X  X  distributional similarity score decreases more significantly than other measures for low frequency nouns. We leave the investigation of other distributional similarity scores and the application to smaller corpora as areas for further study. 4.2 The Semantic Similarity Scores
WordNet is widely used for research in WSD because it is publicly available and there are a number of associated sense-tagged corpora (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes. WordNet; for sss we experiment with two of these, as implemented in the WordNet Similarity Package (Patwardhan and Pedersen 2003).
 et al. (2004b) experimented with six of these for the sss used in the prevalence score,
Equation (2). In the experiments reported here we use the two scores that performed and Pedersen (2003) give a more detailed discussion. The scores measure the similarity between two WordNet senses ( s 1and s 2). lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-566 jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes text. We used raw data from the BNC to create the IC files. There are various parameters that can be set in the WordNet Similarity Package when creating these files; we used the RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop words provided with the package, and no smoothing.
 only to nouns and verbs because it relies on IC counts which are obtained using the hyponym links and these only exist for nouns and verbs. 15 jcn for verbs because in previous experiments (McCarthy et al. 2004c) the lesk measure outperformed jcn because the structure of the hyponym hierarchy is very shallow for verbs and the measure is therefore considerably less informative for verbs than it is for nouns. 4.3 An Example
We illustrate the application of our measure with an example. For star ,ifweset and have the dss for the previously given neighbors as in the first row of Table 6, and
Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03) celebrity 0.42 0.53 0.02 0.01 celestial body 0.01 0.01 0.68 0.10 shape 0.0 0.0 0.02 0.78 zodiac 0.03 0.03 0.21 0.01
Total 0.46 0.57 0.93 0.90 the sss between the senses and the neighbors as in the remaining rows, the prevalence score for celebrity would be:
The prevalence score for each of the senses would be: so the method would select celebrity as the predominant sense. 5. Experimental Setup 5.1 The Distributional Thesauruses
The three thesauruses used in our first two experiments were all created automatically from raw corpus data, based either on grammatical relations between words computed by syntactic parsers or alternatively on word proximity relations.
 produced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words of the  X  X ritten X  portion of the British National Corpus (Leech 1992), for all polysemous nouns, verbs, adjectives, and adverbs in WordNet. For each word we considered co-occurring words in the grammatical contexts listed in Table 7.
 distributional thesauruses, produced by Dekang Lin from 125M words of text from the
Wall Street Journal , San Jose Mercury News , and AP Newswire, using the same similarity measure. The thesauruses are publicly available. 17 One was constructed based on word 568
PoS Grammatical contexts noun verb in direct object or subject relation, adjective or noun modifier verb noun as direct object or subject adjective modified noun, modifying adverb adverb modified adjective or verb
PoS Thesaurus types NISC NITH noun BNC 7,090 2,436 115 noun DEP 6,583 2,176 217 noun PROX 6,582 2,176 217 verb BNC 2,958 553 45 adjective BNC 3,659 1,208 123 adverb BNC 505 132 38 similarities computed from syntactic dependencies produced by and the other was constructed based on textual proximity relationships between words. We refer below to the original corpus as NEWSWIRE , and these two thesauruses as and PROX , respectively. We restricted our experiments to the nouns in these thesauruses. words contained in these thesauruses, the number of words in SemCor that were not were not in SemCor ( NISC ).
 method as that proposed for the BNC thesaurus, however the data source is different and is described in those sections. 5.2 The Sense Inventory
We use WordNet version 1.6 as the sense inventory for our first three experiments, and 1.7.1 for our last experiment. 18
Pedersen 2003). 6. Experiments
In this section we describe four experiments using our method for acquiring predomi-nant sense information.
 parts of speech, using SemCor as the test corpus. This extends previous work which had only evaluated all PoS on Senseval-2 (Cotton et al. 2001) and Senseval-3 (Mihalcea and Edmonds 2004) data. The SemCor corpus is composed of 220,000 words, in contrast to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words). We examine the effects of using the two different semantic similarity scores that performed tions. We compare three thesauruses: one is derived from the
NEWSWIRE corpus. The two from the NEWSWIRE corpus examine the requirement for a parser by contrasting results obtained when the thesaurus is built using parsed data compared to a proximity approach. We contrast the results of the a simplified version of the prevalence score which uses the number of the k neighbors closest to a sense for ranking without using the dss and without sharing the credit for of words for which a predominant sense was found that differed from that given by
SemCor, identifying and giving an indication of the frequencies of the main sources of error.
 in this experiment we explore the benefits of an automatic first sense heuristic when there is inadequate data in available resources. Although McCarthy et al. (2004c) show that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor outperforms the automatic method, we look at whether the method X  X  performance is relatively stronger on words for which there is little data in SemCor. This is important because, as we have shown in Table 5, low frequency words are used often in senses other than the sense that is ranked first according to SemCor.
 frequency will depend on the domain of the data. In the third experiment, we revisit some previous work on noun senses and domain (McCarthy et al. 2004a) using corpora of news text about sports and finance. Using distributional thesauruses computed from potential for computing domain-specific predominant senses for parts of speech other than nouns.
 dominant senses, the fourth experiment compares results when we train and test on domain-specific corpora, where the training data is (1) manually categorized for domain and from the same corpus as the gold-standard test data, and (2) where the training data is harvested automatically from another corpus which is categorized automatically. 6.1 Experiment 1: All Parts of Speech
In this experiment, we evaluate the accuracy of automatically acquired predominant senses for all open class parts of speech, taking SemCor as the gold standard. For nouns We use the three distributional thesauruses BNC , DEP ,and 570 of SemCor, rather than the processed data provided in the cntlist file in the WordNet distribution. The released SemCor files contain only the tagged data from the Brown
Corpus and do not include data from The Red Badge of Courage .Weusethereleaseddata rather than that in cntlist because this includes the actual tagged examples which are marked for genre by the Brown files. We envisage the possibility of further experiments with these genre markers. We only evaluate on instances where a single, unique sense is supplied by the annotators. So, for example, we ignore instances like the following with multiple wnsn values:
We also only evaluate on polysemous words (according to WordNet) having one sense in SemCor which is more frequent than any other, and for which both SemCor and our thesauruses have at least a minimal amount of data. Specifically, a word must occur three or more times in SemCor; it must also occur in ten or more grammatical relations in the parsed version of the BNC and have neighbors in the distributional thesaurus, or be present in Dekang Lin X  X  thesaurus. 19 ber of accuracy measures, both type-based and token-based. PS word types in SemCor which have one sense which occurs more than any other. It is the accuracy of identifying the predominant sense in SemCor. If the automatic ranking has a tie for the top ranked sense then we score that word as incorrect. in the thesaurus. | correct typ | is the number of these where the automatically acquired predominant sense matches the first sense in SemCor.
 by using the first sense heuristic with the automatically acquired predominant sense information, in cases where there was a unique automatic top ranked sense:
SemCor having an automatically acquired first sense ( SCtokens from the test data itself. RBL is the random baseline for the word X  X  senses.
 similarity measures ( lesk and jcn ), the BNC and DEP thesauruses, and the
PROX thesauruses using the  X  2 test of significance with one degree of freedom (Siegel similarity measures and thesauruses for the type-based measure PS
The differences between lesk and jcn are significant for the token-based measure WSD for both the BNC and PROX thesauruses (both p &lt; .001), however not when comparing lesk and jcn for the DEP thesaurus. Although lesk is more accurate than jcn ,atleaston the WSD task, jcn is much faster because of the precompilation of IC in the WordNet similarity package; however, lesk has the additional benefit of being applicable to other parts of speech. The method gives particularly good results for adjectives, given that they have a similar random baseline to nouns. It does not do so well for adverbs and verbs, but still performs well above the random baseline which is low for verbs due to their high degree of polysemy. Given that the first sense heuristic from SemCor is particularly strong for adverbs, it is disappointing that the automatic method does not adverbs are often less strongly associated to the verbs that they modify than adjectives are to the nouns that they modify, so the distributional thesaurus information is less thesaurus and also in WordNet.

PoS Settings No. PS acc PS acc BL No. WSD sc SC FS RBL noun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7 noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6 noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6 noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7 noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6 noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6 verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1 adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9 adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0 572
Even though the differences between jcn DEP and jcn PROX are significant, the absolute differences are nevertheless relatively small; this bodes well for applying the automatic predominant sense method to languages less well resourced than English, because between jcn BNC and jcn DEP for nouns are statistically significant ( p &lt;. 001). better accuracy with DEP may be because the NEWSWIRE corpus is larger than the results between lesk BNC and lesk DEP for nouns are not significant. associate each neighbor with just one sense and use the number of neighbors associated where each sense s i  X  senses ( w ) is assigned a score as follows: where Prevalence Score of 0 because it does not have the highest sss for any of the neighbors.
This supports our intuition that a combination of both the number of neighbors and their distributional similarity scores is important for determining predominance. The
Equation (1). 6.1.2 Error Analysis. We took a random sample of 80 words that occurred more than five times in SemCor, 20 words for each PoS, from those where the automatically identified predominant sense was different from the SemCor first sense when using the lesk sss call the automatically identified sense AUTO FS, and the SemCor sense SemCor FS. We
PoS Settings No. PS acc PS acc BL No. WSD sc SC FS RBL noun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7 noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8 verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4 adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8 adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0 manually inspected the data for each of the words to find the source of the problem. we examined the parses, grammatical relations, and sense definitions for the words to see why the AUTO FS was ranked above the SemCor FS. We found the following main types of error: 22 corpora The difference appears to be due to genuine divergence between the related The automatic predominant sense is closely related to the SemCor first sense. competing Two or more related senses are ranked highly but they are overtaken by 574 neighbors There are not many neighbors related to the sense. There can be various spurious similarity The WordNet similarity scores were misled by spurious relation-differences between the training (BNC) and testing (SemCor) corpora are not a major comprising only 200,000 words) is not large enough to build a thesaurus with entries for a reasonable portion of the words, we did build a thesaurus from the entire Brown in the Brown data (to make the results more comparable, because the corpora are of such different sizes). We also show the average results for 10 random selections of a 1 million word random sample of the BNC . To do this we randomly selected tuples. 24 The differences in the WSD sc for the BNC 1 90 significant ( p &lt; .01 on the  X  2 ), but the differences in PS the entire BNC produced better results than the Brown data, this is undoubtedly due to the difference in size of the corpus. Taking a comparably sized sample, the results are slightly better from Brown which is the corpus from which SemCor is taken. higher ranking simply because the credit for some neighbors associated with another served for other parts of speech, possibly because the AUTO the SemCor FS.
 of such problems is due to the ambiguity of the neighbor; in the future we will look at reducing this source of error by removing neighbors which have a value for s
Equation (2) which is not the same as that preferred by the other senses of the target word ( w ). For adverbs, all the cases that were categorized as spurious similarity were also noted to be related to the SemCor FS, though they were not categorized as related as this was not considered the primary cause of the error.
 often have senses that are related. Furthermore, the structure of the WordNet verb tro-ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there corpora 12 1 1 5 related 812 13 8 41 competing 20 0 0 2 neighbors 43 2 2 11 spurious similarity 53 4 9 21
Thesaurus PS acc % WSD sc % full BNC 53.8 44.9 BNC 46.6 40.8
Brown 47.2 41.7 are more possibilities for spurious similarities from overlap of glosses. So, although we tried to identify the main problem source, for verbs the problems usually arose from a combination of factors and the relatedness of the senses was usually one of these. have been various attempts to group WordNet senses both manually and automati-cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum 2007). Indeed, McCarthy demonstrated that distributional and semantic similarity can sense heuristics, including the automatic method proposed here.

For example, the noun bar has 11 senses in WordNet 1.6. These include the pub sense as well as the counter sense and these are related to a certain extent. One might want to group them when acquiring predominant senses, but there may be situations where they should be distinguished. For example, if one were to ask a robot to  X  X o to the bar X  one would hope it could use the context to go get the drinks rather than replying that it may be helpful to have a coarse-grained prior and then use contextual features to tease apart subtle sense distinctions.
 neighbors ) was not a major source of error, but still causes some problems. One possible remedy might be to identify these cases by looking for neighbors which relate strongly to a sense which none of the other neighbors relate to and weighting the contribution from these neighbors more. This may however give rise to further errors because of the noise introduced by focusing on individual neighbors. We will explore such directions in future work. 576 we hope to use the score for probability estimation, and combine this with contextual information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng (2005). 6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic acquired predominant sense information. We carried out the evaluation with respect to
SemCor in order to have as much test data as possible. To obtain reasonably reliable gold-standard first-sense data and first-sense heuristic upper bounds, we limited the evaluation to words occurring at least three times in SemCor. Clearly this scenario is
SemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNet do not occur at all in SemCor. Thus, even for English, which has substantial manually sense-tagged resources, coverage is severely limited for many words.
 now change to a different test corpus, the Senseval-2 English all-words task data set. We focus on nouns and evaluate using all words regardless of their frequencies in SemCor.
We examine the effect of frequency in SemCor on performance of a SemCor-derived heuristic in comparison to results from our automatic method on the same words. Our hypothesis is that although automatically acquired predominant sense information may not outperform first-sense data obtained from a hand-tagged resource over all words in a text, the information may well be more accurate for low frequency items. 2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense numbers) with 1.6 sense numbers. As well as examining the performance of our method with two or more occurrences in the Senseval-2 data and where one sense occurs more than any of the others. We calculate type, precision, and recall, using this Senseval-2 first-sense as the gold standard. The recall measure is the same as PS previously, except that we include items which do not have entries in the thesaurus, items with a joint automatic ranking. We also calculate token precision and recall (
These measures relate to WSD sc , but again, recall includes words not in the thesaurus which are scored incorrect, and precision does not include items with a joint automatic ranking. We also separately compute WSD precision for words not in SemCor ( The results are shown in Table 13. 26 in the table) are approaching the SemCor-derived results (third line from the bottom of the table). The NISC results are particularly encouraging, but with the caveat that there
Settings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%) lesk BNC 56.3 53.7 54.6 53.4 58.3 lesk DEP 52.0 47.2 52.6 48.7 58.3 lesk PROX 52.0 47.2 52.3 48.5 58.3 jcn
BNC 52.4 50.0 51.8 50.6 66.7 jcn
DEP 52.0 47.2 58.0 53.7 83.3 jcn PROX 53.1 48.1 57.3 53.1 83.3 SemCor 64.8 63.0 58.5 57.3 0.0 Senseval-2  X   X  90.8 60.1 100.0
RBL 26.5 26.5 26.0 26.0 50.0 overall figure. This is because the nouns involved are less frequent so tend to be less polysemous and consequently have a higher random baseline. There are a few nouns that are not in the automatic ranking, but this is due to the fact that neighbors were not collected for these nouns in the thesaurus because of tagging or parser errors or range of grammatical relations, or use proximity-based relations, so that neighbors can joint top ranked senses to increase coverage.
 thesaurus does well in identifying the first sense of a word (the type results), the and DEP thesauruses from the NEWSWIRE corpus return better used with the jcn measure. This is possibly because jcn works well for more frequent items due to its incorporation of frequency information, and the NEWSWIRE corpus has more data for frequent words, although coverage is not as good as the seen by the bigger differences in precision and recall and the figures in Table 8. The lower coverage may be due to the narrower domain and genre of the NEWSWIRE corpus, though spelling and capitalization differences probably also account for some differences.
 in SemCor. (The differences between the DEP and PROX thesauruses are negligible at frequencies of 10 or below, so for those we report only the results for predominant sense information than SemCor. The low number of test items at frequency when compared with SemCor ( p = .05); however the lesk WSD better ( p &lt;. 01 for the  X  1 threshold and p &lt;. 05 for the see that the automatic method, using either jcn or lesk and any of the three thesauruses, tend to give better results than SemCor on nouns which have low coverage in SemCor. items have a frequency at or below given thresholds in SemCor. Although the manually 578 produced SemCor first-sense heuristic outperforms the automatic methods over all the comparable and in some cases competitive with the SemCor heuristic. 6.3 Experiment 3: The Influence of Domain
In this experiment, we investigate the potential of the automatic ranking method for computing predominant senses with respect to particular domains. We have previ-ously demonstrated that the method produces intuitive domain-specific models for nouns (McCarthy et al. 2004a), and that these can be more accurate than first senses de-rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).
Here we investigate the behavior for other parts of speech, using a similar experimental setup to that of McCarthy et al. That work used the subject field codes ( and Cavagli ` a 2000) 27 as a gold standard. In SFC annotated with at least one domain label, selected from a set of about 200 labels. These labels are organized in a tree structure. Each synset of WordNet 1.6 is labeled with one 580 has subcategories such as art , religion ,and psychology . Some subcategories are further divided in subcategories (e.g., dance , music ,and theatre are subcategories of art ). for finding predominant senses. The corpora were obtained from the Reuters Corpus, Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes. The two domain-specific corpora were: SPORTS (Reuters topic code GSPO ), 9.1 million words FINANCE (Reuters topic codes ECAT and MCAT ), 32.5 million words least one synset with an economy SFC label and one with a sport acquired from the SPORTS corpus and more economy labels assigned to those from the
FINANCE corpus. The predominant senses from both domains had a similarly high percentage of factotum (domain-independent) labels. We reproduce the results here (in
Figure 4) for ease of reference, and for comparison with other results presented in this section. The y -axis in this figure shows the percentage of the predominant sense labels for these 38 nouns that have the SFC label indicated by the x -axis.
 although we suspected that these would show less domain-specific tendencies and there would be fewer candidate words to work with. The SFC polysemous words (excluding multiwords) in the various parts of speech are shown in
Table 15. We see from the distribution of factotum labels across the parts of speech that nouns are certainly the PoS most likely to be influenced by domain.
 synset with a sport label and one with an economy label. There were 20 such verbs but the experiment only with verbs. To do this we used the SPORTS as before, computing thesauruses for verbs using the grammatical relations specified in Table 7. The results for the distribution of domain labels of the predominant senses adjective factotum 67.8 adverb factotum 81.4 582 acquired from the SPORTS and FINANCE corpora are shown in Figure 5. We see the same tendency for sport labels for predominant senses from the less marked compared with nouns because of the high proportions of factotum senses in both corpora for verbs. We believe that acquisition of domain-specific predominant senses should be focused on those words which show domain-specific tendencies. We hope to put more work into automatic detection of these tendencies using indicators such as domain salience and words that have different sense rankings in a given domain compared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005). 6.4 Experiment 4: Domain-Specific Predominant Sense Acquisition
In the final set of experiments we evaluate the acquired predominant senses for domain-specific corpora. The first of the two experiments was reported by Koeling, McCarthy, and Carroll (2005), but we extend it by the second experiment reported subsequently.
Because there are no publicly available domain-specific manually sense-tagged corpora, we created our own gold standard. The two chosen domains ( the domain-neutral corpus ( BNC ) are the same as we used in the previous experiment.
We selected 40 words and we sampled (randomly) sentences containing these words from the three corpora and asked annotators to choose the correct sense for the target words. The set consists of 17 words which have at least one sense assigned an economy receiver, running ; eight words that are particular salient in the BNC 40.7 43.3 33.2 FINANCE 39.1 49.9 24.0 SPORTS 25.7 19.7 43.7 Random BL 19.8 19.6 19.4 SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8) standard.
 domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on data from the same source. In a second experiment we build a text classifier, use the text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the
Reuters corpus. The second experiment eliminates issues about dependencies between training and test data and will shed light on the question of how robust the acquired predominant sense method is with respect to noise in the input data. At the same time, the second experiment paves the way towards creating predominant sense inventories for any conceivable domain. 6.4.1 Experiment Using Hand-Labeled Data. In this section we focus on the predominant sense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).
After running the predominant sense finding algorithms on the raw text of the two do-main corpora ( SPORTS and FINANCE ) and the domain-neutral corpus ( the accuracy of performing WSD on the sample of 40 words purely with the first sense heuristic using all nine combinations of training and test corpora. The results (as given in Table 16) are compared with a random baseline ( X  X andom BL X ) using the first sense heuristic from SemCor ( X  X emCor FS X ). senses are acquired using the appropriate domain (i.e., test and training data from the same domain). Moreover, when trained on the domain-relevant corpora, the random baseline as well as the baseline provided by SemCor are comfortably beaten. It can be observed from these results that apparently the BNC is more similar to the corpus than it is to the SPORTS corpus. The results for the results for the FINANCE domain by almost 6 percentage points. This could be because 584 BNC 40.7 43.3 33.2 FINANCE 38.2 44.0 29.0 SPORTS 27.0 23.4 45.0 Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8) of the smaller amount of training data available (32M words versus 9M words), but it could also be an artifact of this particular selection of words. 6.4.2 Experiment Using Automatically Classified Data. Although the previous experiment shows that it is possible to acquire domain-specific predominant senses successfully, the usefulness of doing this will be far greater if there is no need to classify corpora with respect to domain by hand. There is no such thing as a standard domain specification advantageous if we could automatically obtain a user-/application-specific corpus from which to acquire predominant senses.
 extracted bags of domain-specific words from WordNet for all the defined domains by collecting all the word senses (synsets) and corresponding glosses associated with each domain label. These bags of words are the fingerprints for the domains and we used them to train a Support Vector Machine ( SVM ) text classifier using TwentyOne. all the classes (domains) it recognizes and an associated confidence score reflecting the certainty that the document belongs to that particular domain. We classified 10 months X  worth of data from the English Gigaword Corpus using this classifier and assigned each document to the corpus belonging to the highest scoring class of the classifier X  X  output. The level of confidence was ignored at this stage.
 corpus of about 27M words. The predominant sense finding algorithm was run on the raw text of these two corpora and we followed exactly the same evaluation strategy as in the previous section. The results are summarized in Table 17 and are very similar to those based on hand-labeled corpora. Again, the best results are obtained when test and training data are derived from the same domain. The is slightly worse, but is still well above both Random and the SemCor baseline. The
SPORTS  X  SPORTS result has slightly improved over the result reported in the previous section. The reason for these differences may well be because the the hand-labeled experiment.
 corpora. This experiment to test the robustness of our method for finding predominant noise and will allow us to find the right balance between corpus size and corpus quality. 7. Conclusions
In this article we have argued that information on the predominant sense of words is important, and that it is desirable to be able to infer this automatically from unlabeled text. We presented a number of evaluations investigating various facets of a previously proposed method for automatically acquiring this information (McCarthy et al. 2004a).
The evaluations extend ones in previous publications in a number of ways: they use larger, balanced test data sets, and they compare alternative semantic similarity scores and distributional thesauruses derived from different corpora and based on different kinds of relations. We also looked in detail at areas where the method performs well and also where it does not, and carried out a manual error analysis to identify the types of mistakes it makes.
 586 8. Further Work We are continuing to work on automatic ranking of word senses for will be to use the numeric values of sense prevalence scores to compare the skews in the distributions of word senses across different corpora and see if this enables us to detect automatically words for which a domain-or genre-specific ranking is warranted. Looking at skews should also help in predicting words for which contextual approach to unsupervised context-based WSD which uses the collocates of the distri-butional neighbors associated with each of the senses as contextual features. caused by the sense distinctions in WordNet being particularly fine-grained. We have recently (Koeling and McCarthy 2007) evaluated our method on the coarse-grained
English all words task at SemEval (Navigli, Litkowski, and Hargraves 2007). We will fol-low work on finding relationships between WordNet senses to induce coarser-grained classes (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002) and adapt our method to acquire prevalence rankings for these. The granularity of the inventories for WSD within the context of a task, such as lexical substitution (McCarthy and Navigli 2007).
 approach to other languages for which sense tagged resources of the size of SemCor are not available. Given the good results with Lin X  X  proximity based thesaurus we believe our method should work even for languages which do not have high quality parsers available.
 Acknowledgments 588
