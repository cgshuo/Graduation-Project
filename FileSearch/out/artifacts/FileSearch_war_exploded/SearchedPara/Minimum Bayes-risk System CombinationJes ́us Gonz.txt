 Once statistical models are trained, a decoding ap-proach determines what translations are finally se-lected. Two parallel lines of research have shown consistent improvements over the max X  X erivation decoding objective, which selects the highest prob-ability derivation. Consensus decoding procedures select translations for a single system with a mini-mum Bayes risk (MBR) (Kumar and Byrne, 2004). System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best frag-ments of these outputs (Frederking and Nirenburg, 1994). In this paper, we present minimum Bayes risk system combination, a technique that unifies these two approaches by learning a consensus trans-lation over multiple underlying component systems.
MBR system combination operates directly on the outputs of the component models. We perform an MBR decoding using a linear combination of the component models X  probability distributions. In-stead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target lan-guage. By using a loss function based on BLEU (Pa-pineni et al., 2002), we avoid the hypothesis align-ment problem that is central to standard system com-bination approaches (Rosti et al., 2007). MBR sys-tem combination assumes only that each translation model can produce expectations of n -gram counts; the latent derivation structures of the component sys-tems can differ arbitrary. This flexibility allows us to combine a great variety of SMT systems.

The key contributions of this paper are three: the usage of a linear combination of distributions within the MBR decoding, which allows multiple SMT models to be involved in, and makes the computa-tion of n -grams statistics to be more accurate; the decoding in an extended search space, which allows to find better hypotheses than the evidences pro-vided by the component models; and the use of an expected BLEU score instead of the sentence-wise BLEU, which allows to efficiently apply MBR de-coding in the huge search space under consideration.
We evaluate in a multi-source translation task ob-taining improvements of up to +2 . 0 BLEU abs. over the best single system max-derivation, and state-of-the-art performance in the system combination task of the ACL 2010 workshop on SMT. MBR system combination is a multi-system gener-alization of MBR decoding where the space of hy-potheses is not constrained to the space of evidences. We expand the space of hypotheses following some underlying ideas of system combination techniques. 2.1 Minimum Bayes risk In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N -best list of translations produced by a first pass de-coder (Kumar and Byrne, 2004). Different tech-niques to widen the search space have been de-scribed (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). These works extend the traditional MBR algorithms based on N -best lists to work with lattices.

The use of MBR to combine the outputs of vari-ous MT systems has also been explored previously. Duan et al. (2010) present an MBR decoding that makes use of a mixture of different SMT systems to improve translation accuracy. Our technique differs in that we use a linear combination instead of a mix-ture, which avoids the problem of component sys-tems not sharing the same search space; perform the decoding in a search space larger than the outputs of the component models; and optimize an expected BLEU score instead of the linear approximation to it described in (Tromble et al., 2008).

DeNero et al. (2010) present model combination , a multi-system lattice MBR decoding on the con-joined evidences spaces of the component systems. Our technique differs in that we perform the search in an extended search space not restricted to the pro-vided evidences, have fewer parameters to learn, and optimizes an expected BLEU score instead of the linear BLEU approximation.

Another MBR-related technique to combine the outputs of various MT systems was presented by Gonz  X  alez-Rubio and Casacuberta (2010). They use different median string (Fu, 1982) algorithms to combine various machine translation systems. Our approach differs in that we take into account the pos-terior distribution over translations instead of con-sidering each translation equally likely, optimize the expected BLEU score instead of a sentence-wise measure such as the edit distance or the sentence-level BLEU, and take into account the quality dif-ferences by associating a tunable scaling factor to each system. 2.2 System Combination System combination techniques in MT take as in-put the outputs { e 1 ,  X  X  X  , e N } of N translation sys-tems, where e n is a structured translation object (or N -best lists thereof), typically viewed as a se-quence of words. The dominant approach in the field chooses a primary translation e p as a backbone, then finds an alignment a n to the backbone for each e . A new search space is constructed from these backbone-aligned outputs and then a voting proce-dure of feature-based model predicts a final consen-sus translation (Rosti et al., 2007). MBR system combination entirely avoids this alignment prob-lem by considering hypotheses as n -gram occur-rence vectors rather than word sequences. MBR sys-tem combination performs the decoding in a larger search space and includes statistics from the compo-nents X  posteriors, whereas system combination tech-niques typically do not.

Despite these advantages, system combination may be more appropriate in some settings. In par-ticular, MBR system combination is designed pri-marily for statistical systems that generate N -best or lattice outputs. MBR system combination can in-tegrate non-statistical systems that generate either a single or an unweighted output. However, we would not expect the same strong performance from MBR system combination in these constrained settings. MBR decoding aims to find the candidate hypothesis that has the least expected loss under a probability model (Bickel and Doksum, 1977). We begin with a review of MBR for SMT.

SMT can be described as a mapping of a word se-quence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D ( f ) . If the reference translation e is known, the decoder performance can be mea-sured by the loss function L ( e , D ( f )) . Given such a loss function L ( e , e 0 ) between an automatic transla-tion e 0 and a reference e , and an underlying proba-bility model P ( e | f ) , MBR decoding has the follow-ing form (Goel and Byrne, 2000; Kumar and Byrne, 2004): where R ( e 0 ) denotes the Bayes risk of candidate translation e 0 under loss function L , and E repre-sents the space of translations.

If the loss function between any two hypotheses can be bounded: L ( e , e 0 )  X  L max , the MBR de-coder can be rewritten in term of a similarity func-tion S ( e , e 0 ) = L max  X  X  ( e , e 0 ) . In this case, in-stead of minimizing the Bayes risk, we maximize the Bayes gain G ( e 0 ) :
MBR decoding can use different spaces for hy-pothesis selection and gain computation ( arg max and summatory in Eq. (4)). Therefore, the MBR de-coder can be more generally written as follows: where E h refers to the hypotheses space form where the translations are chosen and E e refers to the evi-dences space that is used to compute the Bayes gain. We will investigate the expansion of the hypotheses space while keeping the evidences space as provided by the decoder. MBR system combination is a multi-system gener-alization of MBR decoding. It uses the MBR de-cision rule on a linear combination of the probabil-ity distributions of the component systems. Unlike existing MBR decoding methods that re-rank trans-lation outputs, MBR system combination search for the minimum risk hypotheses on the complete set of finite-length hypotheses over the output vocabulary. We assume the component systems to be statistically independent and define the Bayes gain as a linear combination of the Bayes gains of the components. Each system provides its own space of evidences D n ( f ) and its posterior distribution over translations P n ( e | f ) . Given a sentence f in the source language, MBR system combination is written as follows:  X  e = arg max where N is the total number of component systems, E h represents the hypotheses space where the search is performed, G n ( e 0 ) is the Bayes gain of hypothe-sis e 0 given by the n th component system and  X  n is a scaling factor introduced to take into account the differences in quality of the component models. It is worth mentioning that by using a linear combination instead of a mixture model, we avoid the problem of component systems not sharing the same search space (Duan et al., 2010).

MBR system combination parameters training and decoding in the extended hypotheses space are described below. 4.1 Model Training We learn the scaling factors in Eq. (8) using min-imum error rate training (MERT) (Och, 2003). MERT maximizes the translation quality of  X  e on a held-out set, according to an evaluation metric that compares to a reference set. We used BLEU, choos-ing the scaling factors to maximize BLEU score of the set of translations predicted by MBR sys-tem combination. We perform the maximization by means of the down-hill simplex algorithm (Nelder and Mead, 1965). 4.2 Model Decoding In most MBR algorithms, the hypotheses space is equal to the evidences space. Following the underly-ing idea of system combination, we are interested in extend the hypotheses space by including new sen-tences created using fragments of the hypotheses in the evidences spaces of the component models. We perform the search ( argmax operation in Eq. (8)) Algorithm 1 MBR system combination decoding. Require: Initial hypothesis e Require: Vocabulary the evidences  X  1:  X  e  X  e 2: repeat 4: for j = 1 to | e cur | do 6: for a  X   X  do 7: e 0 s  X  Substitute ( e cur ,a,j ) 8: if G ( e 0 s ) &gt; G (  X  e s ) then 10:  X  e d  X  Delete ( e cur ,j ) 12: for a  X   X  do 13: e 0 i  X  Insert ( e cur ,a,j ) 14: if G ( e 0 i ) &gt; G (  X  e i ) then 17: until G (  X  e ) 6 &gt; G ( e cur ) Ensure: G ( e cur )  X G ( e ) using the approximate median string (AMS) algo-rithm (Mart  X   X nez et al., 2000). AMS algorithm per-form a search on a hypotheses space equal to the free monoid  X   X  of the vocabulary of the evidences  X  = V oc ( E e ) .
 The AMS algorithm is shown in Algorithm 1. AMS starts with an initial hypothesis e that is mod-ified using edit operations until there is no improve-ment in the Bayes gain (Lines 3  X  16 ). On each posi-tion j of the current solution e cur , we apply all the possible single edit operations: substitution of the j th word of e cur by each word a in the vocabulary (Lines 5  X  9 ), deletion of the j th word of e cur (Line 10 ) and insertion of each word a in the vocabulary in the j th position of e cur (Lines 11  X  15 ). If the Bayes gain of any of the new edited hypotheses is higher than the Bayes gain of the current hypothesis (Line 17 ), we repeat the loop with this new hypotheses  X  e , in other case, we return the current hypothesis.
AMS algorithm takes as input an initial hypothe-sis e and the combined vocabulary of the evidences spaces  X  . Its output is a possibly new hypothesis whose Bayes gain is assured to be higher or equal than the Bayes gain of the initial hypothesis. The complexity of the main loop (lines 2 -17 ) is O ( | e cur | X |  X  | X  C G ) , where C G is the cost of com-puting the gain of a hypothesis, and usually only a moderate number of iterations ( &lt; 10 ) is needed to converge (Mart  X   X nez et al., 2000). We are interested in performing MBR system com-bination under BLEU. BLEU behaves as a score function: its value ranges between 0 and 1 and a larger value reflects a higher similarity. Therefore, we rewrite the gain function G (  X  ) using single evi-dence (or reference) BLEU (Papineni et al., 2002) as the similarity function:
G n ( e
BLEU = where r is the length of the evidence, c the length of the hypothesis, m k the number of n -gram matches of size k , and c k the count of n -grams of size k in the hypothesis.

The evidences space D n ( f ) may contain a huge number of hypotheses 1 which often make impracti-cal to compute Eq. (9) directly. To avoid this prob-lem, Tromble et al. (2008) propose linear BLEU , an approximation to the BLEU score to efficiently per-form MBR decoding when the search space is repre-sented with lattices. However, our hypotheses space is the full set of finite-length strings in the target vo-cabulary and can not be represented in a lattice.
In Eq. (9), we have one hypothesis e 0 that is to be compared to a set of evidences e  X  D n ( f ) which follow a probability distribution P n ( e | f ) . Instead of computing the expected BLEU score by calcu-lating the BLEU score with respect to each of the evidences, our approach will be to use the expected n -gram counts and sentence length of the evidences to compute a single-reference BLEU score. We re-place the reference statistics ( r and m n in Eq. (10)) by the expected statistics ( r 0 and m 0 n ) given the pos-terior distribution P n ( e | f ) over the evidences:
G n ( e where N k ( e 0 ) is the set of n -grams of size k in the hypothesis, C e 0 ( ng ) is the count of the n -gram ng in the hypothesis and C 0 ( ng ) is the expected count of ng in the evidences. To compute the n -gram match-ings m 0 k , the count of each n -gram is truncated, if necessary, to not exceed the expected count for that n -gram in the evidences.

We have replaced a summation over a possibly ex-ponential number of items ( e 0  X  D n ( f ) in Eq. (9)) with a summation over a polynomial number of n -grams that occur in the evidences 2 . Both, the ex-pected length of the evidences r 0 and their expected n -gram counts m 0 k can be pre-computed efficiently from N -best lists and translation lattices (Kumar et al., 2009; DeNero et al., 2010). We report results on a multi-source translation task. From the Europarl corpus released for the ACL 2006 workshop on MT (WMT2006), we se-lect those sentence pairs from the German X  X nglish (de X  X n), Spanish X  X nglish (es X  X n) and French X  English (fr X  X n) sub-corpora that share the same En-glish translation. We obtain a multi-source corpus with German, Spanish and French as source lan-guages and English as target language. All the ex-periments were carried out with the lowercased and tokenized version of this corpus.

We report results using BLEU (Papineni et al., 2002) and translation edit rate (Snover et al., 2006) (TER). We measure statistical significance using de  X  en es  X  en fr  X  en Best MAX 30.9  X  53.3  X  30.8  X  53.4  X  95% confidence intervals computed using paired bootstrap re-sampling (Zhang and Vogel, 2004). In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 6.1 Base Systems We combine outputs from three systems, each one translating from one source language (German, Spanish or French) into English. Each individual system is a phrase-based system trained using the Moses toolkit (Koehn et al., 2007). The parame-ters of the systems were tuned using MERT (Och, 2003) to optimize BLEU on the development set. Each base system yields state-of-the-art perfor-mance, summarized in Table 1. For each system, we report the performance of max-derivation decod-ing (MAX) and 1000 -best 3 MBR decoding (Kumar and Byrne, 2004). 6.2 Experimental Results Table 2 compares MBR system combination (MBR-SC) to the best MAX and MBR systems. Both Best Best MBR 30.9 53.4 MBR-SC Expected 30.9 53.5 MBR-SC E/Conjoin 32.4 52.1 MBR-SC E/C/evidences-best 30.9 53.5 MBR-SC E/C/hypotheses-best 31.8 52.5 MBR-SC E/C/Extended 32.7 52.3 MBR-SC E/C/Ex/MERT 32.8 52.3 MBR and MBR-SC were computed on 1000 -best lists. MBR-SC uses expected BLEU as gain func-tion using the conjoined evidences spaces of the three systems to compute expected BLEU statistics. It performs the search in the free monoid of the out-put vocabulary, and its model parameters were tuned using MERT on the development set. This is the standard setup for MBR system combination, and we refer to it as MBR-SC-E/C/Ex/MERT in Table 3. MBR system combination improves single Best MAX system by +2 . 0 BLEU points in test, and al-ways improves over MBR. This improvement could arise due to multiple reasons: the expected BLEU gain, the larger evidences space, the extended hy-potheses space, or the MERT tuned scaling factor values. Table 3 teases apart these contributions. We first apply MBR-SC to the best system (MBR-SC-Expected). Best MBR and MBR-SC-Expected differ only in the gain function: MBR uses sentence level BLEU while MBR-SC-Expected uses the ex-pected BLEU gain described in Section 5. MBR-SC-Expected performance is comparable to MBR decoding on the 1000 -best list from the single best system. The expected BLEU approximation per-forms as well as sentence-level BLEU and addition-ally requires less total computation.

We now extend the evidences space to the con-joined 1000 -best lists (MBR-SC-E/Conjoin). MBR-SC-E/Conjoin is much better than the best MBR on a single system. This implies that either the ex-pected BLEU statistics computed in the conjoined evidences space are stronger or the larger conjoined evidences spaces introduce better hypotheses.
When we restrict the BLEU statistics to be com-puted from only the best system X  X  evidences space (MBR-SC-E/C/evidences-best), BLEU scores dra-matically decrease relative to MBR-SC-E/Conjoin. This implies that the expected BLEU statistics com-puted over the conjoined 1000 -best lists are stronger than the corresponding statistics from the single best system. On the other hand, if we restrict the search space to only the 1000 -best list of the best sys-tem (MBR-SC-E/C/hypotheses-best), BLEU scores also decrease relative to MBR-SC-E/Conjoin. This implies that the conjoined search space also con-tains better hypotheses than the single best system X  X  search space.

These results validate our approach. The linear combination of the probability distributions in the conjoined evidences spaces allows to compute much stronger statistics for the expected BLEU gain and also contains some better hypotheses than the single best system X  X  search space does.

We next expand the conjoined evidences spaces using the decoding algorithm described in Sec-tion 4.2 (MBR-SC-E/C/Extended). In this case, the expected BLEU statistics are computed from the conjoined 1000 -best lists of the three systems, but the hypotheses space where we perform the decod-ing is expanded to the set of all possible finite-length hypotheses over the vocabulary of the evi-dences. We take the output of MBR-SC-E/Conjoin as the initial hypotheses of the decoding (see Algo-rithm 1). MBR-SC-E/C/Extended improves BLEU score of MBR-SC-E/Conjoin but obtains a slightly worse TER score. Since these two systems are iden-tical in their expected BLEU statistics, the improve-ments in BLEU imply that the extended search space has introduced better hypotheses. The degradation in TER performance can be explained by the use of a BLEU-based gain function in the decoding process.
We finally compute the optimum values for the scaling factors of the different system us-ing MERT (MBR-SC-E/C/Ex/MERT). MBR-SC-E/C/Ex/MERT slightly improves BLEU score of MBR-SC-E/C/Extended. This implies that the op-timal values of the scaling factors do not deviate much from 1 . 0 ; a similar result was reported in (Och and Ney, 2001). We hypothesize that this is because the three component systems share the same SMT model, pre-process and decoding. We expect to ob-tain larger improvements when combining systems implementing different MT paradigms. MBR-SC-E/C/Ex/MERT is the standard setup for MBR system combination and, from now, on we will refer to it as MBR-SC.

We next evaluate performance of MBR system combination on N -best lists of increasing sizes, and compare it to MBR-SC-E/C/Extended and MBR-SC-E/Conjoin in the same N -best lists. We list the results of the Best MAX system for comparison.
Results in Figure 1 confirm the conclusions ex-tracted from results displayed in Table 3. MBR-SC-Conjoin is consistently better than the Best MAX system, and differences in BLEU increase with the size of the evidences space. This implies that the linear combination of posterior probabilities al-low to compute stronger statistics for the expected BLEU gain, and, in addition, the larger the evi-dences space is, the stronger the computed statistics are. MBR-SC-C/Extended is also consistently better than MBR-SC-Conjoin with an almost constant im-provement of +0 . 4 BLEU points. This result show that the extended search space always contains bet-ter hypotheses than the conjoined evidences spaces; also confirms the soundness of Algorithm 1 that al-lows to reach them. Finally, MBR-SC also slightly improves MBR-SC-C/Extended. The optimization of the scaling factors allows only small improve-ments in BLEU.

Figure 2 display the MBR system combination translation and compare it to the max-derivation translations of the three component systems. Refer-ence translation is also listed for comparison. MBR-SC adds word  X  point  X  to create a new translation equal to the reference. MBR-SC is able to detect that this is valuable word even though it does not appear in the max-derivation hypotheses. 6.3 Comparison to System Combination Figure 3 compares MBR system combination (MBR-SC) with state-of-the-art system combination techniques presented to the system combination task of the ACL 2010 workshop on MT (WMT2010). All system combination techniques build a  X  X ord sausage X  from the outputs of the different compo-nent systems and choose a path trough the sausage with the highest score under different models. A de-scription of these systems can be found in (Callison-Burch et al., 2010).

In this task, the output of the component systems are single hypotheses or unweighted lists thereof. Therefore, we lack of the statistics of the com-ponents X  posteriors which is one of the main ad-vantages of MBR system combination over sys-tem combination techniques. However, we find that, even in these constrained setting, MBR system com-bination performance is similar to the best sys-tem combination techniques for all translation di-rections. These experiments validate our approach. MBR system combination yields state-of-the-art performance while avoiding the challenge of align-ing translation hypotheses. MBR system combination integrates consensus de-coding and system combination into a unified multi-system MBR technique. MBR system combination uses the MBR decision rule on a linear combina-tion of the component systems X  probability distri-butions to search for the sentence with the mini-mum Bayes risk on the complete set of finite-length strings in the output vocabulary. Component sys-tems can have varied decoding strategies; we only require that each system produce an N -best list (or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, Leusch et al. (2010) generate intermediate transla-tions in several pivot languages, translate them sep-arately into the target language, and generate a con-sensus translation out of these using a system combi-nation technique. Likewise, these pivot translations could be combined via MBR system combination.
MBR system combination has two significant ad-vantages over current approaches to system combi-nation. First, it does not rely on hypothesis align-ment between outputs of individual systems. Align-ing translation hypotheses can be challenging and has a substantial effect on combination perfor-mance (He et al., 2008). Instead of aligning the sen-tences, we view the sentences as vectors of n -gram counts and compute the expected statistics of the BLEU score to compute the Bayes gain. Second, we do not need to pick a backbone system for combina-tion. Choosing a backbone system can also be chal-lenging and also affects system combination per-formance (He and Toutanova, 2009). MBR system combination sidesteps this issue by working directly on the conjoined evidences space produced by the outputs of the component systems, and allows the consensus model to express system preferences via scaling factors.

Despite its simplicity, MBR system combination provides strong performance by leveraging different consensus, decoding and training techniques. It out-performs best MAX or MBR derivation on each of the component systems. In addition, it obtains state-of-the-art performance in a constrained setting better suited for dominant system combination techniques. Work supported by the EC (FEDER/FSE) and the Spanish MEC/MICINN under the MIPRCV  X  X on-solider Ingenio 2010 X  program (CSD2007-00018), the iTrans2 (TIN2009-14511) project, the UPV under grant 20091027 and the FPU scholarship AP2006-00691. Also supported by the Spanish MITyC under the erudito.com (TSI-020110-2009-439) project and by the Generalitat Valenciana under grant Prometeo/2009/014.

