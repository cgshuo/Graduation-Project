 The efficient computation of the similarity (or overlap) between sets is a central operation in a variety of applications, such as word associations (e.g., [13]), data cleaning (e.g., [40, 9]), data mining (e.g., [14]), selectivity estimation (e.g., [30]) or duplicate document detection [3, 4]. In machine learning applications, binary (0/1) vectors can be naturally viewed as sets. For scenarios where the underlying data size is sufficiently large to make storing them (in main memory) or processing them in their entirety impractical, probabilistic techniques have been proposed for this task. Word associations (collocations, co-occurrences) If one inputs a query NIPS machine learning , all major search engines will report the number of pagehits (e.g., one reports 829,003), in addition to the top ranked URLs. Although no search engines have revealed how they estimate the numbers of pagehits, one natural approach is to treat this as a set intersection estimation problem. Each word can be represented as a set of document IDs; and each set belongs to a very large space  X  . It is expected that |  X  | &gt; 10 10 . Word associations have many other applications in Computational Linguistics [13, 38], and were recently used for Web search query reformulation and query suggestions [42, 12]. Here is another example. Commercial search engines display various form of  X  X ertical X  content (e.g., images, news, products ) as part of Web search. In order to determine from which  X  X ertical X  to display information, there exist various techniques to select verticals. Some of these (e.g., [29, 15]) use the number of documents the words in a search query occur in for different text corpora representing various verticals as features. Because this selection is invoked for all search queries Moreover, the accuracy of vertical selection depends on the number/size of document corpora that can be processed within the allotted time [29], i.e., the processing speed can directly impact quality. Now, because of the large number of word-combinations in even medium-sized text corpora (e.g., the Wikipedia corpus contains &gt; 10 7 distinct terms), it is impossible to pre-compute and store the associations for all possible multi-term combinations (e.g., &gt; 10 14 for 2-way and &gt; 10 21 for 3-way); instead the techniques described in this paper can be used for fast estimates of the co-occurrences. Database query optimization Set intersection is a routine operation in databases, employed for example during the evaluation of conjunctive selection conditions in the presence of single-column indexes. Before conducting intersections, a critical task is to (quickly) estimate the sizes of the intermediate results to plan the optimal intersection order [20, 8, 25]. For example, consider the task of intersecting four sets of record identifiers: A  X  B  X  C  X  D . Even though the final outcome will be the same, the order of the join operations, e.g., ( A  X  B )  X  ( C  X  D ) or (( A  X  B )  X  C )  X  D , can significantly affect the performance, in particular if the intermediate results, e.g., A  X  B  X  C , become too large for main memory and need to be spilled to disk. A good query plan aims to minimize the total size of intermediate results. Thus, it is highly desirable to have a mechanism which can estimate join sizes very efficiently, especially for the lower-order (2-way and 3-way) intersections, which could potentially result in much larger intermediate results than higher-order intersections. Duplicate Detection in Data Cleaning: A common task in data cleaning is the identification of duplicates (e.g., duplicate names, organizations, etc.) among a set of items. Now, despite the fact that there is considerable evidence (e.g., [10]) that reliable duplicate-detection should be based on local properties of groups of duplicates, most current approaches base their decisions on pairwise similarities between items only. This is in part due to the computational overhead associated with more complex interactions, which our approach may help to overcome.
 Clustering Most clustering techniques are based on pair-wise distances between the items to be clustered. However, there are a number of natural scenarios where the affinity relations are not pairwise, but rather triadic, tetradic or higher (e.g. [1, 43]). Again, our approach may improve the performance in these scenarios if the distance measures can be expressed in the form of set-overlap. Data mining A lot of work in data mining has focused on efficient candidate pruning in the context of pairwise associations (e.g., [14]), a number of such pruning techniques leverage minwise hashing to prune pairs of items, but in many contexts (e.g., association rules with more than 2 items) multi-way associations are relevant; here, pruning based on pairwise interactions may perform much less well than multi-way pruning. 1.1 Ultra-high dimensional data are often binary For duplicate detection in the context of Web crawling/search, each document can be represented as a set of w -shingles ( w contiguous words); w = 5 or 7 in several studies [3, 4, 17]. Normally only the abscence/presence (0/1) information is used, as a w -shingle rarely occurs more than once in a page if w  X  5 . The total number of shingles is commonly set to be |  X  | = 2 64 ; and thus the set intersection corresponds to computing the inner product in binary data vectors of 2 64 dimensions. Interestingly, even when the data are not too high-dimensional (e.g., only thousands), empirical studies [6, 23, 26] achieved good performance using SVM with binary-quantized (text or image) data. 1.2 Minwise Hashing and SimHash Two of the most widely adopted approaches for estimating set intersections are minwise hashing [3, 4] and sign (1-bit) random projections (also known as simhash ) [7, 34], which are both special instances of the general techniques proposed in the context of locality-sensitive hashing [7, 24]. These techniques have been successfully applied to many tasks in machine learning, databases, data mining, and information retrieval [18, 36, 11, 22, 16, 39, 28, 41, 27, 5, 2, 37, 7, 24, 21]. Limitations of random projections The method of random projections (including simhash) is limited to estimating pairwise similarities. Random projections convert any data distributions to (zero-mean) multivariate normals, whose density functions are determined by the covariance matrix which contains only the pairwise information of the original data. This is a serious limitation. 1.3 Prior work on b-Bit Minwise Hashing Instead of storing each hashed value using 64 bits as in prior studies, e.g., [17], [35] suggested to store only the lowest b bits. [35] demonstrated that using b = 1 reduces the storage space at least by a factor of 21.3 (for a given accuracy) compared to b = 64 , if one is interested in resemblance  X  0 . 5 , the threshold used in prior studies [3, 4]. Moreover, by choosing the value b of bits to be retained, it becomes possible to systematically adjust the degree to which the estimator is  X  X uned X  towards higher similarities as well as the amount of hashing (random permutations) required. [35] concerned only the pairwise resemblance. To extend it to the multi-way case, we have to solve new and challenging probability problems. Compared to the pairwise case, our new estimator is significantly different. In fact, as we will show later, estimating 3-way resemblance requires b  X  2 . 1.4 Notation Fig. 1 describes the notation used in 3-way intersections for three sets S 1 , S 2 , S 3  X   X  , |  X  | = D . We define three 2-way resemblances ( R 12 , R 13 , R 23 ) and one 3-way resemblance ( R ) as: which, using our notation, can be expressed in various forms: assumed to be known, we can compute resemblances from intersections and vice versa:
Thus, estimating resemblances and estimating intersection sizes are two closely related problems. 1.5 Our Main Contributions The recommended procedure for estimating 3-way resemblances (in sparse data) is shown as Alg. 1. Algorithm 1 The b -bit minwise hashing algorithm, applied to estimating 3-way resemblances in a collection of N sets. This procedure is suitable for sparse data, i.e., r i = f i /D  X  0 . Minwise hashing applies k random permutations  X  j :  X   X  X  X   X  ,  X  = { 0 , 1 , ..., D  X  1 } , and then estimates R 12 (and similarly other 2-way resemblances) using the following probability: This method naturally extends to estimating 3-way resemblances for three sets S 1 , S 2 , S 3  X   X  : To describe b -bit hashing, we define the minimum values under  X  and their lowest b bits to be: To estimate R , we need to computes the empirical estimates of the probabilities P ij,b and P b , where The main theoretical task is to derive P b . The prior work[35] already derived P ij,b ; see Appendix A. To simplify the algebra, we assume that D is large, which is virtually always satisfied in practice. Theorem 1 Assume D is large.
 where u = r 1 + r 2 + r 3  X  s 12  X  s 13  X  s 23 + s , and Theorem 1 naturally suggests an iterative estimation procedure, by writing Eq. (6) as s = P b u  X  Z . Figure 2: P b , for verifying the probability formula in Theorem 1. The empirical estimates and the theoretical predictions essentially overlap regardless of the sparsity measure r i = f i /D . A Simulation Study For the purpose of verifying Theorem 1, we use three sets corresponding to the occurrences of three common words ( X  X F X ,  X  X ND X , and  X  X R X ) in a chunk of real world Web crawl data. Each (word) set is a set of document (Web page) IDs which contained that word at least once. The three sets are not too sparse and D = 2 16 suffices to represent their elements. The r i = f i D values are 0.5697, 0.5537, and 0.3564, respectively. The true 3-way resemblance is R = 0 . 47 . We can also increase D by mapping these sets into a larger space using a random mapping, with D = 2 16 , 2 18 , 2 20 , or 2 22 . When D = 2 22 , the r i values are 0 . 0089 , 0.0087, 0.0056. Fig. 2 presents the empirical estimates of the probability P b , together with the theoretical predictions by Theorem 1. The empirical estimates essentially overlap the theoretical predictions. Even though the proof assumes D  X  X  X  , D does not have to be too large for Theorem 1 to be accurate. The basic probability formula (Theorem 1) we derive could be too complicated for practical use. To obtain a simpler formula, we leverage the observation that in practice we often have r i = f i D  X  0 , even though both f i and D can be very large. For example, consider web duplicate detection [17]. Here, D = 2 64 , which means that even for a web page with f i = 2 54 shingles (corresponding to the can be still large. Recall the resemblances (2) and (3) are only determined by these ratios. We analyzed the distribution of f i D using two real-life datasets: the UCI dataset containing 3  X  10 5 NYTimes articles; and a Microsoft proprietary dataset with 10 6 news articles [19]. For the UCI-NYTimes dataset, each document was already processed as a set of single words. For the anonymous dataset, we report results using three different representations: single words (1-shingle), 2-shingles (two contiguous words), and 3-shingles. Table 1 reports the summary statistics of the f i D values. For truly large-scale applications, prior studies [3, 4, 17] commonly used 5-shingles. This means that real world data may be significantly more sparse than the values reported in Table 1. 3.1 The Simplified Probability Formula and the Practical Estimator Theorem 2 Assume D is large. Let T = R 12 + R 13 + R 23 . As r 1 , r 2 , r 3  X  0 , Interestingly, if b = 1 , then P 1 = 1 4 (1 + T ) , i.e., no information about the 3-way resemblance R is contained. Hence, it is necessary to use b  X  2 to estimate 3-way similarities.
 Alg. 1 uses  X  P b and  X  P ij,b to respectively denote the empirical estimates of the theoretical probabilities P b and P ij,b . Assuming r 1 , r 2 , r 3  X  0 , the proposed estimator of R , denoted by Theorem 3 Assume D is large and r 1 , r 2 , r 3  X  0 . Then  X  R b in (8) is unbiased with the variance It is interesting to examine several special cases: 3.2 Simulations for Validating Theorem 3 We now present a simulation study for verifying Theorem 3, using the same three sets used in Fig. 2. Fig. 3 presents the resulting empirical biases: E (  X  R b )  X  R b . Fig. 4 presents the empirical mean square errors (MSE = bias 2 +variance) together with the theoretical variances V ar (  X  R b ) in Theorem 3. Figure 3: Bias of  X  R b (8). We used 3 (word) sets:  X  X F X ,  X  X ND X , and  X  X R X  and four D values: 2 16 , 2 18 , 2 20 , and 2 22 . We conducted experiments using b = 2 , 3 , and 4 as well as the original minwise hashing (denoted by  X  X  X ). The plots verify that as r i decreases (to zero), the biases vanish. Note that the set sizes f i remain the same, but the relative values r i = f i D decrease as D increases. Figure 4: MSE of  X  R b (8). The solid curves are the empirical MSEs (=var+bias 2 ) and the dashed lines are the theoretical variances (9), under the assumption of r i  X  0 . Ideally, we would like to see the solid and dashed lines overlap. When D = 2 20 and D = 2 22 , even though the r i values are not too small, the solid and dashed lines almost overlap. Note that, at the same sample size k , we always estimator. We can see that, V ar (  X  R 3 ) and V ar (  X  R 4 ) are very close to V ar (  X  R M ) . We can summarize the results in Fig. 3 and Fig. 4 as follows: While we believe the simple estimator in (8) and Alg. 1 should suffice in most applications, we demonstrate here that the sparsity assumption of r i  X  0 is not essential if one is willing to use the more sophisticated estimation procedure provided by Theorem 1.
 By Eq. (6), s = P b u  X  Z , where Z contains s , s ij , r i etc. We first estimate s ij (from the estimated R ij ) using the precise formula for the two-way case; see Appendix A. We then iteratively solve for s using the initial guess provided by the estimator  X  R b in (8). Usually a few iterations suffice. Fig. 5 reports the bias (left most panel, only for D = 2 16 ) and MSE, corresponding to Fig. 3 and Fig. 4. In Fig. 5, the solid curves are obtained using the precise estimation procedure by Theorem 1. The dashed curves are the estimates using the simplified estimator  X  R b which assumes r i  X  0 . Even when the data are not sparse, the precise estimation procedure provides unbiased estimates as verified by the leftmost panel of Fig. 5. Using the precise procedure results in noticeably more accurate estimates in non-sparse data, as verified by the second panel of Fig. 5. However, as long as the data are reasonably sparse (the right two panels), the simple estimator  X  R b in (8) is accurate. Figure 5: The bias (leftmost panel) and MSE of the precise estimation procedure, using the same data used in Fig. 3 and Fig. 4. The dashed curves correspond to the estimates using the simplified estimator  X  R b in (8) which assumes r i  X  0 . This section is devoted to analyzing the improvements of b -bit minwise hashing, compared to using 64 bits for each hashed value. Throughout the paper, we use the terms  X  sample  X  and  X  sample size  X  (denoted by k ). The original minwise hashing stores each  X  X ample X  using 64 bits (as in [17]). For b -bit minwise hashing, we store each  X  X ample X  using b bits only. Note that V ar (  X  R 64 ) and V ar (  X  R M ) (the variance of the original minwise hashing) are numerically indistinguishable.
 As we decrease b , the space needed for each sample will be smaller; the estimation variance at the same sample size k , however, will increase. This variance-space trade-off can be quantified by B ( b ) = b  X  Var ratio B (64) B ( b ) precisely characterizes the improvements of b -bit hashing compared to using 64 bits. Fig. 6 confirms the substantial improvements of b -bit hashing over the original minwise hashing using 64 bits. The improvements in terms of the storage space are usually 10 (or 15) to 25-fold when the sets are reasonably similar (i.e., when the 3-way resemblance &gt; 0 . 1 ). When the three sets are very similar (e.g., the top left panel), the improvement will be even 25 to 30-fold. bits. Since the variance (9) contains both R and T = R 12 + R 13 + R 23 , we compare variances using different T/R ratios. As 3 R  X  T always, we let T =  X R , for some  X   X  3 . Since T  X  3 , we know R  X  3 / X  . Practical applications are often interested in cases with reasonably large R values. We conducted a duplicate detection experiment on a public (UCI) collection of 300,000 NYTimes news articles. The task is to identify 3-groups with 3-way resemblance R exceeding a threshold R 0 . We used a subset of the data; the total number of 3-groups is about one billion. We experimented with b = 2 , 4 and the original minwise hashing. Fig. 7 presents the precision curves for a represen-tative set of thresholds R 0  X  X . Just like in [35], the recall curves are not shown because they could not differentiate estimators. These curves confirm the significant improvement of using b -bit minwise hashing when the threshold R 0 is quite high (e.g., 0.3). In fact, when R 0 = 0 . 3 , using b = 4 re-sulted in similar precisions as using the original minwise hashing (i.e., a 64/4=16-fold reduction in storage). Even when R 0 = 0 . 1 , using b = 4 can still achieve similar precisions as using the original minwise hashing by only slightly increasing the sample size k . Figure 7: Precision curves on the UCI collection of news data. The task is to retrieve news article 3-groups with resemblance R  X  R 0 . For example, consider R 0 = 0 . 2 . To achieve a precision of at least 0.8, 2-bit hashing and 4-bit hashing require about k = 500 samples and k = 260 samples respectively, while the original minwise hashing (denoted by M ) requires about 170 samples. Computing set similarities is fundamental in many applications. In machine learning, high-dimensional binary data are common and are equivalent to sets. This study is devoted to simul-taneously estimating 2-way and 3-way similarities using b -bit minwise hashing. Compared to the prior work on estimating 2-way resemblance [35], the extension to 3-way is important for many application scenarios (as described in Sec. 1) and is technically non-trivial.
 For estimating 3-way resemblance, our analysis shows that b -bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy, when the set similarities are not extremely low (e.g., 3-way resemblance &gt; 0 . 02 ). Many applications such as data cleaning and de-duplication are mainly concerned with relatively high set similarities. For many practical applications, the reductions in storage directly translate to improvements in pro-cessing speed as well, especially when memory latency is the main bottleneck, which, with the advent of many-core processors, is more and more common.
 Future work : We are interested in developing a b -bit version for Conditional Random Sampling (CRS) [31, 32, 33], which requires only one permutation (instead of k permutations) and naturally extends to non-binary data. CRS is also provably more accurate than minwise hashing for binary data. However, the analysis for developing the b -bit version of CRS appears to be very difficult. Theorem 4 ([35]) Assume D is large.

  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  Minwise hashing is a standard technique in the context of sea rch  X  We suggest to store only the lowest b -bits, e.g., b = 2 ( Document similarity  X  Set intersection in ultra-high dimensions. Resemblance ( R ) : R = | S 1  X  S 2 | Minwise Hashing : Apply a random permutation  X  :  X   X  X  X   X  on S Then, the two minimums are equal with a probability R . Define: z Theorem : Consequence : 3. Similar improvements in computation time.
 Tensor algebra, ....
 -bit Hashing for estimating R  X  The collision probability formula is extremely complicated .
