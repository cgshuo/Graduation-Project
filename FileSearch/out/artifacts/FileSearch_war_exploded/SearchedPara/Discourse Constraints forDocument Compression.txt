 University of Illinois at Urbana-Champaign University of Edinburgh
Sentence compression holds promise for many applications ranging from summarization to subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming. Experimen-tal results show significant improvements over a state-of-the-art discourse agnostic approach. 1. Introduction
Recent years have witnessed increasing interest in sentence compression. The task encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality. The popularity of sentence compression is largely due to its relevance for applications. Summarization is a case in point here. Most summarizers to date aim to produce informative summaries at a given compression rate. If we can have a compression component that reduces sentences to a minimal length and still retains the most important content, then we should be able to pack more information content into a fixed size summary. In other words, sentence compression would allow summarizers to increase the overall amount of information extracted used as a post-processing step in order to render summaries more coherent and less repetitive (Mani, Gates, and Bloedorn 1999).
 text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid for the blind (Grefenstette 1998). Sentence compression could also benefit information retrieval by eliminating extraneous information from the documents indexed by the retrieval engine. This way it would be possible to store less information in the index without dramatically affecting retrieval performance (Olivers and Dolan 1999). deletion, substitution, insertion, and word reordering. In practice, however, the task is commonly defined as a word deletion problem: Given an input sentence of words x = x 1 , x 2 , ... , x n , the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Many sentence compression models aim to learn dele-tion rules from a parsed parallel corpus of source sentences and their target compres-sions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007;
Cohn and Lapata 2009). For example, Knight and Marcu (2002) learn a synchronous context-free grammar (Aho and Ullman 1969) from such a corpus. The grammar rules have weights (essentially probabilities estimated using maximum likelihood) and are used to find the best compression from the set of all possible compressions for a given sentence. Other approaches exploit syntactic information without making explicit use of a parallel grammar X  X or example, by learning which words or constituents to delete from a parse tree (Riezler et al. 2003; Nguyen et al. 2004; McDonald 2006; Clarke and Lapata 2008).
 require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account. This is in marked contrast with common practice in summarization. Pro-fessional abstractors often rely on contextual cues while creating summaries (Endres-
Niggemeyer 1998). This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and
Moens 2002). Determining which information is important in a sentence is not merely a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is less likely). A variety of contextual factors can play a role, such as the discourse topic, whether the sentence introduces new entities or events that have not been mentioned before, or the reader X  X  background knowledge.
 cations which aim to create a shorter document rather than a single sentence. The resulting document must not only be grammatical but also coherent if it is to function as a replacement for the original. However, this cannot be guaranteed without knowledge of how the discourse progresses from sentence to sentence. To give a simple example, a contextually aware compression system could drop a word or phrase from the current sentence, simply because it is not mentioned anywhere else in the document and is continuity.
 for both documents and sentences. Luckily, a variety of discourse theories have been developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi 1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a context-sensitive compression model we are faced with three important questions: (1) Which type of discourse information is useful for compression? (2) Is it amenable to automatic processing (there is little hope for interfacing our compression model with applications if discourse-level cues cannot be identified robustly)? and (3) How are sentence-and document-based information best integrated in a unified modeling framework? 412 of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains (Morris and Hirst 1991). Both approaches capture local coherence  X  X he way adjacent sentences bind together to form a larger discourse. They also both share the view that discourse coherence revolves around discourse entities and the way they are intro-duced and discussed. We first automatically augment our documents with annotations pertaining to centering and lexical chains, which we subsequently use to inform our compression model. The latter is an extension of the integer linear programming for-mulation proposed by Clarke and Lapata (2008). In a nutshell, sentence compression is modeled as an optimization problem. Given a long sentence, a compression is formed by retaining the words that maximize a scoring function coupled with a small number of constraints ensuring that the resulting output is grammatical. The constraints are en-coded as linear inequalities whose solution is found using integer linear programming (ILP; Winston and Venkataramanan 2003; Vanderbei 2001). Discourse-level information can be straightforwardly incorporated by slightly changing the compression objective X  we now wish to compress entire documents rather than isolated sentences X  X nd aug-menting the constraint set with discourse-specific constraints. We use our model to compress whole documents (rather than sentences sequentially) and evaluate whether the resulting text is understandable and informative using a question-answering task.
We show that our method yields significant improvements over discourse agnostic state-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008). of related work. In Section 3 we present the ILP framework and compression model we employ in our experiments. We introduce our discourse-related extensions in Sections 4 and 5. Section 6 discusses our experimental set-up and evaluation methodology. Our results are presented in Section 7. Discussion of future work concludes the paper. 2. Related Work
Sentence compression has been extensively studied across different modeling para-digms and has received both generative and discriminative formulations. Most gen-erative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley and
McKeown 2007) are instantiations of the noisy-channel model, whereas discriminative formulations include decision-tree learning (Knight and Marcu 2002), maximum en-margin learning (McDonald 2006; Cohn and Lapata 2009). These models are trained on a parallel corpus and learn either which constituents to delete or which words to place adjacently in the compression output. Relatively few approaches dispense with the parallel corpus and generate compressions in an unsupervised manner using either a scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rules that are approximated from a non-parallel corpus such as the Penn Treebank (Turner and Charniak 2005).
 without taking into account any discourse information. However, there are two notable exceptions. Jing (2000) uses information from the local context as evidence for and against the removal of phrases during sentence compression. The idea here is that words or phrases which have more links to the surrounding context are more indicative instead the importance of each phrase is determined by the number of lexical links within the local context. A link is created between two words if they are repetitions, morphologically related, or associated in WordNet (Fellbaum 1998) through a lexical relation (e.g., hyponymy, synonymy). Links have weights X  X or example, repetition is considered more important than hypernymy. Each word is assigned a context weight based on the number of links to the local context and the importance of each relation type. Phrases are scored by the sum of their children X  X  context scores. The decision to phrase X  X  grammatical role and previous evidence from a parallel corpus.
 pression. Given a document D = w 1 , w 2 , ... , w n the goal is to produce a summary , S ,by dropping any subset of words from D . Their system uses the discourse structure of a document and the syntactic structure of each of its sentences in order to decide which words to drop. Specifically, they extend Knight and Marcu X  X  (2002) noisy-channel model so that it can be applied to entire documents. In its simpler sentence compression instan-tiation, the noisy-channel model has two components, a language model and a channel model, both of which act on probabilistic context-free grammar (PCFG) representations. Daum  X  e III and Marcu define a noisy-channel model over syntax and discourse trees.
Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent documents by trees whose leaves correspond to elementary discourse units ( edus )and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., Contrast , Elaboration ). Discourse units are further characterized in terms of their text importance: nuclei denote central seg-ments, whereas satellites denote peripheral ones. Their model therefore learns not only which syntactic constituents to drop but also which discourse units are unimportant. neously delete words and sentences from a document, the majority of summarization systems to date simply select and present to the user the most important sentences in a text (see Mani [2001] for a comprehensive overview of the methods used to achieve this). Discourse-level information plays a prominent role here as the overall document organization can indicate whether a sentence should be included in the summary. A variety of approaches have focused on cohesion (Halliday and Hasan 1976) and the devices responsible for making the elements of a text appear unified or connected.
Examples include word repetition, anaphora, ellipsis, and the use of synonyms or superordinates. The underlying assumption is that sentences connected to many other sentences are likely to carry salient information and should therefore be included in the summary (Sjorochod X  X o 1972). In exploiting cohesion for summarization, it is necessary to somehow represent cohesive ties. For instance, Boguraev and Kennedy (1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad (1997) operationalize cohesion via lexical chains  X  X equences of related words spanning a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic relations based on synonymy, antonymy, hypernymy, and holonymy (we discuss their approach in more detail in Section 4.1).
 and rhetorical relations. Documents are commonly represented as trees (Mann and
Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al. 2001) and the position of a sentence in a tree is indicative of its importance. To give an ex-ample, Marcu (2000) proposes a summarization algorithm based on RST. Assuming that nuclei are more salient than satellites, the importance of sentential or clausal units can be determined based on tree depth. Alternatively, discourse structure can be repre-sented as a graph (Wolf and Gibson 2004) and sentence importance is determined in 414 graph-theoretic terms, by using graph connectivity measures such as in-degree or
PageRank (Brin and Page 1998). Although a great deal of research in summarization has focused on global properties of discourse structure, there is evidence that local coherence may also be useful without the added complexity of computing discourse representations. (Unfortunately, discourse parsers have yet to achieve levels of perfor-mance comparable to syntactic parsers.) Teufel and Moens (2002) identify discourse relations on a sentence-by-sentence basis without presupposing an explicit discourse structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995) X  X  theory salience of discourse entities X  X r  X  asan (2003) proposes a summarization algorithm that extracts sentences with at least one entity in common. The idea here is that summaries containing sentences referring to the same entity will be more coherent. Other work has relied on centering not so much to create summaries but to assess whether they are readable (Barzilay and Lapata 2008).
 key respects. First, we present a compression model that is contextually aware; decisions on whether to remove or retain a word (or phrase) are informed by its discourse prop-erties (e.g., whether it introduces a new topic, or whether it is semantically related to the previous sentence). Unlike Jing (2000) we explicitly identify topically important words and assume specific representations of discourse structure. Secondly, in contrast to
Daum  X  e III and Marcu (2002) and other summarization work, we adopt a less global and more shallow representation of discourse based on Centering Theory and lexical chains. One of our aims is to exploit discourse features that can be computed efficiently and relatively cheaply. Thirdly, our compression model can be applied to isolated sentences as well as to entire documents. We claim the latter is more in the spirit of real-world applications where the goal is to generate a condensed and coherent text. Unlike
Daum  X  e III and Marcu (2002) our model can delete words but not sentences, although it could be used to compress documents of any type, even summaries. 3. The Compression Model
Our model is an extension of the approach put forward in Clarke and Lapata (2008) where they formulate sentence compression as an optimization problem. Given a long sentence, a compression is created by retaining the words that maximize a scoring func-tion. The latter is essentially a language model coupled with a few constraints ensuring that the resulting output is grammatical. The language model and the constraints are encoded as linear inequalities whose solution is found using ILP. text genres, while delivering state-of-the-art results (see Clarke and Lapata [2008] for details). Importantly, discourse-level information can be easily incorporated in two ways: Firstly, by applying the compression objective to entire documents rather than individual sentences; and secondly, by augmenting the constraint set with discourse-the noisy channel model) where compression is modeled by grammar rules indicating which constituents to delete in a syntactic context. Moreover, ILP delivers a globally optimal solution by searching over the entire compression space heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples).
 a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), rela-concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007).

Sections 4 X 5 present our extensions and modifications. 3.1 Language Model compression. We use x 0 to denote the  X  X tart X  token. We introduce a decision variable for each word in the source and constrain it to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes the word in the target compression. Let:
A trigram language model forms the backbone of the compression model. The language model is formulated as an integer linear program with the introduction of extra decision variables indicating which word sequences should be retained or dropped from the compression. Let:
The objective function is expressed in Equation (1). It is the sum of all possible trigrams multiplied by the appropriate decision variable where n is the length of the sentence (note all probabilities throughout this paper are log-transformed). The objective func-tion also includes a significance score I ( x i ) for each word x 416 variable for that word (see the first summation term in Equation (1)). This score high-lights important content words in a sentence and is defined in Section 3.2.
Note that we add a weighting factor,  X  , to the objective, in order to counterbalance the importance of the language model and the significance score.

As we explain shortly (Equations (7) and (8)) the compressions our model generates are subject to a prespecified compression rate. For instance we may wish to create com-pressions at a minimum rate of 40% and maximum rate of 70%. The compression rate constraint can be violated with a penalty,  X  , which applies to each word.  X  number of words under the compression rate and  X  max the number of words over the compression rate. Thus, the more the output violates the compression rate, the larger the penalty will be. In other words, the term  X  min  X   X   X  providing a means to guide the compression towards the desired rate. The violation penalty  X  is tuned experimentally and may vary depending on the desired compression rate or application.
 selected. As a result, invalid trigram sequences (e.g., two or more trigrams containing the  X  X nd X  token) could appear in the target compression. We avoid this situation by introducing sequential constraints (on the decision variables  X  restrict the set of allowable trigram combinations.

Constraint 1. Exactly one word can begin a sentence.
Constraint 2. If a word is included in the sentence it must either start the sentence or be preceded by two other words or one other word and the  X  X tart X  token x
Constraint 3. If a word is included in the sentence it must either be preceded by one word and followed by another or it must be preceded by one word and end the sentence.
Constraint 4. If a word is in the sentence it must be followed by two words or followed by one word and then the end of the sentence or it must be preceded by one word and end the sentence.
Constraint 5. Exactly one word pair can end the sentence.
Note that Equations (2) X (6) are merely well-formedness constraints and differ from the compression-specific constraints which we discuss subsequently. Any language model formulated as an ILP would require similar constraints.

Compression rate constraints. Depending on the application or the task at hand, we may require that the compressions fall within a specific compression rate. We assume here that our model is given a compression rate range, c min constraints that penalize compressions which do not fall within this range: sentence,  X  is the number of words over or under the compression rate, and c c max are the limits of the range. 3.2 Significance Score
The significance score is an attempt at capturing the gist of a sentence. The score has two components which correspond to document and sentence importance, respectively.
Given a sentence and its syntactic parse, we define I ( x 418 where x i is a topic word, f i is x i  X  X  document frequency, F sum of all topic words in the corpus; l is the number of clause constituents above x N is the deepest level of clause embedding in the parse.
 important in the document and should therefore not be dropped. The score is not applied indiscriminately to all words in a sentence but solely to topic-related words, which are approximated by nouns and verbs. This is offset by the importance of these words in the specific sentence being compressed. Intuitively, in a sentence with multiply nested clauses, more deeply embedded clauses tend to carry more semantic content. This is illustrated in Figure 1, which depicts the clause embedding for the sentence Mr
Here, the most important information is conveyed by clauses S ( if he is not reselected ), which are embedded. Accordingly, we should give more weight to words found in these clauses than in the main clause (S to enforce this is to give clauses weight proportional to the level of embedding (see the second term in Equation (9)). Therefore in Figure 1, the term clauses. We obtain syntactic information in our experiments from RASP (Briscoe and
Carroll 2002), a domain-independent, robust parsing system for English. However, any other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes. contribution of tf  X  idf versus level of embedding. Although we found in our exper-iments that the latter term was as important as tf  X  idf in producing meaningful com-pressions, there may be applications or data sets where the contribution of the two terms varies. This could be easily remedied by introducing a weighting factor. 3.3 Sentential Constraints
In its original formulation, the model also contains a small number of sentence-level constraints. Their aim is to preserve the meaning and structure of the original sentence as much as possible. The majority of constraints revolve around modification and argument structure and are defined over parse trees or grammatical relations which as mentioned earlier we extract from RASP.

Modifier Constraints. Modifier constraints ensure that relationships between head words and their modifiers remain grammatical in the compression:
Equation (10) guarantees that if we include a non-clausal modifier compression (such as an adjective or a noun) then the head of the modifier must also be included; this is repeated for determiners ( detmod ) in Equation (11).
 in the compression. For example, Equation (12) enforces not in the compression when the head is included. A similar constraint is added for possessive modifiers (e.g., his , our ), including genitives (e.g., John X  X  gift ), as shown in Equation (13).
Argument Structure Constraints. Argument structure constraints make sure that the re-sulting compression has a canonical argument structure. The first constraint (Equa-tion (14)) ensures that if a verb is present in the compression then so are its arguments, and if any of the arguments are included in the compression then the verb must also be included.
Another constraint forces the compression to contain at least one verb provided the source sentence contains one as well: 420
Other constraints apply to prepositional phrases and subordinate clauses and force the introducing term (i.e., the preposition, or subordinator) to be included in the compres-sion if any word from within the syntactic constituent is also included:
By subordinator ( SUB )wemean wh -words (e.g., who, which, how, where ), the word that , and subordinating conjunctions (e.g., after, although, because ). The reverse is also true X  that is, if the introducing term is included, at least one other word from the syntactic constituent should also be included. parse trees or dependency graphs. In the following sections we present our discourse-specific constraints. But first we discuss how we represent and automatically detect discourse-related information. 4. Discourse Representation
Obtaining an appropriate representation of discourse is the first step toward creating a compression model that exploits document-level information. Our goal is to annotate documents automatically with discourse-level information which will subsequently be used to inform our compression procedure. As mentioned in Section 2 previous summa-rization work has mainly focused on cohesion (Sjorochod X  X o 1972; Barzilay and Elhadad 1997) or global discourse structure (Marcu 2000; Daum  X  e III and Marcu 2002). We also opt for a cohesion-based representation of discourse operationalized by lexical chains (Morris and Hirst 1991). Computing global discourse structure robustly and accurately is far from trivial. For example, Daum  X  e III and Marcu (2002) employ an RST parser but find that it produces noisy output for documents containing longer sentences.
We therefore focus on the less ambitious task of characterizing local coherence X  X he way adjacent sentences bind together to form a larger discourse. Although it does not explicitly capture long distance relationships between sentences, local coherence is still an important prerequisite for maintaining global coherence. Specifically, we turn to Centering Theory (Grosz, Weinstein, and Joshi 1995) and adopt an entity-based representation of discourse.
 describe our algorithms for obtaining discourse annotations. 4.1 Lexical Chains
Lexical cohesion refers to the degree of semantic relatedness observed among lexical items in a document. The term was coined by Halliday and Hasan (1976), who observed that coherent documents tend to have more related terms or phrases than incoherent ones. A number of linguistic devices can be used to signal cohesion; these range from repetition, to synonymy, hyponymy, and meronymy. Lexical chains are a representation of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991).
There is a close relationship between discourse structure and cohesion. Related words discourse structure and can be identified through lexical chains.

For example, a document containing the chain { house , home , loft , house bly describe a situation involving a house. Documents often have multiple topics (or themes) and consequently will contain many different lexical chains. Some of these topics will be peripheral and thus represented by short chains whereas main topics will correspond to dense longer chains. Words participating in the latter chains are important for our compression task X  X hey reveal what the document is about X  X nd in all likelihood should not be deleted.
 extractive text summarization. In their approach chains of semantically related expres-sions are used to select sentences for inclusion in a summary. Their algorithm uses
WordNet (Fellbaum 1998) to build chains of nouns (and noun compounds). Nouns are considered related if they are repetitions or linked in WordNet via synonymy, antonymy, hypernymy, and holonymy. Computing lexical chains would be relatively straightforward if each word was always represented by a single sense. However, due to the high level of polysemy inherent in WordNet, algorithms developed for computing lexical chains must adopt some strategy for disambiguating word senses. For example,
Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered by selecting the sense most strongly related to existing chain members, whereas Barzilay and Elhadad (1997) consider all possible alternatives of word senses and then choose the best one among them.
 more topical, and should therefore be included in a summary. Barzilay and Elhadad (1997) rank their chains heuristically by a score based on their length and homogeneity.
They generate summaries by extracting sentences corresponding to strong chains ,that is, chains whose score is two standard deviations above the average score. Analogously, we also wish to determine which lexical chains indicate the most prevalent discourse document X  X  main focus and should therefore be retained in the compressed output.
Barzilay and Elhadad X  X  (1997) scoring function aims to identify sentences (for inclusion in a summary) that have a high concentration of chain members. In contrast, we are interested in chains that span several sentences. We thus score chains according to the number of sentences their terms occur in. For example, the hypothetical chain score of two as the terms occur only in two sentences. We assume that a chain signals a prevalent discourse topic if it occurs throughout more sentences than the average chain.
The scoring algorithm is outlined more formally as: 1. Compute the lexical chains for the document. 422 2. Score ( Chain ) = Sentences ( Chain ). 3. Discard chains for which Score ( Chain ) &lt; Average ( Score ). 4. Mark terms from the remaining chains as being the focus of the document.
We use the method of Galley and McKeown (2003) to compute lexical chains for each document. 5 It improves on Barzilay and Elhadad X  X  (1997) original algorithm by providing better word sense disambiguation and linear runtime. The algorithm pro-ceeds in three steps. Initially, a graph is built representing all possible interpretations of the document under consideration. The text is processed sequentially, comparing each word against all words previously read. If a relation exists between the senses of the current word and any possible sense of a previous word, a connection is formed between the appropriate words and senses. The strength of the connection is a function of the type of relationship and of the distance between the words in the text (in terms of words, sentences, and paragraphs). Words are represented as nodes in the graph and semantic relations as weighted edges. The relations considered by Galley and McKeown (2003) are all first-order WordNet relations, with the addition of siblings  X  X wo words are considered siblings if they are both hyponyms of the same hypernym. Next, all the strength of all connections involving that sense are summed, giving that sense a unified score. The sense with the highest unified score is chosen as the correct sense for the target word. Lastly, the lexical chains are constructed by collecting same sense words into the same chain.
 (taken from our test set). Chains are shown in oval boxes; members of the same chain have the same index. The algorithm identifies three chains in the first document: rate } , { today, day, yesterday } ,and { miles, ft } . In the second document the chains are and { month, night } , and in the third { policeman, police
As can be seen, members of a chain represent a shared concept (e.g.,  X  X ime X ,  X  X inear unit X , or  X  X erson X ). In some cases important topics are missed. For instance, in the first document no chains were created with the words lava or debris . The second document is about Mrs Allan and contains many references to her. However, because Mrs Allan is not listed in WordNet it is not possible to create any chains for this word or any of its coreferents (e.g., she, her ). A similar problem is observed in the third document where
Anderson is not included in any chain even though he is one of the main protagonists throughout the text. We next turn to Centering Theory as a means of identifying which entities are prominent in a document. 4.2 Centering Theory
Centering Theory (Grosz, Weinstein, and Joshi 1995) is an entity-orientated theory of local coherence and salience. One of the main ideas underlying centering is that certain entities mentioned in an utterance are more central than others. This in turn imposes constraints on the use of referring expressions and in particular on the use of pronouns. can be phrases, clauses, sentences, or even paragraphs. At any point in discourse, some entities are considered more salient than others, and are expected to exhibit different properties. Specifically, although each utterance may contain several entities, it is assumed that a single entity is  X  X entered, X  thereby representing the current discourse focus. One of the main claims underlying centering is that discourse segments in which successive utterances contain common centers are more coherent than segments where the center repeatedly changes.
 a unique backward-looking center , C b ( U j ). C f ( U j invoked by U j according to their salience. Thus, some entities in the discourse are deemed more important than others. The C b of the current utterance U ranked element in C f ( U j  X  1 )thatisalsoin U j . (Centering hypothesizes that the C grammatical function, namely, subjects are ranked more highly than objects, which are more highly ranked than the rest (Grosz, Weinstein, and Joshi 1995). The C the previous discourse, but it does so locally since C b ( U tween adjacent utterances. Grosz, Weinstein, and Joshi (1995) distinguish between three 424 types of transitions. In C ONTINUE transitions, C b ( U most highly ranked element entity in U j .InR ETAIN transitions C
C ( U j ) is not the most highly ranked element entity in U
C ( U j ) = C b ( U j  X  1 ). These transitions are ordered: C TAIN s, which are preferred over S HIFT s. And discourses with many C tions are considered more coherent than those which repeatedly to the other.

Prince (1998). (1) a. Jeff helped Dick wash the car.

Here, the first utterance does not have a backward-looking center but has three forward-looking centers Jeff , Dick ,and car . To determine the backward-looking center of (1b) we find the highest ranked entity among the forward-looking centers in (1a) which also occurs in (1b). This is Jeff as it is the subject (and thus most salient entity) in (1a) and present (as a pronoun) in (1b). The same procedure is applied for utterance (1c). Also note that (1a) and (1b) are linked via a C ONTINUE transition. The same is true for (1b) and (1c).
 interested in characterizing our texts in terms of entity transitions. Because they are all written by humans, we can assume they are more or less coherent. Nonetheless, identi-fying the centers in discourse seems important. These will indicate what the document is about, who the main protagonists are, and how the discourse focus progresses. We would probably not want to delete entities functioning as backward-looking centers. are instantiated. A great deal of research has been devoted to fleshing these out and many different instantiations have been developed in the literature (see Poesio et al. [2004] for details). In our case, the instantiation will have a bearing on the reliability of the algorithm to detect centers. If the parameters are too specific then it may not be possible to accurately determine the center for a given utterance. Because our aim is to identify centers in discourse automatically, our parameter choice is driven by two considerations: robustness and ease of computation.
 subordinate and adjunct clauses). This is a simplistic view of an utterance; however it is in line with our compression task, which also operates over sentences. We determine which entities are invoked by a sentence using two methods. First, we perform named entity identification and coreference resolution on each document using LingPipe, publicly available system. Named entities are not the only type of entity to occur in our data, thus to ensure a high entity recall we add named entities and all remaining nouns to the C f list. Entity matching between sentences is required to determine the C tence. This is done using the named entity X  X  unique identifier (as provided by LingPipe) or by the entity X  X  surface form in the case of nouns not classified as named entities. grammatical roles; subjects are ranked more highly than objects, which are in turn ranked higher than other grammatical roles; ties are broken using left-to-right ordering of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles using RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows (where U j corresponds to sentence j ): 1. Extract entities from U j . 2. Create C f ( U j ) by ranking the entities in U j according to their grammatical 3. Find the highest ranked entity in C f ( U j  X  1 ) which occurs in C
This procedure involves several automatic steps (named entity recognition, coreference resolution, and identification of grammatical roles) and will unavoidably produce some noisy annotations. There is no guarantee, therefore, that the right C that all sentences will be marked with a C b . The latter situation also occurs in passages that contain abrupt changes in topic. In such cases, none of the entities realized in U means of capturing local content within a document.
 document lava and debris are marked as centers, in the second document Mrs Allan (and its coreferents), and in the third one Peter Anderson and allotment . When comparing the annotations produced by centering and the lexical chains, we observe that they tend to be complementary. Proper nouns that lexical chains miss out on are often identi-fied by centering. When the latter fails, due to errors in coreference resolution or the identification of grammatical relations, lexical chains can be more robust because only
WordNet is required for their computation. As an example consider the third document in Figure 2. Here, lexical chains provide a better insight into the text. Were we to rely solely on centering, we would obtain annotations only for two entities, namely, Peter
Anderson and allotment . 5. The Discourse-Inspired Compression Model
We now turn our attention to incorporating discourse information into our compression model. Before compression takes place, all documents are processed using the center-ing and lexical chain algorithms described earlier. In each sentence we annotate the center C b ( U j ) if one exists. Words (or phrases) that are present in the current sentence and function as the center in the next sentence C b ( U j + 1 426 additional knowledge our model takes a (sentence-separated) source document as input and generates a compressed version by applying sentence-level and discourse-level constraints to the entire document rather than to each sentence sequentially. In our earlier formulation of the compression task (Clarke and Lapata 2008), we create and solve an ILP for every sentence, whereas now an ILP is solved for each document.
This makes sense from a discourse perspective as compression decisions are not made independently of each other. Also note that this latter formulation brings compression closer to summarization as we can manipulate the document compression rate directly, for example, by adding a constraint that forces the target document to be less than b to-kens. This allows the model to choose how much to compress each individual sentence without requiring that they all have the same compression rate. Accordingly, we modify our objective function by introducing a sum over all sentences (assuming l sentences are present in the document) and adding an additional index g to each decision variable to track the sentence it came from: rather than sentences. This allows some sentences to violate the compression rate with-out incurring a penalty, provided the compression rate of the document falls within the specified range.

Document Compression Rate Constraints. We wish to penalize compressions which do not fall within a desired compression rate range ( c min %  X  c makes use of all the sentence-level constraints introduced in Section 3.3, but is crucially enhanced with three discourse constraints explained in the following. 5.1 Discourse Constraints
Our first goal to is preserve the focus of each sentence. If the center, C the source sentence it must be retained in the target compression. If present, the entity realized as the C b in the following sentence should also be retained to ensure the focus is preserved from one sentence to the next. Such a condition is easily captured with the following ILP constraint: As an example, consider the first discourse in Figure 2. The constraints generated from
Equation (21) will require the compression to retain lava in the first two sentences and debris in the second and third sentences.
 nology that is not 100% accurate (named entity detection, parsing, and coreference resolution). Therefore, the algorithm can only approximate the center for each sen-complementary annotation of the topic or theme of the document using information which is not restricted to adjacent sentences. Recall that once chains are created, they are scored, and chains with scores less than the average are discarded. We consider all remaining lexical chains as topical and require that words in these be retained in the compression.
Consider again the first text in Figure 2. Here, flow and rate are members of the same chain (marked with subscript 1). According to constraint (22) both words must be included in the compressed document. In the third document the words relating to  X  X olice X  ( police , policeman ) and  X  X eople X  ( woman , boyfriend , man ) also would be retained in the compression.
 sonal pronouns (whose antecedent may not always be identified) to be included in the compression. the discourse flow of the source document and will preserve terms indicative of important topics. Document compression aside, the discourse constraints will also benefit sentence-level compression. They provide our model, which so far relied on syntactic evidence and surface level document characteristics (i.e., word frequencies), additional evidence for retaining (discourse) relevant words. 428 5.2 Applying the Constraints
As explained earlier we apply the model and the constraints to each document. In our highlight which nouns and verbs should be included in the compression. As far as nouns are concerned, our discourse constraints perform a similar task. Thus, when a sentence contains discourse annotations, we are inclined to trust them more and only calculate the significance score for verbs.
 multaneously (see Equations (21) X (23)) results in relatively long compressions. To progressively less reliable information. Our back-off model works as follows: If center-ing information is present, we apply the appropriate constraints (Equation (21)). If no centers are present, we back off to the lexical chain information using Equation (22), and in the absence of the latter we back off to the pronoun constraint (Equation (23)). Finally, nificance score. Sentential constraints are applied throughout irrespective of discourse constraints. We determined this ordering (i.e., centering first, then lexical chains, and then pronouns) on the development set. Centering tends to be more precise, whereas lexical chains have high recall but lower precision in terms of identifying which entities are in focus and should therefore not be dropped. In our test data (see Section 6 for details), the centering constraint was used in 68.6% of the sentences. The model backed was applied in 8.5%. Finally, the noun and verb significance score was used on the remaining 9.2%. Examples of our system X  X  output for the texts in Figure 2 are given in
Figure 3. 6. Experimental Set-up
In this section we present our experimental set-up for assessing the performance of the compression model. We describe the compression corpus used in our study, briefly introduce the model used for comparison with our approach, and explain how system output was evaluated. 6.1 Compression Corpus
Previous work on sentence compression has used almost exclusively the Ziff-Davis cor-pus, a compression corpus derived automatically from document X  X bstract pairs (Knight and Marcu 2002). Unfortunately, this corpus is not suitable for our purposes because it consists of isolated sentences taken from several different documents. We thus created a document-based compression corpus manually. Specifically, annotators were pre-sented with one document at a time and asked to compress sentences sequentially by removing tokens. They were free to remove any words they deemed superfluous, provided their deletions (a) preserved the most important information in the source sen-tence, and (b) ensured the compressed sentence remained grammatical. If they wished, they could leave a sentence uncompressed. They were not allowed to delete whole sentences even if they believed they contained no information content with respect to the story, as this would blur the task with summarization. Following these guidelines, the annotators created compressions for 82 stories (1,629 sentences) from the BNC and the LA Times and Washington Post. 8 Forty-eight (48) documents (962 sentences) were used for training, 3 for development (63 sentences), and 31 for testing (604 sentences). 6.2 Comparison with State-of-the-Art
The discourse-based compression model was evaluated against our earlier sentence-based ILP model (without the discourse constraints). In addition, we compared our ap-proach against a state-of-the-art model which does not take discourse-level information into account, does not use ILP, and is sentence-based. We give a brief description in the following, and refer the interested reader to McDonald (2006) for details.
 criminative large-margin learning framework: Pairs of words from the source sentence are classified as being adjacent or not in the target compression. Let x = x denote a source sentence with a target compression y = y curs in x . The function L ( y i )  X  X  1 ... n } maps word y thewordinthesource, x (subject to the constraint that L ( y defines the score of a compression y for a sentence x as the dot product between 430 a high-dimensional feature representation over bigrams and a corresponding weight vector:
Decoding in this framework amounts to finding the combination of bigrams that maxi-mize the scoring function in Equation (24). The maximization is solved using dynamic programming (see McDonald [2006] for details).
 (MIRA; Crammer and Singer 2003), a discriminative large-margin online learning tech-nique. This algorithm learns by compressing each sentence and comparing the result with the gold standard. The weights are updated so that the score of the correct com-margin proportional to their loss. The loss function is the number of words falsely re-tained or dropped in the incorrect compression relative to the gold standard. McDonald employs a rich feature set defined over words, parts of speech, phrase structure trees, and dependencies. These are gathered over adjacent words in the compression and the words in between which were dropped.
 highly competitive performance compared with Knight and Marcu X  X  (2002) noisy-channel and decision-tree models. Due to its discriminative nature, the model is able to use a large feature set and to optimize compression accuracy directly. In other words,
McDonald X  X  model has a head start against our own model which does not utilize a large parallel corpus and has only a few constraints. The comparison of the two systems allows us to establish that we have a competitive state-of-the-art system, even without discourse constraints.
 sentences). Our implementation used an identical feature set, the only difference being that our phrase structure and dependency features were extracted from the output of Roark X  X  (2001) parser. McDonald uses Charniak X  X  (2000) parser, which performs comparably. We also employed a slightly modified loss function to encourage compres-sion on our data set. McDonald X  X  results were reported on the Ziff-Davis corpus. The language model required for the ILP system was trained on 80 million tokens from the English GigaWord corpus (LDC2007T07) using the SRI Language Modeling Toolkit with
Kneser-Ney discounting. The significance score was calculated on 80 million tokens from the same corpus. The ILP model presented in Equation (1) implements a weighted combination of the significance score with a language model. The weight was tuned on the development set which consisted of three source documents and their target compressions. Our optimization procedure used Powell X  X  method (Press et al. 1992) and a loss function based on the grammatical relations F1 between the gold standard and system output. The optimal weight was approximately 9.0. Note that the development set was the only source of parallel data our model had access to.

McDonald [2006]) on an equal footing, we ensured that their compression rates were similar. To do this, we first run McDonald X  X  model on our data and then set the com-pression rate for our ILP models so that it is comparable to his output. This can be done relatively straightforwardly by adjusting the compression rate range soft constraint. In our experiments we set the minimum compression rate to 57%, the upper rate to 62%, compression rate can be removed or specifically tuned to suit the application. 6.3 Evaluation
Previous studies evaluate the well-formedness of automatically generated compres-sions out of context. The target sentences are typically rated by naive subjects on two dimensions, grammaticality and importance (Knight and Marcu 2002). Automatic eval-uation measures have also been proposed. Riezler et al. (2003) compare the grammatical relations found in the system output against those found in a gold standard using F1.
Although F1 conflates grammaticality and importance into a single score, it neverthe-less has been shown to correlate reliably with human judgments (Clarke and Lapata 2006).
 whether our discourse constraints improve the compressions for individual sentences.
There is no hope for generating shorter documents if the compressed sentences are either too wordy or too ungrammatical. Secondly and more importantly, our goal was to evaluate the compressed documents as a whole by examining whether they are readable and the degree to which they retain key information when compared to the originals. We evaluated sentence-based compressions automatically using F1 and the grammatical relations annotations provided by RASP (Briscoe and Carroll 2002). This parser is suited to the compression task as it provides parses for both full sentences and sentence fragments and is generally robust enough to analyze semi-grammatical sentences. We computed F1 over all the relations provided by RASP (e.g., subject, direct/indirect object, modifier; 17 in total). We compared the output of our discourse system on the test set (31 documents, 604 sentences) against the sentence-based ILP model and McDonald (2006).
 pressed documents readable? and (2) How much key information is preserved between the source document and its target compression? The readability of a document is fairly straightforward to measure by asking participants to provide a rating (e.g., on a seven-point scale). Measuring how much information is preserved in the compressed document is more involved. Under the assumption that the target document is to function as a replacement for the source, we can measure the extent to which the compressed version can be used to find answers for questions which have been derived from the source and are representative of its core content. We thus created questions from the source and then determined whether it was possible to find their answers by reading the compressed target. The more questions a hypothetical compression system can answer, the better it is at compressing the document as a whole.
 summaries and text compression. Morris, Kasper, and Adams (1992) performed one of the first Q&amp;A evaluations to investigate the degree to which documents could be summarized before reading comprehension diminished. Their corpus consisted of four passages randomly selected from a set of sample Graduate Management Aptitude Test (GMAT) reading comprehension tests. The texts covered a range of topics including medieval literature, 18th-century Japan, minority-operated businesses, and Florentine art. Accompanying each text were eight multiple-choice questions, each containing five possible answers. The questions were provided by the Educational Testing Service and were designed to measure the subjects X  reading comprehension. Subjects were 432 given various textual treatments: the full text, a human-authored abstract, three system-generated extracts, and a final treatment where merely the questions were presented without any text. The questions-only treatment was used as a control to investigate if subjects could answer questions without any source material. Subjects were instructed to read the passage (if provided) and answer the multiple choice questions. hension test, is that Q&amp;A pairs are provided along with a method for scoring answers (the correct answer is one among five possible choices). However, our corpora do not contain ready prepared Q&amp;A pairs; thus we require a methodology for constructing questions and their answers and scoring documents against the answers. One such methodology is presented in the TIPSTER Text Summarization Evaluation (SUMMAC;
Mani et al. 2002). SUMMAC was concerned with producing summaries tailored to specific topics. The Q&amp;A task involved an evaluation where a topic-related summary for a document was evaluated in terms of its  X  X nformativeness, X  namely, the degree to which it contained answers found in the source document to a set of topic-related questions. For each topic (three in total), 30 relevant documents were chosen to generate a single summary. One annotator per topic came up with no more than five questions relating to the obligatory aspects of the topic. An obligatory aspect of a topic was defined as information that must be present in the document for the document to be annotating the passages and phrases from the documents which provided the answers to the questions. In the SUMMAC evaluation, the annotator for each topic was tasked with scoring the system summaries. Scoring involved comparing the summaries against the answer key (annotated passages from the source documents) while judging whether the summary provided a Correct , Partially Correct ,or Missing answer. If a summary con-tained an answer key and sufficient context the summary was deemed correct; however, summaries would be considered partially correct if the answer key was present but with insufficient context. If context was completely missing, misleading, or the answer key was absent then the summary was judged missing.
 spired by the SUMMAC evaluation exercise (Mani et al. 2002). Rather than creating questions for document sets (or topics) our questions were derived from individual documents. Two annotators were independently instructed to read the documents from our (test) corpus and create Q&amp;A pairs. Each annotator drafted no more than ten questions and answers per document, related to its content. Annotators were asked to create fact-based questions which required an unambiguous answer; these were typically who, what, where, when, and how X  X tyle questions. The purpose of using two annotators per document was to allow annotators to compare and revise their Q&amp;A pairs; this process was repeated until a common agreed-upon set of questions was reached. Revisions typically involved merging and simplifying questions to make them clearer, and in some cases splitting a question into multiple questions. Documents for which too few questions were agreed upon and for which the questions and answers were too ambiguous were removed. This left an evaluation set of six documents with between five to eight concise questions per document. Figure 4 shows a document from our test set and the questions and answers our annotators created for it.

Instead of asking the annotator who constructed the questions to check the document compressions for the answers, we ask naive participants to read the compressed doc-uments and answer the questions as best as they can. During evaluation, the source document is not shown to our subjects; thus, if the compression is difficult to read, the participants have no point of reference to help them understand the compression. This is a departure from previous evaluations within text generation tasks, where the source text is available at judgment time; in our case only the system output is available. a custom-built Web interface. Upon loading the Web interface, participants were pre-sented with a set of instructions that explained the Q&amp;A task and provided examples. 434
Subjects were first asked to read the compressed document and then rate its readability on a seven-point scale where 7 = excellent, and 1 = terrible. Next, questions were presented one at a time (the order being is defined by the annotators) and participants were encouraged to consult the document for the answer. Answers were written directly into a text field on the Web interface which allowed free-form text to be submitted. Once a participant provided an answer and confirmed the answer, the interface locked the answer to ensure it was not modified later. This was necessary because later questions could reveal information which would help answer previous questions.
 dard, using the ILP sentence-based model, the ILP discourse model, and McDonald X  X  (2006) model. A Latin square design was used to prevent participants from seeing multiple treatments (compressions) of the same document thus removing any learning effect. A total of 116 unpaid volunteers completed the experiment. They were recruited through student mailing lists and the Language Experiments Web site. provided by our subjects were scored against an answer key. A correct answer was marked with a score of one, and zero otherwise. In cases where two answers were required, a score of 0.5 was awarded to each correct answer. The score for a compressed document is the average of its question scores. All subsequent tests and comparisons are performed on the document score. 7. Results We first assessed the compressions produced by the two ILP models (Discourse and
Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the compression rates (CompR) for the three systems and evaluates the quality of their output using grammatical relations F1. As can be seen, all three systems produce comparable compression rates. The Discourse ILP compressions are slightly longer than
McDonald X  X  (2006) (61.0% vs. 60.1%) and slightly shorter than the Sentence ILP model (61.0% vs. 62.1%). The Discourse ILP model is significantly better than McDonald (2006) and Sentence ILP in terms of F1, indicating that discourse-level information is generally helpful. All three systems could use further improvement, as inter-annotator agreement on this data yields an F1 of 65.8% (Clarke 2008).
 the mean readability ratings obtained for each system and the percentage of questions answered correctly. We used an analysis of variance (ANOVA) to examine the effect of compression type (McDonald, Sentence ILP, Discourse ILP, Gold Standard). The
ANOVA revealed a reliable effect on both readability and Q&amp;A. Post hoc Tukey tests showed that McDonald and the two ILP models do not differ significantly in terms of readability. However, they are all significantly less readable than the gold standard (  X &lt; 0 . 01). For the Q&amp;A task, we observe that our system is significantly better than
McDonald (  X &lt; 0 . 01) and Sentence ILP (  X &lt; 0 . 01), but significantly worse than the gold standard (  X &lt; 0 . 05). McDonald and Sentence ILP yield comparable performance (their difference is not statistically significant).
 dard in terms of readability. When reading entire documents, subjects are less tolerant of ungrammatical constructions. We also find out that, despite relatively low readability, the documents are overall understandable. The discourse-based model generates more informative documents X  X he number of questions answered correctly increases by 19% in comparison to McDonald and Sentence ILP. This is an encouraging result suggesting that there are advantages in developing compression models that exploit discourse-level information information.
 test documents. Words that are dropped have been stricken out. As can be seen, the two systems produce different compressions, and the discourse-based output is more coherent. This is corroborated by the readability results where the discourse ILP model received the highest rating. Also note that some of the compressions produced by the sentence-based model distort the meaning of the original text, presumably leading the reader to make wrong inferences. For example, in the second document (Sentence ILP version) one infers that the victim was urged to report the incident. Moreover, important information is often omitted, for example, that the victim was indeed raped or that the strike would be damaging not only to the company but also to its staff (see the Sentence
ILP version in the first document). 8. Conclusions and Future Work
In this article we proposed a novel method for automatic sentence compression. Central in our approach is the use of discourse-level information, which we argue is an impor-tant prerequisite for document (as opposed to sentence) compression. Our model uses integer linear programming for inferring globally optimal compressions in the presence of linguistically motivated constraints. Our discourse constraints aim to capture local coherence and are inspired by Centering Theory and lexical chains. We showed that our 436 Discourse ILP Sentence ILP Discourse ILP Sentence ILP model can be successfully employed to produce compressed documents that preserve most of the original core content.
 helpful in summarization. We also show that this type of information can be identified robustly in free text. Our experiments focused primarily on local discourse structure us-ing two complementary representations. Centering tends to produce more annotations since it tries to identify a center in every sentence. Lexical chains tend to provide more general information, such as the major topics in a document. Due to their approximate nature, there is no one representation that is uniquely suited to the compression task. Rather, it is the synergy between lexical chains and centering that brings improvements.
The discourse annotations proposed here are not specific to our model. They could be easily translated into features and incorporated into discriminative modeling par-adigms (e.g., Nguyen et al. 2004; McDonald 2006; Cohn and Lapata 2009). The same straightforwardly adapted to assess the information content of shorter summaries and potentially used to perform large-scale comparisons within and across systems. fairly long. However, we believe this is the first step to understanding how com-pression can help summarization. An obvious extension would be to interface our compression model with sentence extraction (see Martins and Smith [2009] for an ILP formulation of a model that jointly performs sentence extraction and compression, without, however, taking discourse level information into account). The discourse annotations can help guide the extraction method into selecting topically related sen-tences which can consequently be compressed together. More generally, formulating the summarization process in the ILP framework outlined here would allow the integration of varied and sometimes conflicting constraints during summary generation. Examples include the summary length, and whether it is coherent, grammatical, or repetitive. Ad-ditional flexibility can be introduced by changing some of the constraints from hard to soft (as we did with the compression rate constraints), although determining the penalty for constraint violation manually using prior knowledge is a non-trivial task (Chang,
Ratinov, and Roth 2007) and automatically learning the constraint penalty results in a harder learning problem. Importantly, under the ILP formulation such constraints can be explicitly encoded and applied during inference while finding a globally optimal solution.
 Acknowledgments References 438 440
