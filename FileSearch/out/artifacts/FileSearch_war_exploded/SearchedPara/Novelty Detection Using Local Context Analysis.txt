 H.3.3 [ Information Search and Retrieval ]: Retrieval Models, Information Filtering Experimentation Local Context Analysis, Novelty Detection The aim of this work is to determine the utility of Local Context Analysis (LCA)[5] for retrieval of relevant and novel sentences. LCA has been successful in different areas and we check here whether this method is also useful to drive the selection of novel material. We adopt the Novelty task as defined in the TREC conference [2, 4, 3]. Giving a set of documents associated to a topic, the task consists of finding the relevant and novel sentences. This problem is interesting for many areas, such as text summarization, web informa-tion access, question answering, etc. Some researchers have proposed that the estimation of novelty for a given sentence should be based on the set of seen sentences that share com-mon meanings [6]. In this way, the degree of redundancy of asentence s i is not influenced by past sentences that are totally unrelated to s i . The intuition is that novelty estima-tion might be more robust if focused on this set of terms. In our work we pursue a similar idea because we apply LCA to focus the estimation of novelty on query-related terms.
The groups participating in the Novelty task start from a common ranking of documents for each query. Two different subtask are proposed: 1) to produce a ranking of relevant sentences and 2) to filter out redundant sentences from this ranking. Successful algorithms tested for this task apply usually some popular IR model to rank sentences given a query (e.g. variants of tf-idf applied at the sentence level [1]). Next, in order to estimate how redundant the sen-tences are, some methods have been proposed to compute the overlapping between each sentence and the previously seen sentences. We have chosen two baseline methods which are simple and robust [1]: NewWords and SetDif. New-Words counts the number of wo rds in the current sentence, s , which did not occur in the previously seen sentences: where W s i is the set of words in the sentence s i .
SetDif computes the number of different words between each sentence s i and the previously seen sentence that is the most similar to s i :
We propose variants of these methods to estimate the nov-elty score focusing on query-related terms. We expect to improve performance when the novelty scores consider only terms that are highly related to the query.
LCA is a method based on the idea that a common term from the top-ranked relevant documents (or passages) will tend to co-occur with query terms within the top-ranked documents (or passages) [5]. We apply LCA to produce a set of query-related terms and the novelty scores are adjusted accordingly. The importance of the terms in the top ranked sentencesiscomputedas: where t is a term, N is the number of sentences in the collection, N i is the number of sentences containing the term t i , ft ij is the number of occurrences of the term t in the sentence p j , ft j is the number of occurrences of the term t in th sentence p j , idf i =min(1 . 0 ,log 10 ( N/N af ( t, t i )= zero bel value.

This measure can be applied to rank terms in decreasing order of estimated importance given a query. Selecting the top ranked terms we can conform a query-oriented vocab-ulary ( T q ). Using this vocabulary, we compute NewWords and SetDif for each sentence as follows: and
We used the three different collections of data which were made available in the context of the TREC Novelty tracks in 2002, 2003 and 2004 [2, 4, 3]. In 2002 and 2003, the rank-ing of documents provided by NIST consists only of relevant documents. In 2004, the collection is more realistic because the ranks of documents contain relevant and irrelevant ma-terial.

To generate an initial rank of sentences we applied a vari-ation of tf-idf which proved successful in the past [1]. Given these ranks, the top 25 ranked sentences 1 are mined for se-lecting important terms using LCA. This gives us the query-oriented vocabulary T q and, subsequently, sentences are re-ranked using N LCA nw and N LCA sd . The top 10% sentences of this ranking are used for evaluation. We made experi-ments with varying sizes of this vocabulary to check the sta-bility of the method. The evaluation measures applied are precision at 5 (P@5) and precision at 10 sentences (P@10). In Table 1 we show the performance values using New-Words and NewWords with LCA applying different vocab-ulary sizes (10, 50, 100 and all terms in the top 25 ranked sentences). Analogously, in Table 2 we report results for the SetDif method. Results indicated with a star are statisti-cally significant using a t-test at the p&lt;. 05 level.
Preliminary experiments showed that 25 sentences is a rea-sonable number for estimating the query-oriented vocabu-lary.

In 2003, the baseline performs very well because of the high population of relevant sentences in the collection [4]. Hence, it is very difficult to improve the results because any reasonable sentence retrieval strategy yields a good top 10. In the other two collections the application of LCA yielded significant improvements.

The results indicate that the larger the vocabulary is the better the precision is. With 10 terms the method does not estimate redundance satisfactorily because all the decisions are made based on very few terms. On the other hand, if vocabularies contain all terms in the top 25 sentences then redundance is estimated successfully.

LCA seems useful in terms of P@5 but its utility is ques-tionable if the aim is to retrieve 10 good sentences. In such case, selecting simply all terms in the top 25 sentences is the most robust approach. To the best of our knowledge, this sort of vocabulary selection, which is a form of pseudo-relevance feedback for novelty purposes, has not been ap-plied in the literature.
We have presented the results of our attempts to identify relevant and novel sentences in a ranked list of documents using different methods and their variants using LCA.
Although NewWords and SetDif are competitive methods for novelty detection, our results indicate that precision at top ranks might be further improved if redundancy decisions are made in terms of a more focused vocabulary. Never-theless, it is still unclear whether such vocabulary should be selected using LCA. Given our current results, a simple method (based on extracting the terms appearing in the top 25 sentences) performs well and does not require LCA. Any-way, in the future we will keep studying the effects of the vocabulary size on novelty detection. This work was partially supported by national project TIN2005-08521-C02-01 and Galician network 2006/23. David E. Losada belongs to the  X  X am  X  on y Cajal X  program, whose funds come from MEC and the FEDER program. [1] J. Allan, C. Wade, and A. Bolivar. Retrieval and [2] D. Harman. Overview of the TREC 2002 Novelty [3] I. Soboroff. Overview of the TREC 2004 Novelty Track. [4] I. Soboroff and D. Harman. Overview of the TREC [5] J. Xu and W. B. Croft. Improving the effectiveness of [6] L. Zhao, M. Zhang, and S. Ma. The nature of novelty
