 H.3.4 [ Information Storage and Retrieval ]: Systems and Software Design why -questions, answer extraction, RST, discourse annota-tion
Our research aims at developing a system for answering why -questions ( why -QA). More specifically, we focus on the role that linguistic information and analysis can play in the process of why -QA. In the present paper, we evaluate an answer extraction method that exploits discourse relations in texts.

In approaches to factoid QA, named entity recognition can make a substantial contribution to identifying poten-tial answers. Answers to why -questions on the other hand, cannot be expressed in the form of a noun phrase. Rather, they consist of propositions, and often they span multiple sentences that entertain discourse relations such as  X  X ause X ,  X  X otivation X ,  X  X urpose X , and  X  X xplanation X . Therefore, we decided to approach the answer extraction problem as a dis-course analysis task. In order to investigate to what extent discourse structure enables why -QA, we created a system that uses discourse structure for answer extraction.
In the present paper, we evaluate a method for discourse-based answer extraction using two sets of why -questions: one obtained by elicitation of native speakers and one con-taining questions that are asked to the online QA system answers.com . As a model for discourse annotation, we use Rhetorical Structure Theory (RST) [1]. The answer extraction ap-proach that we consider is proposed by Verberne et al. [4]. This method is based on the idea that the topic of a why -question and its answer are siblings in the RST structure of the document, connected by a relation that is relevant for why -questions. We implemented an algorithm that (1) indexes all text spans from the source document that partic-ipate in a potentially relevant RST relation, (2) matches the input question to each of the text spans in the index, and (3) retrieves the sibling for each of the found spans as an-swer. The result is a list of potential answers, ranked using a probability model based on the general language model for Information Retrieval as defined by Croft and Lafferty [2]
We created two collections of why -questions. For the first data collection, we manually selected seven documents from the RST Treebank [1] of 350 X 550 words each. The RST Treebank contains Wall Street Journal articles that have been manually annotated with RST structures by Carlson et al. We created a set of 372 why -questions obtained from elicitation of native speakers to these annotated texts.
Gathering questions through elicitation runs the risk that participants feel forced to come up with why -questions. This may lead to a set of questions that is not completely rep-resentative for a user X  X  real information need. Therefore, we created a second data set, based on the Webclopedia question collection [3]. The complete Webclopedia collection consists of 17,000 questions downloaded from answers.com , an online domain-independent QA system. 805 questions from the Webclopedia set are why -questions X  X ragmatically defined as questions starting with the word why . The source of these questions guarantees that they originate from users X  information needs. We randomly selected 400 of these why -questions. For analysis and development purposes, we cre-ated a set of answer fragments to these 400 questions, manu-ally extracted from Wikipedia. For 54% of these questions, we were able to find the answer in Wikipedia. In a large majority of cases (94%) the length of the answer did not exceed a single paragraph. We let two experienced annota-tors create RST structures for the answer fragments from Wikipedia. For answer fragments shorter than one para-graph, we selected the complete paragraph for annotation. We also added the previous paragraph or the section head-ing to the fragment if these provided essential information for understanding the paragraph containing the answer. We did not inform the annotators about the purpose of their annotations.

We believe that it is useful to categorize our questions according to their answer type, because this helps the sys-tem select potential answers from the source text. Our two question collections are very different in the types of answers that they expect. In the set of elicited questions, 38% has  X  X otivation X  as answer type and 52%  X  X ause X . For the Web-clopedia set, we found that the proportion of questions ex-pecting a motivation as answer is only 10%. The remaining category,  X  X ause X , appeared to be too general as a class for all other questions. Therefore, we decided to categorize the We-bclopedia question collection into five classes:  X  X otivation X  (10%),  X  X hysical Explanation X  (42%),  X  X on-physical expla-nation X  (30%),  X  X tymology X  (12%), and  X  X onsense X (6%).
We use both our data collections for evaluating our ap-proach to discourse-based answer extraction. We study the theoretically possible contribution of RST to answer extrac-tion by manually analyzing each of the questions for which we have an answer fragment available X  X nd its correspond-ing RST structure. We only considered the questions for which we were able to find an answer in Wikipedia (54% of all questions). We manually matched each question topic to a text span in the answer fragment and selected the span X  X  sibling as answer. Following this procedure, we find a satis-factory answer for 58.0% of the question-answer pairs in our set of elicitation data, and for 60.6% of the question-answer pairs in our Webclopedia set. We see that although the ques-tions in both data collections come from different sources, our answer selection procedure shows highly similar results for both sets.

This analysis shows that the maximum recall that can be achieved using our discourse-based answer extraction ap-proach is around 60%. The remaining 40% suffers from one of the following problems: (1) the question topic is not rep-resented by a text span in the answer fragment; (2) the text span representing the question topic does not participate in an RST relation; (3) the correct answer is not the sibling of the span representing the question topic but it is somewhere else in the RST structure.

If we consider the question-answer pairs where our RST-based approach succeeds, then we see that the most occur-ring successful RST-relations are  X  X xplanation-argumenta-tive X ,  X  X ircumstance X ,  X  X ackground X  and  X  X urpose X . The in-stances of each of these relation types lead to a satisfactory answer in more than 75% of question topics that participate in such a relation. Thus, these relations have the largest predictive power in answer selection using RST.

We implemented a module that automatically maps the question topic onto the correct discourse unit in the text, mainly by measuring lexical overlap between the question topic and discourse units. For the questions related to the RST Treebank documents 88.7% of the question topics could be identified automatically in the Wall Street Journal texts. However, the same procedure could only find 42.7% of the discourse units connected to the Webclopedia questions in the Wikipedia documents. This difference is due to the fact that questions elicited from subjects who are reading a text tend to use the same terms as those in the texts. This sug-gests that the results obtained using the Wall Street Journal texts do not generalize to any other setting. For the Webclo-pedia questions lexical overlap is much smaller because these questions were formulated completely independently from a specific text. Assuming that the Webclopedia/Wikipedia set is representative to an actual question answering setting, we should acknowledge the problem of small lexical overlap in the system under development.
We created two corpora of why -questions consisting of 372 and 400 questions respectively and corresponding answer documents annotated with discourse structure. These data collections may be of interest for other researchers in the field of question answering or discourse analysis. 1
We evaluated an answer extraction method for why -ques-tions based on the idea that question topic and answer are siblings in the RST structure. We found that our proce-dure is potentially successful for 60% of why -questions. The implementation of our procedure can match 42.7% of the question topics from the Webclopedia set to the manually chosen discourse unit in the corresponding Wikipedia frag-ment.

We conclude that discourse structure can be useful in solv-ing at least a subset of why -questions and that some relation types have a predictive power in answer selection. However, our answer extraction approach should be combined with other methods in order to increase recall.

We consider paragraph retrieval as alternative and sup-plementary approach. We studied all Webclopedia ques-tions and the corresponding Wikipedia fragments and we found that for 84.7% of questions, a complete paragraph from Wikipedia is a satisfactory answer. Thus, paragraph retrieval is a good additive solution to discourse-based an-swer extraction. Since some types of RST relations have a high predictive power in answer selection, we aim at develop-ing a method for paragraph retrieval in which we incorporate knowledge about the presence of relevant RST relations.
Moreover, we plan to investigate to what extent we can achieve automatic partial discourse annotations that are spe-cifically equipped to finding answers to why -questions. [1] L. Carlson, D. Marcu, and M. E. Okurowski. Building a [2] W. Croft and J. Lafferty. Language Modeling for [3] E. Hovy, U. Hermjakob, and D. Ravichandran. A [4] S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
We will make the data collection available through the project X  X  web site http://lands.let.ru.nl/~sverbern/
