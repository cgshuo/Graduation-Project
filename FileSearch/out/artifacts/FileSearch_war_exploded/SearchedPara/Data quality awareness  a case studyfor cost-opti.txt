 REGULAR PAPER Laure Berti- X  Equille Abstract The quality of discovered association rules is commonly evaluated by interestingness measures (commonly support and confidence) with the purpose of supplying indicators to the user in the understanding and use of the new discovered knowledge. Low-quality datasets have a very bad impact over the quality of the discovered association rules, and one might legitimately wonder if a so-called  X  X nteresting X  rule noted LHS  X  RHS is meaningful when 30% of the LHS data are not up-to-date anymore, 20% of the RHS data are not accurate, and 15% of the LHS data come from a data source that is well-known for its bad credibility. This paper presents an overview of data quality characterization and management techniques that can be advantageously employed for improving the quality awareness of the knowledge discovery and data mining processes. We propose to integrate data quality indicators for quality aware association rule mining. We propose a cost-based probabilistic model for selecting legitimately interesting rules. Experiments on the challenging KDD-Cup-98 datasets show that variations on data quality have a great impact on the cost and quality of discovered association rules and confirm our approach for the integrated management of data quality indicators into the KDD process that ensure the quality of data mining results.
 Keywords Quality awareness mining  X  Data quality management  X  Data quality metadata  X  Cost model  X  Association rule mining 1 Introduction The quality of data mining results and the validity of results interpretations essentially rely on the data preparation process and on the quality of the analyzed datasets [ 48 ]. Indeed, data mining processes and applications require various forms of data preparation, correction, and consolidation, combining complex data transformation operations and cleaning techniques. This is because the data input to the mining algorithms is assumed to conform to  X  X ice X  data distributions, containing no missing, inconsistent, or incorrect values. This leaves a large gap between the available  X  X irty X  data and the available machinery to process and analyze the data for discovering added-value knowledge and decision-making. Data quality is a multidimensional, complex, and morphing concept [ 14 ]. In the last decade, there has been a significant amount of work in the area of information and data quality management initiated by several research communities (database, statistics, workflow management, and knowledge engineering), ranging from the techniques that assess information quality to the design of large-scale data inte-gration systems over heterogeneous data sources with different degrees of quality and trust. Many data quality definitions, metrics, models, and methodologies have been proposed by academics and practitioners with the aim to tackle the main classes of data quality problems:  X  Duplicate detection and record matching known under various names: record  X  Instance conflict resolution [ 20 ] using data source selection [ 16 , 37 , 41 ]ordata  X  Missing values [ 34 ] and incomplete data [ 55 ];  X  Staleness of data [ 8 , 58 ].
 perfectly clean data, knowledge discovery techniques (such as clustering, mining association rules, or visualization) can be relevantly used as decision-making pro-cesses to automatically derive new knowledge patterns and new concepts from data. Unfortunately, most of the time, this data is neither rigorously chosen from the various heterogeneous information sources with different degrees of quality and trust, nor carefully controlled for quality. Deficiencies in data quality still are a burning issue in many application areas, and become acute for practical applica-tions of knowledge discovery and data mining techniques [ 44 ]. Among traditional descriptive data mining techniques, association rules discovery identifies intra-transaction patterns in a database and describes how much the presence of a set of attributes in a database X  X  record (i.e., a transaction) implicates the presence of other distinct sets of attributes in the same record (respectively in the same trans-action). The quality of association rules is commonly evaluated by the support and confidence measures. The support of a rule measures the occurrence frequency of the pattern in the rule while the confidence is the measure of the strength of impli-cation. The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum sup-port and confidence thresholds. Besides support and confidence, other measures for knowledge quality evaluation (called interestingness measures) have been pro-posed in the literature with the purpose of supplying alternative indicators to the user in the understanding and use of the new discovered knowledge [ 31 , 57 ]. But, to illustrate the impact of low-quality data over discovered association rule qual-ity, one might legitimately wonder whether a so-called  X  X nteresting X  rule noted LHS  X  RHS (with the following semantics: L eft-H and S ide implies R ight-H and S ide) is meaningful when 30% of the LHS data are not up-to-date anymore, 20% of the RHS data are not accurate, and 15% of the LHS data come from a data source that is well-known for its bad reputation and lack of credibility. of data quality characterization and management techniques that should be inte-grated in the KDD process for improving the quality of discovered knowledge and mining results; secondly, we propose a method for scoring association rule qual-ity and a probabilistic cost model that predicts the cost of low-quality data on the quality of discovered association rules. This model is used to select the so-called  X  X egitimately interesting X  association rules. We evaluate our approach using the KDD-Cup-98 dataset and confirm our assumption that interestingness measures are necessary but not self-sufficient for ensuring the quality of discovered knowl-edge.
 on data quality characterization and management techniques. Section 3 presents a case study of quality awareness association rule mining and proposes a proba-bilistic decision model for estimating the cost of low-quality data on discovered association rules. In Sect. 4 , we evaluate our approach using the KDD-Cup-98 dataset for our experiments. Section 5 provides concluding remarks and guide-lines for future extensions of this work. 2 An overview of data quality characterization and management Maintaining a certain level of quality of data is challenging and cannot be limited to one-shot approaches addressing simpler, more abstract versions of the prob-lems of dirty or low-quality data [ 11 , 14 , 32 ]. Solving these problems requires highly domain-and context-dependent information and human expertise. Classi-cally, the database literature refers to data quality management as ensuring: (i) syntactic correctness (e.g., constraints enforcement that prevent  X  X arbage data X  from being entered into the database) and (ii) semantic correctness (i.e., data in traditional approach of data quality management has lead to techniques such as integrity constraints, concurrency control, and schema integration for distributed and heterogeneous systems. A broader vision of data quality management is pre-sented in this chapter (but still with a database orientation). In the last decade, literature on data and information quality across different research communities (including databases, statistics, workflow management, and knowledge engineer-ing) proposed a plethora of:  X  Data quality dimensions with various definitions depending on authors and  X  Data quality dimension classifications that are depending on the audience type:  X  Data quality metrics [ 14 , 25 , 42 , 46 , 47 , 68 ];  X  Conceptual models and methodologies [ 38 , 54 , 61 ];  X  Frameworks to improve or assess data quality in databases [ 1 , 26 , 51 , 63 , 64 ], paradigms for data quality characterization, modeling, measurement, and manage-ment. 2.1 Data quality dimensions Since 1980 with Brodie X  X  proposition [ 10 ], more than 200 dimensions have been collected to characterize data quality in the literature [ 4 , 26 , 51 , 62 ]. The most frequently mentioned data quality dimensions in the literature are accuracy, com-pleteness, timeliness, and consistency, with various definitions:  X  Accuracy is the extent to which collected data is free of errors [ 35 ]oritcanbe  X  Completeness is the percentage of the real-world information entered in the  X  Timeliness is the extent to which data are sufficiently up-to-date for a task  X  Consistency is the coherence of the same data represented in multiple copies into the following four categories: 1. Quality dimensions describing the quality of the management of data by the 2. Quality dimensions describing the quality of the representation of data based 3. Intrinsic data quality dimensions (e.g., accuracy, uniqueness, consistency, 4. Relative data quality dimensions with dependence on the user (e.g., user pref-In practice, assessing data quality in database systems has mainly been conducted by professional assessors with more and more cost-competitive auditing practices. Well-known approaches from industrial quality management and software quality assessment have been adapted for data quality and came up with an extension of metadata management. The use of metadata for data quality evaluation and im-provement have been advocated by many authors, e.g., [ 14 , 37 , 52 ]. Although con-siderable efforts have been invested in the development of metadata standard vo-cabularies for the exchange of information across different applications domains (e.g., for geographic information systems or digital libraries) including substantial work on data quality characterization in theses domains, the obvious fact is that in practice the quality metadata in many application domains remains a luxury. For data warehouse systems, many proposition concern the definition of quality models [ 27 , 59 , 61 ] with particular attention paid to data lineage and data trans-formation logs [ 13 ]. These metadata are very useful for the analysis and the in-terpretation of the probability distributions on the data, and also for debugging, implementing quality feedback loops, and analyzing the causes of data errors. 2.2 Data quality models Several propositions fully integrate the modeling and the management of quality metadata as a whole part of the database design. Among these process-oriented approaches, the TDQM program (Total Data Quality Management) proposed by [ 62 ] provides a methodology including the modeling of data quality in the Entity-Relationship (ER) conceptual data model. It proposes also guidelines for adding step-by-step data quality metadata on each element of the model (entity, attribute, association). Classically, the first step consists in modeling the application domain. Figure 2 is adapted from [ 62 ] and illustrates quality metadata with an example of the purchase of a product by a company. The following step of modeling consists in making explicit of the subjective parameters of data quality for each entity, at-tribute, or association type (thus constituting the first level of quality metadata). These subjective parameters (e.g., reputation) are added as qualifiers to the at-tributes of the ER conceptual model. Objective quality indicators are then added (e.g., age, consistency tests). They correspond to the measurable subjective pa-rameters. They are automatically computed (by statistical methods or constraints verification). The last step of quality metadata modeling consists in integrating the previously defined quality views at the global level of the conceptual model. turing data quality [ 38 , 53 , 54 ]. Most of the proposed data quality models rely on data quality metadata being available, such as data source publishing such in-formation. Unfortunately these approaches rely on precise and accurate metadata. However, such metadata are not always available with no unified standard describ-ing data quality dimensions. 2.3 Data quality measures and quality metadata generation Generating and managing statistical indicators of data quality have been the pri-mary focus of methods of imputation: i.e., inferring missing data from statis-tical patterns of available data, predicting accuracy of the estimates based on the given data, data editing, and automating detection and handling of outliers [ 9 , 14 , 30 , 68 ]. Utilization of statistical techniques for improving correctness of databases through introduction of new integrity constraints were first proposed in [ 25 ]. The constraints are derived from the database instances using the conven-tional statistical techniques (e.g., sampling and regression), and every update of the database is validated against these constraints. If an update does not comply with them, then the data administrator is alerted and prompted to check correctness of the update. Since databases model a portion of the real world which constantly evolves, the data quality estimates become outdated as time passes. Therefore, the estimation process should be repeated periodically depending on the dynamics of what is being modeled. The general trend is the use of artificial intelligence methods (machine learning, knowledge representation schemes, management of uncertainty) for data validation [ 14 , 15 ]. The use of machine learning techniques for data validation and correction was first presented by Parsaye and Chignell: rules inferred from the database instances by machine learning methods were used to identify outliers in data and facilitate the data validation process. Another sim-ilar approach was proposed by [ 56 ].
 providing summaries that characterize data with typical values (e.g., medians and averages), variance, range, quantiles, and correlations. Used as a first pass, EDM methods can be advantageously employed for data pre-processing before carrying out more expensive analyses. EDM aims to be widely applicable while dealing with unfamiliar datasets. These techniques have a quick response time, and have results which are easy to interpret, to store, and to update. EDM can either be driven by the models to facilitate the use of parametric methods (model log-linear, for instance), or be driven by the data without any prior assumptions about inter-relationships between data. Well-known non-parametric techniques for exploring multivariate distributions (such as clustering, hierarchical, or neural networks) can be used. The EDM summaries (e.g., averages, standard deviations, medians, or other quantiles) can be used to characterize the data distribution, the correlations between attributes, or the center of the value distribution of a representative attribute. They can also be used to quantify and describe the dispersion of the attribute values around the center (form, density, symmetry, etc.). Other techniques are used to detect and cope with other problems on data, such as missing values, improbable outliers, and incomplete values. Concerning the techniques of analysis on missing data, the method of imputation through re-gression described by Little and Rubin [ 34 ] is generally used. Other methods such as Markov Chain Monte Carlo (MCMC) [ 55 ] are used to simulate data under the multivariate normal distribution assumption. Other references related to the prob-lem of missing values are described by Pearson [ 44 ]. Concerning the outliers: The techniques of detection are mainly control charts and various techniques based: (i) on a mathematical model, (ii) on geometrical methods for distance measurement in the dataset (called geometric outliers), or (iii) on the distribution (or the density) of data population [ 30 ] extended by the concept of local exception (called local distributional outliers) [ 9 ]. The interested reader is invited to read the survey of Pyle [ 48 ], in particular for the use of entropy as a preliminary data characterization measure ([ 48 ], Sect. 11.3), and [ 14 ] for a description of these techniques. 3 Quality-aware rule mining Our initial assumption is that the quality of an association rule depends on the quality of the data which the rule is computed from. This section will present the formal definitions of our approach that introduces data quality indicators and combines them for determining the quality of association rules. 3.1 Preliminary definitions for association rule quality Let I be a set of literals, called items .An association rule R is an expression LHS  X  RHS ,where LHS , RHS  X  I and LHS  X  RHS = X  . LHS and RHS are conjunctions of variables such as the extension of the L eft-H and S ide LHS of the rule is: g ( LHS ) = x 1  X  x 2  X  ...  X  x n and the extension of the R ight-H and S ide RHS of the rule is g ( RHS ) = y 1  X  y 2  X  ...  X  y n .
 pleteness, freshness, accuracy, consistency, completeness, credibility, etc.). Let q ( I on the quality dimension j ( I i  X  I ). The vector, that keeps the values of all qual-ity dimensions for each dataset I i is called quality vector and noted q ( I i ). The set of all possible quality vectors is called quality space Q .
 Definition 3.1 The quality of the association rule R is defined by a fusion function denoted  X  j specific for each quality dimension j that merges the components of the quality vectors of the datasets constituting the extension of the right-hand and left-hand sides of the rule. The quality of the rule R is defined as a k -dimensional vector such as: q ( R ) = by a weighted sum of the quality vector components of the rule as follows: with w j the weight of the quality dimension j .
 Definition 3.2 Let T be the domain of values of the quality score q j ( I i ) for the dataset I i on the quality dimension j . The fusion function denoted  X  j is commu-tative and associative such as:  X  j : T  X  T  X  T . The fusion function may have different definitions depending on the considered quality dimension j in order to suit the properties of each quality criterion.
 pleteness, and consistency) defined in Sect. 2.1 ,Table 1 presents several definition examples of the fusion function (per quality dimension) based on the combination of quality scores of the two datasets x and y of the rule x  X  y .
 that designates the rule as legitimately interesting (noted D 1 ), potentially interest-ing ( D 2 ), or not interesting ( D 3 ) based both on good interestingness measures and on the actual quality status of the datasets composing the left-hand and right-hand sides of the rule.
 P (i.e.,  X  X ith low-quality X ) with reference to one or more quality dimensions rel-evant to the application (e.g., freshness, accuracy, etc.), and P CC ( x ) denotes the probability that the item x will be classified as  X  X orrect X  (i.e.,  X  X ith correct qual-ity X  in the range of acceptable values for each pre-selected quality dimension). Also, P AE ( x ) represents the probability that the item x is  X  X ctually erroneous X  ( AE ) but detected correct, and P AC ( x ) represents the probability that it is  X  X ctu-ally correct X  ( AC ) but detected erroneous (see Fig. 3 ).
 3.2 Probabilistic decision model for quality-driven and cost-optimal association rule mining For an arbitrary average quality vector q ( R )  X  Q on the datasets in LHS  X  RHS of the rule R ,wedenoteby P ( q  X  Q | CC ) or f CC ( q ) the conditional probability that the average quality vector q corresponds to the datasets that are classified as correct ( CC ). Similarly, we denote by P ( q  X  Q | CE ) or f CE ( q ) the conditional probability that the average quality vector q corresponds to the datasets that are classified erroneous ( CE ). We denote by d the decision of the predicted class of teresting ( D 3 ), and by s the actual status of quality of the datasets upon which the rule has been computed.
 ingly, the joint and the conditional probability that the decision D i is taken, when the actual status of data quality is j (i.e., CC, CE, AE, AC ).
 association rule with the actual data quality status j of the datasets composing the two parts of the rule. As an illustrative example, Table 2 shows tentative unit costs developed by the staff of a direct marketing department on the basis of con-sideration of the consequences of the decisions on selecting and using the dis-covered association rules in both cases: With and without misclassification. In Ta b l e 2 , c 10 is the cost of a confident decision ( D 1 ) for the selection of a legit-imately interesting rule based on correct-quality data ( CC ). c 21 is the cost of a neutral decision ( D 2 ) for the selection of a potentially interesting rule based on low-quality data ( CE ). c 33 is the cost of a suspicious decision ( D 3 ) for the selec-tion of a not interesting rule based on low-quality data but actually detected as correct ( AC ). Based on the example presented in Table 2 where we can see how the cost of decisions could affect the result of the selection among interesting as-sociation rules (i.e., in the Top N list), we need to minimize the mean cost c that results from making such a decision. The corresponding mean cost c is written as follows: From the Bayes theorem, the following is true: where i = 1 , 2 , 3and j = CC , CE , AE , AC . The mean cost c in Eq. ( 4 ) based on Eq. ( 5 ) is written as follows: Let us also assume that q is the average quality vector drawn randomly from the space of all quality vectors of the item sets of the rule. The following equality holds for the conditional probability P ( d = D i | s = j ) : where i = 1 , 2 , 3and j = CC , CE , AE , AC . status is j . Every point q in the quality space Q belongs to the partitions of quality Q 1 or D 1 ,or D 2 or D 3 in such a way that its contribution to the mean cost is minimum. probability of P ( s = AC ) =  X  0 AC , the a priori probability of P ( s = AE ) =  X  0 AE and the a priori probability of P ( s = CE ) = 1  X  ( X  0 +  X  0 AE +  X  0 AC ) . follows: By using Eq. ( 7 ) and by dropping the dependent vector variable q ,Eq.( 6 )be-comes: 3.2.1 Cost-optimal selection of rule with misclassification In the case of misclassification, minimizing the mean cost  X  cinEq.( 11 ) will lead to the optimal selection for the three sets of rules which we denote by D 0 1 , D 0 2 ,and D 3 . To minimize the cost as follows: To D 0 1 iff: To D 0 2 iff: To D 0 3 iff: The three decision areas for rule selection are then defined as follows: threshold values  X  ,  X  ,and  X  (respectively for legitimately , potentially ,and not interesting rules) in the decision space (see Fig. 4 ). D 0 2 can be seen as the union of two subareas representing the potentially legitimately interesting rules and the potentially not interesting rules. 3.2.2 Cost-optimal selection of rule without misclassification classification region P ( s = CE ) could be simplified as 1  X   X  0 .  X  CE is equal to threshold values  X  ,  X  ,and  X  (respectively for legitimately, potentially, and not interesting rules) in the decision space that define concretely the decision regions based on the cost of rule selection decision with the following relationship:  X  = quality awareness in association rule mining for a predictive selection of legiti-mately interesting rules based on the data quality indicators. 4 Experiments and results In order to evaluate our decision model (in both cases with and without misclassi-fication), we built an experimental system. The system relies on a data generator that automatically generates data quality metadata with a priori known charac-teristics. This system also allows us to perform controlled studies so as to estab-lish data quality indicators and quality variations both on datasets and on discov-ered association rules. In this section, we present a set of experiments using the KDD-Cup-98 dataset from the UCI repository. 1 The KDD-Cup-98 dataset con-tains 191,779 records about individuals contacted in the 1997 mailing campaign. Each record is described by 479 non-target variables and two target variables in-dicating the  X  X espond X  /  X  X ot respond X  classes and the actual donation in dol-lars. About 5% of records are  X  X espond X  records and the rest are  X  X ot respond X  records. The KDD-Cup-98 competition task was to build a prediction model of the donation amount. The participants were contested on the sum of actual profit ( actual donation  X  $0 . 68 ) over the validation records with predicted donation greater than the mailing cost $0.68 (see [ 66 ] for details). Because we ignored the quality of the data collected during this campaign, we generated synthetic data quality indicators with different assumptions representing common data pollu-tions. In this experiment, our goal is to demonstrate that data quality variations may have a great impact on the significance of KDD-Cup-98 results (i.e., the top 10 discovered  X  X espond X  rules and profit predictions). Although data quality indi-cators do not affect the top 10 list of discovered association rules, they significantly change the reliability (and the quality) of this mining result and also the cost of the decisions relying on these rules.
 mensions (i.e., freshness, accuracy, completeness, and consistency), average qual-ity scores, and estimated probabilities per variable of the KDD-Cup-98 dataset are giveninTable 3 . For the sake of simplicity, we generated the quality dimension scores such as they are uniformly representative of the quality dimension of the values in the attribute domain. The average quality q per variable in Table 3 is computed from the equi-weighted function given in Eq. ( 2 ). f CC in Table 3 (also noted f CC ( q ( I i )) in our formalism) is the probability density that the dataset I i is considered  X  X orrect X  (i.e., with  X  X orrect quality X ) when the average quality score that the dataset I i is considered  X  X rroneous X  (i.e., with  X  X ow quality X ) when the average quality score of I i is q ( I i ) .
 with the confidence, the support (in number of records), and the quality scores. Ta b l e 4 shows the score per quality dimension and the average quality score for each association rule. The scores are computed from the definitions of the qual-ity dimensions given in Table 1 and the data quality scores previously given per attribute in Table 3 .
 decision cost for rule selection respectively in the two cases: With and without misclassification. We use the decision costs previously mentioned in Table 2 for classifying the rules based on the quality of their data. 4.1 Quality and cost of association rules without misclassification amplitude of decision costs for the rule selection based on Table 2 and Eq. ( 11 ) for the top 10 rules discovered by [ 66 ]. Figure 5 shows this case for the a priori probability  X  0 = 0 . 200 in the absence of misclassification region (i.e., thresholds for the rule selection with the a priori probability  X  0 = 0 . 200. We ob-tain the following thresholds:  X  = 0 . 0131579,  X  = 0 . 125, and  X  = 2 . 25. In order to be consistent with the conditional independency of the quality vector compo-nents we also need to take the logarithms of the thresholds values. By doing this we obtain: log ( X ) = X  1 . 8808, log ( X ) = X  0 . 9031, and log ( X ) = 0 . 3522. Based on the values for these thresholds, we can assign each rule to one of the three decision areas. Table 5 shows the profit per rule predicted by [ 66 ], the decision cost of the rule selection computed from Table 2 and the decision area per rule. teresting among the top 10 rules considering the quality of data they are computed from. With data quality awareness, the other rules (R2, R3, R4, R6, R8) are not interesting despite a good rank in the top 10 list. It is also interesting to notice that the profit per rule predicted by [ 66 ] may be considerably counterbalanced by the cost of the rule computed from low-quality data (although it depends from initial costs defined in Table 2 ). The second best rule R2 whose predicted profit is $61.73 has a cost of $109.5 and is classified as not interesting due to the low quality of its datasets.
 composing the rules. Based on the costs in Table 2 ,Fig. 6 shows the behavior of the decision cost for the rule selection when data quality varies from the initial average quality (Init Quality) down to  X  10,  X  30, and  X  50%andupto + 10, + 30, and + 50% for the a priori probability  X  0 = 0 . 200 in the absence of misclassifica-tion.
 the rules increases the cost of these rules with various amplitudes shown in Fig. 7 (with a maximal quality degradation noted qual  X  50% and a maximal quality ame-lioration noted qual + 50%). Data quality amelioration (from + 30 to + 50%) im-plies a stabilization trend of the decision cost for the rule selection (between $20 and $70).
 lection change simultaneously with the data quality variations. Among the top 10 interesting rules discovered by [ 66 ] with the initial data quality (noted Init Qual), five rules (R1, R5, R7, R9, and R10) were potentially worth being selected based on their average data quality and five rules were not interesting (R2, R3, R4, R6, and R8). While increasing data quality up to + 30%, three rules become legiti-mately interesting (R5, R7, and R9). The others become potentially interesting . 4.2 Quality and cost of association rules with misclassification In the case of misclassification (with f AC = f AE = f CC ) we observe that the cost is high (between $248.40 and $594.30 compared to the case without misclas-sification between $34.7 and $190). The amplitude of the decision cost per rule depending on the a priori probability is stable (see Fig. 9 ) and the rule costs are stratified per rule.
 we compute the values of the three decision thresholds for rule selection with mis-classification and we obtain:  X  = 0 . 1053,  X  = 0 . 1667, and  X  = 4 . 6667. Based on the values for these thresholds, we can assign the rules to one of the three decision areas (see Table 6 ). In the case of misclassification with the a priori probability  X  0 = 0 . 200 it is interesting to notice that the cost per rule may be increased from 1.7 to 13.5 times (respectively for R8 and for R5) compared to the case of correct classification. This is mainly due to the cost of: (i) confident decisions for the se-lection of the rules computed from low-quality data that are incorrectly classified, and (ii) suspicious decisions for the selection of the rules computed from correct-quality that are incorrectly classified. With different variations on the average qual-ity of the datasets composing the rules (with  X  10,  X  30,  X  50% and + 10, + 30, + 50% from Init Quality) and based on the costs given in Table 2 in the case of misclassification, we study the behavior of the decision cost for the rule selection. Figure 10 shows that the costs are relatively stable with smaller amplitudes and more distinct and staggered cost ranges than in the case without misclassification (see Figs. 10 and 11 compared to Figs. 6 and 7 ) with the exceptions of the maxima of data quality variations (i.e.,  X  50%) when the misclassification has more impact on decision costs. In Fig. 12 , only R7 is legitimately interesting among the top 10 rules discovered by [ 66 ] with the initial data quality (Init Qual). Three rules are not interesting (R8, R3, and R4) and the other six rules are potentially interesting . Misclassification globally attenuates the  X  X erdict X  that classifies each rule corre-spondingly to one of the decision areas for the legitimately , potentially ,or not interesting rules. Some rules (e.g., R8) keep the same behavior with or without misclassification when data quality varies.
 research perspectives for both association rule mining and data quality man-agement: First, for proposing a post-filtering rule process based on data quality indicators and optimal decision costs for rule selection, and secondly, for the optimal scheduling of data quality improvement activities (with data cleaning techniques and Extraction-Transformation-Loading tools for instance) that could be relevantly used on targeted datasets for reaching the quality level expected for the mining results and for setting up quality improvement strategies for the KDD process. 5Conclusion This paper gives an overview of data quality characterization and management techniques that can be employed for improving quality awareness of knowledge discovery and data mining processes. The original contribution of this paper is twofold: first, we propose a method for scoring the quality of association rules that combines and integrates measures of data quality; secondly, we propose a prob-abilistic cost model for estimating the cost of selecting legitimately interesting association rules based on correct-quality data. The model defines the thresholds of three decision areas for the predicted class of the discovered rules (i.e., legiti-mately , potentially ,or not interesting ). To validate our approach, our experiments on the KDD-Cup-98 dataset consisted of: (i) generating synthetic data quality in-dicators, (ii) computing the average quality of the top 10  X  X espond X  association rules discovered by [ 66 ], (iii) computing the cost of selecting low-versus correct-quality rules and the decision areas they belong to, (iv) examining the cost and the decision status for rule selection when the quality of underlying data varies. sures are not self-sufficient and the quality of association rules depends on the quality of the data which the rules are computed from. Data quality includes various dimensions (such as data freshness, accuracy, completeness, etc.) which should be combined and exploited for effective data quality-aware mining. model, to propose error estimation and to validate the model with experiments on large real biomedical datasets. This will extend our work in [ 6 ] with exploiting operational bio-data quality indicators for improving data quality awareness in biomedical association rule discovery.
 References
