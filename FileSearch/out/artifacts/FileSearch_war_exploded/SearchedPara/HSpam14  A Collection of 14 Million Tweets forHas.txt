 Hashtag facilitates information di ff usion in Twitter by creating dy-namic and virtual communities for information aggregation from all Twitter users. Because hashtags serve as additional channels for one X  X  tweets to be potentially accessed by other users than her own followers, hashtags are targeted for spamming purposes ( e.g., hash-tag hijacking), particularly the popular and trending hashtags. Al-though much e ff ort has been devoted to fighting against email spam, limited studies are on hashtag-oriented spam in tweets. In this paper, we collected 14 million tweets that matched some trend-ing hashtags in two months X  time and then conducted systematic annotation of the tweets being spam and ham ( i.e., non-spam). We name the annotated dataset HSpam14 . Our annotation process in-cludes four major steps: (i) heuristic-based selection to search for tweets that are more likely to be spam, (ii) near-duplicate cluster based annotation to firstly group similar tweets into clusters and then label the clusters, (iii) reliable ham tweets detection to la-bel tweets that are non-spam, and (iv) Expectation-Maximization (EM)-based label prediction to predict the labels of remaining un-labeled tweets. One major contribution of this work is the creation of HSpam14 dataset, which can be used for hashtag-oriented spam research in tweets. Another contribution is the observations made from the preliminary analysis of the HSpam14 dataset.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Twitter; tweets; hashtag; spam
Popularity of micro-blogging sites such as Twitter and Weibo has drawn attention of not only legitimate users but also spam-mers. Spammers create accounts to promote their products and services or even steal accounts of legitimate users for the same. The issues become more complicated when third-party applications post to Twitter on behalf of the legitimate users with the permission granted by the users. As the result, tweets posted on one X  X  timeline may not be solely composed by the account owner. The tweets from third-party applications may contain spam information. Grier et al. reported that 0.13% of message advertised on Twitter are clicked which is two orders of magnitude higher than email spam [13]. Due to the growing interest of spammers in micro-blogging sites, spam becomes a pervasive problem on Twitter. In their recent study, Thomas et al. reported that 17% of spam users exploit hashtags to make their tweets visible in search and trending topics [35]. Hi-jacking trending topics and popular search terms for spamming has also been observed in web spam and blog spam [14, 40].

Most existing studies on Twitter spam focus on identification of spammers for account blocking. This approach is less e ff spammers who may act as legitimate users by posting non-spam content regularly. This approach may even hurt a legitimate user who happens to grant permission to a third-party application which posts spammy tweets under her username. This legitimate account may be blocked because of the spam tweets posted by the third-party application. While identifying and blocking spammer ac-counts remain a crucial and challenging task, tweet-level spam de-tection is essential to fight against spamming in a more fine-grained level. In this paper, we take the first step to create a large scale tweet collection for hashtag-oriented spam research. Among all di types of spamming, we mainly focus on the identification and an-notation of tweets with hashtags, because hashtags serve as chan-nels to increase the visibility of spam tweets. The cost of hijacking hashtags is very low with the availability of trending hashtags pub-lished on many web sites including Twitter.

We make two major contributions in this research. The first con-tribution is the creation of HSpam14 , a collection of 14 million annotated tweets in English for hashtag-oriented spam research. We collected trending hashtags on daily basis from hashtags.org and used the collected hashtags as query keywords to collect tweets through Twitter streaming API, for two months X  time. We then an-notated the 14 million tweets with four major steps: (i) Heuristic-based tweets selection to search for tweets that are likely to be spam. The selection was based on popular hashtags, and keywords related to adult content, money gain and others. (ii) Near-duplicate cluster based annotation to group similar tweets into clusters and then label the clusters. Tweets are clustered by using MinHash based algorithm. The assumption is that spam tweets are likely to be posted many times with minor changes for more visibility. The annotation process includes both manual annotation and confident k -nearest neighbor based annotation with human intervention in the process. Tweets posted by the same users and tweets containing links from the same domains are also clustered and annotated in this step. (iii) Reliable ham tweets detection to label tweets that are non-spam (also known as ham). Using the labeled tweets in the last step, we have a reasonably large number of examples which are uti-lized to build a classifier to detect reliable ham tweets. A reliable ham tweet does not contain any word that appears with higher probability in spam tweets than other tweets ( i.e., the labeled ham tweets and unlabeled tweets). (iv) EM-based label prediction to predict the labels of the remain-ing unlabeled tweets using an Expectation-Maximization (EM) algorithm. At this step, we have 2.387 million spam tweets and 4.897 million ham tweets already labeled. We then utilize EM-based label prediction algorithm to label the remaining tweets. Through manual inspections after each annotation step, the preci-sion of the labels is above 0.94. We believe such quality of label-ing is good enough for analyzing hashtag-oriented spam tweets for tweet-level spam detection.

The second contribution made in this paper is the preliminary analysis of the HSpam14 dataset. In our analysis, we made the fol-lowing observations. First, spam and ham tweets do not di in the usage of mentions, but the usage of hashtags is significantly di ff erent. More specifically, 76% of ham tweets have no hashtags, and 97% of them have at most 2 hashtags. For spam tweets, only 37% of them have no hashtags, and 40% of them have 3 or more hashtags. Second, spam tweets are likely to contain hashtags in capital letters and with the word  X  X ollow X  as part of the hashtag. The hashtags mainly present in ham tweets are usually in lower case letters or mixture of lower and upper cases, and the hashtags are more topic specific. We also propose a measure named Spammy Index to quantify the extent a hashtag is being used in spam tweets. Last, we observe that spammy hashtags are more likely to co-occur with spammy hashtags. Spam is prevalent in all types of online communication medium. There are di ff erent types of spam: email spam [9], web spam [33], video spam [2], micro-blog spam [1], comment spam [28], and re-view spam [17], to name a few. Spammers have used di strategies in di ff erent platforms, and they also keep changing their strategies to hide themselves from spam detection systems [5]. Re-cently, crowdsourcing systems have also been exploited for post-ing malicious content [21, 37]. Spam detection systems use a wide range of information such as content, link, user behavior, as well as HTTP session information to tackle spam problem [33]. Next, we brief the spam on other platforms ( e.g., email, web, comment, and product review), and then review the spam on Twitter.
 Spam on Email, Web, Comment, and Product Review. Email spam delivers unwanted advertisement, fraud scheme, promotion, or computer malwares designed to hijack the recipient X  X  computer. To identify such emails, machine learning techniques as well as whitelisting and blacklisting of senders, domains or IP addresses have been used [9]. Various techniques such as language model, content duplication, link-based trust and distrust propagation, graph based techniques, and user behavior modeling have been applied to fight against web spamming which can be in the form of con-tent spamming, link spamming, cloaking and click spamming, and comment spamming [5, 28, 33].

Online product reviews are valuable source of information for potential customers to find the opinions of other users about prod-ucts before purchase. Due to the importance of product review, spammers are attracted to promote their products and defame the competitors X  products. Review content and reviewers X  behavior are utilized to detect spam reviews [17, 24, 29]. For example, a topic model based approach was proposed for spam detection in prod-uct reviews in [23]. Features derived from part-of-speech tag, n-gram, and sentiment of the reviews, have been used to detect such spams [22, 31]. As spammers are likely to post many pieces of similar content, near duplicate detection techniques have been ex-ploited to identify such similar content and also the spammers [3, 6, 38]. Duplicate detection and classification technique have also been used for spam review detection [17, 18]. They considered all the duplicate reviews as spam reviews. However, it is not valid to consider all duplicate micro-blog posts as spam posts.
 Spam on Twitter. Thomas et al. reported that there is an under-ground market in Twitter network which provides spam-as-service in a recent study [35]. In their study, they also reported that 77% of spam accounts are identified by Twitter within the first day of cre-ation, and 92% of spam accounts within first three days of creation. The authors also made the observations that 89% of spam accounts have fewer than 10 followers and 17% of spam users exploit hash-tags to make their tweets visible in search and trending topics [35]. This is similar phenomena as in web spam and blog spam, where trending topics and popular search terms are hijacked for spam-ming [14, 40]. However, there is no existing study specifically on hashtag-oriented spam detection.

The reflexive reciprocity indicates that many users simply fol-low back when they are followed by someone for the sake of cour-tesy [39]. It is not di ffi cult for spammers to gain a relatively large number of followers in Twitter. Realizing this fact, social honeypot are deployed to harvest deceptive spam profiles by Lee et al. [19]. The profiles that are captured by social honeypot are classified us-ing standard classification technique. Other features derived from user demographics, follower / following social graph, tweet content and temporal aspect of user behavior have also been analyzed and used to identify content polluter [20]. Castillo et al. analyzed the credibility of tweets on trending topics based on users X  tweeting and retweeting behaviors, tweet content and link present in the tweets [4]. We believe that many of these features can be used for hashtag-oriented spam detection. However, before such research can be conducted, a benchmark dataset needs to be constructed.
User experience in Twitter is adversely a ff ected by spam social bots. Social bots are programs that automatically produce content and interact with humans on social media. Social bots post tweets about popular and focused topics and follow back the users who follow the bots [27]. Using such simple strategies bots could get high influence in Twitter and may pollute timeline of users. These spam bots can be detected by social honeypot traps [19] and also based on features derived from temporal behavior, tweet content, and user profile [7, 10]. In our data collection, we also observe that many tweets share very similar format and are posted by bots. However, not all Twitter users are equally vulnerable to social bots, and the behaviors that make users more susceptible to social bots have been studied in [36].

Other studies on Twitter spam include the studies on the link structure of the users and the links to external resources. As in Web, spammers have also used link farming technique to promote content in Twitter [12, 34]. Tan et al. [34] reported that spammers and non-spammers belong to di ff erent communities in user graph. Spammers may also include unrelated links with trending words ( e.g., hashtags) in tweets [1]. To detect such spam tweets, matrix factorization model was proposed by Hu et al. [15] to learn lexi-cal information from external spam resources. The authors further proposed online learning algorithms to cope with the fast evolution of social spammer in [16].
To collect data for hashtag-oriented spam tweet research, one key issue is to identify candidate hashtags. The tweets containing these candidate hashtags are then collected and labeled. Without the lux-ury of accessing all tweets or all hashtags, we rely on the trend-ing topics ( i.e., hashtags) reported on Hashtags.org. 1 Hashtags.org reports trending hashtags in three categories: trending up , trend-ing down , and most popular hashtags . As expected, we observe that the hashtags in most popular hashtags category do not change much for many days. To cover more varieties of hashtags in our data collection, we used the hashtags in trending up and trending down categories as query keywords to search for tweets on daily basis. On each day, we collect the trending hashtags in these two categories and we then use each collected hashtag as a query key-word to collect tweets using Twitter X  X  streaming API for that day. A tweet is collected through the API if it contains the query key-word as a hashtag , word , or link in its content. On average 135 hashtags were used as query keywords on each day. The data col-lection process last for two months, May and June 2013.

In total, we collected 24.36 million tweets, which were published by 11.97 million users. The collected tweets contain 20.21 mil-lion hashtags and 6.97 million hyperlinks. After resolving the short URLs, there are 3.43 million distinct URLs. Ignoring the deadlinks and the links that require authentication, 2.05 million web pages were successfully downloaded. However, we have not fully uti-lized these web pages for this labeling process.

Among the collected tweets and web pages, 14.07 million tweets and 1.37 web pages are in English. 3 In our following discussion, we mainly focus on the 14.07 million (or simply 14 million in ap-proximation) tweets in English. The same set of tweets will be annotated with spam and ham labels in the next section. User Distribution. The 14 million English tweets were posted by 7.25 million users and the distribution of the number of tweets per user is plotted in Figure 1(a). Shown in the figure, the data demon-strates a typical power-law distribution that is widely observable in many social related studies. While more than 4.83 million users each contributed only one tweet, the most active user contributed 754 tweets to this collection. Recall that this collection covers two-month period and the tweets were collected by using trending hash-tags as queries ( i.e., biased sampling). The few extremely active users may post a larger number of tweets regularly or post many tweets targeting on some trending hashtags. Because we do not have the full profiles of the users in this data collection, the un-derlying reasons cannot be verified. In our following analysis, we mainly focus on the content of the tweets, particularly the usages of hashtags and mentions.
 Usages of Hashtags and Mentions. Figure 1(b) plots the fre-quency distributions of hashtags and mentions. This figure shows that the usages of both hashtags and mentions follow power-law distributions with slightly di ff erent slopes in the log-log plot. On the one hand, this figure suggests that hashtags are more commonly used than mentions in tweets, particularly the extremely popular ones. On the other hand, a small set of usernames are frequently Table 1: The top-20 most popular hashtags. All these 20 hash-tags appeared as trending up / down hashtags which were used for sampling the tweets.
 Hashtag (#) Freq. Hashtag (#) Freq.
 TEAMFOLLOWBACK 666,328 SougoFollow 197,525 TFBJP 527,176 ipad 195,375 gameinsight 510,504 FOLLOWBACK 177,109 android 341,240 THF 165,762 OPENFOLLOW 332,857 FOLLOWNGAIN 149,916 FF 293,748 500aday 146,005 androidgames 286,706 AUTOFOLLOW 141,214 RETWEET 250,992 MUSTFOLLOW 138,040 RT 235,137 TEAMHITFOLLOW 136,043 IPADGAMES 232,335 MUSIC 129,852 being mentioned in tweets. A mention in a tweet is also a form of hyperlink, and clicking a mention leads to the user profile page where much more information about this user is displayed ( e.g., user description and recent tweets). For this reason, mention can be intentionally used for spamming purposes, similar to hashtags
Table 1 lists the top-20 most popular hashtags in the 14 million tweets. Observe that the most popular hashtag #TEAMFOLLOW-BACK appears in more than half million tweets out of the 14 mil-lion. Moreover, 40% of the top-20 most popular hashtags contain the word  X  follow  X . Many of the usernames among the popular men-tions contain the word  X  follow  X . In fact, a quick check of the top-20 most mentioned usernames shows that 11 usernames 5 have been suspended by Twitter. Among the suspended usernames, nearly half contain the word  X  follow  X .

Figure 1(c) plots the per-tweet distribution of hashtags and men-tions. Note that the y-axis is in log-scale. Both distributions show the same interesting pattern: (i) the number of tweets having 1 to 4 hashtags / mentions drops significantly along the increase of the number of hashtags / mentions; (ii) the rate of dropping becomes much smaller for tweets containing 4 to 10 hashtags / mentions; and (iii) the number of tweets again drop significantly with the increase of contained hashtags / mentions from 10 onwards. As expected, very few tweets contain more than 14 mentions or more than 18 hashtags, given the 140-character length limit of tweets.
There are three major challenges in manually labeling millions of tweets: (i) large number of instances , i.e., it is infeasible to man-ually check every single tweet to determine its label as in many other labeling e ff orts [8], (ii) limited amount of information carried by each tweet because of its shortness, and (iii) lack of standard cri-teria for the classification of spam and ham tweets.

To address the first two challenges, we propose to group the near-duplicate tweets into clusters. Spammers keep changing their be-havioral patterns to avoid detection; however, they keep on posting similar promotional tweets exploiting popular keywords. Hence, clustering of near-duplicate tweets captures promotional tweets ef-fectively despite change in behavioral patterns of spammers. Be-cause the tweets in each near-duplicate cluster are nearly identical to each other, the same label applies to all the tweets in the same cluster. The near-duplicate clustering also helps in partially ad-dressing the second challenge. One of the major characteristics of spammers is to post near-duplicate content in repetitive man-ner. If a larger number of near-duplicate tweets are from very few users, then it is relatively easier for us to label spam tweets even if each tweet contain very limited semantics. During the labeling process, we may also access all tweets by a particular user in our data collection to get more contextual information. Regarding the last challenge, we reference to the existing studies on spam detec-tion ( e.g., tweets about quick money gain, adult content are consid-ered spammy). In the context of hashtag-oriented spam detection, we also consider tweets containing semantically unrelated hashtags and quick followership gain as spam tweets.
 Figure 2 illustrates the four main steps in the annotation process. We now brief the four major steps. More details of the steps and the labeling criteria are described in the next sub-sections. Heuristics-based Tweet Selection . It is reasonable to assume that majority of tweets are ham. To minimize human labeling e ff necessary to label only the tweets that are more likely to be spams. To this end, we selected the tweets that contain most popular hash-tags (our assumption is that these hashtags are likely to be hijacked for spamming). We also select the tweets that contain money gain, adult content related keywords, and promoting purposes.
 Cluster-based Manual Annotation . Selected tweets are grouped into near-duplicate clusters. A subset of clusters are then labeled with spam or ham, and all the tweets in the same cluster share the same label. The near-duplicate clusters are obtained by MinHash algorithm [3]. With an initial set of clusters labeled, we adopt k nearest-neighbor approach to  X  X row X  the labels as in [8]. A cluster is labeled by kNN if the prediction of the label is very confident (based on the percentage of the nearest neighbors in the same class), and manually labeled otherwise. Spam tweets may be posted by some spam user accounts. For the top 10,000 users who posted the largest number of tweets in our dataset, we conduct user-based tweet clustering ( i.e., all tweets in each cluster are posted by the same user) and label the clusters. Similarly, based on the domain names of the links embedded in tweets, we cluster the tweets that contain links from the same domain, and label the clusters. The top 10,000 most frequent domains in our dataset are considered. Reliable Ham Tweet Prediction . By the assumption that majority tweets are ham, a large number of tweets, particularly the unse-lected tweets left by the heuristics based selection, remain unla-beled. Here, we adopt the approach in learning from positive ex-amples only in classification [25] to determine reliable ham tweets. Label Prediction by EM Algorithm . After getting a reasonably large number of reliable labels of both spam and ham tweets, the EM algorithm by Nigam et al. [30] for learning from labeled and unlabeled data is adopted to predict the labels of the remaining tweets.
We heuristically target on the tweets that are most likely to be spam, with the assumption that most tweets are ham. More specifi-cally, we select three sets of tweets: (i) tweets containing any of the top-100 most popular hashtags in our collection, (ii) tweets con-taining adult content related keywords, and (iii) tweets containing keywords related to quick money gain, lucky draw, free gift etc. .
The first set of tweets is selected based on the heuristic that spam-mers are likely to exploit popular hashtags to make their content visible to broader audience. Table 1 lists the top-20 most popu-lar hashtags, which is a subset of the 100 selected hashtags. The second set of the selected tweets aligns with most other spam de-tection tasks in email and web domains. A list of keywords related to adult content is used for this purpose. 6 For the third set of tweets selection, we used the following 23 keywords to capture the tweets related to quick money gain, lucky draw, and free gift: awesome, business, hurray, earn, bargain, deal, adventure, money, click, face-book, want, incredible, gift, money, amazon, ebay, amazing, prize, lucky, check out, shopping, sale, giveaway . Note that, there are tweets repeatedly posted by users with similar content, for promot-ing purposes related to major websites like Facebook, Amazon, and eBay. Therefore, these website names are included as keywords.
As the result, 7.10 million tweets posted by 3.93 million users were selected by keyword match. A tweet is selected if the tweet contains any of the selected hashtags or keywords in its content.
To label the tweets more e ff ectively, we group the near-duplicate tweets into clusters using MinHash algorithm [3]. The preprocess-ing of the tweets includes removal of links, removal of mentions ( i.e., @username ), removal of the  X # X  symbols but keep the words in the hashtags, and removal of stop-words using the stop-word list provided in Lucene 7 .

Both MinHash and SimHash are widely used for detecting near-duplicate documents [32]. Here, we adopt the MinHash algorithm on tweet X  X  shinglings for duplicate detection [26]. We consider two tweets t 1 and t 2 are (near-)duplicates if minimum hash values of their uni-gram , bi-gram , and tri-gram representations are the same. More specifically, let h (  X  ) be a hash function, and let tweet t be a n -word sequence  X  w 1 , w 2 ,... w n  X  . The minimum hash value of uni-gram representation of t is h ( w i ) i ff h ( w i )  X  h ( w 1  X  i , j  X  n and i , j . The minimum hash value of bi-gram representation of t is h ( w i w i + 1 ) i ff h ( w i w i any 1  X  i , j  X  n  X  1 and i , j . The minimum hash value of tri-gram representation is computed in a similar manner. For sim-uni-gram, bi-gram, and tri-gram representations of tweet t . Then t and t 2 are near-duplicates i ff h 1 ( t 1 ) = h 1 ( t 2 ) , h h ( t 1 ) = h 3 ( t 2 ) . In our Java implementation, we used the hash-Code() method in String class to get the hash value of a n-gram ( i.e., a word or a shingling). The clustering process is computationally e ff ective, and can be achieved in one scan of the tweets.
 Result and Evaluation. Because our purpose is to label tweets more e ff ectively, the clusters containing fewer than 10 tweets are ignored from manual annotation. The remaining 40,549 clusters contain 3.13 million tweets posted by 1.52 million users. Fig-ure 3 plots the size distribution of these 40,549 clusters. Again, the plot demonstrates a power-law distribution with many clusters each contains 10 or slightly more tweets and very few extremely large clusters. The largest cluster contains 127,559 near-duplicate tweets, mainly due to hashtags #gameinsight , #android , and #an-droidgames . An example tweet is  X  X T @username: I X  X e collected 103,501 gold coins! LINK1 #android #androidgames #gameinsight X  .
In manual annotation, all the tweets in the same near-duplicate cluster will be annotated with the same label ( i.e., spam or ham). For this purpose, we must ensure that the tweets in the same clus-ter are indeed similar to each other, or in other words, are near-duplicate. Table 2 shows two randomly selected near-duplicate clusters containing 4 and 2 example tweets respectively, from each cluster. The examples show that tweets in the same clusters are indeed very similar to each other. However, there is a need to quan-titatively evaluate the quality of the clusters in terms of tweet sim-ilarity. To this end, we randomly selected 4000 clusters ( i.e., about 10% all clusters) and measured the intra-cluster and inter-cluster tweet similarities. For intra-cluster tweet similarity, two tweets are
Table 2: Example tweets from two near-duplicate clusters Table 3: Measures of intra-/ inter-cluster samples of tweets randomly selected from each cluster; for inter-cluster tweet simi-larity, two tweets are randomly selected from two di ff erent clusters without replacement.

Table 3 reports tweet similarity in terms of Jaccard coe ffi and cosine similarity. Cosine similarity is computed based on the bag-of-words model with TFIDF weighting. The length di ff between the randomly selected pairs of tweets in number of words are also reported. Shown in Table 3, tweets from the same cluster show high similarity in terms of both Jaccard coe ffi cient and cosine similarity with values above 0.91 and 0.95 respectively. On the other hand, inter-cluster similarity of tweets is dramatically lower at 0.019 and 0.035 respectively for Jaccard coe ffi cient and cosine similarity. In terms of length di ff erence, tweets from the same clus-ter are nearly identical with length di ff erence of 0.39 word, while a pair of tweets from di ff erent clusters di ff er on average 5.59 words.
In summary, the clustering process is able to produce clusters with high intra-cluster similarity and low inter-cluster similarity. Tweets in the same cluster are of very similar length.
Labeling tweets to be spam or ham is non-trivial for two rea-sons. First, there is no very specific definition of  X  X pam X . However, many definitions, including the description in Wikipedia 9 the phrase  X  X nsolicited message X , and the words  X  X dvertisement X  Table 4: Initial manual annotation of near-duplicate clusters and  X  X epeatedly X . Second, because of the length limit, tweets are very short, and often do not come with context. The notion of near-duplicate clusters partially addresses these two challenges. Each near-duplicate cluster lists the near- X  X epeated X  tweets and it is easy to identify whether the repeated tweets are from very few users or from some specific apps. Because of the grouping of similar tweets, it is also relatively easy to identify whether these similar tweets were posted because of some important events / topics ( e.g., the 4 tweets in the first cluster in Table 2) or were posted for ad-vertisement purposes ( e.g., the 2 tweets in the second cluster in Ta-ble 2). In other words, we are able to infer the context of the tweets in a collective manner. By considering the tweets in each near-duplicate cluster, the users who posted these tweets, and also the usage of hashtags and links, we are able to label the near-duplicate clusters.

We label tweets in a duplicate-cluster to be spam if the tweets in this cluster satisfy one of the five conditions : (i) for quick money gain or quick gain of followers; (ii) repeatedly posted for adver-tisement, promoting products / services, and request for retweet or o ff ering benefits; (iii) about adult content; (iv) contain many unre-lated but popular hashtags; (v) automatically posted by bots for multiple times with near identical content. Note that, many bots / apps post legitimate tweets. For example, by linking Twitter and Foursqaure accounts, users have the option to share check-ins made in Foursqaure to Twitter. Such tweets will not be labeled as spam because these tweets are informative ( i.e., reporting lo-cation information of users). On the other hand, recall that in Section 4.2.1, we report that the largest cluster contains 127,559 near-duplicate tweets in the form of  X  X T @username: I X  X e collected 103,501 gold coins! LINK1 #android #androidgames #gameinsight X  . The only di ff erence between any two tweets is the number of gold coins and the links. Moreover, we observe that many users post only tweets in this form in his / her full user profile in Twitter. We consider such tweets less informative and label them as spam.
With the above criteria, we selectively label the near-duplicate clusters, because labeling more than 40 thousand near-duplicate clusters is infeasible. The labeling is conducted in two phases. In the first phase, the top-1000 largest clusters ranked by number of tweets in each cluster are labeled. Because of the power-law dis-tribution (see Figure 3), the labeling of the 1000 largest clusters cover a large number of tweets. In the second phase, we aim to cover more diverse tweets by considering the keywords we used in Section 4.1. For each of the keywords used for tweet selec-tion, we label around 10 largest clusters containing this keyword if the keyword is not covered in the top-1000 clusters labeled ear-lier. As examples, most clusters containing keywords  X  love  X  and  X  news  X  are ham clusters while clusters containing  X  gameinsight  X  and  X  iphonegame  X  are mostly spam. In the labeling process, we also encounter some cases where we cannot confidently label the tweets to be ham or spam even after manual inspection. In this case, we assign an  X  X nknown X  label to this cluster.
 Result and Analysis. After the two phase labeling, 1.82 million tweets in 2,104 near-duplicate clusters are manually labeled, re-ported in Table 4. Observe that, although the number of spam clus-ters is similar as the number of ham clusters, the number of spam tweets contained in these clusters is significantly larger than the number of ham tweets. This is expected as spam tweets are mostly posted repeatedly. More specifically, the median number of tweets in the labeled spam clusters is 437 while the median number of tweets in ham clusters is 26. The median number of users in the two sets of clusters are 383 and 25 respectively. It is also observed that 84% of labeled spam clusters have at least one user posting the same tweet multiple times. On the other hand, only 25% of labeled ham clusters have at least one user posting the same tweet multi-ple times. More interestingly, 47,454 users who posted at least one spam tweet have 0 followers. Without any followers, these users do not pollute the timelines of other users. However, their tweets may have adverse e ff ect on keyword-based or hashtag-based tweet search. The number of users who posted at least one ham tweet and have 0 followers is 4,822, about 10% of those who posted at least one spam tweet.
Using the 2,104 manually labeled clusters as ground truth, we are able to  X  X row X  labels in a more e ff ective manner with the help of k NN classifier. A similar approach has been adopted in creating benchmark dataset for a web image dataset [8].

The most important issue here is to ensure the quality of labels predicted by the k NN classifier. We address this issue by assigning a label to a cluster only if 80% of its k -nearest neighbors are from the same class ( i.e., spam or ham). Moreover, if this newly labeled cluster c k causes misclassification of a manually labeled cluster c m if the same k NN rule were applied to c m , then we consider this newly labeled cluster c k to be a di ffi cult cluster . The di clusters are then manually inspected and labeled. If a newly labeled cluster does not cause misclassification of any existing manually labeled cluster, then the label of this cluster assigned by k NN is considered as a confident label. The labeling process is conducted in batch mode and thus confidently labeled clusters in each batch are added as labeled examples for the next batch.

Algorithm 1 summarizes the main steps in k NN-based near du-plicate cluster labeling. The algorithm takes the set of manually labeled cluster C ` and unlabeled clusters C u as input, and returns clusters with confident labels C k as output, in four main steps. Step 1 . A subset of clusters C mk  X  C ` is obtained such that within C mk each cluster X  X  label is consistent with the label predicted based on the k NN rule (Lines 2 -5). More specifically, for each cluster c , its k nearest neighbors are obtained from the labeled set C function C N N = k N N ( c , k , C ` ) , where C N N denotes the set of c  X  X  nearest neighbors, k N N ( c , k , C ` ) finds the k nearest neighbors of c within the set C ` . In Line 4, the function ma j orit y Label ( C returns the number of neighbors from the majority class in C ( e.g., either spam or ham). The clusters in C mk will be used to identify the di ffi cult clusters labeled by k NN in later steps. Step 2 . The algorithm classifies unlabeled clusters in a batch mode manually labeled examples (Line 10). For the first batch, the set of training examples is initialized to be the set of manually labeled clusters C ` (Line 7). In the subsequent batches, the training exam-ples will include the newly labeled examples (Lines 22 -23). Step 3 . The algorithm verifies whether the newly labeled examples cause misclassification of some clusters in C mk (Lines 15 -21). More specifically, the k nearest neighbors of each cluster in C now searched from the set of training examples C t and the newly labeled batch C b using the function k N N ( c , k , C t  X  C number of neighbors from majority class is smaller than 80% of k , then we consider that the neighbors which are from the newly labeled batch cause the misclassification. These clusters will then Algorithm 1: k NN Annotation Input : Set of manually labeled clusters C `
Output : Set of labeled near-duplicate clusters C k 1 C mk = {} // clusters consistent with k NN result 2 foreach cluster c  X  C ` do 3 C N N = k N N ( c , k , C ` ) // find nearest neighbors 4 if ma j orit y Label ( C N N )  X  0 . 8  X  k then 5 C mk = C mk  X  X  c } 6 C d = {} 7 C t = C ` // set of training examples 8 C b = {} // a batch of labeled clusters 9 for unlabeled cluster c  X  C u do 10 if | C b | &lt; 0 . 2  X | C ` | then 11 C N N = k N N ( c , k , C t ) 12 if ma j orit y Label ( C N N )  X  0 . 8  X  k then 13 C b = C b  X  X  c } 14 else // verify the current batch 15 foreach cluster c  X  C mk do 16 C N N = k N N ( c , k , C t  X  C b ) 17 if ma j orit y Label ( C N N ) &lt; 0 . 8  X  k then 18 foreach neighbor cluster c n  X  C N N do 19 if c n  X  C b then 20 C b = C b  X  X  c n } 21 C d = C d  X  X  c n } 22 C t = C t  X  C b // add as training examples 23 C b = {} // start a new batch 24 Manually label clusters in C d and add to C t in Line 7 25 Repeat the labeling process (Lines 9 -24) till no more clusters can be labeled 26 Return C k = C t  X  C ` ; be removed from the batch and considered as di ffi cult clusters . The di ffi cult clusters are then manually labeled in the next step. The remaining confidently labeled clusters in C b will be added to C for the next batch labeling (Line 22).
 Step 4 . Observe that the predicted labels of unlabeled clusters in C u are dependent on the order of labeling because the training ex-amples C t are updated after each batch of labeling. For this reason, we run the labeling process (Lines 9 -23) multiple times. Before each run, we manually label the di ffi cult clusters resulted from the last run (Line 24) and add them to C t in Line 7 because these labels are manually assigned. Note that, for concise presentation, we do not explicitly add this  X  X uter loop X  in Algorithm 1. Instead, this step is stated in Line 25 and the iteration continues until there is no unlabeled cluster which has 80% of neighbors from same class. Note that some detailed control structures in the algorithm are ig-nored, for example, the last batch in each run may not reach the size of 0 . 2  X | C ` | .

For nearest neighbor search, the similarity between two clusters is the cosine similarity with TFIDF weighting of the word feature vectors for the two clusters, by aggregating all the tweets in each cluster. The value of k is set to 19 empirically in our k NN based annotation process. The size of each batch is about 420.
 Annotation Result. As the result of k NN-based annotation, 15,571 spam clusters and 15,913 ham clusters are labeled. In the labeling process, 1,178 clusters were identified to be di ffi cult clusters and were then manually labeled. Among the latter, 826 clusters were labeled ham and 275 were labeled spam clusters; the remaining 77 clusters were di ffi cult to label even after manual inspection. These 77 clusters are labeled  X  X nknown X  in our data collection.
After manual and k NN-based annotation, we have labeled more spam tweets than ham tweets. Because spam clusters often contain much repeated tweets, the number of spam tweets is significantly larger than the number of ham tweets. The reason is that, the man-ual annotation process was targeted on spam tweets by using the most suspicious keywords and most popular hashtags. The k NN-based annotation was guided by the then labeled clusters. In other words, most ham tweets are left untouched in the dataset.
We now label repeated tweets posted by spammers and tweets promoting specific links. Instead of restricting to the tweets that match keywords (Section 4.1), we consider all tweets in this step. User-based Cluster Annotation . In this step, tweets posted by the same user are grouped into near-duplicate clusters. Then the clus-ters with more than 10 tweets are labeled. The top 10,000 users who posted the largest number of tweets in our dataset are con-sidered. We observe that most user-based clusters are small clus-ters with fewer than 10 tweets. That is, most of users do not post many similar tweets. As the result, 980 user-based clusters each has more than 10 tweets need to be labeled, consisting of about 25 thousand tweets in total. Among these 980 clusters, 723 are la-beled spam (containing about 20 thousand tweets) and 120 clusters are labeled ham (about 2 thousand tweets). Note that, highly sim-ilar ham tweets may be posted by the same user repeatedly ( e.g., updates of bus services and weather reports). The remaining 137 are labeled unknown, after looking into the content of the tweets and user profiles ( e.g., number of followers / followees, number of tweets) .

Because we consider all tweets in this step, some of the tweets in user-based cluster may have already have been labeled in the earlier steps. If the labels are all from manual annotation, then the cluster is considered labeled. If the labels are given in the kNN annotation, these labels are considered as reference and may be corrected during this manual annotation process.
 Domain-based Cluster Annotation . Recall that a tweet may em-bed links to external sources. Here we aim to label the spam tweets that promote links from specific domains. To this end, the tweets that contain links from any of the top 10,000 most frequent domains in our collection are selected and then grouped into near-duplicate clusters based on domains (all the tweets in the same cluster contain links from the same domain). There are 9,038 such clusters each contains at least 10 tweets. Out of these 9,038 clusters, 4,398 are spam clusters (containing about 221 thousand tweets), and 3,785 are ham clusters (about 121 thousand tweets). The remaining 855 clusters could not be labeled even after manual inspection. Similar to that in user-based cluster annotation, the labels from earlier steps are utilized in this annotation process.

During the annotation process, we observe that domain-based cluster annotation is e ff ective in capturing distributed spam activi-ties in promoting online resources. Examples include links related to topics of working from home, best deals, and coupon codes. Note that, in our labeling process, clusters with tweet content that is clearly ham and links that are from major trustworthy sites ( e.g., BBC, CNN, NBA) are labeled ham without looking the content of the web pages. Clusters containing links from less popular domains are labeled by looking into the user profiles and the content of the linked pages. However, it is found that in most cases, tweets pro-vide good descriptions of the links. It is uncommon to have similar tweets linking to very di ff erent web pages.
 Until this step most ham tweets are left untouched in the dataset. In the next section, we aim to detect the ham tweets from the re-maining unlabeled data.
The remaining unlabeled tweets in our tweet collection may con-tain a large number of ham tweets, but also contain many spam tweets yet to be identified. Given the large number of remaining unlabeled tweets, this problem can be formulated as a PU Learning problem , i.e., learning from P ositive and U nlabeled examples [25].
Inspired by the approach for building text classifiers from posi-tive and unlabeled examples proposed in [25], we utilized the pos-itive words in spam tweets to identify reliable ham tweets. Positive words are the words that appear more frequently in spam tweets than other tweets. Example positive words are  X  followers  X ,  X  gain  X ,  X  gameinsight  X ,  X  please  X ,  X  win  X ,  X  iphone  X  etc. More specifically, let r d f ( w , D s ) = df ( w , D s ) | D word w in the set of spam tweets D s labeled so far, where d f ( w , D denotes the number of tweets containing word w in set D s r d f ( w , D h  X  D u ) be the relative document frequency of word w in the set of labeled ham tweets D h and the set of unlabeled tweets D u . Then w is a positive word if r d f ( w , D s ) &gt; r d f ( w , D Reliable ham tweets are those tweets that do not contain any pos-itive words. In our implementation, words that are shorter than 3 characters in length are ignored. Note that, number of characters in a hashtag does not include the  X # X  symbol.
 Result and Evaluation. This method is simple but found to be very e ff ective in identifying ham tweets. In total 4.093 million tweets are identified as reliable ham tweets. We evaluated the quality of the labeled tweets by manually inspecting a randomly selected sample of 1000 reliable ham tweets. Among them, 32 tweets are incor-rectly labeled as ham, hence the precision of the labels in reliable ham detection is 0.968. Following are three examples of reliable ham tweets detected in this step.  X  Why can X  X  I sleep  X  The first thunder sound scared me  X  @username hey :)) do u know what channel is CNN on???? Lol i forgot it...
Out of the 14 million tweets in our data collection, more than 7 million tweets are now labeled in the earlier steps. With the 2.387 million spam tweets and 4.897 million ham tweets, we have a sizable collection of labeled examples to build a reliable classi-fier for the prediction of labels of the remaining unlabeled tweets. To this end, we adopt the Expectation-Maximization (EM) algo-rithm proposed for text classification using labeled and unlabeled documents [30]. EM has also been used to reduce human e ff evaluating IR systems [11]. A Na X ve Bayes classifier is firstly con-structed by utilizing the existing labeled tweets. In the E-Step, this classifier is used to predict the labels of the remaining unlabeled tweets. In the M-Step, the probabilities of the word features are re-estimated based on the predicted labels. Then the classifier in the E-Step is updated accordingly and is used to re-classify the tweets in the M-Step. This process continues till the number of changes in the predicted labels is below 0.01% of the total unlabeled tweets.
As the result, 5.790 million ham tweets and 0.951 million spam tweets are labeled in this step. To evaluate the quality of the label-ing, we randomly selected 500 ham and 500 spam tweets labeled Table 5: Statistics of the labeled tweets in HSpam14 dataset Annotation step Spam (million) Ham (million)
Manual annotation 1.644 0.226 k NN-based annotation 0.502 0.455 User-based annotation 0.020 0.002 Domain-based annotation 0.221 0.121 Reliable ham tweet detection -4.093 EM-based annotation 0.951 5.790
Total 3.338 10.688 Figure 4: Cumulative percentage of hashtags and mentions per tweet, in spam tweets and ham tweets, respectively. by EM for manual inspection. The precisions for spam and ham labels are 0.94 and 0.96 respectively. After all these steps, we have labeled almost all the 14 million English tweets. Table 5 reports the number of tweets labeled as ham and spam in each step. Note that, the numbers reported under man-ual annotation are slightly larger than that reported in Table 4 be-cause the numbers here include the manual annotations conducted in k NN-based annotation step as well as the annotations made dur-ing the evaluation of subsequent steps. In summary, 3.338 mil-lion spam tweets and 10.688 million ham tweets are labeled in the HSpam14 dataset. There are two possible reasons for the relatively large number of spam tweets. First, the tweets in our collection were collected based on trending hashtags, which are more likely to be hijacked for spamming purpose. Second, many of the spam tweets are repetition of similar content, e.g., 127,559 near-duplicate tweets are due to the hashtag #gameinsight .
We believe the quality of the labels are reasonably high based on the manual inspection after each step, if no manual annotation is involved during that step. In this section, we provide a preliminary analysis of the HSpam14 dataset. We mainly focus on the usage of hashtags in spam and ham tweets. Because of the way the tweets were collected, our collection does not contain full user profiles, which limit the user-based analysis.
 Per-tweet Hashtag / Mention. Figures 4(a) and 4(b) plot the per-tweet usage distributions of hashtags and mentions in spam and ham tweets respectively. Observe that the mention distributions of spam and ham tweets are very similar to each other with ham tweets use slightly fewer number of mentions. For instance, 43% of ham tweets and 38% of spam tweets do not use mention. The hashtag per-tweet distributions are much more interesting. For ham tweets, 76% of tweets have no hashtags, and 97% of tweets have at most 2 hashtags. However, for spam tweets only 37% of them have no hashtags, and the cumulative percentage of tweets having 2 or fewer hashtags is 60%. The remaining 40% of tweets have 3 or 5000, and the hashtags are case-sensitive. more hashtags. That is, the spam tweets use hashtags much more aggressively than ham tweets, probably aiming for more visibility. Hashtag Spammy Index. We propose a measure named Spammy Index to quantify the extent a hashtag is being used in spam tweets. Given a hashtag t , its spammy index, denoted by si ( t ) , is defined in the following equation. In this equation, d f ( t , D ) denotes document frequency, or the num-ber of tweets containing hashtag t in tweets collection D ; D the set of spam tweets where D s  X  D . A hashtag is considered to be more spammy if (i) its document frequency is high, and (ii) the probability of being used in spam tweets is high.

The top-20 hashtags with the highest spammy indexes are re-ported in Table 6(a). For comparison, Table 6(b) lists the 20 hash-tags with the lowest spammy indexes among all hashtags with fre-quency more than 5,000. The two tables show that spammy hash-tags are more likely to use capital letters, probably to draw atten-tion. Frequent words appear in spammy hashtags are  X  X ollow X ,  X  X b X (follow back) and  X  X ame X . Hashtags with low spammy in-dexes are less likely to contain all capital letters and many of them are topic specific on music, sports, or functional tags like #job and #jobs . Comparing Table 6(a) and Table 1, most extremely pop-ular hashtags are hijacked for spamming purpose. Popular hash-tags such as #android , #ipad are hijacked for spamming purpose so they appeared as top 20 spammy hashtags. The popularity of these hashtags may attribute to the large number of near-duplicate tweets automatically generated by bot like #gameinsight .
 For the completeness of the analysis, we also computed the Odds Ratio (OR) of the words (including both case-insensitive hashtags after removing the # symbol and words appear in the content of tweets) in distinguishing spam tweets from ham tweets. The top-20 words with highest OR values are reported in Table 7. The words identified by high OR values are consistent with the hashtags of high spammy indexes, with the dominating word  X  X ollow X .
 Hashtag Co-occurrence Network. The co-occurrence graph for popular hashtags with frequency larger than 25,000 in HSpam14 is plotted in Figure 5. The size of node is proportional to the hash-tag frequency (in log 2 scale). The color of node is proportional to the hashtag X  X  spammy index (most spammy hashtags are in red, least spammy hashtags are in green, intermediate ones are in blue). We make two observations from this co-occurrence network. First, the spammy hashtags are more likely to co-occur with each other. Second, the less spammy hashtags are less likely to co-occur with other hashtags. This can be partially explained by the observation that 97% of ham tweets have at most 2 hashtags.
In this paper, we make the first step for tweet-level spam detec-tion focusing on hashtag-oriented tweet spam. The main contribu-tion of this work is the creation of HSpam14 data collection, which contains 3.338 million spam tweets and 10.688 million ham tweets, collected in two months X  time guided by trending hashtags. While the annotation was not made per tweet basis, the overall quality of the labels are good enough to perform analyses and studies on hashtag-oriented tweet spam detection. Our analysis on this dataset also shows di ff erent usage patterns of mentions and hashtags in spam and ham tweets, and shows that spammy hashtags are likely to co-occur with spammy hashtags.
A limitation of this data collection is the lack of full profiles of the users. As the tweets in this collection were collected with the guide of trending hashtags, we do not have full user profiles ( e.g., all tweets published by each user, list of her followers / in HSpam14 due to the restriction of Twitter steaming APIs. This may limit the study on user-level spam detection with this dataset. For the same reason, we only utilize very limited user-level fea-tures during the annotation process although many other user-level features ( e.g., temporal patterns, social connections) have demon-strated their e ff ectiveness in identifying spammers. Another limi-tation could be the limited keywords used in heuristic-based tweet selection. The existing keywords may not catch spam in specific domains ( e.g., healthcare and medical). Part of our future work is to improve the recall of the spam tweets in specific domains. Acknowledgements: This work was partially supported by Singa-pore MOE AcRF Tier-1 Grant RG142 / 14.
