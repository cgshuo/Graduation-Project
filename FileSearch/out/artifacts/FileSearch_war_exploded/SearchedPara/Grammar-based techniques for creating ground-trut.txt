 ORIGINAL PAPER Scott MacLean  X  George Labahn  X  Edward Lank  X  Mirette Marzouk  X  David Tausky Abstract Although publicly available, ground-truthed cor-pora have proven useful for training, evaluating, and compar-ing recognition systems in many domains, the availability of such corpora for sketch recognizers, and math recognizers in particular, is currently quite poor. This paper presents a general approach to creating large, ground-truthed corpora for structured sketch domains such as mathematics. In the approach, random sketch templates are generated automat-ically using a grammar model of the sketch domain. These templates are transcribed manually, then automatically anno-tated with ground-truth. The annotation procedure uses the generated sketch templates to find a matching between tran-scribed and generated symbols. A large, ground-truthed cor-pus of handwritten mathematical expressions presented in the paper illustrates the utility of the approach. 1 Introduction Many recognition domains have benefited from the crea-tion of large, realistic corpora of ground-truthed input. Such corpora are valuable for training, evaluation, and regression testing of individual recognition systems. They also facili-tate comparison between state-of-the-art recognizers. Acces-sible corpora enable the recognition contests which have proven useful for many fields. For example, the voice rec-ognition, facial recognition, and OCR communities have X  often with the help of third parties like NIST X  X reated large, publicly available corpora, standardized the problem state-ment for recognition, and held regular recognition competi-tions to evaluate state-of-the-art approaches. Such measures have enabled researchers in these fields to train sophisticated recognition models on real, representative data, and to objec-tively measure recognition accuracy, facilitating comparison between recognizers.

Unfortunately, the availability of ground-truthed corpora for sketch recognition domains is poor. Although math rec-ognition, our chosen domain, has a history dating back at least four decades (e.g. [ 1 , 10 , 11 ]), we are not aware of any publicly available corpus of hand-drawn math expres-sions. Instead, math recognition systems have been designed, trained, and evaluated largely on an ad hoc basis, using per-formance metrics specific to a particular recognizer imple-mentation (e.g. [ 3 , 5 , 9 , 17 ]). Such variegated metrics prohibit any meaningful comparison between systems.

Creating large corpora for any hand-drawn sketch domain is challenging. First, capturing hand-drawn content is a lengthy process. Unlike in many scanned image domains, writing styles vary significantly across users, and it is diffi-cult to use techniques like deformation or noise models to create realistic simulated data. (Bunke provides a useful sur-vey of these generative models as they apply to handwriting [ 2 ].) Second, given a set of hand-drawn sketches, obtaining accurate ground-truth is difficult, essentially requiring man-ual interpretation of each diagram. Beyond the time required to interpret expressions, hand-drawn content can be ambig-uous (see Fig. 1 ), raising questions about the accuracy of the ground-truth created by a human reader. Third, in each hand-drawn sketch individual symbols must be segmented, carefully matched with their ground truth, and the relation-ships between adjacent symbols must be inserted. This, also, is a manual task, requiring significant effort.

In this paper, we present two new techniques to help automate the construction of a large, hand-drawn, ground-truthed corpus of math expressions and validate our tech-niques by creating a publicly available corpus of over 4,500 expressions. One technique creates template expres-sions by randomly sampling the space of expressions gen-erated by a grammar. The other automatically labels manual transcriptions of these expressions with ground-truth by find-ing a matching between generated and transcribed symbols. Although this paper focuses on handwritten mathematics, our techniques can be adapted for use in other sketch domains, provided their syntax can be described by a con-text-free grammar. The techniques are based on an analysis of the requirements and practical constraints imposed upon a corpus.

To be broadly useful, a sketch corpus must be accurately ground-truthed at both the syntactic and semantic levels, so that training and testing can be done reliably. It must also con-tain a large sample of sketches representative of the sketch domain and of many drawing styles, so that it is an accu-rate model of real-world inputs. Both of these properties are difficult to achieve in practice. Bias is inherent in any pro-cess of selecting sketches for inclusion in a corpus, limiting representativeness. Corpus designers may unintentionally introduce bias by selecting sketches based on their own knowledge of the sketch domain or of the tendencies of an existing recognizer. In complex domains, it may be impos-sible to anticipate all the possible forms that input may take, limiting domain coverage. In such domains, a single sketch may be reasonably interpreted in several ways, leading to ambiguous ground-truth. Such complexity further compli-cates training and testing processes.

Our corpus was created as a tool for training and testing the math recognition engine of MathBrush, our experimental pen-based system for interactive mathematics [ 8 ]. The sys-tem allows users to write mathematical expressions as they would using a pen and paper, and to edit and manipulate the mathematical expressions using computer algebra sys-tem (CAS) functionality invoked by pen-based interaction.
The rest of this paper is organized as follows. Section 2 explores the issues of bias and ambiguity mentioned above, and proposes a general methodology for creating useful cor-pora. Section 3 develops a technique for deriving random expressions from a grammar while controlling bias. Sec-tion 4 presents and analyzes an algorithm which labels these manually transcribed expressions with ground-truth. Finally, we conclude with some thoughts on how this work may be extended. 2 Corpus creation To be useful, a corpus must capture the relevant properties of the sketch domain and the natural variations on those properties in a complete and representative way, and it must be ground-truthed with very high accuracy. This section explores the difficulties associated with attaining these prop-erties, and describes the approach we have taken to create a large ground-truthed corpus of hand-drawn mathematical expressions. 2.1 Complete and representative coverage To be complete implies that a corpus contains all variations and combinations of syntax for a particular domain. To be representative implies that a corpus reflects the natural distri-bution of these syntactic elements in real-world usage. There is a natural conflict between these properties, limiting the extent to which they may co-exist in practice. A more repre-sentative corpus may lack examples of uncommon syntactic patterns, while a more complete corpus may be unrealistic.
The appropriate balance of these properties should be judged with respect to the domain properties used during rec-ognition, rather than by superficial similarity to natural exam-ples. For example, if a math recognizer determines whether or not two symbols form a superscript relationship by exam-ining only bounding-box geometry, then it is unnecessary to ensure that a realistic distribution of symbol identities appears in superscript expressions in a corpus.

Because representativeness is relative to recognition fea-tures, rather than to human intuition, we propose to use an automatic approach to generating corpus expressions. Using a grammar-based model of handwritten mathematics, we generated random, syntactically valid mathematical expres-sions as L A T E X strings (see Sect. 3 ). These template expres-sions are then converted to images and displayed to users for manual transcription. 2.2 Highly accurate ground-truth The benefits of recognizer training and the validity of recognition accuracy measurements are limited by the correctness of corpus ground-truth. However, in a complex sketch domain, such as handwritten math, it is not always possible to find a single, correct meaning for an expression. For example, the expression shown in Fig. 1 affords several reasonable interpretations: ax + b , aX + b , a x + b , axtb ,etc.
Such examples suggest that it can be difficult, or even impossible, to provide perfect, unambiguous ground-truth. It may therefore be best to include several interpretations of a single expression as ground-truth, although it is not clear how they should be applied during recognizer testing or (espe-cially) training. For our corpus, we have assumed that the writer X  X  intentions represent objective truth. For example, if they intended to write ax + b , then what they wrote represents only that expression. This approach means that our corpus is self-consistent even if it is not fully general in ambiguous cases. 2.3 Mathematical expression corpus Using the techniques described in this paper, we have cre-ated a publicly available corpus of roughly 5,000 hand-drawn mathematical expressions. In this section, we describe the expression transcription process, and the type of ground-truth provided with our corpus. 2.3.1 Transcription process To ensure that the handwritten expressions in our corpus were realistic, we recruited 20 students to manually tran-scribe automatically generated math expressions for 1hour. Each student was reimbursed with a $10 coffee gift certifi-cate.

In our study, automatically generated mathematical exp-ressions were displayed on-screen as images, and partic-ipants transcribed them using custom collection software running on Tablet PCs, as shown in Fig. 2 . Participants were instructed to write reasonably neatly, and to draw expressions and symbols as they would naturally, rather than copying the L A T E X rendering.

Fifty-three manually selected expressions from high-school and early University-level mathematics were also transcribed by each participant. These expressions provided a data set to  X  X all back on X  in case the automatic processes failed to work well. Typical transcribed expressions are shown in Fig. 3 .

Any transcription that was incomplete or illegible to a human expert was discarded (e.g. the transcription shown in Fig. 4 ). In all, 5,119 transcriptions were collected from the 20 participants. Of these, 109 were blank and 355 were discarded, resulting in 4,655 usable hand-drawn expressions. Comprising these expressions are 25,963 symbols drawn and 21,264 relationships between subexpressions. 2.3.2 Corpus ground-truth In mathematical expressions, as with other structured sketch domains (e.g. electrical schematics, music), the meaning of a particular sketch is determined not only by the symbols it contains, but also by the relative positions of those sym-bols within the sketch. For example, two letters written side-by-side in mathematics ( ab ) mean something different than if the second letter is written as a superscript ( a b ). To under-stand a sketch, one must therefore simultaneously understand its constituent symbols as well as the relationships between them.

In our corpus, ground-truth is provided for each sketch as an expression tree representing the mathematical expression drawn by the user. Each tree models the syntactic layout and the semantic meaning of a corpus expression. Each terminal symbol in a tree is associated with the strokes which render that symbol in the handwritten transcription. Our ground-truth therefore captures the entire structure of mathematical expressions: symbol identities, two-dimensional relation-ships between symbols, subexpression nesting structures, and semantic interpretation. Figure 5 shows an schematic example of a ground-truth expression tree. In this figure, mathematical semantics are indicated by expression tree nodes, as are the spatial relationships between adjacent subexpressions (e.g.  X  , ). Terminal nodes in the expression tree are matched with the corresponding strokes in the sketched expression. 3 Template expression generation To select math expressions for transcription, we use an auto-mated technique that generates a random derivation from a relational context-free grammar describing mathematical notation. This section defines relational context-free gram-mars, then shows how random derivations can be used to generate template expressions and how the biases of the gen-eration process can be controlled. 3.1 Fuzzy relational context-free grammars To model mathematical structure, we use a variant of the relational context-free grammars used by many authors ([ 4 , 12 , 14 , 16 ], etc.) called a fuzzy relational context-free grammar (fuzzy r-CFG). In this scheme, grammar rules formalize the spatial patterns of symbol arrangements that construct mathematical meaning, and fuzzy sets provide a formalism for representing the degree to which a particular interpretation of a sketched expression is valid. Fuzziness is not used by our generation algorithm, but is used by the automatic ground-truthing technique described in the next section.
 Definition 1 A fuzzy relational context-free grammar (fuzzy r-CFG) G is a tuple ( , N , S , T , R , r , P ) , where:  X  is a finite set of terminal symbols,  X  N is a finite set of nonterminal symbols,  X  S  X  N is the start symbol ,  X  T is the set of observables,  X  R is a set of fuzzy relations on ( T , T ) ,  X  r is a fuzzy relation on ( , T ) , and  X  P is a set of productions, each of the form A grammars. They provide an alphabet out of which expres-sions can be constructed, and a set of labels for various types of subexpressions. S is a label for a valid math expres-sion.

For our application, we take T to be the set of all possible sketches that a user can draw, r to be the output of a sym-bol recognizer, and R to be the set of relevant relationships between subexpressions (i.e. inline, superscript, subscript, inline vertical, and containment). The membership grade of a pairs of sketches in these relations represent recognition confidence in a particular spatial arrangement. These grades are combined with symbol recognition results to obtain con-fidence scores for parse trees.

The productions in P have the same fundamental structure as the productions of regular context-free grammars, except that the relation r appearing above the  X  production sym-bol indicates a requirement that the relation r is satisfied by adjacent elements of the RHS.

For example, denoting nonterminals by [  X  ] , the produc-tion [ADD] cal infix addition notation, while [SUP]  X  [SUP_BASE] [SUP_EXP] models exponentiation.

To model real-world meaning, nonterminal symbols may be associated with mathematical semantics. For example, expanding the [ADD] production above may imply that an addition operation is being generated. Other nonterminals may inherit the semantics of symbols expanded later. For example, the production [REL_OP]  X  [LESS_THAN] | [GREATER_THAN_OP] acts as a placeholder aggregating similar operations together. The semantics of [REL_OP] may therefore differ depending upon which production is selected. In such a case, the semantics are inherited. 3.1.1 Derivations Recall that a derivation sequence describes the expansion of nonterminal symbols as grammar productions are applied. For fuzzy r-CFGs, we extend these sequences to describe subexpression structure using parentheses.

For example, consider the toy grammar defined in Fig. 6 . [ADD] is the start symbol for this grammar.

Using this grammar, the structure of the expression ab + b is completely and uniquely captured by the following deri-vation sequence:
Note that the parentheses indicate subexpression group-ing. We call the final, fully parenthesized step in a derivation sequence a derivation string . Derivation strings completely and uniquely capture the idealized two-dimensional structure of a mathematical expression. 3.2 Random derivations The essential idea of generating a random derivation is quite basic: given a current nonterminal, choose a grammar pro-duction having that nonterminal on its LHS arbitrarily, and recurse on each nonterminal in the RHS. However, care must be taken to avoid three problems: 1. Recursing blindly may generate extremely large expres-2. The structure of productions in the grammar definition 3. Ascender (e.g. upper-case symbols, some lower-case 3.2.1 Managing expression length To limit expression length, we introduce a parameter 0 &lt; p simply choosing a production, it draws x from a uniform distribution on [ 0 , 1 ] .If x &lt; p , a derivation leading to a single terminal symbol is selected if possible; otherwise p is incremented by p inc , a random production is selected, and the algorithm recurses. This process guarantees a maximum expression depth, since eventually p  X  1, but still allows the expression length and complexity to vary considerably. 3.2.2 Managing semantic distribution The structure of a grammar can bias the distribution of gen-erated expressions. To see why, consider the grammar given in Fig. 6 . The grammar is capable of generating four distinct types of mathematical expressions: addition (from [ADD] ), multiplication and fractions (from [TERM] ), and variables (from [VAR] ).

Assume that the template generation algorithm chooses productions from a given nonterminal uniformly at random. Then, starting from [ADD] , it will generate an addition expression half the time. But, to generate a multiplication expression, it must first produce [TERM] (with probability 1 / 2), and then produce [VAR][TERM] (with probability 1 / 3, since there are three productions with LHS [TERM] ). Thus, multiplication expressions will only be generated 1 of the time. Clearly, we cannot rely on a mechanism like this to reflect a reasonable distribution of mathematical struc-tures.

We are not aware of any report on the large-scale distri-bution of semantics in mathematical writing, so we do not attempt to model the distribution of expression types encoun-tered in practice. Instead, we distribute uniformly over all expression types supported by our grammar, as follows. For each nonterminal symbol N , the algorithm constructs a list of all mathematical expression types derivable from N . Then, whenever it expands an instance of N , it selects an expression type uniformly at random, rather than a production.
To repeat the example above, suppose the algorithm is expanding an instance of [ADD] . Rather than choosing ran-domly between the two productions with LHS [ADD] ,it randomly selects one of the expression types derivable from [ADD] (i.e. one of  X  X ddition X ,  X  X ultiplication X ,  X  X raction X , or  X  X ariable X ) and generates the intermediate derivation steps all at once. For example, if  X  X raction X  is selected, then the derivation sequence [ADD]  X  [TERM]  X  ( [ADD]  X  X  X  X  [ADD] ) is generated in a single step.

Using this approach, it is possible to approximate any desired distribution over mathematical semantics. One could, for instance, bias expression generation toward fractions to obtain a number of deeply nested fractions when designing a testing corpus. Similarly, one could empirically approxi-mate the frequency with which various mathematical oper-ators occur and  X  X lug in X  those values to obtain a more realistic distribution of expression types. However, basing generation on measured frequencies re-introduces problems related to unintentional bias of corpus designers, as the fre-quencies may be a function of the mathematical domain that was sampled. Furthermore, to truly obtain a randomized, but realistic-looking, corpus would require a more sophisticated domain model. There is a significant difference between simply choosing, say, integration ten percent of the time, compared with choosing integration in appropriate contexts, with appropriate variable names, integrands, limits, etc. 3.2.3 Managing bounding-box shape Finally, to obtain broader coverage of relative bounding-box positions, the Latin and Greek letter symbols in the grammar were grouped into classes based on their characteristic shape with respect to a baseline (ascender, descender, baseline). The grammar was modified so that each class is produced by a single non-terminal. Although the grammar contains a preponderance of ascender symbols (since every capital let-ter is an ascender), this approach of random class selection yields a uniform distribution over symbol shapes rather than symbol identities. 3.3 Output Each production in our grammar is equipped with a string generator which converts the internal grammar representa-tion into a string. We used these generators to produce L A T strings representing each generated expression. These strings were then converted to images for display in our transcrip-tion program by using standard tools. We also produced a derivation string for each generated expression. Derivation strings are used by the automatic ground-truthing algorithm described in the next section.

Figure 7 shows two examples of expressions generated by the above process. 4 Automatic ground-truthing Usingaprogramtoautomaticallyprovideground-truthlabels would significantly reduce the amount of manual labor required to construct a large corpus. However, using a par-ticular recognition system R to provide ground-truth labels may limit the capabilities of a new recognition system trained on that ground truth, because the available training data is restricted by the capabilities of R . Our approach to automatic ground-truthing avoids this situation.

We wish to avoid making any assumptions that could impact the validity of training and testing procedures using our corpus. Therefore, to label a transcribed expression with ground-truth, we use only symbol and relation classifiers that have no knowledge of mathematical semantics, and were trained in isolation from our corpus data. Using the derivation string associated with the transcription, the labeling problem is simply to match each terminal symbol in the string to the corresponding ink strokes in the transcription. This section describes our labeling algorithm and evaluates its accuracy with respect to three different metrics. Each metric is appli-cable for different uses of corpus data. 4.1 Algorithm Given a derivation string D and a corresponding transcrip-tion, our goal is to find a function f mapping each terminal symbol in D to the set of ink strokes representing that sym-bol. This task is not trivial because, although we know which symbols should appear in the transcription, we do not know exactly where they are. There may be multiple instances of some symbols (as in x 3 + 3 x 2 + 2 x + 3, for example), and handwriting is ambiguous, so each group of strokes may be recognizable as several different symbols.

More formally, a derivation string D is either a sin-gle terminal symbol, D =  X  , or it contains a number of smaller derivation strings concatenated by a relation, D = (
D tion score for terminal  X  on a group t of ink strokes. Similarly, fidence that relation r applies between groups t 1 , t 2 of ink strokes.

Fixaderivationstring D ,andlet f ( X ) beafunctionmatch-ing each terminal symbol  X  appearing in D to a group of ink strokes in the transcription. Define the goodness of the matching f on derivation string D , X ( f , D ) inductively, as follows:
Using these definitions, we wish to find f  X  = argmax f  X  ( f , D ) . Note that we only need to consider matchings f that map each terminal  X  appearing in D to a group g of strokes such that S ( X , g )&gt; 0.

Unfortunately, this formulation is not convenient for computation. If the symbol recognizer reports n i potential instances for each terminal  X  i appearing in D , then there are O i n i feasible matchings f . Furthermore, consider a natural recursive solution in which we find f for a der-ivation string ( D 1 r ... rD k ) by dividing the transcription into k subexpressions and recursing. To be matched, the i th subexpression must contain exactly the terminal symbols appearing in D i , so in the worst case, there are O i n i subdivisions to recurse on! Clearly, we cannot afford to enu-merate all the possibilities.

Instead, we use a simplification. Rather than measuring the relation r ( f ( D i ), f ( D i + 1 )) between subexpressions, we measure r ( f ( D i ), f ( X  h )) , where  X  h is the first terminal symbol to appear in D i + 1 . This replacement avoids enumer-ating subdivisions explicitly and facilitates a straightforward best-first search algorithm.
Using this simplification, we cannot guarantee that the ground-truthing algorithm will find the optimal matching f But, intuitively, if there is a relation between two subexpres-sions, we expect this relation to hold between the first sub-expression and the first symbol of the second subexpression as well. For example, in e x + 2 , the subexpression x + 2 and its leading symbol x are both in superscript positions relative to the e . In the fraction a b + c ,the b + c subexpression and its leading symbol b are both below the fraction bar. Note that, depending on the behavior of the symbol and relational clas-sifiers, the optimal matching may be incorrect, or matchings may not exist at all (see Fig. 10 and associated discussion.)
The algorithm matches the terminals in the derivation string from left to right, keeping track of where subexpres-sions start and end. Suppose that the terminal symbols in the derivation string are  X  1 , X  2 ,..., X  n , and that a matching f has been obtained for  X  1 ,..., X  m  X  1 with running score z . The ink used by the matched symbols is marked  X  X sed X . All other ink is currently  X  X nused X . The ink to measure the rela-tion into  X  m against is called I pr e v . To find the rest of the matching, the algorithm proceeds as follows: if m = n + 1 then Let r be the incoming relation to  X  m
Order all  X  X nused X  occurrences of  X  m in the input ink in decreasing order of confidence for each possible occurrence I curr of  X  m do end for return failure
When m = 1 (i.e. when the algorithm is matching against the first symbol), we sort all occurrences of  X  1 by the sum of spatial relationship scores from all other possible sym-bols, in increasing order. The first candidate under this order-ing is unlikely to be a good choice to follow any symbol in the derivation string, and is therefore likely to be the first symbol. Loosely speaking, this choice corresponds to the top-left symbol in the drawing. It works because the spatial relations used are unidirectional (e.g. we use a Right relation but not Left), so the matching process consumes symbols roughly in a top-left to bottom-right order.

In the worst case, the algorithm will eventually try all pos-sible matchings. It is therefore guaranteed to obtain a match-ing f if one exists. Heuristically, the algorithm X  X  best-first search strategy should find a good matching quickly, and we found this to be the case in practice. If no matchings exist, then the algorithm will reject the input as unmatchable after exhausting all possible search paths. The number of search paths varies depending on the input, the derivation string, and the symbol and relational classification results. The the-oretical worst case occurs when every symbol appears as a recognition candidate for every subset of strokes and every relation score is non-zero. If there are n strokes in the input and k terminal symbols in the derivation string, then the num-ber of search paths is roughly bounded by k n . (This bound simply counts the number of assignments of the n strokes to the k symbols.) Note, however, that this theoretical worst case is virtually impossible in practice. When symbol and rela-tion classification are accurate enough, then the algorithm is linear in k , as it matches each terminal symbol without backtracking. 4.2 Experiments and results To test the accuracy of our algorithm, the entire corpus of 4,655 expressions was annotated manually. Automatically annotated data was then compared to the manually annotated data using two scenarios: 1. Running the algorithm on every transcription. 2. Pre-training the symbol recognizer for each user on
Scenario 2 was introduced because the our model-based symbol recognizer often possessed no models similar to how participants drew certain symbols, causing many transcrip-tions to be rejected in Scenario 1. This scenario was intended to isolate the ground-truthing algorithm from symbol recog-nition errors as much as possible.

Thesymbolrecognizerfirstidentifiesgroupsofinkstrokes which may correspond to distinct symbols, then generates a fixed number of symbol candidates for each group. These groups may overlap. In each scenario, we varied the number of candidates that were generated for each group to gauge the effect on rejection rate and accuracy. 4.2.1 Measures of accuracy A natural way to measure the accuracy of our ground-truthing algorithm is to measure the similarity between automat-ically X  X nd manually X  X round-truthed symbol bounding boxes. We evaluated our ground-truthing algorithm under the following three accuracy metrics. 1. Full expression: In this measurement, each annotated 2. Symbol-based: In this measurement, each symbol is 3. Bounding-box overlap: In this measurement, each sym-
Figure 8 illustrates the difference between exact and over-lap bounding-box accuracy.

These three measurements are defined in increasing order of permissiveness of match. Full expression accuracy requires every symbol in a transcription to be annotated exactly as in the manual ground-truth, while bounding-box similarity accuracy allows symbol bounding boxes to dis-agree slightly but still count as a close match.

The ground-truthing algorithm produces highly accu-rate ground-truth for hand-drawn input when compared to manual ground-truth, as shown by Fig. 9 . In these graphs, the wide black bars indicate the proportion of transcrip-tions which were rejected by the ground-truthing algorithm. The narrower bars indicate the labeling accuracy on the non-rejected data under each of the measurement schemes described above. The x-axis values indicate how many rec-ognition candidates were returned from the symbol recog-nizer for each group of strokes identified as a potential symbol.

The algorithm rejects fewer transcriptions as the number of available symbol candidates increases. This behavior is expected, as it becomes more likely that all of the required terminal symbols will be present in the symbol recognizer X  X  output when more candidates are reported. Providing the algorithm with more candidates also reduces its accuracy, but only slightly. For example, in Scenario 1, the rejection rate dropped from about 34 to 19%, while the symbol-level accuracy dropped from 97 to 95%, and overlap accuracy only fell from 99 to 97%.

The main difference between Scenarios 1 and 2 is the rejection rate. Pre-training the symbol recognizer signifi-cantly increased the number of transcriptions that the algo-rithm was able to label, indicating that symbol recognition quality was a main cause of rejection in Scenario 1. The rejection rate for Scenario 2 was about 20% when 8 sym-bol candidates were used, almost 15% lower than the similar experiment in Scenario 1. This reduction was present in all cases, but became less marked as the number of symbol can-didates increased.

The type and degree of accuracy required from ground-truth varies with the intended use of a corpus. If annotated data is used to train spatial relation classifiers on bound-ing-box information, it is appropriate to count similar, but not identical, bounding-box annotations as partially correct because they may still provide useful training data. If one uses ground-truthed corpus expressions to test the accuracy of an isolated symbol recognition system, then full expres-sion or exact bounding-box accuracy may be a more appro-priate measurement. 4.3 Discussion There are two sources of errors for the labeling algorithm: rejection, and inaccurate labeling. Inaccuracy is a more seri-ous error than rejection X  X ince we would like to use the algo-rithm X  X  output as ground-truth, it is imperative that the output is as accurate as possible. We prefer to have no labeling at all than an incorrect labeling.

Rejection is only possible when symbol recognition fails to identify all of the required symbols, or the relational clas-sifiers assign zero confidence to the required symbol arrange-ments. Figure 10 shows examples of each of these cases. The transcription on the left was rejected because the symbol rec-ognizer failed to identify the  X  X  X  symbol. The transcription on the right was rejected because of the simplification used in the algorithm. The relational classifiers did not detect a rela-tion between the  X + X  and the numerator  X   X   X  in the denom-inator of the top-level fraction. In our experiments, symbol recognition was the dominant cause of rejection.

Incorrect labelings occured almost exclusively on tran-scriptions containing symbols drawn with a different num-ber of strokes than the symbol recognizer expected. In such a situation, the symbol recognizer will never output the mis-matched symbol as a candidate for the correct group of strokes. For example, in Fig. 11 , the right parenthesis is split into two strokes. The manual ground-truth groups these strokes together as the parenthesis, but the labeling algorithm simply picked the single stroke with the higher recognition score. Cases like this one illustrate the complexity of judging whether ground-truth is  X  X orrect X , and point out the utility of the overlap accuracy metric, which counts this labeling as about 95% correct.

Using an off-line, image-based symbol recognizer could help to avoid this problem. While we currently focus on creating corpora for online mathematics, our labeling tech-niqueisalsoapplicabletotheoff-linedomain.Givenaccurate ground truth for scanned hand-written mathematical expres-sions, symbols could be segmented from the background, recognized using an ICR engine that returns an n-best list, and then matched using our algorithm.

A more significant theoretical source of inaccuracy is when the derivation string does not match the actual structure of the transcription. This may happen in cases where there are multiple ways of writing an expression, and the tran-scriber chose a way different from the one appearing in the automatically generated derivation string (e.g. if a template expression 1 2 was transcribed horizontally as 1 / 2). In these cases, if the labeler finds a matching, then the ground-truth will be incorrect because it does not reflect the actual spatial relationships appearing in the transcription. In our study, this type of inaccuracy was very rare, probably because math-ematics has a fairly standardized syntax. In more flexible domains, though, it could be a more significant problem. 5 Conclusions and future work The sketch recognition community currently lacks publicly available, ground-truthed corpora for training and testing recognition systems. In this paper, we advocate the use of automated techniques to aid in the creation of such cor-pora.Wepresentedagrammar-basedapproachforgenerating sketch templates, which may then be manually transcribed at minimal cost to construct large sketch corpora. We also presented an automatic ground-truthing method working in conjunction with our generation technique. These techniques were applied during the creation of our ground-truthed cor-pus of hand-drawn mathematical expressions. The corpus is available at http://www.cs.uwaterloo.ca/scg/mathbrush/ mathdata.shtml .

We are not aware of any work on grammar-based algo-rithms to aid in the generation and labeling of large on-line sketch corpora. However, OCR researchers have developed several techniques for automatically generating ground-truthed training and testing data. These techniques generally either generate perfectly ground-truthed synthetic data (e.g. [ 6 , 13 ]), or match real inputs to separate ground-truth created by hand, potentially with mistakes in both matching and ground-truthing (e.g. [ 15 ]). Occasionally aspects of both approaches are combined as in [ 7 ].
Our techniques for generating and labeling corpus data have much in common with both approaches, but also have some important differences. Synthetic data is generated as in the first approach, but this data is intended to be transcribed by human users. Real input is matched to separate ground-truth, but the ground-truth is automatically generated and is free from errors. By decoupling expression generation from ground-truth generation, we are free to experiment with algo-rithms for each task separately.
 While the template generation algorithm presented in Sect. 3 workedwellforgeneratingsyntacticallycorrectmath-ematical expressions, it is difficult to argue that these expres-sions are truly representative of the expressions commonly used by working mathematicians or students. To generate more realistic expressions, we intend to undertake a study of the notations and normative rules commonly used in math-ematical writing. Using this information, we plan to aug-ment our grammar with probability distributions to generate more realistic corpora focusing on particular mathematical domains.

The ground-truthing algorithm presented in Sect. 4 was very accurate, but always rejected over 10% of its inputs. We wish to reduce this rejection rate further. One way to do so is by modifying our collection protocol. Pre-training the recognizer proved to be an effective way to reduce the rejec-tion rate, but since we trained on randomly selected samples, the reduction was likely far from optimal. In future studies, we will ask participants to transcribe a number of specially designed expressions intended to capture their writing style for each relevant symbol, and then set these transcriptions aside specifically for pre-training.

Another way to reduce the rejection rate is to change the labeling algorithm. The simplification we used to obtain a tractable labeling algorithm was a significant, but not pri-mary, cause for rejection. One way to mitigate this problem is to use relational classifiers specifically trained to detect relations between a subexpression and the first symbol in the next subexpression. Another way is to include more of the two-dimensional structure of handwritten math in the labeling algorithm. We intend to investigate both of these possibilities, while still keeping the labeler tractable. References
