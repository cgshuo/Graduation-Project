 The brain of human beings has powerful ability of incremental learning. There-fore, how to develop brain-like computing model, how to implement incremental learning is one challenge problem in machine learning research. In real world applications, there are three scenarios n eed incremental learn ing: all training data cannot be gathered at one time for the cost of collecting data. As a result the data are acquired batch by batch; some real world applications need instant learning once some training data obtained; all training data cannot be loaded into the memory of computers if the training set is very large. According to Jantke [1], incremental lea rning is to construct new hypothesis by using only the hypothesis before and the recent informa tion on hand. Zhou and Chen [2] distin-guished three kinds of incremental learning tasks: Example-incremental learning (E-IL); Class-increm ental learning (C-IL); and Attr ibute-incremental learning (A-IL). However, C-IL and A-IL have no t been received much attention so far. Syed et al. [3] introduced two types of incremen tal learning methods: instance learning, which uses one example at a time, and block by block learning, which uses a suitable-size subset of samples at a time.

At present, however, the essence of the training algorithms of various kinds of artificial learning systems is an optimization procedure that aims to ensure the generalization ability based on the current learning environment. There-fore, all the current machine learning algorithms don X  X  adapt for incremental learning in nature. The non-adaption lies in that the computation model lacks the ability to get new knowledge or cannot retain the knowledge learned be-fore [4]. The training of artificial neur al networks is a gradient descent process, and therefore the modification of conn ection weights will damage the learned knowledge. The training of SVMs is a global optimization based on all train-ing data. As a result, new added training data will make support vectors change [5].

Classifier combining is a useful method for machine learning [6] [7] [8]. Many scholars have applied classifier combining techniques to incremental learning. Polikar et al. proposed Learn++ based on AdaBoost algorithm [9]. Lu and Ichikawa proposed an incremental lea rning model based on emergence theory [10]. Macek proposed incremental learning algorithms based on bagging and boosting and successfully applied them to EEG data classification [11]. Wang et al. used weighted ensemble classifiers to mi ne concept-drifting data stream [12]. Like bagging, a model of incremental learning by classifier combining (ILbyCC) is proposed in this paper. 2.1 Definition of Batch Incremental Learning Definition 1. Given a sequence of training datasets S 1 ,S 2 , ..., S m ,where S i = { cates the set of class label in training dataset S i .Lets E 1 denotes the classifier trained on S 1 , the batch incremental learning procedure IL can be illustrated as: IL ( S i ,E i  X  1 )= E i , 2  X  i  X  m .
 In this paper, we only consider the case where the number of class labels don X  X  decrease, i.e., L 1  X  L 2  X  ...  X  L m .

ILbyCC takes a frame of modular architecture. Modular architecture can make classifier easy adapt to incr emental learning. ILbyCC trains a new classifier on an incremental batch and saves it. All the classifiers trained by far are combined into one combined classifier. The training algorithm of ILbyCC can be illustrated bining, and E i denotes the current combined classifier. 2.2 Combining Classifiers by Averaged Bayes Given m classifiers that can output posterior probability information, when a test input x comes, the j -th classifier outputs the posterior probability of x belonging to all the classes:
According to Averaged Bayes, the combined classifier E m computes the pos-terior probability of x belonging to all classes as follows:
According to Bayes rule, x canbeclassifiedasthe i -th class: 2.3 Incremental Learning Algorithm by Classifier Combining ILbyCC algorithm is described as Fig.1. 3.1 Datasets In order to evaluate the performance of ILbyCC algorithm, experiments are run on four data sets. The first three data sets, Optical Digits Database, Vehicle Silhouette Database, and Concentric Circle Database, are took from Poliker X  X  paper [9] and used as Poliker X  X  strategy. The fourth data set is a part of Yomiuri News Corpus database. We select all the in stances of nine classes, such as crime, sport, Asian-Pacific, North-South-American, health, accident, by-time, society, and finance, which will be called as class 1 through class 9. The training data set is randomly divided into 9 incremental batches, S 1 through S 9 ,where S 1 through S 3 have instances from classes 1, 2, and 3; S 4 through S 6 contain instances from classes 1 through 6; and S 7 to S 9 have instances from classes 1 through 9. The statistics of the tasks are illustrated in Table.1. The parameters used in SVMs are selected by cross-validation.

In order to test ILbyCC X  X  performance on incremental learning when differ-ent incremental step takes different par ameters. Optimal p arameters in each incremental step were c hosen among 25 pairs of ( C,  X  ) by 10-cross-validation. 25 pairs of ( C,  X  ) were generated around the values of ( C,  X  )inTable.1bya product factor of 2.

In order to ensure the reliability of the experimental results, the first three experiments were repeated 10 times and a veraged results were presented. Only the last experiment was run one time for its large size. In order to evaluate the performance of ILbyCC , several exsiting algorithms were run for a compari-son study. We adopted the algorithm of Syed [3] that was denoted as ILbySV for convenience. In addition, the basic incremental learning algorithm is Batch-training , i.e. when the i -th incremental batch comes, the classifiers trained before are all discarded and S 1 S 2 ... S i is used to train a new classifier. Obvi-ously, Batch-training should keep all training data gotten by far, and further, catastrophic forgetting takes place wh en new data comes. In order to compare ILbyCC with Learn++, the paper direct ly quotes the experimental results of Learn++ [9]. For convenience, when all the training sessions of ILbyCC uses the same parameters, ILbyCC is denoted as ILbyCC1, when different session of ILbyCC use different parameters, ILbyCC is denoted as ILbyCC2. 3.2 Results and Analysis Both Fig. 2 and Fig. 4 show that ILbyCC was able to preserve the knowledge learned before and acquire new information. Fig. 3 and Fig. 5 illustrate that ILbyCC can incrementally learn successfully, ILbyCC1 and ILbyCC2 have nearly the same generalization ability, and ILbyCC is slightly good then Learn++. Because all incremental batches are not always in the same distribution, the incremental learning perfo rmance of ILbySV fluctuates.

Fig.6 and Fig.8 show that the generalization performance of ILbyCC first decreases slightly when new classes are in troduced and increases when training data with the same class labels are continuously added, indicating that ILbyCC can preserve the learned knowledge. From Fig. 7 and Fig.9, it seems that a large improvement on the performance is obtained after new classes that were not available earlier are introduced, but only minor improvements in the perfor-mance can be observed from the test accu racy curves when new classes are not introduced, indicating that ILbyCC ca n learn from new introduced classes.
In Fig. 10, it can be seen that the training time of ILbyCC is far smaller than the training time of Batch-training and ILbySV. The large speedup of ILbyCC can compensate the slight decrease of it s generalization per formance compared with Batch-training.

Why can ILbyCC work effectively? Accord ing to the theory of bias-variance [13], decomposing training data will introduce bias and makes the generalization ability of single classifier decrease, however, decomposing training data will in-crease the variances between all classifiers and increase the generalization ability of the combined classifier, which compensates the decrease of the generalization ability caused by decomposition. Therefore, ILbyCC has nearly the same test accuracy with Batch-training. In addition, the combining rule (2) can automat-ically invalidate the classifiers that is not much confident of its outputs, i.e., influenced by the outputs of the j -th classifier. Therefore, Averaged Bayes can automatically select the classifiers that is confident of its outputs to combine.
Note that the performance of ILbyCC1 and ILbyCC2 in all the simulations are nearly the same, it is v ery interesting to observ e that the time complexity for selecting optimal parameters is decreased by training data decomposition. It is not reasonable for incremental learning algorithm to wait for all training data collected to select opti mal parameters. It is also not reasonable to apply the parameters, which is gotten from the first incremental batch, to the following incremental steps. Therefore, ILbyCC n ot only decreases the time complexity of parameter selection but also makes incremental learning possible. 3.3 Discussions Compared with Learn++, the proposed ILbyCC satisfies the criteria proposed by Polikar [9] and has comparable incremental learning ability, but ILbyCC can be implemented more simply. Learn++ is a kind of AdaBoost in essence, Learn++ should use more parameters and train more classifiers. Note that ILbyCC is a bagging-like algorithm, ILbyCC can be parallized for training speedup, while Learn++ can only be implemented in serial. In addition, ILbyCC needs no com-munication between classifiers, it can we ll protect the privacy of data. The work in this paper can prove the availability of the algorithm estimating the posterior probabilistic of SVMs. To our best knowledge, ILbyCC is the first application to apply posterior probabilistic SVMs to real problem.
 In this paper, we have proposed a novel in cremental learning algorithm ILbyCC that uses Averaged Bayes rule to combine c lassifiers. The experimental results indicate that ILbyCC can not only pres erve the knowledge learned before but also can learn new knowledge from new added data and further new knowledge from new introduced classes. Three main advantages of ILbyCC over existing algorithms are simply implementing, sma ll time complexity for parameter selec-tion, and training time saving. In addition, the proposed algorithm is a general framework of incremental learning and any machine learning algorithm that can output posterior probabilistic can be integrated into ILbyCC.

