 Web search is always in a state of flux: queries, their intent, and the most relevant content are changing over time, in predictable and un-predictable ways. Modern search technology has made great strides in keeping up to pace with these changes, but there remain cases of failure where the organic search results on the search engine re-sult page (SERP) are outdated, and no relevant result is displayed. Failing SERPs due to temporal drift are one of the greatest frus-trations of web searchers, leading to search abandonment or even search engine switch. Detecting failed SERPs timely and providing access to the desired out-of-SERP results has huge potential to im-prove user satisfaction. Our main findings are threefold: First, we refine the conceptual model of behavioral dynamics on the web by including the SERP and defining (un)successful SERPs in terms of observable behavior. Second, we analyse typical patterns of tem-poral change and propose models to predict query drift beyond the current SERP, and ways to adapt the SERP to include the desired results. Third, we conduct extensive experiments on real world search engine traffic demonstrating the viability of our approach. Our analysis of behavioral dynamics at the SERP level gives new insight in one of the primary causes of search failure due to tem-poral query intent drifts. Our overall conclusion is that the most detrimental cases in terms of (lack of) user satisfaction lead to the largest changes in information seeking behavior, and hence to ob-servable changes in behavior we can exploit to detect failure, and moreover not only detect them but also resolve them.

The information seeking behavior of users on the web is inher-ently sensitive to changes happening in world [25, 28]. As the web reflects the world around us, content is changing in predictable and unpredictable ways, affecting the search intent and queries issued by users. Added to that, searchers express their complex informa-tion needs in short queries, causing an inherent ambiguity in their statements of request: the query intent is specific to the context of the user and the point in time. It is a formidable achievement of modern search engines that they manage to keep up to pace with changing content, at equally formidable costs in crawling and up-dating search engines indexes. In particular, for updating rankers, click through information in interaction logs are crucial [2, 21]
Yet, there remain cases of failure where the organic search re-sults on the search engine result page ( SERP ) are outdated, and no relevant result is displayed. This can be caused by temporal query intent drift, where the desired pages for a query are changing over time, and the historical transaction logs privilege the outdated results. For example, if users were searching for  X  X alaysia airlines flight X  in March 2014 they most likely wanted to see news about the Malaysian flight 370 that disappeared. However, if users issued the same query in July 2014 they mostly likely were searching for in-formation about the Malaysian flight 17 that is presumed to be shot down. Figure 1 shows daily Wikipedia page views for the MH17 and MH370 pages over 2014, with striking increases from 0 to 100s of thousands of page views when the events happened.

The Malaysian Airlines example can characterized as a  X  X ud-den X  drift which may cause the SERP to become outdated. Such changes can be associated with the news, and received the most attention in research community [12, 13]. However, changes may happening over a longer period of time and not necessarily bring an increase in the volume of traffic. For instance, if users issued the query  X  X IKM conference X  in 2014 they were satisfied with results referring to the page http://cikm2014.fudan.edu.cn/ and this page gets a majority of clicks. However, the conference site has been changed and the same query issued in 2015 should be linked to the different page http://www.cikm-2015.org/ . The CIKM example can be characterized as an  X  X ncremental X  drift where the intent of the original query is changing over a longer period of time.
In this paper, we examine a generic approach to detect SERPs that become out of sync with the query intent. Specifically, users issue a query Q and a search engine returns search result page ( SERP ) that is a ranked list of URLs: Our users are expected to click on some url i on the SERP isfies their information need, and the order of URLs on the is based on various features and optimized to fit a history of user interactions with a pair h Q, SERP i . As a result, the h Q, SERP i shown at a given point in time will reflect the user preferences over an earlier period of time. However, this gives no guarantee on the quality of the current h Q, SERP i as user preferences are sensitive to time and events happening in world. We aim to detect cases of SERP failure due to a significant drift in query intent Figure 1: Wikipedia page views per day over 2014 for https: //en.wikipedia.org/wiki/Malaysia_Airlines_Flight_17 and https:// en.wikipedia.org/wiki/Malaysia_Airlines_Flight_370 . over time. Our aim is to detect failed SERPs due to intent drift in an unsupervised way not relying on signals from other sources than the web traffic, language independent and not relying on rules or templates, independent of volume capturing both head and tail query drift. Hence we use behavioral signals as indicators of user (dis)satisfaction, such as click-through information [2, 22] and in particular query reformulations [1, 17, 24]. Specifically, in this pa-per, we are trying to answer the following main research question:
By analyzing behavioral dynamics at the SERP level, can we de-tect an important class of detrimental cases (such as search failure) based on changes in observable behavior caused by low user satis-faction? We break up the main research problem into three different parts. Our first concrete research question is: RQ1: How to include the SERP into the conceptual model of be-We conduct a conceptual analysis of behavioral dynamics from the SERP  X  X  perspective, and introduce failure and success at the SERP level, analyzing their behavioral consequences identifying indicators of success and failure. We then analyze success and fail-ure in light of changing query intents over time, and identify an important case of SERP failure due to query intent drift, and sug-gest an approach to detect a failed SERP due to query intent drift by significant changes in behavioral indicators of failure. Our second concrete research question is: RQ2: Can we distinguish different types of SERP failure due to We study different types of possible query intent drift inspired by the literature on concept drift [15]: sudden, incremental, gradual and reoccurring. It is important to be able to classify the type of changes in user satisfaction because a sequence of actions a search engine should perform to normalize a situation can be different. We identify relevant parameters, such as the window of change, volume or popularity of queries, and relevant behavioral indicators, such as the probability of reformulation, abandonment rates, and click through rates. For the two main categories of intent drift, we define an unsupervised approach to detect failed SERPs . We also show how the detected changes can be used to improve a ranking of search results.
 Our third concrete research question is: RQ3: How effective is our approach on a realistic sample of traffic We validate our approach on twelve months of search interaction logs of a major commercial search engine. We run a simplified ver-sion of our algorithm on a massive transaction log, and detected pairs of h Q, SERP i suspected of failing due to drifting query in-tents. We investigate the accuracy of drift detection and the accu-racy of the clicked URLs of the revision to include on the of the original query. We look at the effectiveness of our approach for both sudden and incremental changes in query intent, by vary-ing the duration of the window to detect failed SERPs .
The remainder of this paper is organized as follows. Section 2 introduces earlier work on behavioral dynamics on the web, and behavioral indicators of user satisfaction focusing on the level. Then, Section 3 introduces the concept of SERP success and failure, and outlines behavioral cues for their detecting becoming out of sync over time. Followed by Section 4 zooming in on different types of query drift causing failed SERPs , and outlin-ing practical ways of detecting them. Finally, Section 5 reports on extensive experiments demonstrating the real-world utility of our approach.
In this section we will study related work, focusing on research on topic and concept drift, on the behavioral dynamics of the web, and on user satisfaction signals on the SERP level.
Topic or query drift has been studied for long in IR, usually in the context of evolving information needs as may happen in rout-ing tasks [4], or the opposite negative effect of retrieving off-topic documents lower in the ranking [33]. In particular in adaptive filter-ing, topic models are continuously updated when new data comes available [5]. The focus is on a general topic or standing profile that monitors a stream of data and selects relevant documents. Our focus is on the SERP, serving results to a population of users with subtle or less subtle variation in query intent, taking into account changes in the query intent over time.

Topic drift is distinct from concept drift [15, 30, 36] which, in a machine learning setting, refers to changes in the conditional distri-bution of the output (i.e., target variable) given the input (i.e., input features), while the distribution of the input may stay unchanged. We will use a concept drift approach in the next sections, to model changes in features indicating lack of search satisfaction, and for determining thresholds for drift detection.
The changes in query popularity over time have been studied ex-tensively in prior work. Moreover, researchers have also examined the relationship between query behavior and events [27]. There are algorithms for identifying queries that are related to breaking news and for blending relevant news results into core search re-sults [12, 26].
Prior work on behavioral dynamics is based on three factors: (1) on changes in query dynamics and in this case authors are concen-trated on the  X  X ead X  queries [25, 28, 29, 31]; (2) on changes in web content dynamics and user interaction with dynamic content [25]; and (3) how information about changes can be used: Additionally, Kulkarni et al. [25] explored how queries, their as-sociated documents, and the intents corresponding to the queries change over time. Radinsky et al. [29] have done an extensive studies how time-series analysis methods can be applied to predict dynamics on the web. Shokouhi [31] proposed using time-series decomposition techniques for identifying seasonal queries.
In summary, the prior studies cited above examine how general changes in content, specific content features, or query volume can be used to improve web search experience. Although much has been done to understand user web search behavior over time, few efforts have sought to construct underlying models to understand changes h Q, SERP i and even used to automatically fix the ob-served problems. We present the construction of models for behav-iors over time that can explain observed changes in user satisfaction with h Q, SERP i .
User satisfaction with the SERP has been researched exten-sively. It is widely adopted as a subjective measure of search ex-perience. User clicks are reasonably accurate on average to eval-uate user satisfaction with pairs h Q, SERP i [2, 22], using click-through information. This user satisfaction scenario is successfully applied to navigational queries. It is called query-level satisfaction . However, we have to take into account the fact that user clicks are biased: 1. to the page position in the SERP [9, 21]; 2. to the quality of the page X  X  snippet [37]; and 3. to the domain of the returned URLs [19].

Al-Maskari et al. [3] claim that the search scenario for infor-mational queries is different. Users can run follow-up queries if they are unsatisfied with the derived results, and reformulations can lead users to the desired information. This scenario is called task-level satisfaction [12]. On the one hand, earlier research pro-posed different methods for identifying successful sessions. Hassan et al. [18] used a Markov model to predict success at the end of a task. Ageev et al. [1] exploited an expertise-dependent difference in search behavior by using a Conditional Random Fields model to predict a search success. On the other hand, separate researches are interested in situations when users are frustrated. Feild et al. [14] proposed a method for understanding user frustration with the pair h Q, SERP i based on query log and physical sensor features. Kiseleva et al. [24] showed how to automatically detect changes in user satisfaction using reformulation signal.

Earlier, White and Dumais [35] gave users difficult information seeking assignments and evaluated their level of dissatisfaction via query log features and physical sensors. They demonstrated that the prediction model gets the highest quality when it is built based on query log features. One type of user behavior that can be clearly as-sociated with frustration is search engine switching. Guo et al. [16] showed that one of the primary reasons users switched their search engine was due to dissatisfaction with the results on the recent study [20] shows a method to predict finer-grained, graded satisfaction levels. This paper significantly extends earlier work [24], analyzing behavioral dynamics at the SERP level, and ex-plaining how and why the changes are happening. In this work we propose a methodology to define a type of changes in user satisfac-tion and how this information can be used to improve a ranker.
Summarizing, in this section, we presented an overview of prior work on behavioral dynamics and user satisfaction on the web, with a special focus on the SERP level. In the rest of the paper, we will study variations in user satisfaction with h Q, SERP i time, starting with a conceptual analysis of success and failure at the SERP level in the next section.
In this section we will study RQ1: How to include the SERP into the conceptual model of behavioral dynamics on the web? How to identify (un)successful SERPs in terms of drastic changes in ob-servable user behavior? We first introduce the notions of successful and unsuccessful SERPs as a conceptual model. Recall from the above that we looks at the pair h Q, SERP i , with a query Q and a search engine result page ( SERP ) consisting of a ranked list of URLs in response to query Q . That is,
Let us further assume that queries are issued for a purpose and that the intent of query Q can be represented as a non-empty set of desired pages INTENT q . For example, conceptually speaking a navigational query will have a singleton set INTENT q , and an informational query will have a larger set of desired pages. Over a population of users there may be a distribution of intents, each giv-ing rise to a different sets of desired pages, and it is straightforward to incorporate this into the conceptual model, but for simplicity and clarity we use a single set of desired pages here.

We define a successful and unsuccessful SERPs in the follow-ing way: D EFINITION 1. (a) A SERP q is a successful SERP for query Q if and only if 9 url q 2 INTENT q such that url q 2 SERP q (b) A SERP q is a failed SERP for query Q if and only if 8 url INTENT q such that url q 62 SERP q .

A user issuing query Q may respond to the SERP in differ-ent ways. One of the possible scenario of user interaction with the SERP , which is widely studied, is an event when users do not click on presented results. This case is called search abandonment that is known as a metrics of how successful a SERP is. Research on search abandonment [7, 8, 11, 34] studied two primary aban-donment cases: bad abandonment indicating user frustration and dissatisfaction; good abandonment suggesting satisfaction without needing to click. Assume we have a successful SERP in the sense of the conceptual definition above, and observe no clicked result, this suggests a case of good abandonment. Good abandonment is quite common in modern search engines because direct answers such as weather and stock quotes are returned for queries with ex-plicit intent. Moreover, snippets can also satisfy users X  information needs directly. However, if we assume a failed SERP , then a lack of clicked results suggests bad abandonment. Diriye et al. [11] re-port roughly equal fractions of good and bad abandonment, hence the abandonment rate is a secondary indicator of SERP success or failure. Behavior Failed SERP Successful SERP No clicks Bad abandonment Good abandonment Clicked result DSAT clicks SAT clicks Revised query Negative reformulation Positive reformulation
The other possible scenario is for users to interact with a re-trieved SERP . Web search users often click on the SERP and/or follow up with other queries. Many researchers have showed that clicks and reformulations can be used for a variety of tasks. How-ever, clicks are usually considered to be as a positive sign [2, 22] to detect user satisfaction with the pair h Q, SERP i . In the con-ceptual model, we can distinguish between satisfaction (SAT) and dissatisfaction of clicks based on the desired pages: D EFINITION 2. (a) A click on url i 2 SERP q for query Q is a SAT click if and only if url i 2 INTENT q . (b) A click on url i 2 SERP q for query Q is a DSAT click if and only if url i 62 INTENT q .

It is an immediate corollary that there cannot be SAT clicks on a failed SERP , and that we can expect SAT clicks, but cannot exclude DSAT clicks, on a successful SERP . A practical ap-proximation to detect the difference in satisfaction is the use of dwell time, either with simple thresholds such as 30 seconds, or by advanced classification models [23].

Apart from consulting the results on the SERP , users may also decide to revise the query. Query reformulations have been used as indicator of search satisfaction [1, 17, 24]. Query revision may happen both in case of successful and unsuccessful SERPs . In case of the a successful SERP , for example after interacting with some relevant results, a user may refine her query to explore a fur-ther sub-topic or aspect of the query. In want of a better term, we call this type of revisions a positive reformulation . In case of an unsuccessful SERP , our frustrated user may opt to formulate her query for example by spelling out her information need more ex-plicitly, in the hope to arrive at a successful SERP . We call this type of revision a negative reformulation .

Table 1 summarizes the relation between the concept of suc-cessful and failed SERP and indicators of user satisfaction such as search abandonment, (dis)satisfied clicks, or query revisions. While the detection of failed and successful SERPs in practice is non-trivial, the conceptual analysis allows us to simply assume the existence of abstract concepts like the set of desired pages, and clear up the exact meaning of core concepts and their dependencies and consequences.
We now look in detail at the impact of changing query intent over time on the SERP , and how this affect the pair h Q, SERP i Specifically, we look at the transition between the some time point or period t i and a later time t i +1 : Assume that at t i we have a successful SERP , hence it contains at least one page satisfying the intent of query Q at that point in time. Due to a satisfaction click on a result, the ranker will reinforce the SERP  X  X  content and like present the same organic results at Many queries such as navigational requests, are very stable and resulting in a successful SERP at time t i +1 . However, there is also an important fraction of queries that has a changing intent due to something happening in the world, which may cause the SERP to become unsuccessful at time t i +1 .

This requires detecting when a SERP becomes out of sync due to changes in the query intent. There are of course subtle changes in query intent over time, leading to small changes in the click dis-tribution with the SERP for a Q as studied in previous work for updating rankers. But these do not lead to an unsuccessful as defined in the paper. Hence we aim to distinguish cases where the desired page is not part of the SERP , at least not part of top n of the organic ranking (e.g., the top 10 results).

There are cases when users are looking for a desired page that does not exist, either no longer exists or was not created or up-dated yet. For example, a newly created page with a winner or an outcome of an election, users are looking for the next version of iPhone, etc. Although even in these cases, there is usually a surro-gate desired page that explains that the page doesn X  X  exist, and may inform or speculate on the time when the information will become available.
Our analysis leads to the following scenario when user satisfac-tion at time t i with h Q, SERP i turn into user frustration at with the same h Q, SERP i . In other words we aim to detect situa-tions when at time t i users were satisfied with a pair h Q, SERP i and at some moment in time t i +1 users are no longer satisfied with the same pair h Q, SERP i , due to changes in the query intent for example due to some event happening in world.

We consider the following types of behavior BF j on the SERP as a sign of user frustration (lack of search satisfaction) with the SERP :
The intuition of our approach is that, over a population of users issuing a query, if we see a sufficient amount of negative reformula-tions, DSAT and low-ranked clicks, and bad search abandonment, then we flag the SERP as failed , and use information about the ultimately clicked page to update the SERP for the original query  X  X ence avoid failure for future requests.

In order to satisfy a requirement about sufficient number we use the phenomenon of concept drift [15, 30, 36]. The real concept drift refers to changes in the conditional distribution of the output (i.e., target variable) given the input (input features), while the dis-tribution of the input may stay unchanged. For our problem we can formally define concept drift between time point t i and time point t 9 BF j : p t i ( BF j , h Q, SERP i ) 6 = p t i +1 ( BF j where p t of input variables h Q, SERP i and the target variable BF approach is explained in detail in the next section.

We not only intend to detect failure, but also to find and to inject the missing page to the SERP . In case of revisions with following SAT clicks, or low-ranked clicks, we have a clear indication of the  X  X issing X  page and boost it X  X  ranking so that it will surface on for the original query X  X  SERP for future users issuing the same query.
Summarizing, in this section, we introduced the concept of a successful and failed SERP and analyzed their behavioral conse-quences identifying indicators of success and failure. We then ana-lyzed success and failure in light of changing query intents over Figure 2: A representation of sudden and incremental types of the drifts. time, and identified an important case of SERP failure due to query intent drift. This suggests an approach to detect a failed SERP due to query intent drift by significant changes in behav-ioral indicators of failure. Our general conclusion is that more detrimental cases in terms of user satisfaction lead to larger changes in observable user behavior and hence more handles to detect them.
In this section we will study RQ2: Can we distinguish different types of SERP failure due to query intent drift (e.g., sudden, incre-mental), and when and how should we update the SERP to reflect these changes?
This section proposes a method to detect the types of changes in user satisfaction. The detection of the type of change is important because it defines a strategy to fix a failed SERP that is a result of changes in user satisfaction. We focus on two main types of the drifts: sudden and incremental, and will argue that the other types (i.e. gradual and reoccurring) can be represented as a combination of sudden and incremental types.

We use an increase in the query reformulations as a sign of user frustration with a shown SERP . The main criteria to distinguish between sudden and incremental drifts is the size of the testing win-dow: (1) the sudden drift should be detected during the short period of time and (2) the incremental drift can be characterized by much longer testing period. An example is shown in Figure 2. Moreover, we are proposing a list of secondary metrics that can be used to characterize the drifts: 1. if the drift is related to the query popularity (i.e.,  X  X ead X  or 2. if the volume of initial queries is changing a lot; 3. if search abandonment is observed frequently on the initial
Let us characterize in details the types of changes we are study-ing in this work: Sudden change. This kind of change gets the most attention in the literature [10, 12, 25, 26, 28] because they bring a most harm-ful and visible effect to user experience. This drift can be charac-terized by a growth of query popularity over a short period of time (e.g.,  X  X reaking news queries X ), as shown on the left hand side of Figure 2. In order to detect sudden drifts we use a short duration of the testing window (e.g., a couple of hours until a couple of days). Using the secondary metrics the sudden drift can be defined as: 1. the sudden drift is likely concerning more popular or  X  X ead X  2. the volume of an initial query Q is changing during the test-3. search abandonment is a frequent behavior on the initial Incremental change. This kind of change is less often studied and can be characterized by a slow change in query intent over a long period of time, as shown on the right hand side of Figure 2. An example it the reformulation of the query  X  X IKM conference X  to include the specific year or location. This drift is a more diffi-cult to detect because it does not necessary require an increasing query volume. However, changes in the fraction of query refor-mulations [24] can be used to detect incremental drift. Using the secondary metrics the incremental drift can be defined as: 1. the incremental drift is likely concerning less popular or  X  X ail X  2. the volume of initial queries is not changing much during the 3. search abandonment is a frequent behavior of initial SERPs
We identify two other types of query intent drift that can be rep-resented as a combination of sudden and incremental types. Gradual change. This is a different type of change that is pre-sented in Figure 3. It can be viewed as a combination of the sudden and (or) incremental types of changes happening over time. For ex-ample, we consider the query  X  X ovak djokovic X  (the famous tennis player) that may change its intents over time. For example, it has a drift in September 2013 on the term  X  X ianc X e X  because the ten-nis player got engaged. Therefore, SERP for the initial query is missing information about his fianc X e and users tend to reformulate because they are interested in this topic.

At some moment in time the couple celebrate their wedding and users X  interest changes again. Therefore the news about an en-gagement of famous tennis player becomes outdated and users start to reformulate the query  X  X ovak djokovic X  using the reformulation  X  X edding X  (if this information is missing from the SERP ). As a logical continuation of this story the couple have a baby and users are interested to see information about this news, etcetera. It is im-portant to note that the described changes are happening without any pattern, so some of these drifts may be sudden and some may be incremental.
 Reoccurring change. The final special case of drifts is presented in Figure 4. A specific characteristic of the the reoccurring change is that it has a regular type of behavior [31]. The example in Fig-ure 4 shows that users reformulate the query  X  X ovies premieres X  regularly according dates. It is important to note that the reoccur-ring change is a combination of the same type drifts (sudden or incremental).

It is important to track changes for queries over time in order to understand if the SERP is fixed when it is needed. A positive drift is typical for those cases where the reformulation signal is growing over time. Basically, the detected positive drift is a sign of a failing SERP (shown as  X + X  in Figure 3). A negative drift is typical for those cases where the probability to reformulate a query is decreasing dramatically (shown as  X - X  in Figure 3). The negative drift may be interpreted in the two following ways: 1. the system has reacted to the positive change first and changed 2. the system has not reacted to the positive change in the refor-A detailed algorithm how to identify if a detected drift has positive or negative signs is presented in the next section.
Let us first define formally a set of features { F j } 4 j =1 detect changes in user satisfaction with the pair h Q, SERP i reformulation signal ( RS ), (2) a search abandonment signal ( (3) a query volume signal ( VS ), and (4) an average clicked position signal ( CS ): Algorithm 1 Algorithm for Detection the Type of Drift in user SATisfaction ( DTDSAT ). We leave out variables, i.e., drif t stands for drif t Q,Q 0 ,w , for readability.
 Require: the train period t =[ t i ,t i +1 ] ; Ensure: drif t true, f alse ; 1: for { Q, Q 0 } N k =1 do 2: RS =  X  ( RS t + w ( Q, Q 0 ))  X  ( RS t ( Q, Q 0 )) 3: if | RS | e RS then 4: drif t true 5: if RS &gt; 0 then 6: drif t _ positive true 7: AS =  X  ( AS t + w ( Q ))  X  ( AS t ( Q )) 8: CS =  X  ( CS t + w ( Q ))  X  ( CS t ( Q )) 9: VS =  X  ( VS t + w ( Q ))  X  ( VS t ( Q )) 10: if ( | AS | e AS or | CS | e CS ) then 11: serp _ fail true 12: url Q getMissingT opURL () 13: drif tT ype getDriftT ype ( | w | , VS ) 14: fixSerp ( url Q ,driftType ) 15: else 16: serp _ fail false 17: end if 18: else 19: drif t _ positive false 20: end if 21: else 22: drif t false 23: end if 24: end for 25: return drif t , drif t _ positive , serp _ fail We call F 1 as a primary drift metric of drift and { F j } of secondary drift metrics. Each of them can be estimated straight-forwardly based on observed frequencies in the period: for we calculate the probability of reformulation per day, and for the period use the (observed) average over days (  X  ).

The proposed algorithm DTDSAT to discover types of changes in user satisfaction is presented in in Algorithm 1. It is a straight-forward application of the adaptive windowing algorithm from con-cept drift detection [6], which calculates a theoretically motivated threshold e for observing a significant drift based on a confidence value . We will first explain how DTDSAT works.
 DTDSAT Input. We assume that we can detect the sudden drift within a short period of time ( w 1 ) such as from three days up to two weeks. In contrast, the incremental drift is detected on a larger time slot ( w 2 ) from more than two weeks and up to one month. As the train window ( t ), we use fixed period of time for both considered types of drifts. We calculate an error thresholds: e RS , e VS using a standard method described in [6].
 DTDSAT Output. The algorithm DTDSAT returns an alarm as an output if the drift happens. Additionally, it produces an extra information about a detected drift: a sign: (1) the  X  X ositive sign X  means we need to fix SERP ; (2) the  X  X egative sign X  means users are no longer reformulating Q , so no action should be taken. A collected sequence of positive and negative drifts for Q can be used to build a dynamics of changes for Q . This dynamics may help to understand if Q has a gradual drift (e.g., Figure 3) or a reoccurring one (e.g., Figure 4) over some longer period of time (e.g., 6 months or 1 year).

The Algorithm 1 includes the following methods:
Summarizing, in this section, we studied different types of pos-sible query intent drift inspired by the literature on concept drift [15]: sudden, incremental, gradual and reoccurring. We identified relevant parameters, such as the window of change, volume or pop-ularity of queries, and relevant behavioral indicators, such as the probability of reformulation, abandonment rates, and click through rates. For the two main categories of intent drift, we define an unsu-pervised approach to detect failed SERPs caused by drift. We also showed how the detected changes can be used to improve a ranking of search results.
In this section we will study RQ3: How effective is our approach on a realistic sample of traffic of a major internet search engine?
Our experimental data comprises of massive raw and unfiltered search logs of a major commercial search engine that were col-lected during the whole 2014 year. Our audience consists of about 25 millions users per day . Our traffic consists of approximately 150 millions of queries per day . In our experimentation, we are dealing with a multilingual traffic that has at least five dominant languages.
We now describe our methodology to evaluate the quality of drifts detection algorithm.

Our algorithm is unsupervised, and detects drift in query intent based on a concept drift technique using a simple, theoretically mo-tivated threshold, needing only a single linear pass through the data. To the best of our knowledge, there is no alternative approach to detect failed SERPs that could function as a baseline. We run our algorithm of Section 4 in a simplified form on the logs. First, we choose three fixed windows of 3, 7, and 14 days rather than calcu-late the optimal window based on the threshold. Second, we use the change in the probability of a revision ( RS ) as the criterion to se-lect data. Third, we use a single threshold ( e RS ) based on with values in the range of [0 . 2 , 0 . 5] , as we did not observe major differences between the settings.
 Figure 5: A fragment of an evaluation task for the annotators. We suggest the most clicked url after a query revision.
As an approximation, we define the drift type by the size of the test window, so a three days windows size is related to a sudden drift type. However, it is important to note that a fresh intent classi-fier (based on mostly a query popularity) is already working within the search engine. Let us call it FIC . Therefore, most popular changes in query intents might picked up by FIC and SERP is already fixed for  X  X ead queries X  . As was shown in [26] this can be done within very short period of time.

For evaluation, we selected randomly about 150 examples from different batches. Therefore, in total, we selected 450 examples of drifts for test set which we use to report the final results. Each de-tected drift in our test set was evaluated by three judges and we re-port overall scores. As a evaluation metrics we use accuracy rates. In the current settings we are more interested to obtain rather pre-cise results.

In order to evaluate the obtained results we set up the following evaluation task. Every judge is supplied with the definition of sud-den and incremental types of drift. We gave to judges the following explanation for the drift labels: 1. Drift is detected:  X  X eal X  drift in users intent i.e. new target 2. Drift is detected:  X  X ew drift X  in users intent i.e. an another/new 3. Positive reformulation is detected: it signals about a shift
In this section, we limited ourselves by the varying time win-dows of detection due to patterns of sudden and incremental drifts. Figure 6 shows the frequencies of detected query reformulations over time. What we observe is that we detect the same drifts on consecutive days, but also that the revisions may disappear after a period of time. Anecdotal evidence suggests that this can be both due to another drift in query intent, for example for revisions spe-cific to events or months of the year, or due to updates of the SERP served for the original query. This supports to importance of de-tecting both positive and negative drift patterns, and also look at gradual and reoccurring drifts.

Summarizing, in this section, we ran a simplified version of our algorithm on a massive transaction log, and detected over 200,000 pairs of h Q, SERP i suspected of failing due to drifting query in-tents. We observed a reasonable accuracy of drift detection (72%) and a high accuracy of candidate URLs to be included on the of the original query. For incremental change over the longer de-tection period of 14 days, we detected failed SERPs due to query intent drift with an 80% accuracy. Under the specific conditions of the recency optimized search engine, the performance for detecting sudden change over shorter periods was less effective.
This paper investigated how the dynamic nature of web content and user intents have consequences for the SERP to be displayed for a particular query. In particular, there remain cases of failure where the organic search results on the search engine result page (
SERP ) are outdated, and no relevant result is displayed. This can be caused by temporal query intent drift, where the desired pages for a query are changing over time, and the historical transac-tion logs privilege the outdated results. Our main research question was: By analyzing behavioral dynamics at the SERP level, can we detect an important class of detrimental cases (such as search fail-ure) based on changes in observable behavior caused by low user satisfaction?
We presented an overview of prior work on topic and concept drift, behavioral dynamics, and user satisfaction on the web, with a special focus on the SERP level. We conducted a conceptual analysis of success and failure at the SERP level in order to an-swer our first research question: How to include the SERP into the conceptual model of behavioral dynamics on the web? How to identify (un)successful SERPs in terms of drastic changes in ob-servable user behavior? Specifically, we introduced the concept of a successful and failed SERP and analyzed their behavioral conse-quences identifying indicators of success and failure. By analyzing success and failure in light of changing query intents over time, we identified an important case of SERP failure due to query intent drift. This suggested an approach to detect a failed SERP due to query intent drift by significant changes in behavioral indicators of failure.

We continued our analysis of different types of drifts in query intent over time, answering our second research question: Can we distinguish different types of SERP failure due to query intent drift (e.g., sudden, incremental), and when and how should we update the SERP to reflect these changes? Inspired by the literature on con-cept drift [15], we studied different changes in query intent: sud-den, incremental, gradual and reoccurring, and identified relevant parameters, such as the window of change, volume or popularity of queries, and relevant behavioral indicators, such as the probability of reformulation, abandonment rates, and click through rates. For the two main categories of intent drift, we define an unsupervised approach to detect failed SERPs caused by drift, requiring only a single pass through a transaction log.

Finally, we ran experiments on massive raw search logs, answer-ing our third research question: How effective is our approach on a realistic sample of traffic of a major internet search engine? We ran a simplified version of our algorithm and detected over 200,000 pairs of h Q, SERP i suspected of failing due to drifting query in-tents, observing a reasonable accuracy of drift detection (72%) and a high accuracy of candidate URLs to be included on the SERP of the original query. For incremental change over the longer de-tection period of 14 days, we detected failed SERPs due to query intent drift with an 80% accuracy, but under the specific conditions of the recency optimized search engine the performance for detect-ing sudden change over shorter periods was less effective.
As future work, we are further developing the conceptual model, and are running further offline experiments exploring further win-dow sizes, and further features of user dissatisfaction. We are also planning to do online evaluation of how the discovered drifts are useful for fixing SERPs. In addition to the unsupervised methods of this paper, we are also experimenting with tuning the optimal parameters and theshold based on behavioral features and initial results suggest futher improvements.

Real data is messy and has many intricate dependencies, such as continually changing ranking, personalization, customization and localization, and specific tools to update the ranker fast on other signals (i.e., recency ranking). This makes data-driven research a difficult enterprise, and we strongly feel that this should be coupled with theoretical and conceptual analysis. We made a first attempt at this in the current paper, where we conduct conceptual analysis to clarify the meaning of core concepts and their relations and depen-dencies. And as a conceptual model, work with an idealized model that abstracts away from other factors outside the scope of our in-terest. For example, we observed in the experimental data relatively few or popular queries as those are tackled within hours by recency ranking methods. We view the experimental part more as initial val-idation experiments, mostly used to inform the conceptual model as well as identify the most useful features in the context of real world traffic. For this reason we did not  X  X ptimize X  for the data using su-pervised methods, but collected a single set of data for three time windows and analyzed this to assess the value of the variables in the conceptual model, and to further develop our model. We strongly belief that conceptual and experimental research should go hand in hand, and without denying the value of  X  X hings that work in prac-tice X  we should put equal value on experiments that contribute to our conceptual or theoretical understanding.
 This research has been partly supported by STW and it is the part of the Context Aware Predictive Analytics (CAPA) project. [1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if [2] E. Agichtein, E. Brill, and S. T. Dumais. Improving web [3] A. Al-Maskari, M. Sanderson, and P. Clough. The [4] J. Allan. Incremental relevance feedback for information [5] A. Arampatzis and A. van Hameran. The score-distributional [6] A. Bifet and R. Gavald X . Learning from time-changing data [7] A. Chuklin and P. Serdyukov. How query extensions reflect [8] A. Chuklin and P. Serdyukov. Good abandonments in factoid [9] N. Craswell, O. Zoeter, M. J. Taylor, and B. Ramsey. An [10] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank [11] A. Diriye, R. White, G. Buscher, and S. T. Dumais. Leaving [12] A. Dong, Y. Chang, Z. Zheng, G. Mishne, J. Bai, R. Zhang, [13] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, Y. Chang, [14] H. A. Feild, J. Allan, and R. Jones. Predicting searcher [15] J. Gama, I.  X  liobait  X  e, A. Bifet, M. Pechenizkiy, and [16] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and S. T. [17] A. Hassan and R. W. White. Personalized models of search [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: user [19] S. Ieong, N. Mishra, E. Sadikov, and L. Zhang. Domain bias [20] J. Jiang, A. H. Awadallah, X. Shi, and R. W. White. [21] T. Joachims. Optimizing search engines using clickthrough [22] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. [23] Y. Kim, A. Hassan, R. W. White, and I. Zitouni. Modeling [24] J. Kiseleva, E. Crestan, R. Brigo, and R. Dittel. Modelling [25] A. Kulkarni, J. Teevan, K. M. Svore, and S. T. Dumais. [26] D. Lefortier, P. Serdyukov, and M. de Rijke. Online [27] K. Radinsky, S. Davidovich, and S. Markovitch. Predicting [28] K. Radinsky, K. Svore, S. T. Dumais, J. Teevan, A. Bocharov, [29] K. Radinsky, K. M. Svore, S. T. Dumais, M. Shokouhi, [30] J. C. Schlimmer and R. H. Granger. Beyond incremental [31] M. Shokouhi. Detecting seasonal queries by time-series [32] M. Shokouhi and K. Radinsky. Time-sensitive query [33] A. Shtok, O. Kurland, D. Carmel, F. Raiber, and [34] Y. Song, X. Shi, R. White, and A. H. Awadallah.
 [35] R. W. White and S. T. Dumais. Characterizing and [36] G. Widmer and M. Kubat. Learning in the presence of [37] Y. Yue, R. Patel, and H. Roehrig. Beyond position bias:
